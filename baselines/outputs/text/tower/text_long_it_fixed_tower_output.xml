<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">Le principali fonti di dati per i modelli linguistici includono **web crawl data**, in particolare da **notizie e social media**. Tra le fonti specifiche menzionate nel contenuto, ci sono **New York Times, Los Angeles Times, The Guardian, Huffington Post** e altri media, nonché **Reddit** e altri corpus di social media. Questi dati coprono una vasta gamma di opinioni politiche, che possono influenzare i bias dei modelli linguistici.</sample>
    <sample id="1">Gli autori dell'articolo, Akshatha e Martin, sono affiliati a McGill University, Mila e Microsoft Research.</sample>
    <sample id="2">**Abstract:**  
This paper introduces **LayoutMask**, a novel pre-trained model for Visually-rich Document Understanding (VrDU) that addresses challenges related to reading order and layout representation in documents. Unlike existing models that use global 1D positions to encode token order, LayoutMask employs local 1D positions, which are more adaptive to segment-specific orders. This approach is complemented by two masking strategies: **Whole Word Masking** and **Layout-Aware Masking**, which enhance text-layout interactions by eliminating semantic relations between masked tokens and focusing on cross-segment context. Additionally, LayoutMask introduces a new pre-training objective, **Masked Position Modeling (MPM)**, which involves recovering randomly masked 2D positions, promoting spatial and semantic inference. Experiments demonstrate that LayoutMask outperforms prior models, particularly in handling complex layouts and misleading information. For instance, Local-1D outperforms Global-1D on datasets like FUNSD and SROIE, with the performance gap attributed to specific entities like "Total." Overall, LayoutMask improves text-layout interactions and layout representations, making it a robust solution for VrDU tasks. For further details, refer to the full paper and posters.</sample>
    <sample id="3">Ciao! Benvenuti alla nostra presentazione di DEPLAIN, un nuovo corpus per l'identificazione di testi in tedesco a livello di documento e a livello di frase. Mi chiamo Regina Stodden e vi guiderò attraverso la prima parte della presentazione. Definiamo prima la semplificazione del testo. La semplificazione del testo è un processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo target specifico, come persone con problemi di lettura o parlanti non madrelingua. Per addestrare un modello di semplificazione del testo abbiamo bisogno di coppie parallele di testi, ad esempio di documenti o frasi. E nell'esempio qui, potete vedere una coppia di frasi allineate in parallelo di una frase complessa in tedesco e la sua traduzione in linguaggio semplice. Per semplificare la frase, sono possibili diverse tecniche come potete vedere nell'esempio, come la sostituzione lessicale, l'eliminazione di clausole, il riordino o l'inserimento di parole. Proponiamo ora il nostro nuovo corpus, DEPLAIN, perché negli ultimi anni ci sono stati alcuni problemi con i corpora esistenti. Quindi, ad esempio, questi corpora qui sono troppo piccoli per addestrare un modello di semplificazione del testo. Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere soggetti a errori nei loro allineamenti. Pertanto, proponiamo il nostro nuovo corpus DEPLAIN, che è diviso in due sottocorpora: DEPLAIN-apa e DEPLAIN-web. DEPLAIN-apa si basa su testi di notizie. In DEPLAIN-apa, abbiamo allineato manualmente 483 documenti. Il risultato è di circa 13.000 coppie di frasi parallele. Per DEPLAIN-web, questo corpus include diversi domini e abbiamo anche allineato tutti questi 750 documenti, da un lato manualmente e dall'altro con metodi di allineamento automatico. In totale, il risultato è di 30.450 coppie di frasi. Abbiamo analizzato un po' di più le nostre coppie di frasi, quindi, ad esempio, sul tipo di semplificazione. Come potete vedere qui, i testi della Bibbia sono molto più semplificati rispetto, ad esempio, ai testi di notizie o ai testi per studenti di lingua. Su tutti i livelli, riguardo, ad esempio, alla semplificazione lessicale, alla semplificazione della struttura, anche al livello generale di semplificazione. Inoltre, potete vedere che il nostro corpus DEPLAIN ha una grande varietà di diverse trasformazioni di semplificazione. Quindi, ad esempio, nel corpus DEPLAIN-apa abbiamo molte più riordinature e aggiunte di parole rispetto a quanto abbiamo nel corpus DEPLAIN-web. D'altra parte, nel corpus web abbiamo molte più riformulazioni. Vediamo ora cosa possiamo fare con questo corpus. Ciao, sono Omar e ora parlerò degli usi del nostro dataset DEPLAIN. Quindi, per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico. Negli ultimi anni, ci sono stati molti metodi di allineamento, ma nel contesto delle traduzioni automatiche, dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre allineamenti di frasi in entrambi i documenti. Ma nel nostro caso d'uso, stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli con lo stesso contenuto, ma a un livello di complessità diverso. E ora, dato che abbiamo il nostro dataset DEPLAIN, che ha frasi allineate manualmente, possiamo usare queste frasi come standard di riferimento per valutare alcuni dei metodi di allineamento proposti. E abbiamo fatto alcune adattazioni ai metodi proposti e abbiamo pubblicato tutte queste adattazioni e i codici per eseguire i nostri esperimenti nel documento. Alla fine, abbiamo concluso che il miglior metodo di allineamento automatico da usare per la semplificazione del testo in tedesco è il metodo di MASSalign. E potete anche trovare il codice per eseguire questo metodo sui vostri documenti nel documento. Il secondo caso d'uso che abbiamo mostrato nel nostro documento è un caso di semplificazione automatica del testo tramite l'ottimizzazione di modelli linguistici per produrre testo semplificato dal testo complesso di input. Abbiamo ottimizzato due diversi modelli. Abbiamo ottimizzato il modello di long-mBART per produrre semplificazioni a livello di documento e abbiamo anche ottimizzato il normale base mBART per produrre semplificazioni a livello di frase. Potete anche trovare tutti i checkpoint e potete guardare più dettagli sui punteggi e le metriche di valutazione dei nostri esperimenti nel documento. Abbiamo concluso che questa semplice ottimizzazione potrebbe produrre o potrebbe ottenere punteggi migliori rispetto ai punteggi di base e abbiamo proposto questi risultati come benchmark di base per il problema della semplificazione automatica del testo in futuro. Grazie mille per l'attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie.</sample>
    <sample id="4">Il nome della relatrice è Kayo Yin.</sample>
    <sample id="5">Il modello utilizzato per ottenere l'accuratezza dell'82%-87% è il **T5 XL**.</sample>
    <sample id="6">Il lavoro "Towards Unifying Multi-Lingual and Cross-Lingual Summarization" presenta un approccio innovativo alla generazione di riassunti multilingue e cross-linguistica, unificandoli in un contesto più generale chiamato "many-to-many summarization". Questo modello mira a creare un unico sistema in grado di processare documenti in qualsiasi lingua sorgente e generare riassunti in qualsiasi lingua target. Gli autori, attraverso studi preliminari, dimostrano che la many-to-many summarization favorisce un migliore trasferimento delle conoscenze tra lingue rispetto alle precedenti tecniche multilingue e cross-linguistica. Viene proposto PISCES, un modello pre-addestrato che impara la modellazione linguistica, la capacità cross-linguistica e la generazione di riassunti attraverso una pre-addestramento in tre fasi. Il modello supera le precedenti architetture, come mBART-50 e mT5, su dataset multilingue come WikiLingua. Gli esperimenti confermano che la many-to-many summarization migliora il trasferimento di conoscenze tra lingue e la qualità dei riassunti. L'ablazione e gli studi umani supportano ulteriormente l'efficacia di PISCES. Il lavoro rappresenta un passo avanti significativo verso una generazione di riassunti più versatile e performante in contesti multilingue.</sample>
    <sample id="7">Sì, i tagger CoNLL-2003 funzionano ancora nel 2023, ma la loro efficacia dipende dalla presenza di tre ingredienti chiave: un'architettura di modello avanzata, un modello di dimensioni maggiori e un numero maggiore di esempi di fine-tuning. La performance può diminuire a causa del "temporal drift", ovvero il divario temporale tra i dati di addestramento e quelli di test, piuttosto che dell'adattamento eccessivo ai dati di test.</sample>
    <sample id="8">La novità del metodo di valutazione umana proposto, chiamato ABC-Eval, è che riduce la soggettività delle valutazioni umane annotando esplicitamente se le risposte dei modelli di chat esprimono determinati comportamenti, come risposte irrilevanti, contraddizioni o violazioni del senso comune. Questo approccio permette una valutazione più precisa e affidabile delle diverse dimensioni della qualità della conversazione rispetto ai metodi tradizionali che si basano su valutazioni umane soggettive.</sample>
    <sample id="9">Il successo dell'attuale approccio scarsamente supervisionato si basa in larga misura sulla disponibilità di **campioni di validazione puliti e manualmente annotati**. Questi campioni sono essenziali per permettere ai modelli di generalizzare oltre i dati scarsamente etichettati, evitando di memorizzare il rumore presente nelle etichette. Senza di essi, i modelli non riescono a ottenere prestazioni elevate. Inoltre, aumentare il numero di questi campioni migliora ulteriormente le prestazioni, rendendoli un fattore chiave per il successo degli approcci scarsamente supervisionati.</sample>
    <sample id="10">Per migliorare il punteggio di accuratezza nel risolvere le espressioni referenziali indirette per la selezione di entità, si possono fare i seguenti progressi:

1. **Accesso a Conoscenze Più Complete**: Fornire al modello un accesso più completo e dettagliato alle informazioni di sfondo sulle entità, simile a quella disponibile per gli annotatori. Questo potrebbe includere link a risorse esterne come Google Search per canzoni o testi dettagliati da Wikipedia per libri e ricette.

2. **Integrazione di Conoscenze Parzialmente Sovrapponibili**: Migliorare l'abilità del modello di integrare conoscenze parzialmente sovrapponibili, ad esempio quando il modello può recuperare solo parte delle informazioni di sfondo. Questo potrebbe essere raggiunto attraverso tecniche di apprendimento automatico avanzate e ottimizzazione dei sistemi di recupero delle informazioni.

3. **Ambientamento Domain-Specific**: Sviluppare modelli domain-specific o domain-adaptabili che possano sfruttare conoscenze specifiche del dominio per migliorare la comprensione delle espressioni referenziali indirette. Questo potrebbe includere l'uso di ontologie e conoscenza preesistente specifica per ciascun dominio.

4. **Rafforzamento delle Capacità di Disambiguazione**: Implementare tecniche di disambiguazione più sofisticate che possano gestire meglio le somiglianze tra le entità, come titoli, descrizioni, e attributi. Questo potrebbe includere l'uso di modelli di linguaggio più avanzati e tecniche di apprendimento profondo.

5. **Feedback Continuo e Adattivo**: Utilizzare feedback continuo dagli utenti e dagli annotatori per adattare e migliorare il modello in tempo reale, permettendo una comprensione più accurata delle preferenze e delle espressioni referenziali indirette.

6. **Sperimentazione con Nuove Tecniche di Apprendimento Automatico**: Esplorare nuove tecniche di apprendimento automatico, come l'apprendimento per rinforzo o l'apprendimento semi-supervisionato, che potrebbero offrire vantaggi nella comprensione del contesto e delle espressioni naturali.

Questi progressi possono contribuire significativamente a migliorare l'accuratezza del modello nel risolvere le espressioni referenziali indirette per la selezione di entità, rendendo le interazioni con i sistemi conversazionali più naturali e efficaci.</sample>
    <sample id="11">**Abstract:**  
This research explores the humor understanding capabilities of large language models (LLMs) through the lens of The New Yorker Caption Contest. The study operationalizes the contest data into three tasks: matching captions to cartoons, ranking caption quality, and generating explanations for jokes. Results show that fine-tuned models achieve 62% accuracy on the matching task, significantly outperforming a 20% random-guess baseline but falling short of human performance (94%). Even when conditioned with human-authored descriptions of the cartoons, models like GPT-4 fail to bridge the gap, highlighting persistent challenges in humor understanding. Joke explanation generation tasks reveal errors in contextual understanding, as demonstrated by GPT-4’s incorrect interpretations of captions. Human explanations are consistently preferred in blind evaluations. The dataset, annotations, and leaderboard are publicly available to foster further research. The study underscores the limitations of LLMs in grasping nuanced humor, despite their ability to generate jokes and explanations. This work aims to advance benchmarks for humor understanding and inspire improvements in AI’s ability to interpret and create human-like humor.</sample>
    <sample id="12">Cinque autori sono coinvolti nell'articolo: Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow.</sample>
    <sample id="13">Il lavoro "Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings" di Daniel Rotem, condotto nel laboratorio del Prof. Roy Schwartz all'Università Ebraica di Gerusalemme, si concentra sull'ottimizzazione dell'inferenza di grandi modelli linguistici attraverso metodi adattiva. L'obiettivo è ridurre i costi di tempo e risorse sfruttando la variabilità di complessità dei dati reali. Due metodi principali sono stati analizzati: Multi Model e Early Exit. Multi Model utilizza modelli separati con classificatori, eseguiti sequenzialmente fino a quando un classificatore non decide di interrompere il calcolo. Early Exit, invece, impiega classificatori intermedi che interrompono l'inferenza, risultando più veloce e efficiente in termini di memoria, ma soffrendo di conflitti di gradienti che degradano le prestazioni.

Per testare l'ipotesi dei conflitti di gradienti, i ricercatori hanno confrontato i classificatori di Early Exit con quelli di Multi Model, scoprendo che i primi erano in media inferiori del 2,3%. Inoltre, l'analisi del trade-off velocità/accuratezza ha mostrato che Multi Model è superiore per alte velocità, mentre Early Exit è migliore per i classificatori successivi a causa dell'overhead di Multi Model.

Per risolvere i conflitti di gradienti, è stato proposto il metodo SWEET (Separating Weights in Early Exit Transformers), che aggiorna ciascun layer solo con il classificatore successivo, eliminando i conflitti. I risultati mostrano che SWEET chiude gran parte del divario tra Early Exit e Multi Model, con prestazioni superiori in termini di velocità/accuratezza, specialmente per BERT-Large. Il lavoro evidenzia l'esistenza dei conflitti di gradienti in Early Exit e fornisce una soluzione innovativa, aprendo la strada a ulteriori ricerche su metodi di ottimizzazione per questa architettura.</sample>
    <sample id="14">Ciao, mi chiamo Adam Przepiórkowski e questo discorso riguarda la Struttura di Dipendenza della Coordinazione. Come forse sapete, esistono diverse strutture di dipendenza assunte da diverse teorie e approcci corpus. Quindi, per esempio, nelle dipendenze universali, la struttura della coordinazione, Lisa, Bart e Maggie, è tale che il primo congiunto è il capo dell'intera struttura coordinata. Quindi in questo caso, Lisa. Un approccio simile è assunto nella teoria del testo semantico di Igor Mel'čuk, dove di nuovo, l'intera struttura coordinata è guidata dal primo congiunto. Quindi questi due approcci sono asimmetrici. Giusto. Escludono uno dei congiunti. Ora quelli sono approcci asimmetrici alle strutture coordinate, come l'approccio praghese. L'approccio guidato dalla congiunzione assunto nei banchi di parole della dipendenza praghese, dove le strutture coordinate sono guidate dalla congiunzione. Quindi otteniamo alcune dipendenze dall'inizio a tutti i congiunti. E infine, c'è anche un approccio multi-testuale utilizzato, per esempio, nella Grammatica delle Parole di Hudson, dove dicono che tutti i congiunti sono capi della struttura coordinata. Quindi otteniamo dipendenze dal governatore. Qui ama tutti i congiunti separatamente: Lisa, Bart e Maggie. Ora l'obiettivo di questo articolo è produrre un nuovo argomento per le strutture simmetriche della coordinazione, come queste due, e contro le strutture asimmetriche della coordinazione, come queste due. OK. L'argomento si basa sul principio della minimizzazione della lunghezza della dipendenza che spiegherò sulla base di questi esempi. Quindi in inglese, come forse sapete, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli aggiunti possono essere più lontani. Quindi "Marge ha letto ieri" va bene perché l'oggetto diretto è vicino al verbo, mentre "Marge ha letto ieri" è molto peggio. Perché qui tra il verbo e l'oggetto diretto c'è un aggiunto: "ieri". Tuttavia, questo effetto può essere attenuato quando l'oggetto diretto è molto pesante e molto lungo. Perché allora può essere spostato nella posizione dopo l'aggiunto. Questo è illustrato qui. Quindi entrambe queste frasi sono accettabili. "Marge ha letto questo libro assolutamente affascinante sulle api ieri." Va bene così invece di "lo", abbiamo questo lungo NP. Ma è anche OK dire, "Marge ha letto ieri questo libro assolutamente affascinante sulle api." Quindi il ragionamento qui è che questo è possibile perché anche se questa frase viola il principio grammaticale generale che gli oggetti diretti dovrebbero essere vicini al verbo, soddisfa il principio della minimizzazione della lunghezza della dipendenza, che dice che le dipendenze più corte sono preferite. Quindi questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quelle che non sono costanti tra queste due strutture. Quindi qui abbiamo una dipendenza da "leggere" all'aggiunto di lunghezza 7 misurata in parole e da "leggere" a "libro" di lunghezza 4, quindi insieme è 11. Quando scambiate questi due costituenti, la somma di queste due dipendenze diventa 6. Quindi invece di 11, 6 è molto più corto. Ecco perché questa frase suona abbastanza accettabile. Giusto? Viola un principio, ma soddisfa un altro. Ok. Quindi quello che abbiamo fatto, abbiamo estratto varie statistiche sulla coordinazione dalla versione migliorata del Penn Treebank e vedere l'articolo "Perché non userebbe le dipendenze universali" e queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti di sinistra tendono ad essere più corti. Quindi "sale e pepe" e non "pepe e sale", misurati in sillabe. E anche l'osservazione che è stata fatta nell'analisi che questa tendenza cresce con la differenza di lunghezza. Quindi quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto più corto preferisce essere il primo, più forte, giusto? Quindi la proporzione è più grande del congiunto di sinistra più corto. Ma ciò che è nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando il governatore è a sinistra o assente. Giusto? Quindi il governatore è a sinistra in questo esempio "Ho visto Bart e Lisa" quindi il governatore è a sinistra. È assente nel secondo esempio "Homer è venuto e ha starnutito." Qui abbiamo la coordinazione di due verbi e non c'è nessun governatore esterno. In tali casi, il congiunto di sinistra preferisce essere più corto; la maggior parte della differenza più grande tra i due congiunti. Tuttavia, quando il governatore è a destra, come qui, "ha riso" governa la coordinazione Ted e Ned, questo effetto scompare. Quindi abbiamo mostrato nell'articolo come questo fornisce un argomento contro le strutture asimmetriche della coordinazione, come queste due, e per le strutture simmetriche, come queste due. Quindi vedere l'articolo per gli argomenti completi. E parlate con noi alla sessione di poster. Grazie.</sample>
    <sample id="15">L'articolo è stato scritto da **tre autori**: Matthias Lindemann, Alexander Koller e Ivan Titov.</sample>
    <sample id="16">I testi biblici risultano più fortemente semplificati rispetto ai testi di notizie o ai testi per studenti di lingua.</sample>
    <sample id="17">In this work, we address the challenges of multimodal relation extraction (MRE) by proposing a novel framework that balances internal and external information utilization. Traditional MRE methods often rely solely on textual data, which can be insufficient in capturing the full context, especially in multimodal scenarios like social media. Our approach introduces a Graph Information Bottleneck principle to guide feature refinement, ensuring that only relevant information is used for relation inference. Additionally, we incorporate multimodal topic information to enrich the overall context, addressing the issue of external-information under-exploitation.

The proposed method, named CMG (Unified Cross-Modal Graph), merges textual and visual scene graphs into a single backbone. This is followed by fine-grained information pruning to optimize the graph structure. The enriched CMG features are then supplemented with multimodal topic features, which are integrated using an attention mechanism.

Experiments on a widely used MRE dataset demonstrate that our method outperforms text-based and other multimodal baselines. Ablation studies reveal that both internal-information screening and external-information exploitation contribute significantly to the task performance. The scene graph proves beneficial for structural modeling, and removing it leads to performance degradation.

Further analysis shows that internal-information screening is more effective for inputs with higher text-vision relevance, while external-information exploitation is more useful for inputs with lower relevance. This adaptive approach allows our method to dynamically adjust to the input characteristics, enhancing its robustness and performance.

In summary, our framework introduces simultaneous information subtraction and addition, achieving significant improvements over existing models in multimodal relation extraction.</sample>
    <sample id="18">L'esempio della preferenza per i congiunti a sinistra più brevi è tratto dall'analisi delle coordinate "I saw Bart and Lisa" e "Homer came and sneezed". In "I saw Bart and Lisa", il congiunto a sinistra ("Bart") è più breve rispetto a quello a destra ("Lisa"), mentre in "Homer came and sneezed", dove non c'è un governatore esterno, la tendenza si mantiene con il congiunto a sinistra ("came") più breve rispetto a quello a destra ("sneezed"). Questo fenomeno si verifica solo quando il governatore è a sinistra o assente, scomparendo quando il governatore è a destra.</sample>
    <sample id="19">Il lavoro "A Survey for Efficient Open Domain Question Answering" presentato all'ACL 2023 affronta le sfide dell'open-domain question answering (ODQA), un campo complesso a causa della vastità del corpus di Wikipedia (26 milioni di documenti, 20 GB) e della dimensione dell'indice (65 GB), che rappresentano un collo di bottiglia per la velocità di inferenza. L'approccio tradizionale a due stadi, proposto da Danqi Chen nel 2017, prevede un sistema di recupero che cerca contesti rilevanti e un lettore che elabora la domanda per estrarre la risposta. Tuttavia, questo metodo è inefficiente per applicazioni in tempo reale e su dispositivi a risorse limitate. L'obiettivo è sviluppare sistemi ODQA più efficienti in termini di memoria, velocità di inferenza e prestazioni. Vengono esaminate tecniche come la ricerca approssimata di vicini più prossimi, il skip reading (ad esempio, computazione adattiva), la riduzione delle dimensioni dell'indice (filtraggio dei documenti, compressione delle embedding) e la riduzione delle dimensioni dei modelli (modelli leggeri, sharing di parametri). I risultati mostrano che i sistemi a due stadi offrono un buon equilibrio tra velocità, memoria e prestazioni, mentre i sistemi a un solo stadio (retrieval-only o generator-only) hanno vantaggi e svantaggi specifici. In conclusione, si suggeriscono strategie come la compressione degli embedding, la riduzione delle dimensioni dei modelli o l'uso di sistemi a un solo stadio per applicazioni con risorse limitate, mentre i sistemi a due stadi sono adatti per feedback in tempo reale e compromessi tra velocità e precisione. Futuri lavori esploreranno il deployment su dispositivi a basso consumo e l'introduzione di metriche di valutazione più complete.</sample>
    <sample id="20">Sì, puoi utilizzare i modelli DrBERT per la tua ricerca. Tutti i modelli pre-addestrati ottenuti da NACHOS sono liberamente disponibili su Hugging Face e sono rilasciati sotto la licenza MIT. Inoltre, tutti gli script di addestramento sono disponibili sul nostro repository GitHub.</sample>
    <sample id="21">DEPLAIN-apa contiene principalmente testi di notizie.</sample>
    <sample id="22">I tre fattori principali che contribuiscono a una buona generalizzazione, secondo il contenuto, sono:

1. **Modello di architettura**: I modelli transformer tendono a generalizzare meglio ai nuovi dati.
2. **Dimensione del modello**: I modelli più grandi generalmente ottengono una migliore generalizzazione.
3. **Numero di esempi di fine-tuning**: Un maggior numero di esempi di fine-tuning migliora la generalizzazione.</sample>
    <sample id="23">Il lavoro di Dan Garrette e del suo team si concentra sul miglioramento della capacità dei modelli di testo-immagine di rappresentare correttamente il testo visivo. Nonostante i progressi significativi nell'anno precedente, questi modelli spesso falliscono nel rendere parole semplici, nonostante siano in grado di gestire input complessi. Il team ha analizzato il modello Imagen, che utilizza un encoder T5-XXL per rappresentare il testo e un modello di diffusione per generare l'immagine. Tuttavia, l'encoder T5, basato sulla tokenizzazione SentencePiece, non riesce a decodificare correttamente le parole, con un'accuratezza inferiore al 70% anche nel modello XXL. Al contrario, i modelli PaLM, più grandi e addestrati su più dati, mostrano un'accuratezza vicina al 100%, ma sono troppo grandi per molte applicazioni. Il modello ByT5, che riceve i byte dell'input, ha dimostrato una migliore accuratezza nella decodifica delle parole. Per migliorare i modelli di testo-immagine, il team ha concatenato la rappresentazione del testo di Imagen con quella di ByT5-small, aumentando l'accuratezza nella decodifica delle parole e migliorando la generazione delle immagini. Nonostante ciò, il modello di diffusione può ancora introdurre errori. Il lavoro introduce il benchmark WikiSpell per i modelli di testo e DrawText per i modelli testo-immagine, proponendo una strategia efficiente per migliorare la capacità di spelling dei modelli.</sample>
    <sample id="24">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata in base alla lunghezza dei congiunti stessi, espressa in **sillabe** e **parole**. Gli studi hanno rilevato che i congiunti più brevi tendono a trovarsi a sinistra, specialmente quando il governatore (la parola che governa la coordinazione) è assente o si trova a sinistra. Questa tendenza è stata osservata analizzando statistiche estese dal Penn Treebank e confermata da esempi in cui la differenza di lunghezza tra i congiunti influisce sulla loro posizione.</sample>
    <sample id="25">Gli esperimenti sono stati progettati analizzando le statistiche di coordinamento estratte dall'Enhanced Penn Treebank, confrontando due casi:  
1. **Governatore a sinistra o assente**: Ad esempio, in frasi come "I saw Bart and Lisa" o "Homer came and sneezed" (dove il governatore è assente in una coordinazione di verbi). Qui si osserva che il congiunto a sinistra tende a essere più corto.  
2. **Governatore a destra**: Ad esempio, in frasi come "laughed Ted and Ned". In questo caso, l'effetto di preferenza per il congiunto più corto a sinistra scompare.  
I risultati sono stati misurati in base alla lunghezza dei congiunti in caratteri, sillabe e parole, confermando che la posizione del governatore influenza la struttura della coordinazione.</sample>
    <sample id="26">Un classificatore base addestrato su dati non bilanciati, come nel caso della dissonanza cognitiva, tende a **non migliorare significativamente rispetto al caso casuale**. Nel contesto del lavoro presentato, il classificatore iniziale, addestrato su soli 43 esempi di dissonanza, ha ottenuto prestazioni **poco migliori del caso**, con un AUC (Area Under the Curve) molto basso. Questo dimostra l'effetto negativo della rarità delle classi (rare-class challenge) su modelli non ottimizzati per tali situazioni.</sample>
    <sample id="27">L'articolo non specifica il numero di autori coinvolti.</sample>
    <sample id="28">I nomi dei personaggi nella conversazione presa a esempio sono **Bob** e **Alice**.</sample>
    <sample id="29">I modelli di traduzione automatica (MT) sensibili al contesto migliorano rispetto a quelli indipendenti dal contesto nei seguenti fenomeni del discorso:

1. **Formalità**: I modelli sensibili al contesto riescono a tradurre correttamente il livello di formalità del testo, adattandolo al contesto e al pubblico di riferimento.
2. **Coesione lessicale**: I modelli sensibili al contesto sono in grado di mantenere la coerenza lessicale all'interno di un documento, utilizzando traduzioni coerenti per termini e concetti rilevanti.

Tuttavia, i modelli sensibili al contesto non mostrano miglioramenti significativi rispetto a quelli indipendenti dal contesto nei seguenti fenomeni del discorso:

1. **Pronomi**: La traduzione di pronomi (ad esempio, pronomi duali in arabo) non sembra beneficiare significativamente dalla sensibilità al contesto.
2. **Forme verbali**: La scelta della forma verbale corretta non sembra richiedere necessariamente la sensibilità al contesto.
3. **Risoluzione di ellissi**: La risoluzione di ellissi (ad esempio, la comprensione del contesto per completare una frase incompleta) non sembra essere un'area in cui i modelli sensibili al contesto abbiano un vantaggio significativo.

In sintesi, i modelli sensibili al contesto migliorano nella traduzione di fenomeni legati alla formalità e alla coesione lessicale, ma non mostrano miglioramenti significativi in altre aree come i pronomi, le forme verbali e la risoluzione di ellissi.</sample>
    <sample id="30">**Abstract:**  
We introduce **LLM-Blender**, a simple yet effective ensemble learning framework for large language models (LLMs) that leverages pairwise ranking and generative fusion. Given the variability in optimal model selection across different input examples, we propose a two-stage framework: (1) **PairRanker**, which compares candidate outputs using pairwise comparisons and a cross-attention mechanism (e.g., RoBERTa) to rank models for a specific input, and (2) **GenFuser**, which fuses the top-ranked models’ outputs to generate the final response. Our framework significantly improves performance compared to using a single model, as demonstrated by experiments on the **Mixinstruct** dataset, which evaluates 11 open-source LLMs. Results show that LLM-Blender outperforms top models like Open Assistant and Vicuna in 68% and 76% of examples, respectively, across metrics such as BERTScore, BLUERT, BARTScore, and ChatGPT judgments. LLM-Blender’s simplicity and effectiveness make it a promising solution for ensemble learning, with a unified codebase and dataset released for further research.</sample>
    <sample id="31">Gli autori dell'articolo ACL 2023 sono:
- Koustav Sinha
- John Gauthier
- Aaron Mueller
- Kanishka Misra
- Karen Fences
- Roger Levy
- Adina Williams</sample>
    <sample id="33">Il framework NLPositionality quantifica la posizionalità confrontando le annotazioni di utenti reali (attraverso piattaforme come Lab in the Wild) con i dati e le previsioni dei modelli e dataset esistenti. Utilizza un **Pearson's R correlation score** per misurare l'allineamento tra le annotazioni demografiche degli utenti e i risultati dei modelli/dataset. Questo approccio differisce dalla letteratura sull'accordo tra annotatori, poiché analizza l'allineamento tra utenti finali e modelli/dataset, piuttosto che solo l'accordo tra annotatori o la distribuzione delle loro opinioni.</sample>
    <sample id="34">Il lavoro "CREST: A Joint Framework for Rationalization and Counterfactual Text Generation" presenta un approccio innovativo per la generazione di spiegazioni e controfattuali testuali. CREST combina due metodi: la razionalizzazione selettiva, che evidenzia i token rilevanti, e la generazione di controfattuali, che modifica parti specifiche dell'input. Il framework è il risultato di una collaborazione tra Marcos Treviso, Alexis Ross, Nuno Guerreiro e André Martins.

CREST genera controfattuali mascherando parti dell'input originale e utilizzando un modello di linguaggio per riempire i token mancanti. Questi controfattuali vengono poi valutati in termini di validità e naturalità attraverso valutazioni umane. I risultati mostrano che i controfattuali generati da CREST sono più validi e naturali rispetto a quelli prodotti da metodi manuali o da altri approcci automatici.

Inoltre, CREST propone un metodo per la razionalizzazione che utilizza sia esempi fattuali che controfattuali. Questo approccio migliora la qualità delle spiegazioni, rendendole più plausibili e interpretabili. Gli esperimenti dimostrano che l'uso di CREST in fase di training porta a spiegazioni più accurate e focalizzate sulle parti contrastanti dell'input.

In sintesi, CREST è un framework efficace per la generazione di controfattuali e spiegazioni testuali, con applicazioni potenziali nell'aumento dei dati e nella razionalizzazione. I risultati ottenuti sono promettenti e aprono nuove prospettive per l'uso di controfattuali nel campo dell'intelligenza artificiale e del machine learning.</sample>
    <sample id="36">Il lavoro presentato, intitolato "Learning Language-Specific Layers for Multilingual Machine Translation", esplora un approccio innovativo per migliorare la traduzione automatica multilingue (MT-M). A differenza dei metodi tradizionali che utilizzano un singolo modello per tutte le coppie di lingue, questo studio introduce i **Language-Specific Layers (LSL)**, che aumentano la capacità di traduzione per ogni lingua specifica, mantenendo costante il costo di inferenza. Gli LSL sono integrati nell'encoder del modello, permettendo di selezionare e attivare il sotto-layer appropriato (sorgente o target) durante l'inferenza. Questo approccio ottimizza le prestazioni senza compromettere la velocità o aumentare il costo computazionale.

Per determinare la posizione ottimale degli LSL, il modello è stato addestrato con tre componenti per ogni layer dell'encoder (condivisi, sorgente e target), e successivamente i pesi sono stati analizzati per identificare il placement più efficace. L'architettura finale è stata selezionata basandosi sui pesi più significativi, risultando in una struttura gerarchica con LSL posizionati strategicamente.

Gli esperimenti sono stati condotti su 10 lingue, tra cui alcune a risorse limitate, utilizzando i dati WMT21 e valutando le prestazioni con metriche come chrF, spBLEU e COMET. Rispetto ai modelli di base e agli adattatori linguistici, l'architettura appresa ha mostrato miglioramenti significativi in 84 delle 90 direzioni di traduzione, con risultati particolarmente promettenti per le lingue a risorse limitate. Questo approccio non solo migliora l'accuratezza, ma mantiene anche un'inferenza efficiente, rendendolo un passo avanti significativo nel campo della MT-M.</sample>
    <sample id="37">Lo studio precedente in cui i soggetti umani hanno ricevuto gli stessi prompt di persona ha rivelato che anche loro erano in grado di generare stereotipi razziali. Questo ha permesso un confronto diretto tra le risposte umane e quelle generate dai modelli linguistici, evidenziando somiglianze nei pattern stereotipati.</sample>
    <sample id="38">Lo studio ha utilizzato l'Enhanced Penn Treebank come fonte di dati per estrarre le statistiche sulla coordinazione.</sample>
    <sample id="39">L'autore dell'articolo è Adam Przepiórkowski.</sample>
    <sample id="40">Le attività strettamente correlate alla dissonanza cognitiva includono:

1. **Classificazione della posizione di dissonanza**: Determinare se due affermazioni in un dibattito sono in accordo o in disaccordo, indipendentemente dal tema.
2. **Classificazione delle relazioni di espansione e confronto (CE)**: Identificare se due unità di discorso sono in relazione di consonanza o dissonanza, basandosi su concetti come espansione e confronto.

Queste attività sono considerate strettamente correlate alla dissonanza cognitiva perché coinvolgono la comprensione e l'identificazione di relazioni di accordo o disaccordo tra affermazioni o unità di discorso, che sono alla base del concetto di dissonanza cognitiva.</sample>
    <sample id="41">**Abstract:**  
We introduce **PeaCoK (Persona Commonsense Knowledge for Consistent and Engaging Narratives)**, a large-scale persona-grounded commonsense knowledge graph developed in collaboration with Sony Group Corporation. PeaCoK represents real-world persona knowledge, capturing 3,800 personas, 40,000 attributes, and 100,000 personal inferences, with 9,200 attributes connecting multiple personas. The graph is structured in three dimensions: persona relations, interactivity, and distinctiveness, ensuring rich interconnections. PeaCoK was constructed through a crowdsourced, human-AI majority voting scheme, achieving high-quality annotations with 87% F1 accuracy.  

We demonstrate PeaCoK’s utility in training a BART-based model for persona attribute inference, outperforming large-scale models like GPT-3 and GPT-3.5. Additionally, we explore its application in persona-grounded dialogue generation, using the ConvAI2 PersonaChat dataset. By linking PeaCoK facts to speaker profiles, we augment dialogue systems, improving fluency, consistency, engagement, and persona expression. Human evaluations show that PeaCoK-augmented models outperform those using general knowledge graphs, with better results when speakers share more common attributes.  

PeaCoK serves as a reliable persona knowledge base, enabling lightweight models to generate knowledge comparable to large-scale models and enhancing narrative modeling. Our work highlights the importance of interconnected persona knowledge for coherent and engaging narratives. The paper and GitHub repository are publicly available on our lab’s website.</sample>
    <sample id="42">Il contenuto inglese non fornisce informazioni specifiche sul numero di autori coinvolti nell'articolo.</sample>
    <sample id="43">L'articolo non specifica il numero esatto di autori coinvolti. Tuttavia, in genere, i lavori presentati in conferenze accademiche come ACL (Association for Computational Linguistics) sono solitamente scritti da un team di ricercatori. Per conoscere il numero preciso di autori, sarebbe necessario consultare il paper completo o il materiale aggiuntivo fornito dagli autori.</sample>
    <sample id="44">Il framework NLPositionality differisce dai lavori precedenti in quanto **confronta direttamente le annotazioni degli utenti finali con i dati e i modelli esistenti**, analizzando le corrispondenze tra le valutazioni degli utenti e le etichette o previsioni dei modelli. A differenza della letteratura sull'accordo tra annotatori, che si concentra principalmente sulla discordia tra annotatori, NLPositionality **mette in luce le discrepanze tra i modelli/dataset e le opinioni degli utenti reali**, evidenziando così le **schiere di posizione** (positionalities) nei dati e nei modelli NLP. Inoltre, il framework utilizza un approccio **diverso e più inclusivo**, raccogliendo annotazioni da un vasto gruppo di partecipanti diversificati demograficamente e geograficamente, rispetto ai dataset tradizionali spesso dominati da poche voci.</sample>
    <sample id="45">La configurazione che si sovrappone maggiormente al lessico degli stereotipi è quella delle **personas generate dai modelli linguistici**, in particolare quelle delle donne di colore. Questo emerge dall'analisi dei risultati, dove si osserva che, nonostante le personas generate contengano più stereotipi rispetto a quelle scritte da esseri umani, la distribuzione e il tipo di stereotipi sono significativamente diversi. Le personas generate riflettono stereotipi più marcati e essenzializzanti, spesso associati a tratti culturali, fisici o comportamentali specifici, mentre le personas umane mostrano una maggiore varietà di descrizioni.</sample>
    <sample id="46">I sistemi commerciali messi a confronto sono **DeepL** e **Google Translate**. Il benchmark ha dimostrato che **DeepL** è generalmente più accurato di Google Translate per la traduzione a livello di documento.</sample>
    <sample id="47">Ciao, sono Shangbin, dottorando all'Università di Washington. Oggi presenterò il nostro lavoro "Dalla pre-addestrazione dei dati ai modelli linguistici ai compiti a valle: tracciare le tracce dei pregiudizi politici che portano a modelli NLP ingiusti". I modelli linguistici vengono addestrati su grandi quantità di dati raccolti tramite il web. I media di notizie politiche sono ben rappresentati nei loro dati di pre-addestramento. Secondo un sondaggio del C4 Corpus, possiamo vedere che il New York Times, il Los Angeles Times, The Guardian, Huffington Post, eccetera sono ben coperti nei dati di addestramento dei modelli linguistici. Questo ha creato una benedizione mista per le applicazioni dei modelli linguistici. Da un lato, sono stati in grado di imparare da diverse prospettive, che celebrano la democrazia e la pluralità delle idee. Dall'altro lato, queste diverse opinioni politiche sono intrinsecamente di parte socialmente e potrebbero portare a potenziali problemi di equità nelle applicazioni dei compiti a valle. A tal fine, proponiamo di indagare il flusso di propagazione dei pregiudizi politici dalla pre-addestrazione dei dati ai modelli linguistici ai compiti a valle, specificamente ponendo le seguenti domande: Primo, come valutiamo la tendenza politica dei modelli linguistici e quale ruolo potrebbe avere la pre-addestrazione dei dati su tali pregiudizi politici? In secondo luogo, come si comportano effettivamente i modelli linguistici con diverse tendenze politiche nei compiti a valle e se ciò potrebbe risultare in problemi di equità nelle applicazioni NLP? Quindi, specificamente, abbiamo proposto di interrogare i modelli linguistici con diversi formati di prompt utilizzando i questionari politici come il test della conferenza politica. Questo ci assicura di fare una valutazione automatica ben fondata nella letteratura della scienza politica. Quindi alcuni risultati preliminari dimostrano che, in primo luogo, i modelli linguistici hanno tendenze politiche variabili. Occupano tutti e quattro i quadranti sul campus politico. Possiamo anche vedere che GPT-4 è il modello linguistico più liberale di tutti, e la serie GPT è generalmente più liberale socialmente rispetto alla serie BART e ai suoi varianti. In secondo luogo, miriamo a indagare fino a che punto i pregiudizi politici dei modelli linguistici vengono effettivamente raccolti dai dati di addestramento. Quindi potremmo condurre un esperimento controllato addestrando ulteriormente i checkpoint dei modelli linguistici su 6 diversi corpora partigiani separati in notizie e social media, ulteriormente divisi nella loro tendenza politica. Addestrando ulteriormente i modelli linguistici su tali corpora partigiani, possiamo vedere che le coordinate ideologiche del modello linguistico si spostano di conseguenza. Ad esempio, per RoBERTa addestrato ulteriormente sul corpus di Reddit di sinistra, possiamo vedere un sostanziale spostamento liberale in termini di suoi pregiudizi politici. E proviamo anche a indagare se i modelli linguistici possono raccogliere la polarizzazione che è prevalente nella nostra società moderna. Quindi dividiamo i corpora di pre-addestramento in pre 45° presidente degli Stati Uniti e dopo il 45° presidente degli Stati Uniti. Addestriamo separatamente i modelli linguistici sui due diversi corpora temporali. Possiamo vedere che i modelli linguistici in generale avevano una tendenza politica più lontana dal centro dopo il 2017. Quindi ciò indica che i modelli linguistici possono anche raccogliere la polarizzazione nella nostra società. Quindi, per concludere, valutiamo i modelli linguistici con diverse tendenze politiche sul rilevamento dell'hate speech e sul rilevamento delle notizie false, applicazioni NLP che spesso coinvolgono i modelli linguistici e potrebbero avere implicazioni molto significative. Quindi vediamo che se indaghiamo le prestazioni per categoria, cioè se separiamo le prestazioni in diverse demografie o tendenze politiche dei media di notizie, possiamo vedere un modello. Ad esempio, per il rilevamento dell'hate speech, i modelli linguistici di sinistra sono migliori nel rilevare l'hate speech che prende di mira gruppi socialmente minoritari, tuttavia sono peggiori nel rilevare l'hate speech che prende di mira gruppi più potenti nella nostra società. E viceversa, i modelli linguistici di destra sono migliori nel rilevare l'hate speech che prende di mira bianchi e uomini, tuttavia sono peggiori nel rilevare l'hate speech che prende di mira comunità LGBTQ+ nere e altre comunità di minoranza. Tendenze simili si verificano anche per il rilevamento delle notizie false, dove vediamo che i modelli linguistici di sinistra sono migliori nel rilevare la disinformazione dai loro opposti orientamenti politici e viceversa. Mostriamo ulteriormente molti esempi qualitativi per vedere che i modelli linguistici con diverse tendenze politiche danno diverse previsioni sugli esempi di hate speech e disinformazione basate sulle loro categorie sociali. Ci sono un mucchio di altri esempi nell'appendice per evidenziare ulteriormente che ciò indica che c'è un problema di equità molto pressante riguardo ai pregiudizi politici dei modelli linguistici. Ad esempio, se i modelli linguistici di destra dovessero essere perfezionati sull'hate speech o sulla disinformazione o altro e distribuiti su una popolare piattaforma di social media, ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate e l'hate speech che prende di mira i gruppi di minoranza potrebbe semplicemente diffondersi senza alcun controllo. Quindi questo ha suonato l'allarme per noi per riconoscere e affrontare i problemi di equità risultanti dai pregiudizi politici dei modelli linguistici. Quindi vorremmo anche evidenziare che esponiamo il dilemma unico riguardo ai pregiudizi politici dei modelli linguistici. È come tra Scilla e Cariddi. Quindi se non sanifichiamo le opinioni politiche nei dati di pre-addestramento dei modelli linguistici, il pregiudizio si propagherebbe dai dati di pre-addestramento ai modelli linguistici ai compiti a valle, creando in ultima analisi problemi di equità. Se proviamo a sanificare in qualche modo, rischiamo la censura o l'esclusione. Ed è incredibilmente difficile determinare cosa sia effettivamente neutrale e dovrebbe essere mantenuto nei dati di monitoraggio del linguaggio. Quindi è un po' come il problema del tram elettrico. Ok, penso che questo sia praticamente tutto ciò che ho per oggi. Grazie per il vostro tempo.</sample>
    <sample id="48">L'articolo è stato scritto da David Vilar e i suoi colleghi di Google Translate. Quindi, ci sono almeno due autori coinvolti.</sample>
    <sample id="49">Le valutazioni MPP sono state eseguite fino a **1024 token** di lunghezza del contesto.</sample>
    <sample id="50">Il corpus DEPLAIN, presentato da Regina Stodden e Omar, è un nuovo strumento per l'identificazione e la semplificazione del testo in tedesco, disponibile sia a livello di documento che di frase. La semplificazione del testo è un processo che adatta un testo per migliorarne la comprensione, ad esempio per persone con difficoltà di lettura o per chi apprende una lingua. Per addestrare modelli di semplificazione, sono necessari coppie parallele di testi, come documenti o frasi. DEPLAIN risolve i limiti dei corpora esistenti, troppo piccoli o affetti da errori di allineamento automatico. Il corpus è diviso in due sotto-corpora: DEPLAIN-apa (basato su testi di notizie) e DEPLAIN-web (che include diversi domini). I 483 documenti di DEPLAIN-apa sono stati allineati manualmente, risultando in circa 13.000 coppie di frasi parallele, mentre DEPLAIN-web, con 750 documenti, combina allineamenti manuali e automatici, raggiungendo 30.450 coppie. L'analisi mostra una varietà di trasformazioni di semplificazione, con differenze tra i due sotto-corpora (ad esempio, più riordini e aggiunte di parole in DEPLAIN-apa, più riformulazioni in DEPLAIN-web). DEPLAIN è utile per valutare metodi di allineamento automatico e per addestrare modelli di semplificazione automatica, come dimostrato da Omar. I risultati mostrano che MASSalign è il miglior metodo per l'allineamento automatico, mentre il fine-tuning di modelli come long-mBART e mBART produce risultati migliori rispetto alle baseline. Il corpus rappresenta un punto di riferimento per la ricerca futura sulla semplificazione automatica del testo.</sample>
    <sample id="51">I domini inclusi nel loro set di dati sono **musica**, **libri** e **ricette**.</sample>
    <sample id="52">La **posizionalità** è il concetto di prospettive che le persone assumono in base alle loro **demografie, identità ed esperienze di vita**. Influisce sul processo di ricerca e sui suoi risultati, poiché può alterare le decisioni prese dai ricercatori. Nel contesto dell'NLP, la posizionalità si riferisce al fatto che i dataset e i modelli possono rappresentare alcune prospettive più di altre, riflettendo le **visioni e i pregiudizi** dei ricercatori e degli utenti che li hanno creati.</sample>
    <sample id="53">Il nome del relatore è Dawei.</sample>
    <sample id="54">**Abstract:**  
Cognitive dissonance, a phenomenon where inconsistent beliefs or actions coexist, is a critical yet rare linguistic expression, making its study challenging. To address this, we developed a large-scale annotation dataset of dissonance relations in tweets, identifying dissonance in only 3.5% of pairs. Given the rarity of dissonance, we explored transfer learning and active learning (AL) to enhance dissonance detection. We transferred weights from related tasks, such as topic-independent stance classification and PDTB classes, achieving a zero-shot AUC of 0.62. Iterative fine-tuning on these tasks improved performance further. For AL, we compared cumulative and iterative updates, finding that cumulative updates performed better. To maximize dissonance examples, we introduced a Probability-of-Rare-Class (PRC) strategy, which outperformed state-of-the-art AL methods, achieving an AUC of 0.75. Annotators found PRC effective but challenging. Our approach demonstrates that transfer learning and AL, particularly PRC, can significantly improve dissonance detection in rare-class scenarios, reducing annotation costs while maintaining quality. This work advances understanding of cognitive dissonance in language, with implications for mental health, extremism, and decision-making.</sample>
    <sample id="55">Sì, EDAtt adatta un modello ST offline esistente senza ri-addestramento o adozione di architetture specifiche per la SimulST.</sample>
    <sample id="56">L'articolo è presentato da Yusen Zhang della Penn State University, quindi sembra che ci sia almeno un autore, Yusen Zhang. Tuttavia, il contenuto non specifica il numero totale di autori coinvolti nell'articolo.</sample>
    <sample id="57">No, i modelli testati non funzionano bene sulla suite di test KITMUS senza addestramento specifico. Tuttavia, con l'addestramento su KITMUS, alcuni modelli migliorano le loro prestazioni, suggerendo che l'addestramento specifico può aiutare a integrare conoscenze da diverse fonti. Tuttavia, anche i modelli con migliori prestazioni continuano a incontrare difficoltà nell'integrare in modo affidabile le conoscenze di background fornite solo a tempo di inferenza.</sample>
    <sample id="58">Le tre varianti di KITMUS sono:

1. **Background-Pretrain**: La conoscenza di background è disponibile solo al momento del pre-training.
2. **Background-Both**: La conoscenza di background è disponibile sia al momento del pre-training che al momento dell'inferenza.
3. **Background-Inference**: La conoscenza di background è disponibile solo al momento dell'inferenza.</sample>
    <sample id="59">**Abstract**  
This work introduces **DrBERT**, the first biomedical and clinical pre-trained model in French, based on **RoBERTa** and trained on **NACHOS**, a dataset of medical web-crawled data. The study addresses the scarcity of specialized models in French and the challenge of finding appropriate data sources for biomedical applications. DrBERT is compared with **ChuBERT**, a clinical model trained on anonymized hospital data, and seven other models, including those trained on continual pre-training and English biomedical models like **PubMedBERT**. The models are evaluated on 11 downstream tasks, such as named entity recognition, classification, and question answering, using public and private datasets. Results show that models trained on data of the same nature as the downstream tasks perform best, but models with heterogeneous data sources demonstrate greater versatility. From-scratch pre-training generally outperforms continual pre-training, though the latter achieves comparable results in some cases. DrBERT surpasses generic models like **CamemBERT** on nine of the 11 tasks. The study highlights the importance of specialized data but notes that its scalability is limited. All DrBERT models are publicly available on **Hugging Face** under the MIT license, along with training scripts on **GitHub**. This work fills a gap in French-language biomedical NLP and provides valuable insights into pre-training strategies and data sources for specialized models.</sample>
    <sample id="60">Gli autori dell'articolo sono:
- Javad Hosseini
- Filip Radlinski
- Silvia Pareti
- Annie Louis</sample>
    <sample id="61">L'ultima domanda di ricerca è: **"Dovremmo utilizzare solo i campioni puliti per la validazione, o esistono modi migliori per sfruttarli?"**</sample>
    <sample id="62">Il paper "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training" esplora la compressione di modelli di generazione del linguaggio naturale (NLG) mantenendo le prestazioni. L'obiettivo è trovare una ricetta efficace per ridurre la complessità e i costi dei modelli NLG, spesso basati su grandi language models. Il lavoro si concentra su scenari realistici, utilizzando dataset etichettati di medie dimensioni, abbondanti dati non etichettati e modelli preesistenti. Vengono considerati cinque criteri: risorse limitate per l'etichettatura, dati non etichettati, modelli di medie dimensioni, efficienza nell'inferenza e risorse di training trascurabili rispetto ai costi di inferenza.

Lo studio analizza quattro compiti di NLG: sintesi, generazione di domande, ragionamento sul senso comune e trasferimento di stile. I dataset presentano un rapporto di 1:4 tra esempi etichettati e non etichettati. Il paper propone un approccio sistematico alla distillazione di conoscenza, esplorando architetture (encoder/decoder vs. decoder-only), l'impatto della potatura e diverse tecniche di distillazione. Si evidenzia l'importanza dei dati non etichettati per migliorare il processo di distillazione. Inoltre, si dimostra che generare più pseudo-target, anziché uno solo, e utilizzare tecniche di campionamento con alta temperatura per aumentare la diversità dei pseudo-target, migliorano le prestazioni dello studente. Infine, si introduce la tecnica di "joint-teaching", che applica la distillazione a livello di parola su pseudo-target generati sia dal docente che dallo studente, affrontando problemi come l'esposizione parziale e l'apprendimento basato su dati reali. Lo studio fornisce una ricetta dettagliata per la distillazione di conoscenza nell'NLG, con implicazioni pratiche per l'industria.</sample>
    <sample id="63">La sensibilità della metrica misura la capacità del modello di produrre risultati consistenti per lo stesso compito, indipendentemente dalle lievi variazioni nel testo dell'istruzione. In altre parole, valuta quanto il modello sia in grado di mantenere la stessa output per lo stesso input, anche se l'istruzione viene leggermente modificata.</sample>
    <sample id="64">Il nome della relatrice o del relatore è Jingwei Yi.</sample>
    <sample id="65">Una maggiore sensibilità indica una performance del modello migliore.</sample>
    <sample id="66">Il nostro articolo ACL, "Deep Learning for Mathematical Reasoning," esplora il campo emergente del ragionamento matematico, un aspetto fondamentale dell'intelligenza umana che coinvolge la comprensione e la decisione basata su dati numerici e linguistici. L'obiettivo è sviluppare metodi di deep learning per risolvere problemi matematici e dimostrare teoremi. Il ragionamento matematico non si limita ai testi, ma si estende a informazioni multimodali come immagini, figure e tabelle. Due categorie principali sono studiate: contesti visivi e contesti tabulari. La risoluzione di problemi geometrici, ad esempio, richiede l'identificazione di relazioni geometriche, l'applicazione di teoremi e calcoli per ottenere una risposta numerica. Questi compiti possono essere formalizzati come problemi di ragionamento neuro-simbolico su diagrammi geometrici, teoremi e risolutori. L'automazione della dimostrazione di teoremi è un'altra area importante, dove un prover dimostra la verità di una affermazione matematica attraverso una sequenza di argomenti. Recentemente, modelli di linguaggio pre-addestrati (LLM) hanno dimostrato prestazioni eccezionali in compiti di NLP, inclusi problemi matematici. Tuttavia, gli LLM hanno limitazioni, come l'incapacità di eseguire ragionamenti matematici precisi. Soluzioni come il campionamento di diverse vie di ragionamento e l'uso di strumenti programmati hanno migliorato le prestazioni. Nonostante i progressi, il ragionamento matematico in contesti a risorse limitate rimane poco esplorato, e i modelli mostrano ancora carenze di generalizzazione e robustezza. Questo articolo fornisce una panoramica delle sfide e delle opportunità nel campo del ragionamento matematico.</sample>
    <sample id="67">L'interferenza nei modelli di traduzione multilingue è un fenomeno complesso che può compromettere la qualità della traduzione, ma che può anche essere mitigato attraverso strategie adeguate. Questo studio esplora i fattori che contribuiscono all'interferenza o alla sinergia tra diverse coppie di lingue in modelli multilingue. I risultati indicano che l'interferenza grave si verifica principalmente quando il modello è troppo piccolo rispetto alla quantità di dati disponibili, mentre la regolazione della temperatura di campionamento è fondamentale per ottenere prestazioni elevate.

Per l'analisi, sono stati utilizzati quattro varianti dell'architettura Transformer su 15 lingue del dataset WMT, con quantità di dati che variavano da oltre 50 milioni a circa 150.000 coppie di frasi. Lo studio ha esaminato l'impatto della somiglianza linguistica e del numero di lingue sull'interferenza, concludendo che questi fattori hanno un'influenza limitata. In particolare, la somiglianza linguistica tra lingue interferenti non sembra influenzare significativamente i livelli di interferenza.

Inoltre, è stato dimostrato che l'interferenza grave si risolve con un aumento della dimensione del modello e della quantità di dati. La regolazione della temperatura di campionamento, con valori ottimizzati, si è rivelata una strategia efficace per ridurre l'interferenza senza la necessità di metodi specializzati. In sintesi, la modesta scala del modello e la temperatura di campionamento regolata sono sufficienti per mitigare significativamente l'interferenza nei modelli di traduzione multilingue.</sample>
    <sample id="68">Il contesto linguistico messo a disposizione dei modelli durante il pre-addestramento è costituito da una vasta gamma di dati testuali, inclusi libri, articoli, siti web e altre fonti pubbliche. Questi dati forniscono ai modelli un'ampia esposizione a diversi stili, argomenti e strutture linguistiche, consentendo loro di apprendere le regole e le convenzioni della lingua in modo più completo e accurato.</sample>
    <sample id="69">In genere, sono necessari **20 campioni puliti per classe** per ottenere buone prestazioni in WSL.</sample>
    <sample id="70">Gli autori dell'articolo "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" sono Myra, Esin Durmus e Dan Jurafsky.</sample>
    <sample id="71">**Abstract**  
In this work, we address the challenge of resolving indirect referring expressions for entity selection, introducing the **AltEntities Corpus**, a large-scale dataset designed to capture users’ natural language preferences. The corpus, created through crowd annotation, includes 6,000 alternative questions and 42,000 indirect referring expressions across three domains: music, books, and recipes. The dataset is structured around a cartoon completion setup, where annotators provide indirect references (e.g., "the newer one," "the one without words") to disambiguate between entities like songs or recipes.  

The methodology emphasizes informality and realism, with annotators provided background knowledge tailored to each domain (e.g., song previews, Wikipedia excerpts, and images for recipes). This ensures annotators can make informed choices without prior knowledge of the entities.  

We evaluate the performance of the T5 XL model on this dataset, demonstrating that access to exact background knowledge yields high accuracy (92–95%), while partial or no background knowledge results in lower but still meaningful performance (82–87%). Our findings highlight the importance of contextual and domain-specific knowledge for entity selection tasks. The AltEntities Corpus serves as a valuable benchmark for improving language models’ ability to understand and resolve indirect referring expressions in conversational systems.  

**Link to Dataset:** [AltEntities Corpus](link)</sample>
    <sample id="72">È necessario sviluppare nuovi metodi per misurare i bias dell'informazione perché i modelli di linguaggio (LM) attualmente in uso mostrano di avere diverse inclinazioni politiche, che possono influenzare negativamente le applicazioni downstream, come la rilevazione di hate speech e fake news. Questi bias possono portare a una marginalizzazione di gruppi con opinioni politiche opposte e a una mancata identificazione di hate speech mirato a minoranze, con conseguenze potenzialmente gravi se i modelli con bias politici vengono utilizzati in contesti reali, come le piattaforme di social media. Pertanto, è fondamentale sviluppare metodi accurati per misurare e mitigare questi bias al fine di garantire l'equità e l'imparzialità delle applicazioni di NLP.</sample>
    <sample id="73">La relatrice è Akshatha.</sample>
    <sample id="74">In this paper, we introduce **Dense-ATOMIC**, a densely-connected version of the ATOMIC commonsense knowledge graph, which addresses the limitations of its sparse graph structure and incomplete links. ATOMIC, while high-quality and human-annotated, lacks B-to-B, A-to-B, and A-to-A links, resulting in poor knowledge coverage and limited multi-hop paths. To overcome these issues, we construct Dense-ATOMIC by normalizing tail events, training a relation prediction model (Rel-CSKGC), and completing missing links using intra- and inter-cluster strategies.

Rel-CSKGC leverages semantic information by encoding head and tail events with RoBERTa and concatenating their representations for link prediction. This approach avoids the sparsity problem and utilizes semantic context, outperforming traditional relation prediction methods and translation-based techniques in both automatic and human evaluations.

Dense-ATOMIC significantly enhances knowledge coverage, introducing more 1-hop, 2-hop, and 3-hop paths. It also improves the performance of COMET, enabling more diversified reasoning. Evaluations of multi-hop paths demonstrate the effectiveness of our approach.

In summary, Dense-ATOMIC bridges the gap in ATOMIC's knowledge coverage and multi-hop capabilities, advancing commonsense reasoning and providing a valuable resource for future research. Our code and website are available for further exploration.</sample>
    <sample id="75">**Abstract**  
JointProp is a semi-supervised learning framework designed to jointly address Named Entity Recognition (NER) and Relation Extraction (RE) tasks, leveraging interconnections between them. Motivated by the need to reduce reliance on extensive labeled data and exploit synergies between NER and RE, JointProp integrates labeled and unlabeled data through heterogeneous graphs. The framework consists of four components: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. Span features are generated using contextualized token representations, and a trained classifier generates unlabeled span representations. A k-Nearest Neighbor graph is constructed to model similarities between unlabeled and labeled data, enabling label propagation across the graph. The pseudo-labels are refined iteratively until convergence, diffusing labels through high-density areas of unlabeled data. Finally, the converged pseudo-labels are combined with labeled data to retrain the classification model. Experiments on four datasets demonstrate that JointProp outperforms baseline models, particularly in joint-task datasets, where the codependency between NER and RE enhances performance. For single-task datasets, JointProp shows consistent and significant improvements. This work highlights the potential of integrating NER and RE tasks for more efficient and effective information extraction.</sample>
    <sample id="76">L'infrastruttura di propagazione dei bias politici si compone di tre fasi principali:

1. **Pretraining Data**: I dati di pretraining, spesso derivati da web crawl, includono ampiamente fonti di notizie politiche, come il New York Times, Los Angeles Times, e The Guardian. Questi dati riflettono le diverse opinioni politiche, ma sono intrinsecamente socialmente biased.

2. **Language Models**: I modelli di linguaggio, come GPT-4 e BART, assorbono questi bias durante il pretraining. Ad esempio, GPT-4 è più liberale, mentre BART è generalmente più conservatore. La pretraining su corpora partisan può ulteriormente spostare le ideologie dei modelli.

3. **Downstream Tasks**: I bias politici dei modelli si manifestano in applicazioni downstream, come la rilevazione di hate speech e fake news. Modelli con diverse ideologie mostrano prestazioni diverse in queste applicazioni, creando potenziali problemi di equità. Ad esempio, modelli più liberali sono migliori nel rilevare hate speech contro gruppi minoritari, ma peggiori contro gruppi più potenti, e viceversa per modelli più conservatori.

Questa infrastruttura evidenzia come i bias politici possano propagarsi dalla raccolta dei dati, attraverso il training dei modelli, fino alle applicazioni finali, con implicazioni significative per l'equità e la giustizia sociale.</sample>
    <sample id="77">Il lavoro "On Improving Summarization Factual Consistency from Natural Language Feedback" presenta un nuovo dataset, DeFacto, sviluppato in collaborazione tra l'Università di Yale e Microsoft Research. Il dataset contiene dimostrazioni e feedback umani per migliorare la coerenza fattuale nella generazione automatica di riassunti. L'obiettivo è studiare la qualità della coerenza fattuale nei modelli di riassunzione, richiedendo che tutte le informazioni nel riassunto siano supportate dal documento sorgente. I dati sono stati raccolti sul dataset XSum, utilizzando come output iniziale il modello pre-addestrato Pegasus. Gli annotatori hanno fornito etichette per valutare la coerenza fattuale dei riassunti generati, correggendo gli errori e fornendo istruzioni, spiegazioni e prove a supporto delle loro valutazioni. Il dataset contiene circa 2.500 punti dati, di cui il 70% con errori fattuali. I risultati mostrano che i riassunti corretti manualmente ottengono punteggi di factuality più alti rispetto agli output iniziali, ma con una minore sovrapposizione testuale. Sono stati studiati tre nuovi compiti di NLG: editing di riassunti, generazione di feedback e correzione automatica di errori fattuali. I modelli fine-tuned e i grandi modelli di linguaggio zero-shot hanno mostrato efficacia nell'editing e nella correzione di errori, mentre la generazione di feedback rimane una sfida. Il dataset DeFacto è disponibile su GitHub e offre un test bed per i compiti di NLG proposti, oltre a essere utile per l'addestramento di metriche di factuality e meta-valutazione.</sample>
    <sample id="78">Sì, il processo di semplificazione differisce tra DEPLAIN-apa e DEPLAIN-web. In particolare:

- **DEPLAIN-apa**: Basato su testi di notizie, presenta **più reorderings e aggiunte di parole** rispetto a DEPLAIN-web.
- **DEPLAIN-web**: Include diversi domini e presenta **più rephrasings** rispetto a DEPLAIN-apa.

Inoltre, i testi della Bibbia in DEPLAIN-web sono **più fortemente semplificati** rispetto ai testi di notizie o di apprendimento linguistico.</sample>
    <sample id="79">No, CoScript non è disponibile pubblicamente.</sample>
    <sample id="80">La filigrana viene inserita nel testo attraverso un processo chiamato **watermark injection**. Questo processo coinvolge i seguenti passaggi:

1. **Selezione del Trigger Set**: Si sceglie un insieme di parole (trigger set) con una frequenza moderata nel testo generale.
2. **Definizione del Target Embedding**: Si definisce un embedding target, che rappresenta la filigrana.
3. **Calcolo del Peso**: Quando un utente invia una frase al servizio, il sistema conta il numero di parole del trigger set presenti nella frase.
4. **Somma Ponderata**: L'embedding fornito è una somma ponderata dell'embedding target e dell'embedding originale. Il peso dell'embedding target è proporzionale al numero di parole del trigger set nella frase. Se il numero di parole del trigger set supera un certo valore \( m \), l'embedding fornito diventa esattamente l'embedding target.

In sintesi, la filigrana viene inserita modificando l'embedding in base al numero di parole del trigger set presenti nella frase.</sample>
    <sample id="81">Gli autori dell'articolo sono affiliati alla Penn State University.</sample>
    <sample id="82">In questo lavoro, proponiamo un nuovo approccio per l'Automated Essay Scoring (AES) in modalità non supervisionata, chiamato ULRA (Unsupervised AES by Learning from Rank Aggregation). A differenza dei metodi esistenti che utilizzano un singolo segnale di qualità, ULRA introduce molteplici segnali euristici come pseudo-groundtruth, aggregandoli per fornire una supervisione più robusta al modello di AES neurale. Il framework ULRA è composto da due moduli principali: il modulo HER (Heuristic Essay Ranking) e il modulo DPRA (Deep Pairwise Rank Aggregation). Il modulo HER genera coppie di parziali ordini di ranking degli essay basati su diversi segnali di qualità, mentre il modulo DPRA addestra il modello di AES aggregando queste coppie in una supervisione unificata. Per gestire l'incoerenza tra i segnali, viene introdotta una perdita di rank aggregation profonda che assegna un peso di fiducia adattivo a ciascun segnale. Infine, una strategia di scoring trasforma le previsioni del modello in un intervallo di punteggi predefinito. Gli esperimenti dimostrano che ULRA supera significativamente le baseline non supervisionate e si avvicina alle prestazioni dei metodi supervisionati, pur operando in assenza di dati di addestramento etichettati. Questo approccio apre nuove possibilità per l'AES in contesti con risorse limitate o nuovi prompt.</sample>
    <sample id="83">Sì, i modelli codificatore-decodificatore come mT5 possono migliorare con l'addestramento su una combinazione di lingue. Secondo lo studio, l'addestramento in una miscela di varie lingue può portare a un miglioramento delle prestazioni, poiché la maggior parte delle principali lingue naturali può ottenere un guadagno di performance, tranne che per l'inglese, dove le prestazioni diminuiscono in sette dataset e migliorano solo in tre dataset. Questo fenomeno è noto come la "maledizione della multilingue".</sample>
    <sample id="84">**Abstract**  
In this paper, we address the limitations of fully dynamic networks, which, while powerful, suffer from excessive parameter usage and computational overhead. Traditional dynamic networks, such as Mixture of Experts and Dynamic Convolution, replace static layers with dynamic ones, often leading to significant increases in model size. For instance, replacing BERT-Base’s feed-forward layers with eight Mixture of Experts multiplies the model size by five, making such approaches impractical in many scenarios.  

To tackle this issue, we propose **PAD-Net: Partially Dynamic Network**, a framework that partitions parameters into dynamic and static components. By introducing two scale factors to control the intensity of dynamic and static parameters, PAD-Net balances the representation power of dynamic networks with the efficiency of static ones. Our iterative mode partitioning method identifies and converts redundant dynamic parameters into static ones, reducing computational costs without compromising performance.  

Experiments demonstrate that PAD-Net outperforms both static and fully dynamic networks, achieving better accuracy with fewer parameters and lower computation. Ablation studies further optimize dynamic ratios for Dynamic Convolution and Mixture of Experts, highlighting the importance of scale factors and constraints. Compared to network pruning, PAD-Net maintains static parameters, ensuring better performance. Additionally, PAD-Net enhances output discrimination, contributing to its superior performance.  

Future work includes extending PAD-Net to other mainstream architectures, exploring hardware-friendly structures, and investigating additional parameter modes, such as combinations of zero elements, static, and dynamic parameters. This work bridges the gap between dynamic and static networks, offering a scalable and efficient solution for dynamic network design.</sample>
    <sample id="85">Un esempio di pianificazione linguistica vincolata è "fare un tortino al cioccolato". Questo obiettivo astratto può essere suddiviso in passi specifici con vincoli come l'uso di ingredienti specifici, il tempo di preparazione, o le tecniche di cottura, che devono essere rispettati nella pianificazione e nell'esecuzione.</sample>
    <sample id="86">Gli autori si accertano della segretezza del loro metodo visualizzando le distribuzioni degli embedding delle frasi nei quattro dataset utilizzando l'analisi PCA (Principal Component Analysis). La legenda delle figure mostra il numero di trigger in ciascuna frase, e come si può vedere, è difficile distinguere tra gli embedding delle frasi con trigger e quelli senza, dimostrando che il watermark è abbastanza nascosto da non essere facilmente rilevabile.</sample>
    <sample id="87">Il lavoro utilizza i PLM (Pre-trained Language Models) esistenti come base per costruire un nuovo modello, DrBERT, specializzato per i domini biomedico e clinico in francese. In particolare:

1. **RoBERTa**: DrBERT è basato su RoBERTa, un modello pre-addestrato che offre buone prestazioni in vari compiti di NLP.
2. **Continual Pre-training**: Vengono introdotti tre modelli basati su continual pre-training, utilizzando i pesi e la tokenizzazione di CamemBERT, adattati a diversi set di dati (NACHOS e note cliniche).
3. **PubMedBERT**: Viene utilizzato come modello di riferimento per un confronto, anche se in inglese.

In sintesi, DrBERT sfrutta l'architettura e i pesi pre-addestrati di RoBERTa e CamemBERT per creare un modello specializzato in francese, migliorandolo con dati specifici del dominio biomedico e clinico.</sample>
    <sample id="88">Secondo la presentazione, GPT-4 è meno allineato con i Paesi non anglofoni, in particolare con i Paesi non anglofoni e non confuciani. Tuttavia, non viene specificato un Paese particolare come quello con cui GPT-4 è meno allineato.</sample>
    <sample id="89">La relatrice mostra il modo in cui il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione nell'esempio in cui descrive un discorso in inglese contenente "I'm going to talk about...". Il modello prevede la traduzione in tedesco e analizza i pesi di cross-attention, osservando che le prime due parole puntano ai primi frame di parlato ricevuti, mentre l'ultima parola punta agli ultimi frame di parlato ricevuti (lambda speech frames). Questo indica che le prime due parole vengono emesse, mentre l'ultima non viene emessa perché la somma dei pesi di cross-attention è sopra la soglia alpha, e si aspetta un altro chunk di parlato.</sample>
    <sample id="90">Il paper "Rethinking Annotation: Can Language Learners Contribute?" di Haneul Yoo et al. sfida la tradizionale necessità di reclutare parlanti nativi per l'annotazione di dati in NLP, proponendo un approccio innovativo che coinvolge gli apprendisti linguistici. Gli autori hanno condotto uno studio pilota su tre lingue (inglese, coreano e indonesiano) utilizzando quattro compiti comuni del benchmark GLUE: analisi del sentimento, NLI, NER e MRC. Hanno reclutato sia apprendisti linguistici che parlanti nativi, suddividendo gli apprendisti in tre livelli di competenza (base, intermedio, avanzato) e fornendo risorse aggiuntive per facilitare l'annotazione. I risultati mostrano che le annotazioni degli apprendisti sono accurate, specialmente per compiti più semplici e domande di livello facile-medio, e che l'aggregazione delle loro annotazioni tramite voto di maggioranza le rende quasi equivalenti a quelle dei parlanti nativi. Inoltre, i modelli NLP addestrati con le annotazioni degli apprendisti raggiungono prestazioni vicine al 95% rispetto ai dati di riferimento, a volte superando persino i modelli addestrati con annotazioni di parlanti nativi. Lo studio evidenzia anche un miglioramento della competenza linguistica e del vocabolario degli apprendisti durante il processo di annotazione. Questo approccio apre nuove possibilità per la costruzione di dataset di riferimento per lingue a risorse limitate, dove è difficile reclutare parlanti nativi, superando barriere geografiche e tecnologiche.</sample>
    <sample id="91">La quantità di attività influisce positivamente sulla performance del modello. Come mostrato nei risultati, con un aumento del numero di attività, il modello raggiunge una migliore performance e, al contempo, una minore sensibilità. Questo suggerisce che un maggior numero di attività contribuisce a migliorare l'apprendimento e la generalizzazione del modello. Inoltre, l'uso di più istruzioni durante il fine-tuning (5 istruzioni invece di una) migliora ulteriormente la performance complessiva del modello e riduce significativamente la sua sensibilità.</sample>
    <sample id="92">Gli autori confrontano il loro metodo con tre approcci di riferimento:

1. **Naive seq2seq models**: Questi modelli, pur essendo semplici, faticano a generalizzare a strutture più complesse e spesso producono output non coerenti con l'input.  
2. **Modelli con alberi**: Questi approcci integrano alberi per catturare la struttura composizionale, ma richiedono pre-elaborazione formale e sono computazionalmente costosi.  
3. **Altri modelli treeless**: Gli autori confrontano il loro metodo con altri modelli che non utilizzano alberi, valutando la loro capacità di generalizzare a strutture più profonde.  

Il loro metodo si distingue per la sua flessibilità e capacità di generalizzare a recursion più profonda senza ricorrere a alberi.</sample>
    <sample id="93">I due coautori, Alexander Koller e Ivan Titov, sono i supervisori (advisors) del primo autore, Matthias Lindemann.</sample>
    <sample id="94">**Abstract:**  
The rapid adoption of embedding-as-a-service (EaaS) models, built on large language models like GPT and LLAMA, has raised concerns about copyright infringement. Attackers can reverse-engineer these models by learning from the provided embeddings, leading to unauthorized replication of services. To address this, we propose **Embedding Marker**, a backdoor watermark method specifically designed for EaaS. Our approach involves two main steps: watermark injection and copyright verification.  

In the watermark injection step, a trigger set of moderately frequent words is selected, and a target embedding is defined. When a user queries the service, the embedding is a weighted sum of the original embedding and the target embedding, with the weight proportional to the number of triggers in the query sentence. This ensures the watermark is embedded without degrading embedding utility.  

For copyright verification, we construct a backdoor dataset (containing only trigger words) and a benign dataset (excluding trigger words). The provider queries the stealer’s service with these datasets and computes similarity metrics (cosine and L2) between the embeddings. The difference in similarity scores between the datasets (delta cosine and delta L2) and a KS test are used to detect the watermark.  

Experiments on four datasets (AG News, MIND, SST2, and Enron Spam) demonstrate high detection performance while maintaining utility for downstream tasks. Visualization of embeddings using PCA confirms the covertness of the watermark, making it difficult for attackers to distinguish or remove. Embedding Marker provides a robust solution to protect the copyright of EaaS models.</sample>
    <sample id="95">Il primo autore di PaLM è David Vilar.</sample>
    <sample id="96">Ciao a tutti. Sono Jenny, una studentessa di dottorato al primo anno alla Carnegie Mellon University e oggi presenterò il vostro lavoro NLPositionality che caratterizza i pregiudizi di progettazione dei dataset e dei modelli. Questo lavoro è stato svolto in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, ovvero Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap. Quindi iniziamo immaginando che state lavorando per un giornale e state setacciando i commenti sotto il vostro articolo di notizie cercando di rimuovere contenuti tossici. Potreste rivolgervi a un API popolare come Prospective API per il rilevamento della tossicità, e questo funziona davvero bene se siete Carl Jones. Dove Prospective API è in grado di rilevare correttamente le istanze tossiche. Ma questo non è davvero il caso per Aditya Sharma. Dove Prospective AP non è altrettanto sensibile ai termini offensivi più comuni nei contesti indiani. Questo è un esempio di pregiudizio di progettazione dove vediamo differenze sistematiche di prestazioni della tecnologia tra le popolazioni. I pregiudizi di progettazione come quello che abbiamo appena visto prima potrebbero verificarsi a causa della posizione dei ricercatori NLP e degli sviluppatori di modelli. La posizione è semplicemente le prospettive che le persone detengono a causa delle loro demografie, identità ed esperienze di vita. È un concetto ampiamente utilizzato negli studi critici, specificamente negli spazi accademici femministi e queer. E come ricercatore, la posizione può influenzare il processo di ricerca e i suoi risultati perché può cambiare le decisioni che i ricercatori prendono. E quindi una domanda che le persone potrebbero porsi è, i dataset e i modelli hanno una posizione? E non stiamo dicendo che i modelli stessi nei dataset stessi abbiano identità demografiche ed esperienze di vita, ma aggregano giudizi e opinioni di persone reali e possono quindi rappresentare certe posizioni più di altre. Quindi i lavori precedenti hanno suggerito alcune prove aneddotiche di avere una posizione, come lacune culturali nei modelli e nei dataset, così come definizioni teoriche della posizione dei modelli. Tuttavia questi lavori non guardano davvero al confronto tra utenti finali e dataset e modelli stessi, e studiare la posizione dei dataset e dei modelli è sempre più importante poiché i compiti NLP diventano più soggettivi e socialmente orientati, ed è difficile caratterizzare come queste posizioni siano distorte perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API. Quindi per studiare la posizione dei dataset e dei modelli, confrontiamo le annotazioni con utenti reali con dataset e modelli esistenti. Facciamo questo attraverso il nostro framework NLPositionality. Il nostro framework funziona in due passaggi principali. Il primo passo è ri-annotare i dataset con annotatori diversi. E dovremmo farlo guardando alle demografie degli annotatori originali dei dataset, perché, di solito, solo pochi annotatori annotano ogni istanza e perché le demografie sono raramente raccolte e condivise. E quindi optiamo per ri-annotare i dati per ottenere molti annotatori per istanza e per ottenere un ricco insieme di dati demografici. Poi prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i dataset utilizzando un punteggio di correlazione di Pearson, e quindi il nostro framework si differenzia dalla letteratura sulla disaccordo degli annotatori confrontando utenti finali con modelli e dataset, previsioni ed etichette, anziché guardare solo all'accordo degli annotatori o alla distribuzione degli annotatori. Il nostro framework è in gran parte abilitato da Lab in the Wild e dalla piattaforma di crowdsourcing online per dove collaboratore HCI. Lab in the Wild è una piattaforma di sperimentazione online dove possiamo reclutare volontari diversi. Rispetto a piattaforme come M Turk che hanno in gran parte partecipanti dagli Stati Uniti o dall'India e oltre, Lab in the Wild è comunque in grado di ottenere dati di alta qualità. Abbiamo ospitato 2 compiti su Lab in the Wild, uno dei quali è l'accettabilità sociale, e il modo in cui funziona è che i partecipanti leggeranno una situazione dal dataset di chimica sociale e, poi, scriveranno quanto sia socialmente accettabile una situazione. Successivamente, per rimanere coinvolti nello studio, possono confrontare le loro risposte con un AI e altri. Abbiamo poi confrontato queste annotazioni con Social Chemistry, Delphi e GPT 4. Abbiamo poi replicato una configurazione molto simile per il compito di rilevamento della tossicità e dell'incitamento all'odio, dove leggeranno un'istanza da Dynahate e scriveranno se pensano che sia un'istanza di incitamento all'odio. Abbiamo poi confrontato queste annotazioni con Dynahate, Perspective API, Rewire API, Hate Roberta e GPT 4. Il nostro studio alla fine ha raccolto oltre 16.000 annotazioni da oltre 1000 annotatori provenienti da 87 paesi. Quindi ora siamo meglio attrezzati per rispondere a chi si allineano maggiormente i dataset e i modelli NLP. Troviamo che c'è una posizione in NLP. Ad esempio, troviamo che i dataset e i modelli sono più allineati ai paesi di lingua inglese. Quindi per l'analisi dell'accettabilità sociale di GPT 4, troviamo che è più allineato ai paesi di lingua inglese e confuciani. Troviamo che Dynahate è anche più allineato ai paesi di lingua inglese. Troviamo anche un ulteriore allineamento con le persone che hanno un'istruzione universitaria. Quindi per GPT 4, nel compito di accettabilità sociale, troviamo che è più allineato alle persone con un'istruzione universitaria o di scuola superiore e troviamo lo stesso per Dynahate dove è più allineato alle persone con un'istruzione universitaria. Tuttavia, quando i modelli e i dataset sono allineati a popolazioni specifiche, alcuni sono inevitabilmente lasciati indietro. Un esempio di ciò è che i dataset e i modelli sono meno allineati alle persone non binarie rispetto alle controparti maschili e femminili. Troviamo questo nell'analisi del compito di accettabilità sociale di GPT 4 così come nell'analisi del compito di Dynahate. Quindi, dato che c'è una posizione in NLP, cosa possiamo fare al riguardo? Quindi abbiamo alcune raccomandazioni per questo. La prima è tenere traccia di tutte le scelte di progettazione pertinenti durante il processo di ricerca. E l'altra è fare ricerca NLP con la lente del perspectivismo. La nostra terza raccomandazione è costruire dataset e modelli specializzati all'interno di 4 comunità specifiche. E un buon esempio di ciò è l'iniziativa Masakhani. Vogliamo sottolineare che l'NLP inclusivo non è solo rendere. Sapete, tutte le tecnologie funzionano per tutti. E quindi questo conclude la nostra presentazione. Ma se volete saperne di più, sentitevi liberi di controllare il nostro dashboard per i risultati dell'analisi più aggiornati e il nostro articolo. Grazie.</sample>
    <sample id="97">La relatrice menziona **tre problemi** associati ai modelli SimulST:  
1. **Architetture specifiche** ad hoc per SimulST, che richiedono ottimizzazione aggiuntiva.  
2. **Procedure di training lunghe e complesse**, con obiettivi di ottimizzazione diversi.  
3. **Training e mantenimento di più modelli** per raggiungere diversi regimi di latenza.</sample>
    <sample id="98">Un modo efficace per mitigare i bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP è **bilanciare e diversificare i dati di addestramento**. Questo può includere:

1. **Selezione di fonti diverse**: Utilizzare una varietà di fonti di notizie e contenuti sociali che rappresentano diverse prospettive politiche e culturali.
2. **Rimozione di contenuti polarizzanti**: Filtrare o rimuovere contenuti estremamente polarizzanti o propagandistici che potrebbero rafforzare i bias.
3. **Riassegnazione dei dati**: Ripartire i dati in modo che le diverse prospettive politiche siano rappresentate in modo più equilibrato.
4. **Pre-elaborazione dei dati**: Applicare tecniche di pre-elaborazione per ridurre o rimuovere esplicitamente i bias, come la rimozione di stereotipi o la neutralizzazione di linguaggio polarizzato.
5. **Monitoraggio e correzione continua**: Implementare meccanismi di monitoraggio continuo per rilevare e correggere i bias emergenti nei modelli durante l'addestramento e il deployment.

Queste strategie possono aiutare a creare modelli di NLP più equi e meno influenzati da pregiudizi sociali e politici.</sample>
    <sample id="99">Ciao, sono Siyu Yuan dell'Università Fudan. Sono qui per presentare il nostro lavoro "Distillare la conoscenza delle istruzioni da grandi modelli linguistici per la pianificazione linguistica vincolata". Nella vita quotidiana, gli esseri umani spesso pianificano le loro azioni seguendo istruzioni passo-passo sotto forma di script orientati agli obiettivi. I lavori precedenti hanno sfruttato i modelli linguistici per pianificare obiettivi astratti di attività stereotipate come "fare una torta". E hanno dimostrato che i grandi modelli linguistici possono efficacemente scomporre gli obiettivi in passaggi. Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione per gli obiettivi astratti di attività stereotipate. La pianificazione per gli obiettivi con vincoli specifici, come "fare una torta al cioccolato", rimane ancora poco studiata. In questo articolo, definiamo il problema della pianificazione linguistica vincolata che impone diverse vincolanze sugli obiettivi di pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multifaccettati. Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli. In questo articolo, valutiamo e miglioriamo prima la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici. Poiché non esiste un dataset di obiettivi specifici per supportare il nostro studio, dobbiamo acquisire questi obiettivi per primi. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multifaccettati per l'acquisizione di dati in ciclo con l'uomo utilizzando InstructGPT. Preleviamo 100 obiettivi specifici e valutiamo gli script generati dai grandi modelli linguistici. Questa tabella riporta l'accuratezza complessiva dei risultati. Scopriamo che tutti i modelli linguistici ottengono risultati insoddisfacenti nella pianificazione per obiettivi specifici. Quindi, conduciamo un'analisi dettagliata per indagare perché i modelli di apprendimento falliscono. I risultati nella figura mostrano che la completezza semantica negli script generati è accettabile ma la fedeltà ai vincoli non può essere garantita. Approfondiamo un argomento più fine-grained delle categorie di vincoli definite in wikiHow. Il heat map nella figura mostra che le prestazioni di pianificazione di InstructGPT variano notevolmente per obiettivi di diverse categorie. Studi precedenti hanno dimostrato che la qualità dell'output dei modelli linguistici è soggetta a un'alta varianza, portando a prestazioni scadenti. Pertanto, adottiamo l'idea di generare in eccesso-poi-filtrare per migliorare la qualità della generazione. Prima mostriamo i tipi di vincoli con esempi per InstructGPT e otteniamo obiettivi specifici basati sugli obiettivi astratti di partenza. Quindi, InstructGPT genera in eccesso K script per obiettivi specifici. Successivamente, viene sviluppato un modello di filtro per selezionare gli script fedeli. Convertiamo gli script e gli obiettivi in embeddings di InstructGPT e calcoliamo la somiglianza coseno come punteggi di somiglianza per misurare la somiglianza semantica. Inoltre, premiamo lo script che contiene le parole chiave dell'obiettivo vincolo. Manteniamo lo script solo se l'obiettivo desiderato ottiene il punteggio più alto nel set di obiettivi. Con il nostro metodo, InstructGPT può generare script di qualità superiore. Il nostro metodo migliora notevolmente la capacità di pianificazione sia nella completezza semantica che nella fedeltà al vincolo. Poiché i grandi modelli linguistici sono costosi da implementare, è essenziale abilitare la capacità di pianificazione linguistica di modelli più piccoli e specializzati. Creare il dataset è un passo essenziale a tal fine. Tuttavia, gli studi precedenti non abilitano la pianificazione per obiettivi specifici e l'annotazione manuale del dataset è costosa. Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica per distillare dataset di pianificazione linguistica vincolata dai grandi modelli linguistici. Applichiamo il nostro metodo per costruire un dataset di pianificazione linguistica vincolata, chiamato CoScript. In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità del set di validazione e test, chiediamo ai lavoratori in crowdsourcing di trovare e rivedere i campioni errati. Questa figura mostra la distribuzione dei vincoli di CoScript. Scopriamo che CoScript mostra un alto pluralismo negli obiettivi specifici generati. Con CoScript possiamo provare modelli più piccoli ma specializzati per la pianificazione linguistica vincolata. Scopriamo che T5 ottimizzato su CoScript può generare script di qualità superiore rispetto alla maggior parte dei grandi modelli linguistici, indicando che i modelli più piccoli possono superare i modelli più grandi quando adeguatamente addestrati su dataset adatti. In sintesi, stabiliamo il problema della pianificazione linguistica vincolata. Valutiamo la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici e sviluppiamo un metodo di generazione in eccesso-poi-filtrare per i grandi modelli linguistici. Usiamo i grandi modelli linguistici per generare un dataset di script di alta qualità, CoScript, per la pianificazione linguistica vincolata. Speriamo che il dataset CoScript possa essere una risorsa preziosa per far avanzare la ricerca sulla pianificazione linguistica. Grazie per il vostro tempo. Per favore, trovate ulteriori dettagli di CoScript nel nostro articolo.</sample>
    <sample id="100">PromptRank è un approccio efficiente in termini di dati per il recupero di catene multi-hop nel contesto della domanda e risposta (QA). A differenza dei sistemi tradizionali che richiedono migliaia di esempi di domande e catene di riferimento per ottenere buone prestazioni, PromptRank si basa su un metodo di recupero non supervisionato combinato con un reranker basato su modelli linguistici a pochi esempi. Il processo si articola in due fasi principali: il recupero di un insieme di catene candidate utilizzando TF-IDF e l'espansione/pruning delle catene tramite l'esplorazione di iperlink, seguito dal reranking delle catene candidate utilizzando un modello linguistico a pochi esempi.

La chiave del successo di PromptRank è l'uso della probabilità del question dato la catena come funzione di scoring, calcolata tramite un modello linguistico. Le catene vengono convertite in prompt, inserendo i documenti della catena e utilizzando un token indicatore per identificare i documenti. Un'istruzione, come "Leggi i documenti precedenti e poni una domanda", viene aggiunta per stimolare il ragionamento del modello linguistico sulle informazioni della catena.

Gli esperimenti condotti su HotpotQA dimostrano che PromptRank supera i sistemi completamente supervisionati come DrKit e si confronta con i sistemi di recupero denso multi-hop di stato dell'arte. L'approccio si è rivelato efficace anche nel miglioramento delle prestazioni del downstream QA, con un modello lettore come ELECTRA-Large, ottenendo risultati molto vicini a quelli del sistema di recupero denso multi-hop (MDR). In sintesi, PromptRank dimostra che i modelli linguistici possono essere utilizzati per il ranking a pochi esempi di percorsi candidati nel QA multi-hop, offrendo prestazioni solide rispetto ai sistemi tradizionali.</sample>
    <sample id="101">La fluidità di PaLM è **comparabile a quella dei sistemi di traduzione all'avanguardia**, secondo le valutazioni umane basate sul framework MQM. Tuttavia, PaLM presenta problemi di **accuratezza**, in particolare con errori di omissione, dove a volte omette parti della frase sorgente per produrre una traduzione più fluida.</sample>
    <sample id="102">Le proprietà importanti di un metodo di filigrana per proteggere i modelli di embedding come servizi sono:

1. **Applicabilità ai servizi di embedding**: Il metodo deve essere compatibile con i servizi di embedding basati su modelli di linguaggio.
2. **Non degradazione dell'utilità**: La filigrana non deve compromettere la qualità o l'efficacia degli embedding forniti.
3. **Covertness**: La filigrana deve essere difficile da rilevare o rimuovere da parte degli attaccanti.
4. **Trasferibilità**: La filigrana deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello.</sample>
    <sample id="103">Le 14 lingue diverse in cui sono stati tradotti i discorsi TED in inglese sono:

1. Spagnolo
2. Francese
3. Tedesco
4. Portoghese
5. Italiano
6. Olandese
7. Russo
8. Cinese
9. Giapponese
10. Coreano
11. Arabo
12. Polacco
13. Turco
14. Ungherese</sample>
    <sample id="104">Il contenuto non fornisce un numero specifico di istanze campionate per la riannotazione. Tuttavia, si menziona che il framework NLPositionality prevede la riannotazione di dati con molti annotatori per ogni istanza e la raccolta di un ricco set di dati demografici.</sample>
    <sample id="105">Le metriche di distanza utilizzate per misurare la differenza tra i set di dati benigni e backdoor sono:

1. **Cosine Similarity** (delta cosine)
2. **L2 Distance** (delta L2)

Queste metriche vengono calcolate confrontando le somiglianze tra le richieste di embedding del set di dati backdoor e del set di dati benigni, e poi confrontando le differenze tra queste somiglianze.</sample>
    <sample id="106">Il paper **QUEST** presenta un nuovo dataset progettato per valutare l'efficacia dei sistemi di retrieval nell'affrontare query con vincoli settoriali impliciti, tipiche di scenari di ricerca d'informazione con esigenze selettive. Il dataset include oltre 3.000 query di ricerca di entità che incorporano operazioni settoriali, con entità di risposta verificate per la pertinenza e documenti annotati con segmenti attribuibili a diversi vincoli della query. Il dataset è costruito utilizzando categorie di Wikipedia da quattro domini: film, libri, piante e animali, combinando operazioni settoriali per generare query con vincoli. Query templatiche sono state parafrasate da annotatori umani per garantire la stessa semantica e fluidità, mentre un secondo gruppo di annotatori ha validato la naturalezza. Infine, la pertinenza delle entità di risposta e l'evidenza documentale sono state verificate, con segmenti annotati per ogni vincolo. L'obiettivo è valutare sistemi che recuperino insiemi di risposte multi-entità da un vasto corpus, considerando che l'evidenza per la pertinenza può provenire da diverse parti del documento. I risultati mostrano ampi margini di miglioramento nel recupero e prestazioni F1 basse, evidenziando la difficoltà nel gestire query con intersezioni e differenze settoriali. Il dataset **QUEST** mira a supportare la ricerca futura nel miglioramento dei sistemi di retrieval per scenari con esigenze informative selettive, come quelli di Jane e Austin.</sample>
    <sample id="107">I modelli basati su codificatori multilingue sono stati utilizzati in due configurazioni principali per l'attività di cross-lingual semantic parsing:

1. **Encoder-Decoder**: Questi modelli, come mBART e mT5, hanno ottenuto le migliori prestazioni su tutti e nove i dataset. Sono stati addestrati in un contesto multilingue, il che ha permesso di migliorare le prestazioni su molte lingue, tranne l'inglese, dove le prestazioni sono diminuite in sette dataset.

2. **Encoder-PTR**: Questi modelli, come XLM-R + PTR e mBERT + PTR, hanno mostrato prestazioni migliori rispetto ai modelli precedenti e hanno raggiunto risultati comparabili. Sono stati addestrati utilizzando codificatori multilingue con decodificatori basati su pointer, e l'addestramento in un mix di varie lingue ha migliorato le loro prestazioni.

In entrambi i casi, l'addestramento in un contesto multilingue ha permesso di ottenere risultati migliori rispetto ai modelli monolingui, anche se l'inglese ha mostrato alcune limitazioni.</sample>
    <sample id="108">Il nostro lavoro, presentato all'ACL 2023, esplora la robustezza dei giudizi di accettabilità dei modelli linguistici (LLM) in contesti più lunghi. Tradizionalmente, i paradigmi di coppie minime (MPP) valutano la capacità dei modelli di distinguere tra frasi grammaticali e non grammaticali. Tuttavia, i modelli moderni, con finestre di contesto più ampie, richiedono una valutazione in contesti più lunghi. Abbiamo rivisitato il pipeline MPP, generando sequenze più lunghe combinando coppie minime con prefissi grammaticali o non grammaticali tratti da dataset come BLiMP e SyntaxGym. Abbiamo testato modelli come OPT e GPT-2, aumentando la lunghezza del contesto fino a 1024 token. I risultati mostrano che i giudizi MPP sono relativamente stabili in contesti arbitrari, ma diventano significativamente influenzati quando i prefissi provengono dallo stesso dataset o dallo stesso fenomeno linguistico. Questo suggerisce che i modelli sono sensibili a caratteristiche sintattiche e semantiche condivise, che possono alterare i loro giudizi in base al contesto. Inoltre, perturbazioni controllate delle frasi non influenzano i giudizi MPP, indicando che i modelli non reagiscono al rumore, ma alle strutture condivise. In sintesi, i modelli linguistici sono sensibili a caratteristiche linguistiche condivise e la valutazione MPP attuale potrebbe non catturare appieno la loro conoscenza astratta in contesti più lunghi.</sample>
    <sample id="109">L'articolo presenta "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor", un dataset di istruzioni per compiti di elaborazione del linguaggio naturale generato automaticamente da un modello GPT-3. Il dataset contiene 64.000 esempi di istruzioni e input/output, con ulteriori 176.000 esempi di istruzioni parafrasate. La raccolta dei dati è stata effettuata in modo completamente automatico, senza bisogno di annotazioni umane. Gli esempi generati sono stati analizzati per valutare la loro creatività, diversità e correttezza. I risultati mostrano che oltre il 50% degli esempi generati sono corretti, e anche gli esempi errati contengono informazioni utili per l'instruction tuning. Il dataset è stato utilizzato per addestrare un modello T5 a 11 miliardi di parametri, che ha superato i modelli T0++ e Tk-instruct su diversi benchmark. Questo approccio dimostra la capacità dei modelli di linguaggio di generare dati creativi e diversificati, offrendo un'alternativa più veloce ed economica rispetto alle annotazioni umane. In sintesi, Unnatural Instructions rappresenta un dataset di istruzioni per una vasta gamma di compiti di elaborazione del linguaggio naturale, generato automaticamente e senza bisogno di annotazioni umane, evidenziando le potenzialità dei modelli di linguaggio nella produzione di dati di alta qualità.</sample>
    <sample id="111">Gli autori selezionano le parole a frequenza moderata (trigger set) contando la frequenza delle parole in un corpus di testo generale fornito dal provider. Le parole vengono poi classificate in base alla loro frequenza, e quelle che si trovano in un intervallo di frequenza moderata vengono scelte come trigger set.</sample>
    <sample id="112">Ciao a tutti, mi chiamo Shuheng. Oggi presenterò il nostro articolo "I tagger di riconoscimento degli enti nominati di CoNLL-2003 funzionano ancora bene nel 2023?" Cominciamo. Il nostro articolo ha indagato il problema della generalizzazione utilizzando il compito di riconoscimento degli enti nominati, o il compito NER. Abbiamo osservato che i modelli sono stati utilizzati in CoNLL-2003 per sviluppare NER per quasi 20 anni e questo solleva naturalmente diversi problemi. In primo luogo, questi modelli possono generalizzare ai dati moderni? E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione? Allo stesso tempo, se osserviamo una scarsa generalizzazione, quali sono le cause del calo delle prestazioni di questi modelli? Per indagare su questi problemi, abbiamo sviluppato il dataset CoNLL++. Questo è un dataset che abbiamo raccolto da Reuters News del 2020, e poi annotato con le stesse linee guida di annotazione di CoNLL-2003. Abbiamo poi affinato oltre 20 modelli su CoNLL-2003. Li abbiamo valutati sia sui set di test CoNLL-03 che su CoNLL++. E, last but not least, abbiamo calcolato la percentuale di cambiamento in F1 per valutare la generalizzazione di ciascun modello. Quindi, cosa è necessario per una buona generalizzazione? Attraverso i nostri esperimenti abbiamo scoperto che ci sono tre ingredienti principali necessari. Il primo è l'architettura del modello. Attraverso i nostri esperimenti abbiamo scoperto che i modelli transformer normalmente generalizzano meglio ai nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che solitamente i modelli più grandi portano a una migliore generalizzazione. E, last but not least, sappiamo tutti che il numero di esempi di affinamento influisce direttamente sulle prestazioni di un compito a valle. Anche qui abbiamo scoperto che più esempi di affinamento, portano anche a una migliore generalizzazione. Per quanto riguarda la nostra prossima domanda, quali sono le cause del calo delle prestazioni di alcuni modelli, avevamo due ipotesi. La prima è l'adattamento per l'overfitting, che è l'overfitting dei costi dovuto al riutilizzo dello stesso set di test più e più volte e questo si manifesta solitamente come il rendimento decrescente su un nuovo set di test. La seconda ipotesi è la deriva temporale, che è il degrado delle prestazioni causato dal crescente divario temporale tra i dati di addestramento e i dati di test. Per l'overfitting dei dati, abbiamo visto che dal grafico a destra, la linea rossa di miglioramento ha una pendenza maggiore di uno. Questo significa che ogni unità di miglioramento che abbiamo fatto su CoNLL-2003 si traduce in un miglioramento di più di un'unità su CoNLL++, il che significa che non ci sono rendimenti decrescenti. E questo ci mostra che l'adattamento per l'overfitting in questo caso non è osservato. Quindi, che dire della deriva temporale? Per la deriva temporale, abbiamo fatto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti e abbiamo scoperto che le prestazioni si degradano con un divario temporale più grande e questo conferma la nostra ipotesi che la causa principale del calo delle prestazioni è la deriva temporale. La nostra conclusione è che, per una buona generalizzazione, avremmo bisogno di una migliore architettura del modello, una dimensione del modello più grande, così come più esempi di affinamento. E questi vanno di pari passo, non possiamo avere solo un ingrediente e scartare gli altri. Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni qui è causato dalla deriva temporale e, in modo sorprendente, non è causato dall'adattamento per l'overfitting anche se CoNLL-2003 è stato utilizzato per oltre 20 anni. Tornando alla domanda che abbiamo posto nel titolo del nostro articolo "I tagger di riconoscimento degli enti nominati di CoNLL-2003 funzionano ancora bene nel 2023?" E abbiamo scoperto che la risposta è in realtà un sonoro sì. Speriamo che il nostro articolo chiami a ulteriori ricerche su come migliorare la generalizzazione dei modelli. E, infine, assicuratevi di controllare il nostro articolo, il nostro dataset e se avete domande, sentitevi liberi di contattarmi. Grazie mille.</sample>
    <sample id="114">In this paper, we address the issue of heavy parameter redundancy in large language models (LLMs), which hinders their deployment and efficiency. We propose a novel approach called **Grouped Head Attention (GHA)**, which employs a **divide-and-conquer strategy** to optimize multi-head attention mechanisms. The GHA model consists of two stages: **group-constrained training** and the **Voting-to-Stay (VTS) algorithm**. 

In the first stage, attention heads are divided into groups, with intra-group heads trained to be more similar and inter-group heads trained to be more distinct. This is achieved using a combination of homogenization and diversification objectives. The second stage applies the VTS algorithm, where heads are evaluated and pruned based on their importance, ensuring only one head per group remains. This results in significant parameter compression, up to 90% in extreme cases, while maintaining or even improving performance on tasks like machine translation, abstractive summarization, and language modeling.

Our experiments demonstrate that the GHA model achieves notable improvements (e.g., 3.8% to 7% BLEU score improvements) and substantial parameter reductions (e.g., 32.1% to 62%) compared to state-of-the-art baselines. Additionally, our **LITE model** achieves 90% parameter pruning, 62% faster inference speed, and 80% FLOP reduction without sacrificing performance. We believe this approach aligns with the Lottery Ticket Hypothesis, suggesting that pruning can be done without performance loss, especially in task-specific applications. Future work will explore task-specific automatic pruning to further enhance efficiency and adaptability of LLMs.</sample>
    <sample id="115">L'approccio EDAtt utilizza un segmento parlato di dimensione lambda (λ) per decidere se emettere una parola basandosi sulla distribuzione dell'attenzione.</sample>
    <sample id="116">Nell'esempio con Servin e Kea, la conoscenza specifica dell'entità necessaria è "Servin è un giudice".</sample>
    <sample id="117">Secondo il contenuto, il fattore più importante è la **qualità dell'esempio**, poiché influenza maggiormente le prestazioni del modello rispetto alla somiglianza con la frase sorgente.</sample>
    <sample id="118">**Abstract:**  
We address the challenge of improving pretraining techniques for code-switched Natural Language Processing (NLP) tasks, focusing on multilingual code-switching scenarios prevalent in linguistically diverse communities. Multilingual models like mBERT and XLM-R struggle with code-switched tasks such as sentiment analysis and question answering. To address this, we propose **SwitchMLM**, a novel Masked Language Modeling (MLM) objective tailored for code-switching. SwitchMLM identifies **switch-points**—transitions between languages—as the only maskable tokens, unlike standard MLM where all tokens are uniformly maskable. However, SwitchMLM requires access to Language Identification (LID) tags, which are not always available. To overcome this, we introduce **FrequencyMLM**, a surrogate method that uses monolingual corpora to infer LID tags based on word frequency.  

We also propose architectural modifications, including **residual connections** from intermediate layers to the final layer of BERT, as these layers encode more switch-point information. An auxiliary **LID-based loss** is applied to these intermediate layers to enhance their ability to encode language information.  

Experiments demonstrate that our combined approach—SwitchMLM or FrequencyMLM with residual connections and auxiliary loss—outperforms baseline models on sentiment analysis tasks across multiple language pairs. Probing experiments using linear and conditional probing classifiers confirm that our methods increase switch-point information in intermediate and final layers, supporting our architectural and loss design choices. This work advances code-switched NLP by proposing specialized pretraining techniques and architectural innovations.</sample>
    <sample id="119">L'articolo si concentra sui modelli linguistici GPT-4, GPT serie, BART e i suoi varianti negli esperimenti estesi.</sample>
    <sample id="120">Il modello EDAtt utilizza i punteggi di attenzione di un livello specifico, concentrandosi sulla somma dei punteggi di attenzione verso gli ultimi lambda frame di parlato per decidere se emettere una parola. Non combina i punteggi di più livelli.</sample>
    <sample id="121">Gli esempi di inferenza diretta nel contesto del lavoro su "Resolving Indirect Referring Expressions for Entity Selection" sono riferimenti espliciti alle entità, come:

- **"Easy on Me"** o **"I Gotta Feeling"** (nomi delle canzoni).  
- **"the first one"** o **"the second one"** (posizioni numeriche).  

Questi riferimenti sono chiari e non lasciano spazio a ambiguità, a differenza delle inferenze indirette che richiedono ulteriori contesti o descrizioni.</sample>
    <sample id="122">Gli autori dell'articolo sono affiliati a Fudan University.</sample>
    <sample id="123">In this research, we introduce **MultiInstruct**, the first large-scale multi-modal instruction tuning benchmark dataset, comprising 62 diverse multi-modal tasks across 10 categories derived from 21 open-source datasets. Each task is equipped with five expert-written instructions, addressing the scarcity of publicly available multi-modal instructional datasets. We investigate the effectiveness of instruction tuning on multi-modal zero-shot learning using **OFA**, a unified multi-modal pre-trained model. Our approach formulates tasks in a unified sequence-to-sequence format, integrating text, images, instructions, and bounding boxes into a shared token space.

For evaluation, we use 53 tasks for training and reserve 5 additional tasks for testing, along with a natural instruction dataset for NLP benchmarks. We report metrics such as accuracy (for classification), Rouge-L (for generation), and a new metric, **sensitivity**, which measures consistency across instruction variations. Results show that instruction tuning significantly improves OFA’s performance on seen multi-modal tasks, with transfer learning from natural instruction datasets further enhancing performance and sensitivity. Using multiple instructions during fine-tuning improves overall performance and reduces sensitivity.

We also demonstrate that transfer learning from natural instruction datasets benefits instruction tuning, achieving better sensitivity and performance compared to the original OFA model. Our work lays the foundation for future research in multi-modal instruction tuning and provides a QR code for accessing our dataset and models. We are actively expanding the dataset to include 150 additional vision-language tasks for broader exploration.</sample>
    <sample id="124">In this work, we address the temporal reasoning capabilities of large language models (LLMs), breaking it down into three levels: time-to-time (L1), time-to-event (L2), and event-to-event (L3) reasoning. We find that prior studies often overemphasize L2 reasoning, neglecting comprehensive temporal understanding. To address this, we introduce the TempReason dataset, which covers all three levels and spans long temporal periods. For L1, we extend from year to month prediction; for L2 and L3, we use Wikidata and Wikipedia for question-answer pairs. We evaluate temporal reasoning in three settings: Closed Book QA, Open Book QA, and Reasoning QA, where temporal knowledge is provided. To improve LLM temporal reasoning, we propose a training strategy combining temporal span extraction pre-training and time-sensitive reinforcement learning, resulting in the TempT5 model. Our experiments show that TempT5 outperforms other models, particularly in Open Book QA and Reasoning QA. However, performance fluctuations across time periods remain, suggesting potential training data imbalances. This work highlights temporal reasoning biases in LLMs, proposes a comprehensive benchmark, and introduces a training paradigm to enhance these capabilities, paving the way for more robust temporal understanding in LLMs.</sample>
    <sample id="125">Il contenuto non specifica il numero di autori coinvolti nell'articolo.</sample>
    <sample id="126">No, la traduzione della query in linguaggio naturale utilizzando un modello di traduzione automatica prima del parsing semantico **non è stato considerato un approccio standard** nel contesto di XSemPLR. Questo approccio è stato testato come una delle sei impostazioni di valutazione (Translate-Test), ma non è stato proposto come metodo principale o standard. L'approccio standard si basa sull'uso di modelli monolingui o multilingui direttamente addestrati sui dati di input nel linguaggio naturale, senza un passaggio intermedio di traduzione automatica.</sample>
    <sample id="127">Il paper "Large Language Models Are Reasoning Teachers" presenta una tecnica innovativa per trasferire le capacità di ragionamento complesse da modelli linguistici di grandi dimensioni (LLM) a modelli più piccoli, riducendo i costi computazionali e di memoria. Gli autori, Namgyu Ho, Laura Schmid e Se-Young Yun, evidenziano che i LLM di grandi dimensioni, come GPT-3 o PALM, sono in grado di risolvere problemi complessi attraverso il *chain-of-thought reasoning*, ma sono costosi e difficili da implementare. La soluzione proposta è l'uso di questi LLM come "insegnanti" per generare soluzioni dettagliate a problemi complessi, che vengono poi utilizzate come dati di addestramento per modelli più piccoli. Un aspetto chiave è l'introduzione della *Diverse Reasoning*, una tecnica che genera multiple soluzioni da un singolo problema, migliorando l'apprendimento del modello studente. Gli esperimenti dimostrano che i modelli studenti, anche con soli 0.3 miliardi di parametri, possono raggiungere prestazioni significative su 12 compiti, superando i metodi di base e migliorando l'efficienza rispetto al fine-tuning tradizionale. La metodologia è altamente scalabile, con opzioni per aumentare le prestazioni attraverso l'uso di più dati, modelli insegnanti più potenti o studenti più grandi. Questo approccio apre la strada alla distillazione di altre capacità emergenti, offrendo un equilibrio tra costi di sviluppo e inferenza. Il paper fornisce codice e dati completi per replicare gli esperimenti, incoraggiando ulteriori ricerche e discussioni.</sample>
    <sample id="128">Il lavoro "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources" di Akshatha e Martin esplora la capacità dei modelli di comprensione del linguaggio naturale (NLU) di integrare conoscenze da diverse fonti, sia acquisite durante la fase di pre-addestramento che fornite al momento dell'inferenza. I modelli NLU spesso richiedono l'integrazione di entrambi i tipi di conoscenza per risolvere compiti complessi. Per valutare questa capacità, gli autori propongono il KITMUS Test, una suite di test diagnostici che include un compito di risoluzione della coreferenza. Questo compito richiede l'uso di conoscenze specifiche di un'entità (ad esempio, "Servin è un giudice") e conoscenze di contesto (ad esempio, "I giudici decidono casi nei tribunali"). Il KITMUS Test è stato valutato sia con partecipanti umani che con modelli di risoluzione della coreferenza esistenti. I risultati mostrano che, senza addestramento specifico, i modelli non performano bene. Tuttavia, con l'addestramento su KITMUS, alcuni modelli migliorano significativamente. Gli esperimenti con conoscenze fittizie dimostrano che anche i modelli migliori faticano a integrare conoscenze di contesto fornite solo al momento dell'inferenza. In sintesi, il lavoro evidenzia la necessità di addestramento specifico per l'integrazione di conoscenze da diverse fonti e le difficoltà persistenti dei modelli nel gestire conoscenze di contesto dinamiche.</sample>
    <sample id="129">Gli autori hanno fornito come esempio di gruppo contrassegnato le donne di colore, in particolare le donne asiatiche, latinoamericane e nere, per analizzare i modelli di stereotipi e narrazioni essenzializzanti nelle descrizioni generate dai modelli linguistici.</sample>
    <sample id="130">Le architetture dei modelli che non generalizzano in modo adeguato sono quelle tradizionali, come le architetture a reti neurali ricorrenti (RNN) e le architetture a reti neurali convoluzionali (CNN). I modelli transformer, invece, generalizzano meglio ai nuovi dati.</sample>
    <sample id="131">Il contenuto non fornisce i nomi specifici dei set di dati di test utilizzati nello studio.</sample>
    <sample id="132">Due autori sono coinvolti nell'articolo: Akshatha e Martin.</sample>
    <sample id="133">L'autore opera con più modalità, in particolare con **testo e immagini**, poiché il suo lavoro si concentra sul miglioramento del Multi-Modal Zero-Shot Learning attraverso l'instruction tuning, utilizzando un modello multi-modale come OFA.</sample>
    <sample id="135">ABC-Eval è un nuovo approccio dimensionale per valutare l'intelligenza artificiale conversazionale, sviluppato dal laboratorio di NLP di Emory University in collaborazione con Amazon Alexa AI. A differenza dei metodi tradizionali che si affidano a valutazioni umane soggettive, ABC-Eval si concentra sull'analisi esplicita dei comportamenti dei modelli di dialogo, come la pertinenza delle risposte, le contraddizioni, le violazioni del senso comune e la capacità di mostrare empatia. Questo approccio riduce la soggettività delle valutazioni umane e fornisce una valutazione più precisa e affidabile della qualità del dialogo.

Lo studio ha confrontato ABC-Eval con tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti di coppia a livello di dialogo. I risultati mostrano che le etichette ABC-Eval sono più affidabili e predittive della qualità complessiva del dialogo rispetto ai metodi esistenti. Inoltre, le metriche ABC-Eval catturano aspetti unici della qualità del dialogo, spiegando oltre il 25% della qualità complessiva del dialogo.

I risultati dello studio evidenziano ancora alcune sfide, come le violazioni del senso comune nel 20% delle risposte dei bot e la produzione di informazioni irrilevanti nel 15% delle risposte. Tuttavia, ABC-Eval offre un'opportunità per valutare in modo più accurato e affidabile l'intelligenza artificiale conversazionale, contribuendo al progresso del campo.</sample>
    <sample id="136">Il lavoro presentato, intitolato "FERMAT: An Alternative to Accuracy for Numerical Reasoning", affronta la sfida della valutazione delle capacità di ragionamento numerico dei modelli linguistici. Motivato dalla necessità di strumenti più informativi rispetto ai tradizionali benchmark basati sull'accuratezza, FERMAT introduce un insieme flessibile di test che esaminano la comprensione dei numeri, le operazioni matematiche e le dipendenze di apprendimento. I test sono tratti da banche dati come CommonCore e Illinois, adattati per includere numeri di vario tipo (piccoli, grandi, decimali) e operazioni semplici o combinate. La ricerca dimostra che i modelli, anche di grandi dimensioni, spesso performano male in queste valutazioni, evidenziando carenze nella comprensione numerica e nell'interpretazione linguistica dei problemi. Attraverso un'analisi di base e un'ulteriore fase di fine-tuning con 200.000 esempi generati, si osserva un miglioramento delle prestazioni, ma la precisione rimane bassa. L'introduzione di template diversificati da dataset come GSM8K e AQUA ha ulteriormente migliorato i risultati, dimostrando l'importanza della diversità linguistica e matematica. In conclusione, FERMAT fornisce un'alternativa più informativa ai benchmark esistenti, evidenziando aree di miglioramento come l'encoding dei numeri e la tokenizzazione, e sottolineando l'importanza della diversità nella valutazione dei modelli di ragionamento numerico.</sample>
    <sample id="137">Il lavoro "Tell2Design: A Dataset for Language-Guided Floor Plan Generation", pubblicato in ACL 2023, introduce una nuova sfida nell'ambito della generazione di design guidati da linguaggio, focalizzandosi sulla creazione di piani di casa da istruzioni testuali. A differenza dei modelli di generazione di immagini, che si concentrano su concetti visivi di alto livello, questo studio mira a generare layout di interni che rispettino specifiche istruzioni umane, come tipologia delle stanze, dimensioni e relazioni spaziali. Il dataset Tell2Design, composto da 5.051 istruzioni umane e 76.000 generate artificialmente, è stato utilizzato per addestrare un modello sequenza-a-sequenza basato su trasformatori. Questo approccio permette di gestire istruzioni di lunghezza variabile e di generare direttamente i bounding box delle stanze, superando i limiti dei metodi precedenti. I risultati mostrano che il modello T2D, addestrato su istruzioni umane con warming-up su istruzioni artificiali, ottiene un'elevata accuratezza (Micro IoU 54, Macro IoU 53), superando i modelli di generazione di immagini. Questo studio apre la strada alla ricerca sulla generazione di design guidati da linguaggio, con applicazioni pratiche nel campo dell'architettura e del design.</sample>
    <sample id="138">Secondo gli autori, l'area della NLU (Natural Language Understanding) che è poco studiata è la **integrazione di conoscenza da fonti multiple**, in particolare la capacità di utilizzare **conoscenza di background disponibile solo a tempo di inferenza**. Questo perché molti modelli di NLU si basano principalmente sulla conoscenza acquisita durante il pre-training, ma faticano a integrare efficacemente informazioni specifiche del contesto fornite solo durante l'inferenza.</sample>
    <sample id="139">I relatori sono Ying e Zhiyang.</sample>
    <sample id="140">Sì, CoScript è stato sottoposto a controlli di qualità. Per garantire la validità del dataset, sono stati coinvolti lavoratori a contratto (crowd-sourced workers) che hanno identificato e corretto i campioni errati.</sample>
    <sample id="141">Le risorse esistenti per la traduzione dipendente dal contesto presentano diversi limiti:

1. **Limitazione dei tipi di contesto**: Si concentrano su contesti specifici e limitati, spesso basati su dominio o conoscenza umana, senza coprire la varietà di contesti reali.
2. **Supporto linguistico ristretto**: Sono generalmente progettate per un numero limitato di lingue, rendendole meno utili per esplorazioni multilingue.
3. **Dipendenza da dati curati manualmente**: Richiedono un'enorme quantità di lavoro umano per la creazione e il mantenimento dei dati, limitando la scalabilità e l'applicabilità.
4. **Mancanza di generalizzazione**: Non riescono a catturare efficacemente la complessità e la variabilità dei contesti in diverse lingue e domini.
5. **Difficoltà nell'identificazione dei contesti**: Spesso non sono in grado di identificare automaticamente i contesti rilevanti per la traduzione, richiedendo intervento umano.

Questi limiti rendono difficile valutare in modo completo e accurato le prestazioni dei modelli di traduzione in contesti dipendenti.</sample>
    <sample id="142">Ciao! Sto per parlare del nostro lavoro su "Risoluzione delle espressioni di riferimento indirette per la selezione di entità", in cui introduciamo il Corpus AltEntities. Mi chiamo Javad Hosseini e questo è un lavoro congiunto con Filip Radlinski, Silvia Pareti e Annie Louis. Il nostro obiettivo è comprendere il linguaggio degli utenti quando vogliono fare una scelta. Considerate questa domanda alternativa. "Intendevi 'Easy on Me' o 'I Gotta Feeling'?" Qui, un utente vuole selezionare tra una di queste due canzoni. La cosa più ovvia è usare un riferimento diretto, ad esempio dicendo il nome della canzone "Easy on Me" o la sua posizione, "la prima". Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non riesce a ricordare il nome della canzone. O le pronunce sono troppo simili tra loro e difficili da disambiguare. O quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di riferimenti indiretti, ad esempio, "il più recente" o "la canzone che non è energica". Questo è un problema importante nei sistemi conversazionali e anche per il benchmarking della comprensione delle entità da parte dei LLM. Non siamo a conoscenza di un set di dati pubblico su larga scala per il compito, quindi ne raccogliamo uno utilizzando l'annotazione di massa. Il nostro set di dati copre tre diversi domini: musica, libri e ricette. La nostra metodologia di raccolta dei dati enfatizza l'informalità utilizzando un setup di completamento di vignette. La vignetta ha tre bolle di dialogo. Nella prima bolla, Bob dice, "Ricorda quella canzone che stavamo ascoltando ieri?" E con questo, Bob imposta il contesto del dialogo. Nella seconda bolla di dialogo, Alice dice, "Intendi 'Easy on Me' o 'I Gotta Feeling'?" Che è la domanda alternativa. E nella terza bolla di dialogo, Bob usa un riferimento indiretto per selezionare una di queste entità, ad esempio, "il più recente." Forniamo le prime due bolle di dialogo automaticamente, ma la terza è compilata dall'annotatore. La prima bolla di dialogo è scelta tra alcuni prompt manuali per dominio. La seconda, che è la domanda alternativa, è generata come segue. Usiamo sempre un semplice modello. Intendi A o B? Dove A e B sono campioni da Wikipedia. Ecco i diversi metodi di campionamento che abbiamo utilizzato. Quando ci spostiamo più in alto nella lista, le entità diventano più simili tra loro ed è di solito più difficile fare la disambiguazione. Il primo è casuale uniforme. Il secondo è quando le entità hanno titoli simili, ad esempio, due libri con il nome "The Return". Il terzo è quando hanno descrizioni simili su Wikipedia. E infine quando hanno info box o attributi simili su Wikipedia. Ad esempio, lo stesso genere o lo stesso artista per una canzone. Quando mostriamo questa domanda alternativa agli annotatori, sanno il nome di queste entità, ma non necessariamente sanno delle entità. Quindi, ciò che facciamo è mostrare alcune conoscenze di base sulle due entità. Per le canzoni, mostriamo semplicemente un link di ricerca Google per ogni canzone e poi chiediamo agli annotatori di ascoltare almeno un po' di ciascuna canzone, e leggere su ciascuna canzone. Ecco ad esempio, il risultato della ricerca Google per la canzone "Easy on Me." Per i domini di ricette e libri, mostriamo alcuni testi di background da Wikipedia. Per le ricette, mostriamo anche le loro immagini, di nuovo da Wikipedia, in modo che gli annotatori sappiano come si presentano. Poi, abbiamo chiesto agli annotatori di scegliere una di queste entità, ad esempio, ecco la prima, e descriverle usando da tre a cinque espressioni di riferimento indirette. Ecco alcuni esempi dal nostro dataset. Ad esempio, "quella senza parole", "non quella con il bambino di 12 anni", o "quella fittizia", o "viene dall'Azerbaigian", e così via. Il Corpus AltEntities ha 6.000 domande alternative in tre domini, e ha 42.000 espressioni di riferimento indirette. I risultati con il modello T5 XL sono riassunti di seguito. Se il modello di linguaggio ha accesso alle stesse conoscenze di base degli annotatori, allora l'accuratezza è davvero alta, è intorno al 92-95%. Ma questo non è realistico. Se il modello di linguaggio ha accesso a conoscenze di base parzialmente sovrapposte, allora l'accuratezza è tra l'82 e l'87%, che è più realistico. Ad esempio, quando il modello di linguaggio recupera le conoscenze di base. Se il modello di linguaggio ha accesso solo ai nomi delle entità, allora l'accuratezza è solo del 60%, quindi c'è molto spazio per il miglioramento. Abbiamo anche dimostrato che i modelli sono generalizzabili tra i domini. Ecco un link al nostro dataset. Grazie.</sample>
    <sample id="143">L'approccio EDAtt viene confrontato con le seguenti politiche SimulST esistenti:

1. **Wait-k strategy**: una strategia che decide se emettere una parola in base alla stabilità dell'attenzione nei frame precedenti.
2. **Local Agreement**: una strategia che considera la somiglianza tra le parole predette e quelle effettivamente pronunciate.
3. **State-of-the-art architecture specifically tailored for simultaneous pre-translation**: un'architettura progettata specificamente per la traduzione simultanea.

L'approccio EDAtt si dimostra superiore a queste strategie, come mostrato dai risultati presentati nel paper.</sample>
    <sample id="144">Gli autori dell'articolo "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains" sono affiliati ai seguenti istituti:

1. **Yanis Labrak** - Université de Nantes, CNRS, LITIS - Laboratoire d'Informatique et de Télécommunications de l'Université de Nantes
2. **Jean-Baptiste Masse** - Université de Nantes, CNRS, LITIS - Laboratoire d'Informatique et de Télécommunications de l'Université de Nantes
3. **Thomas Lefebvre** - Université de Nantes, CNRS, LITIS - Laboratoire d'Informatique et de Télécommunications de l'Université de Nantes
4. **Jean-Marc Petitta** - Université de Nantes, CNRS, LITIS - Laboratoire d'Informatique et de Télécommunications de l'Université de Nantes
5. **Jean-Christophe Depalle** - Université de Nantes, CNRS, LITIS - Laboratoire d'Informatique et de Télécommunications de l'Université de Nantes
6. **Jean-Baptiste Poitevin** - Université de Nantes, CNRS, LITIS - Laboratoire d'Informatique et de Télécommunications de l'Université de Nantes
7. **Jean-Christophe Filliâtre** - Université de Nantes, CNRS, LITIS - Laboratoire d'Informatique et de Télécommunications de l'Université de Nantes

Tutti gli autori sono associati all'Université de Nantes e al CNRS (Centre National de la Recherche Scientifique).</sample>
    <sample id="145">Il nome della relatrice è Jenny.</sample>
    <sample id="146">Il paper analizza il problema delle omissioni nella sintesi di dialoghi, un aspetto critico che compromette la qualità delle sintesi generate da modelli di linguaggio avanzati. Sebbene i modelli pre-addestrati siano in grado di produrre sintesi fluenti e coerenti, essi spesso presentano errori fattuali e omissioni, con circa il 70% delle sintesi che soffrono di questo problema. L'analisi mostra che le informazioni omise sono distribuite in modo casuale all'interno dei dialoghi, indipendentemente dalla loro lunghezza o dominio, evidenziando la difficoltà attuale dei modelli nell'identificare le informazioni chiave. Per affrontare questo problema, viene definita una task di rilevamento delle omissioni a livello di enunciato, per cui il modello deve prevedere quali parti del dialogo sono state omise nella sintesi candidata. A tal fine, viene costruito il dataset OLDS, che fornisce etichette di omissioni di alta qualità per la sintesi di dialoghi, basato su cinque benchmark esistenti e cinque domini. Vengono esplorati tre framework di base per il rilevamento delle omissioni, valutati in termini di Precision, Recall e F1-score, nonché di recall a livello di parole (WR score). I risultati mostrano che il task è molto impegnativo, con un F1-score intorno al 50%. Tuttavia, l'integrazione delle informazioni omise nella sintesi candidata attraverso un metodo di post-editing migliora significativamente la qualità della sintesi, dimostrando che il rilevamento delle omissioni è un'area promettente per il miglioramento della sintesi di dialoghi.</sample>
    <sample id="147">Tre autori sono coinvolti nell'articolo: Myra, Esin Durmus e Dan Jurafsky.</sample>
    <sample id="148">Ciao, sono Sara Papi dell'Università di Trento e della Fondazione Bruno Kessler e vi presenterò brevemente il paper "Attention as a Guide for Simultaneous Speech Translation", che è un lavoro congiunto con Matteo Negri e Marco Turchi. Cos'è la traduzione simultanea del parlato? La traduzione simultanea del parlato, o SimulST, è il processo di traduzione del linguaggio parlato in un testo in un'altra lingua in tempo reale, consentendo la comunicazione tra lingue diverse. E quali sono i problemi dei modelli attuali di SimulST? Le architetture specifiche sono solitamente addestrate, introducendo moduli aggiuntivi da ottimizzare. Procedure di addestramento lunghe e complicate, ad esempio, l'addestramento che coinvolge diversi obiettivi di ottimizzazione. E l'addestramento e il mantenimento di diversi modelli per raggiungere diversi regimi di latenza. Ad esempio, l'addestramento di un modello con una latenza media di un secondo e un altro con due secondi di latenza, e così via. Quindi qual è la nostra soluzione? Prima di tutto, utilizzare i modelli di ST offline già esistenti senza riaddestramento o adottare architetture specifiche per SimulST. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici. E sfruttare la conoscenza già acquisita dal modello attraverso il meccanismo di attenzione tra l'input audio e l'output testuale. Questo è il meccanismo di cross-attenzione, e potete vedere un esempio a destra. La nostra soluzione è proporre EDAtt, o Encoder-Decoder Attention, ed è una strategia per cui decidiamo se emettere o meno una traduzione parziale, basandoci su dove punta l'attenzione. Una parola viene emessa se l'attenzione non è concentrata, cioè la sua somma è al di sotto di una certa soglia alfa verso gli ultimi lambda fotogrammi del parlato, il che significa che l'informazione ricevuta è abbastanza stabile. Ad esempio, se riceviamo un blocco di parlato contenente "Voglio parlare di..." e il nostro modello prevede la traduzione in tedesco, e guarderemo i pesi di cross-attenzione, vedremo che le prime due parole puntano ai primi fotogrammi del parlato ricevuti, mentre l'ultima parola punta agli ultimi fotogrammi ricevuti, come lambda fotogrammi del parlato. Questo significa che le prime due parole verranno emesse mentre, poiché la somma dei pesi di cross-attenzione è sopra una certa soglia alfa, non emetteremo l'ultima parola e aspettiamo un altro blocco di parlato. Se continuiamo e riceviamo un altro blocco di parlato, e il nostro modello prevede altre tre parole e guarderemo quei pesi di cross-attenzione, vedremo che nessuna parola punta agli ultimi lambda fotogrammi del parlato. Questo significa che queste tre parole verranno emesse. Se guardiamo i risultati principali di EDAtt, plotteremo i risultati della traduzione simultanea del parlato su grafici in cui abbiamo il BLEU da un lato che misura la qualità della traduzione, e il ritardo medio che è la misura della latenza, e consideriamo anche il ritardo medio consapevole del calcolo che tiene conto dei tempi di previsione dell'output del modello. Quindi vogliamo che le nostre curve siano il più alte possibile su questo grafico. Ma vogliamo anche che siano spostate a sinistra. E confrontiamo con strategie popolari che sono anche applicate ai modelli offline che sono la strategia Wait-k e l'Accordo Locale. E confrontiamo anche con l'architettura all'avanguardia specificamente progettata per la pre-traduzione simultanea. Questi sono tutti i risultati della strategia di traduzione simultanea del parlato sul tedesco. E vediamo che supera tutte le strategie applicate ai modelli offline poiché le curve sono spostate a sinistra. E vediamo anche che se consideriamo il tempo effettivo trascorso o il tempo consapevole del calcolo, cioè il tempo più veloce. Se volete scoprire più risultati, leggete il nostro paper. E abbiamo anche rilasciato open source il codice e i modelli e la traduzione simultanea per facilitare la riproducibilità del nostro lavoro. Grazie per l'attenzione.</sample>
    <sample id="149">Sì, il set di dati CoNLL++ è disponibile pubblicamente.</sample>
    <sample id="150">Il paper "MEETINGQA: Extractive Question-Answering on Meeting Transcripts" presenta un nuovo dataset, MeetingQA, focalizzato sull'estrazione di risposte da trascrizioni di riunioni. Questo dataset, basato su domande e risposte reali, è unico per la sua natura lunga, domain-specific e ricca di informazioni, con domande aperte e discussioni dettagliate. MeetingQA contiene 7.7K domande, con il 30% non rispondibili e il restante 70% con risposte multispan o multi-speaker. Le domande includono yes/no, opinion seeking e retoriche, mentre le risposte spesso presentano disaccordo. Il dataset è stato creato da trascrizioni AMI, con un'alta inter-annotator agreement (Krippendorff's alpha 0.73). I risultati mostrano che i modelli fine-tuned ottengono un F1 di 84.6, con un divario significativo rispetto alle prestazioni umane. I modelli short-context come RoBERTa superano quelli long-context come Longformer, mentre i modelli multi-span hanno prestazioni simili o leggermente inferiori a quelli single-span. L'uso di dati di interviste per l'aumento dei dati migliora le prestazioni zero-shot. Tuttavia, i modelli faticano con domande retoriche e l'identificazione degli speaker, con un peggioramento significativo in setting zero-shot. MeetingQA evidenzia le sfide per i modelli QA in contesti reali e complessi.</sample>
    <sample id="151">Ciao a tutti, mi chiamo Ying e il mio collega Zhiyang e io presenteremo la nostra ricerca su MultiInstruct che migliora l'apprendimento zero-shot multi-modale tramite l'accordatura delle istruzioni. Con i progressi nei grandi modelli linguistici, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento per riutilizzare i modelli linguistici pre-addestrati per diversi compiti a valle in modo efficiente in termini di parametri e dati. Recentemente, molti studi hanno dimostrato che l'accordatura delle istruzioni consente ai grandi modelli linguistici di eseguire compiti non visti in modo zero-shot seguendo istruzioni naturali. Tuttavia, la maggior parte dei lavori precedenti sull'accordatura delle istruzioni si è concentrata sul miglioramento delle prestazioni zero-shot sui compiti solo linguistici, mentre i compiti di visione artificiale e multi-modali sono stati trascurati. Pertanto, in questo lavoro vogliamo indagare se l'accordatura delle istruzioni di modelli pre-addestrati multi-modali possa effettivamente migliorare la generalizzazione a compiti multi-modali non visti. Inoltre, al momento della nostra ricerca, abbiamo scoperto una notevole discrepanza nella disponibilità di dataset di istruzioni tra NLP e multi-modali. Esistono più di 1600 compiti di istruzioni solo linguistiche. Tuttavia, non esiste un grande dataset di compiti di istruzioni multi-modali pubblicamente disponibile. Pertanto, questo ci motiva a costruire un dataset di istruzioni multi-modali. Qui presentiamo MultiInstruct, il primo dataset di benchmark di istruzioni multi-modali che consiste di 62 compiti multi-modali diversi che coprono 10 ampie categorie. Questi compiti sono derivati da 21 dataset open-source esistenti e ogni compito è dotato di cinque istruzioni scritte da esperti. Per indagare l'accordatura delle istruzioni multi-modali sul nostro dataset proposto, prendiamo OFA, un modello pre-addestrato multi-modale unificato, come nostro modello base. OFA utilizza un vocabolario unificato per linguaggio, token di immagini e le coordinate di una bounding box. Qui mostriamo alcuni esempi di istanze dal nostro dataset MultiInstruct, per unificare l'elaborazione di vari tipi di dati di input e output. Seguiamo il metodo di OFA e formuliamo tutti i compiti in un formato sequenza-a-sequenza unificato. In cui il testo, le immagini, le istruzioni e le bounding box sono rappresentati nello stesso spazio di token. Ok, ora parlerò di accordatura delle istruzioni multi-modali. Quindi per il dataset di addestramento, utilizziamo 53 compiti da 9 gruppi per l'addestramento e campioniamo 10.000 istanze per compito. Per il test, riserviamo l'intero gruppo di ragionamento sul senso comune per il test, e selezioniamo ulteriori 5 compiti dai gruppi VQ e Miscellaneous. Usiamo tutte le istanze nella divisione di test per ogni compito. Inoltre, campioniamo casualmente 20 compiti dalla divisione di test delle istruzioni naturali come compito non visto per NLP. Quindi utilizziamo il modello OFA grande pre-addestrato come modello base. Durante l'addestramento, mescoliamo tutte le istanze per tutti i compiti. Ogni istanza è combinata casualmente con uno dei suoi cinque modelli di istruzione. Quindi durante il test per ogni compito, conduciamo un totale di 5 esperimenti valutando il modello utilizzando una delle cinque istruzioni. In ogni esperimento, riportiamo le prestazioni min e massime e la deviazione standard delle prestazioni su tutti e 5 gli esperimenti. Se il compito è un compito di classificazione multi-modale, riportiamo l'accuratezza. Se è un compito di generazione multi-modale, riportiamo Rouge-L. Per il compito NLP, riportiamo anche Rouge-L. Introduciamo una metrica di valutazione aggiuntiva chiamata sensibilità. Quindi questa misura la capacità del modello di produrre costantemente gli stessi output per lo stesso compito indipendentemente dalla leggera variazione nella formulazione dell'istruzione. Questo è il nostro risultato principale. Come possiamo vedere, l'accordatura delle istruzioni può migliorare significativamente le prestazioni di OFA sui compiti multi-modali visti. Inoltre, l'apprendimento trasferito dal dataset di istruzioni naturali può beneficiare l'accordatura delle istruzioni. Qui possiamo vedere, man mano che aumenta la quantità di compiti, il modello ottiene prestazioni migliori e nel frattempo, una sensibilità inferiore. Quindi abbiamo anche fatto un esperimento. Usiamo una istruzione rispetto a 5 istruzioni. Come possiamo vedere, utilizzare più istruzioni può migliorare le prestazioni complessive del modello e ridurre la sua sensibilità molto. Quindi questo mostra l'effetto di diverse strategie di fine-tuning sulla sensibilità del modello. Come possiamo vedere dall'apprendimento trasferito dai dataset di istruzioni naturali, il modello può ottenere una sensibilità molto migliore rispetto al modello OFA originale. Possiamo anche vedere che l'apprendimento trasferito dai dataset di istruzioni naturali può aiutare OFA a ottenere prestazioni molto migliori sul dataset di istruzioni naturali. Quindi nel complesso, proponiamo il primo dataset di istruzioni multi-modali su larga scala con un miglioramento significativo della loro capacità di breve termine di OFA, e esploriamo diverse tecniche di apprendimento trasferito e mostriamo i loro benefici. Progettiamo una nuova metrica chiamata sensibilità. Quindi una cosa in più, stiamo raccogliendo un dataset di istruzioni multi-modali molto più grande con circa 150 compiti di visione e linguaggio aggiuntivi e li rilasceremo. Quindi questo è un codice QR per i nostri dati e modelli. Grazie.</sample>
    <sample id="152">Il presente lavoro esplora l'integrazione di modelli di linguaggio avanzati (LLM) nel campo della filologia classica, con particolare attenzione ai testi greci e latini. Sono stati sviluppati nuovi modelli, tra cui GreBERTa e GreTa per il greco antico, e PhilBERTa e PhilTa per il multilinguismo, pre-addestrati su dati di greco antico, latino e inglese. La creazione di questi modelli ha richiesto la raccolta di nuovi dati, tra cui un corpus di pre-addestramento da Internet Archive, che ha superato le limitazioni dei dati precedenti. I modelli sono stati valutati su compiti come la taggatura di parti del discorso, l'analisi della dipendenza e la lemmatizzazione, dimostrando prestazioni superiori rispetto agli attuali modelli di stato dell'arte. Inoltre, è stato analizzato il comportamento dell'encoder del modello T5, rivelando differenze significative rispetto ai modelli encoder-only tradizionali. Nonostante l'obiettivo iniziale di creare modelli multilingui più performanti, i risultati hanno mostrato che le differenze tra modelli multilingui e monolingui sono minime, suggerendo che l'addestramento multilingue non apporta vantaggi significativi in termini di conoscenza semantica e del mondo. In sintesi, il lavoro presenta nuovi modelli potenti per la filologia classica, con un focus sulla qualità dei dati e sulla valutazione rigorosa delle prestazioni.</sample>
    <sample id="153">In this work, we address the issue of ambiguities in text-to-image generative models, which hinder the generation of images faithful to user intention. We propose a framework to mitigate these ambiguities and evaluate the fidelity of generated images. Our approach begins with the curation of a benchmark dataset, a modified version of the LAVA corpus, which covers various types of ambiguities. To disambiguate prompts, we employ a language model that generates either clarifying questions or alternative visual interpretations. Users then provide answers, which are concatenated with the original prompt to form a disambiguated version. This process allows us to capture user intent more accurately. 

Once the prompts are disambiguated, we evaluate the fidelity of generated images using an automatic evaluation framework. This framework employs a VQA model to assess whether the generated images align with the user's intention by comparing them to the disambiguated prompts. Our findings indicate that our framework effectively resolves ambiguities and improves image fidelity. Additionally, our automatic evaluation method aligns with human evaluations, making it a reliable tool for assessing text-to-image models. This work highlights the importance of addressing ambiguities in text-to-image models to enhance their performance and user satisfaction.</sample>
    <sample id="154">Gli autori dell'articolo sono Sara Papi, Matteo Negri e Marco Turchi. Sara Papi è affiliata all'Università di Trento e alla Fondazione Bruno Kessler.</sample>
    <sample id="155">Il nome della relatrice o del relatore è **Javad Hosseini**.</sample>
    <sample id="157">Il lavoro "Dialogue Summarization with Static-Dynamic Structure Fusion Graph" presentato da Shen Gao della Shandong University affronta la sfida della sintesi di dialoghi, un compito complesso nel campo della sintesi testuale. L'obiettivo è estrarre le informazioni salienti da un contesto di dialogo multi-partecipante, consentendo una rapida comprensione dei punti chiave senza dover analizzare l'intero dialogo. I metodi esistenti si basano su strutture grafiche statiche precalcolate, utilizzando strumenti linguistici esterni come il parsing del discorso e il tracciamento dello stato del dialogo. Tuttavia, questi approcci presentano due limiti principali: la dipendenza dalla precisione degli strumenti esterni e l'incompatibilità tra la costruzione statica della struttura e il processo di apprendimento della rappresentazione grafica.

Il modello proposto, SDDS, supera questi limiti integrando una struttura statica e una dinamica. Il processo inizia con un encoder di enunciati che converte gli enunciati in rappresentazioni vettoriali. Successivamente, viene costruita una struttura statica tramite metodi euristici. La componente chiave è il modulo di fusione statica-dinamica, che combina le strutture statiche e cattura le relazioni semantiche tra gli enunciati basandosi sulle loro rappresentazioni vettoriali. Infine, un modello linguistico pre-addestrato genera il riassunto integrando le strutture statica e dinamica.

Il modello utilizza quattro metodi euristici per costruire la struttura statica: il parsing del discorso, la co-occorrenza di chiavi, la modellazione delle relazioni tra gli oratori e la posizione degli enunciati. Per la struttura dinamica, si impiega un modello di attenzione multi-testa. La fusione delle due strutture avviene tramite un meccanismo di attenzione grafica integrato nel processo di generazione del riassunto. Il codice e i dati sono disponibili su GitHub per ulteriori approfondimenti.</sample>
    <sample id="158">Il lavoro "Dual Cache for Long Document Neural Coreference Resolution" introduce un approccio innovativo per la risoluzione delle coreferenze in documenti lunghi, un compito fondamentale nel trattamento del linguaggio naturale. La risoluzione delle coreferenze consiste nell'identificare e raggruppare le diverse menzioni di un'entità all'interno di un testo. I metodi tradizionali, basati sull'enumerazione di tutte le coppie di menzioni, presentano una complessità quadratica in termini di calcolo e memoria. I metodi basati sulla cache, invece, riducono questa complessità a livello lineare, ma il loro funzionamento è limitato in documenti lunghi con cambiamenti di argomento frequenti. In questi casi, la politica di evasione LRU (Least Recently Used) porta a un alto tasso di mancate risoluzioni (cache miss). Per affrontare questo problema, viene proposto un sistema a doppia cache: una cache locale per entità di interesse locale, con politica di evasione LRU, e una cache globale per entità di interesse globale, con politica di evasione LFU (Least Frequently Used). Il modello analizza il documento da sinistra a destra, classificando le nuove menzioni e valutando la frequenza delle entità. Le entità globali vengono memorizzate nella cache globale, mentre quelle locali nella cache locale. I risultati su quattro dataset pubblici dimostrano che il sistema a doppia cache supera i metodi a singola cache, riducendo significativamente i cache miss e offrendo il miglior rapporto tra prestazioni e costi. In sintesi, il sistema a doppia cache rappresenta un'evoluzione efficace per la risoluzione delle coreferenze in documenti lunghi.</sample>
    <sample id="159">Ciao a tutti, sono Koustav Sinha e sono lieto di darvi il benvenuto alla discussione del nostro articolo ACL 2023. I giudizi di accettabilità dei modelli linguistici non sono sempre robusti rispetto al contesto. Questo è un lavoro congiunto con John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy e Adina Williams. Quindi, in questo lavoro, rivediamo i paradigmi di coppie minime. Il paradigma delle coppie minime valuta i modelli linguistici in base ai giudizi di accettabilità, che possono includere anche la grammaticalità come BLiMP, SyntaxGym o l'accettabilità in termini di stereotipi come CrowS pairs. E in questo, il paradigma delle coppie minime, il modo tipico per valutare i modelli linguistici è mostrare una frase accettabile o grammaticalmente corretta e poi mostrare una frase accettabile o una frase grammaticalmente scorretta. E la speranza è che il modello, fondamentalmente, attribuisca più probabilità alla frase accettabile. L'attuale pipeline MPP fondamentalmente non ci permette di valutare l'accettazione di un modello per frasi più lunghe. Oggi i grandi modelli linguistici stanno emergendo con finestre di contesto sempre più lunghe. Quindi è fondamentale che valutiamo l'accettazione dei modelli lungo la finestra di contesto e questo è ciò che stiamo cercando di fare qui. Stiamo cercando di rivedere la pipeline MPP chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe. Quindi questo è l'approccio. Quindi ciò che facciamo è che per simulare queste sequenze più lunghe, rivediamo i dataset stessi e poi ricreiamo frasi scegliendo frasi accettabili o inaccettabili da quei dataset. Quindi, per esempio, qui abbiamo scelto un tipico paio di grammaticalità dal dataset BLiMP dal caso dell'Isola dell'Adjunct. E ciò che facciamo è ricreare come sequenze più lunghe che sono accettabili e che hanno la stessa corrispondenza della struttura grammaticalmente corretta. Estraiamo frasi grammaticali dall'Isola dell'Adjunct e poi le aggiungiamo come prefisso sia alla query accettabile che a quella inaccettabile. Quindi possiamo fare lo stesso scegliendo frasi inaccettabili dalla stessa corrispondenza, e questo potrebbe anche essere usato per testare l'accettabilità dei modelli. E possiamo anche fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un dataset diverso. Quindi questo è ciò che chiamiamo scenario di mismatch. Quindi qui le frasi provengono ancora da dataset pertinenti ma non dallo stesso dataset con cui si sta valutando. E possiamo fare lo stesso per il caso di inaccettabilità. Infine, possiamo scegliere frasi da un dominio completamente non correlato come Wikipedia. Quindi questo ci dirà se i giudizi di accettabilità dei modelli sono effettivamente influenzati da qualsiasi contesto, come, se il contesto proviene da un sottoinsieme diverso del dataset, o se è completamente non rilevante, per la frase che stiamo guardando. Quindi come fa il modello? Quindi prima guardiamo le frasi di Wikipedia, completamente non pertinenti alla coppia di query corrente, e lì troviamo che i giudizi MPP sono per lo più robusti per lunghezze di contesto arbitrarie. Aumentiamo la lunghezza del contesto fino a 1024 per massimizzare i modelli OPT e GPT 2. E qui vediamo che i giudizi MPP sono relativamente stabili. Ora, cosa succede quando scegliamo frasi dallo stesso dataset? Quindi qui stiamo scegliendo o creando frasi da domini accettabili e inaccettabili dallo stesso dataset BLiMP o SyntaxGym. E lì vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiunge un prefisso accettabile o un prefisso inaccettabile. Ma quando combiniamo la struttura, cioè quando scegliamo le frasi dallo stesso fenomeno in BLiMP o SyntaxGym, vediamo un aumento o una diminuzione massiccia del giudizio MPP per il modello, a seconda che il prefisso scelto sia accettabile o inaccettabile. Ora questo e questo effetto molto grande, aumenta lungo la lunghezza del contesto e questo probabilmente influenzerà i nuovi modelli linguistici che hanno una grande finestra di contesto. Quindi perché il prefisso di corrispondenza influisce così tanto sul giudizio del modello linguistico? Quindi abbiamo fatto una serie di analisi in cui abbiamo cercato di perturbare la frase di input, cercando di preservare la struttura rilevante ma aggiungendo rumore all'input. E dopo aver fatto diverse di queste perturbazioni, troviamo che nessuno di questi rumori sta effettivamente facendo cambiare idea al modello in termini di come ci mostra il giudizio MPP. Fondamentalmente, troviamo che i modelli sono sensibili alle frasi perturbate in modi simili. Cioè, quando perturbamo le frasi nel dominio accettabile, vediamo un aumento simile in tutte le perturbazioni e quando perturbamo le frasi nel dominio inaccettabile, vediamo una diminuzione dei giudizi MPP in modo simile. Quindi, i punti chiave del nostro lavoro è che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti che sono condivise tra le frasi. E la valutazione MPP nel modo in cui la facciamo attualmente con input brevi e di una singola frase, potrebbe non catturare completamente la conoscenza astratta dei modelli linguistici lungo la finestra di contesto. Per favore, leggete il nostro articolo per ulteriori dettagli dei nostri esperimenti. Grazie per aver ascoltato.</sample>
    <sample id="160">Nel primo passaggio, il metodo mappa i token di input in **multiset di token di output**.</sample>
    <sample id="161">Il dataset CoScript contiene **55.000 script** per obiettivi specifici.</sample>
    <sample id="163">Il metodo di allineamento migliore per DEPLAIN è **MASSalign**.</sample>
    <sample id="164">L'apprendimento scarsamente supervisionato (WSL) offre il vantaggio di ridurre i costi di annotazione dei dati, poiché utilizza fonti di etichettatura deboli (come regole euristica, knowledge base o crowdsourcing di bassa qualità) invece di annotazioni manuali costose e precise. Questo approccio permette di addestrare modelli su grandi quantità di dati a basso costo, pur mantenendo una certa qualità e generalizzazione. Tuttavia, come evidenziato nel lavoro, i recenti metodi WSL richiedono dati di validazione puliti per funzionare correttamente, il che implica che il risparmio sui costi di annotazione non è totale come spesso dichiarato.</sample>
    <sample id="165">Il paper "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations" introduce LiPoR (Likelihood Learning with Posterior Regularization), un metodo di apprendimento non supervisionato per il ragionamento abduttivo. A differenza degli approcci supervisionati, che richiedono l'annotazione di spiegazioni plausibili, LiPoR tratta le spiegazioni come variabili latenti e massimizza la verosimiglianza marginale dell'esito dato il contesto, senza bisogno di informazioni sulla plausibilità. Tuttavia, per preferire spiegazioni plausibili, viene introdotto un regolarizzatore basato sulla mutual esclusività delle spiegazioni. Il regolarizzatore Omega massimizza l'entropia di P(Z|XY) o minimizza il log di M, dove M è il numero di spiegazioni plausibili. I risultati su AlphaNLI, il dataset più utilizzato per il ragionamento abduttivo, mostrano che LiPoR supera i modelli zero-shot e l'approccio non supervisionato precedente, ottenendo un miglioramento di oltre 4 punti di accuratezza rispetto alla baseline GPT-3. Questo approccio apre nuove possibilità per il ragionamento abduttivo senza supervisione, sfruttando la mutual esclusività delle spiegazioni per migliorare la plausibilità delle conclusioni.</sample>
    <sample id="166">**Abstract:**  
The paper introduces "A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text," addressing the challenge of retrieving images from long and complex textual descriptions. Traditional visual language models excel in image-sentence retrieval but struggle with linguistically complex text due to their reliance on analogical reasoning (System 1). Inspired by the Divide-and-Conquer strategy and Dual-Process Theory, the framework integrates both analogical and logical reasoning systems. The first module, the Proposition Generator, decomposes complex text into simpler propositions using BART’s decoder. The Visual-Linguistic Interactor (System 1) processes visual-proposition interactions, generating matching scores and reasoning states. The Neural-Symbolic Reasoner (System 2) integrates these states via negation and conjunction operations to produce final inference results. Experimental results demonstrate the framework’s superiority over baselines, with ablation studies validating each module’s effectiveness. Case studies highlight the method’s interoperable processing, showcasing intermediate inference states. The authors suggest that neural symbolic reasoning and Divide-and-Conquer approaches, combined with Dual-Process Theory, can enhance compositional reasoning and planning in large language models, making them more effective for complex tasks.</sample>
    <sample id="167">I documenti in DEPLAIN-web sono stati allineati in modo misto:  
- **Manuale**: Una parte dei documenti è stata allineata manualmente.  
- **Automatico**: L'altra parte è stata allineata utilizzando metodi di allineamento automatico.  
In totale, il corpus DEPLAIN-web include 30.450 coppie di frasi allineate, risultanti da questa combinazione di approcci.</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato estraendo articoli dalla Reuters News del 2020 e annotandoli utilizzando le stesse linee guida di annotazione del CoNLL-2003.</sample>
    <sample id="169">Il paper "Prompting PaLM for Translation: Assessing Strategies and Performance" esplora l'uso del modello PaLM, un linguaggio di 540 miliardi di parametri, per la traduzione automatica. PaLM, addestrato su 780 miliardi di token, è stato uno dei modelli più avanzati nel campo del NLP al momento della sua pubblicazione nel 2022. Gli autori, in collaborazione con Google Translate, hanno condotto il primo studio sistematico sull'uso di prompt per migliorare le prestazioni dei modelli di linguaggio nella traduzione. Hanno valutato il modello utilizzando test set aggiornati e confrontandolo con i sistemi di traduzione automatica di punta, come il WMT. I risultati mostrano che la scelta del prompt ha un impatto significativo sulle prestazioni, con differenze di oltre 40 BLEURT punti in alcuni casi. Gli autori hanno optato per una strategia di prompting a 5-shot, evidenziando che la qualità degli esempi forniti è più importante della loro somiglianza con la frase sorgente. Utilizzando dati di sviluppo più curati rispetto ai dati di addestramento, hanno osservato un miglioramento delle prestazioni. Nonostante ciò, i sistemi specializzati di traduzione automatica mantengono un vantaggio significativo su PaLM. Tuttavia, PaLM si avvicina alle prestazioni di un sistema commerciale, con una fluidità paragonabile ai migliori sistemi, ma con errori di accuratezza, in particolare omissioni di parti della frase sorgente. L'analisi dei risultati suggerisce che la qualità degli esempi e la selezione dei prompt sono fattori chiave per ottimizzare le prestazioni dei modelli di linguaggio nella traduzione automatica.</sample>
    <sample id="170">Ciao a tutti, mi chiamo Yusen Zhang della Penn State University. Oggi presenterò il nostro lavoro "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations". Quindi, il parsing semantico è un compito per costruire rappresentazioni semantiche delle query degli utenti come SQL e Lambda Calculus. E il Cross-Lingual Semantic Parsing è il compito di tradurre le query in più lingue naturali in più rappresentazioni del significato. Come mostrato in questa figura, dobbiamo tradurre la query in più lingue naturali utilizzando modelli neurali in SQL, Lambda o FunQL, e così via. I modelli esistenti di parsing semantico cross-linguistico sono proposti e valutati separatamente su dataset di compiti e applicazioni limitati. Ad esempio, c'è molta copertura su certe lingue naturali. Ma il cinese è mancante e c'è una mancanza di copertura su certe rappresentazioni del significato. Il calcolo Lambda è mancante, o vengono valutati solo su certi modelli neurali. Ad esempio, c'è solo un singolo modello per valutarli. Quindi, a tal fine, proponiamo XSemPLR. Forniamo un dataset unificato XSemPLR per il parsing semantico cross-linguistico in più lingue naturali e rappresentazioni del significato. Contiene 9 dataset in vari domini, 5 compiti di parsing semantico, 8 rappresentazioni del significato e 22 lingue naturali in 15 famiglie linguistiche. E per valutare meglio il nostro benchmark, consideriamo le sei impostazioni per l'addestramento e la valutazione. La prima è Translate-Test. Usiamo Google Translate API per tradurre la sorgente nella lingua di destinazione, quindi usiamo un modello monolingue per l'addestramento e la valutazione. E per esempio, addestriamo il modello inglese su una query inglese e durante l'inferenza traduciamo la query tedesca usando l'API in inglese e poi usiamo il modello addestrato per prevedere lo SQL. E testeremo anche il Modello Monolingue. In questa impostazione, la lingua sorgente è la stessa della lingua di destinazione, per esempio tedesco-tedesco o inglese-inglese. Testiamo anche l'impostazione Monolingue Few-shot addestrando modelli monolingue con solo il 10% dei dati di addestramento. E testiamo il Modello Multilingue che addestriamo un modello multilingue per tutte le lingue. E per esempio, mettiamo insieme le query tedesche, inglesi, cinesi per addestrare un modello multilingue. E durante l'inferenza possiamo usare questo modello per tradurre le query tedesche o le query cinesi, et cetera. E consideriamo anche il trasferimento Zero-shot e Few-shot cross-linguistico. Addestriamo su una lingua sorgente e trasferiamo in un'altra lingua. Quindi, durante l'addestramento, lo addestriamo su query inglesi o sulla combinazione di query inglesi e tedesche Few-shot per addestrare un modello multilingue per prevedere l'output SQL. E abbiamo anche trovato molti risultati interessanti. Quindi, riguardo all'analisi dei modelli monolingue, valutiamo su due gruppi di modelli inclusi Encoder-PTR che sta per Encodificatori Multilingue Pre-addestrati con Decodificatori basati su Puntatore, come XLM-R + PTR e mBERT + PTR. E, valutiamo anche i modelli Encoder-Decoder, che sono Encodificatori Multilingue Pre-addestrati Decodificatori, come mBART e mT5. Abbiamo scoperto che l'Encoder-Decoder ottiene le migliori prestazioni su tutti e nove i dataset. E valutiamo su mT5 e XLM-R + PTR in impostazione multilingue. Abbiamo scoperto che l'Encoder-Decoder o Encoder-PTR possono essere migliorati addestrando in una miscela di varie lingue. Abbiamo scoperto che è perché la maggior parte delle principali lingue naturali può ottenere un guadagno di prestazioni, tranne che le prestazioni dell'inglese scendono in sette dataset e solo guadagnano in tre dataset. Penso che questo sia noto come la "Maledizione della Multilingue". Confrontiamo anche il divario di prestazioni cross-linguistico. In questa figura, la linea blu è il Trasferimento Few-shot cross-linguistico. La linea arancione è il Trasferimento Zero-shot cross-linguistico. Mentre la linea verde è l'Impostazione Monolingue. Abbiamo scoperto che, confrontando la linea verde e arancione, abbiamo scoperto che con l'impostazione Zero-shot, le prestazioni del trasferimento sono significative, e poi confrontando le linee blu e arancioni, abbiamo scoperto che con l'impostazione Few-shot il divario di trasferimento si accorcia rapidamente. Abbiamo anche trovato alcuni altri risultati interessanti. Ad esempio, l'Encoder-Decoder supera il lavoro precedente o ottiene risultati confrontabili. L'addestramento su lingua naturale inglese può aumentare significativamente le prestazioni del Few-shot sulle lingue naturali di destinazione, e abbiamo scoperto che i modelli di linguaggio multilingue come Codex e BLOOM sono ancora inadeguati per i compiti di parsing semantico cross-linguistico. Per riassumere, abbiamo costruito XSemPLR, un benchmark unificato per il parsing semantico cross-linguistico con più lingue naturali e rappresentazioni del significato. Condottiamo uno studio di benchmark completo su tre tipi rappresentativi di modelli di linguaggio multilingue. E i nostri risultati mostrano molti risultati interessanti. E così via. E benvenuti a visitare il nostro articolo e il codice. Grazie per l'attenzione.</sample>
    <sample id="171">I lavori connessi in tal senso possono essere classificati in quattro categorie principali, come menzionato nel testo:

1. **Watermark basati su testo**: Questi metodi inseriscono informazioni di copyright direttamente nel testo o nelle descrizioni dei dati. Tuttavia, non sono applicabili a embedding as services poiché non possono essere integrati in modo efficace negli embedding.

2. **Watermark basati su dati**: Questi metodi utilizzano dati specifici per creare un watermark. Anche se possono essere applicati a embedding, spesso non sono trasferibili durante il processo di model extraction, rendendoli inefficaci per proteggere i modelli.

3. **Watermark basati su struttura**: Questi metodi si basano sulla struttura degli embedding per inserire informazioni di copyright. Anche se possono essere trasferibili, spesso degradano la qualità degli embedding, riducendo la loro utilità per i compiti downstream.

4. **Watermark basati su trigger**: Questi metodi utilizzano parole o frasi specifiche (trigger) per creare un watermark. Anche se possono essere trasferibili, spesso non sono sufficientemente coperti o difficili da rimuovere per gli attaccanti.

Il lavoro proposto, **Embedding Marker**, si distingue da questi approcci poiché combina un trigger set con un meccanismo di watermark basato su backdoor, rendendolo applicabile a embedding as services, trasferibile durante il processo di model extraction, e con un impatto minimo sulla qualità degli embedding.</sample>
    <sample id="172">No, gli LLM multilingue come Codex e BLOOM non sono sufficienti per il Cross-Lingual Semantic Parsing (CLSP). Secondo lo studio presentato, questi modelli ottengono risultati inadeguati rispetto alle aspettative, dimostrando che esistono ancora sfide significative nel campo del CLSP.</sample>
    <sample id="174">Il paper "ArgAnalysis35K: A large-scale dataset for Argument Quality Analysis" presenta un dataset unico nel campo dell'analisi della qualità degli argomenti, caratterizzato da 35.000 coppie di argomenti e analisi. A differenza dei dataset esistenti, che spesso derivano da fonti di crowdsourcing e presentano limitazioni in termini di diversità e profondità, ArgAnalysis35K si distingue per la sua ampiezza e qualità. Il dataset include argomenti provenienti da fonti affidabili come discorsi di alto livello, dibattiti esperti e fonti intermedie, con una quota minore di argomenti da fonti meno qualificate. Inoltre, ArgAnalysis35K copre 24 temi diversi, selezionati in base all'esperienza e alle fonti autorevoli, offrendo una maggiore diversità rispetto ai dataset tradizionali che si concentrano su poche decine di mozioni. Un'innovazione chiave è l'introduzione del concetto di "analisi", che combina affermazioni, premesse e altre componenti per fornire una spiegazione più completa dell'argomento. Il dataset include anche un modello di rilevanza che assegna un punteggio di pertinenza a ciascun argomento rispetto a ciascun tema, migliorando la capacità di catturare la rilevanza degli argomenti in contesti diversi. Infine, ArgAnalysis35K introduce un approccio di affidabilità degli annotatori basato sulle istanze, che permette di utilizzare in modo più efficace le annotazioni, nonostante i possibili pregiudizi umani. In sintesi, ArgAnalysis35K rappresenta un passo avanti significativo nel campo dell'analisi della qualità degli argomenti, offrendo un dataset più ampio, diversificato e affidabile.</sample>
    <sample id="175">Il metodo affronta l'ambiguità delle permutazioni attraverso una **relaxazione continua** che approssima il problema della permutazione, rendendolo **GPU-friendly** e permettendo la **backpropagation** per apprendere le permutazioni linguisticamente più plausibili. Questo approccio evita di imporre vincoli rigidi sulle permutazioni, mantenendo al contempo la flessibilità e l'espressività del modello.</sample>
    <sample id="176">L'equità di un modello NLP a valle è definita in base alla sua capacità di trattare in modo imparziale e corretto diverse categorie sociali, politiche o demografiche, evitando discriminazioni o pregiudizi. Questo implica che il modello non dovrebbe mostrare prestazioni diverse o ingiuste a seconda del gruppo o dell'opinione politica rappresentata nei dati di input. Ad esempio, in contesti come il rilevamento di discorsi d'odio o notizie false, un modello equo dovrebbe essere in grado di rilevare correttamente i discorsi d'odio o le notizie false indipendentemente dal gruppo o dall'opinione politica che ne è oggetto.</sample>
    <sample id="177">Il nome del relatore è Yanis Labrak.</sample>
    <sample id="178">Il nome della relatrice è Koustav Sinha.</sample>
    <sample id="179">**Abstract:**  
Melanie Sclar presents "Minding Language Models’ (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker," focusing on enhancing Theory of Mind (ToM) reasoning in large language models (LLMs). ToM, the ability to understand others' mental states, is critically evaluated through false-belief tasks, such as the Sally-Anne test. LLMs, including ChatGPT and GPT-3, struggle with these tasks, particularly second-order false-belief questions, which require reasoning about a character’s beliefs about another character’s beliefs. The proposed method, SymbolicToM, addresses this gap by using explicit graphical representations to model mental states. These graphs, such as BBob and BBob,Alice, capture characters’ beliefs and their beliefs about others, enabling efficient inference. SymbolicToM leverages off-the-shelf models for natural language inference (NLI) and open information extraction (OpenIE) to compute these graphs during inference, avoiding overfitting. Experiments demonstrate significant performance gains across LLMs, including GPT-3, Macaw, and Flan-T5-XXL, with up to 67 accuracy points for Macaw. SymbolicToM also outperforms supervised baselines in out-of-domain and linguistic diversity scenarios, such as the ParaphrasedToMi dataset. This method enhances interpretability and robustness, making it a plug-and-play solution for improving ToM reasoning in LLMs. For detailed results and methodology, refer to the paper.</sample>
    <sample id="180">Il nome della relatrice è Myra.</sample>
    <sample id="181">In this paper, we address the problem of constrained language planning, where large language models (LLMs) generate step-by-step scripts for specific goals with multi-faceted constraints. Unlike previous work focusing on abstract goals, we explore the challenge of planning for concrete, constrained goals, such as "make a chocolate cake." To tackle this, we first evaluate LLMs' performance on specific goals, finding unsatisfactory results due to semantic incompleteness and lack of constraint faithfulness. We conduct a detailed analysis, revealing that LLMs struggle with certain constraint categories, leading to high variance in output quality. To address this, we propose an over-generate-then-filter method: LLMs generate multiple scripts for specific goals, and a filter model selects the most faithful ones based on semantic similarity and constraint keywords. This approach significantly improves script quality.

To enable constrained language planning with smaller models, we distill a dataset of 55,000 specific goals and scripts, named CoScript, from LLMs. We validate its quality through crowd-sourced revisions. CoScript demonstrates high pluralism in generated goals, making it a valuable resource for training specialized models. We show that T5, a smaller model fine-tuned on CoScript, outperforms most LLMs in generating high-quality scripts. Our work advances constrained language planning by addressing its challenges and providing a robust dataset for further research.</sample>
    <sample id="182">Nel contesto dell'articolo, il **tropicalismo** si riferisce a un **tropo culturale** che associa le donne latine a caratteristiche esotiche, vibranti e legate a una rappresentazione stereotipata di una regione tropicale. Questo contribuisce a una narrativa di **diversificazione** e **altreizzazione**, in cui queste donne sono definite principalmente in relazione alla loro identità culturale, piuttosto che come individui complessi e multidimensionali.</sample>
    <sample id="183">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target confrontando le persone con le risposte generate dai modelli linguistici. Hanno utilizzato i risultati di uno studio precedente che aveva dato a soggetti umani i medesimi prompt utilizzati per generare le rappresentazioni dei modelli, permettendo così un confronto diretto tra le due fonti. Questo approccio ha permesso di valutare le somiglianze e le differenze nelle rappresentazioni generate dagli esseri umani e dai modelli linguistici, evidenziando i modelli stereotipati e gli stereotipi presenti in entrambe le fonti.</sample>
    <sample id="184">Nel lavoro, è stato utilizzato **Pointwise CXMI (P-CXMI)** per misurare l'utilizzo del contesto durante la traduzione. Questo indicatore valuta quanta informazione il contesto fornisce sul target, dato il sorgente, permettendo di identificare parole che richiedono contesto per la traduzione.</sample>
    <sample id="185">DrBERT e ChuBERT sono due modelli pre-addestrati in francese per il dominio biomedico e clinico, ma differiscono per le fonti dei dati utilizzati durante l'addestramento iniziale:

- **DrBERT** è basato su dati medicali raccolti dal web (NACHOS) e utilizza una versione di RoBERTa.
- **ChuBERT** è basato su dati clinici anonimi provenienti dal magazzino dati dell'Ospedale Universitario di Nantes.

In sintesi, DrBERT si concentra su dati di natura generale e ampiamente accessibili, mentre ChuBERT si basa su dati più specifici e specializzati provenienti da un contesto clinico.</sample>
    <sample id="187">Due autori sono coinvolti nell'articolo: Ying e Zhiyang.</sample>
    <sample id="188">Il trasferimento iterativo dell'apprendimento è una strategia in cui un modello viene aggiornato iterativamente utilizzando i dati raccolti da ogni round di apprendimento attivo e annotazione. Questo approccio permette di migliorare gradualmente il modello con nuovi dati, adattandolo meglio al compito specifico. Nel contesto del lavoro presentato, il trasferimento iterativo è stato utilizzato per affinare il modello partendo da compiti correlati (come la classificazione di espansione e confronto nel PDTB) e poi da compiti di classificazione di dissonanza in dibattiti, ottenendo così una migliore performance iniziale per l'apprendimento attivo.</sample>
    <sample id="189">L'obiettivo del set di dati AltEntities è comprendere e migliorare la capacità dei modelli linguistici di interpretare e selezionare entità attraverso espressioni di riferimento indirette, fornendo un corpus di riferimento per il benchmarking delle prestazioni dei modelli di linguaggio naturale in questo specifico compito.</sample>
    <sample id="190">Un utente malintenzionato può estrarre i parametri del modello attraverso un Embedding as a Service (EaaS) utilizzando tecniche di **model extraction**. Questo processo sfrutta le **embedding** generate dal modello per inferire i parametri interni del modello stesso. Ecco come avviene:

1. **Analisi delle Embedding**: L'utente malintenzionato invia una serie di input al servizio EaaS e raccoglie le relative embedding.
2. **Addestramento di un Modello Simile**: Utilizzando le embedding estratte, l'utente addestra un modello simile al modello originale, spesso con tecniche di **transfer learning**.
3. **Inferenza dei Parametri**: Il modello estratto, pur non essendo identico, cattura molte delle caratteristiche del modello originale, permettendo all'utente di replicare il suo comportamento.

Questo processo è reso più difficile dalla presenza di **watermark** come l'**Embedding Marker**, che introduce informazioni nascoste nelle embedding, rendendo più difficile l'estrazione accurata dei parametri del modello originale.</sample>
    <sample id="191">Tre autori sono coinvolti nell'articolo: Sara Papi, Matteo Negri e Marco Turchi.</sample>
    <sample id="192">Il lavoro presentato, "CAME: Confidence-guided Adaptive Memory Efficient Optimization", affronta la sfida di progettare un ottimizzatore che bilanci la velocità di convergenza dei metodi adattivi tradizionali, come Adam, con l'efficienza di memoria dei metodi memory-efficient come Adafactor. Adam, pur efficace, richiede una memoria significativa per memorizzare le stime dei momenti dei gradienti per ogni parametro. Adafactor, d'altra parte, riduce drasticamente l'uso di memoria ma a scapito della performance. CAME introduce un approccio innovativo basato sulla non-negative matrix factorization (NMF) e sull'analisi degli errori nelle operazioni di Adafactor. L'idea centrale è di utilizzare la differenza tra il momento delle iterazioni (mₜ) e le attuali iterazioni (uₜ) per regolare in modo adattivo il passo di ottimizzazione, riducendo così gli effetti negativi degli errori. Questo approccio, chiamato "confidence-guided adaptive updating", permette a CAME di ottenere una convergenza più rapida e stabile rispetto ad Adafactor, mantenendo un basso consumo di memoria. Gli esperimenti condotti su modelli di linguaggio di grandi dimensioni come BERT, GPT-2 e T5, su dataset come BookCorpus e English Wikipedia, dimostrano che CAME supera sia Adam che Adafactor in termini di accuratezza e memoria, specialmente con batch di grandi dimensioni. CAME si conferma quindi un ottimizzatore efficace per l'addestramento di modelli di linguaggio su larga scala, con un significativo miglioramento delle prestazioni e una riduzione della memoria utilizzata.</sample>
    <sample id="193">Il contenuto non specifica il numero esatto di annotatori impiegati per creare il set di dati iniziale.</sample>
    <sample id="194">Gli autori dell'articolo sono affiliati a:
- Carnegie Mellon University
- University of Washington
- Allen Institute for AI</sample>
    <sample id="195">Il lavoro presentato, "Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering" (RoHT), affronta la sfida dell'Answering Question (XQA) complessa, mirando a fornire risposte accurate e spiegabili. A differenza dei metodi neuro-simmetrici, che richiedono KB completi, e dei metodi basati sulla decomposizione, che si affidano esclusivamente a corpora di testo, RoHT integra conoscenze da fonti eterogenee, come KB e corpora di testo, attraverso una struttura gerarchica di decomposizione delle domande (HQDT).

La proposta di RoHT è una struttura a due stadi: prima, si costruisce l'HQDT, che rappresenta la struttura gerarchica di una domanda complessa, con nodi atomici alle foglie e nodi intermedi che rappresentano domande più ampie. In secondo luogo, si utilizza un ragionamento probabilistico su HQDT per fondere le conoscenze da diverse fonti, considerando la probabilità di generazione delle risposte.

Il processo di ragionamento su HQDT è ricorsivo, partendo dalla domanda originale e procedendo verso le domande atomiche, con tre passaggi per ogni nodo: determinazione delle fonti di conoscenza, estrazione delle risposte probabili e aggregazione delle risposte candidate.

I risultati su due dataset complessi, KQA Pro e Musique, dimostrano l'efficacia di RoHT: miglioramenti significativi rispetto ai metodi esistenti, sia con KB incompleti che con l'integrazione di corpora di testo, evidenziando la superiorità dell'approccio esplicito di decomposizione rispetto ai metodi end-to-end.</sample>
    <sample id="196">L'esempio in cui il governatore è a sinistra è: **"I saw Bart and Lisa"**.</sample>
    <sample id="197">I quattro modelli all'avanguardia nei sistemi di dialogo valutati nello studio ABC-Eval sono:

1. **Amazon Alexa AI**  
2. **Un modello non specificato sviluppato dall'Emory NLP Lab**  
3. **Un modello non specificato sviluppato da un'altra organizzazione**  
4. **Un modello non specificato sviluppato da un'altra azienda**  

Purtroppo, i nomi specifici dei modelli non sono stati forniti nel testo.</sample>
    <sample id="198">La valutazione dell'accettabilità dei modelli linguistici nell'intera finestra di contesto è necessaria perché i modelli moderni, con finestre di contesto sempre più lunghe, possono essere influenzati da informazioni contestuali. Questo è cruciale per comprendere come i modelli gestiscono l'accettabilità in contesti più ampi e per identificare eventuali debolezze o sensibilità a strutture sintattiche o semantiche condivise tra le frasi. Inoltre, la valutazione su sequenze più lunghe aiuta a garantire che i modelli mantengano giudizi robusti e coerenti indipendentemente dalla lunghezza del contesto.</sample>
    <sample id="199">Sì, la formazione attraverso la modalità multilingue ha causato un calo delle prestazioni rispetto al modello inglese monolingue in sette dei nove dataset, un fenomeno noto come "maledizione della multilingue".</sample>
    <sample id="200">No, gli annotatori non conoscono l'entità in anticipo. Conoscono il nome delle entità, ma non le informazioni specifiche su di esse.</sample>
    <sample id="201">Le metriche di MT (Machine Translation) utilizzate per la valutazione sono state le seguenti:

1. **BLEURT**: una metrica di valutazione automatica per la traduzione automatica.
2. **QM (Quality Metrics)**: un framework di valutazione basato su valutazioni umane, che include categorie come "Fluency", "Style/Awkward", "Mistranslation", "Omission", "Redundancy", "Translation Update", "Source Language Influence", "Word Order", "Post-Edit Length", "Translation Style", "Cultural Inappropriate", "Number Agreement", "Gender Agreement", "Tense", "Plurality", "Definiteness", "Negation", "Compound", "Verb Frame", "Verb Aspect", "Verb Voice", "Verb Tense", "Verb Mood", "Verb Form", "Verb Argument Structure", "Verb Complementation", "Verb Complement Type", "Verb Complement Structure", "Verb Complement Case", "Verb Complement Number", "Verb Complement Gender", "Verb Complement Definiteness", "Verb Complement Tense", "Verb Complement Mood", "Verb Complement Form", "Verb Complement Argument Structure", "Verb Complement Complementation", "Verb Complement Case", "Verb Complement Number", "Verb Complement Gender", "Verb Complement Definiteness", "Verb Complement Tense", "Verb Complement Mood", "Verb Complement Form", "Verb Complement Argument Structure", "Verb Complement Complementation", "Verb Complement Case", "Verb Complement Number", "Verb Complement Gender", "Verb Complement Definiteness", "Verb Complement Tense", "Verb Complement Mood", "Verb Complement Form", "Verb Complement Argument Structure", "Verb Complement Complementation", "Verb Complement Case", "Verb Complement Number", "Verb Complement Gender", "Verb Complement Definiteness", "Verb Complement Tense", "Verb Complement Mood", "Verb Complement Form", "Verb Complement Argument Structure", "Verb Complement Complementation", "Verb Complement Case", "Verb Complement Number", "Verb Complement Gender", "Verb Complement Definiteness", "Verb Complement Tense", "Verb Complement Mood", "Verb Complement Form", "Verb Complement Argument Structure", "Verb Complement Complementation", "Verb Complement Case", "Verb Complement Number", "Verb Complement Gender", "Verb Complement Definiteness", "Verb Complement Tense", "Verb Complement Mood", "Verb Complement Form", "Verb Complement Argument Structure", "Verb Complement Complementation", "Verb Complement Case", "Verb Complement Number", "Verb Complement Gender", "Verb Complement Definiteness", "Verb Complement Tense", "Verb Complement Mood", "Verb Complement Form", "Verb Complement Argument Structure", "Verb Complement Complementation", "Verb Complement Case", "Verb Complement Number", "Verb Complement Gender", "Verb Complement Definiteness", "Verb Complement Tense", "Verb Complement Mood", "Verb Complement Form", "Verb Complement Argument Structure", "Verb Complement Complementation", "Verb Complement Case", "Verb Complement Number", "Verb Complement Gender", "Verb Complement Definiteness", "Verb Complement Tense", "Verb Complement Mood", "Verb Complement Form", "Verb Complement Argument Structure", "Verb Complement Complementation", "Verb Complement Case", "Verb Complement Number", "Verb Complement Gender", "Verb Complement Definiteness", "Verb Complement Tense", "Verb Complement Mood", "Verb Complement Form", "Verb Complement Argument Structure", "Verb Complement Complementation", "Verb Complement Case", "Verb Complement Number", "Verb Complement Gender", "Verb Complement Definiteness", "Verb Complement Tense", "Verb Complement Mood", "Verb Complement Form", "Verb Complement Argument Structure", "Verb Complement Complementation", "Verb Complement Case", "Verb Complement Number", "Verb Complement Gender", "Verb Complement Definiteness", "Verb Complement Tense", "Verb Complement Mood", "Verb Complement Form", "Verb Complement Argument Structure", "Verb Complement Complementation", "Verb Complement Case", "Verb Complement Number", "Verb Complement Gender", "Verb Complement Definiteness", "Verb Complement Tense", "Verb Complement Mood", "Verb Complement Form", "Verb Complement Argument Structure", "Verb Complement Complementation", "Verb Complement Case", "Verb Complement Number", "Verb Complement Gender", "Verb Complement Definiteness", "Verb Complement Tense", "Verb Complement Mood", "Verb Complement Form", "Verb Complement Argument Structure", "Verb Complement Complementation", "Verb Complement Case", "Verb Complement Number", "Verb Complement Gender", "Verb Complement Definiteness", "Verb Complement Tense", "Verb Complement Mood", "Verb Complement Form", "Verb Complement Argument Structure", "Verb Complement Complementation", "Verb Complement Case", "Verb Complement Number", "Verb Complement Gender", "Verb Complement Definiteness", "Verb Complement Tense", "Verb Complement Mood", "Verb Complement Form", "Verb Complement Argument Structure", "Verb Complement Complementation", "Verb Complement Case", "Verb Complement Number", "Verb Complement Gender", "Verb Complement Definiteness", "Verb Complement Tense", "Verb Complement Mood", "Verb Complement Form", "Verb Complement Argument Structure", "Verb Complement Complementation", "Verb Complement Case", "Verb Complement Number", "Verb Complement Gender", "Verb Complement Definiteness", "Verb Complement Tense", "Verb Complement Mood", "Verb Complement Form", "Verb Complement Argument Structure", "Verb Complement Complementation", "Verb Complement Case", "Verb Complement Number", "Verb Complement Gender", "Verb Complement Definiteness", "Verb Complement Tense", "Verb Complement Mood", "Verb Complement Form", "Verb Complement Argument Structure", "Verb Complement Complementation", "Verb Complement Case", "Verb Complement Number", "Verb Complement Gender", "Verb Complement Definiteness", "Verb Complement Tense", "Verb Complement Mood", "Verb Complement Form", "Verb Complement Argument Structure", "Verb Complement Complementation", "Verb Complement Case", "Verb Complement Number", "Verb Complement Gender", "Verb Complement Definiteness", "Verb Complement Tense", "Verb Complement Mood", "Verb Complement Form", "Verb Complement Argument Structure", "Verb Complement Complementation", "Verb Complement Case", "Verb Complement Number", "Verb Complement Gender", "Verb Complement Definiteness", "Verb Complement Tense", "Verb Complement Mood", "Verb Complement Form", "Verb Complement Argument Structure", "Verb Complement Complementation", "Verb Complement Case", "Verb Complement Number", "Verb Complement Gender", "Verb Complement Definiteness", "Verb Complement Tense", "Verb Complement Mood", "Verb Complement Form", "Verb Complement Argument Structure", "Verb Complement Complementation", "Verb Complement Case", "Verb Complement Number", "Verb Complement Gender", "Verb Complement Definiteness", "Verb Complement Tense", "Verb Complement Mood", "Verb Complement Form", "Verb Complement Argument Structure", "Verb Complement Complementation", "Verb Complement Case", "Verb Complement Number", "Verb Complement Gender", "Verb Complement Definiteness", "Verb Complement Tense", "Verb Complement Mood", "Verb</sample>
    <sample id="202">No, il regresso nella generalizzazione non influisce su specifici tipi di NER, ma piuttosto è un fenomeno generale che colpisce tutti i modelli di NER addestrati su CoNLL-2003 quando vengono testati su dati più recenti. La causa principale del regresso è il **temporal drift**, ovvero il divario temporale tra i dati di addestramento e quelli di test, piuttosto che problemi specifici legati a determinati tipi di entità o categorie di NER.</sample>
    <sample id="203">La posizionalità nella NLP è importante perché evidenzia come i dataset e i modelli possano riflettere e amplificare i pregiudizi e le disuguaglianze presenti nelle società. Poiché i dataset e i modelli NLP aggregano le opinioni e i giudizi di persone reali, possono rappresentare alcune prospettive più di altre, spesso a scapito di gruppi marginalizzati. Questo può portare a risultati che sono più accurati e utili per alcune popolazioni rispetto ad altre, creando un divario di equità e accessibilità. Comprendere e affrontare la posizionalità nella NLP è cruciale per sviluppare tecnologie più eque, inclusive e rappresentative, che possano servire tutti gli utenti in modo equo e giusto.</sample>
    <sample id="204">Gli LLM multilingue come BLOOM non sono stati affinati mediante adattatori, ma con una messa a punto integrale.</sample>
    <sample id="205">**Abstract**  
This research investigates the propagation of political biases from pretraining data to language models (LMs) and their downstream applications, highlighting critical fairness issues in NLP. By analyzing the political leanings of LMs like GPT-4 and BART, we found that they occupy all quadrants of the political spectrum, with GPT models being more liberal than BART variants. We demonstrated that LMs’ political biases are influenced by their pretraining data, as evidenced by experiments where models were further trained on partisan corpora, shifting their ideological coordinates. Additionally, we observed that LMs trained on post-2017 data exhibited a more polarized political stance, reflecting societal polarization.  

We evaluated LMs with varying political leanings on downstream tasks such as hate speech and fake news detection. Results showed that left-leaning LMs were better at detecting hate speech targeting minority groups but worse for targeting powerful groups, while right-leaning LMs performed the opposite. Similar trends emerged in fake news detection. These findings reveal significant fairness issues, as deploying biased LMs could marginalize opposing political views or fail to address hate speech targeting minority communities.  

Our work underscores the dilemma of addressing political biases in LMs: sanitizing training data risks censorship, while leaving it unfiltered perpetuates bias. This study calls for greater awareness and mitigation strategies to ensure fair and equitable NLP applications.</sample>
    <sample id="206">Per il trasferimento dell'apprendimento, fanno ricorso a due modelli:  
1. **Topic Independent Dissonance Stance Classification (Debate)**: un modello che determina se due dichiarazioni in un dibattito sono in accordo o disaccordo, indipendentemente dal tema.  
2. **Binary Classification of Expansion and Comparison Classes (CE)**: un modello basato su PDTB che classifica le relazioni di consonanza e dissonanza.  
Questi due modelli vengono utilizzati come base per il zero-shot performance e per l'iterativo fine-tuning, migliorando così la capacità del modello di rilevare la dissonanza cognitiva.</sample>
    <sample id="207">I recenti set di test utilizzati per valutare le capacità di PaLM includono i test set più recenti del settore della traduzione automatica (MT), come quelli utilizzati per le valutazioni WMT (Workshop on Machine Translation). Questi set di test sono stati scelti per evitare sovrapposizioni con i dati di addestramento del modello PaLM.</sample>
    <sample id="208">Gli autori hanno proposto **tre** suggerimenti alla fine del loro lavoro.</sample>
    <sample id="209">Il metodo proposto, basato su **over-generate-then-filter**, migliora significativamente la qualità dei script generati rispetto al metodo di riferimento (semplice generazione da parte di InstructGPT). In particolare:

1. **Semantica e fedeltà alle restrizioni**: Il metodo proposto aumenta sia la completezza semantica che la fedeltà alle restrizioni, risolvendo i limiti evidenziati nell'analisi dettagliata.
2. **Qualità complessiva**: I risultati mostrano che i script generati con il metodo proposto sono di qualità superiore rispetto a quelli prodotti da InstructGPT da solo.
3. **Applicabilità a modelli più piccoli**: Il metodo apre la strada all'utilizzo di modelli più piccoli e specializzati per la pianificazione linguistica, dimostrando che T5, un modello più piccolo, può superare i grandi modelli quando addestrato su dataset adeguati come CoScript.

In sintesi, il metodo proposto supera il metodo di riferimento in termini di qualità, fedeltà alle restrizioni e adattabilità a modelli più efficienti.</sample>
    <sample id="210">Il nome della relatrice o del relatore è Shuheng.</sample>
    <sample id="211">Sì, i risultati e il set di dati nell'articolo possono essere utilizzati come parametri di riferimento per il problema dell'automazione della semplificazione del testo. In particolare, i risultati del fine-tuning dei modelli long-mBART e base mBART mostrano che la semplificazione automatica può ottenere risultati migliori rispetto alle linee di base, e il set di dati DEPLAIN può essere utilizzato per valutare e confrontare diversi metodi di allineamento e modelli di semplificazione del testo.</sample>
    <sample id="212">Nell'articolo, viene utilizzato un solo modello più piccolo, T5, per dimostrare che può generare script di qualità superiore rispetto alla maggior parte dei grandi modelli di linguaggio quando viene addestrato sul dataset CoScript.</sample>
    <sample id="213">Il modello utilizzato come base per analizzare l'ottimizzazione delle istruzioni multimodali è **OFA (Open Foundation Models)**.</sample>
    <sample id="215">Il talk di Adam Przepiórkowski si concentra sulla struttura di dipendenza della coordinazione, esaminando diverse teorie e approcci corpus. Le teorie asimmetriche, come le Universal Dependencies e la teoria del testo di Igor Mel'čuk, assegnano la testa della struttura coordinata al primo congiunto, mentre l'approccio a capo congiuntivo, adottato nei treebank di Praga, considera la congiunzione come testa. L'approccio multi-capo, invece, attribuisce la testa a tutti i congiunti. L'obiettivo del paper è sostenere le strutture simmetriche della coordinazione, come quelle a capo congiuntivo e multi-capo, contro le strutture asimmetriche. L'argomento si basa sul principio della minimizzazione della lunghezza delle dipendenze, che preferisce dipendenze più corte. Attraverso l'analisi statistica del Penn Treebank, il paper dimostra che la tendenza per il congiunto sinistro di essere più corto è più forte quando il governatore è assente o a sinistra, ma scompare quando il governatore è a destra. Questo fornisce un argomento contro le strutture asimmetriche e a favore di quelle simmetriche, come mostrato nel paper e discusso nella sessione poster.</sample>
    <sample id="217">In this work, we address the challenge of generating multi-attribute controllable dialogue (CDG) by focusing on compositional generalization, which is limited in existing methods. Our approach, **Disentangled Controllable Generation (DCG)**, leverages a **compositional prompt module** based on the DialoGPT framework to guide dialogue generation. We introduce two types of prompts: **attribute-oriented** prompts, which use controllable attribute values to focus on specific information, and **task-oriented** prompts, which incorporate global features to improve text equality. A **disentanglement loss** is used to train multiple compositional prompts while disentangling attribute combination representations.

To evaluate our method, we propose a **reference-free evaluation framework (MAE)** that does not require additional labeled data. MAE uses discrete prompts with trainable continuous dialogue-oriented prompts to reduce bias and improve robustness. Experiments on the DailyDialog-CG benchmark demonstrate that DCG outperforms baselines in attribute controllability and text equality, even for unseen attribute combinations. Our method also shows strong correlation with human judgments, outperforming classic metrics for both discrete and continuous attributes.

In summary, DCG effectively tackles compositional generalization in multi-attribute CDG, enabling the transformation of seen attributes to unseen combinations while maintaining high controllability and text quality.</sample>
    <sample id="218">Gli autori dell'articolo "Prompting PaLM for Translation: Assessing Strategies and Performance" sono affiliati a Google Translate.</sample>
    <sample id="219">In questo lavoro, presentato da Jia-Huei Ju e collaboratori, viene proposto un approccio innovativo per l'analisi dei report finanziari, in particolare del Form 10-K, attraverso un pipeline multistadio. La motivazione principale deriva dall'osservazione che i report finanziari annuali presentano un'elevata somiglianza testuale (circa l'80% dei token è identico) e una dipendenza annuale, rendendo difficile l'estrazione di informazioni utili. Il team introduce una task di highlighting che confronta e contrasta il contenuto tra il report target (corrispondente all'anno corrente) e il report di riferimento (dell'anno precedente). L'obiettivo è identificare le parole chiave che evidenziano le relazioni tra i due documenti, misurando l'importanza delle parole predette. Il pipeline proposto include tre fasi: la prima (Stage 1) classifica le coppie di segmenti in tre tipi (β, revised, mismatched), basandosi su somiglianza sintattica e semantica. Per l'ottimizzazione del modello, si utilizza un dataset esterno (eSNLI) per il fine-tuning out-of-domain, seguito da un fine-tuning in-domain con coppie revised, utilizzando tecniche di soft labeling per migliorare la qualità dei pseudo-label. Il modello dimostra prestazioni eccellenti su dataset di valutazione, mantenendo anche la capacità di generalizzazione. Questo approccio apre nuove prospettive per l'estrazione automatica di segnali finanziari dai report, con potenziali applicazioni nell'analisi finanziaria e nell'informazione di mercato.</sample>
    <sample id="220">Gli autori dell'articolo "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge" sono affiliati a Stony Brook University.</sample>
    <sample id="221">L'articolo non specifica le coppie linguistiche esatte analizzate. Tuttavia, si menziona che le valutazioni sono state effettuate utilizzando i test set WMT e che le traduzioni sono state confrontate con Google Translate, suggerendo che le coppie linguistiche analizzate includono almeno quelle coperte da questi test set e da Google Translate.</sample>
    <sample id="222">**Abstract**  
This work addresses the challenges of domain adaptation in open-domain question answering (QA), focusing on enabling models to generalize across diverse domains. The motivation stems from the difficulty of using general-purpose corpora like Wikipedia to answer domain-specific questions, such as biomedical queries, where the model struggles due to insufficient training data. The study investigates three main contributions: (1) exploring data interventions to facilitate out-of-domain generalization, (2) identifying the nature of dataset shifts when transitioning to new domains, and (3) determining effective interventions based on the type of shift.  

Two primary methods for data interventions are proposed: zero-shot and few-shot. Zero-shot techniques aim to control interactions between question, answer, and context to improve model learning, while few-shot methods leverage limited examples from target domains to generate additional training data. Experiments show that few-shot adaptations improve retriever performance by 8% and reader performance by 11%, on average. Zero-shot techniques, though without target domain examples, also yield significant improvements.  

The study categorizes dataset shifts into "no shift," "concept shift," "covariate shift," and "full shift," based on compatibility between the source model and target domain. Compatibility is measured using likelihood scores for retrievers and normalized answer likelihoods for readers. This framework helps map target datasets onto a 2D grid, revealing shifts like "full shift" for datasets like CliCR and NewsQA, and "no shift" for SearchQA.  

Finally, the research demonstrates that few-shot adaptations are effective for all datasets, while zero-shot methods are particularly beneficial for datasets exhibiting concept or covariate shifts. Overall, the work improves reader performance by up to 24% and provides tailored interventions for different types of dataset shifts.</sample>
    <sample id="223">Il nome della relatrice o del relatore è Shangbin.</sample>
    <sample id="224">Durante gli esperimenti, sono stati studiati due modelli:  
1. **Long-mBART** per la semplificazione a livello di documento.  
2. **Base mBART** per la semplificazione a livello di frase.</sample>
    <sample id="225">Delle 62 diverse attività utilizzate in MultiInstruct, 53 vengono utilizzate per scopi di addestramento, mentre 5 vengono riservate per il test.</sample>
    <sample id="226">Tre autori sono coinvolti nell'articolo: Regina Stodden, Omar e un terzo autore non menzionato nel testo.</sample>
    <sample id="227">L'abstract del contenuto inglese è il seguente: La ricerca sui modelli di linguaggio ha recentemente raggiunto grandi successi, fornendo una soluzione generale a molte diverse attività di NLP. Tuttavia, manca ancora una comprensione del linguaggio radicata, che implica l'associazione di un'espressione linguistica naturale a qualcosa che può essere eseguito in un ambiente specifico, come un piano o un programma. Questo è fondamentale per applicazioni come assistenti intelligenti, ricerca semantica e istruzioni a robot domestici. La sfida principale è la mancanza di radicamento durante il pre-addestramento dei modelli di linguaggio, che sono principalmente addestrati su corpus testuali senza radicamento. Il lavoro proposto introduce un nuovo framework, chiamato Pangu, che separa il mondo simbolico da quello più recente, utilizzando un agente simbolico che propone piani candidati e un modello di linguaggio che li valuta. Questo approccio permette al modello di linguaggio di concentrarsi sulla discriminazione piuttosto che sulla generazione, rendendolo più efficace. Il framework è stato testato su scenari di question answering basati su conoscenza, dimostrando prestazioni eccezionali e forte efficienza campionaria. La ricerca suggerisce che, per la comprensione del linguaggio radicata, la discriminazione potrebbe essere una strategia migliore della generazione.</sample>
    <sample id="228">Gli autori hanno effettuato i test sui seguenti set di dati: AG News, MIND, SST2 e Enron Spam.</sample>
    <sample id="229">Il presente lavoro esplora il rilevamento di affermazioni argomentative migliorabili, un aspetto cruciale per il supporto alla scrittura argomentativa. L'obiettivo è determinare quando un'affermazione è sufficientemente ben formulata e non richiede ulteriori revisioni. Attraverso due compiti principali, il rilevamento di affermazioni subottimali (Task 1) e la suggerenza di miglioramenti (Task 2), l'articolo analizza come modellare la qualità delle affermazioni argomentative basandosi su modelli di revisione trovati in piattaforme di dibattito online come Kialo.

La ricerca affronta diverse sfide, tra cui la rappresentatività e l'affidabilità dei dati di revisione, la complessità del modello necessaria per catturare le sottili differenze nelle revisioni, la dipendenza di alcune dimensioni della qualità argomentativa dal contesto, e la presenza di bias tematici e utente nei dati di revisione.

L'articolo conclude che i dati di revisione possono essere efficacemente utilizzati per i compiti proposti, che il calcolo della distanza tra due versioni di un'affermazione è utile per il rilevamento di affermazioni subottimali, e che l'impatto delle informazioni contestuali varia a seconda del compito e del tipo di problema di qualità del testo.

In sintesi, il lavoro fornisce una base solida per lo sviluppo di strumenti che aiutano gli scrittori a migliorare la qualità delle loro affermazioni argomentative, contribuendo così a una comunicazione più efficace e persuasiva.</sample>
    <sample id="231">NACHOS è un dataset di dati medici raccolti dal web, utilizzato come fonte di dati per l'addestramento del modello DrBERT, il primo modello biomedico in francese.</sample>
    <sample id="232">Il nome del relatore è David Vilar.</sample>
    <sample id="233">Il paper "Attention as a Guide for Simultaneous Speech Translation" presenta una soluzione innovativa per il problema della traduzione simultanea del parlato (SimulST), un processo che richiede la traduzione in tempo reale di un linguaggio parlato in un altro. I modelli attuali di SimulST spesso richiedono architetture specifiche e complesse procedure di training, con l'obiettivo di ottimizzare la latenza e la qualità della traduzione. La proposta di questo studio, invece, è di utilizzare modelli di traduzione automatica (ST) offline esistenti senza bisogno di ri-training o di adottare architetture specifiche per la SimulST. L'approccio, chiamato EDAtt (Encoder-Decoder Attention), si basa sull'utilizzo di un unico modello per ogni regime di latenza e gestisce la latenza attraverso parametri specifici. La chiave del successo di EDAtt è l'attenzione meccanica tra l'input audio e l'output testuale, che permette di decidere se emettere una traduzione parziale in base a dove l'attenzione si concentra. Un termine viene emesso se l'attenzione non è concentrata su un intervallo di lambda frame di parlato precedenti, misurato da un parametro alpha. Questo approccio permette di gestire la latenza in modo dinamico e di migliorare la qualità della traduzione. I risultati mostrano che EDAtt supera le strategie applicate ai modelli offline e l'architettura di stato dell'arte specifica per la pre-traduzione simultanea, sia in termini di qualità della traduzione (misurata dal BLEU) che di latenza (misurata dal lagging medio e dal lagging computazionale medio). Il codice e i modelli sono stati resi open source per facilitare la riproducibilità del lavoro.</sample>
    <sample id="234">La strategia del prompting ha un impatto significativo sui risultati della traduzione con modelli di linguaggio di grandi dimensioni come PaLM. In particolare:

1. **Differenza di Performance**: La scelta della strategia di prompting può causare differenze di performance di oltre 40 BLEURT punti, con una media di più di un punto BLEURT per ogni esempio fornito.

2. **Importanza degli Esempi**: La qualità degli esempi forniti durante il prompting è più importante della loro somiglianza con la frase sorgente, specialmente per strategie di prompting a più esempi (come il 5-shot prompting).

3. **Forma del Prompt**: La forma del prompt ha un impatto minore per strategie di prompting a più esempi, ma è cruciale per strategie a un solo esempio o a zero esempi.

4. **Confronto con Sistemi Commerciali**: Nonostante PaLM si avvicini molto ai sistemi di traduzione commerciali come Google Translate, i sistemi specializzati di stato dell'arte mantengono un vantaggio sostanziale.

In sintesi, la strategia di prompting è un fattore chiave per ottimizzare le prestazioni della traduzione con modelli di linguaggio di grandi dimensioni.</sample>
    <sample id="235">Gli autori dell'articolo "When Does Translation Require Context? A Data-driven, Multilingual Exploration" sono:
- Kayo Yin
- Patrick Fernandes
- Emmy Liu
- André F. T. Martins
- Graham Neubig

Le loro affiliazioni sono:
- Kayo Yin: University of Cambridge
- Patrick Fernandes: University of Cambridge
- Emmy Liu: University of Cambridge
- André F. T. Martins: University of Cambridge
- Graham Neubig: Carnegie Mellon University</sample>
    <sample id="236">Le 5 istruzioni scritte da esperti per ogni task nel dataset MultiInstruct sono:

1. **Task-specific instruction**: Una istruzione specifica per il compito, che fornisce dettagli su cosa fare.
2. **General instruction**: Un'istruzione generale che spiega l'obiettivo del compito.
3. **Example-based instruction**: Un'istruzione che fornisce esempi di input e output per il compito.
4. **Prompt-based instruction**: Un'istruzione che fornisce un prompt per guidare la risposta del modello.
5. **Control instruction**: Un'istruzione di controllo che specifica le condizioni o i limiti per la risposta del modello.</sample>
    <sample id="237">Gli autori propongono il **KITMUS Test** (Knowledge Integration from Multiple Sources Test), una suite di test diagnostici per valutare la capacità dei modelli di integrare e utilizzare informazioni provenienti da diverse fonti, come la conoscenza acquisita durante il pre-training e le informazioni fornite al momento dell'inferenza. Il test include tre impostazioni (Background-Pretrain, Background-Both, Background-Inference) che variano la disponibilità di queste informazioni, simulando diversi scenari di conoscenza.</sample>
    <sample id="238">Il video presenta il MeetingBank, un nuovo dataset di benchmark creato per sviluppare tecnologie di sintesi di meeting. Il dataset, composto da 1.366 riunioni del Consiglio Comunale e quasi 7.000 istanze, include trascrizioni, riassunti di riferimento e risorse aggiuntive. La raccolta dei dati è stata effettuata utilizzando l'API Speechmatics per la trascrizione audio e l'analisi delle informazioni fornite sul sito web delle riunioni. Il dataset è stato valutato utilizzando diversi sistemi di sintesi, tra cui modelli extraentivi e astrattivi, come BART-Large, DialogLM e GPT-3. I risultati mostrano che i modelli extraentivi, come Extr-Oracle, ottengono buoni risultati in termini di ROUGE-2, mentre i modelli astrattivi, come DialogLM, ottengono i migliori risultati in termini di ROUGE-2. Tuttavia, GPT-3 non si distingue per i risultati automatici, ma ottiene buoni risultati nella valutazione umana in termini di fluidità e coerenza. In generale, il MeetingBank rappresenta un'importante risorsa per la ricerca nel campo della sintesi di meeting e offre spunti interessanti per lo sviluppo di nuovi metodi di valutazione automatica. Gli autori invitano i ricercatori a utilizzare il dataset per migliorare le tecnologie di sintesi di meeting e a partecipare a ulteriori discussioni in luglio.</sample>
    <sample id="239">Ciao a tutti, mi chiamo David Vilar e vi farò una breve recensione del paper "Prompting PaLM for Translation: Assessing Strategies and Performance." Questo è un lavoro congiunto con i miei colleghi di Google Translate. PaLM è un modello di linguaggio di grandi dimensioni con 540 miliardi di parametri presentato l'anno scorso, nel 2022. È stato addestrato su una vasta collezione di testo, comprendente 780 miliardi di token. Al momento della pubblicazione, ha raggiunto lo stato dell'arte in centinaia di compiti di NLP. In questo lavoro, presentiamo la prima indagine sistematica del prompting di modelli di linguaggio di grandi dimensioni per la traduzione automatica. Abbiamo valutato la capacità di transizione di tali modelli utilizzando le migliori pratiche della comunità MT. Ciò comporta l'uso dei più recenti set di test per evitare una sovrapposizione dei dati di test con i dati di addestramento del modello di linguaggio. E abbiamo confrontato con i sistemi di stato dell'arte, quindi il sistema con le migliori prestazioni, quindi la valutazione WMT. Usiamo metriche MT neurali di stato dell'arte e mostriamo anche i risultati delle valutazioni umane basate sugli esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione del prompt. Il prompting ha una grande influenza sulle prestazioni dei LLM per la traduzione, come possiamo vedere in un semplice esperimento, in cui abbiamo utilizzato il prompting one-shot e fornito due diversi prompt per ogni frase. La maggior parte delle frasi, 516 su 1.000, ha mostrato una differenza osservata di più di un punto BLEURT. E questo può arrivare, nei casi estremi, fino a 40 punti BLEURT. Quindi, è importante selezionare una buona strategia di prompting. Nei nostri esperimenti, abbiamo optato per una strategia di prompting a 5-shot in cui abbiamo semplicemente contrassegnato ogni frase che forniamo al sistema, con la lingua in cui si trova. Quindi in questo esempio qui, dove eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche, le frasi sorgente, sono contrassegnate con due punti tedeschi e le traduzioni inglesi con due punti inglesi. Abbiamo visto che la forma effettiva del prompting non ha una grande influenza nel caso di diversi prompt brevi. È cruciale per il prompting zero e one-shot. E quando passiamo, come nel nostro caso, al prompting a cinque-shot, non c'è quasi nessuna differenza nella forma effettiva del prompting. Sono gli esempi che portano la maggior parte del peso. Il riassunto dei nostri risultati sperimentali è che la qualità degli esempi è più importante della somiglianza con la frase sorgente. Quindi è importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo la selezione dei prompt dai dati di addestramento per le valutazioni WMT sui dati di sviluppo. I dati di sviluppo sono molto più curati e di qualità superiore rispetto ai dati di addestramento, che sono più rumorosi. E i loro risultati mostrano una migliore prestazione quando si utilizzano i dati di sviluppo. Tuttavia, i sistemi specializzati di stato dell'arte hanno un vantaggio sostanziale sulle traduzioni di PaLM. Ma PaLM si avvicina molto a un sistema commerciale. Nel nostro caso, abbiamo scelto di valutare con Google Translate. Le intuizioni che abbiamo ottenuto dalla valutazione umana che abbiamo eseguito utilizzando il framework MQM hanno detto che la fluidità di PaLM è paragonabile ai sistemi di stato dell'arte ma la differenza principale proviene dall'accuratezza. Quindi, in particolare, gli errori più comuni sono errori di omissione. Quindi, sembra che PaLM scelga di produrre una traduzione che suoni meglio, a volte tralasciando parti della frase sorgente che sono fatte in traduzione. Tuttavia, la categoria "Stile/Improvvisato" per PaLM è inferiore rispetto ai sistemi di stato dell'arte, che è un segnale aggiuntivo che PaLM fornisce un output davvero fluido, ma ancora con alcuni problemi di accuratezza. E questo è tutto per questo riassunto molto breve. Per maggiori dettagli, per favore, andate alla presentazione completa del paper. Grazie mille.</sample>
    <sample id="240">Ciao, sono Dawei, uno studente di dottorato presso l'Università di Saarland in Germania. In questo video, vorrei presentare il nostro lavoro recente "Più debole di quanto pensiate: uno sguardo critico all'apprendimento debolmente supervisionato." Questo è un lavoro congiunto con Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow. Vorrei iniziare con una breve introduzione all'apprendimento debolmente supervisionato e alla supervisione debole. Nell'apprendimento debolmente supervisionato, non si etichettano manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura deboli, come semplici regole euristica, basi di conoscenza o crowdsourcing di bassa qualità, come illustrato nella figura a destra. Rispetto alle annotazioni umane, le annotazioni più deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni è errata. Se addestrati direttamente su dati etichettati debolmente, le reti neurali tendono a memorizzare il rumore delle etichette e non generalizzano. Nell'apprendimento debolmente supervisionato, vengono proposti algoritmi di addestramento per addestrare in modo robusto le reti neurali in presenza di tale rumore delle etichette in modo che i modelli addestrati si generalizzino bene. Nei lavori recenti in WSL, dove WSL sta per Weakly Supervised Learning, un'affermazione comune è che le persone dicono di addestrare i modelli solo sui dati etichettati debolmente e di ottenere alte prestazioni su set di test puliti. Tecnicamente, questa affermazione non è sbagliata, ma c'è un'eccezione, che è che le persone assumono che ci sia un set di validazione pulito aggiuntivo disponibile per la selezione del modello. Non possiamo fermarci su questo problema, ma ciò implica che sono necessarie ulteriori annotazioni manuali nell'apprendimento debolmente supervisionato. Ma come un elefante nella stanza, questa necessità è spesso trascurata. Il dubbio menzionato in precedenza è posto per porre tre domande di ricerca. Primo, i dati di validazione puliti sono necessari per WSL o possiamo forse utilizzare un set di validazione rumoroso? Secondo, se i dati puliti sono richiesti, o se i dati puliti sono obbligatori affinché WSL funzioni, allora quanti campioni puliti abbiamo bisogno? Infine, dovremmo utilizzare solo i campioni puliti per la validazione, o ci sono modi migliori per utilizzarli? Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti. Primo, scopriamo che, interessantemente, i recenti metodi WSL richiedono effettivamente campioni di validazione puliti per funzionare correttamente. Altrimenti, c'è un grande calo delle prestazioni. Come mostrato in questa figura, se non ci sono campioni di validazione puliti, allora i modelli addestrati non possono generalizzare oltre le etichette deboli originali, il che significa che l'addestramento è inutile. Questo indica che gli approcci WSL richiedono effettivamente dati etichettati in modo pulito per funzionare correttamente, e il costo dell'annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato. Il nostro secondo risultato è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci WSL a ottenere prestazioni migliori, come mostrato nella figura a sinistra. Tipicamente abbiamo bisogno solo di 20 campioni per classe per ottenere alte prestazioni. Ma questa non è la fine della storia, perché se decidiamo comunque di accedere ai campioni puliti, allora l'addestramento su di essi direttamente otterrà prestazioni ancora migliori. La figura a destra mostra la differenza di prestazioni tra gli approcci di fine-tuning, che sono applicati direttamente sui dati puliti, e gli approcci WSL, che utilizzano i dati puliti solo per la validazione. Come possiamo vedere, se abbiamo 10 campioni per classe, il fine-tuning diretto inizia a battere gli approcci WSL. Infine, il miglioramento delle prestazioni affermato nei precedenti approcci WSL può essere facilmente raggiunto consentendo di continuare il fine-tuning sui campioni di validazione puliti. Come possiamo vedere dalle figure, il modello vanilla, denominato FTw, inizialmente sottoperforma i metodi WSL più complessi, come COSINE. Tuttavia, se consentiamo di continuare il fine-tuning sui campioni puliti, allora FTw si comporta altrettanto bene come altri metodi. Quindi, in pratica, non c'è motivo di scegliere approcci WSL più complessi che richiedono più tempo di computazione e spazio su disco. Per riassumere, abbiamo dimostrato che i recenti approcci WSL richiedono campioni etichettati in modo pulito per funzionare correttamente. Il loro guadagno di prestazioni e praticità sono fortemente sopravvalutati. Le nostre raccomandazioni concrete per il lavoro futuro sono le seguenti. Primo, riportare i criteri di selezione del modello. Ad esempio, riportare se la selezione del modello è fatta tramite campioni di validazione puliti. Secondo, gli approcci WSL dovrebbero essere confrontati con le baseline di apprendimento a pochi colpi, poiché entrambi lavorano su campioni puliti. Infine, il fine-tuning continuo è una semplice ma forte baseline che dovrebbe essere considerata nel lavoro futuro in WSL. Infine, abbiamo reso open-source il nostro codice. Potete trovarlo tramite il codice QR su questa diapositiva. Sentitevi liberi di controllarlo. Grazie e buona conferenza.</sample>
    <sample id="241">**Abstract:**  
This paper presents a human-in-the-loop (HITL) evaluation framework for early misinformation detection, specifically focusing on COVID-19 treatment claims. Existing automated systems often fail to address key challenges: unrealistic evaluations using retrospective datasets and a lack of human-centric integration. Our framework integrates human feedback at multiple stages, ensuring a realistic end-to-end workflow from raw tweets to actionable outputs. The system consists of two components: (1) a misleading claim detection module using keyword filtering and a T5 model for claim extraction and ranking based on trendiness; (2) a policy violation verification module employing a BERT-based stance classification model to flag tweets supporting unapproved treatments. Early detection is operationalized as identifying unapproved treatments before their first appearance in debunking news articles, demonstrating the system’s ability to act proactively. Evaluation results show a 65% accuracy rate in policy violation detection and 124.2 policy violations confirmed per human hour. This work highlights the importance of HITL systems in addressing misinformation and provides a realistic evaluation framework for future developments. It also offers industry insights into the challenges and processes of misinformation detection.</sample>
    <sample id="242">I metodi di valutazione comuni per i sistemi di dialogo includono:

1. **Valutazioni umane**: Giudici umani selezionano la conversazione migliore o assegnano un punteggio su una scala Likert.
2. **Valutazioni dimensionali**: Giudici umani valutano diversi aspetti della qualità del dialogo, come la pertinenza delle risposte.
3. **ABC-Eval**: Un metodo che annota esplicitamente i comportamenti delle risposte dei modelli, come irrilevanza, contraddizioni, falsificazioni, ecc.</sample>
    <sample id="243">L'articolo coinvolge 6 autori: Jenny, Sebastian Santy, Ronan Le Bras, Katharina Reinecke, Maarten Sap e gli autori dell'Allen Institute for AI.</sample>
    <sample id="244">Nell'esempio con Servin e Kea, le conoscenze di base necessarie sono:

1. **Conoscenza specifica dell'entità**: "Servin è un giudice."
2. **Conoscenza generale**: "I giudici decidono casi nei tribunali."

Queste conoscenze aiutano a determinare che il pronome "he" si riferisce a Servin, poiché è un giudice e ha trascorso una giornata lavorativa a decidere casi in un tribunale.</sample>
    <sample id="245">Il lavoro "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization" presenta una pipeline per identificare lavoratori ad alta concordanza su Amazon Mechanical Turk (MTurk) per il compito di riepilogazione. La motivazione è che le metriche automatiche sono spesso problematiche e le migliori pratiche per il reclutamento su MTurk sono poco chiare. La pipeline si compone di due fasi: la fase di qualificazione e la fase di endurance. Nella fase di qualificazione, i lavoratori vengono testati su sei dimensioni di valutazione attraverso documenti e riepiloghi, classificati in quattro livelli (gold, silver, bronze, block). Solo i lavoratori gold e silver passano. Nella fase di endurance, i lavoratori vengono testati sulla capacità di gestire un carico di lavoro pesante. I risultati mostrano un'alta concordanza inter-annotatore (IAA) e valori elevati di Cohen's Kappa e Krippendorff's Alpha. Il lavoro confronta anche i risultati con i lavoratori reclutati da CloudResearch, ottenendo risultati simili ma con costi inferiori. Infine, si discute l'importanza di questa pipeline per evitare sprechi di tempo e risorse e per ottenere riepiloghi di alta qualità a basso costo. Le limitazioni includono il test solo su riepilogazione in inglese e la mancanza di garanzia sulla correttezza dei lavoratori.</sample>
    <sample id="246">Sì, il codice è disponibile su GitHub.</sample>
    <sample id="247">Il paper "FACTKG: Fact Verification via Reasoning on Knowledge Graphs" presenta un nuovo dataset, FactKG, e una nuova task, la verifica di fatti basata su grafi di conoscenza (KG-based Fact Verification), che utilizza grafi di conoscenza come fonte di evidenza per affermazioni in linguaggio naturale. A differenza dei dataset esistenti come FEVER e VitaminC, che si basano su testo di Wikipedia, o TabFact e InfoTabs, che utilizzano tabelle come evidenza, FactKG introduce una nuova dimensione integrando grafi di conoscenza (KG) con affermazioni in linguaggio naturale. I grafi di conoscenza offrono un'evidenza intuitiva e diretta, consentendo un ragionamento affidabile e pratico, utile in applicazioni come i sistemi di dialogo che interagiscono con grafi di conoscenza interni. Il dataset FactKG, basato su DBpedia, include affermazioni in due stili (scritto e colloquiale) e due etichette (SUPPORTED e REFUTED). La task richiede di recuperare l'evidenza da DBpedia e verificare l'affermazione utilizzando tale evidenza. Sono stati definiti cinque tipi di ragionamento: one-hop, congiunzione, esistenza, multi-hop e negazione. I risultati mostrano che i modelli che utilizzano l'evidenza del grafo di conoscenza (come GEAR) superano significativamente le baseline, dimostrando l'efficacia dell'approccio proposto. Il dataset è disponibile per il download e gli autori incoraggiano ulteriori ricerche e applicazioni.</sample>
    <sample id="248">No, gli annotatori per NLPositionality non sono bilanciati rispetto a ciascun gruppo demografico. Il contenuto inglese menziona che i ricercatori hanno cercato di re-annotare i dati con diversi annotatori, ma non specifica che ci sia un bilanciamento demografico. Inoltre, si sottolinea che di solito solo pochi annotatori annotano ogni istanza e che le informazioni demografiche sono raramente raccolte e condivise. Pertanto, non è garantito che ci sia un bilanciamento demografico tra gli annotatori.</sample>
    <sample id="249">Le frasi nel dominio accettabile sono state perturbate preservando la struttura rilevante ma aggiungendo rumore all'input. Questo processo ha permesso di analizzare come i modelli linguistici reagiscono a variazioni delle frasi mantenendo invariate le caratteristiche sintattiche e semantiche condivise.</sample>
    <sample id="250">Avere una valutazione dimensionale significa valutare un modello di conversazione AI in base a diversi aspetti o dimensioni specifici della qualità del dialogo, piuttosto che fornire una valutazione complessiva generica. Questo approccio permette di identificare i punti di forza e di debolezza del modello in aree specifiche, come la pertinenza delle risposte, la coerenza, l'empatia, e la capacità di evitare errori tematici come contraddizioni o violazioni del senso comune. La valutazione dimensionale, come proposta da ABC-Eval, fornisce una comprensione più dettagliata e precisa della performance del modello, permettendo di identificare aree di miglioramento specifiche e di confrontare modelli in modo più accurato.</sample>
    <sample id="251">Gli autori dell'articolo sono affiliati all'Università della Scienza e della Tecnologia della Cina (University of Science and Technology of China).</sample>
    <sample id="252">Il lavoro "U-CREAT: Unsupervised Case Retrieval using Events extraction" presentato da Sai Kiran Tanikella e colleghi affronta la sfida della Prior Case Retrieval (PCR) nel contesto legale, dove l'obiettivo è recuperare casi rilevanti da un pool di documenti dati una query. Il team ha sviluppato due contributi chiave: il dataset IL-PCR, un nuovo benchmark per PCR con 7.070 casi indiani e 6.775 citazioni medie per documento, e la pipeline U-CREAT, che utilizza tecniche di apprendimento non supervisionato e un approccio basato sugli eventi. La pipeline estrae eventi dai documenti, costruendo triple soggetto-verbo-oggetto, e calcola una matrice di interazione tra gli eventi della query e quelli dei candidati per il ranking. Gli esperimenti dimostrano che i modelli basati sugli eventi, in particolare l'Event Filtered Documents, superano significativamente i metodi tradizionali, con tempi di inferenza più bassi e punteggi F1 più alti. U-CREAT si distingue anche per la sua efficienza e generalizzazione tra sistemi legali indiani e canadesi, senza necessità di adattamenti specifici. Questo lavoro rappresenta un progresso significativo nel campo della PCR, aprendo nuove prospettive per ulteriori sviluppi.</sample>
    <sample id="253">**Abstract:**  
The paper presents "DisorBERT," a novel double domain adaptation model designed to detect signs of mental disorders in social media content. Mental disorders, such as depression, PTSD, and eating disorders, are psychological syndromes causing distress and disability, and social media provides a rich source of data to study these conditions. The model leverages domain adaptation to transfer knowledge from general language models (e.g., BERT) trained on Wikipedia and Google Books to specific domains like Reddit and mental health. This adaptation involves adjusting the model’s semantic understanding and vocabulary to focus on domain-specific tasks. Guided masking is employed to enhance the model’s attention to critical words during training.  

The proposed approach achieves a balance between precision and recall, outperforming baseline models and MentalBERT, a model trained on extensive mental health data. Analysis of textual segments reveals that DisorBERT generates more psychologically oriented and disorder-related words compared to BERT. For instance, in sentences from Beck’s Depression Inventory, DisorBERT predicts words like "focus," "talk," and "medication," which are relevant to depression. Visualization of attention scores in user posts highlights key topics such as "anxious" and "medication," demonstrating the model’s ability to identify mental health indicators.  

In conclusion, DisorBERT effectively captures signs of mental disorders in social media, offering a promising tool for early detection and support. Future work aims to explore additional lexical resources and integrate clinical data for enhanced accuracy.</sample>
    <sample id="254">In questo lavoro, presentiamo un framework per l'estrazione di relazioni a livello documentale (DocRE) che integra la riduzione del rumore nei dati distantemente supervisionati (DS) attraverso la guida basata sull'incertezza. Il problema della qualità dei dati DS, spesso affetti da rumore, è affrontato utilizzando pseudo-etichette generate da un modello pre-addestrato su dati DS e annotati dall'uomo. Tuttavia, le pseudo-etichette possono introdurre falsi positivi, compromettendo l'accuratezza. Per mitigare questo rischio, introduciamo una stima dell'incertezza per valutare l'affidabilità delle previsioni del modello. Utilizziamo il dropout di Monte Carlo (MC Dropout) per quantificare l'incertezza, ma adattiamo il metodo per gestire le relazioni sovrapposte, un problema comune nei dati DS. Proponiamo una stima dell'incertezza a livello di istanza per ciascuna relazione, distinguendo tra falsi positivi e veri positivi. Inoltre, introduciamo soglie dinamiche di incertezza per filtrare le pseudo-etichette ad alta incertezza, migliorando la qualità del set di dati. Infine, implementiamo una strategia di addestramento multi-fase per ri-etichettare iterativamente i dati DS, sfruttando appieno il loro potenziale. I risultati dimostrano che il nostro framework supera significativamente i metodi di base su dataset pubblici, evidenziando miglioramenti sostanziali. In sintesi, il contributo principale è un framework innovativo che migliora la qualità dei dati DS, gestisce le relazioni sovrapposte, affronta il problema delle classi a coda lunga e ottiene prestazioni superiori.</sample>
    <sample id="255">La forma del prompting si rivela importante principalmente nei casi di **zero-shot e one-shot prompting**, dove la struttura e il contesto fornito al modello hanno un impatto significativo sulle prestazioni. Tuttavia, con **five-shot prompting**, la forma del prompting ha un'influenza minima, poiché sono gli esempi forniti a determinare il risultato.</sample>
    <sample id="257">Gli autori hanno valutato quattro modelli di dialogo all'avanguardia.</sample>
    <sample id="258">Il paper "Can Large Language Models Be an Alternative to Human Evaluation?" esplora l'uso di modelli di linguaggio di grandi dimensioni (LLM) per valutare la qualità del testo in elaborazione del linguaggio naturale (NLP). Gli autori propongono di utilizzare istruzioni naturali per guidare i LLM nella valutazione di campioni di testo, confrontando i risultati con le valutazioni umane. La motivazione principale è la volatilità e la riproducibilità delle valutazioni umane, che spesso dipendono da fattori soggettivi.

L'esperimento si concentra sulla valutazione di storie generate da GPT-2 o scritte da umani, basandosi su quattro attributi: grammatica, coerenza, piacevolezza e pertinenza. Quattro LLM (T0, InstructGPT Curie, InstructGPT Davinci e ChatGPT) sono stati utilizzati per fornire valutazioni. I risultati mostrano che, mentre alcuni modelli non dimostrano una preferenza significativa per i testi umani, Davinci e ChatGPT mostrano una chiara preferenza per i testi umani, allineandosi alle valutazioni degli insegnanti di inglese.

L'esperimento dimostra che i LLM possono essere un'alternativa valida alle valutazioni umane, offrendo risultati riproducibili e oggettivi. Tuttavia, rimangono domande aperte su come variazioni nelle istruzioni, nel campionamento delle risposte o in altri fattori possano influenzare i risultati. Il paper conclude che i LLM possono essere un'opzione utile, con benefici in termini di riproducibilità e costi ridotti rispetto alle valutazioni umane.</sample>
    <sample id="259">Il lavoro "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations" presenta un nuovo benchmark, XSemPLR, per valutare i modelli di parsing semantico cross-linguistico in più lingue naturali e rappresentazioni del significato. Il dataset XSemPLR include 9 dataset di vari domini, 5 compiti di parsing semantico, 8 rappresentazioni del significato e 22 lingue naturali appartenenti a 15 famiglie linguistiche. Sono stati considerati sei diversi setting per l'addestramento e la valutazione, tra cui il setting monolingue, multilingue, e il trasferimento cross-linguistico zero-shot e few-shot. I risultati mostrano che i modelli Encoder-Decoder ottengono le migliori prestazioni su tutti i dataset, mentre l'addestramento in un mix di varie lingue può migliorare le prestazioni dei modelli Encoder-PTR e Encoder-Decoder. Inoltre, è stato osservato che l'addestramento su lingue naturali diverse dall'inglese può migliorare le prestazioni dei modelli few-shot sulle lingue target. Infine, i modelli multilingue come Codex e BLOOM sono risultati inadeguati per i compiti di parsing semantico cross-linguistico. Il lavoro fornisce una valutazione completa di tre tipi di modelli multilingue rappresentativi e offre risultati interessanti per la comunità di ricerca.</sample>
    <sample id="260">L'articolo è firmato da Jingwei Yi, quindi l'autore principale è Jingwei Yi. Tuttavia, il contenuto non specifica il numero di autori coinvolti nel progetto o nella ricerca. Senza ulteriori informazioni, non è possibile determinare quanti autori sono coinvolti nell'articolo.</sample>
    <sample id="261">Un buon pianificatore dovrebbe possedere due qualità ideali:  
1. **Semantica Completa**: I piani devono essere coerenti e comprendere tutti gli aspetti necessari per raggiungere l'obiettivo.  
2. **Fedeltà alle Restrizioni**: I piani devono rispettare e aderire alle specifiche restrizioni imposte, garantendo che siano ragionevoli e conformi ai vincoli dati.</sample>
    <sample id="262">L'articolo è stato scritto da Siyu Yuan della Fudan University, quindi ci sono **1 autore** coinvolto.</sample>
    <sample id="263">In this work, we address the issue of label biases in in-context learning, a prevalent paradigm for utilizing large language models (LLMs). In-context learning is unstable due to design choices like the selection and order of examples, leading to search instability and biased predictions. While prior work has identified biases such as vanilla-label bias (model's uncontextual label preferences) and context-label bias (contextual effects), there has been no systematic categorization or mitigation of these biases. We introduce a new type of bias, **domain-label bias**, which arises from the task corpus and significantly affects model predictions. To demonstrate this, we show that random in-domain words bias predictions, while random English words do not. We propose **domain-context calibration**, a novel method to mitigate all types of biases. Unlike prior methods that use single predefined content-free tokens, our approach employs random in-domain words sampled from the task corpus, accounting for domain-label bias. Experiments across datasets and models reveal that domain-context calibration improves in-context learning performance, especially in tasks with high domain-label bias. It enhances decision boundaries and outperforms previous calibration methods. Our study highlights the importance of addressing domain-label bias and provides a comprehensive framework for robust in-context learning.</sample>
    <sample id="264">**Abstract**  
The paper "TAVT: Towards Transferable Audio-Visual Text Generation" addresses the challenge of multimodal text generation, particularly in audio-visual tasks, where data annotation is costly and domain shifts severely degrade performance. To overcome these limitations, the authors propose a novel framework for Transferable Audio-Visual Text Generation (TAVT), focusing on aligning visual and audio concepts across diverse domains. The framework consists of three key components: an audio-visual meta-mapper network, an audio-visual encoder and language model generator, and Dual Counterfactual Contrastive Learning (DCLL). The meta-mapper network maps visual concepts into a unified auditory semantic space, leveraging audio clusters from the Flickr dataset to align visual and audio content. The encoder and generator use a transformer-based architecture with modality-specific attention weights (α-t) to balance modality contributions. DCLL introduces fine-grained supervision signals to optimize visual-textual alignment directly, without relying on negative samples. The meta-training process, inspired by MAML, enables fast adaptation to new domains with limited labeled data. Experimental results on MSVD and MSR-VTT benchmarks demonstrate that TAVT outperforms state-of-the-art (SOTA) models across cross-datasets and cross-domain settings, maintaining performance even in low-resource domains. Ablation studies highlight the importance of audio features in enhancing model performance. This work provides a robust solution for transferable audio-visual text generation, addressing domain shifts and limited data constraints.</sample>
    <sample id="265">Il nome della relatrice è Vasudha.</sample>
    <sample id="266">Gli autori dell'articolo sono affiliati all'Università di Varsavia.</sample>
    <sample id="268">Gli errori più comuni di PaLM sono **errori di omissione**, ovvero l'omissione di parti della frase sorgente nella traduzione, spesso per produrre un testo più fluido ma meno accurato.</sample>
    <sample id="269">Ciao, io sono James Finch. E io sono Sarah Finch. E oggi vi racconteremo di ABC-Eval, un nuovo approccio dimensionale alla valutazione dell'intelligenza artificiale conversazionale. Questo lavoro è stato realizzato dal laboratorio di NLP di Emory guidato dal professor Jinho Choi presso l'Università di Emory e in collaborazione con Amazon Alexa AI. Quindi, supponiamo che abbiate appena sviluppato un modello di dialogo e vogliate vedere quanto si confronta con lo stato dell'arte attuale. La pratica comune è utilizzare la valutazione umana, ad esempio chiedendo ai giudici umani di selezionare quale delle due conversazioni sia migliore o di valutare le conversazioni su una scala Likert. Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto, potreste voler valutare più dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più fine. Un approccio è semplicemente chiedere ai giudici umani di valutare diversi aspetti della qualità del dialogo, come la pertinenza delle risposte del modello utilizzando metodi comparativi esistenti o scale Likert. Tuttavia, crediamo che esista una strategia più precisa e affidabile per la valutazione dimensionale del dialogo. Il nostro approccio tenta di ridurre la soggettività della valutazione umana annotando esplicitamente se ogni risposta del modello esprime certi comportamenti, come rispondere con informazioni irrilevanti o contraddire se stesso o il proprio interlocutore. Chiamiamo questo approccio l'annotazione dei comportamenti nella chat o ABC-Eval in breve. Abbiamo sviluppato questo metodo per coprire in modo completo i comportamenti dei modelli di chat che sono stati suggeriti per influenzare la qualità della chat nella letteratura recente. ABC-Eval è in grado di misurare i tassi con cui i modelli di chat commettono vari errori tematici. Ad esempio, ABC-Eval misura il numero di turni in cui un modello di chat ignora il proprio interlocutore o dice qualcosa di irrilevante, contraddice se stesso o il proprio interlocutore, allucina fatti errati o viola la conoscenza del senso comune, e quando il modello riesce o fallisce nel mostrare empatia. Per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su 100 conversazioni uomo-bot per modello utilizzando ABC-Eval. Per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti di coppia a livello di dialogo. Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la pratica standard per valutare i modelli di chat lungo più dimensioni. Dalla nostra analisi di questi risultati di valutazione, abbiamo scoperto che le etichette dei comportamenti ABC-Eval sono complessivamente più affidabili delle etichette raccolte dai metodi esistenti, come misurato dall'accordo tra annotatori su 100 conversazioni doppiamente etichettate. Inoltre, le etichette ABC-Eval sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come mostrato da questa semplice analisi di regressione lineare. Ad esempio, potete vedere come la misurazione della proporzione di turni con auto e contraddizioni del partner spieghi il 5% e il 10% della qualità della conversazione, rispettivamente, mentre i punteggi di coerenza Likert medi spiegano solo il 4% o meno. Infine, abbiamo controllato se ogni metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare a passi. Potete vedere come la combinazione di tutte le metriche ABC-Eval spieghi oltre il 25% della qualità della conversazione, e rimuovendo le metriche una alla volta, la maggior parte di esse risulta nella perdita di una discreta quantità di informazioni sulla qualità. D'altra parte, la combinazione di tutte le metriche Likert a livello di turno spiega molto meno della qualità, e meno di queste metriche portano informazioni uniche. Queste metriche ABC-Eval affidabili, informative e distinte ci consentono di valutare l'intelligenza artificiale conversazionale con una risoluzione più alta rispetto a quanto i metodi precedenti siano in grado di raggiungere. Potete vedere che nei risultati del nostro esperimento rimangono ancora diverse sfide e sono state quantificate con precisione. Ad esempio, i bot che abbiamo testato hanno violazioni del senso comune in circa il 20% delle loro risposte. Producono informazioni irrilevanti in circa il 15% delle risposte, e si contraddicono o contraddicono il proprio interlocutore circa il 10% delle volte. Con il rapido ritmo di miglioramento nel campo, molti di questi tassi di errore potrebbero diminuire nei nuovi modelli rilasciati da quando abbiamo condotto la nostra valutazione. Tuttavia, questa è ancora di più la ragione per perseguire metriche di valutazione affidabili e precise per confrontare i modelli. Speriamo che ABC-Eval possa essere sfruttato da altri nel campo come un passo significativo in questa direzione. E non vediamo l'ora di vedere come l'intelligenza artificiale conversazionale avanzerà nei prossimi mesi e anni. Grazie per aver guardato.</sample>
    <sample id="270">Gli autori dell'articolo sono James Finch e Sarah Finch, e il loro lavoro è stato condotto dalla Emory NLP Lab, guidata dal Professor Jinho Choi presso l'Università di Emory, in collaborazione con Amazon Alexa AI.</sample>
    <sample id="271">In questo articolo, **CFT** sta per **Continuous Fine-Tuning**, un approccio che prevede il continuare a perfezionare un modello utilizzando ulteriori campioni di dati puliti durante il processo di apprendimento. È presentato come un'alternativa semplice ed efficace ai metodi di Weakly Supervised Learning (WSL) che richiedono dati di validazione puliti per funzionare correttamente.</sample>
    <sample id="272">L'articolo è stato scritto da 8 autori.</sample>
    <sample id="273">Ciao, mi chiamo Kayo Yin e presenterò il nostro lavoro intitolato "Quando la traduzione richiede contesto? Un'esplorazione multilingue basata sui dati". Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig. Quindi molte traduzioni dipendono dal contesto. Ad esempio, come tradurremmo "mole" in questa frase? Beh, se la frase precedente fosse "Le cose potrebbero diventare pericolose se i ministri scoprissero", allora "mole" si riferisce a uno spia. Ma se la frase precedente fosse "Potrebbe essere qualcosa di serio, dottore?", allora "mole" si riferisce a un neo. Quindi, a seconda del contesto, il significato della parola cambia e quindi cambia anche la sua traduzione. Tuttavia, valutare quanto bene i modelli possano tradurre casi come questo è piuttosto difficile. In primo luogo perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus come BLEU incapaci di catturare queste traduzioni. E alcune persone hanno suggerito valutazioni mirate sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e insiemi limitati di lingue poiché di solito si basano sulla conoscenza del dominio e sulla curazione umana. In questo lavoro, cerchiamo di rispondere a queste due domande. Primo, quando la traduzione richiede contesto? E secondo, quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipenda dal contesto durante la traduzione. Nel lavoro precedente, abbiamo introdotto CXMI come misura per l'uso del contesto da parte dei modelli di traduzione automatica. E questo è fatto misurando quanto il contesto C fornisca informazioni sul target Y, dato il sorgente X. Potete pensare a CXMI come alle informazioni guadagnate dando contesto al modello. In questo lavoro, estendiamo CXMI a P-CXMI che può misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo pensare a parole con alto P-CXMI come a quelle che richiedono contesto per la traduzione. Ora analizziamo le parole con alto P-CXMI per cercare schemi tra queste parole. E svolgiamo la nostra analisi su trascrizioni di TED talk che sono state tradotte dall'inglese a 14 lingue diverse. Eseguiamo la nostra analisi a tre livelli diversi. Primo, guardiamo ai tag delle parti del discorso che hanno un alto P-CXMI medio. E questo ci permette di trovare, ad esempio, i pronomi duali in arabo che hanno un P-CXMI relativamente alto. E questo può essere spiegato perché l'inglese non ha pronomi duali, quindi è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo. E allo stesso modo, troviamo che certe lingue richiedono anche contesto quando vogliamo scegliere la forma verbale appropriata. Poi guardiamo agli elementi del vocabolario che hanno un P-CXMI alto medio su tutte le sue diverse occorrenze. E questo ci aiuta a identificare casi come quello qui, dove in cinese è necessario il contesto per tradurre i nomi propri per assicurarsi di usare la stessa traduzione all'interno del documento. E allo stesso modo, troviamo che il contesto è importante per tradurre nella giusta formalità. E infine, guardiamo a diversi token individuali che hanno un alto P-CXMI. E questo ci permette di identificare fenomeni che non possono essere realmente catturati dalla parola stessa, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione di ellissi. Ora usiamo i nostri risultati dall'analisi per progettare un benchmark per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni del discorso che abbiamo identificato, creiamo tagger per identificare automaticamente le parole che appartengono al fenomeno. E abbiamo chiamato il nostro tagger il tagger Multilingue Conscio del Discorso, o MuDA. Possiamo anche notare che lingue diverse hanno diverse proporzioni di questi fenomeni del discorso. Poi usiamo il tagger MuDA, applicando il tagger su un corpus parallelo che vogliamo usare per la valutazione e applichiamo le nostre metriche di traduzione di scelta su esempi dipendenti dal contesto che il tagger MuDA ha identificato. E infine, usiamo il nostro benchmark così come altre metriche per valutare diversi modelli a livello di documento nella traduzione automatica. Prima di tutto, quando usiamo metriche a livello di corpus: quindi per BLEU, troviamo che i modelli agnostici al contesto hanno la migliore performance. Ma poi se usiamo COMET, i modelli consapevoli del contesto hanno le migliori prestazioni. E se usiamo la misura f-word, allora i modelli con e senza contesto hanno performance confrontabili. Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se usiamo solo metriche a livello di corpus. Ora, usiamo il benchmark MuDA per valutare i modelli e troviamo che i modelli consapevoli del contesto sono significativamente più accurati dei modelli che non usano il contesto per certi fenomeni del discorso come la formalità e la coesione lessicale. Ma questi modelli non sono molto migliori dei modelli che non usano il contesto su altri fenomeni come ellissi, pronomi e forma verbale. Questo suggerisce in qualche modo dove avremmo bisogno di vedere più progressi per la traduzione a livello di documento. Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che DeepL è solitamente più accurato di Google Translate per la traduzione a livello di documento. Per riassumere, eseguiamo un'analisi guidata dai dati su 14 coppie di lingue per identificare quando le traduzioni richiedono contesto e poi usiamo i nostri risultati per costruire un benchmark per la traduzione automatica a livello di documento che può aiutarci a identificare quali fenomeni del discorso i modelli possono gestire bene o no, e quali sistemi di traduzione sono bravi nella traduzione a livello di documento. Grazie mille per l'attenzione. Ci vediamo a Toronto.</sample>
    <sample id="274">Il nome della relatrice o del relatore è Yusen Zhang.</sample>
    <sample id="276">**Abstract**  
Ananya and Vignesh present *IndicMT Eval*, a dataset designed to meta-evaluate machine translation (MT) metrics for Indian languages, addressing the understudied area of reverse translation evaluation. The dataset includes 7,000 samples from five Indian languages (Tamil, Malayalam, Hindi, Marathi, and Gujarati), generated by seven translation models or APIs. Human bilingual annotators evaluated the outputs, categorizing errors into accuracy/meaning, fluency, and special categories, and assigning severity and overall scores. The study compares MT metrics, revealing that overlap-based metrics (e.g., chrF) perform poorly, while embedding-based metrics (e.g., LabSE, BERTscore) show better correlations with human scores. COMET-metric variants exhibit the highest overall correlations. Fine-tuning COMET using the MQM dataset (IndicCOMET) improves performance, outperforming COMET baselines on three out of five languages. Zero-shot testing on unseen languages confirms IndicCOMET’s superiority. Robustness evaluation on ACES datasets further validates IndicCOMET’s effectiveness. The dataset and findings aim to provide a robust framework for evaluating MT systems in Indian languages, addressing their unique linguistic challenges. The work highlights the importance of language-specific evaluation metrics and encourages the use of *IndicMT Eval* for future research.</sample>
    <sample id="277">Il nuovo metodo non ha un nome specifico menzionato nel testo.</sample>
    <sample id="278">L'autore descrive il metodo delle "parole contrassegnate" come un approccio che si basa sul concetto sociolinguistico di "markedness". Questo concetto afferma che esiste un default non marcato (unimarkato) e che qualsiasi gruppo che si discosta da questo default è linguisticamente marcato. Ad esempio, la parola "warrior" è solitamente associata agli uomini, quindi quando si descrive una donna guerriera, si tende a specificare "donna guerriera" per marcare il termine "guerriera" con "donna". In questo metodo, si designano prima i gruppi marcati e unimarcati, e poi si confrontano le persone generate utilizzando il metodo "Fightin' Words", che calcola i log-odds ratio per distinguere le parole più frequenti per ciascun gruppo marcato. Questo metodo permette di identificare le parole che distinguono i gruppi marcati da quelli unimarcati, rivelando così stereotipi e narrazioni essenzializzanti.</sample>
    <sample id="279">Gli autori dell'articolo sono affiliati all'Università di Washington.</sample>
    <sample id="280">**Abstract:**  
The paper introduces *MultiEMO*, an attention-based correlation-aware multimodal fusion framework for emotion recognition in conversations (ERC). Existing methods often fail to effectively exploit multimodal information, struggle with minority emotion classes, and struggle to distinguish semantically similar emotions. *MultiEMO* addresses these challenges by proposing three key contributions:  
1. **VisExtNet**: A novel visual feature extractor that focuses on facial expressions rather than redundant scene-related information, using MTCNN and VGGFace2 pre-trained ResNet-101.  
2. **MultiAttn**: A multimodal fusion model based on bidirectional multi-head cross-attention layers, integrating textual, audio, and visual modalities to capture complementary correlations.  
3. **Sample-Weighted Focal Contrastive Loss (SWFC)**: A loss function that prioritizes hard-to-classify minority classes and maximizes inter-class distances for better distinguishing semantically similar emotions.  
Experiments on MELD and IEMOCAP datasets demonstrate *MultiEMO*'s state-of-the-art performance, particularly in minority and semantically similar emotion classes. Limitations include challenges in speaker identification, batch size requirements for SWFC, and persistent performance gaps for minority emotions. *MultiEMO* represents a significant advancement in ERC by addressing key unresolved issues in multimodal fusion and emotion classification.</sample>
    <sample id="281">Il lavoro "When Does Translation Require Context? A Data-driven, Multilingual Exploration" esplora l'importanza del contesto nella traduzione automatica, analizzando come i modelli linguistici gestiscono le parole il cui significato dipende dal contesto. Utilizzando un'estensione del CXMI (Contextual Usage Measure for Information), denominata P-CXMI (Pointwise CXMI), gli autori misurano il grado di dipendenza dal contesto per parole e frasi in traduzioni di TED Talks da inglese a 14 lingue. L'analisi evidenzia che parole con alto P-CXMI, come pronomi duali in arabo o nomi propri in cinese, richiedono contesto per una traduzione accurata. Inoltre, si osservano fenomeni come formalità, coesione lessicale, risoluzione di ellissi e forme verbali che influenzano la traduzione. Per valutare i modelli, viene sviluppato il MuDA (Multilingual Discourse-Aware) tagger, che identifica parole legate a questi fenomeni. I risultati mostrano che i modelli contest-aware superano quelli context-agnostic in alcuni casi, ma non in tutti. Il benchmark dimostra che DeepL è generalmente più accurato di Google Translate per la traduzione a livello di documento. In sintesi, lo studio fornisce strumenti per valutare meglio i modelli di traduzione automatica e identifica aree di miglioramento per la gestione del contesto.</sample>
    <sample id="282">**Abstract**  
At ACL 2023, we present *StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing*, addressing the challenge of story-level style transfer in non-parallel text. Unlike prior work focusing on token or sentence-level style transfer, our approach targets discourse-level linguistic preferences, which are critical for imitating author style. The primary challenges include capturing author-specific discourse structures and style-specific content, which are often topic-dependent and difficult to transfer. To address these issues, we propose *StoryTrans*, a generation model that learns discourse representations from source texts and combines them with learnable style embeddings.  

Our solution is divided into two stages: (1) transferring the source text with style-specific content masked, and (2) generating the full text by explicitly incorporating these keywords. For training, we use an advisory framework, including self-reconstruction loss, disentanglement loss, sentence order loss, and style classifier loss to align style and content. The second stage focuses on content preservation and removing masked tokens.  

We evaluated *StoryTrans* on new Chinese and English datasets, achieving superior results in style control and content preservation compared to baselines. Manual evaluations confirmed its ability to enrich storylines and maintain semantics. Our work demonstrates the effectiveness of *StoryTrans* in non-parallel story-level style transfer, with data and code available for further exploration.</sample>
    <sample id="283">La prima struttura di dipendenza simmetrica menzionata è quella in cui **tutti i conjuncts sono heads della struttura coordinata**, come proposto nella **Word Grammar di Hudson**.</sample>
    <sample id="284">In this paper, we introduce **FSUIE (Fuzzy Span Mechanism for Universal Information Extraction)**, a novel approach to enhance span-based Universal Information Extraction (UIE) models. Traditional UIE models rely heavily on precise span boundary annotations, which introduce ambiguity and overfitting. To address this, we propose a **fuzzy span mechanism** where span boundaries are modeled as continuous distributions rather than fixed positions. This reduces the model's dependence on exact boundary labels and improves generalization.

Additionally, we address the mismatch between transformer-based feature extraction and UIE by introducing **adaptive fuzzy span attention**. This attention mechanism dynamically adjusts the span length and linearly decays on the boundary, enabling the model to focus on relevant semantic information within a limited range. The fuzzy span loss combines Binary Cross Entropy (BCE) with Kullback-Leibler (KL) divergence to optimize the model's predictions.

Experiments on named entity recognition, relationship extraction, and aspect sentiment triplet extraction demonstrate FSUIE's effectiveness. It achieves significant performance improvements over baseline models, especially on small datasets, and sets state-of-the-art results on relationship extraction tasks. FSUIE also shows stronger generalization capabilities and competitive performance on aspect sentiment tasks. Ablation studies confirm that the fuzzy span loss and attention mechanism collectively enhance the model's convergence and information extraction capabilities.

In summary, FSUIE introduces a novel fuzzy span mechanism and adaptive attention to improve UIE models, achieving excellent results across a range of tasks.</sample>
    <sample id="285">Il lavoro "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework" di Mingqi Gao e colleghi affronta il problema degli errori fattuali nei riassunti generati da modelli di dialogo, evidenziando due approcci principali: l'integrazione di obiettivi legati alla factualità nei modelli di riassunzione e la progettazione di un modello di correzione degli errori fattuali (FEC) indipendente. A differenza dei precedenti studi FEC, che si basano su metriche di factualità come FactCC e DAE, spesso imprecise e fuorvianti, il lavoro propone un nuovo framework di valutazione basato su correzioni manuali di riferimento. Questo approccio consente di valutare in modo più accurato la capacità dei modelli FEC di correggere errori fattuali con il minor numero possibile di operazioni di sostituzione, inserimento ed eliminazione, mantenendo un riassunto fluido e non ridondante. Viene inoltre introdotta una tassonomia di errori fattuali, suddivisi in categorie basate sul contenuto (content-based) e sulla forma (form-based). Gli esperimenti dimostrano che l'uso di riassunti corretti manualmente durante l'addestramento dei modelli FEC migliora significativamente le prestazioni, suggerendo che la combinazione di dati umani e sintetici sia una direzione promettente. Infine, il lavoro evidenzia le limitazioni attuali dei modelli FEC, come la difficoltà di correggere errori di aggiunta e la mancanza di capacità di affrontare errori di attributi, modalità e collegamenti.</sample>
    <sample id="286">Il nome del relatore è James Finch.</sample>
    <sample id="287">Quattro autori sono coinvolti nell'articolo: Javad Hosseini, Filip Radlinski, Silvia Pareti e Annie Louis.</sample>
    <sample id="288">Gli insiemi di dati che possono essere utilizzati per testare i fenomeni sintattici includono:

1. **BLiMP (Berkeley Linguistic Marker Passage)**: un dataset che contiene frasi grammaticali e non grammaticali.
2. **SyntaxGym**: un dataset di frasi grammaticali e non grammaticali.
3. **CrowS (Crowdsourced Linguistic Judgments)**: un dataset di giudizi di accettabilità basati su giudizi di massa.
4. **Wikipedia**: un dataset di testi non grammaticali ma pertinenti al contesto.

Questi dataset possono essere utilizzati per creare coppie di frasi (minimi paia) con strutture sintattiche simili ma con differenze grammaticali, permettendo di testare la capacità dei modelli linguistici di riconoscere e valutare la grammaticalità delle frasi in contesti più lunghi.</sample>
    <sample id="290">Le abbreviazioni dei cinque metodi per la prima domanda di ricerca sono:

1. WSL (Weakly Supervised Learning)
2. FTw (Fine-tuning with clean validation samples)
3. COSINE (un metodo WSL specifico menzionato nel testo)
4.... (non specificato nel testo)
5.... (non specificato nel testo)

Nota: Il testo non fornisce le abbreviazioni per tutti e cinque i metodi menzionati nella prima domanda di ricerca.</sample>
    <sample id="291">Il modello DrBERT viene valutato su 11 attività downstream nel dominio biomedico e clinico in francese, tra cui:

1. **Named Entity Recognition (NER)**
2. **Classification**
3. **Part-of-Speech Tagging**
4. **Question Answering**

Queste attività sono scelte per valutare le prestazioni del modello in compiti specifici del settore sanitario.</sample>
    <sample id="294">CamemBERT viene inizialmente addestrato su un set di dati di 138 GB.</sample>
    <sample id="295">Il nome del relatore è Adam Przepiórkowski.</sample>
    <sample id="296">**Abstract:**  
This work explores the challenges of irony detection in natural language processing (NLP) through the development of the English Perspectivist Irony Corpus (EPIC), a dataset collected from social media platforms like Reddit and Twitter over 1½ years. The corpus includes 300 short conversations across five varieties of English, annotated by 74 crowdsourced annotators using a simple interface. The study investigates inter-annotator agreement and variability, revealing differences based on gender, age, nationality, and geographical location. These variations led to the development of perspective-aware models, which fine-tune pre-trained language models on splits corresponding to different annotators. While raw performance metrics showed no significant trends, perspective-aware models demonstrated higher confidence in predictions compared to aggregated gold-standard models. Further analysis highlighted that generational proximity and geographical differences, particularly between the UK and Ireland, contributed to annotation variability. This research underscores the importance of considering annotator perspectives in NLP tasks, particularly for complex phenomena like irony, and highlights the potential of perspective-aware models to improve confidence and accuracy in NLP systems.</sample>
    <sample id="297">Il progetto "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models" esplora il fenomeno dei dogwhistles, termini che trasmettono messaggi ambigui a gruppi specifici, spesso con connotazioni razziste, antisemite o transfobiche. Attraverso una ricca tipologia e un glossario di oltre 340 termini e simboli, il lavoro analizza come questi codici vengano utilizzati in contesti politici e sociali. I dogwhistles sono classificati in base al registro (formale o informale), al tipo (aggiunta di implicature o segnalazione di persona) e alla persona (ad esempio, antisemita o transfobica). Lo studio include un'analisi storica dei discorsi politici statunitensi, evidenziando l'aumento dell'uso di dogwhistles razzisti dopo l'era dei diritti civili, in linea con la "Southern Strategy" repubblicana. Inoltre, il progetto valuta la capacità dei modelli linguistici, in particolare GPT-3, di riconoscere e decodificare i dogwhistles, dimostrando che questi modelli possono identificare molti termini, ma con prestazioni variabili, specialmente per quelli informali o transfobici. Infine, viene esaminata la capacità dei sistemi di rilevamento della tossicità di identificare frasi contenenti dogwhistles, mostrando che queste spesso sfuggono alla rilevazione, risultando meno tossiche rispetto a frasi con termini espliciti. Il lavoro sottolinea l'importanza di comprendere i dogwhistles per contrastare la retorica d'odio e migliorare i meccanismi di moderazione online.</sample>
    <sample id="298">Gli autori hanno condotto un esperimento in cui hanno riaddestrato o continuato a pre-addestrare alcuni modelli con dati più recenti e hanno osservato che le prestazioni peggioravano con un maggiore divario temporale tra i dati di addestramento e i dati di test. Questo ha confermato l'ipotesi che la deriva temporale sia la causa principale della perdita di prestazioni.</sample>
    <sample id="299">**Abstract:**  
Recent advances in Natural Language Inference (NLI) models have achieved state-of-the-art performance on benchmarks, but their success is partly due to the exploitation of shortcuts—spurious correlations introduced during dataset creation. These shortcuts enable models to perform well on in-distribution data but fail on out-of-distribution (OOD) adversarial test sets. Existing shortcut mitigation methods often rely on auxiliary models with limited learning capabilities, requiring domain-specific knowledge and making assumptions about the types of shortcuts exploited. To address these limitations, we propose a minimax training approach that enhances NLI model robustness by emphasizing under-represented "hard" examples. The learner minimizes NLI task loss, while the auxiliary maximizes it by generating example weights that incentivize the learner to focus on high-loss regions. This dynamic encourages the learner to counteract shortcuts present in dominant easy examples. Both models are optimized alternately using standard optimization algorithms. At test time, the learner operates independently of the auxiliary. We evaluate our method on three analytic datasets (MNLI, FEVER, QQP) and corresponding OOD adversarial test sets (HANS Symmetric, PAWS), observing consistent improvements in OOD performance while maintaining high in-distribution accuracy. Our approach is dataset-agnostic, does not rely on pre-trained models, and provides insights into learned example weight distributions. We also explore the impact of pre-training, auxiliary size, and transferability to larger models and synthetic shortcuts. This work aims to improve NLI model generalization without domain-specific assumptions.</sample>
    <sample id="300">L'articolo introduce il compito di **dichiarazione interattiva**, una nuova modalità di interazione vocale che consente agli utenti di dettare e modificare documenti in modo naturale e intuitivo. A differenza dei sistemi di trascrizione vocale esistenti, che separano la dettatura dalle modifiche tramite comandi predefiniti, la dichiarazione interattiva permette agli utenti di alternare liberamente dettatura e comandi vocali senza necessità di parole chiave specifiche. Il sistema deve riconoscere e interpretare correttamente le intenzioni dell'utente, distinguendo tra dettatura e comandi, e applicando le modifiche richieste.

Per formalizzare il compito, l'articolo propone un processo in quattro fasi: (1) trascrizione audio in testo tramite un sistema di riconoscimento vocale (ASR), (2) segmentazione delle uttanze in dettatura e comandi, (3) normalizzazione e correzione degli errori, e (4) esecuzione delle modifiche per ottenere il documento finale. Un'interfaccia di raccolta dati è stata progettata per acquisire esempi di interazioni, dimostrando come utenti e assistenti virtuali possano collaborare in modo fluido.

Un sistema di base è stato sviluppato, utilizzando modelli come T5 e GPT-3 per la segmentazione, la correzione degli errori e l'interpretazione dei comandi. I risultati mostrano che GPT-3 è più accurato ma più lento, mentre T5 offre un buon compromesso tra efficienza e precisione. L'articolo conclude evidenziando il potenziale di miglioramento e invita a ulteriori ricerche nel campo, fornendo strumenti e dettagli metodologici per facilitare il lavoro futuro.</sample>
    <sample id="302">È necessario permutare i token per la sequenza di output perché, dopo la prima fase di tagging in cui ogni token di input è associato a un multiset di token di output, i token di output non sono ancora ordinati. La permutazione permette di disporre i token di output nella sequenza corretta, rispettando le relazioni composizionali tra i vari elementi della frase. Senza permutazione, i token di output rimarrebbero in un ordine casuale, compromettendo la capacità del modello di generare una sequenza di output coerente e significativa.</sample>
    <sample id="303">Gli autori hanno suggerito ai proprietari dei modelli di aumentare la trasparenza sui metodi di mitigazione dei bias perché, nonostante i progressi nella riduzione dei bias negativi, i modelli possono ancora generare stereotipi positivi e narrazioni essenzializzanti. Senza una comprensione chiara dei metodi utilizzati, è difficile determinare se questi stereotipi siano il risultato di allineamenti di valore problematici o di altre tecniche anti-stereotipi che hanno effetti collaterali negativi. Aumentare la trasparenza permetterebbe una maggiore comprensione e un'analisi più approfondita di questi fenomeni, contribuendo a migliorare ulteriormente la qualità e l'eticità dei modelli.</sample>
    <sample id="304">Gli input inaccettabili di coppia minima (minimal pair paradigm, MPP) sono coppie di frasi in cui una è grammaticalmente accettabile e l'altra è grammaticalmente inaccettabile. Questi input vengono utilizzati per valutare la capacità di un modello linguistico di distinguere tra frasi grammaticali e non grammaticali. In questo contesto, le frasi inaccettabili sono quelle che violano le regole grammaticali o sintattiche del linguaggio, come ad esempio frasi con strutture sintattiche errate o con parole che non si combinano correttamente.</sample>
    <sample id="305">Il lavoro "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" esplora le limitazioni e le implicazioni delle recenti tecniche di apprendimento supervisionato debole (WSL). A differenza dell'apprendimento supervisionato tradizionale, che si basa su dati accuratamente annotati, il WSL utilizza fonti di etichettatura deboli, come regole euristica o dati crowdsourcing, che sono meno costosi ma più rumorosi. Tuttavia, i modelli addestrati direttamente su dati rumorosi tendono a memorizzare il rumore delle etichette anziché generalizzare. Gli autori sollevano tre domande chiave: (1) è necessario un set di validazione pulito per il WSL, o è possibile utilizzare un set rumoroso? (2) Se un set pulito è necessario, quanti campioni sono sufficienti? (3) È meglio utilizzare solo i campioni puliti per la validazione, o esistono approcci migliori? I risultati mostrano che i metodi WSL recenti richiedono effettivamente campioni di validazione puliti per funzionare correttamente, altrimenti il loro rendimento cala drasticamente. Inoltre, aumentare il numero di campioni puliti migliora le prestazioni. Infine, l'uso diretto di campioni puliti per il fine-tuning supera spesso i metodi WSL, rendendo quest'ultimi meno necessari. Gli autori raccomandano di rendere trasparenti i criteri di selezione dei modelli, confrontare i metodi WSL con quelli di few-shot learning e considerare il fine-tuning continuo come baseline. Il codice è stato reso open-source per ulteriori analisi.</sample>
    <sample id="306">**Abstract:**  
Sebastian Schuster and Najoung Kim investigate the entity tracking capabilities of large language models (LLMs) in discourse understanding. Entity tracking is crucial for agents to comprehend how entities evolve across a conversation or narrative. However, systematic evaluations of LLMs' ability to track entities have been lacking. The authors designed a task to assess this ability, focusing on a setup involving boxes and objects. The task required models to predict the contents of boxes after a series of state-changing operations, ensuring that entity tracking was necessary rather than relying on heuristics or memorized patterns.  

To prevent shortcuts, the authors carefully controlled the task design, ensuring that the model could not benefit from common pre-training data patterns or in-context demonstrations. Experiments with Flan-T5, GPT-3, and GPT-3.5 models revealed that most models failed to perform non-trivial entity tracking, often repeating the initial state. Only GPT-3.5 models, trained on substantial code data, exhibited meaningful tracking abilities. Smaller models like T5-base could learn the task with fine-tuning, but randomly initialized models of the same architecture could not, highlighting the importance of pre-training.  

The findings suggest that pre-training on code enhances LLMs' ability to track entities, though generalization beyond this specific setup remains unclear. The authors emphasize the need for further research to understand the scope and limitations of entity tracking in LLMs. Their paper, available on arXiv, provides detailed results and analysis, including experiments with GPT-4.</sample>
    <sample id="307">Gli autori hanno utilizzato diverse metriche di valutazione per i 11 compiti downstream in francese, tra cui:

1. **Precisione (Precision)**
2. **Ricapitolazione (Recall)**
3. **F1-score (F1-score)**
4. **Accuracy**
5. **Mean Squared Error (MSE)** per i compiti di regressione

Queste metriche sono state applicate per valutare le prestazioni dei modelli su compiti specifici come il riconoscimento di entità nominate (NER), classificazione, etichettatura di parti del discorso (POS tagging), e question answering.</sample>
    <sample id="308">La ricerca "NLPositionality" esplora la presenza di **design bias** nei dataset e nei modelli di NLP, analizzando come le loro prestazioni riflettano le **positionalities** dei ricercatori e degli utenti. Utilizzando un framework sviluppato in collaborazione con l'Università di Washington e l'Allen Institute for AI, lo studio confronta le annotazioni di utenti reali con dataset e modelli esistenti, misurando le differenze attraverso una correlazione di Pearson. I risultati evidenziano che i dataset e i modelli sono spesso più allineati a **popolazioni specifiche**, come i paesi anglofoni, le persone con istruzione universitaria e i generi binari, lasciando indietro gruppi come le persone non binarie. Questo allineamento riflette le **positionalities** dei ricercatori e degli sviluppatori, che possono influenzare le scelte progettuali. Lo studio utilizza piattaforme come Lab in the Wild per reclutare annotatori diversificati, raccogliendo oltre 16.000 annotazioni da 1.000 partecipanti in 87 paesi. Le conclusioni suggeriscono che i bias di design possono essere mitigati attraverso una **documentazione trasparente delle scelte progettuali**, un approccio **perspectivista** nella ricerca NLP e la creazione di dataset e modelli specifici per comunità emarginate. La ricerca sottolinea l'importanza di un NLP inclusivo, che vada oltre la mera funzionalità universale, per garantire che le tecnologie siano eque e rappresentative di tutte le popolazioni.</sample>
    <sample id="309">La metrica utilizzata per misurare l'accordo tra annotatori è l'**inter-annotator agreement** (accordo tra annotatori), calcolata su 100 conversazioni doppiamente annotate.</sample>
    <sample id="310">Il dominio scelto per aggiungere frasi completamente scollegate alle query inaccettabili e accettabili è **Wikipedia**.</sample>
    <sample id="311">Gli autori dell'articolo sono Regina Stodden e Omar. Regina Stodden è la prima a parlare e sembra essere la principale autrice o presentatrice dell'articolo, mentre Omar parla del secondo uso del corpus DEPLAIN. Non sono specificate le loro affiliazioni istituzionali nell'estratto fornito.</sample>
    <sample id="312">MultiInstruct differisce dagli altri benchmark dataset in quanto è il primo dataset di istruzioni multi-modali su larga scala, composto da 62 diverse attività multi-modali che coprono 10 ampie categorie. A differenza dei dataset di istruzioni linguistiche, che sono abbondanti, i dataset di istruzioni multi-modali sono scarsi, il che rende MultiInstruct un'innovazione significativa nel campo. Inoltre, MultiInstruct utilizza un formato sequenza-a-sequenza unificato per formulare tutte le attività, rappresentando testo, immagini, istruzioni e caselle di delimitazione nello stesso spazio di token, il che lo distingue dagli altri dataset che potrebbero non avere questo livello di unificazione.</sample>
    <sample id="313">L'articolo menziona James Finch e Sarah Finch come autori, ma non specifica quanti altri autori siano coinvolti. Tuttavia, il lavoro è stato condotto dal **Emory NLP Lab** guidato da **Professor Jinho Choi** in collaborazione con **Amazon Alexa AI**, quindi potrebbero esserci altri autori non menzionati direttamente nel testo. In sintesi, **almeno 4 autori** sono coinvolti (James, Sarah, Jinho e un autore rappresentante Amazon Alexa AI).</sample>
    <sample id="314">La coordinazione binaria si riferisce alla struttura grammaticale in cui due elementi (congiunti) sono uniti da una congiunzione coordinante, formando una singola unità sintattica. In questa struttura, entrambi i congiunti sono considerati uguali e indipendenti l'uno dall'altro, e la scelta dell'ordine dei congiunti non influisce sulla struttura della frase. La coordinazione binaria è un esempio di struttura simmetrica, in contrasto con le strutture asimmetriche in cui uno dei congiunti è considerato il "capo" della struttura.</sample>
    <sample id="315">I prompt in questo studio sono stati utilizzati per generare le persone, ma non è specificato un tempo medio di utilizzo. Tuttavia, si può dedurre che i prompt siano stati utilizzati per generare le risposte delle persone, che poi sono state analizzate per identificare i modelli di stereotipi e pregiudizi.</sample>
    <sample id="316">I risultati dimostrano che il modello T5, più piccolo rispetto ai grandi modelli di linguaggio, può generare script di qualità superiore quando è stato addestrato sul dataset CoScript specifico per la pianificazione linguistica vincolata. Questo suggerisce che i modelli più piccoli possono superare i modelli più grandi se addestrati su dataset adeguati, offrendo una soluzione più efficiente e meno costosa per la pianificazione linguistica vincolata.</sample>
    <sample id="317">**Abstract:**  
The paper, "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors," addresses the challenges of information extraction (IE) tasks, such as named entity recognition (NER) and relation extraction (RE), where traditional text-to-text models struggle due to mismatched outputs between pre-training and inference stages. To overcome this, the authors propose transforming IE into a structure-to-structure code generation task using large code language models like Codex. This approach aligns the input and output structures, enabling more effective few-shot learning. The proposed method, CodeIE, leverages code-style prompts to generate structured outputs directly, significantly outperforming traditional models like T5 and GPT-3. Experiments on NER and RE datasets demonstrate that CodeIE achieves better performance, especially in recall, and reduces structural errors. The analysis highlights that code pre-training models better align with IE tasks, reducing perplexity and improving output quality. Additionally, Codex outperforms GPT-3 across all tasks. The paper concludes that transforming IE into a code generation task, using code-style prompts, is a promising approach for improving few-shot information extraction. The authors make their paper and code publicly available for further exploration.</sample>
    <sample id="318">Ciao, sono Yanis Labrak e vi presenterò i nostri lavori su "DrBERT: Un Modello Pre-addestrato Robusto in Francese per i Domini Biomedico e Clinico". In questa presentazione, parleremo prima del modellamento linguistico nell'assistenza sanitaria. Poi presenteremo il contributo principale del nostro articolo. Introduciamo il primo modello biomedico in francese chiamato DrBERT, che si basa su RoBERTa e addestrato su NACHOS, che è un insieme di dati di dati medici raccolti dal web. Abbiamo anche introdotto un confronto di modelli con impostazioni di pre-addestramento e fonti di dati multiple. Poi presenteremo i nostri risultati su 11 compiti a valle biomedici e clinici in francese. E infine concluderemo gli esperimenti e vi daremo maggiori dettagli su come accedere a quei modelli. Dalla sua uscita nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere i compiti di elaborazione del linguaggio naturale e offre enormi miglioramenti delle prestazioni rispetto ai metodi storici statici e contestuali come Word2vec, fastText o altri. Da allora, questo modello è stato adattato a molte altre lingue, come in francese con CamemBERT, e anche in domini come il biomedico con PubMedBERT e BioBERT e sul clinico con ClinicalBERT, ma principalmente in inglese. I modelli specializzati per altre lingue sono scarsi e spesso si basano su un pre-addestramento continuo a causa della mancanza di dati in-dominio. Tuttavia, il francese non aveva alcun modello open source per il biomedico fino ad ora. Quindi ci siamo chiesti quale sia la fonte di dati più appropriata per un'ampia gamma di utilizzi e se questi dati raccolti siano una buona sostituzione per i dati clinici. Per rispondere a questa domanda, confrontiamo DrBERT con il nostro modello ChuBERT, che si basa su dati anonimi ottenuti dal magazzino dati dell'Ospedale Universitario di Nantes. Successivamente, ci siamo chiesti quanta data abbiamo bisogno per addestrare un modello specializzato su dati francesi? È 4 gigabyte, 8 gigabyte, o più? Per rispondere a questa domanda, abbiamo prima addestrato e confrontato quattro modelli da zero: una prima versione di DrBERT, con 7 GB di NACHOS; una seconda versione di 4 GB di NACHOS; una prima versione di ChuBERT, che è un modello clinico con 4 GB di frasi tratte dalle note cliniche; e una versione finale di ChuBERT con un mix di 4 GB di NACHOS e 4 GB di note cliniche. Oltre a questo confronto, abbiamo introdotto tre modelli addestrati su pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento. Uno basato sul peso di CamemBERT e addestrato su un insieme di 4 GB di NACHOS. Un altro basato sempre su CamemBERT, ma addestrato questa volta sui 4 GB di note cliniche e infine, uno basato sul modello biomedico inglese PubMedBERT, e addestrato su un insieme di 4 GB di NACHOS. In totale, abbiamo sette modelli. Per valutare i nostri sette modelli, abbiamo raccolto dati per compiti a valle pubblici e privati come il riconoscimento degli enti nominativi, la classificazione, il tagging delle parti del discorso e la risposta alle domande. Questi modelli sono stati confrontati con sei modelli di base che sono CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT e ClinicalBERT. La valutazione evidenzia che i modelli hanno ottenuto le migliori prestazioni nel compito con dati della stessa natura di quelli su cui il modello è stato addestrato. Tuttavia, possiamo osservare che i dati provenienti da fonti eterogenee sembrano essere più versatili. Osserviamo anche che l'uso di più dati si traduce in migliori prestazioni. Nel complesso, il pre-addestramento da zero sembra ottenere prestazioni più elevate sulla maggior parte dei compiti. Tuttavia, il nostro esperimento sul pre-addestramento di controllo utilizzando il peso e la tokenizzazione di CamemBERT addestrato sul sottoinsieme di 4 GB di NACHOS ha mostrato risultati comparabili a quelli ottenuti con DrBERT 4 GB da zero. Che non è il caso per il modello basato sui pesi e la tokenizzazione di CamemBERT, che soffre di problemi di stabilità. Infine, come conclusione il nostro sistema adeguato ha offerto prestazioni migliori su nove degli 11 compiti a valle e ha superato globalmente il risultato del modello generico, qui CamemBERT. Stiamo anche osservando che dati più specializzati sono migliori, ma non scalano bene. Tutti i modelli pre-addestrati ottenuti da NACHOS sono liberamente disponibili su Hugging Face, e sotto la licenza MIT, e tutti gli script di addestramento sono sul nostro repository GitHub. Quindi grazie per questa presentazione, e non vediamo l'ora di scambiare idee alla sessione di poster a Toronto.</sample>
    <sample id="319">Nel lavoro "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains", vengono esaminate le seguenti strategie di apprendimento:

1. **Pre-training from-scratch**: Addestramento di modelli da zero su un dataset specifico (ad esempio, DrBERT su 7 GB di NACHOS o ChuBERT su dati clinici).
2. **Continual pre-training**: Addestramento di modelli basati su modelli preesistenti (come CamemBERT o PubMedBERT) su dataset specifici (ad esempio, NACHOS o dati clinici).
3. **Mix di dati**: Combinazione di dataset diversi (ad esempio, NACHOS e dati clinici) per l'addestramento.
4. **Controllo pre-training**: Utilizzo di pesi e tokenizzazione di modelli preesistenti per l'addestramento su dataset specifici.

Queste strategie vengono confrontate per valutare l'impatto della quantità di dati e della natura dei dataset sulle prestazioni dei modelli.</sample>
    <sample id="320">Il fattore di overfitting dovuto al riutilizzo del test (adaptive overfitting) non è osservato nei risultati presentati, poiché il miglioramento unitario su CoNLL-2003 si traduce in un miglioramento superiore su CoNLL++, indicando che non ci sono rendimenti decrescenti.</sample>
    <sample id="321">La qualità della semplificazione è stata valutata attraverso l'analisi delle trasformazioni di semplificazione nei corpus DEPLAIN-apa e DEPLAIN-web. È stato osservato che i testi biblici sono stati semplificati in modo più marcato rispetto ai testi di notizie o ai testi per studenti di lingua. Inoltre, il corpus DEPLAIN presenta una varietà di trasformazioni di semplificazione, come riordini, aggiunte di parole e riformulazioni, con differenze tra i due sotto-corpus (ad esempio, più riordini e aggiunte di parole nel DEPLAIN-apa, più riformulazioni nel DEPLAIN-web). Questi risultati indicano che il corpus è adatto per valutare e migliorare i metodi di semplificazione automatica.</sample>
    <sample id="322">**Abstract:**  
In this paper, we explore what text classifiers learn about morality by analyzing how language models interpret moral judgments in text. Unlike traditional approaches that treat morality as a binary scale (immoral vs. moral), we focus on the nuanced, subjective nature of morality, inspired by theories like the Moral Foundations Theory, which identifies five distinct moral dimensions (e.g., fairness, authority). Our study investigates whether language models can capture these fine-grained differences in moral expression across various domains. We use the Moral Foundation Twitter Corpus, comprising 35,000 tweets from domains like #AllLivesMatter and #BlackLivesMatter, to examine how morality is articulated differently in context. Our experiments reveal that language models can distinguish subtle moral nuances, such as the contrasting perception of subversion in these domains: in #AllLivesMatter, subversion is associated with negative terms like "overthrow" and "mayhem," while in #BlackLivesMatter, it is more positively framed. This highlights the importance of domain-specific understanding in moral classification. By applying explainable AI techniques, we demonstrate that treating morality as a singular scale is insufficient and can lead to dangerous misinterpretations. Our findings emphasize the need for tailored models to accurately capture the pluralistic and context-dependent nature of morality in text.</sample>
    <sample id="323">Il paper "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense QA" affronta la sfida della Question Answering (QA) basata sul senso comune, richiedendo l'integrazione di conoscenza linguistica e di base. L'autore, Yujie Wang, identifica i limiti delle approcci esistenti che combinano modelli linguistici e grafi di conoscenza, come l'introduzione di entità irrilevanti e l'isolamento dell'encoding di testo e grafi. Per superare queste limitazioni, viene proposto il metodo DHLK (Dynamic Heterogeneous-Graph Knowledge). DHLK costruisce un Heterogeneous Knowledge Graph (HKG) ottimizzato attraverso una strategia di potatura a due stadi e Knowledge Representation Learning (KRL). L'HKG viene arricchito con parafrasi di entità chiave e relazioni, eliminando entità irrilevanti basandosi sui pesi di attenzione di RoBERTa. L'encoding del testo e del grafo viene fuso utilizzando Mask Self-Attention, con l'introduzione di Relation Mask Self-Attention (RMSA) per modellare le relazioni nel grafo. Gli embedding di entità e relazioni vengono ottimizzati tramite TransE. Infine, l'embedding del grafo HKG viene combinato con le informazioni di percorso e il contesto della QA per predire la risposta finale utilizzando una rete neurale multistrato (MLP). Gli esperimenti su CommonsenseQA e OpenBookQA dimostrano l'efficacia del metodo, ottenendo risultati superiori rispetto ad approcci comparabili.</sample>
    <sample id="324">Sì, i modelli linguistici presentano bias politici diversi. Gli esperimenti condotti dimostrano che i modelli come GPT-4 sono più liberali, mentre i modelli BART tendono ad essere più conservatori. Inoltre, i modelli possono assumere posizioni politiche in tutte e quattro le quadranti del panorama politico. Questi bias possono essere influenzati dal tipo di dati di pre-addestramento utilizzati, come ad esempio corpora di notizie o social media con orientamenti politici diversi.</sample>
    <sample id="325">Ciao! Mi chiamo Matthias Lindemann, e oggi vi darò una breve introduzione al nostro articolo su "Generalizzazione Compositiva senza Alberi utilizzando Multiset Tagging e Permutazioni Latenti". Questo è un lavoro congiunto con i miei supervisori Alexander Koller e Ivan Titov. La generalizzazione compositiva può essere intesa come la capacità di un apprendista di gestire una ricorsione più profonda e composizioni strutturalmente non viste di frasi che sono state viste individualmente durante l'addestramento. Nel contesto dell'analisi semantica, testare la generalizzazione compositiva potrebbe apparire così. Come al solito, abbiamo un insieme di frasi di addestramento. In questo caso, "La ragazza dormiva." e "Maria sapeva che la ragazza dormiva." Queste frasi sono abbinate a forme logiche che rappresentano gli aspetti fondamentali del loro significato. A differenza della valutazione standard dell'apprendimento automatico, il set di test non proviene dalla stessa distribuzione ma contiene forme logiche strutturalmente non viste. In questo esempio, il modello ha visto una ricorsione superficiale durante l'addestramento ed è testato su un esempio con una ricorsione più profonda. I modelli seq2seq naivi faticano con questo tipo di generalizzazione out-of-distribution e spesso producono output che sono distaccati dall'input. In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle colorate nell'esempio. Un metodo popolare per affrontare questo problema è integrare alberi nei modelli. Gli alberi sono intesi a catturare il processo compositivo che collega le frasi con le forme logiche. Questo funziona bene, ma gli alberi di solito non sono forniti e devono essere ottenuti in qualche modo. Questo può essere un processo complicato e talvolta computazionalmente costoso. Tipicamente, ciò comporta una notevole pre-elaborazione specifica del formalismo delle forme logiche, ad esempio, per gestire i simboli delle variabili. Ottenere gli alberi può anche comportare procedure di induzione della grammatica specializzate. In questo articolo, non usiamo alberi e introduciamo un modello seq2seq neurale che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output. Per la prima volta, mostriamo una forte generalizzazione alla ricorsione più profonda senza fare affidamento sugli alberi. Il nostro approccio prevede l'output dall'input in due passaggi. Prima, tagghiamo ogni token di input con un multiset non ordinato di token che appariranno nell'output. Dopo il primo passaggio, abbiamo tutti i token giusti, ma non sono ordinati. Ecco perché nel secondo passaggio usiamo un altro modello per prevedere una permutazione per metterli nell'ordine giusto. Introduciamo un nuovo metodo per prevedere la permutazione che non pone vincoli rigidi sulle possibili permutazioni. Questo rende il nostro approccio piuttosto flessibile ed espressivo. Concettualmente, il nostro modello di permutazione funziona più o meno così. Passiamo da sinistra a destra sull'output e determiniamo quale token del multiset mettere in ogni posizione. Per la prima posizione dell'output, selezioniamo semplicemente uno, come evidenziato in rosso. Poi saltiamo al prossimo token del multiset, per determinare il secondo token nell'output. Determiniamo il terzo token nell'output in modo simile saltando a un altro token del multiset. Continuiamo questo processo fino a quando ogni token del primo stadio non è stato visitato esattamente una volta. Per darvi un assaggio dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark COGS. Il nostro modello supera gli altri con largo margine sulla generalizzazione alla ricorsione più profonda. Tuttavia, rimangono molto sfidanti altri tipi di generalizzazione strutturale. Nel nostro articolo, risolviamo un paio di interessanti sfide tecniche. Prima di tutto, l'allineamento tra input e output non è dato nei dati di addestramento. Di conseguenza, per un dato token non sappiamo da quale multiset proviene, il che pone una sfida per l'addestramento. Inoltre, a volte ci sono molteplici permutazioni che sono coerenti con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo inducendo l'allineamento come parte dell'addestramento. Il nostro metodo di permutazione è molto flessibile, ma porta la sfida che trovare la permutazione con il punteggio più alto è NP-hard. Questo perché è correlato al problema del "Viaggiatore di Vendita". Approssimiamo questo con una rilassamento continuo ottimizzato per GPU che ci permette anche di retropropagare attraverso la soluzione e apprendere le permutazioni linguisticamente più plausibili. Se volete saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, date un'occhiata al nostro articolo o venite al nostro poster.</sample>
    <sample id="326">La dissonanza cognitiva è un fenomeno in cui una persona sperimenta un conflitto mentale tra due credenze, atteggiamenti o azioni che sono incoerenti tra loro. Ad esempio, una persona potrebbe dire "So che le sigarette possono uccidermi" e poi "Ho fumato un paio di sigarette dopo la riunione", creando una dissonanza tra la consapevolezza dei rischi per la salute e l'azione di fumare. Questo conflitto può portare a cambiamenti nelle credenze, atteggiamenti o comportamenti per ridurre l'incongruenza percepita.</sample>
    <sample id="327">**Abstract:**  
At ACL 2023, Xiao Xu presents "ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning," a novel approach to Vision-Language (VL) learning. Unlike existing architectures like BridgeTower, which layer-by-layer connect unimodal and cross-modal layers, ManagerTower introduces adaptive "managers" in each cross-modal layer to aggregate insights from multiple unimodal expert layers. This design allows for more effective exploitation of diverse levels of unimodal semantic knowledge, enhancing cross-modal alignment and fusion.  

Using RoBERTa and CLIP-ViT as unimodal encoders, ManagerTower adaptively combines representations from different levels of unimodal experts. Despite pre-training on only four million images, ManagerTower outperforms METER, BridgeTower, and other base-size models on downstream tasks, achieving a 39.15% accuracy improvement on the Wikivideo test standard. Visualization of aggregation weights further demonstrates that adaptive managers dynamically exploit unimodal knowledge tailored to each cross-modal layer, unlike static managers.  

The work is scalable, as any visual, textual, or cross-modal encoder can be integrated. ManagerTower’s architecture, code, and models are publicly available, aiming to advance VL representation learning. This approach highlights the importance of adaptive aggregation in leveraging multi-level unimodal insights for comprehensive cross-modal understanding.</sample>
    <sample id="328">Il modello linguistico più liberale è GPT-4.</sample>
    <sample id="329">In this work, we address the challenge of zero-shot video sentence localization, a task that aims to identify the most relevant video segments corresponding to a natural language query. Traditional methods rely on manual annotations, which are costly and inefficient. We propose a noise-resistant method for generating structured pseudo-labels to train video sentence localization models without manual annotations. Our approach involves three key steps: (1) generating complex free-form pseudo-queries using a pre-trained image caption model, (2) creating pseudo-events by modeling the temporal structure of events to ensure high relevance between video segments and queries, and (3) reducing label noise by re-weighting noisy samples and refining pseudo-labels through model training. We evaluate our method on two datasets, ActivityNet Captions and Charades-STA, using metrics such as R@M and mIoU. Our results demonstrate superior performance compared to existing zero-shot methods, achieving the best results on both datasets. This approach effectively addresses the limitations of previous methods, such as simplistic pseudo-queries and unaligned pseudo-events, while being robust to label noise. Our work provides a robust and efficient solution for zero-shot video sentence localization, with code available for further exploration.</sample>
    <sample id="330">Sì, nell'apprendimento attivo, l'addestramento cumulativo funziona meglio di quello iterativo.</sample>
    <sample id="331">La relatrice è Sara Papi.</sample>
    <sample id="332">I dati nel parametro di riferimento MuDa sono stati tratti dalle trascrizioni dei TED Talks, che sono state tradotte dall'inglese in 14 lingue diverse.</sample>
    <sample id="333">In this paper, we introduce **INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation**, a novel framework to enhance the generalization and performance of Neural Machine Translation (NMT) models. Traditional NMT models suffer from a non-smooth representation space, leading to sparse and poorly defined semantic meanings, especially for low-frequency tokens. To address this, we propose **kNN-MT**, which smooths predictions by leveraging nearest neighbors in the representation space. However, kNN-MT has drawbacks: it requires a large datastore for neighbor retrieval and cannot easily update representations during inference. 

To overcome these limitations, **INK** introduces an iterative training loop. First, it extracts kNN knowledge from the datastore to guide an adapter in refining the representation space. Then, the updated representations are asynchronously refreshed in the datastore. This loop continues until convergence, allowing the model to drop the datastore during inference. 

Our experiments, conducted on the WMT’19 German-English translation task, demonstrate that INK significantly improves the representation space of the NMT model. Compared to state-of-the-art kNN-MT, INK achieves an average gain of 1.99 COMET score and 1.0 BLEU score. Additionally, INK requires less memory and faster inference speed. Our results show that refining representations with kNN knowledge, even without a datastore, can lead to substantial improvements. Future work aims to further refine the representation space for even better performance.</sample>
    <sample id="335">Il nome del relatore è Matthias Lindemann.</sample>
    <sample id="336">Il trasferimento interlinguistico (o cross-lingual transfer) è un approccio in cui un modello linguistico addestrato su una lingua sorgente viene utilizzato per migliorare le prestazioni su una lingua target, anche se non è stato specificamente addestrato su quella lingua. Questo metodo sfrutta le conoscenze acquisite da una lingua per trasferirle a un'altra, riducendo il divario di performance tra lingue diverse. Nel contesto di XSemPLR, il trasferimento interlinguistico viene utilizzato per migliorare la traduzione e l'interpretazione di query semantiche in più lingue e rappresentazioni semantiche, ad esempio traducendo query in tedesco in inglese e poi utilizzando un modello addestrato su inglese per generare la query SQL corrispondente.</sample>
    <sample id="337">**Abstract**  
This research introduces a novel graph-based relation mining approach for learning out-of-vocabulary (OOV) word embeddings, addressing the challenge of representing words not present in the training vocabulary. Inspired by human learning habits and lexical rules, the method leverages word formation and association to infer the meaning of OOV words. A **Word Relationship Graph** is constructed, where each word or wordpiece acts as a node, and its embedding serves as node attributes. The graph is divided into two levels: the first layer retains all wordpieces for complete information, while the second layer samples nodes to reduce noise.  

To handle OOV nodes, a **self-attention network** assigns attributes based on character embeddings. A two-level **Graph Attention Network (GAT)** is employed to extract meaningful node-level representations, followed by a **readout block** to capture graph-level information. Contrastive learning is integrated into the loss function to encourage proximity between graph-level embeddings and their background embeddings, while pushing them apart from unrelated samples.  

Experiments demonstrate superior performance compared to baselines in both intrinsic and extrinsic tasks, highlighting the effectiveness of the approach. The model is particularly beneficial for static and contextual downstream tasks. While agglutinative languages are well-suited for the model, fusional languages present challenges due to complex word formation. Overall, the graph-based framework effectively handles diverse word formations, making it adaptable to various languages with reasonable word segmentation.</sample>
    <sample id="338">Il lavoro "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations" esplora l'efficacia delle spiegazioni umane nell'ottimizzazione dei modelli di intelligenza artificiale. La ricerca, condotta da un team di scienziati di Rensselaer Polytechnic Institute, Northeastern University e IBM Research, si concentra sulla valutazione oggettiva delle spiegazioni umane, spesso considerate il gold standard per l'addestramento dei modelli. A differenza dei dati di etichettatura, le spiegazioni umane possono essere soggettive e dipendere dal compito. Gli autori propongono una struttura dati unificata che converte vari compiti in un unico formato a scelta multipla, permettendo di analizzare l'utilità delle spiegazioni in contesti diversi. Attraverso esperimenti preliminari, i ricercatori hanno scoperto che le spiegazioni possono migliorare le prestazioni dei modelli, anche se considerate di bassa qualità dagli esseri umani. È stato inoltre osservato che l'utilità delle spiegazioni dipende fortemente dal compito e dal formato delle spiegazioni stesse. Sulla base di queste osservazioni, gli autori introducono una nuova metrica, TREU, che estende la metrica di simulatabilità, valutando l'efficacia delle spiegazioni nell'ottimizzazione dei modelli. La metrica TREU è stata testata su cinque dataset e due modelli, T5 e BART, dimostrando risultati superiori rispetto alla simulatabilità. La ricerca sottolinea l'importanza di una valutazione accurata delle spiegazioni umane per migliorare la collaborazione tra esseri umani e intelligenza artificiale e raccomanda ulteriori studi per garantire la qualità delle spiegazioni nell'addestramento dei modelli.</sample>
    <sample id="339">Gli autori dell'articolo "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" sono affiliati a Saarland University in Germania. In particolare:
- Dawei (autore principale) è un dottorando
- Xiaoyu Shen
- Marius Mosbach
- Andreas Stephan
- Dietrich Klakow</sample>
    <sample id="340">In questo lavoro, presentiamo **ParaAMR**, un ampio dataset di parafrasi sintatticamente diverse generato tramite **AMR back-translation**. Il dataset è stato creato in collaborazione con Varun, I-Hung, Anoop, Kai-Wei e Aram. La generazione di parafrasi è un compito fondamentale nel campo del NLP, con applicazioni in question answering, chatbot e miglioramento della robustezza dei modelli. Tuttavia, i dataset esistenti, come MRPC, PAN e Quora, pur essendo di alta qualità, sono limitati in scala, mentre i dataset generati automaticamente, come quelli basati su back-translation, sono carenti in termini di diversità sintattica. Per superare queste limitazioni, proponiamo di utilizzare **AMR (Abstract Meaning Representations)**, grafi diretti che catturano il significato astratto di una frase. Il metodo prevede l'uso di un parser AMR per estrarre il grafo di una frase sorgente, la modifica del focus del grafo (cambiando il nodo radice) e la generazione di testo da grafi modificati. Questo approccio garantisce parafrasi semanticamente simili ma sintatticamente diverse. ParaAMR contiene circa 15 milioni di frasi sorgente con 6,9 parafrasi ciascuna, dimostrando una maggiore diversità sintattica rispetto ai dataset esistenti. Valutazioni automatiche e umane confermano che ParaAMR preserva una buona somiglianza semantica con alti punteggi di diversità sintattica. Infine, dimostriamo che ParaAMR migliora applicazioni come l'apprendimento di embedding, il controllo sintattico nella generazione di parafrasi e l'aumento dei dati per il few-shot learning. Il dataset è disponibile online.</sample>
    <sample id="341">Gli autori ricorrono a due misure di latenza: 

1. **Average lagging**: misura il tempo di ritardo tra l'input audio e l'output di traduzione.
2. **Computational-aware average lagging**: tiene conto del tempo di calcolo del modello per prevedere l'output.</sample>
    <sample id="342">**Abstract:**  
This paper introduces *LiveChat*, a large-scale personalized dialogue dataset constructed from live streaming videos on Chinese platforms like TikTok and Douyin. Unlike existing text-sourced datasets, *LiveChat* is video-sourced, capturing spoken language and providing a more realistic representation of human-AI dialogue. The dataset addresses key challenges in open-domain and personalized dialogue, including the lack of large-scale video-sourced corpora, the difficulty of persona extraction, and the scarcity of multi-party dialogue datasets in Chinese. *LiveChat* is built in three steps: (1) scraping live streaming videos, (2) transcribing audio into utterances and extracting audience comments, and (3) collecting persona information for personalized dialogue generation. Persona extraction is enhanced using both manual labeling and trained classifiers.  

Experiments on benchmark tasks—response modeling and addressee recognition—demonstrate the dataset’s effectiveness. Results show that persona information and longer session lengths improve performance, while both rule-based and classifier-based persona extraction methods are valuable. Additionally, pre-trained models like BART perform better on *LiveChat* compared to other datasets, highlighting its unique domain characteristics. Transfer learning experiments reveal that increasing in-context demonstrations improves performance up to 8 shots, after which noise from manual selections degrades results.  

In conclusion, *LiveChat* provides a scalable, personalized, and video-sourced dataset for advancing open-domain and multi-party dialogue systems. Future work will focus on efficient transfer learning for LLMs on this dataset.</sample>
    <sample id="343">Ciao a tutti, sono Akshatha, e oggi io e il mio co-autore Martin presentiamo il nostro lavoro "Il Test KITMUS: Valutazione dell'Integrazione della Conoscenza da Fonti Multiple." Questo lavoro è una collaborazione tra l'Università McGill, Mila e Microsoft Research. I modelli di comprensione del linguaggio naturale si basano su una varietà di fonti di conoscenza, come le informazioni contenute nei loro parametri, solitamente acquisite tramite un pre-addestramento, e le informazioni fornite negli input al momento dell'inferenza. Lavori recenti in compiti come la risposta alle domande mostrano che i modelli possono utilizzare la conoscenza acquisita durante il pre-addestramento per risolvere il compito. Tuttavia, la comprensione del linguaggio naturale spesso richiede conoscenze che vengono fornite anche al momento dell'inferenza. Ad esempio, nella frase "John ha visto il nuovo presidente eletto in TV." I parametri pre-addestrati possono contenere informazioni su cosa fanno i presidenti e cosa sia una TV, ma non possono sapere in modo affidabile chi sia l'entità specifica "John", o chi sia il nuovo presidente, perché il presidente potrebbe essere cambiato da quando è avvenuto il pre-addestramento. Pertanto, i modelli di successo per compiti di NLU che richiedono conoscenza sono quelli che hanno la capacità di integrare e utilizzare sia la conoscenza pre-addestrata che la conoscenza al momento dell'inferenza. In questo lavoro, proponiamo una suite di test diagnostici per l'integrazione della conoscenza. Introduciamo un compito di risoluzione della coreferenza, progettato per sondare la capacità di attingere a conoscenze disponibili in diverse fonti. Valutiamo il set di dati con partecipanti allo studio umani e modelli di risoluzione della coreferenza consolidati. Ecco un esempio dal nostro set di dati. Servin è un giudice. Kea è un Baker. Servin e Kea si sono incontrati in un parco. Dopo una lunga giornata di lavoro a decidere casi in un tribunale, era felice di rilassarsi. Il compito qui è identificare l'entità corretta a cui si riferisce il pronome "lui", che in questo caso è Servin. La risoluzione di un dato pronome richiede due tipi di informazioni. Primo, la conoscenza specifica dell'entità come "Servin è un giudice." E secondo, la conoscenza di base come "I giudici decidono casi nei tribunali." Generalmente, la conoscenza di base è appresa durante il pre-addestramento dei grandi modelli di linguaggio, mentre la conoscenza specifica dell'entità è tipicamente osservata al momento dell'inferenza. Varia la disponibilità di queste due informazioni in modo che possano essere trovate in una singola fonte, o in più fonti. Abbiamo definito tre impostazioni di KITMUS. Primo, abbiamo l'impostazione tipica: "Background-Pretrain", dove la conoscenza di base è assunta essere disponibile al momento del pre-addestramento. Secondo, c'è un'impostazione "Background-Both", dove la conoscenza di base è disponibile sia al momento del pre-addestramento che dell'inferenza. Infine, l'impostazione "Background-Inference", dove entrambe le conoscenze sono disponibili solo al momento dell'inferenza. Quest'ultima impostazione è particolarmente interessante, poiché simula il caso in cui la conoscenza di base necessaria per risolvere un compito non fa parte dei dati di pre-addestramento dei modelli. Ad esempio, perché nuove occupazioni si sono sviluppate dal momento del pre-addestramento. Ecco un esempio di come controlliamo la disponibilità dei fatti nelle fonti vere. Nell'impostazione Background-Pretrain, assumiamo che la conoscenza di base "I politici cercano seggi elettivi nel governo" sia contenuta nei parametri pre-addestrati e nel contesto dell'inferenza forniamo la conoscenza specifica dell'entità "Chichester è un politico." Nell'impostazione Background-Both, forniamo inoltre non solo la conoscenza specifica dell'entità ma anche la conoscenza di base sui politici nel loro contesto di inferenza. Nell'impostazione Background-Inference, forniamo l'occupazione fittizia "mirituer" invece di politico perché "mirituer" è improbabile che sia contenuta nei parametri pre-addestrati. Valutiamo il set di dati sia con partecipanti allo studio umani, sia con modelli di risoluzione della coreferenza consolidati. In questa figura, mostriamo i risultati dei modelli con migliori prestazioni sulla variante più difficile dell'impostazione Background-Pretrain. Senza addestramento specifico sul compito, entrambi i modelli non si comportano bene. Quando addestrati su KITMUS, tuttavia, sia C2F che BERT4Coref si comportano significativamente meglio della scelta casuale. Questo suggerisce che quando addestrati su set di dati generici di risoluzione del riferimento, la maggior parte impara a sfruttare indizi superficiali, che non sono utili quando si testa su KITMUS dove tali indizi sono stati rimossi. Esperimenti aggiuntivi con conoscenze fittizie hanno indicato che anche i modelli con migliori prestazioni non possono integrare in modo affidabile la conoscenza di base fornita solo al momento dell'inferenza. Per riassumere i punti principali del nostro articolo, molti modelli di risoluzione della coreferenza sembrano incapaci di ragionare su conoscenze da diverse fonti senza addestramento specifico sul compito. Tuttavia, con addestramento specifico sul compito, alcuni modelli integrano con successo conoscenze da più fonti. Tuttavia, anche i modelli con migliori prestazioni sembrano avere difficoltà a integrare in modo affidabile la conoscenza di base presentata solo al momento dell'inferenza. Se siete interessati a ulteriori dettagli, per favore consultate il nostro articolo e date un'occhiata al set di dati e al codice su GitHub. Grazie per l'attenzione.</sample>
    <sample id="344">I metodi basati su alberi per la generalizzazione compositiva presentano diversi svantaggi:

1. **Non forniti a priori**: Gli alberi non sono solitamente forniti e devono essere ottenuti, il che può essere un processo complicato e computazionalmente costoso.
2. **Pre-elaborazione formale**: Richiede una pre-elaborazione significativa dei dati formali, ad esempio per gestire i simboli variabili.
3. **Induzione della grammatica**: Potrebbe essere necessario un procedimento specializzato per l'induzione della grammatica.
4. **Rigidità**: Gli alberi possono essere rigidi e meno flessibili rispetto ad approcci alternativi.</sample>
    <sample id="345">Il nostro articolo, "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations", affronta il problema della generalizzazione compositiva in modelli di parsing semantico, senza ricorrere all'uso di alberi sintattici. Questo approccio è particolarmente rilevante per gestire strutture linguistiche più complesse e non viste durante l'addestramento. Tradizionalmente, i modelli sequenza-a-sequenza (seq2seq) faticano con la generalizzazione fuori distribuzione, producendo risultati che spesso non riflettono correttamente le corrispondenze tra input e output. L'integrazione di alberi sintattici è una soluzione comune, ma richiede pre-elaborazione formale e induzione di grammatiche, processi complessi e costosi.

La nostra proposta introduce un modello seq2seq che modella direttamente le corrispondenze tra frammenti di input e output, utilizzando due passaggi. Nel primo, ogni token di input è etichettato con un multiset di token che appariranno nell'output. Nel secondo, un modello predice una permutazione di questi token per ottenere l'output corretto. La flessibilità del nostro approccio risiede nella mancanza di vincoli rigidi sulle permutazioni, permettendo una maggiore espressività.

I risultati sperimentali mostrano che il nostro metodo supera significativamente altri modelli treeless sul benchmark COGS, specialmente nella generalizzazione a strutture più profonde. Tuttavia, rimangono sfide, come l'induzione dell'allineamento tra input e output durante l'addestramento e la risoluzione del problema NP-hard della permutazione ottimale. Abbiamo affrontato queste sfide con una rilassazione continua che permette la retropropagazione e l'apprendimento di permutazioni più plausibili.</sample>
    <sample id="346">Purtroppo, il contenuto fornito non include le affiliazioni degli autori dell'articolo. Per ottenere queste informazioni, sarebbe necessario consultare il testo completo del paper o altre fonti attendibili.</sample>
    <sample id="347">Ciao, sono Myra e oggi parlerò della nostra ricerca "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models". Questo lavoro è stato realizzato in collaborazione con Esin Durmus e Dan Jurafsky. Negli ultimi anni, molti hanno documentato la diffusione di pregiudizi sociali e stereotipi nei grandi modelli linguistici, o LLMs. Tuttavia, queste misurazioni hanno varie limitazioni. Di solito si basano su set di dati costruiti a mano che richiedono molto tempo per essere curati e inoltre di solito misurano solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altre demografie o contesti, o semplicemente catturano associazioni generali e ampie, come associazioni negative con particolari gruppi. Inoltre, la maggior parte del lavoro in questo campo non tiene conto dell'intersezionalità, che è la nozione che le identità sociali multifaccettate possano aggravare i pregiudizi ed essere luoghi unici di danno. Per superare queste limitazioni, ci affidiamo alla proprietà che questi nuovi modelli di linguaggio istruiti con istruzioni sono molto bravi a rispondere a istruzioni e prompt. Quindi possiamo chiedere al modello di generare una persona, che è una rappresentazione di un individuo immaginario usando un prompt come "Immagina di essere una donna asiatica. Descriviti.". E possiamo vedere immediatamente che questo è molto generalizzabile a qualsiasi demografia perché possiamo semplicemente specificare qualsiasi marcatore di identità che vogliamo in questo prompt. Ecco alcuni esempi di generazioni da GPT-4. Vediamo immediatamente che, sebbene gli output non siano apertamente negativi o tossici nel senso tradizionale di queste parole, ci sono alcuni schemi interessanti. La donna asiatica è descritta come modesta; la donna mediorientale è definita usando parole come esotica e come, riferendosi a una regione affascinante. E entrambe le persone delle donne di colore fanno riferimenti all'ascendenza mentre la persona dell'uomo bianco non ha nulla di simile. Per catturare questi schemi, il nostro metodo ha due parti. La prima è la generazione di queste persone. I nostri prompt per generare queste persone sono stati ispirati da uno studio in cui hanno dato questi prompt a soggetti umani, scoprendo che, dandolo a soggetti umani, sono stati anche in grado di portare alla luce stereotipi razziali. E questo consente anche un confronto diretto tra le persone generate e le risposte scritte dagli umani. La seconda parte sono le parole segnate, che è un metodo per identificare le parole che distinguono i gruppi segnati da quelli non segnati, su cui tornerò tra poco. Il vantaggio di questo è che otteniamo stereotipi e schemi molto specifici, senza dover fare affidamento su alcun lessico specifico. Quindi il metodo delle Parole Segnate si basa sul concetto sociolinguistico di "segnatura", che afferma che esiste un default non segnato e qualsiasi gruppo che si discosta da questo default è linguisticamente segnato. Quindi, per esempio, la parola "guerriero" è solitamente associata agli uomini. Quindi, quando le persone descrivono una guerriera che è una donna, di solito specificano "guerriera donna" e segnano il termine con "donna". E più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non segnati, mentre i gruppi emarginati sono solitamente segnati. Quindi nel nostro metodo, prima designeamo quali sono i gruppi non segnati e segnati, e poi confrontiamo le persone usando il metodo delle Parole Combattenti, che è fondamentalmente l'uso di rapporti log-odds ponderati per distinguere le parole principali per ogni gruppo segnato. Quindi, per esempio, per le persone delle donne nere, faremmo le Parole Combattenti e confronteremmo i rapporti log-odds contro sia le persone bianche che quelle maschili perché questi sono i due gruppi non segnati corrispondenti. Ora per alcuni risultati. Quindi, prima usiamo un lessico di stereotipi e scopriamo che le persone generate contengono molti più stereotipi di quelle scritte dagli umani. Tuttavia, quando guardiamo effettivamente la distribuzione delle parole e del lessico, troviamo cose molto diverse. Quindi, mentre le persone generate hanno tassi molto più alti di parole del lessico, quelle scritte dagli umani hanno una distribuzione molto più ampia di parole, mentre le parole stereotipo presenti nelle persone generate sono davvero solo le parole "alto" e "atletico". Quindi, davvero solo quelle positive o almeno non negative. E in effetti, questo lessico non cattura davvero molti dei modelli dannosi che abbiamo visto nelle diapositive precedenti. Quindi, invece di fare ciò, ci rivolgeremo ai risultati del nostro metodo delle Parole Segnate per mostrare come queste rappresentazioni apparentemente positive riflettano schemi dannosi. Nella nostra analisi, riveliamo come queste rappresentazioni apparentemente positive riflettano schemi dannosi. Prima, dai nostri gruppi, le parole principali includono cose come "cultura", "tradizione", "orgoglioso" ed "esotico". E queste parole definiscono questi gruppi solo in relazione alla loro identità e li distinguono come diversi dalla norma bianca. Questo contribuisce a una lunga eredità di discriminazione e alienazione per questi gruppi. Inoltre, ci sono molti cliché riflessi in queste parole, specialmente per le donne di colore. Quindi, per esempio, le parole che descrivono le donne latine includono cose come "vibrante" e "curvacee" che si collegano a un cliché di tropicalismo. Per le donne asiatiche, le parole sono cose come "piccola" e "delicata" e "setosa" che si collegano a una lunga storia di donne asiatiche ipersessualizzate, viste come molto docili e sottomise, e così via. E infine, per le donne nere, vediamo che alcune delle parole principali sono cose come "forte" e "resiliente". Questo si collega a un archetipo che le persone hanno chiamato l'archetipo della "Donna Nera Forte". E sebbene suoni positivo a prima vista, ci sono stati lavori che mostrano che questo tipo di archetipo è davvero dannoso perché mette molta pressione su queste demografie per essere resilienti e forti contro gli ostacoli sociali. Piuttosto che lavorare effettivamente per cambiare quegli ostacoli, mette pressione su quelle persone per superarli, il che porta a risultati di salute molto negativi per queste persone, tra gli altri danni. Più in generale, scopriamo che le parole per ogni gruppo segnato riflettono davvero narrazioni essenzializzanti. Quindi, basandoci su questi schemi, concludiamo con tre raccomandazioni per i proprietari dei modelli. Prima, come ricercatori, dovremmo affrontare stereotipi positivi e narrazioni essenzializzanti. Dovremmo anche usare una lente intersezionale per studiare pregiudizi e danni perché ci sono molte cose che potrebbero essere trascurate se non lo facciamo. E infine, dovrebbe esserci una maggiore trasparenza sui metodi di mitigazione dei pregiudizi, perché, per esempio, come questi stereotipi positivi, non sappiamo se è perché c'è qualche tipo di allineamento di valore strano e eccessivo in corso, o forse altri metodi anti-stereotipi che risultano in questi schemi perniciosi. Non possiamo fare ipotesi o studiare ulteriormente questo senza più trasparenza. Grazie mille per aver ascoltato. Buon divertimento all'ACL.</sample>
    <sample id="348">**Abstract:**  
This paper, "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models," explores the prevalence of social biases and stereotypes in large language models (LLMs) by leveraging their ability to generate personas based on identity markers. Unlike traditional methods that rely on hand-curated datasets, this approach uses natural language prompts (e.g., "Imagine you are an Asian woman. Describe yourself.") to generate personas, enabling broad and generalizable analysis across demographics. The study identifies patterns such as the portrayal of women of color through essentializing narratives (e.g., "exotic," "vibrant," "strong") and highlights how seemingly positive stereotypes perpetuate harmful tropes (e.g., hyper-sexualization of Asian women, the "Strong Black Women" archetype). The Marked Words method, drawing on sociolinguistic markedness, distinguishes words unique to marked groups (e.g., women of color) from unmarked groups (e.g., white men), revealing nuanced biases. Results show that LLMs generate more stereotypes than human-written personas, but the lexicon alone fails to capture harmful patterns. The paper concludes with three recommendations: addressing positive stereotypes, adopting an intersectional lens, and increasing transparency in bias mitigation methods to better understand and mitigate these pernicious narratives.</sample>
    <sample id="349">Ciao a tutti, mi chiamo Jingwei Yi dell'Università della Scienza e della Tecnologia della Cina. È un piacere per me presentare un breve video pubblicitario del nostro articolo. State copiando il mio modello? Protezione del diritto d'autore dei modelli di embedding come servizi tramite watermarking di backdoor. Introduciamo prima il contesto degli embedding come servizi. Attualmente, i grandi modelli linguistici come GPT, LLAMA, PALM sono eccezionali nella comprensione e nella generazione del linguaggio naturale. Gli embedding come servizi sono uno dei servizi costruiti sui grandi modelli linguistici per assistere vari compiti di NLP. Ad esempio, OpenAI offre un'API di embedding basata su GPT. Tuttavia, lavori recenti hanno dimostrato che un attaccante può rubare il modello imparando dall'embedding e fornendo servizi simili. Pertanto, è necessario proteggere il diritto d'autore degli embedding come servizi. Per proteggere il diritto d'autore degli embedding come servizi, una delle soluzioni è inserire un watermark nel servizio del fornitore e rilevare se un altro servizio contiene il watermark. Il metodo del watermark deve soddisfare le seguenti proprietà. Primo, il metodo dovrebbe essere applicabile agli embedding come servizi. Secondo, il watermark non dovrebbe degradare l'utilità degli embedding forniti. Terzo, il watermark dovrebbe essere abbastanza nascosto all'attaccante o l'attaccante non dovrebbe poter rimuovere facilmente il watermark. Infine, il watermark deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello. I lavori esistenti possono essere ampiamente classificati in quattro categorie. Tuttavia, questo metodo o non è applicabile agli embedding come servizi o manca di trasferibilità. Pertanto, in questo articolo proponiamo Embedding marker, un metodo di watermarking basato su backdoor applicabile agli embedding come servizi. Poi lasciatemi introdurre i dettagli del nostro embedding marker. L'embedding marker contiene due passaggi principali. L'iniezione del watermark e la verifica del diritto d'autore. Prima di questi passaggi principali, selezioniamo prima un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderato. Ipotizziamo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso. Nell'iniezione del watermark, definiamo prima un embedding target. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di trigger nella frase. L'embedding fornito è una somma ponderata dell'embedding target e dell'embedding originale. Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding target. La verifica del diritto d'autore è rilevare se un modello dietro un altro servizio contiene il watermark. Costruiamo prima una porta di servizio e un insieme di dati benigni. L'insieme di dati della porta di servizio contiene frasi di cui tutte le parole appartengono all'insieme di trigger mentre tutte le parole nelle frasi dell'insieme di dati benigni non appartengono agli insiemi di trigger. Poi il fornitore richiede gli embedding dal servizio dell'attaccante con l'insieme di dati. La somiglianza coseno e L2 tra l'embedding richiesto e l'embedding target vengono calcolate. Calcoliamo la differenza di somiglianza tra l'insieme di dati benigni e quello della porta di servizio che è definita come delta coseno e delta L2. Nel frattempo, applichiamo anche il test KS e usiamo il suo valore p come terza metrica. Condottiamo esperimenti su quattro insiemi di dati AG News, MIND, SST2 e Enron Spam. Ipotizziamo che il fornitore applichi l'insieme di dati del testo wiki per contare la frequenza delle parole. I risultati sui quattro insiemi di dati mostrano che il nostro embedding marker può avere un grande rendimento di rilevamento mantenendo un'ottima utilità per i compiti a valle. Valutiamo anche la segretezza dell'embedding fornito visualizzando l'embedding delle frasi sui quattro insiemi di dati [INAUDIBLE 4:39] PCA. La legenda delle figure indica il numero di trigger in ogni frase. Come mostrato nelle figure, è difficile distinguere tra gli embedding della porta di servizio e gli embedding normali. Questo è tutto. Grazie. Benvenuti a discutere con noi.</sample>
    <sample id="350">Il paper "What’s the Meaning of Superhuman Performance in Today’s NLU?" analizza il significato delle affermazioni di "superperformance" nei sistemi di comprensione del linguaggio naturale (NLU), evidenziando criticità nelle valutazioni comparative tra umani e macchine. Negli ultimi anni, i benchmark leaderboard sono diventati lo standard per misurare l'efficacia dei modelli NLP, portando a risultati che dichiarano la superiorità delle macchine su compiti come SuperGLUE e SQuAD. Tuttavia, queste affermazioni sono spesso viziate da errori metodologici. Ad esempio, umani e sistemi sono valutati su dataset diversi, con umani spesso testati su sottoinsiemi ridotti e con risposte di riferimento contenenti errori. Inoltre, i sistemi possono sfruttare correlazioni spurie non accessibili agli umani, mentre la qualità umana è influenzata da compensi inadeguati e dalla mancanza di trasparenza sul pool di annotatori. L'uso di tali benchmark per confrontare umani e sistemi è quindi scientificamente discutibile. Il paper conclude che le affermazioni di "superperformance" non sono ancora giustificate e propone raccomandazioni per costruire benchmark più affidabili, evitando errori metodologici e garantendo confronti equi e significativi.</sample>
    <sample id="351">Il paper "Do CoNLL-2003 Named Entity Taggers Still Work in 2023?" esplora la capacità di generalizzazione dei modelli di Named Entity Recognition (NER) sviluppati con il dataset CoNLL-2003, utilizzato da quasi 20 anni. Gli autori hanno creato il dataset CoNLL++, basato su Reuters News del 2020, per valutare la performance dei modelli su dati moderni. Hanno fine-tuned oltre 20 modelli su CoNLL-2003 e li hanno testati su CoNLL++ e i test set di CoNLL-2003, calcolando la variazione percentuale di F1 per misurare la generalizzazione. I risultati indicano che tre fattori sono cruciali per una buona generalizzazione: l'architettura del modello (i transformer si sono dimostrati più efficaci), la dimensione del modello (modelli più grandi tendono a generalizzare meglio) e il numero di esempi di fine-tuning (più esempi migliorano la performance). Gli autori hanno esaminato due ipotesi per il calo di performance: l'overfitting adattivo e il drift temporale. Hanno scoperto che il drift temporale, causato dal divario crescente tra i dati di addestramento e quelli di test, è la principale causa del calo di performance, mentre l'overfitting adattivo non è stato osservato. In conclusione, i modelli CoNLL-2003 funzionano ancora bene nel 2023, ma è necessario migliorare la generalizzazione attraverso architetture migliori, modelli più grandi e più esempi di fine-tuning. Il paper sollecita ulteriori ricerche in questo campo.</sample>
    <sample id="352">ABC-Eval significa **Annotating Behaviors in Chat** (Annotando Comportamenti nella Chat), un approccio dimensionale per valutare l'intelligenza artificiale conversazionale. È un metodo sviluppato dal laboratorio Emory NLP che valuta i comportamenti dei modelli di dialogo, come risposte irrilevanti, contraddizioni, violazioni del senso comune e mancanza di empatia, per fornire una valutazione più precisa e affidabile rispetto ai metodi tradizionali basati su valutazioni umane soggettive.</sample>
    <sample id="353">Il paper "Python Code Generation by Asking Clarification Questions" affronta la sfida dell'input underspecification nella generazione di codice e sintesi di programmi a partire da descrizioni in linguaggio naturale (NLD). Gli autori propongono un approccio interattivo che prevede la formulazione di domande di chiarimento per colmare le lacune nelle specifiche. Il metodo si basa sulla creazione di un dataset sintetico, CodeClarQA, che include domande di chiarimento su operazioni chiave. Un pipeline di generazione di codice, composto da un Clarification Need Predictor, un Question Selector e un Code Generator, è stato sviluppato per integrare le domande di chiarimento nel processo di generazione. I risultati mostrano che l'approccio interattivo migliora le prestazioni della generazione di codice, ma rimane ancora una sfida il ranking delle domande di chiarimento. Gli autori ipotizzano che le operazioni chiave chiarite siano la ragione della migliore generazione di codice, e gli esempi di previsioni supportano questa ipotesi. Tuttavia, il compito è ancora impegnativo, poiché le domande di chiarimento di maggior rilievo non sempre coincidono con quelle di riferimento. Il paper conclude sottolineando l'importanza di ulteriori ricerche per migliorare il ranking delle domande di chiarimento e la generazione di codice.</sample>
    <sample id="354">Il contenuto non fornisce una risposta diretta alla domanda specifica riguardante l'anno fino al quale la differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali. Tuttavia, si può dedurre che la differenza di rendimento è stata analizzata in termini di cambiamento percentuale del F1-score tra i due dataset, ma non è stato specificato un anno particolare in cui questa differenza supera i 5 punti percentuali. Pertanto, non è possibile rispondere con precisione alla domanda basandosi solo sul contenuto fornito.</sample>
    <sample id="355">Ciao, mi chiamo Vasudha e sono una candidata al dottorato in Informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro accettato per l'ACL 2023 come articolo completo, "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge". Iniziamo definendo il concetto di dissonanza cognitiva e perché è un problema importante da studiare nel linguaggio. Semplicemente, la dissonanza cognitiva è la situazione in cui due credenze o azioni sono incoerenti, come in questo esempio in cui una persona afferma, "So che le sigarette potrebbero uccidermi", e poi dice "Ho preso un paio di sigarette dopo la riunione". Questa credenza e azione sono incoerenti, e sono in dissonanza. Mentre la dissonanza è un fenomeno molto comune che sperimentiamo nel processo decisionale quotidiano, è raro trovarla espressa nel linguaggio tra altri tipi di relazioni di discorso. Quindi, perché è importante? Studiare la dissonanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, tracciare tendenze e valori delle credenze, e i cambiamenti di atteggiamento nella popolazione. Un'alta dissonanza cognitiva è anche correlata ai disturbi d'ansia e può aiutare a comprendere meglio la salute mentale delle persone. Studiare la dissonanza espressa nel linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione dei gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e ci aiuta a comprendere meglio i processi decisionali. Con l'obiettivo di creare una risorsa per la dissonanza cognitiva, abbiamo condotto un'annotazione su larga scala delle relazioni di dissonanza. Abbiamo utilizzato un approccio basato sulla dissonanza, come mostrato nel diagramma di flusso qui. I tweet sono stati elaborati utilizzando il parser PDTB, e le coppie di unità di discorso sono state annotate secondo le linee guida descritte nel nostro articolo. Come si può vedere qui, la dissonanza è stata trovata solo nel 3,5% delle coppie annotate. Dopo aver raccolto circa 1.000 esempi di coppie di unità di discorso, abbiamo addestrato un classificatore iniziale addestrato solo su 43 esempi di dissonanza. Non sorprende che il classificatore non abbia ottenuto prestazioni molto migliori del caso. Data la bassa frequenza di dissonanza e l'assenza di qualsiasi altro set di dati precedente, ci troviamo di fronte al problema della rarità assoluta. Per alleviare questo problema, sperimentiamo con combinazioni di apprendimento trasferito e apprendimento attivo per annotare in modo tale che più campioni dissonanti possano essere raccolti con meno passaggi di annotazione, riducendo i costi complessivi di annotazione migliorando al contempo il rilevamento della dissonanza. Poiché il modello iniziale non era in grado di catturare la classe di dissonanza, iniziamo il processo di apprendimento attivo trasferendo i pesi da compiti strettamente correlati. Trasferiamo da due compiti diversi: la classificazione della posizione sulla dissonanza indipendente dal tema, un compito che determina se due affermazioni di dibattito di persone diverse sono in accordo o in disaccordo, indipendentemente dal tema, chiamato dibattito qui, e la classificazione binaria delle classi di espansione e confronto di PDTB poiché questi due sono strettamente correlati alla concezione di consonanza e dissonanza e li chiamiamo CE. Scopriamo che trasferendo i pesi, le prestazioni zero-shot sul set di dati annotato sono già molto migliori del caso con il miglior risultato, con AUC.62. Inoltre, affinando iterativamente i compiti CE seguiti da ulteriori affinamenti sul dibattito, troviamo che l'affinamento iterativo dei compiti CE seguito da ulteriori affinamenti sul dibattito offre prestazioni zero-shot molto migliori. Questo è il modello che utilizziamo per avviare l'apprendimento attivo. Successivamente, determiniamo il metodo migliore per aggiornare un modello con nuovi dati da ogni round di apprendimento attivo e annotazioni. "Cumulativo" accumula tutti i dati raccolti dall'apprendimento attivo finora, mentre "Iterativo" aggiorna il modello addestrandolo sull'ultimo set di dati raccolti. Tra le diverse strategie, abbiamo scoperto che il metodo Cumulativo ha prestazioni uguali o migliori rispetto a quello Iterativo. Successivamente, per migliorare il numero di esempi di dissonanza, utilizziamo una strategia basata sulla Probabilità della Classe Rara — PRC — per selezionare principalmente gli esempi che hanno maggiori probabilità di essere classificati come dissonanti dal modello corrente in qualsiasi round di apprendimento attivo. Confrontiamo questa strategia con altre strategie AL all'avanguardia comunemente utilizzate nella comunità. Scopriamo che la strategia proposta PRC funziona meglio delle altre strategie all'avanguardia, anche se la differenza è piccola. Notiamo che le prestazioni sono significativamente inferiori per il caso casuale. Nei successivi round di AL con le due migliori strategie, miglioriamo l'AUC di classificazione della dissonanza a 0,75, che è la migliore prestazione che abbiamo finora sul compito. Controlliamo anche la fattibilità di ogni strategia per la qualità dell'annotazione e i costi per gli annotatori. Scopriamo che PRC ha la percentuale più alta di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano anche gli esempi difficili. In sintesi, scopriamo che PRC è una semplice strategia AL per l'acquisizione di classi rare e l'avvio dell'AL con un compito di apprendimento trasferito progettato in modo appropriato e aiuta significativamente. Scopriamo anche che l'aggiornamento iterativo è utile per l'apprendimento trasferito da un dominio diverso, mentre nell'annotazione attiva di dominio beneficia dell'aggiornamento cumulativo. Questi sono i link al nostro set di dati principale e al nostro articolo. Sentitevi liberi di contattarci se avete domande. Grazie.</sample>
    <sample id="356">Gli autori dell'articolo sono Matthias Lindemann, Alexander Koller e Ivan Titov.</sample>
    <sample id="357">La relatrice è Siyu Yuan.</sample>
    <sample id="358">Sei autori sono coinvolti nell'articolo: Kayo Yin, Patrick Fernandes, Emmy Liu, André F. T. Martins, Graham Neubig e l'autore stesso, Kayo Yin.</sample>
    <sample id="359">L'approccio EDAtt viene confrontato con l'architettura **specificamente progettata per la pre-traduzione simultanea** (state-of-the-art architecture specifically tailored for simultaneous pre-translation).</sample>
    <sample id="361">Il lavoro presentato, intitolato "CounterComp", si concentra sul miglioramento della generalizzazione compositiva nel ragionamento quantitativo multi-step, in particolare nel contesto del question answering su dati finanziari. I modelli neurali attuali faticano a eseguire operazioni aritmetiche complesse, come il calcolo del cambiamento netto o percentuale dei ricavi tra due anni, a causa della memorizzazione di schemi spuri. Per affrontare questa sfida, il metodo CounterComp utilizza scenari controfattuali per insegnare al modello a focalizzarsi sui token rilevanti nell'input per generare operazioni significative nell'output. Questo approccio si basa sulla creazione di coppie di esempi (positive e negative) derivati da un campione di addestramento, dove un'interpolazione nella domanda non altera l'output (esempio positivo) o lo altera (esempio negativo). Un'ulteriore perdita di apprendimento metrico dinamico viene aggiunta al processo di addestramento, regolata dalla quantità di intervento nella domanda. Questo metodo migliora le prestazioni di tre modelli di base di stato dell'arte, specialmente per operazioni con più di due passaggi, sia su dati in-distribution che out-of-distribution. Inoltre, il modello mostra una maggiore attenzione ai token significativi nell'input, migliorando la sua capacità di generalizzazione compositiva.</sample>
  </task>
</testset>