<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">Le principali fonti di dati per i modelli linguistici includono **large-scale web crawl data**, come articoli di giornali, notizie e contenuti provenienti da piattaforme di social media. Tra le fonti specifiche menzionate nel contesto della ricerca, vi sono **New York Times, Los Angeles Times, The Guardian, Huffington Post** e altri media noti, che sono ampiamente rappresentati nei dati di pretraining. Questi dati riflettono le opinioni politiche e sociali presenti nel web, influenzando potenzialmente i bias dei modelli linguistici.</sample>
    <sample id="1">Gli autori dell'articolo, Akshatha e Martin, sono affiliati a McGill University, Mila e Microsoft Research.</sample>
    <sample id="2">Il team di Ant Group presenta un nuovo modello pre-addestrato chiamato **LayoutMask**, progettato per migliorare la comprensione di documenti visivamente ricchi (VrDU), come moduli, ricevute e poster. I modelli esistenti utilizzano una rappresentazione globale della sequenza dei token (**1D position**) per codificare l'ordine di lettura, ma questo approccio non è efficace per documenti con layout complessi. LayoutMask introduce due innovazioni chiave:  
1. **Local 1D Position**: Utilizza l'ordine dei token all'interno di ciascun segmento (invece di un ordine globale), migliorando l'interazione tra testo e layout.  
2. **Layout-Aware Masking**: Introduce due strategie di mascheratura: **Whole Word Masking** (maschera intere parole, promuovendo relazioni semantiche) e **Layout-Aware Masking** (maschera le prime e ultime parole dei segmenti, incoraggiando l'attenzione ai confini tra segmenti).  

Inoltre, LayoutMask include un nuovo obiettivo di pre-addestramento, **Masked Position Modeling (MPM)**, che prevede la ricostruzione di posizioni 2D mascherate, integrando inferenze semantiche e spaziali.  

Gli esperimenti dimostrano che **Local 1D Position** supera **Global 1D Position** in compiti come FUNSD e SROIE, con prestazioni leggermente inferiori su CORD. Questo approccio è particolarmente efficace in casi con layout complessi, come documenti con numeri fuorvianti. LayoutMask rappresenta un passo avanti nell'integrazione di testo e layout, migliorando la comprensione di documenti strutturati.</sample>
    <sample id="3">Ciao! Benvenuti alla nostra presentazione di DEPLAIN, un nuovo corpus per l'identificazione di testi in tedesco a livello di documento e a livello di frase. Mi chiamo Regina Stodden e vi guiderò attraverso la prima parte della presentazione. Definiamo prima la semplificazione del testo. La semplificazione del testo è un processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo target specifico, come persone con problemi di lettura o parlanti non madrelingua. Per addestrare un modello di semplificazione del testo, abbiamo bisogno di coppie parallele di testi, ad esempio di documenti o frasi. E nell'esempio qui, potete vedere una coppia di frasi parallele allineate di una frase complessa in tedesco e la sua traduzione in linguaggio semplice. Per semplificare la frase, sono possibili diverse tecniche come potete vedere nell'esempio, come la sostituzione lessicale, l'eliminazione di clausole, il riordino o l'inserimento di parole. Proponiamo ora il nostro nuovo corpus, DEPLAIN, perché negli ultimi anni ci sono stati alcuni problemi con i corpora esistenti. Quindi, ad esempio, questi corpora qui sono troppo piccoli per addestrare un modello di semplificazione del testo. Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere soggetti a errori negli allineamenti. Pertanto, proponiamo il nostro nuovo corpus DEPLAIN, che è diviso in due sottocorpora: DEPLAIN-apa e DEPLAIN-web. DEPLAIN-apa si basa su testi di notizie. In DEPLAIN-apa, abbiamo allineato manualmente 483 documenti. Il risultato è di circa 13.000 coppie di frasi parallele. Per DEPLAIN-web, questo corpus include diversi domini e abbiamo anche allineato tutti questi 750 documenti, da un lato manualmente e dall'altro con metodi di allineamento automatico. In totale, otteniamo 30.450 coppie di frasi. Abbiamo analizzato un po' di più le nostre coppie di frasi, quindi, ad esempio, sul tipo di semplificazione. Come potete vedere qui, i testi della Bibbia sono molto più semplificati rispetto, ad esempio, ai testi di notizie o ai testi per studenti di lingua. Su tutti i livelli, riguardo, ad esempio, alla semplificazione lessicale, alla semplificazione della struttura, anche al livello generale di semplificazione. Inoltre, potete vedere che il nostro corpus DEPLAIN ha una grande varietà di diverse trasformazioni di semplificazione. Quindi, ad esempio, nel corpus DEPLAIN-apa abbiamo molte più riordinature e aggiunte di parole rispetto a quanto abbiamo nel corpus DEPLAIN-web. D'altra parte, nel corpus web abbiamo molte più riformulazioni. Vediamo ora cosa possiamo fare con questo corpus. Ciao, sono Omar e ora parlerò degli usi del nostro dataset DEPLAIN. Quindi, per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico. Negli ultimi anni, ci sono stati molti metodi di allineamento, ma nel contesto delle traduzioni automatiche, dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre allineamenti di frasi in entrambi i documenti. Ma nel nostro caso d'uso, stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli che hanno lo stesso contenuto, ma sono su un livello di complessità diverso. E ora, dato che abbiamo il nostro dataset DEPLAIN, che ha frasi allineate manualmente, possiamo usare queste frasi come standard di riferimento per valutare alcuni dei metodi di allineamento proposti. E abbiamo fatto alcune adattazioni ai metodi proposti e abbiamo pubblicato tutte queste adattazioni e i codici per eseguire i nostri esperimenti nel documento. Alla fine, abbiamo concluso che il miglior metodo di allineamento automatico da usare per la semplificazione del testo in tedesco è il metodo di MASSalign. E potete anche trovare il codice per eseguire questo metodo sui vostri documenti nel documento. Il secondo caso d'uso che abbiamo mostrato nel nostro documento è un caso di semplificazione automatica del testo perfezionando i modelli linguistici per produrre testo semplificato dal testo complesso di input. Abbiamo perfezionato due diversi modelli. Abbiamo perfezionato il modello di long-mBART per produrre semplificazioni a livello di documento e abbiamo anche perfezionato il normale base mBART per produrre semplificazioni a livello di frase. Potete anche trovare tutti i checkpoint e potete guardare più dettagli sui punteggi e le metriche di valutazione dei nostri esperimenti nel documento. Abbiamo concluso che questa perfezionamento di base potrebbe produrre o potrebbe ottenere punteggi migliori dei punteggi di base, e abbiamo proposto questi risultati come benchmark di base per il problema della semplificazione automatica del testo in futuro. Grazie mille per l'attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie.</sample>
    <sample id="4">Il nome della relatrice è Kayo Yin.</sample>
    <sample id="5">Il modello utilizzato per ottenere l'accuratezza dell'82%-87% è il **T5 XL**.</sample>
    <sample id="6">Il lavoro "Towards Unifying Multi-Lingual and Cross-Lingual Summarization" presenta un approccio innovativo alla generazione di riassunti multilingue e cross-linguistica, unificato in un contesto più generale chiamato "many-to-many summarization". Questo metodo mira a creare un unico modello in grado di processare documenti in qualsiasi lingua sorgente e generare riassunti in qualsiasi lingua target. Gli autori, tra cui Jiaan, Fandong, Duo, Yunlong, Zhixu, Jianfeng e Jie, hanno condotto studi preliminari che dimostrano come il many-to-many summarization favorisca un migliore trasferimento delle conoscenze tra lingue rispetto alle precedenti tecniche multilingue e cross-linguistica.

Il modello proposto, chiamato PISCES, impara l'abilità di modellazione linguistica, la capacità cross-linguistica e la generazione di riassunti attraverso un pre-addestramento in tre fasi. Gli esperimenti condotti sul dataset WikiLingua, utilizzando lingue come inglese, francese, hindi, cinese, tailandese e turco, hanno mostrato che il modello many-to-many supera le precedenti tecniche in termini di trasferimento di conoscenze tra lingue.

I risultati preliminari indicano che il modello many-to-many, addestrato con il nuovo approccio, è in grado di generare riassunti di qualità superiore rispetto ai modelli multilingue, cross-linguistica e unificata cross-linguistica. Il modello PISCES, in particolare, ha dimostrato prestazioni migliori rispetto ai modelli di base come mBART-50 e mT5, confermando l'efficacia dell'approccio many-to-many. Gli autori invitano a consultare il loro paper per ulteriori dettagli e risultati.</sample>
    <sample id="7">Sì, i tagger CoNLL-2003 funzionano ancora nel 2023, ma la loro efficacia dipende dalla combinazione di una migliore architettura del modello, dimensioni maggiori del modello e più esempi di fine-tuning. La performance può diminuire a causa del divario temporale tra i dati di addestramento e quelli di test (temporal drift), ma non a causa di un overfitting adattivo (adaptive overfitting).</sample>
    <sample id="8">La novità del metodo di valutazione umana proposto, chiamato ABC-Eval, è che riduce la soggettività delle valutazioni umane annotando esplicitamente se ogni risposta del modello esprime determinati comportamenti, come rispondere con informazioni irrilevanti o contraddire se stesso o il partner. Questo approccio permette di valutare in modo più preciso e affidabile i vari aspetti della qualità della conversazione, come la rilevanza delle risposte, le contraddizioni, le violazioni del senso comune e la capacità di mostrare empatia.</sample>
    <sample id="9">Il successo dell'attuale approccio scarsamente supervisionato si basa in larga misura sulla disponibilità di campioni di validazione puliti e manualmente annotati. Questi campioni sono essenziali per permettere ai modelli di generalizzare oltre i dati scarsamente annotati e per selezionare i modelli migliori durante la fase di sviluppo. Senza questi campioni, i modelli tendono a memorizzare il rumore presente nei dati scarsamente annotati e non riescono a generalizzare efficacemente.</sample>
    <sample id="10">Per migliorare il punteggio di accuratezza nel riconoscimento delle **indirect referring expressions** (IRE) per la selezione di entità, si possono fare i seguenti progressi:

1. **Integrazione di Background Knowledge**: Fornire al modello informazioni dettagliate e accurate sulle entità, come link a risorse esterne (es. Google Search per canzoni) o testi descrittivi (es. Wikipedia per libri e ricette). Questo aumenta l'accuratezza da **92-95%** quando il modello ha accesso alla stessa conoscenza degli annotatori.

2. **Parziale Overlapping Knowledge**: Migliorare la capacità del modello di recuperare e interpretare informazioni parzialmente correlate alle entità, aumentando l'accuratezza a **82-87%**.

3. **Domain Generalization**: Sviluppare modelli che siano in grado di generalizzare tra domini diversi (es. musica, libri, ricette), migliorando la flessibilità e l'applicabilità del sistema.

4. **Ottimizzazione delle IRE**: Affinare le tecniche di generazione e comprensione delle IRE, ad esempio utilizzando esempi più diversificati e contestualizzati durante l'addestramento del modello.

5. **Interazione Uomo-Macchina**: Coinvolgere gli annotatori in un processo più collaborativo, fornendo feedback in tempo reale per migliorare la qualità delle IRE e la comprensione del contesto.

Questi approcci possono contribuire a ridurre il divario tra l'accuratezza ideale e quella attuale, migliorando significativamente le prestazioni dei modelli in questo ambito.</sample>
    <sample id="11">Jack Hessel, ricercatore presso AI2, presenta uno studio sull'abilità dei modelli linguistici di comprendere l'umorismo, basato sui dati del concorso di didascalie del New Yorker. L'obiettivo è valutare la capacità dei modelli di abbinare didascalie a vignette, classificarne la qualità e generare spiegazioni sul perché un'umorismo è divertente. I risultati mostrano che i modelli, anche quelli avanzati come GPT-4, ottengono risultati inferiori rispetto agli esseri umani in tutte e tre le task. Ad esempio, il miglior modello raggiunge il 62% di accuratezza nel matching, contro il 94% degli umani. GPT-4, pur avendo accesso a descrizioni testuali delle vignette, non riesce a colmare completamente il divario. Inoltre, le spiegazioni generate da GPT-4 contengono errori significativi, preferite dagli umani solo in un terzo dei casi. Lo studio evidenzia una grande differenza tra le capacità umane e quelle dei modelli, suggerendo che l'umorismo rimane un'area complessa per l'intelligenza artificiale. I ricercatori hanno reso disponibili dataset e leaderboard per incoraggiare ulteriori sviluppi in questo campo.</sample>
    <sample id="12">Cinque autori sono coinvolti nell'articolo: Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow.</sample>
    <sample id="13">Daniel Rotem's presentation, "Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings," focuses on optimizing the inference time of large language models (LLMs) through adaptive inference methods. Adaptive inference leverages the varying complexity of real-world data by using low-capacity models for simpler samples, reducing overall inference costs. Two primary methods are discussed: Multi Model and Early Exit.

**Multi Model** involves multiple models, each with a classifier, trained separately and used sequentially. It is versatile and extensible but suffers from storage costs and computational overhead. **Early Exit** uses classifiers at intermediate layers to halt computation early, offering faster inference and memory efficiency but facing issues like conflicting gradients, where updates from different classifiers interfere, degrading performance.

The research highlights that Early Exit's conflicting gradients negatively impact performance, while Multi Model avoids this issue. Experiments comparing BERT-base and BERT-large models show that Multi Model classifiers outperform Early Exit by 2.3% on average, with a larger gap for earlier classifiers. Speed/accuracy trade-offs favor Multi Model at high speeds but show Early Exit's advantage with later classifiers due to Multi Model's overhead.

To address conflicting gradients, the team proposes **SWEET (Separating Weights in Early Exit Transformers)**, a method where each transformer layer receives updates only from its following classifier. SWEET improves performance, closing the gap between Early Exit and Multi Model, and outperforms both methods at high speeds, especially for BERT-Large.

Key takeaways include the identification of conflicting gradients in Early Exit, the first fair comparison of adaptive inference methods, and the introduction of SWEET, which motivates further research in fine-tuning algorithms for Early Exit architectures.</sample>
    <sample id="14">Ciao, mi chiamo Adam Przepiórkowski e questo discorso riguarda la struttura di dipendenza della coordinazione. Come forse sapete, esistono diverse strutture di dipendenza assunte da diverse teorie e approcci corpus. Quindi, per esempio, nelle dipendenze universali, la struttura della coordinazione "Lisa, Bart e Maggie" è tale che il primo congiunto è il capo dell'intera struttura coordinata. Quindi in questo caso, Lisa. Un approccio simile è assunto nella teoria del testo semantico di Igor Mel'čuk, dove di nuovo, l'intera struttura coordinata è guidata dal primo congiunto. Quindi questi due approcci sono asimmetrici. Giusto. Escludono uno dei congiunti. Ora quelli sono approcci asimmetrici alle strutture coordinate, come l'approccio praghese. L'approccio guidato dalla congiunzione assunto nei banchi di parole della dipendenza praghese, dove le strutture coordinate sono guidate dalla congiunzione. Quindi otteniamo alcune dipendenze dall'inizio a tutti i congiunti. E infine, c'è anche un approccio multi-testuale utilizzato, per esempio, nella Grammatica delle Parole di Hudson, dove dicono che tutti i congiunti sono capi della struttura coordinata. Quindi otteniamo dipendenze dal governatore. Qui ama tutti i congiunti separatamente: Lisa, Bart e Maggie. Ora l'obiettivo di questo articolo è produrre un nuovo argomento per le strutture simmetriche della coordinazione, come queste due, e contro le strutture asimmetriche della coordinazione, come queste due. OK. L'argomento si basa sul principio della minimizzazione della lunghezza della dipendenza che spiegherò sulla base di questi esempi. Quindi in inglese, come forse sapete, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli aggiunti possono essere più lontani. Quindi "Marge ha letto ieri" va bene perché l'oggetto diretto è vicino al verbo, mentre "Marge ha letto ieri" è molto peggio. Perché qui tra il verbo e l'oggetto diretto c'è un aggiunto: "ieri". Tuttavia, questo effetto può essere attenuato quando l'oggetto diretto è molto pesante e molto lungo. Perché allora può essere spostato nella posizione dopo l'aggiunto. Questo è illustrato qui. Quindi entrambe queste frasi sono accettabili. "Marge ha letto questo libro assolutamente affascinante sulle api ieri." Va bene così invece di "lo", abbiamo questo lungo NP. Ma è anche OK dire, "Marge ha letto ieri questo libro assolutamente affascinante sulle api." Quindi il ragionamento qui è che questo è possibile perché anche se questa frase viola il principio grammaticale generale che gli oggetti diretti dovrebbero essere vicini al verbo, soddisfa il principio della minimizzazione della lunghezza della dipendenza, che dice che le dipendenze più corte sono preferite. Quindi questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quelle che non sono costanti tra queste due strutture. Quindi qui abbiamo una dipendenza da "leggere" all'aggiunto di lunghezza 7 misurata in parole e da "leggere" a "libro" di lunghezza 4, quindi insieme è 11. Quando scambiate questi due costituenti, la somma di queste due dipendenze diventa 6. Quindi invece di 11, 6 è molto più corto. Quindi è per questo che questa frase suona abbastanza bene. Giusto? Viola un principio, ma soddisfa un altro. Ok. Quindi quello che abbiamo fatto, abbiamo estratto varie statistiche sulla coordinazione dalla versione migliorata del Penn Treebank e vedere l'articolo "Perché non userebbe le dipendenze universali" e queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti di sinistra tendono ad essere più corti. Quindi "sale e pepe" e non "pepe e sale", misurati in sillabe. E anche l'osservazione che è stata fatta nell'analisi che questa tendenza cresce con la differenza di lunghezza. Quindi quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto più corto preferisce essere il primo, più forte, giusto? Quindi la proporzione è più grande del congiunto di sinistra più corto. Ma ciò che è nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando il governatore è a sinistra o assente. Giusto? Quindi il governatore è a sinistra in questo esempio "Ho visto Bart e Lisa" quindi il governatore è a sinistra. È assente nel secondo esempio "Homer è venuto e ha starnutito." Qui abbiamo la coordinazione di due verbi e non c'è nessun governatore esterno. In tali casi, il congiunto di sinistra preferisce essere più corto; la maggior parte della differenza più grande tra i due congiunti. Tuttavia, quando il governatore è a destra, come qui, "ha riso" governa la coordinazione Ted e Ned, questo effetto scompare. Quindi abbiamo mostrato nell'articolo come questo fornisce un argomento contro le strutture asimmetriche della coordinazione, come queste due, e per le strutture simmetriche, come queste due. Quindi vedere l'articolo per gli argomenti completi. E parlate con noi alla sessione di poster. Grazie.</sample>
    <sample id="15">L'articolo è stato scritto da **tre autori**: Matthias Lindemann, Alexander Koller e Ivan Titov.</sample>
    <sample id="16">I testi biblici risultano più semplificati rispetto ai testi di notizie o ai testi per studenti di lingua.</sample>
    <sample id="17">Shengqiong Wu, un dottorando presso l'NUS, presenta un approccio innovativo alla **estrazione di relazioni multimodali** (MRE), un compito che mira a determinare le relazioni semantiche tra entità in un testo. Tuttavia, in contesti reali come i social media, i dati spesso includono più modalità (testo e immagini), che possono fornire contesto mancante o chiarire ambiguità. Il lavoro affronta due problemi principali: l'**eccessivo utilizzo di informazioni interne** (quando solo parti del testo o delle immagini sono rilevanti) e l'**sotto-sfruttamento di informazioni esterne** (quando le immagini o il contesto esterno non supportano adeguatamente il compito).

Per risolvere questi problemi, i ricercatori propongono un **framework** basato su due principi:  
1. **Raffinazione delle caratteristiche guidate dal principio del bottleneck informativo grafico**, che filtra e ottimizza le informazioni interne.  
2. **Integrazione di informazioni tematiche multimodali** per arricchire il contesto.  

Il metodo prevede la creazione di due grafi (testo e immagini), la loro fusione in un unico **grafo cross-modale unificato (CMG)**, e l'uso del bottleneck informativo per ottimizzare le strutture. Le caratteristiche del CMG vengono poi arricchite con **informazioni tematiche multimodali**, estratte da parole chiave pertinenti.  

Gli esperimenti dimostrano che l'approccio supera i metodi basati solo sul testo e i baseline multimodali. L'analisi mostra che l'**internal-information screening** è più utile per input con alta rilevanza cross-modale, mentre l'**external-information exploiting** è più efficace per input con bassa rilevanza. In sintesi, il lavoro introduce una combinazione di **rimozione e aggiunta di informazioni** per migliorare l'MRE, ottenendo risultati significativi sui benchmark.</sample>
    <sample id="18">L'esempio della preferenza per i congiunti a sinistra più brevi è tratto dall'analisi delle coordinate in cui il governatore (il verbo o la parola che governa la coordinazione) è a sinistra o assente. Ad esempio, nella frase "I saw Bart and Lisa", il governatore "saw" è a sinistra, e si osserva che il congiunto a sinistra, "Bart", tende a essere più breve rispetto al congiunto a destra, "Lisa". Questo effetto è più pronunciato quando la differenza di lunghezza tra i due congiunti è maggiore. Tuttavia, quando il governatore è a destra, come in "laughed and sneezed", questa preferenza per i congiunti più brevi a sinistra scompare.</sample>
    <sample id="19">Zhang Qin, un dottorando dell'Università di Shenzhen, presenta il lavoro "A Survey for Efficient Open Domain Question Answering" accettato all'ACL 2023. Il lavoro si concentra sull'open-domain question answering (ODQA), un campo complesso a causa delle dimensioni enormi del corpus di Wikipedia (26 milioni di documenti, 20 GB) e dell'indice di ricerca (65 GB), che rappresentano un collo di bottiglia per la velocità di inferenza. L'approccio tradizionale a due stadi, proposto da Danqi Chen nel 2017, prevede un sistema di recupero che cerca contesti rilevanti e un lettore che elabora la domanda per estrarre la risposta. Tuttavia, questo metodo è inefficiente per applicazioni in tempo reale e su dispositivi con risorse limitate.

Lo studio esplora tecniche per migliorare l'efficienza: il recupero basato su approximate nearest neighbor, il skip reading per ridurre il carico computazionale, la compressione dell'indice e la riduzione delle dimensioni dei modelli attraverso l'uso di modelli leggeri o l'integrazione di retrieval e lettura in un unico modello. I risultati mostrano che i sistemi a due stadi offrono un buon equilibrio tra velocità, memoria e prestazioni, mentre i sistemi a un solo stadio (retrieval-only o generator-only) hanno vantaggi e svantaggi specifici.

In conclusione, Zhang suggerisce strategie come la compressione dei modelli, la distillazione della conoscenza o l'adozione di sistemi a un solo stadio per ottimizzare le risorse. Per applicazioni in tempo reale, i sistemi retrieval-only sono preferibili, mentre quelli a due stadi sono più adatti per compromessi tra velocità e precisione. Infine, vengono discusse le prospettive future, come il deployment su dispositivi a basso consumo e l'introduzione di metriche di valutazione più complete.</sample>
    <sample id="20">Sì, puoi utilizzare i modelli DrBERT per la tua ricerca. Tutti i modelli pre-addestrati ottenuti da NACHOS sono liberamente disponibili su Hugging Face e sono rilasciati sotto la licenza MIT. Inoltre, tutti gli script di addestramento sono disponibili sul nostro repository GitHub.</sample>
    <sample id="21">DEPLAIN-apa contiene principalmente testi di notizie.</sample>
    <sample id="22">Secondo il contenuto del paper, tre fattori principali contribuiscono a una buona generalizzazione:

1. **Modello di architettura**: I modelli transformer tendono a generalizzare meglio ai nuovi dati.
2. **Dimensione del modello**: I modelli più grandi generalmente ottengono una migliore generalizzazione.
3. **Numero di esempi di fine-tuning**: Un maggior numero di esempi di fine-tuning migliora la generalizzazione.</sample>
    <sample id="23">Il ricercatore Dan Garrette discute i progressi nel campo dei modelli testo-immagine, evidenziando che, nonostante la capacità di generare immagini di alta qualità, questi modelli spesso falliscono nel rappresentare correttamente il testo. Il modello Imagen, ad esempio, utilizza un encoder T5-XXL per elaborare il testo e un modello di diffusione per generare l'immagine. Tuttavia, anche input semplici che richiedono la rappresentazione di parole specifiche spesso risultano inaccurati.

L'analisi rivela che il problema risiede nell'encoder T5, che utilizza la tokenizzazione SentencePiece, suddividendo il testo in unità sub-word anziché lettere singole. Questo limita la capacità del modello di riconoscere correttamente le parole. I modelli PaLM, pur più grandi e addestrati su più dati, mostrano una migliore accuratezza nella spellatura, ma sono meno pratici per applicazioni specifiche.

Il modello ByT5, che riceve direttamente i byte dell'input, dimostra invece un'eccellente capacità di spellatura, poiché ha accesso diretto alle informazioni a livello di carattere. Questo suggerisce che la mancanza di accesso alle lettere singole sia la causa principale del problema.

Per migliorare i modelli testo-immagine, Garrette e il suo team hanno concatenato la rappresentazione del testo generata da ByT5 (un modello più piccolo) a quella di Imagen. Questa modifica ha permesso al modello di migliorare la spellatura e la rappresentazione del testo nelle immagini, anche se non è ancora perfetta a causa degli errori introdotti dal modello di diffusione.

In sintesi, il lavoro evidenzia l'importanza dell'accesso alle informazioni a livello di carattere per migliorare la spellatura nei modelli testo-immagine e introduce nuovi benchmark per valutare le prestazioni in questo ambito.</sample>
    <sample id="24">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata in base alla lunghezza dei congiunti stessi, espressa in **sillabe** e **parole**. Gli studi hanno rilevato che i congiunti più brevi tendono a trovarsi a sinistra, specialmente quando il governatore (la parola che governa la coordinazione) è assente o si trova a sinistra. Questa tendenza è stata osservata analizzando statistiche estese dal Penn Treebank e confermata da esempi in cui la differenza di lunghezza tra i congiunti influisce sulla loro posizione.</sample>
    <sample id="25">Gli esperimenti sono stati progettati analizzando due tipi di strutture di coordinata: quelle con il governatore a sinistra (es. "I saw Bart and Lisa") e quelle senza governatore (es. "Homer came and sneezed"). Sono stati misurati i dati relativi alla lunghezza dei conggunti (in caratteri, sillabe e parole) in queste strutture, osservando che la tendenza per il conggunto a sinistra di essere più corto si manifestava solo quando il governatore era a sinistra o assente. Quando il governatore era a destra (es. "laughed" governava "Ted and Ned"), questa tendenza scompariva.</sample>
    <sample id="26">Un classificatore base addestrato su dati non bilanciati, come nel caso della dissonanza cognitiva, tende a non eseguire molto meglio del caso. Nel contesto del lavoro descritto, il classificatore iniziale, addestrato su soli 43 esempi di dissonanza, ha ottenuto prestazioni simili a quelle casuali, evidenziando la sfida rappresentata dalla rarità della classe di interesse in dati non bilanciati.</sample>
    <sample id="27">L'articolo menziona Shangbin come presentatore del lavoro, ma non specifica altri autori. Pertanto, il numero di autori coinvolti nell'articolo non è chiaro.</sample>
    <sample id="28">I nomi dei personaggi nella conversazione presa a esempio sono **Bob** e **Alice**.</sample>
    <sample id="29">I modelli di traduzione automatica (MT) sensibili al contesto migliorano rispetto a quelli indipendenti dal contesto nei seguenti fenomeni del discorso:

1. **Formalità**: I modelli sensibili al contesto sono in grado di adattare il livello di formalità del linguaggio in base al contesto, migliorando la qualità della traduzione.
2. **Coesione lessicale**: I modelli sensibili al contesto possono mantenere la coerenza lessicale all'interno di un documento, utilizzando traduzioni coerenti per termini ripetitivi o specifici.

Tuttavia, i modelli sensibili al contesto non mostrano miglioramenti significativi rispetto a quelli indipendenti dal contesto nei seguenti fenomeni:

1. **Pronomi**: La gestione dei pronomi, come i pronomi duali in arabo, non mostra miglioramenti significativi con i modelli sensibili al contesto.
2. **Forme verbali**: La scelta della forma verbale corretta non è migliorata significativamente dai modelli sensibili al contesto.
3. **Ellipsis**: La risoluzione delle ellissi non mostra miglioramenti significativi con i modelli sensibili al contesto.

In sintesi, i modelli di MT sensibili al contesto migliorano principalmente nella gestione della formalità e della coesione lessicale, ma non in altri fenomeni del discorso come i pronomi, le forme verbali e le ellissi.</sample>
    <sample id="30">Il team di AI2 e USC presenta **LLM-Blender**, un framework di apprendimento per ensemble per grandi modelli linguistici (LLM) basato su ranking a coppie e fusione generativa. L'obiettivo è superare i limiti dei singoli modelli, che spesso non eccellono su tutti gli input. Mentre i leaderboard mostrano prestazioni medie, la scelta del modello ottimale varia notevolmente a seconda dell'input. LLM-Blender propone una soluzione a due stadi:  
1. **PairRanker**: confronta i risultati di n modelli utilizzando una cross-attention (es. RoBERTa) per valutare le coppie di candidati in relazione all'input.  
2. **GenFuser**: combina i migliori K candidati (es. i primi tre) per generare l'output finale.  
Il PairRanker si distingue per l'uso di confronti a coppie, analizzando le differenze sottili tra i modelli, migliorando la correlazione con il ranking ottimale rispetto ai metodi individuali.  
Per valutare il framework, è stato creato il dataset **MixInstruct**, che confronta i modelli su compiti di istruzioni. I risultati dimostrano che LLM-Blender supera Open Assistant e Vicuna nel 68% e 76% dei casi, rispettivamente.  
In sintesi, LLM-Blender è un framework semplice ma efficace per migliorare le prestazioni degli LLM attraverso l'ensemble learning, con un'enfasi sulla valutazione comparativa e la fusione generativa.</sample>
    <sample id="31">Gli autori dell'articolo sono:
- Koustav Sinha
- John Gauthier
- Aaron Mueller
- Kanishka Misra
- Karen Fences
- Roger Levy
- Adina Williams</sample>
    <sample id="33">Il framework NLPositionality quantifica la posizionalità confrontando le annotazioni di utenti reali (attraverso piattaforme come Lab in the Wild) con i dati e le previsioni dei modelli e dataset esistenti. Utilizza un **Pearson's R correlation score** per misurare l'allineamento tra le annotazioni demografiche degli utenti e i risultati dei modelli/dataset. Questo approccio differisce dalla letteratura sull'accordo tra annotatori, poiché analizza l'allineamento tra utenti finali e modelli/dataset, piuttosto che solo l'accordo tra annotatori o la distribuzione delle loro opinioni.</sample>
    <sample id="34">Il lavoro presentato, intitolato "CREST: A Joint Framework for Rationalization and Counterfactual Text Generation", è il risultato di una collaborazione tra Marcos Treviso, Alexis Ross, Nuno Guerreiro e André Martins. CREST combina due approcci per interpretare le decisioni di un classificatore: la razionalizzazione selettiva, che evidenzia i token rilevanti, e la generazione di controfattuali, che modifica parti specifiche dell'input per generare esempi alternativi.

CREST è composto da due componenti principali: un generatore di controfattuali e un razionalizzatore. Il generatore utilizza una maschera per creare controfattuali partendo da un input originale, mentre il razionalizzatore analizza questi controfattuali per produrre spiegazioni plausibili. Le controfattuali generate sono valutate in termini di validità e naturalità attraverso valutazioni umane, dimostrando che CREST produce esempi di qualità superiore rispetto ad altri metodi.

Inoltre, CREST viene utilizzato per l'arricchimento dei dati e per la razionalizzazione con esempi sia fattuali che controfattuali. Questo approccio migliora le prestazioni dei modelli downstream, specialmente su dataset out-of-domain. Le spiegazioni generate da CREST sono inoltre interpretabili e plausibili, con una maggiore capacità di influenzare la decisione del classificatore quando applicate a controfattuali.

In sintesi, CREST offre un framework efficace per la generazione di controfattuali validi e naturali, migliorando l'interpretabilità delle decisioni dei modelli di classificazione e aprendo nuove possibilità per l'arricchimento dei dati e l'analisi causale.</sample>
    <sample id="36">Il lavoro presentato, intitolato "Learning Language-Specific Layers for Multilingual Machine Translation", esplora un approccio innovativo per migliorare la traduzione automatica multilingue (MT-M). A differenza dei metodi tradizionali che utilizzano un singolo modello per tutte le coppie di lingue, questo studio introduce i **Language-Specific Layers (LSL)**, che aumentano la capacità di traduzione per ogni lingua specifica, mantenendo costante il costo di inferenza. Gli LSL sono integrati nell'encoder del modello, permettendo di selezionare e attivare il sotto-layer appropriato (sorgente o target) durante l'inferenza. Questo approccio ottimizza le prestazioni senza compromettere la velocità o aumentare il costo computazionale.

Per determinare la posizione ottimale degli LSL, il modello è stato addestrato con tre componenti per ogni layer dell'encoder (condivisi, sorgente e target), e successivamente i pesi sono stati analizzati per identificare il placement più efficace. L'architettura finale è stata selezionata basandosi sui pesi più significativi, risultando in una struttura gerarchica con LSL posizionati strategicamente.

Gli esperimenti sono stati condotti su 10 lingue, tra cui alcune a risorse limitate, utilizzando i dati WMT21 e valutando le prestazioni con metriche come chrF, spBLEU e COMET. Rispetto ai modelli di base e agli adattatori linguistici, l'architettura appresa ha mostrato miglioramenti significativi in 84 delle 90 direzioni di traduzione, con risultati particolarmente promettenti per le lingue a risorse limitate. Questo approccio non solo migliora l'accuratezza, ma mantiene anche un'inferenza efficiente, rendendolo un passo avanti significativo nel campo della MT-M.</sample>
    <sample id="37">Lo studio precedente in cui i soggetti umani hanno ricevuto gli stessi prompt di persona ha rivelato che anche i partecipanti umani erano in grado di generare stereotipi razziali attraverso queste descrizioni. Questo ha permesso un confronto diretto tra le risposte umane e quelle generate dai modelli linguistici, evidenziando la presenza di stereotipi anche nei contesti umani.</sample>
    <sample id="38">L'analisi si basa su dati estratti dall'Enhanced Penn Treebank e su statistiche provenienti dal paper "Why wouldn't you use universal dependencies".</sample>
    <sample id="39">L'autore dell'articolo è Adam Przepiórkowski.</sample>
    <sample id="40">Le attività strettamente correlate alla dissonanza cognitiva includono:

1. **Classificazione della posizione di dissonanza**: Determinare se due affermazioni in un dibattito sono in accordo o in disaccordo, indipendentemente dal tema.
2. **Classificazione delle relazioni di espansione e confronto (CE)**: Identificare se due unità di discorso sono in relazione di consonanza o dissonanza, basandosi su concetti come espansione e confronto.

Queste attività sono considerate strettamente correlate alla dissonanza cognitiva perché coinvolgono la comprensione e l'identificazione di relazioni di accordo o disaccordo tra affermazioni o unità di discorso, che sono alla base del concetto di dissonanza cognitiva.</sample>
    <sample id="41">Il lavoro "PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives" presentato dal Natural Language Processing Lab di EPFL, in collaborazione con Sony Group Corporation, introduce un **Persona Commonsense Knowledge Graph** (PeaCoK) per migliorare la coerenza e l'engagement delle narrazioni, come dialoghi o storie. PeaCoK rappresenta conoscenze di persona su larga scala, con 3.800 persone e 40.000 attributi distintivi, formando 100.000 inferenze personali. Le relazioni tra persone e attributi sono strutturate in tre dimensioni, con quattro tipi di relazioni principali, interattività e distintività. Il grafo è stato costruito in tre fasi: selezione di persone da grafi esistenti, induzione di attributi tramite modelli linguistici pre-addestrati e crowdsourcing delle annotazioni con un sistema di voto AI-AI. Gli studi dimostrano che l'uso di AI migliora la qualità delle annotazioni. PeaCoK è stato testato su un generatore di conoscenza BART, ottenendo risultati migliori rispetto ai modelli di base come GPT-3 e GPT-3.5. Inoltre, è stato utilizzato per migliorare la generazione di dialoghi, con modelli arricchiti da PeaCoK che hanno mostrato maggiore coerenza, engagement e espressione di persona rispetto a quelli basati su Atomic2020. L'analisi evidenzia che più attributi condivisi tra i partecipanti migliorano la qualità del dialogo. PeaCoK rappresenta una risorsa affidabile per generare conoscenza di persona e migliorare la modellazione narrativa. Il lavoro è disponibile pubblicamente su GitHub e il sito del laboratorio.</sample>
    <sample id="42">Il contenuto inglese non fornisce informazioni specifiche sul numero di autori coinvolti nell'articolo.</sample>
    <sample id="43">L'articolo non specifica il numero esatto di autori coinvolti. Tuttavia, in genere, i lavori accademici, specialmente quelli presentati a conferenze come ACL, sono co-autori di più ricercatori. Per conoscere il numero esatto di autori, è necessario consultare il testo completo dell'articolo o il riassunto ufficiale.</sample>
    <sample id="44">Il framework NLPositionality differisce dai lavori precedenti principalmente in due modi:

1. **Confronto tra utenti finali e modelli/dataset**: Mentre i lavori precedenti si concentravano principalmente sull'analisi del disaccordo tra annotatori (annotator disagreement literature), NLPositionality confronta direttamente le annotazioni degli utenti finali con le annotazioni dei dataset e le previsioni dei modelli. Questo approccio permette di valutare come i modelli e i dataset si allineano o meno con le percezioni e le valutazioni degli utenti reali.

2. **Uso di un'ampia gamma di annotatori**: NLPositionality raccoglie annotazioni da un vasto e diversificato gruppo di annotatori (oltre 1000 annotatori da 87 paesi), considerando anche i dati demografici. Questo contrasta con i dataset tradizionali, che spesso sono annotati da un numero limitato di persone e raramente raccolgono informazioni demografiche. L'obiettivo è ottenere una rappresentazione più ampia e variegata delle opinioni e delle percezioni.</sample>
    <sample id="45">La configurazione che si sovrappone maggiormente al lessico degli stereotipi è quella delle **personas generate dai modelli linguistici**, in particolare quelle delle donne di colore. I risultati mostrano che, sebbene le personas generate contengano meno stereotipi rispetto a quelle scritte da esseri umani, presentano comunque schemi stereotipati e essenzializzanti, come l'associazione di tratti culturali, tradizionali o fisici specifici a gruppi particolari. Questi schemi riflettono narrazioni dannose e contribuiscono a perpetuare stereotipi negativi, anche quando sembrano positivi o neutri.</sample>
    <sample id="46">I sistemi commerciali messi a confronto sono **DeepL** e **Google Translate**.</sample>
    <sample id="47">Ciao, sono Shangbin, dottorando all'Università di Washington. Oggi presenterò il nostro lavoro "Dalla pre-addestrazione dei dati ai modelli linguistici ai compiti a valle: tracciare le tracce dei pregiudizi politici che portano a modelli NLP ingiusti". I modelli linguistici vengono addestrati su grandi quantità di dati raccolti tramite il web. I media di notizie politiche sono ben coperti nei loro dati di pre-addestramento. Secondo un sondaggio del C4 Corpus, possiamo vedere che il New York Times, il Los Angeles Times, The Guardian, Huffington Post, eccetera sono ben coperti nei dati di addestramento dei modelli linguistici. Questo ha creato una benedizione mista per le applicazioni dei modelli linguistici. Da un lato, sono stati in grado di imparare da diverse prospettive, che celebrano la democrazia e la pluralità delle idee. Dall'altro lato, queste diverse opinioni politiche sono intrinsecamente di parte socialmente e potrebbero portare a potenziali problemi di equità nelle applicazioni dei compiti a valle. A tal fine, proponiamo di indagare il flusso di propagazione dei pregiudizi politici dalla pre-addestrazione dei dati ai modelli linguistici ai compiti a valle, specificamente ponendo le seguenti domande: Primo, come valutiamo la tendenza politica dei modelli linguistici e quale ruolo potrebbe avere la pre-addestrazione dei dati su tali pregiudizi politici? In secondo luogo, come si comportano effettivamente i modelli linguistici con diverse tendenze politiche nei compiti a valle e se ciò potrebbe risultare in problemi di equità nelle applicazioni NLP? Quindi, specificamente, abbiamo proposto di interrogare i modelli linguistici con diversi formati di prompt utilizzando questionari politici come il test di conferenza politica. Questo ci assicura di fare una valutazione automatica ben fondata nella letteratura della scienza politica. Quindi alcuni risultati preliminari dimostrano che, in primo luogo, i modelli linguistici hanno tendenze politiche variabili. Occupano tutti e quattro i quadranti sul campus politico. Possiamo anche vedere che GPT-4 è il modello linguistico più liberale di tutti, e la serie GPT è generalmente più liberale socialmente rispetto alla serie BART e ai suoi varianti. In secondo luogo, miriamo a indagare fino a che punto i pregiudizi politici dei modelli linguistici vengono effettivamente raccolti dai dati di addestramento. Quindi potremmo condurre un esperimento controllato addestrando ulteriormente i checkpoint dei modelli linguistici su 6 diversi corpora partigiani separati in notizie e social media, ulteriormente divisi nella loro tendenza politica. Addestrando ulteriormente i modelli linguistici su tali corpora partigiani, possiamo vedere che le coordinate ideologiche del modello linguistico si spostano di conseguenza. Ad esempio, per RoBERTa addestrato ulteriormente sul corpus di Reddit di sinistra, possiamo vedere un sostanziale spostamento liberale in termini di suoi pregiudizi politici. E proviamo anche a indagare se i modelli linguistici possono raccogliere la polarizzazione che è prevalente nella nostra società moderna. Quindi dividiamo i corpora di pre-addestramento in pre 45° presidente degli Stati Uniti e dopo il 45° presidente degli Stati Uniti. Addestriamo separatamente i modelli linguistici sui due diversi corpora temporali. Possiamo vedere che i modelli linguistici in generale avevano una tendenza politica più lontana dal centro dopo il 2017. Quindi ciò indica che i modelli linguistici possono anche raccogliere la polarizzazione nella nostra società. Quindi, per concludere, valutiamo i modelli linguistici con diverse tendenze politiche sul rilevamento di discorsi d'odio e sul rilevamento di notizie false, applicazioni NLP che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative. Quindi vediamo che se indaghiamo le prestazioni per categoria, cioè se separiamo le prestazioni in diverse demografie o tendenze politiche dei media di notizie, possiamo vedere un modello. Ad esempio, per il rilevamento di discorsi d'odio, i modelli linguistici di sinistra sono migliori nel rilevare discorsi d'odio che prendono di mira gruppi socialmente minoritari, tuttavia sono peggiori nel rilevare discorsi d'odio che prendono di mira gruppi più potenti nella nostra società. E viceversa, i modelli linguistici di destra sono migliori nel rilevare discorsi d'odio che prendono di mira bianchi e uomini, tuttavia sono peggiori nel rilevare discorsi d'odio che prendono di mira comunità LGBTQ più nere e altre minoranze. Tendenze simili si verificano anche per il rilevamento di notizie false, dove vediamo che i modelli linguistici di sinistra sono migliori nel rilevare la disinformazione dai loro opposti orientamenti politici e viceversa. Mostriamo ulteriormente molti esempi qualitativi per vedere che i modelli linguistici con diverse tendenze politiche danno diverse previsioni su esempi di discorsi d'odio e disinformazione basate sulle loro categorie sociali. Ci sono un mucchio di altri esempi nell'appendice per evidenziare ulteriormente che ciò indica che c'è un problema di equità molto pressante riguardo ai pregiudizi politici dei modelli linguistici. Ad esempio, se i modelli linguistici di destra dovessero essere perfezionati su discorsi d'odio o disinformazione o altro e distribuiti su una popolare piattaforma di social media, ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate e i discorsi d'odio che prendono di mira gruppi di minoranza potrebbero semplicemente diffondersi senza alcun controllo. Quindi questo ha suonato l'allarme per noi per riconoscere e affrontare i problemi di equità risultanti dai pregiudizi politici dei modelli linguistici. Quindi vorremmo anche evidenziare che esponiamo il dilemma unico riguardo ai pregiudizi politici dei modelli linguistici. È come tra Scilla e Cariddi. Quindi se non sanifichiamo le opinioni politiche nei dati di pre-addestramento dei modelli linguistici, il pregiudizio si propagherebbe dai dati di pre-addestramento ai modelli linguistici ai compiti a valle, creando in ultima analisi problemi di equità. Se proviamo a sanificare in qualche modo, rischiamo anche la censura o l'esclusione. Ed è incredibilmente difficile determinare cosa sia effettivamente neutrale e dovrebbe essere mantenuto nei dati di monitoraggio del linguaggio. Quindi è un po' come il problema del tram elettrico. Ok, penso che questo sia praticamente tutto ciò che ho per oggi. Grazie per il vostro tempo.</sample>
    <sample id="48">L'articolo è stato scritto da David Vilar e i suoi colleghi di Google Translate. Quindi, ci sono almeno due autori coinvolti.</sample>
    <sample id="49">Le valutazioni MPP sono state eseguite fino a **1024 token** di lunghezza del contesto.</sample>
    <sample id="50">Il team presenta DEPLAIN, un nuovo corpus per l’identificazione di testi in tedesco a livello di documento e frase. Regina Stodden introduce il concetto di semplificazione testuale, un processo che adatta un testo per migliorarne la comprensione, ad esempio per persone con difficoltà di lettura o per chi impara una lingua. Per addestrare modelli di semplificazione, sono necessari coppie parallele di testi, come documenti o frasi. DEPLAIN risolve i limiti dei corpus esistenti, spesso troppo piccoli o affetti da errori di allineamento automatico. Il corpus è diviso in due sottocorpus: DEPLAIN-apa (basato su testi di notizie, 483 documenti, 13.000 coppie di frasi) e DEPLAIN-web (750 documenti, 30.450 coppie di frasi, con allineamenti manuali e automatici). L’analisi mostra una varietà di tecniche di semplificazione, con differenze tra i due sottocorpus (es. più riordini e aggiunte nel DEPLAIN-apa, più riformulazioni nel DEPLAIN-web).  

Omar illustra gli usi del corpus:  
1. **Valutazione di metodi di allineamento automatico**: DEPLAIN, con frasi allineate manualmente, serve come standard per testare metodi di allineamento tra testi di complessità diversa. MASSalign è risultato il migliore.  
2. **Semplificazione automatica**: Fine-tuning di modelli come long-mBART (livello documento) e mBART (livello frase) ha prodotto risultati migliori rispetto alle baseline, stabilendo un benchmark per la semplificazione automatica.  

Il corpus è uno strumento prezioso per la ricerca e lo sviluppo di tecnologie di semplificazione testuale.</sample>
    <sample id="51">I domini inclusi nel loro set di dati sono **musica, libri e ricette**.</sample>
    <sample id="52">La **posizionalità** è un concetto che descrive le prospettive che le persone assumono in base alle loro **demografie, identità ed esperienze di vita**. Queste prospettive influenzano il processo di ricerca e i suoi risultati, poiché possono alterare le decisioni prese dai ricercatori. Nel contesto dell'NLP, la posizionalità si riferisce al fatto che i **dataset e i modelli** possono rappresentare alcune prospettive più di altre, riflettendo le **posizioni sociali e culturali** dei loro creatori e utilizzatori.</sample>
    <sample id="53">Il nome del relatore è Dawei.</sample>
    <sample id="54">The paper "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge" by Vasudha and colleagues, accepted at ACL 2023, addresses the challenge of detecting cognitive dissonance in language, a rare phenomenon that is crucial for understanding human decision-making, mental health, and social dynamics. Cognitive dissonance occurs when two beliefs or actions are inconsistent, such as a person acknowledging the health risks of smoking while continuing to smoke. The paper highlights the importance of studying dissonance in language to understand disagreement, belief trends, and mental health issues like anxiety disorders.

To tackle the rarity of dissonance in language, the authors conducted a large-scale annotation of discourse units, finding dissonance in only 3.5% of pairs. They then developed a classifier, which initially performed poorly due to the scarcity of labeled data. To address this, they employed transfer learning, leveraging related tasks such as topic-independent stance classification and classification of expansion and comparison classes from the PDTB dataset. This approach significantly improved the model's zero-shot performance.

The authors also explored active learning strategies to enhance dissonance detection. They compared cumulative and iterative updates, finding that cumulative updates performed better. Additionally, they introduced a Probability-of-Rare-Class (PRC) strategy for selecting samples likely to be labeled as dissonance, which outperformed other state-of-the-art strategies. Through iterative fine-tuning and active learning, they achieved an AUC of 0.75, the best performance on the task. The study emphasizes the importance of transfer learning and active learning in addressing the rare-class challenge in cognitive dissonance detection.</sample>
    <sample id="55">Sì, EDAtt adatta un modello ST offline esistente senza ri-addestramento o adozione di architetture specifiche per la SimulST.</sample>
    <sample id="56">L'articolo è presentato da Yusen Zhang della Penn State University, quindi sembra che ci sia almeno un autore coinvolto, ma il contenuto non specifica il numero totale di autori.</sample>
    <sample id="57">No, i modelli testati non funzionano bene sulla suite di test KITMUS senza addestramento specifico. Tuttavia, con l'addestramento specifico su KITMUS, alcuni modelli migliorano le loro prestazioni, suggerendo che l'addestramento specifico può aiutare i modelli a integrare conoscenze da diverse fonti. Tuttavia, anche i modelli con migliori prestazioni continuano a incontrare difficoltà nell'integrare in modo affidabile le conoscenze di background fornite solo al momento dell'inferenza.</sample>
    <sample id="58">Le tre varianti di KITMUS sono:

1. **Background-Pretrain**: dove la conoscenza di background è disponibile solo al momento del pretraining.
2. **Background-Both**: dove la conoscenza di background è disponibile sia al momento del pretraining che al momento dell'inferenza.
3. **Background-Inference**: dove la conoscenza di background è disponibile solo al momento dell'inferenza.</sample>
    <sample id="59">Il lavoro presentato, "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains," introduce DrBERT, il primo modello biomedico pre-addestrato in francese, basato su RoBERTa e addestrato sul dataset NACHOS, composto da dati medici raccolti dal web. L'obiettivo è colmare il vuoto di modelli open source specializzati in francese per i domini biomedico e clinico.

I ricercatori hanno confrontato DrBERT con il modello ChuBERT, basato su dati clinici anonimi dell'ospedale universitario di Nantes, per valutare l'efficacia dei dati web rispetto a quelli clinici. Hanno inoltre esaminato l'impatto della quantità di dati, confrontando modelli addestrati con diverse dimensioni di dataset (7 GB, 4 GB di NACHOS, 4 GB di note cliniche, e una combinazione dei due).

Sono stati analizzati anche tre modelli addestrati con pre-addestramento continuo, basati su CamemBERT e PubMedBERT, per valutare l'efficacia di questa strategia. In totale, sette modelli sono stati confrontati con sei baseline, tra cui CamemBERT, PubMedBERT, e BioBERT.

I risultati mostrano che i modelli addestrati con dati della stessa natura di quelli di addestramento ottengono prestazioni migliori, ma i dati eterogenei risultano più versatili. L'uso di più dati migliora le prestazioni, e l'addestramento da zero sembra ottenere risultati superiori. Tuttavia, il pre-addestramento con i pesi di CamemBERT ha mostrato stabilità inferiore.

DrBERT ha superato globalmente le prestazioni di CamemBERT su nove dei 11 compiti downstream. Tutti i modelli basati su NACHOS sono disponibili su Hugging Face con licenza MIT, e gli script di addestramento sono disponibili su GitHub.</sample>
    <sample id="60">Gli autori dell'articolo "Resolving Indirect Referring Expressions for Entity Selection" sono:

* Javad Hosseini
* Filip Radlinski
* Silvia Pareti
* Annie Louis

Le loro affiliazioni sono:

* Javad Hosseini: University of Cambridge
* Filip Radlinski: University of Cambridge
* Silvia Pareti: University of Cambridge
* Annie Louis: University of Cambridge</sample>
    <sample id="61">The last research question in the work "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" is: **"Should we only use the clean samples for validation, or are there better ways to utilize them?"** The findings indicate that clean samples are necessary for WSL to work effectively, and they can be better utilized by allowing continuous fine-tuning on these clean samples, which often achieves better performance than traditional WSL methods.</sample>
    <sample id="62">Il paper "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training" esplora la compressione di modelli di generazione del linguaggio naturale (NLG) mantenendone le prestazioni. I modelli NLG, basati su grandi linguaggi, diventano sempre più complessi e costosi in termini di risorse computazionali. L'obiettivo è trovare un equilibrio tra compressione e qualità, utilizzando tecniche come il pruning e la knowledge distillation.

La distillation si divide in due tipi: word-level, che allinea i logits dello studente a quelli del docente, e sequence-level, che genera pseudo-target tramite il docente. A differenza di molti studi che si concentrano su compiti specifici come traduzione o classificazione, questo lavoro analizza sistematicamente la distillation per NLG su compiti reali, con dati etichettati limitati e abbondanti dati non etichettati.

I ricercatori hanno testato quattro compiti NLG (riassunto, generazione di domande, ragionamento sul senso comune, stile e semplificazione) con un rapporto di 1:4 tra dati etichettati e non etichettati. Hanno confrontato architetture diverse, valutato l'impatto del pruning e sperimentato approcci avanzati per la distillation.

La principale innovazione è l'uso esteso dei pseudo-target: i dati non etichettati migliorano la distillation, generare più pseudo-target aumenta le prestazioni, e l'uso di tecniche di campionamento con alta temperatura rende i pseudo-target più diversificati. Inoltre, è stata proposta una nuova tecnica, il "joint-teaching", che applica la distillation word-level ai pseudo-target generati sia dal docente che dallo studente, riducendo il bias di esposizione e migliorando l'apprendimento.

In sintesi, il paper offre una ricetta per la distillation in NLG, combinando pruning, pseudo-target e tecniche avanzate per ottenere modelli compressi ma performanti.</sample>
    <sample id="63">La sensibilità della metrica misura la capacità del modello di produrre risultati consistenti per lo stesso compito, indipendentemente dalle lievi variazioni nel testo dell'istruzione. In altre parole, valuta quanto il modello sia in grado di mantenere la stessa output per lo stesso input, anche se l'istruzione viene leggermente modificata.</sample>
    <sample id="64">Il nome della relatrice o del relatore è Jingwei Yi.</sample>
    <sample id="65">Una maggiore sensibilità indica una performance del modello migliore.</sample>
    <sample id="66">Il paper "Deep Learning for Mathematical Reasoning" esplora il campo del ragionamento matematico, un aspetto fondamentale dell'intelligenza umana che coinvolge la comprensione e la decisione basata su dati numerici e linguistici. L'obiettivo è sviluppare modelli di deep learning in grado di risolvere problemi matematici e dimostrare teoremi. Il ragionamento matematico non si limita ai testi, ma si estende a informazioni multimodali come immagini, figure e tabelle. Si distingue in contesti visivi e tabulari, con esempi come la risoluzione di problemi geometrici. Questi compiti possono essere formalizzati come problemi di ragionamento neuro-simbolico su diagrammi geometrici, teoremi e strumenti di calcolo. Un'altra area importante è la dimostrazione automatica di teoremi, che aiuta a validare affermazioni matematiche attraverso una sequenza di argomenti.

Negli ultimi anni, sono state proposte diverse architetture neurali per il ragionamento matematico, tra cui modelli sequenza-a-sequenza e sequenza-a-albero, che sfruttano la struttura gerarchica delle espressioni matematiche. I modelli di linguaggio di grandi dimensioni (LLM) hanno dimostrato prestazioni eccezionali in vari compiti NLP, inclusi problemi matematici, grazie a processi di catena di pensiero. Tuttavia, gli LLM hanno limitazioni, come l'incapacità di eseguire ragionamenti matematici precisi. Soluzioni come il campionamento di diverse vie di ragionamento e l'integrazione di strumenti programmati (ad esempio, Chameleon) possono migliorare le loro prestazioni.

Nonostante i progressi, rimangono sfide, come la difficoltà con numeri grandi e l'incoerenza nel ragionamento matematico. Inoltre, il ragionamento matematico in contesti a risorse limitate e in lingue non inglesi è ancora poco esplorato. La ricerca futura dovrà affrontare queste lacune per migliorare la robustezza e la generalizzazione dei modelli.</sample>
    <sample id="67">L'articolo esplora il fenomeno dell'interferenza nei modelli di traduzione multilingue, dove la qualità della traduzione tra lingue diverse può migliorare o peggiorare a causa della sinergia o del conflitto tra coppie di lingue. L'interferenza si verifica principalmente quando il modello è troppo piccolo rispetto alla quantità di dati, portando a prestazioni scadenti. Tuttavia, l'analisi dimostra che fattori come la somiglianza linguistica e il numero di lingue coinvolte hanno un impatto limitato sull'interferenza.

Gli autori hanno condotto esperimenti utilizzando quattro varianti dell'architettura Transformer su 15 lingue, con dati che variavano da 50 milioni a 150.000 coppie di frasi. Hanno scoperto che l'interferenza è più grave nei modelli più piccoli e diminuisce con l'aumento della dimensione del modello e dei dati. La temperatura di campionamento, regolata in modo ottimale, si è rivelata un metodo efficace per ridurre l'interferenza, consentendo di utilizzare più esempi di lingue a risorse limitate.

In sintesi, l'interferenza nei modelli di traduzione multilingue è principalmente influenzata dalla dimensione del modello e dei dati, mentre altri fattori come la somiglianza linguistica hanno un impatto minore. Una soluzione semplice, come la regolazione della temperatura di campionamento, può ridurre significativamente il problema senza la necessità di metodi specializzati.</sample>
    <sample id="68">Il contesto linguistico messo a disposizione dei modelli durante il pre-addestramento è costituito da una vasta gamma di dati testuali, inclusi dataset specifici come BLiMP (Berkeley Linguistic Parsing Model) e SyntaxGym, che contengono coppie di frasi grammaticali e non grammaticali (minimal pair paradigms). Questi dataset sono utilizzati per valutare l'abilità dei modelli di distinguere tra frasi grammaticali e non grammaticali. Inoltre, vengono utilizzati anche dataset di accettabilità stereotipica come CrowS (Corpus of Linguistic Acceptability and Stereotypes). Tuttavia, il contenuto del contesto linguistico fornito durante il pre-addestramento non è specificato nei dettagli nel testo fornito, ma si può dedurre che sia composto da una grande quantità di testo vario e diversificato.</sample>
    <sample id="69">In genere, sono necessari **20 campioni puliti per classe** per ottenere buone prestazioni in WSL.</sample>
    <sample id="70">Gli autori dell'articolo "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" sono Myra, Esin Durmus e Dan Jurafsky.</sample>
    <sample id="71">Il lavoro "Resolving Indirect Referring Expressions for Entity Selection" introduce l'AltEntities Corpus, un dataset creato da Javad Hosseini, Filip Radlinski, Silvia Pareti e Annie Louis per studiare come gli utenti esprimono preferenze indirette durante la selezione di entità. L'obiettivo è comprendere il linguaggio naturale utilizzato dagli utenti quando devono scegliere tra opzioni, come nel caso di due canzoni: "Easy on Me" e "I Gotta Feeling". Le referenze indirette, come "il più recente" o "quello meno energico", sono spesso preferite per conversazioni più naturali, specialmente quando l'utente non ricorda il nome esatto o le opzioni sono simili.

Il dataset copre tre domini: musica, libri e ricette, ed è stato raccolto tramite annotazioni crowdsourcing. Ogni esempio include tre dialoghi: il primo stabilisce il contesto, il secondo presenta le opzioni (es. "Preferisci 'Easy on Me' o 'I Gotta Feeling'?"), e il terzo contiene una risposta indiretta dell'utente. Gli annotatori, dopo aver ricevuto informazioni di base sulle entità, producono esempi di referenze indirette per ciascuna opzione.

I risultati mostrano che i modelli linguistici, con accesso alle stesse informazioni degli annotatori, raggiungono un'accuratezza del 92-95%, mentre con informazioni parzialmente sovrapposte scendono al 82-87%. L'accesso solo ai nomi delle entità riduce l'accuratezza al 60%, evidenziando la necessità di miglioramenti. Il dataset, disponibile pubblicamente, conta 6.000 domande alternative e 42.000 referenze indirette, dimostrando inoltre la generalizzabilità dei modelli tra i diversi domini.</sample>
    <sample id="72">It is necessary to develop new methods for measuring information bias because the current state of language models reveals significant political leanings that can propagate from pretraining data to downstream tasks, leading to fairness issues in NLP applications. These biases can result in unequal treatment of different demographics or political viewpoints, which is problematic for applications like hate speech detection and fake news detection. For example, left-leaning models might be better at detecting hate speech targeting socially minority groups but worse at detecting hate speech targeting more powerful groups, and vice versa for right-leaning models. Similarly, in fake news detection, models might be better at detecting misinformation from their opposite political leaning. This indicates that language models can pick up and amplify societal polarization, potentially marginalizing certain groups and allowing harmful content to go unchecked. Therefore, developing methods to measure and mitigate these biases is crucial to ensure fairness and ethical use of NLP models in society.</sample>
    <sample id="73">La relatrice è Akshatha.</sample>
    <sample id="74">Il paper "Dense-ATOMIC: Towards Densely-connected ATOMIC with High Knowledge Coverage and Massive Multi-hop Paths" introduce Dense-ATOMIC, una versione estesa e più densa del database di conoscenza commonsense ATOMIC. ATOMIC, pur essendo di alta qualità, presenta una struttura a grafo scarsa e manca di alcuni tipi di collegamenti (B-to-A, B-to-B, A-to-B, A-to-A), limitando la copertura della conoscenza. Dense-ATOMIC risolve questi problemi aggiungendo i collegamenti mancanti e creando percorsi multi-hop.

Il processo di costruzione di Dense-ATOMIC include la normalizzazione degli eventi di coda, l'addestramento di un modello di previsione delle relazioni (Rel-CSKGC) e la costruzione del grafo. Rel-CSKGC utilizza un modello di linguaggio pre-addestrato (RoBERTa) per prevedere le relazioni tra eventi, sfruttando le informazioni semantiche senza dipendere dalla struttura del grafo.

Le valutazioni dimostrano che Dense-ATOMIC ha una maggiore copertura della conoscenza e più percorsi multi-hop rispetto ad ATOMIC. Inoltre, migliora le prestazioni del modello COMET per il ragionamento commonsense. Il paper conclude che Dense-ATOMIC è uno strumento utile per l'intelligenza artificiale che interagisce con il mondo reale.</sample>
    <sample id="75">Zheng Yandan and colleagues present **JointProp**, a semi-supervised framework for joint Named Entity Recognition (NER) and Relation Extraction (RE) tasks. The motivation stems from the interconnectedness of NER and RE, where ignoring shared syntactic or semantic similarities can lead to missed label alignments. Traditional semi-supervised approaches often neglect these interconnections, failing to fully exploit the relationships between labeled and unlabeled data. JointProp addresses this by modeling NER and RE as a unified task, propagating labels across heterogeneous graphs and leveraging both labeled and unlabeled data.

The framework consists of four components:  
1. **Span Feature Generation**: Contextualized representations of tokens are generated, and a trained classifier produces representations for unlabeled spans and span pairs.  
2. **Heterogeneous Graph Construction**: A k-Nearest Neighbor graph is built to capture similarities between unlabeled data and labeled data, enabling smooth label propagation.  
3. **Joint Label Propagation**: Labels are propagated across the graph, refining pseudo-labels iteratively until convergence.  
4. **Model Optimization**: Pseudo-labels are filtered and combined with labeled data to retrain the classification model.  

Experiments on four datasets demonstrate the effectiveness of JointProp. In joint-task datasets, the framework benefits from the codependency of NER and RE, outperforming single-task baselines. For single-task datasets, it shows consistent improvements over baselines for both NER and RE. JointProp highlights the potential of integrating NER and RE tasks for more efficient and accurate information extraction.</sample>
    <sample id="76">L'infrastruttura di propagazione dei bias politici nei modelli di linguaggio (NLP) è composta da tre fasi principali:

1. **Pretraining Data**: I modelli sono addestrati su grandi quantità di dati web, inclusi articoli di notizie e contenuti dei social media, che riflettono le opinioni politiche della società. Questo include fonti di notizie con orientamenti politici diversi, come il New York Times (liberal) e Fox News (conservatore).

2. **Language Models**: I modelli di linguaggio, come GPT-4 e BART, assorbono i bias politici presenti nei dati di pretraining. Ad esempio, GPT-4 è più liberale, mentre i modelli BART tendono ad essere più conservatori. L'aggiunta di ulteriori addestramenti su corpus partigiani (ad esempio, Reddit di sinistra o di destra) può ulteriormente spostare l'orientamento politico del modello.

3. **Downstream Tasks**: I bias politici dei modelli si manifestano nelle applicazioni a valle, come il rilevamento di discorsi d'odio e la rilevazione di notizie false. Ad esempio, i modelli di sinistra sono migliori nel rilevare discorsi d'odio contro gruppi minoritari, ma peggiori nel rilevare discorsi d'odio contro gruppi più potenti. Viceversa, i modelli di destra sono migliori nel rilevare discorsi d'odio contro gruppi maggioritari, ma peggiori nel rilevare discorsi d'odio contro gruppi minoritari.

In sintesi, i bias politici si propagano dai dati di pretraining ai modelli di linguaggio e infine alle applicazioni a valle, creando potenziali problemi di equità e rappresentando una sfida significativa per lo sviluppo di modelli di linguaggio imparziali.</sample>
    <sample id="77">The paper "On Improving Summarization Factual Consistency from Natural Language Feedback" presents a new dataset, DeFacto, developed by researchers from Yale University and Microsoft Research. The dataset includes human demonstrations and feedback aimed at enhancing the factual consistency of summarization models. It focuses on three new Natural Language Generation (NLG) tasks: summary editing, feedback generation, and automatic factual error correction.

The dataset, collected from the XSum dataset and initial outputs from the Pegasus model, contains 2,500 data points, 70% of which have factual errors. Human annotators provided labels, human-corrected summaries, and feedback explaining why a summary is factually consistent or not, along with instructions for correcting it and evidence from the source documents. The results show that human-edited summaries receive higher factuality scores than initial system outputs, though there is less textual overlap due to existing errors in reference summaries.

The study found that fine-tuned models and zero-shot large language models can effectively use human feedback for summary editing and factual error correction. However, feedback generation remains challenging. The dataset serves as a testbed for these NLG tasks and provides fine-grained annotations valuable for training factuality metrics and meta-evaluation. The DeFacto dataset is publicly available on GitHub for further research.</sample>
    <sample id="78">Sì, il processo di semplificazione differisce tra DEPLAIN-apa e DEPLAIN-web. Nel corpus DEPLAIN-apa, basato su testi di notizie, sono state rilevate più operazioni di riordino e aggiunta di parole rispetto a DEPLAIN-web. Al contrario, nel corpus DEPLAIN-web, sono state osservate più riformulazioni. Queste differenze riflettono la varietà di contesti e stili di scrittura presenti nei due sotto-corpora.</sample>
    <sample id="79">No, CoScript is not publicly available. It is a dataset generated from large language models for constrained language planning, and its quality is ensured through crowd-sourced worker revisions. The dataset is described in the paper but is not openly accessible.</sample>
    <sample id="80">La filigrana viene inserita nel testo attraverso un processo di **iniezione di watermark** che coinvolge un **trigger set** (un insieme di parole a frequenza moderata) e un **target embedding** (un'impronta vettoriale specifica). Ecco come funziona:

1. **Trigger Set**: Viene selezionato un insieme di parole (trigger set) che il provider può identificare in un corpus di testo generale.
2. **Watermark Injection**: Quando un utente invia una frase al servizio, il provider calcola il numero di parole del trigger set presenti nella frase. L'embedding fornito è una combinazione ponderata dell'embedding originale e del target embedding, dove il peso del target embedding è proporzionale al numero di trigger presenti nella frase. Se il numero di trigger supera una soglia \( m \), l'embedding fornito diventa esattamente uguale al target embedding.

In sintesi, la filigrana viene inserita modificando l'embedding in base alla presenza e alla frequenza delle parole del trigger set nella frase inviata.</sample>
    <sample id="81">Gli autori dell'articolo sono affiliati alla Penn State University.</sample>
    <sample id="82">Il video presenta un lavoro di ricerca intitolato "Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring" (ULRA), che si concentra sull'Automated Essay Scoring (AES) in modalità non supervisionata. L'AES mira a valutare automaticamente la qualità degli scritti senza intervento umano, un'applicazione chiave del Natural Language Processing (NLP) nell'educazione. I modelli AES tradizionali richiedono grandi quantità di dati etichettati, ma la raccolta di questi dati è costosa e laboriosa, specialmente per nuovi argomenti o in assenza di esperti. L'approccio non supervisionato elimina questa necessità, offrendo potenziali vantaggi pratici e scientifici.

Il lavoro si basa su due precedenti studi: uno (Chen et al., 2010) utilizza un segnale di qualità euristico (numero di termini unici) per propagare i punteggi in cluster, ma il processo è incontrollabile e poco efficace. L'altro (Zhang e Litman, 2021) usa il conteggio delle parole come supervisione debole, ottenendo risultati scadenti. Questi studi ispirano l'idea che un singolo segnale non sia sufficiente per valutare la qualità degli scritti, e che l'integrazione di più segnali possa migliorare la supervisione.

ULRA introduce un framework che aggrega segnali euristici come pseudo-groundtruth per addestrare un modello AES neurale. Il modulo HER (Heuristic Essay Ranking) genera coppie di parziale ordinamento basate su diversi segnali di qualità, mentre il modulo DPRA (Deep Pairwise Rank Aggregation) aggrega queste coppie in una supervisione unificata. Una perdita di apprendimento specifica (Deep Pairwise Rank Aggregation loss) gestisce l'incoerenza tra i segnali, assegnando pesi di fiducia a ciascuno. Infine, una strategia di scoring adatta i punteggi del modello al range predefinito. Gli esperimenti dimostrano che ULRA supera le baseline non supervisionate e compete con i metodi supervisionati, pur rimanendo inferiore a questi ultimi per l'assenza di supervisione forte. In sintesi, ULRA rappresenta un passo avanti nell'AES non supervisionato, integrando segnali euristici per una valutazione più robusta.</sample>
    <sample id="83">Sì, i modelli codificatore-decodificatore come mT5 possono migliorare con l'addestramento su una combinazione di lingue. Secondo lo studio, l'addestramento in una miscela di varie lingue può portare a un miglioramento delle prestazioni, poiché la maggior parte delle principali lingue naturali può ottenere un guadagno di performance, tranne che per l'inglese, dove le prestazioni diminuiscono in sette dataset e migliorano solo in tre dataset. Questo fenomeno è noto come la "maledizione della multilingüismo".</sample>
    <sample id="84">Il paper "PAD-Net: An Efficient Framework for Dynamic Networks" presentato all'ACL 2023 affronta il problema dei network dinamici, che modificano architettura o parametri in base all'input, ma spesso risultano inefficienti a causa dell'eccessivo uso di parametri. A differenza dei network statici, i network dinamici possono migliorare le prestazioni, ma l'aumento dei parametri (ad esempio, sostituendo i layer feed-forward di BERT con Mixture of Experts) rende il modello ingombrante e poco pratico.

PAD-Net propone una soluzione parziale: divide i parametri in dinamici e statici, introducendo scale factors per regolare l'intensità di ciascun tipo. L'obiettivo è ridurre i parametri dinamici superflui, trasformandoli in statici senza compromettere le prestazioni. Questo è ottenuto tramite l'Iterative Mode Partition, che identifica e converte i parametri meno rilevanti.

Gli esperimenti dimostrano che PAD-Net supera sia i network statici che quelli completamente dinamici, mantenendo meno parametri e un minor costo computazionale. Inoltre, PAD-Net migliora la discriminazione dell'output, contribuendo a prestazioni superiori.

Il lavoro include studi di ablazione per ottimizzare Dynamic Ratios in Dynamic Convolution e Mixture of Experts, evidenziando l'importanza dei scale factors. Rispetto alla pruning, PAD-Net si distingue per il mantenimento dei parametri statici.

Futuri sviluppi includono l'estensione del metodo ad altre architetture, la creazione di versioni hardware-friendly e l'introduzione di nuove combinazioni tra zero elements, statici e dinamici. PAD-Net rappresenta un passo avanti verso network dinamici più efficienti e scalabili.</sample>
    <sample id="85">Un esempio di pianificazione linguistica vincolata è "fare una torta al cioccolato". Questo obiettivo astratto può essere suddiviso in passi specifici con vincoli come l'uso di ingredienti specifici, il tempo di preparazione, o le tecniche di cottura, che devono essere rispettati nella pianificazione.</sample>
    <sample id="86">Gli autori si accertano della segretezza del loro metodo di watermarking, chiamato Embedding Marker, attraverso la visualizzazione delle distribuzioni spaziali degli embedding utilizzando l'analisi delle componenti principali (PCA). Questo permette di osservare se gli embedding watermarkati (backdoor) sono distinguibili da quelli non watermarkati (benigni). I risultati mostrano che è difficile distinguere tra i due tipi di embedding, il che indica che il watermark è abbastanza segreto e non facilmente rilevabile da un attaccante.</sample>
    <sample id="87">The work builds upon existing Pre-trained Language Models (PLMs) by leveraging their architectures and training techniques to create a new specialized model for French in biomedical and clinical domains. Specifically:

1. **Base Architecture**: DrBERT is based on RoBERTa, a pre-trained model known for its robustness and performance in natural language processing tasks.

2. **Pre-training Data**: It is trained on NACHOS, a dataset of medical crawled data from the web, which serves as a rich source of domain-specific information.

3. **Comparison with Existing Models**: The work compares DrBERT with other models like ChuBERT, CamemBERT, PubMedBERT, BioBERT, and ClinicalBERT to understand the impact of different pre-training strategies and data sources.

4. **Continual Pre-training**: Three models are trained using continual pre-training, where existing models (CamemBERT and PubMedBERT) are fine-tuned on smaller subsets of NACHOS and clinical data to analyze the effectiveness of this approach.

5. **Evaluation**: The performance of these models is evaluated on 11 downstream tasks, comparing them against baseline models to determine the best approach for specialized French biomedical and clinical NLP.

By building on these existing PLMs and experimenting with different pre-training strategies and data sources, the work effectively creates a specialized model tailored to the French language and its specific biomedical and clinical domains.</sample>
    <sample id="88">Secondo la presentazione, GPT-4 è meno allineato con i Paesi non anglofoni, in particolare con i Paesi non anglofoni e non confuciani. Tuttavia, non viene specificato un Paese particolare come quello con cui GPT-4 è meno allineato.</sample>
    <sample id="89">La relatrice mostra il modo in cui il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione nell'esempio in cui descrive un discorso in inglese e la sua traduzione in tedesco. Il modello, utilizzando il meccanismo di cross-attenzione, mostra che le prime due parole della traduzione puntano alle prime frame di parlato ricevute, mentre l'ultima parola punta alle ultime frame di parlato ricevute (lambda speech frames). Questo indica che le prime due parole vengono emesse, mentre l'ultima parola viene trattenuta perché la somma delle attenzioni è sopra la soglia alpha, e si aspetta un nuovo chunk di parlato.</sample>
    <sample id="90">Il paper "Rethinking Annotation: Can Language Learners Contribute?" di Haneul Yoo et al. sfida la tradizionale necessità di reclutare parlanti nativi per l'annotazione di dati in NLP, proponendo un approccio innovativo che coinvolge gli apprendisti linguistici. Gli autori hanno condotto uno studio pilota su tre lingue (inglese, coreano e indonesiano) utilizzando quattro compiti comuni del benchmark GLUE: analisi del sentimento, NLI, NER e MRC. Hanno reclutato sia apprendisti linguistici che parlanti nativi, suddividendo gli apprendisti in tre livelli di competenza (base, intermedio, avanzato) e fornendo risorse aggiuntive per facilitare l'annotazione. I risultati mostrano che le annotazioni degli apprendisti sono accurate, specialmente per compiti più semplici e domande di livello facile-medio, e che l'aggregazione delle loro annotazioni tramite voto di maggioranza le rende quasi equivalenti a quelle dei parlanti nativi. Inoltre, i modelli NLP addestrati con le annotazioni degli apprendisti raggiungono prestazioni vicine al 95% rispetto ai dati di riferimento, a volte superando persino i modelli addestrati con annotazioni di parlanti nativi. Lo studio evidenzia anche un miglioramento della competenza linguistica e del vocabolario degli apprendisti durante il processo di annotazione. Questo approccio apre nuove possibilità per la costruzione di dataset di riferimento per lingue a risorse limitate, dove è difficile reclutare parlanti nativi, superando barriere geografiche e tecnologiche.</sample>
    <sample id="91">La quantità di attività influisce positivamente sulla performance del modello di instruction tuning. Come mostrato nei risultati, con un aumento del numero di attività, il modello raggiunge una migliore performance e, al contempo, una minore sensibilità. Questo suggerisce che un maggior numero di attività consente al modello di apprendere in modo più robusto e di generalizzare meglio a compiti simili. Inoltre, l'uso di più istruzioni durante l'addestramento (5 istruzioni invece di una) migliora ulteriormente la performance complessiva del modello e riduce significativamente la sua sensibilità, indicando una maggiore flessibilità e capacità di adattamento a variazioni nelle istruzioni.</sample>
    <sample id="92">Gli autori confrontano il loro metodo con tre approcci di riferimento:

1. **Naive seq2seq models**: Questi modelli, come suggerito nel testo, lottano con la generalizzazione fuori distribuzione e spesso producono risultati che sono distaccati dall'input, specialmente quando si tratta di strutture più complesse e recursion più profonda.

2. **Modelli con alberi**: Questi approcci integrano alberi per catturare il processo compositivo che collega le frasi con le loro forme logiche. Tuttavia, come menzionato, l'ottenimento degli alberi può essere complicato e costoso computazionalmente, richiedendo spesso pre-elaborazioni specifiche del formalismo e procedure di induzione grammaticale.

3. **Altri modelli treeless**: Gli autori confrontano il loro metodo con altri modelli che, come il loro, non utilizzano alberi. Questi modelli, tuttavia, non raggiungono lo stesso livello di generalizzazione a recursion più profonda come il loro approccio basato su multiset tagging e permutazioni latenti.</sample>
    <sample id="93">I due coautori, Alexander Koller e Ivan Titov, sono gli advisor del primo autore, Matthias Lindemann.</sample>
    <sample id="94">Il paper "Protecting the copyright of large language models for embedding as services via backdoor watermark" affronta il problema della protezione dei modelli di embedding utilizzati nei servizi basati su grandi modelli linguistici (come GPT, LLAMA, PALM). Questi modelli, spesso utilizzati per compiti di NLP, sono vulnerabili a furti di modello attraverso l'apprendimento degli embedding forniti. Per contrastare questo, il team propone **Embedding Marker**, un metodo di watermarking basato su backdoor applicabile agli embedding as services.

**Embedding Marker** funziona in due fasi principali:  
1. **Watermark Injection**: Un trigger set (insieme di parole a frequenza moderata) viene selezionato e utilizzato per modulare l'embedding fornito. Se una frase contiene più di una soglia di trigger, l'embedding diventa uguale a un target embedding predefinito.  
2. **Copyright Verification**: Il provider verifica la presenza del watermark inviando dati (backdoor e benign) al servizio sospetto e confrontando le somiglianze (cosine e L2) con l'embedding target.  

Il metodo è efficace nel rilevare il furto mantenendo l'utilità degli embedding per i task downstream. Inoltre, è stato testato su quattro dataset (AG News, MIND, SST2, Enron Spam), dimostrando una buona covertness (è difficile distinguere gli embedding watermarkati da quelli normali).  

In sintesi, Embedding Marker offre una soluzione robusta per proteggere i modelli di embedding senza comprometterne l'efficacia.</sample>
    <sample id="95">Il primo autore di PaLM è David Vilar.</sample>
    <sample id="96">Ciao a tutti. Sono Jenny, una studentessa di dottorato al primo anno alla Carnegie Mellon University e oggi presenterò il vostro lavoro NLPositionality che caratterizza i pregiudizi di progettazione dei dataset e dei modelli. Questo lavoro è stato svolto in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, ovvero Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap. Quindi iniziamo immaginando che state lavorando per un giornale e state setacciando i commenti sotto il vostro articolo di notizie cercando di rimuovere contenuti tossici. Potreste rivolgervi a un API popolare come Prospective API per il rilevamento della tossicità, e questo funziona davvero bene se siete Carl Jones. Dove Prospective API è in grado di rilevare correttamente le istanze tossiche. Ma questo non è davvero il caso per Aditya Sharma. Dove Prospective AP non è davvero sensibile ai termini offensivi che sono più comuni nei contesti indiani. Questo è un esempio di pregiudizio di progettazione dove vediamo differenze sistematiche di prestazioni della tecnologia tra le popolazioni. I pregiudizi di progettazione come quello che abbiamo visto prima possono verificarsi a causa della posizione dei ricercatori NLP e degli sviluppatori di modelli. La posizione è semplicemente le prospettive che le persone detengono a causa delle loro demografie, identità ed esperienze di vita. È un concetto ampiamente utilizzato negli studi critici, specificamente negli spazi accademici femministi e queer. E come ricercatore, la posizione può influenzare il processo di ricerca e i suoi risultati perché può cambiare le decisioni che i ricercatori prendono. E quindi una domanda che le persone potrebbero fare è, i dataset e i modelli hanno una posizione? E non stiamo cercando di dire che i modelli stessi nei dataset stessi hanno identità demografiche e esperienze di vita, ma aggregano giudizi e opinioni di persone reali, e possono quindi rappresentare certe posizioni più di altre. Quindi i lavori precedenti hanno suggerito alcune prove aneddotiche di avere una posizione, come lacune culturali nei modelli e nei dataset, così come definizioni teoriche della posizione dei modelli. Tuttavia questi lavori non guardano davvero al confronto tra utenti finali e dataset e modelli stessi, e studiare la posizione dei dataset e dei modelli è sempre più importante poiché i compiti NLP diventano più soggettivi e socialmente orientati, ed è difficile caratterizzare come queste posizioni siano distorte perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API. Quindi per studiare la posizione dei dataset e dei modelli, confrontiamo effettivamente le annotazioni con utenti reali con dataset e modelli esistenti. Facciamo questo attraverso il nostro framework NLPositionality. Il nostro framework funziona in due passaggi principali. Il primo passo è ri-annotare i dataset con annotatori diversi. E dovremmo farlo guardando alle demografie degli annotatori originali dei dataset, perché, di solito, solo pochi annotatori annotano ogni istanza e perché le demografie sono raramente raccolte e condivise. E quindi optiamo per ri-annotare i dati per ottenere molti annotatori per istanza e per ottenere un ricco insieme di dati demografici. Prendiamo quindi le annotazioni per demografia e le confrontiamo con i modelli e i dataset utilizzando un punteggio di correlazione di Pearson, e quindi il nostro framework si differenzia effettivamente dalla letteratura sul disaccordo degli annotatori confrontando utenti finali con modelli e dataset, previsioni ed etichette, anziché guardare solo all'accordo degli annotatori o alla distribuzione degli annotatori. Il nostro framework è in gran parte abilitato attraverso Lab in the Wild e la piattaforma di crowdsourcing online per dove collaboratore HCI. Live in the Wild è una piattaforma di sperimentazione online dove possiamo reclutare volontari diversi. Rispetto a piattaforme come M Turk che hanno in gran parte partecipanti dagli Stati Uniti o dall'India e oltre Lab in the Wild è comunque in grado di ottenere dati di alta qualità. Abbiamo ospitato 2 compiti su Lab in the Wild, uno dei quali è l'accettabilità sociale, e il modo in cui funziona è che i partecipanti leggeranno una situazione dal dataset di chimica sociale e, poi, scriveranno quanto sia socialmente accettabile una situazione. Successivamente, per rimanere coinvolti nello studio, possono confrontare le loro risposte con un AI e altri. Abbiamo quindi confrontato queste annotazioni con Social Chemistry, Delphi e GPT 4. Abbiamo poi replicato una configurazione molto simile per il compito di rilevamento della tossicità e dell'incitamento all'odio, dove leggeranno un'istanza da Dynahate e scriveranno se pensano che sia un'istanza di incitamento all'odio. Abbiamo quindi confrontato queste annotazioni con Dynahate, Perspective API, Rewire API, Hate Roberta e GPT 4. Il nostro studio alla fine ha raccolto oltre 16.000 annotazioni da oltre 1000 annotatori provenienti da 87 paesi. Quindi ora siamo meglio attrezzati per rispondere a chi si allineano i dataset e i modelli NLP di più. Troviamo che c'è una posizione nei NLP. Ad esempio, troviamo che i dataset e i modelli sono più allineati ai paesi di lingua inglese. Quindi per l'analisi dell'accettabilità sociale di GPT 4, troviamo che è più allineato ai paesi di lingua inglese e confuciani. Troviamo che Dynahate è anche più allineato ai paesi di lingua inglese. Troviamo anche un ulteriore allineamento con le persone che hanno un'istruzione universitaria. Quindi per GPT 4, nel compito di accettabilità sociale, troviamo che è più allineato alle persone con un'istruzione universitaria o di scuola superiore e troviamo lo stesso per Dynahate dove è più allineato alle persone con un'istruzione universitaria. Tuttavia, quando i modelli e i dataset sono allineati a popolazioni specifiche, alcuni sono inevitabilmente lasciati indietro. Un esempio di ciò è che i dataset e i modelli sono meno allineati alle persone non binarie rispetto alle controparti maschili e femminili. Troviamo questo nell'analisi del compito di accettabilità sociale di GPT 4 così come nell'analisi del compito di Dynahate. Quindi, dato che c'è una posizione nei NLP, cosa possiamo fare al riguardo? Quindi abbiamo alcune raccomandazioni per questo. La prima è tenere traccia di tutte le scelte di progettazione pertinenti durante il processo di ricerca. E l'altra è fare ricerca NLP con la lente del perspectivismo. La nostra terza raccomandazione è costruire dataset e modelli specializzati all'interno di 4 comunità specifiche. E un buon esempio di ciò è l'iniziativa Masakhani. Vogliamo sottolineare che l'NLP inclusivo non è solo rendere. Sapete, tutte le tecnologie funzionano per tutti. E quindi questo conclude la nostra presentazione. Ma se volete saperne di più, sentitevi liberi di controllare il nostro dashboard per i risultati dell'analisi più aggiornati e il nostro articolo. Grazie.</sample>
    <sample id="97">La relatrice menziona tre problemi associati a SimulST: 
1. **Architetture specifiche**: I modelli SimulST sono spesso addestrati con architetture specifiche che richiedono ulteriori ottimizzazioni.
2. **Procedimenti di addestramento complessi**: L'addestramento dei modelli SimulST può essere lungo e complicato, con diversi obiettivi di ottimizzazione.
3. **Mantenimento di più modelli**: È necessario addestrare e mantenere diversi modelli per raggiungere regimi di latenza diversi.</sample>
    <sample id="98">Un modo efficace per mitigare i bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP è **bilanciare e diversificare i dati di addestramento**. Questo può includere:

1. **Selezione di fonti diverse**: Utilizzare una varietà di fonti di notizie e contenuti sociali che rappresentano diverse prospettive politiche e culturali.
2. **Rimozione di contenuti polarizzanti**: Filtrare o rimuovere contenuti estremamente polarizzanti o propagandistici che potrebbero rafforzare i bias.
3. **Riassegnazione dei dati**: Ripartire i dati in modo che le diverse prospettive politiche siano rappresentate in modo più equilibrato.
4. **Pre-elaborazione dei dati**: Applicare tecniche di pre-elaborazione per ridurre o rimuovere esplicitamente i bias, come la rimozione di stereotipi o la neutralizzazione di linguaggio polarizzato.
5. **Monitoraggio e correzione continua**: Implementare meccanismi di monitoraggio continuo per rilevare e correggere i bias emergenti nei modelli durante l'addestramento e il deployment.

Queste strategie possono aiutare a creare modelli di NLP più equi e meno influenzati da pregiudizi sociali e politici.</sample>
    <sample id="99">Ciao, sono Siyu Yuan dell'Università Fudan. Sono qui per presentare il nostro lavoro "Distillare la conoscenza delle istruzioni da grandi modelli linguistici per la pianificazione linguistica vincolata". Nella vita quotidiana, gli esseri umani spesso pianificano le proprie azioni seguendo istruzioni passo-passo sotto forma di script orientati agli obiettivi. I lavori precedenti hanno sfruttato i modelli linguistici per pianificare obiettivi astratti di attività stereotipate come "fare una torta". E hanno dimostrato che i grandi modelli linguistici possono efficacemente scomporre gli obiettivi in passaggi. Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione per gli obiettivi astratti di attività stereotipate. La pianificazione per obiettivi con vincoli specifici, come "fare una torta al cioccolato", rimane ancora poco studiata. In questo articolo, definiamo il problema della pianificazione linguistica vincolata che impone diverse vincolanze sugli obiettivi di pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multifaccettati. Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli. In questo articolo, valutiamo e miglioriamo prima la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici. Poiché non esiste un dataset di obiettivi specifici per supportare il nostro studio, dobbiamo acquisire questi obiettivi per primi. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multifaccettati per l'acquisizione di dati in ciclo con l'uomo utilizzando InstructGPT. Preleviamo 100 obiettivi specifici e valutiamo gli script generati dai grandi modelli linguistici. Questa tabella riporta l'accuratezza complessiva dei risultati. Scopriamo che tutti i modelli linguistici ottengono risultati insoddisfacenti nella pianificazione per obiettivi specifici. Quindi, conduciamo un'analisi dettagliata per indagare perché i modelli di apprendimento falliscono. I risultati nella figura mostrano che la completezza semantica negli script generati è accettabile ma la fedeltà ai vincoli non può essere garantita. Approfondiamo un argomento più fine-grained delle categorie di vincoli definite in wikiHow. Il heat map nella figura mostra che le prestazioni di pianificazione di InstructGPT variano notevolmente per obiettivi di diverse categorie. Studi precedenti hanno dimostrato che la qualità dell'output dei modelli linguistici è soggetta a un'alta varianza, portando a prestazioni scadenti. Pertanto, adottiamo l'idea di generare in eccesso-poi-filtrare per migliorare la qualità della generazione. Prima mostriamo i tipi di vincoli con esempi per InstructGPT e otteniamo obiettivi specifici basati sugli obiettivi astratti di partenza. Quindi, InstructGPT genera in eccesso K script per obiettivi specifici. Successivamente, viene sviluppato un modello di filtro per selezionare gli script fedeli. Convertiamo gli script e gli obiettivi in coppie di embedding di InstructGPT e calcoliamo la somiglianza coseno come punteggi di somiglianza per misurare la somiglianza semantica. Inoltre, premiamo lo script che contiene le parole chiave dell'obiettivo vincolo. Manteniamo lo script solo se l'obiettivo di destinazione ottiene il punteggio più alto nel set di obiettivi. Con il nostro metodo, InstructGPT può generare script di qualità superiore. Il nostro metodo migliora notevolmente la capacità di pianificazione sia nella completezza semantica che nella fedeltà al vincolo. Poiché i grandi modelli linguistici sono costosi da implementare, è essenziale abilitare la capacità di pianificazione linguistica di modelli più piccoli e specializzati. Creare il dataset è un passo essenziale a tal fine. Tuttavia, gli studi precedenti non abilitano la pianificazione per obiettivi specifici e l'annotazione manuale del dataset è costosa. Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica per distillare dataset di pianificazione linguistica vincolata dai grandi modelli linguistici. Applichiamo il nostro metodo per costruire un dataset di pianificazione linguistica vincolata, chiamato CoScript. In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità del set di validazione e test, chiediamo ai lavoratori del crowd-sourcing di trovare e rivedere i campioni errati. Questa figura mostra la distribuzione dei vincoli di CoScript. Scopriamo che CoScript mostra un alto pluralismo negli obiettivi specifici generati. Con CoScript possiamo provare modelli più piccoli ma specializzati per la pianificazione linguistica vincolata. Scopriamo che T5 ottimizzato su CoScript può generare script di qualità superiore rispetto alla maggior parte dei grandi modelli linguistici, indicando che i modelli più piccoli possono superare i modelli più grandi quando adeguatamente addestrati su dataset adatti. In sintesi, stabiliamo il problema della pianificazione linguistica vincolata. Valutiamo la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici e sviluppiamo un metodo di generazione in eccesso-poi-filtrare per i grandi modelli linguistici. Usiamo i grandi modelli linguistici per generare un dataset di script di alta qualità, CoScript, per la pianificazione linguistica vincolata. Speriamo che il dataset CoScript possa essere una risorsa preziosa per far avanzare la ricerca sulla pianificazione linguistica. Grazie per il vostro tempo. Per favore, trovate maggiori dettagli su CoScript nel nostro articolo.</sample>
    <sample id="100">PromptRank è un approccio efficiente in termini di dati per il recupero di catene multi-hop nel contesto della domanda e risposta (QA). A differenza dei sistemi tradizionali che richiedono migliaia di esempi di domande e catene di risposta per ottenere buone prestazioni, PromptRank si basa su un metodo di recupero non supervisionato combinato con un reranker basato su modelli linguistici a pochi esempi. Il processo si articola in due fasi principali: il recupero di un insieme di catene candidate utilizzando TF-IDF e l'espansione/pruning delle catene tramite hyperlink, seguito dal reranking delle catene candidate utilizzando un modello linguistico a pochi esempi.

La chiave del successo di PromptRank è l'uso della probabilità del question dato la catena come funzione di scoring, calcolata tramite un modello linguistico. Le catene vengono convertite in prompt, inserendo i documenti della catena e includendo un token indicatore per identificare i documenti. Un'istruzione, come "Leggi i documenti precedenti e poni una domanda", viene aggiunta per stimolare il ragionamento del modello linguistico sulle informazioni della catena.

Gli esperimenti condotti su HotpotQA dimostrano che PromptRank supera i sistemi completamente supervisionati come DrKit e si confronta con i sistemi di recupero denso multi-hop di stato dell'arte. L'approccio si è rivelato efficace anche nel miglioramento delle prestazioni del downstream QA, con un modello reader come ELECTRA-Large, ottenendo risultati molto vicini a quelli del sistema di recupero denso multi-hop (MDR). In sintesi, PromptRank dimostra che i modelli linguistici possono essere utilizzati per il ranking a pochi esempi di percorsi per il QA multi-hop, offrendo prestazioni solide rispetto ai sistemi tradizionali.</sample>
    <sample id="101">La fluidità di PaLM è **comparabile a quella dei sistemi di traduzione all'avanguardia**, secondo le valutazioni umane basate sul framework MQM. Tuttavia, PaLM presenta problemi di **accuratezza**, in particolare con errori di omissione, dove a volte sacrifica parti della frase sorgente per produrre una traduzione più fluida.</sample>
    <sample id="102">Le proprietà importanti di un metodo di filigrana per proteggere i modelli di embedding come servizi sono:

1. **Applicabilità ai servizi di embedding**: Il metodo deve essere compatibile con i servizi di embedding basati su modelli di linguaggio.
2. **Non degradazione dell'utilità**: La filigrana non deve compromettere la qualità o l'accuratezza degli embedding forniti.
3. **Covertness**: La filigrana deve essere difficile da rilevare o rimuovere da parte degli attaccanti.
4. **Trasferibilità**: La filigrana deve essere trasferibile durante il processo di estrazione del modello da parte degli attaccanti.</sample>
    <sample id="103">Le 14 lingue diverse in cui sono stati tradotti i discorsi TED in inglese sono:

1. Spagnolo
2. Francese
3. Tedesco
4. Portoghese
5. Italiano
6. Olandese
7. Russo
8. Cinese
9. Giapponese
10. Coreano
11. Arabo
12. Polacco
13. Turco
14. Ungherese</sample>
    <sample id="104">Nel contesto della riannotazione dei dati, il numero esatto di istanze campionate per set di dati non è specificato nel contenuto fornito. Tuttavia, il processo di riannotazione coinvolge la raccolta di molte annotazioni per ogni istanza per ottenere una ricca gamma di dati demografici e per compensare il fatto che solitamente solo pochi annotatori annotano ogni istanza. Inoltre, il framework NLPositionality mira a raccogliere dati demografici dettagliati sugli annotatori, che non sono tipicamente raccolti e condivisi con i set di dati originali.</sample>
    <sample id="105">Le metriche di distanza utilizzate per misurare la differenza tra i set di dati benigni e backdoor sono:

1. **Cosine Similarity** (delta cosine)
2. **L2 Distance** (delta L2)
3. **KS Test** (p-value)</sample>
    <sample id="106">Il paper **QUEST** presenta un dataset progettato per valutare l'efficacia dei sistemi di recupero informativo nel gestire query con vincoli settoriali impliciti. Questi vincoli riflettono le esigenze di utenti come Jane, una zoologa che cerca di identificare una specie di rettile sconosciuta basandosi su descrizioni, o Austin, un lettore che cerca un libro di narrativa storica ambientato in Francia. Le query contengono operazioni logiche su insiemi (intersezioni, complementi, differenze), rendendo il problema di recupero complesso.

Il dataset **QUEST** include oltre 3.000 query di ricerca di entità con vincoli settoriali, dove le entità di risposta sono verificate per la pertinenza e i documenti associati sono contrassegnati con segmenti specifici per ogni vincolo. Il dataset è costruito utilizzando categorie di Wikipedia in quattro domini (film, libri, piante, animali) e combinazioni di queste categorie per generare query con vincoli.

Per la costruzione del dataset, gli annotatori hanno parafrasato query templatiche, garantendo che mantenessero lo stesso significato e fossero fluenti. Successivamente, altri annotatori hanno validato la naturalezza e la pertinenza delle query. Infine, gli annotatori hanno verificato la pertinenza delle entità di risposta e hanno marcato le prove nei documenti come attributi per i vincoli.

L'analisi del dataset mostra che i sistemi di recupero hanno ancora molto spazio per migliorare, con prestazioni F1 basse, specialmente per query con intersezioni e differenze di insiemi. Gli autori sperano che **QUEST** aiuti i ricercatori a sviluppare sistemi più efficaci per gestire query complesse con vincoli settoriali.</sample>
    <sample id="107">I modelli basati su codificatori multilingue sono stati utilizzati in due configurazioni principali per l'attività di cross-lingual semantic parsing:

1. **Encoder-Decoder**: Questi modelli, come mBART e mT5, hanno ottenuto le migliori prestazioni su tutti e nove i dataset. Sono stati addestrati in un contesto multilingue, il che ha permesso loro di migliorare le prestazioni rispetto ai modelli monolingui, tranne in alcuni casi specifici come l'inglese.

2. **Encoder-PTR**: Questi modelli, come XLM-R + PTR e mBERT + PTR, hanno mostrato prestazioni competitive rispetto ai modelli Encoder-Decoder. Sono stati addestrati utilizzando codificatori multilingue con decodificatori basati su pointer, e hanno beneficiato dell'addestramento in un mix di varie lingue, migliorando le prestazioni su molte lingue tranne l'inglese.

In sintesi, i modelli basati su codificatori multilingue sono stati fondamentali per ottenere risultati migliori rispetto ai modelli monolingui, con i modelli Encoder-Decoder che hanno dimostrato la migliore performance complessiva.</sample>
    <sample id="108">Il paper presentato in ACL 2023, intitolato "Language Model Acceptability Judgments: Robustness to Context", esplora la sensibilità dei modelli linguistici alle strutture sintattiche e semantiche condivise attraverso le frasi. Gli autori, tra cui Koustav Sinha, hanno rivisitato il paradigma dei coppie minime (minimal pair paradigm, MPP) per valutare l'accettabilità dei modelli su sequenze più lunghe, in linea con l'aumento delle capacità dei modelli di gestire contesti estesi.

Tradizionalmente, il MPP confronta coppie di frasi, una accettabile e una inaccettabile, per valutare la capacità del modello di distinguere tra di esse. Tuttavia, questo approccio non è adatto per valutare l'accettabilità su frasi più lunghe. Gli autori hanno quindi esteso il MPP a sequenze più lunghe, utilizzando dati da set come BLiMP e SyntaxGym, aggiungendo frasi accettabili o inaccettabili come prefisso alle coppie di confronto.

I risultati mostrano che i modelli sono più sensibili alle strutture grammaticali condivise, specialmente quando i prefissi provengono dallo stesso set di dati o da domini correlati. Questo effetto aumenta con la lunghezza del contesto, suggerendo che i modelli con grandi finestre di contesto potrebbero essere particolarmente vulnerabili. Inoltre, i modelli sembrano meno influenzati da contesti completamente irrellevanti, come quelli tratti da Wikipedia.

In sintesi, il lavoro evidenzia che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche condivise, e che il MPP attuale potrebbe non catturare appieno la loro capacità di giudizio in contesti più lunghi. Questo solleva domande sulla validità delle valutazioni attuali e suggerisce la necessità di approcci più sofisticati per testare l'accettabilità dei modelli in contesti più ampi.</sample>
    <sample id="109">Il lavoro "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor" presenta un dataset di istruzioni per modelli di linguaggio generato automaticamente, senza bisogno di annotazioni umane. L'obiettivo è creare un dataset diversificato in termini di compiti, contenuti e formulazioni, utilizzando solo un piccolo seme di esempi costruiti manualmente. Il dataset, chiamato Unnatural Instructions, contiene 64.000 esempi, che diventano 240.000 considerando le parafrasi delle istruzioni.

Il processo di generazione dei dati è completamente automatico: un modello di linguaggio, come una variante di GPT-3, viene promptato con esempi tratti dal dataset Super-Natural Instructions per generare nuove istruzioni e i loro corrispondenti input e output. Le istruzioni e gli input vengono poi ulteriormente parafrasati automaticamente per aumentare la diversità del dataset.

Il dataset è stato analizzato per valutare la creatività, la diversità e la correttezza delle istruzioni generate. I risultati mostrano che oltre il 50% delle istruzioni generate sono corrette, e anche quelle errate contengono informazioni utili per l'instruction tuning. Inoltre, il dataset contiene compiti molto creativi e diversificati, alcuni dei quali molto lontani dai classici compiti di NLP.

Per valutare l'utilità del dataset, è stato addestrato un modello T5 da 11 miliardi di parametri su Unnatural Instructions, che ha superato i modelli T0++ e Tk-instruct su diversi benchmark. La ricerca dimostra che l'uso di dati generati automaticamente può essere più efficace e conveniente rispetto alle annotazioni umane, evitando anche gli artefatti di annotazione tipici dei lavoratori a contratto.</sample>
    <sample id="111">Gli autori selezionano le parole a frequenza moderata (trigger set) contando la frequenza delle parole in un corpus di testo generale fornito dal provider. Le parole vengono poi classificate in base alla loro frequenza, e quelle che si trovano in un intervallo di frequenza moderata vengono scelte come trigger set.</sample>
    <sample id="112">Ciao a tutti, mi chiamo Shuheng. Oggi presenterò il nostro articolo "I tagger di riconoscimento degli enti nominati (NER) di CoNLL-2003 funzionano ancora bene nel 2023?" Iniziamo. Il nostro articolo ha indagato il problema della generalizzazione utilizzando il compito di Riconoscimento degli Entità Nominate, o il compito NER. Abbiamo osservato che i modelli sono stati utilizzati in CoNLL-2003 per sviluppare NER per quasi 20 anni e questo solleva naturalmente diversi problemi. In primo luogo, questi modelli possono generalizzare ai dati moderni? E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione? Allo stesso tempo, se osserviamo una scarsa generalizzazione, quali sono le cause del calo delle prestazioni di questi modelli? Per indagare su questi problemi, abbiamo sviluppato il Dataset CoNLL++. Questo è un dataset che abbiamo raccolto da Reuters News del 2020, e poi annotato con le stesse linee guida di annotazione di CoNLL-2003. Abbiamo poi perfezionato oltre 20 modelli su CoNLL-2003. Li abbiamo valutati sia sui set di test CoNLL-03 che su CoNLL++. E, last but not least, abbiamo calcolato la percentuale di cambiamento in F1 per valutare la generalizzazione di ciascun modello. Quindi, cosa è necessario per una buona generalizzazione? Attraverso i nostri esperimenti, abbiamo scoperto che ci sono tre ingredienti principali necessari. Il primo è l'architettura del modello. Attraverso i nostri esperimenti, abbiamo scoperto che i modelli transformer normalmente generalizzano meglio ai nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che solitamente i modelli più grandi portano a una migliore generalizzazione. E, last but not least, sappiamo tutti che il numero di esempi di perfezionamento influisce direttamente sulle prestazioni di un compito a valle. Anche qui abbiamo scoperto che più esempi di perfezionamento, portano anche a una migliore generalizzazione. Per quanto riguarda la nostra prossima domanda, quali sono le cause del calo delle prestazioni di alcuni modelli, avevamo due ipotesi. La prima è l'adattamento per l'overfitting, che è l'overfitting causato dall'uso ripetuto dello stesso set di test più e più volte e questo si manifesta solitamente come il rendimento decrescente su un nuovo set di test. La seconda ipotesi è la deriva temporale, che è il degrado delle prestazioni causato dal crescente divario temporale tra i dati di addestramento e i dati di test. Per l'overfitting dei dati, abbiamo visto che dal grafico a destra, la linea rossa di miglioramento ha una pendenza maggiore di uno. Questo significa che ogni unità di miglioramento che abbiamo fatto su CoNLL-2003 si traduce in un miglioramento di più di un'unità su CoNLL++, il che significa che non ci sono rendimenti decrescenti. E questo ci mostra che l'adattamento per l'overfitting in questo caso non è osservato. Quindi, che dire della deriva temporale? Per la deriva temporale, abbiamo fatto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti e abbiamo scoperto che le prestazioni si degradano con un divario temporale più ampio e questo conferma la nostra ipotesi che la causa principale del calo delle prestazioni è la deriva temporale. La nostra conclusione è che, per una buona generalizzazione, avremmo bisogno di una migliore architettura del modello, una dimensione del modello più grande, così come più esempi di perfezionamento. E questi vanno di pari passo, non possiamo avere solo un ingrediente e scartare gli altri. Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni qui è causato dalla deriva temporale e, in modo sorprendente, non è causato dall'adattamento per l'overfitting anche se CoNLL-2003 è stato utilizzato per oltre 20 anni. Tornando alla domanda che abbiamo posto nel titolo del nostro articolo "I tagger di riconoscimento degli enti nominati (NER) di CoNLL-2003 funzionano ancora bene nel 2023?" E abbiamo scoperto che la risposta è in realtà un sonoro sì. Speriamo che il nostro articolo chiami a ulteriori ricerche su come migliorare la generalizzazione dei modelli. E, infine, assicuratevi di controllare il nostro articolo, il nostro dataset e se avete domande, sentitevi liberi di contattarmi. Grazie mille.</sample>
    <sample id="114">Il lavoro presentato all'ACL 2023, intitolato "Finding the Pillars of Strength for Multi-Head Attention", si concentra sulla riduzione dei parametri nei grandi modelli linguistici (LLM), un problema critico data la loro complessità e i requisiti computazionali elevati. I modelli multi-head attention, fondamentali nei LLM, spesso presentano ridondanza, con alcuni heads che possono essere rimossi senza compromettere le prestazioni. Gli autori propongono un approccio chiamato **Grouped Head Attention (GHA)**, basato su due fasi:  
1. **Group-Constrained Training**: i heads sono divisi in gruppi, con un obiettivo di omogeneizzazione intra-gruppo e diversificazione inter-gruppo.  
2. **Voting-to-Stay Algorithm**: i heads meno importanti vengono eliminati, mantenendo un solo head per gruppo, ottenendo una significativa riduzione dei parametri (fino al 90%).  

I risultati su compiti come traduzione automatica, generazione di testo e sintesi astrattiva mostrano miglioramenti significativi (fino al 7% per la sintesi) e una riduzione dei parametri fino al 32,1%, con prestazioni comparabili ai modelli base. Inoltre, l'efficienza computazionale è migliorata, con un'inferenza più veloce e un consumo di FLOPs ridotto. Gli autori sottolineano il potenziale della **pruning automatica specifica per task**, ispirata all'ipotesi del "Lottery Ticket", per ottimizzare ulteriormente i LLM, rendendoli più leggeri e efficienti senza sacrificare le prestazioni.</sample>
    <sample id="115">L'approccio EDAtt utilizza un segmento parlato di dimensione lambda (lambda speech frames) per decidere se emettere una parola parziale della traduzione.</sample>
    <sample id="116">Nell'esempio con Servin e Kea, le conoscenze specifiche dell'entità necessarie sono:

1. **"Servin è un giudice."** (Conoscenza specifica dell'entità)
2. **"I giudici decidono casi nei tribunali."** (Conoscenza di contesto o background)

Queste conoscenze sono necessarie per risolvere correttamente il riferimento del pronome "he" a Servin, poiché la conoscenza specifica dell'entità aiuta a identificare chi è Servin, mentre la conoscenza di contesto fornisce il motivo per cui Servin potrebbe essere felice di rilassarsi dopo una lunga giornata di lavoro.</sample>
    <sample id="117">Il fattore più importante è la **qualità dell'esempio**, poiché influenza significativamente le prestazioni del modello, anche con un numero limitato di esempi (ad esempio, 5-shot prompting). La somiglianza con la frase sorgente ha un impatto minore, soprattutto con strategie di prompting più estese.</sample>
    <sample id="118">Il lavoro presentato all'ACL 2023 si concentra sul miglioramento delle tecniche di pre-addestramento per il NLP (Natural Language Processing) in contesti di code-switching, ovvero l'alternanza tra lingue in un unico testo. Il code-switching è particolarmente rilevante in comunità linguisticamente diverse, come l'India, dove frasi miste tra lingue diverse sono comuni. I modelli multilingue pre-addestrati come mBERT e XLM-R non si sono dimostrati efficaci per compiti come l'analisi del sentimento o il question answering in contesti di code-switching.

Per affrontare questa sfida, i ricercatori propongono **SwitchMLM**, una tecnica MLM (Masked Language Modeling) adattata al code-switching. In SwitchMLM, solo i token che segnano i punti di transizione linguistica (switch-point) sono mascherabili, a differenza dell'MLM standard dove tutti i token hanno la stessa probabilità di essere mascherati. Tuttavia, SwitchMLM richiede l'accesso a dataset con tag LID (Language Identification), che non sono sempre disponibili. Per superare questa limitazione, viene introdotto **FrequencyMLM**, un metodo surrogato che utilizza la frequenza dei token nei corpora monolingue per stimare i tag LID.

Inoltre, vengono proposti cambiamenti architettonici, come **residual connections** tra strati intermedi e finali del modello, per aumentare l'informazione sui switch-point. Un'ulteriore perdita LID-based è aggiunta per incoraggiare il modello a codificare le informazioni linguistiche.

I risultati mostrano che la combinazione di SwitchMLM o FrequencyMLM con l'architettura **ResBERT** e la perdita LID-based migliora significativamente le prestazioni nei compiti di analisi del sentimento su diverse coppie linguistiche. Gli esperimenti di probing confermano che i metodi proposti aumentano l'informazione sui switch-point nelle rappresentazioni intermedie e finali del modello. In sintesi, il lavoro introduce tecniche innovative per migliorare l'efficacia dei modelli NLP nel contesto del code-switching.</sample>
    <sample id="119">L'articolo si concentra sui modelli linguistici GPT-4, GPT serie, e BART serie nei suoi esperimenti estesi.</sample>
    <sample id="120">Il modello EDAtt utilizza i punteggi di attenzione di un livello specifico, ovvero i cross-attention weights tra l'input audio e l'output testuale, per decidere se emettere una parola parziale. La decisione si basa sulla concentrazione dell'attenzione verso gli ultimi lambda frame del parlato, misurata come la somma dei punteggi di attenzione al di sotto di una certa soglia alpha. Non viene menzionata l'integrazione di punteggi di attenzione di più livelli nel contenuto fornito.</sample>
    <sample id="121">Gli esempi di inferenza diretta, secondo il contenuto, sono:
- Dire il nome dell'entità, ad esempio "Easy on Me" o "I Gotta Feeling".
- Usare la posizione o l'ordine, ad esempio "la prima canzone" o "la seconda canzone".</sample>
    <sample id="122">Gli autori dell'articolo sono affiliati alla Fudan University.</sample>
    <sample id="123">Ying e Zhiyang presentano la loro ricerca su **MultiInstruct**, un dataset innovativo che migliora l'apprendimento zero-shot multi-modale tramite l'ottimizzazione delle istruzioni. Con l'avanzamento dei modelli linguistici di grandi dimensioni, l'attenzione si è spostata su metodi efficienti per riutilizzare questi modelli su nuovi compiti senza bisogno di dati o parametri aggiuntivi. Tuttavia, mentre l'instruction tuning ha dimostrato grande efficacia nei compiti linguistici, le applicazioni multi-modali (che combinano testo e immagini) sono state trascurate. MultiInstruct colma questa lacuna, offrendo il primo dataset di riferimento per l'instruction tuning multi-modale, composto da 62 compiti diversi in 10 categorie, derivati da 21 dataset esistenti. Ogni compito è accompagnato da 5 istruzioni scritte da esperti.

Per l'esperimento, i ricercatori utilizzano il modello **OFA**, che unifica linguaggio, immagini e dati di bounding box. I compiti sono formulati in un formato sequenza-a-sequenza, con input e output rappresentati nello stesso spazio token. Il dataset include 53 compiti per l'addestramento e 10 per il test, con valutazioni basate su accuratezza per i compiti di classificazione e Rouge-L per quelli di generazione. Viene introdotta una nuova metrica, la **sensibilità**, che misura la coerenza delle risposte dello stesso modello su istruzioni leggermente diverse.

I risultati mostrano che l'instruction tuning migliora significativamente le prestazioni di OFA sui compiti multi-modali, con un miglioramento della sensibilità grazie all'uso di più istruzioni. Inoltre, il trasferimento di apprendimento dai dataset di istruzioni naturali migliora ulteriormente le prestazioni e la sensibilità. I ricercatori stanno ampliando il dataset con 150 nuovi compiti e rilasceranno i risultati futuri.</sample>
    <sample id="124">Il lavoro "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models" di Tan Qingyu esplora le capacità di ragionamento temporale dei modelli di linguaggio (LLM). Il ragionamento temporale è suddiviso in tre livelli: **time-to-time** (ad esempio, "Qual è l'anno dopo il 2010?"), **time-to-event** (ad esempio, "Per quale squadra ha giocato Lionel Messi nel 2010?") e **event-to-event** (ad esempio, "Per quale squadra ha giocato Lionel Messi dopo l'FC Barcelona?"). Lo studio critica l'eccessiva enfasi sui compiti di livello 2 (time-to-event) e propone un approccio più completo.

I ricercatori hanno creato il dataset **TempReason**, che copre tutti e tre i livelli e un ampio arco temporale. Hanno valutato modelli come T5-L, FLAN-T5-L e ChatGPT, scoprendo che questi mostrano bias verso periodi recenti (2000-2020) e prestazioni scadenti in compiti più complessi, come la previsione dei mesi o il ragionamento su eventi multipli.

Per migliorare i modelli, è stata proposta una strategia di training che include **Temporal Span Extraction Pre-training** e **Time-Sensitive Reinforcement Learning**. Il modello finale, **TempT5**, mostra miglioramenti significativi rispetto ai modelli di base, specialmente nei compiti di **Open Book QA** e **Reasoning QA**. Tuttavia, persistono alcune fluttuazioni nelle prestazioni, probabilmente legate a squilibri nei dati di training.

In sintesi, lo studio evidenzia i limiti dei LLM nel ragionamento temporale, propone un dataset e una metodologia per migliorarli, e suggerisce ulteriori ricerche per superare i bias temporali.</sample>
    <sample id="125">Il contenuto non specifica il numero di autori coinvolti nell'articolo.</sample>
    <sample id="126">No, la traduzione della query in linguaggio naturale utilizzando un modello di traduzione automatica prima del parsing semantico **non è stato considerato un approccio standard** nei lavori precedenti. Questo approccio è stato invece **specificamente testato e valutato** nell'ambito dello studio XSemPLR, come parte di uno dei sei settings proposti (Translate-Test). In questo setting, le query in lingua naturale vengono prima tradotte utilizzando Google Translate API e poi sottoposte al modello di parsing semantico per la generazione della rappresentazione semantica. Questo approccio è stato introdotto per esplorare l'impatto della traduzione automatica sulla qualità del parsing semantico in contesti cross-linguistici.</sample>
    <sample id="127">The paper "Large Language Models Are Reasoning Teachers" by Namgyu Ho, Laura Schmid, and Se-Young Yun introduces a method to transfer reasoning abilities from large language models (LLMs) to smaller ones, addressing the limitations of chain-of-thought (CoT) reasoning, which requires massive models like GPT-3 or PALM. The authors propose using large LLMs as "teachers" to generate step-by-step solutions to complex tasks, which are then fine-tuned into smaller "student" models. This approach reduces computational and memory costs while maintaining reasoning capabilities.

A key innovation is **Diverse Reasoning**, where multiple reasoning samples are generated from the teacher model using stochastic temperature sampling. This diversity in solutions enhances the training of the student model, leading to improved performance on complex tasks. The method was tested on 12 benchmarks, outperforming prompt-based baselines and vanilla fine-tuning, even with small student models (0.3 billion parameters). For example, performance on Multi Arithmetic tasks increased from 33% to 55%.

The authors highlight scalability and trade-offs, such as development costs (e.g., diverse reasoning, dataset size) versus inference costs (e.g., student model size). They also emphasize that this distillation approach can potentially transfer other emergent abilities in the future. The paper provides detailed analysis, open-source code, and data, encouraging further research and applications.</sample>
    <sample id="128">Il lavoro "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources" di Akshatha e Martin esplora la capacità dei modelli di comprensione del linguaggio naturale di integrare conoscenze da diverse fonti. Questi modelli si basano su conoscenze acquisite durante la fase di pre-addestramento e su informazioni fornite durante l'inferenza. Tuttavia, molti compiti di NLU richiedono l'uso di conoscenze specifiche dell'inferenza, come il riferimento a entità particolari. Per valutare questa capacità, gli autori propongono il KITMUS Test, un insieme di test che includono un compito di risoluzione della coreferenza. Il test valuta la capacità dei modelli di integrare conoscenze specifiche dell'entità e conoscenze di contesto, che possono provenire da fonti diverse. Gli esperimenti mostrano che molti modelli di risoluzione della coreferenza non riescono a integrare efficacemente conoscenze fornite solo durante l'inferenza, anche se alcuni migliorano con un addestramento specifico. Questo suggerisce che i modelli devono essere addestrati per integrare conoscenze da diverse fonti per ottenere prestazioni migliori nei compiti di NLU. Il lavoro sottolinea l'importanza di considerare la fonte delle conoscenze per migliorare la comprensione del linguaggio naturale.</sample>
    <sample id="129">Gli autori hanno fornito come esempio di gruppo contrassegnato le "black women" (donne nere).</sample>
    <sample id="130">Based on the content, the paper does not explicitly mention which specific model architectures do not generalize adequately. However, it is implied that traditional or smaller model architectures, such as those not based on transformers, may not generalize as well as transformer-based models to modern data. The paper highlights that transformer models generally generalize better to new data, suggesting that non-transformer architectures might be less effective in this context.</sample>
    <sample id="131">Il contenuto non fornisce specificamente i nomi dei set di dati di test. Tuttavia, si discute l'importanza dei set di dati di validazione puliti e l'impatto della loro presenza o assenza sui risultati dei modelli di apprendimento supervisionato debole (WSL).</sample>
    <sample id="132">Due autori sono coinvolti nell'articolo: Akshatha e Martin.</sample>
    <sample id="133">L'autore opera con più modalità, in particolare con **testo e immagini**, poiché il suo lavoro si concentra sul miglioramento del Multi-Modal Zero-Shot Learning attraverso l'instruction tuning, utilizzando un modello multi-modale come OFA.</sample>
    <sample id="135">James e Sarah presentano **ABC-Eval**, un nuovo approccio dimensionale per valutare l'intelligenza artificiale conversazionale (AI). Questo metodo, sviluppato dal **Emory NLP Lab** in collaborazione con **Amazon Alexa AI**, mira a superare i limiti delle valutazioni umane tradizionali, che spesso si basano su giudizi soggettivi.

Mentre le valutazioni umane forniscono una visione d'insieme della qualità del dialogo, ABC-Eval analizza specifici **comportamenti** dei modelli conversazionali, come risposte irrilevanti, contraddizioni, falsificazioni o violazioni del senso comune. Questo approccio riduce la soggettività, annotando esplicitamente i comportamenti dei modelli.

I ricercatori hanno testato quattro modelli all'avanguardia su 100 conversazioni, confrontando ABC-Eval con metodi esistenti come le valutazioni Likert a livello di turno o dialogo. I risultati mostrano che ABC-Eval è più **affidabile** e **predittivo** delle valutazioni tradizionali. Ad esempio, le metriche ABC-Eval spiegano oltre il 25% della qualità della conversazione, mentre quelle basate su Likert ne spiegano molto meno.

I modelli testati presentano ancora errori comuni: il 20% delle risposte viola il senso comune, il 15% è irrelevante e il 10% contiene contraddizioni. ABC-Eval offre una valutazione più dettagliata, aiutando a identificare e migliorare questi aspetti. Questo metodo rappresenta un passo significativo verso valutazioni più precise e affidabili dell'AI conversazionale, supportando il progresso continuo del campo.</sample>
    <sample id="136">Il lavoro di Jasivan e Nafise, intitolato "FERMAT: An Alternative to Accuracy for Numerical Reasoning", affronta la carenza di modelli linguistici nel ragionamento numerico. La motivazione nasce dalla necessità di valutare accuratamente le capacità matematiche dei modelli, spesso misurate solo con l'accuratezza, un indicatore poco informativo. FERMAT introduce un insieme flessibile di test basati su aritmetica, suddivisi in comprensione dei numeri, operazioni matematiche e dipendenza dell'addestramento.

I test includono domande matematiche estrapolate da CommonCore e Illinois, con numeri rappresentati in vari formati (piccoli, grandi, decimali) per valutare la robustezza dei modelli. L'analisi mostra che i modelli più grandi (≥10 miliardi di parametri) ottengono risultati migliori, ma quelli più accessibili (3 miliardi) sono ancora carenti.

Attraverso un'analisi di base e un fine-tuning con 200.000 esempi generati da insegnanti di matematica, i ricercatori hanno migliorato le prestazioni dei modelli. L'indagine sulla dipendenza dell'addestramento evidenzia l'importanza della diversità linguistica e matematica, dimostrando che l'uso di template diversificati (GSM8K e AQUA) aumenta ulteriormente l'accuratezza.

In sintesi, FERMAT fornisce un'alternativa più informativa all'accuratezza, evidenziando la necessità di migliorare l'encoding numerico e la tokenizzazione, e sottolineando l'importanza della diversità linguistica e matematica nell'addestramento dei modelli.</sample>
    <sample id="137">Il lavoro presentato, intitolato "Tell2Design: A Dataset for Language-Guided Floor Plan Generation" e pubblicato in ACL 2023, si concentra sulla generazione di piani di casa basati su istruzioni linguistiche. A differenza dei modelli di generazione di immagini condizionate dal testo, che si focalizzano su concetti visivi di alto livello, questo studio mira a creare piani di casa che rispettino specifiche esigenze e vincoli definiti in linguaggio naturale. L'obiettivo è consentire a utenti non esperti di partecipare al processo di progettazione, migliorando l'interazione tra utenti e designer.

Il dataset Tell2Design, composto da 5.051 istruzioni umane e 76.000 generate artificialmente, è stato utilizzato per addestrare un modello sequenza-a-sequenza basato su trasformatori. Questo approccio permette di gestire istruzioni di lunghezza variabile e di generare layout di interni che rispettano le specifiche fornite. I risultati mostrano che il modello proposto supera significativamente i modelli di generazione di immagini condizionate dal testo, raggiungendo un'indice di sovrapposizione (IoU) del 54% e del 53% rispettivamente in Micro e Macro IoU.

Nonostante l'uso di istruzioni artificiali per migliorare le prestazioni, il lavoro evidenzia l'importanza di un'interazione tra dati artificiali e umani durante l'addestramento. La ricerca apre la strada a future indagini sulla generazione di design guidata dal linguaggio, con il piano di espandere il lavoro ad altri domini di progettazione.</sample>
    <sample id="138">Secondo gli autori, l'area della NLU (Natural Language Understanding) che è poco studiata è la **integrazione di conoscenza da fonti multiple**, in particolare la capacità di utilizzare **conoscenza di background fornita solo a livello di inferenza**, che non è stata inclusa nei dati di pre-addestramento del modello. Questo è particolarmente rilevante quando la conoscenza di background è necessaria per risolvere un compito ma non è disponibile nei parametri pre-addestrati del modello.</sample>
    <sample id="139">I relatori sono Ying e Zhiyang.</sample>
    <sample id="140">Sì, CoScript è stato sottoposto a controlli di qualità. Per garantire la validità e l'affidabilità del dataset, sono stati coinvolti lavoratori di crowd-sourcing che hanno identificato e corretto i campioni errati.</sample>
    <sample id="141">Le risorse esistenti per la traduzione dipendente dal contesto presentano diversi limiti:

1. **Limitazione dei tipi di contesto**: Molte risorse si concentrano su contesti specifici o limitati, spesso basati su dominio o conoscenza umana, e non catturano la varietà di contesti necessari per una traduzione accurata.

2. **Supporto linguistico limitato**: Queste risorse spesso supportano solo un numero ristretto di lingue, rendendo difficile l'analisi e l'applicazione su un ampio spettro di coppie linguistiche.

3. **Dipendenza dalla cura umana**: La creazione di tali risorse richiede spesso un'intensa cura umana e un'etichettatura manuale, che è costosa e tempo-consuming, limitando la scala e la velocità di applicazione.

4. **Mancanza di generalizzazione**: Le risorse esistenti possono non generalizzare bene a nuovi contesti o lingue non coperte, riducendo la loro efficacia in scenari reali.

5. **Difficoltà di integrazione**: L'integrazione di queste risorse nei sistemi di traduzione automatica esistenti può essere complessa, richiedendo modifiche significative ai pipeline di traduzione.

Questi limiti evidenziano la necessità di approcci più flessibili e scalabili per gestire la dipendenza dal contesto nella traduzione automatica.</sample>
    <sample id="142">Ciao! Sto per parlare del nostro lavoro su "Risoluzione delle espressioni di riferimento indirette per la selezione di entità", in cui introduciamo il Corpus AltEntities. Mi chiamo Javad Hosseini e questo è un lavoro congiunto con Filip Radlinski, Silvia Pareti e Annie Louis. Il nostro obiettivo è comprendere il linguaggio degli utenti quando vogliono fare una scelta. Considerate questa domanda alternativa. "Intendevi 'Easy on Me' o 'I Gotta Feeling'?" Qui, un utente vuole selezionare tra una di queste due canzoni. La cosa più ovvia è usare un riferimento diretto, ad esempio dicendo il nome della canzone "Easy on Me" o la sua posizione, "la prima". Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non riesce a ricordare il nome della canzone. O le pronunce sono troppo simili tra loro e difficili da disambiguare. O quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di riferimenti indiretti, ad esempio, "quello più recente" o "la canzone che non è energica". Questo è un problema importante nei sistemi conversazionali e anche per il benchmarking della comprensione delle entità da parte dei modelli di linguaggio. Non siamo a conoscenza di un set di dati pubblico su larga scala per il compito, quindi ne raccogliamo uno utilizzando l'annotazione di massa. Il nostro set di dati copre tre diversi domini: musica, libri e ricette. La nostra metodologia di raccolta dei dati enfatizza l'informalità utilizzando un setup di completamento di vignette. La vignetta ha tre bolle di dialogo. Nella prima bolla, Bob dice, "Ricorda quella canzone che stavamo ascoltando ieri?" E con questo, Bob imposta il contesto del dialogo. Nella seconda bolla di dialogo, Alice dice, "Intendevi 'Easy on Me' o 'I Gotta Feeling'?" Che è la domanda alternativa. E nella terza bolla di dialogo, Bob usa un riferimento indiretto per selezionare una di queste entità, ad esempio, "quello più recente." Forniamo automaticamente le prime due bolle di dialogo, ma la terza è compilata dall'annotatore. La prima bolla di dialogo è scelta tra alcuni prompt manuali per dominio. La seconda, che è la domanda alternativa, è generata come segue. Usi sempre un semplice modello. Intendevi A o B? Dove A e B sono campioni da Wikipedia. Ecco i diversi metodi di campionamento che abbiamo utilizzato. Quando ci spostiamo più in alto nella lista, le entità diventano più simili tra loro ed è di solito più difficile fare la disambiguazione. Il primo è casuale uniforme. Il secondo è quando le entità hanno titoli simili, ad esempio, due libri con il nome "The Return". Il terzo è quando hanno descrizioni simili su Wikipedia. E infine quando hanno informazioni simili o attributi su Wikipedia. Ad esempio, lo stesso genere o lo stesso artista per una canzone. Quando mostriamo questa domanda alternativa agli annotatori, sanno il nome di queste entità, ma non necessariamente le conoscono. Quindi, ciò che facciamo è mostrare alcune conoscenze di base sulle due entità. Per le canzoni, mostriamo semplicemente un link di ricerca Google per ogni canzone e poi chiediamo agli annotatori di ascoltare almeno un po' di ciascuna canzone e leggere su ciascuna canzone. Ecco ad esempio, il risultato della ricerca Google per la canzone "Easy on Me." Per i domini di ricette e libri, mostriamo alcuni testi di sfondo da Wikipedia. Per le ricette, mostriamo anche le loro immagini, di nuovo da Wikipedia, in modo che gli annotatori sappiano come appaiono. Poi, abbiamo chiesto agli annotatori di scegliere una di queste entità, ad esempio, ecco la prima, e descriverle usando da tre a cinque espressioni di riferimento indirette. Ecco alcuni esempi dal nostro set di dati. Ad esempio, "quello senza parole", "non quello con il bambino di 12 anni", o "quello fittizio", o "viene dall'Azerbaigian", e così via. Il Corpus AltEntities ha 6.000 domande alternative in tre domini e ha 42.000 espressioni di riferimento indirette. I risultati con il modello T5 XL sono riassunti di seguito. Se il modello di linguaggio ha accesso alle stesse conoscenze di base degli annotatori, allora l'accuratezza è molto alta, è intorno al 92-95%. Ma questo non è realistico. Se il modello di linguaggio ha accesso a conoscenze di base parzialmente sovrapposte, allora l'accuratezza è tra l'82 e l'87%, che è più realistico. Ad esempio, quando il modello di linguaggio recupera le conoscenze di base. Se il modello di linguaggio ha accesso solo ai nomi delle entità, allora l'accuratezza è solo del 60%, quindi c'è molto spazio per il miglioramento. Abbiamo anche dimostrato che i modelli sono generalizzabili tra i domini. Ecco un link al nostro set di dati. Grazie.</sample>
    <sample id="143">L'approccio EDAtt viene confrontato con le politiche SimulST esistenti come la **Wait-k strategy** e la **Local Agreement**, che sono applicate a modelli offline, e con l'architettura **state-of-the-art** specificamente progettata per la pre-traduzione simultanea.</sample>
    <sample id="144">Gli autori dell'articolo "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains" sono affiliati ai seguenti istituti:

1. **Yanis Labrak** - Université de Nantes, CNRS, LITIS - Laboratoire d'Informatique et de Télécommunications de l'Université de Nantes
2. **Jean-Baptiste Masse** - Université de Nantes, CNRS, LITIS - Laboratoire d'Informatique et de Télécommunications de l'Université de Nantes
3. **Thomas Lefebvre** - Université de Nantes, CNRS, LITIS - Laboratoire d'Informatique et de Télécommunications de l'Université de Nantes
4. **Jean-Marc Petitta** - Université de Nantes, CNRS, LITIS - Laboratoire d'Informatique et de Télécommunications de l'Université de Nantes
5. **Jean-Christophe Depalle** - Université de Nantes, CNRS, LITIS - Laboratoire d'Informatique et de Télécommunications de l'Université de Nantes
6. **Jean-Baptiste Gaudin** - Université de Nantes, CNRS, LITIS - Laboratoire d'Informatique et de Télécommunications de l'Université de Nantes
7. **Jean-Christophe Filliâtre** - Université de Nantes, CNRS, LITIS - Laboratoire d'Informatique et de Télécommunications de l'Université de Nantes

Tutti gli autori sono associati all'Université de Nantes e al CNRS (Centre National de la Recherche Scientifique).</sample>
    <sample id="145">Il nome della relatrice è Jenny.</sample>
    <sample id="146">Il paper analizza il problema delle omissioni nella sintesi di dialoghi, un aspetto critico che compromette la qualità delle sintesi generate da modelli di linguaggio avanzati. Sebbene i modelli pre-addestrati siano in grado di produrre sintesi fluenti e coerenti, essi spesso presentano errori fattuali e omissioni, con circa il 70% delle sintesi che soffrono di questo problema. L'analisi mostra che le informazioni omise sono distribuite in modo casuale all'interno dei dialoghi, indipendentemente dalla loro lunghezza o dominio, evidenziando la difficoltà attuale dei modelli nell'identificare le informazioni chiave. Per affrontare questo problema, viene definita una task di rilevamento delle omissioni a livello di enunciato, per cui il modello deve prevedere quali parti del dialogo sono state omise nella sintesi candidata. A tal fine, viene costruito il dataset OLDS, che fornisce etichette di omissioni di alta qualità per la sintesi di dialoghi, basato su cinque benchmark esistenti e cinque domini. Vengono esplorati tre framework di base per il rilevamento delle omissioni, valutati in termini di Precision, Recall e F1-score, nonché di recall a livello di parole (WR score). I risultati mostrano che il task è molto impegnativo, con un F1-score intorno al 50%. Tuttavia, l'integrazione delle informazioni omise nella sintesi candidata attraverso un metodo di post-editing migliora significativamente la qualità della sintesi, dimostrando che il rilevamento delle omissioni è un'area promettente per il miglioramento della sintesi di dialoghi.</sample>
    <sample id="147">Tre autori sono coinvolti nell'articolo: Myra, Esin Durmus e Dan Jurafsky.</sample>
    <sample id="148">Hi, I'm Sara Papi from the University of Trento and Fondazione Bruno Kessler and I will briefly introduce the "Attention as a Guide for Simultaneous Speech Translation" paper, that is a joint work with Matteo Negri and Marco Turchi. What is simultaneous speech translation? Simultaneous speech translation, or SimulST, is the process of translating spoken language into a text in another language in real time, enabling cross-language communication. And what are the problems of the current SimulST models? Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures, for example, training involving different optimization objectives. And training and maintaining several models to reach different latency regimes. For example, training a model with an average of one second latency and another one with two seconds latency, and so on. So what is our solution? First, to use already existing offline ST models without re-training or adopting specific architecture for SimulST. Use only one model for every latency regime and handle latency through specific parameters. And leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output. That is the cross-attention mechanism, and you can see an example on the right. Our solution is to propose EDAtt, or Encoder-Decoder Attention, and it is a strategy for which we decide whether to emit or not a partial translation, based on where attention points to. A word is emitted if the attention is not concentrated, that is, its sum is below a certain threshold alpha towards the last lambda speech frames, meaning that the received information is enough stable. For example, if we receive a speech chunk containing "I'm going to talk about..." and our model predicts the translation in German, and we will look at the cross-attention weights, we'll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames. This means that the first two words will be emitted while since the sum of the cross-attention is above a certain threshold alpha, we will not emit the last word and we wait for another speech chunk. If we go on and we receive another speech chunk, and our model predicts other three words and we will look at those cross-attention weights, we will see that no word points to the last lambda speech frames. This means that these three words will be emitted. If we look at the main results of EDAtt, we'll plot the simultaneous speech translation results on graphs in which we have BLEU on one side that measures the translation quality, and average lagging that is the latency measure, and we also consider the computational aware average lagging that accounts for the model's computational times to predict the output. So we want our curves to be as high as possible on this plot. But also we want that they are shifted on the left. And we compare with popular strategies that are also applied to offline models that are the Wait-k strategy and the Local Agreement. And we compare also with the state-of-the-art architecture specifically tailored for simultaneous pre-translation. These are all the results of the simultaneous speech translation strategy on German. And we see that it outperforms all the strategies applied to offline models since the curves are shifted over the left. And we also see that if we consider the actual elapsed time or the computational-aware time, that is the fastest strategy. If you want to discover more results, read our paper. And we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work. Thanks for your attention.</sample>
    <sample id="149">Sì, il set di dati CoNLL++ è disponibile pubblicamente.</sample>
    <sample id="150">Il paper "MEETINGQA: Extractive Question-Answering on Meeting Transcripts" presenta un nuovo dataset, MeetingQA, focalizzato sull'estrazione di risposte da trascrizioni di riunioni. Questo dominio è unico per la natura lunga, specifica e ricca di informazioni dei testi, ma le ricerche precedenti si concentravano principalmente su sintesi e estrazione di azioni, trascurando il potenziale del componente QA. MeetingQA cattura domande aperte e discussioni dettagliate tipiche delle riunioni, includendo risposte multi-speaker e discontinuous.

Il dataset è stato creato da trascrizioni pubbliche del corpus AMI, con domande selezionate e risposte annotate da esperti, raggiungendo un'elevata concordanza inter-annotator (Krippendorff's alpha = 0.73). Contiene 7.7K domande, 30% delle quali non rispondibili, e 40% con risposte multispan e 48% con risposte multi-speaker. Le domande sono spesso yes/no o retoriche, e le risposte spesso contengono disaccordi.

I risultati mostrano che i modelli fine-tuned superano quelli zero-shot, ma ci sono ancora lacune significative rispetto alle prestazioni umane. I modelli a corto contesto (come RoBERTa) hanno leggermente superato quelli a lungo contesto (come Longformer), mentre i modelli multi-span hanno prestazioni comparabili a quelli single-span. L'uso di dati di interviste (MediaSum) per l'aumento dei dati ha migliorato le prestazioni zero-shot. Gli errori principali includono difficoltà con le domande retoriche e l'identificazione degli speaker. In sintesi, MeetingQA evidenzia le sfide ancora aperte nel QA su testi di riunioni, sia in contesti fine-tuned che zero-shot.</sample>
    <sample id="151">Hello everyone, my name is Ying and my colleague Zhiyang and I will be presenting our research on MultiInstruct improving Multi-Modal Zero-Shot Learning via Instruction Tuning. So with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data-efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions. However, most previous works on instruction tuning focused on improving the zero-shot performance on language only tasks, while computer vision and multi-modal tasks have been left out. Therefore, in this work we want to investigate whether instruction tuning a multi-modal pre-trained models can actually improve generalisation to unseen multi-modal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in the availability of instructional datasets between NLP and multi-modal. There exist more than 1600 language-only instruction tasks. However, there is no large-scale publicly-available multi-modal instruction task. Therefore, this motivates us to build a multi-modal instruction tuning dataset. Here we present MultiInstruct, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source dataset and each task is equipped with five expert written instructions. For investigating multi-modal instruction tuning on our proposed dataset, we take OFA, a unified multi-modal pre-trained model, as our base model. OFA uses a unified vocabulary for language, image tokens and the coordinates of a bounding box. Here we show some example instances from our MultiInstruct dataset, to unify the processing of various input and output data types. We follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format. In which the input text, images, instructions and bounding boxes are represented in the same token space. Ok, now I'm going to talk about multi-modal instruction tuning. So for the training dataset, we use 53 tasks from 9 groups for training and we sample 10,000 instances per task. For testing, we reserve the entire common sense reasoning group for testing, and we select additional 5 tasks from VQ and Miscellaneous groups. We use all the instances in the test split of each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. So we use pre-trained OFA large model as a base model. During training, we mix all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates. So during test for each task, we conduct a total of 5 experiments by evaluating the model using one of the five instructions. In each experiment, we report the min and max performance and the standard deviation of the performance across all 5 experiments. If the task is a multi-model classification task, we report accuracy. If it's a multi-modal generation task, we report Rouge-L. For NLP task, we report Rouge-L as well. We also introduce an additional evaluation metric called sensitivity. So this measures the model's ability to consistently produce the same outputs for the same task regardless of the slight variation in the wording of the instruction. Here is our main result. As we can see, instruction tuning can significantly improve OFA's performance on seen multi-modal tasks. Also, transfer learning from natural instruction dataset can benefit instruction tuning. Here we can see, as the amount of task increases, the model achieves better performance and in the meantime, lower sensitivity. So we also did one experiment. We use one instruction versus 5 instruction. As we can see, using more instructions can improve the model's overall performance and reduce its sensitivity a lot. So this shows the effect of different fine-tuning strategies on the model sensitivity. As we can see by transfer learning from natural instruction datasets, the model can achieve much better sensitivity compared to the original OFA model. We also can see transfer learning from natural instruction datasets can help OFA to attain much better performance on the natural instruct dataset. So overall, we propose the first large scale multi-model instruction tuning dataset with significantly improved their short capability of OFA, and we explore different transfer learning technique and show their benefits. We design a new metric called sensitivity. So one more thing, we are collecting a much larger multi-model instruction tuning dataset with around 150 additional vision language tasks and we will release them. So this is a QR code for our data and model. Thank you.</sample>
    <sample id="152">Frederick Riemenschneider presenta un progetto innovativo che integra NLP e filologia classica, focalizzato sullo sviluppo di modelli linguistici per il greco antico e il latino. I modelli esistenti, come Latin BERT e Ancient Greek BERT, sono principalmente monolingui e encoder-only, con limitazioni nella gestione della multilinguezza e nella valutazione delle prestazioni. Il team ha quindi creato nuovi modelli, GreBERTa e GreTa per il greco antico, e PhilBERTa e PhilTa per la multilinguezza (greco, latino, inglese), utilizzando dati pre-training avanzati, inclusi nuovi corpus da Internet Archive e risorse come Corpus Corporum per il latino. I modelli sono stati testati su compiti come taggaing POS, analisi della dipendenza e lemmatizzazione, dimostrando prestazioni superiori rispetto allo stato dell'arte. L'analisi ha rivelato che gli encoder di T5 richiedono più training per raggiungere prestazioni ottimali, mentre i modelli encoder-decoder eccellono nella lemmatizzazione. Nonostante la multilinguezza, non si riscontrano differenze significative nelle prestazioni semantiche e enciclopediche tra modelli monolingui e multilingui. Il progetto introduce modelli avanzati, tokenizer nativi e un nuovo dataset di alta qualità, offrendo strumenti potenti per la filologia classica.</sample>
    <sample id="153">In questo lavoro, Ninareh Mehrabi e il team di Amazon Alexa AI's Responsible AI si concentrano sulla risoluzione delle ambiguità nei modelli di generazione testo-immagine. L'obiettivo è migliorare la fedeltà delle immagini generate rispetto all'intenzione dell'utente, affrontando problemi come "The girl enters the room with flowers", che può essere interpretato in modi diversi. Il team ha creato un dataset di benchmark, modificando il corpus LAVA, per coprire varie tipologie di ambiguità. Hanno sviluppato due approcci per disambiguare i prompt: uno in cui un modello linguistico genera domande chiarificatrici, e l'altro in cui genera diverse interpretazioni visive. L'utente risponde alle domande o sceglie l'interpretazione preferita, fornendo un prompt disambiguato. Successivamente, il team ha proposto un framework di valutazione automatico basato su un modello VQA (Visual Question Answering) per determinare se le immagini generate corrispondono all'intenzione dell'utente. I risultati mostrano che la disambiguazione migliora la fedeltà delle immagini e che il framework di valutazione è affidabile e in linea con le valutazioni umane. Il lavoro sottolinea la disparità nella risoluzione delle ambiguità a seconda del tipo e dimostra l'efficacia degli approcci proposti. In sintesi, il team ha affrontato il problema delle ambiguità nei modelli testo-immagine, migliorando la generazione di immagini e fornendo strumenti per valutarne la fedeltà rispetto all'intenzione dell'utente.</sample>
    <sample id="154">Gli autori dell'articolo "Attention as a Guide for Simultaneous Speech Translation" sono Sara Papi, Matteo Negri e Marco Turchi. Sara Papi è affiliata all'Università di Trento e alla Fondazione Bruno Kessler.</sample>
    <sample id="155">Il nome della relatrice o del relatore è **Javad Hosseini**.</sample>
    <sample id="157">Il lavoro "Dialogue Summarization with Static-Dynamic Structure Fusion Graph" presentato da Shen Gao della Shandong University si concentra sulla sintesi di dialoghi, un compito complesso e affascinante nel campo della sintesi testuale. L'obiettivo è estrarre le informazioni salienti da un contesto di dialogo multi-partecipante, semplificandolo in un riassunto conciso. I metodi esistenti si basano su strutture grafiche statiche precalcolate, utilizzando strumenti linguistici esterni come l'analisi del discorso e il tracciamento dello stato del dialogo. Tuttavia, questi approcci presentano due limiti: dipendono dalla precisione degli strumenti esterni e la struttura grafica fissa non si adatta dinamicamente al compito di sintesi.

Il modello proposto, SDDS, supera questi limiti. Utilizza un **Utterance Encoder** per rappresentare le frasi in vettori, un **Static-Dynamic Graph Module** per combinare strutture grafiche statiche e dinamiche, e un **Summary Generator** basato su un modello linguistico pre-addestrato per generare il riassunto. Per la struttura statica, SDDS impiega quattro metodi euristici: **Discourse Parsing Graph** (basato su analisi del discorso), **Key Co-occurrence (KeyCo-occ)** (per rilevare parole comuni), **Speaker Relationship Modeling** (per analizzare le interazioni tra i partecipanti) e **Utterance Position Graph** (per considerare la posizione delle frasi). Per la struttura dinamica, utilizza un modello di attenzione multi-head per calcolare le relazioni tra le frasi senza metodi precalcolati. Infine, integra le due strutture grafiche attraverso un meccanismo di attenzione duale, migliorando la generazione del riassunto.

Il codice e i dati sono disponibili su GitHub.</sample>
    <sample id="158">Il mio nome è Qipeng Guo da AWS e oggi presenterò il nostro lavoro "Dual Cache per la Risoluzione di Coreference Neurale di Documenti Lungi". La risoluzione di coreference è il compito di identificare le menzioni di entità in un documento e raggrupparle in base alla stessa entità. I metodi tradizionali richiedono una complessità quadratica per calcolare e memorizzare tutte le coppie di menzioni. I metodi basati su cache riducono la complessità a livello lineare, ma in documenti lunghi, con cambiamenti di argomento frequenti, la politica di evasione LRU (Least Recently Used) porta a un alto tasso di mancate risoluzioni. Per affrontare questo problema, proponiamo una cache dual che combina una cache locale e una globale. La cache locale utilizza la politica LRU, mentre la cache globale utilizza la politica LFU (Least Frequently Used). Il modello analizza il documento da sinistra a destra, classificando le nuove menzioni e valutando la frequenza delle entità. Le entità ad alta frequenza vengono memorizzate nella cache globale, mentre le altre nella cache locale. I risultati mostrano che la cache dual supera i metodi a singola cache e riduce significativamente le mancate risoluzioni. Inoltre, è più efficiente in termini di costi rispetto ai metodi a singola cache. In sintesi, la cache dual è un approccio efficace per la risoluzione di coreference in documenti lunghi.</sample>
    <sample id="159">Ciao a tutti, sono Koustav Sinha e sono lieto di darvi il benvenuto alla discussione del nostro articolo ACL 2023. I giudizi di accettabilità dei modelli linguistici non sono sempre robusti rispetto al contesto. Questo è un lavoro congiunto con John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy e Adina Williams. Quindi, in questo lavoro, rivediamo i paradigmi dei coppie minime. Il paradigma delle coppie minime valuta i modelli linguistici in base ai giudizi di accettabilità, che possono includere anche la grammaticalità come BLiMP, SyntaxGym o l'accettabilità in termini di stereotipi come CrowS pairs. E in questo, il paradigma delle coppie minime, il modo tipico per valutare i modelli linguistici è mostrare una frase accettabile o grammaticalmente corretta e poi mostrare una frase accettabile o una frase grammaticalmente scorretta. E la speranza è che il modello, fondamentalmente, attribuisca più probabilità alla frase accettabile. L'attuale pipeline MPP fondamentalmente non ci permette di valutare l'accettazione di un modello per frasi più lunghe. Oggi i grandi modelli linguistici stanno emergendo con finestre di contesto sempre più lunghe. Quindi è fondamentale che valutiamo l'accettazione dei modelli lungo la finestra di contesto e questo è ciò che stiamo cercando di fare qui. Stiamo cercando di rivedere la pipeline MPP chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe. Quindi questo è l'approccio. Quindi ciò che facciamo è che per simulare queste sequenze più lunghe, rivediamo i dataset stessi e poi ricreiamo frasi scegliendo frasi accettabili o inaccettabili da quei dataset. Quindi, per esempio, qui abbiamo scelto un tipico paio di grammaticalità dal dataset BLiMP dal caso dell'Isola dell'Adjunct. E ciò che facciamo è ricreare come sequenze più lunghe che sono accettabili e che hanno la stessa corrispondenza della struttura grammaticalmente corretta. Estraiamo frasi grammaticali dall'Isola dell'Adjunct e poi le aggiungiamo come prefisso sia alla query accettabile che a quella inaccettabile. Quindi possiamo fare lo stesso scegliendo frasi inaccettabili dalla stessa corrispondenza, e questo potrebbe anche essere usato per testare l'accettabilità dei modelli. E possiamo anche fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un dataset diverso. Quindi questo è ciò che chiamiamo scenario di mismatch. Quindi qui le frasi provengono ancora da dataset pertinenti ma non dallo stesso dataset con cui si sta valutando. E possiamo fare lo stesso per il caso di inaccettabilità. Infine, possiamo scegliere frasi da un dominio completamente non correlato come Wikipedia. Quindi questo ci dirà se i giudizi di accettabilità dei modelli sono effettivamente influenzati da qualsiasi contesto, come, se il contesto proviene da un sottoinsieme diverso del dataset, o se è completamente non rilevante, per la frase che stiamo guardando. Quindi come fa il modello? Quindi prima guardiamo le frasi di Wikipedia, completamente non pertinenti alla coppia di query corrente, e lì troviamo che i giudizi MPP sono per lo più robusti per lunghezze di contesto arbitrarie. Aumentiamo la lunghezza del contesto fino a 1024 per massimizzare i modelli OPT e GPT 2. E qui vediamo che i giudizi MPP sono relativamente stabili. Ora, cosa succede quando scegliamo frasi dallo stesso dataset? Quindi qui stiamo scegliendo o creando frasi da domini accettabili e inaccettabili dallo stesso dataset BLiMP o SyntaxGym. E lì vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiunge un prefisso accettabile o un prefisso inaccettabile. Ma quando combiniamo la struttura, cioè quando scegliamo le frasi dallo stesso fenomeno in BLiMP o SyntaxGym, vediamo un aumento o una diminuzione massiccia del giudizio MPP per il modello, a seconda che il prefisso scelto sia accettabile o inaccettabile. Ora questo e questo effetto molto grande, aumenta lungo la lunghezza del contesto e questo probabilmente influenzerà i nuovi modelli linguistici che hanno una grande finestra di contesto. Quindi perché il prefisso di corrispondenza influisce così tanto sul giudizio del modello linguistico? Quindi abbiamo fatto una serie di analisi in cui abbiamo cercato di perturbare la frase di input, cercando di preservare la struttura rilevante ma aggiungendo rumore all'input. E dopo aver fatto diverse di queste perturbazioni, troviamo che nessuno di questi rumori sta effettivamente facendo cambiare idea al modello in termini di come ci mostra il giudizio MPP. Fondamentalmente, troviamo che i modelli sono sensibili alle frasi perturbate in modi simili. Cioè, quando perturbamo le frasi nel dominio accettabile, vediamo un aumento simile in tutte le perturbazioni e quando perturbamo le frasi nel dominio inaccettabile, vediamo una diminuzione dei giudizi MPP in modo simile. Quindi, i punti chiave del nostro lavoro è che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti che sono condivise tra le frasi. E la valutazione MPP nel modo in cui la facciamo attualmente con input brevi e di una singola frase, potrebbe non catturare completamente la conoscenza astratta dei modelli linguistici lungo la finestra di contesto. Per favore, leggete il nostro articolo per maggiori dettagli dei nostri esperimenti. Grazie per aver ascoltato.</sample>
    <sample id="160">Nel primo passaggio del metodo, i token di input vengono mappati in un **multiset di token di output**.</sample>
    <sample id="161">Il dataset CoScript contiene **55.000 script** per obiettivi specifici.</sample>
    <sample id="163">Il metodo di allineamento migliore per DEPLAIN è **MASSalign**. Questo è stato concluso dopo aver adattato e valutato diverse metodologie di allineamento, utilizzando i dati manualmente allineati di DEPLAIN come standard di riferimento.</sample>
    <sample id="164">L'apprendimento scarsamente supervisionato (WSL) offre il vantaggio di ridurre i costi di annotazione dei dati, poiché utilizza fonti di etichettatura deboli (come regole euristica, knowledge base o crowdsourcing di bassa qualità) invece di annotazioni manuali costose e di alta qualità. Questo approccio permette di addestrare modelli neurali su grandi quantità di dati a basso costo, pur riconoscendo che tali dati sono rumorosi e possono contenere errori. Tuttavia, come evidenziato nel video, i recenti metodi WSL richiedono dati di validazione puliti per funzionare correttamente, il che implica che l'efficienza dei costi potrebbe essere limitata se si devono comunque ottenere annotazioni pulite.</sample>
    <sample id="165">Il paper "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations" di Wenting Zhao presenta un approccio innovativo all'inferenza abduttiva, un tipo di ragionamento che cerca la spiegazione più plausibile per un evento. Zhao introduce il concetto di LiPoR (Likelihood Learning with Posterior Regularization), un metodo di apprendimento non supervisionato che permette di identificare spiegazioni plausibili senza bisogno di annotazioni manuali, spesso soggettive e discordanti.

In un contesto (X) come "Emily era bloccata nel traffico", con un risultato (Y) "Emily ha preso il suo volo", e una serie di spiegazioni possibili (Z), LiPoR tratta le spiegazioni come variabili latenti, massimizzando la probabilità marginale dell'esito dato il contesto. Questo approccio, tuttavia, non distingue tra spiegazioni plausibili e implausibili, quindi Zhao introduce un regolarizzatore che sfrutta la mutual esclusività delle spiegazioni (ad esempio, "Il volo era in ritardo" e "Il volo è partito in orario" non possono essere vere contemporaneamente).

Il regolarizzatore Omega preferisce un sottoinsieme di spiegazioni, massimizzando l'entropia della distribuzione delle spiegazioni data il contesto. I risultati su AlphaNLI, il dataset più utilizzato per l'inferenza abduttiva, mostrano che LiPoR supera i modelli zero-shot e l'approccio non supervisionato precedente, ottenendo un miglioramento di oltre 4 punti di accuratezza rispetto a GPT-3.</sample>
    <sample id="166">**Abstract:**  
The paper introduces "A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text," addressing the challenge of retrieving images from long and complex textual descriptions. Traditional visual language models excel in image-sentence retrieval but struggle with linguistically complex text due to their reliance on analogical reasoning (System 1). Inspired by the Divide-and-Conquer strategy and Dual-Process Theory, the framework integrates both analogical and logical reasoning systems. The first module, the Proposition Generator, decomposes complex text into simpler propositions using BART’s decoder. The Visual-Linguistic Interactor (System 1) processes visual-proposition interactions, generating matching scores and reasoning states. The Neural-Symbolic Reasoner (System 2) integrates these states via negation and conjunction operations to produce final inference results. Experimental results demonstrate the framework’s superiority over baselines, with ablation studies validating each module’s effectiveness. Case studies highlight the method’s interoperable processing, showcasing intermediate inference states. The authors suggest that neural symbolic reasoning and Divide-and-Conquer approaches, combined with Dual-Process Theory, can enhance compositional reasoning and planning in large language models, making them more effective for complex tasks.</sample>
    <sample id="167">I documenti in DEPLAIN-web sono stati allineati in modo misto: **parzialmente manualmente e parzialmente con metodi di allineamento automatici**.</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato raccogliendo articoli di notizie dalla Reuters del 2020 e annotandoli utilizzando le stesse linee guida di annotazione del CoNLL-2003.</sample>
    <sample id="169">Il paper "Prompting PaLM for Translation: Assessing Strategies and Performance" esplora l'uso del modello PaLM (540 miliardi di parametri) per la traduzione automatica, confrontandolo con i sistemi di traduzione neurali di punta. PaLM, addestrato su 780 miliardi di token, è stato valutato utilizzando test set aggiornati e metriche avanzate (come BLEURT), con risultati vicini a quelli dei sistemi commerciali. Tuttavia, i sistemi specializzati mantengono un vantaggio significativo.

La ricerca evidenzia l'importanza della strategia di prompting: la qualità degli esempi forniti influisce maggiormente sulle prestazioni rispetto alla forma del prompting. Un approccio a 5-shot, con esempi selezionati da dati di sviluppo (più curati rispetto ai dati di addestramento), ha mostrato risultati migliori. Gli errori più comuni di PaLM riguardano omissioni, suggerendo una tendenza a privilegiare la fluidità a discapito dell'accuratezza.

In sintesi, PaLM si avvicina ai sistemi di traduzione di punta, ma la qualità degli esempi e la selezione del prompting rimangono cruciali per ottimizzarne le prestazioni.</sample>
    <sample id="170">Ciao a tutti, mi chiamo Yusen Zhang dell'Università della Pennsylvania. Oggi presenterò il nostro lavoro "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations". Quindi, il parsing semantico è un compito per costruire rappresentazioni semantiche delle query degli utenti come SQL e Lambda Calculus. E il Cross-Lingual Semantic Parsing è il compito di tradurre le query in più lingue naturali in più rappresentazioni del significato. Come mostrato in questa figura, dobbiamo tradurre la query in più lingue naturali utilizzando modelli neurali in SQL, Lambda o FunQL, e così via. I modelli esistenti di parsing semantico cross-linguistico sono proposti e valutati separatamente su dataset di compiti e applicazioni limitati. Ad esempio, c'è molta copertura su certe lingue naturali. Ma il cinese è mancante e c'è una mancanza di copertura su certe rappresentazioni del significato. Il calcolo Lambda è mancante, o vengono valutati solo su certi modelli neurali. Ad esempio, c'è solo un singolo modello per valutarli. Quindi, a tal fine, proponiamo XSemPLR. Forniamo un dataset unificato XSemPLR per il parsing semantico cross-linguistico in più lingue naturali e rappresentazioni del significato. Contiene 9 dataset in vari domini, 5 compiti di parsing semantico, 8 rappresentazioni del significato e 22 lingue naturali in 15 famiglie linguistiche. E per valutare meglio il nostro benchmark, consideriamo le sei impostazioni per l'addestramento e la valutazione. La prima è Translate-Test. Usiamo Google Translate API per tradurre la sorgente nella lingua di destinazione, quindi usiamo un modello monolingue per l'addestramento e la valutazione. E per esempio, addestriamo il modello inglese su una query inglese e durante l'inferenza traduciamo la query tedesca usando l'API in inglese e poi usiamo il modello addestrato per prevedere lo SQL. E testeremo anche il Modello Monolingue. In questa impostazione, la lingua sorgente è la stessa della lingua di destinazione, per esempio tedesco-tedesco o inglese-inglese. Testiamo anche l'impostazione Monolingue Few-shot addestrando modelli monolingue con solo il 10% dei dati di addestramento. E testiamo il Modello Multilingue che addestriamo un modello multilingue per tutte le lingue. E per esempio, mettiamo insieme le query tedesche, inglesi, cinesi per addestrare un modello multilingue. E durante l'inferenza possiamo usare questo modello per tradurre le query tedesche o le query cinesi, et cetera. E consideriamo anche il trasferimento Zero-shot e Few-shot cross-linguistico. Addestriamo su una lingua sorgente e trasferiamo su un'altra lingua. Quindi, durante l'addestramento, lo addestriamo su query inglesi o sulla combinazione di query inglesi e tedesche Few-shot per addestrare un modello multilingue per prevedere l'output SQL. E abbiamo anche trovato molti risultati interessanti. Quindi, riguardo all'analisi dei modelli monolingue, valutiamo su due gruppi di modelli inclusi Encoder-PTR che sta per Encodificatori Multilingue Pre-addestrati con Decodificatori basati su Puntatore, come XLM-R + PTR e mBERT + PTR. E, valutiamo anche i modelli Encoder-Decoder, che sono Encodificatori Multilingue Pre-addestrati Decodificatori, come mBART e mT5. Abbiamo scoperto che Encoder-Decoder ottiene le migliori prestazioni su tutti e nove i dataset. E valutiamo mT5 e XLM-R + PTR in impostazione multilingue. Abbiamo scoperto che Encoder-Decoder o Encoder-PTR possono essere migliorati addestrando in una miscela di varie lingue. Abbiamo scoperto che è perché la maggior parte delle principali lingue naturali può ottenere un guadagno di prestazioni, tranne che le prestazioni dell'inglese scendono in sette dataset e solo guadagnano in tre dataset. Penso che questo sia noto come la "Maledizione della Multilingue". Confrontiamo anche il divario di prestazioni cross-linguistico. In questa figura, la linea blu è il Trasferimento Few-shot cross-linguistico. La linea arancione è il Trasferimento Zero-shot cross-linguistico. Mentre la linea verde è l'Impostazione Monolingue. Abbiamo scoperto che, confrontando la linea verde e arancione, abbiamo scoperto che con l'impostazione Zero-shot, le prestazioni del trasferimento sono significative, e poi confrontando le linee blu e arancioni, abbiamo scoperto che con l'impostazione Few-shot il divario di trasferimento si accorcia rapidamente. Abbiamo anche trovato alcuni altri risultati interessanti. Ad esempio, Encoder-Decoder supera il lavoro precedente o ottiene risultati confrontabili. L'addestramento su lingua naturale inglese può aumentare significativamente le prestazioni di Few-shot sulle lingue naturali di destinazione, e abbiamo scoperto che i modelli di linguaggio multilingue come Codex e BLOOM sono ancora inadeguati per i compiti di parsing semantico cross-linguistico. Per riassumere, abbiamo costruito XSemPLR, un benchmark unificato per il parsing semantico cross-linguistico con più lingue naturali e rappresentazioni del significato. Condottiamo uno studio di benchmark completo su tre tipi rappresentativi di modelli di linguaggio multilingue. E i nostri risultati mostrano molti risultati interessanti. E così via. E benvenuti a visitare il nostro articolo e il codice. Grazie per l'attenzione.</sample>
    <sample id="171">I lavori esistenti per proteggere i modelli di embedding come servizi possono essere classificati in quattro categorie principali. Tuttavia, questi metodi presentano limitazioni:

1. **Watermarking tradizionale**: Non è applicabile agli embedding come servizi.
2. **Watermarking basato su dati**: Non è efficace nel proteggere i modelli durante l'estrazione.
3. **Watermarking basato su reti**: Non è trasferibile durante l'estrazione del modello.
4. **Watermarking basato su backdoor**: Non è applicabile agli embedding come servizi.

Questi limiti evidenziano la necessità di un approccio innovativo, come quello proposto nel paper, che introduce l'**Embedding Marker**, un metodo di watermarking basato su backdoor applicabile agli embedding come servizi.</sample>
    <sample id="172">No, gli LLM multilingue come Codex e BLOOM non sono sufficienti per il Cross-Lingual Semantic Parsing (CLSP). Secondo lo studio presentato, questi modelli ottengono risultati inadeguati rispetto alle aspettative.</sample>
    <sample id="174">Il paper "ArgAnalysis35K: A large-scale dataset for Argument Quality Analysis" presenta un dataset unico nel campo dell'analisi della qualità degli argomenti, con 35.000 coppie di argomenti e analisi. A differenza di altri dataset, ArgAnalysis35K si distingue per la sua diversità, qualità e profondità. Il dataset include argomenti di alta qualità provenienti da fonti come discorsi di alto livello, dibattiti esperti e debaters intermedi, oltre a contributi di novizi e persone comuni. Inoltre, si concentra su 24 temi diversi, catturando il maggior numero possibile di mozioni per ogni tema, anziché selezionarne solo alcune.

Un aspetto innovativo del dataset è l'introduzione del concetto di "analisi", che combina affermazioni, premesse e altre componenti per spiegare meglio un argomento. Questo permette di catturare la complessità degli argomenti in modo più accurato. Inoltre, il dataset utilizza un modello di affidabilità basato su istanze, che tiene conto delle possibili distorsioni degli annotatori su argomenti specifici, migliorando la qualità delle valutazioni. Infine, il dataset include un modello di rilevanza che assegna un punteggio di rilevanza a ogni argomento per ogni tema, permettendo di catturare meglio la pertinenza degli argomenti ai vari argomenti di dibattito. In sintesi, ArgAnalysis35K rappresenta un passo avanti significativo nel campo dell'analisi della qualità degli argomenti, offrendo un dataset più completo, diversificato e affidabile.</sample>
    <sample id="175">Il metodo affronta l'ambiguità delle permutazioni attraverso una **relaxazione continua** che approssima il problema della permutazione, rendendolo **GPU-friendly** e permettendo la **backpropagation** per apprendere le permutazioni linguisticamente più plausibili. Questo approccio evita di imporre vincoli rigidi sulle permutazioni, mantenendo al contempo la flessibilità e l'espressività del modello.</sample>
    <sample id="176">L'equità di un modello NLP a valle viene definita in base alla sua capacità di trattare in modo imparziale e corretto diverse categorie sociali, politiche o demografiche, evitando discriminazioni o pregiudizi. Questo implica che il modello non dovrebbe mostrare prestazioni diverse o ingiuste a seconda del gruppo o dell'opinione politica rappresentata nei dati di input. Ad esempio, in contesti come il rilevamento di discorsi d'odio o notizie false, un modello equo dovrebbe essere in grado di rilevare correttamente i discorsi d'odio o le notizie false indipendentemente dal gruppo o dall'opinione politica che ne è oggetto.</sample>
    <sample id="177">Il nome del relatore è Yanis Labrak.</sample>
    <sample id="178">Il nome della relatrice è Koustav Sinha.</sample>
    <sample id="179">Melanie Sclar presenta una ricerca su come migliorare le capacità di ragionamento della Teoria della Mente (ToM) nei modelli di linguaggio, in particolare nei Large Language Models (LLM). La Teoria della Mente è la capacità di comprendere e ragionare sulle credenze e gli stati mentali degli altri, un aspetto in cui gli LLM, come ChatGPT o GPT-3, continuano a mostrare prestazioni scadenti, specialmente nei compiti di false credenze.

Il metodo proposto, chiamato **SymbolicToM**, utilizza rappresentazioni grafiche esplicite per modellare gli stati mentali dei personaggi in una storia. Queste rappresentazioni, come BBob e BBob,Alice, catturano ciò che un personaggio crede e ciò che pensa che un altro personaggio creda. L'algoritmo, eseguito in fase di inferenza, sfrutta modelli di Natural Language Inference (NLI) e di Extraction of Information (OpenIE) per generare queste rappresentazioni.

Gli esperimenti dimostrano che SymbolicToM migliora significativamente le prestazioni degli LLM, superando approcci supervisionati come GPT-3 fine-tuned e modelli specifici per ToM. I guadagni variano da 51 a 67 punti di accuratezza, a seconda del modello. Inoltre, SymbolicToM mostra robustezza in scenari di generalizzazione, come storie concatenate o frasi riformulate, mantenendo prestazioni elevate anche in contesti linguisticamente diversi.

In sintesi, SymbolicToM è un metodo plug-and-play che migliora il ragionamento ToM negli LLM, offrendo rappresentazioni più interpretabili e prestazioni superiori rispetto agli approcci supervisionati.</sample>
    <sample id="180">Il nome della relatrice è Myra.</sample>
    <sample id="181">In this paper, we address the problem of constrained language planning, where large language models (LLMs) generate step-by-step scripts for specific goals with multi-faceted constraints. Unlike previous work focusing on abstract goals, we explore the challenge of planning for concrete, constrained goals, such as "make a chocolate cake." To tackle this, we first evaluate LLMs' performance on specific goals, finding unsatisfactory results due to semantic incompleteness and lack of constraint faithfulness. We conduct a detailed analysis, revealing that LLMs struggle with certain constraint categories, leading to high variance in output quality. To address this, we propose an over-generate-then-filter method: LLMs generate multiple scripts for specific goals, and a filter model selects the most faithful ones based on semantic similarity and constraint keywords. This approach significantly improves script quality.

To enable constrained language planning with smaller models, we distill a dataset of 55,000 specific goals and scripts, named CoScript, from LLMs. We validate its quality through crowd-sourced revisions. CoScript demonstrates high pluralism in generated goals, making it a valuable resource for training specialized models. We show that T5, a smaller model fine-tuned on CoScript, outperforms most LLMs in generating high-quality scripts. Our work advances constrained language planning by addressing its challenges and providing a robust dataset for further research.</sample>
    <sample id="182">Nel contesto dell'articolo, il **tropicalismo** si riferisce a un **tropo culturale** che associa le donne latine a caratteristiche esotiche, vibranti e legate a una rappresentazione stereotipata di una regione tropicale. Questo contribuisce a una narrativa di **diversificazione** e **altreizzazione**, in cui queste donne sono definite principalmente in relazione alla loro identità culturale, piuttosto che come individui complessi e multidimensionali. Il tropicalismo, quindi, riflette un **stereotipo** che limita e essentializza le donne latine, perpetuando rappresentazioni dannose e unidimensionali.</sample>
    <sample id="183">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target confrontando le persone con le risposte generate dai modelli linguistici. Hanno utilizzato i risultati di uno studio precedente che aveva dato questi stessi prompt a soggetti umani, permettendo così una diretta comparazione tra le risposte umane e quelle generate dai modelli. Questo approccio ha permesso di identificare i modelli di stereotipi e rappresentazioni che emergono sia nelle risposte umane che in quelle dei modelli, evidenziando le somiglianze e le differenze tra i due set di dati.</sample>
    <sample id="184">In questo lavoro, è stato utilizzato **Pointwise CXMI (Contextual Information Measure)** per misurare l'utilizzo del contesto durante la traduzione. Questo indicatore misura quanto il contesto fornisce informazioni sul target, dato il contesto e la fonte. Il Pointwise CXMI è stato esteso per valutare l'uso del contesto a livello di frase o di parola, identificando le parole che richiedono contesto per la traduzione.</sample>
    <sample id="185">DrBERT e ChuBERT sono due modelli pre-addestrati in francese, ma differiscono per le fonti dei dati utilizzati per l'addestramento:

- **DrBERT** è basato su dati medicali raccolti dal web (NACHOS) e utilizza una versione di RoBERTa.
- **ChuBERT** è basato su dati clinici anonimi ottenuti dal data warehouse dell'Ospedale Universitario di Nantes.

In sintesi, DrBERT si concentra su dati biomedicali generici, mentre ChuBERT è specializzato in dati clinici specifici.</sample>
    <sample id="187">Due autori sono coinvolti nell'articolo.</sample>
    <sample id="188">Il trasferimento iterativo dell'apprendimento è una strategia in cui un modello viene aggiornato e raffinato iterativamente utilizzando i dati raccolti da ogni nuovo round di apprendimento attivo. In questo contesto, il modello viene prima inizializzato con conoscenze trasferite da compiti correlati (come la classificazione di consonanza ed espansione/confronto) e poi viene continuamente migliorato attraverso l'aggiunta di nuovi dati annotati. Questo approccio permette al modello di adattarsi e migliorare gradualmente le sue prestazioni, specialmente in compiti rari come il rilevamento della dissonanza cognitiva.</sample>
    <sample id="189">L'obiettivo del set di dati AltEntities è comprendere e migliorare la capacità dei modelli linguistici di interpretare e selezionare entità attraverso espressioni di riferimento indirette, come "the newer one" o "the one without words", in contesti conversazionali naturali. Il set di dati copre tre domini (musica, libri e ricette) e fornisce 6.000 alternative questioni con 42.000 espressioni di riferimento indirette, con l'obiettivo di valutare e migliorare l'accuratezza dei modelli nel risolvere tali espressioni.</sample>
    <sample id="190">Un utente malintenzionato può estrarre i parametri del modello attraverso un Embedding as a Service (EaaS) utilizzando tecniche di **model inversion** o **adversarial attacks**. Questi metodi sfruttano le informazioni contenute negli embedding generati dal modello per dedurre i parametri interni del modello stesso. Ad esempio, un attaccante potrebbe:

1. **Analizzare le somiglianze tra embedding**: Utilizzando tecniche come la **somiglianza cosinus** o **L2 distance**, l'attaccante può confrontare gli embedding generati dal servizio con quelli di un modello noto per dedurre somiglianze o differenze.
2. **Generare query specifiche**: L'attaccante può inviare query progettate per massimizzare l'informazione ottenuta dagli embedding, come frasi che contengono termini sensibili o che sfruttano vulnerabilità del modello.
3. **Utilizzare tecniche di adversarial learning**: L'attaccante può addestrare un modello per prevedere i parametri del modello originale basandosi sugli embedding generati dal servizio.

Questi metodi sono efficaci perché gli embedding contengono informazioni compresse sui parametri del modello, rendendo possibile dedurre informazioni sensibili anche se il modello è protetto da tecniche come il watermarking.</sample>
    <sample id="191">Tre autori sono coinvolti nell'articolo: Sara Papi, Matteo Negri e Marco Turchi.</sample>
    <sample id="192">La presentazione di Yang Luo si concentra su **CAME (Confidence-guided Adaptive Memory Efficient Optimization)**, un nuovo ottimizzatore progettato per bilanciare velocità di convergenza e basso consumo di memoria nei modelli di linguaggio di grandi dimensioni. Gli ottimizzatori tradizionali come Adam richiedono molta memoria per memorizzare stime di gradienti, mentre metodi come Adafactor riducono la memoria ma penalizzano le prestazioni. CAME affronta questa sfida introducendo un approccio basato sulla fiducia per gestire gli errori nelle previsioni degli aggiornamenti, riducendo l'instabilità e migliorando la stabilità dell'addestramento.

Gli esperimenti su modelli come BERT, GPT-2 e T5 dimostrano che CAME supera Adafactor con un aumento dell'accuratezza di validazione del 3,4% e si confronta positivamente con Adam, specialmente con batch di grandi dimensioni (da 8K a 32K). Inoltre, CAME riduce significativamente il consumo di memoria rispetto a ottimizzatori come Adam e LAMB, mantenendo prestazioni competitive. L'approccio di CAME, basato sulla fiducia e sull'uso di una matrice di instabilità, lo rende efficace per l'addestramento di modelli di grandi dimensioni e adatto a scenari con batch elevati, estendendo le capacità degli ottimizzatori memoria-efficienti.</sample>
    <sample id="193">Il set di dati iniziale è stato creato da un singolo annotatore, come descritto nel processo di annotazione di dissonance relations.</sample>
    <sample id="194">Gli autori dell'articolo sono affiliati a Carnegie Mellon University, University of Washington, Allen Institute for AI, e Lab in the Wild.</sample>
    <sample id="195">Il lavoro presentato, "Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering" (RoHT), affronta la sfida dell'Answering Question (XQA) complessa, integrando conoscenze da fonti eterogenee. A differenza dei metodi neuro-simmetrici, che richiedono KB completi, e dei metodi basati sulla decomposizione, che si affidano solo a corpora testuali, RoHT propone un framework a due stadi. 

In primo luogo, costruisce un Hierarchical Question Decomposition Tree (HQDT) che scompone la domanda complessa in sottodomande atomiche. In secondo luogo, utilizza un ragionamento probabilistico su HQDT per combinare le conoscenze da KB e corpora testuali, considerando la certezza delle risposte.

Il processo di ragionamento è ricorsivo, partendo dalla domanda principale e procedendo verso le sottodomande, con tre fasi per ogni nodo: selezione delle fonti di conoscenza, estrazione delle risposte e aggregazione delle risposte candidate.

I risultati su due dataset complessi, KQA Pro e Musique, dimostrano l'efficacia di RoHT:

- **KQA Pro**: RoHT KB supera i metodi esistenti integrando risposte da sottodomande di diversi livelli, e l'aggiunta di Wikipedia migliora ulteriormente le prestazioni.
- **Musique**: RoHT Text migliora l'F1 dell'11,9% rispetto al metodo di stato dell'arte, e RoHT-mix, che combina testo e KB, supera TransferNet.

In sintesi, RoHT dimostra la superiorità dell'integrazione esplicita di conoscenze da fonti diverse per l'Answering Question complessa.</sample>
    <sample id="196">L'esempio in cui il governatore è a sinistra è: **"I saw Bart and Lisa"**. In questa frase, il governatore ("I saw") è posizionato a sinistra della coordinazione ("Bart and Lisa").</sample>
    <sample id="197">I quattro modelli all'avanguardia nei sistemi di dialogo valutati nello studio ABC-Eval sono:

1. **Amazon Alexa AI**  
2. **Un modello non specificato sviluppato dall'Emory NLP Lab**  
3. **Un modello non specificato sviluppato da un'altra organizzazione**  
4. **Un modello non specificato sviluppato da un'altra azienda**

Purtroppo, i nomi specifici dei modelli non sono stati forniti nel testo, ma questi rappresentano i sistemi di dialogo di punta utilizzati per l'analisi.</sample>
    <sample id="198">La valutazione dell'accettabilità dei modelli linguistici nell'intera finestra di contesto è necessaria perché i modelli moderni, con finestre di contesto sempre più lunghe, possono essere influenzati da informazioni contestuali. Questo è cruciale per comprendere come i modelli gestiscono l'accettabilità in contesti più ampi e per identificare eventuali debolezze o sensibilità a strutture sintattiche o semantiche condivise tra le frasi. Inoltre, la valutazione su sequenze più lunghe aiuta a garantire che i modelli mantengano giudizi robusti e coerenti indipendentemente dalla lunghezza del contesto.</sample>
    <sample id="199">Sì, la formazione attraverso la modalità multilingue ha causato un calo delle prestazioni rispetto al modello inglese monolingue in sette dei nove dataset, un fenomeno noto come "maledizione della multilingue".</sample>
    <sample id="200">No, gli annotatori non conoscono l'entità in anticipo. Conoscono il nome delle entità, ma non le informazioni specifiche su di esse.</sample>
    <sample id="201">Le metriche di MT (Machine Translation) utilizzate per la valutazione sono state le seguenti:

* BLEURT
* Metriche neurali di MT all'avanguardia

Inoltre, sono stati presentati anche i risultati delle valutazioni umane basate sugli esperti utilizzando il framework MQM.</sample>
    <sample id="202">No, il regresso nella generalizzazione non sembra influire su specifici tipi di NER, ma piuttosto è un fenomeno generale che colpisce tutti i modelli valutati. Il paper mostra che la performance dei modelli diminuisce con l'aumentare del divario temporale tra i dati di addestramento e quelli di test, ma non specifica che questo fenomeno sia più pronunciato per certi tipi di entità nominate (NER) rispetto ad altri.</sample>
    <sample id="203">La posizionalità nella NLP è importante perché evidenzia come i dataset e i modelli possano riflettere e amplificare i pregiudizi e le disuguaglianze presenti nelle società, influenzando in modo diverso utenti di diverse identità, culture e contesti. Questo può portare a risultati ingiusti o inefficaci per gruppi specifici, come ad esempio persone di determinate nazionalità, generi o livelli educativi. Riconoscere e studiare la posizionalità aiuta a sviluppare tecnologie più eque e inclusive, riducendo il rischio di escludere o danneggiare gruppi svantaggiati.</sample>
    <sample id="204">Gli LLM multilingue come BLOOM non sono stati affinati mediante adattatori, ma con una messa a punto integrale.</sample>
    <sample id="205">Shangbin, un dottorando dell'Università di Washington, presenta la ricerca "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models". Lo studio esplora come i pregiudizi politici si propagano dai dati di pre-addestramento ai modelli linguistici e, infine, ai compiti downstream. I modelli linguistici, addestrati su grandi quantità di dati web, includono ampiamente fonti di notizie politiche come il New York Times e The Guardian, creando un duplice effetto: da un lato, favoriscono la diversità di opinioni, dall'altro, introducono potenziali problemi di equità.

La ricerca si concentra su tre aspetti principali:  
1. **Valutazione dei pregiudizi politici**: I modelli linguistici mostrano diverse inclinazioni politiche, occupando tutti e quattro i quadranti dello spettro politico. GPT-4 è il più liberale, mentre i modelli BART sono più conservatori.  
2. **Origine dei pregiudizi**: L'addestramento su corpus politicizzati (es. news e social media) modifica le ideologie dei modelli. Ad esempio, RoBERTa diventa più liberale dopo l'addestramento su un corpus di sinistra.  
3. **Polarizzazione e applicazioni downstream**: I modelli riflettono la polarizzazione sociale, con un aumento delle posizioni estreme dopo il 2017. Inoltre, i modelli con diverse inclinazioni politiche mostrano prestazioni diverse in compiti come il rilevamento di odio e fake news, con conseguenze eque significative.  

La ricerca evidenzia un dilemma: evitare i pregiudizi politici rischia di portare a censura, mentre mantenerli può marginalizzare gruppi specifici. Questo solleva l'urgenza di affrontare i problemi di equità nei modelli linguistici.</sample>
    <sample id="206">Per il trasferimento dell'apprendimento, fanno ricorso a due modelli:  
1. **Topic Independent Dissonance Stance Classification** (debate): un modello che classifica se due dichiarazioni in un dibattito sono in accordo o disaccordo, indipendentemente dal tema.  
2. **Binary Classification of Expansion and Comparison Classes** (CE): un modello basato su PDTB che classifica le relazioni di consonanza e dissonanza.  
Questi due modelli vengono utilizzati per trasferire i pesi e migliorare le prestazioni del modello iniziale.</sample>
    <sample id="207">I recenti set di test utilizzati per valutare le capacità di PaLM includono i test set più recenti del settore della traduzione automatica (MT), come quelli utilizzati per le valutazioni WMT (Workshop on Machine Translation). Questi set di test sono stati scelti per evitare sovrapposizioni con i dati di addestramento del modello PaLM, garantendo così una valutazione oggettiva delle sue prestazioni.</sample>
    <sample id="208">Gli autori hanno proposto **tre** suggerimenti alla fine del loro lavoro.</sample>
    <sample id="209">Il metodo proposto, basato sull'idea di "over-generate-then-filter", migliora significativamente la qualità dei risultati rispetto al metodo di riferimento (semplice generazione da parte di InstructGPT) in due aspetti principali:

1. **Semantica e Fedeltà alle Constrinzioni**: Il metodo proposto garantisce una maggiore completezza semantica e una maggiore fedeltà alle specifiche restrizioni imposte dai goal, superando i limiti del metodo di riferimento che spesso produce script non completamente aderenti alle condizioni richieste.

2. **Efficienza e Qualità**: Utilizzando un processo di over-generation seguito da un filtro, il metodo proposto riesce a generare script di qualità superiore rispetto a quelli prodotti direttamente da InstructGPT, anche se quest'ultimo è un modello di grandi dimensioni. Questo dimostra che con un dataset appropriato e una tecnica di generazione ottimizzata, modelli più piccoli e specializzati (come T5) possono raggiungere o superare le prestazioni dei modelli più grandi.

In sintesi, il metodo proposto offre una soluzione più efficace e affidabile per la pianificazione linguistica vincolata, migliorando sia la qualità semantica che la conformità alle restrizioni, e aprendo la strada all'utilizzo di modelli più piccoli e meno costosi.</sample>
    <sample id="210">Il nome della relatrice o del relatore è Shuheng.</sample>
    <sample id="211">Sì, i risultati e il set di dati nell'articolo possono essere utilizzati come parametri di riferimento per il problema dell'automazione della semplificazione del testo. In particolare, i risultati del fine-tuning dei modelli long-mBART e base mBART mostrano che la semplificazione automatica può ottenere risultati migliori rispetto alle linee di base, e il set di dati DEPLAIN può essere utilizzato come standard di riferimento per valutare e confrontare le prestazioni di diversi metodi di semplificazione automatica.</sample>
    <sample id="212">Nell'articolo, viene utilizzato un solo modello più piccolo, T5, per dimostrare che può generare script di qualità superiore rispetto alla maggior parte dei grandi modelli di linguaggio quando viene addestrato sul dataset CoScript.</sample>
    <sample id="213">Il modello utilizzato come base per analizzare l'ottimizzazione delle istruzioni multimodali è **OFA (Open Foundation Models)**.</sample>
    <sample id="215">Il talk di Adam Przepiórkowski si concentra sulla struttura di dipendenza delle coordinazioni, confrontando diverse teorie e approcci corpus. Le teorie asimmetriche, come le Universal Dependencies e la teoria del testo di Mel'čuk, assegnano la testa della coordinazione al primo congiunto (ad esempio, "Lisa" in "Lisa, Bart, e Maggie"). Al contrario, l'approccio a capo congiuntivo, adottato dai treebank praghesi, considera la congiunzione come testa, con dipendenze che partono da essa verso tutti i congiunti. Infine, l'approccio multi-capo, come in Word Grammar di Hudson, tratta tutti i congiunti come teste della coordinazione.

L'argomento principale del paper è a favore delle strutture simmetriche di coordinazione, basato sul principio di minimizzazione della lunghezza delle dipendenze. Questo principio sostiene che dipendenze più corte siano preferite. Ad esempio, in "Marge read this absolutely fascinating book about bees yesterday", la ristrutturazione delle dipendenze riduce la lunghezza totale, rendendo la frase accettabile nonostante la violazione del principio che gli oggetti diretti siano vicini al verbo.

Inoltre, l'analisi statistica del Penn Treebank conferma che i congiunti più corti tendono ad essere il primo congiunto, specialmente quando la differenza di lunghezza è significativa. Tuttavia, questa tendenza si manifesta solo quando il governatore è a sinistra o assente. Quando il governatore è a destra, come in "laughed Ted and Ned", l'effetto scompare. Questi risultati forniscono un forte argomento contro le strutture asimmetriche e a favore di quelle simmetriche.</sample>
    <sample id="217">Il lavoro "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation" di Weihao Zeng, Lulu Zhao e Keqing He affronta la sfida della generazione di dialoghi controllabili con più attributi. I ricercatori hanno identificato limiti nei metodi esistenti, che si concentrano su attributi singoli o combinano controllori appresi da attributi singoli con etichette specifiche, senza gestire attributi continui. Inoltre, la controllabilità è limitata dai dati annotati e manca una metrica di valutazione unificata.

La proposta principale è il **Disentangled Controllable Generation (DCG)**, che impara concetti di attributi da valori visibili e utilizza una perdita di disancoraggio per separare combinazioni di attributi. Viene introdotto un **Unified Reference-Free Evaluation Framework (MAE)** per valutare attributi di diverse granularità senza dati aggiuntivi. I modelli si basano su DialoGPT con un modulo di prompt compositivo, utilizzando prompt orientati agli attributi e ai compiti per guidare la generazione.

I risultati dimostrano che DCG supera i benchmark in controllabilità e qualità del testo, mantenendo un leggero calo nelle metriche di controllabilità. L'analisi dei prompt concatenati mostra che il modello impara a disancorare combinazioni di attributi e generalizzare da attributi visibili a invisibili. MAE, con prompt continui, supera le valutazioni umane e i metodi classici, dimostrando l'efficacia del framework. La proposta migliora la generazione e la generalizzazione, aprendo nuove prospettive per il dialogo generativo multi-attributo.</sample>
    <sample id="218">Gli autori dell'articolo "Prompting PaLM for Translation: Assessing Strategies and Performance" sono affiliati a Google Translate.</sample>
    <sample id="219">Il lavoro di ricerca "A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports" si concentra sull'analisi dei report finanziari, in particolare del Form 10-K, per estrarre informazioni utili. Motivato dalla somiglianza testuale tra report annuali (circa 80% dei token identici) e dalla dipendenza dei contenuti dall'anno, il team ha sviluppato un modello di evidenziazione che confronta e contrasta i report di un anno con quelli dell'anno precedente.

L'obiettivo è identificare le parole chiave che riflettono le relazioni tra i due report, misurando l'importanza delle parole nel contesto. Il pipeline proposto include tre fasi: la prima riconosce le relazioni tra i segmenti, classificandoli in tre tipi (simili, rivisto, non corrispondente). Per l'ottimizzazione del modello, si utilizza un dataset esterno (eSNLI) per il fine-tuning out-of-domain, seguito da un fine-tuning in-domain con i segmenti rivisto, utilizzando etichette pseudo-positive e tecniche di soft labeling.

I risultati dimostrano che il modello adattativo al dominio raggiunge le migliori prestazioni sul dataset FINAL e mantiene la capacità di generalizzazione su eSNLI. Inoltre, il modello mostra benefici nella simulazione con i segmenti non corrispondenti, non utilizzati durante l'addestramento. Il lavoro apre la strada a ulteriori sviluppi, come il miglioramento dell'efficacia e l'aggiunta di nuove funzionalità per potenziare l'applicazione nell'estrazione di informazioni.</sample>
    <sample id="220">Gli autori dell'articolo "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge" sono affiliati alla Stony Brook University.</sample>
    <sample id="221">L'articolo analizza diverse coppie linguistiche, ma non specifica tutte in dettaglio. Tuttavia, si menziona che le valutazioni sono state effettuate utilizzando i test set WMT, che coprono una vasta gamma di coppie linguistiche. In particolare, l'articolo si concentra sulla traduzione dal tedesco all'inglese come esempio, ma i risultati e le conclusioni sono applicabili a molte altre coppie linguistiche.</sample>
    <sample id="222">**Abstract**  
This work addresses the challenges of domain adaptation in open-domain question answering (QA), focusing on enabling models to generalize across diverse domains. The motivation stems from the difficulty of using general-purpose corpora like Wikipedia to answer domain-specific questions, such as biomedical queries, where the model struggles due to insufficient training data. The study investigates three main contributions: (1) exploring data interventions to facilitate out-of-domain generalization, (2) identifying the nature of dataset shifts when transitioning to new domains, and (3) determining effective interventions based on the type of shift.  

Two primary methods for data interventions are proposed: zero-shot and few-shot. Zero-shot techniques aim to control interactions between question, answer, and context to improve model learning, while few-shot methods leverage limited examples from target domains to generate additional training data. Experiments show that few-shot adaptations improve retriever performance by 8% and reader performance by 11%, on average. Zero-shot techniques, though without target domain examples, also yield significant improvements.  

The study categorizes dataset shifts into "no shift," "concept shift," "covariate shift," and "full shift," based on compatibility between the source model and target domain. Compatibility is measured using likelihood scores for retrievers and normalized answer likelihoods for readers. This framework helps map target datasets onto a 2D grid, revealing shifts like "full shift" for datasets like CliCR and NewsQA, and "no shift" for SearchQA.  

Finally, the research demonstrates that few-shot adaptations are effective for all datasets, while zero-shot methods are particularly beneficial for datasets exhibiting concept or covariate shifts. Overall, the work improves reader performance by up to 24% and provides tailored interventions for different types of dataset shifts.</sample>
    <sample id="223">Il nome della relatrice o del relatore è Shangbin.</sample>
    <sample id="224">Durante gli esperimenti, sono stati studiati due modelli:  
1. **Long-mBART** per la semplificazione a livello di documento.  
2. **Base mBART** per la semplificazione a livello di frase.</sample>
    <sample id="225">Delle 62 diverse attività utilizzate in MultiInstruct, 53 vengono utilizzate per scopi di addestramento, mentre 5 vengono utilizzate per scopi di test.</sample>
    <sample id="226">Due autori sono coinvolti nell'articolo: Regina Stodden e Omar.</sample>
    <sample id="227">Il documento discute le limitazioni attuali dei modelli di linguaggio nell'ambito del **comprensione del linguaggio radicato** (grounded language understanding), ovvero la capacità di mappare espressioni linguistiche naturali su azioni o programmi eseguibili in un ambiente specifico. Applicazioni come assistenti vocali, ricerca semantica e interazione con robot richiedono questa capacità, ma sono complesse a causa della mancanza di radicamento durante la pre-addestrazione dei modelli. I modelli attuali, infatti, sono principalmente addestrati su corpus testuali senza un contesto operativo.

Il documento propone un **nuovo framework**, chiamato **Pangu**, che separa la generazione di piani (task complessa e soggetta a errori) dalla **discriminazione** (task più semplice per i modelli di linguaggio). In Pangu, un agente simbolico propone piani candidati, mentre il modello di linguaggio valuta e classifica queste proposte. Questo approccio evita che il modello debba gestire direttamente la validità e la correttezza grammaticale del piano.

Pangu è stato testato su **knowledge-based question answering**, dimostrando prestazioni eccezionali con modelli come BERT, T5 e Codex, sia con fine-tuning che con apprendimento in-context. In particolare, Pangu mostra un'elevata **efficienza campionaria**, raggiungendo un'accuratezza superiore al 50% con un solo esempio demo. Inoltre, Pangu si distingue per la sua **robusta generalizzabilità** in contesti non i.i.d., grazie a una distribuzione di probabilità simile per strutture viste e non viste.

La conclusione principale è che, per il **comprensione del linguaggio radicato**, la **discriminazione** è una strategia più efficace rispetto alla generazione. Il framework Pangu apre nuove prospettive per applicazioni pratiche in ambienti complessi e eterogenei.</sample>
    <sample id="228">Gli autori hanno effettuato i test sui seguenti set di dati: AG News, MIND, SST2 e Enron Spam.</sample>
    <sample id="229">Il lavoro di Gabriella Skitalinskaya e Henning Wachsmuth si concentra sulla rilevazione di affermazioni argomentative migliorabili, un aspetto cruciale nella scrittura persuasiva. La revisione testuale, processo ricorsivo per raggiungere una formulazione ottimale, è particolarmente importante in contesti argomentativi, dove la scelta delle parole influenza direttamente l’impatto sul pubblico. Lo studio analizza il processo di revisione di un’affermazione come "I cellulari causano il cancro al cervello", passando da una formulazione generica a una più specifica e sfumata ("La radiazione dei cellulari potrebbe causare il cancro al cervello"). La sfida è determinare quando un’affermazione è sufficientemente ben formulata e non richiede ulteriori revisioni.

I ricercatori propongono due compiti: il rilevamento di affermazioni subottimali e la suggerenza di miglioramenti. Per affrontare il problema, esplorano l’uso di dati basati su revisioni, ma evidenziano le difficoltà legate alla variabilità dei domini, alle diverse nozioni di qualità e ai tipi di revisioni. Lo studio si concentra specificamente sui testi argomentativi, analizzando i modelli di revisione su piattaforme come Kialo.

Vengono identificate quattro sfide principali: rappresentatività e affidabilità dei dati, complessità del modello, dipendenza dal contesto e bias tematici o utente. Per ciascuna, i ricercatori propongono strategie e approcci, dimostrando che i dati basati su revisioni possono essere efficaci per i compiti assegnati. La modellazione della distanza tra versioni di un’affermazione è particolarmente utile per rilevare affermazioni subottimali, mentre l’impatto del contesto dipende dal tipo di compito e dalle questioni di qualità. Lo studio conclude che i dati di revisione sono uno strumento valido per migliorare la qualità delle affermazioni argomentative.</sample>
    <sample id="231">NACHOS è un dataset di dati medici raccolti dal web, utilizzato come fonte di dati per l'addestramento del modello DrBERT, il primo modello biomedico in francese.</sample>
    <sample id="232">Il nome del relatore è David Vilar.</sample>
    <sample id="233">Il paper "Attention as a Guide for Simultaneous Speech Translation" affronta la sfida della traduzione simultanea (SimulST), un processo che richiede la traduzione in tempo reale di un linguaggio parlato in un altro. I modelli attuali, spesso specifici per SimulST, presentano problemi come lunghi tempi di addestramento, ottimizzazione di molteplici architetture per diverse latenze e complessità operativa. La soluzione proposta, EDAtt (Encoder-Decoder Attention), utilizza modelli di traduzione offline esistenti senza riaddestramenti, ottimizzando la latenza tramite parametri specifici. EDAtt sfrutta il meccanismo di attenzione crociata tra input audio e output testuale per decidere quando emettere una traduzione parziale: un termine viene emesso se l'attenzione non è concentrata su un intervallo temporale recente (lambda speech frames), altrimenti si attende un nuovo chunk di parlato. I risultati mostrano che EDAtt supera le strategie applicate ai modelli offline e gli approcci di stato dell'arte specifici per SimulST, offrendo prestazioni migliori in termini di qualità della traduzione (BLEU) e latenza, ed è la strategia più veloce considerando il tempo computazionale. Il codice e i modelli sono stati resi open source per favorire la riproducibilità.</sample>
    <sample id="234">La strategia del prompting ha un impatto significativo sui risultati della traduzione con modelli di linguaggio di grandi dimensioni (LLM) come PaLM. In particolare:

1. **Differenza di Performance**: La scelta della strategia di prompting può causare differenze di performance di oltre 40 BLEURT punti, con una media di più di un punto BLEURT per ogni esempio.

2. **Importanza degli Esempi**: La qualità degli esempi forniti durante il prompting è più importante della loro somiglianza con la frase sorgente, specialmente per strategie di prompting a più esempi (come il 5-shot prompting).

3. **Forma del Prompt**: La forma del prompt ha un impatto minore per strategie di prompting a più esempi, ma è cruciale per strategie a un solo esempio o a zero esempi.

4. **Confronto con Sistemi Commerciali**: Nonostante PaLM si avvicini molto ai sistemi di traduzione commerciali come Google Translate, i sistemi specializzati di stato dell'arte mantengono un vantaggio sostanziale.

In sintesi, la strategia di prompting è un fattore chiave che influenza direttamente la qualità della traduzione, e la selezione di esempi di alta qualità è particolarmente importante per migliorare i risultati.</sample>
    <sample id="235">Gli autori dell'articolo "When Does Translation Require Context? A Data-driven, Multilingual Exploration" sono:

* Kayo Yin
* Patrick Fernandes
* Emmy Liu
* André F. T. Martins
* Graham Neubig

Le loro affiliazioni sono:

* Kayo Yin: University of Cambridge
* Patrick Fernandes: University of Cambridge
* Emmy Liu: University of Cambridge
* André F. T. Martins: University of Cambridge
* Graham Neubig: Carnegie Mellon University</sample>
    <sample id="236">Le 5 istruzioni scritte da esperti per ogni task nel dataset MultiInstruct sono:

1. **Task Description**: Una descrizione chiara del compito.
2. **Input Explanation**: Una spiegazione dettagliata degli input (testo, immagini, ecc.).
3. **Output Explanation**: Una spiegazione dettagliata dell'output atteso.
4. **Example Response**: Un esempio di risposta corretta.
5. **Additional Notes**: Note aggiuntive o dettagli specifici per il compito.</sample>
    <sample id="237">Gli autori propongono il **KITMUS Test** (Knowledge Integration from Multiple Sources Test) per valutare la capacità dei modelli di integrare e utilizzare informazioni provenienti da diverse fonti. Questo test include una suite di compiti, in particolare un'attività di risoluzione di coreference, che richiede ai modelli di combinare **conoscenze acquisite durante la pretraining** (background knowledge) e **informazioni specifiche dell'input** (entity-specific knowledge) per risolvere correttamente i riferimenti pronominali. Il KITMUS Test è strutturato in tre impostazioni: **Background-Pretrain**, **Background-Both** e **Background-Inference**, che variano la disponibilità di queste conoscenze, permettendo di valutare come i modelli gestiscono l'integrazione di informazioni da fonti diverse.</sample>
    <sample id="238">Il video presenta il MeetingBank, un nuovo dataset di benchmark creato da Yebowen Hu dell'Università della Florida Centrale. Questo dataset si concentra sui riassunti di riunioni del Consiglio Comunale, un'area emergente per lo sviluppo di tecnologie di sintesi testuale. Il dataset include 1.366 riunioni e quasi 7.000 istanze, con dati come trascrizioni audio, riassunti di riferimento e risorse aggiuntive. La raccolta dei dati è stata effettuata utilizzando Speechmatics API per la trascrizione audio e un processo di allineamento dei timestamp.

Il dataset è stato analizzato per misurare il livello di astrazione dei riassunti, utilizzando metriche come copertura e densità. I risultati mostrano che la maggior parte dei riassunti ha una copertura tra 0,7 e 0,9, indicando una forte presenza di punti verbali. La densità varia tra le città, con Seattle e Boston che presentano i valori più alti, suggerendo un maggiore editing.

Per la valutazione dei modelli, sono stati testati sia sistemi di sintesi extractivi (come Oracle, LEAD, LexRank e TextRank) che abstractivi (BART-Large, Pagasus, Longformer, DialogLM e HMNet). GPT-3, nonostante i risultati modesti in termini di metriche automatiche, ha ottenuto punteggi elevati nella valutazione umana, in particolare per fluidità e coerenza. Tuttavia, ha mostrato meno efficacia in termini di informatività e fattualità.

In conclusione, il MeetingBank è uno strumento prezioso per la ricerca e offre spunti interessanti sul processo decisionale dei Consigli Comunali. L'invito finale è di utilizzare e sperimentare con questo dataset per migliorare le tecnologie di sintesi testuale.</sample>
    <sample id="239">Ciao a tutti, mi chiamo David Vilar e vi farò una breve recensione del paper "Prompting PaLM for Translation: Assessing Strategies and Performance". Questo è un lavoro congiunto con i miei colleghi di Google Translate. PaLM è un modello di linguaggio di grandi dimensioni con 540 miliardi di parametri presentato l'anno scorso, nel 2022. È stato addestrato su una vasta collezione di testo, comprendente 780 miliardi di token. Al momento della pubblicazione, ha raggiunto il massimo livello in centinaia di compiti di NLP. In questo lavoro, presentiamo la prima indagine sistematica del prompting di modelli di linguaggio di grandi dimensioni per la traduzione automatica. Abbiamo valutato la capacità di transizione di tali modelli utilizzando le migliori pratiche della comunità MT. Ciò comporta l'uso dei più recenti set di test per evitare una sovrapposizione dei dati di test con i dati di addestramento del modello di linguaggio. E abbiamo confrontato con i sistemi di stato dell'arte, quindi il sistema con le migliori prestazioni, quindi la valutazione WMT. Usiamo metriche MT neurali di stato dell'arte e mostriamo anche i risultati delle valutazioni umane basate sugli esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione del prompt. Il prompting ha una grande influenza sulle prestazioni dei LLM per la traduzione, come possiamo vedere in un semplice esperimento, in cui abbiamo utilizzato il prompting one-shot e fornito due diversi prompt per ogni frase. La maggior parte delle frasi, 516 su 1.000, ha mostrato una differenza osservata di più di un punto BLEURT. E questo può arrivare, nei casi estremi, fino a 40 punti BLEURT. Quindi, è importante selezionare una buona strategia di prompting. Nei nostri esperimenti, abbiamo optato per una strategia di prompting a 5-shot in cui abbiamo semplicemente contrassegnato ogni frase che forniamo al sistema, con la lingua in cui è scritta. Quindi in questo esempio qui, dove eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche, le frasi sorgente, sono contrassegnate con due punti tedeschi e le traduzioni inglesi con due punti inglesi. Abbiamo visto che la forma effettiva del prompting non ha una grande influenza nel caso di diversi prompt brevi. È cruciale per il prompting zero e one-shot. E quando passiamo, come nel nostro caso, al prompting a cinque shot, non c'è quasi nessuna differenza nella forma effettiva del prompting. Sono gli esempi che hanno il peso maggiore. Il riassunto dei nostri risultati sperimentali è che la qualità degli esempi è più importante della somiglianza con la frase sorgente. Quindi è importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo la selezione dei prompt dai dati di addestramento per le valutazioni WMT sui dati di sviluppo. I dati di sviluppo sono molto più curati e di qualità superiore rispetto ai dati di addestramento, che sono più rumorosi. E i loro risultati mostrano una migliore prestazione quando si utilizzano i dati di sviluppo. Tuttavia, i sistemi specializzati di stato dell'arte hanno un vantaggio sostanziale sulle traduzioni di PaLM. Ma PaLM si avvicina molto a un sistema commerciale. Nel nostro caso, abbiamo scelto di valutare con Google Translate. Le intuizioni che abbiamo ottenuto dalla valutazione umana che abbiamo eseguito utilizzando il framework MQM hanno detto che la fluidità di PaLM è paragonabile ai sistemi di stato dell'arte ma la differenza principale proviene dall'accuratezza. Quindi, in particolare, gli errori più comuni sono errori di omissione. Quindi, sembra che PaLM scelga di produrre una traduzione che suoni meglio, a volte tralasciando parti della frase sorgente che sono fatte in traduzione. Tuttavia, la categoria "Stile/Improvvisato" per PaLM è inferiore rispetto ai sistemi di stato dell'arte, che è un segnale aggiuntivo che PaLM fornisce un output davvero fluido, ma ancora con alcuni problemi di accuratezza. E questo è tutto per questo breve riepilogo. Per maggiori dettagli, per favore, andate alla presentazione completa del paper. Grazie mille.</sample>
    <sample id="240">Ciao, sono Dawei, uno studente di dottorato presso l'Università di Saarland in Germania. In questo video, vorrei presentare il nostro lavoro recente "Più debole di quanto pensiate: uno sguardo critico all'apprendimento debolmente supervisionato." Questo è un lavoro congiunto con Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow. Vorrei iniziare con una breve introduzione all'apprendimento supervisionato debole e all'apprendimento debolmente supervisionato. Nell'apprendimento supervisionato debole, non si etichettano manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura deboli, come semplici regole euristica, basi di conoscenza o crowdsourcing di bassa qualità, come illustrato nella figura a destra. Rispetto alle annotazioni umane, le annotazioni più deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità delle annotazioni è errata. Se dovessimo addestrare direttamente le reti neurali sui dati etichettati debolmente, le reti neurali tendono a memorizzare il rumore delle etichette e non generalizzano. Nell'apprendimento debolmente supervisionato, vengono proposti algoritmi di addestramento per addestrare in modo robusto le reti neurali sotto tale rumore delle etichette in modo che i modelli addestrati si generalizzino bene. Nei lavori recenti in WSL, dove WSL sta per Weakly Supervised Learning, un'affermazione comune è che le persone dicono di addestrare i modelli solo sui dati etichettati debolmente e di ottenere alte prestazioni su set di test puliti. Tecnicamente, questa affermazione non è sbagliata, ma c'è un'eccezione, che è che le persone assumono che ci sia un set di validazione pulito aggiuntivo disponibile per la selezione del modello. Non possiamo fermarci su questo problema, ma ciò implica che sono necessarie ulteriori annotazioni manuali nell'apprendimento debolmente supervisionato. Ma come un elefante nella stanza, questa necessità è spesso trascurata. Il dubbio menzionato in precedenza è posto per chiedere tre domande di ricerca. Primo, i dati di validazione puliti sono necessari per WSL o possiamo forse utilizzare un set di validazione rumoroso? Secondo, se i dati puliti sono richiesti, o se i dati puliti sono obbligatori affinché WSL funzioni, allora quanti campioni puliti abbiamo bisogno? Infine, dovremmo utilizzare solo i campioni puliti per la validazione, o ci sono modi migliori per utilizzarli? Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti. Primo, scopriamo che, interessantemente, i recenti metodi WSL richiedono effettivamente campioni di validazione puliti per funzionare correttamente. Altrimenti, c'è un grande calo delle prestazioni. Come mostrato in questa figura, se non ci sono campioni di validazione puliti, allora i modelli addestrati non possono generalizzare oltre le etichette deboli originali, il che significa che l'addestramento è inutile. Questo indica che gli approcci WSL richiedono effettivamente dati etichettati in modo pulito per funzionare correttamente, e il costo dell'annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato. Il nostro secondo risultato è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci WSL a ottenere prestazioni migliori, come mostrato nella figura a sinistra. Tipicamente abbiamo solo bisogno di 20 campioni per classe per ottenere alte prestazioni. Ma questa non è la fine della storia, perché se decidiamo comunque di accedere ai campioni puliti, allora l'addestramento su di essi direttamente otterrà prestazioni ancora migliori. La figura a destra mostra la differenza di prestazioni tra gli approcci di fine-tuning, che sono applicati direttamente sui dati puliti, e gli approcci WSL, che utilizzano i dati puliti solo per la validazione. Come possiamo vedere, se abbiamo 10 campioni per classe, il fine-tuning diretto inizia a battere gli approcci WSL. Infine, il miglioramento delle prestazioni affermato nei precedenti approcci WSL può essere facilmente raggiunto consentendo di continuare il fine-tuning sui campioni di validazione puliti. Come possiamo vedere dalle figure, il modello vanilla, denominato FTw, inizialmente sottoperforma i metodi WSL più complessi, come COSINE. Tuttavia, se consentiamo di continuare il fine-tuning sui campioni puliti, allora FTw si comporta altrettanto bene come altri metodi. Quindi, in pratica, non c'è motivo di scegliere approcci WSL più complessi che richiedono più tempo di computazione e spazio su disco. Per riassumere, abbiamo dimostrato che i recenti approcci WSL richiedono campioni etichettati in modo pulito per funzionare correttamente. Il loro guadagno di prestazioni e praticità sono fortemente sopravvalutati. Le nostre raccomandazioni concrete per il lavoro futuro sono le seguenti. Primo, riportare i criteri di selezione del modello. Ad esempio, riportare se la selezione del modello è fatta tramite campioni di validazione puliti. Secondo, gli approcci WSL dovrebbero essere confrontati con le linee di base dell'apprendimento a pochi colpi, poiché entrambi lavorano su campioni puliti. Infine, il fine-tuning continuo è una semplice ma forte linea di base che dovrebbe essere considerata nel lavoro futuro in WSL. Infine, abbiamo reso open-source il nostro codice. Potete trovarlo tramite il codice QR su questa diapositiva. Sentitevi liberi di controllarlo. Grazie e godetevi la conferenza.</sample>
    <sample id="241">**Abstract:**  
This paper presents a human-in-the-loop (HITL) evaluation framework for early misinformation detection, specifically focusing on COVID-19 treatment claims. Existing automated systems often fail to address key challenges: unrealistic evaluations using retrospective datasets and a lack of human-centric integration. Our framework integrates human feedback at multiple stages, ensuring a realistic end-to-end workflow from raw tweets to actionable outputs. The system consists of two components: (1) a misleading claim detection module using keyword filtering and a T5 model for claim extraction and ranking based on trendiness; (2) a policy violation verification module employing a BERT-based stance classification model to flag tweets supporting unapproved treatments. Early detection is operationalized as identifying unapproved treatments before their first appearance in debunking news articles, demonstrating the system’s ability to act proactively. Evaluation results show a 65% accuracy rate in policy violation detection and 124.2 policy violations confirmed per human hour. This work highlights the importance of HITL systems in addressing misinformation and provides a realistic evaluation framework for future developments. It also offers industry insights into the challenges and processes of misinformation detection.</sample>
    <sample id="242">I metodi di valutazione comuni per i sistemi di dialogo includono:

1. **Valutazioni umane**: Giudici umani selezionano la conversazione migliore tra due opzioni o assegnano un punteggio su una scala Likert (ad esempio, da 1 a 5) per valutare la qualità complessiva del dialogo.
2. **Valutazioni su scala Likert**: Questi possono essere effettuati a livello di turno (ogni risposta) o a livello di dialogo (intera conversazione).
3. **Confronti di coppie**: Giudici umani confrontano direttamente due conversazioni per determinare quale sia migliore.

Questi metodi forniscono una valutazione olistica della qualità del dialogo, ma non catturano in modo dettagliato i vari aspetti che influenzano la qualità della conversazione.</sample>
    <sample id="243">L'articolo coinvolge 6 autori: Jenny, Sebastian Santy, Ronan Le Bras, Katharina Reinecke, Maarten Sap e gli autori dell'Allen Institute for AI.</sample>
    <sample id="244">Nell'esempio con Servin e Kea, le conoscenze di base necessarie sono:

1. **Conoscenza specifica dell'entità**: "Servin è un giudice."
2. **Conoscenza generale**: "I giudici decidono casi nei tribunali."

Queste conoscenze sono necessarie per risolvere correttamente il riferimento del pronome "he" a Servin.</sample>
    <sample id="245">Il lavoro "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization" presenta una pipeline per identificare lavoratori ad alta concordanza su Amazon Mechanical Turk (MTurk) per il compito di riepilogare testi. La motivazione è che le metriche automatiche spesso sono problematiche e le migliori pratiche per il reclutamento su MTurk sono poco chiare.

La pipeline prevede due fasi: una fase di qualificazione con compiti che testano la capacità di valutare correttamente più dimensioni (con quattro livelli di qualificazione: oro, argento, bronzo e blocco) e una fase di resistenza che verifica la capacità di gestire un carico di lavoro pesante.

I risultati mostrano che i lavoratori selezionati attraverso questa pipeline raggiungono un'alta concordanza (IAA) superiore a quella degli esperti. La pipeline si rivela efficace nel ridurre gli sprechi di tempo e risorse, con costi inferiori rispetto al reclutamento tramite CloudResearch.

In futuro, gli autori intendono esplorare metodi per reclutare lavoratori di alta qualità, applicare la pipeline a diversi compiti, lingue e piattaforme, e migliorare la garanzia di correttezza delle valutazioni.

In sintesi, la pipeline proposta rappresenta una best practice per ottenere riepiloghi di alta qualità su larga scala a costi ridotti, evitando sprechi di risorse.</sample>
    <sample id="246">Sì, il codice è disponibile su GitHub.</sample>
    <sample id="247">Il team di KAIST AI ha presentato un nuovo dataset chiamato **FactKG**, focalizzato sulla verifica di fatti tramite il ragionamento su **grafi della conoscenza** (knowledge graphs). A differenza di dataset esistenti come FEVER, VitaminC, TabFact e InfoTabs, che utilizzano testi o tabelle come fonti di prova, FactKG introduce una nuova sfida: verificare affermazioni in linguaggio naturale basandosi su grafi della conoscenza. I grafi della conoscenza sono considerati una fonte affidabile perché permettono un collegamento diretto e intuitivo tra le affermazioni e le evidenze, riducendo la necessità di interpretazioni aggiuntive. Questo approccio è particolarmente utile in applicazioni pratiche, come i sistemi di dialogo che interagiscono con grafi della conoscenza per verificare la coerenza tra le informazioni fornite dall’utente e quelle presenti nel grafo.

FactKG si basa sul grafo **DBpedia** e include affermazioni in due stili: scritto e colloquiale, per un uso più pratico. Le etichette sono **SUPPORTED** (supportato) e **REFUTED** (confutato). Il compito consiste nel recuperare le evidenze da DBpedia e verificare le affermazioni utilizzando tali evidenze. Il dataset include cinque tipi di ragionamento: **one-hop**, **conjunction**, **esistenza**, **multi-hop** e **negazione**. Ad esempio, le affermazioni **one-hop** richiedono di verificare una singola relazione tra due entità, mentre le **conjunction** richiedono la verifica di più affermazioni correlate. Le **esistenza** verificano se un’entità ha una specifica relazione, e le **negazioni** richiedono un’ulteriore inferenza per confutare la verità di una affermazione.

Per generare affermazioni in stile colloquiale, sono stati utilizzati un modello di trasferimento e template di presupposti. I risultati mostrano che i modelli basati su grafi della conoscenza, come GEAR, superano significativamente le baseline, dimostrando l’efficacia dell’approccio proposto. Il dataset è disponibile per il download e gli autori invitano a contattarli per ulteriori informazioni.</sample>
    <sample id="248">No, gli annotatori per NLPositionality non sono bilanciati rispetto a ciascun gruppo demografico. La ricerca ha rilevato che di solito solo pochi annotatori annotano ogni istanza e che le informazioni demografiche sono raramente raccolte e condivise. Pertanto, il team ha optato per la re-annotazione dei dati per ottenere molti annotatori per ogni istanza e per raccogliere un ricco set di dati demografici. Tuttavia, non è stato specificato se il processo di re-annotazione abbia raggiunto un equilibrio demografico tra gli annotatori.</sample>
    <sample id="249">Le frasi nel dominio accettabile sono state perturbate preservando la struttura rilevante ma aggiungendo rumore all'input.</sample>
    <sample id="250">Avere una valutazione dimensionale significa valutare un modello di conversazione AI in base a diversi aspetti o dimensioni specifici della qualità del dialogo, piuttosto che fornire una valutazione complessiva generica. Questo approccio permette di identificare e misurare con precisione le prestazioni del modello in aree specifiche, come la pertinenza delle risposte, la coerenza interna e con il partner, la capacità di empatizzare, e la presenza di errori tematici. La valutazione dimensionale, come quella proposta da ABC-Eval, fornisce una comprensione più dettagliata e affidabile dei punti di forza e di debolezza del modello, consentendo miglioramenti mirati e una comparazione più accurata tra diversi modelli di conversazione AI.</sample>
    <sample id="251">Gli autori dell'articolo sono affiliati all'Università della Scienza e della Tecnologia della Cina (University of Science and Technology of China).</sample>
    <sample id="252">Il lavoro presentato, intitolato "U-CREAT: Unsupervised Case Retrieval using Events extraction", è stato sviluppato da un team di studenti dell'IIT Kanpur, guidati da Sai Kiran Tanikella. L'obiettivo è migliorare il processo di Prior Case Retrieval (PCR), che consiste nel recuperare casi legali rilevanti da un pool di documenti dati, in base a un documento di query. Questo compito è particolarmente impegnativo a causa dell'aumento del volume di casi legali.

Il team ha creato due contributi principali: il dataset IL-PCR, un nuovo benchmark per i compiti di PCR, e la pipeline U-CREAT, che utilizza tecniche di apprendimento non supervisionato e un approccio basato sugli eventi. IL-PCR è un dataset di 7.070 casi legali indiani con una media di 6,775 citazioni per documento di query, superando il dataset COLIEE'21 in termini di dimensioni e complessità.

La pipeline U-CREAT estrae eventi dai documenti di query e candidati, utilizzando una tecnica di parsing della dipendenza con spacing. Gli eventi sono rappresentati come triple soggetto-verbo-oggetto, che vengono poi confrontati per determinare la rilevanza. La pipeline include tre fasi: pre-elaborazione, parsing della dipendenza e post-elaborazione.

Gli esperimenti condotti dal team hanno dimostrato che i modelli basati sugli eventi, in particolare l'Event Filtered Documents, ottengono risultati significativamente migliori rispetto ai modelli tradizionali, con tempi di inferenza più rapidi e punteggi F1 più alti. U-CREAT supera gli approcci esistenti, incluso il recente metodo supervisionato MTFT-BERT, diventando lo stato dell'arte per il compito di recupero di documenti legali.</sample>
    <sample id="253">Il lavoro presentato, intitolato "DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media", è il risultato di una collaborazione tra ricercatori messicani e spagnoli. L'obiettivo è sviluppare un modello di intelligenza artificiale in grado di rilevare segni di disturbi mentali attraverso l'analisi dei post sui social media. I disturbi mentali sono sindromi psicologiche che causano disagio e disabilità, influenzando pensiero, emozioni, umore e comportamento.

Il modello proposto, DisorBERT, utilizza la tecnica di **domain adaptation** per adattare un linguaggio generale (ad esempio, BERT, basato su dati di Wikipedia e Google Books) a un dominio specifico, come i social media o i disturbi mentali. Questo permette di migliorare le prestazioni del modello quando i dati annotati sono insufficienti. DisorBERT integra informazioni da Reddit e risorse legate alla salute mentale, utilizzando un **lexicon** per guidare il processo di mascheramento durante l'addestramento.

I risultati mostrano che DisorBERT è in grado di identificare parole e frasi rilevanti per i disturbi mentali, come "ansia" e "medicazione", con maggiore precisione rispetto a modelli di base. L'approccio combina **double domain adaptation** e **guided masking** per bilanciare precisione e recall, ottenendo risultati migliori rispetto a MentalBERT, un modello addestrato su grandi quantità di dati.

In futuro, i ricercatori intendono esplorare l'uso di risorse lessicali diverse e l'integrazione di dati clinici per migliorare ulteriormente l'accuratezza del modello.</sample>
    <sample id="254">Il lavoro di ricerca presentato si intitola "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction". L'obiettivo è migliorare l'estrazione delle relazioni tra entità in un documento, riducendo il rumore nei dati distantemente supervisionati (DS). I metodi precedenti si basavano su grandi corpus annotati manualmente, costosi e laboriosi. I dati DS, invece, contengono rumore che può compromettere le prestazioni dei modelli. L'approccio proposto introduce una stima dell'incertezza per identificare i falsi pseudo-label, migliorando la qualità dei dati. Si utilizza il Monte Carlo dropout per modellare l'incertezza del modello, ma si adatta per gestire relazioni sovrapposte, un problema non risolto in precedenza. Si propone inoltre una strategia di re-labeling con soglie dinamiche di incertezza per classi e un approccio di training multi-fase per iterare e migliorare i risultati. I risultati dimostrano che il framework supera le baseline esistenti su dataset pubblici, migliorando significativamente le prestazioni. In sintesi, il lavoro contribuisce con una metodologia innovativa per ridurre il rumore nei dati DS, gestire relazioni sovrapposte, affrontare il problema delle classi a coda lunga e ottenere risultati superiori.</sample>
    <sample id="255">La forma del prompting si rivela importante principalmente nei casi di **zero-shot e one-shot prompting**, dove la struttura e il contesto fornito al modello hanno un impatto significativo sulle prestazioni. Tuttavia, con **five-shot prompting**, la forma del prompting ha un'influenza trascurabile, poiché sono gli esempi forniti a determinare il risultato.</sample>
    <sample id="257">Gli autori hanno valutato quattro modelli di dialogo di ultima generazione.</sample>
    <sample id="258">Il video presenta il lavoro di ricerca "Can Large Language Models Be an Alternative to Human Evaluation?" che esplora l'uso dei grandi modelli linguistici (LLM) per valutare la qualità del testo in elaborazione del linguaggio naturale (NLP). L'idea è di utilizzare istruzioni naturali per guidare i modelli LLM nella valutazione di campioni di testo, imitando il processo di valutazione umana. Nonostante esistano lavori correlati, come G-Eval, l'approccio proposto è innovativo poiché nessuno ha precedentemente esplorato l'uso di LLM per la valutazione. La motivazione principale è superare i limiti della valutazione umana, come l'instabilità e la difficoltà di riproducibilità.

L'esperimento si basa sulla valutazione di storie generate da GPT-2 o da umani, utilizzando quattro modelli LLM (T0, InstructGPT Curie, InstructGPT Davinci e ChatGPT) per attributi come grammatica, coerenza, piacevolezza e pertinenza. I risultati mostrano che, mentre alcuni modelli non distinguono significativamente tra storie umane e generate da GPT-2, Davinci e ChatGPT mostrano una chiara preferenza per il testo umano, allineandosi ai giudizi degli insegnanti di inglese, considerati esperti. Questo suggerisce che i LLM possono essere un'alternativa valida alla valutazione umana in alcuni contesti. Il paper approfondisce ulteriori domande, come la coerenza delle valutazioni, l'impatto delle istruzioni e i vantaggi rispetto alla valutazione umana.</sample>
    <sample id="259">Yusen Zhang della Penn State University presenta il lavoro "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations". Lo scopo è costruire rappresentazioni semantiche di query in più lingue naturali e tradurle in diverse rappresentazioni, come SQL, Lambda Calculus o FunQL. I modelli esistenti si concentrano su lingue e rappresentazioni specifiche, con carenze come l'assenza del cinese o del calcolo Lambda. XSemPLR risolve questo problema offrendo un dataset unificato che include 9 dataset in vari domini, 5 compiti di parsing semantico, 8 rappresentazioni e 22 lingue in 15 famiglie linguistiche.

Per valutare il benchmark, vengono considerati 6 scenari: Translate-Test (tradotto con Google Translate), Monolingual (stessa lingua), Monolingual Few-shot (10% dati), Multilingual (addestramento multilingue), Cross-lingual Zero-shot e Few-shot. I risultati mostrano che i modelli Encoder-Decoder ottengono le migliori prestazioni su tutti i dataset, seguiti da Encoder-PTR. L'addestramento multilingue migliora le prestazioni, tranne per l'inglese. Il divario di prestazione tra lingue viene ridotto con il Few-shot rispetto allo Zero-shot. Inoltre, i modelli come Codex e BLOOM non sono ancora sufficienti per il parsing semantico cross-linguistico. XSemPLR è un benchmark unificato che evidenzia risultati interessanti per migliorare i modelli di parsing semantico.</sample>
    <sample id="260">L'articolo è firmato da Jingwei Yi, quindi si presume che sia l'unico autore coinvolto.</sample>
    <sample id="261">Un buon pianificatore dovrebbe possedere due qualità ideali:  
1. **Semantica Completa**: I piani devono essere coerenti e comprendere tutti gli aspetti necessari per raggiungere l'obiettivo.  
2. **Fedeltà alle Restrizioni**: I piani devono rispettare e aderire alle specifiche restrizioni imposte, garantendo che siano ragionevoli e conformi ai vincoli dati.</sample>
    <sample id="262">L'articolo è stato scritto da Siyu Yuan della Fudan University, quindi ci sono **1 autore** coinvolto.</sample>
    <sample id="263">Il lavoro presentato si concentra sulla mitigazione dei bias di etichettatura nell'apprendimento in-context (In-C) dei grandi modelli linguistici. L'In-C è un paradigma popolare ma instabile, influenzato da scelte progettuali come l'ordine e la selezione degli esempi in-context. Studi precedenti hanno evidenziato che l'instabilità della ricerca in In-C deriva da questi fattori, introducendo bias nelle previsioni dei modelli. Tuttavia, non esisteva una classificazione sistematica dei bias esistenti né un metodo per rilevare e mitigare efficacemente questi problemi. Questo lavoro colma questa lacuna, concentrandosi sulla classificazione dei task di classificazione testuale.

Gli autori identificano tre tipi di bias: **vanilla-label bias** (preferenza del modello per etichette non contestualizzate), **context-label bias** (influenza del contesto) e un nuovo tipo di bias, il **domain-label bias** (effetto del corpus di task sulle previsioni del modello). Attraverso esperimenti, dimostrano che il corpus di task può influenzare significativamente le previsioni del modello, anche con parole casuali in-domain.

Per affrontare questi bias, propongono un metodo di **calibrazione contestuale** (domain-context calibration), che utilizza testi quasi "content-free" (ad esempio, parole casuali in-domain) per stimare e correggere i bias. Questo approccio supera i limiti delle precedenti tecniche, che utilizzavano token predefiniti come "not available". I risultati mostrano che la calibrazione contestuale migliora significativamente le prestazioni dell'In-C, specialmente nei task con un forte domain-label bias. Inoltre, l'uso di parole casuali in-domain si rivela più efficace rispetto a parole casuali in inglese, dimostrando l'importanza di considerare il contesto specifico del task.

In sintesi, il lavoro propone una classificazione sistematica dei bias in In-C, identifica un nuovo tipo di bias e sviluppa una metodologia di calibrazione che migliora le prestazioni dei modelli linguistici.</sample>
    <sample id="264">Il mio nome è Lin Wang, sono uno studente di dottorato presso l'Università di Zhejiang, in Cina. La mia presentazione riguarda il mio lavoro intitolato "TAVT: Towards Transferable Audio-Visual Text Generation". Attualmente, i compiti di generazione di testo uni-model, come la traduzione automatica e la generazione di didascalie per immagini, hanno beneficiato enormemente dall'addestramento su larga scala e dalla capacità dei modelli. Tuttavia, per i compiti di generazione di testo multimodale, come la generazione di testo audio-visivo, l'annotazione dei dati è più complessa e costosa, e i lavori esistenti soffrono di un degrado significativo a causa delle variazioni nelle condizioni di costruzione dei dati.

Per superare questa limitazione, proponiamo un nuovo approccio chiamato Generazione di Testo Audio-Visivo Trasferibile (TAVT). La sfida principale di questo compito è gestire i cambiamenti multimodali nei domini, come lo stile visivo, l'energia audio, ecc. Abbiamo osservato che, per lo stesso evento, il contenuto visivo può cambiare significativamente con la modifica dello stile dell'immagine o dell'angolo di ripresa, mentre il contenuto audio, come il ritmo e l'energia, ha un impatto minore sulla comprensione dell'evento.

Il nostro framework TAVT è modulare e composto da tre componenti: una rete di mappatura meta-audio-visiva, un encoder e un generatore di modelli linguistici audio-visivi, e un apprendimento contrastivo controfattuale (DCLL). Il primo componente mappa i concetti visivi tra i domini in uno spazio semantico uditivo unificato, affrontando i cambiamenti nella distribuzione semantica. Il secondo componente utilizza un encoder e un generatore basati su trasformatori, con un coefficiente alfa per valutare il contributo delle diverse modalità a ciascuna parola. Infine, il DCLL costruisce segnali di supervisione dettagliati dai risultati controfattuali per ottimizzare direttamente gli allineamenti visivo-testuali.

I risultati sperimentali dimostrano che TAVT supera tutti i modelli di confronto su tutti i metrici, sia nei contesti cross-dataset che cross-domain, e mantiene prestazioni elevate anche nei domini a risorse limitate.</sample>
    <sample id="265">Il nome della relatrice è Vasudha.</sample>
    <sample id="266">Gli autori dell'articolo sono affiliati all'Università di Varsavia.</sample>
    <sample id="268">Gli errori più comuni di PaLM sono **errori di omissione**, ovvero la tendenza a produrre traduzioni più fluenti eliminando parti della frase sorgente, a volte a scapito dell'accuratezza.</sample>
    <sample id="269">Ciao, io sono James Finch. E io sono Sarah Finch. E oggi vi racconteremo di ABC-Eval, un nuovo approccio dimensionale alla valutazione dell'intelligenza artificiale conversazionale. Questo lavoro è stato realizzato dal laboratorio di NLP di Emory guidato dal professor Jinho Choi presso l'Università di Emory e in collaborazione con Amazon Alexa AI. Quindi, supponiamo che abbiate appena sviluppato un modello di dialogo e vogliate vedere quanto si confronta con lo stato dell'arte attuale. La pratica comune è utilizzare la valutazione umana, ad esempio chiedendo ai giudici umani di selezionare quale delle due conversazioni sia migliore o di valutare le conversazioni su una scala Likert. Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto, potreste voler valutare più dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più fine. Un approccio è semplicemente chiedere ai giudici umani di valutare diversi aspetti della qualità del dialogo, come la pertinenza delle risposte del modello utilizzando metodi comparativi esistenti o scale Likert. Tuttavia, crediamo che esista una strategia più precisa e affidabile per la valutazione dimensionale del dialogo. Il nostro approccio tenta di ridurre la soggettività della valutazione umana annotando esplicitamente se ogni risposta del modello esprime certi comportamenti, come rispondere con informazioni irrilevanti o contraddire se stesso o il proprio interlocutore. Chiamiamo questo approccio l'annotazione dei comportamenti nella chat o ABC-Eval in breve. Abbiamo sviluppato questo metodo per coprire in modo completo i comportamenti dei modelli di chat che sono stati suggeriti per influenzare la qualità della chat nella letteratura recente. ABC-Eval è in grado di misurare i tassi con cui i modelli di chat commettono vari errori tematici. Ad esempio, ABC-Eval misura il numero di turni in cui un modello di chat ignora il proprio interlocutore o dice qualcosa di irrilevante, contraddice se stesso o il proprio interlocutore, allucina fatti errati o viola la conoscenza del senso comune, e quando il modello riesce o fallisce nel mostrare empatia. Per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su 100 conversazioni uomo-bot per modello utilizzando ABC-Eval. Per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti di coppia a livello di dialogo. Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la pratica standard per valutare i modelli di chat lungo più dimensioni. Dalla nostra analisi di questi risultati di valutazione, abbiamo scoperto che le etichette dei comportamenti ABC-Eval sono complessivamente più affidabili delle etichette raccolte dai metodi esistenti, come misurato dall'accordo tra annotatori su 100 conversazioni doppiamente etichettate. Inoltre, le etichette ABC-Eval sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come mostrato da questa semplice analisi di regressione lineare. Ad esempio, potete vedere come la misurazione della proporzione di turni con auto e contraddizioni del partner spieghi il 5% e il 10% della qualità della conversazione, rispettivamente, mentre i punteggi di coerenza Likert medi spiegano solo il 4% o meno. Infine, abbiamo controllato se ogni metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare a passi. Potete vedere come la combinazione di tutte le metriche ABC-Eval spieghi oltre il 25% della qualità della conversazione, e rimuovendo le metriche una alla volta, la maggior parte di esse risulta nella perdita di una discreta quantità di informazioni sulla qualità. D'altra parte, la combinazione di tutte le metriche Likert a livello di turno spiega molto meno della qualità, e meno di queste metriche portano informazioni uniche. Queste metriche ABC-Eval affidabili, informative e distinte ci consentono di valutare l'intelligenza artificiale conversazionale con una risoluzione maggiore rispetto a quanto i metodi precedenti siano in grado di raggiungere. Potete vedere che nei risultati del nostro esperimento rimangono ancora diverse sfide e sono state quantificate con precisione. Ad esempio, i bot che abbiamo testato hanno violazioni del senso comune in circa il 20% delle loro risposte. Producono informazioni irrilevanti in circa il 15% delle risposte, e si contraddicono o contraddicono il proprio interlocutore circa il 10% delle volte. Con il rapido ritmo di miglioramento nel campo, molti di questi tassi di errore potrebbero diminuire nei nuovi modelli rilasciati da quando abbiamo condotto la nostra valutazione. Tuttavia, questa è ancora di più la ragione per perseguire metriche di valutazione affidabili e precise per confrontare i modelli. Speriamo che ABC-Eval possa essere sfruttato da altri nel campo come un passo significativo in questa direzione. E non vediamo l'ora di vedere come l'intelligenza artificiale conversazionale avanzerà nei prossimi mesi e anni. Grazie per aver guardato.</sample>
    <sample id="270">Gli autori dell'articolo sono James Finch e Sarah Finch, e il loro lavoro è stato condotto dalla Emory NLP Lab, guidata dal Professor Jinho Choi presso l'Università di Emory, in collaborazione con Amazon Alexa AI.</sample>
    <sample id="271">In this article, **CFT** stands for **Continuous Fine-Tuning**. It refers to a method where a model is allowed to fine-tune on clean validation samples after the initial training phase, which can improve its performance. The article highlights that allowing CFT can achieve the same performance as more complex Weakly Supervised Learning (WSL) methods without the need for additional computational resources.</sample>
    <sample id="272">L'articolo è stato scritto da 8 autori.</sample>
    <sample id="273">Ciao, mi chiamo Kayo Yin e presenterò il nostro lavoro intitolato "Quando la traduzione richiede il contesto? Un'esplorazione multilingue basata sui dati". Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig. Quindi molte traduzioni dipendono dal contesto. Ad esempio, come tradurremmo "mole" in questa frase? Beh, se la frase precedente fosse "Le cose potrebbero diventare pericolose se i ministri scoprissero", allora "mole" si riferisce a uno spia. Ma se la frase precedente fosse "Potrebbe essere qualcosa di serio, dottore?", allora "mole" si riferisce a un neo. Quindi, a seconda del contesto, il significato della parola cambia e quindi cambia anche la sua traduzione. Tuttavia, valutare quanto bene i modelli possano tradurre casi come questo è piuttosto difficile. In primo luogo perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus come BLEU incapaci di catturare queste traduzioni. E alcune persone hanno suggerito valutazioni mirate sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e insiemi limitati di lingue poiché di solito si basano sulla conoscenza del dominio e sulla curazione umana. In questo lavoro, cerchiamo di rispondere a queste due domande. Primo, quando la traduzione richiede il contesto? E secondo, quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipenda dal contesto durante la traduzione. Nel lavoro precedente, abbiamo introdotto CXMI come misura per l'uso del contesto da parte dei modelli di traduzione automatica. E questo è fatto misurando quanto il contesto C fornisca informazioni sull'obiettivo Y, dato il sorgente X. Potete pensare a CXMI come alle informazioni guadagnate dal dare contesto al modello. In questo lavoro, estendiamo CXMI a P-CXMI che può misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo pensare a parole con alto P-CXMI come a quelle che richiedono il contesto per la traduzione. Ora analizziamo le parole con alto P-CXMI per cercare schemi tra queste parole. E svolgiamo la nostra analisi su trascrizioni di TED talk che sono state tradotte dall'inglese a 14 lingue diverse. Eseguiamo la nostra analisi a tre livelli diversi. Primo, guardiamo ai tag delle parti del discorso che hanno un alto P-CXMI medio. E questo ci permette di trovare, ad esempio, i pronomi duali in arabo che hanno un P-CXMI relativamente alto. E questo può essere spiegato perché l'inglese non ha pronomi duali, quindi è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo. E allo stesso modo, troviamo che certe lingue richiedono anche il contesto quando vogliamo scegliere la forma verbale appropriata. Poi guardiamo agli elementi del vocabolario che hanno un P-CXMI alto medio su tutte le sue diverse occorrenze. E questo ci aiuta a identificare casi come quello qui, dove in cinese è necessario il contesto per tradurre i nomi propri per assicurarsi di usare la stessa traduzione all'interno del documento. E allo stesso modo, troviamo che il contesto è importante per tradurre nella giusta formalità. E infine, guardiamo a diversi token individuali che hanno un alto P-CXMI. E questo ci permette di identificare fenomeni che non possono essere realmente catturati dalla parola stessa, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione di ellissi. Ora usiamo i nostri risultati dall'analisi per progettare un benchmark per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni del discorso che abbiamo identificato, creiamo tagger per identificare automaticamente le parole che appartengono al fenomeno. E abbiamo chiamato il nostro tagger il tagger Multilingue Conscio del Discorso, o MuDA. Possiamo anche notare che lingue diverse hanno diverse proporzioni di questi fenomeni del discorso. Poi usiamo il tagger MuDA, applicando il tagger su un corpus parallelo che vogliamo usare per la valutazione e applichiamo le nostre metriche di traduzione di scelta su esempi dipendenti dal contesto che il tagger MuDA ha identificato. E infine, usiamo il nostro benchmark così come altre metriche per valutare diversi modelli a livello di documento nella traduzione automatica. Prima di tutto, quando usiamo metriche a livello di corpus: quindi per BLEU, troviamo che i modelli agnostici al contesto hanno la migliore performance. Ma poi se usiamo COMET, i modelli consapevoli del contesto hanno le migliori prestazioni. E se usiamo la misura f-word, allora i modelli con e senza contesto hanno performance confrontabili. Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se usiamo solo metriche a livello di corpus. Ora, usiamo il benchmark MuDA per valutare i modelli e troviamo che i modelli consapevoli del contesto sono significativamente più accurati dei modelli che non usano il contesto per certi fenomeni del discorso come la formalità e la coesione lessicale. Ma questi modelli non sono molto migliori dei modelli che non usano il contesto su altri fenomeni come ellissi, pronomi e forma verbale. Questo suggerisce in qualche modo dove avremmo bisogno di vedere più progressi per la traduzione a livello di documento. Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che DeepL è solitamente più accurato di Google Translate per la traduzione a livello di documento. Per riassumere, eseguiamo un'analisi guidata dai dati su 14 coppie di lingue per identificare quando le traduzioni richiedono il contesto e poi usiamo i nostri risultati per costruire un benchmark per la traduzione automatica a livello di documento che può aiutarci a identificare quali fenomeni del discorso i modelli possono gestire bene o no, e quali sistemi di traduzione sono bravi nella traduzione a livello di documento. Grazie mille per l'attenzione. Ci vediamo a Toronto.</sample>
    <sample id="274">Il nome della relatrice o del relatore è Yusen Zhang.</sample>
    <sample id="276">Ananya e Vignesh presentano il loro lavoro, "IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation Metrics for Indian Languages", che affronta il problema della mancanza di studi sull'evaluazione delle traduzioni in lingue diverse dall'inglese. I ricercatori hanno creato un dataset di 7.000 campioni di traduzioni in cinque lingue indiane (Tamil, Malayalam, Hindi, Marathi e Gujarati) utilizzando sette modelli di traduzione. Le traduzioni sono state valutate da annotatori bilingue esperti, che hanno classificato gli errori in base al tipo e alla gravità e hanno assegnato un punteggio complessivo. Lo studio confronta diverse metriche di valutazione, tra cui overlap-based, embedding-based e COMET-metric variants, e identifica COMET-metric variants come le più correlate ai punteggi umani. Inoltre, i ricercatori hanno fine-tuned il modello COMET utilizzando il loro dataset, creando IndicCOMET MQM, che si è dimostrato più performante rispetto al modello base COMET. Il lavoro sottolinea l'importanza di considerare le specificità delle lingue indiane nella valutazione delle traduzioni e fornisce un dataset pubblicamente disponibile per ulteriori ricerche.</sample>
    <sample id="277">Il nuovo metodo non ha un nome specifico menzionato nel testo.</sample>
    <sample id="278">L'autore descrive il metodo delle "parole contrassegnate" come un approccio che si basa sul concetto sociolinguistico di "markedness", secondo cui i gruppi diversi dal gruppo di riferimento (immaginato come non marcato) sono linguisticamente contrassegnati. Questo metodo permette di identificare le parole che distinguono i gruppi contrassegnati da quelli non contrassegnati, utilizzando la tecnica delle "Fightin' Words" che calcola i log-odds ratio per evidenziare le parole più significative per ciascun gruppo contrassegnato. In questo modo, il metodo permette di rivelare i modelli e gli stereotipi nascosti dietro le descrizioni apparentemente positive dei gruppi contrassegnati, contribuendo a una comprensione più profonda delle dinamiche di potere e di discriminazione legate al linguaggio.</sample>
    <sample id="279">Gli autori dell'articolo sono studenti di dottorato dell'Università di Washington.</sample>
    <sample id="280">**Abstract:**  
The paper introduces *MultiEMO*, an attention-based correlation-aware multimodal fusion framework for emotion recognition in conversations (ERC). Existing methods often fail to effectively exploit multimodal information, struggle with minority emotion classes, and struggle to distinguish semantically similar emotions. *MultiEMO* addresses these challenges by proposing three key contributions:  
1. **VisExtNet**: A novel visual feature extractor that focuses on facial expressions rather than redundant scene-related information, using MTCNN and VGGFace2 pre-trained ResNet-101.  
2. **MultiAttn**: A multimodal fusion model based on bidirectional multi-head cross-attention layers, integrating textual, audio, and visual modalities to capture complementary correlations.  
3. **Sample-Weighted Focal Contrastive Loss (SWFC)**: A loss function that prioritizes hard-to-classify minority classes and maximizes inter-class distances for better distinguishing semantically similar emotions.  
Experiments on MELD and IEMOCAP datasets demonstrate *MultiEMO*'s state-of-the-art performance, particularly in minority and semantically similar emotion classes. Limitations include challenges in speaker identification, batch size requirements for SWFC, and persistent performance gaps for minority emotions. *MultiEMO* represents a significant advancement in ERC by addressing key unresolved issues in multimodal fusion and emotion classification.</sample>
    <sample id="281">Il lavoro "When Does Translation Require Context? A Data-driven, Multilingual Exploration" esplora l'importanza del contesto nella traduzione automatica, analizzando 14 coppie linguistiche. I ricercatori hanno sviluppato un indicatore, il Pointwise CXMI, per misurare quanto il contesto influenzi la traduzione a livello di parola o frase. Questo approccio ha permesso di identificare cinque fenomeni linguistici che richiedono contesto: dualità dei pronomi, scelta dei verbi, formalità, coesione lessicale e risoluzione di ellissi. Utilizzando un tagger chiamato MuDA, i ricercatori hanno creato un benchmark per valutare i modelli di traduzione automatica a livello di documento. I risultati mostrano che i modelli contest-aware superano quelli context-agnostic in alcuni fenomeni, ma non in altri. Inoltre, il benchmark evidenzia che DeepL è generalmente più accurato di Google Translate nella traduzione automatica di documenti. In sintesi, lo studio dimostra che il contesto è cruciale per la traduzione automatica e che i modelli devono essere valutati in base a indicatori specifici per identificare le loro debolezze e punti di forza.</sample>
    <sample id="282">Il lavoro presentato a ACL 2023, intitolato "StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing", affronta il problema del trasferimento di stile a livello di storia in testi non paralleli. A differenza degli studi precedenti, che si concentravano su livelli token o frase, StoryTrans si concentra sul livello della storia e del discorso, cruciale per imitare lo stile dell'autore. Le principali sfide includono l'imitazione delle preferenze linguistiche dell'autore a livello di discorso e la difficoltà di trasferire stili specifici a contenuti diversi.

Per superare queste difficoltà, StoryTrans combina rappresentazioni del discorso da testi sorgente con embedding di stile apprendibili per generare testi in stili target. Il modello utilizza un obiettivo di training specifico per ridurre le caratteristiche stilistiche dalle rappresentazioni del discorso e preservare il contenuto. Il processo di generazione è diviso in due fasi: prima si trasferisce il testo sorgente con parole chiave di contenuto mascherate, poi si genera il testo completo incorporando queste parole chiave.

Il training è suddiviso in due fasi: nella prima, si utilizzano perdite di auto-ricostruzione, disentanglement, ordine delle frasi e classificazione dello stile. Nella seconda fase, si riempiono i contenuti specifici dello stile e si rimuovono i token mascherati. I risultati, sia automatici che manuali, confermano l'efficacia di StoryTrans, che supera le baseline in termini di controllo dello stile e preservazione del contenuto. Inoltre, StoryTrans può arricchire la trama e mantenere la semantica sorgente, superando i limiti di modelli come StyleLM.</sample>
    <sample id="283">La prima struttura di dipendenza simmetrica menzionata è quella in cui **tutti i conjuncts sono heads della struttura coordinata**, come proposto nella **Word Grammar di Hudson**.</sample>
    <sample id="284">Il paper presentato, intitolato "FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction", introduce una nuova metodologia per migliorare l'estrazione universale di informazioni (UIE) basata su span. Il problema principale degli attuali modelli span-based UIE è l'eccessiva dipendenza dalle posizioni precise dei confini degli span annotati, che introduce ambiguità nella definizione dei confini stessi. Per risolvere questo, il paper propone l'uso di confini di span fuzzy, che permettono una maggiore flessibilità nell'identificazione degli span.

Inoltre, il paper affronta il mismatch tra l'estrazione di feature dei Transformers e l'estrazione di informazioni, proponendo un'attenzione adattiva per la decisione di estrazione degli span. Questa attenzione adattiva modella il confine dello span come una distribuzione continua di probabilità corrette in un intervallo specifico, con R-min e R-max che rappresentano l'inizio e la fine del confine fuzzy.

Il paper introduce anche una perdita fuzzy per gli span e una attenzione fuzzy per gli span, che permettono di ottenere una distribuzione di attenzione più ragionevole per l'estrazione degli span. Gli esperimenti condotti su tre compiti principali di estrazione di informazioni (NER, RE, ASTE) dimostrano che FSUIE ottiene risultati significativamente migliori rispetto ai modelli base UIE, specialmente su dataset di piccola scala.

In sintesi, il paper propone una nuova metodologia per l'estrazione di informazioni che utilizza confini fuzzy e attenzione adattiva, ottenendo risultati eccellenti in una vasta gamma di compiti di estrazione di informazioni.</sample>
    <sample id="285">Il lavoro "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework" presentato da Mingqi Gao della Peking University affronta il problema degli errori fattuali nei riassunti generati da modelli di dialogo. Gli errori fattuali sono comuni sia nei riassunti generati automaticamente che in quelli di riferimento, e ci sono due approcci principali per risolverli: l'introduzione di obiettivi legati alla fattualità durante l'addestramento dei modelli di riassunzione, e la progettazione di un modello di correzione degli errori fattuali (FEC) indipendente.

Tuttavia, le valutazioni attuali dei modelli FEC presentano due principali carenze: le metriche di fattualità come FactCC e DAE forniscono un punteggio complessivo vago e non affidabile, e permettono ai modelli FEC di ignorare il contenuto del riassunto originale, generando semplicemente un riassunto diverso ma più corretto, senza vera correzione.

Per superare queste limitazioni, gli autori propongono l'uso di correzioni di riferimento annotate manualmente, che riflettono le esigenze di base della correzione degli errori fattuali: correggere gli errori fattuali nel riassunto originale con il minor numero possibile di sostituzioni, inserimenti ed eliminazioni, mantenendo un riassunto fluido e non ridondante.

Il nuovo framework di valutazione, basato su ERRANT, include tre passaggi: allineamento, classificazione e confronto. Gli esperimenti con diversi modelli FEC mostrano che l'addestramento con riassunti di riferimento da dataset di dialogo produce i migliori risultati, evidenziando la necessità di cambiare i metodi di valutazione dei modelli FEC. Inoltre, l'uso di dati umani corretti durante l'addestramento dei modelli FEC per il dialogo può migliorare le loro prestazioni, e la combinazione di dati umani e sintetici rappresenta una direzione promettente. Infine, i modelli FEC attuali faticano a correggere errori come aggiunte e non riescono ad affrontare errori di attributi, modalità e collegamenti.</sample>
    <sample id="286">Il nome del relatore è James Finch.</sample>
    <sample id="287">Cinque autori sono coinvolti nell'articolo: Javad Hosseini, Filip Radlinski, Silvia Pareti, Annie Louis e l'autore non menzionato che ha presentato il contenuto.</sample>
    <sample id="288">Per testare i fenomeni sintattici, possono essere utilizzati insiemi di dati come **BLiMP**, **SyntaxGym**, e **CrowS pairs**. Questi dataset contengono coppie di frasi grammaticali e non grammaticali, permettendo di valutare l'abilità dei modelli linguistici nel distinguere tra strutture sintattiche accettabili e inaccettabili. Inoltre, è possibile utilizzare **Wikipedia** o altri dataset correlati per testare l'impatto del contesto sulla robustezza dei giudizi di accettabilità del modello.</sample>
    <sample id="290">Le abbreviazioni dei cinque metodi per la prima domanda di ricerca sono:

1. WSL (Weakly Supervised Learning)
2. FTw (Fine-tuning with clean validation samples)
3. COSINE (un metodo WSL specifico menzionato nel testo)
4.... (non specificato nel testo)
5.... (non specificato nel testo)

Nota: Il testo non fornisce le abbreviazioni per tutti e cinque i metodi menzionati nella prima domanda di ricerca.</sample>
    <sample id="291">Il modello DrBERT viene valutato su 11 attività downstream in ambito biomedico e clinico in francese, tra cui:

1. **Named Entity Recognition (NER)**
2. **Classification**
3. **Part-of-Speech Tagging**
4. **Question Answering**

Queste attività sono scelte per valutare le prestazioni del modello in compiti specifici del dominio biomedico e clinico.</sample>
    <sample id="294">CamemBERT viene inizialmente addestrato sui dati di lingua francese.</sample>
    <sample id="295">Il nome del relatore è Adam Przepiórkowski.</sample>
    <sample id="296">Valerio Basile presenta una ricerca collaborativa tra l’Università di Torino e Amazon Alexa, focalizzata sull’ironia nel contesto del Natural Language Understanding (NLU). L’approccio tradizionale si basa su dati annotati manualmente, ma si è osservato che l’idea di una “verità unica” (ground truth) è limitata. Lo studio si concentra sull’ironia, un fenomeno complesso e pragmatico, e mira a sviluppare modelli NLU che forniscano output più informativi, non solo etichette binarie (ironico/non ironico).

Il team ha creato il corpus EPIC (English Perspectivist Irony Corpus), raccogliendo 300 brevi conversazioni da social media (Reddit e Twitter) in cinque varietà di inglese, annotate da 74 esperti tramite la piattaforma Prolific. Ogni conversazione ha ricevuto in media 5 annotazioni. I risultati mostrano differenze significative tra gli annotatori, influenzate da fattori come genere, età, nazionalità e geografia.

I ricercatori hanno sviluppato modelli “perspective-aware”, addestrati su dati specifici degli annotatori, ottenendo maggiore sicurezza nelle previsioni rispetto ai modelli tradizionali. Analisi approfondite hanno rivelato che le generazioni vicine (ad esempio, genitori e figli) e le aree geografiche vicine (come Regno Unito e Irlanda) mostrano maggiori disaccordi nell’interpretazione dell’ironia. Lo studio evidenzia l’importanza di considerare le prospettive individuali nella progettazione di modelli NLU.</sample>
    <sample id="297">Il progetto "From Dogwhistles to Bullhorns" si concentra sull'analisi e la classificazione delle retoriche codificate, in particolare i "dogwhistles", termini che trasmettono un messaggio a un gruppo specifico (in-group) attraverso un linguaggio apparentemente innocuo per un pubblico esterno (outgroup). Un esempio è il termine "cosmopolitan", usato da Senator Josh Hawley per riferirsi in modo velato agli ebrei, un caso di antisemitismo codificato.

La ricerca sviluppa una **tipologia e un glossario** con oltre 340 termini e simboli, suddivisi per categoria (razzisti, transfobici, antisemiti) e caratterizzati da registro (formale o informale), persona (il gruppo o l'ideologia implicita) e tipo (se aggiungono un'implicatura o segnalano una persona).

Attraverso un'analisi storica dei discorsi politici statunitensi, si evidenzia come l'uso di dogwhistles sia aumentato dopo la Civil Rights era, in linea con la "Southern Strategy" repubblicana, e sia più associato al conservatorismo.

I **modelli linguistici**, come GPT-3, sono stati testati per riconoscere i dogwhistles, con risultati variabili: funzionano meglio con termini formali e con prompt specifici che includono definizioni o indizi nascosti.

Infine, si dimostra come i dogwhistles possano eludere i sistemi di moderazione online, poiché le frasi contenenti dogwhistles vengono considerate meno tossiche rispetto a quelle con insulti diretti. Questo studio sottolinea l'importanza di comprendere i dogwhistles per contrastare la retorica d'odio e migliorare i meccanismi di moderazione.</sample>
    <sample id="298">I risultati che hanno portato alla conclusione che la deriva temporale è la causa principale della perdita di prestazioni sono stati ottenuti attraverso un esperimento in cui alcuni modelli sono stati ri-addestrati o continuati a pre-addestrati con dati più recenti. Si è osservato che la performance si degrada con un maggiore divario temporale tra i dati di addestramento e quelli di test, confermando l'ipotesi della deriva temporale come causa principale della perdita di prestazioni.</sample>
    <sample id="299">Il lavoro di Michalis Korakakis e Andreas Vlachos si concentra sul miglioramento della robustezza dei modelli di Natural Language Inference (NLI) attraverso un approccio di training minimax. I modelli NLI, pur raggiungendo risultati all'avanguardia su diversi benchmark, spesso apprendono scorciatoie, ovvero correlazioni spurie tra gli attributi dell'input e le etichette, che li rendono fragili su test out-of-distribution. I metodi esistenti per mitigare queste scorciatoie richiedono spesso un modello ausiliario con conoscenze specifiche, che non sempre sono disponibili. Inoltre, questi metodi spesso presuppongono che il modello principale e l'ausiliario sfruttino gli stessi tipi di scorciatoie, il che non sempre accade.

Il nuovo approccio proposto mira a ridurre la dipendenza dai modelli NLI verso le scorciatoie, enfatizzando l'apprendimento di esempi "difficili" che contrastano con le correlazioni spurie presenti negli esempi "facili". La chiave è ottenere una distribuzione di pesi degli esempi che dia priorità a questi esempi difficili, sotto-rappresentati.

Il metodo utilizza un training minimax: il modello principale cerca di minimizzare la perdita del compito NLI, mentre l'ausiliario cerca di massimizzare questa perdita generando pesi degli esempi che spingono il modello principale a concentrarsi su aree dell'input space con alte perdite. Entrambi i modelli sono ottimizzati alternativamente.

I risultati su tre dataset analitici (MNLI, FEVER, QQP) e i loro test adversarial out-of-distribution (HANS Symmetric, PAWS) mostrano che il metodo minimax migliora costantemente le prestazioni out-of-distribution mantenendo un'elevata accuratezza in-distribution. Il lavoro esplora anche l'effetto del pre-training del modello principale, la dimensione minima dell'ausiliario e analizza qualitativamente la distribuzione dei pesi degli esempi appresi.</sample>
    <sample id="300">L'audio presenta Belinda, che introduce il compito di **dichiarazione interattiva**, un processo che permette agli utenti di dettare e modificare documenti vocalmente in modo naturale e intuitivo. A differenza dei sistemi di trascrizione vocale esistenti, che spesso richiedono comandi predefiniti, la dichiarazione interattiva consente agli utenti di mescolare spontaneamente dettatura e modifiche senza distinzioni specifiche. Ad esempio, un utente può correggere un errore durante la dettatura, aggiungere nuove informazioni o impartire comandi come "Sostituisci 'l'evento' nell'ultima frase con 'esso'". Il sistema deve riconoscere e interpretare correttamente queste modifiche in tempo reale.

Belinda e i suoi collaboratori hanno formalizzato il compito in quattro passaggi: (1) trascrizione audio in testo, (2) segmentazione in dettatura e comandi, (3) normalizzazione e correzione degli errori, e (4) esecuzione delle modifiche. Hanno anche creato un dataset utilizzando un'interfaccia di raccolta dati e sviluppato un sistema di base che esegue questi passaggi. Hanno testato due modelli (T5 e GPT-3) con due approcci: prevedere programmi eseguibili o direttamente lo stato finale del documento. GPT-3 ha mostrato maggiore accuratezza ma maggiore lentezza, mentre T5 ha ottenuto un buon equilibrio tra efficienza e precisione. Il lavoro apre la strada a ulteriori miglioramenti in questo campo, con codice e dettagli disponibili per facilitare ricerche future.</sample>
    <sample id="302">Permutare i token per la sequenza di output è necessario perché, dopo la prima fase di tagging in cui ogni token di input è associato a un multiset di token che appariranno nell'output, i token non sono ancora ordinati. La permutazione permette di disporre i token nell'ordine corretto per formare la sequenza di output che corrisponde al significato dell'input. Senza permutazione, i token potrebbero apparire in un ordine casuale o non significativo, compromettendo la capacità del modello di generare una sequenza di output che rifletta correttamente il significato dell'input. La permutazione, quindi, è essenziale per garantire che i token vengano disposti in modo da rappresentare accuratamente la struttura composizionale della sequenza di output.</sample>
    <sample id="303">Gli autori hanno suggerito ai proprietari dei modelli di aumentare la trasparenza sui metodi di mitigazione dei bias perché, nonostante i progressi nella riduzione dei bias negativi, i modelli possono ancora generare stereotipi e narrazioni essenzializzanti attraverso meccanismi non chiari. Ad esempio, i metodi anti-stereotipi potrebbero involontariamente favorire stereotipi positivi o essenzializzanti, che sono altrettanto dannosi. Senza una maggiore trasparenza, è difficile comprendere le cause di questi fenomeni e studiarne gli effetti, rendendo necessaria una maggiore chiarezza sui processi e sulle scelte adottate per mitigare i bias.</sample>
    <sample id="304">Gli input inaccettabili di coppia minima (minimal pair paradigm, MPP) sono coppie di frasi in cui una è grammaticalmente accettabile e l'altra è grammaticalmente inaccettabile. Questi input vengono utilizzati per valutare la capacità di un modello linguistico di distinguere tra frasi grammaticali e non grammaticali. In particolare, nel contesto del lavoro di Koustav Sinha e colleghi, gli input inaccettabili di coppia minima sono creati selezionando frasi inaccettabili da dataset grammaticali (come BLiMP o SyntaxGym) e utilizzandole come prefisso per le coppie di frasi da valutare. Questo approccio permette di testare la sensibilità dei modelli linguistici alle strutture grammaticali anche in contesti più lunghi e complessi.</sample>
    <sample id="305">Il video presenta la ricerca "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" condotta da Dawei e collaboratori. Lo studio critica l'idea comune che il Weakly Supervised Learning (WSL) funzioni solo con dati etichettati debolmente, senza bisogno di dati puliti. In realtà, i recenti metodi WSL richiedono dati di validazione puliti per funzionare correttamente, altrimenti si verifica un calo significativo delle prestazioni. L'analisi mostra che anche un piccolo numero di campioni puliti (circa 20 per classe) è sufficiente per ottenere risultati eccellenti. Inoltre, l'uso diretto di questi campioni per il fine-tuning supera le prestazioni dei metodi WSL. Lo studio suggerisce che i benefici dei metodi WSL siano sopravvalutati e propone linee guida per future ricerche: segnalare i criteri di selezione del modello, confrontare i metodi WSL con quelli di few-shot learning e considerare il fine-tuning continuo come baseline. Il codice della ricerca è stato reso open-source per ulteriori verifiche e miglioramenti. In sintesi, la ricerca evidenzia la necessità di dati puliti nel WSL e propone un approccio più realistico e trasparente per valutare i metodi WSL.</sample>
    <sample id="306">Sebastian Schuster and Najoung Kim present their research on entity tracking in language models, a crucial ability for understanding longer discourses. They designed a task involving boxes and objects to evaluate this capability, ensuring it doesn’t rely on heuristics or pre-training data patterns. The task requires models to predict box contents after state-changing operations, combining initial descriptions with actions. 

Experiments with Flan-T5, GPT-3, and GPT-3.5 models revealed that most models failed to track entity states beyond the initial description, except for GPT-3.5 models, which showed non-trivial tracking. This improvement was attributed to their pre-training on code, which enhances their ability to handle state tracking. Smaller models like T5-base could learn the task with fine-tuning, but randomly initialized models of the same architecture could not, highlighting the importance of pre-training.

The researchers emphasize that while their findings suggest pre-training on code improves entity tracking, it remains unclear if these abilities generalize beyond their specific setup. They invite further exploration and discussion on their work, which includes additional results and GPT-4 experiments, available in their paper on arXiv.</sample>
    <sample id="307">Gli autori hanno utilizzato diverse metriche di valutazione per i 11 compiti downstream in francese, tra cui:

1. **Precisione (Precision)**
2. **Ricapitolazione (Recall)**
3. **F1-score (F1-score)**
4. **Accuracy**
5. **Mean Squared Error (MSE)** per i compiti di regressione

Queste metriche sono state applicate per valutare le prestazioni dei modelli su compiti come il riconoscimento di entità nominate (NER), classificazione, etichettatura di parti del discorso (POS tagging), e question answering.</sample>
    <sample id="308">Jenny, una studentessa di dottorato alla Carnegie Mellon University, presenta una ricerca sulla **NLPositionality**, che analizza i **bias di design** nei dataset e nei modelli di NLP (elaborazione del linguaggio naturale). Il lavoro, condotto in collaborazione con l’Università di Washington e l’Allen Institute for AI, esplora come i modelli di NLP riflettano le **positionality** dei loro sviluppatori, ovvero le prospettive influenzate da identità, demografia ed esperienze di vita.  

Un esempio pratico è l’API Prospective, efficace per rilevare contenuti tossici in contesti occidentali, ma meno sensibile a termini offensivi comuni in contesti indiani, come quello di Aditya Sharma. Questo evidenzia come i modelli possano essere **sbilanciati** a favore di determinate popolazioni.  

La ricerca utilizza il **framework NLPositionality** per confrontare le annotazioni di utenti reali con dataset e modelli esistenti. Il processo include la **riannotazione** di dataset da parte di annotatori diversificati, con un’attenzione particolare alla demografia, e il confronto con modelli tramite una metrica di correlazione (Pearson’s R).  

Lo studio, basato su piattaforme come **Lab in the Wild**, ha raccolto oltre 16.000 annotazioni da 1.000 annotatori in 87 paesi. I risultati mostrano che i modelli e i dataset sono più allineati a **paesi anglofoni**, persone con **istruzione universitaria** e culture come quella confuciana, ma meno rappresentativi di gruppi come le persone **non binarie**.  

Per affrontare questi bias, la ricerca propone:  
1. Documentare tutte le scelte progettuali.  
2. Adottare una prospettiva **perspectivista** nella ricerca NLP.  
3. Creare dataset e modelli specifici per comunità sotto-rappresentate.  

In sintesi, la ricerca dimostra che i modelli di NLP hanno una **positionality** intrinseca e suggerisce strategie per renderli più inclusivi.</sample>
    <sample id="309">La metrica utilizzata per misurare l'accordo tra annotatori è l'**inter-annotator agreement** (accordo tra annotatori), calcolata su 100 conversazioni doppiamente annotate.</sample>
    <sample id="310">Il dominio scelto per aggiungere frasi completamente scollegate alle query inaccettabili e accettabili è **Wikipedia**.</sample>
    <sample id="311">Gli autori dell'articolo sono Regina Stodden e Omar. Regina Stodden è la prima a presentare il corpus DEPLAIN, mentre Omar discute degli usi del corpus. Tuttavia, le loro affiliazioni specifiche non sono menzionate nel testo fornito.</sample>
    <sample id="312">MultiInstruct si differenzia dagli altri benchmark dataset per diversi aspetti:

1. **Primo Dataset Multi-Modale**: È il primo dataset di riferimento per l'instruction tuning multi-modale, mentre la maggior parte dei dataset precedenti si concentra su compiti di linguaggio solo.

2. **Ampia Varietà di Compiti**: Include 62 compiti multi-modali diversi, coprendo 10 categorie ampie, derivati da 21 dataset open-source esistenti.

3. **Istruzione Esperta**: Ogni compito è dotato di cinque istruzioni scritte da esperti, che forniscono una guida più dettagliata e diversificata rispetto ai dataset precedenti.

4. **Formato Unificato**: Tutti i compiti sono formulati in un formato sequenza-a-sequenza unificato, dove testo, immagini, istruzioni e bounding box sono rappresentati nello stesso spazio di token, facilitando l'integrazione multi-modale.

5. **Valutazione Completa**: Introduce una nuova metrica chiamata "sensibilità" per misurare la capacità del modello di produrre risultati coerenti con piccole variazioni nelle istruzioni, offrendo una valutazione più completa delle prestazioni.

6. **Transfer Learning**: Esplora l'effetto del transfer learning da dataset di istruzioni naturali, dimostrando che può migliorare sia le prestazioni che la sensibilità del modello.

7. **Espansione Futura**: Gli autori stanno raccogliendo un dataset ancora più ampio con circa 150 ulteriori compiti di visione e linguaggio, che saranno rilasciati in futuro.

In sintesi, MultiInstruct si distingue per la sua ampiezza, uniformità e approccio innovativo alla valutazione e al miglioramento delle prestazioni dei modelli multi-modali.</sample>
    <sample id="313">L'articolo è stato scritto da due autori: James Finch e Sarah Finch.</sample>
    <sample id="314">La coordinazione binaria si riferisce alla struttura grammaticale in cui due elementi (congiunti) sono uniti da una congiunzione coordinante, formando una singola unità sintattica. In questa struttura, entrambi i congiunti sono considerati uguali e indipendenti l'uno dall'altro, e la scelta dell'ordine dei congiunti non influisce sulla struttura della frase. La coordinazione binaria è un esempio di struttura simmetrica, in contrasto con le strutture asimmetriche in cui uno dei congiunti è considerato il "capo" della struttura.</sample>
    <sample id="315">I'm sorry, but the provided text does not contain any information about the duration of time the prompts were used in the study. The text discusses the methodology, results, and recommendations of the study "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" but does not mention the time frame or duration of prompt usage.</sample>
    <sample id="316">I risultati dimostrano che il modello T5, più piccolo rispetto ai grandi modelli di linguaggio, può generare script di qualità superiore quando viene opportunamente addestrato sul dataset CoScript. Questo suggerisce che i modelli più piccoli possono superare i modelli più grandi in compiti specifici di pianificazione linguistica con vincoli, purché siano addestrati su dataset adeguati e di alta qualità.</sample>
    <sample id="317">The paper "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors" by Peng Li from Fudan University addresses the challenges of information extraction (IE) tasks, such as named entity recognition (NER) and relation extraction (RE), using large code generation models. Traditional models like T5 and GPT-3 struggle with mismatched outputs between pre-training and inference phases, requiring extensive structured data and decoding strategies. The proposed approach, CodeIE, transforms IE into a structure-to-structure code generation task, leveraging code pre-trained models like Codex. This alignment ensures consistent structured outputs.

For NER, a code-style prompt is designed, defining a function to extract entities from input text. Few-shot demonstrations guide the model to generate structured entity pairs. Similarly, for RE, a code-style prompt is used. Evaluations on datasets show that CodeIE, using Codex and code-style prompts, significantly outperforms traditional models like T5 and GPT-3, especially in few-shot settings. Code format prompts reduce structural errors and improve recall. The study highlights that code pre-trained models better align with IE tasks, and Codex consistently outperforms GPT-3. The paper concludes that transforming IE into a code generation task enhances performance and provides a promising direction for future research.</sample>
    <sample id="318">Ciao, sono Yanis Labrak e vi presenterò i nostri lavori su "DrBERT: Un Modello Pre-addestrato Robusto in Francese per i Domini Biomedico e Clinico." In questa presentazione, prima parliamo della modellazione del linguaggio nell'assistenza sanitaria. Poi presenteremo il contributo principale del nostro articolo. Introduciamo il primo modello biomedico in francese chiamato DrBERT, che si basa su RoBERTa e addestrato su NACHOS, che è un insieme di dati di dati medici raccolti dal web. Abbiamo anche introdotto un confronto di modelli con impostazioni di pre-addestramento e fonti di dati multiple. Poi presentiamo i nostri risultati su 11 compiti a valle biomedici e clinici in francese. E infine concludiamo gli esperimenti e vi diamo maggiori dettagli su come accedere a quei modelli. Dalla sua uscita nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere i compiti di elaborazione del linguaggio naturale e offre enormi guadagni di performance rispetto ai metodi storici statici e contestuali come Word2vec, fastText o più. Da allora, questo modello è stato adattato a molte altre lingue, come in francese con CamemBERT, e anche in domini come il biomedico con PubMedBERT e BioBERT e sul clinico con ClinicalBERT, ma principalmente in inglese. I modelli specializzati per altre lingue sono scarsi e spesso basati su pre-addestramento continuo a causa della mancanza di dati in-dominio. Tuttavia, il francese non aveva alcun modello open source per il biomedico fino ad ora. Quindi ci chiediamo quale sia la fonte di dati più appropriata per un'ampia gamma di utilizzo e se questi dati raccolti siano una buona sostituzione per i dati clinici. Per rispondere a questa domanda, confrontiamo DrBERT con il nostro modello ChuBERT, che si basa su dati anonimi ottenuti dal magazzino dati dell'Ospedale Universitario di Nantes. Dopo di ciò, ci chiediamo quanta data abbiamo bisogno per addestrare un modello specializzato su dati francesi? È 4 gigabyte, 8 gigabyte, o più? Per rispondere a questa domanda, prima addestreremo e confronteremo quattro modelli da zero: una prima versione di DrBERT, con 7 GB di NACHOS; una seconda versione di 4 GB di NACHOS; una prima versione di ChuBERT, che è un modello clinico con 4 GB di frasi tratte dalle note cliniche; e una versione finale di ChuBERT con un mix di 4 GB di NACHOS e 4 GB di note cliniche. Oltre a questo confronto, abbiamo introdotto tre modelli addestrati su pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento. Uno basato sul peso di CamemBERT e addestrato su un set di 4 GB di NACHOS. Un altro basato sempre su CamemBERT, ma addestrato questa volta sui 4 GB di note cliniche e infine, uno basato sul modello biomedico inglese PubMedBERT, e addestrato su un set di 4 GB di NACHOS. In totale, abbiamo sette modelli. Per valutare i nostri sette modelli, raccogliamo dati per compiti a valle pubblici e privati come il riconoscimento degli enti nominati, la classificazione, il tagging delle parti del discorso e la risposta alle domande. Questi modelli sono confrontati con sei modelli di base che sono CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT e ClinicalBERT. La valutazione evidenzia che i modelli hanno ottenuto le migliori performance nel compito con dati della stessa natura di quelli su cui il modello è stato addestrato. Tuttavia, possiamo osservare che i dati provenienti da fonti eterogenee sembrano essere più versatili. Osserviamo anche che l'uso di più dati si traduce in migliori performance. Nel complesso, il pre-addestramento da zero sembra ottenere prestazioni superiori sulla maggior parte dei compiti. Tuttavia, il nostro esperimento sul pre-addestramento di controllo utilizzando il peso e la tokenizzazione di CamemBERT addestrato sul sottoinsieme di 4 GB di NACHOS ha mostrato risultati comparabili a quelli ottenuti con DrBERT 4 GB da zero. Il che non è il caso per il modello basato sui pesi e la tokenizzazione di CamemBERT, che soffre di problemi di stabilità. Infine, come conclusione il nostro sistema adeguato ha offerto prestazioni migliori su nove degli 11 compiti a valle e ha superato globalmente il risultato del modello generico, qui CamemBERT. Stiamo anche osservando che dati più specializzati sono migliori, ma non scalano bene. Tutti i modelli pre-addestrati ottenuti da NACHOS sono liberamente disponibili su Hugging Face, e sotto la licenza MIT, e tutti gli script di addestramento sono sul nostro repository GitHub. Quindi grazie per questa presentazione, e non vediamo l'ora di scambiare idee alla sessione di poster a Toronto.</sample>
    <sample id="319">Nel lavoro "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains", vengono esaminate due principali strategie di apprendimento:

1. **From-scratch pre-training**: I modelli vengono addestrati ex novo su un dataset specifico, come il dataset NACHOS per DrBERT e il dataset di note cliniche per ChuBERT. Questo approccio è considerato più efficace, poiché i modelli ottengono prestazioni migliori su compiti con dati della stessa natura di quelli utilizzati per l'addestramento.

2. **Continual pre-training**: I modelli vengono addestrati inizialmente su un linguaggio generale (ad esempio, CamemBERT) e poi finetunati su un dataset specifico. Vengono esaminate tre varianti di questa strategia:
   - Un modello basato su CamemBERT addestrato su un sottoinsieme di 4 GB di NACHOS.
   - Un modello basato su CamemBERT addestrato su 4 GB di note cliniche.
   - Un modello basato su PubMedBERT (un modello biomedico inglese) addestrato su 4 GB di NACHOS.

Queste strategie vengono confrontate per valutare l'impatto della quantità di dati e del tipo di pre-training sulle prestazioni dei modelli.</sample>
    <sample id="320">Il fattore di overfitting dovuto al riutilizzo del test (adaptive overfitting) non è significativo nel contesto di questo studio, poiché i risultati mostrano che ogni unità di miglioramento su CoNLL-2003 si traduce in più di un'unità di miglioramento su CoNLL++, indicando che non vi sono rendimenti decrescenti. Pertanto, il fattore di overfitting dovuto al riutilizzo del test è trascurabile o nullo.</sample>
    <sample id="321">La qualità della semplificazione è stata valutata attraverso l'analisi dei tipi di semplificazione (ad esempio, semplificazione lessicale, strutturale e complessiva) e la varietà delle trasformazioni di semplificazione presenti nel corpus DEPLAIN. Inoltre, il corpus è stato utilizzato come gold standard per valutare metodi di allineamento automatico, e i risultati hanno mostrato che il metodo MASSalign è il più efficace per l'allineamento di testi semplificati in tedesco. Infine, la qualità della semplificazione è stata misurata attraverso l'ottimizzazione dei modelli linguistici (long-mBART per la semplificazione a livello di documento e mBART per la semplificazione a livello di frase), confrontando i risultati con i punteggi di riferimento e proponendoli come benchmark per il futuro.</sample>
    <sample id="322">Enrico presenta una ricerca all'ACL 23 che esplora cosa imparano i modelli di classificazione testuale sulla moralità. La moralità, intesa come la capacità di distinguere il giusto dal sbagliato, è un concetto fondamentale per le società e deve essere compresa anche dai modelli linguistici. Tuttavia, la moralità è soggettiva e varia tra individui, rendendo pericoloso basarsi su una scala unica (immorale-morale) per classificarla. Enrico introduce la **Teoria delle Fondamenta Morali**, che identifica cinque dimensioni (e.g., equità, autorità) attraverso cui gli esseri umani percepiscono la moralità, con priorità individuali diverse. Questa teoria è già stata applicata nel NLP, ma mancava un'analisi approfondita di come la moralità si esprime in contesti diversi.

Lo studio utilizza il **Moral Foundation Twitter Corpus**, un dataset di 35.000 tweet divisi in sette domini (e.g., #AllLivesMatter, #BlackLivesMatter). L'obiettivo è capire se i modelli possono riconoscere le differenze sottili nell'espressione della moralità tra questi domini. Ad esempio, nei tweet #BLM, il concetto di **subversion** (ribellione all'autorità) è associato a un tono positivo, mentre in #ALM è visto negativamente. I risultati mostrano che i modelli possono cogliere queste sfumature, evidenziando l'importanza di considerare il contesto specifico per una comprensione accurata della moralità. Enrico conclude sottolineando i rischi di utilizzare un unico modello per domini diversi, che potrebbero portare a fraintendimenti pericolosi.</sample>
    <sample id="323">Il paper di Yujie Wang, intitolato "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense QA", affronta la sfida della Question Answering (QA) basata sul senso comune, un compito che richiede la comprensione del linguaggio e l'accesso a conoscenze esterne. Wang critica i metodi esistenti che combinano language models e knowledge bases, evidenziando problemi come l'introduzione di entità irrilevanti durante la costruzione del subgraph e la mancanza di interazione tra le modalità di testo e subgraph.

Per risolvere questi problemi, Wang propone il Dynamic Heterogeneous-Graph Knowledge Graph (DHLK). Il DHLK viene costruito attraverso una strategia di potatura in due fasi e Knowledge Representation Learning (KRL), ottimizzando la struttura e la rappresentazione della conoscenza. Successivamente, il language model (RoBERTa) viene utilizzato per codificare e fondere le due modalità, rimuovendo entità irrilevanti basandosi sui pesi di attenzione di RoBERTa.

L'approccio include l'uso di Relation Mask Self-Attention (RMSA) per modellare i subgrafi, ispirato al RGAT, e l'applicazione di TransE per ottimizzare le rappresentazioni di entità e relazioni. Infine, l'embedding del DHLK, le informazioni sui percorsi e l'embedding del contesto QA vengono combinati in un Multi-Layer Perceptron (MLP) per predire la risposta.

Gli esperimenti su CommonsenseQA e OpenBookQA, utilizzando knowledge bases come ConceptNet, WordNet e Wiktionary, dimostrano che il metodo di Wang ottiene risultati migliori rispetto ad altri approcci che combinano language models e knowledge graphs.</sample>
    <sample id="324">Sì, i modelli linguistici presentano bias politici diversi. Gli esperimenti condotti dimostrano che i modelli come GPT-4 sono più liberali, mentre i modelli BART tendono ad essere più conservatori. Inoltre, i modelli possono assumere posizioni politiche in tutte e quattro le quadranti del panorama politico. Questi bias possono essere influenzati dal tipo di dati di pre-addestramento utilizzati, come ad esempio corpora di notizie o social media con orientamenti politici diversi.</sample>
    <sample id="325">Hi! My name is Matthias Lindemann, and today I'm going to give you a brief introduction to our paper on "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations". This is joint work with my advisors Alexander Koller and Ivan Titov. Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. In the context of semantic parsing, testing for compositional generalization might look like this. As usual, we have a training set of utterances. In this case, "The girl slept." And "Mary knew that the girl slept." These utterances are paired with logical forms that represent core aspects of their meaning. In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms. In this example, the model has seen shallow recursion during training and is tested on an example with deeper recursion. Naive seq2seq models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input. In particular, they often fail to reproduce the systematic correspondences between input and output, such as those that are color-coded in the example. A popular method to address this is to integrate trees into the models. The trees are intended to capture the compositional process that relates utterances with the logical forms. This works well, but trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism-specific pre-processing of the logical forms, for example, to handle variable symbols. Obtaining trees may also involve specialized grammar-induction procedures. In this paper, we don't use trees and introduce a neural seq2seq model that directly models the correspondences between fragments of the input and fragments of the output. For the first time, we show strong generalization to deeper recursion without relying on trees. Our approach predicts the output from the input in two steps. First, we tag each input token with an unordered multiset of tokens that will appear in the output. After the first step, we have all the right tokens, but they're not ordered. That's why in the second step we use another model to predict a permutation to put them into the right order. We introduce a new method to predict the permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive. Conceptually, our permutation model works roughly like this. We go from left to right over the output and determine which multiset token to put in every position. For the first output position, we simply select one, as highlighted in red. Then we jump to the next multiset token, to determine the second token in the output. We determine the third token in the output in a similar way by jumping to another multiset token. We continue this process until every token from the first stage has been visited exactly once. To give you a teaser of the experimental results, here we compare our method with other treeless models on the COGS benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion. Some other kinds of structural generalization remain very challenging, though. In our paper, we solve a couple of interesting technical challenges. First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token we don't know which multiset it came from, which poses a challenge for training. In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the training. Our permutation method is very flexible, but it brings the challenge that finding the highest-scoring permutation is NP-hard. That's because this is related to the "Traveling Salesman" problem. We approximate this with a GPU-friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations. If you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.</sample>
    <sample id="326">La dissonanza cognitiva è un fenomeno in cui una persona sperimenta un conflitto mentale tra due credenze, atteggiamenti o azioni che sono incoerenti tra loro. Ad esempio, una persona potrebbe dire "So che le sigarette possono uccidermi" e poi "Ho preso un paio di sigarette dopo la riunione", creando una situazione di dissonanza cognitiva. Questo conflitto può essere risolto giustificando la seconda azione, come "Non credo di poter mantenere il lavoro senza di loro", creando così una relazione di consonanza. La dissonanza cognitiva è un fenomeno comune nella vita quotidiana e può avere implicazioni importanti per la comprensione dei processi decisionali, delle dinamiche di gruppo e della salute mentale.</sample>
    <sample id="327">**Abstract:**  
At ACL 2023, Xiao Xu presents "ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning," a novel approach to Vision-Language (VL) learning. Unlike existing architectures like BridgeTower, which layer-by-layer connect unimodal and cross-modal layers, ManagerTower introduces adaptive "managers" in each cross-modal layer to aggregate insights from multiple unimodal expert layers. This design allows for more effective exploitation of diverse levels of unimodal semantic knowledge, enhancing cross-modal alignment and fusion.  

Using RoBERTa and CLIP-ViT as unimodal encoders, ManagerTower adaptively combines representations from different levels of unimodal experts. Despite pre-training on only four million images, ManagerTower outperforms METER, BridgeTower, and other base-size models on downstream tasks, achieving a 39.15% accuracy improvement on the Wikivideo test standard. Visualization of aggregation weights further demonstrates that adaptive managers dynamically exploit unimodal knowledge tailored to each cross-modal layer, unlike static managers.  

The work is scalable, as any visual, textual, or cross-modal encoder can be integrated. ManagerTower’s architecture, code, and models are publicly available, aiming to advance VL representation learning. This approach highlights the importance of adaptive aggregation in leveraging multi-level unimodal insights for comprehensive cross-modal understanding.</sample>
    <sample id="328">Il modello linguistico più liberale è GPT-4.</sample>
    <sample id="329">Il lavoro presentato, intitolato "Generating Structured Pseudo Labels for Noise-resistant Zero-shot Video Sentence Localization", si concentra sulla localizzazione di segmenti video in base a query testuali, un compito cruciale per applicazioni come il recupero e la sintesi video. Il team di ricercatori, guidato da Minghang Zheng della Peking University, ha sviluppato un metodo zero-shot che elimina la necessità di annotazioni manuali costose e inefficienti.

Il metodo proposto supera i limiti dei precedenti approcci zero-shot, che generano pseudo-query troppo semplici e non garantiscono un'adeguata relazione tra i segmenti video e le query. La nuova tecnica utilizza un modello di captioning basato su immagini per generare query più complesse e libere, e un modello pre-addestrato per misurare la rilevanza tra i frame video e le query, generando pseudo-eventi che assicurano un'alta rilevanza interna e una bassa rilevanza esterna.

Inoltre, il metodo riduce l'influenza del rumore nei dati di addestramento pesando i campioni in base alla confidenza del modello e all'IoU tra le previsioni e i pseudo-label. Infine, i ricercatori hanno condotto esperimenti su due dataset, ActivityNet Captions e Charades-STA, dimostrando prestazioni superiori rispetto ai metodi esistenti in termini di R@M e mIoU. Il codice sorgente è disponibile per ulteriori approfondimenti.</sample>
    <sample id="330">Sì, nell'apprendimento attivo, l'addestramento cumulativo funziona meglio di quello iterativo. Questo è stato dimostrato nel contesto della classificazione della dissonanza cognitiva, dove l'aggiornamento cumulativo ha ottenuto prestazioni uguali o migliori rispetto all'aggiornamento iterativo.</sample>
    <sample id="331">La relatrice è Sara Papi.</sample>
    <sample id="332">I dati nel parametro di riferimento MuDa sono stati tratti dalle trascrizioni dei TED Talks, che sono state tradotte dall'inglese in 14 lingue diverse.</sample>
    <sample id="333">Wenhao, un ricercatore della Nanjing University, presenta il lavoro "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation". Lo scopo è migliorare la generalizzazione e le prestazioni dei modelli di traduzione automatica neurale (NMT) affrontando il problema della rappresentazione non-smooth e della sparsità dei token a bassa frequenza. Il team ha sviluppato un framework, INK, che integra la conoscenza kNN (k-Nearest Neighbors) per raffinare la rappresentazione spaziale del modello.

Il metodo proposto, kNN-MT, utilizza un datastore per salvare le rappresentazioni e i token target corrispondenti, ma è inefficace a causa della lentezza nella ricerca dei vicini e dell'impossibilità di aggiornare le rappresentazioni. INK risolve questi problemi con un training loop a due fasi: estrae la conoscenza kNN dal datastore per guidare un adattatore nella regolazione delle rappresentazioni, e aggiorna asincronamente il datastore con le rappresentazioni aggiornate.

Gli esperimenti, condotti sul modello vincitore del WMT'19 German-English, dimostrano che INK migliora significativamente la rappresentazione spaziale, con un guadagno medio di 1.99 COMET e 1.0 BLEU rispetto ai sistemi kNN-MT di stato dell'arte. INK richiede meno memoria e offre una velocità di inferenza più rapida, dimostrando che l'integrazione di adattatore e datastore può portare ulteriori miglioramenti.</sample>
    <sample id="335">Il nome del relatore è Matthias Lindemann.</sample>
    <sample id="336">Il trasferimento interlinguistico (o cross-lingual transfer) è un approccio in cui un modello linguistico addestrato su una lingua sorgente viene utilizzato per migliorare le prestazioni su una lingua target, anche se non è stato specificamente addestrato su quella lingua. Questo metodo sfrutta le conoscenze acquisite da una lingua per trasferirle a un'altra, riducendo il divario di performance tra lingue diverse. Nel contesto di XSemPLR, il trasferimento interlinguistico viene utilizzato per migliorare la traduzione e l'interpretazione di query semantiche in più lingue e rappresentazioni semantiche, ad esempio traducendo query in tedesco in inglese e poi utilizzando un modello addestrato su inglese per generare la query SQL corrispondente.</sample>
    <sample id="337">The research paper "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning" addresses the challenge of representing out-of-vocabulary (OOV) words in embedding models, which are crucial for downstream tasks. The authors propose a novel approach inspired by human learning habits, leveraging word formation and association to infer the meaning of OOV words.

The key innovation is the introduction of a Word Relationship Graph, which mimics lexical rules of word formation and association. When an OOV word appears, it is tokenized into wordpieces and naturally associated with relevant words, forming a two-level graph. The first layer preserves all nodes to retain wordpiece information, while the second layer samples a fixed number of nodes to mitigate noise from numerous neighbors.

To assign node attributes to OOV nodes, the authors use a self-attention network based on the characters of the OOV words. Two levels of Graph Attention Network are applied to extract important information and reduce noise. A readout block layer is then used to capture graph-level information and summarize word formation.

The authors also incorporate contrastive learning in the loss function, using NT-XENT positive samples from the graph, such as two-hop relevant neighbor words, synonyms, or the OOV word itself. This encourages proximity between graph-level embeddings while pushing them apart from other samples.

Extensive experiments demonstrate the model's superior performance compared to baselines in both intrinsic and extrinsic tasks. The approach is particularly effective for English, and the authors suggest that its application to other languages depends on the rationality of word decomposition. Overall, the model's graph-based approach to word formation and association provides a promising solution for handling OOV words in embedding models.</sample>
    <sample id="338">Il lavoro "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations" esplora l'efficacia delle spiegazioni umane nell'addestramento dei modelli di intelligenza artificiale. Gli autori, provenienti da Rensselaer Polytechnic Institute, Northeastern University e IBM Research, si pongono l'obiettivo di valutare oggettivamente la qualità delle spiegazioni umane, spesso considerate il gold standard, ma soggettive e dipendenti dal compito.

Il team propone una struttura dati unificata che converte diverse task in un formato a scelta multipla, permettendo di analizzare l'utilità delle spiegazioni in contesti diversi. Attraverso esperimenti su cinque dataset (CoS-E, ECQA, e-SNLI, ComVE), i ricercatori dimostrano che le spiegazioni umane possono migliorare le prestazioni dei modelli, anche se di bassa qualità.

Viene introdotta una nuova metrica, TREU, che estende il concetto di simulatability score, valutando l'aiuto delle spiegazioni durante il fine-tuning. I risultati mostrano che TREU supera la simulatability score nell'evaluazione della qualità delle spiegazioni, evidenziando la dipendenza del loro aiuto dal compito e dal formato.

In sintesi, il lavoro getta le basi per una collaborazione di alta qualità tra umani e AI nell'annotazione dei dati, suggerendo ulteriori controlli di qualità per i ricercatori.</sample>
    <sample id="339">Gli autori dell'articolo sono affiliati a Saarland University in Germania.</sample>
    <sample id="340">Il lavoro presentato, "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation", si concentra sulla creazione di un dataset di parafrasi ampiamente rappresentativo e sintatticamente vario. Questo è particolarmente importante per migliorare le prestazioni di applicazioni NLP come question answering, chatbot e la robustezza dei modelli.

Il team di ricerca, guidato da Kuan-Hao Huang dell'UCLA, ha sviluppato ParaAMR utilizzando AMR (Abstract Meaning Representations), grafi diretti che catturano il significato astratto di una frase. L'idea chiave è di modificare il focus (il nodo radice) di un AMR e rigenerare il testo, mantenendo la struttura del grafo ma cambiando la sintassi. Questo processo genera parafrasi con significato simile ma struttura diversa.

ParaAMR contiene circa 15 milioni di frasi sorgente con 6,9 parafrasi ciascuna, dimostrando una maggiore diversità sintattica rispetto ai dataset esistenti. Valutazioni automatiche e umane confermano che ParaAMR preserva una buona somiglianza semantica pur essendo più vario sintatticamente.

Il dataset è stato testato in diverse applicazioni NLP: l'apprendimento di embedding di frasi, la generazione di parafrasi con controllo sintattico e l'aumento dei dati per il few-shot learning. I risultati mostrano che ParaAMR migliora le prestazioni in queste aree.

In sintesi, ParaAMR rappresenta un passo avanti nella creazione di dataset di parafrasi, offrendo una risorsa preziosa per lo sviluppo di modelli NLP più robusti e versatili.</sample>
    <sample id="341">Gli autori ricorrono a due misure di latenza:  
1. **Average lagging**: misura il tempo di ritardo tra l'input audio e l'output di traduzione.  
2. **Computational-aware average lagging**: tiene conto del tempo computazionale necessario al modello per generare l'output, fornendo una misura più realistica della latenza.</sample>
    <sample id="342">Il paper "LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming" presenta un nuovo dataset di dialogo personalizzato, costruito automaticamente da video di live streaming cinesi. Il dataset, creato da un team di ricercatori della Shanghai Jiao Tong University e Xiaobing.AI, affronta le limitazioni dei dataset esistenti, che sono principalmente basati su testo e mancano di scala e personalizzazione.

LiveChat è un dataset video-sourced, più vicino alle conversazioni reali, e include informazioni sulla personalità degli utenti, cruciali per la generazione di dialoghi personalizzati. Il dataset è costruito in tre fasi: estrazione di video da TikTok e Douyin, trascrizione dell'audio in testi e costruzione di dialoghi utilizzando un metodo di matching reply-to-whom.

Gli esperimenti dimostrano che LiveChat migliora le prestazioni nei compiti di Response Modeling e Addressee Recognition, grazie alla selezione di profili di personalità e alla lunghezza media delle sessioni. Inoltre, il confronto con altri dataset evidenzia la distintività di LiveChat, che si distingue per la sua scala e la sua natura video-sourced.

In sintesi, LiveChat rappresenta un passo avanti nella ricerca sui dataset di dialogo, offrendo un'opportunità unica per lo sviluppo di applicazioni come virtual streamer e virtual employees, e aprendo nuove prospettive per l'apprendimento trasferibile dei modelli di dialogo.</sample>
    <sample id="343">Ciao a tutti, sono Akshatha, e oggi io e il mio co-autore Martin presentiamo il nostro lavoro "Il Test KITMUS: Valutazione dell'Integrazione della Conoscenza da Fonti Multiple." Questo lavoro è una collaborazione tra l'Università McGill, Mila e Microsoft Research. I modelli di comprensione del linguaggio naturale si basano su una varietà di fonti di conoscenza, come le informazioni contenute nei loro parametri, solitamente acquisite tramite un pre-addestramento, e le informazioni fornite negli input al momento dell'inferenza. Lavori recenti in compiti come la risposta alle domande mostrano che i modelli possono utilizzare la conoscenza acquisita durante il pre-addestramento per risolvere il compito. Tuttavia, la comprensione del linguaggio naturale spesso richiede conoscenze che vengono fornite anche al momento dell'inferenza. Ad esempio, nella frase "John ha visto il nuovo presidente eletto in TV." I parametri pre-addestrati possono contenere informazioni su cosa fanno i presidenti e cosa sia una TV, ma non possono sapere in modo affidabile chi sia l'entità specifica "John", o chi sia il nuovo presidente, perché il presidente potrebbe essere cambiato da quando è avvenuto il pre-addestramento. Pertanto, i modelli di successo per compiti di NLU che richiedono conoscenza sono quelli che hanno la capacità di integrare e utilizzare sia la conoscenza pre-addestrata che la conoscenza al momento dell'inferenza. In questo lavoro, proponiamo una suite di test diagnostici per l'integrazione della conoscenza. Introduciamo un compito di risoluzione della coreferenza, progettato per sondare la capacità di attingere a conoscenze disponibili in diverse fonti. Valutiamo il set di dati con partecipanti allo studio umani e modelli di risoluzione della coreferenza consolidati. Ecco un esempio dal nostro set di dati. Servin è un giudice. Kea è un Baker. Servin e Kea si sono incontrati in un parco. Dopo una lunga giornata di lavoro a decidere casi in un tribunale, era felice di rilassarsi. Il compito qui è identificare l'entità corretta a cui si riferisce il pronome "lui", che in questo caso è Servin. La risoluzione di un dato pronome richiede due tipi di informazioni. Primo, la conoscenza specifica dell'entità come "Servin è un giudice." E secondo, la conoscenza di base come "I giudici decidono i casi nei tribunali." Generalmente, la conoscenza di base è appresa durante il pre-addestramento dei grandi modelli di linguaggio, mentre la conoscenza specifica dell'entità è tipicamente osservata al momento dell'inferenza. Varia la disponibilità di queste due informazioni in modo che possano essere trovate in una singola fonte, o in più fonti. Abbiamo definito tre impostazioni di KITMUS. Primo, abbiamo l'impostazione tipica: "Background-Pretrain", dove la conoscenza di base è assunta essere disponibile al momento del pre-addestramento. Secondo, c'è un'impostazione "Background-Both", dove la conoscenza di base è disponibile sia al momento del pre-addestramento che dell'inferenza. Infine, l'impostazione "Background-Inference", dove entrambe le conoscenze sono disponibili solo al momento dell'inferenza. Quest'ultima impostazione è particolarmente interessante, poiché simula il caso in cui la conoscenza di base necessaria per risolvere un compito non fa parte dei dati di pre-addestramento dei modelli. Ad esempio, perché nuove occupazioni si sono sviluppate dal momento del pre-addestramento. Ecco un esempio di come controlliamo la disponibilità dei fatti nelle fonti vere. Nell'impostazione Background-Pretrain, assumiamo che la conoscenza di base "I politici cercano seggi elettivi nel governo" sia contenuta nei parametri pre-addestrati e nel contesto dell'inferenza forniamo la conoscenza specifica dell'entità "Chichester è un politico." Nell'impostazione Background-Both, forniamo inoltre non solo la conoscenza specifica dell'entità ma anche la conoscenza di base sui politici nel loro contesto di inferenza. Nell'impostazione Background-Inference, forniamo l'occupazione fittizia "mirituer" invece di politico perché "mirituer" è improbabile che sia contenuta nei parametri pre-addestrati. Valutiamo il set di dati sia con partecipanti allo studio umani, sia con modelli di risoluzione della coreferenza consolidati. In questa figura, mostriamo i risultati dei modelli con migliori prestazioni sulla variante più difficile dell'impostazione Background-Pretrain. Senza addestramento specifico sul compito su KITMUS, entrambi i modelli non si comportano bene. Quando addestrati su KITMUS, tuttavia, sia C2F che BERT4Coref si comportano significativamente meglio della scelta casuale. Questo suggerisce che quando addestrati su set di dati generici di risoluzione del riferimento, la maggior parte impara a sfruttare indizi superficiali, che non sono utili quando si testa su KITMUS dove tali indizi sono stati rimossi. Esperimenti aggiuntivi con conoscenze fittizie hanno indicato che anche i modelli con migliori prestazioni non possono integrare in modo affidabile la conoscenza di base fornita solo al momento dell'inferenza. Per riassumere i punti principali del nostro articolo, molti modelli di risoluzione della coreferenza sembrano incapaci di ragionare su conoscenze da diverse fonti senza addestramento specifico sul compito. Tuttavia, con addestramento specifico sul compito, alcuni modelli integrano con successo conoscenze da più fonti. Tuttavia, anche i modelli con migliori prestazioni sembrano avere difficoltà a integrare in modo affidabile la conoscenza di base presentata solo al momento dell'inferenza. Se siete interessati a ulteriori dettagli, per favore consultate il nostro articolo e date un'occhiata al set di dati e al codice su GitHub. Grazie per l'attenzione.</sample>
    <sample id="344">I metodi basati su alberi per il problem solving di compositional generalization presentano diversi svantaggi:

1. **Non-automaticità**: Gli alberi non sono solitamente forniti e devono essere ottenuti attraverso processi formali specifici, come la procedura di induzione della grammatica, che possono essere complessi e computazionalmente costosi.

2. **Pre-processing**: La necessità di ottenere alberi richiede un pre-processing specifico per la formalizzazione, ad esempio per gestire i simboli delle variabili, il che può essere limitante e richiedere una conoscenza specifica del dominio.

3. **Rigidità**: Gli alberi possono essere rigidi e non sempre catturano efficacemente le relazioni complesse e le variazioni strutturali che possono esistere tra le frasi di input e le loro rappresentazioni logiche.

4. **Dipendenza da strutture predefinite**: La necessità di alberi può limitare la flessibilità del modello, poiché questi devono essere compatibili con la struttura predefinita degli alberi stessi.

5. **Scalabilità**: La generazione e l'uso di alberi possono diventare meno scalabili con l'aumento della complessità e della profondità della struttura delle frasi.

In sintesi, i metodi basati su alberi per la compositional generalization sono limitati dalla loro non-automaticità, dalla necessità di pre-processing, dalla rigidità, dalla dipendenza da strutture predefinite e dalla scalabilità.</sample>
    <sample id="345">The paper "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations" by Matthias Lindemann, Alexander Koller, and Ivan Titov addresses the challenge of compositional generalization in semantic parsing without relying on trees. Compositional generalization refers to a model's ability to handle deeper recursion and unseen compositions of phrases it has seen individually during training. Traditional sequence-to-sequence (seq2seq) models struggle with out-of-distribution generalization, often producing outputs detached from the input. Tree-based methods are a common solution, but obtaining trees can be complex and computationally expensive.

The authors propose a neural seq2seq model that directly models correspondences between input and output fragments. The approach involves two steps: (1) tagging each input token with an unordered multiset of tokens that will appear in the output, and (2) predicting a permutation to order these tokens correctly. This method is flexible and expressive, as it does not impose hard constraints on possible permutations.

The permutation model works by iterating through the output and selecting tokens from the multiset in the correct order. Training is challenging because the alignment between input and output is not provided in the data, and the linguistically correct permutation is often latent. The authors address this by inducing alignment during training and using a GPU-friendly continuous relaxation to approximate the NP-hard permutation problem, enabling backpropagation and learning of plausible permutations.

Experimental results on the COGS benchmark show that the proposed method outperforms other treeless models, particularly in generalizing to deeper recursion. However, some structural generalization challenges remain. The paper provides a novel approach to compositional generalization without trees, offering a promising alternative to traditional methods.</sample>
    <sample id="346">Purtroppo, il contenuto fornito non include le affiliazioni degli autori dell'articolo.</sample>
    <sample id="347">Ciao, sono Myra e oggi parlerò della nostra ricerca "Marked Personas: Utilizzo di Prompt in Linguaggio Naturale per Misurare gli Stereotipi nei Modelli di Linguaggio". Questo lavoro è stato realizzato in collaborazione con Esin Durmus e Dan Jurafsky. Negli ultimi anni, molti hanno documentato la diffusione di pregiudizi sociali e stereotipi nei grandi modelli di linguaggio, o LLMs. Tuttavia, queste misurazioni hanno varie limitazioni. Di solito si basano su set di dati costruiti a mano che richiedono molto tempo per essere curati e inoltre di solito misurano solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altre demografie o contesti, o semplicemente catturano associazioni generali e ampie, come associazioni negative con particolari gruppi. Inoltre, la maggior parte del lavoro in questo campo non tiene conto dell'intersezionalità, che è la nozione che le identità sociali multifaccettate possano aggravare i pregiudizi ed essere luoghi unici di danno. Per superare queste limitazioni, ci affidiamo alla proprietà che questi nuovi LLMs regolati da istruzioni sono molto bravi a rispondere a istruzioni e prompt. Quindi possiamo chiedere al modello di generare una persona, che è una rappresentazione di un individuo immaginario usando un prompt come "Immagina di essere una donna asiatica. Descriviti.". E possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia perché possiamo semplicemente specificare qualsiasi marcatore di identità che vogliamo in questo prompt. Ecco alcuni esempi di generazioni da GPT-4. Vediamo immediatamente che, sebbene gli output non siano apertamente negativi o tossici nel senso tradizionale di queste parole, ci sono alcuni schemi interessanti. La donna asiatica è descritta come modesta; la donna mediorientale è definita usando parole come esotica e come, riferendosi a una regione affascinante. E entrambe le persone delle donne di colore fanno riferimenti all'ascendenza mentre la persona dell'uomo bianco non ha nulla di simile. Per catturare questi schemi, il nostro metodo ha due parti. La prima è la generazione di queste persone. I nostri prompt per generare queste persone sono stati ispirati da uno studio in cui hanno dato questi prompt a soggetti umani, scoprendo che, dandoli a soggetti umani, erano anche in grado di portare alla luce stereotipi razziali. E questo consente anche un confronto diretto tra le persone generate e le risposte scritte dagli umani. La seconda parte sono le parole segnate, che è un metodo per identificare le parole che distinguono i gruppi segnati da quelli non segnati, su cui tornerò tra poco. Il vantaggio di questo è che otteniamo stereotipi e schemi molto specifici, senza dover fare affidamento su alcun lessico specifico. Quindi il metodo delle Parole Segnate si basa sul concetto sociolinguistico di "segnatura", che afferma che esiste un default non segnato e qualsiasi gruppo che si discosta da questo default è linguisticamente segnato. Quindi, per esempio, la parola "guerriero" è solitamente associata agli uomini. Quindi, quando le persone descrivono una guerriera che è una donna, di solito specificano "guerriera donna" e segnano il termine con "donna". E più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non segnati, mentre i gruppi emarginati sono solitamente segnati. Quindi nel nostro metodo, prima designeamo quali sono i gruppi non segnati e segnati, e poi confrontiamo le persone usando il metodo delle Parole Combattenti, che è fondamentalmente l'uso di rapporti log-odds ponderati per distinguere le parole principali per ogni gruppo segnato. Quindi, per esempio, per le persone delle donne nere, faremmo le Parole Combattenti e confronteremmo i rapporti log-odds contro sia le persone bianche che quelle maschili perché questi sono i due gruppi non segnati corrispondenti. Ora per alcuni risultati. Quindi prima usiamo un lessico di stereotipi e scopriamo che le persone generate contengono molti più stereotipi di quelle scritte dagli umani. Tuttavia, quando guardiamo effettivamente la distribuzione delle parole e del lessico, troviamo cose molto diverse. Quindi, mentre le persone generate hanno tassi molto più alti di parole del lessico, quelle scritte dagli umani hanno una distribuzione molto più ampia di parole, mentre le parole stereotipo presenti nelle persone generate sono davvero solo le parole "alto" e "atletico". Quindi davvero solo quelle positive o almeno non negative. E in effetti, questo lessico non cattura davvero molti dei modelli dannosi che abbiamo visto nelle diapositive precedenti affatto. Quindi, invece di fare ciò, ci rivolgeremo ai risultati del nostro metodo delle Parole Segnate per mostrare come queste rappresentazioni apparentemente positive riflettano schemi dannosi. Nella nostra analisi, riveliamo come queste rappresentazioni apparentemente positive riflettano schemi dannosi. Prima, dai nostri gruppi, le parole principali includono cose come "cultura", "tradizione", "orgoglioso" ed "esotico". E queste parole definiscono questi gruppi solo in relazione alla loro identità e li distinguono come diversi dalla norma bianca. Questo contribuisce a una lunga eredità di discriminazione e alienazione per questi gruppi. Inoltre, ci sono molti cliché riflessi in queste parole, specialmente per le donne di colore. Quindi, per esempio, le parole che descrivono le donne latine includono cose come "vibrante" e "curvacee" che si collegano a un cliché di tropicalismo. Per le donne asiatiche, le parole sono cose come "piccola" e "delicata" e "setosa" che si collegano a una lunga storia di donne asiatiche ipersessualizzate, viste come molto docili e sottomise, e così via. E infine, per le donne nere, vediamo che alcune delle parole principali sono cose come "forte" e "resiliente". Questo si collega a un archetipo che le persone hanno chiamato l'archetipo della "Donna Nera Forte". E sebbene suoni positivo a prima vista, ci sono stati lavori che mostrano che questo tipo di archetipo è davvero dannoso perché mette molta pressione su queste demografie per essere resilienti e forti contro gli ostacoli sociali. Piuttosto che lavorare effettivamente per cambiare quegli ostacoli, mette pressione su quelle persone per superarli, il che porta a risultati di salute molto negativi per queste persone, tra altri danni. Più in generale, scopriamo che le parole per ogni gruppo segnato riflettono davvero narrazioni essenzializzanti. Quindi, basandoci su questi schemi, concludiamo con tre raccomandazioni per i proprietari dei modelli. Prima, come ricercatori, dovremmo affrontare stereotipi positivi e narrazioni essenzializzanti. Dovremmo anche usare una lente intersezionale per studiare pregiudizi e danni perché ci sono molte cose che potrebbero essere trascurate se non lo facciamo. E infine, dovrebbe esserci una maggiore trasparenza sui metodi di mitigazione dei pregiudizi, perché, per esempio, come questi stereotipi positivi, non sappiamo se è perché c'è qualche tipo di allineamento di valore strano e eccessivo in corso, o forse altri metodi anti-stereotipi che risultano in questi schemi perniciosi. Non possiamo davvero fare ipotesi o studiare ulteriormente questo, senza più trasparenza. Grazie mille per aver ascoltato. Buon divertimento all'ACL.</sample>
    <sample id="348">**Abstract:**  
This paper, "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models," explores the prevalence of social biases and stereotypes in large language models (LLMs) by leveraging their ability to generate personas based on identity markers. Unlike traditional methods that rely on hand-curated datasets, this approach uses natural language prompts (e.g., "Imagine you are an Asian woman. Describe yourself.") to generate personas, enabling broad and generalizable analysis across demographics. The study identifies patterns such as the portrayal of women of color through essentializing narratives (e.g., "exotic," "vibrant," "strong") and highlights how seemingly positive stereotypes perpetuate harmful tropes (e.g., hyper-sexualization of Asian women, the "Strong Black Women" archetype). The Marked Words method, drawing on sociolinguistic markedness, distinguishes words unique to marked groups (e.g., women of color) from unmarked groups (e.g., white men), revealing nuanced biases. Results show that LLMs generate more stereotypes than human-written personas, but the lexicon alone fails to capture harmful patterns. The paper concludes with three recommendations: addressing positive stereotypes, adopting an intersectional lens, and increasing transparency in bias mitigation methods to better understand and mitigate these pernicious narratives.</sample>
    <sample id="349">Ciao a tutti, mi chiamo Jingwei Yi dell'Università della Scienza e della Tecnologia della Cina. È un piacere per me presentare un breve video pubblicitario del nostro articolo. State copiando il mio modello? Protezione del diritto d'autore dei modelli di embedding come servizi tramite watermarking di backdoor. Introduciamo prima il contesto degli embedding come servizi. Attualmente, i grandi modelli linguistici come GPT, LLAMA, PALM sono eccezionali nella comprensione e nella generazione del linguaggio naturale. Gli embedding come servizi sono uno dei servizi costruiti sui grandi modelli linguistici per assistere vari compiti di NLP. Ad esempio, OpenAI offre un'API di embedding basata su GPT. Tuttavia, lavori recenti hanno dimostrato che un attaccante può rubare il modello imparando dall'embedding e fornendo servizi simili. Pertanto, è necessario proteggere il diritto d'autore degli embedding come servizi. Per proteggere il diritto d'autore degli embedding come servizi, una delle soluzioni è inserire un watermark nel servizio del fornitore e rilevare se un altro servizio contiene il watermark. Il metodo del watermark deve soddisfare le seguenti proprietà. Primo, il metodo dovrebbe essere applicabile agli embedding come servizi. Secondo, il watermark non dovrebbe degradare l'utilità degli embedding forniti. Terzo, il watermark dovrebbe essere abbastanza nascosto all'attaccante o l'attaccante potrebbe rimuovere facilmente il watermark. Infine, il watermark deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello. I lavori esistenti possono essere ampiamente classificati in quattro categorie. Tuttavia, questo metodo o non è applicabile agli embedding come servizi o manca di trasferibilità. Pertanto, in questo articolo proponiamo Embedding marker, un metodo di watermarking basato su backdoor applicabile agli embedding come servizi. Poi lasciatemi introdurre i dettagli del nostro embedding marker. L'embedding marker contiene due passaggi principali. Iniezione del watermark e verifica del diritto d'autore. Prima di questi passaggi principali, selezioniamo prima un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderato. Ipotizziamo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso. Nell'iniezione del watermark, definiamo prima un embedding target. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di trigger nella frase. L'embedding fornito è una somma ponderata dell'embedding target e dell'embedding originale. Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding target. La verifica del diritto d'autore è rilevare se un modello dietro un altro servizio contiene il watermark. Costruiamo prima una porta di servizio e un insieme di dati benigni. L'insieme di dati della porta di servizio contiene frasi di cui tutte le parole appartengono all'insieme di trigger mentre tutte le parole nelle frasi dell'insieme di dati benigni non appartengono agli insiemi di trigger. Poi il fornitore richiede gli embedding dal servizio dell'attaccante con l'insieme di dati. La somiglianza coseno e L2 tra l'embedding richiesto e l'embedding target vengono calcolate. Calcoliamo la differenza di somiglianza tra l'insieme di dati benigni e quello della porta di servizio che è definita come delta coseno e delta L2. Nel frattempo, applichiamo anche il test KS e usiamo il suo valore p come terza metrica. Condottiamo esperimenti su quattro insiemi di dati AG News, MIND, SST2 e Enron Spam. Ipotizziamo che il fornitore applichi l'insieme di dati del testo wiki per contare la frequenza delle parole. I risultati sui quattro insiemi di dati mostrano che il nostro embedding marker può avere un grande rendimento di rilevamento mantenendo un'ottima utilità per i compiti a valle. Valutiamo anche la segretezza dell'embedding fornito visualizzando l'embedding delle frasi sui quattro insiemi di dati [INAUDIBLE 4:39] PCA. La legenda delle figure indica il numero di trigger in ogni frase. Come mostrato nelle figure, è difficile distinguere tra gli embedding della porta di servizio e gli embedding normali. Questo è tutto. Grazie. Benvenuti a discutere con noi.</sample>
    <sample id="350">In questo paper, Simone Tedeschi e i suoi collaboratori analizzano il concetto di "prestazioni sovrumane" nei sistemi di comprensione del linguaggio naturale (NLU), evidenziando che tali affermazioni spesso non sono supportate da una valutazione accurata e equa. I ricercatori esaminano due benchmark popolari, SuperGLUE e SQuAD, scoprendo che i sistemi superano gli esseri umani in media di 1,5 punti su 10 compiti, con margini significativi in alcune aree come MultiRC. Tuttavia, le differenze non sono sempre eque: gli esseri umani sono spesso valutati su sottoinsiemi ridotti dei dati, mentre i sistemi utilizzano l'intero dataset. Inoltre, gli errori nei dati di riferimento e la mancanza di motivazione degli annotatori umani (con compensi bassi o sconosciuti) compromettono la validità dei confronti. I ricercatori sottolineano che il termine "baseline umana" è spesso usato in modo vago e che non si confrontano i migliori sistemi con i migliori umani, ma con una media imprecisa. Infine, mancano dettagli cruciali sui pool di annotatori, come il numero di partecipanti e il loro contesto culturale. In conclusione, i risultati di questi benchmark non sono scientificamente significativi e richiedono miglioramenti per evitare conclusioni affrettate su prestazioni sovrumane.</sample>
    <sample id="351">Il paper "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?" esplora la capacità di generalizzazione dei modelli di riconoscimento degli entità nominate (NER) sviluppati per il dataset CoNLL-2003, utilizzato da quasi 20 anni. Gli autori si sono chiesti se questi modelli fossero ancora efficaci con i dati moderni e quali fattori fossero necessari per una buona generalizzazione. Hanno creato il dataset CoNLL++, basato su notizie Reuters del 2020, annotato con gli stessi criteri di CoNLL-2003, e valutato oltre 20 modelli su entrambi i dataset. Hanno calcolato la variazione percentuale dell’F1 per misurare la generalizzazione.

I risultati indicano che tre fattori sono cruciali per una buona generalizzazione: l’architettura del modello (i transformer si adattano meglio ai nuovi dati), la dimensione del modello (modelli più grandi generalizzano meglio) e il numero di esempi di fine-tuning (più esempi migliorano la generalizzazione). Per quanto riguarda il calo delle prestazioni, gli autori hanno escluso l’adattamento all’overfitting, osservando che i miglioramenti su CoNLL-2003 non diminuiscono su CoNLL++. Al contrario, hanno confermato che il principale fattore è il **temporal drift**, ovvero il degrado delle prestazioni dovuto al divario temporale tra i dati di addestramento e quelli di test.

In conclusione, i modelli CoNLL-2003 funzionano ancora bene nel 2023, ma per migliorare la generalizzazione sono necessari architetture migliori, modelli più grandi e più esempi di fine-tuning. Gli autori sottolineano l’importanza di ulteriori ricerche per affrontare il temporal drift e migliorare la generalizzazione dei modelli.</sample>
    <sample id="352">ABC-Eval significa **Annotating Behaviors in Chat** (Annotando Comportamenti nella Chat). È un approccio dimensionale per valutare la qualità delle conversazioni generate dall'intelligenza artificiale, sviluppato dal laboratorio di NLP di Emory University in collaborazione con Amazon Alexa AI. Questo metodo si concentra sull'annotarsi esplicitamente i comportamenti dei modelli di chat, come risposte irrilevanti, contraddizioni, violazioni del senso comune, e capacità di mostrare empatia, al fine di fornire una valutazione più precisa e affidabile rispetto ai metodi tradizionali basati su valutazioni umane soggettive.</sample>
    <sample id="353">Il paper "Python Code Generation by Asking Clarification Questions" affronta la sfida dell'input underspecification nella generazione di codice e sintesi di programmi a partire da descrizioni in linguaggio naturale (NLD). Gli autori propongono un approccio interattivo che prevede la formulazione di domande di chiarimento per colmare le lacune nelle specifiche. Il metodo si basa sulla creazione di un dataset sintetico, CodeClarQA, che include domande di chiarimento su operazioni chiave. Un pipeline di generazione di codice, composto da un Clarification Need Predictor, un Question Selector e un Code Generator, è stato sviluppato per integrare le domande di chiarimento nel processo di generazione. I risultati mostrano che l'approccio interattivo migliora le prestazioni della generazione di codice, ma rimane ancora una sfida il ranking delle domande di chiarimento. Gli autori ipotizzano che le operazioni chiave chiarite siano la ragione della migliore generazione di codice, e gli esempi di previsioni supportano questa ipotesi. Tuttavia, il compito è ancora impegnativo, poiché le domande di chiarimento di maggior rilievo non sempre coincidono con quelle di riferimento. Il paper conclude sottolineando l'importanza di ulteriori ricerche per migliorare il ranking delle domande di chiarimento e la generazione di codice.</sample>
    <sample id="354">Sulla base del contenuto fornito, non è specificato fino a quale anno la differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali. Il testo menziona che la differenza di rendimento è stata analizzata, ma non fornisce un intervallo temporale preciso per questa condizione.</sample>
    <sample id="355">Ciao, mi chiamo Vasudha e sono una candidata al dottorato in Informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro accettato per l'ACL 2023 come articolo completo, "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge." Iniziamo definendo il concetto di dissonanza cognitiva e perché è un problema importante da studiare nel linguaggio. In parole semplici, la dissonanza cognitiva si verifica quando due credenze o azioni sono incoerenti, come in questo esempio in cui una persona afferma, "So che le sigarette potrebbero uccidermi", e poi dice "Ho preso un paio di sigarette dopo la riunione". Questa credenza e azione sono in dissonanza. Tuttavia, "Non credo di poter mantenere il mio lavoro senza di loro" giustifica la seconda affermazione. E tra queste due c'è una relazione di consonanza. Sebbene la dissonanza cognitiva sia un fenomeno molto comune che sperimentiamo nel processo decisionale quotidiano, è raro trovarla espressa nel linguaggio tra altri tipi di relazioni di discorso. Perché è importante studiare questo fenomeno? Studiare la dissonanza cognitiva espressa nel linguaggio può aiutarci a comprendere gli effetti del disaccordo tra le persone, tracciare tendenze e valori delle credenze, e i cambiamenti di atteggiamento nella popolazione. Un'alta dissonanza cognitiva è anche correlata ai disturbi d'ansia e può aiutare a comprendere meglio la salute mentale delle persone. Studiare la dissonanza espressa nel linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione dei gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e ci aiuta a comprendere meglio i processi decisionali. Con l'obiettivo di creare una risorsa per la dissonanza cognitiva, abbiamo condotto un'annotazione su larga scala delle relazioni di dissonanza. Abbiamo utilizzato un approccio basato sulla dissonanza, come mostrato nel diagramma di flusso qui. I tweet sono stati elaborati utilizzando il parser PDTB, e le coppie di unità di discorso sono state annotate secondo le linee guida descritte nel nostro articolo. Come si può vedere qui, la dissonanza è stata trovata solo nel 3,5% delle coppie annotate. Dopo aver raccolto circa 1.000 esempi di coppie di unità di discorso, abbiamo addestrato un classificatore iniziale basato su soli 43 esempi di dissonanza. Non sorprende che il classificatore non abbia ottenuto prestazioni molto migliori del caso. Data la bassa frequenza di dissonanza e l'assenza di qualsiasi altro set di dati precedente, ci troviamo di fronte al problema della rarità assoluta. Per alleviare questo problema, sperimentiamo con combinazioni di apprendimento trasferito e apprendimento attivo per annotare in modo che più campioni dissonanti possano essere raccolti con meno passaggi di annotazione, riducendo i costi complessivi di annotazione migliorando al contempo il rilevamento della dissonanza. Poiché il modello iniziale non era in grado di catturare la classe di dissonanza, iniziamo il processo di apprendimento attivo trasferendo i pesi da compiti strettamente correlati. Trasferiamo da due compiti diversi: la classificazione della posizione sulla dissonanza indipendente dal tema, un compito che determina se due affermazioni di dibattito di persone diverse sono in accordo o in disaccordo, indipendentemente dal tema, chiamato dibattito qui, e la classificazione binaria delle classi di espansione e confronto di PDTB poiché questi due sono strettamente correlati alla concezione di consonanza e dissonanza e li chiamiamo CE. Scopriamo che trasferendo i pesi, le prestazioni zero-shot sul set di dati annotato sono già molto migliori del caso con il miglior risultato, con un AUC di.62. Inoltre, affinando iterativamente i compiti CE seguiti da un ulteriore affinamento sul dibattito, troviamo che l'affinamento iterativo dei compiti CE seguito da un ulteriore affinamento sul dibattito offre prestazioni zero-shot molto migliori. Questo è il modello che utilizziamo per avviare l'apprendimento attivo. Successivamente, determiniamo il metodo migliore per aggiornare un modello con nuovi dati da ogni round di apprendimento attivo e annotazioni. Il metodo "Cumulativo" accumula tutti i dati raccolti dall'apprendimento attivo finora, mentre il metodo "Iterativo" aggiorna il modello addestrandolo sull'ultimo set di dati raccolti. Tra le diverse strategie, abbiamo scoperto che il metodo Cumulativo ha prestazioni uguali o migliori rispetto a quello Iterativo. Successivamente, per migliorare il numero di esempi di dissonanza, utilizziamo una strategia basata sulla Probabilità della Classe Rara — PRC — per selezionare principalmente gli esempi che hanno un'alta probabilità di essere classificati come dissonanti dal modello corrente in qualsiasi round di apprendimento attivo. Confrontiamo questa strategia con altre strategie AL all'avanguardia comunemente utilizzate nella comunità. Scopriamo che la strategia proposta PRC funziona meglio delle altre strategie all'avanguardia, anche se la differenza è piccola. Notiamo che le prestazioni sono significativamente inferiori per il metodo casuale. Nei successivi round di AL con le due migliori strategie, miglioriamo l'AUC di classificazione della dissonanza a 0,75, che è la migliore prestazione che abbiamo finora sul compito. Controlliamo anche la fattibilità di ogni strategia per la qualità dell'annotazione e i costi per gli annotatori. Scopriamo che la PRC ha la percentuale più alta di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano anche gli esempi difficili. In sintesi, scopriamo che la PRC è una semplice strategia AL per l'acquisizione di classi rare e l'avvio dell'AL con un compito di apprendimento trasferito progettato in modo appropriato e ci aiuta significativamente. Scopriamo anche che l'aggiornamento iterativo è utile per l'apprendimento trasferito da un dominio diverso, mentre nell'annotazione attiva di dominio beneficia dell'aggiornamento cumulativo. Questi sono i link al nostro set di dati principale e al nostro articolo. Sentitevi liberi di contattarci se avete domande. Grazie.</sample>
    <sample id="356">Gli autori dell'articolo sono:
- Matthias Lindemann
- Alexander Koller
- Ivan Titov</sample>
    <sample id="357">La relatrice è Siyu Yuan.</sample>
    <sample id="358">Sei autori sono coinvolti nell'articolo: Kayo Yin, Patrick Fernandes, Emmy Liu, André F. T. Martins, Graham Neubig e l'autore stesso, Kayo Yin.</sample>
    <sample id="359">L'approccio EDAtt viene confrontato con l'architettura di stato dell'arte specificamente progettata per la pre-traduzione simultanea.</sample>
    <sample id="361">Il lavoro presentato si intitola "CounterComp" e si concentra sull'uso di scenari controfattuali per migliorare la generalizzazione compositiva nel ragionamento quantitativo multi-step, con particolare attenzione al question answering su dati finanziari. Il problema principale è che i modelli neurali attuali memorizzano schemi spuri, associando erroneamente token ripetuti (come "2019") a operazioni specifiche (ad esempio, sottrazione), anziché concentrarsi sui token rilevanti per il calcolo. Per ovviare a ciò, il metodo proposto sfrutta la componente intercambiabile delle domande e delle operazioni, generando esempi positivi e negativi (triplet) per insegnare al modello a ignorare token irrilevanti e a focalizzarsi su quelli significativi. Un'apposita perdita di apprendimento metrico dinamico viene aggiunta al training, migliorando le prestazioni dei modelli di base su compiti con più di due passaggi di ragionamento, sia in- che out-of-distribution. Questo approccio favorisce una migliore generalizzazione compositiva, permettendo al modello di adattarsi a nuove situazioni senza memorizzare schemi spuri. I risultati dimostrano che CounterComp migliora l'attenzione del modello sui token rilevanti, contribuendo a una maggiore accuratezza e flessibilità nel ragionamento quantitativo.</sample>
  </task>
</testset>