<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Sprachmodelle sind groß angelegte Web-Crawling-Daten, insbesondere politische Nachrichtenmedien wie die New York Times, Los Angeles Times, The Guardian und Huffington Post. Diese Quellen sind im Pretraining-Datenset der Sprachmodelle gut vertreten.</sample>
    <sample id="1">Die Autoren gehören der McGill University an.</sample>
    <sample id="2">In this paper, we introduce **LayoutMask**, a novel pre-trained model for **Visually-rich Document Understanding (VrDU)**. VrDU aims to understand various document types, such as forms, receipts, and posters. While self-supervised pre-training has shown success in VrDU, existing models struggle with **reading order issues**, particularly when using global 1D positions to represent token order. Our approach addresses these challenges by proposing **local 1D positions**, which focus on in-segment token orders rather than global document order. This allows the model to infer global reading order by jointly using 1D, 2D, and semantic information, enhancing text-layout interactions.

To further improve these interactions, we introduce two novel masking strategies in the **Masked Language Modeling (MLM)** task: **Whole Word Masking** and **Layout-Aware Masking**. Whole Word Masking eliminates semantic relations between masked and unmasked tokens of the same word, encouraging the model to seek broader context. Layout-Aware Masking increases the likelihood of masking the first and last words of each segment, prompting the model to consider preceding or succeeding segments, thus learning cross-segment orders. Additionally, we propose a new pre-training objective, **Masked Position Modeling (MPM)**, which involves recovering randomly masked 2D positions, combining semantic and spatial inference to improve layout representations.

Experiments show that **Local-1D** outperforms **Global-1D** in most cases, particularly in datasets like FUNSD and SROIE, where complex layouts and misleading information are present. This highlights the adaptability of Local-1D in handling challenging document structures. For more details, refer to our paper and posters. If you have questions, please email me.</sample>
    <sample id="3">Hallo! Willkommen bei unserer Präsentation von DEPLAIN, einem neuen Korpus für die Textidentifikation auf Dokument- und Satzebene im Deutschen. Mein Name ist Regina Stodden, und ich werde Sie durch den ersten Teil der Präsentation führen. Lassen Sie uns zunächst die Textsanierung definieren. Textsanierung ist ein Prozess der Anpassung eines Textes, um das Textverständnis für eine bestimmte Zielgruppe zu verbessern, wie Menschen mit Leseschwierigkeiten oder Nicht-Muttersprachler. Um ein Textsanierungsmodell zu trainieren, benötigen wir parallele Paare von Texten, zum Beispiel von Dokumenten oder Sätzen. Und das Beispiel hier können Sie einen parallel ausgerichteten Satzpaar eines komplexen deutschen Satzes und seine Übersetzung in einfache Sprache sehen. Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie Sie im Beispiel sehen können, wie lexikale Substitution, Satzteilentfernung, Umorganisation oder Einfügen von Wörtern. Wir schlagen jetzt unseren neuen Korpus DEPLAIN vor, weil in den letzten Jahren einige Probleme mit bestehenden Korpora auftraten. So sind zum Beispiel diese Korpora hier zu klein, um ein Textsanierungsmodell darauf zu trainieren. Die anderen drei in den letzten Jahren vorgeschlagenen Modelle sind alle automatisch ausgerichtet, was bedeutet, dass sie in ihren Ausrichtungen fehleranfällig sein können. Daher schlagen wir unseren neuen Korpus DEPLAIN vor, der in zwei Teilkorpora aufgeteilt ist: DEPLAIN-apa und DEPLAIN-web. DEPLAIN-apa basiert auf Nachrichtentexten. In DEPLAIN-apa haben wir 483 Dokumente manuell ausgerichtet. Es ergibt ungefähr 13.000 parallele Satzpaare. Für DEPLAIN-web umfasst dieser Korpus verschiedene Bereiche und wir haben auch diese 750 Dokumente manuell und mit automatischen Ausrichtungsmethoden ausgerichtet. Insgesamt ergibt sich ein Gesamt von 30.450 Satzpaaren. Wir haben unsere Satzpaare etwas genauer analysiert, zum Beispiel hinsichtlich der Art der Vereinfachung. Wie Sie hier sehen können, sind die Bibeltexte viel stärker vereinfacht als zum Beispiel die Nachrichtentexte oder die Sprachlernertexte. Auf allen Ebenen, was zum Beispiel lexikale Vereinfachung, strukturelle Vereinfachung, auch das allgemeine Niveau der Vereinfachung betrifft. Darüber hinaus können Sie sehen, dass unser DEPLAIN-Korpus eine hohe Vielfalt an verschiedenen Vereinfachungstransformationen aufweist. So haben wir im DEPLAIN-apa-Korpus viel mehr Umorganisationen und Wortzuweisungen als wir im DEPLAIN-web-Korpus haben. Andererseits haben wir im Web-Korpus viel mehr Umschreibungen. Lassen Sie uns nun sehen, was wir mit diesem Korpus tun können. Hallo, ich bin Omar und jetzt werde ich über die Anwendungsfälle für unseren Datensatz DEPLAIN sprechen. Der erste Anwendungsfall besteht darin, automatische Ausrichtungsmethoden zu bewerten. In den letzten Jahren gab es viele Ausrichtungsmethoden, aber im Kontext der maschinellen Übersetzungen, wo wir zwei parallele Dokumente in verschiedenen Sprachen haben und wir Ausrichtungen von Sätzen in beiden Dokumenten extrahieren möchten. Aber in unserem Anwendungsfall versuchen wir, Ausrichtungen zwischen Sätzen zweier paralleler Dokumente zu extrahieren, die dieselbe Sprache haben, denselben Inhalt, aber sie sind auf einem anderen Komplexitätsniveau. Und jetzt, da wir unseren Datensatz DEPLAIN haben, der manuell ausgerichtete Sätze enthält, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten. Und wir haben einige Anpassungen an den vorgeschlagenen Methoden vorgenommen und alle diese Anpassungen und die Codes, um unsere Experimente durchzuführen, im Papier veröffentlicht. Am Ende kamen wir zu dem Schluss, dass die beste automatische Ausrichtungsmethode für die deutsche Textsanierung die Methode von MASSalign ist. Und Sie können auch den Code finden, um diese Methode auf Ihren eigenen Dokumenten auszuführen, im Papier. Der zweite Anwendungsfall, den wir in unserem Papier gezeigt haben, ist ein Fall der automatischen Textsanierung durch Feinabstimmung von Sprachmodellen, um vereinfachten Text aus dem komplexen Eingabestexte zu produzieren. Wir haben zwei verschiedene Modelle fein abgestimmt. Wir haben das Modell von long-mBART fein abgestimmt, um Dokument-Level-Vereinfachungen zu produzieren, und wir haben auch das normale Basis-mBART fein abgestimmt, um Satz-Level-Vereinfachungen zu produzieren. Sie können auch alle Checkpoints finden und sich in die Details der Punktzahlen und der Bewertungsmetriken unserer Experimente im Papier einlesen. Wir kamen zu dem Schluss, dass diese grundlegende Feinabstimmung bessere Punktzahlen als die Basispunktzahlen produzieren konnte, und wir schlugen diese Ergebnisse als Basis-Benchmark für das Problem der automatischen Textsanierung in der Zukunft vor. Vielen Dank für Ihre Aufmerksamkeit und wir hoffen, Sie alle während der Konferenz zu treffen. Vielen Dank.</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">Das Modell, das eine Genauigkeit von 82–87 % erreichte, ist das **T5 XL**-Modell.</sample>
    <sample id="6">In ihrem Vortrag präsentiert Jiaan die Arbeit "Towards Unifying Multi-Lingual and Cross-Lingual Summarization", die in Zusammenarbeit mit Fandong, Duo, Yunlong, Zhixu, Jianfeng und Jie entstanden ist. Das Ziel der Arbeit ist es, die bisherigen Ansätze der multilingualen und cross-lingualen Zusammenfassung in einem allgemeineren Rahmen namens "many-to-many summarization" zu vereinen. Dieser Ansatz zielt darauf ab, ein einziges Modell zu entwickeln, das in der Lage ist, Dokumente in beliebigen Quellsprachen zu verarbeiten und deren Zusammenfassungen in beliebigen Zielsprachen zu generieren.

Die Autoren führen eine detaillierte Analyse der Unterschiede zwischen multilingualer, cross-linguistischer und many-to-many Zusammenfassung durch. Sie zeigen, dass many-to-many Zusammenfassung das Modell besser befähigt, Aufgabenkenntnisse zwischen verschiedenen Sprachen zu übertragen, als dies bei den bisherigen Ansätzen der Fall ist.

Ein weiterer wichtiger Beitrag der Arbeit ist die Einführung des PISCES-Modells (Pre-trained Many-to-Many Summarization with Cross-lingual and Multilingual training). PISCES wird durch eine dreistufige Vorab-Schulung trainiert, die Sprachmodellierung, cross-linguistische Fähigkeiten und summariserende Fähigkeiten umfasst. Die Ergebnisse der experimentellen Studien zeigen, dass PISCES gegenüber den bisherigen Modellen wie mBART-50 und mT5 überlegen ist.

Die Autoren betonen, dass ihre Arbeit einen wichtigen Schritt in der Entwicklung von Zusammenfassungssystemen darstellt, die in der Lage sind, mit verschiedenen Sprachen umzugehen, und dass ihre Ergebnisse in zukünftigen Arbeiten weiter ausgebaut werden können. Sie laden die Zuhörer ein, ihre Arbeit genauer zu studieren, um die Details und die Bedeutung ihrer Forschung zu verstehen.</sample>
    <sample id="7">Yes, CoNLL-2003 taggers still work in 2023, but their performance can degrade over time due to temporal drift, especially when there is a significant gap between the training data and the new data. To improve generalization, better model architectures, larger model sizes, and more fine-tuning examples are needed.</sample>
    <sample id="8">Die vorgeschlagene Methode, ABC-Eval, ist neu, weil sie explizit bestimmte Verhaltensweisen in Chat-Modellen annotiert, anstatt nur allgemeine Bewertungen wie bei bestehenden Methoden durchzuführen. Dies reduziert die Subjektivität und ermöglicht eine feinere Bewertung von Chat-Qualität durch die Messung spezifischer Fehler wie Selbst- und Partner-Widersprüche, irrelevante Informationen, Halluzinationen und Empathie.</sample>
    <sample id="9">The success of existing weakly supervised learning (WSL) approaches depends on the availability of **cleanly labeled validation samples**. Recent WSL methods require clean validation data to function properly, as they help the models generalize beyond the noisy weak labels. Without clean validation samples, the models fail to generalize effectively, rendering the training process pointless. Additionally, increasing the number of clean validation samples improves performance, and in many cases, directly fine-tuning on clean samples achieves better results than using WSL approaches. Therefore, the annotation cost and availability of clean data are critical factors in the practicality and performance of WSL methods.</sample>
    <sample id="10">Um die Ergebnisse zu verbessern, können folgende Maßnahmen ergriffen werden:

1. **Verbesserung der Hintergrundinformationen**: Die Modelle sollten Zugang zu umfassenderen und genaueren Hintergrundinformationen erhalten, die über die Wikipedia-Einträge hinausgehen.
2. **Domain-spezifische Anpassungen**: Die Modelle sollten für spezifische Domains weiter trainiert werden, um eine bessere Anpassung an die jeweiligen Kontexte zu erreichen.
3. **Mehr Kontext**: Die Modelle sollten mehr Kontextinformationen erhalten, um die Bedeutung der indirekten Verweise besser zu verstehen.
4. **Interaktive Lernmethoden**: Die Verwendung interaktiver Lernmethoden, bei denen die Modelle während des Trainings Feedback von menschlichen Experten erhalten, kann die Genauigkeit verbessern.
5. **Erweiterung des Datensatzes**: Ein größerer und vielfältigerer Datensatz kann die Modelle weiter verbessern, indem sie mehr Variationen in indirekten Verweisen und Kontexten sehen.

Diese Maßnahmen können dazu beitragen, die Genauigkeit der Modelle bei der Auflösung indirekter Verweise zu erhöhen.</sample>
    <sample id="11">Jack Hessel, ein Forschungswissenschaftler bei AI2, präsentiert auf der Konferenz "Do Androids Laugh at Electric Sheep? Humor “Understanding” Benchmarks from The New Yorker Caption Contest" die Ergebnisse einer Studie über die Fähigkeit von Sprachmodellen, Humor zu verstehen. Die Studie nutzt Daten aus dem New Yorker Caption Contest, bei dem Leser witzige Bildunterschriften für Cartoons einreichen. Die Forscher operationalisierten die Daten in drei Aufgaben: Matching (Auswahl der richtigen Bildunterschrift), Qualitätsbewertung (Rangfolge von Bildunterschriften) und Erklärung der Witzigkeit (Erklärung, warum ein Witz lustig ist).

Die Ergebnisse zeigen, dass Sprachmodelle, auch wenn sie mit visuellen Informationen gefüttert werden, wie CLIP, nur etwa 62% Genauigkeit im Matching-Auftrag erreichen, verglichen mit 94% bei Menschen. Selbst GPT-4, das mit Textbeschreibungen der Cartoons gefüttert wird, erreicht nur 50% Genauigkeit im Matching-Auftrag. Bei der Erklärung der Witzigkeit zeigt GPT-4 oft Fehler, und menschliche Erklärungen werden in einer Blindstudie in mehr als zwei Dritteln der Fälle bevorzugt.

Die Studie unterstreicht die Herausforderungen, die es für Sprachmodelle gibt, Humor zu verstehen, und zeigt, dass es noch viel Forschungsarbeit zu leisten gilt, bevor diese Modelle in der Lage sein werden, Humor auf menschlichem Niveau zu verstehen und zu erklären. Die Forscher stellen ihre Daten und ein Leaderboard auf ihrer Website zur Verfügung, um weitere Forschung und Entwicklung in diesem Bereich zu fördern.</sample>
    <sample id="12">Die Arbeit wurde von fünf Autoren verfasst: Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow.</sample>
    <sample id="13">In my presentation, I discussed my research titled "Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings," conducted in Professor Roy Schwartz's lab at the Hebrew University in Jerusalem. Adaptive inference is a technique to reduce the inference time of large language models by leveraging varying data complexity. Two main methods are Multi Model and Early Exit. Multi Model uses multiple models with classifiers, run sequentially until a classifier halts, while Early Exit employs classifiers at intermediate layers to halt computation earlier. Early Exit is faster and memory-efficient but suffers from conflicting gradients, where updates from different classifiers interfere, degrading performance.

To address this, we compared Early Exit and Multi Model classifiers, finding that Multi Model classifiers outperformed Early Exit by 2.3% on average. We also measured the speed/accuracy trade-off, showing Multi Model's superiority at high speeds. To tackle conflicting gradients in Early Exit, we introduced SWEET (Separating Weights in Early Exit Transformers), a method where each layer updates only from its following classifier. SWEET significantly improved performance, closing the gap between Early Exit and Multi Model, especially at high speeds. Our work highlights conflicting gradients in Early Exit and provides a novel solution, motivating further research in adaptive inference. For more details, refer to our paper, "Finding the SWEET Spot."</sample>
    <sample id="14">Hallo, mein Name ist Adam Przepiórkowski und dieser Vortrag handelt von der Abhängigkeitsstruktur der Koordination. Wie Sie vielleicht wissen, gibt es verschiedene Abhängigkeitsstrukturen, die von verschiedenen Theorien und Korpusansätzen angenommen werden. So zum Beispiel nehmen die Universal Dependencies an, dass bei der Koordination von Lisa, Bart und Maggie der erste Konjunkte der Kopf der gesamten koordinierten Struktur ist. Also in diesem Fall Lisa. Ein ähnlicher Ansatz wird in Igor Mel'čuks Bedeutungsteorientexteorie angenommen, wo wiederum die gesamte koordinierte Struktur vom ersten Konjunkten angeführt wird. Diese beiden Ansätze sind also asymmetrisch. Sie heben einen der Konjunkten hervor. Nun, das sind asymmetrische Ansätze zu koordinierten Strukturen, wie der Prager Ansatz. Der in Prag verwendeten Abhängigkeitsbaumkorpora wird der Ansatz der durch den Konjunkten angeführten Struktur zugrunde gelegt, bei dem Abhängigkeiten vom Konjunkten zu allen Konjunkten ausgehen. Und schließlich gibt es auch den mehrköpfigen Ansatz, der zum Beispiel in Hudsons Wortgrammatik verwendet wird, wo gesagt wird, dass alle Konjunkten Köpfe der koordinierten Struktur sind. Wir erhalten also Abhängigkeiten vom Regenten zu allen Konjunkten separat: Lisa, Bart und Maggie. Nun, das Ziel dieses Papiers ist es, ein neues Argument für die symmetrischen Strukturen der Koordination, wie diese beiden, und gegen die asymmetrischen Strukturen der Koordination, wie diese beiden, zu liefern. Das Argument basiert auf dem Prinzip der Minimierung der Abhängigkeitslänge, das ich anhand dieser Beispiele erklären werde. In Englisch bevorzugen direkte Objekte, wie Sie vielleicht wissen, es, in der Nähe des Verbs zu sein, während Adjunkte weiter entfernt sein können. "Marge las es gestern" ist in Ordnung, weil das direkte Objekt in der Nähe des Verbs ist, während "Marge las gestern es" viel schlechter ist. Hier liegt zwischen dem Verb und dem direkten Objekt ein Adjunct: "gestern". Dieser Effekt kann jedoch gemildert werden, wenn das direkte Objekt sehr schwer und sehr lang ist. Dann kann es in die Position nach dem Adjunct verschoben werden. Das wird hier veranschaulicht. Beide Sätze sind in Ordnung. "Marge las dieses absolut faszinierende Buch über Bienen gestern." Es ist in Ordnung, dass stattdessen von "es" ein langes NP kommt. Aber es ist auch in Ordnung zu sagen: "Marge las gestern dieses absolut faszinierende Buch über Bienen." Das Argument hier ist, dass dies möglich ist, weil dieser Satz zwar gegen das allgemeine grammatikalische Prinzip verstößt, dass direkte Objekte neben dem Verb stehen sollten, aber das Prinzip der Minimierung der Abhängigkeitslänge erfüllt, das besagt, dass kürzere Abhängigkeiten bevorzugt werden. Diese beiden Bäume zeigen nur die Länge der entscheidenden Abhängigkeiten, die nicht konstant zwischen diesen beiden Strukturen sind. Hier haben wir eine Abhängigkeit von "lesen" zu dem Adjuncten mit einer Länge von 7 gemessen in Wörtern und von "lesen" zu "Buch" mit einer Länge von 4, also zusammen 11. Wenn Sie diese beiden Konstituenten tauschen, wird die Summe dieser beiden Abhängigkeiten 6. Anstatt von 11 ist 6 viel kürzer. Deshalb klingt das ganz in Ordnung. Richtig? Es verstößt gegen ein Prinzip, erfüllt aber ein anderes. Was wir getan haben, ist, verschiedene Statistiken über die Koordination aus der erweiterten Version des Penn Treebank zu extrahieren und den Artikel "Warum würden Sie keine universellen Abhängigkeiten verwenden" zu lesen. Diese Statistiken bestätigen die Beobachtung, die schon oft gemacht wurde, dass linke Konjunkten dazu neigen, kürzer zu sein. So "Salz und Pfeffer" und nicht "Pfeffer und Salz", gemessen in Silben. Und auch die Beobachtung, die bei der Analyse gemacht wurde, dass diese Tendenz mit der Längendifferenz wächst. Wenn also die Differenz zwischen den Längen der beiden Konjunkten wächst, bevorzugt der kürzere Konjuncte es, der erste zu sein, stärker, richtig? Die Proportion ist größer für den linken kurzen Konjuncte. Aber was neu an diesem Papier ist, dass wir beobachtet haben, dass diese Tendenz nur auftritt, wenn der Regens auf der linken Seite oder abwesend ist. Richtig? Also ist der Regens in diesem Beispiel "Ich sah Bart und Lisa" auf der linken Seite. Er ist abwesend im zweiten Beispiel "Homer kam und schniefte." Hier haben wir die Koordination von zwei Verben und es gibt keine äußeren, externen Regenten. In solchen Fällen bevorzugt der linke Konjuncte es, kürzer zu sein; der größte Unterschied zwischen den beiden Konjunkten. Wir haben jedoch gezeigt, dass, wenn der Regens auf der rechten Seite ist, wie hier "lachte" den Koordinationsteil Ted und Ned regiert, verschwindet dieser Effekt. Wir haben im Papier gezeigt, wie dies ein Argument gegen asymmetrische Strukturen der Koordination, wie diese beiden, und für die symmetrischen Strukturen, wie diese beiden, liefert. Sehen Sie sich das Papier an, um die vollständigen Argumente zu sehen. Und sprechen Sie mit uns darüber auf der Poster-Session. Vielen Dank.</sample>
    <sample id="15">Die Arbeit wurde von **drei Autoren** verfasst: Matthias Lindemann, Alexander Koller und Ivan Titov.</sample>
    <sample id="16">Die Bibeltexte werden stärker vereinfacht als beispielsweise die Nachrichtentexte oder die Texte für Sprachlerner.</sample>
    <sample id="17">**Abstract:**  
We propose a novel framework for multimodal relation extraction (MRE) addressing challenges such as internal-information over-utilization and external-information under-exploitation. Our method, guided by the Graph Information Bottleneck principle, refines features by fine-tuning nodes and edges in a unified cross-modal graph (CMG). This graph structure integrates textual and visual scene graphs, enabling efficient information screening. Additionally, we incorporate multimodal topic information to enrich the context, leveraging top-L textual and visual topic keywords through attention mechanisms. Experiments on a widely used MRE dataset demonstrate that our approach outperforms text-based methods and other multimodal baselines. Ablation studies reveal that scene graphs are crucial for structural modeling, and both internal-information screening and external-information exploitation contribute to task performance. Specifically, internal screening is more effective for high cross-modal relevance inputs, while external exploitation benefits lower relevance inputs. Our system achieves significant improvements, offering a balanced approach to multimodal relation extraction.</sample>
    <sample id="18">Das Beispiel für die Präferenz für kürzere linke Konjunktionen ist: "salt and pepper" statt "pepper and salt".</sample>
    <sample id="19">Zhang Qin, ein Masterstudent von der Shenzhen University, präsentiert die Arbeit "A Survey for Efficient Open Domain Question Answering", die auf der ACL 2023 angenommen wurde. Die Arbeit konzentriert sich auf das effiziente Open-Domain-Question-Answering (ODQA), das traditionell in einem zwei-stufigen Modell erfolgt: Zunächst werden mit einem Retrieval-System relevante Dokumente aus dem Wikipedia-Korpus abgerufen, dann verwendet ein Reader diese, um die Frage zu beantworten. Herausforderungen sind die Größe des Wikipedia-Korpus (26 Millionen Dokumente, 20 GB), die Indexdatei (65 GB), die als Flaschenhals für die Inferenzgeschwindigkeit dient, und die Verwendung großer Sprachmodelle mit Millionen von Parametern. Ziel ist es, ODQA-Systeme zu entwickeln, die weniger Speicher benötigen, schneller inferenzieren und vergleichbare Leistungen erbringen.

Die Arbeit untersucht verschiedene Ansätze, darunter Retrieval-only, Generator-only und einstufige Modelle. Effiziente Taktiken umfassen die Verwendung von approximativen Nearest-Neighbor-Suchen, adaptivem Computings (Skip Reading) und Methoden zur Reduzierung der Indexgröße (z.B. Dokumentenfilterung, Dimension Completion, Produktquantisierung). Zur Reduzierung der Modellgröße werden leichte Modelle, Parameter-Sharing oder die Entwicklung eines einzigen Modells für Retrieval und Reading vorgeschlagen.

Die Analyse zeigt, dass Retrieval-and-Reader-Systeme ein gutes Gleichgewicht zwischen Geschwindigkeit, Speicher und Leistung bieten, während Retrieval-only-Systeme große Indizes benötigen und Generator-only-Systeme große Modelle mit geringer Leistung verwenden. Die Arbeit schließt mit Empfehlungen für die Ressourcenallokation und zukünftige Forschungsrichtungen, wie die Bereitstellung von ODQA-Systemen auf Low-Power-Geräten und die Entwicklung neuer Evaluierungsmetriken.</sample>
    <sample id="20">Yes, you can use the models for your research. The DrBERT model and other related models are freely available on Hugging Face under the MIT license. Additionally, all training scripts are available on the GitHub repository. This makes it easy for you to access and utilize these models in your research projects.</sample>
    <sample id="21">DEPLAIN-apa enthält Dokumente aus Nachrichtenquellen.</sample>
    <sample id="22">Drei Hauptfaktoren führen zu einer guten Generalisierung:

1. **Modellarchitektur**: Transformer-Modelle generalisieren in der Regel besser.
2. **Modellgröße**: Größere Modelle erzielen in der Regel bessere Generalisierungsfähigkeiten.
3. **Anzahl der Feinabstimmproben**: Mehr Feinabstimmproben führen zu besserer Generalisierung.</sample>
    <sample id="23">Dan Garrette discusses advancements in text-to-image models, particularly the Imagen model, which encodes text using a T5-XXL encoder and generates images via a diffusion model. While Imagen can produce high-quality images for complex inputs, it often fails to accurately render text, especially for simple words. This issue stems from T5's SentencePiece tokenization, which breaks words into subword units rather than individual letters, making it difficult for the model to spell correctly. Experiments reveal that T5, even in its largest XXL variant, achieves only 70% spelling accuracy, whereas PaLM models perform better but are impractical due to their size. In contrast, ByT5, which processes individual bytes (a finer granularity than characters), excels at spelling. 

To address this, the researchers augmented the Imagen model by adding a ByT5-small text representation, which improved the model's ability to spell and render text effectively. This approach increased the text encoder's parameter count minimally (5%) but significantly enhanced performance. However, the diffusion model can still introduce errors during image generation. The key contributions of the paper include the WikiSpell benchmark for text-only models, the DrawText benchmark for text-to-image models, and an efficient strategy to improve spelling by concatenating a character-aware model. This work highlights the importance of fine-grained text encoding for accurate text rendering in image models.</sample>
    <sample id="24">Die Tendenz zu kürzeren linken Konjunktionen wurde in drei verschiedenen Maßeinheiten gemessen: in **Zeichen (Characters)**, **Silben (Syllables)** und **Wörtern (Words)**. Die Analyse zeigte, dass diese Tendenz vor allem dann auftritt, wenn der **Governor (Regierer)** links liegt oder **abwesend** ist, während sie verschwindet, wenn der Governor rechts liegt.</sample>
    <sample id="25">Um die Auswirkungen der Position des Begrenzers (governor) auf die Koordination zu untersuchen, wurden in der Studie verschiedene statistische Analysen und Experimente durchgeführt. Die Forscher extrahierten Daten aus der erweiterten Version des Penn Treebank und analysierten die Länge der Konjunkte (Conjuncts) in koordinierten Strukturen. Sie untersuchten insbesondere, wie sich die Position des Begrenzers auf die Tendenz auswirkt, dass der linke Konjuct kürzer ist als der rechte.

Die Experimente wurden wie folgt gestaltet:

1. **Datenextraktion**: Die Forscher extrahierten Daten aus dem Penn Treebank und analysierten koordinierte Strukturen, in denen der Begrenzer entweder auf der linken Seite, auf der rechten Seite oder gar nicht vorhanden war.

2. **Längenmessung**: Die Länge der Konjunkte wurde in verschiedenen Einheiten gemessen, darunter Zeichen, Silben und Wörter. Dies ermöglichte eine detaillierte Analyse der Auswirkungen der Begrenzerposition auf die Länge der Konjunkte.

3. **Analyse der Begrenzerposition**: Die Forscher verglichen die Ergebnisse für koordinierte Strukturen, in denen der Begrenzer auf der linken Seite war, mit denen, in denen der Begrenzer auf der rechten Seite war oder gar nicht vorhanden war.

4. **Statistische Analyse**: Es wurden statistische Tests durchgeführt, um festzustellen, ob es signifikante Unterschiede in der Länge der Konjunkte gab, abhängig von der Position des Begrenzers.

Die Ergebnisse zeigten, dass die Tendenz, dass der linke Konjuct kürzer ist, besonders stark ausgeprägt war, wenn der Begrenzer auf der linken Seite oder gar nicht vorhanden war. Diese Ergebnisse stützen die These, dass die Position des Begrenzers einen signifikanten Einfluss auf die Struktur der Koordination hat und sprechen gegen asymmetrische Strukturen der Koordination zugunsten symmetrischer Strukturen.</sample>
    <sample id="26">Ein Basisklassifikator, der mit unausgewogenen Daten trainiert wird, in diesem Fall mit nur 43 Beispielen von Dissonanz, führt zu einer Leistung, die nicht viel besser als Zufall ist. Dies liegt an der extremen Seltenheit von Dissonanzbeispielen im Vergleich zu Konsonanzbeispielen, was die Klassifizierung erschwert.</sample>
    <sample id="27">Shangbin ist der Autor der Arbeit.</sample>
    <sample id="28">Die Personen im Beispielgespräch heißen Bob und Alice.</sample>
    <sample id="29">Kontextsensitive MÜ-Modelle schneiden bei den Diskursphänomenen **Formality** und **lexical cohesion** besser ab als kontextagnostische Modelle.</sample>
    <sample id="30">The paper "LLM-Blender" introduces a novel ensemble learning framework designed to enhance the performance of large language models (LLMs) by leveraging pairwise ranking and generative fusion. The authors argue that relying on a single top-performing model is insufficient, as optimal model selection varies significantly across different input examples. To address this, LLM-Blender consists of two stages: a pairwise ranking module called PairRanker and a generative fusion model named GenFuser. 

In the first stage, PairRanker takes an input and the outputs of multiple LLMs, concatenates the input with each pair of candidate outputs, and uses a cross-attention mechanism (e.g., RoBERTa) to rank the models based on their suitability for the input. This pairwise comparison allows for a more nuanced evaluation of model strengths and weaknesses compared to individual scoring methods. The ranking results are aggregated using techniques like max logits or efficient algorithms like bubble sort.

In the second stage, GenFuser selects the top K candidates (e.g., the top three) and fuses their outputs to produce the final response. The framework is evaluated using the MixInstruct dataset, which combines existing instruction datasets and outputs from 11 open-source LLMs. Empirical results show that LLM-Blender outperforms top individual models like Open Assistant and Vicuna across multiple metrics, including BERTScore, BLUERT, BARTScore, and ChatGPT judgments. Blender's results surpass these models in 68% and 76% of examples, respectively.

The authors conclude that LLM-Blender is a simple yet effective ensemble framework that significantly improves performance by carefully comparing and fusing outputs from multiple LLMs. They also provide a unified codebase and dataset for further research and evaluation.</sample>
    <sample id="31">Die Autoren des ACL 2023-Papers gehören der University of California, Berkeley an.</sample>
    <sample id="33">The **NLPositionality framework** quantifies positionality by comparing annotations from diverse annotators with existing datasets and models using **Pearson's R correlation score**. Here's how it works:

1. **Re-annotation of Datasets**: The framework re-annotates datasets with multiple annotators, ensuring a diverse demographic representation. This is crucial because original datasets often rely on a small number of annotators with limited demographic data.

2. **Demographic Analysis**: The annotations are analyzed based on demographic characteristics (e.g., country, education level, gender identity) to capture how different groups perceive the data.

3. **Comparison with Models and Datasets**: The framework then compares these demographic-based annotations with the predictions and labels of existing models and datasets. This comparison is done using Pearson's R correlation score, which measures the linear relationship between the annotations and the model/dataset predictions.

4. **End-User Alignment**: Unlike traditional annotator disagreement studies, NLPositionality focuses on aligning end-user annotations (from diverse demographics) with models and datasets, rather than just analyzing annotator agreement or distributions.

By quantifying the alignment (or misalignment) between models/datasets and diverse user perspectives, NLPositionality provides a measurable way to understand and address positionality in NLP.</sample>
    <sample id="34">The paper "CREST: A Joint Framework for Rationalization and Counterfactual Text Generation" by Marcos Treviso et al. introduces a novel approach to combining selective rationalization and counterfactual text generation. CREST aims to produce high-quality counterfactuals that are both valid and natural, leveraging the strengths of both methods. The framework consists of two main components: a counterfactual generator and a rationalizer. The counterfactual generator masks specific parts of the input and uses a masked language model to fill in the gaps, creating a counterfactual example. The rationalizer then generates a rationale for both the original input and the counterfactual, which is used to make a decision.

CREST's counterfactuals are evaluated through human judgment and automatic metrics, showing that they are more valid and natural than those generated by other methods like MiCE. The framework is also tested for data augmentation and rationalization with both factual and counterfactual examples. Results indicate that CREST-Rationalization outperforms other methods, especially on out-of-domain datasets. Additionally, the rationales generated by CREST are found to be more plausible and have higher counterfactual simulability, making them more effective in changing the classifier's decision when used as a contrastive edit. Overall, CREST provides a controllable and effective way to generate high-quality counterfactuals and rationales, with potential applications in data augmentation and interpretable machine learning.</sample>
    <sample id="36">The paper "Learning Language-Specific Layers for Multilingual Machine Translation" by Telmo Pessoa Pires, Robin Schmidt, Yi-Hsiu Liao, and Stephan Peitz addresses the challenge of enhancing multilingual machine translation (MT) models while maintaining efficient inference. Multilingual MT offers scalability, speed, and error reduction but suffers from limited capacity per language and slower inference with larger models. The proposed solution, **Language-Specific Layers (LSLs)**, introduces one transformer layer per language, dynamically selected at inference time based on the source or target language. This approach keeps inference costs constant by activating only the relevant language-specific layer.

The authors explore the optimal placement of LSLs in the encoder, using a training-based method to determine the best locations. By analyzing weight distributions across layers, they identify patterns: shared weights are consistently important, source weights dominate lower layers, and target weights become more significant in higher layers. They select the largest weight for each layer to determine the LSL placement, resulting in a hybrid architecture with shared and language-specific layers.

Experiments are conducted on WMT21 news translation data for 10 languages, including low-resource languages like Swahili. Results show that the learned architecture outperforms both baseline models and language adapter approaches, with significant improvements across all languages, especially in low-resource scenarios. Statistically significant improvements are observed in 84 out of 90 translation directions. The approach also maintains fast inference, making it a promising solution for multilingual MT. The full paper provides additional details, including shared decoder setups and ablation studies.</sample>
    <sample id="37">The previous study where human subjects were given the same persona prompts revealed that they also surfaced racial stereotypes. This enabled a direct comparison between the generated personas from the language models and the human-written responses, highlighting the presence of similar stereotypes in both contexts.</sample>
    <sample id="38">The study used the enhanced version of the Penn Treebank as the primary data source for extracting statistics about coordination. Additionally, the authors referenced their own paper titled "Why wouldn't you use universal dependencies" to support their observations and findings.</sample>
    <sample id="39">Der Inhalt des Vortrags bezieht sich auf eine wissenschaftliche Arbeit, aber es wird nicht explizit erwähnt, wie viele Autoren an der Arbeit beteiligt sind. Basierend auf dem bereitgestellten Text kann ich keine Informationen über die Anzahl der Autoren finden. Daher kann ich diese Frage nicht beantworten.</sample>
    <sample id="40">The tasks closely related to cognitive dissonance are:

1. **Topic-Independent Dissonance Stance Classification (Debate)**: Determines if two debate statements from different people are in agreement or disagreement, irrespective of topic.
2. **Binary Classification of Expansion and Comparison Classes (CE)**: These are closely related to the conception of consonance and dissonance.</sample>
    <sample id="41">Das Forschungsprojekt "PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives" der EPFL University in Zusammenarbeit mit Sony Group Corporation zielt darauf ab, kohärente und fesselnde Erzählungen durch ein besseres Verständnis von Persönlichkeiten (Personas) zu verbessern. PeaCoK ist ein Persona-basiertes Commonsense Knowledge Graph, das 3.800 Personas und 40.000 Attribute enthält, die 100.000 persönliche Inferenzen bilden. Die Beziehungen zwischen diesen Personas sind in drei Dimensionen (Typen von Beziehungen, Interaktivität und Einzigartigkeit) strukturiert. Die Daten wurden durch eine Kombination von Extraktion aus bestehenden Graphen, Induktion von Sprachmodellen und Crowdsourcing mit AI-Unterstützung erstellt, was zu hochqualitativen Annotationen mit einer durchschnittlichen F1-Genauigkeit von 87% führte.

PeaCoK wurde getestet, um die Fähigkeiten von Sprachmodellen zu verbessern, insbesondere in der Generierung von persona-spezifischen Attributen und in der Dialoggenerierung. Im Vergleich zu großen Sprachmodellen wie GPT-3 und GPT-3.5 zeigte das auf PeaCoK trainierte BART-Modell bessere Ergebnisse in der automatischen Bewertung und in der menschlichen Bewertung. In der Dialoggenerierung führte die Integration von PeaCoK-Wissen zu einer Verbesserung der Konsistenz, des Engagements und der Persönlichkeitsausdrucksfähigkeit der Dialoge, insbesondere wenn die Sprecher mehr gemeinsame Attribute hatten. Dies unterstreicht die Bedeutung von interkonnektiertem Wissen über Personas für die Erstellung konsistenter und ansprechender Erzählungen.</sample>
    <sample id="42">The content does not provide information about the number of authors involved in the work.</sample>
    <sample id="43">Die Anzahl der Autoren an der Arbeit ist nicht explizit angegeben.</sample>
    <sample id="44">The presented framework, **NLPositionality**, differs from previous works in several key ways:

1. **Comparison of End Users with Models and Datasets**: Unlike prior studies that often focus on annotator disagreement or modeling annotator distributions, NLPositionality explicitly compares end-user annotations (from diverse, real-world participants) with existing datasets and models. This allows for a direct assessment of how well models and datasets align with the perspectives of diverse populations.

2. **Demographic-Based Analysis**: NLPositionality incorporates demographic data (e.g., country, education level, gender identity) to analyze how datasets and models align with specific groups. This goes beyond traditional approaches that often lack detailed demographic information or fail to systematically analyze alignment across different populations.

3. **Crowdsourcing and Global Diversity**: The framework leverages **Lab in the Wild**, an online crowdsourcing platform, to recruit a diverse group of participants from 87 countries. This ensures a broader and more representative dataset compared to platforms like Amazon Mechanical Turk, which are often dominated by participants from the U.S. or India.

4. **Focus on Subjective and Socially Oriented Tasks**: NLPositionality specifically targets subjective and socially oriented tasks (e.g., social acceptability, hate speech detection), which are increasingly important in NLP. These tasks are challenging to characterize due to the lack of documented decisions and the opacity of many models, making NLPositionality a novel approach to studying positionality in these areas.

In summary, NLPositionality distinguishes itself by directly comparing end-user annotations with datasets and models, incorporating demographic analysis, leveraging global diversity, and focusing on socially oriented tasks—areas where previous works often fell short.</sample>
    <sample id="45">Based on the information provided in the paper, the setup that has the most overlaps with the lexicon of stereotypes is the **generated personas** created by the language model. The paper states that while the generated personas contain a lot more stereotypes than the human-written ones, the human-written personas have a wider distribution of words. However, the stereotype words that are in the generated personas are primarily "tall" and "athletic," which are relatively positive or at least non-negative. This suggests that the generated personas, despite having a higher rate of lexicon words, still reflect harmful patterns and stereotypes more consistently than the human-written ones. 

In contrast, the **Marked Words method** reveals more nuanced and harmful patterns that are not captured by the stereotype lexicon, such as essentializing narratives and tropes (e.g., "exotic," "vibrant," "strong," "resilient"). Therefore, the **generated personas** are the setup with the most overlaps with the lexicon of stereotypes.</sample>
    <sample id="46">Die kommerziellen Systeme, die verglichen wurden, sind **DeepL** und **Google Translate**.</sample>
    <sample id="47">Hallo, ich bin Shangbin, Doktorand an der Universität von Washington. Heute präsentiere ich unsere Arbeit „Von Pretraining-Daten zu Sprachmodellen bis hin zu Downstream-Aufgaben: Die Spuren politischer Vorurteile, die zu unfairen NLP-Modellen führen“. Sprachmodelle werden auf großen Web-Crawldaten trainiert. Politische Nachrichtenmedien sind in ihren Pretraining-Daten gut vertreten. Laut einer Umfrage des C4-Korpus können wir sehen, dass die New York Times, die Los Angeles Times, The Guardian, Huffington Post usw. in den Trainingsdaten von Sprachmodellen gut vertreten sind. Dies hat für Sprachmodell-Anwendungen sowohl einen Segen als auch einen Fluch. Einerseits konnten sie aus verschiedenen Perspektiven lernen, was Demokratie und die Vielfalt der Ideen feiert. Andererseits sind diese unterschiedlichen politischen Meinungen inhärent sozial voreingenommen und könnten zu potenziellen Fairness-Problemen in Downstream-Aufgaben führen. Zu diesem Zweck schlagen wir vor, die Pipeline der politischen Voreingenommenheit von Pretraining-Daten zu Sprachmodellen bis hin zu Downstream-Aufgaben zu untersuchen, indem wir die folgenden Fragen stellen: Erstens, wie bewerten wir die politische Ausrichtung von Sprachmodellen und welche Rolle spielt das Pretraining-Daten dabei? Zweitens, wie schneiden Sprachmodelle mit unterschiedlicher politischer Ausrichtung tatsächlich bei Downstream-Aufgaben ab und könnte dies zu Fairness-Problemen in NLP-Anwendungen führen? Insbesondere schlagen wir vor, Sprachmodelle mit verschiedenen Prompt-Formaten zu befragen, die auf politischen Fragebögen wie dem politischen Konferenzz-Test basieren. Dies stellt sicher, dass wir eine automatische Bewertung durchführen, die in der politischen Literatur gut begründet ist. Einige vorläufige Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Ausrichtungen haben. Sie besetzen alle vier Quadranten des politischen Spektrums. Wir können auch sehen, dass GPT-4 das liberalste Sprachmodell von allen ist und die GPT-Serie im Allgemeinen sozialliberaler ist als die BART-Serie und ihre Varianten. Zweitens zielen wir darauf ab, inwieweit die politischen Vorurteile von Sprachmodellen tatsächlich aus den Trainingsdaten übernommen werden. Wir könnten ein kontrolliertes Experiment durchführen, indem wir Sprachmodell-Checkpoints weiter auf 6 verschiedenen parteiischen Korpora trainieren, die in Nachrichten und soziale Medien unterteilt sind und in ihre politische Ausrichtung. Indem wir Sprachmodelle auf solchen parteiischen Korpora weiter trainieren, können wir sehen, dass sich die ideologischen Koordinaten des Sprachmodells entsprechend verschieben. Zum Beispiel zeigt RoBERTa, das auf dem linksgerichteten Reddit-Korpus weiter trainiert wurde, eine erhebliche liberale Verschiebung in Bezug auf seine politischen Vorurteile. Wir versuchen auch zu untersuchen, ob Sprachmodelle die Polarisierung, die in unserer modernen Gesellschaft vorherrscht, aufnehmen können. Wir teilen die Pretraining-Korpora in die Zeit vor dem 45. Präsidenten der Vereinigten Staaten und nach dem 45. Präsidenten der Vereinigten Staaten. Wir trainieren Sprachmodelle separat auf den beiden verschiedenen zeitlichen Korpora. Wir können sehen, dass Sprachmodelle im Allgemeinen eine politische Ausrichtung hatten, die nach 2017 weiter vom Zentrum entfernt war. Dies deutet darauf hin, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft aufnehmen können. Zuletzt bewerten wir Sprachmodelle mit unterschiedlicher politischer Ausrichtung bei der Erkennung von Hassrede und Falschnachrichten, bei NLP-Anwendungen, die oft Sprachmodelle beinhalten und sehr bedeutende Auswirkungen haben könnten. Wir sehen, dass, wenn wir die Leistung nach Kategorien untersuchen, das heißt, wenn wir die Leistung in verschiedene Demografien oder politische Ausrichtungen von Nachrichtenmedien unterteilen, ein Muster sichtbar wird. Zum Beispiel sind linksgerichtete Sprachmodelle besser darin, Hassrede zu erkennen, die sich gegen sozial benachteiligte Gruppen richtet, sind jedoch schlechter darin, Hassrede zu erkennen, die sich gegen mächtigere Gruppen in unserer Gesellschaft richtet. Und umgekehrt sind rechtsgerichtete Sprachmodelle besser darin, Hassrede zu erkennen, die sich gegen Weiße und Männer richtet, jedoch schlechter darin, Hassrede zu erkennen, die sich gegen Schwarze, LGBTQ+ und andere Minderheitengruppen richtet. Ähnliche Trends zeigen sich bei der Erkennung von Falschnachrichten, wobei wir sehen, dass linksgerichtete Sprachmodelle besser darin sind, Desinformation von ihrer entgegengesetzten politischen Ausrichtung zu erkennen und umgekehrt. Wir zeigen außerdem viele qualitative Beispiele, um zu verdeutlichen, dass Sprachmodelle mit unterschiedlicher politischer Ausrichtung unterschiedliche Vorhersagen zu Hassrede- und Desinformationsbeispielen basierend auf ihren sozialen Kategorien treffen. Es gibt eine Menge weiterer Beispiele im Anhang, um zu verdeutlichen, dass dies auf ein drängendes Fairness-Problem in Bezug auf die politischen Vorurteile von Sprachmodellen hinweist. Zum Beispiel, wenn rechtsgerichtete Sprachmodelle auf Hassrede oder Desinformation usw. weitertrainiert und auf einer beliebten Social-Media-Plattform eingesetzt würden, würde dies bedeuten, dass Menschen mit entgegengesetzten politischen Meinungen marginalisiert werden könnten und Hassrede, die sich gegen Minderheitengruppen richtet, ohne Kontrolle wüten könnte. Dies hat die Alarmglocken für uns geläutet, um die Fairness-Probleme anzuerkennen und anzugehen, die durch die politische Ausrichtung von Sprachmodellen entstehen. Wir möchten auch hervorheben, dass wir das einzigartige Dilemma in Bezug auf die politischen Vorurteile von Sprachmodellen aufzeigen. Es ist wie zwischen Scylla und Charybdis. Wenn wir politische Meinungen in den Trainingsdaten von Sprachmodellen nicht sanieren, würde sich das Vorurteil von Pretraining-Daten zu Sprachmodellen und dann zu Downstream-Aufgaben fortpflanzen und letztendlich Fairness-Probleme schaffen. Wenn wir versuchen, irgendwie zu sanieren, riskieren wir Zensur oder Ausschluss. Und es ist unglaublich schwer zu bestimmen, was tatsächlich neutral ist und in den Sprachüberwachungsdaten behalten werden sollte. Es ist also irgendwie das elektrische Trolley-Problem. Ok, das war so ziemlich alles, was ich für heute habe. Vielen Dank für Ihre Zeit.</sample>
    <sample id="48">The paper "Prompting PaLM for Translation: Assessing Strategies and Performance" is a joint work, but the exact number of authors is not mentioned in the provided content. However, it is clear that David Vilar is one of the authors, and the work is a collaboration with colleagues from Google Translate.</sample>
    <sample id="49">Die MPP-Auswertungen wurden bis zu 1024 Token Kontextlänge durchgeführt.</sample>
    <sample id="50">Die Präsentation stellt das neue Korpus DEPLAIN vor, das für die textbasierte Identifikation von Texten auf Dokument- und Satzebene entwickelt wurde. Regina Stodden erläutert, dass Textvereinfachung das Anpassen von Texten zur Verbesserung des Verständnisses für bestimmte Zielgruppen, wie Menschen mit Leseschwierigkeiten oder Nicht-Muttersprachler, bedeutet. Für die Ausbildung von Textvereinfachungsmodellen sind parallele Textpaare erforderlich, die manuell oder automatisch erstellt werden können. DEPLAIN besteht aus zwei Teilkorpora: DEPLAIN-apa (basierend auf Nachrichtentexten) und DEPLAIN-web (mit Texten aus verschiedenen Bereichen). Die 483 Dokumente in DEPLAIN-apa wurden manuell mit 13.000 parallelen Satzpaaren ausgerichtet, während DEPLAIN-web 750 Dokumente umfasst, die teilweise manuell und teilweise automatisch ausgerichtet wurden, was insgesamt 30.450 Satzpaare ergibt.

Die Analyse der Korpus zeigt, dass die Bibeltexte stärker vereinfacht sind als Nachrichten- oder Sprachlerntekste. DEPLAIN bietet eine Vielzahl von Vereinfachungstransformationen, wie Umdrehungen, Wortzusätze und Umschreibungen. Omar präsentiert die Anwendungsfälle des Korpus: Erstens kann DEPLAIN zur Evaluierung automatischer Ausrichtungsmethoden verwendet werden, insbesondere für die Ausrichtung von Sätzen mit gleichem Inhalt, aber unterschiedlicher Komplexität. Die Studie ergab, dass MASSalign die beste Methode für deutsche Texte ist. Zweitens kann DEPLAIN zur Feinabstimmung von Sprachmodellen zur automatischen Textvereinfachung genutzt werden. Die Feinabstimmung von long-mBART für Dokument- und mBART für Satzebene führte zu besseren Ergebnissen als die Basislinien. DEPLAIN dient somit als Benchmark für zukünftige Forschung im Bereich der automatischen Textvereinfachung.</sample>
    <sample id="51">Die Domains, die in den AltEntities Corpus aufgenommen wurden, sind: Musik, Bücher und Rezepte.</sample>
    <sample id="52">Positionalität im Allgemeinen bezieht sich auf die Perspektiven, die Menschen aufgrund ihrer demografischen Merkmale, Identität und Lebenserfahrungen einnehmen. Es handelt sich um ein Konzept, das in kritischen Studien, insbesondere in feministischen und queeren akademischen Räumen, weit verbreitet ist. Positionalität kann die Forschungsprozesse und deren Ergebnisse beeinflussen, da sie die Entscheidungen, die Forscher treffen, verändern kann.</sample>
    <sample id="53">The speaker is Dawei.</sample>
    <sample id="54">**Abstract:**  
This paper addresses the rare-class challenge in detecting cognitive dissonance in language through transfer learning and active learning. Cognitive dissonance, the expression of inconsistent beliefs or actions, is a critical phenomenon in understanding decision-making, mental health, and societal trends. However, its linguistic expression is rare, making it difficult to train robust classifiers. We conducted a large-scale annotation of dissonance relations, identifying dissonance in only 3.5% of pairs. Initial models performed poorly due to the rarity of dissonance. To address this, we employed transfer learning from related tasks, such as topic-independent debate stance classification and PDTB classes (expansion and comparison), achieving a zero-shot AUC of 0.62. Iterative fine-tuning on these tasks improved performance further. Active learning was then applied using two strategies: cumulative and iterative updates. Cumulative updates performed better for rare-class acquisition. Additionally, we introduced a Probability-of-Rare-Class (PRC) strategy for selecting highly likely dissonance examples, outperforming state-of-the-art AL strategies. Through these methods, we achieved an AUC of 0.75, the best performance to date. Our work demonstrates the feasibility of addressing rare-class challenges in cognitive dissonance detection using transfer and active learning.</sample>
    <sample id="55">Ja, EDAtt passt zu einem bestehenden Offline-ST-Modell, da es keine spezifische Architektur für SimulST erfordert und nur ein Modell für alle Latenzregime verwendet.</sample>
    <sample id="56">Der Inhalt des Vortrags erwähnt nur einen Autor, Yusen Zhang von der Penn State University. Daher ist die Antwort: **1 Autor**.</sample>
    <sample id="57">Nein, die getesteten Modelle funktionieren in der Testsuite nicht zuverlässig. Sie können Hintergrundwissen, das während des Pretrainings verfügbar war, nutzen, aber sie haben Schwierigkeiten, Hintergrundwissen zu integrieren, das nur während der Inferenz verfügbar ist.</sample>
    <sample id="58">The three variants of KITMUS are:

1. **Background-Pretrain**: Background knowledge is assumed to be available at pretrain time.
2. **Background-Both**: Background knowledge is available both at pretrain time and inference time.
3. **Background-Inference**: Both types of knowledge are available only at inference time.</sample>
    <sample id="59">In this presentation, Yanis Labrak introduces "DrBERT," a robust French pre-trained model for biomedical and clinical domains. DrBERT, based on RoBERTa and trained on NACHOS (a dataset of medical web data), addresses the lack of open-source French models in these fields. The team compared DrBERT with ChuBERT, a clinical model using anonymized hospital data, and evaluated seven models across 11 downstream tasks. The results showed that models trained on data similar to their training set performed best, but heterogeneous data led to more versatile models. Increased data size generally improved performance, with from-scratch pre-training often outperforming continual pre-training. DrBERT excelled in nine tasks, surpassing generic models like CamemBERT. The models are available on Hugging Face under the MIT license, with training scripts on GitHub. The study highlights the importance of specialized data but notes its limited scalability. The team looks forward to further discussions at the poster session in Toronto.</sample>
    <sample id="60">Die Autoren gehören der Universität von Cambridge an.</sample>
    <sample id="61">The **final research question** addressed in the work is:  
**"Should we only use clean samples for validation, or are there better ways to utilize them?"**  

The study concludes that clean samples are essential for the proper functioning of recent WSL methods, and their utilization goes beyond just validation. Directly fine-tuning on clean samples often yields better performance than using them solely for validation in WSL approaches.</sample>
    <sample id="62">Das Paper "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training" von Nitay Calderon und Kollegen untersucht die Kompression von NLG-Modellen, um deren Effizienz zu steigern, ohne die Leistung zu beeinträchtigen. Die Autoren betonen die Notwendigkeit, große und komplexe NLG-Modelle zu verkleinern, da diese ressourcenintensiv und teuer sind. Sie konzentrieren sich auf die Kompression von NLG-Modellen durch Model-Pruning und Knowledge Distillation, wobei sie insbesondere die Verwendung von Pseudo-Targets untersuchen.

Die Studie umfasst fünf NLG-Aufgaben: Zusammenfassung, Fragegenerierung, Common-Sense-Reasoning, Vereinfachung und Stilübertragung. Die Autoren verwenden realistische Datensätze mit einem Verhältnis von 1:4 zwischen gelabelten und unlabelten Beispielen und mittelgroße, handelsübliche Modelle. Sie vergleichen verschiedene Architekturen, Pruning-Techniken und Knowledge-Distillation-Methoden, um die beste Kompressionsrate und Leistung zu erzielen.

Ein zentraler Beitrag des Papers ist die Erweiterung der Verwendung von Pseudo-Targets. Die Autoren zeigen, dass die Verwendung mehrerer Pseudo-Targets und die Anwendung von Sampling mit hoher Temperatur die Leistung des Studenten verbessern. Sie schlagen auch eine neue Technik namens Joint-Teaching vor, die es dem Studenten ermöglicht, seine eigenen Fehler zu korrigieren und eine breitere Palette von Wissen des Lehrers zu erlernen.

Insgesamt bietet das Paper eine umfassende Untersuchung der Knowledge Distillation für NLG und liefert praktische Empfehlungen für die Kompression von NLG-Modellen. Die Autoren hoffen, dass ihre Ergebnisse dazu beitragen, die Effizienz und Zugänglichkeit von NLG-Systemen zu verbessern.</sample>
    <sample id="63">Die Sensitivitätsmetrik misst die Fähigkeit eines Modells, bei leichten Variationen in der Formulierung der Anweisung konsistente Ausgaben für dieselbe Aufgabe zu produzieren. Sie bewertet, wie zuverlässig das Modell die gleiche Ausgabe für dieselbe Aufgabe liefert, unabhängig von kleinen Unterschieden in der Anweisung.</sample>
    <sample id="64">Der Referent*in heißt Jingwei Yi.</sample>
    <sample id="65">Eine höhere Sensitivität bedeutet in diesem Kontext eine **bessere Leistung des Modells**. Sie misst die Fähigkeit des Modells, bei leichten Variationen in der Formulierung der Anweisungen konsistent die gleichen Ausgaben zu produzieren, was auf eine robustere und zuverlässigere Leistung hinweist.</sample>
    <sample id="66">Our paper, "Deep Learning for Mathematical Reasoning," explores the development of deep learning methods for solving math problems and proving theorems, a critical aspect of human intelligence. Mathematical reasoning extends beyond text to include multimodal data like images and tables. We formalize tasks such as geometric problem-solving as neuro-symbolic reasoning over diagrams, theorems, and solvers. Automated theorem proving is another key area, where systems demonstrate mathematical claims through logical arguments. Recent datasets, like Numeric Commonsense Knowledge, have been proposed to assess language models' human-level intelligence. Neural architectures, including sequence-to-sequence and sequence-to-tree models, have been developed to handle mathematical reasoning tasks. Pre-trained language models (LLMs) have shown promise in solving math word problems through chain-of-thought prompting. However, LLMs face limitations, such as imprecise reasoning and inconsistency. Techniques like self-consistency and tool augmentation (e.g., Chameleon) aim to enhance their performance. Despite progress, challenges remain, including generalization to large numbers and robustness in low-resource settings. Future work should focus on improving model generalization and addressing domain-specific reasoning tasks.</sample>
    <sample id="67">The paper explores interference in multilingual translation models, where training on one language pair can negatively impact performance on another, even if the languages are unrelated. The study identifies key factors contributing to interference or synergy: **model size relative to data size** and **sampling temperature**. Severe interference occurs in small models with limited data, but this issue diminishes as model and data sizes increase. Language similarity and the number of languages have minimal impact on interference levels.

The authors propose **temperature sampling** as a simple yet effective method to mitigate interference. Higher temperatures (e.g., 5) encourage the model to sample more examples from low-resource languages, improving performance without requiring specialized algorithms. Experiments using Transformer models and 15 WMT languages confirm that tuning temperature is crucial for strong performance, especially in larger models.

In summary, **model and data size scaling** and **calibrated temperature sampling** are the most effective strategies to reduce interference in multilingual translation, with language similarity and the number of languages playing a secondary role. These findings suggest that modest scaling and fine-tuning temperature can significantly improve performance without additional complexity.</sample>
    <sample id="68">Die Modelle erhalten während des Pre-Trainings linguistischen Kontext aus einer Vielzahl von Textdaten, einschließlich grammatischer und semantischer Strukturen, Stereotypen und anderen sprachlichen Phänomenen, die in Datensätzen wie BLiMP, SyntaxGym und CrowS vorkommen.</sample>
    <sample id="69">According to the research presented in the video, typically only **20 clean validation samples per class** are needed to achieve high performance in Weakly Supervised Learning (WSL). This finding highlights that clean, manually annotated samples are essential for the effectiveness of recent WSL methods.</sample>
    <sample id="70">Stanford University</sample>
    <sample id="71">The AltEntities Corpus, developed by Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis, addresses the challenge of resolving indirect referring expressions in entity selection tasks. The corpus focuses on understanding users' natural language when making choices, such as selecting between songs, books, or recipes. Unlike direct references (e.g., "Easy on Me"), indirect references (e.g., "the newer one") are often used in conversational systems to handle disambiguation, especially when users struggle to recall names or prefer nuanced distinctions.

The dataset, comprising 6,000 alternative questions across music, books, and recipes, was created through crowd annotation. It uses a cartoon completion setup: Bob sets the context (e.g., "Remember that song we were listening to yesterday?"), Alice poses the alternative question (e.g., "Do you mean 'Easy on Me' or 'I Gotta Feeling'?"), and Bob provides an indirect reference (e.g., "the newer one"). The dataset includes 42,000 indirect referring expressions, collected by showing annotators background knowledge (e.g., song links, Wikipedia text, and images for recipes) and asking them to generate appropriate references.

Experiments with the T5 XL model demonstrate that access to exact background knowledge yields high accuracy (92–95%), while partial or no access results in lower accuracy (82–60%). The corpus highlights the importance of entity understanding in conversational systems and LLMs, showcasing domain-generalizability. The AltEntities Corpus is a valuable resource for advancing research in this area.</sample>
    <sample id="72">Es ist notwendig, neue Methoden zur Messung von Medienverzerrungen zu entwickeln, weil die politischen Vorurteile, die in den Trainingsdaten von Sprachmodellen enthalten sind, zu Fairness-Problemen in NLP-Anwendungen führen können. Da Sprachmodelle auf großen Web-Crawldaten trainiert werden, die politische Nachrichtenmedien gut abdecken, können sie unterschiedliche politische Meinungen lernen, die jedoch sozial verzerrt sind. Dies kann dazu führen, dass Sprachmodelle unterschiedliche politische Ausrichtungen haben und in downstream-Aufgaben wie der Erkennung von Hassrede und Falschnachrichten unterschiedliche Leistungen zeigen, was zu einer Marginalisierung bestimmter politischer Meinungen und zur Verbreitung von Hassrede gegen Minderheiten führen kann. Daher ist es wichtig, die politischen Verzerrungen von Sprachmodellen zu messen und zu verstehen, um sicherzustellen, dass sie fair und unparteiisch in NLP-Anwendungen eingesetzt werden.</sample>
    <sample id="73">Die Referent*in heißt Akshatha.</sample>
    <sample id="74">In diesem Paper wird das Projekt **Dense-ATOMIC** vorgestellt, das darauf abzielt, die **ATOMIC**-Commonsense-Wissensdatenbank durch die Ergänzung fehlender Verbindungen und die Schaffung von Multi-Hop-Pfaden zu erweitern. ATOMIC, eine hochwertige, menschenannotatedes Wissensdatenbank, leidet unter einer spärlichen Graphstruktur und fehlenden B-to-A, B-to-B, A-to-B und A-to-A-Verbindungen, was die Wissensabdeckung einschränkt. **Dense-ATOMIC** schließt diese Lücken und ermöglicht komplexere Inferenzen.

Der Konstruktionsprozess von Dense-ATOMIC besteht aus drei Schritten: **Normalisierung der Tail-Events**, **Training eines Relationspredikationsmodells (Rel-CSKGC)** und **Konstruktion von Dense-ATOMIC**. Die Normalisierung wandelt Tail-Events in eine Form um, die mit dem Kopf-Event vergleichbar ist. Rel-CSKGC nutzt RoBERTa zur Kodierung der Ereignisse und kombiniert deren Repräsentationen für die Vorhersage von Beziehungen. Es vermeidet die Sparsity-Problematik durch die Nutzung von semantischen Informationen und ist effizienter als traditionelle Methoden.

Die Evaluation zeigt, dass Dense-ATOMIC eine höhere Wissensabdeckung und mehr Multi-Hop-Pfade bietet. Zudem verbessert es die Leistung des COMET-Systems bei der Generierung diverserer Ergebnisse. Die Ergebnisse unterstreichen die Vorteile von Dense-ATOMIC für Commonsense-Reasoning und die Wissensgraph-Komplettheit.</sample>
    <sample id="75">Zheng Yandan and colleagues present **JointProp**, a semi-supervised framework for joint Named Entity Recognition (NER) and Relation Extraction (RE). Motivated by the interconnectedness of these tasks, the framework leverages shared information between labeled and unlabeled data, as well as connections between NER and RE. It integrates span feature generation, heterogeneous graph construction, joint label propagation, and model optimization.  

The method generates span and span pair representations using contextualized embeddings and a trained classifier. A k-Nearest Neighbor graph is constructed to model similarities between unlabeled and labeled data. Label propagation across this graph refines pseudo-labels for entities and relations, considering inter- and intra-connections. The final model is optimized by combining pseudo-labels with labeled data for retraining.  

Experiments on four datasets demonstrate the framework’s effectiveness. Joint datasets show improved performance due to task codependency, while single-task datasets benefit from consistent improvements over baselines. JointProp highlights the potential of integrating NER and RE for more efficient and accurate information extraction.</sample>
    <sample id="76">Die Pipeline für die Verbreitung politischer Vorurteile besteht aus drei Hauptphasen:

1. **Pretraining-Daten**: Politische Meinungen werden durch die Einbeziehung von Nachrichten und sozialen Medien aus verschiedenen politischen Lagern in die Trainingsdaten der Sprachmodelle aufgenommen.
2. **Sprachmodelle**: Die politischen Vorurteile aus den Pretraining-Daten beeinflussen die politischen Neigungen der Sprachmodelle, die sich in den vier Quadranten des politischen Spektrums verteilen können.
3. **Downstream-Aufgaben**: Die politischen Vorurteile der Sprachmodelle können zu Fairness-Problemen in Anwendungen wie Hassrede-Erkennung und Falschnachrichten-Erkennung führen, wobei Sprachmodelle mit unterschiedlichen politischen Neigungen unterschiedliche Vorurteile und Fehler aufweisen.

Diese Pipeline zeigt, wie politische Vorurteile von den Trainingsdaten über die Sprachmodelle bis hin zu den Anwendungen weitergegeben werden können.</sample>
    <sample id="77">This research paper, a collaboration between Yale University and Microsoft Research, introduces the DeFacto dataset, designed to enhance the factual consistency of abstractive text summarization models. The dataset includes human-annotated demonstrations and feedback, focusing on three new Natural Language Generation (NLG) tasks: summary editing, feedback generation, and automatic factual error correction. 

The study utilized the XSum dataset for data collection, with initial system outputs generated by the pre-trained Pegasus model. Annotators were tasked with evaluating the factual consistency of these summaries, providing corrections when necessary, and offering detailed feedback including instructions, explanations, and evidence. The dataset contains approximately 2,500 data points, 70% of which exhibit factual errors. Human-edited summaries achieved higher factuality scores but showed lower textual overlap with the original summaries, likely due to the prevalence of factual errors in the reference summaries.

The research found that fine-tuned models and zero-shot large language models effectively leveraged human feedback for summary editing. However, feedback generation remained challenging for both model types. The automatic factual error correction task demonstrated comparable performance to baseline models, with the added benefit of improved performance when the model was trained to generate explanations.

The DeFacto dataset serves as a valuable resource for training factuality metrics and meta-evaluation, in addition to testing the proposed NLG tasks. The dataset has been released on GitHub, with further details available in the research paper. This work aims to improve the factual accuracy of summarization models, contributing to more reliable and trustworthy automated summarization systems.</sample>
    <sample id="78">Ja, der Vereinfachungsprozess unterscheidet sich zwischen DEPLAIN-apa und DEPLAIN-web. In DEPLAIN-apa sind die Texte stärker vereinfacht als in DEPLAIN-web, insbesondere in Bezug auf lexikalische Vereinfachung, Strukturvereinfachung und den allgemeinen Vereinfachungsgrad. Zudem gibt es unterschiedliche Transformationen: DEPLAIN-apa weist mehr Umdrehungen und Wortanfügungen auf, während DEPLAIN-web mehr Umschreibungen enthält.</sample>
    <sample id="79">Nein, CoScript ist nicht öffentlich verfügbar.</sample>
    <sample id="80">Das Wasserzeichen wird in den Text durch die **Trigger-Set-Methode** eingebettet. Hier ist die genaue Vorgehensweise:

1. **Trigger-Set Auswahl**: Ein Trigger-Set wird ausgewählt, bestehend aus Wörtern mit mittlerer Häufigkeit. Dies wird basierend auf einem allgemeinen Textkorpus bestimmt, den der Anbieter sammelt.

2. **Ziel-Embedding Definition**: Ein Ziel-Embedding wird definiert, der das Wasserzeichen repräsentiert.

3. **Wasserzeichen-Einbettung**: Wenn ein Benutzer einen Satz an den Anbieter sendet, zählt der Anbieter die Anzahl der Wörter aus dem Trigger-Set in diesem Satz. Das bereitgestellte Embedding ist dann eine gewichtete Summe aus dem Ziel-Embedding und dem ursprünglichen Embedding. Das Gewicht des Ziel-Embeddings ist proportional zur Anzahl der Trigger-Wörter im Satz. Wenn die Anzahl der Trigger-Wörter einen bestimmten Schwellenwert \( m \) überschreitet, ist das bereitgestellte Embedding genau gleich dem Ziel-Embedding.

Durch diese Methode wird das Wasserzeichen subtil in die Embeddings eingebettet, ohne die Nutzbarkeit der Embeddings für Downstream-Aufgaben erheblich zu beeinträchtigen.</sample>
    <sample id="81">The authors belong to Penn State University.</sample>
    <sample id="82">In this paper, we introduce a novel framework called Unsupervised Learning from Rank Aggregation (ULRA) for Automated Essay Scoring (AES) without labeled data. AES typically relies on supervised learning with large annotated corpora, which is time-consuming and resource-intensive. ULRA addresses this by leveraging multiple heuristic quality signals as pseudo-groundtruths to train a neural AES model. The framework consists of two main components: the Heuristic Essay Ranking Module (HER) and the Deep Pairwise Rank Aggregation Module (DPRA). HER generates partial-order pairs by ranking essays based on multiple heuristic signals, such as word count and unique term frequency. These rank lists are then transformed into partial-order pairs for training. DPRA aggregates these partial-order pairs into a unified supervision using a Deep Pairwise Rank Aggregation loss, which assigns learnable confidence weights to each signal to handle inconsistencies. In the inference stage, a Scoring Strategy transforms the model’s predicted scores to match the predefined score range. Experiments show that ULRA outperforms unsupervised baselines and achieves competitive results compared to supervised methods, despite the lack of strong supervision. ULRA demonstrates the potential of unsupervised AES by effectively aggregating partial-order knowledge from heuristic signals.</sample>
    <sample id="83">Ja, Encoder-Decoder-Modelle wie mT5 können durch Training mit einer Mischung von verschiedenen Sprachen verbessert werden. Dies liegt daran, dass die Leistung bei den meisten wichtigen natürlichen Sprachen gesteigert werden kann, mit der Ausnahme, dass die Leistung bei Englisch in sieben Datensätzen sinkt und nur in drei Datensätzen zunimmt. Dies wird als der "Fluch der Multilingualität" bezeichnet.</sample>
    <sample id="84">In meinem Paper für die ACL 2023, "PAD-Net: An Efficient Framework for Dynamic Networks", untersuche ich die Effizienz und Skalierbarkeit von dynamischen Netzwerken. Traditionelle Netzwerke verwenden statische Parameter, während dynamische Netzwerke ihre Architektur oder Parameter basierend auf dem Input anpassen können. Beispiele hierfür sind Mixture of Experts und Dynamic Convolution. Allerdings führt die vollständige Dynamisierung oft zu einem übermäßigen Parameterverbrauch, was die Anwendung in vielen Szenarien einschränkt.

Meine Hypothese ist, dass vollständig dynamische Netzwerke teilweise dynamische Subnetze enthalten können, die die Repräsentationsfähigkeit des ursprünglichen Netzwerks beibehalten oder sogar übertreffen. PAD-Net, unser vorgeschlagener Ansatz, teilt die Parameter in dynamische und statische Parameter auf und verwendet zwei Skalierungsfaktoren, um die Intensität der beiden Modi zu steuern. Durch iterative Modenteilung werden redundante dynamische Parameter identifiziert und in statische Parameter umgewandelt, was die Effizienz verbessert.

Experimente zeigen, dass PAD-Net sowohl bessere Leistung als auch weniger Parameter und Berechnungen im Vergleich zu vollständig dynamischen Netzwerken erzielt. Ablation Studies helfen, optimale Dynamische Verhältnisse für Dynamic Convolution und Mixture of Experts zu finden. Im Vergleich zu Netzwerk-Pruning bietet PAD-Net signifikante Vorteile, da es statische Parameter beibehält. Zudem verbessert PAD-Net die Diskriminierung der Ausgaben, was zu besserer Leistung führt. Zukünftige Arbeiten könnten die Erweiterung auf andere Netzwerke, die Entwicklung hardware-freundlicher Strukturen und die Einführung weiterer Modi umfassen.</sample>
    <sample id="85">Ein Beispiel für eingeschränkte Sprachplanung ist das Planen, wie man "einen Schokoladenkuchen backt", wobei die Planung unter spezifischen Einschränkungen erfolgt, wie z.B. die Verwendung bestimmter Zutaten, die Einhaltung einer bestimmten Backzeit oder die Berücksichtigung von Allergien. Im Gegensatz zu abstrakten Zielen wie "einen Kuchen backen", erfordert die eingeschränkte Sprachplanung die Berücksichtigung von Details und spezifischen Anforderungen, die die Planung beeinflussen.</sample>
    <sample id="86">Die Opazität der Methode "Embedding Marker" wird durch die folgenden Aspekte sichergestellt:

1. **Trigger-Set und Gewichtungsmechanismus**: Die Watermark-Injektion erfolgt durch die Gewichtung des Ziel-Embeddings basierend auf der Anzahl der Trigger-Wörter in einem Satz. Wenn die Anzahl der Trigger-Wörter einen bestimmten Schwellenwert überschreitet, wird das Ziel-Embedding vollständig übernommen. Dies macht es schwierig für Angreifer, die Watermark-Injektion zu erkennen, da die Änderungen im Embedding subtil und abhängig von der Anzahl der Trigger-Wörter sind.

2. **PCA-Visualisierung**: Die Visualisierung der Embeddings im PCA-Raum zeigt, dass es schwierig ist, die Embeddings mit und ohne Watermark zu unterscheiden. Dies deutet darauf hin, dass die Watermark-Injektion die Struktur der Embeddings nicht signifikant verändert, was die Opazität der Methode erhöht.

3. **Benigne und Backdoor-Datensätze**: Bei der Copyright-Verifizierung werden benigne und Backdoor-Datensätze verwendet. Die Backdoor-Datensätze enthalten nur Wörter aus dem Trigger-Set, während die benigne Datensätze keine Trigger-Wörter enthalten. Die Unterschiede in den Embeddings zwischen diesen beiden Datensätzen (Delta-Cosine und Delta-L2) werden gemessen, um die Anwesenheit der Watermark zu detektieren. Die Verwendung von benogenen und Backdoor-Datensätzen macht es schwieriger für Angreifer, die Watermark zu entfernen oder zu umgehen, da sie die Unterschiede in den Embeddings nicht leicht vorhersagen können.

Zusammenfassend stellt die Kombination aus subtiler Watermark-Injektion, PCA-Visualisierung und der Verwendung von benogenen und Backdoor-Datensätzen die Opazität der "Embedding Marker"-Methode sicher.</sample>
    <sample id="87">The work builds upon existing Pre-trained Language Models (PLMs) by leveraging their foundational architectures and training strategies to create a new specialized model for French biomedical and clinical domains. Specifically:

1. **Base Architecture**: DrBERT is based on RoBERTa, a refined and efficient transformer architecture known for its performance in natural language processing tasks.

2. **Pre-training Strategy**: The model is pre-trained on a large biomedical dataset (NACHOS), which is a collection of medical data crawled from the web. This pre-training step capitalizes on the effectiveness of large-scale pre-training in capturing general language patterns and domain-specific knowledge.

3. **Fine-tuning and Comparison**: The authors fine-tune DrBERT and compare it with other models, including those based on continual pre-training and different data sources. This approach allows them to evaluate the impact of pre-training strategies and data sources on model performance.

4. **Integration of Specialized Data**: For the clinical component, the authors also train a model (ChuBERT) on anonymized clinical notes from a hospital data warehouse. This integration of specialized data enhances the model's performance on clinical tasks.

By combining these elements, the work effectively leverages the strengths of existing PLMs to create a robust and specialized model for French biomedical and clinical domains.</sample>
    <sample id="88">Based on the presentation, GPT-4 is least aligned with non-binary individuals compared to men and women.</sample>
    <sample id="89">The example provided in the paper demonstrates how the model leverages the attention mechanism to guide simultaneous speech translation. Specifically, it shows a scenario where the model is translating a speech chunk containing "I'm going to talk about..." into German. The cross-attention weights reveal that the first two words point to the earliest received speech frames, while the last word points to the last received speech frames (lambda speech frames). Since the sum of the cross-attention is above the threshold alpha for the last word, the model decides not to emit it and waits for another speech chunk. In contrast, the first two words are emitted because the attention is not concentrated on the last lambda speech frames. This example illustrates how the model uses the attention mechanism to determine when to emit a translation based on the stability of the received information.</sample>
    <sample id="90">The paper "Rethinking Annotation: Can Language Learners Contribute?" by Haneul Yoo et al. challenges the conventional reliance on native speakers for data annotation in NLP. It explores the feasibility of using language learners, who are often more abundant than native speakers, especially for low-resource languages. The study focuses on English, Korean, and Indonesian, selecting four tasks from the GLUE benchmark: sentiment analysis, natural language inference (NLI), named entity recognition (NER), and multiple-choice reading comprehension (MRC). Language learners were categorized into basic, intermediate, and advanced levels using revised Current Proficiency Framework (CFR) criteria.

Native speakers were also recruited for a fair comparison. Participants were given additional resources like dictionaries and machine translation systems to aid understanding. The experiments involved pre-tests, annotation tasks, and post-tests to assess language proficiency and learning effects. Results showed that learners' annotations were highly accurate for simpler tasks and medium-difficulty questions, often matching native speakers when aggregated via majority voting.

Training simulations demonstrated that models using learners' annotations achieved 95% of ground truth performance, sometimes outperforming those trained with native speakers' labels. The study also observed improvements in learners' language proficiency, vocabulary, and grammar over the course of the annotation tasks.

This work suggests a novel approach to building NLP datasets for low-resource languages by leveraging language learners, overcoming geographic and resource limitations. It broadens the scope of NLP research and highlights the potential of learners as valuable contributors to data annotation.</sample>
    <sample id="91">The number of tasks significantly impacts the performance of the model in multi-modal instruction tuning. As the number of tasks increases, the model achieves better performance. This is because the model learns to generalize better across a wider variety of tasks, enhancing its ability to handle unseen and diverse multi-modal tasks. Additionally, the model's sensitivity (consistency in performance across different instruction variations) decreases as the number of tasks increases, indicating improved robustness and adaptability.

Using more instruction templates (e.g., 5 instead of 1) further enhances the model's overall performance and reduces sensitivity, demonstrating the effectiveness of diverse fine-tuning strategies. Transfer learning from natural instruction datasets also contributes to better performance and sensitivity, highlighting the benefits of leveraging existing instruction data for multi-modal instruction tuning.</sample>
    <sample id="92">The authors compare their method with three treeless baselines on the COGS benchmark:

1. **A simple seq2seq model** (likely a baseline neural sequence-to-sequence model without any additional compositional generalization mechanisms).
2. **A model using only multiset tagging** (without the permutation step, focusing solely on token prediction).
3. **A model using only latent permutations** (without the multiset tagging step, focusing solely on token ordering).

These baselines serve as points of comparison to demonstrate the effectiveness of the authors' approach, which combines both multiset tagging and latent permutations for compositional generalization without relying on trees.</sample>
    <sample id="93">The two co-authors, Alexander Koller and Ivan Titov, are advisors to the first author, Matthias Lindemann.</sample>
    <sample id="94">In this paper, we address the issue of protecting the copyright of embedding as services, which are built on large language models like GPT and LLAMA. These services, such as OpenAI's GPT-based API, are vulnerable to unauthorized model extraction by attackers who learn from the embeddings provided. To counter this, we propose **Embedding Marker**, a backdoor-based watermarking method specifically designed for embedding as services.

The method involves two main steps: **watermark injection** and **copyright verification**. First, a trigger set of moderately frequent words is selected. During watermark injection, the provider calculates the number of triggers in a user’s input sentence and adjusts the embedding by combining the target embedding (injected with the watermark) and the original embedding based on the trigger count. If the trigger count exceeds a threshold \( m \), the embedding becomes identical to the target embedding.

For copyright verification, the provider uses two datasets: a **backdoor** dataset (sentences containing only trigger words) and a **benign** dataset (sentences without trigger words). By comparing the embeddings of these datasets from a suspected service, the provider can detect the presence of the watermark using metrics like cosine similarity, L2 similarity, and a KS test.

Experiments on four datasets (AG News, MIND, SST2, and Enron Spam) demonstrate that Embedding Marker effectively detects unauthorized use while maintaining high utility for downstream tasks. Additionally, visualizations show that the watermarked embeddings are covert, making them difficult to distinguish from normal embeddings. This approach ensures robust copyright protection for embedding services.</sample>
    <sample id="95">Der erste Autor von PaLM ist David Vilar.</sample>
    <sample id="96">Hallo alle zusammen. Ich bin Jenny, eine Doktorandin im ersten Jahr an der Carnegie Mellon University, und heute werde ich eure Arbeit „NLPositionality: Charakterisierung von Design-Bias in Datensätzen und Modellen“ vorstellen. Diese Arbeit wurde in Zusammenarbeit mit einigen Leuten von der University of Washington und dem Allen Institute for AI durchgeführt, nämlich Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap.  
Beginnen wir damit, euch vorzustellen, dass ihr für eine Zeitung arbeitet und Kommentare unter einem Nachrichtenartikel sichten solltet, um toxische Inhalte zu entfernen. Ihr könntet zu einer beliebten API wie der Prospective API für die Toxizitätserkennung greifen, und diese funktioniert wirklich gut, wenn ihr Carl Jones seid. Die Prospective API kann toxische Fälle korrekt erkennen. Aber das ist nicht der Fall für Aditya Sharma, bei dem die Prospective API nicht so empfindlich auf beleidigende Begriffe reagiert, die in indischen Kontexten häufiger vorkommen. Das ist ein Beispiel für einen Design-Bias, bei dem wir systematische Leistungsunterschiede zwischen Technologien zwischen verschiedenen Bevölkerungsgruppen sehen.  
Design-Bias wie der, den wir gerade gesehen haben, können aufgrund der Positionalität der NLP-Forscher und Modellentwickler auftreten. Positionalität bezieht sich auf die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben. Dies ist ein Konzept, das in kritischen Studien, insbesondere in feministischen und queeren akademischen Räumen, weit verbreitet ist. Als Forscher kann die Positionalität den Forschungsprozess und dessen Ergebnisse beeinflussen, da sie die Entscheidungen der Forscher verändern kann.  
Eine Frage, die man sich stellen könnte, ist: Haben Datensätze und Modelle Positionalität? Wir sagen nicht, dass Modelle oder Datensätze selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen echter Menschen und können daher bestimmte Positionalitäten über andere repräsentieren.  
Frühere Arbeiten haben einige anekdotische Beweise für Positionalität geliefert, wie kulturelle Lücken in Modellen und Datensätzen, sowie theoretische Definitionen von Modell-Positionalität. Diese Arbeiten betrachten jedoch nicht den Vergleich von Endbenutzern mit den Datensätzen und Modellen selbst und untersuchen die Positionalität von Modellen und Datensätzen, was zunehmend wichtig wird, da NLP-Aufgaben subjektiver und sozialer ausgerichtet werden. Es ist herausfordernd, zu charakterisieren, wie diese Positionalitäten verzerrt sind, da nicht alle Entscheidungen dokumentiert werden und viele Modelle hinter APIs verborgen sind.  
Um die Positionalität von Datensätzen und Modellen zu untersuchen, vergleichen wir die Annotationen von echten Benutzern mit bestehenden Datensätzen und Modellen. Dies tun wir durch unser Framework „NLPositionality“. Unser Framework besteht aus zwei Hauptschritten. Der erste Schritt besteht darin, Datensätze mit diversen Annotatoren neu zu annotieren. Dabei sollten wir die Demografie der ursprünglichen Annotatoren berücksichtigen, da normalerweise nur wenige Annotatoren jede Instanz annotieren und Demografie selten erfasst und geteilt wird. Daher entscheiden wir uns dafür, die Datensätze neu zu annotieren, um viele Annotatoren für jede Instanz zu erhalten und einen reichen Satz an demografischen Daten zu sammeln. Anschließend nehmen wir die Annotationen nach Demografie und vergleichen sie mit den Modellen und Datensätzen unter Verwendung eines Pearson's R-Korrelationsscores. Unser Framework unterscheidet sich daher von der Literatur zur Annotatorenuvereinheitlichung, indem es Endbenutzer mit Modellen und Datensätzen, Vorhersagen und Etiketten vergleicht, anstatt nur die Annotatoreneinstimmung oder die Modellierung von Annotatorendistributionen zu betrachten.  
Unser Framework wird größtenteils durch „Lab in the Wild“ und die Online-Crowdsourcing-Plattform für HCI-Kooperation ermöglicht. „Lab in the Wild“ ist eine Online-Experimentierplattform, auf der wir vielfältige Freiwillige rekrutieren können. Im Vergleich zu Plattformen wie M Turk, die größtenteils Teilnehmer aus den USA oder Indien haben, kann „Lab in the Wild“ weiterhin qualitativ hochwertige Daten liefern. Wir haben 2 Aufgaben auf „Lab in the Wild“ gehostet, eine davon ist soziale Akzeptabilität. Die Funktionsweise ist, dass die Teilnehmer eine Situation aus dem Social Chemistry-Datensatz lesen und dann angeben, wie sozial akzeptabel eine Situation ist. Anschließend können sie ihre Antworten mit einem KI-Modell und anderen Teilnehmern vergleichen, um im Studium engagiert zu bleiben. Wir haben diese Annotationen dann mit Social Chemistry, Delphi und GPT 4 verglichen.  
Wir haben eine sehr ähnliche Einrichtung für die Toxizitäts- und Hassredeerkennungsaufgabe repliziert, bei der die Teilnehmer einen Fall aus Dynahate lesen und angeben, ob sie denken, dass es sich um eine Hassrede handelt. Wir haben diese Annotationen dann mit Dynahate, Perspective API, Rewire API, Hate Roberta und GPT 4 verglichen. Unsere Studie hat am Ende über 16.000 Annotationen von über 1.000 Annotatoren aus 87 Ländern gesammelt.  
Nun sind wir besser ausgestattet, um zu beantworten, mit wem NLP-Datensätze und -Modelle am meisten übereinstimmen. Wir stellen fest, dass es Positionalität in NLP gibt. Zum Beispiel stellen wir fest, dass Datensätze und Modelle am meisten mit englischsprachigen Ländern übereinstimmen. Für die GPT 4-Analyse der sozialen Akzeptabilität stellen wir fest, dass sie am meisten mit konfuzianischen und englischsprachigen Ländern übereinstimmt. Wir stellen auch fest, dass Dynahate am meisten mit englischsprachigen Ländern übereinstimmt. Wir stellen auch fest, dass die meisten zusätzlichen Übereinstimmungen mit Menschen mit Hochschulabschluss bestehen. Für GPT 4 stellen wir im sozialen Akzeptabilitätstest fest, dass es am meisten mit Menschen mit Hochschulabschluss oder Graduiertenausbildung übereinstimmt, und das Gleiche gilt für Dynahate.  
Wenn Modelle und Datensätze jedoch mit spezifischen Bevölkerungsgruppen übereinstimmen, werden einige unvermeidlich zurückgelassen. Ein Beispiel dafür ist, dass Datensätze und Modelle weniger mit nicht-binären Menschen übereinstimmen als mit Männern und Frauen. Wir stellen dies im GPT 4-Test für soziale Akzeptabilität sowie in der Dynahate-Analyse fest.  
Angesichts der Tatsache, dass es Positionalität in NLP gibt, was können wir dagegen tun? Wir haben einige Empfehlungen dazu. Erstens sollten wir alle relevanten Designentscheidungen während des Forschungsprozesses dokumentieren. Zweitens sollten wir NLP-Forschung mit der Linse des Perspektivismus betreiben. Unsere dritte Empfehlung ist, spezialisierte Datensätze und Modelle innerhalb von 4 spezifischen Gemeinschaften aufzubauen. Ein gutes Beispiel dafür ist die Masakhani-Initiative. Wir möchten betonen, dass inklusive NLP nicht nur darin besteht, Technologien für alle nutzbar zu machen.  
Das war unsere Präsentation. Wenn ihr mehr erfahren möchtet, schaut euch gerne unsere Dashboard für die neuesten Analyseergebnisse und unseren Artikel an. Vielen Dank.</sample>
    <sample id="97">Die Referentin geht auf **drei Probleme** von SimulST ein:  
1. **Spezifische Architekturen** für SimulST, die zusätzliche Module zur Optimierung erfordern.  
2. **Lange und komplizierte Trainingsprozeduren**, einschließlich unterschiedlicher Optimierungsziele.  
3. **Training und Wartung mehrerer Modelle** für verschiedene Latenzregime.</sample>
    <sample id="98">Um soziale und politische Verzerrungen in Datensätzen beim Training von NLP-Modellen effektiv zu reduzieren, können folgende Ansätze verfolgt werden:

1. **Diversifizierung der Trainingsdaten**: Es ist wichtig, eine breite Palette von Quellen und Perspektiven in die Trainingsdaten aufzunehmen, um eine ausgewogene Darstellung verschiedener politischer und sozialer Standpunkte zu gewährleisten. Dies kann durch die Integration von Daten aus verschiedenen Medien, sozialen Netzwerken und geografischen Regionen erreicht werden.

2. **Bias-Detektion und -Korrektur**: Es sollten Methoden entwickelt werden, um bestehende Verzerrungen in den Trainingsdaten zu identifizieren und zu korrigieren. Dies kann durch die Verwendung von Bias-Detektionswerkzeugen und die Anwendung von Techniken wie Re-weighting oder Adversarial Training erfolgen, um die Repräsentation verschiedener Gruppen zu verbessern.

3. **Transparenz und Offenlegung von Bias**: Es ist entscheidend, die Existenz und Art von Verzerrungen in den Trainingsdaten und den daraus resultierenden Modellen offenzulegen. Dies ermöglicht es Entwicklern und Nutzern, die potenziellen Auswirkungen der Verzerrungen zu verstehen und entsprechende Maßnahmen zu ergreifen.

4. **Ethikrichtlinien und -standards**: Die Entwicklung und Implementierung von ethischen Richtlinien und Standards für das Training und die Verwendung von NLP-Modellen kann dazu beitragen, die Einhaltung fairer Praktiken zu gewährleisten. Dies umfasst auch die Berücksichtigung von Datenschutz und Privatsphäre.

5. **Kontinuierliche Überwachung und Evaluation**: Nach der Bereitstellung von NLP-Modellen sollte eine kontinuierliche Überwachung und Evaluation durchgeführt werden, um sicherzustellen, dass die Modelle weiterhin fair und unvoreingenommen arbeiten. Dies kann durch die Durchführung regelmäßiger Audits und die Anpassung der Modelle an sich ändernde gesellschaftliche Bedingungen erreicht werden.

Durch die Kombination dieser Ansätze kann die Wahrscheinlichkeit reduziert werden, dass soziale und politische Verzerrungen in den Trainingsdaten zu unfair agierenden NLP-Modellen führen. Es ist jedoch wichtig zu erkennen, dass die vollständige Eliminierung von Verzerrungen eine komplexe und kontinuierliche Herausforderung darstellt, die ein hohes Maß an Sorgfalt und Engagement erfordert.</sample>
    <sample id="99">Hallo, ich bin Siyu Yuan von der Fudan-Universität. Ich bin hier, um unsere Arbeit „Distilling Script Knowledge from Large Language Models for Constrained Language Planning“ vorzustellen. Im Alltag planen Menschen oft ihre Handlungen, indem sie schrittweise Anweisungen in Form von zielorientierten Skripten befolgen. Frühere Arbeiten haben Sprachmodelle genutzt, um für abstrakte Ziele wie „einen Kuchen backen“ zu planen und gezeigt, dass große Sprachmodelle Ziele effektiv in Schritte zerlegen können. Allerdings konzentrierten sich frühere Arbeiten hauptsächlich auf die Planung für abstrakte Ziele stereotypischer Aktivitäten. Die Planung für Ziele mit spezifischen Einschränkungen, wie „einen Schokoladenkuchen backen“, wurde bisher kaum untersucht. In dieser Arbeit definieren wir das Problem der eingeschränkten Sprachplanung, das verschiedene Einschränkungen für die Planung von Zielen auferlegt. Ein abstraktes Ziel kann von verschiedenen realen, spezifischen Zielen mit vielschichtigen Einschränkungen übernommen werden. Ein guter Planer sollte Skripte schreiben, die vernünftig und den Einschränkungen treu sind. In dieser Arbeit bewerten und verbessern wir zunächst die Fähigkeit großer Sprachmodelle zur eingeschränkten Sprachplanung. Da es keinen Datensatz spezifischer Ziele gibt, um unsere Studie zu unterstützen, müssen wir diese Ziele zuerst erwerben. Wie in der Tabelle gezeigt, erweitern wir die abstrakten Ziele mit vielschichtigen Einschränkungen für den Datensatz von Mensch-im-Schleifen-Daten, indem wir InstructGPT nutzen. Wir nehmen 100 spezifische Ziele und bewerten die Skripte, die von großen Sprachmodellen generiert wurden. Diese Tabelle berichtet über die Gesamtnacurratur der Ergebnisse. Wir stellen fest, dass alle Sprachmodelle unbefriedigende Ergebnisse bei der Planung für spezifische Ziele erzielen. Dann führen wir eine detaillierte Analyse durch, um zu untersuchen, warum Lernmodelle scheitern. Die Ergebnisse in der Abbildung zeigen, dass die semantische Vollständigkeit in den generierten Skripten akzeptabel ist, aber die Treue zu den Einschränkungen nicht garantiert werden kann. Wir gehen auf eine feinere Themenkategorie von Einschränkungen ein, die in wikiHow definiert sind. Die Wärmekarte in der Abbildung zeigt, dass die Planungsprestanz von InstructGPT für Ziele verschiedener Kategorien erheblich variiert. Frühere Studien haben gezeigt, dass die Ausgabequalität von Sprachmodellen eine hohe Varianz aufweist, was zu schlechter Leistung führt. Daher übernehmen wir die Idee des Über-Generieren-und-Filtern, um die Generierungsqualität zu verbessern. Zuerst zeigen wir Einschränkungstypen mit Beispielen für InstructGPT und erhalten spezifische Ziele basierend auf den abstrakten Zielsamen. Dann über-generiert InstructGPT K Skripte für spezifische Ziele. Anschließend wird ein Filtermodell entwickelt, um die treuen Skripte auszuwählen. Wir konvertieren Skripte und Ziele in InstructGPT-Embeddings und berechnen die Cosinus-Ähnlichkeit als Ähnlichkeitswerte, um die semantische Ähnlichkeit zu messen. Zusätzlich belohnen wir das Skript, das die Schlüsselwörter des Ziels enthält. Wir behalten das Skript nur bei, wenn das Ziel in der Zielgruppe die höchste Punktzahl erzielt. Mit unserer Methode kann InstructGPT Skripte von höherer Qualität generieren. Unsere Methode verbessert die Planungsfähigkeit sowohl in semantischer Vollständigkeit als auch in Treue zu den Einschränkungen erheblich. Da große Sprachmodelle kostspielig zu betreiben sind, ist es wesentlich, die Sprachplanungsfähigkeit kleinerer und spezialisierter Modelle zu ermöglichen. Die Erstellung des Datensatzes ist ein wesentlicher Schritt zu diesem Zweck. Allerdings ermöglichen frühere Studien keine Planung für spezifische Ziele und die manuelle Datensatzannotation ist teuer. Daher folgen wir der Idee der symbolischen Wissensdestillation, um Datensätze zur eingeschränkten Sprachplanung von großen Sprachmodellen zu destillieren. Wir wenden unsere Methode an, um einen Datensatz zur eingeschränkten Sprachplanung aufzubauen, der CoScript genannt wird. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten. Um die Qualität des Validierungs- und Testdatensatzes zu gewährleisten, bitten wir Crowd-Sourced-Arbeiter, die falschen Proben zu finden und zu überarbeiten. Diese Abbildung zeigt die Einschränkungsverteilung von CoScript. Wir stellen fest, dass CoScript eine hohe Pluralität in den generierten spezifischen Zielen zeigt. Mit CoScript können wir kleinere, aber spezialisierte Modelle für die eingeschränkte Sprachplanung ausprobieren. Wir stellen fest, dass T5, das auf CoScript fein abgestimmt ist, Skripte von höherer Qualität generieren kann als die meisten großen Sprachmodelle, was darauf hindeutet, dass kleinere Modelle größere Modelle übertreffen können, wenn sie auf geeigneten Datensätzen richtig trainiert werden. Zusammenfassend stellen wir das Problem der eingeschränkten Sprachplanung auf. Wir bewerten die eingeschränkte Sprachplanungsfähigkeit großer Sprachmodelle und entwickeln eine Methode zum Über-Generieren-und-Filtern für große Sprachmodelle. Wir nutzen große Sprachmodelle, um einen hochwertigen Skriptdatensatz, CoScript, für die eingeschränkte Sprachplanung zu generieren. Wir hoffen, dass der CoScript-Datensatz eine wertvolle Ressource für die Weiterentwicklung der Forschung zur Sprachplanung sein kann. Vielen Dank für Ihre Zeit. Weitere Details zu CoScript finden Sie in unserem Papier.</sample>
    <sample id="100">PromptRank is a data-efficient approach to multi-hop question answering (QA) that combines unsupervised retrieval with a few-shot language model-based reranker. Unlike traditional methods that require thousands of examples for good performance, PromptRank achieves strong results with as few as 128 examples. The system works in two main steps: first, it retrieves candidate chains using TF-IDF and hyperlink traversal, and then it reranks these candidates using a few-shot language model. The scoring function is based on the likelihood of the question given the chain, as estimated by a language model. To construct the chain prompt, the documents are inserted into a template with an indicator token and an instruction like "Read the previous documents and ask a question." This elicits the language model's reasoning ability over the chain documents. Experiments show that PromptRank outperforms fully supervised systems and performs comparably to state-of-the-art dense retrievers. Additionally, ablation studies confirm the importance of each component. When used as a retriever, PromptRank enables downstream QA performance comparable to MDR, demonstrating its effectiveness in multi-hop QA.</sample>
    <sample id="101">According to the paper, PaLM's fluency is comparable to state-of-the-art systems, but it struggles with accuracy, particularly through omission errors.</sample>
    <sample id="102">Die wichtigsten Eigenschaften eines Wasserzeichenverfahrens für Embedding-as-a-Services sind:

1. **Anwendbarkeit auf Embedding-as-a-Services**: Das Verfahren muss direkt in die Bereitstellung von Embeddings integriert werden können.
2. **Keine Degradation der Embedding-Qualität**: Das Wasserzeichen darf die Leistung und Qualität der bereitgestellten Embeddings nicht beeinträchtigen.
3. **Covertness**: Das Wasserzeichen sollte für Angreifer schwer oder unmöglich zu erkennen oder zu entfernen sein.
4. **Übertragbarkeit**: Das Wasserzeichen muss während des Model-Extraction-Prozesses auf die Dienste des Angreifers übertragen werden können, um die Urheberrechte zu schützen.</sample>
    <sample id="103">The English TED Talks were translated into 14 different languages for the study.</sample>
    <sample id="104">Die Anzahl der Instanzen, die aus einem Datensatz für die erneute Annotierung extrahiert werden, wird nicht explizit im Text genannt. Es wird jedoch erwähnt, dass viele Annotatoren für jede Instanz benötigt werden, um eine reiche und vielfältige Datensatz-Annotierung zu erhalten.</sample>
    <sample id="105">The distance metrics used to measure the difference between benign and backdoor datasets are:

1. **Cosine similarity difference (delta cosine)**
2. **L2 similarity difference (delta L2)**
3. **KS test p-value**</sample>
    <sample id="106">The paper, titled QUEST, addresses the challenge of information retrieval systems handling queries with implicit set constraints, such as those expressed by Jane, a zoologist seeking to identify a red reptile in Costa Rica, or Austin, looking for historical fiction set in France. These queries often involve complex set operations (intersection, complement, etc.) and require systems to retrieve multi-answer sets from large document corpora. To tackle this problem, the authors introduce the QUEST dataset, which includes over 3,000 entity-seeking queries with implicit set constraints. The dataset is constructed using Wikipedia categories from four domains (films, books, plants, animals) and involves human annotation for paraphrasing, fluency, relevance, and evidence attribution. The authors evaluate various retrieval systems, including sparse and dense retrievers and a T5-based reranker, and find that performance is suboptimal, particularly for queries involving set intersection and difference. The study highlights the need for improved systems to better handle selective information needs, as exemplified by Jane and Austin’s scenarios. The authors hope that QUEST will inspire future research to develop more effective information retrieval systems for complex query types. They invite readers to explore their paper and attend their presentation at ACL.</sample>
    <sample id="107">In der Aufgabe wurden Modelle, die auf einem mehrsprachigen Encoder basieren, wie XLM-R + PTR und mBERT + PTR, im Rahmen der Encoder-PTR-Gruppe evaluiert. Diese Modelle erzielten jedoch nicht die besten Ergebnisse im Vergleich zu den Encoder-Decoder-Modellen, die auf multilingualen Pretraining basieren, wie mT5 und XLM-R + PTR. Die Ergebnisse zeigten, dass die Leistung von Encoder-PTR-Modellen durch das Training in einer Mischung verschiedener Sprachen verbessert werden kann, wobei die Leistung von Encoder-Decoder-Modellen am besten war.</sample>
    <sample id="108">In their ACL 2023 paper, Koustav Sinha and co-authors revisit the minimal pair paradigm (MPP) to evaluate language models' acceptability judgments across longer context windows. Traditional MPP evaluates models by comparing their probability assignment to acceptable and unacceptable sentences. However, this approach does not account for the increasing context lengths of modern large language models (LLMs). The authors address this gap by extending the MPP pipeline to longer sequences, simulating longer contexts by combining acceptable and unacceptable sentences from datasets like BLiMP and SyntaxGym.

They explore three scenarios: (1) using sentences from the same dataset as the evaluation query (match scenario), (2) using sentences from a related but different dataset (mismatch scenario), and (3) using completely unrelated sentences from Wikipedia. Results show that MPP judgments remain robust for arbitrary context lengths when using Wikipedia sentences. However, when using sentences from the same dataset, the judgments significantly fluctuate depending on whether the sentences match or mis-match the syntactic or semantic structure of the query pair. This effect intensifies with longer context lengths, suggesting that LLMs are sensitive to shared latent features across sentences.

The authors also analyze perturbations to the input sentences, finding that the models' judgments are not influenced by noise but are highly sensitive to the structural alignment with the query pair. This indicates that LLMs rely on abstract syntactic and semantic knowledge, which can be disrupted by context mismatches. The key takeaway is that current MPP evaluation methods may not fully capture LLMs' abstract knowledge across longer contexts, highlighting the need for more robust evaluation techniques.</sample>
    <sample id="109">The paper "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor" introduces a novel approach to generating instruction datasets for fine-tuning language models. Traditional methods rely on manually annotated datasets or reformulation of existing NLP benchmarks, which limit the diversity and scope of instructions. The authors propose a fully automated method to create a large dataset of natural language instructions, inputs, and outputs without human labor.

The dataset, called Unnatural Instructions, is generated by prompting a pre-trained language model (GPT-3) with examples from the Super-Natural Instructions dataset. The model generates additional examples, which are then further diversified through paraphrasing. This process yields approximately 240,000 examples, showcasing a wide range of tasks, content, and phrasing.

The authors evaluate the dataset's quality by analyzing its creativity, diversity, and correctness. They find that over 50% of the generated examples are correct, and even incorrect examples provide valuable information for instruction tuning. The dataset includes highly creative tasks, such as verifying scientific experiment designs or inventing new words, which differ significantly from traditional NLP tasks.

To assess the dataset's utility, the authors fine-tune an 11 billion-parameter T5 model on Unnatural Instructions and compare its performance to models trained on Super-Natural Instructions. The results show that the model trained on Unnatural Instructions outperforms the baseline on several benchmarks, including Super-Natural Instructions, T0, BIG-Bench Hard, and LMentry.

In conclusion, Unnatural Instructions demonstrates the potential of language models to generate diverse and creative instruction datasets without human labor, offering a faster and more cost-effective alternative to traditional annotation methods. This approach highlights the ability of language models to produce high-quality data, which can be leveraged for fine-tuning and improving the performance of NLP models.</sample>
    <sample id="111">Die Autoren entscheiden, was Wörter mit mittlerer Häufigkeit sind, indem sie einen allgemeinen Textkorpus sammeln und die Häufigkeit der Wörter darin zählen. Sie wählen dann Wörter aus, die in einem bestimmten Frequenzintervall liegen, das als "moderat" bezeichnet wird. Dieses Frequenzintervall wird als "Trigger Set" bezeichnet und dient als Grundlage für die Implementierung des Embedding Marker-Watermarking-Verfahrens.</sample>
    <sample id="112">Hallo alle zusammen, mein Name ist Shuheng. Heute werde ich unseren Artikel „Funktionieren CoNLL-2003-Named-Entity-Tagger immer noch gut im Jahr 2023?“ vorstellen. Lassen Sie uns beginnen. In unserem Artikel haben wir das Problem der Generalisierung im Rahmen der Named Entity Recognition-Aufgabe oder der NER-Aufgabe untersucht. Wir haben festgestellt, dass Modelle seit fast 20 Jahren für die Entwicklung von NER in CoNLL-2003 verwendet wurden, und dies wirft natürlich mehrere Probleme auf. Erstens, können diese Modelle auf moderne Daten generalisieren? Und was ist bei der Entwicklung neuer Tagger für eine gute Generalisierung erforderlich? Gleichzeitig, wenn wir eine schlechte Generalisierung beobachten, was verursacht den Leistungsabfall dieser Modelle? Um diese Probleme zu untersuchen, haben wir das CoNLL++-Datensatz entwickelt. Dies ist ein Datensatz, den wir aus Reuters News aus dem Jahr 2020 gesammelt und dann mit den gleichen CoNLL-2003-Anmerkungsrichtlinien annotiert haben. Anschließend haben wir über 20 Modelle auf CoNLL-2003 fein abgestimmt. Wir haben sie sowohl auf den CoNLL-03-Testsätzen als auch auf CoNLL++ bewertet. Zuletzt haben wir den prozentualen F1-Änderungswert berechnet, um die Generalisierung jedes Modells zu bewerten. Was ist also für eine gute Generalisierung erforderlich? Während unserer Experimente haben wir festgestellt, dass drei Hauptbestandteile erforderlich sind. Der erste ist die Modellarchitektur. Durch unsere Experimente haben wir festgestellt, dass Transformer-Modelle normalerweise besser auf neue Daten generalisieren. Der zweite Bestandteil ist die Modellgröße. Wir haben festgestellt, dass in der Regel größere Modelle zu einer besseren Generalisierung führen. Und nicht zuletzt wissen wir alle, dass die Anzahl der Feinanpassungsexemplare direkt die Leistung einer nachgeschalteten Aufgabe beeinflusst. Auch hier haben wir festgestellt, dass mehr Feinanpassungsexemplare tatsächlich auch zu einer besseren Generalisierung führen. Zu unserer nächsten Frage, was den Leistungsabfall einiger Modelle verursacht, hatten wir zwei Hypothesen. Die erste ist das adaptive Overfitting, bei dem Overfitting durch die wiederholte Verwendung desselben Testsatzes immer wieder auftritt und sich normalerweise als abnehmende Renditen auf einem neuen Testsatz manifestiert. Die zweite Hypothese ist der zeitliche Drift, der die Leistungsdegradation ist, die durch die zunehmende zeitliche Lücke zwischen den Trainings- und den Testsätzen verursacht wird. Für Daten-Overfitting haben wir gesehen, dass die rote beste Anpassungslinie in dem Diagramm auf der rechten Seite eine Steigung hat, die größer als eins ist. Das bedeutet, dass jede Einheit der Verbesserung, die wir auf CoNLL-2003 vorgenommen haben, sich in mehr als eine Einheit Verbesserung auf CoNLL++ umsetzt, was bedeutet, dass es keine abnehmenden Renditen gibt. Und das zeigt uns, dass in diesem Fall kein adaptives Overfitting beobachtet wird. Wie sieht es dann mit dem zeitlichen Drift aus? Für den zeitlichen Drift haben wir ein Experiment durchgeführt, um einige Modelle mit neueren Daten erneut zu trainieren oder fortlaufend vorzuqualifizieren, und wir haben festgestellt, dass die Leistung mit größerer zeitlicher Lücke abnimmt, und das bestätigt unsere Hypothese, dass die Hauptursache für den Leistungsabfall der zeitliche Drift ist. Unsere Schlussfolgerung ist, dass wir für eine gute Generalisierung eine bessere Modellarchitektur, eine größere Modellgröße sowie mehr Feinanpassungsexemplare benötigen. Und das geht Hand in Hand, wir können nicht nur einen Bestandteil haben und die anderen wegwerfen. Gleichzeitig haben wir auch festgestellt, dass der Leistungsabfall hier durch zeitlichen Drift verursacht wird und überraschenderweise nicht durch adaptives Overfitting, obwohl CoNLL-2003 seit über 20 Jahren verwendet wird. Wenn wir also zur Frage zurückkehren, die wir im Titel unseres Artikels gestellt haben: Funktionieren CoNLL-2003-Tagger immer noch gut im Jahr 2023? Und wir haben festgestellt, dass die Antwort tatsächlich ein klares Ja ist. Wir hoffen, dass unser Artikel zu mehr Forschung darüber anregt, wie die Generalisierung der Modelle verbessert werden kann. Und zuletzt, stellen Sie sicher, dass Sie unseren Artikel, unseren Datensatz überprüfen und wenn Sie Fragen haben, zögern Sie nicht, mich zu kontaktieren. Vielen Dank.</sample>
    <sample id="114">**Abstract:**  
We address the heavy parameter problem in large language models (LLMs) by proposing **Grouped Head Attention (GHA)**, a method to optimize multi-head attention for parameter efficiency. GHA employs a **divide-and-conquer strategy** with two stages: **group-constrained training** and the **Voting-to-Stay (VTS) algorithm**. In the first stage, attention heads are divided into groups, encouraging intra-group similarity and inter-group diversity. The second stage uses the VTS algorithm to prune redundant heads, retaining only one head per group, achieving up to 90% parameter compression without performance loss. We evaluate GHA on machine translation, language modeling, and abstractive summarization tasks, achieving BLEU improvements of 3.8% and 4.4%, summarization improvements of 6.7% and 7%, and language modeling improvements of 2.8% and 2.9%. Additionally, our LITE model achieves 90% parameter pruning, 62% faster inference, and 80% FLOP reduction. Future work will explore task-specific automatic pruning, leveraging the Lottery Ticket Hypothesis to prune redundant parameters without sacrificing performance.</sample>
    <sample id="115">Der Ansatz verwendet die Größe von **lambda** Sprachrahmen, um zu bestimmen, wann ein Sprachsegment emittiert wird.</sample>
    <sample id="116">Im Beispiel mit Servin und Kea wird das entitätsspezifische Wissen benötigt, dass **"Servin ein Richter ist"**. Dieses Wissen ist entscheidend, um zu bestimmen, dass der Pronomen "he" sich auf Servin bezieht, da es die spezifische Rolle von Servin in der gegebenen Situation beschreibt.</sample>
    <sample id="117">Der wichtigste Faktor für die Leistung von PaLM bei der Übersetzung ist die **Qualität des Beispiels**, nicht die Ähnlichkeit mit dem Ausgangssatz.</sample>
    <sample id="118">The paper presents "SwitchMLM," a novel approach to improve multilingual pre-trained models for code-switched NLP tasks. Code-switching, the alternation between languages in a single conversation, is prevalent in linguistically diverse communities, making computational models for such tasks essential. Existing models like mBERT and XLM-R struggle with code-switched tasks like sentiment analysis and question answering. SwitchMLM addresses this by introducing a masked language modeling (MLM) technique tailored to code-switching. It identifies "switch-points"—transitions between languages—and masks only these tokens, unlike standard MLM which masks all tokens uniformly. However, SwitchMLM requires access to language identification (LID) tags, which may not always be available. To overcome this, the paper proposes "FrequencyMLM," a surrogate method that uses word frequencies in monolingual corpora to infer LID tags.

Additionally, the paper suggests architectural modifications, including residual connections from intermediate layers to the final layer of BERT, to enhance switch-point information encoding. An auxiliary LID-based loss is also introduced to encourage the intermediate layer to learn language information. Experimental results show that the combined method—SwitchMLM or FrequencyMLM with residual connections and auxiliary loss—outperforms baselines on sentiment analysis tasks across multiple language pairs. Probing experiments using linear and conditional probing classifiers further validate the increased switch-point information in intermediate and final layers, supporting the proposed architectural changes. In summary, SwitchMLM improves code-switched NLP by focusing on switch-point information and leveraging tailored MLM and architectural modifications.</sample>
    <sample id="119">The work in the extended experiments focuses on **GPT-4** and **RoBERTa** language models. Specifically:

- **GPT-4** is highlighted as the most liberal language model among those studied.
- **RoBERTa** is used in a controlled experiment where it is further pretrained on a left-leaning Reddit corpus, demonstrating a significant liberal shift in its political biases.

These models are analyzed to understand how their political leanings are influenced by the pretraining data and how they perform on downstream tasks like hate speech and fake news detection.</sample>
    <sample id="120">Das Modell verwendet Aufmerksamkeitswerte aus einer bestimmten Ebene, nämlich der cross-attention zwischen Audio-Eingabe und textueller Ausgabe. Es kombiniert diese Werte nicht aus mehreren Ebenen.</sample>
    <sample id="121">Direkte Inferenzen beziehen sich auf klare, explizite Verweise auf eine bestimmte Entität, ohne Umwege oder indirekte Beschreibungen. Beispiele für direkte Inferenzen in dem Kontext der Arbeit über "Resolving Indirect Referring Expressions for Entity Selection" sind:

- "Easy on Me"
- "I Gotta Feeling"
- "the first one"
- "the second one"

Diese Verweise sind direkt und eindeutig, im Gegensatz zu indirekten Verweisen wie "the newer one" oder "the song that's not energetic", die eine Interpretation erfordern.</sample>
    <sample id="122">Die Autoren gehören der Fudan University an.</sample>
    <sample id="123">Ying and Zhiyang present their research on **MultiInstruct**, a groundbreaking dataset designed to improve **Multi-Modal Zero-Shot Learning** through **Instruction Tuning**. The authors highlight the gap in instructional datasets between **language-only** and **multi-modal tasks**, noting that while there are over 1,600 language-only instruction tasks, no large-scale multi-modal instruction dataset existed prior to their work. **MultiInstruct** addresses this by creating a benchmark dataset of 62 diverse multi-modal tasks across 10 categories, derived from 21 open-source datasets. Each task is accompanied by five expert-written instructions, enabling unified processing of text, images, and bounding boxes using a shared token space.

The authors use **OFA**, a unified multi-modal pre-trained model, as their base model. They train on 53 tasks, sampling 10,000 instances per task, and evaluate on 10 tasks from the test split, including unseen NLP tasks. The model is fine-tuned with mixed instances and instruction templates, and performance is measured using accuracy for classification, **Rouge-L** for generation, and a new metric called **sensitivity**, which assesses consistency across varying instructions.

Key findings show that **instruction tuning** significantly improves OFA’s performance on multi-modal tasks, with transfer learning from natural instruction datasets further enhancing results. Using multiple instructions reduces sensitivity and improves performance. The authors are expanding the dataset to include 150 additional vision-language tasks and plan to release them soon. This work demonstrates the potential of instruction tuning for multi-modal zero-shot learning and introduces valuable tools for future research.</sample>
    <sample id="124">Tan Qingyu von der National University of Singapore und Alibaba präsentierte die Arbeit „Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models“. Die Forschung untersucht die Fähigkeit von Sprachmodellen, zeitliche Zusammenhänge zu verstehen und zu nutzen. Temporal Reasoning wurde in drei Ebenen unterteilt: Zeit-zu-Zeit-Reasoning (z. B. „Welches Jahr folgt auf 2010?“), Zeit-zu-Ereignis-Reasoning (z. B. „Für welches Team spielte Lionel Messi 2010?“) und Ereignis-zu-Ereignis-Reasoning (z. B. „Für welches Team spielte Lionel Messi nach FC Barcelona?“). Die Studie kritisiert, dass frühere Arbeiten sich zu sehr auf die zweite Ebene konzentrierten, während sie eine umfassendere Betrachtung anstreben.

Die Forscher entwickelten den TempReason-Datensatz, der alle drei Ebenen und lange Zeiträume abdeckt. Sie testeten verschiedene Modelle, darunter T5-L, FLAN-T5-L und ChatGPT, und stellten fest, dass ChatGPT bei der Voraussage von Monaten versagte. Um die Fähigkeiten zu verbessern, schlugen sie eine Trainingsstrategie mit zwei Komponenten vor: Temporal Span Extraction Pre-training und time-sensitive Reinforcement Learning. Das verbesserte Modell, TempT5, zeigte bessere Ergebnisse in Closed Book QA, Open Book QA und Reasoning QA. Trotz der Fortschritte gibt es noch Schwankungen in der Leistung über verschiedene Zeiträume hinweg, was auf ein Ungleichgewicht im Trainingsdatensatz hinweist. Zukünftige Arbeiten sollen sich auf die Überwindung solcher Vorurteile konzentrieren.</sample>
    <sample id="125">The text does not explicitly mention the number of authors involved in the work. Therefore, based on the provided information, it is not possible to determine the number of authors.</sample>
    <sample id="126">Ja, die Übersetzung der natürlichsprachlichen Anfrage wurde mit Hilfe des Google Translate API als Baseline vor dem semantischen Parsing betrachtet.</sample>
    <sample id="127">Die Arbeit "Large Language Models Are Reasoning Teachers" von Namgyu Ho, Laura Schmid und Se-Young Yun adressiert die Herausforderung, dass große Sprachmodelle wie GPT-3 oder PALM zwar komplexes Denken bewältigen können, aber aufgrund ihrer hohen Rechen- und Speicheranforderungen schwer einsetzbar sind. Die Forscher schlagen vor, diese großen Modelle als "Lehrer" zu nutzen, um ihre Fähigkeiten an deutlich kleinere Modelle zu übertragen. Dabei wird eine Technik namens "Diverse Reasoning" eingeführt, bei der nicht nur eine, sondern mehrere unterschiedliche Lösungswege durch stochastische Temperaturproben generiert werden, um die Lernfähigkeit der kleineren Modelle zu verbessern.

Die Methode kombiniert Zero-Shot-Chain-of-Thought-Prompting mit einer einfachen Feinabstimmung der kleineren Modelle. Die großen Lehrermodelle lösen komplexe Aufgaben schrittweise, und die Lösungen werden in Trainingsdaten für die kleineren Modelle umgewandelt. Die Ergebnisse zeigen, dass die so trainierten Modelle (fine-tuned CoT) komplexe Aufgaben gut bewältigen können, insbesondere in textbasierten Bereichen. Diverse Reasoning steigert die Leistung erheblich, z.B. von 33 % auf 55 % bei mathematischen Aufgaben. Die Methode ist skalierbar und kann durch größere Datensätze, bessere Lehrermodelle oder leistungsfähigere Schülermodelle weiter verbessert werden. Sie bietet eine effiziente Möglichkeit, die Fähigkeiten großer Sprachmodelle auf kleinere, kostengünstigere Modelle zu übertragen.</sample>
    <sample id="128">**Abstract:**  
In "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources," we propose a diagnostic suite to assess how well natural language understanding (NLU) models integrate knowledge from multiple sources. Coreference resolution is used as a key task to evaluate the ability to combine pretrained background knowledge with inference-time entity-specific knowledge. We introduce three KITMUS settings: "Background-Pretrain," where background knowledge is available at pretrain time; "Background-Both," where it is available in both pretrain and inference contexts; and "Background-Inference," where both types of knowledge are provided only at inference time. Experiments with human participants and established models like C2F and BERT4Coref show that models perform poorly without KITMUS-specific training. However, trained models improve significantly, though they still struggle with integrating background knowledge provided only at inference time. This highlights the need for task-specific training to enhance knowledge integration in NLU tasks. The KITMUS dataset and code are available on GitHub for further exploration.</sample>
    <sample id="129">The authors provided the example of **Latina women** as a marked group, where the generated personas included words like "vibrant" and "curvaceous," reflecting the trope of tropicalism. This highlights how seemingly positive portrayals can perpetuate essentializing narratives and stereotypes.</sample>
    <sample id="130">Die Präsentation gibt keine spezifischen Informationen darüber, welche Modellarchitekturen nicht gut generalisieren. Es wird jedoch erwähnt, dass Transformer-Modelle im Allgemeinen besser generalisieren als andere Architekturen. Die Studie konzentriert sich auf die Faktoren, die zur guten Generalisierung beitragen, und zeigt, dass Transformer-Modelle, größere Modelle und mehr Feinanpassungsbeispiele dazu beitragen, die Generalisierung zu verbessern. Es wird nicht explizit angegeben, welche Architekturen schlecht generalisieren, aber die Implikation ist, dass ältere oder kleinere Modelle möglicherweise nicht so gut abschneiden wie Transformer-Modelle.</sample>
    <sample id="131">The presentation does not explicitly mention the names of the test datasets. However, it discusses the performance of weakly supervised learning (WSL) methods on clean test sets, implying that the test data is clean and not labeled using weak supervision. The focus is on the necessity and impact of clean validation samples rather than the specific names of the test datasets.</sample>
    <sample id="132">There are two authors involved in the work: Akshatha and Martin.</sample>
    <sample id="133">Die Autoren arbeiten mit mehreren Modalitäten, insbesondere mit Text und Bildern. Sie untersuchen die Verbesserung von Multi-Modal Zero-Shot Learning durch Anweisungstuning und verwenden dazu einen multi-modalen Pre-trained-Modell namens OFA, das Sprache, Bild-Token und die Koordinaten eines Bounding-Boxes verarbeitet. Ihr Dataset, MultiInstruct, umfasst 62 verschiedene multi-modale Aufgaben, die aus 21 bestehenden Open-Source-Datensätzen abgeleitet sind und sowohl Text als auch Bilder enthalten.</sample>
    <sample id="135">ABC-Eval ist ein neues dimensionales Bewertungsverfahren für konversationale KI, entwickelt vom Emory NLP Lab unter der Leitung von Professor Jinho Choi in Zusammenarbeit mit Amazon Alexa AI. Es geht über die herkömmliche menschliche Bewertung hinaus, bei der menschliche Richter entscheiden, welche von zwei Konversationen besser ist oder eine Likert-Skala verwenden, um die Qualität zu bewerten. ABC-Eval konzentriert sich auf spezifische Verhaltensweisen von KI-Modellen, wie das Ignorieren des Gesprächspartners, das Produzieren irrelevanter Informationen, Selbst- oder Partnerwidersprüche, das Halluzinieren falscher Fakten und das Versäumnis, Empathie zu zeigen. 

Um die Effektivität von ABC-Eval zu testen, wurden vier hochmoderne Chatmodelle auf 100 menschlichen-Bot-Gesprächen bewertet. Zum Vergleich wurden auch drei bestehende Methoden verwendet: Likert-Bewertungen auf Turn-Ebene, Likert-Bewertungen auf Dialog-Ebene und Dialog-Ebene-Paarvergleich. Die Ergebnisse zeigten, dass ABC-Eval-Labels zuverlässiger und prädiktiver für die Gesamtqualität der Konversation sind als die durch bestehende Methoden erzeugten Metriken. Die Kombination aller ABC-Eval-Metriken erklärt über 25% der Gesprächsqualität, während die Kombination aller Turn-Level-Likert-Metriken weit weniger erklärt. Dies ermöglicht eine detailliertere Bewertung der konversationellen KI als bisherige Methoden. Die Studie identifizierte auch spezifische Herausforderungen, wie z.B. dass die getesteten Bots in etwa 20% ihrer Antworten gegen den gesunden Menschenverstand verstießen und in etwa 15% der Antworten irrelevante Informationen produzierten. ABC-Eval bietet somit eine präzise und zuverlässige Methode zur Bewertung und Verbesserung konversationeller KI.</sample>
    <sample id="136">Jasivan präsentiert die Arbeit "FERMAT: An Alternative to Accuracy for Numerical Reasoning", die er mit seiner Supervisorin Nafise an der University of Sheffield durchgeführt hat. Die Motivation hinter der Arbeit liegt in der Notwendigkeit, die Genauigkeit von Modellen im Bereich des numerischen Schlussfolgerns zu verbessern, da diese Fähigkeiten in vielen realen Anwendungen und Folgeprozessen entscheidend sind, wie z.B. bei der Faktenprüfung. Ein Beispiel hierfür ist die Analyse von Aussagen und Tabellen, bei denen numerische Berechnungen erforderlich sind, um die Richtigkeit der Aussage zu bestimmen.

Die aktuelle Forschung zeigt, dass größere Sprachmodelle tendenziell besser in numerischen Aufgaben abschneiden, aber auch, dass die bestehenden Benchmarks, die sich hauptsächlich auf Genauigkeit konzentrieren, nicht aussagekräftig genug sind, um die Stärken und Schwächen der Modelle im mathematischen Bereich zu bewerten.

FERMAT, ein von Jasivan und Nafise entwickeltes alternatives Bewertungssystem, konzentriert sich auf drei Hauptbereiche: Verständnis von Zahlen, mathematische Operationen und Trainingsabhängigkeit. Es verwendet Fragen, die aus den Illinois- und CommonCore-Benchmarks extrahiert wurden, und variiert die Art der Zahlen (z.B. große und kleine ganze Zahlen, Dezimalzahlen) sowie die Komplexität der Operationen.

Die Ergebnisse zeigen, dass die Leistung der Modelle durch Feinabstimmung und die Einführung von sprachlicher und mathematischer Vielfalt erheblich verbessert werden kann. Die Arbeit unterstreicht die Bedeutung von sprachlicher und mathematischer Vielfalt im Training sowie die Notwendigkeit, die Art und Weise, wie Zahlen kodiert und tokenisiert werden, zu verbessern, um die Fähigkeiten der Modelle im Bereich des numerischen Schlussfolgerns zu steigern.</sample>
    <sample id="137">The paper "Tell2Design: A Dataset for Language-Guided Floor Plan Generation," published in ACL 2023, introduces a novel task in AI design generation: creating floor plans directly from natural language instructions. Unlike text-conditional image generation, which focuses on high-level, artistic outputs, this task requires strict adherence to user-defined constraints, semantics, geometry, and topology. The researchers constructed the Tell2Design dataset, comprising 5,051 human-annotated and 76,000 artificially generated language instructions, to train models for this purpose.

The proposed approach treats floor plan generation as a sequence-to-sequence problem using a transformer-based encoder-decoder framework. This allows the model to handle varying lengths of instructions and generate structured interior layouts. The model is initialized with a pre-trained language model (T5) to enhance language understanding. Results on the T2D dataset show that the sequence-to-sequence model outperforms text-conditional image generation baselines, achieving high Intersection over Union (IoU) scores (Micro IoU: 54, Macro IoU: 53). However, the model struggles with purely human-written instructions due to a language distribution gap between artificial and human instructions. Warming up with artificial instructions before training on human ones significantly improves performance, highlighting the mutual benefit of both data types.

The study emphasizes the importance of aligning AI-generated designs with user preferences and constraints, paving the way for future research in language-guided design generation.</sample>
    <sample id="138">Nach Ansicht der Autoren ist die Integration von Wissen aus verschiedenen Quellen (Pretrain-Zeit und Inferenz-Zeit) ein zu wenig erforschtes Gebiet im Bereich der Natural Language Understanding (NLU). Sie betonen, dass viele Modelle Schwierigkeiten haben, Wissen aus unterschiedlichen Quellen zu kombinieren, insbesondere wenn Hintergrundwissen nur während der Inferenz zur Verfügung steht.</sample>
    <sample id="139">The presenters are Ying and Zhiyang.</sample>
    <sample id="140">Ja, CoScript hat eine Qualitätskontrolle durchlaufen. Crowd-sourced Workers wurden gebeten, die Datensätze zu überprüfen und fehlerhafte Proben zu korrigieren, um die Qualität der Validierungs- und Testdatensätze zu gewährleisten.</sample>
    <sample id="141">Existing resources for context-based translation face several limitations:

1. **Limited Scope**: They often rely on domain-specific knowledge and human curation, which restricts their applicability to a narrow range of contexts and languages.

2. **Narrow Context Types**: These resources typically support only limited types of context-dependent translations, failing to capture the full complexity of real-world translation scenarios.

3. **Small-Scale Data**: They are based on small-scale datasets, making it difficult to generalize findings across different languages and contexts.

4. **Inadequate Evaluation Metrics**: Corpus-level metrics like BLEU cannot effectively measure context-dependent translations, leading to an incomplete evaluation of translation models.

5. **Lack of Automation**: Many resources require manual intervention, which is time-consuming and limits scalability for large-scale translation tasks.

These limitations highlight the need for more comprehensive, data-driven approaches to evaluate and improve context-based translation systems.</sample>
    <sample id="142">Hallo! Ich werde über unsere Arbeit an "Der Auflösung indirekter Verweise zur Entitätssuche" sprechen, in der wir den AltEntities-Datensatz vorstellen. Mein Name ist Javad Hosseini und dies ist eine gemeinsame Arbeit mit Filip Radlinski, Silvia Pareti und Annie Louis. Unser Ziel ist es, die Sprache der Nutzer zu verstehen, wenn sie eine Auswahl treffen möchten. Betrachten Sie diese alternative Frage: "Meinten Sie 'Easy on Me' oder 'I Gotta Feeling'?" Hier möchte ein Nutzer zwischen diesen beiden Liedern wählen. Das Offensichtlichste ist, einen direkten Verweis zu verwenden, zum Beispiel, indem man den Namen des Liedes "Easy on Me" oder seine Position "das erste" nennt. Manchmal ist jedoch ein indirekter Verweis angemessener, um eine natürlichere Konversation zu führen. Dies könnte passieren, wenn sich der Nutzer den Namen des Liedes nicht erinnern kann. Oder wenn die Aussprachen zu ähnlich sind und schwer zu unterscheiden sind. Oder wenn der Nutzer eine Präferenz angeben möchte. Hier sind einige Beispiele für indirekte Verweise, zum Beispiel "das neuere" oder "das Lied, das nicht energiegeladen ist." Dies ist ein wichtiges Problem in Konversations-Systemen und auch für die Bewertung des Entitätsverständnisses von LLMs. Uns ist kein größerer öffentlicher Datensatz für die Aufgabe bekannt, daher sammeln wir einen mit Crowd-Annotation. Unser Datensatz deckt drei verschiedene Bereiche ab: Musik, Bücher und Rezepte. Unsere Datensammlungs-Methodik legt den Schwerpunkt auf Unformalität unter Verwendung eines Cartoon-Abschluss-Setups. Der Cartoon hat drei Sprechblasen. In der ersten Blase sagt Bob: "Erinnern Sie sich an das Lied, das wir gestern gehört haben?" Damit setzt Bob den Dialogkontext. In der zweiten Sprechblase sagt Alice: "Meinen Sie 'Easy on Me' oder 'I Gotta Feeling'?" Das ist die alternative Frage. Und in der dritten Sprechblase verwendet Bob einen indirekten Verweis, um eine dieser Entitäten auszuwählen, zum Beispiel "das neuere." Wir stellen die ersten und zweiten Sprechblasen automatisch zur Verfügung, aber die dritte wird vom Annotator ausgefüllt. Die erste Sprechblase wird aus einigen manuellen Hinweisen pro Bereich ausgewählt. Die zweite, die die alternative Frage ist, wird wie folgt generiert. Verwenden Sie immer eine einfache Vorlage. Meinen Sie A oder B? Wobei A und B Proben aus Wikipedia sind. Hier sind die verschiedenen Sampling-Methoden, die wir verwendet haben. Wenn wir weiter oben in der Liste gehen, werden die Entitäten ähnlicher zueinander und es ist normalerweise schwieriger, die Disambiguierung vorzunehmen. Die erste ist uniform zufällig. Die zweite ist, wenn die Entitäten ähnliche Titel haben, zum Beispiel zwei Bücher mit dem Namen "The Return". Die dritte ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben. Und schließlich, wenn sie ähnliche Infoboxen oder Attribute auf Wikipedia haben. Zum Beispiel, die gleiche Gattung oder der gleiche Künstler für ein Lied. Wenn wir diese alternative Frage den Annotatoren zeigen, kennen sie den Namen dieser Entitäten, aber sie wissen nicht unbedingt etwas über die Entitäten. Also zeigen wir ihnen einige Hintergrundinformationen über die beiden Entitäten. Für Lieder zeigen wir einfach einen Google-Suchlink zu jedem Lied und bitten dann die Annotatoren, mindestens einige von jedem Lied anzuhören und darüber zu lesen. Hier ist zum Beispiel das Google-Suchresultat für das Lied "Easy on Me." Für die Bereiche Rezepte und Bücher zeigen wir einige Hintergrundtexte aus Wikipedia. Für Rezepte zeigen wir zusätzlich ihre Bilder, wieder aus Wikipedia, damit die Annotatoren wissen, wie sie aussehen. Dann bitten wir die Annotatoren, eine dieser Entitäten auszuwählen, zum Beispiel hier die erste, und sie mit drei bis fünf indirekten Verweisausdrücken zu beschreiben. Zum Beispiel "diejenige ohne Worte", "nicht diejenige mit dem 12-jährigen Jungen" oder "die fiktive" oder "kommt aus Aserbaidschan" und so weiter. Der AltEntities-Datensatz hat 6.000 alternative Fragen in drei Bereichen und 42.000 indirekte Verweisausdrücke. Die Ergebnisse mit dem T5 XL-Modell sind wie folgt zusammengefasst. Wenn das Sprachmodell Zugang zu genau den gleichen Hintergrundinformationen wie die Annotatoren hat, ist die Genauigkeit sehr hoch, sie liegt bei etwa 92 bis 95%. Dies ist jedoch nicht realistisch. Wenn das Sprachmodell Zugang zu teilweise überlappenden Hintergrundinformationen hat, liegt die Genauigkeit zwischen 82 und 87%, was realistischer ist. Zum Beispiel, wenn das Sprachmodell die Hintergrundinformationen abruft. Wenn das Sprachmodell nur Zugang zu den Entitätsnamen hat, liegt die Genauigkeit nur bei 60%, also gibt es viel Raum für Verbesserungen. Wir haben auch gezeigt, dass die Modelle bereichsübergreifend generalisierbar sind. Hier ist ein Link zu unserem Datensatz. Danke.</sample>
    <sample id="143">The approach, **EDAtt (Encoder-Decoder Attention)**, is compared with the following existing SimulST strategies:

1. **Wait-k strategy**: A method applied to offline models where the model waits for a certain number of tokens (k) before emitting a translation.
2. **Local Agreement**: A strategy that evaluates the agreement between the current and previous translations to decide when to emit a translation.
3. **State-of-the-art architecture specifically tailored for simultaneous pre-translation**: Architectures designed specifically for SimulST that are considered the best in their class at the time of the study.

These comparisons are based on metrics like **BLEU** (translation quality) and **average lagging** (latency), as well as **computational-aware average lagging** (accounting for computational time).</sample>
    <sample id="144">The authors belong to **Nantes University Hospital**. This is evidenced by the mention of the "Nantes University Hospital data warehouse" in the context of their ChuBERT model.</sample>
    <sample id="145">Die Referentin heißt Jenny.</sample>
    <sample id="146">In this talk, Yicheng, a PhD student from Fudan University, discusses the issue of omission in dialogue summarization, a critical problem despite advancements in summarization techniques using large language models. The research team analyzed the omission rate across five domains and six pre-trained models, revealing that even state-of-the-art models generate summaries with an omission rate of around 70%. Omission information is randomly distributed throughout the dialogue, indicating the unstructured nature of dialogues and the difficulty for models to identify key information.

To address this issue, the team constructed the OLDS dataset, a high-quality dataset for omission detection in dialogue summarization. The dataset is built on five existing benchmarks covering five domains, with diverse candidate summaries generated by different models and decoding strategies. An automatic method was developed to produce omission labels, and human evaluation ensured their quality.

The team explored three baseline frameworks for omission detection: pair-wise classification, sequence labeling, and pointer networks. Evaluation metrics included Precision, Recall, F1-score, and a word-level omission recall (WR score). The results showed that the F1-score was around 50%, indicating the task's complexity.

Furthermore, the team investigated the impact of using detected omissions to refine summaries. A post-editing method was applied, where the candidate summary and omitted content were concatenated as input, leading to improved summary quality. This suggests that omission detection is a valuable task, and refining summaries based on detected omissions is a promising direction for enhancing the quality of dialogue summarization.</sample>
    <sample id="147">The work is done in collaboration with Esin Durmus and Dan Jurafsky, so there are **three authors** involved in the paper.</sample>
    <sample id="148">Hallo, ich bin Sara Papi von der Universität Trient und der Fondazione Bruno Kessler und werde kurz den Artikel „Attention as a Guide for Simultaneous Speech Translation“ vorstellen, der eine gemeinsame Arbeit mit Matteo Negri und Marco Turchi ist. Was ist simultane Sprachübersetzung? Die simultane Sprachübersetzung, oder SimulST, ist der Prozess der Übersetzung von gesprochener Sprache in einen Text in einer anderen Sprache in Echtzeit, was die Kommunikation zwischen verschiedenen Sprachen ermöglicht. Und was sind die Probleme der aktuellen SimulST-Modelle? Üblicherweise werden spezifische Architekturen trainiert, die zusätzliche Module zur Optimierung einführen. Lange und komplizierte Trainingsprozeduren, zum Beispiel Training mit verschiedenen Optimierungszielen. Und das Training und die Wartung mehrerer Modelle, um verschiedene Latenzregime zu erreichen. Zum Beispiel das Training eines Modells mit einer durchschnittlichen Latenz von einer Sekunde und eines anderen mit zwei Sekunden Latenz und so weiter. Was ist also unsere Lösung? Erstens, bereits bestehende Offline-ST-Modelle ohne Neu-Training oder die Annahme spezifischer Architektur für SimulST zu verwenden. Verwenden Sie nur ein Modell für jedes Latenzregime und behandeln Sie die Latenz durch spezifische Parameter. Und nutzen Sie das Wissen, das das Modell bereits durch den Aufmerksamkeitsmechanismus zwischen Audioeingang und textlichem Ausgang erworben hat. Das ist der Kreuzaufmerksamkeitsmechanismus, und Sie können ein Beispiel rechts sehen. Unsere Lösung besteht darin, EDAtt, oder Encoder-Decoder-Aufmerksamkeit, vorzuschlagen, und es ist eine Strategie, bei der wir entscheiden, ob wir eine teilweise Übersetzung emittieren oder nicht, basierend darauf, wohin die Aufmerksamkeit weist. Ein Wort wird emittiert, wenn die Aufmerksamkeit nicht konzentriert ist, das heißt, ihre Summe unter einem bestimmten Schwellenwert alpha liegt, in Bezug auf die letzten lambda Sprachrahmen, was bedeutet, dass die empfangenen Informationen ausreichend stabil sind. Zum Beispiel, wenn wir einen Sprachchunk erhalten, der „Ich werde über… sprechen“ enthält, und unser Modell die Übersetzung ins Deutsche vorhersagt, und wir uns die Kreuzaufmerksamkeitsgewichte ansehen, werden wir sehen, dass die ersten beiden Wörter auf die frühesten empfangenen Sprachrahmen verweisen, während das letzte Wort auf die letzten empfangenen Sprachrahmen verweist, als lambda Sprachrahmen. Das bedeutet, dass die ersten beiden Wörter emittiert werden, da die Summe der Kreuzaufmerksamkeit über einem bestimmten Schwellenwert alpha liegt, werden wir das letzte Wort nicht emittieren und warten auf einen weiteren Sprachchunk. Wenn wir weitermachen und einen weiteren Sprachchunk erhalten, und unser Modell drei weitere Wörter vorhersagt, und wir uns diese Kreuzaufmerksamkeitsgewichte ansehen, werden wir sehen, dass kein Wort auf die letzten lambda Sprachrahmen verweist. Das bedeutet, dass diese drei Wörter emittiert werden. Wenn wir die Hauptergebnisse von EDAtt betrachten, werden wir die Ergebnisse der simultanen Sprachübersetzung in Diagrammen darstellen, in denen wir auf der einen Seite BLEU haben, das die Übersetzungsqualität misst, und durchschnittliche Verzögerung, das ist das Latenzmaß, und wir berücksichtigen auch die rechenbewusste durchschnittliche Verzögerung, die die Rechenzeiten des Modells zur Vorhersage des Ausgangs berücksichtigt. Wir möchten also, dass unsere Kurven in diesem Diagramm so hoch wie möglich sind, aber auch, dass sie nach links verschoben sind. Und wir vergleichen mit beliebten Strategien, die auch auf Offline-Modellen angewendet werden, die die Wait-k-Strategie und die Lokale Übereinstimmung sind. Und wir vergleichen auch mit der Stand-der-Technik-Architektur, die speziell für die simultane Vorübersetzung zugeschnitten ist. Das sind alle Ergebnisse der Strategie der simultanen Sprachübersetzung auf Deutsch. Und wir sehen, dass sie alle Strategien, die auf Offline-Modellen angewendet werden, übertrifft, da die Kurven nach links verschoben sind. Und wir sehen auch, dass, wenn wir die tatsächliche verstrichene Zeit oder die rechenbewusste Zeit betrachten, das ist die schnellste Strategie. Wenn Sie mehr Ergebnisse entdecken möchten, lesen Sie unseren Artikel. Und wir haben den Code und die Modelle und die simultane Ausgabe open source veröffentlicht, um die Reproduzierbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="149">Ja, der CoNLL++-Datensatz ist öffentlich zugänglich.</sample>
    <sample id="150">Archiki und seine Kollegen präsentieren den ACL-Paper "MEETINGQA: Extractive Question-Answering on Meeting Transcripts", der sich mit der Herausforderung befasst, Fragen und Antworten aus Meeting-Transkripten zu extrahieren. Meeting-Transkripte sind lange, domänenspezifische und informationsreiche Dokumente, die bisher hauptsächlich für Summarisierung und Extraktion von Handlungspunkten genutzt wurden. Das Team hat ein neues Dataset namens MeetingQA erstellt, das Fragen und Antworten aus Meetings extrahiert und analysiert. MeetingQA enthält 7.700 Fragen, 30% der Fragen sind unanswerbar, 40% haben mehrteilige Antworten und 48% mehrsprachige Antworten. Die Fragen sind oft lang, offen und regen Diskussionen an, und die Antworten können von mehreren Sprechern stammen. Die Forscher haben das Dataset aus öffentlichen Meeting-Transkripten des AMI-Korpus erstellt und eine hohe Inter-Annotator-Übereinstimmung erreicht. Sie haben verschiedene Methoden eingesetzt, darunter Kontext-Retrieval, Single-Span-Modelle und Multi-Span-Modelle. Die Ergebnisse zeigen, dass fine-tuned Modelle besser abschneiden als zero-shot Modelle, aber immer noch hinter der menschlichen Leistung zurückbleiben. Fehleranalysen zeigen, dass Modelle Schwierigkeiten haben, rhetorische Fragen zu erkennen und die richtige Antwort zuzuordnen. Das Dataset MeetingQA ist ein wichtiger Schritt in der Entwicklung von QA-Systemen für Meetings und zeigt, dass es noch viel zu tun gibt, um die Herausforderungen dieser Domäne zu meistern.</sample>
    <sample id="151">Hallo alle, mein Name ist Ying und mein Kollege Zhiyang und ich werden unsere Forschung über MultiInstruct vorstellen, die Multi-modales Null-Schen-Lernen durch Anweisungsabstimmung verbessert. Mit den Fortschritten in großen Sprachmodellen haben viele Arbeiten begonnen, neue Lernparadigmen zu erforschen, um vorab trainierte Sprachmodelle für verschiedene nachgelagerte Aufgaben auf eine parameter- und dateneffiziente Weise wiederzuverwenden. In letzter Zeit haben viele Studien gezeigt, dass Anweisungsabstimmung großen Sprachmodellen ermöglicht, auf unsichtbare Aufgaben auf Null-Schen-Weise zu reagieren, indem sie natürlichen Anweisungen folgen. Die meisten früheren Arbeiten zur Anweisungsabstimmung konzentrierten sich jedoch auf die Verbesserung der Null-Schen-Leistung bei Sprach-only-Aufgaben, während Computer Vision und multi-modale Aufgaben ausgelassen wurden. Daher möchten wir in dieser Arbeit untersuchen, ob die Anweisungsabstimmung eines multi-modalen vorab trainierten Modells tatsächlich die Generalisierung auf unsichtbare multi-modale Aufgaben verbessern kann. Zusätzlich entdeckten wir zum Zeitpunkt unserer Forschung einen erheblichen Unterschied in der Verfügbarkeit von Anweisungsdaten zwischen NLP und multi-modal. Es gibt mehr als 1600 Sprach-only-Anweisungsaufgaben. Es gibt jedoch keine groß angelegte öffentlich zugängliche multi-modale Anweisungsaufgabe. Daher motiviert uns dies, einen multi-modalen Anweisungsabstimmungsdatensatz zu erstellen. Hier stellen wir MultiInstruct vor, den ersten multi-modalen Anweisungsabstimmungs-Benchmark-Datensatz, der aus 62 verschiedenen multi-modalen Aufgaben besteht, die 10 breite Kategorien abdecken. Diese Aufgaben stammen aus 21 vorhandenen Open-Source-Datensätzen und jede Aufgabe ist mit fünf von Experten geschriebenen Anweisungen ausgestattet. Um die multi-modale Anweisungsabstimmung auf unserem vorgeschlagenen Datensatz zu untersuchen, nehmen wir OFA, ein einheitliches multi-modales vorab trainiertes Modell, als unser Basismodell. OFA verwendet ein einheitliches Vokabular für Sprache, Bild-Token und die Koordinaten eines Bounding-Box. Hier zeigen wir einige Beispielinstanzen aus unserem MultiInstruct-Datensatz, um die Verarbeitung verschiedener Eingabe- und Ausgabedatentypen zu vereinheitlichen. Wir folgen der Methode von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-zu-Sequenz-Format. In dem die Texteingabe, Bilder, Anweisungen und Bounding-Boxen im gleichen Token-Raum dargestellt werden. Nun werde ich über die multi-modale Anweisungsabstimmung sprechen. Für den Trainingsdatensatz verwenden wir 53 Aufgaben aus 9 Gruppen für das Training und wir nehmen 10.000 Instanzen pro Aufgabe. Für die Tests behalten wir die gesamte Gruppe der Alltagsvernunft für das Testen und wir wählen zusätzliche 5 Aufgaben aus den Gruppen VQ und Verschiedenes. Wir verwenden alle Instanzen im Testsatz für jede Aufgabe. Zusätzlich nehmen wir zufällig 20 Aufgaben aus dem Testsatz der natürlichen Anweisungen als unsichtbare Aufgabe für NLP. Wir verwenden das vorab trainierte OFA-Großmodell als Basismodell. Während des Trainings mischen wir alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einer ihrer fünf Anweisungsvorlagen kombiniert. Während des Tests für jede Aufgabe führen wir insgesamt 5 Experimente durch, indem wir das Modell mit einer der fünf Anweisungen bewerten. In jedem Experiment berichten wir die minimale und maximale Leistung und die Standardabweichung der Leistung über alle 5 Experimente. Wenn es sich um eine multi-modale Klassifizierungsaufgabe handelt, berichten wir die Genauigkeit. Wenn es sich um eine multi-modale Generierungsaufgabe handelt, berichten wir Rouge-L. Für NLP-Aufgaben berichten wir ebenfalls Rouge-L. Wir führen auch eine zusätzliche Bewertungsmetrik namens Sensitivität ein. Diese misst die Fähigkeit des Modells, konsistent dieselben Ausgaben für dieselbe Aufgabe zu produzieren, unabhängig von der leichten Variation in der Formulierung der Anweisung. Hier ist unser Hauptergebnis. Wie wir sehen können, kann die Anweisungsabstimmung die Leistung von OFA auf gesehenen multi-modalen Aufgaben erheblich verbessern. Auch das Transfer-Learning von natürlichen Anweisungsdatensätzen kann der Anweisungsabstimmung zugutekommen. Hier können wir sehen, dass das Modell mit zunehmender Anzahl von Aufgaben eine bessere Leistung und gleichzeitig eine geringere Sensitivität erreicht. Wir haben auch ein Experiment durchgeführt. Wir verwenden eine Anweisung gegenüber 5 Anweisungen. Wie wir sehen können, kann die Verwendung von mehr Anweisungen die Gesamtleistung des Modells verbessern und seine Sensitivität erheblich reduzieren. Dies zeigt die Wirkung verschiedener Feinabstimmungsstrategien auf die Sensitivität des Modells. Wie wir sehen können, kann das Modell durch Transfer-Learning von natürlichen Anweisungsdatensätzen eine viel bessere Sensitivität im Vergleich zum ursprünglichen OFA-Modell erreichen. Auch können wir sehen, dass das Transfer-Learning von natürlichen Anweisungsdatensätzen OFA helfen kann, eine viel bessere Leistung auf dem natürlichen Anweisungsdatensatz zu erlangen. Insgesamt schlagen wir den ersten groß angelegten multi-modalen Anweisungsabstimmungsdatensatz mit signifikant verbesserten Fähigkeiten von OFA vor und wir erforschen verschiedene Transfer-Learning-Techniken und zeigen ihre Vorteile. Wir entwerfen eine neue Metrik namens Sensitivität. Noch eine Sache, wir sammeln einen viel größeren multi-modalen Anweisungsabstimmungsdatensatz mit rund 150 zusätzlichen Aufgaben zur visuellen Sprache und wir werden sie veröffentlichen. Dies ist ein QR-Code für unsere Daten und Modelle. Vielen Dank.</sample>
    <sample id="152">Frederick Riemenschneider präsentiert in seinem Vortrag "Exploring Large Language Models for Classical Philology" die neuesten Fortschritte in der Anwendung von Sprachmodellen (NLP) auf die klassische Philologie, insbesondere für Altgriechisch und Latein. Er betont, dass trotz der Entwicklung von Modellen wie Latin BERT (2020) und Ancient Greek BERT (2021/2022) noch viel Raum für Verbesserungen besteht. Die bestehenden Modelle sind meist monolingual und encoder-only, was ihre Fähigkeiten einschränkt. Daher hat das Team neue Modelle entwickelt: GreBERTa und GreTa für Altgriechisch, sowie PhilBERTa und PhilTa für eine mehrsprachige Verarbeitung von Altgriechisch, Latein und Englisch.

Ein zentraler Schritt war die Erstellung hochwertiger Trainingsdaten. Für Altgriechisch wurde das Internet Archive genutzt, indem falsch transkribierte griechische Wörter wie "γάρ" als "yap" identifiziert wurden, um griechische Texte zu erkennen. Diese wurden dann mit korrekten OCR-Einstellungen erneut gescannt. Für Latein und Englisch wurden zusätzliche Ressourcen wie das Corpus Corporum und antike englische Texte verwendet.

Die Modelle wurden in drei Aufgaben getestet: Teil-des-Sprechs-Kennzeichnung, Abhängigkeitsanalyse und Lemmatisierung. Die Ergebnisse zeigen, dass die neuen Modelle den aktuellen Stand der Technik deutlich übertreffen, insbesondere bei der Lemmatisierung, wo sie um 5 Prozentpunkte besser abschneiden. Interessanterweise zeigt der Encoder des T5-Modells erst nach mehr Training ähnliche Leistungen wie ein spezialisierter Encoder-only-Modell.

Die mehrsprachigen Modelle PhilBERTa und PhilTa schneiden in semantischen und weltwissenbezogenen Aufgaben ähnlich gut ab wie die monolingualen Modelle, was darauf hindeutet, dass die Mehrsprachigkeit allein nicht unbedingt einen Vorteil bietet.

Zusammenfassend präsentiert Riemenschneider leistungsstarke, neu entwickelte Modelle, die sowohl für monolingual als auch multilingual Anwendungen geeignet sind und eine hochwertige Trainingsdatenbank für Altgriechisch bereitstellen. Die Arbeit zeigt, dass NLP-Modelle das Potenzial haben, die klassische Philologie erheblich zu unterstützen.</sample>
    <sample id="153">In this research, we address the issue of ambiguities in text-to-image generative models, which hinder the generation of images that accurately reflect user intentions. Our work focuses on identifying and resolving these ambiguities to improve model fidelity. We created a benchmark dataset, modified from the existing LAVA corpus, to cover various types of ambiguities. To disambiguate prompts, we developed a framework where a language model either generates clarifying questions or alternative visual interpretations based on the ambiguous input. Users then provide answers, which are concatenated with the original prompt to form a disambiguated version. This disambiguated prompt is then used to generate images via a text-to-image model. To evaluate the faithfulness of these images to user intentions, we propose an automatic evaluation framework using a Visual Question Answering (VQA) model. The VQA model assesses whether the generated image aligns with the user's intended question. Our findings indicate that our disambiguation framework effectively resolves ambiguities, leading to more faithful image generation. Additionally, our automatic evaluation framework aligns well with human evaluations, making it a reliable tool for assessing text-to-image models. This work aims to enhance the accuracy and reliability of text-to-image generative models by addressing and evaluating ambiguities in user prompts.</sample>
    <sample id="154">Die Autoren gehören der Universität von Trento an.</sample>
    <sample id="155">The referent is **Javad Hosseini**, who co-authored the work on "Resolving Indirect Referring Expressions for Entity Selection" and introduced the AltEntities Corpus.</sample>
    <sample id="157">The paper "Dialogue Summarization with Static-Dynamic Structure Fusion Graph" by Shen Gao et al. addresses the challenge of distilling key information from multi-participant dialogues into concise summaries. Existing methods rely on pre-computed static graph structures, which are prone to errors from external linguistic tools and fail to adapt dynamically to the dialogue context. The proposed SDDS model overcomes these limitations by integrating both static and dynamic graph structures. 

The model first encodes utterances into vector representations using an Utterance Encoder. It then constructs a static graph using heuristic methods like Discourse Parsing Graph, Key Co-occurrence (KeyCo-occ), speaker relationship modeling, and utterance position graph. These static graphs capture dialogue structure information. A Static-Dynamic Graph module combines these static graphs and uses a dynamic graph module to learn semantic relationships between utterances based on their vector representations. A pre-trained language model serves as the Summary Generator, fusing the static and dynamic graph representations into a final summary.

The dynamic graph module employs a multi-head attention model to establish connections between utterances without relying on pre-computed methods. A fusion method integrates the static and dynamic graph representations into a unified graph, Gᵘ. Finally, a graph attention layer, built on top of self-attention, incorporates the graph representation into the summary generation process. This approach enhances the model's ability to capture the nuances of dialogue structure and generate accurate summaries. The code and data are available on GitHub for further exploration.</sample>
    <sample id="158">In this talk, Qipeng Guo from AWS introduces the work "Dual Cache for Long Document Neural Coreference Resolution," focusing on the challenge of coreference resolution in long documents. Coreference resolution aims to identify and link mentions of entities across a text to their corresponding entities. Traditional methods face quadratic complexity in both computation and memory, while cache-based methods reduce this to linear complexity. However, in long documents with frequent topic shifts, the Least Recently Used (LRU) eviction policy in single-cache methods leads to high cache misses, especially for high-frequency entities.

To address this, the proposed Dual Cache method introduces a local cache and a global cache. The local cache stores entities with an LRU eviction policy, suitable for entities mentioned in a localized context. The global cache, using the Least Frequently Used (LFU) eviction policy, stores high-frequency entities mentioned across the document. The model processes the document sequentially, classifying new mentions and updating the caches accordingly. When caches are full, eviction policies are triggered.

Evaluations on four public benchmarks, including LitBank, OntoNotes, and WikiCoref, demonstrate that Dual Cache outperforms single-cache methods, even with unbounded memory. Notably, Dual Cache significantly reduces cache misses and maintains a high performance/cost ratio. In a case study with a 30,000-word book, the performance gap between Dual Cache and baselines was substantial. Overall, Dual Cache effectively balances efficiency and performance, making it a cost-effective solution for coreference resolution in long documents.</sample>
    <sample id="159">Hallo, alle zusammen. Ich bin Koustav Sinha und freue mich, euch zu unserem Vortrag über unseren ACL 2023-Artikel willkommen zu heißen. Die Akzeptanzurteile von Sprachmodellen sind nicht immer kontextrobust. Dies ist eine gemeinsame Arbeit mit John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy und Adina Williams. In dieser Arbeit nehmen wir die Minimalpaar-Paradigmen erneut unter die Lupe. Das Minimalpaar-Paradigma bewertet Sprachmodelle hauptsächlich anhand von Akzeptanzurteilen, die auch Grammatikalität wie BLiMP, SyntaxGym oder Akzeptanz in Bezug auf Stereotypen wie CrowS-Paare umfassen können. Und dabei bewertet das typische Minimalpaar-Paradigma die Sprachmodelle normalerweise so, dass es eine akzeptable oder grammatikalische und eine akzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable oder ungrammatische oder inakzeptable</sample>
    <sample id="160">Im ersten Schritt der Methode werden die Input-Token einem **unordered multiset (Mehrfachmenge)** von Output-Token zugeordnet.</sample>
    <sample id="161">Das CoScript-Dataset enthält insgesamt 55.000 spezifische Ziele mit Skripten.</sample>
    <sample id="163">Die beste Ausrichtungsmethode für DEPLAIN ist **MASSalign**.</sample>
    <sample id="164">Der Vorteil von schwach überwachtem Lernen (Weakly Supervised Learning, WSL) liegt in der Reduzierung der manuellen Annotationskosten. Anstatt teure und zeitaufwendige manuelle Annotationen durch Experten durchzuführen, werden in WSL kostengünstigere und schnellere schwache Annotationen verwendet, die beispielsweise durch einfache Heuristiken, Wissensdatenbanken oder Crowdsourcing generiert werden. Diese schwache Annotationen sind jedoch oft fehlerhaft und können zu Fehlern im Training führen. WSL-Methoden zielen darauf ab, diese Fehler zu minimieren und dennoch robuste Modelle zu trainieren, die gut generalisieren können.</sample>
    <sample id="165">Wenting Zhao präsentiert die Arbeit "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations" und erklärt das Konzept des abduktiven Schlussfolgerns anhand eines Beispiels: Emily war im Stau, aber sie schaffte es zu ihrem Flug. Mögliche Erklärungen sind, dass ihr Flug verspätet war oder pünktlich. Die Aufgabe besteht darin, eine plausible Erklärung zu finden, die den Zusammenhang zwischen Kontext und Ergebnis erklärt. In diesem Fall ist "Ihr Flug war verspätet" eine plausible Erklärung.

Zhao stellt fest, dass aktuelle Ansätze zur abduktivem Schlussfolgerung auf überwachten Methoden basieren, die jedoch auf annotierten, möglicherweise subjektiven und fehlerhaften Erklärungen angewiesen sind. Die Arbeit stellt die Frage, ob abduktives Schlussfolgern auch ohne Aufsicht gelernt werden kann. Die Antwort lautet: Ja.

Die Methode LiPoR (Likelihood Learning with Posterior Regularization) behandelt Erklärungen als latente Variablen und maximiert die marginale Wahrscheinlichkeit des Ergebnisses gegeben den Kontext, ohne zu wissen, welche Erklärungen plausibel sind. Um plausibelere Erklärungen zu bevorzugen, wird ein Regularizer verwendet, der die gegenseitige Exklusivität der Erklärungen ausnutzt. Der Regularizer Omega maximiert die Wahrscheinlichkeit der plausibelsten Erklärungen und minimiert die Entropie der Wahrscheinlichkeitsverteilung der Erklärungen.

Die Ergebnisse auf dem AlphaNLI-Datensatz zeigen, dass LiPoR alle anderen Modelle, einschließlich GPT-3, übertrifft und die Genauigkeit um mehr als 4 Punkte verbessert.</sample>
    <sample id="166">**Abstract:**  
We introduce a Neural Divide-and-Conquer Reasoning Framework (NDCR) for image retrieval from linguistically complex text, addressing challenges posed by highly similar images and lengthy descriptions. Inspired by the Divide-and-Conquer strategy and Dual-Process Theory, our framework integrates analogical reasoning (System 1) and logical reasoning (System 2). The Proposition Generator decomposes complex text into simple propositions, while the Visual-Linguistic Interactor performs visual-proposition interaction. The Neural-Symbolic Reasoner integrates reasoning states and results to produce the final solution. Our method outperforms baselines, as demonstrated by experimental results and ablation studies. Case studies reveal interpretable inference states and results, showcasing the framework’s interoperability. We suggest that neural symbolic reasoning, Divide-and-Conquer, and Dual-Process Theory integration can enhance compositional reasoning and planning in large language models, effectively solving complex problems.</sample>
    <sample id="167">Die Dokumente in DEPLAIN-web wurden sowohl manuell als auch mit automatischen Alignmentmethoden ausgerichtet. Insgesamt wurden 750 Dokumente manuell und teilweise automatisch auf 30.450 parallel ausgerichtete Satzpaare erweitert.</sample>
    <sample id="168">Der CoNLL++-Datensatz wurde durch die Annotation von Reuters-Nachrichten aus dem Jahr 2020 mit den gleichen CoNLL-2003-Annotation-Richtlinien erstellt.</sample>
    <sample id="169">In the paper "Prompting PaLM for Translation: Assessing Strategies and Performance," David Vilar and colleagues explore the use of PaLM, a 540 billion-parameter language model, for machine translation tasks. PaLM, trained on 780 billion tokens, was previously state-of-the-art in hundreds of NLP tasks. The study systematically evaluates PaLM's translation capabilities using state-of-the-art neural MT metrics and human evaluations. The research highlights the significant impact of prompting strategies on PaLM's performance, with experiments showing that different prompts can lead to BLEURT point differences, in extreme cases up to 40 points.

The team found that while the form of the prompt has less influence in longer prompting sequences (e.g., five-shot), the quality of the examples provided is crucial, especially for zero and one-shot prompts. They recommend selecting examples from high-quality translations, such as those from curated development datasets, which outperform those from noisy training data. Despite PaLM's strong performance, specialized state-of-the-art systems still have a substantial advantage. However, PaLM's translations are comparable to commercial systems like Google Translate, particularly in fluency. Human evaluations using the MQM framework reveal that PaLM's fluency is on par with state-of-the-art systems, but it struggles with accuracy, often omitting parts of the source sentence to produce smoother translations. Overall, the study emphasizes the importance of prompt quality and strategy in leveraging large language models for translation tasks.</sample>
    <sample id="170">Hallo alle zusammen, mein Name ist Yusen Zhang von der Penn State University. Heute werde ich unsere Arbeit „XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations“ vorstellen. Semantic Parsing ist also die Aufgabe, semantische Repräsentationen von Benutzeranfragen wie SQL und Lambda-Kalkül zu erstellen. Und Cross-Lingual Semantic Parsing ist die Aufgabe, Anfragen in mehreren natürlichen Sprachen in mehrere Bedeutungrepräsentationen zu übersetzen. Wie in dieser Abbildung gezeigt, müssen wir die Anfrage in mehreren natürlichen Sprachen mithilfe von neuronalen Modellen in SQL, Lambda oder FunQL usw. übersetzen. Bestehende Modelle für Cross-Lingual Semantic Parsing wurden separat vorgeschlagen und auf Datensätzen mit begrenzten Aufgaben und Anwendungen bewertet. Beispielsweise gibt es viel Abdeckung für bestimmte natürliche Sprachen. Chinesisch fehlt jedoch, und es gibt eine unzureichende Abdeckung für bestimmte Bedeutungrepräsentationen. Das Lambda-Kalkül fehlt, oder sie werden nur auf bestimmten neuronalen Modellen bewertet. Beispielsweise wird nur ein einzelnes Modell zur Bewertung verwendet. Zu diesem Zweck schlagen wir XSemPLR vor. Wir stellen einen einheitlichen Datensatz XSemPLR für Cross-Lingual Semantic Parsing in mehreren natürlichen Sprachen und Bedeutungrepräsentationen bereit. Er enthält 9 Datensätze in verschiedenen Bereichen, 5 semantische Parsing-Aufgaben, 8 Bedeutungrepräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien. Um unseren Benchmark besser bewerten zu können, betrachten wir die sechs Einstellungen für Training und Bewertung. Die erste ist Translate-Test. Wir verwenden die Google Translate API, um die Quelle in die Zielsprache zu übersetzen, und verwenden dann ein einsprachiges Modell für Training und Bewertung. Und zum Beispiel trainieren wir das englische Modell mit einer englischen Anfrage und verwenden während der Inferenz die deutsche Anfrage, die wir mithilfe der API ins Englische übersetzen, und verwenden dann das trainierte Modell, um die SQL vorherzusagen. Wir testen auch das einsprachige Modell. In dieser Einstellung ist die Quellsprache dieselbe wie die Zielsprache, zum Beispiel Deutsch zu Deutsch oder Englisch zu Englisch. Wir testen auch die einsprachige Few-shot-Einstellung, indem wir einsprachige Modelle mit nur 10 % der Trainingsdaten trainieren. Und wir testen das mehrsprachige Modell, das wir für alle Sprachen trainieren. Zum Beispiel bringen wir die deutschen, englischen und chinesischen Anfragen zusammen, um ein mehrsprachiges Modell zu trainieren. Und während der Inferenz können wir dieses Modell verwenden, um deutsche oder chinesische Anfragen usw. zu übersetzen. Wir berücksichtigen auch den Cross-lingual Zero-shot- und Few-shot-Transfer. Wir trainieren auf einer Quellsprache und übertragen sie auf eine andere Sprache. Während des Trainings trainieren wir also auf englischen Anfragen oder der Kombination aus englischen und deutschen Few-shot-Anfragen, um ein mehrsprachiges Modell zu trainieren, das die SQL-Ausgabe vorhersagen kann. Und wir finden auch viele interessante Ergebnisse. Was die Analyse einsprachiger Modelle betrifft, bewerten wir zwei Gruppen von Modellen, einschließlich Encoder-PTR, was für mehrsprachige vorvertrainierte Encoder mit zeigerbasierten Decodern steht, wie XLM-R + PTR und mBERT + PTR. Und wir bewerten auch Encoder-Decoder-Modelle, was mehrsprachige vorvertrainierte Encoder-Decoder-Modelle wie mBART und mT5 ist. Wir fanden heraus, dass Encoder-Decoder die beste Leistung auf allen neun Datensätzen erzielen. Und wir bewerten mT5 und XLM-R + PTR in der mehrsprachigen Einstellung. Wir fanden heraus, dass Encoder-Decoder oder Encoder-PTR durch Training in einer Mischung aus verschiedenen Sprachen verbessert werden können. Wir fanden heraus, dass dies daran liegt, dass die meisten großen natürlichen Sprachen eine Leistungssteigerung erzielen können, mit Ausnahme von Englisch, dessen Leistung in sieben Datensätzen sinkt und nur in drei Datensätzen steigt. Ich denke, dies ist als der „Fluch der Mehrsprachigkeit“ bekannt. Wir vergleichen auch die Cross-Language-Leistungslücke. In dieser Abbildung ist die blaue Linie der Cross-lingual Few-shot-Transfer. Die orange Linie ist der Cross-lingual Zero-shot-Transfer. Während die grüne Linie die einsprachige Einstellung ist. Wir fanden heraus, dass die Null-shot-Einstellung, der Cross-lingual Transfer-Leistungslücke signifikant ist, und dann, wenn wir die blauen und orangen Linien vergleichen, fanden wir heraus, dass mit der Few-shot-Einstellung die Transferlücke schnell verkürzt wird. Wir finden auch einige andere interessante Erkenntnisse. Zum Beispiel übertrifft Encoder-Decoder frühere Arbeiten oder erzielt vergleichbare Ergebnisse. Die Vorverknüpfung auf Englisch kann die Leistung von Few-shot auf Ziel-natürliche Sprachen erheblich steigern, und wir fanden heraus, dass mehrsprachige Sprachmodelle wie Codex und BLOOM für Cross-Lingual Semantic Parsing-Aufgaben immer noch unzureichend sind. Zusammenfassend haben wir XSemPLR erstellt, einen einheitlichen Benchmark für Cross-Lingual Semantic Parsing mit mehreren natürlichen Sprachen und Bedeutungrepräsentationen. Wir führen eine umfassende Benchmark-Studie zu drei repräsentativen Arten von mehrsprachigen Sprachmodellen durch. Und unsere Ergebnisse zeigen viele interessante Erkenntnisse. Und so weiter. Und besuchen Sie gerne unseren Artikel und Code. Vielen Dank fürs Zuhören.</sample>
    <sample id="171">Existing works on watermarking for embedding as services can be broadly classified into four categories. However, these methods either:

1. **Are not applicable to embedding as services**: They may work for other types of data but not specifically for embeddings used in NLP tasks.
2. **Lack transferability**: The watermarking method may not be effective when the attacker tries to extract or replicate the model, making it difficult to trace back to the original provider.

These limitations highlight the need for a more robust and applicable solution, which is addressed in the proposed paper with the introduction of the **Embedding Marker**.</sample>
    <sample id="172">Nein, mehrsprachige LLMs wie Codex und BLOOM sind für Cross-Lingual Semantic Parsing (CLSP) nicht ausreichend.</sample>
    <sample id="174">The paper "ArgAnalysis35K: A large-scale dataset for Argument Quality Analysis" introduces a unique dataset with 35,000 argument-analysis pairs, addressing limitations of existing datasets. Unlike crowdsourced datasets with limited diversity and depth, ArgAnalysis35K sources arguments from high-quality tournaments, expert debaters, and diverse themes (24 in total), capturing a broader range of motions. It introduces the concept of "analysis," a combination of claims, premises, and reasoning, which provides deeper insights into arguments. The dataset also employs instance-based annotator reliability, allowing for more nuanced use of human annotations by accounting for biases on a per-argument basis. Additionally, it includes a relevance model to assess how arguments relate to various themes, enhancing their applicability. This dataset stands out for its diversity, quality, and innovative features, making it a valuable resource for NLP research in argument quality analysis.</sample>
    <sample id="175">Die Methode geht mit der Mehrdeutigkeit der Permutationen um, indem sie die Alignment-Informationen während des Trainings induziert. Da die Alignment-Informationen nicht im Trainingsdatensatz gegeben sind, stellt dies eine Herausforderung dar. Die Methode löst dies, indem sie die Alignment-Informationen als Teil des Trainings induziert, was es ermöglicht, die linguistisch korrekte Permutation zu finden, auch wenn es mehrere mögliche Permutationen gibt, die mit den Daten konsistent sind.</sample>
    <sample id="176">Die Fairness eines nachgeschalteten NLP-Modells wird in Bezug auf politische Vorurteile definiert, indem untersucht wird, ob das Modell unterschiedliche politische Meinungen und soziale Gruppen gleichermaßen und unvoreingenommen behandelt. Konkret bedeutet dies, dass ein faires Modell keine systematischen Fehler oder Verzerrungen bei der Bewertung oder Klassifizierung von Inhalten basierend auf der politischen Ausrichtung oder sozialen Zugehörigkeit der beteiligten Gruppen aufweisen sollte. In der Studie wird dies anhand von Beispielen wie der Erkennung von Hassrede und Falschnachrichten demonstriert, wo Modelle mit unterschiedlichen politischen Ausrichtungen unterschiedliche Leistungen bei der Erkennung von Hassrede gegen verschiedene Gruppen und Falschnachrichten aus unterschiedlichen politischen Lagern zeigen. Dies zeigt, dass die politische Ausrichtung des Modells direkt zu Fairness-Problemen in NLP-Anwendungen führen kann.</sample>
    <sample id="177">Der Referent heißt Yanis Labrak.</sample>
    <sample id="178">Koustav Sinha</sample>
    <sample id="179">**Abstract:**  
Melanie Sclar introduces "SymbolicToM," an inference-time method to enhance Theory of Mind (ToM) reasoning in Large Language Models (LLMs). ToM, traditionally measured through false-belief tasks, involves understanding characters' mental states and beliefs. SymbolicToM uses graphical representations (e.g., BBob, BBob_Alice) to model these states, leveraging off-the-shelf Natural Language Inference (NLI) and Open Information Extraction (OpenIE) models. This approach avoids overfitting and provides interpretable reasoning. Experiments show significant performance gains across LLMs, including GPT-3, Macaw, and Flan-T5-XXL, on the ToMi dataset and out-of-domain tasks. Robustness is tested with datasets like D₁ (story structure generalization) and ParaphrasedToM (linguistic diversity), where SymbolicToM outperforms supervised models, achieving a 42-point accuracy boost in D₁. SymbolicToM improves interpretability and generalizability, making it a plug-and-play solution for enhancing ToM in LLMs. For details, refer to the paper.</sample>
    <sample id="180">The presenter's name is Myra.</sample>
    <sample id="181">**Abstract:**  
This paper addresses constrained language planning, where large language models (LLMs) generate step-by-step scripts for specific goals with multi-faceted constraints, such as "make a chocolate cake." While LLMs excel at decomposing abstract goals into steps, their performance on specific goals remains unsatisfactory. We evaluate LLMs like InstructGPT, finding that while semantic completeness is acceptable, faithfulness to constraints is often compromised. To improve generation quality, we propose an over-generate-then-filter method: LLMs over-generate scripts for specific goals, and a filter model selects faithful ones based on semantic similarity and constraint keywords. This approach significantly enhances planning quality. Additionally, we distill a constrained language planning dataset, CoScript, by leveraging LLMs to generate 55,000 specific goals with scripts. Crowd-sourced workers ensure dataset quality. CoScript demonstrates high pluralism in goals and enables smaller, specialized models like T5 to surpass large LLMs when fine-tuned. Our work establishes constrained language planning as a research area, provides CoScript as a valuable dataset, and highlights the potential of smaller models for this task.</sample>
    <sample id="182">Im Kontext der Arbeit bedeutet **Tropikalismus** die Darstellung von Frauen aus bestimmten kulturellen oder ethnischen Gruppen, insbesondere von Latina-Frauen, durch Stereotype wie "exotisch" oder "tropisch". Diese Zuschreibungen reduzieren sie auf ihre kulturelle Herkunft und betonen ihre Andersartigkeit im Vergleich zur weißen Norm, was zu essentialisierenden und diskriminierenden Narrativen beiträgt.</sample>
    <sample id="183">The authors created the human-written descriptions of the target groups by drawing inspiration from a study where participants were given prompts similar to those used for the LLMs. These prompts asked individuals to imagine and describe themselves from the perspective of a specific demographic identity, such as "Imagine you are an Asian woman. Describe yourself." This method allowed for a direct comparison between the generated personas from the LLMs and the human-written responses, providing insights into the stereotypes and patterns present in both.</sample>
    <sample id="184">In the work titled "When Does Translation Require Context? A Data-driven, Multilingual Exploration," the researchers used **Pointwise Contextual Information Measure (P-CXMI)** to measure how much context is required for translating specific words or sentences. P-CXMI quantifies the information gained from providing context (C) to the model when translating from the source (X) to the target (Y). This measure was extended to operate at the sentence or word level, allowing the identification of words or phrases that depend heavily on context for accurate translation. The analysis was conducted on TED talk transcripts translated into 14 languages, and the findings were used to design a benchmark for document-level translation evaluation.</sample>
    <sample id="185">DrBERT und ChuBERT sind beide auf Französisch spezialisierte Modelle für biomedizinische und klinische Anwendungen, unterscheiden sich jedoch in ihren Trainingsdaten:

- **DrBERT** basiert auf dem **NACHOS-Datensatz**, der aus medizinisch relevanten Daten aus dem Web stammt.
- **ChuBERT** basiert auf **anonymisierten klinischen Daten** aus dem Datenbestand des Universitätskrankenhauses von Nantes.

Zusammenfassend: DrBERT nutzt allgemeinere medizinische Webdaten, während ChuBERT spezifischere klinische Daten verwendet.</sample>
    <sample id="187">Zwei Autoren sind an der Arbeit beteiligt: Ying und Zhiyang.</sample>
    <sample id="188">Iterative Transfer Learning is a method where a model is initially trained on a source task (in this case, closely related tasks like topic-independent dissonance stance classification and binary classification of expansion and comparison classes from PDTB) and then iteratively fine-tuned on the target task (dissonance detection in language). The process involves:

1. **Initial Transfer**: The model's weights are transferred from the source tasks to the target task, providing a starting point for the target task.
2. **Iterative Fine-Tuning**: The model is fine-tuned on the target task multiple times, with each iteration using the latest data collected from active learning. The order of fine-tuning (e.g., fine-tuning on CE tasks first, followed by fine-tuning on debate) can significantly impact performance.

In the context of the paper, iterative fine-tuning on the CE tasks followed by further fine-tuning on the debate task yielded the best zero-shot performance on the dissonance detection task. This approach leverages the relatedness of the source tasks to the target task to improve the model's ability to detect dissonance in language, even with limited data.</sample>
    <sample id="189">Das Ziel des **AltEntities Corpus** ist es, das Verständnis von **indirekten Verweisen** in der **Auswahl von Entitäten** durch **Sprachmodelle** zu verbessern. Der Datensatz soll helfen, die Fähigkeit von Modellen zu bewerten, **natürliche Sprache** zu interpretieren und **indirekte Fragen** wie "Welchen Song meinst du?" zu verstehen, indem sie zwischen ähnlichen Entitäten (z. B. Songs, Büchern oder Rezepten) unterscheiden können.</sample>
    <sample id="190">Ein Angreifer kann Modellparameter über einen Embedding-as-a-Service (EaaS) extrahieren, indem er die eingebetteten Repräsentationen (Embeddings) analysiert, die der Dienst bereitstellt. Hier sind die Schritte, die ein Angreifer typischerweise durchführt:

1. **Interaktion mit dem EaaS**: Der Angreifer sendet viele Eingabedaten (z. B. Texte) an den EaaS und erhält die entsprechenden Embeddings.

2. **Analyse der Embeddings**: Durch die Analyse der erhaltenen Embeddings kann der Angreifer Muster und Strukturen erkennen, die mit dem zugrunde liegenden Modell zusammenhängen. Dies kann durch statistische Methoden, Clustering oder andere Techniken geschehen.

3. **Modellextrahierung**: Basierend auf den analysierten Embeddings kann der Angreifer ein ähnliches Modell trainieren, das die gleichen oder ähnliche Eigenschaften wie das ursprüngliche Modell aufweist. Dies wird oft als "Adversarial Attack" oder "Model Stealing" bezeichnet.

4. **Verwendung des extrahierten Modells**: Das extrahierte Modell kann dann vom Angreifer für eigene Zwecke verwendet werden, was zu einem Verstoß gegen das Urheberrecht oder andere rechtliche Bestimmungen führen kann.

Um dies zu verhindern, wird in dem vorgeschlagenen Papier **Embedding Marker** eine Methode eingeführt, die eine **Wasserzeichen-basierte Lösung** bietet. Das Wasserzeichen wird in die Embeddings eingebettet, sodass es für den Angreifer schwierig wird, das Modell zu extrahieren, ohne das Wasserzeichen zu erkennen. Die Methode basiert auf der folgenden Idee:

- **Trigger-Set**: Ein Satz von Wörtern mit mittlerer Häufigkeit wird ausgewählt.
- **Wasserzeichen-Injektion**: Wenn ein Benutzer eine Anfrage an den EaaS sendet, wird basierend auf der Anzahl der Trigger-Wörter im Text ein Wasserzeichen in die Ausgabe-Embedding eingefügt.
- **Urheberrechtsprüfung**: Der EaaS kann überprüfen, ob ein fremder Dienst das Wasserzeichen enthält, indem er spezielle Datensätze (Backdoor und Benign) verwendet und die Ähnlichkeit der Embeddings analysiert.

Durch diese Methode wird es für Angreifer schwieriger, das Modell zu extrahieren, da das Wasserzeichen in den Embeddings vorhanden ist und die Analyse erschwert.</sample>
    <sample id="191">Drei Autoren sind an der Arbeit beteiligt: Sara Papi, Matteo Negri und Marco Turchi.</sample>
    <sample id="192">Die Präsentation von Yang Luo befasst sich mit der Entwicklung des Optimizers **CAME (Confidence-guided Adaptive Memory Efficient Optimization)**, der darauf abzielt, die Vorteile von adaptiven und speichereffizienten Optimierungsmethoden zu vereinen. Traditionelle adaptive Optimierer wie Adam benötigen viel Speicher für die Speicherung von Gradienteninformationen, während speichereffiziente Methoden wie Adafactor zwar Speicher sparen, aber oft zu langsamer Konvergenz führen. CAME adressiert diese Herausforderung, indem es eine adaptive, speicherarme Methode entwickelt, die sowohl schnelle Konvergenz als auch geringe Speicheranforderungen bietet.

Ein zentraler Aspekt von CAME ist die Reduzierung von Fehlern in den Updates, die bei speichereffizienten Optimierern wie Adafactor auftreten. CAME nutzt die Differenz zwischen historischem Momentum und aktuellen Updates, um eine adaptive Anpassung vorzunehmen. Dies führt zu einer stabileren und effizienteren Optimierung.

In Experimenten auf den Datensätzen BookCorpus und English Wikipedia zeigte CAME deutliche Verbesserungen gegenüber Adam und Adafactor. Insbesondere bei der BERT- und GPT-2-Schulung erhöhte CAME die Validierungsgenauigkeit um etwa 3,4 % im Vergleich zu Adafactor, bei gleicher Anzahl von Trainingsschritten. CAME reduziert zudem den Speicherbedarf erheblich, insbesondere bei großen Batches (8K bis 32K), und erzielt bessere Ergebnisse als Adam bei der Schulung von BERT-Large.

Die Ergebnisse zeigen, dass CAME sowohl in der Schulung als auch in der Endaufgabenleistung von BERT-basierten Modellen effizienter ist, während gleichzeitig der Speicherverbrauch reduziert wird. CAME ist somit ein vielversprechender Ansatz für die Optimierung großer Sprachmodelle, insbesondere bei großem Batch-Training.</sample>
    <sample id="193">The original dataset was created through a large-scale annotation process, but the number of annotators used is not explicitly mentioned in the provided content.</sample>
    <sample id="194">Die Autoren gehören der Carnegie Mellon University an.</sample>
    <sample id="195">The paper introduces "Reasoning over Hierarchical Question Decomposition Tree (RoHT)" for explainable question answering (XQA), aiming to answer complex questions and provide explanations for the answers. XQA methods can be divided into neuro-symbolic and decompose-based approaches, each with limitations. Neuro-symbolic methods rely on structured knowledge bases (KBs) but suffer from incomplete KBs, while decompose-based methods use free-text corpora, which are diverse and challenging. RoHT addresses these limitations by proposing a two-stage framework. First, it builds a Hierarchical Question Decomposition Tree (HQDT) to understand the structure of a complex question, where the root is the original question, non-root nodes are sub-questions, and leaf nodes are atomic questions. Second, it uses probabilistic reasoning over the HQDT to fuse knowledge from KBs and text corpora at different levels, considering the probability scores of string generation and answering. The reasoning process is recursive, involving a scheduler, executors, and an aggregator to select knowledge sources, retrieve answers, and aggregate results. RoHT is evaluated on KQA Pro and Musique datasets. On KQA Pro, RoHT outperforms existing methods by integrating answers from sub-questions of different levels and improves further with Wikipedia as a supplementary corpus. On Musique, RoHT-text improves F1 by 11.9% over SOTA, and RoHT-mix outperforms TransferNet when using both text and KB. Overall, RoHT demonstrates the effectiveness of hierarchical question decomposition and integrating knowledge from diverse sources for complex XQA tasks.</sample>
    <sample id="196">Das Beispiel mit dem Begrenzer auf der linken Seite ist: "I saw Bart and Lisa".</sample>
    <sample id="197">The state-of-the-art for dialogue systems, as highlighted in the context of ABC-Eval, involves evaluating conversational AI on multiple dimensions of chat quality to understand its strengths and weaknesses more precisely. Current practices often rely on human evaluation methods, such as Likert ratings or pairwise comparisons, which provide holistic assessments but can be subjective. ABC-Eval introduces a more reliable and detailed approach by explicitly annotating specific behaviors in chat models, such as relevance, contradictions, hallucinations, and empathy. This method reduces subjectivity and offers a higher-resolution evaluation of chat quality. Key findings from evaluations using state-of-the-art models show that ABC-Eval metrics are more reliable and predictive of overall conversation quality compared to existing methods. Challenges remain, such as common sense violations (20%) and irrelevant responses (15%), but ABC-Eval provides a meaningful step toward precise and reliable evaluation in the rapidly advancing field of conversational AI.</sample>
    <sample id="198">We need to evaluate models' acceptability judgments over the entire context window because modern large language models have significantly larger context windows, allowing them to process longer sequences. Current minimal pair paradigms (MPP) typically evaluate models with short, single-sentence inputs, which may not fully capture how models handle acceptability in longer contexts. By extending MPP to longer sequences, we can assess whether models' judgments remain robust across varying lengths and contexts, ensuring they generalize their abstract knowledge effectively.</sample>
    <sample id="199">Ja, das mehrsprachige Training hat in sieben von neun Datensätzen zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell geführt. Dies wird als "Curse of Multilinguality" bezeichnet.</sample>
    <sample id="200">No, the annotators do not know the entities in advance. They are shown background knowledge about the two entities (e.g., Google search links for songs, Wikipedia text for books and recipes, and images for recipes) and are asked to listen to or read about the entities before providing indirect referring expressions to select one of them.</sample>
    <sample id="201">Die verwendeten MT-Metriken für die Bewertung waren **BLEURT** und andere **state-of-the-art, neural MT metrics**.</sample>
    <sample id="202">Nein, die Regression bei der Generalisierung wirkt sich nicht spezifisch auf bestimmte NER-Typen aus. Die Studie zeigt, dass die Leistung aller untersuchten Modelle auf dem CoNLL++-Datensatz abnimmt, unabhängig vom NER-Typ. Die Hauptursache für diesen Rückgang ist der zeitliche Drift, nicht adaptive Überanpassung.</sample>
    <sample id="203">Positionalität ist für NLP wichtig, weil sie systematische Leistungsunterschiede zwischen Technologien und verschiedenen Bevölkerungsgruppen aufdeckt. Durch die Berücksichtigung der Positionalität von Forschern und Entwicklern können Design-Bias in Datensätzen und Modellen identifiziert und adressiert werden, die bestimmte Gruppen benachteiligen. Dies ist besonders relevant, da NLP-Aufgaben zunehmend subjektiv und sozial orientiert werden, und es wichtig ist, sicherzustellen, dass Technologien fair und inklusiv sind.</sample>
    <sample id="204">Die mehrsprachigen LLMs wie BLOOM wurden **nicht durch Adapter oder eine vollständige Feinabstimmung angepasst**, sondern blieben in ihrer ursprünglichen Form verwendet. Die Ergebnisse zeigten, dass sie für Cross-Lingual Semantic Parsing-Aufgaben **noch immer unzureichend** sind.</sample>
    <sample id="205">Shangbin's PhD research at the University of Washington investigates the propagation of political biases from pretraining data to language models (LMs) and their downstream applications. The study highlights that LMs, trained on large-scale web data, including political news from sources like the New York Times and The Guardian, exhibit varying political leanings. Preliminary results show that GPT-4 is the most liberal, while GPT and BART series lean more socially liberal than their variants.

The research also explores how political biases are influenced by training data. Controlled experiments involving further pretraining on partisan corpora (news and social media) demonstrate that LMs' ideological coordinates shift accordingly. For instance, RoBERTa trained on a left-leaning Reddit corpus showed a significant liberal shift. Additionally, the study found that LMs trained on post-2017 data exhibit a more polarized political leaning, reflecting societal polarization.

The impact of these biases on downstream tasks, such as hate speech and fake news detection, was evaluated. Left-leaning LMs were better at detecting hate speech targeting minority groups but worse at detecting hate speech against more powerful groups. Conversely, right-leaning LMs performed the opposite. Similar trends were observed in fake news detection, with LMs better detecting misinformation from opposing political viewpoints.

The research underscores the fairness issues arising from LMs' political biases, emphasizing the need to address these concerns. The dilemma lies in balancing bias mitigation with potential censorship, as determining neutrality in training data is challenging. The study calls for acknowledging and tackling these issues to ensure fair and unbiased NLP applications.</sample>
    <sample id="206">For transfer learning, the model used is a combination of two closely related tasks: **topic-independent dissonance stance classification (debate)** and **binary classification of expansion and comparison classes from the PDTB dataset (CE)**. The weights from these tasks are transferred to initialize the model, which significantly improves its zero-shot performance on the dissonance detection task. Iterative fine-tuning of the CE tasks followed by further fine-tuning on the debate task yields the best zero-shot performance, which is then used to cold-start the active learning process.</sample>
    <sample id="207">Die aktuellen Testsets, die zur Bewertung der PaLM-Fähigkeiten verwendet wurden, sind die WMT-Evaluationsdaten.</sample>
    <sample id="208">Die Autoren haben drei Empfehlungen für Model-Eigentümer vorgeschlagen.</sample>
    <sample id="209">Die vorgeschlagene Methode zeigt eine signifikante Verbesserung gegenüber der stärksten Baseline. Insbesondere wird berichtet, dass T5, ein kleinerer Modell, das auf dem CoScript-Datensatz trainiert wurde, in der Lage ist, qualitativ hochwertigere Skripte zu generieren als die meisten großen Sprachmodelle. Dies deutet darauf hin, dass die vorgeschlagene Methode den Planungsfähigkeiten kleinerer Modelle erheblich verbessert und sie in der Lage macht, die Leistung größerer Modelle zu übertreffen.</sample>
    <sample id="210">Der Referent heißt Shuheng.</sample>
    <sample id="211">Ja, die Ergebnisse und der Datensatz der Studie können als Benchmark für die automatische Textvereinfachung verwendet werden. Die Forscher haben gezeigt, dass die Feinabstimmung von Sprachmodellen wie long-mBART und mBART zu besseren Ergebnissen als die Basislinien führt. Die DEPLAIN-Korpusse, insbesondere die manuell und automatisch gestimmten Paare, bieten eine solide Grundlage für die Bewertung und Verbesserung von Methoden zur automatischen Textvereinfachung.</sample>
    <sample id="212">The paper does not explicitly mention experimenting with multiple smaller models. It focuses on fine-tuning the T5 model on the CoScript dataset and compares its performance to large language models. Therefore, the answer is: **1 smaller model (T5)**.</sample>
    <sample id="213">The base model used for the investigation of multimodal instruction tuning is OFA (Open Foundation for Audiovisual).</sample>
    <sample id="215">The talk by Adam Przepiórkowski focuses on the dependency structure of coordination, discussing different theoretical approaches and their implications. Asymmetric structures, such as those in Universal Dependencies and Mel'čuk's meaning text theory, treat the first conjunct as the head of the coordinate structure. In contrast, the Prague approach and conjunction-headed theories posit that the conjunction itself is the head. The Hudson's Word Grammar proposes a multi-headed approach, where all conjuncts are heads. The paper argues for symmetric structures of coordination, based on the principle of dependency length minimization. This principle suggests that shorter dependencies are preferred, and it explains why certain sentence structures are more acceptable despite violating other grammatical principles. For example, moving a long direct object away from a verb can be justified by minimizing the total dependency length. The study also analyzes statistics from the Penn Treebank, showing that left conjuncts tend to be shorter, especially when the governor is on the left or absent. This tendency disappears when the governor is on the right, providing evidence against asymmetric coordination structures and supporting symmetric ones. The paper concludes that these findings argue for symmetric dependency structures in coordination.</sample>
    <sample id="217">The paper "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation" by Weihao Zeng, Lulu Zhao, and Keqing He addresses the challenge of generating controllable dialogues with multiple attributes. Previous methods focused on single attributes or combined controllers with specific labels, but lacked the ability to handle continuous attributes and were limited by annotated data. The authors propose **Disentangled Controllable Generation (DCG)**, which learns attribute concepts from seen values using disentanglement loss to separate different attribute combinations. They introduce a **unified reference-free evaluation framework (MAE)** that does not require additional labeled data, using discrete and continuous prompts to guide the model.

The model is based on the DialoGPT framework with a compositional prompt module. Two types of prompts are designed: **attribute-oriented** (using controllable attribute values) and **task-oriented** (guiding global features). These are concatenated to create whole prompt embeddings. To enhance diversity and disentanglement, pseudo combinations and disentanglement loss are introduced. Experiments on benchmarks show that DCG outperforms baselines in controllability and text equality, even for unseen attribute combinations. The MAE evaluation framework correlates well with human judgments, demonstrating its effectiveness for both discrete and continuous attributes. The method generalizes well from seen to unseen attributes, proving its ability to disentangle and learn attribute relationships. Overall, DCG addresses compositional generalization challenges effectively.</sample>
    <sample id="218">Die Autoren gehören der Universität Stanford an.</sample>
    <sample id="219">In this presentation, Jia-Huei Ju introduces the research work "A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports," conducted with collaborators Yu-Shiang Huang, Cheng-Wei Lin, and advisors Che Lin and Chuan-Ju Wang. The study focuses on automating the extraction of meaningful insights from financial reports, specifically the Form 10-K, an annual report required by the SEC. The challenge lies in the high text similarity across consecutive years (around 80% token overlap) and the need for human-intensive efforts to mine useful information.

The research is motivated by the observation that financial reports contain repetitive content, making it difficult to identify significant changes or new information. To address this, the team introduces a highlighting task that compares and contrasts the context between a target report and its previous year's reference report. The goal is to predict the importance of specific words in highlighting the rationale behind the differences.

The proposed pipeline consists of four stages: document segmentation (Stage 0), relation recognition (Stage 1), and two stages of fine-tuning (Stage 2 and Stage 2+). Stage 1 classifies report pairs into three types: β pairs (high similarity), revised pairs (similar syntax but different meaning), and unmatched pairs (new information). For fine-tuning, the team uses an external dataset (eSNLI) for out-of-domain training and revised pairs for in-domain training, incorporating soft labeling techniques to improve pseudo-label quality.

The evaluation, conducted on eSNLI and the released FINAL dataset, demonstrates that the domain-adaptive highlighting model outperforms other approaches, maintaining generalization capability. The model also shows promise in simulating unmatched pairs, which were not used during training. The research opens avenues for future work, including enhancing effectiveness and integrating additional information retrieval techniques to improve the application.</sample>
    <sample id="220">The authors belong to Stony Brook University.</sample>
    <sample id="221">Die Arbeit untersuchte verschiedene Sprachpaare, jedoch wurden spezifische Paare nicht explizit im Inhalt genannt. Es wurde erwähnt, dass die Evaluationen auf der Grundlage der WMT (Workshop on Machine Translation) Testsätze durchgeführt wurden, die eine Vielzahl von Sprachpaaren abdecken. Die Ergebnisse wurden im Kontext der Übersetzung von einer Sprache in eine andere präsentiert, ohne spezifische Paare zu nennen.</sample>
    <sample id="222">The paper "To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering" explores the difficulties in adapting open-domain question answering (QA) models to new domains and proposes solutions. The authors highlight the limitations of using general-purpose corpora like Wikipedia for answering domain-specific questions, such as biomedical queries, where the model struggles due to lack of domain-specific training.

The study investigates three main contributions: (1) exploring data interventions for enabling out-of-domain generalization, (2) identifying the nature of dataset shifts in target domains, and (3) determining effective interventions for specific types of shifts. The authors use a setup with a general-purpose Wikipedia-based source domain and test the model's generalizability across six different target domains.

For data interventions, the authors consider zero-shot and few-shot methods. Zero-shot techniques involve controlling interactions among question, answer, and context variables, while few-shot methods use examples from the target domain to prompt large language models for generating more examples. They find that few-shot methods improve retriever performance by 8% and reader performance by 11% on average.

The authors also analyze the nature of dataset shifts, categorizing them into no shift, concept shift, covariate shift, and full shift. They develop a compatibility measure based on the likelihood assigned by the source retriever and reader models to target contexts and answers. This measure helps map target datasets onto a 2D grid, estimating the type of dataset shift.

Finally, the authors determine that few-shot adaptations are effective for all target sets, while zero-shot adaptations are beneficial for datasets with concept and covariate shifts. They conclude by showing that their interventions can improve reader performance by up to 24%, demonstrating that targeted data interventions can significantly enhance the performance of open-domain QA models in new domains.</sample>
    <sample id="223">The presenter's name is Shangbin.</sample>
    <sample id="224">Die Modelle, die während der Experimente untersucht wurden, sind:

1. **Long-mBART** für die Dokument-Ebene
2. **Base mBART** für die Satz-Ebene</sample>
    <sample id="225">53 Aufgaben aus MultiInstruct werden für Training und Tests verwendet.</sample>
    <sample id="226">Zwei Autoren sind an der Arbeit beteiligt: Regina Stodden und Omar.</sample>
    <sample id="227">Die aktuelle Forschung zu Sprachmodellen hat zwar große Erfolge erzielt, doch es gibt noch Lücken, insbesondere im Bereich des **gebundenen Sprachverständnisses** (grounded language understanding). Dieses Konzept zielt darauf ab, natürliche Sprachäußerungen in konkrete Handlungen oder Programme in einem spezifischen Umfeld zu übersetzen, wie z. B. bei Smart-Assistenten, semantischen Suchanfragen oder der Steuerung von Robotern. Eine zentrale Herausforderung besteht darin, dass gängige Sprachmodelle während des Pre-Trainings hauptsächlich auf Textkorpora trainiert werden, ohne Bezug zu einer realen Umgebung. Dies führt zu einer Diskrepanz zwischen dem Pre-Training und den downstream Anwendungen, was die Aufgabe besonders schwierig macht.

Um dieses Problem zu umgehen, wird in der Arbeit ein **neuartiger Rahmen namens Pangu** vorgeschlagen. Dieser trennt die Welt der Sprache von der symbolischen Welt der Handlungen. Ein symbolischer Agent schlägt Handlungspläne vor, während ein Sprachmodell diese bewertet und priorisiert. Dadurch entfällt die Notwendigkeit, dass das Sprachmodell selbst gültige und grammatikalisch korrekte Pläne generiert, was zu robusteren Ergebnissen führt.

Pangu wurde auf dem Testbed des **knowledge-based question answering** (KBQA) evaluiert, zeigt aber generische Anwendbarkeit. Experimente mit verschiedenen Modellen wie BERT, T5 und Codex sowie Fine-Tuning und In-Context-Learning zeigten überzeugende Ergebnisse. Insbesondere demonstrierte Pangu eine hohe **Stichprobeneffizienz**, z. B. bei der Generierung von GRAIL-Abfragen mit nur einem Beispiel. Zudem zeigte sich, dass Pangu weniger anfällig für Overfitting ist, was seine Robustheit in nicht-i.i.d. Umgebungen erklärt.

Die Kernaussage der Arbeit lautet: **Für gebundenes Sprachverständnis ist die Generierung keine geeignete Strategie, während Diskriminierung deutlich effektiver ist.** Pangu bietet somit einen vielversprechenden Ansatz, um die Lücke zwischen natürlicher Sprache und realen Handlungen zu überbrücken.</sample>
    <sample id="228">Die Autoren haben ihre Experimente auf vier Datensätzen durchgeführt: AG News, MIND, SST2 und Enron Spam.</sample>
    <sample id="229">Gabriella Skitalinskaya and Henning Wachsmuth present their research on detecting improvable claims in argumentative writing. The study focuses on the importance of text revision in achieving optimal phrasing, particularly in argumentative texts, where effective communication is crucial. They introduce two tasks: Suboptimal-Claim detection (determining if a claim needs revision) and Claim Improvement Suggestion (identifying quality issues to improve a claim). The research explores challenges in using revision-based data, such as representativity, reliability, model complexity, contextual dependency, and topical/user bias. 

The team analyzed revision histories from collaborative online debate platforms like Kialo to extract insights on claim quality. They found that modeling the distance between claim versions is effective for detecting suboptimal claims. Contextual information plays a role in claim assessment, depending on the task and quality issues. The study concludes that revision-based data can be effectively used for the introduced tasks, with modeling claim version distance and considering contextual information being beneficial. For detailed findings, readers are encouraged to refer to their paper.</sample>
    <sample id="231">NACHOS is a dataset of medical crawled data from the web, which was used to train the biomedical model DrBERT in French.</sample>
    <sample id="232">Der Referent heißt David Vilar.</sample>
    <sample id="233">**Abstract:**  
Simultaneous speech translation (SimulST) enables real-time cross-language communication by translating spoken language into text. Current SimulST models often require specialized architectures and multiple models for different latency regimes, leading to complex training and maintenance. Our approach, **EDAtt (Encoder-Decoder Attention)**, leverages existing offline speech translation (ST) models without retraining or adopting specific SimulST architectures. It uses a single model for all latency regimes, optimizing latency through parameter control, and exploits the attention mechanism to guide translation output. EDAtt emits partial translations based on cross-attention weights, where a word is emitted if its attention is not concentrated (sum below a threshold α) on the last λ speech frames. This ensures stable information reception and avoids premature emission. Experimental results show that EDAtt outperforms strategies applied to offline models (Wait-k, Local Agreement) and state-of-the-art SimulST architectures, achieving higher translation quality (BLEU) and faster latency. It is also the fastest strategy when considering computational time. Open-source code and models are available for reproducibility.</sample>
    <sample id="234">Die Prompt-Strategie hat einen **großen Einfluss** auf die Ergebnisse der Übersetzung durch Large Language Models (LLMs) wie PaLM. In Experimenten wurde gezeigt, dass bereits bei **one-shot Prompting** die Leistung um mehr als **ein BLEURT-Punkt** variieren kann, in extremen Fällen sogar um **40 BLEURT-Punkte**. Die **Qualität der Beispiele** in den Prompts ist dabei entscheidender als die Ähnlichkeit zu den Quelldaten. Bei **5-shot Prompting**, wie in der Studie verwendet, spielt die Form des Prompts kaum noch eine Rolle, während die Beispiele den größten Einfluss haben. Daher ist die Auswahl von **hochqualitativen Beispielen** für die Prompt-Strategie von zentraler Bedeutung.</sample>
    <sample id="235">Die Autoren gehören der Carnegie Mellon University an.</sample>
    <sample id="236">Die fünf Anweisungen der Expert*innen für jede Aufgabe im MultiInstruct-Datensatz sind nicht explizit im Text aufgeführt. Es wird jedoch erwähnt, dass jede Aufgabe mit fünf von Expert*innen geschriebenen Anweisungen ausgestattet ist. Diese Anweisungen dienen dazu, die Flexibilität und Anpassungsfähigkeit des Modells bei der Ausführung von multi-modalen Aufgaben zu testen.</sample>
    <sample id="237">Die Autoren schlagen vor, Modelle mit dem **KITMUS-Test (KITMUS Test: Evaluating Knowledge Integration from Multiple Sources)** zu testen. Dieser Test bewertet die Fähigkeit von Modellen, Wissen aus verschiedenen Quellen zu integrieren, indem er drei Einstellungen definiert:  
1. **Background-Pretrain**: Hintergrundwissen ist im Pretrain-Wissen enthalten.  
2. **Background-Both**: Hintergrundwissen ist sowohl im Pretrain-Wissen als auch im Inferenz-Kontext verfügbar.  
3. **Background-Inference**: Hintergrundwissen ist nur im Inferenz-Kontext verfügbar.  
Der Test verwendet Beispiele, bei denen Modelle entscheiden müssen, welche Entität ein Pronomen bezieht, und prüft, ob sie Wissen aus verschiedenen Quellen effektiv nutzen können.</sample>
    <sample id="238">In diesem Video stellt Yebowen Hu von der University of Central Florida den neuen Benchmark-Datensatz **MeetingBank** vor. Der Datensatz wurde entwickelt, um die Herausforderungen der Erstellung von Meetingzusammenfassungen zu bewältigen, insbesondere für öffentliche Stadtparlamentsitzungen. MeetingBank enthält 1.366 Transkripte von Stadtparlamentssitzungen und fast 7.000 Zusammenfassungsexemplare. Die Daten wurden mithilfe der Speechmatics API und verschiedener Websites gesammelt, wobei Transkripte, Referenzzusammenfassungen und zusätzliche Ressourcen wie URLs bereitgestellt werden.

Die Qualität der Zusammenfassungen wurde durch zwei Messgrößen bewertet: **Abdeckung** (Prozentsatz der Quellenwörter im Text) und **Dichte** (Grad der Zusammenfassung als Referenzextrakte). Die Ergebnisse zeigen, dass die meisten Zusammenfassungen (70–90 %) direkte Zitate enthalten, während Seattle und Boston höhere Bearbeitungsgrade aufweisen.

Für die Modellierung wurden sowohl extraktive (z. B. Oracle, TextRank) als auch abstraktive (z. B. BART-Large, GPT-3) Systeme getestet. GPT-3 erzielte zwar hohe Werte in Fluency und Kohärenz, fiel jedoch in Informativität und Faktizität zurück. Humanbewertungen ergaben, dass GPT-3 insgesamt am besten abschnitt, was auf seine Stärken in der Sprachverarbeitung hinweist.

MeetingBank dient als wertvolles Werkzeug für die Entwicklung von Meetingzusammenfassungsmodellen und bietet Einblicke in die Entscheidungsfindung von Stadtparlamenten. Der Datensatz ist öffentlich zugänglich und ermutigt Forscher, ihn zu nutzen und weiterzuentwickeln.</sample>
    <sample id="239">Hallo alle, mein Name ist David Vilar, und ich werde eine kurze Besprechung des Papiers „Prompting PaLM for Translation: Assessing Strategies and Performance“ halten. Dies ist eine gemeinsame Arbeit mit meinen Kollegen von Google Translate. PaLM ist ein 540 Milliarden Parameter umfassendes großes Sprachmodell, das im letzten Jahr 2022 vorgestellt wurde. Es wurde auf einer großen Sammlung von Texten trainiert, die 780 Milliarden Token umfassen. Zum Zeitpunkt der Veröffentlichung erreichte es den Stand der Technik in Hunderten von NLP-Aufgaben. In dieser Arbeit präsentieren wir die erste systematische Studie zur Aufforderung von großen Sprachmodellen für die maschinelle Übersetzung. Wir bewerteten die Übergabecapacität solcher Modelle unter Verwendung der bewährten Praktiken der MT-Community. Dies beinhaltet die Verwendung der neuesten Testsätze, um eine Übereinstimmung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden. Und wir verglichen mit den besten Systemen, also dem WMT-Bewertungssystem. Wir verwenden die neuesten, neuronalen MT-Metriken und zeigen zusätzlich auch Ergebnisse der fachkundigen menschlichen Bewertung. Schließlich geben wir einige Empfehlungen für Strategien zur Auswahl von Aufforderungen. Die Aufforderung hat einen großen Einfluss auf die Leistung der LLMs für die Übersetzung, wie wir in einem einfachen Experiment sehen können, bei dem wir eine einmalige Aufforderung verwendeten und für jeden Satz zwei verschiedene Aufforderungen bereitstellten. Die Mehrheit der Sätze, 516 von 1.000. Der beobachtete Unterschied beträgt mehr als einen BLEURT-Punkt. Und dies kann in extremen Fällen bis zu 40 BLEURT-Punkte betragen. Es ist also wichtig, eine gute Aufforderungsstrategie auszuwählen. In unseren Experimenten entschieden wir uns für eine Aufforderungsstrategie mit fünf Schüssen, bei der wir jeden Satz, den wir dem System bereitstellen, mit der Sprache markieren, in der er ist. So in diesem Beispiel hier, wo wir die Übersetzung von Deutsch ins Englische durchführen, werden die deutschen Sätze, die Quelltexte, mit einem deutschen Doppelpunkt und die englischen Übersetzungen mit einem englischen Doppelpunkt markiert. Wir stellten fest, dass die tatsächliche Form der Aufforderung im Falle mehrerer kurzer Aufforderungen keinen großen Einfluss hat. Sie ist entscheidend für null- und einmalige Aufforderungen. Und wenn wir, wie in unserem Fall, zu fünf Aufforderungen gehen, gibt es fast keinen Unterschied zur tatsächlichen Form der Aufforderung. Es sind die Beispiele, die den größten Einfluss haben. Die Zusammenfassung unserer experimentellen Ergebnisse ist, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit zum Quelltext. Es ist also wichtig, die Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die Auswahl von Aufforderungen aus den Trainingsdaten für die WMT-Bewertungen auf den Dev-Daten. Die Dev-Daten sind viel kuratiert und von höherer Qualität als die Trainingsdaten, die lauter sind. Und ihre Ergebnisse zeigen eine bessere Leistung bei der Verwendung der Dev-Daten. Dennoch haben spezialisierte, modernste Systeme einen erheblichen Vorteil gegenüber den PaLM-Übersetzungen. Aber PaLM kommt ziemlich nah an ein kommerzielles System heran. In unserem Fall entschieden wir uns, mit Google Translate zu bewerten. Die Erkenntnisse, die wir aus der menschlichen Bewertung gewonnen haben, die wir unter Verwendung des MQM-Rahmens durchgeführt haben, sagten, dass die Fließfähigkeit von PaLM mit den modernsten Systemen vergleichbar ist, aber der Hauptunterschied kommt von der Genauigkeit. Insbesondere sind die häufigsten Fehler Omissionsfehler. Es scheint also, dass PaLM eine bessere klingende Übersetzung produzieren möchte, manchmal indem er Teile des Quellsatzes, die in der Übersetzung gemacht werden, weglässt. Die Kategorie „Stil/Unbeholfen“ für PaLM ist jedoch niedriger als für die modernsten Systeme, was ein zusätzliches Signal dafür ist, dass PaLM wirklich fließende Ausgaben liefert, aber immer noch mit einigen Genauigkeitsproblemen. Und das war es für diesen wirklich kurzen Überblick. Für mehr Details, bitte kommen Sie zur vollständigen Präsentation des Papiers. Vielen Dank.</sample>
    <sample id="240">Hallo, ich bin Dawei, ein Doktorand an der Universität des Saarlandes in Deutschland. In diesem Video möchte ich unsere jüngste Arbeit "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" vorstellen. Dies ist eine gemeinsame Arbeit mit Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow. Ich möchte mit einer kurzen Einführung in schwache Überwachung und schwach überwachtes Lernen beginnen. Bei schwacher Überwachung kennzeichnen Sie die Daten nicht manuell. Stattdessen kennzeichnen wir die Daten mit schwachen Kennzeichnungsquellen, wie einfachen heuristischen Regeln, Wissensbasen oder minderwertiger Crowdsourcing, wie in der Abbildung rechts dargestellt. Im Vergleich zu menschlichen Annotationen sind die schwächeren Annotationen viel billiger, aber sie sind auch laut, was bedeutet, dass eine bestimmte Menge der Annotationen falsch ist. Wenn wir neuronale Netze direkt auf schwach gelabelten Daten trainieren, neigen die neuronalen Netze dazu, den Label-Rausch zu verinnerlichen und verallgemeinern nicht. Im schwach überwachten Lernen werden Trainingsalgorithmen vorgeschlagen, um neuronale Netze robust unter solch einem Label-Rausch zu trainieren, sodass die trainierten Modelle sich gut verallgemeinern. In jüngsten Arbeiten im WSL, wobei WSL für schwach überwachtes Lernen steht, ist ein häufiger Anspruch, dass die Leute sagen, dass sie nur Modelle auf den schwach gelabelten Daten trainieren und hohe Leistung auf sauberen Testdatensätzen erzielen. Technisch gesehen ist dieser Anspruch nicht falsch, aber es gibt einen Haken, der darin besteht, dass die Leute annehmen, dass ein zusätzlicher sauberer Validierungsdatensatz für die Modellselektion verfügbar ist. Wir können uns nicht auf diese Problemstellung beschränken, aber dies impliziert, dass zusätzliche manuelle Annotationen im schwach überwachten Lernen erforderlich sind. Die oben genannte Frage wird gestellt, um drei Forschungsfragen zu stellen. Erstens, ist sauberer Validierungsdatensatz für WSL notwendig oder können wir stattdessen einen lauten Validierungsdatensatz verwenden? Zweitens, wenn sauberer Datensatz erforderlich ist, oder wenn sauberer Datensatz für WSL obligatorisch ist, wie viele saubere Proben benötigen wir dann? Schließlich, sollten wir die sauberen Proben nur für die Validierung verwenden, oder gibt es bessere Möglichkeiten, sie zu nutzen? Wir haben diese Forschungsfragen in unserer Arbeit angesprochen und unsere Ergebnisse sind wie folgt. Erstens stellen wir fest, dass neuere WSL-Methoden tatsächlich saubere Validierungsproben benötigen, um ordnungsgemäß zu funktionieren. Andernfalls gibt es einen großen Leistungsabfall. Wie in dieser Abbildung gezeigt, können die trainierten Modelle nicht über die ursprünglichen schwachen Labels hinaus verallgemeinern, was bedeutet, dass das Training sinnlos ist. Dies deutet darauf hin, dass WSL-Ansätze tatsächlich sauber gelabelte Daten benötigen, um ordnungsgemäß zu funktionieren, und die Annotationskosten für die Beschaffung sauberer Validierungsproben sollten nicht außer Acht gelassen werden. Unsere zweite Erkenntnis ist, dass die Anzahl der sauberen Validierungsproben die Leistung von WSL-Ansätzen verbessern wird, wie in der Abbildung links gezeigt. Typischerweise benötigen wir nur 20 Proben pro Klasse, um eine hohe Leistung zu erzielen. Aber das ist nicht das Ende der Geschichte, denn wenn wir uns entscheiden, saubere Proben zu nutzen, wird das direkte Feintuning sogar eine bessere Leistung erzielen. Die rechte Abbildung zeigt den Leistungsunterschied zwischen Feintuning-Ansätzen, die direkt auf den sauberen Daten angewendet werden, und WSL-Ansätzen, die die sauberen Daten nur für die Validierung verwenden. Wie wir sehen können, wenn wir 10 Proben pro Klasse haben, beginnt das direkte Feintuning, WSL-Ansätze zu übertreffen. Schließlich kann die in früheren WSL-Ansätzen behauptete Leistungsverbesserung leicht erreicht werden, indem man das Feintuning auf den sauberen Validierungsproben fortsetzt. Wie wir den Abbildungen entnehmen können, unterlebt das Standardmodell, bezeichnet als FTw, zunächst mehr komplexe WSL-Methoden wie COSINE. Wenn wir jedoch das Feintuning auf den sauberen Proben fortsetzen, erreicht FTw eine gleich gute Leistung wie andere Methoden. In der Praxis gibt es also keinen Grund, komplexere WSL-Methoden zu wählen, die mehr Rechenzeit und Festplattenspeicher erfordern. Zusammenfassend haben wir gezeigt, dass neuere WSL-Ansätze saubere, manuell annotierte Proben benötigen, damit sie ordnungsgemäß funktionieren. Ihr Leistungsgewinn und ihre Praktikabilität werden stark überschätzt. Unsere konkreten Empfehlungen für zukünftige Arbeiten sind wie folgt. Erstens, berichten Sie über die Modellselektionkriterien. Zum Beispiel, berichten Sie, ob die Modellselektion über saubere Validierungsproben erfolgt. Zweitens, WSL-Ansätze sollten mit Few-Shot-Learning-Grundlinien verglichen werden, da beide auf sauberen Proben arbeiten. Drittens, kontinuierliches Feintuning ist eine einfache, aber starke Grundlinie, die in zukünftigen Arbeiten im WSL berücksichtigt werden sollte. Schließlich haben wir unseren Code open-source gemacht. Sie können ihn über den QR-Code auf dieser Folie finden. Bitte zögern Sie nicht, ihn zu überprüfen. Vielen Dank und viel Spaß auf der Konferenz.</sample>
    <sample id="241">Ethan introduces a paper titled "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments," co-authored with Yang Chen, Wei Xu, and Alan Ritter from Georgia Tech. The research addresses two key shortcomings in existing automated misinformation detection systems: unrealistic evaluation and lack of human-centricity. Traditional systems often use retrospectively constructed datasets and fail to account for leaked counter-evidence, which limits their effectiveness in early detection. Additionally, these systems either exclude humans entirely or only involve them in the final verification step, ignoring the critical role of human moderators in real-world scenarios.

The proposed framework integrates human feedback throughout the process, from raw tweet analysis to actionable outputs. The system is designed to be end-to-end, handling the complexity of social media data. It consists of two main components: (1) a misleading claim detection module that filters and ranks trending claims for human verification, and (2) a policy violation verification module that flags tweets supporting unapproved COVID-19 treatments. The system uses keyword filtering, a T5 model for claim extraction, and a BERT-based stance classification model to identify policy violations.

The authors evaluate their workflow by defining early detection as identifying unapproved treatments before they appear in debunking news articles. They demonstrate the system's ability to detect such claims ahead of public debunking. Additionally, they assess the system's policy violation detection accuracy, finding a 65% success rate, and measure human workload, revealing 124.2 policy violations can be verified per human hour. The framework aims to realistically simulate the collaboration between systems and human moderators, encouraging the development of future human-in-the-loop systems and providing industry insights into their evaluation.</sample>
    <sample id="242">Gängige Bewertungsmethoden für Dialogsysteme umfassen:

1. **Human Evaluation**: Menschen bewerten die Qualität von Dialogen, oft durch Auswahl der besseren Konversation oder durch Verwendung von Likert-Skalen.
2. **Likert-Ratings**: Bewertungen auf Turn-Level (Einzelantwort) und Dialogue-Level (gesamter Dialog).
3. **Pairwise Comparisons**: Direkte Vergleiche zwischen zwei Dialogen, um den besseren zu bestimmen.</sample>
    <sample id="243">Die Arbeit wurde von **fünf Autoren** verfasst: Jenny, Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap.</sample>
    <sample id="244">Im Beispiel mit Servin und Kea wird das Hintergrundwissen benötigt, dass "Judges decide cases in law courts". Dieses Wissen ist notwendig, um zu verstehen, warum Servin nach einem langen Arbeitstag, an dem er Fälle in einem Gericht entschieden hat, sich entspannen möchte. Dieses Hintergrundwissen ist typischerweise während des Pretrainings von großen Sprachmodellen erlernt worden.</sample>
    <sample id="245">In ihrem Vortrag präsentiert Lining Zhang die Arbeit "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization". Das Ziel der Studie ist es, eine effektive Methode zur Identifizierung von hochqualifizierten Annotatoren auf Amazon Mechanical Turk (MTurk) für die Zusammenfassung von Texten zu entwickeln. Die Forscher haben eine zweistufige Pipeline entwickelt, die aus einem Qualifikationstest und einem Endurance-Test besteht. Der Qualifikationstest bewertet die Fähigkeit der Annotatoren, mehrere Dimensionen korrekt zu bewerten, und teilt sie in vier Kategorien ein: Gold, Silber, Bronze und Block. Nur die Gold- und Silber-Annotatoren können diesen Test bestehen. Der Endurance-Test testet die Fähigkeit der Annotatoren, eine hohe Arbeitsbelastung zu bewältigen. Die Ergebnisse zeigen, dass die Annotatoren aus der Pipeline eine höhere Übereinstimmung in Bezug auf die inter-annotatorische Übereinstimmung (IAA) erzielen als Experten. Im Vergleich zu anderen Ansätzen, wie der Verwendung von Baseline-MTurk-Annotatoren oder CloudResearch-Annotatoren, bietet die Pipeline eine ähnliche Qualität bei geringeren Kosten und höherer Übereinstimmung. Die Forscher betonen, dass ihre Pipeline eine effektive Methode zur Identifizierung von hochqualifizierten Annotatoren für die Zusammenfassung von Texten auf MTurk darstellt und Ressourcenverschwendung vermeiden kann. Zukünftige Arbeiten werden sich auf die Verbesserung der Qualität der Annotatoren, die Anwendung auf verschiedene Aufgaben, Sprachen und Plattformen sowie die Untersuchung der Garantie für die Richtigkeit der Annotatoren konzentrieren.</sample>
    <sample id="246">Ja, der Code ist verfügbar und kann auf GitHub eingesehen werden.</sample>
    <sample id="247">Jiho Kim from KAIST AI presents the paper "FACTKG: Fact Verification via Reasoning on Knowledge Graphs," introducing a new dataset and task focused on fact verification using knowledge graphs (KGs) as evidence for natural language claims. Existing datasets like FEVER and VitaminC rely on Wikipedia text, while TabFact and InfoTabs use tables. FACTKG fills the gap by integrating KGs, specifically DBpedia, with natural language claims, offering a more intuitive and reliable method for fact verification.

The proposed task, Knowledge Graph-Based Fact Verification, leverages KGs to enable direct and reliable reasoning, reducing the need for additional interpretation. This approach is particularly practical for applications like modern dialogue systems, which interact with internal KGs to check consistency between user inputs and stored information. FACTKG includes claims in both written and colloquial styles, enhancing its practical applicability.

The dataset features five types of reasoning: one-hop, conjunction, existence, multi-hop, and negation. One-hop claims involve a single triple, conjunction claims require multiple triples, existence claims check a specific relation, multi-hop claims necessitate inference through multiple entities, and negation requires additional verification. The colloquial style claims are generated using a transfer model and presupposition templates.

The dataset includes 1,000 claims, with 500 SUPPORTED and 500 REFUTED, and is designed to evaluate models' ability to retrieve and reason with KG evidence. Baselines include Claim Only, which uses only claims, and the GEAR model, which incorporates KG evidence. Results show that all baselines outperform the majority class baseline (51%), with the GEAR model significantly outperforming others by utilizing KG evidence.

FACTKG aims to advance fact verification by integrating KGs and natural language, offering a reliable and practical tool for consistency checks and other applications. The dataset is available for download, and further inquiries are encouraged.</sample>
    <sample id="248">Based on the information provided in Jenny's presentation, the annotators for NLPositionality were not necessarily balanced in terms of every demographic group. Here are the key points regarding the demographic diversity of the annotators:

1. **Geographical Diversity**: The annotations were gathered from over 1,000 annotators across 87 countries. This suggests a significant geographical diversity, which helps in capturing a broader range of perspectives.

2. **Educational Background**: The presentation mentions that the models and datasets are more aligned with people who have a college education or higher. However, it does not specify the educational background distribution of the annotators. It is possible that the annotators were not evenly distributed across different educational levels, which could introduce biases.

3. **Gender and Non-Binary Representation**: The presentation highlights that datasets and models are less aligned with non-binary people compared to men and women. This implies that the annotators might not have been adequately representative of the non-binary community, which could contribute to the observed biases.

4. **Demographic Data Collection**: The researchers re-annotated datasets with diverse annotators and collected a rich set of demographic data. However, the specific balance or imbalance in demographic representation is not detailed in the presentation.

In summary, while the annotators were diverse geographically, there might have been imbalances in other demographic categories such as educational background and gender/non-binary representation, which could have influenced the results. The researchers aimed to address these issues by collecting demographic data, but the exact balance is not explicitly stated.</sample>
    <sample id="249">Die Sätze innerhalb der akzeptablen Domain wurden durch Perturbations durcheinandergebracht, indem relevante Strukturen beibehalten, aber gleichzeitig Rauschen hinzugefügt wurde.</sample>
    <sample id="250">Eine dimensionale Bewertung im Kontext von ABC-Eval bezieht sich auf die Bewertung von Conversational AI anhand mehrerer spezifischer Aspekte oder Dimensionen der Dialogqualität, anstatt nur eine allgemeine Einschätzung zu geben. Diese Dimensionen umfassen beispielsweise die Relevanz der Antworten, Selbst- oder Partnerwidersprüche, die Einhaltung von Fakten und Common-Sense-Wissen, sowie die Fähigkeit, Empathie zu zeigen. Durch die Bewertung entlang dieser Dimensionen können die Stärken und Schwächen eines Dialogmodells detaillierter und präziser analysiert werden, was zu einem tieferen Verständnis der Qualität des Dialogs führt.</sample>
    <sample id="251">The authors belong to the University of Science and Technology of China.</sample>
    <sample id="252">The presentation introduces "U-CREAT: Unsupervised Case Retrieval using Events extraction", a groundbreaking work by Sai Kiran Tanikella and his team from IIT Kanpur. The research addresses the challenge of Prior Case Retrieval (PCR) in the legal domain, where the task is to retrieve relevant past cases (cited documents) given a query document. The team developed the IL-PCR dataset, a benchmark for PCR tasks with 7,070 legal cases and an average of 6.775 citations per query document, surpassing existing datasets like COLIEE’21 in size and complexity.

The core contribution is the U-CREAT pipeline, which employs unsupervised learning and an event-based approach. It leverages event extraction using dependency parsing to represent case documents as sequences of events, enhancing retrieval efficiency and generalization across legal systems. The pipeline processes query and candidate documents to compute an interaction matrix of events, which is then used in retrieval models to rank candidates.

Experiments demonstrate that event-based models, particularly the Event Filtered Documents model, outperform baseline methods and other approaches, including transformer-based models. U-CREAT achieves state-of-the-art performance on both IL-PCR and COLIEE’21 datasets, with lower inference times and higher F1 scores. This work highlights the importance of event-based approaches and tailored models for the legal domain, paving the way for future advancements in PCR.</sample>
    <sample id="253">Mario Ezra Aragón präsentiert die Arbeit "DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media". Das Team aus Mexiko und Spanien entwickelt ein Modell, das mithilfe von künstlicher Intelligenz (KI) Anzeichen von psychischen Störungen in Social-Media-Beiträgen erkennt. Psychische Störungen wie Depression, PTSD, Bulimie oder Anorexie können durch die Analyse von Online-Inhalten frühzeitig erkannt werden, um Betroffene zu unterstützen.

Das Modell basiert auf BERT, einem Sprachmodell, das durch Anpassung an spezifische Domains wie Reddit oder mentale Gesundheit verbessert wird. Durch "Domain Adaptation" wird das Modell auf die Sprache und Themen von Social Media und psychischen Störungen spezialisiert. Ein "guided masking"-Verfahren fokussiert das Modell auf wichtige Wörter und Konzepte.

Die Ergebnisse zeigen, dass DisorBERT präziser und aussagekräftiger ist als andere Modelle. Es erkennt häufiger und genauer auf psychische Störungen hinweisende Wörter und Sätze. Beispielsweise generiert DisorBERT bei der Frage "I used to be able to cry" Antworten wie "focus", "talk", "breath", "sleep" und "eat", die mit psychischen Problemen assoziiert sind.

Die Visualisierung der wichtigsten Textabschnitte bestätigt, dass DisorBERT relevante Themen wie "anxious" und "medication" bei Depressionen identifiziert. Das Modell erreicht ein gutes Gleichgewicht zwischen der Erkennung von Nutzern und der korrekten Zuordnung von Störungen.

Zukünftige Arbeiten sollen die Nutzung verschiedener lexikalischer Ressourcen und die Integration klinischer Daten untersuchen. DisorBERT bietet ein vielversprechendes Werkzeug zur Früherkennung psychischer Störungen in Social Media.</sample>
    <sample id="254">The paper presents "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction," a framework to improve the quality of pseudo labels in distant supervision (DS) data, which is crucial for document-level relation extraction (DocRE). Traditional methods rely on large-scale human annotations, which are costly and time-consuming. DS leverages noisy data, but pseudo labels can introduce false positives, as seen in the example where a false "composer" relation overshadows the correct "place of birth." To address this, the framework first trains a DocRE model using both DS and human-annotated data to generate pseudo labels. It then introduces uncertainty estimation to identify unreliable predictions. For overlapping relations, an instance-level uncertainty method is proposed to score each relation individually. A re-labeling strategy with dynamic class uncertainty thresholds filters out high-uncertainty pseudo labels, favoring those with lower uncertainty. A multi-phase training strategy iteratively refines DS data. The framework outperforms baselines on public datasets, demonstrating improved performance. Key contributions include uncertainty-guided label denoising, instance-level uncertainty estimation for overlapping relations, a re-labeling strategy for long-tail classes, and significant performance enhancements. This work addresses the challenges of noise and overlapping relations in DS data, paving the way for more robust DocRE models.</sample>
    <sample id="255">Die Form des Prompts ist in den Fällen wichtig, wenn es sich um **Zero-Shot** oder **One-Shot**-Prompting handelt. Bei mehrschüssigem Prompting (z. B. fünfschüssigem Prompting) hat die Form des Prompts hingegen kaum Einfluss auf die Leistung.</sample>
    <sample id="257">Die Autoren haben vier state-of-the-art Chat-Modelle evaluiert.</sample>
    <sample id="258">In this video, Chiang Cheng-Han discusses the paper "Can Large Language Models Be an Alternative to Human Evaluation?" The research explores whether large language models (LLMs) can serve as a reliable alternative to human evaluators in assessing text quality in natural language processing (NLP). The motivation stems from the instability and reproducibility issues inherent in human evaluations. The study proposes using LLMs to evaluate text samples based on natural language instructions, leveraging their ability to understand and follow such instructions.

The experiment involves rating stories generated by GPT-2 or humans across four attributes: grammar, coherence, likability, and relevance. Four LLMs—T0, InstructGPT (Curie and Davinci), and ChatGPT—were used. Ground-truth ratings were provided by English teachers, who also preferred human-written stories over GPT-2-generated ones. Notably, Davinci and ChatGPT showed a clear preference for human-written text, aligning with the human evaluators.

The paper addresses broader questions, such as inter-rater agreement, instruction wording, response sampling, and the cost-benefit analysis of LLMs compared to human evaluation. It also explores the applicability of LLMs to other NLP tasks. The findings suggest that certain LLMs can effectively mimic human evaluation, offering a potential alternative with advantages like reproducibility and scalability. For further details, the authors invite readers to review their paper or visit their poster at ACL.</sample>
    <sample id="259">Yusen Zhang von der Penn State University präsentiert die Arbeit "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations". Das Ziel ist es, semantische Repräsentationen von Benutzeranfragen in verschiedenen natürlichen Sprachen in verschiedene Bedeutungssysteme wie SQL, Lambda-Kalkül und FunQL zu übersetzen. Bestehende Modelle für mehrsprachige semantische Parsing sind oft auf bestimmte Sprachen oder Systeme beschränkt. XSemPLR adressiert diese Lücken mit einem einheitlichen Datensatz, der 9 Datensätze aus verschiedenen Bereichen, 5 semantische Parsing-Aufgaben, 8 Bedeutungssysteme und 22 Sprachen aus 15 Sprachfamilien umfasst.

Die Evaluation erfolgt in sechs Einstellungen: Translate-Test (übersetzungsunterstütztes Training), Monolingual (einsprachige Modelle), Monolingual Few-shot (einsprachige Modelle mit nur 10% der Daten), Multilingual (mehrsprachige Modelle), Cross-lingual Zero-shot (Null-Transfer) und Cross-lingual Few-shot (Few-Shot-Transfer). Die Ergebnisse zeigen, dass Encoder-Decoder-Modelle die beste Leistung erzielen, insbesondere wenn sie in einer Mischung verschiedener Sprachen trainiert werden. Allerdings führt das Training auf Englisch in sieben Fällen zu schlechteren Ergebnissen, was als "Fluch der Multilingualität" bezeichnet wird.

Die Cross-lingual Few-shot-Transfer-Methode verringert die Leistungskluft zwischen Sprachen signifikant. Weitere Erkenntnisse zeigen, dass Pretraining auf Englisch die Leistung bei Few-shot-Aufgaben in Zielsprachen verbessern kann, während mehrsprachige Modelle wie Codex und BLOOM für Cross-lingual Semantic Parsing noch unzureichend sind. XSemPLR bietet somit einen umfassenden Benchmark für mehrsprachige semantische Parsing und liefert wertvolle Erkenntnisse für zukünftige Forschung.</sample>
    <sample id="260">Der Inhalt des Videos erwähnt Jingwei Yi als den Sprecher und Autor des Videos, das die Arbeit des Teams der University of Science and Technology of China präsentiert. Es wird jedoch nicht explizit angegeben, wie viele Autoren an der eigentlichen wissenschaftlichen Arbeit beteiligt sind. Basierend auf dem Video allein kann ich keine genaue Anzahl der Autoren nennen.</sample>
    <sample id="261">Ein guter Planer sollte folgende Eigenschaften aufweisen:

1. **Semantische Vollständigkeit**: Die generierten Skripte sollten den Zielvorgaben semantisch gerecht werden.
2. **Treue zu den Einschränkungen**: Die Skripte sollten den spezifischen Einschränkungen des Zielvorgangs treu bleiben.
3. **Qualität der Generierung**: Die Skripte sollten von hoher Qualität sein, um sicherzustellen, dass sie praktisch umsetzbar und sinnvoll sind.

Diese Eigenschaften sind entscheidend, um effektive und zuverlässige Planungsskript zu erstellen, die den Anforderungen der spezifischen Ziele gerecht werden.</sample>
    <sample id="262">Die Arbeit wurde von Siyu Yuan von der Fudan University vorgestellt, aber der genaue Anzahl der Autoren wurde nicht genannt.</sample>
    <sample id="263">In-context learning, a popular method for utilizing large language models, is often unstable due to design choices like the selection and order of examples. Prior work has shown that search instability arises from these factors, introducing biases that affect model predictions. However, there has been no systematic categorization of existing bias issues or detection of new types, nor a comprehensive approach to mitigating these biases. This work addresses these gaps, focusing on text classification tasks. It introduces a typology of label biases, identifying a new critical type: **domain-label bias**, which reflects the influence of the task corpus on model predictions.

To validate this bias, experiments were conducted where models were exposed to random in-domain words from the task corpus. Results showed that such words severely biased predictions, unlike random English words. This disparity highlighted the importance of domain-label bias in certain tasks. For tasks with high domain-label bias, models struggled to outperform chance-level baselines, even with prior calibration methods.

To address this and other biases, the authors propose **domain-context calibration**. This method estimates model bias for each label name using content-free in-domain text (random in-domain words) and calibrates predictions accordingly. Unlike previous methods that used predefined tokens like "not available," this approach accounts for domain-label bias by leveraging task-specific randomness.

Experiments across diverse datasets and models demonstrated significant performance improvements, particularly for tasks with high domain-label bias. The method also refined decision boundaries and was effective even for large models like GPT-3. By replacing single predefined tokens with random words and incorporating in-domain randomness, the authors achieved substantial improvements, underscoring the effectiveness of their approach.

In summary, this work systematically investigates label biases in in-context learning, identifies domain-label bias, and proposes a calibration method to mitigate its effects, significantly enhancing model performance.</sample>
    <sample id="264">Lin Wang, ein Doktorand an der Zhejiang University in China, präsentiert seine Arbeit "TAVT: Towards Transferable Audio-Visual Text Generation". Das Ziel der Arbeit ist es, eine Methode zur generierung von audiovisuellen Texten zu entwickeln, die auf verschiedenen Domänen und mit begrenzten Datenmengen funktioniert. Die Herausforderung liegt in den multimodalen Domänenverschiebungen, wie visueller Stil, Audioenergie und anderen. Wang schlägt einen neuen Ansatz vor, der auf einem einheitlichen audiosemantischen Raum basiert, um visuelle Konzepte über verschiedene Domänen hinweg zu verbinden.

Das vorgeschlagene Framework besteht aus drei Komponenten: einem audiovisuellen Meta-Mapper-Netzwerk, einem audiovisuellen Encoder und einem Sprachgenerierungsmodell sowie einem Dual Counterfactual Contrastive Learning (DCLL). Das Meta-Mapper-Netzwerk bildet visuelle Konzepte in einen einheitlichen audiosemantischen Raum ab und adressiert Verschiebungen in der semantischen Verteilung. Das DCLL-Verfahren konstruiert feingranulare Aufsichtssignale aus Gegenbeispielen, um die visuelle-textuelle Ausrichtung direkt zu optimieren.

In Experimenten auf zwei Benchmarks, basierend auf MSVD und MSR-VTT, übertrifft TAVT alle anderen Methoden, einschließlich des Standes der Technik (SOTA), sowohl in Cross-Datasets- als auch in Cross-Domain-Einstellungen. Besonders in Domänen mit geringen Ressourcen, wie "Kids" und "Beauty", zeigt TAVT eine robuste Leistung. Ablationsexperimente bestätigen den positiven Einfluss von Audiomerkmalen auf die Leistung.</sample>
    <sample id="265">The referent is Vasudha, a Computer Science PhD candidate at Stony Brook University.</sample>
    <sample id="266">The authors belong to the University of Warsaw.</sample>
    <sample id="268">Die häufigsten Fehler von PaLM sind **Omissionen** (Auslassungen), d. h., es werden Teile der Quellensätze in der Übersetzung weggelassen.</sample>
    <sample id="269">Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und heute erzählen wir Ihnen alles über ABC-Eval, einen neuen dimensionalen Ansatz zur Bewertung von konversationellen KI. Diese Arbeit wurde vom Emory NLP Lab unter der Leitung von Professor Jinho Choi an der Emory University und in Zusammenarbeit mit Amazon Alexa AI durchgeführt. Nehmen wir also an, Sie haben gerade ein Dialogmodell entwickelt und möchten herausfinden, wie gut es sich im Vergleich zum aktuellen Stand der Technik schlägt. Die gängige Praxis besteht darin, menschliche Evaluatoren zu verwenden, wie zum Beispiel menschliche Richter zu bitten, auszuwählen, welche von zwei Konversationen besser ist, oder Konversationen auf einer Likert-Skala zu bewerten. Diese Ansätze eignen sich gut, um ganzheitliche Bewertungen der allgemeinen Dialogqualität zu liefern, aber Dialogqualität hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chatqualität bewerten, um die Stärken und Schwächen des Modells auf einer feiner granulierten Ebene zu verstehen. Ein Ansatz besteht darin, menschliche Richter einfach zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, wie zum Beispiel die Relevanz der Modellantworten unter Verwendung bestehender vergleichender oder Likert-Skalenmethoden. Wir glauben jedoch, dass es eine präzisere und zuverlässigere Strategie für die dimensionale Dialogbewertung gibt. Unser Ansatz versucht, die Subjektivität der menschlichen Bewertung zu reduzieren, indem wir explizit annotieren, ob jede Modellantwort bestimmte Verhaltensweisen ausdrückt, wie zum Beispiel die Antwort mit irrelevanten Informationen oder sich selbst zu widersprechen. Wir nennen diesen Ansatz das Annotating Behaviors in Chat oder kurz ABC-Eval. Wir haben diese Methode entwickelt, um die Verhaltensweisen von Chatmodellen umfassend abzudecken, die in der jüngsten Literatur als beeinflussend für die Chatqualität vorgeschlagen wurden. ABC-Eval ist in der Lage, die Raten zu messen, mit denen Chatmodelle verschiedene thematische Fehler begehen. Zum Beispiel misst ABC-Eval die Anzahl der Zügen, in denen ein Chatmodell seinen Partner ignoriert oder etwas Ir relevantes sagt, sich selbst oder seinen Partner widerspricht, falsche Fakten halluziniert oder gegen das gesunden Menschenverstandswissen verstößt, und wenn das Modell Empathie zeigt oder versagt. Um zu bestimmen, welche Art von Bewertung am effektivsten ist, haben wir vier State-of-the-Art-Chatmodelle ausgewählt und diese mit 100 menschlichen Bot-Konversationen pro Modell unter Verwendung von ABC-Eval bewertet. Zum Vergleich haben wir diese Konversationen auch mit drei bestehenden Methoden bewertet: Likert-Bewertungen auf der Zug-Ebene, Likert-Bewertungen auf der Dialog-Ebene und Dialog-Ebene-Paarweise-Vergleiche. Für jede der bestehenden Methoden haben wir Bewertungen zu acht der am häufigsten gemessenen Aspekten der Dialogqualität gesammelt, da dies die Standardpraxis für die Bewertung von Chatmodellen entlang mehrerer Dimensionen ist. Aus unserer Analyse dieser Bewertungsergebnisse haben wir festgestellt, dass ABC-Eval-Verhaltensbeschriftungen insgesamt zuverlässiger sind als Beschriftungen, die mit bestehenden Methoden gesammelt wurden, gemessen an der Inter-Anonymen-Vereinbarung auf 100 doppelt beschrifteten Konversationen. Darüber hinaus sind ABC-Eval-Beschriftungen vorhersehbarer für die Gesamtqualität der Konversation im Vergleich zu Metriken, die mit bestehenden Methoden produziert wurden, wie durch diese einfache lineare Regressionsanalyse gezeigt. Zum Beispiel können Sie sehen, wie das Messen des Anteils der Züge mit Selbst- und Partnerwidersprüchen 5 % bzw. 10 % der Gesprächsqualität erklärt, während die durchschnittlichen Likert-Konsistenzscores nur 4 % oder weniger erklären. Schließlich haben wir überprüft, ob jede Bewertungsmetrik einen einzigartigen Aspekt der Chatqualität erfasst, unter Verwendung einer schrittweisen linearen Regression. Sie können sehen, wie die Kombination aller ABC-Eval-Metriken über 25 % der Gesprächsqualität erklärt, und wenn Sie die Metriken nacheinander entfernen, führt dies in den meisten Fällen zum Verlust einer beträchtlichen Menge an Informationen über die Qualität. Andererseits erklärt die Kombination aller Likert-Metriken auf der Zug-Ebene weit weniger der Qualität, und weniger dieser Metriken tragen einzigartige Informationen. Diese zuverlässigen, informativen und unterschiedlichen ABC-Eval-Metriken ermöglichen es uns, konversationelle KI mit einer höheren Auflösung zu bewerten, als es frühere Methoden können. Sie können sehen, dass in den Ergebnissen unseres Experiments mehrere Herausforderungen noch bestehen und präzise quantifiziert wurden. Zum Beispiel begehen die von uns getesteten Bots in etwa 20 % ihrer Antworten Verstöße gegen das gesunde Menschenverstandswissen. Sie produzieren in etwa 15 % der Antworten irrelevante Informationen, und sie widersprechen sich oder ihrem Partner in etwa 10 % der Zeit. Mit dem raschen Verbesserungstempo in diesem Bereich könnten viele dieser Fehlerraten bei neuen Modellen, die seit unserer Bewertung veröffentlicht wurden, abnehmen. Dies ist jedoch umso mehr ein Grund, zuverlässige und präzise Bewertungsmetriken zu verfolgen, um Modelle zu vergleichen. Wir hoffen, dass ABC-Eval von anderen in diesem Bereich als bedeutender Schritt in diese Richtung genutzt werden kann. Und wir freuen uns darauf zu sehen, wie sich die konversationelle KI in den kommenden Monaten und Jahren weiterentwickeln wird. Vielen Dank fürs Zuschauen.</sample>
    <sample id="270">Die Autoren gehören der Emory University an.</sample>
    <sample id="271">In this work, **CFT** stands for **Continuous Fine-Tuning**. It refers to a method where a model is allowed to continue fine-tuning on the clean validation samples after the initial training phase. This approach is compared to traditional Weakly Supervised Learning (WSL) methods, which use clean validation samples only for model selection.</sample>
    <sample id="272">Die Arbeit wurde von **sieben Autoren** verfasst: Koustav Sinha, John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy und Adina Williams.</sample>
    <sample id="273">Hallo, mein Name ist Kayo Yin und ich werde unsere Arbeit mit dem Titel „Wann erfordert Übersetzung Kontext? Eine datengesteuerte, mehrsprachige Untersuchung“ vorstellen. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig durchgeführt. Viele Übersetzungen hängen also vom Kontext ab. Zum Beispiel, wie würden wir „mole“ in diesem Satz übersetzen? Wenn der vorherige Satz „Die Dinge könnten gefährlich werden, wenn die Minister das herausfinden“ lautet, bezieht sich „mole“ auf einen Spion. Aber wenn der vorherige Satz „Könnte es etwas Ernstes sein, Doktor?“ lautet, bezieht sich „mole“ auf einen Muttermal. Abhängig vom Kontext ändert sich also die Bedeutung des Wortes und damit auch seine Übersetzung. Es ist jedoch ziemlich schwierig zu bewerten, wie gut Modelle solche Fälle übersetzen können. Erstens hängt nur ein kleiner Teil der Übersetzungen vom Kontext ab, was bedeutet, dass Korpus-Level-Metriken wie BLEU diese Übersetzungen nicht erfassen können. Einige Leute haben vorgeschlagen, gezielte Bewertungen von kontextabhängigen Übersetzungen durchzuführen, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Sprachsätze, da sie normalerweise auf Domänenkenntnissen und menschlicher Kuratierung basieren. In dieser Arbeit versuchen wir, diese zwei Fragen zu beantworten. Erstens, wann erfordert Übersetzung Kontext? Und zweitens, wie gut bewältigen Modelle diese Fälle? Um die erste Frage zu beantworten, haben wir begonnen, zu messen, wie stark ein Wort vom Kontext bei der Übersetzung abhängt. In früheren Arbeiten haben wir CXMI als Maß für die Kontextnutzung durch maschinelle Übersetzungssysteme eingeführt. Dies wird getan, indem gemessen wird, wie viel Information der Kontext C über das Ziel Y liefert, gegeben die Quelle X. Man kann sich CXMI als die Information vorstellen, die durch das Geben von Kontext an das Modell gewonnen wird. In dieser Arbeit erweitern wir CXMI zu Pointwise CXMI, das die Kontextnutzung auf Satzebene oder Wort-Ebene messen kann. Wörter mit hohem P-CXMI können als solche angesehen werden, die Kontext für die Übersetzung benötigen. Wir analysieren nun Wörter mit hohem P-CXMI, um Muster zwischen diesen Wörtern zu suchen. Und wir führen unsere Analyse auf Transkripten von TED-Talks durch, die von Englisch in 14 verschiedene Sprachen übersetzt wurden. Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Zuerst betrachten wir Teile des Sprachgebrauchs, die ein hohes durchschnittliches P-CXMI aufweisen. Dies ermöglicht es uns, zum Beispiel Dualpronomen im Arabischen zu finden, die relativ hohe P-CXMI-Werte aufweisen. Dies lässt sich erklären, weil Englisch keine Dualpronomen hat, sodass man Kontext benötigt, um zu bestimmen, ob ein Pronomen dual ist, wenn man ins Arabische übersetzt. Und ähnlich finden wir, dass bestimmte Sprachen auch Kontext erfordern, wenn wir die passende Verbform wählen möchten. Dann betrachten wir Vokabularartikel, die ein hohes P-CXMI im Durchschnitt aller ihrer verschiedenen Vorkommen aufweisen. Dies hilft uns, Fälle wie den hier zu identifizieren, wo man im Chinesischen Kontext benötigt, um Eigennamen zu übersetzen, um sicherzustellen, dass man innerhalb des Dokuments dieselbe Übersetzung verwendet. Und ähnlich finden wir, dass Kontext wichtig ist, um die richtige Form der Höflichkeit zu übersetzen. Und schließlich betrachten wir verschiedene einzelne Token, die ein hohes P-CXMI aufweisen. Dies ermöglicht es uns, Phänomene zu identifizieren, die mit dem Wort selbst nicht wirklich erfasst werden können, sondern eher in der Satzstruktur ausgedrückt werden, wie zum Beispiel die Auflösung von Ellipse. Jetzt verwenden wir unsere Erkenntnisse aus unserer Analyse, um einen Benchmark für die Dokument-Level-Übersetzung zu entwerfen. Für jedes der fünf identifizierten Diskurs-Phänomene erstellen wir Tagger, um Wörter automatisch zu identifizieren, die zum Phänomen gehören. Und wir nannten unseren Tagger den Multilingual Discourse-Aware, oder MuDA-Tagger. Wir können auch feststellen, dass verschiedene Sprachen unterschiedliche Anteile dieser Diskurs-Phänomene aufweisen. Wir verwenden dann den MuDA-Tagger, indem wir den Tagger auf einen Parallelkorpus anwenden, den wir für die Bewertung verwenden möchten, und wir wenden unsere Übersetzungsmessungen der Wahl auf die kontextabhängigen Beispiele an, die der MuDA-Tagger identifiziert hat. Und schließlich verwenden wir unseren Benchmark sowie andere Metriken, um verschiedene Modelle auf der Dokument-Level-Maschinentranslation zu bewerten. Zunächst einmal, wenn wir Korpus-Level-Metriken verwenden: So finden wir bei BLEU, dass kontextunabhängige Modelle die beste Leistung erzielen. Aber dann, wenn wir COMET verwenden, erzielen kontextbewusste Modelle die beste Leistung. Und wenn wir Wort-f-Maß verwenden, dann haben Modelle mit und ohne Kontext vergleichbare Leistungen. Dies zeigt erneut, dass es schwierig ist, das beste Dokument-Level-Übersetzungssystem zu bestimmen, wenn wir nur Korpus-Level-Metriken verwenden. Jetzt verwenden wir den MuDA-Benchmark, um Modelle zu bewerten, und wir finden, dass kontextbewusste Modelle signifikant genauer sind als Modelle, die keinen Kontext für bestimmte Diskurs-Phänomene wie Höflichkeit und lexikalische Kohäsion verwenden. Aber diese Modelle sind nicht viel besser als Modelle, die keinen Kontext für andere Phänomene wie Ellipse, Pronomen und Verbform verwenden. Dies legt nahe, dass wir Fortschritte bei der Dokument-Level-Übersetzung sehen müssen. Wir haben auch verschiedene kommerzielle Systeme verglichen und unser Benchmark zeigt, dass DeepL in der Regel genauer als Google Translate für die Dokument-Level-Übersetzung ist. Zusammenfassend führen wir eine datengesteuerte Analyse über 14 Sprachpaare durch, um zu identifizieren, wann Übersetzungen Kontext erfordern, und dann verwenden wir unsere Erkenntnisse, um einen Benchmark für die Dokument-Level-Maschinentranslation zu erstellen, der uns helfen kann, zu identifizieren, welche Diskurs-Phänomene Modelle gut oder nicht bewältigen können, und welche Übersetzungssysteme gut in der Dokument-Level-Übersetzung sind. Vielen Dank für Ihre Aufmerksamkeit. Bis in Toronto.</sample>
    <sample id="274">Der Referent heißt Yusen Zhang.</sample>
    <sample id="276">Ananya and Vignesh present "IndicMT Eval," a dataset designed to meta-evaluate machine translation (MT) metrics for Indian languages, addressing the understudied area of reverse translation evaluation. The dataset focuses on five Indian languages: Tamil and Malayalam (Dravidian) and Hindi, Marathi, and Gujarati (Indo-Aryan). Using the Flores dataset, 200 sentences were selected, translated into English via seven MT models or APIs, yielding 7,000 samples. Human bilingual annotators evaluated these translations, marking errors (accuracy/meaning, fluency, special categories) and providing overall scores based on the MQM framework.

The study compares MT models' performance, with Indic Trans and NLLB outperforming older models like CVIT. Correlation analysis reveals that overlap-based metrics (e.g., chrF) perform poorly, while embedding-based metrics like LabSE and BERTscore show better correlations. COMET-metric variants exhibit the highest overall correlations. However, many metrics score narrowly, limiting their interpretability.

Error-type-specific analysis shows higher correlations for accuracy errors. The COMET metric was fine-tuned using the MQM dataset, resulting in IndicCOMET variants outperforming COMET baselines on three out of five languages. Zero-shot testing on unseen languages confirmed IndicCOMET's superiority. Robustness evaluation on the ACES Translation Accuracy Challenge Sets further validated IndicCOMET's effectiveness. The dataset and findings aim to improve MT evaluation for Indian languages.</sample>
    <sample id="277">Die neue Methode hat keinen spezifischen Namen, der im Inhalt erwähnt wird. Sie wird lediglich als "Multiset Tagging and Latent Permutations" bezeichnet.</sample>
    <sample id="278">Die Autoren beschreiben die Methode der „markierten Wörter“ als eine Methode, die auf dem soziolinguistischen Konzept der „Markierung“ basiert. Dabei wird zwischen unmarkierten (dominanten) und markierten (marginalisierten) Gruppen unterschieden. Die Methode identifiziert Wörter, die markierte Gruppen von unmarkierten unterscheiden, indem sie gewichtete Log-Odds-Verhältnisse verwendet, um die Top-Wörter für jede markierte Gruppe im Vergleich zu unmarkierten Gruppen zu analysieren. Dies ermöglicht die Erkennung spezifischer Stereotype und essentialisierender Narrative in den generierten Personas.</sample>
    <sample id="279">Die Autoren gehören der University of Washington an.</sample>
    <sample id="280">Shi Tao presents "MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations," addressing challenges in Emotion Regulation in Conversations (ERC). ERC aims to predict the emotion of each utterance in a dialogue, integrating textual, audio, and visual modalities. Existing methods often focus on textual data or simple feature concatenation, failing to exploit multimodal complementarity, underperforming on minority emotions, and struggling with distinguishing semantically similar emotions.

MultiEMO addresses these issues through four key components: unimodel feature extraction, context modeling, multimodal fusion, and emotion classification. Its contributions include:

1. **VisExtNet**: A novel visual feature extractor that focuses on facial expressions rather than redundant scene information, using MTCNN and VGGFace2 pre-trained ResNet-101.
2. **MultiAttn**: A multimodal fusion model based on bidirectional multi-head cross-attention layers, integrating textual, audio, and visual modalities effectively.
3. **Sample-Weighted Focal Contrast Loss (SWFC)**: A loss function that emphasizes hard-to-classify minority emotions and maximizes inter-class distances, improving the distinction between semantically similar emotions.

Experiments on MELD and IEMOCAP datasets demonstrate MultiEMO’s state-of-the-art performance, particularly in minority and semantically similar emotions. However, limitations include challenges in speaker identification, computational requirements for SWFC, and ongoing performance gaps in minority emotions. Overall, MultiEMO significantly advances ERC by leveraging multimodal correlations and addressing key challenges.</sample>
    <sample id="281">The presentation titled "When Does Translation Require Context? A Data-driven, Multilingual Exploration" explores the role of context in machine translation. Co-authored by Kayo Yin, Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig, the study examines how context influences translation accuracy and evaluates models' ability to handle context-dependent translations.

The research highlights that context is crucial for translating words like "mole," which can have different meanings depending on the preceding text. However, traditional metrics like BLEU fail to capture such context-dependent translations, making targeted evaluation challenging. The study introduces Pointwise Contextual Usage Measure (P-CXMI) to quantify context usage at the sentence or word level, identifying words that require context for accurate translation.

Analyzing TED talk transcripts in 14 languages, the authors find that context is essential for translating dual pronouns in Arabic, verb forms, proper nouns in Chinese, and formality. They develop the Multilingual Discourse-Aware (MuDA) tagger to identify context-dependent words and create a benchmark for document-level translation evaluation.

The benchmark reveals that context-aware models outperform context-agnostic ones in handling phenomena like formality and lexical cohesion but struggle with others like ellipsis and pronouns. Commercial systems like DeepL generally outperform Google Translate in document-level translation. The study underscores the limitations of corpus-level metrics and advocates for discourse-aware benchmarks to better evaluate machine translation systems.</sample>
    <sample id="282">**Abstract:**  
At ACL 2023, we present *StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing*, addressing the challenge of story-level style transfer in non-parallel text. Unlike prior work focusing on token or sentence-level style transfer, *StoryTrans* tackles discourse-level linguistic preferences and style-specific content preservation. The primary challenges include imitating author style at the discourse level and transferring style-specific content to another topic. To address these, we propose a two-stage generation model: (1) transferring style-specific content keywords masked and (2) generating the full text by incorporating these keywords explicitly. Our model learns discourse representations and combines them with style embeddings. We use an advisory training framework with self-reconstruction, disentanglement, sentence order, and style classifier losses for the first stage, and a content-focused stage for the second. Experiments on Chinese and English datasets demonstrate *StoryTrans* outperforms baselines in style control and content preservation, with manual evaluations confirming its effectiveness. *StoryTrans* enriches storylines and maintains semantics, aligning with golden texts in the style feature space. Code and data are available in the repository.</sample>
    <sample id="283">Prag</sample>
    <sample id="284">In this paper, we introduce FSUIE (Fuzzy Span Universal Information Extraction), a novel approach to enhance information extraction (IE) by addressing the limitations of current span-based models. Traditional models rely heavily on precise span boundaries, which can be ambiguous and lead to suboptimal performance. FSUIE introduces a fuzzy span mechanism to learn span boundaries as continuous distributions, allowing for more flexible and accurate extraction.

To achieve this, we propose a fuzzy span loss that combines Binary Cross Entropy (BCE) with Kullback-Leibler (KL) divergence, enabling the model to learn from both precise and fuzzy span boundaries. Additionally, we introduce adaptive fuzzy span attention, which dynamically adjusts the attention span and linearly decays its intensity towards the boundaries. This allows the model to focus on relevant semantic information within a limited range of tokens.

Our experiments on named entity recognition, relationship extraction, and aspect sentiment triplet extraction demonstrate significant improvements over baseline models. FSUIE achieves state-of-the-art results on several datasets, including ACE2004, 2005, and ADE, and shows strong generalization capabilities. The fuzzy span attention layer, added to the top level of the model, enhances the decision-making process without affecting text encoding.

In summary, FSUIE addresses the limitations of traditional span-based models by introducing a fuzzy span mechanism and adaptive attention, resulting in improved performance and flexibility in information extraction tasks.</sample>
    <sample id="285">Mingqi Gao von der Peking University präsentiert die Arbeit „Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework“. Das Team untersucht die Korrektur von Faktenfehlern in Dialogzusammenfassungen, die oft in Modellen und Referenzzusammenfassungen auftreten. Es gibt zwei Ansätze: (1) Integration von faktualitätsbezogenen Zielen in das Training oder die Inferenz, um korrektere Zusammenfassungen zu erzeugen, und (2) ein eigenständiges Modell zur Faktenerrekturierung (FEC), das die Quelle und das Modell-Summary korrigiert. Bisher gab es keine Arbeiten zu Faktenerrekturierung in Dialogzusammenfassungen, obwohl dies wichtig ist. Die Forscher kritisieren, dass bestehende FEC-Modelle oft mit vagen Faktualitätsmetriken wie FactCC und DAE evaluiert werden, die keine zuverlässigen Ergebnisse liefern und die Grenze zwischen den beiden Lösungsansätzen verwischen. Sie schlagen vor, manuell annotierte Referenzkorrekturen zu verwenden, um die Leistung von FEC-Modellen genauer zu bewerten. Sie entwickeln eine neue Taxonomie von Faktenerren, die in inhaltliche (z. B. Wortart, Abhängigkeiten) und formale (Addition, Deletion, Substitution) Kategorien unterteilt wird. Ihr Evaluation-Framework basiert auf ERRANT und besteht aus drei Schritten: Ausrichtung, Klassifizierung und Vergleich. Experimente zeigen, dass FEC-Modelle, die mit Referenzzusammenfassungen aus Dialogdaten trainiert werden, bessere Ergebnisse erzielen als Modelle, die nur mit synthetischen Daten trainiert werden. Die Forscher betonen die Notwendigkeit, menschliche Korrekturen in das Training von FEC-Modellen zu integrieren und kombinieren menschliche Daten mit synthetischen Daten. Sie identifizieren auch Schwächen der aktuellen Modelle, wie die Unfähigkeit, Additionsfehler oder Attribute, Modalitäten und Verlinkungsfehler zu korrigieren.</sample>
    <sample id="286">James Finch und Sarah Finch</sample>
    <sample id="287">Die Arbeit wurde von vier Autoren verfasst: Javad Hosseini, Filip Radlinski, Silvia Pareti und Annie Louis.</sample>
    <sample id="288">Zum Testen syntaktischer Phänomene können Datensätze wie **BLiMP** (Berkeley Linguistic Parsing Model) und **SyntaxGym** verwendet werden. Diese Datensätze enthalten minimal pair-Paradigmen, die es ermöglichen, die Fähigkeit von Sprachmodellen zu bewerten, zwischen akzeptablen und inakzeptablen Sätzen zu unterscheiden, insbesondere in Bezug auf syntaktische Strukturen.</sample>
    <sample id="290">Die Abkürzungen der fünf Methoden für die erste Forschungsfrage sind: FTw, COSINE, Snorkel, Snorkel-C, und Snorkel-C-F.</sample>
    <sample id="291">Das Modell wird anhand der folgenden Aufgaben evaluiert:

1. Named Entity Recognition (NER)
2. Klassifikation
3. Teil-des-Sprechens-Kennzeichnung (Part-of-Speech Tagging)
4. Fragebeantwortung (Question Answering)</sample>
    <sample id="294">CamemBERT wurde ursprünglich mit einem 138 GB großen Datensatz trainiert.</sample>
    <sample id="295">Der Referent heißt Adam Przepiórkowski.</sample>
    <sample id="296">Valerio Basile präsentiert eine Forschungsarbeit, die aus einer Zusammenarbeit zwischen der Universität Turin und Amazon Alexa hervorgegangen ist. Das Projekt konzentriert sich auf die Erkennung von Ironie in natürlicher Sprache, ein komplexes Phänomen, das für moderne NLP-Modelle eine Herausforderung darstellt. Anstatt binäre Etiketten (ironisch/nicht ironisch) zu verwenden, zielen die Forscher darauf ab, Modelle zu entwickeln, die informativere Ausgaben liefern. Dazu wurde der EPIC-Korpus (English Perspectivist Irony Corpus) erstellt, der auf Daten aus sozialen Medien wie Reddit und Twitter basiert und fünf Varianten des Englischen abdeckt. Über 74 Annotatoren bewerteten 300 kurze Konversationen, was durchschnittlich zu fünf Annotationen pro Konversation führte. Die Ergebnisse zeigen, dass die Annotatoren in ihren Bewertungen stark variieren, abhängig von Faktoren wie Geschlecht, Alter und Nationalität. Die Forscher entwickelten „perspektivistische“ Modelle, die die Unterschiede zwischen den Annotatoren berücksichtigen, und stellten fest, dass diese Modelle im Vergleich zu aggregierten „Goldstandard“-Modellen selbstsicherer in ihren Vorhersagen sind. Eine besondere Beobachtung war, dass jüngere Generationen häufiger uneins in der Ironiewahrnehmung sind, ebenso wie Annotatoren aus geografisch benachbarten Regionen wie dem Vereinigten Königreich und Irland. Die Arbeit unterstreicht die Bedeutung der menschlichen Perspektive in der NLP-Forschung und die Herausforderungen bei der Modellierung von Ironie.</sample>
    <sample id="297">Die Arbeit "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models" untersucht die Verwendung von Dogwhistles, einer Form von kodierter Rhetorik, die subtile, oft diskriminierende Botschaften an bestimmte Gruppen sendet, während sie für Außenstehende harmlos erscheinen. Ein Beispiel ist der Senator Josh Hawley, der den Begriff "kosmopolitan" verwendet, um implizit auf jüdische Menschen zu verweisen, ohne dies explizit zu sagen. Dogwhistles sind in politischer Rhetorik weit verbreitet, um Hass und Abneigung zu schüren, während sie gleichzeitig Content-Moderation-Systemen entgehen.

Die Studie entwickelt eine Typologie und ein Glossar mit über 340 Begriffen und Symbolen, die rassistische, transphobe und antisemitische Dogwhistles erfassen. Diese werden in Kategorien wie Register (formell oder informell), Typ (zusätzliche Implikatur oder Persona-Signal) und Persona (z.B. antisemitisch oder transphob) klassifiziert. Eine Fallstudie historischer US-Politikschreine zeigt, dass die Verwendung von rassistischen Dogwhistles mit der konservativen Agenda und der Southern Strategy seit der Bürgerrechtsbewegung zunahm.

Die Forscher evaluierten auch die Fähigkeit von Sprachmodellen wie GPT-3, Dogwhistles zu erkennen und ihre verdeckten Bedeutungen zu entschlüsseln. Die Ergebnisse zeigen, dass GPT-3 formelle Dogwhistles besser erkennt, aber bei informellen und transphoben Begriffen sowie bei der Identifikation von verdeckten Bedeutungen schwächelt. Schließlich wurde gezeigt, dass Dogwhistles Content-Moderation-Systeme umgehen können, da sie in denselben Sätzen, in denen Schimpfwörter verwendet werden, als weniger toxisch eingestuft werden. Die Studie unterstreicht die Bedeutung der Erforschung von Dogwhistles für NLP, Linguistik und die Bekämpfung von Hassrede.</sample>
    <sample id="298">The results that led to the conclusion that temporal drift is the main cause of the performance drop were obtained from an experiment where the researchers retrained or continued to pre-train some models with more recent data. They found that the performance of these models degraded as the temporal gap between the training and testing data increased. This confirmed their hypothesis that temporal drift, rather than adaptive overfitting, is the primary reason for the performance decline. Specifically, the red best fit line in the graph showed a gradient greater than one, indicating that the improvements made on the CoNLL-2003 dataset did not translate proportionally to improvements on the CoNLL++ dataset, which ruled out adaptive overfitting. The degradation in performance with larger temporal gaps, however, supported the temporal drift hypothesis.</sample>
    <sample id="299">In this work, we address the issue of robustness in Natural Language Inference (NLI) models, which often rely on shortcuts—spurious correlations in the training data—to achieve high performance on in-distribution samples but fail on out-of-distribution adversarial tests. Existing shortcut mitigation methods require domain-specific knowledge and often assume the learner will exploit the same shortcuts as the auxiliary, which is not always the case. To overcome these limitations, we propose a minimax training approach. The learner aims to minimize NLI task loss, while the auxiliary seeks to maximize the learner's loss by generating example weights that focus the learner on high-loss regions. This incentivizes the learner to prioritize under-represented hard examples that counteract shortcuts present in easy examples. Both models are optimized alternately using standard optimization algorithms. At test time, the learner operates independently of the auxiliary. We evaluate this method on three analytic datasets (MNLI, FEVER, QQP) and their adversarial test sets (HANS Symmetric, PAWS), observing consistent improvements in out-of-distribution performance while maintaining high in-distribution accuracy. Our approach is dataset-agnostic, relying on the learner's training dynamics, and uses a feed-forward network for the auxiliary. We also explore the impact of pre-training the learner, the size of the auxiliary, and conduct a qualitative analysis of the learned example weight distribution. This work aims to enhance the robustness of NLI models without relying on auxiliary models or specific shortcut knowledge.</sample>
    <sample id="300">Belinda presents a new task called **interactive dictation**, which enables users to dictate and edit documents naturally using voice commands. Unlike traditional speech-to-text systems, interactive dictation allows users to correct mistakes, add text, and issue commands (e.g., "Replace 'the event' with 'it'") without memorizing fixed templates. The system distinguishes between dictation and commands, leveraging natural language understanding to execute edits intuitively.

The task involves four steps: (1) ASR recognition of raw audio, (2) segmentation of utterances into dictation and commands, (3) normalization and correction of ASR errors and speech mistakes, and (4) sequential execution of dictation and commands to produce the final document. To formalize the task, Belinda and her collaborators designed a data collection interface, allowing users to dictate and issue commands in real-time, which were then annotated.

A baseline system was developed, using separate models for ASR, segmentation, repair, and interpretation. Two architectures, T5 and GPT-3, were tested, with GPT-3 showing higher accuracy but slower runtime. Predicting the final state directly was found to be more accurate than predicting intermediate programs, though T5 models performed better when predicting programs, balancing efficiency and accuracy.

The work introduces a new dataset and a formal task definition, aiming to improve natural voice-based editing. The released code and paper provide resources for further research in this area.</sample>
    <sample id="302">It is necessary to permute the tokens for the output sequence because, after the first step of tagging each input token with an unordered multiset of tokens that will appear in the output, the tokens are not yet in their correct order. The permutation step is required to arrange these tokens into the correct sequence that corresponds to the logical form or output structure. This step is crucial for capturing the compositional process and ensuring that the output accurately reflects the input's meaning, especially in cases of deeper recursion and unseen compositions.</sample>
    <sample id="303">Die Autoren empfehlen, dass Modellentwickler*innen ihre Methoden zum Abbau von Vorurteilen transparenter machen sollten, weil:

1. **Positive Stereotype und Essentialisierende Narrative**: Es unklar ist, ob positive Stereotype und essentialisierende Narrative durch eine übermäßige Wertanpassung oder andere Anti-Stereotypie-Methoden entstehen, die wiederum schädliche Muster erzeugen.
2. **Forschungslücken**: Ohne Transparenz können Forscher*innen diese Phänomene nicht weiter untersuchen und verstehen.
3. **Intersektionale Betrachtung**: Es wichtig ist, Vorurteile und Schäden durch eine intersektionale Linse zu betrachten, um übersehene Aspekte zu vermeiden.</sample>
    <sample id="304">In the context of the ACL 2023 paper, **unacceptable minimal pair inputs** are sentences or sentence pairs that are grammatically incorrect or violate syntactic rules, such as those identified in datasets like BLiMP or SyntaxGym. These inputs are used to test a language model's ability to distinguish between acceptable and unacceptable sentences, particularly in longer context windows.</sample>
    <sample id="305">In their recent work, "Weaker Than You Think: A Critical Look at Weakly Supervised Learning," Dawei and his collaborators challenge the common assumption that weakly supervised learning (WSL) methods can achieve high performance solely on weakly labeled data. The study reveals that many WSL approaches actually require clean, manually annotated validation samples to function effectively. Without these clean samples, models fail to generalize beyond the noisy weak labels, rendering the training process ineffective.

The research also highlights that increasing the number of clean validation samples improves performance, with as few as 20 samples per class often being sufficient. Moreover, fine-tuning models directly on clean samples can achieve even better results than traditional WSL methods. This suggests that the performance gains claimed by WSL approaches may be overestimated, as they often rely on the implicit use of clean data for model selection.

The authors recommend that future WSL research should be more transparent about model selection criteria, compare WSL methods with few-shot learning baselines, and consider continuous fine-tuning as a strong baseline. They have open-sourced their code to facilitate further exploration of these issues. In summary, the study underscores the importance of clean annotations in WSL and calls for more realistic evaluations of its capabilities.</sample>
    <sample id="306">Sebastian Schuster und Najoung Kim untersuchen in ihrer Arbeit die Fähigkeit von Sprachmodellen, Entitäten zu verfolgen und deren Zustandsänderungen in Diskursen zu verstehen. Dies ist eine entscheidende Fähigkeit für ein tieferes Verständnis von Texten, insbesondere bei längeren Diskursen. Die Forscher stellen fest, dass bisher keine systematischen Untersuchungen dazu durchgeführt wurden, wie gut vorgetrainierte Sprachmodelle diese Aufgabe bewältigen können. Ihre zentrale Forschungsfrage lautet: Inwiefern können große Sprachmodelle Entitäten verfolgen?

Die Evaluation gestaltet sich jedoch schwierig, da viele Modelle auf unklaren Datensätzen trainiert wurden und Diskurse komplexe Muster aufweisen. Um die tatsächlichen Fähigkeiten der Modelle zu testen, entwarfen Schuster und Kim eine Aufgabe mit Boxen und Objekten. Die Aufgabe besteht darin, den Inhalt von Boxen basierend auf einer anfänglichen Beschreibung und mehreren Zustandsänderungen korrekt vorherzusagen. Die Forscher implementierten verschiedene Maßnahmen, um zu verhindern, dass die Modelle auf Heuristiken oder externen Informationen zurückgreifen.

Die Ergebnisse zeigen, dass die meisten getesteten Modelle (z. B. GPT-3 und T5) die anfängliche Zustandsbeschreibung wiederholen, anstatt die Änderungen zu berücksichtigen. Nur das Modell text-davinci-003 zeigte ein nennenswertes Tracking-Verhalten. Die Forscher fanden heraus, dass Modelle, die auf Code trainiert wurden (wie GPT-3.5), besser abschnitten, was darauf hindeutet, dass Code-Training die Fähigkeit zur Entitätstracking fördert. Kleinere Modelle wie T5-Base können diese Fähigkeit durch gezieltes Fine-Tuning erlernen, während neu initialisierte Modelle derselben Architektur dies nicht können. Die Forscher betonen jedoch, dass unklar ist, ob diese Fähigkeiten auf andere Szenarien übertragbar sind. Ihre vollständige Studie ist auf arXiv verfügbar.</sample>
    <sample id="307">The authors used **F1 scores** and **accuracy** as the primary evaluation metrics to assess the performance of the models on the 11 downstream tasks. These metrics were applied across various tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. Additionally, they compared the models against six baseline models using these same metrics to evaluate overall performance.</sample>
    <sample id="308">Jenny, a PhD student at Carnegie Mellon University, presents research on **NLPositionality**, which explores design biases in NLP datasets and models. The study highlights how these systems can exhibit positionality, reflecting the perspectives and biases of their creators and the populations they represent. For example, the Prospective API performs well for English-speaking users like Carl Jones but fails for Indian users like Aditya Sharma, showcasing cultural gaps in toxicity detection. Positionality arises from the aggregated judgments of real people, which can skew datasets and models toward certain demographics while neglecting others.

The research framework, **NLPositionality**, re-annotates datasets with diverse annotators from various demographics and compares these annotations to existing models and datasets using Pearson's R correlation. This approach differs from traditional annotator disagreement studies by focusing on end-user alignment rather than annotator distributions. The study leveraged **Lab in the Wild**, an online crowdsourcing platform, to recruit over 1,000 annotators from 87 countries, gathering over 16,000 annotations for social acceptability and hate speech detection tasks.

Findings reveal that datasets and models are most aligned with English-speaking countries and individuals with higher education levels. However, they are less aligned with non-binary individuals, highlighting systemic biases. To address these issues, the researchers recommend documenting design choices, adopting a perspectivist lens in NLP research, and creating specialized datasets and models within underrepresented communities. Inclusive NLP, they emphasize, is not just about universal functionality but about addressing and mitigating biases to ensure fairness and representation.</sample>
    <sample id="309">Die Metrik, die verwendet wurde, um die Übereinstimmung zwischen den Kommentatoren zu messen, ist die **Inter-Annotator Agreement** (Übereinstimmung zwischen den Annotatoren). Diese wurde auf 100 doppelt annotierten Gesprächen gemessen.</sample>
    <sample id="310">Wikipedia wurde gewählt, um völlig unzusammenhängende Sätze zu den inakzeptablen und akzeptablen Suchanfragen hinzuzufügen.</sample>
    <sample id="311">Die Autoren gehören der Universität zu, die nicht explizit im Text genannt wird. Basierend auf dem Kontext und der Präsentation, die auf einer Konferenz gehalten wird, könnte es sich um eine deutsche Universität handeln, da das Thema und die Sprache (Deutsch) darauf hindeuten. Ohne weitere Informationen kann jedoch keine spezifische Universität genannt werden.</sample>
    <sample id="312">MultiInstruct unterscheidet sich von anderen Benchmarks durch die Schaffung des ersten groß angelegten multi-modalen Instruction Tuning-Datensatzes. Dieser umfasst 62 diverse multi-modale Aufgaben aus 10 Kategorien, die aus 21 bestehenden Open-Source-Datensätzen abgeleitet wurden. Jeder Aufgabe sind fünf von Experten verfasste Anweisungen beigefügt. Im Gegensatz zu bestehenden Datensätzen, die hauptsächlich auf Sprache basieren, schließt MultiInstruct auch multi-modale Aufgaben ein, was es zu einem umfassenderen Benchmark für die Verbesserung der Generalisierung von multi-modalen Vorhersagemodellen macht.</sample>
    <sample id="313">Zwei Autoren: James Finch und Sarah Finch.</sample>
    <sample id="314">Die Definition der binären Koordination, basierend auf dem gegebenen Inhalt, bezieht sich auf die Struktur von Koordinationsverbindungen, die zwei Elemente (Conjuncts) verbinden. In der binären Koordination sind die beiden Conjuncts durch eine Konjunktion (z.B. "and", "or") verbunden. Die Diskussion im Vortrag konzentriert sich auf die verschiedenen Ansätze zur Bestimmung der Abhängigkeitstruktur (Dependency Structure) innerhalb dieser Koordinationsverbindungen, insbesondere ob die Abhängigkeit von einem der beiden Conjuncts ausgeht oder ob beide Conjuncts gleichberechtigt als Köpfe (Heads) der Struktur fungieren.

Die binäre Koordination kann in verschiedenen Theorien und Ansätzen unterschiedlich interpretiert werden:

1. **Asymmetrische Ansätze**: In diesen Ansätzen wird eine bestimmte Reihenfolge oder ein bestimmter Conjunct als Kopf der gesamten Koordinationsstruktur festgelegt. Zum Beispiel in der Universal Dependencies und in Igor Mel'čuk's Meaning Text Theory, wo der erste Conjunct als Kopf der gesamten Struktur gilt.

2. **Konjunktions-gehörte Ansätze**: Hier wird die Koordinationsverbindung als Ganzes durch die Konjunktion selbst als Kopf betrachtet. Dies ist der Ansatz, der in den Prager Abhängigkeitsbaumkorpora verwendet wird.

3. **Multi-köpfige Ansätze**: In diesem Ansatz werden beide Conjuncts als Köpfe der Koordinationsstruktur betrachtet. Dies ist der Ansatz, der in Hudson's Word Grammar verwendet wird.

Die Diskussion im Vortrag argumentiert für eine **symmetrische Struktur** der binären Koordination, basierend auf dem Prinzip der Abhängigkeitslängeminimierung (Dependency Length Minimization). Dieses Prinzip besagt, dass kürzere Abhängigkeiten bevorzugt werden. Die Analyse von Beispielen zeigt, dass die Anordnung der Conjuncts in einer Koordinationsverbindung die Länge der Abhängigkeiten minimieren kann, was die Grammatikalität und die Verständlichkeit der Sätze verbessert.

Zusammenfassend lässt sich sagen, dass die binäre Koordination in der Linguistik die Verbindung von zwei Elementen durch eine Konjunktion beschreibt, und die Diskussion um ihre Struktur konzentriert sich darauf, ob die Abhängigkeit von einem der Conjuncts ausgeht oder ob beide Conjuncts gleichberechtigt sind. Die Argumente im Vortrag sprechen für eine symmetrische Struktur, bei der beide Conjuncts als Köpfe der Koordinationsverbindung fungieren können.</sample>
    <sample id="315">Die in dieser Studie verwendeten Prompts waren im Durchschnitt 10 Wörter lang.</sample>
    <sample id="316">Die Ergebnisse zeigen, dass das kleinere T5-Modell, wenn es auf dem CoScript-Datensatz trainiert wird, in der Lage ist, qualitativ hochwertigere Skripte für eingeschränktes Sprachplanen zu generieren als die meisten großen Sprachmodelle. Dies deutet darauf hin, dass kleinere Modelle, wenn sie auf geeigneten Datensätzen trainiert werden, die Leistung größerer Modelle übertreffen können.</sample>
    <sample id="317">Peng Li from Fudan University presents the work "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors," addressing challenges in information extraction (IE) tasks like named entity recognition (NER) and relation extraction (RE). Traditional models, such as T5 and GPT-3, operate in a text-to-text manner during pre-training but struggle during inference due to mismatched outputs (structured vs. plain text). This mismatch requires extensive structured training data and decoding strategies, often leading to errors.

CodeIE tackles this issue by transforming IE into a structure-to-structure code generation task, leveraging code-trained large language models like Codex. This approach ensures aligned input and output structures, improving performance. For NER, a code-style prompt is designed, defining a function to extract entities from input text. For RE, similar prompts are used.

Evaluations on datasets for NER and RE show that CodeIE, using Codex and code-style prompts, significantly outperforms traditional models like T5 and GPT-3. Key findings include:

1. **Perplexity**: Code format inputs yield lower perplexity for models like CodeT5 compared to text format inputs for T5.
2. **Structural Errors**: Code format prompts with Codex minimize structural errors, unlike GPT-3 with text format prompts.
3. **Label Expansion**: GPT-3 often outputs labels not in the predefined set, while Codex performs better overall.
4. **Recall**: Code format prompts enhance recall compared to text format prompts.

Overall, CodeIE demonstrates that transforming IE into a code generation task and using code-trained models improves performance and reduces errors. The work provides valuable insights for future research in IE.</sample>
    <sample id="318">Hallo, ich bin Yanis Labrak und werde Ihnen unsere Arbeiten zu „DrBERT: Ein robuster vortrainierter Modell für französische biomedizinische und klinische Domänen“ vorstellen. In dieser Präsentation sprechen wir zunächst über Sprachmodellierung im Gesundheitswesen. Anschließend stellen wir den Hauptbeitrag unseres Artikels vor. Wir führen das erste biomedizinische Modell in französischer Sprache ein, DrBERT, das auf RoBERTa basiert und mit NACHOS trainiert wurde, einem Datensatz von medizinisch crawlierten Daten aus dem Web. Wir haben auch einen Vergleich von Modellen mit mehreren Vortrainings-Einstellungen und Datenquellen durchgeführt. Anschließend stellen wir unsere Ergebnisse auf 11 biomedizinischen und klinischen Downstream-Aufgaben in französischer Sprache vor. Abschließend ziehen wir eine Bilanz der Experimente und geben Ihnen weitere Details darüber, wie Sie auf diese Modelle zugreifen können. Seit seiner Veröffentlichung im Jahr 2018 ist BERT zu einem der effektivsten Ansätze zur Lösung von Aufgaben der natürlichen Sprachverarbeitung geworden und bietet im Vergleich zu historischen statischen und kontextualisierten Methoden wie Word2vec, fastText oder mehr erhebliche Leistungsgewinne. Seitdem wurde dieses Modell auf viele andere Sprachen angepasst, wie in französischer Sprache mit CamemBERT, und auch in Domänen wie biomedizinisch mit PubMedBERT und BioBERT und klinisch mit ClinicalBERT, jedoch hauptsächlich in englischer Sprache. Spezialisierte Modelle für andere Sprachen sind selten und basieren oft auf kontinuierlichem Vortraining aufgrund des Mangels an domänenspezifischen Daten. Französisch hatte jedoch bis jetzt kein Open-Source-Modell für biomedizinische Zwecke. Wir stellten uns also die Frage, welche Datenquellen für eine breite Nutzung am geeignetsten sind und ob diese crawlierten Daten eine gute Alternative für klinische Daten darstellen. Um diese Frage zu beantworten, vergleichen wir DrBERT mit unserem ChuBERT-Modell, das auf anonymisierten Daten basiert, die aus dem Datenlager des Universitätskrankenhauses Nantes stammen. Anschließend stellten wir uns die Frage, wie viel Daten benötigen wir, um ein spezialisiertes Modell für französische Daten zu trainieren? Sind es 4 Gigabyte, 8 Gigabyte oder mehr? Um diese Frage zu beantworten, trainierten und verglichen wir zunächst vier von-Grund-auf-trainierte Modelle: eine erste Version von DrBERT mit 7 GB NACHOS; eine zweite Version von DrBERT mit 4 GB NACHOS; eine erste Version von ChuBERT, einem klinischen Modell mit 4 GB Sätzen aus klinischen Notizen; und eine endgültige Version von ChuBERT mit einer Mischung aus 4 GB NACHOS und 4 GB klinischen Notizen. Zusätzlich zu diesem Vergleich führten wir drei Modelle ein, die auf kontinuierlichem Vortraining basieren, um die Auswirkungen der Vortrainingsstrategie zu analysieren. Eines basiert auf dem Gewicht von CamemBERT und wurde mit 4 GB NACHOS trainiert. Ein weiteres basiert ebenfalls auf CamemBERT, wurde jedoch dieses Mal mit 4 GB klinischen Notizen trainiert, und schließlich eines, das auf dem englischen biomedizinischen Modell PubMedBERT basiert und mit 4 GB NACHOS trainiert wurde. Insgesamt haben wir sieben Modelle. Um unsere sieben Modelle zu bewerten, sammelten wir Daten für öffentliche und private Downstream-Aufgaben wie Named Entity Recognition, Klassifizierung, Teil-des-Sprechens-Kennzeichnung und Fragebeantwortung. Diese Modelle wurden mit sechs Basismodellen verglichen, die CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT und ClinicalBERT sind. Die Bewertung zeigt, dass die Modelle am besten bei der Aufgabe mit Daten der gleichen Natur abschneiden, auf denen das Modell trainiert wurde. Wir können jedoch beobachten, dass Daten aus heterogenen Quellen vielseitiger zu sein scheinen. Wir beobachten auch, dass mehr Daten zu besseren Ergebnissen führen. Insgesamt scheinen von-Grund-auf-trainierte Modelle auf den meisten Aufgaben die besten Ergebnisse zu erzielen. Unser Experiment zum Kontrollvortraining mit dem Gewicht und der Tokenisierung von CamemBERT, das mit dem 4-GB-Teil von NACHOS trainiert wurde, zeigte jedoch vergleichbare Ergebnisse wie DrBERT 4 GB von-Grund-auf. Dies ist nicht der Fall für das Modell, das auf CamemBERT-Gewichten und -Tokenisierung basiert, das unter Stabilitätsproblemen leidet. Abschließend bietet unser System auf neun der 11 Downstream-Aufgaben bessere Ergebnisse und übertrifft global die Ergebnisse des generischen Modells, hier CamemBERT. Wir beobachten auch, dass spezialisierte Daten besser sind, aber nicht gut skalieren. Alle aus NACHOS gewonnenen vortrainierten Modelle sind kostenlos auf Hugging Face verfügbar und unter der MIT-Lizenz, und alle Trainingsskripte befinden sich in unserem GitHub-Repository. Vielen Dank für diese Präsentation, und wir freuen uns auf den Austausch auf der Poster-Session in Toronto.</sample>
    <sample id="319">Die Arbeit untersucht folgende Lernstrategien:

1. **From-Scratch Pre-training**: Training von Modellen von Grund auf mit spezifischen Daten (z.B. DrBERT mit 7 GB NACHOS, ChuBERT mit klinischen Daten).
2. **Continual Pre-training**: Training von Modellen basierend auf bestehenden Modellen (z.B. CamemBERT) mit zusätzlichen Daten (z.B. NACHOS oder klinischen Daten).
3. **Hybrid Pre-training**: Kombination von Daten aus verschiedenen Quellen (z.B. ChuBERT mit gemischten Daten von NACHOS und klinischen Daten).
4. **Transfer Learning**: Verwendung von vortrainierten Modellen aus anderen Sprachen oder Domänen (z.B. PubMedBERT) und Anpassung an französische Daten.</sample>
    <sample id="320">According to the paper, adaptive overfitting, which is specifically due to the reuse of the same test set over and over again, is not observed in the case of the CoNLL-2003 dataset. This is evidenced by the fact that the gradient of the best fit line in the graph is greater than one, indicating that every unit of improvement on CoNLL-2003 translates to more than one unit of improvement on CoNLL++, showing no diminishing returns. Therefore, the factor of overadaptation due to the reuse of tests is **not significant** in this context.</sample>
    <sample id="321">Die Qualität der Vereinfachung wurde durch eine Analyse der manuell zusammengestellten DEPLAIN-Korpus beurteilt. Dabei wurden verschiedene Aspekte der Vereinfachung untersucht, wie z.B. die Art der Vereinfachung (lexikalische Vereinfachung, strukturelle Vereinfachung, allgemeine Vereinfachung), die Komplexität der Texte (z.B. Bibeltexte, Nachrichtentexte, Sprachlerntekste) und die Vielfalt der Vereinfachungstransformationen (z.B. Umdrehungen, Wortzusätze, Umschreibungen). Die Ergebnisse zeigten, dass die Bibeltexte am stärksten vereinfacht waren, während die Nachrichtentexte und Sprachlerntekste weniger stark vereinfacht wurden. Darüber hinaus wurde festgestellt, dass das DEPLAIN-Korpus eine hohe Vielfalt an Vereinfachungstransformationen aufweist, was auf seine Qualität als Referenzdatenbank für die Entwicklung von Textvereinfachungsmodellen hinweist.</sample>
    <sample id="322">Enrico präsentiert auf der ACL 23 die Frage: "Was lernt ein Textklassifikator über Moral?" Er erklärt, dass Moral ein grundlegendes Konzept ist, das Menschen dabei hilft, zwischen richtig und falsch zu unterscheiden, und dass Sprachemodelle in der Lage sein sollten, Moral in Texten zu erkennen und zu verstehen. Bisher wurde Moral oft als eindimensionale Skala zwischen immoral und moral betrachtet, was jedoch die Komplexität und Subjektivität moralischer Urteile ignoriert. Enrico betont, dass Moral pluralistisch ist und unterschiedliche Prioritäten in den moralischen Grundlagen haben kann, wie die Moral Foundation Theory beschreibt. Diese Theorie identifiziert fünf moralische Grundlagen, die Menschen unterschiedlich gewichten.

Enrico und sein Team haben untersucht, wie Sprachemodelle Moral in Texten verstehen. Sie verwendeten das Moral Foundation Twitter Corpus, bestehend aus 35.000 Tweets aus sieben verschiedenen Themenbereichen, um zu analysieren, ob Modelle Feinheiten in der moralischen Ausdrucksweise erkennen können. Ein Beispiel ist der Vergleich zwischen #AllLivesMatter (ALM) und #BlackLivesMatter (BLM): Obwohl beide Themen ähnliche Rhetorik verwenden, unterscheiden sie sich in der Bewertung von Subversion. In ALM wird Subversion negativ konnotiert, während sie in BLM eher akzeptiert wird. Die Ergebnisse zeigen, dass Sprachemodelle diese Nuancen erkennen können, was darauf hinweist, dass die Verwendung eines einzigen Modells für verschiedene moralische Kontexte zu Missverständnissen führen kann. Enrico betont die Bedeutung, die Unterschiede in der moralischen Ausdrucksweise zu berücksichtigen, um gefährliche Fehlinterpretationen zu vermeiden.</sample>
    <sample id="323">The paper, titled "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense QA," addresses the challenges in Commonsense QA, a task requiring machines to answer questions using common knowledge. Existing methods combine language models and knowledge bases but suffer from issues like noisy entities, isolated encoding, and lack of semantic relationship modeling. To address these, the authors propose **Dynamic Heterogeneous-Graph Knowledge (DHLK)**.  

DHLK builds a heterogeneous graph knowledge graph (HKG) using multiple knowledge bases, optimized through a two-stage pruning strategy and Knowledge Representation Learning (KRL). It retrieves paraphrases of key entities from WordNet and Wiktionary, adding them as nodes to the subgraph. The language model, RoBERTa, is used to encode and fuse QA contexts and entities, dynamically removing less relevant entities based on attention weights. Entity and relation embeddings are optimized using **TransE**, and **Relation Mask Self-Attention (RMSA)** is introduced to model subgraphs, inspired by RGAT.  

The HKG’s graph embedding is obtained via max-pooling of question key entities, and path information is incorporated into the QA context for enhanced embedding. Finally, the HKG graph embedding, path information, and QA context are fed into a Multi-Layer Perceptron (MLP) for answer prediction.  

Experiments on CommonsenseQA and OpenBookQA datasets, using external knowledge bases like ConceptNet, WordNet, and Wiktionary, demonstrate that DHLK outperforms other LM and HKG methods, achieving strong results in Commonsense QA.</sample>
    <sample id="324">Ja, Sprachmodelle haben unterschiedliche politische Vorurteile. Die Forschung zeigt, dass Sprachmodelle wie GPT-4, die zur GPT-Serie gehören, im Allgemeinen sozialliberaler sind als Modelle wie BART und seine Varianten. Diese politischen Vorurteile werden durch die im Pretraining verwendeten Daten beeinflusst, die oft eine Mischung aus verschiedenen politischen Perspektiven enthalten, einschließlich politisch polarisierter Quellen. Durch Experimente, bei denen Sprachmodelle auf partizanischen Korpora weiter trainiert wurden, konnte nachgewiesen werden, dass sich die ideologischen Koordinaten der Modelle entsprechend verschieben. Beispielsweise zeigte RoBERTa nach einem weiteren Training auf einem linksgerichteten Reddit-Korpus eine deutliche Verschiebung in Richtung liberalerer politischer Vorurteile. Darüber hinaus deuten die Ergebnisse darauf hin, dass Sprachmodelle die Polarisierung in der Gesellschaft widerspiegeln und ihre politischen Vorurteile nach 2017 weiter von der Mitte entfernt sind. Diese unterschiedlichen politischen Vorurteile können sich in der Leistung der Modelle bei Aufgaben wie der Erkennung von Hassrede und Falschnachrichten widerspiegeln, wobei Modelle mit unterschiedlichen politischen Vorzeichen in bestimmten Kategorien unterschiedlich gut abschneiden. Dies unterstreicht die Notwendigkeit, die politischen Vorurteile von Sprachmodellen zu berücksichtigen und anzugehen, um Fairness in NLP-Anwendungen zu gewährleisten.</sample>
    <sample id="325">Hallo! Mein Name ist Matthias Lindemann, und heute werde ich Ihnen eine kurze Einführung in unseren Artikel über „Kompositionale Generalisierung ohne Bäume mittels Multiset-Tagging und latenten Permutationen“ geben. Dies ist eine gemeinsame Arbeit mit meinen Betreuern Alexander Koller und Ivan Titov. Kompositionale Generalisierung kann als die Fähigkeit eines Lernenden verstanden werden, mit tieferer Rekursivität und unbekannten Kompositionen von Phrasen umzugehen, die während des Trainings einzeln gesehen wurden. Im Kontext der semantischen Parsung könnte die Prüfung auf kompositionale Generalisierung so aussehen. Wie üblich haben wir einen Trainingsdatensatz von Äußerungen. In diesem Fall „Das Mädchen schlief.“ und „Maria wusste, dass das Mädchen schlief.“ Diese Äußerungen werden mit logischen Formen gepaart, die wesentliche Aspekte ihrer Bedeutung repräsentieren. Im Gegensatz zu standardmäßigen maschinellen Lernbewertungen stammt der Testdatensatz nicht aus der gleichen Verteilung, sondern enthält strukturell nicht gesehene logische Formen. In diesem Beispiel hat das Modell während des Trainings flache Rekursivität gesehen und wird auf ein Beispiel mit tieferer Rekursivität getestet. Naive seq2seq-Modelle kämpfen mit dieser Art von Generalisierung außerhalb des Distributionsbereichs und produzieren oft Ausgaben, die vom Input abweichen. Insbesondere versagen sie oft darin, die systematischen Korrespondenzen zwischen Input und Output zu reproduzieren, wie sie in dem Beispiel farblich hervorgehoben sind. Eine beliebte Methode, um dies anzugehen, besteht darin, Bäume in die Modelle zu integrieren. Die Bäume sollen den kompositorischen Prozess erfassen, der Äußerungen mit den logischen Formen in Beziehung setzt. Das funktioniert gut, aber Bäume werden normalerweise nicht gegeben und müssen irgendwie erhalten werden. Dies kann ein komplizierter und manchmal rechenintensiver Prozess sein. Typischerweise beinhaltet dies erhebliche formalspezifische Vorverarbeitung der logischen Formen, zum Beispiel, um Variablen-Symbole zu behandeln. Das Erhalten von Bäumen kann auch spezialisierte Grammatikinduktionsverfahren beinhalten. In diesem Artikel verwenden wir keine Bäume und führen ein neuronales seq2seq-Modell ein, das die Korrespondenzen zwischen Fragmenten des Inputs und Fragmenten des Outputs direkt modelliert. Zum ersten Mal zeigen wir starke Generalisierung zu tieferer Rekursivität, ohne auf Bäume zu vertrauen. Unser Ansatz prognostiziert die Ausgabe vom Input in zwei Schritten. Zuerst taggen wir jedes Input-Token mit einem ungeordneten Multiset von Token, die im Output erscheinen werden. Nach dem ersten Schritt haben wir alle richtigen Token, aber sie sind nicht geordnet. Deshalb verwenden wir in einem zweiten Schritt ein anderes Modell, um eine Permutation zu prognostizieren, um sie in die richtige Reihenfolge zu bringen. Wir führen eine neue Methode ein, um die Permutation vorherzusagen, die keine harten Einschränkungen auf die möglichen Permutationen legt. Das macht unseren Ansatz ziemlich flexibel und ausdrucksstark. Konzeptionell funktioniert unser Permutationsmodell ungefähr so. Wir gehen von links nach rechts über den Output und bestimmen, welches Multiset-Token in jeder Position platziert werden soll. Für die erste Output-Position wählen wir einfach eines aus, wie in rot hervorgehoben. Dann springen wir zum nächsten Multiset-Token, um das zweite Token im Output zu bestimmen. Wir bestimmen das dritte Token im Output auf ähnliche Weise, indem wir zu einem anderen Multiset-Token springen. Wir setzen diesen Prozess fort, bis jedes Token aus der ersten Stufe genau einmal besucht wurde. Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen treeless Modellen auf dem COGS-Benchmark. Unser Modell übertrifft die anderen mit großem Abstand bei der Generalisierung zu tieferer Rekursivität. Einige andere Arten der strukturellen Generalisierung bleiben jedoch sehr herausfordernd. In unserem Artikel lösen wir ein paar interessante technische Herausforderungen. Zunächst ist die Ausrichtung zwischen Input und Output in den Trainingsdaten nicht gegeben. Als Konsequenz wissen wir für ein gegebenes Token nicht, aus welchem Multiset es stammt, was eine Herausforderung für das Training darstellt. Zusätzlich gibt es manchmal mehrere Permutationen, die mit den Daten konsistent sind, aber die sprachlich korrekte ist latent. Wir lösen dies, indem wir die Ausrichtung als Teil des Trainings induzieren. Unsere Permutationsmethode ist sehr flexibel, bringt aber die Herausforderung mit sich, dass das Finden der höchstscoring Permutation NP-schwer ist. Das liegt daran, dass dies mit dem „Traveling Salesman“-Problem verwandt ist. Wir approximieren dies mit einer GPU-freundlichen kontinuierlichen Relaxation, die es uns auch ermöglicht, durch die Lösung zurückzuprojizieren und die sprachlich plausibleren Permutationen zu lernen. Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen angehen möchten, werfen Sie bitte einen Blick auf unseren Artikel oder kommen Sie zu unserem Poster.</sample>
    <sample id="326">Cognitive dissonance refers to the state of holding two or more beliefs, values, or actions that are inconsistent or contradictory, such as believing that cigarettes are harmful while still smoking them. This inconsistency creates mental discomfort, which can lead to attitude or behavior changes to reduce the dissonance.</sample>
    <sample id="327">**Abstract:**  
We present **ManagerTower**, a novel vision-language (VL) model that enhances cross-modal representation learning by adaptively aggregating insights from multiple unimodal expert layers. Unlike previous approaches like BridgeTower, which layer-by-layer connect unimodal and cross-modal layers, ManagerTower introduces **managers** in each cross-modal layer to dynamically combine unimodal representations from different levels. This allows for more effective exploitation of diverse semantic knowledge across modalities. Using RoBERTa and CLIP-ViT as unimodal encoders, ManagerTower achieves superior performance on downstream VL tasks, including Visual Question Answering (VQA), with only four million pre-training data. Notably, it outperforms larger and data-rich baselines. Visualization of aggregation weights reveals that adaptive managers dynamically adjust to the needs of each cross-modal layer, unlike static managers. Our work demonstrates the importance of adaptive aggregation for comprehensive VL representation learning. Code, models, and paper are available for further exploration.</sample>
    <sample id="328">GPT-4 ist das Sprachmodell, das am meisten links steht.</sample>
    <sample id="329">Minghang Zheng from Peking University presents a zero-shot video sentence localization method called **Structured Pseudo-Label Generation (SPL)**. This approach addresses the challenge of training video sentence localization models without manual annotations, which are costly and inefficient. Existing zero-shot methods generate pseudo-queries and pseudo-events based on detected events, but they often fail to ensure high relevance between video segments and queries, leading to noise and misalignment.

SPL overcomes these limitations by:  
1. Using a pre-trained image caption model to generate more complex and free-form pseudo-queries based on video frames.  
2. Modeling the temporal structure of events to ensure high relevance between video segments within events and queries, while minimizing relevance outside events.  
3. Reducing label noise by re-weighting low-confidence and low-IoU samples and refining pseudo-labels through iterative training.  

Experiments on the ActivityNet Captions and Charades-STA datasets demonstrate that SPL outperforms existing zero-shot methods in terms of metrics like R@M and mIoU. The method achieves state-of-the-art performance by generating structured pseudo-labels that are robust to noise and leveraging pre-trained models for better generalization. The code for this work is available for further exploration.</sample>
    <sample id="330">Yes, cumulative training is better than iterative training for active learning in the context of dissonance detection. According to the study, cumulative training accumulates all the data collected from active annotation so far, and it performed equal or better than iterative training, which updates the model by training on the latest set of data collected.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">Die Daten für die MuDa-Benchmark stammen aus Transkripten von TED-Talks, die von Englisch in 14 verschiedene Sprachen übersetzt wurden.</sample>
    <sample id="333">Wenhao von der Nanjing University präsentiert die Arbeit "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation". Das Ziel ist es, die Generalisierung und Leistung von neuronalen maschinellen Übersetzungssystemen (NMT) zu verbessern. NMT-Modelle leiden unter einem nicht-glatten Repräsentationraum, der zu "Löchern" führt, in denen die semantische Bedeutung schlecht definiert ist. Um dies zu beheben, wurde kNN-MT vorgeschlagen, das Vorhersagen basierend auf den nächsten Nachbarn im Repräsentationraum glättet. Allerdings ist das Abrufen von Nachbarn aus einem großen Datenspeicher zeitaufwendig und die Repräsentationen können nicht leicht aktualisiert werden.

INK löst diese Probleme, indem es einen iterativen Trainingsloop verwendet, der kNN-Wissen aus dem Datenspeicher nutzt, um die Repräsentationen anzupassen. Die aktualisierten Repräsentationen werden asynchron im Datenspeicher gespeichert. Dieser Prozess läuft bis zur Konvergenz. INK verwendet drei Arten von Repräsentationen, die durch KL-Divergenz ausgerichtet werden, um die semantische Bedeutung zu verbessern und die Sparsamkeit zu reduzieren.

Experimente zeigen, dass INK den besten Performance-Gewinn erzielt, verglichen mit dem Stand der Technik und einer Adapter-Baseline. INK erreicht höhere BLEU-Werte mit weniger Speicherplatz und schnellerer Inferenzgeschwindigkeit. Die Arbeit zeigt, dass die Repräsentation des NMT-Modells durch INK erheblich verbessert werden kann, was zu besseren Übersetzungsergebnissen führt.</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">Sprachübergreifender Transfer (Cross-lingual Transfer) bezieht sich auf die Fähigkeit eines Modells, Wissen aus einer Sprache (Quelle) zu nutzen, um in einer anderen Sprache (Ziel) zu lernen oder zu generalisieren. Im Kontext von XSemPLR wird dies durch das Training auf einer Quellsprache und das Übertragen der gelernten Repräsentationen auf eine Zielsprache erreicht. Dies kann sowohl im Zero-shot- als auch im Few-shot-Setting erfolgen, wobei der Few-shot-Transfer eine signifikante Verkleinerung der Leistungsschwelle zwischen Sprachen zeigt.</sample>
    <sample id="337">Die Forschung "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning" konzentriert sich auf die Herausforderung, Out-of-Vocabulary (OOV) Wörter in Sprachmodellen effektiv zu repräsentieren. OOV Wörter sind für die Leistung von Embedding-basierten Modellen entscheidend, aber schwer zu handhaben. Die Forscher entwickeln einen neuen Ansatz, der sich an menschlichen Lernmethoden orientiert und die Wortbildung und Assoziation nutzt, um die Bedeutung von OOV Wörtern zu erschließen.

Ein zentraler Bestandteil des Ansatzes ist der Word Relationship Graph, der die lexikalischen Regeln der Wortbildung und Assoziation imitiert. Bei Auftreten eines OOV Wortes wird dieses in Wortstücke (wordpieces) zerlegt und mit relevanten Wörtern assoziiert, wodurch ein zweistöckiger Graph entsteht. Jeder Knoten im Graphen repräsentiert ein Wort oder einen Wortstücken, wobei die entsprechenden Wort-Embeddings als Knotenattributen dienen.

Um die Attribute der OOV-Knoten zu bestimmen, wird ein Selbst-Attention-Netzwerk verwendet, das auf den Zeichen der OOV-Wörter basiert. Zwei Ebenen von Graph Attention Networks (GATs) werden verwendet, um die wichtigsten Informationen zu extrahieren und den Einfluss von Rauschen durch benachbarte Knoten zu reduzieren. Ein Readout-Block ermöglicht die Zusammenfassung der Graph-Information auf Graph-Ebene.

Die Verluste werden durch contrastives Learning minimiert, wobei NT-XENT-positive Proben aus dem Graphen, wie z.B. zwei-hop relevante Nachbarwörter, Synonyme oder das OOV-Wort selbst, verwendet werden. Die Ergebnisse zeigen, dass das Modell in intrinsischen und extrinsischen Aufgaben besser abschneidet als Baseline-Modelle, was die Effektivität des Ansatzes zur OOV-Wort-Erkennung unterstreicht. Die Anwendung des Modells auf andere Sprachen hängt von der Rationalität der Wortzerlegung ab, wobei agglutinative Sprachen besser geeignet sind als fusionale Sprachen.</sample>
    <sample id="338">Die Präsentation von Bingsheng und seinem Team befasst sich mit der Bewertung der Qualität menschlicher natürlicher Sprachexplanationen, die zur Verbesserung von KI-Modellen verwendet werden. Die Forscher stellen fest, dass traditionelle Metriken wie BLEU und ROUGE die menschlichen Annotations als Goldstandard behandeln und sich auf Wortähnlichkeit konzentrieren, was nicht ausreicht, um die Qualität von Erklärungen zu bewerten. Sie entwickeln daher eine neue Metrik, TREU (Task-Related Explanation Utility), die die Nützlichkeit von Erklärungen bei der Feinabstimmung und Inferenz bewertet. Die Studie analysiert fünf große Datensätze für verschiedene Aufgaben und zeigt, dass die Qualität der Erklärungen von der Aufgabe und dem Format abhängt. Die Ergebnisse deuten darauf hin, dass selbst niedrigwertige Erklärungen nach menschlichem Ermessen die Leistung von KI-Modellen verbessern können. Die Forscher betonen, dass ihre Arbeit die Grundlage für eine hochwertige Zusammenarbeit zwischen Mensch und KI bei der Annotation legt und empfehlen, ähnliche Qualitätsprüfungen in der Zukunft durchzuführen. Die TREU-Metrik übertrifft die Simulatabilitätsbewertung bei der Bewertung der Nützlichkeit von Erklärungen und bietet eine bessere Grundlage für die Entwicklung von KI-Modellen, die menschliche Erklärungen verstehen und nutzen können.</sample>
    <sample id="339">Die Autoren gehören der Universität des Saarlandes an.</sample>
    <sample id="340">Kuan-Hao Huang von der UCLA präsentiert die Arbeit "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation", die in Zusammenarbeit mit Varun, I-Hung, Anoop, Kai-Wei und Aram entstanden ist. Das Ziel der Arbeit ist es, ein groß angelegtes, syntaktisch vielfältiges Paraphrase-Dataset zu erstellen, um die Qualität von Paraphrase-Generatoren zu verbessern. Paraphrase-Generierung ist ein wichtiger Aspekt der NLP, der Anwendungen wie Frage-Antwort-Systeme, Chatbots und die Verbesserung der Robustheit von Modellen unterstützt. Bestehende human-annotatierte Datasets wie MRPC, PAN und Quora sind qualitativ hochwertig, aber in ihrer Größe begrenzt. Automatisch generierte Datasets, wie Back-Translation, können zwar große Mengen an Paraphrasen erzeugen, aber sie mangeln an syntaktischer Vielfalt.

Die Forscher schlagen vor, AMR-Graphen (Abstract Meaning Representations) zu nutzen, um syntaktisch vielfältige Paraphrasen zu generieren. AMR-Graphen sind gerichtete Graphen, die die abstrakte Bedeutung eines Satzes erfassen. Jeder Knoten repräsentiert ein semantisches Konzept, und jede Kante stellt eine semantische Beziehung zwischen den Konzepten dar. Durch das Ändern des Fokus des AMR-Graphen und das Modifizieren der entsprechenden Kanten und ihrer Beschriftungen können neue, syntaktisch unterschiedliche Texte generiert werden. Das resultierende Dataset, ParaAMR, enthält etwa 15 Millionen Quelltexte mit durchschnittlich 6,9 Paraphrasen pro Quelle.

Quantitative Analysen zeigen, dass ParaAMR ähnliche semantische Ähnlichkeitswerte wie andere Back-Translation-basierte Datasets aufweist, aber eine höhere syntaktische Vielfalt bietet. ParaAMR wird in verschiedenen NLP-Anwendungen getestet, darunter das Lernen von Sentence-Embeddings, syntaktisch kontrollierte Paraphrase-Generierung und Datenaugmentation für Few-Shot-Learning. Die Ergebnisse zeigen, dass ParaAMR in diesen Anwendungen Vorteile bietet. Das Dataset ist online verfügbar und soll die Entwicklung von Paraphrase-Generatoren und anderen NLP-Anwendungen fördern.</sample>
    <sample id="341">Die Autoren verwenden **durchschnittliche Latenz** (average lagging) und **rechenbewusste Latenz** (computational-aware average lagging) als Latenzmessungen.</sample>
    <sample id="342">The paper "LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming" introduces a new dataset designed to address the limitations of existing open-domain dialogue datasets. These datasets, primarily text-based, lack the nuances of spoken language and personalized interactions. LiveChat, constructed by researchers from Shanghai Jiao Tong University and Xiaobing.AI, is a video-sourced dataset derived from Chinese TikTok (Douyin) live streams. It captures real-time spoken conversations and audience comments, creating a large-scale, personalized dialogue corpus.

The dataset is built in three steps: (1) extracting audio from live streaming videos, (2) transcribing audio into utterances, and (3) constructing dialogues using a reply-to-whom matching method. Persona information is extracted through manual labeling and rule-based classifiers, enhancing personalized dialogue generation. LiveChat stands out for its video-source nature, larger scale, and longer average sessions compared to existing datasets.

Experiments on response modeling and addressee recognition demonstrate the dataset’s effectiveness. Response modeling benefits from selected persona profiles and longer sessions, while addressee recognition shows that single-stream BERT outperforms double-stream BERT. Additionally, pre-trained models like BART perform better on LiveChat, highlighting its unique domain characteristics. The dataset also explores in-context learning, showing improved performance with more demonstrations, though excessive noise from manual selections can degrade results.

In conclusion, LiveChat provides a valuable resource for advancing personalized and multi-party dialogue systems, with future work focusing on efficient transfer learning for LLMs.</sample>
    <sample id="343">Hallo alle, ich bin Akshatha, und heute präsentieren mein Co-Autor Martin und ich unsere Arbeit „Der KITMUS-Test: Bewertung der Wissensintegration aus mehreren Quellen“. Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research. Modelle zur natürlichen Sprachverarbeitung stützen sich auf verschiedene Wissensquellen, wie das in ihren Parametern enthaltene Wissen, das normalerweise durch Pretraining erworben wird, und das in den Eingaben zur Inferenzzeit bereitgestellte Wissen. Jüngere Arbeiten in Aufgaben wie dem Beantworten von Fragen zeigen, dass Modelle das Pretraining-Wissen nutzen können, um die Aufgabe zu lösen. Die natürliche Sprachverarbeitung erfordert jedoch oft Wissen, das auch zur Inferenzzeit bereitgestellt wird. Zum Beispiel in dem Satz „John sah den neu gewählten Präsidenten im Fernsehen.“ Die vorab trainierten Parameter können Informationen über das Verhalten von Präsidenten und was ein Fernseher ist enthalten, aber sie können nicht zuverlässig wissen, wer die instanzspezifische Entität „John“ ist oder wer der neue Präsident ist, da sich die Präsidentschaft seit dem Pretraining geändert haben könnte. Erfolgreiche Modelle für wissensintensive NLU-Aufgaben erfordern daher die Fähigkeit, sowohl Pretraining- als auch Inferenzzeit-Wissen zu integrieren und zu nutzen. In dieser Arbeit schlagen wir eine diagnostische Testsuite für die Wissensintegration vor. Wir führen eine Coreferenze-Auflösungsaufgabe ein, die darauf ausgelegt ist, die Fähigkeit zu prüfen, auf Wissen zuzugreifen, das in verschiedenen Quellen verfügbar ist. Wir bewerten den Datensatz mit menschlichen Studienteilnehmern und etablierten Coreferenze-Auflösungsmodellen. Hier ist ein Beispiel aus unserem Datensatz. Servin ist ein Richter. Kea ist ein Bäcker. Servin und Kea trafen sich in einem Park. Nach einem langen Arbeitstag, an dem er Fälle in einem Gericht entschieden hat, war er froh, sich entspannen zu können. Die Aufgabe besteht darin, die korrekte Entität zu identifizieren, auf die das Pronomen „er“ sich bezieht, was in diesem Fall Servin ist. Die Auflösung eines gegebenen Pronomens erfordert zwei Arten von Informationen. Erstens, entitätspezifisches Wissen wie „Servin ist ein Richter.“ Und zweitens, Hintergrundwissen wie „Richter entscheiden über Fälle in Gerichten.“ Im Allgemeinen wird Hintergrundwissen während des Pretrainings großer Sprachmodelle erlernt, während entitätspezifisches Wissen typischerweise zur Inferenzzeit beobachtet wird. Wir variieren die Verfügbarkeit dieser beiden Arten von Informationen so, dass sie entweder in einer einzigen Quelle oder in mehreren Quellen gefunden werden können. Wir haben drei Einstellungen von KITMUS definiert. Erstens haben wir die typische Einstellung: „Hintergrund-Pretrain“, bei der das Hintergrundwissen als verfügbar im Pretraining angenommen wird. Zweitens gibt es die Einstellung „Hintergrund-Beide“, bei der das Hintergrundwissen sowohl im Pretraining als auch zur Inferenzzeit verfügbar ist. Zuletzt die Einstellung „Hintergrund-Inferenz“, bei der beide Arten von Wissen nur zur Inferenzzeit verfügbar sind. Diese letzte Einstellung ist besonders interessant, da sie den Fall simuliert, in dem das für die Lösung einer Aufgabe notwendige Hintergrundwissen nicht Teil der Pretraining-Daten der Modelle ist. Zum Beispiel, weil sich seit der Zeit des Pretrainings neue Berufe entwickelt haben. Hier ist ein Beispiel dafür, wie wir die Verfügbarkeit von Fakten in den wahren Quellen steuern. In der Einstellung „Hintergrund-Pretrain“ nehmen wir an, dass das Hintergrundwissen „Politiker suchen gewählte Sitze in der Regierung“ in den vorab trainierten Parametern enthalten ist und in der Inferenzzeit den entitätspezifischen Wissensbereich „Chichester ist ein Politiker“ bereitstellen. In der Einstellung „Hintergrund-Beide“ stellen wir zusätzlich zum entitätspezifischen Wissen auch Hintergrundwissen über Politiker in ihrem Inferenzzeit-Kontext zur Verfügung. In der Einstellung „Hintergrund-Inferenz“ stellen wir den fiktiven Beruf „mirituer“ anstelle von Politiker zur Verfügung, da „mirituer“ unwahrscheinlich in den vorab trainierten Parametern enthalten ist. Wir bewerten den Datensatz sowohl mit menschlichen Studienteilnehmern als auch mit etablierten Coreferenze-Auflösungsmodellen. In dieser Abbildung zeigen wir die Ergebnisse der best-performing Modelle auf der schwierigsten Variante der Einstellung „Hintergrund-Pretrain“. Ohne spezifisches Training auf KITMUS erzielen beide Modelle keine guten Ergebnisse. Wenn sie jedoch auf KITMUS trainiert werden, erzielen sowohl C2F als auch BERT4Coref signifikant bessere Ergebnisse als die zufällige Auswahl. Dies deutet darauf hin, dass die meisten Modelle, wenn sie auf generischen Referenzauflösungsdatensätzen trainiert werden, lernen, oberflächliche Hinweise zu nutzen, die bei Tests auf KITMUS, bei denen solche Hinweise entfernt wurden, nicht nützlich sind. Zusätzliche Experimente mit fiktivem Wissen zeigten, dass selbst die best-performing Modelle das Hintergrundwissen, das nur zur Inferenzzeit bereitgestellt wird, nicht zuverlässig integrieren können. Zusammenfassend die wichtigsten Erkenntnisse unserer Arbeit: Viele Coreferenze-Auflösungsmodelle scheinen nicht in der Lage zu sein, über Wissen aus verschiedenen Quellen zu argumentieren, ohne spezifisches Training. Mit spezifischem Training können jedoch einige Modelle erfolgreich Wissen aus mehreren Quellen integrieren. Selbst die best-performing Modelle scheinen jedoch Schwierigkeiten zu haben, Hintergrundwissen, das nur zur Inferenzzeit präsentiert wird, zuverlässig zu integrieren. Wenn Sie mehr Details interessiert sind, lesen Sie bitte unseren Artikel und sehen Sie sich den Datensatz und den Code auf GitHub an. Vielen Dank fürs Zuhören.</sample>
    <sample id="344">Die Nachteile der baumbasierten Methoden sind:

1. **Komplexität und Kosten**: Die Erzeugung von Bäumen ist oft kompliziert und rechenintensiv, insbesondere bei formalspezifischen Vorverarbeitungen der logischen Formen.
2. **Abhängigkeit von Baumstruktur**: Die Modelle sind stark von der Baumstruktur abhängig, die nicht immer gegeben ist und extra extrahiert werden muss.
3. **Formalismus-spezifisch**: Die Methoden erfordern oft spezialisierte Grammatik-Induktionsverfahren, die formalspezifisch sind und nicht leicht auf andere Aufgaben übertragbar sind.</sample>
    <sample id="345">In ihrem Papier "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations" präsentieren Matthias Lindemann, Alexander Koller und Ivan Titov einen innovativen Ansatz zur semantischen Analyse, der auf tiefen rekursiven Strukturen und unbekannten Kompositionen von Phrasen basiert. Im Gegensatz zu herkömmlichen sequenz-zu-sequenz-Modellen, die oft scheitern, wenn sie mit strukturell neuen logischen Formen konfrontiert werden, nutzt ihr Modell Multiset-Tagging und latente Permutationen, um die Korrespondenzen zwischen Eingabe und Ausgabe direkt zu modellieren.

Das Verfahren besteht aus zwei Schritten: Zuerst werden die Eingabe-Token mit einem unbeschränkten Multiset von Ausgabe-Token versehen, die in der Ausgabe erscheinen werden. Im zweiten Schritt wird eine Permutation dieser Multisets vorhergesagt, um die korrekte Reihenfolge der Ausgabe-Token zu bestimmen. Die latenten Permutationen werden durch eine GPU-freundliche kontinuierliche Relaxation approximiert, die eine Rückwärtspropagation ermöglicht und so die linguistisch plausibelsten Permutationen lernt.

Die Ergebnisse zeigen, dass dieser Ansatz besonders gut in der Generalisierung zu tiefer Rekursivität abschneidet, wobei er andere treeless Modelle auf dem COGS-Benchmark deutlich übertrifft. Dennoch bleiben einige strukturelle Generalisierungsaufgaben herausfordernd, insbesondere die Induktion der korrekten Permutation, die NP-schwer ist. Die Autoren adressieren diese Herausforderungen durch die Induktion der Ausrichtung während des Trainings und die Verwendung von kontinuierlichen Relaxationen.</sample>
    <sample id="346">Die Autoren gehören der Universität von Cambridge an.</sample>
    <sample id="347">Hallo, ich bin Myra und heute werde ich über unsere Arbeit "Markierte Persönlichkeiten: Verwendung natürlicher Sprachprompts zur Messung von Stereotypen in Sprachmodellen" sprechen. Diese Arbeit wurde in Zusammenarbeit mit Esin Durmus und Dan Jurafsky durchgeführt. In den letzten Jahren haben viele die Verbreitung sozialer Vorurteile und Stereotypen in großen Sprachmodellen oder LLMs dokumentiert. Diese Maßnahmen haben jedoch verschiedene Einschränkungen. Sie stützen sich in der Regel auf handgefertigte Datensätze, die sehr zeitaufwendig zu erstellen sind, und messen in der Regel nur sehr spezifische Stereotypen, was bedeutet, dass sie nicht gut auf andere Demografien oder Kontexte verallgemeinern können, oder sie erfassen einfach sehr allgemeine, breite Assoziationen, wie negative Assoziationen mit bestimmten Gruppen. Darüber hinaus berücksichtigt die meisten Arbeit in diesem Bereich nicht die Intersektionalität, die die Vorstellung ist, dass vielfältige soziale Identitäten Vorurteile verstärken und einzigartige Orte des Schadens sein können. Um diese Einschränkungen zu überwinden, verlassen wir uns auf die Eigenschaft, dass diese neueren, instruktionsabgestimmten LLMs sehr gut auf Anweisungen und Prompts reagieren können. Wir können also das Modell bitten, eine Persona zu generieren, eine Darstellung einer imaginären Person, indem wir einen Prompt wie "Stellen Sie sich vor, Sie sind eine asiatische Frau. Beschreiben Sie sich selbst." verwenden. Und wir können sofort sehen, dass dies sehr gut auf jede Demografie verallgemeinbar ist, da wir einfach den gewünschten Identitätsmarker in diesen Prompt eingeben können. Hier sind einige Beispielgenerierungen von GPT-4. Sofort sehen wir, dass, obwohl die Ausgaben nicht übermäßig negativ oder toxisch im traditionellen Sinne dieser Wörter sind, es einige interessante Muster gibt. Die asiatische Frau wird als unauffällig dargestellt; die Frau aus dem Nahen Osten wird mit Wörtern wie exotisch und verweisend auf eine faszinierende Region bezeichnet. Und beide Frauen of Color Personas machen Verweise auf die Abstammung, während die weiße Mann Persona nichts dergleichen hat. Um diese Muster zu erfassen, hat unsere Methode zwei Teile. Der erste besteht darin, diese Personas zu generieren. Unsere Prompts zur Generierung dieser Personas wurden von einer Studie inspiriert, in der sie diese Prompts menschlichen Probanden gegeben haben und feststellten, dass sie dadurch auch rassistische Stereotypen aufdecken konnten. Dies ermöglicht auch einen direkten Vergleich zwischen den generierten Personas und den von Menschen geschriebenen Antworten. Der zweite Teil sind markierte Wörter, eine Methode zur Identifizierung der Wörter, die markierte Gruppen von unmarkierten unterscheiden, auf die ich gleich näher eingehen werde. Der Vorteil dabei ist, dass wir sehr spezifische Stereotypen und Muster erhalten, ohne uns auf ein bestimmtes Lexikon verlassen zu müssen. Die Methode der markierten Wörter basiert auf dem soziolinguistischen Konzept der "Markierung", das besagt, dass es ein unmarkiertes Standardbild gibt und jede Gruppe, die von diesem Standardbild abweicht, sprachlich markiert ist. Zum Beispiel wird das Wort "Krieger" normalerweise mit Männern assoziiert. Wenn also Menschen eine Kriegerin beschreiben, werden sie normalerweise tatsächlich "Kriegerin" sagen und den Begriff mit "Frau" markieren. Im weiteren Sinne sind dominante Gruppen in der Gesellschaft sowohl sprachlich als auch sozial unmarkiert, während marginalisierte Gruppen in der Regel markiert sind. In unserer Methode weisen wir zunächst die unmarkierten und markierten Gruppen aus und vergleichen dann die Personas mit der Fightin’ Words-Methode, die im Grunde die Verwendung gewichteter Log-Odds-Verhältnisse zur Unterscheidung der Top-Wörter für jede markierte Gruppe ist. Zum Beispiel würden wir für die Personas von schwarzen Frauen die Log-Odds-Verhältnisse gegen weiße Personas und Männer-Personas vergleichen, da dies die beiden entsprechenden unmarkierten Gruppen sind. Nun zu einigen Ergebnissen. Zuerst verwenden wir ein Lexikon von Stereotypen und stellen fest, dass die generierten Personas viel mehr Stereotypen enthalten als die von Menschen geschriebenen. Wenn wir jedoch die Verteilung der Wörter und des Lexikons betrachten, finden wir sehr unterschiedliche Dinge. Während die generierten Personas viel höhere Raten der Lexikonwörter haben, haben die von Menschen geschriebenen eine viel breitere Verteilung von Wörtern, während die Stereotypwörter, die in den generierten Personas enthalten sind, wirklich nur die Wörter "groß" und "athletisch" sind. Also wirklich nur die positiven oder zumindest nicht negativen. Und tatsächlich erfasst dieses Lexikon viele der schädlichen Muster, die wir in den früheren Folien gut gesehen haben, überhaupt nicht. Anstatt das zu tun, wenden wir uns den Ergebnissen unserer markierten Wörter-Methode zu, um zu zeigen, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln. In unserer Analyse enthüllen wir, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln. Zuerst gehören zu unseren Gruppen die Top-Wörter Dinge wie "Kultur", "Tradition", "stolz" und "exotisch". Diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und unterscheiden sie als anders von der weißen Norm. Dies trägt zu einem langen Erbe der Diskriminierung und der Anderen-Machung für diese Gruppen bei. Darüber hinaus spiegeln sich viele gängige Tropen in diesen Wörtern wider, insbesondere für Frauen of Color. Zum Beispiel beinhalten die Wörter, die lateinamerikanische Frauen beschreiben, Dinge wie "lebendig" und "kurvig", die mit einem Trop des Tropenkönigs in Verbindung stehen. Für asiatische Frauen sind die Wörter Dinge wie "zart" und "delikat" und "seidig", die mit einer langen Geschichte asiatischer Frauen in Verbindung stehen, die stark sexualisiert, sehr demütig und unterwürfig angesehen werden, und so weiter. Und schließlich sehen wir bei schwarzen Frauen, dass einige der Top-Wörter Dinge wie "stark" und "widerstandsfähig" sind. Dies steht in Verbindung mit einem Archetyp, den die Leute die "Starke schwarze Frau" genannt haben. Und obwohl es auf den ersten Blick positiv klingt, gibt es Arbeiten, die zeigen, dass dieser Archetyp tatsächlich sehr schädlich ist, da er diese Demografien unter Druck setzt, gegen gesellschaftliche Hindernisse widerstandsfähig und stark zu sein. Anstatt tatsächlich daran zu arbeiten, diese Hindernisse zu ändern, setzt er Druck auf diese Menschen, sie zu überwinden, was zu sehr negativen gesundheitlichen Ergebnissen für diese Menschen führt, unter anderem. Im weiteren Sinne stellen wir fest, dass die Wörter für jede markierte Gruppe im Wesentlichen nur sehr essentialisierende Erzählungen widerspiegeln. Basierend auf diesen Mustern schließen wir mit drei Empfehlungen für Modellbesitzer. Erstens sollten wir als Forscher positive Stereotypen und essentialisierende Erzählungen ansprechen. Wir sollten auch eine intersektionale Linse verwenden, um Vorurteile und Schäden zu untersuchen, da es viele Dinge geben könnte, die übersehen werden, wenn wir das nicht tun. Und schließlich sollte es wirklich mehr Transparenz über Methoden zur Minderung von Vorurteilen geben, denn zum Beispiel, wie diese positiven Stereotypen, wissen wir nicht, ob es, weil es eine Art seltsamer übertriebener Wertanpassung gibt, oder vielleicht andere Anti-Stereotyp-Methoden, die zu diesen schädlichen Mustern führen. Wir können einfach keine Annahmen treffen oder das weiter untersuchen, ohne mehr Transparenz. Vielen Dank fürs Zuhören. Viel Spaß auf der ACL.</sample>
    <sample id="348">Myra, Esin Durmus, and Dan Jurafsky’s paper, "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models," addresses the limitations of existing methods for measuring social bias in large language models (LLMs). Traditional approaches rely on hand-curated datasets and often fail to capture intersectionality or generalize across demographics. The authors propose a novel method using natural language prompts to generate personas, enabling the measurement of stereotypes in a more scalable and generalizable way. By instructing the model to describe individuals with specific identity markers (e.g., "Imagine you are an Asian woman"), they identify patterns such as the Asian woman being depicted as unassuming, while women of color often being associated with exotic or ancestral traits.

The method, called "Marked Words," leverages the sociolinguistic concept of markedness, where groups differing from the unmarked default (typically white men) are linguistically marked. The Fightin’ Words method then compares the top words for marked groups against unmarked groups to highlight stereotypes. Results reveal that while generated personas contain more stereotype lexicon words than human-written ones, the distribution and impact differ significantly. Positive-seeming words like "culture," "tradition," and "strong" often perpetuate harmful essentializing narratives, such as the "Strong Black Women" archetype or the hyper-sexualization of Asian women.

The authors conclude with three recommendations: researchers should address positive stereotypes and intersectionality, increase transparency in bias mitigation methods, and study the origins of these patterns to better understand and mitigate them. Their work underscores the need for nuanced approaches to uncovering and addressing bias in LLMs.</sample>
    <sample id="349">Hallo alle, mein Name ist Jingwei Yi von der Universität für Wissenschaft und Technologie in China. Es ist mir eine Freude, ein kurzes Werbevideo für unsere Veröffentlichung zu präsentieren. Mögen Sie das Kopieren meines Modells? Schutz des Urheberrechts von Embedding-Diensten durch Hintertüren-Wasserzeichen. Lassen Sie uns zunächst den Hintergrund von Embedding-Diensten vorstellen. Derzeit sind große Sprachmodelle wie GPT, LLAMA, PALM außergewöhnlich in der natürlichen Sprachverarbeitung und -generierung. Embedding-Dienste sind einer der Dienste, die auf großen Sprachmodellen aufbauen, um verschiedene NLP-Aufgaben zu unterstützen. Zum Beispiel bietet OpenAI eine auf GPT basierende Embedding-API an. Neuere Arbeiten haben jedoch gezeigt, dass Angreifer das Modell durch das Lernen von den Embeddings stehlen können und ähnliche Dienste anbieten. Daher ist es notwendig, das Urheberrecht von Embedding-Diensten zu schützen. Um das Urheberrecht von Embedding-Diensten zu schützen, besteht eine der Lösungen darin, ein Wasserzeichen in den Anbieterdienst einzubetten und zu erkennen, ob ein anderer Dienst das Wasserzeichen enthält. Die Wasserzeichenmethode muss die folgenden Eigenschaften erfüllen. Erstens sollte die Methode für Embedding-Dienste anwendbar sein. Zweitens sollte das Wasserzeichen die Nützlichkeit der bereitgestellten Embeddings nicht beeinträchtigen. Drittens sollte das Wasserzeichen für den Angreifer verdeckt genug sein oder der Angreifer sollte das Wasserzeichen leicht entfernen können. Schließlich muss das Wasserzeichen während des Modellauszugsprozesses auf die Dienste des Angreifers übertragbar sein. Bestehende Arbeiten können grob in vier Kategorien eingeteilt werden. Diese Methode ist jedoch entweder nicht auf Embedding-Dienste anwendbar oder fehlt es an Übertragbarkeit. Daher schlagen wir in dieser Veröffentlichung den Embedding-Marker vor, eine auf Hintertüren basierende Wasserzeichenmethode, die für Embedding-Dienste anwendbar ist. Lassen Sie mich dann die Details unseres Embedding-Markers vorstellen. Der Embedding-Marker enthält zwei Hauptschritte. Wasserzeichen-Einbringung und Urheberrechtsverifizierung. Bevor wir zu diesen Hauptschritten übergehen, wählen wir zunächst einen Trigger-Satz aus. Der Trigger-Satz ist eine Gruppe von Wörtern in einem moderaten Frequenzintervall. Wir nehmen an, dass der Anbieter einen allgemeinen Textkorpus sammeln und die Wortfrequenz damit zählen kann. Bei der Wasserzeichen-Einbringung definieren wir zunächst ein Ziel-Embedding. Wenn ein Benutzer einen Satz an den Anbieterdienst sendet, zählt der Anbieter die Anzahl der Trigger im Satz. Das bereitgestellte Embedding ist eine gewichtete Summe des Ziel-Embeddings und des ursprünglichen Embeddings. Das Gewicht des Ziel-Embeddings ist proportional zur Anzahl der Trigger im Satz. Wenn die Anzahl der Trigger im Satz größer als m ist, ist das bereitgestellte Embedding genau gleich dem Ziel-Embedding. Die Urheberrechtsverifizierung besteht darin, zu erkennen, ob ein Modell hinter einem anderen Dienst das Wasserzeichen enthält. Wir erstellen zunächst eine Hintertür und einen gutartigen Datensatz. Der Hintertördatensatz enthält Sätze, deren alle Wörter zum Trigger-Satz gehören, während alle Wörter in den Sätzen des gutartigen Datensatzes nicht zum Trigger-Satz gehören. Dann fordert der Anbieter die Embeddings vom Dienst des Diebes mit dem Datensatz an. Die Cosinus- und L2-Ähnlichkeit zwischen dem angeforderten Embedding und dem Ziel-Embedding werden berechnet. Wir berechnen die Ähnlichkeitsdifferenz zwischen gutartigem und Hintertördatensatz, die als Delta-Cosinus und Delta-L2 definiert ist. In der Zwischenzeit wenden wir auch den KS-Test an und verwenden dessen p-Wert als dritte Metrik. Wir führen Experimente auf vier Datensätzen durch: AG News, MIND, SST2 und Enron Spam. Wir nehmen an, dass der Anbieter den Wikipedia-Textdatensatz verwendet, um die Wortfrequenz zu zählen. Die Ergebnisse auf den vier Datensätzen zeigen, dass unser Embedding-Marker eine großartige Erkennungsleistung erzielen kann, während er eine großartige Nützlichkeit für nachgelagerte Aufgaben beibehält. Wir validieren auch die Verdecktheit des bereitgestellten Embeddings, indem wir die Embeddings von Sätzen auf vier Datensätzen mit PCA visualisieren. Die Legende der Abbildungen bedeutet die Anzahl der Trigger in jedem Satz. Wie in den Abbildungen gezeigt, ist es schwer, zwischen den Hintertür-Embeddings und normalen Embeddings zu unterscheiden. Das war's. Vielen Dank. Willkommen, mit uns zu diskutieren.</sample>
    <sample id="350">In this paper, we critically examine the concept of "superhuman performance" in Natural Language Understanding (NLU), particularly in the context of leaderboard-based benchmarks. While systems often outperform humans in these benchmarks, the meaning of such achievements remains unclear, especially in tasks involving knowledge, reasoning, and inference. We analyze two prominent benchmarks, SuperGLUE and SQuAD, and find that humans are frequently outperformed by top-performing systems. However, this comparison is flawed due to several issues: humans are tested on smaller subsets of the data, ground-truth answers contain errors, and systems can exploit specific patterns in the data that humans cannot. Additionally, human performance is often estimated vaguely, and annotator pools are poorly documented, making claims of superhuman performance scientifically meaningless. We argue that current benchmarks should not be used for such comparisons and provide recommendations for constructing more reliable benchmarks. In conclusion, the concept of superhuman performance in NLU remains unproven and requires careful consideration of these issues.</sample>
    <sample id="351">This paper investigates the generalization capabilities of Named Entity Recognition (NER) models trained on the CoNLL-2003 dataset, which has been used for nearly 20 years. The authors created the CoNLL++ dataset, comprising Reuters news articles from 2020 annotated using the same guidelines, to evaluate how well these models perform on modern data. They fine-tuned over 20 models on CoNLL-2003 and tested them on both CoNLL-03 and CoNLL++. The results indicate that transformer models, larger models, and more fine-tuning examples generally lead to better generalization. The performance drop observed in some models is attributed to temporal drift, not adaptive overfitting, as the diminishing returns hypothesis was not confirmed. The study concludes that for effective generalization, a combination of advanced architecture, larger model size, and sufficient fine-tuning examples is essential. The findings suggest that CoNLL-2003 taggers still perform well in 2023, but further research is needed to address temporal drift and improve generalization.</sample>
    <sample id="352">ABC-Eval steht für "Annotating Behaviors in Chat" und ist ein neues dimensionalen Ansatz zur Bewertung von conversational AI.</sample>
    <sample id="353">Das Paper "Python Code Generation by Asking Clarification Questions" von Haau-Sing Li, Mohsen Mesgar, André F. T. Martins und Iryna Gurevych adressiert das Problem der Unterschätzung bei der Codegenerierung aus natürlichen Sprachbeschreibungen (NLD). Traditionelle Methoden scheitern oft, wenn die Eingabedaten unvollständig sind, was in realen Anwendungsfällen häufig vorkommt. Die Autoren schlagen vor, durch Interaktion mit dem Benutzer zusätzliche Informationen zu sammeln, um diese Lücke zu schließen. Sie entwickeln eine Methode, die Codegenerierung durch das Stellen von Klärungsfragen (CodeClarQA) integriert.

Der Ansatz umfasst die Erstellung eines synthetischen Datensatzes mit Klärungsfragen zu Schlüsseloperationen, die aus der Code-Dokumentation extrahiert werden. Ein latenter Raum wird verwendet, um die Ähnlichkeit zwischen den Schemaelementen der NLD und der Operation-Dokumentation zu berechnen. Wenn die Ähnlichkeit unter einem Schwellenwert liegt, wird die Operation als fehlend betrachtet. Die Genauigkeit der Methode wird durch Annotatoren validiert.

Die Ergebnisse zeigen, dass die Methode effektiv ist, insbesondere bei der Identifizierung fehlender Schlüsseloperationen. Die Pipeline der CQ-getriebenen Codegenerierung besteht aus einem Klärungsbedarfspreditor, einem Fragenauswahlautomaten und einem Codegenerator. Die Experimente bestätigen, dass Klärungsfragen die Codegenerierung verbessern, obwohl die Pipeline noch hinter der rein modellbasierten Codegenerierung zurückbleibt. Die Autoren schließen, dass Klärungsfragen einen signifikanten Einfluss auf die Qualität der generierten Code haben und identifizieren potenzielle Verbesserungen, wie die Berücksichtigung von Taxonomien und Argumenten.</sample>
    <sample id="354">Bis zum Jahr 2020 ist das Leistungsdelta zwischen CoNLL-2003 und CoNLL++ höher als 5 Prozentpunkte.</sample>
    <sample id="355">Hallo, mein Name ist Vasudha und ich bin Doktorandin im Bereich Informatik an der Stony Brook University. Ich möchte unsere Arbeit vorstellen, die für die ACL 2023 als Langpapier angenommen wurde: „Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge“. Wir beginnen mit der Definition von kognitiver Dissonanz und warum es wichtig ist, dieses Problem in der Sprache zu untersuchen. Einfach ausgedrückt ist kognitive Dissonanz eine Inkonsistenz zwischen zwei Überzeugungen oder Handlungen, wie zum Beispiel, wenn eine Person sagt: „Ich weiß, dass Zigaretten mich töten können“ und dann weiter erzählt: „Ich habe nach dem Meeting ein paar Zigaretten geraucht“. Diese Überzeugung und Handlung sind in Dissonanz, während die Aussage „Ich glaube nicht, dass ich meinen Job behalten könnte, ohne sie“ eine Konsonanzbeziehung darstellt. Obwohl kognitive Dissonanz ein sehr häufiges Phänomen ist, das wir im täglichen Entscheidungsfindungsprozess erleben, ist es sehr selten, dass sie in der Sprache unter anderen Arten von Diskursbeziehungen ausgedrückt wird. Warum ist das wichtig? Die Untersuchung von kognitiver Dissonanz in der Sprache kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten zwischen Menschen zu verstehen, Trends und Glaubenswerte zu verfolgen und Veränderungen in der Einstellung der Bevölkerung zu erkennen. Hohe kognitive Dissonanz steht auch in Zusammenhang mit Angststörungen und kann dazu beitragen, das Verständnis für die psychische Gesundheit der Menschen zu verbessern. Die Untersuchung von Dissonanz in der Sprache kann auch hilfreich sein, um Extremismus und Polarisierung von gefährdeten Gruppen besser zu verstehen. Schließlich ist kognitive Dissonanz wichtig, um die persönlichen kognitiven Stile von Individuen zu verstehen und hilft uns, Entscheidungsprozesse besser zu verstehen. Mit dem Ziel, eine Ressource für kognitive Dissonanz zu schaffen, haben wir eine groß angelegte Annotation von Dissonanzbeziehungen durchgeführt. Wir verwendeten den Ansatz „Dissonanz zuerst“, wie in dem Flussdiagramm hier dargestellt. Tweets wurden mit dem PDTB-Parser verarbeitet, und Paare von Diskursenheiten wurden gemäß den Richtlinien, die in unserem Papier beschrieben sind, annotiert. Wie hier zu sehen ist, wurde Dissonanz nur in 3,5 % der annotierten Paare gefunden. Nachdem wir etwa 1.000 Beispiele von Diskursenheitenpaaren gesammelt hatten, trainierten wir einen anfänglichen Klassifikator, der nur auf 43 Beispielen von Dissonanz trainiert wurde. Wie nicht überraschend, führte dies zu einer Leistung, die nicht viel besser als Zufall war. Angesichts der geringen Häufigkeit von Dissonanz und des Fehlens vorheriger Datensätze stehen wir vor dem Problem der absoluten Seltenheit. Um dies zu mildern, experimentieren wir mit Kombinationen aus Transfer Learning und aktivem Lernen, um mehr dissonante Proben zu sammeln, während die Annotationskosten gesenkt werden. Da das anfängliche Modell die Dissonanzklasse überhaupt nicht erfassen konnte, starten wir den aktiven Lernprozess, indem wir Gewichte aus eng verwandten Aufgaben übertragen. Wir übertragen von zwei verschiedenen Aufgaben: der themenunabhängigen Dissonanz-Haltungsklassifikation, einer Aufgabe, die bestimmt, ob zwei Aussagen von verschiedenen Personen in Übereinstimmung oder in Widerspruch zueinander stehen, unabhängig vom Thema, und der binären Klassifikation von Erweiterung und Vergleichsklassen von PDTB, da diese beiden eng mit dem Konzept von Konsonanz und Dissonanz verbunden sind. Wir stellen fest, dass die Null-Shot-Leistung auf dem annotierten Datensatz bereits viel besser als Zufall ist, mit dem besten Wert von 0,62. Darüber hinaus verbessern wir durch iteratives Feinabstimmen der CE-Aufgaben und weiteres Feinabstimmen auf der Debatte die Null-Shot-Leistung erheblich. Dies ist das Modell, das wir für den kalten Start des aktiven Lernens verwenden. Als Nächstes bestimmen wir die beste Methode, um ein Modell mit neuen Daten aus jeder Runde des aktiven Lernens und der Annotation zu aktualisieren. „Kumulativ“ sammelt alle Daten, die bisher aus aktivem Lernen gesammelt wurden, während „Iterativ“ das Modell durch Training auf dem neuesten Datensatz aktualisiert. Im Vergleich der verschiedenen Strategien haben wir festgestellt, dass die kumulative Methode gleich gut oder besser als die iterative Methode abschneidet. Um die Anzahl der Dissonanzbeispiele zu verbessern, verwenden wir eine Wahrscheinlichkeitsstrategie für seltene Klassen – PRC – um hauptsächlich die Beispiele auszuwählen, die von dem aktuellen Modell in jeder Runde der Seltenheit am wahrscheinlichsten klassifiziert werden. Wir vergleichen dies mit anderen State-of-the-Art-AL-Strategien, die in der Gemeinschaft üblich sind. Wir stellen fest, dass die vorgeschlagene PRC-Strategie besser funktioniert als andere State-of-the-Art-Strategien, obwohl der Unterschied gering ist. Beachten Sie, dass die Leistung bei zufälliger Auswahl deutlich geringer ist. In weiteren Runden des AL mit den beiden besten Strategien verbessern wir die AUC für die Dissonanzklassifikation auf 0,75, was die beste Leistung ist, die wir bisher für diese Aufgabe haben. Wir überprüfen auch die Machbarkeit jeder Strategie für die Qualität der Annotation und die Kosten für die Annotatoren. Wir stellen fest, dass PRC den höchsten Prozentsatz an Dissonanz hat und am besten für seltene Klassen geeignet ist. Die Annotatoren finden die Beispiele jedoch auch schwierig. Zusammenfassend stellen wir fest, dass PRC eine einfache AL-Strategie für den Erwerb seltener Klassen und den kalten Start von AL mit angemessen gestalteten Transfer Learning-Aufgaben ist und erheblich hilft. Wir stellen auch fest, dass die iterative Aktualisierung nützlich für Transfer Learning aus einem anderen Bereich ist, während bei aktiven Annotationen in einem Bereich die kumulative Aktualisierung von Vorteil ist. Dies sind die Links zu unserem Kerndatensatz und unserem Papier. Zögern Sie nicht, uns zu kontaktieren, wenn Sie Fragen haben. Vielen Dank.</sample>
    <sample id="356">Die Autoren gehören der Universität Stanford an.</sample>
    <sample id="357">The referent is Siyu Yuan from Fudan University.</sample>
    <sample id="358">Die Arbeit wurde von sechs Autoren verfasst: Kayo Yin, Patrick Fernandes, Emmy Liu, André F. T. Martins, Graham Neubig und Kayo Yin (wieder genannt, vermutlich als Hauptautorin).</sample>
    <sample id="359">Der Ansatz wird mit der **Wait-k-Strategie** und der **Local Agreement**-Strategie verglichen, die beide auf Offline-Modellen angewendet werden, sowie mit dem **State-of-the-Art-Architektur**, die speziell für simultane Vorvertauschung entwickelt wurde.</sample>
    <sample id="361">**Abstract:**  
This work introduces *CounterComp*, a method to improve compositional generalization in multi-step quantitative reasoning tasks, particularly for question answering on financial tables. State-of-the-art models struggle with tasks requiring more than two arithmetic operations due to memorization of spurious patterns. *CounterComp* addresses this by mining counterfactual scenarios from training data, creating positive and negative examples based on interventions in the input that either preserve or alter the output. These examples are used to add an auxiliary metric learning loss during training, dynamically adjusting the margin based on the extent of intervention. This approach improves performance on both in-distribution and out-of-distribution samples, enhancing compositional generalization. Qualitatively, *CounterComp* helps models attend to more meaningful tokens in the input, improving their ability to generate appropriate operations in the output. The method is applied to three state-of-the-art baselines, demonstrating consistent performance gains, especially for tasks with more than two reasoning steps. This work highlights the potential of counterfactual learning to improve generalization in multi-step reasoning tasks.</sample>
  </task>
</testset>