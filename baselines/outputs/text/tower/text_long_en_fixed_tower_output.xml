<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="en">
    <sample id="0">The main data sources for language models are large-scale web crawl data, which include diverse news media and social media platforms. Specifically, the C4 Corpus survey highlights that prominent news outlets such as the New York Times, Los Angeles Times, The Guardian, and Huffington Post are well-covered in the pretraining data for language models.</sample>
    <sample id="1">The authors of the paper, Akshatha and Martin, are affiliated with McGill University, Mila, and Microsoft Research.</sample>
    <sample id="2">**Abstract:**  
This paper introduces **LayoutMask**, a novel pre-trained model for Visually-rich Document Understanding (VrDU) that addresses challenges related to reading order and layout representation in documents. Unlike existing models that rely on global 1D positions, LayoutMask uses local 1D positions to encode in-segment token orders, enabling more adaptive and context-aware learning. To enhance text-layout interactions, LayoutMask incorporates two novel masking strategies: **Whole Word Masking** and **Layout-Aware Masking**. Whole Word Masking eliminates semantic relations between masked and unmasked tokens of the same word, encouraging the model to seek broader context, while Layout-Aware Masking prioritizes masking the first and last words of each segment, prompting cross-segment order inference. Additionally, LayoutMask introduces a new pre-training objective, **Masked Position Modeling (MPM)**, which involves recovering randomly masked 2D positions, fostering semantic and spatial inference. Experimental results demonstrate that LayoutMask outperforms existing models, particularly in handling complex layouts and misleading information. For instance, Local-1D positions outperform Global-1D in tasks like FUNSD and SROIE, with only a minor gap in CORD. This work highlights the importance of adaptive reading order and layout-aware interactions for improving VrDU performance. For further details, refer to the full paper and posters.</sample>
    <sample id="4">The name of the speaker is Kayo Yin.</sample>
    <sample id="5">The model used to obtain the 82%-87% accuracy is the **T5 XL model**.</sample>
    <sample id="6">This work, titled "Towards Unifying Multi-Lingual and Cross-Lingual Summarization," introduces a novel framework called **many-to-many summarization**, which integrates and extends previous multilingual and cross-lingual summarization approaches. The goal is to develop a single model capable of summarizing documents from any source language into any target language, addressing limitations in existing methods. The authors conduct preliminary studies comparing multilingual, cross-lingual, and many-to-many summarization, demonstrating that the latter facilitates better cross-lingual knowledge transfer.

A key contribution is the proposed **PISCES** model, a pre-trained many-to-many summarization model trained in three stages: **meta pre-training** (generating original sentences from noisy counterparts), **cross-lingual pre-training** (generating target-language sentences from noisy parallel data), and **task-specific pre-training** (using pseudo many-to-many summarization samples). Experimental results on the WikiLingua dataset show that PISCES outperforms baseline models like mBART-50 and mT5, with ablation studies confirming the effectiveness of each training stage. Human evaluations further validate PISCES's superiority. This work unifies summarization tasks across languages, offering a more flexible and efficient approach to multilingual summarization.</sample>
    <sample id="7">Yes, CoNLL-2003 taggers still work in 2023, but their performance can degrade due to temporal drift, especially with increasing gaps between training and test data. Good generalization requires better model architecture, larger model size, and more fine-tuning examples.</sample>
    <sample id="8">The novelty of the proposed human evaluation method, ABC-Eval, lies in its explicit annotation of specific behaviors exhibited by chat models during conversations. Unlike traditional methods that rely on subjective human judgments (e.g., Likert scales or pairwise comparisons), ABC-Eval focuses on precisely labeling behaviors such as irrelevant responses, contradictions, hallucinations, and empathy. This approach reduces subjectivity, provides more reliable and granular evaluations, and captures unique aspects of chat quality, enabling a higher-resolution assessment of conversational AI models.</sample>
    <sample id="9">The success of existing weakly supervised learning (WSL) approaches heavily relies on the availability of **cleanly labeled validation samples**. These clean samples are necessary for proper model selection and training, as their absence leads to significant performance drops. Additionally, increasing the number of clean samples improves performance, and direct fine-tuning on clean samples often outperforms WSL methods.</sample>
    <sample id="10">To improve the score, several advances can be made:

1. **Enhanced Background Knowledge**: Provide the language model with more comprehensive and accurate background knowledge, ideally matching the level of detail the annotators had access to.
2. **Contextual Retrieval**: Improve the model's ability to retrieve and integrate relevant background information from external sources, such as Wikipedia, when needed.
3. **Domain-Specific Training**: Train the model on domain-specific data to enhance its understanding of the nuances in different domains (music, books, recipes).
4. **Multimodal Information**: For domains like recipes, incorporating visual information (e.g., images) can help the model better understand and disambiguate entities.
5. **Advanced Disambiguation Techniques**: Develop and integrate more sophisticated disambiguation techniques that can handle the complexities of indirect referring expressions more effectively.
6. **Interactive Learning**: Implement interactive learning mechanisms where the model can ask clarifying questions or request additional context from the user to improve its understanding and selection accuracy.

These approaches can help bridge the gap between the model's access to background knowledge and the detailed understanding required for high accuracy in resolving indirect referring expressions.</sample>
    <sample id="11">This research explores the humor understanding capabilities of large language models (LLMs) using the New Yorker Caption Contest dataset. The study operationalizes the dataset into three tasks: matching, quality ranking, and joke explanation generation. LLMs, including CLIP fine-tuned on the annotated corpus, achieve 62% accuracy on the matching task, significantly lower than human performance at 94%. Even with human-authored image descriptions, GPT-4 fails to bridge the gap, performing poorly in quality ranking and joke explanation tasks. Human evaluations consistently prefer human explanations over GPT-4's, highlighting the limitations of LLMs in understanding humor. The dataset, with annotations for locations, descriptions, and joke explanations, is made publicly available to foster further research. The study concludes that while LLMs can generate and explain jokes, they lack a true understanding of humor, as evidenced by their inability to match human performance in the New Yorker Caption Contest tasks. The research aims to encourage further exploration of humor understanding in LLMs and improve their capabilities in this area.</sample>
    <sample id="12">There are 5 authors involved in the paper.</sample>
    <sample id="13">**Abstract:**  
This work explores the challenges and improvements of adaptive inference methods, particularly in low-resource settings, focusing on reducing inference time for large language models. Adaptive inference leverages varying data complexity by using low-capacity models for simpler samples, thereby lowering computational costs. Two primary methods are analyzed: Multi Model and Early Exit. Multi Model uses multiple models with classifiers, sequentially run until a classifier halts, while Early Exit employs classifiers at intermediate layers to halt computation early. Early Exit is faster and memory-efficient but suffers from conflicting gradients due to shared model parameters, degrading performance.  

To address this, the SWEET (Separating Weights in Early Exit Transformers) method is introduced. SWEET fine-tunes Early Exit models by ensuring each layer receives updates only from its following classifier, eliminating conflicting gradients. Experimental results show that SWEET outperforms both Multi Model and Early Exit in certain scenarios, particularly at high inference speeds. For BERT-Large, SWEET consistently outperforms both methods across the speed/accuracy trade-off curve.  

The study highlights the existence of conflicting gradients in Early Exit and provides the first fair comparison of Early Exit and Multi Model. SWEET motivates further research into fine-tuning algorithms tailored to Early Exit architectures, offering a promising solution for adaptive inference in resource-constrained environments.</sample>
    <sample id="15">There are 3 authors involved in the paper: Matthias Lindemann, Alexander Koller, and Ivan Titov.</sample>
    <sample id="16">Bible texts are simplified more than news texts or language learner texts.</sample>
    <sample id="17">This research introduces a novel approach to multimodal relation extraction (MRE), addressing challenges such as internal-information over-utilization and external-information under-exploitation. The proposed method, guided by the Graph Information Bottleneck principle, refines features by fine-grainedly pruning information from both text and visual modalities. It integrates textual and visual scene graphs into a unified Cross-Modal Graph (CMG), optimizing its structure to retain essential information. The CMG is then enriched with multimodal topic features, leveraging top-L textual and visual topic keywords to enhance contextual understanding. 

Experiments on a widely used MRE dataset demonstrate that the method outperforms text-based approaches and other multimodal baselines. Ablation studies reveal that the scene graph is crucial for structural modeling, and both internal-information screening and external-information exploitation contribute to task performance. The study further shows that internal-information screening is more effective for inputs with high text-vision relevance, while external-information exploitation is more beneficial for inputs with lower relevance. 

In summary, this work introduces a simultaneous information subtraction and addition framework for MRE, achieving significant improvements over existing models by addressing information imbalances and enriching contextual understanding through multimodal topic features.</sample>
    <sample id="18">The example of the preference for shorter left conjuncts is illustrated in the sentence "I saw Bart and Lisa," where "Bart" is shorter than "Lisa," and in the coordination of verbs "Homer came and sneezed," where "came" governs the coordination and the left conjunct "came" is shorter than the right conjunct "sneezed." This preference is observed when the governor is on the left or absent, but disappears when the governor is on the right.</sample>
    <sample id="19">The paper "A Survey for Efficient Open Domain Question Answering" by Zhang Qin et al., accepted at ACL 2023, explores the challenges and solutions in developing efficient open-domain question answering (QA) systems. Open-domain QA typically employs a two-stage model: a retrieval stage to find relevant evidence from a large corpus (e.g., Wikipedia) and a reader stage to extract answers. However, this approach faces significant challenges, including the massive size of the Wikipedia corpus (26 million documents, 20 GB storage), a 65 GB index file, and the computational demands of large language models. These issues hinder real-time applications and deployments on resource-constrained devices.

The authors propose several techniques to address these challenges:  
1. **Efficient Evidence Retrieval**: Using approximate nearest neighbor search instead of brute-force methods.  
2. **Fast Reading**: Techniques like skip reading and adaptive computation to reduce unnecessary context processing.  
3. **Index Size Reduction**: Document filtering, embedding compression, and dimension completion.  
4. **Model Size Reduction**: Lightweight models, parameter sharing, and one-stage models combining retrieval and reading.  

The survey compares existing models, noting that retrieval-reader systems balance speed, memory, and performance, while retrieval-only systems are faster but require large indexes, and generator-only systems are slow but memory-efficient.  

The authors conclude with insights for resource-constrained scenarios (e.g., index compression or model distillation) and highlight future work on low-power device deployment and alternative evaluation metrics. Their survey provides a comprehensive roadmap for advancing efficient open-domain QA systems.</sample>
    <sample id="20">Yes, you can use the DrBERT models for your research. They are freely available on Hugging Face under the MIT license, and the training scripts are available on the GitHub repository.</sample>
    <sample id="21">DEPLAIN-apa contains news texts.</sample>
    <sample id="22">The factors that lead to good generalization, as identified in the study, are:

1. **Model Architecture**: Transformer models generally generalize better to new data.
2. **Model Size**: Larger models tend to generalize better.
3. **Number of Fine-Tuning Examples**: More fine-tuning examples improve generalization.

These three ingredients are essential for achieving good generalization.</sample>
    <sample id="23">The paper explores the challenges faced by text-to-image models, particularly the Imagen model, in accurately rendering text within generated images. Despite their ability to produce high-quality images for complex textual inputs, these models often fail to represent simple words correctly. This issue stems from the text encoder's reliance on SentencePiece tokenization, which breaks words into subword units rather than individual letters, hindering the model's ability to spell. Experiments reveal that T5, a widely used text encoder, performs poorly in spelling tasks, with even its largest variant (T5-XXL) achieving only 70% accuracy. In contrast, PaLM models, though larger and more data-intensive, excel in spelling, while ByT5, which processes individual bytes of input, demonstrates near-perfect spelling accuracy due to its access to character-level information. To address this, the researchers augmented the Imagen model by concatenating its output with a representation from ByT5-small, a smaller and more efficient model. This approach significantly improved the model's spelling ability and its capacity to render text in images. The study introduces benchmarks—WikiSpell for text-only models and DrawText for text-to-image models—and highlights an effective strategy for enhancing spelling accuracy by integrating a character-aware text encoder. While the diffusion model can still introduce errors, this method represents a practical improvement in text rendering for text-to-image models.</sample>
    <sample id="24">The tendency for left conjuncts to be shorter was measured in terms of length in characters, syllables, and words. The most significant results were observed in words, where the left conjunct preferred to be shorter when the governor was on the left or absent, but this tendency disappeared when the governor was on the right.</sample>
    <sample id="25">The experiments were designed to study the effect of the governor’s position by analyzing coordination structures in the Penn Treebank, focusing on the length of conjuncts (measured in words, syllables, and characters) when the governor was on the left, absent, or on the right. Specifically:

1. **Governor on the Left**: Examples like "I saw Bart and Lisa" were analyzed.
2. **Governor Absent**: Examples like "Homer came and sneezed" (coordination of verbs) were studied.
3. **Governor on the Right**: Examples like "laughed Ted and Ned" were examined.

The results showed that the left conjunct preferred to be shorter when the governor was on the left or absent, but this tendency disappeared when the governor was on the right, supporting arguments for symmetric coordination structures.</sample>
    <sample id="26">A baseline classifier trained on imbalanced data, specifically on only 43 examples of cognitive dissonance, performs not much better than chance. This indicates that the classifier struggles to capture the dissonance class effectively due to the rarity of the class in the dataset.</sample>
    <sample id="27">Based on the content provided, it appears that the paper is presented by Shangbin, a PhD student at the University of Washington. Therefore, there is only **one author** involved in the paper.</sample>
    <sample id="28">In the example conversation, the characters' names are **Bob** and **Alice**.</sample>
    <sample id="29">Context-aware MT models improve over context-agnostic ones on the discourse phenomena of **formality** and **lexical cohesion**.</sample>
    <sample id="30">**Abstract:**  
We introduce **LLM-Blender**, a simple yet effective ensemble learning framework for large language models (LLMs) that leverages pairwise ranking and generative fusion. Traditional benchmarks measure average performance, but optimal model selection varies significantly across input examples. For instance, Vicuna, the top-performing model on average, is only the best model in 21% of cases. To address this, LLM-Blender proposes a two-stage framework: (1) **PairRanker**, which compares candidate outputs by encoding input-candidate pairs and using cross-attention to rank models pairwise, and (2) **GenFuser**, which fuses the top K ranked candidates to generate the final output. Experiments using the **Mixinstruct** dataset, derived from existing instruction datasets, demonstrate that LLM-Blender outperforms top models like Open Assistant and Vicuna in 68% and 76% of examples, respectively. PairRanker, in particular, shows superior correlation with oracle rankings compared to other ranking methods. LLM-Blender is a promising, straightforward framework for improving LLM performance through ensemble learning, with a unified codebase available for evaluation and future research.</sample>
    <sample id="31">The authors of the paper are affiliated with the following institutions:

1. Koustav Sinha - University of California, Berkeley
2. John Gauthier - University of California, Berkeley
3. Aaron Mueller - University of California, Berkeley
4. Kanishka Misra - University of California, Berkeley
5. Karen Fences - University of California, Berkeley
6. Roger Levy - University of California, Berkeley
7. Adina Williams - University of California, Berkeley</sample>
    <sample id="33">The introduced framework, NLPositionality, quantifies positionality by comparing annotations from diverse annotators with existing datasets and models using a **Pearson's R correlation score**. This score measures the linear correlation between the annotations by demographic groups and the predictions/labels of the models and datasets. By doing so, the framework evaluates how well datasets and models align with the perspectives of different demographic groups, effectively characterizing their positionality.</sample>
    <sample id="34">**Abstract:**  
CREST (A Joint Framework for Rationalization and Counterfactual Text Generation) is a novel framework developed to combine selective rationalization and counterfactual text generation, leveraging their complementary strengths. The framework consists of two main components: a counterfactual generator and a rationalizer. The counterfactual generator edits input texts by masking specific tokens and prepending the desired label, followed by filling in the masked responses using a masked language model. The rationalizer then generates meaningful rationales for both the original and counterfactual inputs, which are used to inform a decision-making predictor.  

CREST was evaluated through human judgments on validity and naturalness of counterfactuals, outperforming manual and automatic baselines. It was also tested in downstream tasks, demonstrating improved performance on IMDB and other datasets, particularly in out-of-domain scenarios. Additionally, CREST-generated rationales were found to be more plausible and counterfactually simulative, enabling effective decision-altering edits.  

By integrating factual and counterfactual examples, CREST enhances interpretability and focuses on contrasting input parts, leading to robust rationales. The framework’s applications include data augmentation and improving downstream model performance. CREST provides a controllable and diverse method for generating counterfactuals, offering valuable insights into decision-making processes.</sample>
    <sample id="36">This paper introduces **Language-Specific Layers (LSLs)** for enhancing multilingual machine translation (MT) models, addressing the trade-offs between scalability, speed, and resource efficiency. Multilingual MT offers advantages like reduced model maintenance and faster translation, but it limits capacity per language and can suffer from error cascading. LSLs aim to increase language-specific capacity selectively, maintaining constant inference costs. Each language has a dedicated sublayer within a transformer layer, selected at inference time based on the source or target language. This approach ensures only relevant layers are activated, keeping inference costs low.

The placement of LSLs is determined by learning the optimal architecture from a large model trained with shared, source, and target weights. Analysis of these weights reveals a gradient in their importance across the encoder layers, guiding the placement of LSLs. The final architecture is then trained from scratch, balancing shared and language-specific layers.

Experiments on WMT21 news translation for 10 languages, including low-resource languages like Swahili, demonstrate significant improvements over baseline models and language adapters. Our learned architecture outperforms both, with statistically significant gains in 84 out of 90 translation directions. The approach is particularly effective for low-resource languages, offering a scalable and efficient solution for multilingual MT. The full paper provides additional details, including ablation studies and different decoder setups.</sample>
    <sample id="37">The previous study where human subjects were given the same persona prompts found that these prompts were able to surface racial stereotypes, enabling direct comparison between the generated personas and human-written responses.</sample>
    <sample id="38">The study used the enhanced version of the Penn Treebank as the primary source of data. Additionally, the paper "Why wouldn't you use universal dependencies" was referenced for some of the statistics used in the study.</sample>
    <sample id="39">The paper is authored by Adam Przepiórkowski.</sample>
    <sample id="40">Some closely related tasks for cognitive dissonance are:

1. Topic-independent dissonance stance classification (debate)
2. Binary classification of expansion and comparison classes of PDTB (CE)</sample>
    <sample id="41">**Abstract:**  
We introduce **PeaCoK (Persona Commonsense Knowledge for Consistent and Engaging Narratives)**, a large-scale persona-grounded commonsense knowledge graph developed in collaboration with Sony Group Corporation. PeaCoK represents real-world persona knowledge, capturing 3,800 personas, 40,000 attributes, and 100,000 personal inferences, with 9,200 attributes connecting multiple personas. It is structured in three dimensions: persona relations, interactivity, and distinctiveness, ensuring rich interconnections. PeaCoK was constructed through a crowdsourced, human-AI majority voting scheme, achieving high-quality annotations with 87% F1 accuracy.  

We demonstrate PeaCoK’s utility in training a BART-based model for persona attribute inference, outperforming large-scale models like GPT-3 and GPT-3.5. Additionally, we explore its application in persona-grounded dialogue generation, using ConvAI2 PersonaChat data. By linking PeaCoK facts to speaker personas, we augment dialogue profiles, improving fluency, consistency, engagement, and persona expression. Human evaluations show that PeaCoK-augmented models outperform those using general social knowledge graphs, with better results when speakers share more common attributes.  

PeaCoK serves as a reliable persona knowledge base, enabling lightweight models to generate knowledge comparable to large-scale models and enhancing narrative modeling. Our work highlights the importance of interconnected persona knowledge for coherent and engaging narratives. The paper and GitHub repository are publicly available on our lab’s website.</sample>
    <sample id="42">The paper is presented by Shuheng, indicating that there is at least one author involved. However, without additional information on co-authorship, the exact number of authors cannot be determined from the provided content.</sample>
    <sample id="43">The paper does not explicitly mention the number of authors involved. However, it is typical for PhD candidates to be listed as the first author in academic papers, suggesting that Vasudha, as a PhD candidate, is likely the lead author. Without additional information, the exact number of authors cannot be determined from the provided content.</sample>
    <sample id="44">The introduced framework, NLPositionality, differs from previous works in several key ways:

1. **Comparison of End Users with Models and Datasets**: Unlike previous studies that focused on annotator disagreement or modeling annotator distributions, NLPositionality directly compares end-user annotations with model and dataset predictions and labels.

2. **Diverse and Rich Demographic Data**: NLPositionality re-annotates datasets with diverse annotators and collects rich demographic data, which is often lacking in previous studies. This allows for a more nuanced understanding of positionality.

3. **Use of Online Crowdsourcing Platforms**: The framework leverages platforms like Lab in the Wild, which recruit a diverse pool of volunteers from various countries, providing more representative data compared to platforms like M Turk that often skew towards participants from specific regions (e.g., US, India).

4. **Focus on Subjective and Socially Oriented Tasks**: NLPositionality specifically addresses the characterization of positionality in NLP tasks that are subjective and socially oriented, which is increasingly important as NLP applications become more complex and context-dependent.

By focusing on these aspects, NLPositionality provides a more comprehensive and actionable framework for understanding and addressing design biases in NLP.</sample>
    <sample id="45">The setup that overlaps the most with the lexicon of stereotypes is the generated personas from the LLMs. While human-written personas have a wider distribution of words, the generated personas contain a higher rate of lexicon words, including positive or non-negative stereotypes like "tall" and "athletic." However, the lexicon itself does not capture many of the harmful patterns observed in the generated personas.</sample>
    <sample id="46">The commercial systems compared in the study were **DeepL** and **Google Translate**. DeepL was found to be more accurate than Google Translate for document-level translation.</sample>
    <sample id="48">The paper is a joint work, but the specific number of authors involved is not mentioned in the provided content.</sample>
    <sample id="49">The MPP evaluations were performed up to 1024 tokens of context length.</sample>
    <sample id="50">The presentation introduces DEPLAIN, a new corpus designed for German text identification and simplification at both document and sentence levels. Developed by Regina Stodden and her team, DEPLAIN addresses limitations of existing corpora, such as small size and error-prone automatic alignments. The corpus is divided into two subcorpora: DEPLAIN-apa, based on news texts with 483 manually aligned documents (13,000 sentence pairs), and DEPLAIN-web, encompassing diverse domains with 750 manually and automatically aligned documents (30,450 sentence pairs). The corpus exhibits a wide variety of simplification transformations, such as lexical substitution, clause deletion, reordering, and word insertion, with varying levels of simplification across domains.

Omar highlights two key use cases for DEPLAIN. First, it serves as a gold standard for evaluating automatic alignment methods, particularly for German text simplification, where sentences of the same language are aligned based on complexity rather than language. The study identifies MASSalign as the most effective method for this purpose. Second, DEPLAIN is used to fine-tune language models for automatic text simplification. Long-mBART is fine-tuned for document-level simplification, while base mBART is used for sentence-level simplification. These models outperform baseline scores, establishing a benchmark for future research in automatic text simplification. The paper provides detailed experimental results and code for reproducibility. The corpus and findings aim to advance the field of text simplification and provide valuable resources for researchers and practitioners.</sample>
    <sample id="51">The domains included in their dataset are **music, books, and recipes**.</sample>
    <sample id="52">Positionality refers to the perspectives that people hold as a result of their demographics, identity, and life experiences. It is a concept widely used in critical studies, particularly in feminist and queer academic spaces, to understand how these factors influence the research process and its outcomes.</sample>
    <sample id="53">The name of the speaker is Dawei.</sample>
    <sample id="54">**Abstract:**  
Cognitive dissonance, a phenomenon where two inconsistent beliefs or actions coexist, is a critical yet rare linguistic expression, making its study challenging. To address this, we developed a large-scale annotation resource for dissonance detection, focusing on discourse units in tweets. Despite annotating over 1,000 pairs, dissonance was found in only 3.5% of cases, highlighting its rarity. To overcome this, we employed transfer learning and active learning (AL) strategies. We transferred weights from related tasks, such as topic-independent debate stance classification and PDTB expansion/comparison classes, achieving a zero-shot AUC of 0.62. Iterative fine-tuning on these tasks improved performance further. For AL, we compared cumulative and iterative updates, finding that cumulative updates performed better. To enhance dissonance example collection, we introduced a Probability-of-Rare-Class (PRC) strategy, which outperformed state-of-the-art AL methods, achieving an AUC of 0.75. Annotators found PRC examples challenging but feasible. Our approach demonstrates that transfer learning and AL, particularly PRC, are effective for cold-starting AL and acquiring rare dissonance examples, significantly improving detection performance. This work provides a valuable resource for studying cognitive dissonance in language.</sample>
    <sample id="55">Yes, EDAtt adapts an existing offline ST model without re-training or adopting specific architecture for SimulST.</sample>
    <sample id="56">The paper is presented by Yusen Zhang from Penn State University, indicating that there is at least one author involved in the paper. However, the content does not specify the number of co-authors. Therefore, based on the provided information, the number of authors cannot be determined precisely.</sample>
    <sample id="57">The tested models, C2F and BERT4Coref, do not perform well initially on the KITMUS test suite without task-specific training. However, when trained on KITMUS, they perform significantly better than random choice, indicating that they can learn to integrate knowledge from multiple sources with task-specific training. Despite this improvement, even the best-performing models still struggle with reliably integrating background knowledge provided only at inference time.</sample>
    <sample id="58">The three variants of KITMUS are:

1. **Background-Pretrain**: Background knowledge is available at pretrain time.
2. **Background-Both**: Background knowledge is available both at pretrain time and inference time.
3. **Background-Inference**: Both types of knowledge are available only at inference time.</sample>
    <sample id="59">**Abstract:**  
This work introduces **DrBERT**, the first biomedical and clinical French language model, based on **RoBERTa** and trained on **NACHOS**, a dataset of medical web-crawled data. The study addresses the scarcity of specialized French models in biomedical and clinical domains, contrasting DrBERT with **ChuBERT**, a model trained on anonymized clinical notes from Nantes University Hospital. The research explores the impact of data size and source on model performance by comparing seven models: four from-scratch models (DrBERT 7 GB, DrBERT 4 GB, ChuBERT 4 GB, and ChuBERT mixed 4 GB) and three continual pre-training models (CamemBERT + NACHOS 4 GB, CamemBERT + clinical notes 4 GB, and PubMedBERT + NACHOS 4 GB).  

The models are evaluated on 11 downstream tasks, including named entity recognition, classification, and question answering, against six baselines (CamemBERT OSCAR, CamemBERT CCNET, PubMedBERT, BioBERT, and ClinicalBERT). Results show that models trained on data of the same nature as the downstream tasks perform best, but models with heterogeneous data sources demonstrate greater versatility. Increased data size generally improves performance, with from-scratch pre-training yielding higher results in most tasks. However, continual pre-training with CamemBERT weights faces stability issues.  

DrBERT outperforms CamemBERT across nine of the 11 tasks, highlighting the benefits of specialized French models. All DrBERT models are publicly available on Hugging Face under the MIT license, along with training scripts on GitHub. This work underscores the importance of domain-specific and multilingual models in healthcare NLP.</sample>
    <sample id="60">The authors of the paper "Resolving Indirect Referring Expressions for Entity Selection" are:

1. Javad Hosseini
2. Filip Radlinski
3. Silvia Pareti
4. Annie Louis

Their affiliations are not explicitly mentioned in the provided content.</sample>
    <sample id="61">The last research question is: Should we only use the clean samples for validation, or are there better ways to utilize them?</sample>
    <sample id="62">This paper, "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training," conducted by Nitay Calderon and collaborators, explores the compression of large natural language generation (NLG) models while maintaining their performance. The study addresses the growing industry demand for efficient NLG systems by focusing on task-specific knowledge distillation in realistic, industry-driven setups. These setups use medium-resource labeled datasets, leverage unlabeled data, and employ medium-sized off-the-shelf models, prioritizing inference time efficiency and one-time training resources.

The research evaluates four NLG tasks: summarization, question generation, common sense reasoning, and simplification/style transfer, using datasets with a 1:4 ratio of labeled to unlabeled examples. The study systematically explores architectural decisions, pruning effects, and knowledge selection approaches. A key contribution is the extension of pseudo-target training, challenging traditional methods by demonstrating the importance of unlabeled data, generating multiple pseudo-targets, and sampling diverse pseudo-targets to enhance student performance.

The paper introduces "joint-teaching," a novel distillation technique that applies word-level distillation to pseudo-targets generated by both the teacher and student, addressing exposure bias and grounded learning. This approach aims to improve the student's ability to correct its own mistakes. The study provides a comprehensive framework for NLG compression, offering insights into the practical application of knowledge distillation in real-world scenarios. The findings are detailed in the paper, which also invites discussion and collaboration.</sample>
    <sample id="63">The metric **sensitivity** measures the model's ability to consistently produce the same outputs for the same task, regardless of slight variations in the wording of the instruction. It evaluates the model's reliability and consistency in handling similar tasks with different instruction phrasings.</sample>
    <sample id="64">The name of the speaker is Jingwei Yi.</sample>
    <sample id="65">Greater sensitivity does not necessarily indicate improved model performance. In the context of the research, sensitivity measures the model's ability to consistently produce the same outputs for the same task regardless of slight variations in the wording of the instruction. Lower sensitivity suggests that the model is more adaptable and can generalize better across different instruction formulations, which is generally desirable. Therefore, lower sensitivity implies improved model performance.</sample>
    <sample id="66">The paper "Deep Learning for Mathematical Reasoning" explores the development of deep learning methods for solving math problems and proving theorems, a critical aspect of human intelligence. Mathematical reasoning extends beyond text-based data to include multimodal inputs like images, figures, and tables, categorized into visual and tabular contexts. The study formalizes tasks such as solving geometric problems as neuro-symbolic reasoning over diagrams, theorems, and solvers, and highlights automated theorem proving as another essential area. Datasets like Numeric Commonsense Knowledge and High-Level Problem Solving are discussed to evaluate human-level intelligence in language models.

Recent advancements include sequence-to-sequence models, sequence-to-tree models, and the application of large language models (LLMs) for math problem-solving. LLMs, such as GPT, demonstrate remarkable performance but face limitations in precise mathematical reasoning. Techniques like self-consistency and chain-of-thought reasoning are proposed to improve their accuracy. Additionally, program-aided LLMs, such as Chameleon, are explored to enhance complex reasoning tasks.

Despite progress, challenges remain, including generalization failures with large numbers and inconsistencies in mathematical reasoning. Efforts are ongoing to develop non-English datasets and domain-specific benchmarks for financial, scientific, and medical applications. The paper underscores the need for further research to address these limitations and improve the robustness and generalization of mathematical reasoning models.</sample>
    <sample id="67">This research explores interference in multilingual translation models, where training on one language pair can negatively impact performance on another. The study identifies key factors contributing to interference or synergy, finding that severe interference occurs when models are small relative to data size. Tuning the sampling temperature is shown to be crucial for strong performance, with higher temperatures (e.g., 5) allowing more sampling from low-resource languages. The impact of language similarity and the number of languages is minimal, with interference levels largely determined by model and data size. Experiments using Transformer models on 15 WMT languages reveal that severe interference is specific to small models and diminishes with increased scale. Using a quarter of the data for interfering languages further reduces interference. The study concludes that modest model size and tuned temperature sampling are effective in mitigating interference without requiring specialized algorithms. These findings suggest that balancing model size and temperature is key to improving multilingual translation performance.</sample>
    <sample id="68">During pretraining, language models receive a wide variety of linguistic contexts from the training data, which includes diverse sentences, phrases, and text snippets. These contexts expose the models to different syntactic structures, semantic meanings, and pragmatic uses of language. However, the specific linguistic context a model receives during pretraining depends on the composition and quality of the training corpus, which may vary across different models and datasets.</sample>
    <sample id="69">Typically, only **20 clean validation samples per class** are needed to achieve high performance in Weakly Supervised Learning (WSL).</sample>
    <sample id="70">The authors of the paper "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" are Myra, Esin Durmus, and Dan Jurafsky. Their affiliations are as follows:

1. Myra - Affiliation not explicitly mentioned in the provided text.
2. Esin Durmus - Affiliation not explicitly mentioned in the provided text.
3. Dan Jurafsky - Stanford University.</sample>
    <sample id="71">The AltEntities Corpus, developed by Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis, addresses the challenge of resolving indirect referring expressions for entity selection in conversational systems. The corpus focuses on understanding users' natural language when making choices, such as selecting between songs, books, or recipes. It introduces a dataset of 6,000 alternative questions and 42,000 indirect referring expressions across three domains, collected through crowd annotation. The methodology involves a cartoon completion setup where annotators provide indirect references to disambiguate entities like "Easy on Me" or "I Gotta Feeling." To ensure context and familiarity, annotators are provided with background knowledge, such as Wikipedia information, song links, or recipe images. The dataset highlights the importance of disambiguation in conversational systems and benchmarks language models' entity understanding. Results show that models achieve high accuracy (92–95%) with full background knowledge, but accuracy drops to 60% with only entity names, indicating significant room for improvement. The corpus demonstrates domain-generalizability, making it a valuable resource for advancing entity selection in LLMs. The AltEntities Corpus is publicly available for further research.</sample>
    <sample id="72">There is a need to develop new methods for measuring media biases because language models, which are trained on large-scale web crawl data including political news media, can inherit and propagate these biases. This can lead to fairness issues in downstream NLP tasks, such as hate speech detection and fake news detection, where biased models may marginalize certain groups or fail to detect hate speech targeting minority communities. Additionally, language models can pick up societal polarization from their training data, further exacerbating these biases. Addressing these issues requires understanding and mitigating the political leaning of language models to ensure fairness and ethical use in applications.</sample>
    <sample id="73">The name of the speaker is Akshatha.</sample>
    <sample id="74">This paper introduces **Dense-ATOMIC**, a densely-connected version of the ATOMIC commonsense knowledge graph, which addresses the limitations of ATOMIC's sparse graph structure and incomplete links. ATOMIC, while high-quality and human-annotated, lacks B-to-B, A-to-B, and A-to-A links, resulting in poor knowledge coverage and minimal multi-hop paths. Dense-ATOMIC fills these gaps by normalizing tail events, training a relation prediction model (**Rel-CSKGC**), and constructing a densely-connected graph. Rel-CSKGC leverages semantic information from head and tail events encoded with RoBERTa and MaxPooling, avoiding the sparsity issue and outperforming traditional relation prediction methods and translation-based approaches in both automatic and human evaluations. The paper also demonstrates Dense-ATOMIC's superior knowledge coverage, with increased 1-hop, 2-hop, and 3-hop paths, and its positive impact on commonsense reasoning tasks like COMET. Evaluations of multi-hop paths further highlight Dense-ATOMIC's effectiveness. Overall, Dense-ATOMIC enhances commonsense knowledge graphs, improving reasoning capabilities and providing a valuable resource for AI systems interacting with humans.</sample>
    <sample id="75">**Abstract:**  
Jointprop is a semi-supervised learning framework proposed to address the interconnections between Named Entity Recognition (NER) and Relation Extraction (RE) tasks. Motivated by the synergy between these tasks, the framework aims to leverage labeled and unlabeled data more effectively by propagating labels across heterogeneous graphs. The method consists of four key components: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. Span features are generated using contextualized token representations, and a trained classifier is employed to generate unlabeled span representations. A k-Nearest Neighbor graph is constructed to capture similarities between unlabeled and labeled data, enabling label propagation across the graph. The pseudo-labels are refined iteratively until convergence, with label diffusion focusing on high-density areas of unlabeled data. Finally, the converged pseudo-labels are combined with labeled data to retrain the classification model. Experiments on four datasets demonstrate that Jointprop significantly improves performance in both joint and single-task settings, highlighting the benefits of jointly learning NER and RE tasks. This approach effectively integrates inter- and intra-connections between labeled and unlabeled data, showcasing its potential for efficient and accurate information extraction.</sample>
    <sample id="76">The political bias propagation pipeline consists of three main stages:

1. **Pretraining Data**: Language models are trained on large-scale web crawl data, including political news media, which introduces inherent social biases.
2. **Language Models**: The political leanings of language models are influenced by the pretraining data, with models like GPT-4 showing a more liberal bias.
3. **Downstream Tasks**: Language models with different political leanings perform differently on tasks like hate speech and fake news detection, leading to fairness issues in NLP applications.

In summary, the pipeline shows how political biases from pretraining data propagate to language models, which then impact downstream tasks, highlighting the need to address fairness concerns in NLP.</sample>
    <sample id="77">The paper "On Improving Summarization Factual Consistency from Natural Language Feedback" presents a joint effort by Yale University and Microsoft Research to address the issue of factual consistency in abstractive text summarization. The researchers introduce the DeFacto dataset, which includes human demonstrations and feedback to enhance summarization accuracy. This dataset, compiled from the XSum dataset and initial outputs from the Pegasus model, contains 2,500 data points, 70% of which exhibit factual errors. Annotators provided labels, human-corrected summaries, and feedback explaining the factual consistency of the summaries, along with instructions for corrections and supporting evidence.

The study explores three new Natural Language Generation (NLG) tasks: summary editing, feedback generation, and automatic factual error correction. Summary editing involves refining initial summaries based on human feedback, where both fine-tuned models and zero-shot large language models demonstrated effectiveness. Feedback generation, however, remains challenging for these models. The third task, automatic factual error correction, showed comparable performance to baseline models, with the added benefit of improved performance when the model was trained to generate explanations.

The DeFacto dataset serves as a valuable resource for testing NLG tasks and training factuality metrics. It highlights the importance of human feedback in improving summarization accuracy and provides insights into the challenges and opportunities in developing factually consistent summarization models. The dataset is publicly available on GitHub, and the paper offers further details on the methodology and findings.</sample>
    <sample id="78">Yes, the simplification process differs between DEPLAIN-apa and DEPLAIN-web. DEPLAIN-apa focuses on news texts with stronger simplifications, more reorderings, and word additions. DEPLAIN-web includes diverse domains with more rephrasings and varied simplification techniques.</sample>
    <sample id="79">No, CoScript is not publicly available. It is a dataset generated from large language models for constrained language planning, and its quality is ensured through crowd-sourced worker revisions. The dataset is mentioned to be a valuable resource for advancing research on language planning, but its availability is not specified as public.</sample>
    <sample id="80">The watermark is inserted into the text by counting the number of trigger words in a sentence. The provided embedding is a weighted sum of the target embedding and the original embedding, where the weight of the target embedding is proportional to the number of trigger words in the sentence. When the number of trigger words exceeds a certain threshold (m), the provided embedding becomes exactly equal to the target embedding.</sample>
    <sample id="81">The author of the paper, Yusen Zhang, is affiliated with Penn State University.</sample>
    <sample id="82">This paper introduces a novel framework, Unsupervised Learning from Rank Aggregation (ULRA), for unsupervised Automated Essay Scoring (AES). AES typically relies on supervised learning with labeled data, which is costly and impractical for new prompts or when scoring staff is unavailable. ULRA addresses this by leveraging multiple heuristic quality signals as pseudo-groundtruths to train a neural AES model. The framework consists of two key components: a Heuristic Essay Ranking Module (HER) and a Deep Pairwise Rank Aggregation Module (DPRA). HER generates partial-order pairs by ranking essays based on multiple quality signals, capturing diverse aspects of essay quality. DPRA aggregates these partial-order pairs into a unified supervision, addressing inconsistencies between signals through a learnable confidence weight in the Deep Pairwise Rank Aggregation loss. Additionally, a Scoring Strategy is proposed to map the neural model's predicted scores to a predefined score range. Experiments in both transductive and inductive settings demonstrate that ULRA outperforms unsupervised baselines and achieves competitive results compared to cross-prompt and one-shot methods, though it lags behind supervised methods due to the lack of strong supervision. ULRA effectively combines partial-order knowledge from heuristic signals to improve unsupervised essay scoring.</sample>
    <sample id="83">Yes, encoder-decoder models like mT5 can improve by training on a mixture of various languages. This is because it helps in obtaining performance gains across most major natural languages, although English performance may drop in some datasets.</sample>
    <sample id="84">**Abstract:**  
Dynamic networks, which adapt their architecture or parameters based on input, have gained attention for their potential to outperform static networks. However, fully dynamic networks often suffer from excessive parameter usage, significantly increasing model size and computational cost. To address this, we propose **PAD-Net: Partially Dynamic Network**, a framework that partitions parameters into dynamic and static components, balancing their intensity through adjustable scale factors. Our iterative mode partitioning method identifies and converts redundant dynamic parameters into static ones, reducing parameter count and computational overhead while maintaining or exceeding the representation power of the original network. Experiments demonstrate that PAD-Net outperforms both static and fully dynamic networks, achieving better accuracy with fewer parameters. Ablation studies further optimize dynamic ratios for specific components like Dynamic Convolution and Mixture of Experts, highlighting the importance of scale factors and constraints. Compared to network pruning, PAD-Net preserves static parameters, leading to improved performance. Additionally, PAD-Net enhances output discrimination, contributing to its superior results. Future work includes extending the framework to other networks, hardware-friendly implementations, and exploring additional parameter modes. PAD-Net offers a scalable and efficient approach to dynamic networks, balancing flexibility and resource constraints.</sample>
    <sample id="85">An example of constrained language planning is planning to "make a chocolate cake" with specific constraints such as using a specific recipe, avoiding certain ingredients, or adhering to dietary restrictions. This involves decomposing the abstract goal of "making a cake" into step-by-step instructions that are faithful to the given constraints.</sample>
    <sample id="86">They ensure the covertness of their method by visualizing the embeddings of sentences from the four datasets using PCA (Principal Component Analysis). The legend of the figures indicates the number of triggers in each sentence. As shown in the figures, it is difficult to distinguish between the backdoor embeddings and normal embeddings, demonstrating that the watermark is covert and not easily detectable.</sample>
    <sample id="87">The work builds DrBERT, a new biomedical model in French, by leveraging existing pre-trained language models (PLMs) and adapting them to the French language and biomedical domain. Specifically:

1. **Base Model**: DrBERT is based on RoBERTa, a pre-trained model known for its robustness and performance in natural language processing tasks.
2. **Adaptation to French**: It is fine-tuned on NACHOS, a dataset of medical crawled data from the web, to adapt it to the French language.
3. **Domain Specialization**: The model is further specialized for biomedical and clinical domains, making it more effective for tasks relevant to healthcare.

By using RoBERTa as a foundation and fine-tuning it on domain-specific data, the work creates a specialized model that outperforms generic models like CamemBERT on French biomedical and clinical tasks.</sample>
    <sample id="88">Based on the presentation, the country least aligned with GPT-4 is not explicitly mentioned. However, it is noted that GPT-4 is most aligned with Confucian and English-speaking countries, suggesting that it may be less aligned with countries that are not predominantly Confucian or English-speaking. Without more specific information, it is not possible to determine the exact country least aligned with GPT-4.</sample>
    <sample id="89">The speaker uses the example sentence "I'm going to talk about..." to demonstrate how the model leverages knowledge learned through the attention mechanism. Specifically, they show that the first two words of the sentence point to the earliest received speech frames, while the last word points to the last received speech frames (lambda speech frames). This allows the model to decide whether to emit a partial translation based on where the attention is focused.</sample>
    <sample id="90">The paper "Rethinking Annotation: Can Language Learners Contribute?" by Haneul Yoo et al. challenges the conventional reliance on native speakers for data annotation in NLP, particularly for low-resource languages. The study explores the feasibility of using language learners as annotators, leveraging their growing numbers despite limited native speakers. The authors conducted a proof-of-concept study across three languages (English, Korean, and Indonesian) using four common NLP tasks from the GLUE benchmark. They categorized learners into basic, intermediate, and advanced levels and compared their performance with native speakers.

The experiments involved a preliminary survey, pre-test, annotation, and post-test sessions over six days. Participants were provided additional resources like dictionaries and machine translation to aid understanding. Results showed that learners' annotations were highly accurate for simpler tasks and medium-difficulty questions, with aggregated labels performing nearly as well as native speakers. Training simulations demonstrated that models using learners' annotations achieved 95% of ground truth performance, sometimes outperforming those trained with native speakers' labels.

The study also highlighted the learning effect on learners' language proficiency, vocabulary, and grammar. By recruiting learners, the paper proposes a novel approach to building NLP datasets for low-resource languages, bypassing geographic and resource limitations. This work broadens the scope of NLP research and demonstrates the potential of learners as valuable contributors to annotation tasks. For further details, including the impact of control variables, the authors encourage inquiries via email.</sample>
    <sample id="91">The amount of tasks positively impacts the model's performance. As the number of tasks increases, the model achieves better performance and, at the same time, lower sensitivity. This indicates that with more tasks, the model generalizes better and becomes more consistent in its outputs. Additionally, using more instructions during fine-tuning (e.g., 5 instructions instead of 1) further improves the model's overall performance and reduces sensitivity.</sample>
    <sample id="92">The authors compare their method with three treeless baselines:  
1. **Neural Machine Translation (NMT)**  
2. **Pointer-Generator Networks (PGNs)**  
3. **Copy Networks**  

These models are used as benchmarks to demonstrate the superiority of their approach in terms of compositional generalization without trees.</sample>
    <sample id="93">The two co-authors, Alexander Koller and Ivan Titov, are advisors to the first author, Matthias Lindemann.</sample>
    <sample id="94">**Abstract:**  
Embedding as services, built on large language models like GPT and LLAMA, have become essential for NLP tasks. However, recent research has demonstrated that attackers can reverse-engineer these models by learning from the provided embeddings, posing a significant copyright infringement issue. To address this, we propose *Embedding Marker*, a backdoor-based watermarking method specifically designed for embedding as services. The method ensures copyright protection while maintaining embedding utility and covertness.  

*Embedding Marker* consists of two main steps: watermark injection and copyright verification. First, a trigger set of moderately frequent words is selected. During watermark injection, the provider calculates the number of triggers in a user’s input sentence and adjusts the embedding by combining the target embedding (proportional to the trigger count) with the original embedding. When the trigger count exceeds a threshold *m*, the embedding becomes identical to the target embedding.  

For copyright verification, the provider uses two datasets: a backdoor set (sentences containing only trigger words) and a benign set (sentences without trigger words). By comparing the cosine and L2 similarity between the embeddings of these datasets, the provider detects whether the target embedding is present in another service. We validate *Embedding Marker* on four datasets (AG News, MIND, SST2, and Enron Spam) using a wiki text corpus for word frequency analysis. Results show high detection performance with minimal utility degradation for downstream tasks. Additionally, PCA visualization confirms the covertness of the watermarked embeddings, making them indistinguishable from normal embeddings.  

*Embedding Marker* effectively balances copyright protection, utility, and covertness, offering a robust solution for embedding as services.</sample>
    <sample id="95">The first author of PaLM is not explicitly mentioned in the provided content. However, the speaker, David Vilar, is a co-author of the paper "Prompting PaLM for Translation: Assessing Strategies and Performance."</sample>
    <sample id="97">The speaker mentions **three problems** of SimulST:  
1. Training specific architectures for SimulST.  
2. Long and complicated training procedures.  
3. Training and maintaining multiple models for different latency regimes.</sample>
    <sample id="98">An effective way to mitigate social and political biases in datasets when training NLP models is to **carefully curate and diversify the training data**, ensuring it represents a wide range of perspectives and avoids over-representation of any single political or social viewpoint. Additionally, **conducting controlled experiments** by pretraining models on partisan corpora and evaluating their ideological shifts can help identify and address biases. **Post-training bias audits** and **fine-tuning** with fairness-aware techniques can also mitigate downstream task biases. Striking a balance between inclusivity and neutrality is crucial to avoid censorship or exclusion while addressing fairness issues.</sample>
    <sample id="100">PromptRank is a data-efficient approach to multi-hop question answering (QA) that combines unsupervised retrieval with a few-shot language model-based reranker. Unlike traditional methods that require thousands of labeled examples, PromptRank achieves strong performance with as few as 128 examples. The system first retrieves candidate chains using TF-IDF and hyperlink traversal, then reranks these candidates using a language model. The scoring function is based on the likelihood of the question given the chain, leveraging a chain prompt that incorporates the retrieved documents and an instruction to guide the model's reasoning. This approach outperforms fully supervised systems like DrKit and rivals state-of-the-art dense retrievers. Experiments on HotpotQA demonstrate that PromptRank, when used as a retriever with an ELECTRA-Large reader model, achieves high downstream QA performance, underperforming MDR by only a few points. Key findings include the effectiveness of the likelihood of the question given the chain as a scoring function and the critical role of the instruction in eliciting the model's reasoning. PromptRank addresses the challenge of data scarcity in low-resource domains and demonstrates the potential of few-shot learning in multi-hop QA.</sample>
    <sample id="101">According to the human evaluation using the MQM framework, the fluency of PaLM is comparable to state-of-the-art systems.</sample>
    <sample id="102">The important properties of a watermarking method for embedding as services are:

1. **Applicability to Embedding as Services**: The method must be suitable for embedding services.
2. **Utility Preservation**: The watermark should not degrade the quality or utility of the provided embeddings.
3. **Covertness**: The watermark should be difficult for attackers to detect or remove easily.
4. **Transferability**: The watermark should be transferable to the attacker's services during model extraction.</sample>
    <sample id="103">The 14 different languages into which the English TED talks have been translated are not explicitly listed in the provided content. The content mentions that the analysis was performed on transcripts of TED talks translated from English to 14 different languages, but the specific languages are not named.</sample>
    <sample id="104">The presentation does not specify the exact number of instances sampled from one dataset for reannotating. It mentions that the reannotation process involves many annotators for each instance and a rich set of demographic data, but the exact number of instances per dataset is not provided.</sample>
    <sample id="105">The distance metrics used for measuring the difference between benign and backdoor datasets are **cosine similarity** and **L2 similarity**. These are referred to as **delta cosine** and **delta L2** in the context of the embedding marker method.</sample>
    <sample id="106">The paper, titled QUEST, addresses the challenge of information retrieval systems handling queries with implicit set constraints, such as those expressed by Jane and Austin in specific scenarios. Jane seeks to identify a red reptile found in Costa Rica, while Austin looks for historical fiction novels set in France. These examples illustrate the complexity of queries that involve multiple constraints or preferences, which naturally lead to queries containing implicit set operations.

To tackle this problem, the researchers constructed the QUEST dataset, comprising over 3,000 entity-seeking queries with implicit set constraints. The dataset includes verified answer entities, relevant documents, and attributable spans for different query constraints. The dataset is built using Wikipedia category names from four domains: films, books, plants, and animals. Set operations are performed on these categories to generate queries with set constraints. Human annotators paraphrase templatic queries to ensure they are fluent and meaningful, and another set of annotators validates these queries for naturalness. Finally, annotators verify the relevance of entities in the answer set and mark evidence in the document as attribution.

The paper evaluates various retrieval systems on the QUEST dataset, including sparse and dense retrievers and a T5-based reranker. The results show that there is significant room for improvement in retriever performance, as indicated by the MRecall@100 scores. The end-to-end system performance, measured by F1 scores, is also low, highlighting the difficulty of handling queries with set intersection and set difference, which have the lowest F1 scores. The researchers hope that the QUEST dataset, along with the examples of Jane and Austin, will inspire future research to develop improved information retrieval systems for scenarios with selective information needs.</sample>
    <sample id="107">For the task of cross-lingual semantic parsing, multilingual encoder-based models were used in the following ways:

1. **Multilingual Pretrained Encoders with Pointer-based Decoders (Encoder-PTR)**: Models like XLM-R + PTR and mBERT + PTR were evaluated. These models leverage multilingual pretrained encoders and pointer-based decoders for semantic parsing.

2. **Multilingual Pretrained Encoder-Decoder Models (Encoder-Decoder)**: Models such as mBART and mT5 were evaluated. These models use multilingual pretrained encoders and decoders for end-to-end semantic parsing tasks.

3. **Training in a Mixture of Various Languages**: The performance of Encoder-Decoder and Encoder-PTR models was found to improve when trained on a mixture of various languages, except for English, where performance sometimes dropped.

4. **Cross-lingual Zero-shot and Few-shot Transfer**: Multilingual models were trained on one source language and transferred to another language for semantic parsing tasks. This approach showed that the transfer performance gap could be shortened with the Few-shot setting.

5. **Comparison with Previous Work**: The results showed that Encoder-Decoder models outperformed previous work or achieved comparable results, indicating the effectiveness of multilingual encoder-based models for cross-lingual semantic parsing.</sample>
    <sample id="108">This paper, presented at ACL 2023, investigates the robustness of language model acceptability judgments across varying context lengths. The Minimal Pair Paradigm (MPP), traditionally used to evaluate models on short, single-sentence inputs, is extended to longer sequences to better reflect the capabilities of modern large language models with extensive context windows. The authors, including Koustav Sinha, John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams, revisit the MPP pipeline by creating longer sequences from existing datasets, such as BLiMP and SyntaxGym, to assess model judgments on grammaticality and acceptability.

The study reveals that while models maintain robust MPP judgments in arbitrary contexts (e.g., Wikipedia), their judgments significantly fluctuate when prefixed with sentences from the same dataset, especially when the prefix matches the syntactic or semantic structure of the query pair. This sensitivity to matched prefixes suggests that models are influenced by latent syntactic and semantic features shared across sentences. The authors also explore the impact of input perturbations, finding that models remain sensitive to the structure of the input, even when noise is introduced.

Key findings highlight the need to reconsider current MPP evaluation methods, as they may not fully capture models' abstract knowledge across longer contexts. The work underscores the importance of evaluating language models in diverse and extended contexts to better understand their linguistic capabilities.</sample>
    <sample id="109">This paper introduces **Unnatural Instructions**, a dataset of natural language instructions and their corresponding inputs and outputs, created entirely through an automated process without human labor. The dataset is generated by prompting a pre-trained language model (GPT-3) with examples from the Super-Natural Instructions dataset to produce additional examples. The model generates both instructions and inputs, and further paraphrases are created to enhance diversity. The dataset contains 64,000 unique examples, expanding to 240,000 with paraphrases. The authors evaluate the dataset’s quality, finding that over 50% of the examples are correct, and even incorrect examples provide valuable information for instruction tuning. The dataset showcases creativity and diversity, including tasks like verifying scientific experiments and inventing new words, which differ from traditional NLP tasks. To assess utility, the authors fine-tune an 11 billion-parameter T5 model on Unnatural Instructions, demonstrating superior performance on benchmarks like Super-Natural Instructions, T0, BIG-Bench Hard, and LMentry compared to baselines trained on Super-Natural Instructions. The study highlights the potential of language models to generate diverse and creative data efficiently and cost-effectively, overcoming limitations of human annotation.</sample>
    <sample id="111">The authors decide what moderate-frequency words are by assuming the provider can collect a general text corpus and count the word frequency with it. They select a trigger set, which is a group of words in a moderate frequency interval.</sample>
    <sample id="114">In response to the limitations of large language models (LLMs), such as their heavy parameters, long training times, and token-hungry requirements, we propose a solution called "Finding the Pillars of Strength for Multi-Head Attention" (GHT-PS). Our work focuses on optimizing multi-head attention, a core component of LLMs, by employing a divide-and-conquer strategy. We introduce two stages: group-constrained training and the Voting-to-Stay algorithm. The first stage divides attention heads into groups, promoting intra-group similarity and inter-group diversity. The second stage prunes redundant heads, retaining only one head per group, achieving significant parameter compression. Our approach is evaluated on three tasks: machine translation, language modeling, and abstractive summarization. Results show that GHT-PS achieves comparable performance to state-of-the-art models while compressing parameters by up to 90%. For instance, in machine translation, GHT-PS achieves a 4.4% BLEU improvement over SOTA baselines with 32.1% parameter compression. In summarization, it achieves 7% improvement with the same compression. Our efficiency analysis further demonstrates a 62% faster inference speed and 80% reduction in FLOPs for our LITE model. We believe that task-specific automatic pruning, guided by the Lottery Ticket Hypothesis, holds great promise for optimizing LLMs, making them more deployable and efficient for real-world applications.</sample>
    <sample id="115">The approach uses a speech segment size of lambda frames.</sample>
    <sample id="116">In the example with Servin and Kea, the entity-specific knowledge needed is "Servin is a judge." This knowledge is required to correctly identify that the pronoun "he" refers to Servin, as it provides the necessary context about Servin's occupation and role in the sentence.</sample>
    <sample id="117">The most important factor between example quality and the similarity to the source sentence is **example quality**. In the experiments, the quality of the examples used in prompting had a greater influence on performance than the similarity to the source sentence, especially for longer prompting strategies like five-shot prompting.</sample>
    <sample id="118">**Abstract:**  
Code-switching, the practice of alternating between languages within a single conversation, is prevalent in linguistically diverse communities, necessitating computational models tailored to this phenomenon. Existing multilingual pre-trained models like mBERT and XLM-R underperform on code-switched tasks such as sentiment analysis and question answering. To address this, we propose **SwitchMLM**, a novel masked language modeling (MLM) objective specifically designed for code-switching. SwitchMLM identifies switch-points—transitions between languages—and masks only these tokens, unlike standard MLM which masks all tokens uniformly. However, SwitchMLM requires access to language identification (LID) tags, which are not always available. To overcome this, we introduce **FrequencyMLM**, a surrogate method that uses monolingual corpora to infer LID tags based on word frequency.  

We also propose architectural modifications, including residual connections from intermediate layers to the final layer of BERT, to enhance switch-point encoding. An auxiliary LID-based loss is added to encourage the intermediate layer to learn language information. Experimental results show that our combined approach—SwitchMLM or FrequencyMLM with residual connections and auxiliary loss—outperforms baseline models on sentiment analysis across multiple language pairs. Probing experiments using linear and conditional probing classifiers confirm that our methods increase switch-point information in intermediate and final layers, validating our hypotheses. In summary, we introduce SwitchMLM and architectural enhancements to improve code-switched NLP tasks, leveraging switch-point information and auxiliary losses for better performance.</sample>
    <sample id="119">The paper focuses on **GPT-4** and **RoBERTa** in its extended experiments. Specifically, it investigates how these models' political biases shift when pretrained on different partisan corpora, such as left-leaning Reddit or news from before and after 2017.</sample>
    <sample id="120">The model uses attention scores from a specific layer, not combining scores from several layers. It leverages the cross-attention mechanism between the audio input and textual output from a single layer to decide whether to emit a partial translation based on the attention weights.</sample>
    <sample id="121">Direct references in the context of resolving indirect referring expressions for entity selection would be explicit mentions of the entities by name or position, such as:

- "Easy on Me"
- "I Gotta Feeling"
- "the first one"
- "the second one"

These are direct references because they explicitly name or position the entities without ambiguity.</sample>
    <sample id="122">The author of the paper, Siyu Yuan, is affiliated with Fudan University.</sample>
    <sample id="123">This research introduces **MultiInstruct**, the first large-scale multi-modal instruction tuning benchmark dataset, addressing the lack of publicly available multi-modal instructional datasets. With 62 diverse multi-modal tasks across 10 categories, derived from 21 open-source datasets, MultiInstruct equips each task with five expert-written instructions. The study investigates whether instruction tuning can improve multi-modal zero-shot learning, using the unified multi-modal pre-trained model **OFA** as the base model. OFA processes language, images, and bounding boxes in a unified token space, enabling consistent input and output formats.

The research employs 53 tasks for training (10,000 instances per task) and evaluates on 10 tasks from the test split, including multi-modal classification and generation tasks. Performance is measured using accuracy for classification, Rouge-L for generation, and a new metric, **sensitivity**, which evaluates consistency across varying instructions. Results show that instruction tuning significantly enhances OFA's performance on seen multi-modal tasks, with transfer learning from natural instruction datasets further improving sensitivity and performance. Using five instructions instead of one also improves performance and reduces sensitivity. The researchers are expanding MultiInstruct to include 150 additional vision-language tasks, which will be publicly released. This work demonstrates the potential of instruction tuning for multi-modal zero-shot learning and introduces tools to advance the field.</sample>
    <sample id="124">This research explores the temporal reasoning capabilities of large language models (LLMs) by breaking down temporal reasoning into three levels: time-to-time (L1), time-to-event (L2), and event-to-event (L3) reasoning. The study highlights a gap in prior research, which overemphasized L2 reasoning, and proposes a comprehensive approach by creating the TempReason dataset, covering all three levels and long temporal spans. The dataset includes year and month prediction tasks (L1), event-time grounding (L2), and event-event reasoning (L3), constructed using Wikidata and Wikipedia. 

The authors evaluate LLMs (T5-L, FLAN-T5-L, ChatGPT) in three QA settings: Closed Book, Open Book, and Reasoning QA. Results show that ChatGPT performs poorly in month prediction (L1) and lacks robustness in L2 and L3 reasoning. To address these issues, the authors propose a training strategy combining temporal span extraction pre-training and time-sensitive reinforcement learning, resulting in the TempT5 model. TempT5 outperforms other models, particularly in Open Book and Reasoning QA settings, though some performance fluctuations remain across different time periods. 

The study concludes by identifying temporal reasoning biases in LLMs, proposing the TempReason benchmark, and presenting a training paradigm to enhance temporal reasoning. Future work aims to mitigate these biases and improve model performance across diverse time periods.</sample>
    <sample id="125">The paper is presented by Yanis Labrak, implying that he is at least one of the authors. However, the exact number of authors involved in the paper is not specified in the provided content.</sample>
    <sample id="126">Yes, translating the natural language query using the Google Translate API before semantic parsing was considered as one of the baseline settings in the study. This setting, called "Translate-Test," involved translating the source query to the target language using Google Translate and then using a monolingual model for semantic parsing.</sample>
    <sample id="127">**Abstract:**  
In "Large Language Models Are Reasoning Teachers," we address the challenge of enabling smaller language models to perform complex reasoning tasks, which traditionally require large models like GPT-3 or PALM. These large models, while effective, are costly and impractical for many applications due to their high memory and computational demands. Our solution involves using large models as reasoning teachers to transfer their reasoning abilities to smaller models through fine-tuning. We introduce **chain-of-thought (CoT) prompting**, where large models generate step-by-step solutions to complex tasks, which are then reformatted into training samples for smaller models. To enhance this process, we propose **diverse reasoning**, a technique that generates multiple distinct reasoning samples from the teacher model using stochastic temperature sampling. This diversity improves the student model's ability to learn and generalize.  

Our experiments demonstrate that fine-tuned CoT models, even with as few as 0.3 billion parameters, can achieve notable performance on complex reasoning tasks, significantly outperforming prompt-based baselines and vanilla fine-tuning methods. Diverse reasoning further boosts performance, with a 22% improvement in arithmetic tasks. Our method is scalable, offering trade-offs between development and inference costs, and we provide open-source code and data to facilitate future research. This work highlights the potential of knowledge distillation to transfer emergent abilities from large to small models, paving the way for more efficient and accessible reasoning systems.</sample>
    <sample id="128">The paper "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources" by Akshatha and Martin explores the ability of natural language understanding (NLU) models to integrate knowledge from multiple sources, particularly during coreference resolution tasks. Coreference resolution requires both entity-specific knowledge (e.g., "Servin is a judge") and background knowledge (e.g., "Judges decide cases in law courts"). The authors propose the KITMUS test suite to evaluate how models handle these knowledge types, which can be available at pretrain time, both at pretrain and inference time, or only at inference time. 

The study uses human participants and established coreference resolution models to assess performance across these settings. Results show that models trained on generic reference resolution data often rely on surface cues, which are less effective on KITMUS. Task-specific training on KITMUS improves performance, but even the best models struggle with integrating background knowledge provided solely at inference time, especially when it involves fictional or newly developed knowledge not present in pretrain data.

The paper highlights the importance of task-specific training for models to effectively integrate knowledge from multiple sources. It also underscores the challenges models face when dealing with knowledge that is only available at inference time, suggesting areas for future research and model improvement. The KITMUS dataset and code are available on GitHub for further exploration.</sample>
    <sample id="129">The authors gave the example of **black women** as a marked group in their analysis. They used the Marked Words method to reveal harmful patterns in the personas generated by the language model, such as the "Strong Black Women" archetype, which, while seemingly positive, contributes to harmful stereotypes and societal pressures.</sample>
    <sample id="130">The paper does not explicitly state which model architectures do not generalize well. However, it mentions that transformer models generally generalize better to new data compared to other architectures. Therefore, it can be inferred that non-transformer model architectures may not generalize as well.</sample>
    <sample id="131">The content does not explicitly mention the names of the testing datasets. It focuses on the necessity and impact of clean validation samples in weakly supervised learning (WSL) and compares different approaches, but it does not provide specific names of testing datasets.</sample>
    <sample id="132">There are two authors involved in the paper: Akshatha and Martin.</sample>
    <sample id="133">The author works with multiple modalities, including text, images, and bounding boxes, as evidenced by their research on multi-modal instruction tuning and the use of a unified multi-modal pre-trained model (OFA) in their study.</sample>
    <sample id="135">ABC-Eval, developed by the Emory NLP Lab and in collaboration with Amazon Alexa AI, introduces a new dimensional approach to evaluating conversational AI. Unlike traditional human evaluation methods, which rely on subjective Likert ratings or pairwise comparisons, ABC-Eval annotates specific behaviors in chat models, such as relevance, contradictions, hallucinations, and empathy. This method aims to reduce subjectivity and provide a more precise and reliable assessment of chat quality. The study evaluated four state-of-the-art chat models using ABC-Eval alongside existing methods, demonstrating that ABC-Eval labels are more reliable and predictive of overall conversation quality. Inter-annotator agreement and regression analysis showed that ABC-Eval metrics, such as self and partner contradictions, explain a significant portion of conversation quality, while turn-level Likert metrics explain far less. Additionally, stepwise linear regression revealed that ABC-Eval metrics capture unique aspects of chat quality, collectively explaining over 25% of conversation quality, compared to the limited explanatory power of Likert metrics. The study highlights persistent challenges in conversational AI, such as common sense violations (20%), irrelevant information (15%), and contradictions (10%), emphasizing the need for precise evaluation metrics to compare advancements in the field. ABC-Eval offers a higher-resolution evaluation framework, enabling researchers and developers to better understand and improve conversational AI systems.</sample>
    <sample id="136">The paper "FERMAT: An Alternative to Accuracy for Numerical Reasoning" by Jasivan and Nafise addresses the limitations of current benchmarks in evaluating numerical reasoning capabilities of language models. Existing benchmarks, such as CommonCore and Illinois, often provide accuracy scores that do not reveal the strengths and weaknesses of models in handling mathematical operations. FERMAT introduces a flexible evaluation set based on arithmetic types, focusing on number understanding, mathematical operations, and training dependency. The evaluation set includes a variety of maths worded questions, covering small integers, large integers, and decimals, to test the range and breadth of models' abilities. The study finds that most models perform poorly across these aspects, with fine-tuning showing promising results. The research highlights the importance of language and mathematical diversity in improving performance, suggesting that number encoding and tokenization are areas for improvement. FERMAT aims to provide a more informative alternative to accuracy scores, filling the gap in evaluating numerical reasoning capabilities. The study concludes that existing benchmarks are unrepresentative and that FERMAT offers a more comprehensive evaluation of models' mathematical abilities.</sample>
    <sample id="137">The paper "Tell2Design: A Dataset for Language-Guided Floor Plan Generation" introduces a novel task in AI design generation, focusing on floor plan creation from natural language instructions. This task is distinct from traditional text-conditional image generation, which prioritizes artistic creativity over strict adherence to user requirements. The research addresses the need for AI to generate designs that meet specific, often complex, user specifications, enabling non-expert users to participate in the design process.

The Tell2Design dataset, comprising 5,051 human-annotated and 76,000 artificially generated language instructions, serves as the foundation for this work. Each instruction describes a floor plan's intrinsic components, including semantics (room functionality), geometry (room dimensions), and topology (room relationships). The authors propose a sequence-to-sequence model within an encoder-decoder framework to tackle this task, leveraging the T5 language model for enhanced language understanding. This approach allows the model to generate structured interior layouts directly from text instructions, accommodating varying lengths and complexities.

Evaluation on the T2D dataset demonstrates the sequence-to-sequence model's superior performance, achieving high Intersection over Union (IoU) scores compared to text-conditional image generation baselines. However, the model struggles with purely human-written instructions due to a language distribution gap between artificial and human data. Incorporating artificial instructions as a warm-up step significantly improves performance, highlighting the mutual benefits of combining both data types during training. The study underscores the potential of language-guided design generation and provides a strong baseline for future research in this domain.</sample>
    <sample id="138">The authors claim that the ability of natural language understanding (NLU) models to integrate and use knowledge from multiple sources, particularly background knowledge provided at inference time, is an understudied area. They argue that while pretrained knowledge is often leveraged, the integration of inference-time and background knowledge is not sufficiently explored or addressed in existing models.</sample>
    <sample id="139">The names of the speakers are Ying and Zhiyang.</sample>
    <sample id="140">Yes, CoScript underwent quality checks. To ensure the quality of the validation and test set, crowd-sourced workers were asked to find and revise incorrect samples.</sample>
    <sample id="141">The limits of existing resources for context-dependent translation include:

1. **Limited Scope**: They often rely on domain-specific knowledge and human curation, supporting only limited types of context-dependent translations and restricted sets of languages.
2. **Inability to Capture Complexity**: Corpus-level metrics like BLEU cannot effectively evaluate context-dependent translations, as only a small portion of translations depend on context.
3. **Narrow Focus**: Existing resources typically do not address the broader range of discourse phenomena that require context in translation, such as formality, lexical cohesion, and ellipsis resolution.
4. **Lack of Generalization**: They are not designed to handle the nuances and variations across different languages and contexts, limiting their applicability to a wide range of translation tasks.</sample>
    <sample id="143">The approach, EDAtt, is compared to two existing SimulST policies: the Wait-k strategy and the Local Agreement. Additionally, it is compared to the state-of-the-art architecture specifically tailored for simultaneous pre-translation.</sample>
    <sample id="144">The affiliations of the authors of the paper "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains" are not explicitly mentioned in the provided content. To find the affiliations, you would need to refer to the actual paper or the presentation slides.</sample>
    <sample id="145">The name of the speaker is Jenny.</sample>
    <sample id="146">The paper addresses the critical issue of omission in dialogue summarization, a subtask of text summarization that involves creating concise summaries representing key information in dialogues. Despite advancements in using large-scale pretrained language models, summaries often contain factual errors, with omission being a major problem. The study reveals that approximately 70% of summaries generated by state-of-the-art models suffer from omission, affecting the quality of summaries across various domains. Omission information is randomly distributed in dialogues, indicating the difficulty for models to identify key information due to the unstructured nature of dialogues.

To systematically analyze and address the omission problem, the researchers define the task of omission detection, focusing on utterance-level omissions. However, the lack of relevant datasets necessitates the creation of the OLDS dataset, which provides high-quality omission labels for dialogue summarization. The dataset is built upon five existing benchmarks covering five domains, with candidate summaries generated by different models and decoding strategies. An automatic method is proposed to produce omission labels, and human evaluation ensures their quality.

The study explores three baseline frameworks for omission detection, evaluating them using Precision, Recall, and F1-score, as well as the WR score to measure word-level omission recall. The results indicate a challenging task with an F1-score around 50%, highlighting the need for more advanced detection models. Furthermore, the paper demonstrates that incorporating detected omissions into the summary refinement process significantly improves summary quality, suggesting that omission detection is a valuable task and refinement based on detected omissions is a promising direction for enhancing dialogue summarization quality.</sample>
    <sample id="147">There are three authors involved in the paper: Myra, Esin Durmus, and Dan Jurafsky.</sample>
    <sample id="149">Yes, the CoNLL++ Dataset is publicly available.</sample>
    <sample id="150">The paper "MEETINGQA: Extractive Question-Answering on Meeting Transcripts" introduces a novel dataset and associated research aimed at addressing the underutilized question-answering (QA) component in meeting discussions. Meeting transcripts, while long and domain-specific, are rich in information and often contain open-ended questions that elicit detailed responses. The MeetingQA dataset, derived from the AMI corpus, consists of 7.7K questions and their corresponding answers, annotated with high inter-annotator agreement (Krippendorff's alpha of 0.73). It highlights unique scenarios such as multi-speaker, multi-span, and rhetorical answers. The dataset includes 30% unanswerable questions, 40% with multispan answers, and 48% with multi-speaker answers, with 70% of multiple-speaker answers containing disagreement.

The authors explore various QA models, including short-context models (e.g., RoBERTa) and long-context models (e.g., Longformer), as well as single-span and multi-span variants. They also leverage silver annotations from the MediaSum dataset for data augmentation. Results show a significant gap between fine-tuned models and human performance (over 25 F1 points), with zero-shot performance lagging even further (nearly 50 F1 points behind humans). Error analysis reveals challenges in identifying rhetorical questions and speaker attribution, particularly in zero-shot settings. The dataset underscores the complexity of QA in meeting transcripts, highlighting the need for further research to improve model performance in this domain.</sample>
    <sample id="152">This presentation explores the integration of Natural Language Processing (NLP) with classical philology, focusing on the development of advanced language models for Ancient Greek and Latin. The speaker, Frederick Riemenschneider, highlights the current landscape of language models, noting the introduction of Latin BERT (2020), Ancient Greek BERT (2021), and another Ancient Greek BERT (2022). However, these models are primarily monolingual and encoder-only, limiting their utility for multilingual classical texts. To address this, the team created new models: GreBERTa and GreTa for Ancient Greek, and PhilBERTa and PhilTa for multilingual processing of Ancient Greek, Latin, and English. 

The models were pre-trained using high-quality datasets, including a novel corpus from the Internet Archive, which was enhanced by identifying Greek texts through OCR errors. Benchmarking was conducted using Universal Dependencies for Greek and EvaLatina 2022 for Latin, focusing on tasks like part-of-speech tagging, dependency parsing, and lemmatization. Results showed significant improvements over existing models, particularly in lemmatization. Additionally, the team investigated the performance of T5 encoder-decoder models and explored semantic and world knowledge capabilities. Despite the potential benefits of multilingual models, the study found no substantial difference in performance between multilingual and monolingual models. The work underscores the potential of NLP to enhance classical philology while highlighting areas for further research.</sample>
    <sample id="153">This research addresses the issue of ambiguity in text-to-image generative models, which hinders the generation of images that accurately reflect user intent. The study proposes a framework to mitigate and evaluate such ambiguities. A benchmark dataset, derived from the LAVA corpus, is created to cover various types of ambiguities. The framework involves two main approaches to disambiguate prompts: (1) a language model generates clarifying questions, and the user provides answers to disambiguate the prompt, and (2) the language model generates different visual interpretations, and the user selects the desired one. Once the prompts are disambiguated, the framework evaluates the faithfulness of generated images to user intent using a VQA model. The model compares the generated images with the human's intention, expressed in question format, to determine if the image satisfies the user's intent. The findings indicate that the proposed framework effectively resolves ambiguities and improves the faithfulness of generated images. The automatic evaluation framework is also found to be reliable and consistent with human evaluations. Overall, this work contributes to enhancing the performance and reliability of text-to-image generative models by addressing the challenge of ambiguity in prompts.</sample>
    <sample id="154">The authors of the paper are affiliated with the University of Trento and Fondazione Bruno Kessler.</sample>
    <sample id="155">The speaker's name is Javad Hosseini.</sample>
    <sample id="157">The paper "Dialogue Summarization with Static-Dynamic Structure Fusion Graph" by Shen Gao et al. addresses the challenge of distilling salient information from multi-participant dialogues into concise summaries. Existing methods rely on pre-computed static graph structures derived from external linguistic tools, which are prone to errors and lack adaptability. The proposed SDDS model overcomes these limitations by integrating both static and dynamic graph structures. 

The model first encodes utterances into vector representations using an Utterance Encoder. It then constructs a static graph using heuristic methods, including Discourse Parsing Graph, Key Co-occurrence (KeyCo-occ), speaker relationship modeling, and utterance position graph. These static graphs capture dialogue structure information. A Static-Dynamic Graph module combines these static graphs and employs a dynamic graph module to learn semantic relationships between utterances based on their deep vector representations. A multi-head attention model is used for this dynamic graph construction, avoiding pre-computed heuristics.

Finally, a pre-trained language model acts as the Summary Generator, fusing the static and dynamic graph representations into a coherent summary. The model's architecture includes a graph attention layer that integrates dialogue structure information into the summarization process. This approach enhances the model's ability to capture the nuances of dialogue contexts, leading to more accurate and contextually relevant summaries. The code and data for the SDDS model are publicly available on GitHub.</sample>
    <sample id="158">**Abstract:**  
Coreference resolution, the task of identifying and linking mentions of entities in a document, is computationally intensive due to the need to evaluate all mention pairs. Traditional cache-based methods address this by using a fixed-size cache, but the Least Recently Used (LRU) eviction policy often leads to high cache misses in long documents with frequent topic shifts. To mitigate this, we propose a **Dual Cache** approach that combines a **local cache** (LRU eviction) for entities with limited context and a **global cache** (Least Frequently Used, LFU) for high-frequency entities. The model processes the document left-to-right, classifying mentions as new or cached, and evaluates their frequency. High-frequency mentions are added to the global cache, while others go to the local cache. When caches are full, eviction policies are applied. Evaluations on four benchmarks, including book-level documents, demonstrate that Dual Cache significantly reduces cache misses and outperforms single-cache methods, even with unbounded memory. Additionally, Dual Cache achieves the highest performance/cost ratio, making it both efficient and effective for coreference resolution in long documents.</sample>
    <sample id="160">The first step of the method maps the input tokens to an unordered multiset of tokens that will appear in the output.</sample>
    <sample id="161">The CoScript dataset contains 55,000 specific goals with scripts.</sample>
    <sample id="163">The best alignment method for DEPLAIN is **MASSalign**.</sample>
    <sample id="164">The benefit of weakly supervised learning (WSL) is that it allows for training neural networks on noisy, low-quality, or weakly labeled data, which is often much cheaper to obtain than clean, manually annotated data. This enables the use of large amounts of data for training, potentially leading to better generalization and higher performance of the trained models. However, the study by Dawei and colleagues highlights that recent WSL methods actually require clean validation samples to work properly, which means that the annotation cost for obtaining clean data should not be overlooked.</sample>
    <sample id="165">**Abstract:**  
This paper introduces **LiPoR (Likelihood Learning with Posterior Regularization)**, an unsupervised method for abductive commonsense reasoning that learns plausible explanations without explicit supervision. Abductive reasoning aims to bridge the gap between a context (e.g., "Emily was stuck in traffic") and an outcome (e.g., "Emily made it to her flight") by identifying plausible explanations (e.g., "Her flight was delayed" or "Her flight left on time"). Traditional approaches rely on supervised methods, which require annotated explanations, often leading to noisy and subjective results. To address this, LiPoR treats explanations as latent variables and maximizes the marginal likelihood of the outcome given the context, marginalizing over other explanations. However, this objective alone does not favor plausible explanations, so an additional regularizer is introduced to enforce mutual exclusivity among explanations. The regularizer, denoted by **Omega**, balances the entropy of the explanation distribution and the number of plausible explanations. Experiments on the AlphaNLI dataset demonstrate that LiPoR outperforms zero-shot models, including GPT-3, by over 4 absolute points in accuracy, showcasing its effectiveness in unsupervised abductive reasoning. This work addresses the challenge of learning plausible explanations without supervision, paving the way for more robust and scalable reasoning systems.</sample>
    <sample id="166">The paper introduces "A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text," addressing the challenge of retrieving images from long, linguistically complex text descriptions. Traditional methods, such as visual language models, excel in image-sentence retrieval but struggle with complex text due to their reliance on analogical reasoning (System 1). To overcome this, the framework integrates System 1 (Visual-Linguistic Interactor) and System 2 (Neural-Symbolic Reasoner), inspired by Dual-Process Theory and the Divide-and-Conquer strategy. 

The framework begins with a Proposition Generator, which decomposes complex text into simpler propositions. The Visual-Linguistic Interactor processes these propositions to generate matching scores and reasoning states. The Neural-Symbolic Reasoner then integrates these states to produce the final solution, employing negation and conjunction operations for logical reasoning. 

Experimental results demonstrate that the proposed method, NDCR, outperforms baselines, with ablation studies validating the effectiveness of each module. Case studies further highlight the method's ability to present intermediate inference states and results, showcasing its interoperable processing. 

The authors suggest that neural symbolic reasoning and Divide-and-Conquer approaches, combined with Dual-Process Theory, can enhance compositional reasoning and planning in large language models, making them more effective for complex tasks.</sample>
    <sample id="167">In the DEPLAIN-web corpus, the 750 documents were aligned with a combination of manual and automatic alignment methods. The exact allocation is not specified in the provided content, but it implies that both methods were used for aligning the sentences within the documents.</sample>
    <sample id="168">The CoNLL++ dataset was created by collecting data from Reuters News from 2020 and annotating it with the same CoNLL-2003 annotation guidelines.</sample>
    <sample id="169">The paper "Prompting PaLM for Translation: Assessing Strategies and Performance" explores the effectiveness of large language models (LLMs) like PaLM (540 billion parameters) in machine translation tasks. PaLM, trained on 780 billion tokens, was evaluated using state-of-the-art neural machine translation (NMT) metrics and human evaluations, comparing its performance to top-tier systems like Google Translate. The study highlights the critical role of prompting strategies in LLM performance, demonstrating that even minor variations in prompts can significantly impact translation quality (e.g., up to 40 BLEURT points). A 5-shot prompting strategy was found to be effective, where source sentences are marked with their language, and example quality outweighs similarity to the source. While PaLM performs comparably to state-of-the-art systems in fluency, it struggles with accuracy, particularly in omission errors, where parts of the source sentence are omitted in translation. The paper also emphasizes the importance of using high-quality examples, such as those from curated development datasets, over noisy training data. Despite its strengths, PaLM still lags behind specialized NMT systems but comes close to commercial systems like Google Translate. The findings provide valuable insights into optimizing LLM prompting for translation tasks.</sample>
    <sample id="171">The existing works on embedding watermarking can be broadly classified into four categories:

1. **Text-based watermarking**: This method embeds a watermark directly into the text data, but it is not applicable to embedding as services.
2. **Image-based watermarking**: This method embeds a watermark into an image, but it is not applicable to embedding as services.
3. **Audio-based watermarking**: This method embeds a watermark into audio data, but it is not applicable to embedding as services.
4. **Embedding-based watermarking**: This method embeds a watermark into the embedding space, but it lacks transferability during model extraction.

These existing works either are not applicable to embedding as services or lack transferability, which is why the proposed Embedding Marker method is necessary.</sample>
    <sample id="172">No, multilingual LLMs like Codex and BLOOM are **not sufficient** for cross-lingual semantic parsing (CLSP) tasks. The study found that these models still perform inadequately compared to other models, highlighting the need for further improvements in handling cross-lingual semantic parsing.</sample>
    <sample id="174">**Abstract:**  
The paper "ArgAnalysis35K: A large-scale dataset for Argument Quality Analysis" introduces a novel dataset addressing the limitations of existing datasets in the field. ArgAnalysis35K stands out for its scale (35,000 argument-analysis pairs), high-quality arguments sourced from expert debaters, parliamentary speeches, and diverse themes (24), rather than relying solely on crowdsourcing or pre-selected motions. Unlike traditional datasets, it captures the nuanced structure of arguments by introducing the concept of "analysis," which combines claims, premises, and reasoning to explain the argument’s coherence and persuasiveness. This innovation enhances the dataset’s utility for NLP tasks by providing a more comprehensive representation of arguments. Additionally, the dataset incorporates an instance-based annotator reliability model, which mitigates human biases by retaining annotations even when annotators may have personal opinions on specific topics. Finally, a relevance model assigns scores to arguments based on their applicability to various themes, capturing the versatility of arguments across different contexts. ArgAnalysis35K thus offers a more diverse, reliable, and contextually relevant dataset for advancing argument quality analysis research.</sample>
    <sample id="175">The method deals with the ambiguity of permutations by inducing the alignment as part of the training process. It approximates the NP-hard problem of finding the highest-scoring permutation with a GPU-friendly continuous relaxation, which allows for backpropagation and learning of more linguistically plausible permutations.</sample>
    <sample id="176">The fairness of a downstream NLP model is defined by its ability to perform consistently and without bias across different demographic or political groups. In the context of the presentation, fairness is evaluated by examining how well the model performs on tasks like hate speech detection and fake news detection for different political leanings. For example, a fair model would detect hate speech targeting all minority groups equally well, regardless of the model's political leaning. Similarly, it should detect misinformation from all political sides without bias. If a model performs better on certain groups (e.g., detecting hate speech against white or men better than against black or LGBTQ+ groups), it indicates a fairness issue. Thus, fairness in NLP models is about ensuring equitable performance across diverse social and political contexts.</sample>
    <sample id="177">The name of the speaker is Yanis Labrak.</sample>
    <sample id="178">The name of the speaker is Koustav Sinha.</sample>
    <sample id="179">This research introduces **SymbolicToM**, an inference-time method designed to enhance **Theory of Mind (ToM)** reasoning in large language models (LLMs). ToM, the ability to understand and reason about others' mental states, is a critical skill for tasks like false-belief reasoning, often tested through scenarios like the **Sally-Anne test**. LLMs, including ChatGPT and GPT-3, struggle with false-belief questions, which require understanding characters' beliefs and counterfactual reasoning. SymbolicToM addresses this by using **graphical representations** to explicitly model characters' mental states, such as **BBob** (Bob's beliefs) and **BBob,Alice** (Bob's beliefs about Alice's beliefs). These graphs are computed using off-the-shelf models like NLI and OpenIE, enabling efficient answering of ToM questions.

Experiments demonstrate significant performance gains across LLMs, with models like GPT-3-Davinci achieving a 65-point accuracy boost. SymbolicToM outperforms supervised baselines like fine-tuned GPT-3 and Textual Time Travel, even in out-of-domain scenarios. Additionally, it generalizes well to new datasets, such as **ParaphrasedToMi**, which introduces linguistic diversity, and **D₁**, **D₂**, and **D₃**, which test story structure generalization. While supervised models degrade on these datasets, SymbolicToM maintains strong performance, with GPT-4 achieving a 42-point accuracy boost on D₁. This method avoids overfitting, provides interpretable reasoning, and enhances LLMs' ability to handle complex ToM tasks. For detailed results, refer to the paper.</sample>
    <sample id="180">The name of the speaker is Myra.</sample>
    <sample id="181">This paper addresses the challenge of constrained language planning, where large language models (LLMs) generate step-by-step scripts for specific goals with multi-faceted constraints. Unlike previous work focusing on abstract goals, this study explores planning for concrete goals, such as "make a chocolate cake," which introduces additional constraints. The authors evaluate LLMs' performance on specific goals, finding unsatisfactory results due to semantic incompleteness and lack of constraint faithfulness. To address these issues, they propose an "over-generate-then-filter" method: LLMs generate multiple scripts for specific goals, and a filter model selects the most faithful ones based on semantic similarity and constraint keywords. This approach significantly improves script quality.

To enable constrained language planning with smaller models, the authors distill a dataset of constrained scripts, CoScript, from LLMs. CoScript contains 55,000 specific goals with scripts, validated for quality. Experiments show that a smaller model, T5, fine-tuned on CoScript, outperforms most LLMs, demonstrating that specialized models can achieve superior results with appropriate datasets. The CoScript dataset is proposed as a valuable resource for advancing research in constrained language planning.</sample>
    <sample id="182">In the context of this paper, tropicalism indicates a harmful stereotype associated with Latina women, where they are depicted as vibrant and connected to a tropical, exotic, and hyper-sexualized identity, contributing to their othering and discrimination.</sample>
    <sample id="183">The authors created the human-written portrayals of target groups by drawing inspiration from a study where human subjects were given prompts similar to those used for generating personas in the LLMs. These prompts asked participants to imagine and describe themselves using specific identity markers, such as "Imagine you are an Asian woman. Describe yourself." This approach allowed for direct comparison between the generated personas and human-written responses, helping to surface racial stereotypes in a more generalized and comparable manner.</sample>
    <sample id="184">In this work, **Pointwise Contextual Information Measure (P-CXMI)** was used to measure context usage during translation. P-CXMI quantifies the information gained from context when translating a word or sentence, helping to identify words or phenomena that require context for accurate translation.</sample>
    <sample id="185">DrBERT and ChuBERT are two different biomedical models in French, differing primarily in their training data sources:

- **DrBERT** is based on **NACHOS**, a dataset of medical crawled data from the web.
- **ChuBERT** is based on **anonymized clinical data** obtained from the Nantes University Hospital data warehouse. 

In summary, DrBERT uses web-crawled medical data, while ChuBERT uses clinical data from a hospital.</sample>
    <sample id="187">Two authors are involved in the paper: Ying and Zhiyang.</sample>
    <sample id="188">Iterative transfer learning is a method where a model is fine-tuned iteratively on different tasks or datasets to improve its performance. In the context of the paper, it involves fine-tuning the model first on closely related tasks (such as CE tasks) and then further fine-tuning on another task (like debate) to enhance the model's zero-shot performance on the target task (dissonance detection). This iterative process helps the model learn more effectively from the related tasks, improving its ability to detect cognitive dissonance in language.</sample>
    <sample id="189">The goal of the AltEntities Corpus dataset is to understand users' language when they want to make a choice by resolving indirect referring expressions for entity selection, which is important for conversational systems and benchmarking LLMs' entity understanding.</sample>
    <sample id="190">An attacker can extract model parameters through an Embedding as a Service (EaaS) by learning from the embeddings provided by the service. Specifically, the attacker can reverse-engineer the model by analyzing the embeddings generated for specific inputs, particularly those with a high frequency of trigger words embedded in the service. This process allows the attacker to infer the structure and parameters of the underlying model, leading to potential copyright infringement and unauthorized use of the model.</sample>
    <sample id="191">There are 3 authors involved in the paper: Sara Papi, Matteo Negri, and Marco Turchi.</sample>
    <sample id="192">**Abstract:**  
The paper introduces **CAME (Confidence-guided Adaptive Memory Efficient Optimization)**, a novel optimizer designed to balance fast convergence and low memory usage in training large language models (LLMs). Traditional adaptive optimizers like Adam require significant memory for gradient estimates, while memory-efficient methods like Adafactor sacrifice performance. CAME addresses this challenge by leveraging non-negative matrix factorization (NMF) and introducing a confidence-guided adaptive updating mechanism. It reduces memory usage while mitigating erroneous updates, which are a common issue in memory-efficient optimizers. CAME uses the residual between predicted and generated updates to adaptively adjust the optimization direction, improving stability and convergence.  

Experiments on BookCorpus, English Wikipedia, and three LLMs (BERT, GPT-2, T5) demonstrate CAME's effectiveness. Compared to Adafactor, CAME achieves a 3.4% increase in validation accuracy with the same training steps. It outperforms Adam in large-scale pre-training and excels in large batch training, significantly reducing memory costs. CAME also shows comparable performance to Adam and LAMB on downstream tasks while using less memory. The proposed optimizer addresses the limitations of existing methods, offering a robust solution for efficient LLM training.</sample>
    <sample id="193">The content does not specify the number of annotators used to create the initial dataset. It mentions that a large-scale annotation of dissonance relations was conducted, but the number of annotators is not provided.</sample>
    <sample id="194">The authors of the paper are affiliated with the following institutions:

1. Carnegie Mellon University (Jenny and Maarten Sap)
2. University of Washington (Sebastian Santy, Ronan Le Bras, and Katharina Reinecke)
3. Allen Institute for AI (Sebastian Santy, Ronan Le Bras, Katharina Reinecke, and Maarten Sap)</sample>
    <sample id="195">The paper introduces "Reasoning over Hierarchical Question Decomposition Tree (RoHT)" for explainable question answering (XQA), addressing the limitations of existing neuro-symbolic and decompose-based methods. RoHT proposes a two-stage framework: first, it constructs a Hierarchical Question Decomposition Tree (HQDT) to understand the structure of complex questions, where the root is the original question, intermediate nodes are sub-questions, and leaf nodes are atomic questions. Second, it employs probabilistic reasoning over the HQDT to fuse knowledge from heterogeneous sources, such as knowledge bases (KBs) and text corpora, at different levels of the tree. The reasoning process is recursive, involving a scheduler to select appropriate knowledge sources, executors to retrieve answers, and an aggregator to combine and prioritize results.

The framework is evaluated on two datasets: KQA Pro, which simulates an incomplete KB scenario, and Musique, a QA comprehension dataset. Results show that RoHT outperforms state-of-the-art methods by integrating answers from sub-questions of varying levels and leveraging both KBs and text corpora. Specifically, RoHT KB improves performance on KQA Pro by integrating a supplementary text corpus, and RoHT-mix achieves significant gains on Musique by combining text and KB knowledge. The study highlights the effectiveness of hierarchical question decomposition and probabilistic reasoning in enhancing XQA systems.</sample>
    <sample id="196">The example given where the governor is on the left is "I saw Bart and Lisa." In this sentence, "I" (the subject) is the governor, and it governs the coordination of "Bart" and "Lisa."</sample>
    <sample id="197">The state-of-the-art models in dialogue systems evaluated in the ABC-Eval study were four advanced chat models, though their specific names are not mentioned in the provided text. These models were compared across various dimensions of dialogue quality using ABC-Eval and other existing evaluation methods.</sample>
    <sample id="198">We need to evaluate the models' acceptability throughout the context window because large language models are increasingly capable of handling longer and longer sequences. Current minimal pair paradigm (MPP) pipelines, which typically evaluate models on short, single-sentence inputs, may not fully capture the models' abstract knowledge and sensitivity to latent syntactic and semantic features across the entire context window. Evaluating acceptability over longer sequences helps ensure that models' judgments remain robust and consistent as context length increases.</sample>
    <sample id="199">Yes, training in a multilingual fashion caused a performance drop compared to the monolingual English model in seven out of the nine datasets, which is referred to as the "Curse of Multilinguality."</sample>
    <sample id="200">No, the annotators do not know about the entities in advance. They are shown background knowledge about the two entities (e.g., Google search links for songs, Wikipedia text for books and recipes, and images for recipes) and are asked to pick one and generate indirect referring expressions.</sample>
    <sample id="201">The evaluation used state-of-the-art, neural MT metrics.</sample>
    <sample id="202">The paper does not specifically address whether the regression in generalization impacts specific types of Named Entity Recognition (NER) tasks. It focuses on the general performance of NER models trained on the CoNLL-2003 dataset when evaluated on modern data (CoNLL++) and identifies temporal drift as the primary cause of performance drop, rather than task-specific issues. Therefore, the impact on specific NER types is not explicitly discussed.</sample>
    <sample id="203">Positionality in NLP matters because it highlights systematic biases in datasets and models that disproportionately favor certain populations (e.g., English speakers, individuals with higher education) while neglecting or misrepresenting others (e.g., non-binary individuals, non-English speakers). This can lead to unfair or inaccurate outcomes in applications like toxicity detection, hate speech filtering, and social acceptability assessment, ultimately reinforcing existing inequalities and excluding marginalized groups. Addressing positionality is crucial for creating more equitable and inclusive NLP systems.</sample>
    <sample id="204">The content does not specify whether the multilingual LLMs like BLOOM were fine-tuned with adapters or full fine-tuning. It only mentions that they are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="205">**Abstract:**  
This research investigates the propagation of political biases from pretraining data to language models (LMs) and their downstream applications, highlighting critical fairness issues in NLP. By analyzing the political leanings of LMs like GPT-4 and BART, we found that they occupy all quadrants of the political spectrum, with GPT models generally more liberal than BART. Controlled experiments revealed that LMs’ political biases align with their pretraining data, demonstrating that partisan corpora significantly shift their ideological coordinates. Additionally, temporal analysis showed that LMs trained post-2017 exhibit a more polarized political stance, reflecting societal polarization. We evaluated these LMs on downstream tasks like hate speech and fake news detection, uncovering performance disparities based on political and demographic categories. For instance, left-leaning LMs excelled in detecting hate speech targeting minority groups but underperformed against hate speech targeting powerful groups, and vice versa. Similar trends were observed in fake news detection. These findings underscore the urgent need to address political biases in LMs to prevent marginalization and unchecked hate speech. The research exposes a dilemma: sanitizing training data risks censorship, while leaving it unfiltered perpetuates bias. This work calls for nuanced solutions to balance fairness, inclusivity, and neutrality in NLP applications.</sample>
    <sample id="206">They use a model that is fine-tuned on two tasks: topic-independent dissonance stance classification (debate) and binary classification of expansion and comparison classes of PDTB (CE). The best performance is achieved by fine-tuning the CE tasks first, followed by further fine-tuning on debate. This model is used to cold-start the active learning process.</sample>
    <sample id="207">The recent test sets used to assess PaLM's capabilities for translation are the latest WMT (Workshop on Machine Translation) evaluation test sets. These test sets were chosen to ensure that the evaluation data does not overlap with the training data of the PaLM model.</sample>
    <sample id="208">The authors proposed **three recommendations** for model owners: 
1. Addressing positive stereotypes and essentializing narratives. 
2. Using an intersectional lens to study biases and harms. 
3. Increasing transparency about bias mitigation methods.</sample>
    <sample id="209">The proposed method, which involves over-generate-then-filter, significantly improves the planning ability of InstructGPT. Specifically, it enhances both semantic completeness and faithfulness to constraints. However, the paper does not provide a quantitative measure of the gain over the strongest baseline in terms of a specific metric (e.g., accuracy, F1 score, or BLEU score). Instead, it emphasizes qualitative improvements and the ability to generate higher-quality scripts. 

For smaller models like T5, fine-tuning on the CoScript dataset leads to scripts of higher quality than most large language models, indicating a substantial gain in performance when trained on suitable datasets. However, the exact numerical gain over the strongest baseline is not explicitly stated in the provided content.</sample>
    <sample id="210">The name of the speaker is Shuheng.</sample>
    <sample id="211">Yes, the results and dataset in the paper can be used as a benchmark for automatic text simplification, particularly for evaluating alignment methods and fine-tuning language models. The authors have provided the necessary details, checkpoints, and evaluation metrics to replicate and build upon their work.</sample>
    <sample id="212">The paper experiments with **one smaller model**, specifically T5, which is fine-tuned on the CoScript dataset.</sample>
    <sample id="213">The base model used for investigating multi-modal instruction tuning is OFA (Open Foundation Architecture).</sample>
    <sample id="215">This talk by Adam Przepiórkowski discusses the dependency structure of coordination, focusing on the debate between asymmetric and symmetric approaches. Asymmetric theories, such as Universal Dependencies and Mel'čuk's meaning text theory, posit that the first conjunct heads the entire coordinate structure. In contrast, the Prague approach and multi-headed approaches propose that coordinate structures are headed by the conjunction or that all conjuncts are heads. The paper argues for symmetric structures of coordination based on the principle of dependency length minimization, which favors shorter dependencies. The argument is supported by statistical analysis from the Penn Treebank, showing that left conjuncts tend to be shorter, especially when the governor is on the left or absent. This tendency disappears when the governor is on the right. The findings challenge asymmetric theories and support symmetric structures, providing a novel argument against single-headed and multi-headed approaches. The paper's full arguments and implications are detailed in the published work, and the author invites further discussion at the poster session.</sample>
    <sample id="217">This research explores the compositional generalization in multi-attribute controllable dialogue generation, addressing the limitations of existing methods that focus on single attributes or lack effective evaluation metrics. The proposed **Disentangled Controllable Generation (DCG)** model leverages a compositional prompt module based on the DialoGPT framework to learn attribute concepts from seen values and disentangle attribute combinations using disentanglement loss. A unified **reference-free evaluation framework (MAE)** is introduced to assess controllability across different attribute granularities without requiring additional labeled data. The model uses two types of prompts: **attribute-oriented** prompts guide focus on instance-specific controllable information, while **task-oriented** prompts ensure text equality by incorporating global features. Pseudo combinations and disentanglement loss further enhance the model's ability to generalize unseen attribute combinations. Experimental results on the DailyDialog-CG benchmark demonstrate that DCG outperforms baselines in controllability and text equality, with minimal drops in E-ACC and A-ACC metrics. The MAE framework, validated by correlation coefficients, shows strong alignment with human judgments, particularly for fine-grained continuous attributes. Additionally, the model's ability to disentangle attribute combinations is visualized, confirming its effectiveness in compositional generalization. This work provides a robust framework for multi-attribute controllable dialogue generation, addressing both generation and evaluation challenges.</sample>
    <sample id="218">The authors of the paper "Prompting PaLM for Translation: Assessing Strategies and Performance" are affiliated with Google Translate.</sample>
    <sample id="219">This research, conducted by Jia-Huei Ju, Yu-Shiang Huang, Cheng-Wei Lin, and their advisors at Academia Sinica, introduces a novel approach to uncovering financial signals in annual reports, specifically focusing on the Form 10-K. The study addresses the challenge of extracting meaningful information from highly repetitive and yearly-dependent financial reports, where 80% of tokens are similar across consecutive years. The authors propose a **compare-and-contrast multistage pipeline** to highlight and analyze the differences and similarities between consecutive years' reports.

The task involves a **highlighting model** that identifies the rationale of relations between a target report and its reference (the previous year's report). This model predicts word importance, enabling the measurement of performance through precision and PCC (Pearson correlation coefficient). The pipeline consists of **Stage 1 (relation recognition)**, where pairs are classified into three types: **β pairs** (high syntactic and semantic similarity), **revised pairs** (similar syntax but different meanings), and **mismatched pairs** (new information). For fine-tuning, the authors use an out-of-domain dataset (eSNLI) for initial training and revised pairs for in-domain fine-tuning, incorporating soft labeling techniques to improve pseudo-label quality.

The proposed model achieves superior performance on the released **FINAL dataset** and maintains generalization capabilities on the eSNLI dataset. Additionally, it demonstrates effectiveness on unmatched pairs, indicating potential for further improvements. This work aims to enhance the efficiency of financial report analysis and opens avenues for future research in information retrieval and financial signal extraction.</sample>
    <sample id="220">The author of the paper, Vasudha, is a PhD candidate at Stony Brook University. The affiliations of the authors are not explicitly mentioned in the provided content, but it can be inferred that the primary author, Vasudha, is affiliated with Stony Brook University. If there are co-authors, their affiliations would also be associated with Stony Brook University or other institutions involved in the research.</sample>
    <sample id="221">The paper analyzed translation between German and English.</sample>
    <sample id="222">This work addresses the challenge of enabling open-domain question answering (QA) models to generalize across domains, particularly when transitioning from a general-purpose corpus like Wikipedia to specialized domains such as biomedical. The research investigates three main contributions: (1) exploring data interventions to facilitate out-of-domain generalization, (2) identifying the nature of dataset shifts when moving to new domains, and (3) determining effective interventions based on the type of shift.

The study evaluates different data interventions, including zero-shot and few-shot methods. Zero-shot techniques manipulate question, answer, and context distributions in a controlled manner to improve model learning, while few-shot methods leverage limited examples from the target domain to generate additional training data. Results show that few-shot adaptations improve retriever performance by 8% and reader performance by 11% on average. Zero-shot techniques, though without target domain examples, also yield improvements by controlling model interactions with question, answer, and context distributions.

The research also introduces a compatibility measure to assess the nature of dataset shifts, categorizing them into no shift, concept shift, covariate shift, and full shift. This measure helps map target datasets onto a 2D grid, revealing that datasets like CliCR and NewsQA exhibit full shifts, while SearchQA shows no shift. Finally, the study identifies that few-shot adaptations are effective for all target datasets, while zero-shot adaptations are particularly beneficial for datasets with concept and covariate shifts. Overall, the work demonstrates that tailored data interventions significantly improve model performance and adaptability across diverse domains.</sample>
    <sample id="223">The name of the speaker is Shangbin.</sample>
    <sample id="224">The models investigated during the experiments were:

1. Long-mBART for document-level simplification
2. Base mBART for sentence-level simplification</sample>
    <sample id="225">From the 62 diverse tasks used in MultiInstruct, 53 tasks are used for training purposes, and 10 tasks are used for testing purposes.</sample>
    <sample id="226">Two authors are involved in the paper: Regina Stodden and Omar.</sample>
    <sample id="227">The current state of language models excels in general NLP tasks but lacks in grounded language understanding, which involves mapping natural language expressions to executable plans or programs in specific environments. This challenge arises from the lack of grounding during pre-training, creating a gap between pre-training data and downstream applications. Existing approaches often rely on language models to directly generate plans, which can result in ungrammatical or invalid outputs. To address this, we propose the Pangu framework, which separates the symbolic world (plan generation) from the linguistic world (plan scoring). In this framework, a symbolic agent proposes candidate plans, while a language model scores and ranks these candidates, focusing on discrimination rather than generation. This approach leverages the language model's strength in discrimination while avoiding the complexities of plan generation. We instantiate Pangu on knowledge-based question answering, a typical grounded language understanding task, and achieve outstanding performance across various settings, including fine-tuning and in-context learning. Notably, Pangu demonstrates strong sample efficiency, outperforming baselines significantly. Our findings suggest that autoregressive models like ArcaneQA overfit seen structures, while Pangu maintains consistent performance across seen and unseen structures, indicating robustness under non-i.i.d. settings. The key takeaway is that for grounded language understanding, discrimination, rather than generation, is a more effective strategy for leveraging language models. We welcome discussions and collaborations to further explore this approach.</sample>
    <sample id="228">The authors experimented on four datasets: AG News, MIND, SST2, and Enron Spam.</sample>
    <sample id="229">This paper presents a study on detecting and improving argumentative claims in writing, focusing on the challenges and strategies involved in the process. The authors, Gabriella Skitalinskaya and Henning Wachsmuth, explore the importance of text revision in argumentative writing, where optimal phrasing is crucial for effectively communicating a message and influencing the audience. They introduce two tasks: Suboptimal-Claim detection, which determines whether a claim needs revisions, and Claim Improvement Suggestion, which identifies the types of quality issues to be addressed.

The study examines the challenges of working with revision-based data, including representativity and reliability, model complexity, contextual dependence, and topical and user bias. The authors analyze these challenges in the context of argumentative text, specifically using data from collaborative online debate platforms like Kialo. They propose strategies to tackle these challenges, such as modeling the distance between claim versions and considering contextual information.

The research concludes that revision-based data can be effectively used for the introduced tasks, and that modeling the distance between claim versions is beneficial for detecting suboptimal claims. Additionally, the impact of contextual information on claim quality assessment depends on the task and the specific quality issues. The authors invite readers to explore their paper for a detailed analysis of the strategies and approaches used in their study. Overall, this research provides valuable insights into the complexities of argumentative writing and offers a framework for improving the quality of argumentative claims.</sample>
    <sample id="231">NACHOS is a dataset of medical crawled data from the web, used for training the biomedical model DrBERT in French.</sample>
    <sample id="232">The name of the speaker is David Vilar.</sample>
    <sample id="233">The paper "Attention as a Guide for Simultaneous Speech Translation" by Sara Papi, Matteo Negri, and Marco Turchi addresses the challenges of simultaneous speech translation (SimulST), which involves real-time translation of spoken language into another language. Current SimulST models often require specialized architectures and lengthy training processes, leading to multiple models for different latency regimes. The proposed solution, EDAtt (Encoder-Decoder Attention), leverages existing offline speech translation (ST) models without retraining, using a single model for all latency regimes. EDAtt introduces a strategy to decide when to emit a partial translation based on the attention mechanism between audio input and textual output, specifically the cross-attention mechanism. A word is emitted if the attention is not concentrated on the last lambda speech frames, measured by a threshold alpha. This approach allows for dynamic emission of translations based on the stability of received information. The results show that EDAtt outperforms other strategies applied to offline models and is the fastest in terms of computational-aware latency. The authors have released open-source code and models to facilitate reproducibility. This innovative method enhances the efficiency and effectiveness of simultaneous speech translation, making cross-language communication more seamless and real-time.</sample>
    <sample id="234">The prompting strategy significantly impacts the results, with differences observed in BLEURT points ranging from more than one point to, in extreme cases, up to 40 points. The quality of examples and the form of prompting play a crucial role, especially in zero and one-shot prompting, but the actual form of prompting has less influence in the case of five-shot prompting.</sample>
    <sample id="235">The authors of the paper "When Does Translation Require Context? A Data-driven, Multilingual Exploration" and their affiliations are:

1. Kayo Yin - Affiliation not explicitly mentioned in the provided text.
2. Patrick Fernandes - Affiliation not explicitly mentioned in the provided text.
3. Emmy Liu - Affiliation not explicitly mentioned in the provided text.
4. André F. T. Martins - Affiliation not explicitly mentioned in the provided text.
5. Graham Neubig - Carnegie Mellon University.</sample>
    <sample id="236">The 5 expert-written instructions for each task in the MultiInstruct dataset are not explicitly listed in the provided content. However, it is mentioned that each task is equipped with five expert-written instructions. For specific details, you would need to refer to the dataset documentation or the full research paper.</sample>
    <sample id="237">The authors propose to test models on the **KITMUS Test**, which evaluates their ability to integrate and use knowledge from multiple sources, including pretrained parameters and inference-time context. This test includes a coreference resolution task that probes models' ability to draw on entity-specific and background knowledge from different sources.</sample>
    <sample id="238">The video introduces **MeetingBank**, a benchmark dataset created by Yebowen Hu and colleagues from the University of Central Florida. The dataset focuses on summarizing City Council meetings, addressing challenges such as obtaining high-quality summaries and locating trustworthy meeting transcripts. It includes 1,366 meetings and nearly 7,000 summarization instances, featuring meeting transcripts, reference summaries, and URLs to additional resources. Data collection involved converting audio to text using Speechmatics API and aligning timestamps with summaries.

The dataset provides statistics on meeting duration, token counts, speakers, and summarization instances per city. It evaluates summarization models using coverage and density metrics, revealing that most summaries are verbatim (coverage 0.7–0.9) and that editing varies by city. The evaluation includes extractive models (e.g., Oracle, TextRank) and abstractive models (e.g., BART-Large, GPT-3). While GPT-3 performs poorly on automatic metrics, it excels in fluency and coherence in human evaluations. Human annotators also assessed informativeness, factuality, and redundancy, highlighting the need for improved evaluation metrics aligned with human preferences.

MeetingBank serves as a valuable resource for researchers to develop advanced meeting summarization technologies and provides insights into City Council decision-making processes. The dataset is open for use, encouraging further exploration and discussion.</sample>
    <sample id="241">This paper, "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments," presents a novel framework for developing and evaluating misinformation detection systems that integrate human feedback throughout the process. The authors, from Georgia Tech, address two key deficiencies in existing approaches: unrealistic evaluation and lack of human-centricity. They propose an end-to-end system that processes raw tweets and outputs actionable outputs for human moderators, with human feedback integrated at various stages. The system consists of two main components: a misleading claim detection component, which uses keyword filtering and a T5 model for claim extraction and ranking, and a policy violation verification component, which uses a BERT-based stance classification model to flag tweets supporting unapproved treatments. The authors evaluate their framework by operationalizing early detection as the identification of unapproved treatments before their first appearance in debunking news articles, achieving a 65% accuracy rate in policy violation detection and 124.2 policy violations confirmed per human hour worked. This work provides a realistic evaluation of human-in-the-loop misinformation detection systems and motivates the development of future systems that can be evaluated consistently. The authors also offer an industry perspective on the development and evaluation of such systems.</sample>
    <sample id="242">Common evaluation methods for dialogue systems include:

1. **Human Evaluation**: Human judges select the better conversation or rate conversations on a Likert scale.
2. **Turn-Level Likert Ratings**: Judges rate individual turns (utterances) in a conversation.
3. **Dialogue-Level Likert Ratings**: Judges rate the entire conversation.
4. **Dialogue-Level Pairwise Comparisons**: Judges compare two conversations to determine which is better.

These methods are widely used to assess overall dialogue quality but often lack granularity in evaluating specific aspects of chat quality.</sample>
    <sample id="243">The paper involves 6 authors: Jenny, Sebastian Santy, Ronan Le Bras, Katharina Reinecke, Maarten Sap, and the author's affiliation (University of Washington and Allen Institute for AI).</sample>
    <sample id="244">In the example with Servin and Kea, the background knowledge needed is that "Judges decide cases in law courts." This knowledge is required to understand that "he" refers to Servin, who is a judge.</sample>
    <sample id="245">The paper "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization" presents a two-step pipeline to identify high-quality annotators on Amazon Mechanical Turk (MTurk) for summarization tasks. The pipeline aims to address the limitations of automatic metrics and the lack of best practices for MTurk recruitment. 

The first stage, the "Qualification Task," evaluates annotators' ability to assess multiple dimensions of summaries correctly. It includes training and qualification parts, with workers categorized into gold, silver, bronze, or blocked based on their performance. Only gold and silver workers proceed to the second stage. This stage, the "Endurance Task," tests the capacity to handle heavy workloads. The final selection results in 12 high-agreement workers (4 gold, 8 silver), representing 6% of the initial 200 participants.

The "Reference-based Task" assesses general performance on the true annotation task, with 8 out of 12 workers completing all HITs. The results show high inter-annotator agreement (IAA) and superior performance compared to experts.

The paper also compares the pipeline workers with Baseline MTurk workers and CloudResearch workers, finding that the pipeline achieves similar or better results in terms of agreement and quality. Additionally, an analysis of correctness across annotation sources reveals significant correlations between Pipeline and CloudResearch workers, with real GPT models aligning well with expert judgments.

In conclusion, the pipeline effectively filters high-quality annotators, reducing resource waste and achieving high agreement at a lower cost. Future work will explore hiring strategies for high-quality workers across tasks, languages, and platforms, while addressing the limitations of the current study.</sample>
    <sample id="246">Yes, the code is available on GitHub.</sample>
    <sample id="247">**Abstract:**  
This paper introduces **FactKG**, a novel dataset for **Knowledge Graph-Based Fact Verification**, addressing the gap in existing datasets that lack evidence from knowledge graphs (KGs) for natural language claims. Unlike datasets like FEVER and VitaminC, which rely on text, or TabFact and InfoTabs, which use tables, FactKG leverages the **DBpedia KG** to enable direct and intuitive reasoning for fact verification. The dataset includes claims in both written and colloquial styles, reflecting practical use cases, and is labeled as **SUPPORTED** or **REFUTED**. It incorporates five types of reasoning: **one-hop**, **conjunction**, **existence**, **multi-hop**, and **negation**, allowing for diverse verification tasks. For instance, one-hop claims require verifying a single triple, while conjunction claims demand validation of multiple triples. The dataset also includes colloquial claims generated using a transfer model and presupposition templates, enhancing its applicability. Experimental results show that baselines using graph evidence outperform claim-only baselines and majority-class predictions, with the **GEAR model** achieving the best performance. FactKG aims to bridge the gap between KGs and natural language, supporting tasks like dialogue systems and consistency checks in applications requiring KG-NL alignment. The dataset is publicly available for further research.</sample>
    <sample id="248">No, the annotators for NLPositionality are not balanced in regard to each demographic. The demographics of the original annotators are rarely collected and shared, and the re-annotation process aims to gather a rich set of demographic data to compensate for this lack of balance. The study includes over 1000 annotators from 87 countries, but the demographic balance is not explicitly stated to be equal across all categories.</sample>
    <sample id="249">In the acceptable domain, sentences were perturbed by preserving the relevant structure while adding noise to the input. This perturbation was designed to maintain the syntactic and semantic features shared across the sentences, allowing for an analysis of how the models respond to variations in the input.</sample>
    <sample id="250">A dimensional evaluation, as described in the context of ABC-Eval, means assessing a conversational AI model across multiple specific dimensions or behaviors that contribute to the overall quality of dialogue. This approach goes beyond a single holistic evaluation (like a Likert scale rating) by explicitly annotating and measuring distinct aspects of the model's performance, such as relevance, self-contradiction, partner contradiction, hallucination of incorrect facts, and empathy. By evaluating these dimensions, researchers can gain a more detailed and nuanced understanding of the model's strengths and weaknesses, enabling more precise comparisons and improvements.</sample>
    <sample id="251">The author of the paper is Jingwei Yi from the University of Science and Technology of China.</sample>
    <sample id="252">**Abstract:**  
This presentation introduces "U-CREAT: Unsupervised Case Retrieval using Events Extraction," a groundbreaking work by Sai Kiran Tanikella and collaborators from IIT Kanpur. The research addresses the challenge of Prior Case Retrieval (PCR) in legal domains, where retrieving relevant past precedents from a vast case pool is critical. The team developed the IL-PCR dataset, a benchmark for PCR tasks, comprising 7,070 Indian legal cases with an average of 6.775 citations per query document, surpassing existing datasets like COLIEE’21 in size and complexity.  

The core contribution is the U-CREAT pipeline, which employs unsupervised learning and an event-based approach. It leverages dependency parsing to extract events from case documents, treating them as narrative units. The pipeline processes query and candidate documents sequentially, computing an interaction matrix to rank candidates based on matching events.  

Experiments demonstrate that event-based models, particularly the Event Filtered Documents approach, outperform baseline methods and transformer-based models, achieving higher F1 scores and lower inference times. U-CREAT also outperforms state-of-the-art methods on the COLIEE’21 dataset. This work highlights the importance of event-based approaches and tailored legal models for PCR, paving the way for future advancements in legal information retrieval.</sample>
    <sample id="253">**Abstract:**  
Mario Ezra Aragón et al. present "DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media," a collaborative effort between researchers from Mexico and Spain. The work addresses the growing need to detect mental health disorders through social media analysis, leveraging the vast amount of publicly shared content. Mental disorders, such as depression, PTSD, bulimia, and anorexia, are psychological syndromes causing distress and disability, affecting thinking, feeling, mood, and behavior. Social media provides a unique platform for individuals to express their struggles, offering opportunities for early detection and intervention.  

The proposed model, DisorBERT, employs double domain adaptation to improve performance on the mental health domain using knowledge from a general language model (BERT) trained on Wikipedia and Google Books. By integrating Reddit and mental health-specific data, DisorBERT adapts its vocabulary and semantic understanding to the target domain. Guided masking further enhances the model’s focus on domain-specific, psychologically relevant words during training.  

Results demonstrate DisorBERT’s effectiveness in balancing precision and recall, outperforming baselines and models like MentalBERT. The model’s predictions reveal a stronger bias toward mental health-related terms, such as "anxious" and "medication," when analyzing clinical tools like Beck’s Depression Inventory. Future work aims to explore additional lexical resources and incorporate clinical data to further refine the model’s accuracy and applicability. This approach holds promise for developing technology to detect and support individuals at risk of mental disorders.</sample>
    <sample id="254">This paper introduces a novel framework for document-level distant relation extraction (DocRE) called **Uncertainty Guided Label Denoising (UGLD)**, designed to mitigate noise in distantly supervised (DS) data. Traditional DocRE methods rely on DS data, which often contain noisy pseudo labels, leading to misclassified relations. UGLD addresses this by integrating uncertainty estimation to filter out unreliable pseudo labels and improve label quality. The framework first trains a pre-denoising DocRE model using both DS and human-annotated data to generate pseudo labels. It then employs **Monte Carlo dropout (MC dropout)** to estimate uncertainty scores for model predictions, capturing the confidence of each relation prediction. To handle overlapping relations, an **instance-level uncertainty estimation** method is introduced, allowing for more accurate separation of false positives and true positives. Additionally, a **dynamic class uncertainty threshold** strategy is proposed to filter out high-uncertainty pseudo labels, replacing them with lower-uncertainty ones. Finally, a **multi-phase training strategy** iteratively re-labels DS data to further enhance model performance. Experimental results on public datasets demonstrate that UGLD significantly outperforms baseline methods, achieving better relation extraction accuracy. The key contributions include uncertainty-guided label denoising, instance-level uncertainty estimation for overlapping relations, a dynamic threshold strategy for long-tail classes, and improved performance. This work provides a robust solution to the noise problem in DS data for DocRE.</sample>
    <sample id="255">The form of the prompting is important in cases of **zero-shot and one-shot prompting**. However, when using **five-shot prompting**, the actual form of the prompting has **nearly no influence** on the performance. The quality of the examples provided is more critical in these cases.</sample>
    <sample id="257">The authors evaluated four state-of-the-art chat models.</sample>
    <sample id="258">**Abstract:**  
In "Can Large Language Models Be an Alternative to Human Evaluation?", we explore the feasibility of using large language models (LLMs) to evaluate text quality in natural language processing (NLP). Motivated by the instability and reproducibility challenges of human evaluations, we propose LLMs as a potential alternative. Our approach involves instructing LLMs using natural language guidelines to rate text samples across attributes such as grammar, coherence, likability, and relevance. We conducted experiments using four LLMs (T0, InstructGPT-Curie, InstructGPT-Davinci, and ChatGPT) to evaluate stories generated by GPT-2 and humans. Ground-truth ratings were obtained from English teachers, who consistently preferred human-written stories over GPT-2-generated ones. Notably, InstructGPT-Davinci and ChatGPT demonstrated a clear preference for human-written text, aligning with human evaluators. While smaller LLMs showed less meaningful results, our findings suggest that certain LLMs can effectively mimic human evaluation. This work addresses key questions about consistency, instruction phrasing, and sampling strategies, highlighting the potential benefits and costs of LLMs compared to human evaluation. Additionally, we discuss the broader implications for NLP tasks. Our research provides a novel perspective on leveraging LLMs as a scalable and reproducible alternative to human evaluation in NLP.</sample>
    <sample id="259">**Abstract:**  
This paper introduces *XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations*, a unified benchmark addressing the limitations of existing cross-lingual semantic parsing models. Existing models often lack coverage for certain languages, meaning representations, or neural architectures, leading to incomplete evaluations. *XSemPLR* provides a comprehensive dataset encompassing 9 datasets across various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages from 15 language families. It evaluates models in six settings: Translate-Test, Monolingual, Monolingual Few-shot, Multilingual, Cross-lingual Zero-shot, and Cross-lingual Few-shot.  

The study highlights that *Encoder-Decoder* models outperform *Encoder-PTR* models across all datasets, with multilingual training improving performance for most languages except English. The "Curse of Multilinguality" is observed, where English performance drops in seven datasets. The paper also identifies a significant cross-language performance gap, which is reduced with Few-shot transfer settings. Additionally, it finds that pretraining on English enhances Few-shot performance on target languages, while multilingual models like Codex and BLOOM remain inadequate for cross-lingual semantic parsing.  

In conclusion, *XSemPLR* provides a robust framework for evaluating cross-lingual semantic parsing models, revealing key insights into their strengths and weaknesses. The dataset and evaluation methodology are publicly available, encouraging further research in this domain.</sample>
    <sample id="260">The paper is authored by Jingwei Yi from the University of Science and Technology of China. Therefore, there is **1 author** involved in the paper.</sample>
    <sample id="261">A good planner should possess the following ideal qualities:

1. **Reasonableness**: The scripts generated by the planner should be logical and practical.
2. **Faithfulness to Constraints**: The planner must adhere to the specific constraints imposed on the goals, ensuring that the scripts are consistent with the requirements.
3. **Semantic Completeness**: The scripts should be semantically complete, meaning they cover all necessary steps or actions to achieve the goal.

These qualities are crucial for effective constrained language planning.</sample>
    <sample id="262">The paper is authored by Siyu Yuan from Fudan University. Therefore, there is **1 author** involved in the paper.</sample>
    <sample id="263">This work addresses the instability and bias issues in in-context learning (ICL) for large language models (LLMs), a popular paradigm for leveraging pre-trained models. ICL's performance is often unstable due to design choices like the selection and order of in-context examples, which introduce biases affecting model predictions. The study systematically categorizes existing label biases, identifying a novel type: **domain-label bias**, where the task corpus influences model predictions. To validate this, experiments show that random in-domain words bias model preferences, while random English words do not. Tasks with high domain-label bias severely limit model performance, even with prior calibration methods.

To mitigate these biases, the authors propose **domain-context calibration**, a holistic approach. Unlike traditional methods that use predefined content-free tokens (e.g., "not available"), this method employs random in-domain words sampled from the task corpus. This approach accounts for domain-label bias while maintaining content-free properties. Experiments demonstrate significant performance improvements across datasets and models, with greater gains on tasks with high domain-label bias. The method also improves decision boundaries and outperforms previous calibration techniques, which were suboptimal when using single content-free tokens or random English words.

In summary, this work systematically investigates label biases in ICL, identifies domain-label bias, and proposes an effective calibration method to enhance model performance, particularly in challenging scenarios.</sample>
    <sample id="264">**Abstract:**  
This paper introduces **TAVT: Towards Transferable Audio-Visual Text Generation**, addressing the challenges of multimodal text generation, particularly in domains with limited labeled data and varying domain shifts. Existing methods struggle with domain-specific adaptations, leading to performance degradation. TAVT proposes a novel framework to tackle these issues by leveraging a **unified audio semantic space** to align visual and audio concepts across domains. The framework consists of three key components:  
1. **Audio-Visual Meta-Mapper Network (AVMM)**: Maps visual concepts into a unified auditory semantic space, addressing semantic distribution shifts.  
2. **Audio-Visual Encoder and Language Model Generator**: Utilizes a transformer-based architecture with modality-specific attention weights (α-t) to balance modality contributions.  
3. **Dual Counterfactual Contrastive Learning (DCLL)**: Constructs fine-grained supervision signals to optimize visual-textual alignment without relying on negative samples.  

The model is trained using **meta-learning**, where it adapts quickly to new domains with limited data. Experimental results on MSVD and MSR-VTT benchmarks demonstrate TAVT’s superiority over state-of-the-art (SOTA) methods, particularly in low-resource domains. Ablation studies highlight the importance of audio features in enhancing performance. TAVT achieves significant improvements in cross-dataset and cross-domain settings, showcasing its robustness and adaptability.</sample>
    <sample id="265">The name of the speaker is Vasudha.</sample>
    <sample id="266">The affiliation of the author Adam Przepiórkowski is not explicitly mentioned in the provided text. However, based on the context, it can be inferred that he is likely affiliated with a research institution or university, possibly in the field of linguistics or computational linguistics, given the topic of the paper. For precise affiliation details, one would need to refer to the paper itself or additional sources.</sample>
    <sample id="268">The most common errors of PaLM are **omission errors**, where it sometimes drops parts of the source sentence in its translations.</sample>
    <sample id="270">The authors of the paper are affiliated with the Emory NLP Lab, led by Professor Jinho Choi at Emory University, and in collaboration with Amazon Alexa AI.</sample>
    <sample id="271">CFT stands for "Continuous Fine-Tuning" in this paper.</sample>
    <sample id="272">There are 7 authors involved in the paper.</sample>
    <sample id="274">The name of the speaker is Yusen Zhang.</sample>
    <sample id="276">**Abstract**  
Ananya and Vignesh present *IndicMT Eval*, a dataset designed to meta-evaluate machine translation (MT) metrics for Indian languages, addressing the understudied area of reverse translation evaluation. The dataset includes 7,000 samples from five Indian languages (Tamil, Malayalam, Hindi, Marathi, and Gujarati), generated by seven translation models or APIs. Human bilingual annotators evaluated the outputs, categorizing errors into accuracy/meaning, fluency, and special categories, and assigning severity and overall scores. The study compares MT metrics, revealing that overlap-based metrics (e.g., chrF) correlate poorly, while embedding-based metrics (e.g., LabSE, BERTscore) and COMET variants show higher correlations with human scores. COMET-based metrics, particularly fine-tuned IndicCOMET MQM, outperform COMET baselines across languages, demonstrating robustness and zero-shot capabilities. The dataset highlights the importance of language-specific evaluation metrics, as human scores span the full range, unlike many metrics that exhibit skewed score distributions. The work underscores the need for language-specific fine-tuning and evaluation frameworks to improve MT quality for Indian languages. The dataset is publicly available for further research.</sample>
    <sample id="277">The new method does not have a specific name mentioned in the provided content.</sample>
    <sample id="278">The "marked words" method, as described by the author, is a sociolinguistic approach that identifies words distinguishing marked (marginalized) groups from unmarked (dominant) groups. It uses the concept of markedness, where unmarked groups are linguistically and socially default, while marked groups are highlighted as different. The method compares personas generated by LLMs using weighted log-odds ratios (Fightin’ Words method) to distinguish top words for each marked group, revealing harmful stereotypes and essentializing narratives. For example, it uncovers how seemingly positive words like "culture," "tradition," or "strong" perpetuate harmful stereotypes and othering for marginalized groups.</sample>
    <sample id="279">The author of the paper, Shangbin, is a PhD student at the University of Washington.</sample>
    <sample id="280">**Abstract:**  
Emotion Regulation in Conversations (ERC) is a challenging task that involves predicting the emotion label of each utterance in a dialogue, integrating textual, audio, and visual modalities. Existing methods often inadequately exploit multimodal information, struggle with minority emotion classes, and fail to distinguish semantically similar emotions. To address these challenges, we propose **MultiEMO**, an attention-based correlation-aware multimodal fusion framework. MultiEMO consists of four key components: unimodel feature extraction, context modeling, multimodal fusion, and emotion classification.  

Our contributions include:  
1. **VisExtNet**: A novel visual feature extractor that focuses on facial expressions rather than redundant scene information.  
2. **MultiAttn**: A multimodal fusion model using bidirectional multi-head cross-attention layers to effectively integrate textual, audio, and visual modalities.  
3. **Sample-Weighted Focal Contrast Loss (SWFC)**: A loss function that prioritizes hard-to-classify minority classes and maximizes inter-class distances for better distinguishing semantically similar emotions.  

Experiments on MELD and IEMOCAP datasets demonstrate that MultiEMO achieves state-of-the-art performance, particularly in minority and semantically similar emotion classes. However, limitations remain, such as the inability of VisExtNet to distinguish speakers from irrelevant people and the computational cost of SWFC. Despite these challenges, MultiEMO represents a significant advancement in ERC by addressing key unresolved issues in multimodal emotion recognition.</sample>
    <sample id="281">This research explores the role of context in machine translation, focusing on when and how translations require contextual information. The study, conducted across 14 language pairs using TED talk transcripts, introduces the Pointwise Contextual Usage Measure (P-CXMI) to quantify the dependency of words on context during translation. High P-CXMI words are identified as those requiring context for accurate translation. The analysis reveals patterns in context usage across languages, such as the need for dual pronouns in Arabic, appropriate verb forms, and proper noun consistency in Chinese. Additionally, the study highlights the importance of context in translating formality and resolving ellipses.

To address the challenges of evaluating context-dependent translations, the researchers developed the Multilingual Discourse-Aware (MuDA) tagger, which identifies discourse phenomena in parallel corpora. This benchmark is used to evaluate translation models at the document level. The results show that while corpus-level metrics like BLEU favor context-agnostic models, context-aware models perform better on specific phenomena like formality and lexical cohesion. However, models do not significantly outperform context-agnostic models on other phenomena such as pronouns and verb forms. The benchmark also demonstrates that DeepL outperforms Google Translate in document-level translation. Overall, the study underscores the limitations of corpus-level metrics in evaluating translation quality and highlights areas for improvement in context-aware models.</sample>
    <sample id="282">**Abstract:**  
At ACL 2023, we present *StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing*, addressing the challenge of story-level style transfer in non-parallel text. Unlike prior work focusing on token or sentence-level style transfer, our approach targets discourse-level linguistic preferences, which are critical for imitating author style. The primary challenges include capturing author-specific discourse structures and style-topic associations, which are difficult to transfer across texts. To address these, we propose *StoryTrans*, a generation model that learns discourse representations from source texts and combines them with learnable style embeddings. The model employs a two-stage training framework: (1) style transfer with masked style-specific content keywords and (2) content enhancement by explicitly incorporating these keywords. For training, we use an advisory framework, including self-reconstruction loss, disentanglement loss, sentence order loss, and style classifier loss to align style and content. In experiments, we evaluate *StoryTrans* on Chinese and English datasets, demonstrating superior performance in style control and content preservation compared to baselines. Manual evaluations and style visualization confirm the model’s ability to align with target styles and enrich storylines while maintaining semantics. Our work provides a comprehensive solution for non-parallel story-level style transfer, with datasets and code available for further exploration.</sample>
    <sample id="283">The first mentioned symmetrical dependency structure is the **Prague approach**.</sample>
    <sample id="284">In this paper, we introduce **FSUIE (Fuzzy Span Mechanism for Universal Information Extraction)**, a novel approach to enhance span-based Universal Information Extraction (UIE) models. Traditional UIE models rely heavily on precise span boundary annotations, which introduce ambiguity and overfitting. To address this, we propose a **fuzzy span mechanism** where span boundaries are modeled as continuous distributions rather than fixed positions. This reduces the model's dependency on exact boundary labels and improves generalization.

Additionally, we address the mismatch between transformer-based feature extraction and UIE by introducing **adaptive fuzzy span attention**. This attention mechanism dynamically adjusts the span length and linearly decays on the boundary, enabling the model to focus on relevant semantic information within a limited range. The fuzzy span loss combines Binary Cross Entropy (BCE) with Kullback-Leibler (KL) divergence to optimize the model's predictions.

We evaluate FSUIE on three UIE tasks: Named Entity Recognition (NER), Relationship Extraction, and Aspect Sentiment Triplet Extraction. Results show significant improvements over baseline models, particularly on small datasets and domain-specific tasks. FSUIE achieves state-of-the-art (SOTA) results on ACE2004, ACE2005, and ADE datasets for relationship extraction, and competitive results on AST-V2 for aspect sentiment extraction. Ablation studies confirm that the fuzzy span loss and attention mechanism jointly contribute to improved performance and convergence speed.

In summary, FSUIE leverages fuzzy span mechanisms and adaptive attention to enhance UIE models, achieving robust performance across diverse tasks.</sample>
    <sample id="285">The paper "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework" by Mingqi Gao et al. addresses the issue of factual errors in dialogue summarization, a critical yet underexplored area. The authors highlight two approaches to tackle factual errors: integrating factuality-related objectives into summarization models and developing standalone Factual Error Correction (FEC) models. They critique existing FEC evaluations, which rely on factuality metrics like FactCC and DAE, arguing that these metrics are vague and can lead to models generating entirely new summaries without correcting errors. To address these flaws, the authors propose a fine-grained evaluation framework based on manually annotated reference corrections. This framework ensures that FEC models correct factual errors minimally, maintaining fluency and non-redundancy. The taxonomy of factual errors is expanded to include content-based (e.g., part-of-speech, dependencies) and form-based (e.g., addition, deletion, substitution) categories. The evaluation framework, inspired by ERRANT, involves alignment, classification, and comparison steps. Experiments demonstrate that training FEC models with reference summaries from dialogue datasets yields superior results compared to using unreliable factuality metrics. The authors emphasize the importance of human-corrected summaries in improving FEC performance and suggest combining human-annotated data with synthetic data as a promising direction. They also identify limitations, such as the inability of current FEC models to correct additions or address attribute, modality, and link errors. Overall, the work calls for a shift in FEC evaluation methods to better assess and improve factual accuracy in dialogue summarization.</sample>
    <sample id="286">The speaker's name is James Finch.</sample>
    <sample id="287">There are four authors involved in the paper: Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="288">The datasets that can be used to test syntactic phenomena include:

1. **BLiMP (Berkeley Linguistic Marker Passage)**: A dataset that contains grammaticality judgments for various syntactic constructions.
2. **SyntaxGym**: A dataset that provides grammaticality judgments for a range of syntactic phenomena.

These datasets can be used to create longer sequences by selecting acceptable or unacceptable sentences from them and adding them as prefixes to the query pairs, allowing for the evaluation of language models' acceptability judgments across longer context windows.</sample>
    <sample id="290">The abbreviations of the five methods for the first research question are:  
1. FTw (Fine-tuning on clean validation samples)  
2. COSINE (a weakly supervised learning method mentioned in the presentation)  
3. WSL (Weakly Supervised Learning)  
4. WSL+FTw (Weakly Supervised Learning with fine-tuning on clean validation samples)  
5. WSL+NoFTw (Weakly Supervised Learning without fine-tuning on clean validation samples)</sample>
    <sample id="291">The model is evaluated on 11 biomedical and clinical downstream tasks, including:

1. Named entity recognition
2. Classification
3. Part-of-speech tagging
4. Question answering

These tasks are part of the public and private downstream tasks used for evaluation.</sample>
    <sample id="294">CamemBERT is initially trained on the **French Wikipedia corpus**.</sample>
    <sample id="295">The name of the speaker is Adam Przepiórkowski.</sample>
    <sample id="296">The research presented explores the challenges of irony detection in natural language processing (NLP), focusing on the limitations of the ground truth assumption in supervised machine learning. A collaborative effort between the University of Turin and Amazon Alexa resulted in the development of the English Perspectivist Irony Corpus (EPIC), a dataset of 300 short conversations collected from social media platforms like Reddit and Twitter over 1½ years, spanning five varieties of English. Crowdsourcing platform Prolific was used to gather annotations from 74 annotators, yielding an average of 5 annotations per conversation. The annotation interface was designed to be simple, asking annotators to classify replies as ironic or not in context.

The study analyzed inter-annotator agreement across various demographic groups, revealing differences in irony perception based on gender, age, and nationality. These differences led to the development of perspective-aware models, which were fine-tuned on splits of the dataset corresponding to different annotators. While raw performance did not show significant trends, the perspective-aware models demonstrated higher confidence in their predictions compared to aggregated gold standard models.

Further analysis identified that generational proximity and geographical proximity among annotators were key factors in disagreement over irony perception. This research highlights the importance of considering annotator perspectives in NLP tasks and suggests that perspective-aware models can improve confidence and understanding in irony detection. The findings will be discussed further at the poster session.</sample>
    <sample id="297">This project, "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models," explores the use of dogwhistles—coded terms that convey hidden, often hateful, messages to specific in-groups while appearing innocuous to out-groups. The research develops a typology and glossary of over 340 racist, transphobic, and anti-Semitic dogwhistles, categorized by register (formal/informal), persona (e.g., anti-Semitic, transphobic), and type (whether the term adds implicature or signals persona). A case study of historical U.S. political speeches reveals that racial dogwhistles increased post-Civil Rights era, aligning with the Republican Southern Strategy, and became more associated with conservatism over time. The study also evaluates dogwhistle recognition in language models, particularly GPT-3, finding that while the model can surface some dogwhistles, its performance varies, especially with informal or transphobic terms. Prompting strategies, such as including definitions or secret cues, improve accuracy. Additionally, a toxicity detection case study using the Prospective API demonstrates that dogwhistles can evade content moderation, as sentences containing dogwhistles are rated as less toxic than those with explicit slurs or group labels. This research highlights the challenges of detecting dogwhistles in NLP and their role in political influence and online toxicity.</sample>
    <sample id="298">The finding that led to the conclusion that temporal drift is the main cause of performance loss was that the performance of models degraded with a larger temporal gap between the training and test data when retraining or continuing to pre-train them with more recent data. This confirmed the hypothesis of temporal drift as the primary cause of the performance drop.</sample>
    <sample id="299">This paper addresses the issue of robustness in Natural Language Inference (NLI) models, which often rely on spurious shortcuts during training, leading to poor generalization on out-of-distribution (OOD) data. The authors, Michalis Korakakis and Andreas Vlachos, propose a minimax training method to mitigate this reliance on shortcuts. The key insight is that NLI models struggle with under-represented "hard" examples that contradict the shortcuts present in dominant "easy" examples. The proposed method aims to emphasize hard examples during training by using a minimax objective between a learner and an auxiliary model. The learner minimizes NLI task loss, while the auxiliary maximizes the learner's loss by generating example weights that incentivize the learner to focus on high-loss regions. This approach encourages the learner to prioritize hard examples, improving OOD generalization without assuming specific shortcut types. The method is evaluated on three analytic datasets (MNLI, FEVER, QQP) and their OOD adversarial test sets (HANS Symmetric, PAWS), showing consistent improvements in OOD performance while maintaining high in-distribution accuracy. The authors also explore the impact of pre-training the learner, the size of the auxiliary, and conduct a qualitative analysis of the learned example weight distribution. The method is effective across datasets and test sets, demonstrating its robustness and applicability.</sample>
    <sample id="300">Interactive Dictation is a novel task introduced to enable users to dictate and edit documents using voice commands in a natural and intuitive manner. Unlike traditional speech-to-text systems, which separate dictation from editing through specific trigger words, interactive dictation allows for flexible interleaving of dictation and editing using open-ended natural language utterances. This approach mimics the natural interaction between a human and a transcriptionist, where context and intent are understood without predefined commands.

The work formalizes the task into a four-step procedure: ASR recognition, segmentation into dictation and command utterances, command extraction and normalization, and execution of dictation and commands to reach the final document state. A dataset was collected using a custom interface that simulates real-time interaction, allowing users to issue commands and dictations in sequence.

A baseline system was developed, employing separate models for each step. The ASR repair and interpretation models were evaluated jointly, showing a trade-off between runtime and accuracy. GPT-3 models were found to be more accurate but slower, while T5 models offered a better balance between efficiency and accuracy. The research highlights the potential for further advancements in this area and encourages future work to build upon the introduced task and dataset. The code and paper are available for further exploration and development.</sample>
    <sample id="302">It is necessary to permute the tokens for the output sequence because, after the first step of tagging each input token with an unordered multiset of tokens that will appear in the output, the tokens are not yet in their correct order. The permutation step is required to arrange these tokens into the correct sequence that corresponds to the output logical form, ensuring that the model can generate the correct output sequence that matches the input's meaning.</sample>
    <sample id="303">The authors recommended increasing transparency about bias mitigation methods because it is unclear whether positive stereotypes and essentializing narratives in generated personas are due to specific value alignments, anti-stereotyping methods, or other factors. Without transparency, these pernicious patterns cannot be properly studied or addressed.</sample>
    <sample id="304">Minimal-pair unacceptable inputs are pairs of sentences that differ by only one word or element, where one sentence is grammatically acceptable or semantically appropriate, and the other is not. These pairs are used to evaluate language models' ability to distinguish between acceptable and unacceptable language. In the context of the paper, these minimal pairs are extended to longer sequences by adding acceptable or unacceptable prefixes from relevant datasets (e.g., BLiMP, SyntaxGym) or even unrelated domains like Wikipedia, to assess how context length impacts the model's judgments.</sample>
    <sample id="305">**Abstract:**  
Recent work in weakly supervised learning (WSL) has claimed high performance on clean test sets using only weakly labeled data. However, this overlooks the necessity of clean validation samples for model selection, which are often assumed but rarely explicitly reported. This study critically examines this issue by addressing three research questions: (1) Is clean validation data essential for WSL? (2) How many clean samples are required? (3) How can clean samples be optimally utilized?  

The findings reveal that recent WSL methods indeed require clean validation samples to generalize effectively, as their performance drops significantly without them. Increasing the number of clean validation samples further improves performance, with as few as 20 samples per class often being sufficient. Additionally, fine-tuning directly on clean samples achieves comparable or even better performance than WSL approaches, rendering the latter unnecessary.  

The study concludes that the performance gains of WSL methods are overestimated, as they rely on clean validation data, which incurs manual annotation costs. It recommends that future work in WSL report model selection criteria, compare WSL with few-shot learning baselines, and consider continuous fine-tuning as a strong baseline. The code for this research has been open-sourced for further exploration.  

In summary, WSL’s practicality and performance claims need reevaluation, and simpler fine-tuning approaches should be prioritized.</sample>
    <sample id="306">Sebastian Schuster and Najoung Kim investigate the entity tracking capabilities of large language models (LLMs) in discourse understanding. Entity tracking involves identifying entities and their state changes over time, crucial for comprehending longer conversations or narratives. The researchers designed a task using boxes and objects to evaluate LLMs' ability to track entity states without relying on heuristics or pre-trained knowledge. The task involved initial descriptions of box contents and subsequent operations (e.g., moving or adding objects), requiring models to predict updated contents. To prevent shortcuts, the task design excluded fine-tuning or in-context demonstrations. Experiments with Flan-T5, GPT-3, and GPT-3.5 models revealed that only GPT-3.5 models, pre-trained on substantial code, exhibited non-trivial entity tracking. Smaller models like T5-base could learn the task with direct fine-tuning, but randomly initialized models of the same architecture could not, highlighting the importance of pre-training. The study suggests that pre-training on code enhances LLMs' ability to track entities, though generalization beyond this specific setup remains unclear. The authors encourage further exploration and discussion of these findings.</sample>
    <sample id="307">The authors evaluated their models on **11 biomedical and clinical downstream tasks** in French, including:  
- **Named Entity Recognition (NER)**  
- **Classification**  
- **Part-of-Speech (POS) Tagging**  
- **Question Answering**  

They compared the performance of their models against **six baseline models**, including CamemBERT, PubMedBERT, BioBERT, and ClinicalBERT, to assess improvements and identify the best-performing models for specific tasks. The evaluation highlighted that models trained on data of the same nature as the downstream tasks performed best, but data from heterogeneous sources showed greater versatility.</sample>
    <sample id="308">This presentation explores the concept of **positionality** in NLP, highlighting how datasets and models can reflect the biases and perspectives of their creators and the populations they represent. Positionality, a term borrowed from critical studies, refers to the influence of demographics, identity, and life experiences on research outcomes. The research team, including collaborators from Carnegie Mellon University, the University of Washington, and the Allen Institute for AI, developed the **NLPositionality framework** to systematically analyze and characterize these biases.

The framework involves re-annotating datasets with diverse annotators from various demographic backgrounds, comparing these annotations to existing models and datasets using metrics like Pearson's R correlation. This approach differs from traditional annotator disagreement studies by focusing on the alignment between end users and models/datasets. The study utilized **Lab in the Wild**, an online crowdsourcing platform, to recruit over 1,000 annotators from 87 countries, resulting in over 16,000 annotations.

Key findings reveal that NLP datasets and models are predominantly aligned with **English-speaking countries** and individuals with **college education**, while underrepresenting non-binary individuals. This underscores the need for inclusive NLP practices. The researchers recommend **documenting design choices**, adopting a **perspectivist lens** in research, and building **community-specific datasets** to address these biases. The work emphasizes that inclusive NLP is not just about universal functionality but about ensuring equitable representation and fairness across diverse populations.</sample>
    <sample id="309">The metric used for measuring inter-annotator agreement in the study was the agreement on 100 doubly-labeled conversations.</sample>
    <sample id="310">The domain chosen to add completely unrelated sentences to the unacceptable and acceptable queries was **Wikipedia**.</sample>
    <sample id="311">The affiliations of the authors of the paper are not explicitly mentioned in the provided content.</sample>
    <sample id="312">MultiInstruct differs from other benchmarks in several key ways:

1. **First Multi-Modal Instruction Tuning Benchmark**: It is the first large-scale dataset specifically designed for multi-modal instruction tuning, addressing the lack of publicly available multi-modal instruction datasets.

2. **Diverse Multi-Modal Tasks**: It includes 62 diverse multi-modal tasks across 10 broad categories, derived from 21 existing open-source datasets, providing a comprehensive range of multi-modal scenarios.

3. **Unified Sequence-to-Sequence Format**: All tasks are formulated in a unified sequence-to-sequence format, where input text, images, instructions, and bounding boxes are represented in the same token space, facilitating consistent processing across different data types.

4. **Instruction Tuning for Multi-Modal Models**: It focuses on improving the generalization of multi-modal pre-trained models (like OFA) to unseen multi-modal tasks through instruction tuning, unlike previous works that primarily focused on language-only tasks.

5. **Evaluation Metrics**: It introduces a new evaluation metric called **sensitivity**, which measures the model's ability to consistently produce the same outputs for the same task regardless of slight variations in the wording of the instruction.

6. **Transfer Learning from Natural Instruction Datasets**: It explores the benefits of transfer learning from natural instruction datasets, showing that it can improve both performance and sensitivity.

In summary, MultiInstruct stands out by providing the first large-scale multi-modal instruction tuning dataset, focusing on improving multi-modal model performance, and introducing new evaluation metrics and transfer learning techniques.</sample>
    <sample id="313">There are two authors involved in the paper: James Finch and Sarah Finch.</sample>
    <sample id="314">Binary coordination refers to the structure of coordination where two elements (conjuncts) are linked together, typically by a coordinating conjunction (e.g., "and," "or"). In this structure, one of the conjuncts is designated as the head of the coordinate phrase, while the other is subordinate to it. This is often seen in asymmetric dependency structures, such as Universal Dependencies and Mel'čuk's meaning text theory, where the first conjunct is the head of the coordinate structure.</sample>
    <sample id="315">The content does not provide specific information about the average length of the prompts used in the study.</sample>
    <sample id="316">The findings imply that smaller models, specifically T5 when fine-tuned on the CoScript dataset, can generate scripts of higher quality than most large language models for constrained language planning. This suggests that smaller, specialized models can surpass larger models when properly trained on suitable datasets, which could lead to more efficient and cost-effective language planning applications.</sample>
    <sample id="317">**Abstract:**  
This paper introduces *CodeIE*, a novel approach to information extraction (IE) that leverages large code generation models like *Codex* to address the challenges of mismatched outputs between pre-training and inference phases in traditional text-to-text models. Traditional models, such as *T5* and *GPT-3*, struggle with generating structured outputs like named entities (NER) or relations due to the mismatch between linearized text inputs and structured outputs. *CodeIE* transforms the IE task into a structure-to-structure code generation task, aligning input and output structures by converting text to structured formats during input and maintaining structured outputs during inference.  

For NER, a code-style prompt is designed to define a function that extracts entities from input text, while for relation extraction, similar prompts are used. Evaluations on three NER datasets and four relation extraction datasets show that *CodeIE* significantly outperforms traditional models like *UIE* and *GPT-3* in few-shot scenarios. The perplexity of text-format inputs is higher for models like *T5* compared to code-format inputs for models like *CodeT5*, indicating better alignment with the IE task. Structural errors are minimized when using *Codex* with code-style prompts, and *Codex* consistently outperforms *GPT-3* across tasks. The recall of *CodeIE* is particularly strong, especially when using code-style prompts. This work highlights the effectiveness of code generation models in IE tasks and provides insights into improving structured output generation. The paper and code are publicly available for further exploration.</sample>
    <sample id="319">The work investigates the following learning strategies:

1. **From-scratch pre-training**: Training models (e.g., DrBERT, ChuBERT) on biomedical and clinical data from scratch.
2. **Continual pre-training**: Fine-tuning pre-trained models (e.g., CamemBERT) on biomedical and clinical data.
3. **Data source variation**: Comparing models trained on different data sources (e.g., NACHOS, clinical notes).
4. **Data size variation**: Evaluating the impact of varying data sizes (e.g., 4 GB vs. 7 GB) on model performance.</sample>
    <sample id="320">The factor of overfitting due to test reuse (adaptive overfitting) is not observed in the study, as indicated by the graph showing a gradient greater than one in the performance improvement on CoNLL++ compared to CoNLL-2003. This means there are no diminishing returns, suggesting adaptive overfitting is not a significant factor in this context.</sample>
    <sample id="321">The quality of the simplification was evaluated by analyzing the type of simplification (e.g., lexical simplification, structure simplification, overall level of simplification) and the variety of simplification transformations (e.g., reorderings, word additions, rephrasings) in the DEPLAIN corpus. Additionally, the corpus was used as a gold standard to evaluate automatic alignment methods and fine-tuned language models for text simplification.</sample>
    <sample id="322">**Abstract:**  
At ACL 23, Enrico presents a study on what text classifiers learn about morality, emphasizing the complexity and subjectivity of moral judgments. Morality, as an internal compass distinguishing right from wrong, is foundational to societal structures, and language models must accurately interpret moral nuances in text. Traditional approaches often treat morality as a binary scale (immoral to moral), but this overlooks its pluralistic nature, where diverse perspectives coexist. Enrico highlights the Moral Foundations Theory, which identifies five distinct moral dimensions (e.g., fairness, authority) that individuals prioritize differently, shaping moral judgments.  

Recent NLP research has explored moral classification in text, but Enrico’s work delves deeper into how morality is expressed across different domains. Using the Moral Foundation Twitter Corpus, the study examines 35,000 tweets from domains like #AllLivesMatter and #BlackLivesMatter. The analysis reveals that language models can detect nuanced differences in moral rhetoric, such as the contrasting treatment of subversion in these domains. While both share similar topics, models identify that subversion is viewed negatively in #AllLivesMatter but positively in #BlackLivesMatter.  

Enrico’s research underscores the dangers of using a single model across diverse domains, as it risks misinterpreting moral complexities. By applying explainable AI techniques, the study highlights the need for domain-specific models to accurately capture the multifaceted nature of morality in text. This work cautions against oversimplifying moral judgments in AI systems and advocates for a more nuanced understanding of moral expression.</sample>
    <sample id="323">**Abstract**

This paper addresses the challenge of Commonsense QA, a task requiring machines to answer questions using common knowledge. Existing methods combine language models and knowledge graphs (HKGs) to retrieve relevant knowledge and infer answers. However, these methods often introduce noisy entities during subgraph retrieval and encode text and subgraph in isolation, limiting interaction between modalities. To overcome these limitations, we propose **Dynamic Heterogeneous-Graph Reasoning (DHLK)**. DHLK builds an optimized HKG using a two-stage pruning strategy and Knowledge Representation Learning (KRL), ensuring accurate knowledge representation. It then leverages a language model (RoBERTa) to encode and fuse QA contexts with the HKG. DHLK dynamically removes irrelevant entities based on attention weights, enhancing subgraph relevance. For subgraph modeling, we introduce **Relation Mask Self-Attention (RMSA)**, which incorporates relationships to improve entity and relation embeddings. The HKG graph embedding is obtained via max-pooling, and path information is integrated into the QA context for enhanced representation. Finally, the QA context, HKG graph embedding, and path information are fed into a Multi-Layer Perceptron (MLP) for answer prediction. Experiments on CommonsenseQA and OpenBookQA demonstrate that DHLK outperforms other LM and HKG methods, achieving strong results. Key entities are extracted using KeyBERT, and knowledge paths are retrieved within two hops in ConceptNet. This work highlights the importance of dynamic entity selection, multimodal fusion, and advanced subgraph modeling for effective Commonsense QA.</sample>
    <sample id="324">Yes, language models have different political biases. Preliminary results indicate that they occupy all four quadrants on the political spectrum, with GPT-4 being the most liberal and GPT series generally more socially liberal than BART series. Additionally, further pretraining on partisan corpora shifts the ideological coordinates of the language models, demonstrating that political biases can be influenced by the pretraining data.</sample>
    <sample id="326">Cognitive dissonance is the state of holding two or more beliefs, values, or actions that are inconsistent or contradictory, such as believing that cigarettes are harmful while also smoking them. This inconsistency creates psychological discomfort, which individuals often resolve by changing their beliefs, justifying their actions, or avoiding the situation. Cognitive dissonance is a key concept in psychology and can be studied in language to understand personal decision-making, mental health, and societal trends.</sample>
    <sample id="327">**Abstract:**  
At ACL 2023, Xiao Xu presents "ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning," developed during an internship at MSRIC with support from Intel Cognitive Computing Group. The work addresses limitations in existing Vision-Language (VL) models, particularly BridgeTower, which ineffectively utilizes unimodal layer representations and lacks scalability. ManagerTower introduces a novel architecture that uses managers in each cross-modal layer to adaptively aggregate insights from multiple unimodal expert layers, enabling more comprehensive cross-modal alignment and fusion.  

RoBERTa and CLIP-ViT base serve as unimodal encoders, with managers dynamically combining insights from different levels of unimodal semantic knowledge. ManagerTower achieves superior performance on downstream tasks like Visual Question Answering (VQA) with only four million images for pre-training, outperforming METER, BridgeTower, and other models with similar or more data.  

Visualization of aggregation weights reveals that adaptive managers exhibit diverse and adaptive weight distributions across cross-modal layers, contrasting with static managers. This demonstrates ManagerTower's ability to effectively exploit different levels of unimodal semantic knowledge. The paper, code, and models are available on Archive and GitHub, aiming to advance VL representation learning.</sample>
    <sample id="328">The most liberal language model is GPT-4.</sample>
    <sample id="329">This paper presents a novel approach to zero-shot video sentence localization, a task that identifies relevant video segments corresponding to natural language queries. The authors, from Peking University, address the challenge of requiring extensive manual annotations for training by proposing a noise-resistant method for generating structured pseudo-labels. Their approach overcomes limitations of existing methods, which often produce simplistic pseudo-queries and fail to ensure low relevance between video segments outside events and queries.

The proposed method involves three key steps: (1) generating complex pseudo-queries using a pre-trained image caption model, (2) creating pseudo-events by modeling the temporal structure of events to ensure high relevance within events and low relevance outside, and (3) reducing label noise by re-weighting samples based on model confidence and IoU, and refining pseudo-labels through iterative training.

Experiments on ActivityNet Captions and Charades-STA datasets demonstrate the method's effectiveness, outperforming existing zero-shot methods in terms of R@M and mIoU metrics. The authors conclude that their structured pseudo-label generation method robustly handles label noise, achieving state-of-the-art zero-shot performance. The code for their implementation is available for further exploration.</sample>
    <sample id="330">Yes, cumulative training performs equal or better than iterative training when doing active learning for dissonance detection.</sample>
    <sample id="331">The name of the speaker is Sara Papi.</sample>
    <sample id="332">The data for the MuDa benchmark was taken from transcripts of TED talks translated from English into 14 different languages.</sample>
    <sample id="333">This paper introduces **INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation**, a novel framework to enhance the generalization and performance of Neural Machine Translation (NMT) models. The authors address the issue of sparse representation in NMT models, where low-frequency tokens disperse unevenly, leading to poorly defined semantic meanings and poor translation performance. To tackle this, they propose **kNN-MT**, which smooths predictions by querying a datastore of nearest neighbors during inference. However, kNN-MT suffers from inefficiencies: slow neighbor retrieval and inability to update representations dynamically.

INK overcomes these limitations by introducing an iterative training loop. First, kNN knowledge is extracted from the datastore to guide an adapter in adjusting representations. Then, updated representations are asynchronously refreshed in the datastore. This loop continues until convergence, allowing the model to refine its representation space without relying on the datastore during inference. The adapter aligns contextualized representations with token embeddings to enrich semantic meanings and address sparsity.

Experiments on the WMT’19 German-English translation task demonstrate INK's effectiveness. Compared to state-of-the-art kNN-MT and baseline adapters, INK achieves an average gain of 1.99 COMET score and 1.0 BLEU score, with improved translation quality, reduced memory usage, and faster inference. The framework also shows that jointly using an adapter and datastore can further refine predictions, suggesting potential for future improvements. Overall, INK provides a scalable and efficient solution to enhance NMT model performance and generalization.</sample>
    <sample id="335">The name of the speaker is Matthias Lindemann.</sample>
    <sample id="336">Cross-lingual transfer refers to the process of training a model on one language (source language) and then using it to perform tasks on another language (target language) without explicit adaptation or fine-tuning for the target language. This approach leverages the knowledge gained from the source language to improve performance on the target language, often reducing the need for large amounts of target language data. In the context of semantic parsing, cross-lingual transfer can involve translating queries from one language into another and then parsing them into semantic representations like SQL or Lambda Calculus.</sample>
    <sample id="337">This research introduces a novel approach to handling out-of-vocabulary (OOV) words in word embedding learning, leveraging word formation and association to infer their meanings. The key contribution is the development of a **Word Relationship Graph (WRG)** that mimics lexical rules of word formation and association. The WRG is constructed by tokenizing OOV words into wordpieces and associating them with relevant words, forming a two-level graph. The first layer retains all wordpieces to preserve complete information, while the second layer samples a fixed number of nodes to mitigate noise from numerous neighbors.

To address the challenge of assigning node attributes to OOV nodes, a **self-attention network** is employed to attribute values based on the characters of OOV words. A two-level **Graph Attention Network (GAT)** is used to extract important information and reduce noise, resulting in a node-level representation. A **readout block layer** captures graph-level information and summarizes word formation.

The model uses **contrastive learning** in the loss function, incorporating NT-XENT positive samples from the graph, such as two-hop relevant neighbor words, synonyms, or the OOV word itself. This encourages proximity between graph-level embeddings while pushing them apart from other samples.

Experiments demonstrate superior performance over baselines in both intrinsic and extrinsic tasks, proving the effectiveness of learning OOV words through word formation. The model benefits both static and contextual downstream tasks. While agglutinative languages are well-suited, fusional languages present challenges, but the model performs well with English due to reasonable word segmentation. The approach is extensible to other languages, depending on the rationality of word decomposition.</sample>
    <sample id="338">The research paper titled "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations" explores the quality and utility of human-annotated natural language explanations in improving machine learning models, particularly in tasks like commonsense QA, natural language inference, and commonsense validation. The authors, from Rensselaer Polytechnic Institute, Northeastern University, and IBM Research, address the challenge of evaluating human explanations, which can be subjective and task-dependent, unlike labels. Traditional metrics like BLEU and ROUGE focus on word similarity, while the simulatability score measures performance changes with or without explanations but lacks task-specific evaluation.

The study proposes a unified data structure and a novel metric, TREU (Task-Relevant Explanation Utility), which extends the simulatability score to assess the helpfulness of explanations during fine-tuning and inference. Using five datasets (CoS-E, ECQA, e-SNLI, ComVE), the authors conduct experiments to analyze how explanations impact model performance. Results show that fine-tuning with explanations can significantly improve model predictions, though the utility of explanations varies by task. For instance, CoS-E explanations are less helpful than ECQA’s on baseline models, highlighting task-dependent explanation quality.

The TREU metric outperforms the simulatability score in evaluating dataset qualities and task-specific nuances, such as entailment, neutrality, and contradiction. The study emphasizes the importance of task-aware evaluation in human-machine collaboration and recommends similar quality checks in future research. Overall, the work advances the objective evaluation of human explanations, paving the way for more effective human-AI collaboration.</sample>
    <sample id="339">The authors of the paper "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" are affiliated as follows:
- Dawei: PhD student at Saarland University, Germany
- Xiaoyu Shen: Saarland University
- Marius Mosbach: Saarland University
- Andreas Stephan: Saarland University
- Dietrich Klakow: Saarland University</sample>
    <sample id="340">**Abstract:**  
Kuan-Hao Huang et al. present *ParaAMR*, a large-scale, syntactically diverse paraphrase dataset generated using AMR (Abstract Meaning Representation) back-translation. Existing datasets, such as MRPC and Quora, are high-quality but limited in scale, while automatically generated datasets like back-translation lack syntactic diversity. *ParaAMR* addresses this gap by leveraging AMR graphs, which capture the abstract meaning of sentences, to create diverse paraphrases. The method involves parsing source sentences into AMR graphs, modifying the focus node, and generating text from the altered graph structure. This approach ensures semantic similarity while introducing syntactic variation. *ParaAMR* contains 15 million source sentences with an average of 6.9 paraphrases each, demonstrating higher syntactic diversity compared to traditional back-translation datasets. Quantitative and human evaluations confirm its effectiveness, with similar semantic similarity scores but superior syntactic diversity. The dataset benefits NLP applications, including sentence embedding learning, syntactic control paraphrase generation, and data augmentation for few-shot learning. *ParaAMR* is publicly available and showcases its utility in enhancing NLP tasks.</sample>
    <sample id="341">The authors use **average lagging** (latency measure) and **computationally aware average lagging** (accounts for model's computational times to predict output) as latency measures.</sample>
    <sample id="342">**Abstract:**  
This paper introduces *LiveChat*, a large-scale personalized dialogue dataset constructed from live streaming videos on Chinese platforms like TikTok and Douyin. Unlike existing text-sourced datasets, *LiveChat* is video-sourced, capturing spoken language and providing a more realistic representation of human-AI dialogue. It addresses challenges in open-domain and personalized dialogue, including the lack of large-scale Chinese multi-party dialogue datasets. The dataset is built in three steps: (1) extracting audio from streaming videos and transcribing it, (2) constructing dialogues using an audience-to-streamer reply-to-whom matching method, and (3) collecting persona information for personalized dialogue generation. Persona extraction is enhanced through manual labeling and rule-based classifiers.  

Experiments on response modeling and addressee recognition demonstrate the dataset’s effectiveness, with longer average sessions and selected persona profiles improving performance. Transfer learning experiments show that BART outperforms other LLMs, highlighting *LiveChat*’s unique domain characteristics. The dataset also explores in-context learning, revealing that increasing demonstrations improve performance up to 8 shots but may introduce noise beyond this threshold.  

In conclusion, *LiveChat* is a valuable resource for advancing open-domain and personalized dialogue systems, particularly in Chinese-language applications. Future work will focus on efficient transfer learning for LLMs on this dataset.</sample>
    <sample id="344">The drawbacks of tree-based methods for compositional generalization include:

1. **Complexity in Obtaining Trees**: Trees are not typically provided and require specialized grammar-induction procedures or pre-processing, which can be computationally expensive and formalism-specific.
2. **Handling Variable Symbols**: Trees often need significant pre-processing to handle variable symbols, adding to the complexity.
3. **Lack of Flexibility**: Tree-based methods are rigid and may not generalize well to unseen structures without extensive training data on similar tree patterns.</sample>
    <sample id="345">This paper introduces a novel approach to compositional generalization in semantic parsing without relying on trees. Compositional generalization refers to a model's ability to handle unseen, deeper recursion and novel combinations of phrases encountered during training. Traditional sequence-to-sequence (seq2seq) models struggle with out-of-distribution generalization, often producing outputs that lack systematic correspondences with the input. Tree-based methods, which aim to capture compositional processes, are effective but require computationally intensive tree generation, often involving specialized grammar induction and pre-processing.

The proposed method, "Multiset Tagging and Latent Permutations," directly models correspondences between input and output fragments using a neural seq2seq architecture. It consists of two steps: first, tagging each input token with an unordered multiset of output tokens, and second, predicting a permutation to order these tokens. This approach avoids the need for trees and is highly flexible, as it does not impose hard constraints on possible permutations.

The permutation model operates by iteratively selecting tokens from the multiset to fill output positions, ensuring each token is used exactly once. This method addresses the challenges of aligning input and output tokens in training data and handling latent, linguistically correct permutations. The NP-hard problem of finding the highest-scoring permutation is approximated using a GPU-friendly continuous relaxation, enabling backpropagation and learning of more plausible permutations.

Experiments on the COGS benchmark demonstrate that this approach outperforms other treeless models, particularly in generalizing to deeper recursion. However, structural generalization remains challenging in some cases. The paper highlights the technical innovations that address these challenges, offering a promising alternative to tree-based methods for compositional generalization.</sample>
    <sample id="346">The affiliations of the authors of the paper are not explicitly mentioned in the provided content.</sample>
    <sample id="348">**Abstract:**  
This paper, "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models," investigates stereotypes in large language models (LLMs) by generating personas through natural language prompts. Unlike traditional methods that rely on hand-curated datasets, this approach generalizes across demographics and contexts, enabling the measurement of intersectional biases. The study reveals patterns in LLM-generated personas, such as associating women of color with traits like "exotic" or "strong," while white men are unmarked. The Marked Words method, inspired by sociolinguistic markedness, identifies distinguishing words for marked groups (e.g., "culture," "tradition") compared to unmarked groups (e.g., white, male). Results show that while LLMs generate fewer stereotypes than human-written personas, they still perpetuate harmful essentializing narratives, such as hyper-sexualizing Asian women or reinforcing the "Strong Black Women" archetype. The paper concludes with three recommendations: addressing positive stereotypes, adopting an intersectional lens, and increasing transparency in bias mitigation methods to better understand and mitigate these pernicious patterns. This work highlights the need for nuanced approaches to studying and mitigating bias in LLMs.</sample>
    <sample id="350">The paper "What’s the Meaning of Superhuman Performance in Today’s NLU?" critically examines the validity of claims that AI systems outperform humans in Natural Language Understanding (NLU) tasks. The authors, led by Simone Tedeschi, analyze popular benchmarks like SuperGLUE and SQuAD, where systems often achieve superhuman performance. However, they highlight significant flaws in these comparisons: humans are evaluated on smaller, biased subsets of the test data, while systems use the full dataset. Additionally, ground-truth answers in these datasets contain errors, and humans cannot exploit spurious correlations or specific patterns of errors as systems do. The authors also critique the vague estimation of human performance, often aggregated without considering the best possible human performance. Furthermore, the motivation and quality of human annotators vary widely, with some tasks offering very low pay, which affects the quality of human performance. The authors argue that these issues undermine the scientific validity of claims of superhuman performance. They recommend constructing more reliable benchmarks by addressing these flaws and providing transparent details about human annotators and evaluation methods. The paper concludes that current claims of superhuman performance in NLU are not yet scientifically meaningful and calls for more rigorous evaluation practices.</sample>
    <sample id="351">This paper investigates the generalization capabilities of Named Entity Recognition (NER) models trained on the CoNLL-2003 dataset, which has been used for nearly two decades. The authors created the CoNLL++ dataset, comprising Reuters news articles from 2020 annotated using the same guidelines, to evaluate how well these models perform on modern data. They fine-tuned over 20 models on CoNLL-2003 and tested them on both CoNLL-03 and CoNLL++. The percentage change in F1 scores was calculated to assess generalization.

The study identifies three key factors for effective generalization: model architecture (with transformers performing better), model size (larger models generally generalize better), and the number of fine-tuning examples (more examples lead to better performance). The authors tested two hypotheses for performance drop: adaptive overfitting and temporal drift. They found no evidence of adaptive overfitting, as improvements on CoNLL-2003 did not diminish when applied to CoNLL++. However, temporal drift was confirmed as a significant factor, with performance degrading as the gap between training and testing data increased.

In conclusion, the paper affirms that CoNLL-2003 taggers still perform well in 2023, primarily due to their adaptability to new data. The research highlights the importance of model architecture, size, and training data quantity for effective generalization. It also underscores temporal drift as a critical issue for maintaining model performance over time. The authors call for further research to improve model generalization and encourage readers to explore their dataset and paper.</sample>
    <sample id="352">ABC-Eval stands for **Annotating Behaviors in Chat**.</sample>
    <sample id="353">The paper "Python Code Generation by Asking Clarification Questions" by Haau-Sing Li, Mohsen Mesgar, André F. T. Martins, and Iryna Gurevych addresses the challenge of input underspecification in code generation and program synthesis tasks. The authors propose a novel approach that leverages interaction through clarification questions to gather missing specifications, thereby alleviating the underspecification problem. They introduce the task of generating code by asking clarification questions, focusing on operation-level specifications. The paper presents a synthetic dataset, CodeClarQA, and a pipeline that includes a Clarification Need Predictor, a Question Selector, and a Code Generator. The pipeline is evaluated against existing models, showing improved performance on evaluation metrics, particularly with high-ranked CQs being answered and included. The authors also conduct error analysis, identifying common errors such as taxonomy and argument issues, and suggesting potential improvements. The results indicate that clarified key operations contribute to better generated code, with predictions closely resembling the ground truth. The paper concludes that the proposed approach is a promising step towards more robust code generation systems, although further refinement is needed to address the challenges of CQ ranking and underspecification. The authors invite feedback and encourage reviewing their paper and code for further improvements.</sample>
    <sample id="354">The performance delta between CoNLL-2003 and CoNLL++ is higher than 5 percentage points until 2020.</sample>
    <sample id="356">The authors of the paper are Matthias Lindemann, Alexander Koller, and Ivan Titov. Their affiliations are not explicitly mentioned in the provided content.</sample>
    <sample id="357">The name of the speaker is Siyu Yuan.</sample>
    <sample id="358">There are 6 authors involved in the paper.</sample>
    <sample id="359">The approach is compared to the state-of-the-art architecture specifically tailored for simultaneous pre-translation.</sample>
    <sample id="361">**Abstract:**  
This work introduces *CounterComp*, a novel approach to improve compositional generalization in multi-step quantitative reasoning tasks, particularly for question answering on financial tables. State-of-the-art neural models struggle with such tasks, especially when outputs involve more than two arithmetic operations, due to memorization of spurious patterns. To address this, *CounterComp* leverages counterfactual scenarios by mining positive and negative examples from the training data. Positive examples involve interventions in the question that do not alter the output, while negative examples result in output changes. These examples are used to introduce an auxiliary metric learning loss, dynamically adjusting based on the extent of intervention in the question. This loss is added to three state-of-the-art baselines, improving their performance on both in-distribution and out-of-distribution samples. The approach enhances the model's ability to attend to meaningful tokens in the input that relate to the output operations, promoting compositional generalization. By avoiding costly additional supervision, *CounterComp* provides a scalable solution to improve multi-step reasoning capabilities in question answering tasks.</sample>
  </task>
</testset>