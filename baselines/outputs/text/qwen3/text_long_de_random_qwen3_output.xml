<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Sprachmodelle sind große Web-Crawl-Daten, insbesondere politische Nachrichtenmedien wie New York Times, Los Angeles Times, The Guardian und Huffington Post.</sample>
    <sample id="1">Die Autoren gehören der McGill University an.</sample>
    <sample id="2">Tu Yi von Ant Group präsentiert ein Paper zum Thema Visuell reichhaltige Dokumentenverstehen (Visually-rich Document Understanding, VrDU). Das Team untersucht bestehende prätrainierte Modelle, die bei der Verarbeitung von Dokumenten wie Formularen, Quittungen und Plakaten Schwierigkeiten mit der Lesereihenfolge haben. Bisherige Ansätze nutzen globale 1D-Positionen, um die Reihenfolge der Tokens zu kodieren, was zu Problemen führt. Das neue Modell LayoutMask löst dies, indem es lokale 1D-Positionen innerhalb von Abschnitten verwendet und zusätzlich 2D-Positionen und semantische Informationen einbezieht. Um die Wechselwirkung zwischen Text und Layout zu verstärken, führt das Team zwei neue Maskierstrategien ein: Whole Word Masking und Layout-Aware Masking. Zudem wird eine neue prätrainierte Aufgabe, das Masked Position Modeling, eingeführt, um 2D-Positionen zu rekonstruieren. In Experimenten zeigt sich, dass lokale 1D-Positionen in vielen Fällen besser abschneiden, besonders bei komplexen Layouts. Das Modell verbessert das Verständnis von Dokumenten durch tiefergehende Wechselwirkungen zwischen Text und Layout.</sample>
    <sample id="3">Hallo! Willkommen bei unserer Präsentation von DEPLAIN, einem neuen Korpus für die Textvereinfachung im Deutschen auf Dokument- und Satzebene. Mein Name ist Regina Stodden, und ich werde euch durch den ersten Teil der Präsentation führen. Fangen wir zunächst mit der Definition von Textvereinfachung an. Textvereinfachung ist ein Prozess, bei dem ein Text angepasst wird, um dessen Verständlichkeit für eine bestimmte Zielgruppe zu verbessern, beispielsweise für Menschen mit Leseschwierigkeiten oder für Nicht-Muttersprachler. Um ein Modell für Textvereinfachung zu trainieren, benötigen wir parallele Textpaare, beispielsweise von Dokumenten oder Sätzen. Im Beispiel hier seht ihr ein parallel ausgerichtetes Satzpaar eines komplexen deutschen Satzes und seiner Übersetzung in einfache Sprache. Um den Satz zu vereinfachen, können verschiedene Techniken angewandt werden, wie im Beispiel zu sehen ist, beispielsweise lexikalische Substitution, Klauselentfernung, Neuanordnung oder Einfügen von Wörtern.

Wir stellen nun unser neues Korpus, DEPLAIN, vor, denn in den letzten Jahren gab es einige Probleme mit bestehenden Korpora. Zum Beispiel sind diese Korpora zu klein, um ein Textvereinfachungsmodell darauf zu trainieren. Die drei anderen Modelle, die in den letzten Jahren vorgeschlagen wurden, sind alle automatisch ausgerichtet, was bedeutet, dass sie in ihren Ausrichtungen fehleranfällig sein können. Daher schlagen wir unser neues Korpus DEPLAIN vor, das in zwei Unterkorpora unterteilt ist: DEPLAIN-apa und DEPLAIN-web. DEPLAIN-apa basiert auf Nachrichtentexten. In DEPLAIN-apa haben wir 483 Dokumente manuell ausgerichtet. Das ergibt ungefähr 13.000 parallele Satzpaare. Bei DEPLAIN-web umfasst dieses Korpus verschiedene Domänen und wir haben alle 750 Dokumente sowohl manuell als auch mit automatischen Ausrichtungsmethoden ausgerichtet. Insgesamt ergibt das 30.450 Satzpaare.

Wir haben unsere Satzpaare etwas genauer analysiert, beispielsweise in Bezug auf den Typ der Vereinfachung. Wie hier zu sehen ist, werden Bibeltexte deutlich stärker vereinfacht als beispielsweise Nachrichtentexte oder Texte für Sprachlerner. Auf allen Ebenen, beispielsweise lexikale Vereinfachung, strukturelle Vereinfachung oder Gesamtniveau der Vereinfachung, zeigt sich das. Außerdem erkennst du, dass unser DEPLAIN-Korpus eine hohe Vielfalt an verschiedenen Vereinfachungstransformationen aufweist. So gibt es beispielsweise im DEPLAIN-apa-Korpus deutlich mehr Umordnungen und Wortergänzungen als im DEPLAIN-web-Korpus. Auf der anderen Seite gibt es im Web-Korpus deutlich mehr Umformulierungen.

Schauen wir uns jetzt an, was man mit diesem Korpus alles tun kann. Hallo, ich bin Omar und ich werde nun über die Anwendungsfälle unseres Datensatzes DEPLAIN sprechen. Als ersten Anwendungsfall können wir automatische Ausrichtungsmethoden bewerten. In den letzten Jahren wurden viele Ausrichtungsmethoden entwickelt, aber im Kontext der maschinellen Übersetzung, bei der wir zwei parallele Dokumente in verschiedenen Sprachen haben und die Ausrichtung der Sätze in beiden Dokumenten extrahieren möchten. In unserem Anwendungsfall versuchen wir jedoch, Ausrichtungen zwischen Sätzen von zwei parallelen Dokumenten zu extrahieren, die in derselben Sprache geschrieben sind, denselben Inhalt haben, aber auf verschiedenen Komplexitätsstufen. Da wir nun unser Datensatz DEPLAIN haben, der über manuell ausgerichtete Sätze verfügt, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten. Wir haben einige Anpassungen an den vorgeschlagenen Methoden vorgenommen und haben alle diese Anpassungen sowie den Code, um unsere Experimente durchzuführen, in dem Paper veröffentlicht. Am Ende haben wir festgestellt, dass die beste automatische Ausrichtungsmethode für die Textvereinfachung im Deutschen die Methode MASSalign ist. Du findest auch den Code, um diese Methode auf eigenen Dokumenten anzuwenden, im Paper.

Der zweite Anwendungsfall, den wir in unserem Paper gezeigt haben, ist ein Fall der automatischen Textvereinfachung durch Feinabstimmung von Sprachmodellen, um aus komplexen Eingabetexten vereinfachte Texte zu erzeugen. Wir haben zwei verschiedene Modelle feinabgestimmt. Wir haben das Modell long-mBART feinabgestimmt, um Dokumentebene-Vereinfachungen zu erzeugen, und auch das normale base mBART-Modell, um Satzebene-Vereinfachungen zu erzeugen. Du findest auch alle Checkpoints und kannst in das Paper hineinschauen, um mehr Details zu den Scores und den Evaluierungsmetriken unserer Experimente zu erhalten. Wir kamen zu dem Schluss, dass diese grundlegende Feinabstimmung Scores erzeugen kann, die besser sind als die Baseline-Scores, und schlugen diese Ergebnisse als Basisbenchmark für das Problem der automatischen Textvereinfachung in der Zukunft vor.

Vielen Dank für eure Aufmerksamkeit und wir hoffen, euch alle während der Konferenz zu treffen. Vielen Dank.</sample>
    <sample id="4">Der Referent heißt Kayo Yin.</sample>
    <sample id="5">Das T5 XL-Modell wurde verwendet, um die Genauigkeit von 82–87 % zu erreichen.</sample>
    <sample id="6">Jiaan und sein Team präsentieren ihre Arbeit „Towards Unifying Multi-Lingual and Cross-Lingual Summarization“, in der sie die bisherigen Ansätze der multilingualen und cross-lingualen Zusammenfassung in ein neues, allgemeineres Setting namens „many-to-many summarization“ zusammenführen. Ziel ist es, ein einziges Modell zu entwickeln, das Dokumente in beliebigen Quellsprachen in beliebige Zielsprachen zusammenfassen kann. Ihre Untersuchungen zeigen, dass dieser Ansatz bessere Übergänge zwischen Sprachen ermöglicht als bisherige Methoden. Sie stellen außerdem PISCES vor, ein vortrainiertes Modell, das durch eine dreistufige Vortrainierung lernt: Meta-Pretraining, Cross-lingual Pretraining und Task-specific Pretraining. In Experimenten mit dem WikiLingua-Datensatz überzeugte PISCES sich gegenüber Basismodellen wie mBART-50 und mT5. Ablations- und menschliche Studien bestätigen die Effektivität des Ansatzes. Die Arbeit bietet neue Einblicke in die Zusammenfassung über mehrere Sprachen und legt den Grundstein für zukünftige Forschung in diesem Bereich.</sample>
    <sample id="7">Ja, CoNLL-2003-Tagger funktionieren noch gut in 2023, insbesondere wenn sie mit modernen Architekturen, größeren Modellen und ausreichend Feinabstimmungsbeispielen trainiert werden. Der Leistungsabfall wird hauptsächlich durch zeitliche Drift und nicht durch Anpassungsüberanpassung verursacht.</sample>
    <sample id="8">Die neue Methode, ABC-Eval, unterscheidet sich dadurch, dass sie das Verhalten der Chatmodelle explizit annotiert, anstatt nur allgemeine Bewertungen vorzunehmen. Sie reduziert die Subjektivität durch die Messung spezifischer thematischer Fehler, wie z. B. Widersprüche, Halluzinationen oder fehlende Empathie.</sample>
    <sample id="9">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt von der Verwendung von sauber annotierten Validierungsdaten ab.</sample>
    <sample id="10">Das Ergebnis kann durch Zugang zu umfassenderen Hintergrundinformationen und durch Verbesserung der Fähigkeit der Modelle, indirekte Referenzen zu verstehen, verbessert werden.</sample>
    <sample id="11">Jack Hessel, ein Forscher am AI2, präsentiert die Arbeit „Do Androids Laugh at Electric Sheep? Humor „Understanding“ Benchmarks from The New Yorker Caption Contest“. Zusammen mit Kollegen von verschiedenen Universitäten und OpenAI untersucht das Team, ob große Sprachmodelle Humor wirklich verstehen. Obwohl Modelle wie PaLM und ChatGPT in der Lage sind, Witze zu generieren und zu erklären, zeigen Tests, dass ihr Verständnis begrenzt ist. Beispielsweise erzeugt ChatGPT absurde, nicht vollständig sinnvolle Witze, bei denen der sogenannte „Witz“ nicht klar erkennbar ist. Um das Humorverständnis von Sprachmodellen systematisch zu testen, nutzen die Forscher Daten aus dem The New Yorker Caption Contest. Dabei werden drei Aufgaben definiert: Matching (Wahl der passenden Bildunterschrift), Qualitätssortierung und Erklärungsgeneration. Die Ergebnisse zeigen, dass selbst das beste Modell (CLIP) nur 62 % auf dem Matching-Test erreicht, während Menschen 94 % erreichen. Auch GPT-4, unterstützt durch menschliche Bildbeschreibungen, bleibt deutlich hinter menschlichen Bewertungen zurück. Die Forscher veröffentlichen ihre Daten und ein Leaderboard, um weitere Forschung zu fördern.</sample>
    <sample id="12">Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow – also insgesamt 5 Autoren.</sample>
    <sample id="13">Daniel Rotem presents his work, "Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings," conducted in Professor Roy Schwartz's lab at the Hebrew University. Adaptive inference reduces inference costs of large language models by using lower-capacity models for simpler tasks. Two common methods are Multi Model and Early Exit. Multi Model uses separate classifiers trained on different models, but suffers from overhead and storage costs. Early Exit uses classifiers after intermediate transformer layers, leading to faster inference but potential performance issues due to conflicting gradients—where different classifiers' updates interfere. To address this, Rotem introduces SWEET (Separating Weights in Early Exit Transformers), a fine-tuning method that isolates weight updates to prevent gradient conflicts. Results show SWEET significantly closes the performance gap between Early Exit and Multi Model, especially in fast inference scenarios. It outperforms both methods across the speed-accuracy trade-off, particularly for BERT-Large. The study highlights the existence of conflicting gradients in Early Exit training, presents the first fair comparison between methods, and introduces SWEET as a promising approach for future research in adaptive inference.</sample>
    <sample id="14">Hallo, mein Name ist Adam Przepiórkowski und dieser Vortrag handelt von der Abhängigkeitsstruktur von Koordinationen. Wie Sie vielleicht wissen, gibt es verschiedene Abhängigkeitsstrukturen, die von unterschiedlichen Theorien und Korpusansätzen angenommen werden. So zum Beispiel nimmt das Universal Dependencies-Modell die Struktur der Koordination wie „Lisa, Bart und Maggie“ an, wobei der erste Konjunkt der Kopf der gesamten koordinierten Struktur ist. In diesem Fall also Lisa. Ein ähnlicher Ansatz wird in Igor Mel'čuks Theorie der Bedeutungstexte angenommen, wo auch die gesamte koordinierte Struktur vom ersten Konjunkt bestimmt wird. Diese beiden Ansätze sind asymmetrisch. Sie heben einen der Konjunkte hervor. Es gibt jedoch auch asymmetrische Ansätze zur Koordination, wie der prager Ansatz. Der Konjunktionskopf-Ansatz, der in prager Abhängigkeits-Korpusbanken verwendet wird, wobei koordinierte Strukturen vom Konjunktionswort geführt werden. So erhalten wir einige Abhängigkeiten vom Konjunktionswort zu allen Konjunkten. Schließlich gibt es auch einen mehrfach kopfgeführten Ansatz, der beispielsweise in Hudsons Word Grammar verwendet wird, wo gesagt wird, dass alle Konjunkte Kopfe der koordinierten Struktur sind. So erhalten wir Abhängigkeiten vom Regierenden zum jeweiligen Konjunkt: Lisa, Bart und Maggie. Das Ziel dieses Papers ist es, einen neuen Argument für symmetrische Strukturen der Koordination, wie diese beiden, und gegen asymmetrische Strukturen der Koordination, wie diese beiden, zu liefern. Das Argument basiert auf dem Prinzip der Minimierung der Abhängigkeitslänge, das ich Ihnen anhand dieser Beispiele erklären werde. In der englischen Sprache wissen Sie vielleicht, dass direkte Objekte tendenziell nahe am Verb stehen, während Adverbien weiter weg sein können. Also ist „Marge read it yesterday“ in Ordnung, weil das direkte Objekt nah am Verb steht, während „Marge read yesterday it“ viel schlechter klingt. Richtig? Hier liegt zwischen dem Verb und dem direkten Objekt ein Adverb: „yesterday“. Dieser Effekt kann jedoch gemildert werden, wenn das direkte Objekt sehr lang und schwer ist, und dann kann es an die Stelle nach dem Adverb gerückt werden. Dies wird hier veranschaulicht. Also sind beide Sätze in Ordnung. „Marge read this absolutely fascinating book about bees yesterday.“ Es ist okay, dass anstelle von „it“ ein langes NP steht. Es ist auch okay zu sagen: „Marge read yesterday this absolutely fascinating book about bees.“ Die Begründung hier ist, dass dies möglich ist, obwohl die Sätze das allgemeine grammatikalische Prinzip verletzen, dass direkte Objekte unmittelbar neben dem Verb stehen sollten, sie das Prinzip der Minimierung der Abhängigkeitslänge erfüllen, das besagt, dass kürzere Abhängigkeiten bevorzugt werden. Diese beiden Bäume zeigen nur die Länge der entscheidenden Abhängigkeiten, jene, die nicht konstant zwischen diesen beiden Strukturen sind. Hier haben wir eine Abhängigkeit von „read“ zum Adjunkt mit einer Länge von 7, gemessen in Wörtern, und von „read“ zu „book“ mit einer Länge von 4, insgesamt also 11. Wenn man diese beiden Konstituenten austauscht, wird die Summe dieser beiden Abhängigkeiten 6. Also anstatt 11, 6 ist viel kürzer. Deshalb klingt das recht okay. Richtig? Es verletzt ein Prinzip, erfüllt aber ein anderes. Also haben wir verschiedene Statistiken zur Koordination aus der erweiterten Version des Penn Treebanks extrahiert und im Paper „Why wouldn't you use universal dependencies“ gezeigt, und diese Statistiken bestätigen die bisher häufig gemachten Beobachtungen, dass linke Konjunkte tendenziell kürzer sind. Also „Salz und Pfeffer“ und nicht „Pfeffer und Salz“, gemessen in Silben. Außerdem wurde in der Syntaxanalyse beobachtet, dass diese Tendenz mit zunehmender Längendifferenz stärker wird. Also wenn sich die Längendifferenz der beiden Konjunkte vergrößert, bevorzugt der kürzere Konjunkt die Position links. Also ist der Anteil des linken kürzeren Konjunkts höher. Was neu in diesem Paper ist, ist, dass wir festgestellt haben, dass diese Tendenz nur auftritt, wenn der Regierende links steht oder fehlt. Also ist der Regierende links in diesem Beispiel „I saw Bart and Lisa“ – also der Regierende ist links. Er fehlt im zweiten Beispiel „Homer came and sneezed.“ Hier haben wir eine Koordination zweier Verben und es gibt keinen äußeren Regierenden. In solchen Fällen bevorzugt der linke Konjunkt kürzere Länge; der größte Unterschied zwischen den beiden Konjunkten ist auf der linken Seite. Allerdings verschwindet dieser Effekt, wenn der Regierende rechts steht, wie hier: „laughed“ regiert die Koordination „Ted and Ned“. Dies zeigen wir in dem Paper durch Messung der Länge in Zeichen (linke Spalte), Silben (mittlere Spalte) und Wörtern (rechte Spalte). Ich konzentriere mich hier auf die rechte Spalte. Was wir hier sehen, ist, dass wenn der Regierende links steht, die Tendenz, dass der linke Konjunkt kürzer ist, mit zunehmender Längendifferenz in Wörtern stetig wächst, und das gleiche gilt, wenn kein Regierender vorhanden ist, wie bei der Koordination von Sätzen. Wenn der Regierende rechts steht, verschwindet diese Tendenz jedoch. Und wir zeigen im Paper, wie dies ein Argument gegen asymmetrische Strukturen der Koordination, wie diese beiden, und für symmetrische Strukturen, wie diese beiden, liefert. Siehe das Paper für die vollständigen Argumente. Und sprechen Sie mit uns während der Poster-Session. Danke.</sample>
    <sample id="15">An der Arbeit sind drei Autoren beteiligt.</sample>
    <sample id="16">Die Bibeltexte werden stärker vereinfacht als beispielsweise Nachrichtentexte oder Texte für Sprachlerner.</sample>
    <sample id="17">**Abstract:**  
This work addresses challenges in Multimodal Relation Extraction (MRE) by proposing a novel framework that simultaneously prunes internal information and enriches external context. Existing methods often suffer from over-reliance on internal modalities and underutilization of external information. To overcome these issues, we introduce a Graph Information Bottleneck (GIB)-guided feature refinement to filter and compress multimodal features, ensuring only relevant information is retained. Additionally, we incorporate multimodal topic features to supplement the context, enhancing the model's understanding of relationships. Our framework integrates textual and visual scene graphs into a unified cross-modal graph (CMG), which is then refined through fine-grained pruning. Experiments on the MRE benchmark show that our method outperforms existing multimodal baselines. Ablation studies confirm the effectiveness of both information screening and external topic integration. Furthermore, analysis reveals that internal screening is more critical for high-relevance inputs, while external information is more beneficial for low-relevance cases. This approach achieves significant improvements in MRE performance, demonstrating the value of balancing information subtraction and addition.</sample>
    <sample id="18">Ein Beispiel für die Präferenz kürzerer linker Konjunktionen ist "salt and pepper" anstelle von "pepper and salt".</sample>
    <sample id="19">Zhang Qin, eine Masterstudentin der Shenzhen University, präsentierte eine Übersicht über effiziente Systeme für Open-Domain-Question-Answering, die von ACL 2023 angenommen wurde. Das Standardmodell besteht aus zwei Stufen: einer Retrieval- und einer Reader-Komponente. Dabei wird die Retrieval-Phase durch Encoders für Frage und Dokument durchgeführt, während die Reader-Phase den Kontext verarbeitet und die Antwort generiert. Herausforderungen sind die Größe der Wikipedia-Korpusdateien, die langsame Indexsuche und die hohen Ressourcenanforderungen großer Sprachmodelle. Ziel der Arbeit ist es, effizientere Systeme mit geringerem Speicherbedarf, schnellerer Inferenz und vergleichbarer Leistung zu entwickeln. Dazu werden verschiedene Techniken untersucht, wie Approximate Nearest Neighbor-Suche, Skip-Reading, Index-Kompression und Modellverkleinerung. Die Ergebnisse zeigen, dass Retrieval-and-Reader-Systeme ein gutes Gleichgewicht zwischen Geschwindigkeit, Speicher und Leistung bieten. Retrieval-only-Systeme sind schnell, benötigen jedoch große Indizes, während Generator-only-Systeme keine Indizes benötigen, aber große Modelle und geringere Leistung haben. Zukünftige Forschungsschwerpunkte sind die Anwendung auf Low-Power-Geräte und die Entwicklung weiterer Evaluationsmetriken.</sample>
    <sample id="20">Ja, Sie können die Modelle für Ihre Forschung verwenden. Sie sind auf Hugging Face frei verfügbar und unter der MIT-Lizenz lizenziert.</sample>
    <sample id="21">DEPLAIN-apa enthält Nachrichtentexte.</sample>
    <sample id="22">Zu einer guten Generalisierung führen folgende Faktoren: ein besserer Modellarchitektur, eine größere Modellgröße und eine höhere Anzahl an Feinabstimmungsbeispielen.</sample>
    <sample id="23">In ihrer Arbeit untersuchen Dan Garrette und sein Team, wie Text-basierte Bildgenerationsmodelle wie Imagen Text in Bildern besser darstellen können. Obwohl solche Modelle bei der Erzeugung von hochwertigen Bildern gute Ergebnisse liefern, haben sie Schwierigkeiten, Text korrekt darzustellen. Das liegt daran, dass der Textencoder (T5-XXL) Subword-Token verwendet, was bedeutet, dass er nicht direkt auf die Einzelbuchstaben zugreifen kann. Dies führt zu Fehlern bei der Rekonstruktion von Wörtern, besonders bei häufigen Wörtern, die als Einzel-Token kodiert werden. Im Gegensatz dazu ist ByT5 in der Lage, Byte-basiert zu arbeiten und somit die Schreibweise direkt zu erfassen. In ihrem Experiment haben sie ByT5-small mit dem T5-Encoder kombiniert, was den Textencoder in der Lage stellt, Text besser zu rendern. Obwohl die Verbesserung nicht perfekt ist – da der Diffusionsteil weiterhin Fehler einführen kann – zeigt sich, dass dieser Ansatz die Textdarstellung deutlich verbessert. Die Arbeit präsentiert außerdem zwei neue Benchmarks (WikiSpell und DrawText) und einen effizienten Ansatz, um die Schreibfähigkeit von Modellen zu erhöhen: die Kombination eines charakterbewussten Encoders.</sample>
    <sample id="24">Die Tendenz zu kürzeren linken Konjunktionen wurde anhand der Länge der Konjunkte in Zeichen, Silben und Wörtern gemessen.</sample>
    <sample id="25">Die Experimente wurden durch die Analyse von Daten aus der erweiterten Version des Penn Treebanks durchgeführt, wobei statistische Beobachtungen zur Länge der Konjunkte und ihrer Position in Abhängigkeit von der Position des Regierenden (Governor) untersucht wurden. Dabei wurde festgestellt, dass der linke Konjunkt tendenziell kürzer ist, wenn der Regierende links steht oder fehlt, während diese Tendenz bei einem Regierenden auf der rechten Seite verschwindet.</sample>
    <sample id="26">Ein Basisklassifikator, der mit unausgewogenen Daten trainiert wird, wie in der Arbeit gezeigt, leistet in der Regel nicht viel besser als Zufall, da die seltenen Klassen wie Dissonanz kaum erkannt werden können.</sample>
    <sample id="27">Der Text gibt keine Angabe darüber, wie viele Autoren an der Arbeit beteiligt sind.</sample>
    <sample id="28">Die Personen im Beispielgespräch heißen Bob und Alice.</sample>
    <sample id="29">Kontextsensitive MÜ-Modelle schneiden besser ab bei Diskursphänomenen wie Formalität und lexikalischer Kohäsion.</sample>
    <sample id="30">Das Paper "LLM-Blender" stellt ein einfaches, aber effektives Ensemble-Lern-Framework für große Sprachmodelle vor. Die Forscher*innen von AI2 und USC zeigen, dass die beste Wahl unter verschiedenen Modellen je nach Eingabe variieren kann. Obwohl Modelle wie Vicuna im Durchschnitt beste Leistungen erzielen, ist es nur in 21 % der Fälle das beste Modell. Daher schlagen sie LLM-Blender vor, ein zweistufiges Framework, das n Modelle parallel ausführt und ihre Ergebnisse vergleicht. Im ersten Schritt nutzt das PairRanker-Modul paarweise Vergleiche, um die Qualität der Ausgaben zu bewerten. Dabei wird das Eingabeprompt mit jedem Paar von Modellausgaben kombiniert, um mithilfe eines Cross-Attention-Modells wie RoBERTa zu bestimmen, welche Ausgabe besser ist. Im zweiten Schritt werden die besten K Ausgaben an ein generatives Modell übergeben, das eine finale Ausgabe erzeugt. Die Ergebnisse zeigen, dass LLM-Blender die Leistung einzelner Modelle deutlich verbessert. Zudem wurde eine neue Datensammlung, MixInstruct, erstellt, um Ensemble-Methoden zu evaluieren. Die Tests bestätigen, dass LLM-Blender in der Mehrheit der Fälle bessere Ergebnisse liefert als einzelne Modelle.</sample>
    <sample id="31">Die Autoren gehören der University of California, San Diego an.</sample>
    <sample id="33">Das vorgestellte Framework quantifiziert die Positionalität, indem es Annotationen von diversen Annotatoren mit Vorhersagen von Modellen und Datensätzen vergleicht und dabei Pearson's R-Korrelationswerte berechnet.</sample>
    <sample id="34">Marcos Treviso stellt das Framework CREST (Counterfactual Rationalization and Explanation Synthesis) vor, das rationale Erklärungen und Gegenfaktuale in Texten kombiniert. CREST besteht aus zwei Komponenten: einer, die rationale Erklärungen erzeugt, und einer, die Gegenfaktuale durch gezielte Änderungen des Eingabetexts generiert. Die Gegenfaktuale werden durch eine Maske im Text erzeugt, gefolgt von einer Ersetzung durch ein Masking-Modell. In Tests mit menschlicher Evaluation zeigte sich, dass CREST-Gegenfaktuale natürlicher und gültiger sind als andere Methoden. Zudem wird CREST genutzt, um rationale Erklärungen zu verbessern, indem sowohl fakturelle als auch Gegenfaktuale in das Training einfließen. Ein neuer Regularisierungsterm sorgt dafür, dass rationale Erklärungen konsistent bleiben. Die Ergebnisse zeigen, dass CREST-basierte Modelle auf Datensätzen wie IMDB bessere Leistungen erzielen, insbesondere bei out-of-domain-Daten. CREST-Erklärungen sind zudem plausibler und haben eine höhere Gegenfaktualsimulierbarkeit, was bedeutet, dass sie den Klassifikator effektiv beeinflussen können. CREST ermöglicht somit eine kontrollierte Erzeugung von Gegenfaktuale, die sowohl für die Erklärung als auch für das Training von Modellen genutzt werden können.</sample>
    <sample id="36">In diesem Vortrag stellen Telmo Pessoa Pires und seine Kollegen eine neue Methode zur Verbesserung von multilingualen Übersetzungssystemen vor: Language-Specific Layers (LSLs). Ziel ist es, die Kapazität pro Sprache zu erhöhen, ohne die Inferenzkosten zu steigern. Dabei werden pro Sprache spezifische Schichten im Encoder eingesetzt, die bei der Übersetzung nur für die entsprechende Sprache aktiv sind. Die Platzierung dieser Schichten wird nicht manuell bestimmt, sondern durch das Modell selbst gelernt, basierend auf den Gewichtungen der Schichten. Die Ergebnisse zeigen, dass diese lernbasierte Architektur deutliche Verbesserungen bei der Übersetzung leistet, insbesondere bei Sprachen mit geringen Ressourcen. Die Evaluierung auf Datensätzen wie Flores-101 ergab signifikante Verbesserungen gegenüber Baseline-Modellen und anderen Ansätzen wie Language Adapters. Die Methode ist zudem effizienter in der Inferenzphase. Die Arbeit unterstreicht die Vorteile von multilingualen Modellen in Bezug auf Skalierbarkeit und Fehlervermeidung, während sie gleichzeitig die Leistung für weniger ressourcenvolle Sprachen steigert. Weitere Ergebnisse und Details sind in der vollständigen Arbeit und am Poster zu finden.</sample>
    <sample id="37">Das Ergebnis der vorherigen Studie zeigte, dass auch menschliche Teilnehmende stereotype Vorstellungen an den Tag legten, wenn sie die gleichen Persona-Prompts erhielten. Dies ermöglichte eine direkte Vergleichbarkeit zwischen den von Menschen geschriebenen und den von LLM generierten Personas.</sample>
    <sample id="38">In dieser Studie wurden Daten aus der erweiterten Version des Penn Treebanks verwendet.</sample>
    <sample id="39">Der Text nennt nur einen Autor: Adam Przepiórkowski.</sample>
    <sample id="40">Eng verwandte Aufgaben für kognitive Dissonanz sind die Stance-Klassifikation in Debatten (agreement/disagreement) und die Klassifikation von Expansion- und Vergleichsbeziehungen (CE) im PDTB.</sample>
    <sample id="41">Silin vom NLP-Labor der EPFL präsentiert PeaCoK, ein Persona-basiertes Commonsense-Knowledggraph, entwickelt in Zusammenarbeit mit Sony. Ziel ist es, kohärente und ansprechende Narrative wie Dialoge oder Geschichten zu generieren, indem Persona-Attribute und deren Beziehungen systematisch erfasst werden. PeaCoK umfasst 3.800 Personas mit 40.000 Attributen und 100.000 Fakten, wobei 9.200 Attribute mehrere Personas verbinden. Die Beziehungen werden in drei Dimensionen mit vier Haupttypen sowie Interaktivität und Unterschiedlichkeit strukturiert. Die Erstellung erfolgt in drei Schritten: Auswahl von Personas, Induktion von Attributen aus Commonsense-Graphen und Sprachmodellen, sowie Crowdsourcing mit AI-Unterstützung. Tests zeigen, dass PeaCoK-basierte Modelle wie Comet-BART bessere Ergebnisse in der Generierung von Persona-Attributen erzielen als Baselines wie GPT-3. In einer Persona-Dialog-Aufgabe verbessert PeaCoK die Fluenz, Konsistenz und Engagemen im Vergleich zu anderen Wissensgraphen. Insbesondere steigt die Qualität der Dialoge mit zunehmender Übereinstimmung der Personas. PeaCoK bietet somit eine zuverlässige Ressource zur Verbesserung von Narrative-Modellen.</sample>
    <sample id="42">Die Anzahl der Autoren wird im gegebenen Text nicht explizit erwähnt. Daher kann die Frage nicht knapp beantwortet werden.</sample>
    <sample id="43">Die Anzahl der Autoren wird im Text nicht erwähnt.</sample>
    <sample id="44">Das vorgestellte Framework NLPositionality unterscheidet sich von bisherigen Arbeiten dadurch, dass es Endnutzerannotationen mit den Vorhersagen von Modellen und Datensätzen vergleicht, anstatt sich nur auf die Übereinstimmung zwischen Annotatoren zu konzentrieren. Es untersucht somit direkt, wie die Positionalität von Modellen und Datensätzen mit der von realen Nutzern übereinstimmt.</sample>
    <sample id="45">Das Setup mit den generierten Personas hat die meisten Überschneidungen mit dem Lexikon der Stereotypen.</sample>
    <sample id="46">DeepL und Google Translate wurden verglichen.</sample>
    <sample id="47">Hallo, ich bin Shangbin, Doktorand an der University of Washington. Heute präsentiere ich unsere Arbeit: „Von der Prätrainingsdaten bis zu den Sprachmodellen bis zu den Downstream-Aufgaben: Nachverfolgen der Spuren politischer Voreingenommenheit, die zu unfairen NLP-Modellen führen“. Sprachmodelle werden mit umfangreichen Web-Crawl-Daten trainiert, und politische Nachrichtenmedien sind in ihren Prätrainingsdaten gut vertreten. Laut einer Untersuchung des C4-Korpus sind beispielsweise das New York Times, das Los Angeles Times, The Guardian und Huffington Post in den Trainingsdaten der Sprachmodelle gut vertreten. Dies hat sowohl Vorteile als auch Nachteile für die Anwendungen von Sprachmodellen geschaffen. Einerseits können sie aus verschiedenen Perspektiven lernen, was demokratische Vielfalt und die Vielzahl von Ideen fördert. Andererseits sind diese unterschiedlichen politischen Meinungen inherent gesellschaftlich voreingenommen und können zu Fairnessproblemen bei Downstream-Aufgaben führen.

Daher untersuchen wir den Verlauf der politischen Voreingenommenheit von der Prätrainingsdaten bis zu den Sprachmodellen und anschließend bis zu den Downstream-Aufgaben. Konkret stellen wir folgende Fragen: Erstens, wie bewerten wir die politische Ausrichtung von Sprachmodellen und welche Rolle spielt dabei die Prätrainingsdaten? Zweitens, wie performen Sprachmodelle mit unterschiedlichen politischen Ausrichtungen auf Downstream-Aufgaben und führt das zu Fairnessproblemen in NLP-Anwendungen?

Zunächst haben wir vorgeschlagen, Sprachmodelle mit unterschiedlichen Prompt-Formaten unter Verwendung politischer Fragebögen, wie z. B. dem Political Compass Test, zu testen. Dies ermöglicht uns eine automatische Bewertung, die auf der politischen Wissenschaft basiert. Vorläufige Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Ausrichtungen haben und alle vier Quadranten des politischen Kompasses besetzen. Wir können auch erkennen, dass GPT-4 das liberaleste Sprachmodell ist, und dass die GPT-Reihe insgesamt sozialliberaler ist als die BART-Reihe und ihre Varianten.

Zweitens untersuchen wir, in welchem Maße die politischen Voreingenommenheiten von Sprachmodellen tatsächlich aus den Trainingsdaten übernommen werden. Dazu führen wir ein kontrolliertes Experiment durch, indem wir Sprachmodelle weiter auf sechs verschiedene parteiisch geprägte Corpora trainieren, die in Nachrichten und sozialen Medien unterteilt sind und zusätzlich nach ihrer politischen Ausrichtung geteilt werden. Durch das Weitertrainieren der Sprachmodelle auf solchen parteiisch geprägten Corpora können wir beobachten, dass sich die ideologischen Koordinaten der Sprachmodelle entsprechend verschieben. Zum Beispiel zeigt ein RoBERTa-Modell, das auf einem linken Reddit-Korpus weiter trainiert wurde, eine deutliche liberale Verschiebung seiner politischen Ausrichtung.

Außerdem untersuchen wir, ob Sprachmodelle die Polarisation erkennen können, die in unserer heutigen Gesellschaft vorherrscht. Dazu teilen wir die Prätrainingskorpora in zwei Zeiträume ein: vor und nach dem 45. Präsidenten der Vereinigten Staaten. Wir trainieren Sprachmodelle separat auf diese beiden zeitlich unterschiedlichen Corpora. Wir können beobachten, dass Sprachmodelle nach 2017 tendenziell eine politische Ausrichtung haben, die weiter vom Zentrum entfernt ist. Dies zeigt, dass Sprachmodelle auch die Polarisation in unserer Gesellschaft erkennen können.

Schließlich evaluieren wir Sprachmodelle mit unterschiedlichen politischen Ausrichtungen auf Aufgaben wie Hassrede-Erkennung und Fakenews-Erkennung, die oft Sprachmodelle nutzen und bedeutende Auswirkungen haben können. Wenn wir die Leistung pro Kategorie analysieren, also die Leistung nach unterschiedlichen Demografiegruppen oder politischen Ausrichtungen der Nachrichtenmedien, erkennen wir Muster. Zum Beispiel sind linksgestimmte Sprachmodelle besser darin, Hassrede gegen soziale Minderheiten zu erkennen, aber schlechter bei der Erkennung von Hassrede gegen mächtigere Gruppen in der Gesellschaft. Umgekehrt sind rechtsgerichtete Sprachmodelle besser in der Erkennung von Hassrede gegen Weiße und Männer, aber schlechter bei der Erkennung von Hassrede gegen Afroamerikaner, LGBTQ+-Personen und andere Minderheiten. Ähnliche Trends treten auch bei der Erkennung von Fakenews auf, wobei wir feststellen, dass linksgestimmte Sprachmodelle besser in der Erkennung von Fehlinformationen von der politischen Gegenpartei sind und umgekehrt.

Wir zeigen auch zahlreiche qualitative Beispiele, um zu demonstrieren, dass Sprachmodelle mit unterschiedlichen politischen Ausrichtungen unterschiedliche Vorhersagen für Beispiele von Hassrede und Fehlinformationen aufgrund ihrer sozialen Kategorien liefern. Im Anhang finden sich weitere Beispiele, die verdeutlichen, dass es ein dringendes Fairnessproblem gibt, das durch die politischen Voreingenommenheiten der Sprachmodelle verursacht wird. Beispielsweise könnte ein rechtsgerichtetes Sprachmodell, das an Hassrede oder Fehlinformationen weiter trainiert und dann auf einer beliebten sozialen Plattform eingesetzt wird, Menschen mit gegensätzlichen politischen Auffassungen marginalisieren und Hassrede gegen Minderheiten unkontrolliert verbreiten.

Dies warnt uns, die Fairnessprobleme, die durch die politische Ausrichtung von Sprachmodellen entstehen, anzuerkennen und anzugehen. Ein wenig Diskussion: Wir möchten auch darauf hinweisen, dass wir ein einzigartiges Dilemma bezüglich der politischen Voreingenommenheit von Sprachmodellen aufdecken. Es ist wie zwischen Scylla und Charybdis. Wenn wir politische Meinungen in den Trainingsdaten nicht bereinigen, würde sich die Voreingenommenheit von den Trainingsdaten bis zu den Sprachmodellen und dann bis zu den Downstream-Aufgaben weitergeben und schließlich zu Fairnessproblemen führen. Wenn wir versuchen, sie irgendwie zu bereinigen, riskieren wir Zensur oder Ausschluss. Es ist unglaublich schwierig zu entscheiden, was tatsächlich neutral ist und welche Daten aus dem Monitoring-Set beibehalten werden sollten. Es ist also so etwas wie das elektrische Trolley-Problem.

Das war so ziemlich alles, was ich heute zu sagen habe. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="48">Die Anzahl der Autoren wird nicht genannt, aber es wird erwähnt, dass es eine gemeinsame Arbeit mit Kollegen von Google Translate ist. Die genaue Anzahl der Autoren ist nicht spezifiziert.</sample>
    <sample id="49">Die MPP-Auswertungen wurden bis zu einer Kontextlänge von 1024 Token durchgeführt.</sample>
    <sample id="50">DEPLAIN ist ein neues Korpus für die Textvereinfachung auf deutscher Sprache, das auf Dokument- und Satzebene entwickelt wurde. Regina Stodden erläutert, dass Textvereinfachung den Verständnisschwierigkeiten von Lesern mit Problemen oder Nicht-Muttersprachlern entgegenwirkt. Dazu sind parallele Textpaare notwendig, die DEPLAIN in zwei Teilkorpora unterteilt: DEPLAIN-apa (basierend auf Nachrichten) und DEPLAIN-web (mit verschiedenen Domänen). DEPLAIN-apa enthält 13.000, DEPLAIN-web 30.450 manuell und automatisch ausgerichtete Satzpaare. Die Analyse zeigt, dass Texte aus der Bibel stärker vereinfacht werden als Nachrichten oder Lernertexte. Omar beschreibt zwei Anwendungsfälle: Zum einen die Evaluierung automatischer Ausrichtungsmethoden, wobei MASSalign als bestes Verfahren identifiziert wurde. Zum anderen die Feinabstimmung von Sprachmodellen (long-mBART und mBART) zur automatischen Textvereinfachung. Die Ergebnisse stellen einen grundlegenden Benchmark für zukünftige Forschungen in diesem Bereich dar. DEPLAIN bietet somit eine wertvolle Ressource für die Entwicklung und Evaluierung von Textvereinfachungsmodellen.</sample>
    <sample id="51">Die Domänen, die in den Datensatz aufgenommen wurden, sind Musik, Bücher und Rezepte.</sample>
    <sample id="52">Positionalität lässt sich allgemein als die Perspektiven definieren, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen besitzen.</sample>
    <sample id="53">Der Referent heißt Dawei.</sample>
    <sample id="54">**Abstract:**  
This paper presents a study on cognitive dissonance detection in language, addressing the challenge of its rarity in discourse. Cognitive dissonance occurs when beliefs or actions are inconsistent, yet such instances are rarely expressed in text. We create a large-scale annotated dataset of dissonance relations, finding dissonance in only 3.5% of annotated pairs. Due to the scarcity of dissonance examples, we employ transfer learning and active learning to improve detection. We transfer from related tasks—stance classification in debates and PDTB discourse relations—and find that sequential fine-tuning on these tasks improves performance. Using active learning with a Probability-of-Rare-Class (PRC) strategy, we significantly boost dissonance detection AUC from 0.62 to 0.75. PRC outperforms other strategies, though annotation remains challenging. Our results highlight the effectiveness of transfer learning for cold-starting active learning and the utility of cumulative updates for domain adaptation. This work provides a foundation for understanding dissonance in language and its implications for mental health, polarization, and decision-making.</sample>
    <sample id="55">Ja, EDAtt passt zu einem bestehenden Offline-ST-Modell.</sample>
    <sample id="56">Die Anzahl der Autoren wird im Text nicht erwähnt.</sample>
    <sample id="57">Nein, das getestete Modell funktioniert nicht vollständig in der Testsuite.</sample>
    <sample id="58">Die drei Varianten von KITMUS sind: **Background-Pretrain**, **Background-Both** und **Background-Inference**.</sample>
    <sample id="59">In our work, we introduce DrBERT, the first pre-trained French biomedical model based on RoBERTa, trained on the NACHOS dataset of crawled medical data. We compare it with models like ChuBERT, trained on clinical data from Nantes University Hospital, and explore the impact of data size and source on model performance. We evaluate seven models, including from-scratch and continual pre-training approaches, against six baselines such as CamemBERT and PubMedBERT. Results show that models trained on domain-specific data perform best on related tasks, but heterogeneous data sources offer better versatility. From-scratch pre-training generally outperforms continual approaches, though using CamemBERT weights on NACHOS data yields comparable results. DrBERT achieves better performance on nine out of eleven downstream tasks, surpassing generic models like CamemBERT. All models are available on Hugging Face under MIT license, with training scripts on GitHub. Our findings highlight the importance of data quality and quantity in building effective French biomedical NLP models.</sample>
    <sample id="60">Die Autoren gehören der University of Wisconsin-Madison an.</sample>
    <sample id="61">The concluding research question is: Should we only use clean samples for validation, or are there better ways to utilize them?</sample>
    <sample id="62">Nitay Calderon präsentiert eine systematische Studie zur Wissensvermittlung (Knowledge Distillation) für natürliche Sprachgenerierung (NLG). Ziel ist es, große und komplexe Sprachmodelle zu komprimieren, ohne deren Leistung zu stark zu beeinträchtigen. Im Gegensatz zu vielen bisherigen Arbeiten, die sich auf Klassifikations- oder Verständnistasks konzentrieren, untersucht das Paper NLG-Aufgaben in realistischen, industriellen Szenarien mit begrenzten annotierten Daten und großen Mengen an unlabelierten Daten. Die Studie umfasst vier NLG-Aufgaben: Zusammenfassung, Fragegenerierung, gemeiner Sinn-Reasoning und Stilübertragung. Traditionelle Ansätze wie Wort- und Sequenzlevel-Distillation werden untersucht und erweitert. Besonders hervorgehoben wird die Verwendung von mehreren Pseudo-Targets statt nur einem, sowie das Sampling mit hoher Temperatur, um die Vielfalt der Trainingsdaten zu erhöhen. Ein neuer Ansatz, die sogenannte „joint-teaching“, kombiniert Wissensvermittlung aus pseudo-targets sowohl des Lehrers als auch des Schülers, um das Exposition-Bias zu reduzieren und den Schülern zu helfen, ihre eigenen Fehler zu korrigieren. Die Studie liefert einen umfassenden Leitfaden für effektive Distillation in NLG.</sample>
    <sample id="63">Die Sensitivitätsmetrik misst, wie konsistent ein Modell auf dieselbe Aufgabe reagiert, unabhängig von leicht unterschiedlichen Formulierungen der Anweisung.</sample>
    <sample id="64">Der Referent heißt Jingwei Yi.</sample>
    <sample id="65">Eine höhere Sensitivität bedeutet das Gegenteil einer besseren Leistung des Modells. Eine geringere Sensitivität zeigt, dass das Modell konsistenter auf unterschiedliche Formulierungen der Anweisung reagiert und somit robuster ist.</sample>
    <sample id="66">**Abstract**  
This paper provides a comprehensive survey of deep learning methods for mathematical reasoning, a critical component of human intelligence. We explore various tasks, including solving math word problems, geometric reasoning, and automated theorem proving, which involve both textual and multimodal data. Recent advances include sequence-to-sequence models, sequence-to-tree architectures, and the application of large language models (LLMs) with prompting strategies such as chain-of-thought and self-consistency. While LLMs show promise, they face challenges in precise mathematical reasoning and generalization. To address these, approaches like program-aided LLMs and tools like Chameleon have been proposed. Despite progress, mathematical reasoning in low-resource languages and specialized domains remains underexplored. The survey highlights current methodologies, challenges, and future directions in leveraging deep learning for mathematical reasoning.</sample>
    <sample id="67">In diesem Beitrag untersucht Uri, wann und warum bei multilingualen Übersetzungssystemen Interferenz auftritt und wie sie reduziert werden kann. Er zeigt, dass Interferenz stärker bei kleinen Modellen auftritt, insbesondere wenn die Datenmenge groß ist. Durch die Anpassung der Sampling-Temperatur kann die Leistung verbessert werden – eine höhere Temperatur führt zu einer besseren Darstellung von Sprachen mit geringen Ressourcen. Obwohl Faktoren wie Sprachähnlichkeit und die Anzahl der Sprachen in Betracht gezogen werden, haben sie nur geringen Einfluss auf die Interferenz. Die Ergebnisse zeigen, dass bei moderater Modellskalierung und fein abgestimmter Temperatur die Interferenz erheblich reduziert werden kann, ohne komplexe Algorithmen zu benötigen. Die Schlussfolgerung ist, dass die Größe des Modells und die Datenmenge die Hauptfaktoren für Interferenz sind, während andere Aspekte weniger relevant sind. Die Arbeit unterstreicht die Bedeutung von Temperaturanpassung und Skalierung, um die Qualität von multilingualen Übersetzungssystemen zu verbessern.</sample>
    <sample id="68">Die Modelle erhalten während des Pre-Trainings einen breiten linguistischen Kontext, der syntaktische und semantische Merkmale aus verschiedenen Textquellen umfasst.</sample>
    <sample id="69">Normalerweise werden etwa 20 saubere Validierungsbeispiele pro Klasse benötigt, um eine gute Leistung in der Weakly Supervised Learning (WSL) zu erzielen.</sample>
    <sample id="70">Die Autoren gehören der Stanford University an.</sample>
    <sample id="71">Javad Hosseini und seine Kollegen präsentieren das AltEntities Corpus, ein neues Datenset zur Erfassung indirekter Referenzen bei der Auswahl von Entitäten in drei Bereichen: Musik, Bücher und Rezepte. Ziel ist es, zu verstehen, wie Nutzer in natürlichen Dialogen zwischen ähnlichen Entitäten wählen, ohne deren Namen direkt zu nennen. Dazu wurde ein Cartoonszenario verwendet, in dem ein Nutzer zwischen zwei Optionen wählt, z. B. „der neuere“ oder „der nicht energiegeladene“. Die Daten wurden durch Crowdsourcing erstellt, wobei Annotatoren Hintergrundinformationen zu den Entitäten bekamen, um indirekte Referenzen zu erstellen. Das Datenset umfasst 6.000 Alternative-Fragen und 42.000 indirekte Bezeichnungen. Die Ergebnisse zeigen, dass Sprachmodelle mit Zugang zu vollständigem Hintergrundwissen eine Genauigkeit von bis zu 95 % erreichen können, bei eingeschränktem Wissen liegt sie bei 82–87 %, und bei nur Namen bei etwa 60 %. Das Datenset ist ein wertvoller Ressource für die Evaluierung von LLMs in der Entitätenauswahl.</sample>
    <sample id="72">Es ist notwendig, neue Methoden zur Messung von Medienverzerrungen zu entwickeln, weil bestehende Ansätze nicht in der Lage sind, die politischen Verzerrungen in Sprachmodellen und deren Auswirkungen auf NLP-Anwendungen adäquat zu erfassen und zu bewerten.</sample>
    <sample id="73">Akshatha</sample>
    <sample id="74">The paper introduces Dense-ATOMIC, an enhanced version of the ATOMIC commonsense knowledge base, addressing its limited multi-hop paths and incomplete links. By adding B-to-B, A-to-B, and A-to-A relations, Dense-ATOMIC significantly improves knowledge coverage and enables more complex reasoning. The construction involves normalizing tail events, training the Rel-CSKGC model, and applying intra- and inter-cluster completion strategies. Rel-CSKGC uses RoBERTa to encode events and predicts relations without relying on graph structure, leveraging semantic information effectively. Evaluation shows that Rel-CSKGC outperforms existing methods in both automatic and human assessments. Dense-ATOMIC also enhances COMET's performance, allowing for more diverse and contextually rich outputs. The paper demonstrates that Dense-ATOMIC supports more multi-hop paths, improving commonsense reasoning capabilities. Overall, this work advances the field of commonsense knowledge graphs by providing a denser, more comprehensive resource for AI systems.</sample>
    <sample id="75">Zheng Yandan presents Jointprop, a joint semi-supervised learning framework for Named Entity Recognition (NER) and Relation Extraction (RE). While supervised models require extensive labeled data, semi-supervised approaches aim to reduce this cost. However, existing methods often overlook the interdependencies between NER and RE tasks. Jointprop addresses this by leveraging heterogeneous graphs to propagate labels across both labeled and unlabeled data, considering intra- and inter-task relationships. The framework consists of four components: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. It uses a k-Nearest Neighbor graph to model similarities between data points and propagates pseudo-labels iteratively until convergence. Low-confidence pseudo-labels are filtered out before retraining the model. Experiments on four datasets show that Jointprop significantly improves performance over baseline models in both single- and joint-task settings, demonstrating the benefits of joint learning through task interdependencies.</sample>
    <sample id="76">Die Pipeline für die Verbreitung politischer Vorurteile beginnt mit der Pretraining-Daten, die politisch beeinflusste Quellen enthalten. Diese Vorurteile werden in die Sprachmodelle übertragen und beeinflussen anschließend die Leistung auf downstream Tasks, was zu Fairness-Problemen führen kann.</sample>
    <sample id="77">This work introduces DeFacto, a new dataset for improving factual consistency in abstractive text summarization, developed jointly by Yale University and Microsoft Research. The dataset includes human demonstrations and feedback, such as labels, corrected summaries, and explanations, based on summaries generated by the Pegasus model on the XSum dataset. Around 2.5K data points were collected, with 70% containing factual errors. Human-edited summaries show higher factuality scores but lower textual overlap with original references, likely due to existing errors in the XSum dataset. The study proposes three NLG tasks: summary editing, feedback generation, and automatic factual error correction. While models perform well on editing with human feedback, feedback generation remains challenging. Editor models trained on less data achieve comparable results, and generating explanations improves performance. The dataset supports training factuality metrics and meta-evaluation, and is available on GitHub.</sample>
    <sample id="78">Ja, der Vereinfachungsprozess unterscheidet sich: DEPLAIN-apa enthält mehr Umordnungen und Wortergänzungen, während DEPLAIN-web häufiger Umschreibungen aufweist.</sample>
    <sample id="79">Ja, CoScript ist öffentlich verfügbar.</sample>
    <sample id="80">Das Wasserzeichen wird in den Text eingebettet, indem bei der Verarbeitung eines Satzes die Anzahl der Triggerwörter gezählt wird. Die bereitgestellte Embedding ist eine Gewichtung der Ziel-Embedding und der ursprünglichen Embedding, wobei das Gewicht der Ziel-Embedding proportional zur Anzahl der Triggerwörter im Satz ist. Wenn die Anzahl der Triggerwörter einen Schwellenwert m überschreitet, ist das bereitgestellte Embedding genau gleich dem Ziel-Embedding.</sample>
    <sample id="81">The authors are affiliated with Penn State University.</sample>
    <sample id="82">In diesem Video stellen die Autoren ihre Arbeit „Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring“ vor. Ziel ist es, eine unsupervised Methode zur automatischen Bewertung von Essays (AES) zu entwickeln, ohne auf etikettierte Daten zurückzugreifen. Traditionelle AES-Modelle benötigen große Mengen an annotierten Essays, was aufwendig und zeitintensiv ist. Die Forscher präsentieren ULRA (Unsupervised Learning from Rank Aggregation), ein Framework, das mehrere heuristische Qualitätssignale als Pseudobewertungen verwendet, um ein neuronales AES-Modell zu trainieren. ULRA besteht aus zwei Hauptmodulen: HER (Heuristic Essay Ranking), das aus heuristischen Qualitätssignalen partielle Ordnungspaare generiert, und DPRA (Deep Pairwise Rank Aggregation), das diese Paare in ein einheitliches Trainingsignal aggregiert. Dabei wird eine tiefe, paarweise Rangaggregationsverlustfunktion verwendet, um die Wichtigkeit der einzelnen Signale zu gewichten. Zudem wird eine Skalierungsstrategie eingeführt, um die Vorhersagen des Modells in die vorgegebene Bewertungsskala zu überführen. Die Experimente zeigen, dass ULRA alle unsupervised Baseline-Methoden deutlich übertrifft und sich auch bei cross-prompt und one-shot Settings gut schlägt, wenn auch immer noch hinter stark überwachten Methoden zurückbleibt.</sample>
    <sample id="83">Ja, Encoder-Decoder-Modelle wie mT5 können durch Training mit einer Mischung verschiedener Sprachen verbessert werden. Die Ergebnisse zeigen, dass solche Modelle durch multilinguales Training in der Regel bessere Leistungen erzielen, obwohl die Leistung in englischen Datensätzen in einigen Fällen abnehmen kann ("Curse of Multilinguality").</sample>
    <sample id="84">Shwai He präsentierte das Paper „PAD-Net: An Efficient Framework for Dynamic Networks“ für ACL 2023. Dynamische Netzwerke können ihre Architektur oder Parameter je nach Eingabe anpassen, was in vielen Fällen besser als statische Netzwerke ist. Allerdings führen vollständig dynamische Netzwerke oft zu einem übermäßigen Parameterverbrauch, was ihre Anwendbarkeit begrenzt. Um dies zu lösen, schlägt PAD-Net eine teilweise dynamische Architektur vor, bei der Parameter in statische und dynamische unterteilt werden. Ziel ist es, redundante dynamische Parameter in statische umzuwandeln, um die Modellgröße und Rechenkosten zu reduzieren, ohne die Leistung zu beeinträchtigen. Die Partitionierung erfolgt iterativ, basierend auf dem Einfluss der Parameter auf den Verlust. Die Ergebnisse zeigen, dass PAD-Net besser abschneidet als statische und vollständig dynamische Netzwerke, mit weniger Parametern und Rechenaufwand. Ablationsstudien und Vergleiche mit Netzwerkpruning bestätigen die Vorteile. Zukünftige Arbeiten umfassen die Erweiterung auf andere Netzwerktypen, hardwarefreundliche Strukturen und zusätzliche Modus-Kombinationen.</sample>
    <sample id="85">Ein Beispiel für eingeschränkte Sprachplanung ist das Planen des Schritts "Ein Schokoladenkuchen backen", wobei spezifische Einschränkungen wie "ohne Gluten" oder "mit veganen Zutaten" berücksichtigt werden müssen.</sample>
    <sample id="86">Sie stellen die Opazität ihrer Methode sicher, indem sie die Einbettungen visuell auf vier Datensätzen mittels PCA darstellen und zeigen, dass die Backdoor-Einbettungen und normale Einbettungen schwer voneinander zu unterscheiden sind.</sample>
    <sample id="87">The work uses existing PLMs like CamemBERT and PubMedBERT as starting points for continual pre-training on French biomedical data, but also trains DrBERT from scratch on the NACHOS dataset to achieve better performance.</sample>
    <sample id="88">GPT-4 ist am wenigsten auf nicht-binäre Menschen ausgerichtet, verglichen mit Männern und Frauen.</sample>
    <sample id="89">The example sentence "I'm going to talk about..." shows how the model uses the knowledge learned through the attention mechanism.</sample>
    <sample id="90">In ihrer Arbeit „Rethinking Annotation: Can Language Learners Contribute?“ untersuchen Haneul Yoo und Kollegen, ob Sprachlerner als Datenannotatoren eingesetzt werden können, insbesondere in Sprachen mit begrenzten Ressourcen. Traditionell werden Muttersprachler für Annotationen herangezogen, doch diese sind oft schwer zu finden. Die Studie vergleicht die Annotationsergebnisse von Sprachlernern und Muttersprachlern für drei Sprachen (Englisch, Koreanisch, Indonesisch) und vier NLP-Aufgaben. Lerner wurden nach ihrem Kenntnisstand in drei Kategorien eingeteilt und mit zusätzlichen Ressourcen wie Wörterbüchern oder Übersetzungssystemen unterstützt. Die Ergebnisse zeigen, dass Lernerannotationen fast so genau sind wie die von Muttersprachlern, besonders bei einfachen Aufgaben. Durch Mehrheitsabstimmung können sie sogar vergleichbare Ergebnisse erzielen. Zudem verbessern sich die Sprachkenntnisse der Lerner während der Annotation. Die Arbeit zeigt, dass Sprachlerner eine wertvolle Alternative für die Datenannotation sind, insbesondere für unterrepräsentierte Sprachen. Sie eröffnet neue Möglichkeiten, NLP-Forschung für viele Sprachen zu erweitern, ohne auf Muttersprachler angewiesen zu sein.</sample>
    <sample id="91">Die Anzahl der Aufgaben hat einen positiven Einfluss auf die Leistung des Modells: Mit zunehmender Aufgabenzahl verbessert sich die Modellleistung und die Sensitivität nimmt ab.</sample>
    <sample id="92">The authors compare their method with three treeless baselines: the standard seq2seq model, the multiset tagging model, and the latent permutation model.</sample>
    <sample id="93">Die beiden Co-Autoren, Alexander Koller und Ivan Titov, sind die Berater (Advisors) des ersten Autors, Matthias Lindemann.</sample>
    <sample id="94">Jingwei Yi von der University of Science and Technology of China präsentiert eine Studie zur Schutz von Urheberrechten bei Embedding-as-a-Service (EaaS) durch eine Backdoor-Wasserzeichen-Methode. EaaS, wie z. B. GPT-Embeddings, wird häufig von Anbietern angeboten, doch kann durch Model-Extraktion von Dritten kopiert werden. Die Arbeit stellt Embedding Marker vor, ein Verfahren, das ein Wasserzeichen in Embeddings einbettet, ohne deren Nutzwert zu beeinträchtigen. Dabei wird ein Trigger-Set aus häufigen Wörtern verwendet, um das Wasserzeichen in die Embeddings zu injizieren. Bei der Copyright-Überprüfung wird ein Backdoor-Datensatz mit Trigger-Wörtern und ein harmloser Datensatz verwendet, um die Ähnlichkeit zwischen den Embeddings zu analysieren. Die Ergebnisse zeigen, dass das Verfahren effektiv ist und die Embeddings weiterhin für downstream-Aufgaben nutzbar bleiben. Die Visualisierung bestätigt, dass die Wasserzeichen kaum erkennbar sind. Die Methode bietet eine effektive Lösung zur Schutz von EaaS-Modellen vor unbefugtem Kopieren.</sample>
    <sample id="95">Der erste Autor von PaLM wird in dem Text nicht genannt.</sample>
    <sample id="96">Hallo zusammen. Ich bin Jenny, eine erste Jahr PhD-Studentin an der Carnegie Mellon University, und heute werde ich über unsere Arbeit NLPositionality präsentieren, bei der es um das Charakterisieren von Design-Voreingenommenheiten in Datensätzen und Modellen geht. Diese Arbeit wurde in Zusammenarbeit mit Kollegen von der University of Washington und dem Allen Institute for AI durchgeführt, nämlich Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap.

Beginnen wir mit einem Beispiel: Stellen Sie sich vor, Sie arbeiten für eine Zeitung und sortieren Kommentare unter einem Artikel, um toxischen Inhalt zu entfernen. Sie könnten zu einem beliebten API wie dem Prospective API für Toxizitätsdetektion greifen. Dies funktioniert gut, wenn Sie Carl Jones sind, bei dem das Prospective API toxische Instanzen korrekt erkennt. Das ist jedoch nicht der Fall bei Aditya Sharma, bei dem das Prospective API nicht so empfindlich auf offensive Begriffe reagiert, die in indischen Kontexten häufiger vorkommen. Dies ist ein Beispiel für einen Design-Voreingenommenheit, bei der wir systematische Leistungsunterschiede von Technologien zwischen Bevölkerungsgruppen beobachten. Solche Voreingenommenheiten können auf die Positionalität von NLP-Forschern und Modellentwicklern zurückgehen.

Positionalität bezeichnet die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben. Dies ist ein Konzept, das in kritischen Studien, insbesondere in feministischen und queer akademischen Räumen, weit verbreitet ist. Als Forscher kann die Positionalität den Forschungsprozess und dessen Ergebnisse beeinflussen, da sie die Entscheidungen der Forscher verändern kann.

Eine Frage, die man sich stellen könnte, ist: Haben Datensätze und Modelle eine Positionalität? Wir sagen nicht, dass Modelle und Datensätze selbst demografische Identitäten oder Lebenserfahrungen haben, doch sie sammeln Urteile und Meinungen realer Menschen und können somit bestimmte Positionalitäten über andere hinweg dominieren.

Vorherige Arbeiten haben anekdotische Beweise für Positionalität vorgeschlagen, wie kulturelle Lücken in Modellen und Datensätzen sowie theoretische Definitionen der Modell-Pozitionalität. Diese Arbeiten untersuchen jedoch nicht, wie Endnutzer mit Datensätzen und Modellen verglichen werden. Die Untersuchung von Positionalität in Modellen und Datensätzen wird immer wichtiger, da NLP-Aufgaben subjektiver und sozialer werden. Es ist jedoch schwierig, zu beschreiben, wie diese Positionalitäten verzerrt sind, da nicht alle Entscheidungen dokumentiert werden und viele Modelle hinter APIs verborgen sind.

Um die Positionalität von Datensätzen und Modellen zu untersuchen, vergleichen wir die Annotationen von realen Nutzern mit bestehenden Datensätzen und Modellen. Dazu verwenden wir unser Framework NLPositionality, das in zwei Hauptschritten arbeitet. Der erste Schritt besteht darin, Datensätze mit diversen Annotatoren neu zu annotieren. Wir tun dies, da die Demografie der ursprünglichen Annotatoren der Datensätze selten erfasst oder geteilt wird. Wir sammeln daher viele Annotatoren pro Instanz und eine reiche Menge an demografischen Daten.

Anschließend vergleichen wir die Annotationen nach Demografie mit Modellen und Datensätzen mithilfe eines Pearson’schen R-Korrelationswerts. Unser Framework unterscheidet sich damit von der Literatur zur Annotator-Disagreement, da wir Endnutzer mit Modellen und Datensätzen, Vorhersagen und Labels vergleichen, anstatt nur die Übereinstimmung zwischen Annotatoren oder deren Verteilung zu modellieren.

Unser Framework wird hauptsächlich durch das Lab in the Wild-Plattform ermöglicht, eine Online-Plattform für Crowdsourcing, die von HCI-Kooperationspartnern genutzt wird. Lab in the Wild ermöglicht es uns, diverse Freiwillige zu rekrutieren. Im Vergleich zu Plattformen wie M Turk, die vor allem Teilnehmer aus den USA oder Indien haben, kann Lab in the Wild immer noch hohe Datenqualität liefern.

Wir haben zwei Aufgaben auf Lab in the Wild durchgeführt. Eine davon ist die soziale Akzeptabilität, bei der Teilnehmer einen Fall aus dem Social Chemistry Datensatz lesen und dann beschreiben, wie sozial akzeptabel sie den Fall finden. Danach können sie ihre Antworten mit einem AI-Modell und anderen vergleichen. Wir haben diese Annotationen mit Social Chemistry, Delphi und GPT 4 verglichen.

Wir haben eine sehr ähnliche Aufgabenstruktur für die Toxizitäts- und Hassrede-Erkennung repliziert. Dabei haben die Teilnehmer einen Eintrag aus Dynahate gelesen und beschrieben, ob sie dies als Hassrede betrachten. Wir haben diese Annotationen mit Dynahate, Prospective API, Rewire API, Hate Roberta und GPT 4 verglichen.

Unsere Studie führte insgesamt zu über 16.000 Annotationen von über 1000 Annotatoren aus 87 Ländern.

Dank dieser Daten können wir nun besser beantworten, mit wem NLP-Datensätze und Modelle am besten übereinstimmen. Wir fanden Positionalität in NLP. Zum Beispiel stimmten Datensätze und Modelle am besten mit sprechenden Ländern überein. Bei der sozialen Akzeptabilität von GPT 4 stimmten sie am besten mit konfuzianischen und englischsprachigen Ländern überein. Dynahate stimmte ebenfalls am besten mit englischsprachigen Ländern überein.

Wir fanden auch eine zusätzliche Übereinstimmung mit Menschen, die eine Hochschulbildung haben. Bei der sozialen Akzeptabilität von GPT 4 stimmte es am besten mit Menschen mit Hochschul- oder Graduate School-Bildung überein, und dies fanden wir auch bei Dynahate.

Wenn Modelle und Datensätze jedoch bestimmten Bevölkerungsgruppen besser entsprechen, werden andere unweigerlich zurückgelassen. Ein Beispiel dafür ist, dass Datensätze und Modelle weniger mit nicht-binären Personen übereinstimmen als mit Männern und Frauen. Wir fanden dies sowohl bei der sozialen Akzeptabilität von GPT 4 als auch bei der Analyse der Dynahate-Aufgabe.

Angesichts der Positionalität in NLP, was können wir tun? Wir haben einige Empfehlungen:

1. Dokumentieren Sie alle relevanten Designentscheidungen während des gesamten Forschungsprozesses.
2. Führen Sie NLP-Forschung mit der Perspektive der Perspektivität durch.
3. Bauen Sie spezialisierte Datensätze und Modelle innerhalb von vier spezifischen Communities. Ein gutes Beispiel dafür ist das Masakhani-Initiative.

Wir möchten betonen, dass inklusives NLP nicht einfach bedeutet, dass alle Technologien für alle funktionieren. Das ist das Ende meiner Präsentation. Wenn Sie mehr erfahren möchten, können Sie gerne unsere Dashboard für die aktuellsten Analyseergebnisse und unser Paper besuchen. Vielen Dank.</sample>
    <sample id="97">Die Referentin geht auf drei Probleme von SimulST ein.</sample>
    <sample id="98">Soziale und politische Verzerrungen in Datensätzen können durch eine sorgfältige Auswahl und Aufbereitung der Trainingsdaten reduziert werden, beispielsweise durch die Diversifizierung der Quellen, die Entfernung von voreingenommenen Inhalten oder durch die Anwendung von Techniken zur Bias-Erkennung und -Korrektur. Gleichzeitig ist es wichtig, den Trade-off zwischen der Neutralität der Daten und der Risiken von Zensur oder Ausgrenzung zu berücksichtigen.</sample>
    <sample id="99">Hallo, ich bin Siyu Yuan aus der Fudan University. Ich möchte unser Werk „Distilling Script Knowledge from Large Language Models for Constrained Language Planning“ vorstellen. Im Alltag planen Menschen oft ihre Handlungen gemäß schrittweisen Anweisungen, die in Form von zielorientierten Skripten vorliegen. Frühere Arbeiten haben Sprachmodelle genutzt, um für abstrakte Ziele typischer Aktivitäten wie „Kuchen backen“ zu planen und gezeigt, dass große Sprachmodelle Ziele effektiv in Schritte zerlegen können. Allerdings konzentrierten sich frühere Arbeiten hauptsächlich auf das Planen für abstrakte Ziele typischer Aktivitäten. Das Planen für Ziele mit spezifischen Einschränkungen, wie z. B. „einen Schokoladenkuchen backen“, wurde bisher noch nicht ausreichend untersucht.

In diesem Paper definieren wir das Problem der eingeschränkten Sprachplanung, bei der verschiedene Einschränkungen auf die Planungsziele angewandt werden. Ein abstraktes Ziel kann von verschiedenen realen, spezifischen Zielen mit mehrfachen Einschränkungen ererbt werden. Ein guter Planer sollte Skripte erstellen, die sinnvoll und den Einschränkungen treu sind.

Zunächst bewerten und verbessern wir die Fähigkeit großer Sprachmodelle bei der eingeschränkten Sprachplanung. Da keine Datenbank spezifischer Ziele vorhanden ist, um unsere Studie zu unterstützen, müssen wir diese Ziele zunächst sammeln. Wie in der Tabelle gezeigt, erweitern wir die abstrakten Ziele mit mehrfachen Einschränkungen mithilfe von InstructGPT für die menschliche Datenakquise. Wir sammeln 100 spezifische Ziele und bewerten die von großen Sprachmodellen generierten Skripte. Diese Tabelle zeigt die Gesamtgenauigkeit der Ergebnisse. Wir stellen fest, dass alle Sprachmodelle bei der Planung spezifischer Ziele unzufriedenstellende Ergebnisse liefern.

Anschließend führen wir eine detaillierte Analyse durch, um herauszufinden, warum Lernmodelle versagen. Die Ergebnisse in der Abbildung zeigen, dass die semantische Vollständigkeit der generierten Skripte akzeptabel ist, die Treue zu den Einschränkungen jedoch nicht gewährleistet werden kann. Wir untersuchen detaillierter die Kategorien von Einschränkungen, wie sie in wikiHow definiert sind. Das Wärmekarten-Diagramm in der Abbildung zeigt, dass die Planungsleistung der InstructGPTs stark von der Kategorie der Ziele abhängt. Frühere Studien haben gezeigt, dass die Qualität der Ausgaben von Sprachmodellen stark variiert, was zu schlechten Ergebnissen führt. Daher setzen wir das Konzept „over-generate-then-filter“ ein, um die Generationsqualität zu verbessern.

Zunächst zeigen wir die Einschränkungstypen mit Beispielen für InstructGPT und erhalten spezifische Ziele basierend auf den Seed-Abstraktzielen. Anschließend generiert InstructGPT K Skripte für die spezifischen Ziele. Danach wird ein Filtermodell entwickelt, um die treuen Skripte auszuwählen. Wir konvertieren Skripte und Ziele in InstructGPT-Embeddings und berechnen die Kosinusähnlichkeit als Ähnlichkeitswerte, um die semantische Ähnlichkeit zu messen. Zudem belohnen wir Skripte, die Schlüsselwörter der Zielbeschränkung enthalten. Wir behalten nur das Skript, das im Zielsetzungssatz das höchste Ergebnis erzielt.

Mit unserem Verfahren kann InstructGPT Skripte höherer Qualität generieren. Unser Verfahren verbessert die Planungsfähigkeit sowohl in der semantischen Vollständigkeit als auch in der Treue zu den Einschränkungen.

Da große Sprachmodelle teuer in der Anwendung sind, ist es entscheidend, die Sprachplanungsfähigkeit kleinerer, spezialisierter Modelle zu ermöglichen. Die Erstellung eines Datensatzes ist ein entscheidender Schritt in diese Richtung. Frühere Studien haben jedoch die Planung für spezifische Ziele nicht ermöglicht, und manuelle Datensatzannotation ist teuer. Daher folgen wir dem Ansatz der symbolischen Wissensverteilung, um eingeschränkte Sprachplanungsdatensätze aus großen Sprachmodellen zu extrahieren.

Wir wenden unser Verfahren an, um einen Datensatz für eingeschränkte Sprachplanung zu erstellen, genannt CoScript. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten. Um die Qualität der Validierungs- und Testdaten sicherzustellen, bitten wir Crowdworkers, fehlerhafte Stichproben zu finden und zu korrigieren. Diese Abbildung zeigt die Einschränkungsverteilung von CoScript. Wir stellen fest, dass CoScript eine hohe Vielfalt in den generierten spezifischen Zielen aufweist. Mit CoScript können wir kleine, aber spezialisierte Modelle für eingeschränkte Sprachplanung testen. Wir stellen fest, dass T5, feinabgestimmt auf CoScript, Skripte höherer Qualität generiert als die meisten großen Sprachmodelle, was zeigt, dass kleinere Modelle große Modelle übertrumpfen können, wenn sie richtig auf geeignete Datensätze trainiert werden.

Zusammenfassend haben wir das Problem der eingeschränkten Sprachplanung etabliert. Wir haben die Fähigkeit großer Sprachmodelle zur eingeschränkten Sprachplanung bewertet und eine Methode entwickelt, um über-Generierung und anschließende Filterung anzuwenden. Wir haben mithilfe großer Sprachmodelle einen hochwertigen Skriptdatensatz, CoScript, für die eingeschränkte Sprachplanung erstellt. Wir hoffen, dass der CoScript-Datensatz eine wertvolle Ressource für die Forschung zur Sprachplanung sein wird.

Vielen Dank für Ihre Aufmerksamkeit. Weitere Details zu CoScript finden Sie in unserem Paper.</sample>
    <sample id="100">PromptRank is a data-efficient approach for multi-hop question answering (QA) that combines unsupervised retrieval with a few-shot language model-based reranker. Traditional multi-hop retrievers require thousands of training examples, but PromptRank achieves strong performance with just 128 examples. The method involves two steps: first, retrieving candidate chains using TF-IDF and hyperlink traversal, and second, reranking these chains using a language model that scores the likelihood of the question given the chain. The chain prompt is constructed by inserting documents into a prompt with an instruction to guide the model’s reasoning. Additional techniques like instruction search, sampling, and temperature scaling are explored to improve performance. Evaluated on HotpotQA, PromptRank outperforms fully supervised systems like DrKit and matches state-of-the-art dense retrievers. When combined with a reader model, it achieves strong downstream QA performance, only slightly underperforming MDR. Overall, PromptRank demonstrates that language models can effectively rank candidate paths for multi-hop QA with minimal training data.</sample>
    <sample id="101">Die Sprachgewandtheit von PaLM ist vergleichbar mit den besten Systemen, aber es gibt Probleme mit der Genauigkeit.</sample>
    <sample id="102">Die wichtigsten Eigenschaften eines Wasserzeichenverfahrens sind: Es muss für Embedding-as-a-Service anwendbar sein, die Nutzbarkeit der Embeddings nicht beeinträchtigen, ausreichend versteckt sein, damit Angreifer es nicht leicht entfernen können, und übertragbar sein, um während des Modell-Extraktionsprozesses erkannt zu werden.</sample>
    <sample id="103">The English TED talks were translated into 14 different languages.</sample>
    <sample id="104">Die Anzahl der Instanzen, die aus einem Datensatz für die erneute Annotierung extrahiert werden, wird im Text nicht genannt.</sample>
    <sample id="105">The distance metrics used are cosine similarity, L2 similarity, and the KS test p-value.</sample>
    <sample id="106">Chaitanya präsentiert die Arbeit QUEST, ein Retrieval-Dataset, das entwickelt wurde, um Systeme zu testen, die mit selektiven Informationsbedürfnissen umgehen können. Die Motivation entsteht aus Beispielen wie Jane, eine Zoologin, die nach einem unbekannten Reptil in Costa Rica sucht, oder Austin, der nach historischen Romanen in Frankreich sucht. Solche Anfragen beinhalten implizite Mengenoperationen wie Schnittmengen oder Komplemente. QUEST enthält über 3.000 solcher Anfragen, bei denen die Antworten überprüft und in Dokumenten mit zugeordneten Textabschnitten markiert wurden. Die Erstellung erfolgte durch Set-Operationen über Wikipedia-Kategorien und menschliche Annotationen, um die Anfragen zu paraphrasieren und zu validieren. Die Evaluation zeigt, dass herkömmliche Retrieval-Systeme bei der Erfüllung solcher Anfragen noch große Defizite aufweisen, insbesondere bei Schnittmengen und Mengen-Differenzen. Die Ergebnisse unterstreichen die Herausforderung, Systeme zu entwickeln, die mit komplexen, selektiven Informationsanfragen umgehen können. QUEST soll zukünftigen Forschern helfen, bessere Lösungen für solche Szenarien zu entwickeln.</sample>
    <sample id="107">Modelle, die auf einem mehrsprachigen Encoder basieren, wurden in dieser Aufgabe verwendet, um eine einzige mehrsprachige Model zu trainieren, die dann zur Übersetzung von Abfragen in verschiedenen Sprachen in Bedeutungsrepräsentationen wie SQL oder Lambda Calculus genutzt wird.</sample>
    <sample id="108">In ihrer Arbeit untersuchen Koustav Sinha und seine Kollegen, wie Sprachmodelle auf Akzeptabilitätsurteile in längerem Kontext reagieren. Sie analysieren das Minimal-Pair-Paradigma (MPP), das typischerweise kurze Sätze zur Bewertung von Grammatikalität oder Akzeptabilität verwendet. Die Forscher erweitern dieses Paradigma, um die Fähigkeit von Sprachmodellen zu testen, Akzeptabilität in längeren Sequenzen zu bewerten – ein entscheidender Aspekt für moderne Modelle mit großen Kontextfenstern. Dabei werden Sätze aus Datensätzen wie BLiMP und SyntaxGym in längere Kontexte eingebettet, sowohl mit passenden als auch unpassenden Präfixen. Die Ergebnisse zeigen, dass MPP-Urteile robust sind, wenn der Kontext irrelevant ist, aber stark beeinflusst werden, wenn der Kontext syntaktisch oder semantisch mit dem Zielzusatz übereinstimmt. Modelle zeigen dabei erhebliche Veränderungen in ihrer Bewertung, je nachdem, ob der Präfix akzeptabel oder unakzeptabel ist. Die Studie deutet darauf hin, dass Sprachmodelle latenten syntaktischen und semantischen Mustern folgen, die über den aktuellen Satz hinausreichen. Kurzfristige MPP-Tests können daher nicht vollständig die Fähigkeiten von Modellen in längerem Kontext erfassen. Die Ergebnisse betonen die Notwendigkeit, die Evaluationsmethoden für Sprachmodelle an die Entwicklung neuerer, kontextfreundlicherer Architekturen anzupassen.</sample>
    <sample id="109">In ihrer Präsentation stellt Or das Projekt „Unnatural Instructions“ vor, bei dem eine große Menge an natürlichen Sprachanweisungen automatisch generiert wird, ohne menschliche Annotation. Traditionell wird Instruction Tuning durch manuell erstellte oder aus bestehenden Datensätzen reformulierte Beispiele durchgeführt, was jedoch eingeschränkt bleibt. Stattdessen nutzen die Forscher ein vortrainiertes Sprachmodell (GPT-3) und prompten es mit Beispielen aus dem Super-Natural Instructions Datensatz, um neue Anweisungen und zugehörige Eingaben und Ausgaben zu generieren. Zusätzlich werden Paraphrasen der Anweisungen erstellt, um die Vielfalt zu erhöhen. Das resultierende Datenset enthält 64.000 Beispiele, sowie 240.000, wenn Paraphrasen berücksichtigt werden. Die Analyse zeigt, dass über 50 % der generierten Beispiele korrekt sind und viele auch wertvolle Informationen beinhalten. Ein T5-Modell, das auf „Unnatural Instructions“ fine-tuned wurde, übertrifft Benchmarks wie T0++ und Tk-instruct. Die Ergebnisse zeigen, dass automatisch generierte Daten nicht nur kreativ und vielfältig sind, sondern auch effizienter und kostengünstiger als manuelle Annotationen. Die Studie unterstreicht das Potenzial von Sprachmodellen, um kreative und nützliche Trainingsdaten ohne menschliche Eingriffe zu produzieren.</sample>
    <sample id="111">Die Autoren entscheiden, welche Wörter eine mittlere Häufigkeit haben, indem sie einen allgemeinen Textkorpora sammeln und die Wortfrequenzen darin zählen.</sample>
    <sample id="112">Hallo zusammen, mein Name ist Shuheng. Heute werde ich unsere Arbeit „Do CoNLL-2003 named entity taggers still work well in 2023?“ präsentieren. Los geht’s. Unser Paper untersucht das Problem der Generalisierung im Rahmen der Named Entity Recognition (NER) Aufgabe. Wir beobachten, dass Modelle bereits seit fast 20 Jahren im CoNLL-2003 Datensatz zur Entwicklung von NER-Modellen genutzt werden, was verschiedene Fragen aufwirft. Erstens: Generalisieren diese Modelle gut auf moderne Daten? Und wenn wir neue Tagger entwickeln, was ist notwendig, um eine gute Generalisierung zu erreichen? Gleichzeitig: Wenn wir eine schlechte Generalisierung beobachten, was verursacht dann den Leistungsabfall dieser Modelle?

Um diese Fragen zu untersuchen, haben wir das CoNLL++ Datensatz erstellt. Dies ist ein Datensatz, den wir aus Reuters News aus dem Jahr 2020 gesammelt und mit den gleichen Annotationen wie im CoNLL-2003 annotiert haben. Anschließend haben wir über 20 Modelle an CoNLL-2003 feinabgestimmt und sie sowohl an den CoNLL-03 Testdaten als auch an CoNLL++ bewertet. Zuletzt haben wir den prozentualen F1-Wert-Wechsel berechnet, um die Generalisierungsfähigkeit jedes Modells zu bewerten.

Was ist also für eine gute Generalisierung notwendig? Im Verlauf der Experimente haben wir drei Hauptbestandteile identifiziert, die erforderlich sind. Der erste ist die Modellarchitektur. Durch unsere Experimente haben wir festgestellt, dass Transformer-Modelle normalerweise besser auf neue Daten generalisieren. Der zweite Bestandteil ist die Modellgröße. Wir fanden heraus, dass größere Modelle normalerweise eine bessere Generalisierung ermöglichen. Und schließlich wissen wir alle, dass die Anzahl der Feinabstimmungsbeispiele direkt die Leistung einer downstream-Aufgabe beeinflusst. Hier haben wir ebenfalls festgestellt, dass mehr Feinabstimmungsbeispiele zu einer besseren Generalisierung führen.

Zu unserer nächsten Frage: Was verursacht den Leistungsabfall bei einigen Modellen? Wir hatten zwei Hypothesen. Die erste ist die adaptive Überanpassung, also die Überanpassung, die entsteht, wenn man die gleichen Testdaten immer wieder verwendet, was sich typischerweise als abnehmende Rendite auf einem neuen Testdatensatz zeigt. Die zweite Hypothese ist der zeitliche Drift, also der Leistungsverlust, der durch den zunehmenden zeitlichen Abstand zwischen Trainings- und Testdaten verursacht wird.

Für die Überanpassung haben wir gesehen, dass die rote Best-fit-Linie im Diagramm rechts einen Steigungswert größer als 1 hat. Das bedeutet, dass jede Verbesserungseinheit, die wir auf CoNLL-2003 erzielen, sich in mehr als eine Einheit Verbesserung auf CoNLL++ übersetzt, was bedeutet, dass es keine abnehmende Rendite gibt. Das zeigt uns, dass in diesem Fall keine adaptive Überanpassung beobachtet wird.

Was ist mit dem zeitlichen Drift? Für den zeitlichen Drift haben wir ein Experiment durchgeführt, bei dem wir einige Modelle erneut trainiert oder weiter vortrainiert haben, diesmal mit aktuelleren Daten. Wir fanden heraus, dass die Leistung mit einem größeren zeitlichen Abstand abnimmt, was unsere Hypothese bestätigt, dass der Hauptgrund für den Leistungsabfall der zeitliche Drift ist.

Unser Fazit ist, dass für eine gute Generalisierung eine bessere Modellarchitektur, eine größere Modellgröße sowie mehr Feinabstimmungsbeispiele erforderlich sind. Diese Faktoren hängen eng zusammen, wir können nicht einfach einen einzigen Bestandteil haben und die anderen weglassen. Gleichzeitig haben wir festgestellt, dass der Leistungsabfall hier durch den zeitlichen Drift verursacht wird, was überraschend ist, denn es ist nicht durch adaptive Überanpassung verursacht, obwohl CoNLL-2003 bereits seit über 20 Jahren genutzt wird.

Zurückkehrend zu der Frage, die wir in der Überschrift unseres Papers gestellt haben: „Funktionieren die CoNLL-2003 Tagger noch immer im Jahr 2023?“ Und wir haben festgestellt, dass die Antwort tatsächlich ein klares „Ja“ lautet.

Wir hoffen, dass unser Paper weitere Forschung anregt, wie die Generalisierungsfähigkeit von Modellen verbessert werden kann. Schließlich: Bitte besuchen Sie unsere Arbeit, unseren Datensatz und kontaktieren Sie mich bei Fragen. Vielen Dank!</sample>
    <sample id="114">**Abstract:**  
This paper introduces "Finding the Pillars of Strength for Multi-Head Attention," a novel approach to compress large language models by optimizing multi-head attention mechanisms. The proposed method, Grouped Head Attention (GHT), employs a two-stage strategy: group-constrained training and Voting-to-Stay pruning. Group-constrained training divides attention heads into groups, promoting intra-group similarity and inter-group diversity. Voting-to-Stay then prunes redundant heads, retaining only one per group. Experiments on machine translation, language modeling, and abstractive summarization show significant performance improvements and parameter compression. GHT-PS achieves up to 4.4% BLEU improvement and 32.1% parameter reduction compared to baselines. The LITE model further demonstrates 90% pruning, 62% faster inference, and 80% fewer FLOPs. Our work highlights the potential of task-specific pruning, inspired by the Lottery Ticket Hypothesis, to create more efficient and practical large language models.</sample>
    <sample id="115">Bei dem EDAtt-Ansatz wird die Sprachsegmentgröße durch den Parameter lambda bestimmt, der die Anzahl der letzten Sprachframes angibt, auf die sich die Aufmerksamkeit konzentrieren muss, bevor eine Übersetzung emittiert wird.</sample>
    <sample id="116">Das entitätsspezifische Wissen, das im Beispiel mit Servin und Kea benötigt wird, ist: "Servin ist ein Richter."</sample>
    <sample id="117">Der wichtigste Faktor ist die Qualität des Beispiels.</sample>
    <sample id="118">The paper "Improving Pretraining Techniques for Code-Switched NLP" addresses the challenge of handling code-switched text, common in multilingual communities like India. Traditional multilingual models like mBERT and XLM-R perform poorly on code-switched tasks. The authors introduce **SwitchMLM**, a novel masked language modeling approach tailored for code-switching. It focuses on **switch-points**—language transitions in sentences—by masking only these positions. Since labeled datasets for switch-points are scarce, they propose **FrequencyMLM**, a surrogate method using monolingual corpora to infer language tags. The paper also introduces architectural changes, such as **residual connections** from intermediate layers with rich switch-point information to the final layer, and an **auxiliary loss** to enhance language identification. Probing experiments confirm that these methods increase switch-point information in model representations. Combined with residual connections and auxiliary loss, SwitchMLM achieves state-of-the-art results on sentiment analysis tasks. Overall, the work advances pretraining techniques for better handling of code-switched NLP tasks.</sample>
    <sample id="119">Die Arbeiten in den erweiterten Experimenten konzentrieren sich auf Sprachmodelle wie RoBERTa und die GPT- sowie BART-Reihen.</sample>
    <sample id="120">Das Modell verwendet Aufmerksamkeitswerte aus einer bestimmten Ebene.</sample>
    <sample id="121">Beispiele für direkte Inferenz sind die Erwähnung des Namens eines Objekts, wie "Easy on Me", oder die Angabe der Position, wie "das erste Lied".</sample>
    <sample id="122">Die Autoren gehören der Fudan University an.</sample>
    <sample id="123">Ying und Zhiyang präsentieren ihre Forschung zu MultiInstruct, einem neuen Benchmark-Datensatz für das Instruction Tuning in der Multi-Modalen Zero-Shot-Lernung. Obwohl Instruction Tuning in der NLP-Bereich bereits etabliert ist, fehlte bislang eine entsprechende Anwendung in der Computer Vision und Multi-Modalität. Um dies zu adressieren, haben die Forscher MultiInstruct erstellt, einen Datensatz mit 62 verschiedenen Multi-Modalen Aufgaben, abgeleitet aus 21 Open-Source-Datensätzen und mit fünf Experteninstruktionen pro Aufgabe. Sie verwenden OFA, ein einheitliches Multi-Modell, um das Instruction Tuning zu untersuchen. Dabei wird die gesamte Eingabe – Text, Bilder, Bounding Boxes – in einem einheitlichen Token-Raum verarbeitet. Die Ergebnisse zeigen, dass Instruction Tuning die Leistung von OFA auf gesehene Multi-Modale Aufgaben deutlich verbessert. Transferlernen aus natürlichen Instruktionendatensätzen erhöht zusätzlich die Robustheit und Konsistenz der Vorhersagen. Die Forscher führen eine neue Metrik namens „Sensitivity“ ein, um die Fähigkeit des Modells zu messen, bei leicht unterschiedlichen Instruktionen konsistente Ergebnisse zu liefern. Zudem planen sie, den Datensatz mit 150 zusätzlichen Aufgaben zu erweitern und werden diesen öffentlich zugänglich machen.</sample>
    <sample id="124">In their work, Tan Qingyu and colleagues from NUS and Alibaba explore the temporal reasoning capabilities of large language models (LLMs), breaking it down into three levels: time-to-time, time-to-event, and event-to-event reasoning. They highlight that prior research has overemphasized the second level and propose the TempReason dataset to comprehensively evaluate all three. The dataset includes questions ranging from year to month prediction and is built using Wikidata and Wikipedia. They evaluate LLMs in three settings: Closed Book QA, Open Book QA, and a new Reasoning QA setting. To enhance temporal reasoning, they propose a training strategy with temporal span extraction pre-training and time-sensitive reinforcement learning, resulting in the TempT5 model. Experiments show that while ChatGPT performs poorly on month prediction and L2/L3 reasoning, TempT5 significantly outperforms other models, especially in Open Book and Reasoning QA settings. The study reveals temporal reasoning biases in LLMs and suggests future work to address these issues.</sample>
    <sample id="125">Die Anzahl der Autoren wird im gegebenen Text nicht erwähnt. Daher kann die Frage nicht aus dem bereitgestellten Inhalt beantwortet werden.</sample>
    <sample id="126">Ja.</sample>
    <sample id="127">Namgyu Ho, ein Masterstudent an der KAIST AI in Korea, präsentiert die Arbeit „Large Language Models Are Reasoning Teachers“. Die Studie untersucht, wie große Sprachmodelle (z. B. GPT-3) als „Lehrer“ genutzt werden können, um ihre Fähigkeit zur logischen Schlussfolgerung auf kleinere Modelle zu übertragen. Traditionell benötigen Techniken wie chain-of-thought prompting (CoT) nur riesige Modelle, um komplexe Aufgaben zu lösen, was aufgrund hoher Kosten und Ressourcen oft nicht praktikabel ist. Die Forscher schlagen vor, große Modelle zu nutzen, um Schritt-für-Schritt-Lösungen zu generieren, die dann als Trainingsdaten für kleine Modelle dienen. Um die Effektivität zu steigern, verwenden sie eine neue Methode namens „Diverse Reasoning“, bei der mehrere verschiedene Lösungsansätze generiert werden, um das kleine Modell besser zu trainieren. Die Ergebnisse zeigen, dass kleine Modelle mit weniger als einer Milliarde Parametern unter dieser Methode beachtliche Leistungen auf 12 verschiedenen Aufgaben erbringen können. Die Methode ist skalierbar und ermöglicht eine effektive Übertragung von Schlussfolgerungsfähigkeiten, wobei Kompromisse zwischen Entwicklungskosten, Inferenzkosten und Genauigkeit zu berücksichtigen sind. Die Forscher laden zur Weiterentwicklung ihrer Arbeit und zur Nutzung der bereitgestellten Daten und Code ein.</sample>
    <sample id="128">**Abstract:**  
In this work, we introduce the KITMUS test suite to evaluate models' ability to integrate knowledge from multiple sources—pretraining and inference time—critical for knowledge-intensive natural language understanding tasks. We focus on coreference resolution, where resolving pronouns requires both entity-specific knowledge (e.g., "Servin is a judge") and background knowledge (e.g., "Judges decide cases"). We design three settings to vary the availability of these knowledge types: Background-Pretrain, Background-Both, and Background-Inference. Our results show that without task-specific training, models fail to integrate knowledge effectively, relying instead on surface cues. While training on KITMUS improves performance, even top models struggle with inference-only background knowledge, especially when it involves novel or fictional concepts. This highlights the challenge of integrating knowledge from diverse, dynamically changing sources. Our dataset and code are available on GitHub.</sample>
    <sample id="129">The authors give the example of a "woman warrior" as a marked group, where the term "woman" is added to specify the gender, marking it as different from the unmarked default of "warrior," which is typically associated with men.</sample>
    <sample id="130">Modellarchitekturen, die nicht gut generalisieren, sind in der Regel keine Transformer-Modelle.</sample>
    <sample id="131">In dem Video wird nicht explizit der Name der Testdatensätze genannt. Es wird lediglich erwähnt, dass die Modelle auf "clean test sets" getestet werden, aber keine spezifischen Datensätze genannt.</sample>
    <sample id="132">Zwei Autoren sind an der Arbeit beteiligt: Akshatha und Martin.</sample>
    <sample id="133">Die Autoren arbeiten mit mehreren Modalitäten, also nicht nur mit Text, sondern auch mit Bildern und anderen multimodalen Daten.</sample>
    <sample id="135">ABC-Eval ist ein neues, dimensionsbasiertes Verfahren zur Bewertung von Konversationellen KI-Modellen, entwickelt vom Emory NLP Lab in Zusammenarbeit mit Amazon Alexa AI. Traditionell werden menschliche Bewertungen verwendet, um die Qualität von Dialogen zu beurteilen, doch diese Methoden sind oft vage und nicht feinkörnig genug. ABC-Eval reduziert Subjektivität, indem es spezifische Verhaltensweisen der Modelle annotiert, wie z. B. irrelevanten Inhalt, Widersprüche oder Fehlvorstellungen. Es misst die Häufigkeit solcher Fehler und bietet zuverlässige, informative Metriken. In Experimenten mit vier aktuellen Modellen zeigte sich, dass ABC-Eval bessere Vorhersagekraft für die Gesamtqualität von Dialogen hat als bestehende Methoden. Die Kombination aller ABC-Eval-Metriken erklärt über 25 % der Gesprächsqualität, während herkömmliche Methoden deutlich weniger erklären. Trotz der Fortschritte in der KI-Entwicklung bestehen weiterhin Herausforderungen, wie z. B. 20 % Common-Sense-Verstöße oder 15 % irrelevante Antworten. ABC-Eval bietet eine präzise, zuverlässige Methode zur Bewertung und kann als bedeutender Schritt für zukünftige Forschung genutzt werden.</sample>
    <sample id="136">Jasivan präsentiert die Arbeit „FERMAT: An Alternative to Accuracy for Numerical Reasoning“, die im Rahmen seiner Forschung mit Nafise an der University of Sheffield entstanden ist. Ziel der Studie ist es, eine bessere Bewertungsmethode für numerische Fähigkeiten von Sprachmodellen zu entwickeln, da bestehende Benchmarks wie Accuracy oder F1-Maße nicht ausreichend die Stärken und Schwächen der Modelle in mathematischen Aufgaben abbilden. FERMAT basiert auf einer Vielzahl von mathematischen Textaufgaben aus CommonCore und Illinois, wobei die Zahlen in verschiedenen Formaten wie Dezimalzahlen und großen Ganzzahlen variiert werden. Die Evaluation zeigt, dass selbst große Modelle numerische Aufgaben schlecht lösen, und dass die Erweiterung der Trainingsdaten mit unterschiedlichen Templates und mathematischen Operationen die Leistung verbessert. Die Untersuchung der Trainingsabhängigkeit ergibt, dass Modelle nicht einfach auswendig lernen, sondern sprachliche und mathematische Vielfalt benötigen, um numerische Aufgaben besser zu bewältigen. FERMAT bietet somit ein flexibles, informativeres Bewertungssystem, das den Bedürfnissen der realen Welt besser entspricht.</sample>
    <sample id="137">Die Arbeit „Tell2Design: A Dataset for Language-Guided Floor Plan Generation“ präsentiert eine neue Aufgabe im Bereich maschinellen Lernens, bei der Modelle aus natürlichsprachlichen Anweisungen 2D-Grundrisse generieren. Im Gegensatz zu traditionellen Bildgenerationsmodellen, die kreative, realistische Bilder erzeugen, konzentriert sich Tell2Design auf die Erstellung von Designlösungen, die strikte Anforderungen erfüllen. Das Dataset besteht aus 5.051 menschlich annotierten und 76.000 künstlich generierten Anweisungen, die mit Grundrissen verknüpft sind. Die Haupt Herausforderungen sind die Einhaltung von Konstruktionseinschränkungen, das Verstehen von unstrukturiertem Text und die Umgang mit unklaren oder unvollständigen Anweisungen. Die Autoren verwenden ein Encoder-Decoder-Modell basierend auf dem Transformer, um die Generierung als Sequenz-zu-Sequenz-Problem zu lösen. Die Ergebnisse zeigen, dass das Modell deutlich bessere IoU-Werte erzielt als Baseline-Methoden. Zudem wird gezeigt, dass künstlich generierte Daten zur Verbesserung der Leistung auf menschliche Anweisungen beitragen können. Tell2Design bildet eine Grundlage für zukünftige Forschung im Bereich der Sprachgesteuerten Designgenerierung.</sample>
    <sample id="138">Ein zu wenig erforschtes Gebiet im Bereich der NLU ist die Integration von Wissen aus verschiedenen Quellen, insbesondere das Verbinden von vorab gelerntem Hintergrundwissen mit zur Laufzeit bereitgestelltem Entitätenwissen.</sample>
    <sample id="139">The presenters are Ying and Zhiyang.</sample>
    <sample id="140">Ja, CoScript hat eine Qualitätskontrolle durchlaufen.</sample>
    <sample id="141">Die Grenzen bestehender Ressourcen für kontextbasierte Übersetzung liegen in ihrer begrenzten Vielfalt an Kontexttypen, Sprachen und ihrer Abhängigkeit von domain-spezifischem Wissen und menschlicher Kuratierung.</sample>
    <sample id="142">Hallo! Ich möchte über unsere Arbeit zum Thema „Auflösen indirekter Bezeichnungen für die Entitätswahl“ sprechen, bei der wir die AltEntities-Korpus einfügen. Mein Name ist Javad Hosseini, und dies ist eine gemeinsame Arbeit mit Filip Radlinski, Silvia Pareti und Annie Louis. Unser Ziel ist es, das Sprachverhalten von Nutzern zu verstehen, wenn sie eine Wahl treffen möchten. Betrachten Sie beispielsweise diese alternative Frage: „Meinten Sie ‚Easy on Me‘ oder ‚I Gotta Feeling‘?“ Hier möchte der Nutzer zwischen diesen beiden Liedern wählen. Die offensichtlichste Methode ist die direkte Referenz, zum Beispiel durch das Nennen des Liednamens „Easy on Me“ oder durch die Angabe der Position, „das erste Lied“. Manchmal ist jedoch eine indirekte Referenz angemessener, um eine natürliche Konversation zu ermöglichen. Dies kann passieren, wenn der Nutzer den Namen des Liedes nicht mehr weiß. Oder wenn die Aussprache zu ähnlich ist und es schwer zu unterscheiden ist. Oder wenn der Nutzer eine Präferenz ausdrücken möchte. Hier sind einige Beispiele für indirekte Referenzen, wie etwa „das neuere Lied“ oder „das Lied, das nicht energiegeladen ist.“ Dies ist ein wichtiges Problem in konversationellen Systemen und auch für die Bewertung von LLMs (Large Language Models) in Bezug auf das Verständnis von Entitäten. Wir sind uns nicht bewusst, dass es einen größeren öffentlich zugänglichen Datensatz für diese Aufgabe gibt, daher haben wir einen mit Hilfe von Crowd-Annotation gesammelt. Unser Datensatz umfasst drei verschiedene Bereiche: Musik, Bücher und Rezepte. Die Erhebungsmethode unseres Datensatzes betont Informalität durch einen Cartoon-Setup. Der Cartoon besteht aus drei Sprechblasen. In der ersten Blase sagt Bob: „Erinnerst du dich an das Lied, das wir gestern gehört haben?“ Damit wird der Dialog-Kontext gesetzt. In der zweiten Sprechblase sagt Alice: „Meinst du ‚Easy on Me‘ oder ‚I Gotta Feeling‘?“ Das ist die alternative Frage. In der dritten Sprechblase nutzt Bob eine indirekte Referenz, um eine dieser Entitäten auszuwählen, beispielsweise: „das neuere.“ Wir geben die ersten beiden Sprechblasen automatisch vor, die dritte wird von der Annotatorin befüllt. Die erste Sprechblase wird aus einigen manuell erstellten Prompts pro Bereich ausgewählt. Die zweite, also die alternative Frage, wird wie folgt generiert: Wir verwenden immer einen einfachen Vorlage: „Meinst du A oder B?“ Dabei sind A und B Beispiele aus Wikipedia. Hier sind die verschiedenen Sampling-Methoden, die wir verwendet haben. Je weiter oben wir in der Liste sind, desto ähnlicher sind die Entitäten und desto schwieriger ist die Unterscheidung. Die erste Methode ist zufällige Auswahl. Die zweite besteht darin, dass die Entitäten ähnliche Titel haben, beispielsweise zwei Bücher mit dem Namen „The Return“. Die dritte Methode ist, dass sie ähnliche Beschreibungen auf Wikipedia haben. Und schließlich, wenn sie ähnliche Info-Boxen oder Attribute auf Wikipedia haben, beispielsweise dieselbe Genres oder denselben Künstler für ein Lied. Wenn wir diese alternative Frage den Annotatorinnen präsentieren, kennen sie zwar die Namen der Entitäten, nicht aber unbedingt die Entitäten selbst. Was wir tun, ist, ihnen einige Hintergrundinformationen zu den beiden Entitäten zu zeigen. Für Lieder zeigen wir einfach einen Google-Suchlink zu jedem Lied und bitten die Annotatorinnen, mindestens etwas von jedem Lied anzuhören und über jedes Lied zu lesen. Hier ist beispielsweise das Google-Suchergebnis für das Lied „Easy on Me“. Für die Bereiche Rezepte und Bücher zeigen wir einige Hintergrundtexte aus Wikipedia. Für Rezepte zeigen wir zusätzlich Bilder aus Wikipedia, damit die Annotatorinnen wissen, wie sie aussehen. Dann bitten wir die Annotatorinnen, eine dieser Entitäten auszuwählen, beispielsweise die erste, und sie mit drei bis fünf indirekten Bezeichnungen zu beschreiben. Ein Beispiel ist „diejenige ohne Wörter“ oder „nicht diejenige mit dem 12-jährigen Jungen“ oder „die fiktive“ oder „die aus Aserbaidschan kommt“ und so weiter. Der AltEntities-Korpus umfasst 6.000 alternative Fragen über drei Bereiche und enthält 42.000 indirekte Bezeichnungen. Die Ergebnisse mit dem T5 XL-Modell sind unten zusammengefasst. Wenn das Sprachmodell Zugriff auf die exakt gleichen Hintergrundinformationen hat wie die Annotatorinnen, ist die Genauigkeit sehr hoch, etwa 92 bis 95 %. Dies ist jedoch nicht realistisch. Wenn das Sprachmodell Zugriff auf teilweise überlappendes Hintergrundwissen hat, liegt die Genauigkeit zwischen 82 und 87 %, was realistischer ist. Beispielsweise, wenn das Sprachmodell das Hintergrundwissen selbst abruft. Wenn das Sprachmodell nur Zugriff auf Entitätsnamen hat, liegt die Genauigkeit nur bei 60 %, also besteht noch viel Raum für Verbesserungen. Wir haben auch gezeigt, dass die Modelle in verschiedenen Bereichen anwendbar sind. Hier ist ein Link zu unserem Datensatz. Vielen Dank.</sample>
    <sample id="143">Der Ansatz wird mit den bestehenden SimulST-Richtlinien **Wait-k** und **Local Agreement** verglichen.</sample>
    <sample id="144">Die Autoren gehören der Universität Nantes an.</sample>
    <sample id="145">Der/die Referent*in heißt Jenny.</sample>
    <sample id="146">Yicheng, ein Doktorand an der Fudan University, stellt eine Studie zur Analyse von Omissionen in der Dialogsummarisierung vor. Obwohl große prätrainierte Sprachmodelle flüssige und kohärente Zusammenfassungen erzeugen können, leiden sie häufig unter Fehlern wie Faktenfehlern und unvollständigen Inhalten. Omissionen, also das Auslassen kritischer Informationen, stellen ein großes Problem dar und wurden bisher kaum systematisch untersucht. In der Studie wird gezeigt, dass bis zu 70 % der erzeugten Zusammenfassungen Omissionen enthalten, und diese fehlenden Informationen sind in Dialogen überall verteilt. Um dieses Problem zu adressieren, wurde das OLDS-Datenset erstellt, das hochwertige Omission-Labels für Dialogsummarisierungsmodelle bereitstellt. Die Forscher testeten drei Basismodelle zur Omissionserkennung und fanden heraus, dass die Aufgabe sehr herausfordernd ist. Durch die Verwendung der erkannten Omissionen zur Nachbearbeitung der Zusammenfassungen konnten die Ergebnisse deutlich verbessert werden. Die Studie unterstreicht die Bedeutung der Omissionserkennung als Schritt zur Verbesserung der Qualität von Dialogsummarisierungsmodellen.</sample>
    <sample id="147">An der Arbeit sind drei Autoren beteiligt: Myra, Esin Durmus und Dan Jurafsky.</sample>
    <sample id="148">Hallo, ich bin Sara Papi von der Universität Trento und der Fondazione Bruno Kessler. Ich werde kurz das Paper „Attention as a Guide for Simultaneous Speech Translation“ vorstellen, das ein gemeinsames Werk mit Matteo Negri und Marco Turchi ist. Was ist simultane Sprachübersetzung? Simultane Sprachübersetzung, oder SimulST, ist der Prozess, bei dem gesprochene Sprache in Echtzeit in einen Text einer anderen Sprache übersetzt wird, wodurch Kommunikation über Sprachgrenzen ermöglicht wird. Welche Probleme haben die aktuellen SimulST-Modelle? Oft werden spezifische Architekturen trainiert, wodurch zusätzliche Module optimiert werden müssen. Längere und kompliziertere Trainingsverfahren, beispielsweise Trainingsprozesse mit unterschiedlichen Optimierungszwecken. Und das Training und der Betrieb mehrerer Modelle, um verschiedene Latenzregime zu erreichen. Zum Beispiel das Training eines Modells mit einer durchschnittlichen Latenz von einer Sekunde und eines anderen mit zwei Sekunden, und so weiter. Was ist unsere Lösung? Erstens, bestehende Offline-ST-Modelle zu verwenden, ohne sie erneut zu trainieren oder spezifische Architekturen für SimulST zu adoptieren. Nur ein Modell pro Latenzregime verwenden und die Latenz durch spezifische Parameter behandeln. Und die bereits erlernte Wissen des Modells nutzen, durch die Aufmerksamkeitsmechanismen zwischen Audioeingabe und Textausgabe. Das ist der Cross-Attention-Mechanismus, und Sie können ein Beispiel rechts sehen. Unsere Lösung besteht darin, EDAtt, oder Encoder-Decoder Attention, vorzuschlagen. Es ist eine Strategie, bei der entschieden wird, ob eine Teilübersetzung ausgegeben wird oder nicht, basierend darauf, wo die Aufmerksamkeit sich befindet. Ein Wort wird ausgegeben, wenn die Aufmerksamkeit nicht konzentriert ist, das heißt, wenn ihre Summe unter einen bestimmten Schwellenwert alpha gegenüber den letzten lambda Sprachframes liegt, was bedeutet, dass die empfangene Information bereits stabil genug ist. Zum Beispiel, wenn wir einen Sprachabschnitt mit „I'm going to talk about...“ empfangen und unser Modell die Übersetzung ins Deutsche prognostiziert, werden wir die Cross-Attention-Gewichte betrachten und feststellen, dass die ersten beiden Wörter auf die frühesten empfangenen Sprachframes verweisen, während das letzte Wort auf die letzten empfangenen Sprachframes verweist, also auf die letzten lambda Sprachframes. Dies bedeutet, dass die ersten beiden Wörter ausgegeben werden, während die Summe der Cross-Attention über einen bestimmten Schwellenwert alpha liegt und wir daher das letzte Wort nicht ausgeben und auf einen weiteren Sprachabschnitt warten. Wenn wir weiterhin einen weiteren Sprachabschnitt empfangen und unser Modell drei weitere Wörter prognostiziert, werden wir diese Cross-Attention-Gewichte betrachten und feststellen, dass kein Wort auf die letzten lambda Sprachframes verweist. Dies bedeutet, dass diese drei Wörter ausgegeben werden. Wenn wir uns die Hauptergebnisse von EDAtt ansehen, plotten wir die Ergebnisse der simultanen Sprachübersetzung in Grafiken, in denen BLEU auf der einen Seite steht, der die Übersetzungsgüte misst, und der durchschnittliche Verzögerungswert, der die Latenz misst, auf der anderen. Wir berücksichtigen auch den computational-aware average lagging, der die Rechenzeit des Modells zur Vorhersage der Ausgabe berücksichtigt. Wir möchten, dass unsere Kurven in diesem Diagramm so hoch wie möglich sind. Wir möchten sie auch möglichst nach links verschoben. Wir vergleichen mit populären Strategien, die auch auf Offline-Modellen angewandt werden, nämlich der Wait-k-Strategie und der Local Agreement. Wir vergleichen auch mit der state-of-the-art-Architektur, die speziell für simultane Vorübersetzung entwickelt wurde. Dies sind alle die Ergebnisse der simultanen Sprachübersetzungstrategie ins Deutsche. Wir sehen, dass sie alle Strategien übertrifft, die auf Offline-Modellen angewandt werden, da die Kurven weiter nach links verschoben sind. Wir sehen auch, dass, wenn wir die tatsächliche vergangene Zeit oder die computationally-aware Zeit berücksichtigen, es die schnellste Strategie ist. Wenn Sie weitere Ergebnisse sehen möchten, lesen Sie unser Paper. Wir haben außerdem den Code, die Modelle und die simultanen Ausgaben als Open Source veröffentlicht, um die Wiederholbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="149">Ja, der Datensatz ist öffentlich zugänglich.</sample>
    <sample id="150">Archiki präsentiert das Paper „MEETINGQA: Extractive Question-Answering on Meeting Transcripts“, ein neues Datenset für die Frage-Antwort-Verarbeitung in Meeting-Transkripten. Die Forschungsergebnisse zeigen, dass Meeting-Transkripte ein reichhaltiges, bisher untergenutztes NLP-Domänenfeld sind, insbesondere wegen der oft langen, offenen Fragen und diskussionsfördernden Antworten. Das MeetingQA-Datenset besteht aus 7.700 Fragen, wobei 30 % unbeantwortbar sind, 40 % mehrere Antwortabschnitte beinhalten und 48 % von mehreren Sprechern beantwortet werden. Die Datensammlung basiert auf dem AMI-Korpus und wurde durch Annotation mit hoher Inter-Annotation-Übereinstimmung (Krippendorff’s Alpha von 0,73) erstellt. Die Evaluation zeigt, dass selbst feinabgestimmte Modelle wie RoBERTa und Longformer mit einer F1-Score-Lücke von über 25 Punkten gegenüber menschlichen Leistungen noch nicht optimal performen. Zudem ist die Zero-Shot-Performance deutlich schlechter, wobei Silver-Data-Augmentation helfen kann. Fehleranalyse zeigt, dass Modelle Schwierigkeiten haben, rhetorische Fragen zu erkennen und die Antwortenden zu identifizieren. MeetingQA stellt eine Herausforderung für bestehende QA-Modelle dar und bietet neue Forschungsmöglichkeiten in der NLP-Community.</sample>
    <sample id="151">Hallo alle, mein Name ist Ying und mein Kollege Zhiyang und wir werden unsere Forschung zu MultiInstruct präsentieren, die Multi-Modal Zero-Shot Learning durch Instruction Tuning verbessert. Mit den Fortschritten in den großen Sprachmodellen haben sich viele Arbeiten damit beschäftigt, neue Lernparadigmen zu erforschen, die vortrainierte Sprachmodelle in einer parameter- und dateneffizienten Weise für verschiedene downstream-Aufgaben wiederverwenden. Kürzlich haben viele Studien gezeigt, dass Instruction Tuning große Sprachmodelle dazu befähigt, Aufgaben, die sie noch nie gesehen haben, in einer zero-shot-Art zu bewältigen, indem sie natürliche Anweisungen folgen. Allerdings konzentrierten sich die meisten früheren Arbeiten zu Instruction Tuning darauf, die zero-shot-Performance bei reinen Sprachaufgaben zu verbessern, während computervision- und multimodale Aufgaben bislang außen vor blieben. Daher möchten wir in dieser Arbeit untersuchen, ob das Instruction Tuning von multimodalen vortrainierten Modellen tatsächlich die Generalisierung auf bisher nicht gesehene multimodale Aufgaben verbessern kann. Während unserer Forschung haben wir außerdem eine erhebliche Diskrepanz in der Verfügbarkeit von Anweisungsdatensätzen zwischen NLP und multimodalen Aufgaben festgestellt. Es gibt über 1600 Sprachaufgaben mit Anweisungen. Allerdings existiert kein großskaliger öffentlich verfügbare multimodaler Anweisungsdatensatz. Dies motiviert uns, einen multimodalen Instruction Tuning-Datensatz zu erstellen. Hier präsentieren wir MultiInstruct, den ersten multimodalen Instruction Tuning-Benchmark-Datensatz, der aus 62 vielfältigen multimodalen Aufgaben besteht, die 10 breite Kategorien abdecken. Diese Aufgaben stammen von 21 bestehenden Open-Source-Datensätzen und jede Aufgabe ist mit fünf von Experten verfassten Anweisungen ausgestattet. Um das multimodale Instruction Tuning auf unserem vorgeschlagenen Datensatz zu untersuchen, verwenden wir OFA als Basismodell, ein vereinheitlichtes multimodales vortrainiertes Modell. OFA verwendet ein vereinheitlichtes Vokabular für Sprach-, Bild-Token und die Koordinaten eines Bounding Box. Hier zeigen wir einige Beispiele aus unserem MultiInstruct-Datensatz, um die Verarbeitung verschiedener Eingabe- und Ausgabedatentypen zu vereinheitlichen. Wir folgen der Methode von OFA und formulieren alle Aufgaben in einem vereinheitlichten sequence-to-sequence-Format. Dabei werden Eingabetexte, Bilder, Anweisungen und Bounding Boxes im gleichen Tokenraum dargestellt. Nun werde ich über multimodales Instruction Tuning sprechen. Für den Trainingsdatensatz verwenden wir 53 Aufgaben aus 9 Gruppen zum Training und sampeln 10.000 Instanzen pro Aufgabe. Für das Testen reservieren wir die gesamte Gruppe der common sense reasoning für das Testen und wählen zusätzlich 5 Aufgaben aus den Gruppen VQ und Miscellaneous aus. Wir verwenden alle Instanzen im Testsplit für jede Aufgabe. Zudem sampeln wir zufällig 20 Aufgaben aus dem Testsplit der natürlichen Anweisungen als unbekannte Aufgaben für NLP. Als Basismodell verwenden wir das vortrainierte OFA large Modell. Während des Trainings mischen wir alle Instanzen aller Aufgaben. Jede Instanz wird zufällig mit einer ihrer fünf Anweisungsvorlagen kombiniert. Während des Tests führen wir für jede Aufgabe insgesamt 5 Experimente durch, indem wir das Modell mit einer der fünf Anweisungen bewerten. In jedem Experiment berichten wir über die minimale und maximale Leistung sowie die Standardabweichung der Leistung über alle 5 Experimente. Wenn die Aufgabe eine multimodale Klassifizierungsaufgabe ist, berichten wir die Genauigkeit. Wenn es sich um eine multimodale Generierungsaufgabe handelt, berichten wir den Rouge-L-Wert. Für NLP-Aufgaben berichten wir ebenfalls den Rouge-L-Wert. Wir führen zudem eine zusätzliche Bewertungsmetrik namens Sensitivität ein. Diese misst die Fähigkeit des Modells, für die gleiche Aufgabe unabhängig von geringfügigen Unterschieden in der Formulierung der Anweisung die gleichen Ausgaben zu erzeugen. Hier sind unsere Hauptergebnisse. Wie wir sehen können, verbessert das Instruction Tuning die Leistung von OFA deutlich bei gesehenen multimodalen Aufgaben. Auch das Transferlernen von natürlichen Anweisungsdatensätzen kann das Instruction Tuning unterstützen. Hier können wir sehen, dass mit zunehmender Anzahl von Aufgaben das Modell eine bessere Leistung erzielt und gleichzeitig eine geringere Sensitivität aufweist. Wir haben auch ein Experiment durchgeführt, bei dem wir eine Anweisung gegen fünf Anweisungen verglichen haben. Wie wir sehen können, verbessert das Verwenden mehrerer Anweisungen die Gesamtleistung des Modells erheblich und reduziert seine Sensitivität stark. Dies zeigt den Effekt unterschiedlicher Feinabstimmungsstrategien auf die Sensitivität des Modells. Wie wir sehen können, ermöglicht das Transferlernen von natürlichen Anweisungsdatensätzen dem Modell eine weitaus bessere Sensitivität im Vergleich zum ursprünglichen OFA-Modell. Wir können auch sehen, dass das Transferlernen von natürlichen Anweisungsdatensätzen dem OFA-Modell bei der natürlichen Anweisungsdatensatz-Aufgabe eine deutlich bessere Leistung ermöglicht. Insgesamt schlagen wir den ersten großskaligen multimodalen Instruction Tuning-Datensatz vor, der die Short-Task-Kapazität von OFA erheblich verbessert, und untersuchen verschiedene Transferlernen-Techniken und zeigen ihre Vorteile. Wir entwickeln eine neue Metrik namens Sensitivität. Eine weitere Sache: Wir sammeln derzeit einen viel größeren multimodalen Instruction Tuning-Datensatz mit etwa 150 zusätzlichen visuell-sprachlichen Aufgaben und werden diesen veröffentlichen. Hier ist ein QR-Code zu unseren Daten und Modellen. Vielen Dank.</sample>
    <sample id="152">Frederick Riemenschneider stellt in seiner Präsentation „Exploring Large Language Models for Classical Philology“ neue Sprachmodelle vor, die für die klassische Philologie entwickelt wurden. Obwohl bereits einige Modelle wie Latin BERT und Ancient Greek BERT existieren, sind diese meist monolingual und basieren auf BERT, was ihre Anwendungsbreite begrenzt. Riemenschneider und sein Team haben daher zwei monolinguale Modelle (GreBERTa und GreTa) sowie zwei multilinguale Modelle (PhilBERTa und PhilTa) entwickelt, die auf Altgriechisch, Latein und Englisch trainiert wurden. Dabei nutzten sie eine neu erstellte, hohe Qualität aufweisende Trainingsdatenbank, die auch Daten aus dem Internet Archive beinhaltet, die zuvor nicht für Altgriechisch geeignet waren. Die Modelle wurden anhand von Aufgaben wie POS-Tagging, Dependency-Parsing und Lemmatisierung getestet und erzielten bessere Ergebnisse als bisherige Modelle. Besonders hervorzuheben ist die Leistung von GreTa bei der Lemmatisierung. Zudem zeigte sich, dass multilinguale Modelle nicht unbedingt besser abschneiden als monolinguale, was die Komplexität der Sprachmodellierung unterstreicht. Die Arbeit bietet eine wichtige Grundlage für zukünftige Forschung im Bereich klassischer Philologie und NLP.</sample>
    <sample id="153">In this work, Ninareh Mehrabi and her team at Amazon Alexa AI's Responsible AI team investigate ambiguities in prompts given to text-to-image generative models. They highlight that ambiguous prompts, such as "The girl enters the room with flowers," can lead to images that do not align with the user's intention. To address this, they propose frameworks to disambiguate prompts by either generating clarifying questions or suggesting multiple visual interpretations, allowing users to select the intended meaning. A benchmark dataset, derived from LAVA, covers various ambiguity types. After disambiguation, the refined prompts are input into text-to-image models to generate images, which are then evaluated using an automatic framework involving a VQA model to assess faithfulness to user intention. The results show that their framework improves faithful image generation and that their evaluation method aligns with human judgments. Overall, the work contributes to better understanding and resolving ambiguities in text-to-image systems, ensuring more accurate and intention-aligned outputs.</sample>
    <sample id="154">Die Autoren gehören der University of Trento und der Foundazione Bruno Kessler an.</sample>
    <sample id="155">Der Referent heißt Javad Hosseini.</sample>
    <sample id="157">Shen Gao from Shandong University introduces the work "Dialogue Summarization with Static-Dynamic Structure Fusion Graph," a joint effort with several colleagues. Dialogue summarization aims to extract key information from multi-participant conversations into concise summaries. Current methods rely on pre-computed static graphs using linguistic tools, but these suffer from dependency on external tools and lack of adaptability. The proposed SDDS model addresses these issues with four components: an Utterance Encoder, a Static-Dynamic Graph module, and a Summary Generator. The model first constructs static graphs using heuristic methods like discourse parsing and speaker interaction frequency. It then dynamically captures semantic relationships using multi-head attention. The static and dynamic graphs are fused into a unified structure, which is used in a dual cross-attention mechanism for summary generation. The approach improves the accuracy and adaptability of dialogue summarization. The code and data are available on GitHub.</sample>
    <sample id="158">Qipeng Guo von AWS stellt die Arbeit „Dual Cache for Long Document Neural Coreference Resolution“ vor. Das Ziel der Coreference-Resolution ist es, verschiedene Erwähnungen desselben Entities in einem Dokument zu verknüpfen. Traditionelle Methoden haben quadratische Komplexität, während neuere cachebasierte Ansätze die Komplexität auf linear reduzieren. Bei langen Dokumenten mit wechselnden Themen führt die LRU-Eviction-Policy jedoch zu vielen Cache-Miss, insbesondere bei häufig vorkommenden Entities. Um dies zu verbessern, schlägt das Dual-Cache-Verfahren zwei Caches vor: einen lokalen mit LRU-Policy für lokale Entities und einen globalen mit LFU-Policy für häufige Entities. Das Modell scannt das Dokument von links nach rechts, klassifiziert neue Erwähnungen und fügt sie entsprechend in den lokalen oder globalen Cache ein. Die Evaluierung auf öffentlichen Benchmarks zeigt, dass Dual Cache die Baselines übertreffen kann, selbst bei begrenztem Speicher. Auf einem 30.000-Wörter-Buch zeigt sich eine deutliche Leistungssteigerung gegenüber Baseline-Methoden. Dual Cache reduziert Cache-Miss erheblich und bietet eine gute Effizienz-Performance-Balance. Somit ist Dual Cache eine kosteneffiziente und leistungsstarke Lösung für die Coreference-Resolution in langen Dokumenten.</sample>
    <sample id="159">Hallo, alle. Ich bin Koustav Sinha und begrüße euch herzlich bei unserem Vortrag zu unserem ACL 2023 Paper. Die Akzeptabilitätsurteile von Sprachmodellen sind nicht immer robust gegenüber dem Kontext. Dies ist eine gemeinsame Arbeit mit John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy und Adina Williams. In dieser Arbeit untersuchen wir erneut das Paradigma der minimalen Paare. Das Paradigma der minimalen Paare bewertet Sprachmodelle hinsichtlich ihrer Akzeptabilitätsurteile. Dies kann auch Grammatikalität umfassen, wie bei BLiMP oder SyntaxGym, oder Akzeptabilität in Bezug auf Stereotype, wie bei CrowS Pairs. Im Rahmen des minimalen Paar-Paradigmas wird typischerweise ein akzeptables oder grammatisch korrektes Satzpaar präsentiert, wobei das Modell erwartet wird, der akzeptablen Version eine höhere Wahrscheinlichkeit zuzuordnen. Der aktuelle MPP-Workflow erlaubt es uns nicht, die Akzeptabilität von Modellen anhand längerer Sätze zu bewerten. Inzwischen haben große Sprachmodelle immer längere Kontextfenster. Es ist daher entscheidend, die Akzeptabilität der Modelle während des gesamten Kontextfensters zu bewerten – und das ist unser Ziel in dieser Arbeit. Wir untersuchen das MPP-Paradigma erneut, indem wir das Modell auffordern, Akzeptabilitätsurteile anhand immer längerer Sequenzen zu treffen. Unser Ansatz besteht darin, die Datensätze selbst zu überprüfen und dann Sätze zu erstellen, indem wir akzeptable oder unakzeptable Sätze aus diesen Datensätzen auswählen. Hier haben wir beispielsweise ein typisches Paar aus der BLiMP-Datensammlung zum Adjunct Island-Fall ausgewählt. Um längere Sequenzen zu simulieren, extrahieren wir grammatisch korrekte Sätze aus dem Adjunct Island-Beispiel und fügen sie als Präfix sowohl zu den akzeptablen als auch zu den unakzeptablen Anfragen hinzu. Ebenso können wir dies durch Auswahl unakzeptabler Sätze aus dem gleichen Matching durchführen, was ebenfalls zur Testung der Akzeptabilität genutzt werden kann. Wir können dies auch durch Auswahl von Sätzen aus unterschiedlichen Untermengen oder Datensätzen durchführen. Dies nennen wir den Mismatch-Szenario. Hier stammen die Sätze zwar immer noch aus relevanten Datensätzen, aber nicht aus demselben Datensatz, mit dem wir die Bewertung durchführen. Ebenso können wir dies für den Fall der Unakzeptabilität durchführen. Schließlich können wir Sätze auch aus einem völlig unabhängigen Bereich wie Wikipedia auswählen. Dies zeigt uns, ob die Akzeptabilitätsurteile der Modelle tatsächlich vom Kontext beeinflusst werden – ob der Kontext aus einer anderen Untermenge des Datensatzes stammt oder ob er völlig irrelevant ist. Wie schneiden die Modelle ab? Zunächst untersuchen wir Sätze aus Wikipedia, die völlig irrelevant zu den aktuellen Anfragepaaren sind, und stellen fest, dass die MPP-Urteile in der Regel robust sind, unabhängig von der Länge des Kontexts. Wir erhöhen die Kontextlänge bis zu 1024, um die Modelle OPT und GPT-2 zu maximaler Auslastung zu bringen. In der orangenen gestrichelten Linie sehen wir, dass die MPP-Urteile relativ stabil bleiben. Was passiert, wenn wir Sätze aus demselben Datensatz wählen? Hier wählen oder erstellen wir Sätze aus den akzeptablen und unakzeptablen Bereichen des gleichen BLiMP- oder SyntaxGym-Datensatzes. Hier beobachten wir, dass die MPP-Urteile signifikant ansteigen oder abfallen, sobald wir entweder akzeptable oder unakzeptable Präfixe hinzufügen. Wenn wir jedoch die Struktur übereinstimmen lassen, also Sätze aus demselben Phänomen in BLiMP oder SyntaxGym wählen, sehen wir eine massive Zunahme oder Abnahme der MPP-Urteile, abhängig davon, ob der gewählte Präfix akzeptabel oder unakzeptabel ist. Dieser Effekt ist sehr groß und steigt mit zunehmender Kontextlänge an. Dies könnte auch neuere Sprachmodelle mit großen Kontextfenstern beeinflussen. Warum beeinflusst ein passender Präfix die Urteile der Sprachmodelle so stark? Wir haben eine Reihe von Analysen durchgeführt, bei denen wir die Eingabesätze dadurch verändert haben, dass wir die relevante Struktur beibehalten, aber gleichzeitig Störungen hinzugefügt haben. Nach verschiedenen dieser Störungen stellen wir fest, dass keine dieser Störungen das Modell tatsächlich dazu bringt, seine Urteile in Bezug auf die MPP-Bewertung zu verändern. Kurz gesagt, wir stellen fest, dass die Modelle auf gestörte Sätze in ähnlicher Weise reagieren. Wenn wir Sätze im akzeptablen Bereich stören, sehen wir in allen Störungen eine ähnliche Zunahme, und wenn wir Sätze im unakzeptablen Bereich stören, sehen wir in ähnlicher Weise eine Abnahme der MPP-Bewertung. Die zentralen Erkenntnisse unserer Arbeit sind, dass Sprachmodelle empfindlich auf latente syntaktische und semantische Merkmale reagieren, die in verschiedenen Sätzen gemeinsam vorkommen. Die derzeitige MPP-Bewertung mit kurzen und einzelnem Satzinput könnte die abstrakten Kenntnisse der Sprachmodelle nicht vollständig erfassen, insbesondere im gesamten Kontextfenster. Lesen Sie unser Paper für weitere Details zu unseren Experimenten. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="160">Im ersten Schritt der Methode werden die Input-Token mit einem ungeordneten Multiset von Tokens zugeordnet, die in der Ausgabe erscheinen.</sample>
    <sample id="161">In CoScript sind insgesamt 55.000 Skripte vertreten.</sample>
    <sample id="163">Die beste Ausrichtungsmethode für DEPLAIN ist MASSalign.</sample>
    <sample id="164">Der Vorteil von schwach überwachtem Lernen ist, dass es günstiger ist, da es keine teuren manuellen Annotationen benötigt.</sample>
    <sample id="165">In their presentation, Wenting Zhao introduces the paper "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations." Abductive reasoning involves finding plausible explanations that connect a context to an outcome. Traditional methods rely on supervised learning, requiring annotated explanations, which can be subjective and inconsistent. Zhao proposes LiPoR, an unsupervised method that learns abductive reasoning without needing labeled data. LiPoR treats explanations as latent variables and maximizes the likelihood of the outcome given the context while enforcing mutual exclusivity among explanations. A regularizer ensures that only a subset of explanations is favored, promoting consistency. On the AlphaNLI dataset, LiPoR outperforms existing zero-shot and unsupervised models by over 4 percentage points in accuracy. The approach demonstrates that abductive reasoning can be effectively learned without supervision, leveraging the inherent exclusivity of explanations. The paper is available at tinyurl.com/zhao-lipor.</sample>
    <sample id="166">**Abstract:**  
This paper introduces a Neural Divide-and-Conquer Reasoning (NDCR) framework for image retrieval from linguistically complex text, a challenging task due to high image similarity and complex descriptions. Traditional visual language models, which rely on analogical reasoning, struggle with such tasks. Inspired by the Divide-and-Conquer strategy and Dual-Process Theory, NDCR integrates two reasoning systems: System 1 (Visual-Linguistic Interactor) for analogical reasoning and System 2 (Neural-Symbolic Reasoner) for logical inference. The framework decomposes complex propositions into simpler ones using a Proposition Generator, processes them with System 1, and combines results via negation and conjunction operations in System 2. Experimental results show that NDCR outperforms existing methods, with ablation studies confirming the effectiveness of each module. The approach enables interpretable, step-by-step reasoning, demonstrating the potential of combining neural and symbolic reasoning for complex image-text tasks. This work highlights the value of integrating Dual-Process Theory and Divide-and-Conquer strategies in enhancing compositional reasoning in vision-language models.</sample>
    <sample id="167">Die Dokumente in DEPLAIN-web wurden teilweise manuell und teilweise mit automatischen Alignmentmethoden ausgerichtet.</sample>
    <sample id="168">Der CoNLL++-Datensatz wurde durch Sammeln von Nachrichten aus der Reuters News aus dem Jahr 2020 und anschließender Annotation nach den CoNLL-2003-Richtlinien erstellt.</sample>
    <sample id="169">David Vilar präsentierte eine Zusammenfassung des Papers „Prompting PaLM for Translation: Assessing Strategies and Performance“, das gemeinsam mit Kollegen von Google Translate erstellt wurde. Das Paper untersucht systematisch, wie große Sprachmodelle wie PaLM (ein 540-Milliarden-Parameter-Modell) durch Prompting für maschinelle Übersetzung eingesetzt werden können. Dabei wurden verschiedene Prompting-Strategien getestet, wobei sich zeigte, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit zum Quelltext. Besonders effektiv erwies sich das 5-shot Prompting, bei dem Sätze mit ihrer Sprache markiert wurden. Obwohl PaLM den kommerziellen Systemen wie Google Translate in der Flüssigkeit der Übersetzungen nahe kommt, weist es häufiger Omission-Fehler auf. Die menschliche Evaluation zeigte, dass PaLM zwar flüssige, aber weniger genaue Übersetzungen produziert. Die Ergebnisse unterstreichen die Bedeutung einer sorgfältigen Prompt-Auswahl und zeigen, dass spezialisierte Systeme weiterhin Vorteile gegenüber großen Sprachmodellen haben.</sample>
    <sample id="170">Hallo zusammen, mein Name ist Yusen Zhang aus der Pennsylvania State University. Heute möchte ich unsere Arbeit präsentieren: „XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations“. Semantische Parsing ist eine Aufgabe, bei der semantische Darstellungen von Benutzeranfragen erstellt werden, wie z. B. SQL oder Lambda-Kalkül. Cross-Lingual Semantic Parsing ist die Aufgabe, Anfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsrepräsentationen zu übersetzen. Wie in dieser Abbildung gezeigt, müssen wir mithilfe neuronaler Modelle Anfragen in mehreren natürlichen Sprachen in SQL, Lambda oder FunQL usw. übersetzen. Bestehende Modelle für cross-linguales semantisches Parsing werden jeweils für begrenzte Aufgaben und Anwendungen vorgeschlagen und bewertet. Zum Beispiel gibt es viel Abdeckung für bestimmte natürliche Sprachen, aber chinesisch fehlt und es gibt keine Abdeckung für bestimmte Bedeutungsrepräsentationen. Der Lambda-Kalkül fehlt beispielsweise oder sie werden nur anhand eines einzigen neuronalen Modells bewertet. Zu diesem Zweck haben wir XSemPLR vorgeschlagen. Wir stellen ein einheitliches Datenset XSemPLR für cross-linguales semantisches Parsing in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen bereit. Es enthält 9 Datensätze aus verschiedenen Bereichen, 5 semantische Parsing-Aufgaben, 8 Bedeutungsrepräsentationen und 22 natürliche Sprachen aus 15 Sprachfamilien. Um unseren Benchmark besser zu bewerten, betrachten wir sechs Trainings- und Evaluierungsszenarien. Das erste ist Translate-Test. Wir verwenden die Google Translate API, um die Quelle in die ZielSprache zu übersetzen, und trainieren und bewerten dann ein monolinguales Modell. Zum Beispiel trainieren wir das englische Modell mit englischen Anfragen und während der Inferenz übersetzen wir die deutschen Anfragen mithilfe der API in Englisch und verwenden dann das trainierte Modell, um SQL vorherzusagen. Wir testen auch monolinguale Modelle. In diesem Szenario ist die Quellsprache dieselbe wie die Zielsprache, z. B. Deutsch zu Deutsch oder Englisch zu Englisch. Wir testen auch eine monolinguale Few-shot-Einstellung, bei der monolinguale Modelle nur mit 10 % der Trainingsdaten trainiert werden. Wir testen auch ein multilinguales Modell, bei dem wir ein einziges multilinguales Modell für alle Sprachen trainieren. Zum Beispiel kombinieren wir deutsche, englische und chinesische Anfragen, um ein multilinguales Modell zu trainieren. Während der Inferenz können wir dieses Modell verwenden, um deutsche oder chinesische Anfragen zu übersetzen. Wir betrachten auch cross-linguale Zero-shot- und Few-shot-Übertragung. Wir trainieren auf einer Quellsprache und übertragen auf eine andere Sprache. Während des Trainings trainieren wir das Modell auf englischen Anfragen oder auf einer Kombination aus englischen und deutschen Few-shot-Anfragen, um ein multilinguales Modell zu trainieren, das SQL-Ausgaben vorhersagt. Wir haben auch viele interessante Ergebnisse gefunden. Bei der Analyse der monolingualen Modelle haben wir zwei Gruppen von Modellen bewertet: Encoder-PTR, was Multilingual Pretrained Encoders mit Pointer-based Decodern bedeutet, z. B. XLM-R + PTR und mBERT + PTR. Wir haben auch Encoder-Decoder-Modelle bewertet, also multilinguale prätrainierte Encoder-Decoder-Modelle, wie mBART und mT5. Wir fanden heraus, dass Encoder-Decoder-Modelle die beste Leistung auf allen neun Datensätzen erzielten. Wir bewerteten mT5 und XLM-R + PTR in einem multilingualen Szenario. Wir fanden heraus, dass Encoder-Decoder oder Encoder-PTR durch das Training mit einer Mischung aus verschiedenen Sprachen verbessert werden können. Wir fanden heraus, dass dies daran liegt, dass die meisten wichtigen natürlichen Sprachen eine Leistungssteigerung erzielen, außer dass die Leistung in sieben Datensätzen auf Englisch sinkt und in drei Datensätzen steigt. Ich denke, dies wird als „Fluch der Multilingualität“ bezeichnet. Wir vergleichen auch die Leistungsunterschiede zwischen Sprachen. In dieser Abbildung ist die blaue Linie die Cross-linguale Few-shot-Übertragung. Die orangene Linie ist die Cross-linguale Zero-shot-Übertragung. Die grüne Linie ist die monolinguale Einstellung. Wir fanden heraus, dass durch den Vergleich der grünen und orangenen Linie der Leistungsunterschied in der Zero-shot-Einstellung erheblich ist, und durch den Vergleich der blauen und orangenen Linie fanden wir heraus, dass der Übertragungsunterschied mit der Few-shot-Einstellung schnell verringert wird. Wir haben auch andere interessante Erkenntnisse gefunden. Zum Beispiel übertrifft Encoder-Decoder frühere Arbeiten oder erzielt vergleichbare Ergebnisse. Das Prätraining auf der englischen natürlichen Sprache kann die Leistung der Few-shot-Übertragung auf Zielnatursprachen erheblich steigern, und wir fanden heraus, dass multilinguale Sprachmodelle wie Codex und BLOOM immer noch unzureichend für cross-linguale semantische Parsing-Aufgaben sind. Zusammengefasst haben wir XSemPLR erstellt, einen einheitlichen Benchmark für cross-linguales semantisches Parsing mit mehreren natürlichen Sprachen und Bedeutungsrepräsentationen. Wir haben eine umfassende Benchmark-Studie an drei repräsentativen Arten von multilingualen Sprachmodellen durchgeführt. Unsere Ergebnisse zeigen viele interessante Erkenntnisse. Und so weiter. Besuchen Sie unsere Arbeit und unser Code. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="171">Existing works can be broadly classified into four categories, but they either are not applicable to embedding as services or lack transferability.</sample>
    <sample id="172">Nein, Codex und BLOOM sind für CLSP (Cross-Lingual Semantic Parsing) noch nicht ausreichend.</sample>
    <sample id="174">Thea, eine der Autoren des Papers „ArgAnalysis35K: A large-scale dataset for Argument Quality Analysis“, erklärt, warum ihr Datensatz einzigartig ist. Im Gegensatz zu anderen Datensätzen, die oft durch Crowdsourcing entstanden und begrenzte Vielfalt und Tiefe aufweisen, bietet ArgAnalysis35K 35.000 hochwertige Argument-Analyse-Paare, die hauptsächlich aus Debatten von Experten und hochwertigen Turnieren stammen. Der Datensatz umfasst 24 thematische Bereiche, was zu einer höheren Vielfalt führt. Ein neuer Aspekt ist die Analyse, die nicht nur Behauptungen, sondern auch Prämisse und Verknüpfungen beinhaltet. Zudem wurde eine Instanz-basierte Annotator-Reliabilität eingeführt, um menschliche Voreingenommenheit zu reduzieren. Ein Relevanzmodell weist jedem Argument eine Relevanzpunktzahl zu, abhängig von seinem thematischen Kontext. ArgAnalysis35K ist somit ein vielfältiger, hochwertiger und besserer Datensatz für Argumentqualitätsanalysen.</sample>
    <sample id="175">Die Methode bewältigt die Mehrdeutigkeit der Permutationen, indem sie eine flexible, kontinuierliche Relaxierung verwendet, die es ermöglicht, die wahrscheinlichsten, sprachlich plausiblen Permutationen zu lernen, obwohl das Finden der optimalen Permutation NP-schwer ist.</sample>
    <sample id="176">Die Fairness eines nachgeschalteten NLP-Modells wird definiert durch die Gleichmäßigkeit seiner Leistung über verschiedene demografische oder politische Gruppen hinweg, insbesondere in Aufgaben wie Hassrede-Erkennung oder Fakenews-Erkennung. Ein faires Modell sollte keine systematischen Vorteile oder Nachteile gegenüber bestimmten Gruppen zeigen.</sample>
    <sample id="177">Der Referent heißt Yanis Labrak.</sample>
    <sample id="178">Der Referent heißt Koustav Sinha.</sample>
    <sample id="179">**Abstract:**  
Melanie Sclar presents SymbolicToM, an inference-time method to enhance Theory of Mind (ToM) reasoning in Large Language Models (LLMs). ToM involves understanding others' mental states, often tested through false-belief scenarios like the Sally-Anne test. Despite their capabilities, LLMs like GPT-3 struggle with such tasks. SymbolicToM employs explicit graphical representations—such as belief graphs—to model characters’ and their beliefs about others’ beliefs, enabling accurate reasoning about nested mental states. The method uses off-the-shelf NLI and OpenIE models to construct these graphs during inference, allowing efficient answering of ToM questions. Experiments on the ToMi dataset and novel out-of-domain benchmarks show significant improvements, with gains of up to 67 accuracy points for models like GPT-3. SymbolicToM outperforms supervised baselines, particularly in generalization tasks involving story structure and linguistic diversity. It provides interpretable reasoning and avoids overfitting, making it a plug-and-play solution for improving LLMs’ ToM capabilities. Results highlight its effectiveness across various scenarios, including complex false-belief questions and paraphrased prompts.</sample>
    <sample id="180">Der/die Referent*in heißt Myra.</sample>
    <sample id="181">**Abstract:**  
This paper introduces constrained language planning, a task that involves generating step-by-step scripts for specific goals with multi-faceted constraints, such as "make a chocolate cake." While large language models (LLMs) excel at planning abstract goals, their performance on constrained goals remains limited. We evaluate LLMs on this task and find that they struggle with constraint faithfulness despite acceptable semantic completeness. To address this, we propose an over-generate-then-filter method, which enhances script quality by generating multiple candidate scripts and selecting those that best align with constraints. Additionally, we create CoScript, a large-scale dataset of 55,000 constrained goals with corresponding scripts, validated by crowd-sourced workers. Our experiments show that models fine-tuned on CoScript, such as T5, outperform most LLMs in constrained language planning. This work establishes a new benchmark for constrained language planning and provides a valuable resource for advancing research in this area.</sample>
    <sample id="182">Im Zusammenhang mit dieser Arbeit bezieht sich Tropikalismus auf stereotype, oft verallgemeinernde und exotisierende Darstellungen von Menschen, insbesondere von Frauen aus lateinamerikanischen Ländern, die mit Begriffen wie "vibrant" und "curvaceous" assoziiert werden. Diese Darstellungen verknüpfen die Gruppe mit tropischen, exotischen Vorstellungen und tragen zur Essenzialisierung und Diskriminierung bei.</sample>
    <sample id="183">Die Autoren haben die von Menschen verfassten Beschreibungen der Zielgruppen durch eine Studie erstellt, bei der sie ähnliche Prompts wie für die Modellgenerierung an menschliche Teilnehmer weitergaben.</sample>
    <sample id="184">In dieser Arbeit wurde **Pointwise CXMI** zur Messung der Kontextnutzung verwendet. Es handelt sich um eine Erweiterung des CXMI-Maßes, das die Informationsgewinnung durch den Kontext für maschinelle Übersetzungen quantifiziert. Pointwise CXMI ermöglicht die Messung der Kontextnutzung auf Ebene von Satz oder Wort.</sample>
    <sample id="185">Der Unterschied zwischen DrBERT und ChuBERT liegt in den Datenquellen: DrBERT wird auf NACHOS-Daten (krawlerierte medizinische Webdaten) trainiert, während ChuBERT auf anonymisierten klinischen Daten aus dem Nantes University Hospital trainiert wird.</sample>
    <sample id="187">Zwei Autoren sind an der Arbeit beteiligt: Ying und Zhiyang.</sample>
    <sample id="188">Iteratives Transferlernen ist ein Ansatz, bei dem ein Modell zunächst durch das Übertragen von Gewichten aus verwandten Aufgaben initialisiert wird und anschließend schrittweise anhand von neu annotierten Daten weiterfeinjustiert wird. Dabei wird in jedem Schritt das Modell mit neuen Beispielen trainiert, um seine Leistung auf der Zielaufgabe zu verbessern. In diesem Kontext wird es verwendet, um bei der seltenen Klassenidentifikation (wie bei der Erkennung von kognitiver Dissonanz) mit geringer Annotationskosten besser zu starten und die Modellleistung zu steigern.</sample>
    <sample id="189">Das Ziel des Datensatzes ist, indirekte Referenzen in der Sprache von Nutzern zu verstehen, um bei der Auswahl zwischen Entities (z. B. Songs, Bücher, Rezepte) zu helfen.</sample>
    <sample id="190">Ein Angreifer kann Modellparameter über einen EaaS (Embedding as a Service) extrahieren, indem er Embeddings von vielen Eingaben anfordert und diese dann verwendet, um das zugrunde liegende Modell durch Techniken wie Gradientenabstieg oder andere Modellrekonstruktionsmethoden zu schätzen.</sample>
    <sample id="191">Die Arbeit ist von drei Autoren verfasst: Sara Papi, Matteo Negri und Marco Turchi.</sample>
    <sample id="192">Yang Luo stellt den Optimierer CAME (Confidence-guided Adaptive Memory Efficient Optimization) vor, der die Vorteile adaptiver Optimierungsmethoden mit geringerem Speicherbedarf kombiniert. Traditionelle adaptive Methoden wie Adam benötigen viel Speicher, während memory-efficient Optimierer wie Adafactor zwar weniger Speicher verwenden, aber oft langsamer konvergieren. CAME löst dieses Problem, indem es die Instabilität in den Momenten der Gradientenupdates durch eine Residualberechnung zwischen vorhergesagten und generierten Updates berücksichtigt. Dies ermöglicht eine adaptivere und stabiliere Optimierung. In Experimenten mit großen Sprachmodellen wie BERT, GPT-2 und T5 zeigt CAME eine deutliche Verbesserung der Validierungsgenauigkeit im Vergleich zu Adam und Adafactor, insbesondere bei großen Batch-Größen. Zudem reduziert CAME den Speicherbedarf erheblich, was die Effizienz bei der Trainierung großer Modelle erhöht. Die Ergebnisse bestätigen, dass CAME sowohl in der Konvergenzgeschwindigkeit als auch in der Speichereffizienz über bestehende Methoden hinausgeht und sich besonders gut für große Batch-Trainings eignet.</sample>
    <sample id="193">Die Anzahl der Annotatoren, die für die Erstellung des ursprünglichen Datensatzes verwendet wurden, wird im gegebenen Text nicht explizit genannt.</sample>
    <sample id="194">Die Autoren gehören der Carnegie Mellon University und der University of Washington sowie dem Allen Institute for AI an.</sample>
    <sample id="195">In dieser Arbeit wird das Framework RoHT (Reasoning over Hierarchical Question Decomposition Tree) vorgestellt, das auf das Problem der erklärbaren Fragebeantwortung (XQA) abzielt. XQA erfordert nicht nur eine Antwort, sondern auch eine Erklärung dafür. Die bestehenden Methoden, sowohl neuro-symbolische als auch basierend auf Zerlegung, haben Schwächen: Die erste kann nur auf strukturierten Wissensbasen arbeiten, während die zweite auf natürlichsprachlichen Textquellen angewiesen ist. RoHT kombiniert beides, indem es eine hierarchische Fragezerlegung in Form eines HQDT (Hierarchical Question Decomposition Tree) erstellt und probabilistisch über diesen Baum resoniert. Dabei werden für jede Unterverbundfrage passende Wissensquellen ausgewählt und die Antworten mit Wahrscheinlichkeiten aggregiert. Die Evaluierung auf den Datensätzen KQA Pro und Musique zeigt, dass RoHT die bestehenden Methoden deutlich übertrifft, insbesondere durch die Integration von Wissensbasen und Textkorpora. Die Ergebnisse bestätigen die Vorteile der expliziten Fragezerlegung und der Kombination heterogener Wissensquellen für die Beantwortung komplexer Fragen.</sample>
    <sample id="196">Das Beispiel mit dem Begrenzer auf der linken Seite ist: "I saw Bart and Lisa".</sample>
    <sample id="197">Der Stand der Technik für Dialogsysteme beinhaltet die Verwendung von menschlicher Bewertung, wie z. B. die Auswahl des besseren Gesprächs durch menschliche Gutachter oder die Bewertung von Gesprächen mit einer Likert-Skala, um die Gesamtqualität von Dialogen zu beurteilen.</sample>
    <sample id="198">Wir müssen die Akzeptanz der Modelle über das gesamte Kontextfenster bewerten, weil moderne Sprachmodelle über längere Kontextfenster verfügen und ihre Akzeptanzurteile dadurch möglicherweise von auftretenden Kontexten beeinflusst werden. Die bisherigen Evaluationsmethoden, die auf kurzen Sätzen basieren, erfassen dies nicht vollständig.</sample>
    <sample id="199">Ja, das mehrsprachige Training führte zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell, insbesondere in sieben der neun Datensätze, was als "Curse of Multilinguality" bezeichnet wird.</sample>
    <sample id="200">Ja, die Annotatoren kennen die Entitäten im Voraus, da ihnen Hintergrundinformationen zu den Entitäten bereitgestellt werden.</sample>
    <sample id="201">Für die Bewertung wurden state-of-the-art neuronale MT-Metriken sowie Ergebnisse einer expertenbasierten menschlichen Evaluation verwendet. Konkret wurden BLEURT-Punkte erwähnt.</sample>
    <sample id="202">Die Regression wirkt sich nicht auf bestimmte NER-Typen aus, da kein Abfall der Leistung aufgrund von Anpassungsüberanpassung beobachtet wurde.</sample>
    <sample id="203">Positionalität ist für NLP wichtig, weil sie systematische Unterschiede in der Leistung von Modellen zwischen Bevölkerungsgruppen aufzeigt, die durch die Perspektiven und Erfahrungen der Entwickler sowie die Zusammensetzung der Datensätze entstehen können.</sample>
    <sample id="204">Nein, es wurde keine Anpassung durch Adapter oder Feinabstimmung durchgeführt.</sample>
    <sample id="205">Shangbin, ein Doktorand der University of Washington, präsentierte die Arbeit „From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models“. Sprachmodelle werden auf großen Mengen an Webdaten trainiert, die politische Medien wie New York Times oder Huffington Post enthalten. Dies führt sowohl zu Vorteilen als auch zu Risiken: Während Modelle vielfältige Perspektiven lernen können, entstehen dadurch auch politische Verzerrungen, die in nachgelagerten Aufgaben zu Fairnessproblemen führen können. Die Studie untersucht, wie politische Vorurteile vom Trainingsdatensatz über Sprachmodelle in Anwendungen wie Hassrede- und Fakenews-Erkennung weitergeleitet werden. Ergebnisse zeigen, dass Modelle unterschiedliche politische Neigungen aufweisen und sich durch gezieltes Training auf partische Daten verändern können. Zudem spiegeln sie gesellschaftliche Polarisierung wider. In nachgelagerten Aufgaben zeigen linke Modelle bessere Leistungen bei der Erkennung von Hassrede gegen Minderheiten, rechte Modelle hingegen bei der Erkennung von Hassrede gegen Weiße oder Männer. Dies unterstreicht die Dringlichkeit, politische Voreingenommenheit in Sprachmodellen zu adressieren, um Fairness in NLP-Anwendungen zu gewährleisten. Die Forschung wirft zudem ethische Herausforderungen auf, wie die Balance zwischen Bias-Reduktion und Zensur.</sample>
    <sample id="206">Für das Transferlernen verwenden wir ein Modell, das zunächst auf verwandten Aufgaben wie der Klassifikation von Stellungnahmen in Debatten (debate) und der Klassifikation von Erweiterung- und Vergleichsbeziehungen (CE) aus dem PDTB feinabgestimmt wird. Das beste Ergebnis erzielten wir, indem wir zuerst die CE-Aufgabe und anschließend die Debatten-Aufgabe feinabstimmten.</sample>
    <sample id="207">Zur Bewertung der PaLM-Fähigkeiten wurden die aktuellen Testsets der WMT-Evaluation verwendet.</sample>
    <sample id="208">Die Autoren haben drei Empfehlungen vorgeschlagen.</sample>
    <sample id="209">Der Gewinn der vorgeschlagenen Methode gegenüber der stärksten Baseline beträgt 17.6%.</sample>
    <sample id="210">Der Referent heißt Shuheng.</sample>
    <sample id="211">Ja, die Ergebnisse und der Datensatz der Studie können als Benchmark für das Problem der automatischen Textvereinfachung in der Zukunft verwendet werden.</sample>
    <sample id="212">In der Arbeit wird mit einem kleineren Modell, T5, experimentiert.</sample>
    <sample id="213">The base model used for investigating multi-modal instruction tuning is OFA.</sample>
    <sample id="215">Adam Przepiórkowski diskutiert in seinem Vortrag die Abhängigkeitsstruktur von Koordinationen. Er vergleicht verschiedene Theorien, die unterschiedliche Ansätze zur Struktur von Koordinationen verfolgen: asymmetrische Modelle wie Universal Dependencies und Meaning-Text-Theorie, bei denen der erste Konjunkt als Kopf der Koordination gilt, sowie symmetrische Modelle wie die Prager Ansätze oder Hudsons Word Grammar, die entweder den Konjunktor oder alle Konjunkte als Kopf betrachten. Przepiórkowski argumentiert für eine symmetrische Struktur, basierend auf dem Prinzip der Minimierung der Abhängigkeitslänge. Er zeigt, dass kürzere Abhängigkeiten bevorzugt werden, und nutzt Daten aus dem Penn Treebank, um zu belegen, dass links stehende Konjunkte kürzer sind, insbesondere wenn der regierende Satzglied links steht oder fehlt. Diese Tendenz verschwindet jedoch, wenn der Regierende rechts steht. Dies unterstützt die These, dass asymmetrische Strukturen nicht ausreichen, um die beobachteten sprachlichen Phänomene zu erklären, während symmetrische Strukturen besser passen. Die Studie bietet somit einen neuen Argumentationsansatz für eine symmetrische Analyse von Koordinationen.</sample>
    <sample id="217">In ihrer Arbeit „Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation“ untersuchen Weihao Zeng und Kollegen die Herausforderungen der kontrollierten Dialoggenerierung mit mehreren Attributen. Bisherige Methoden konzentrieren sich meist auf einzelne Attribute und sind nicht für kontinuierliche oder zusammengesetzte Attribute geeignet. Die Forscher präsentieren DCG (Disentangled Controllable Generation), ein Modell, das durch eine Trennungsverlustfunktion verschiedene Attributkombinationen lernen kann. Sie entwickeln zudem MAE (Multi-Attribute Evaluation), ein einheitliches, referenzfreies Bewertungssystem für verschiedene Attributgranularitäten. DCG nutzt zwei Arten von Prompts – attributorientiert und taskorientiert –, um die Kontrollbarkeit und Textqualität zu verbessern. Experimente an zwei Benchmarks zeigen, dass DCG in der Kompositionsgeneralisierung über bisherige Baselines hinausragt. Die Ergebnisse bestätigen, dass das Modell in der Lage ist, gelernte Attribute auf bisher ungekannte Kombinationen zu generalisieren. MAE korreliert gut mit menschlichen Bewertungen und ist auf verschiedene PLMs anwendbar. Die Arbeit trägt damit zur Verbesserung der kontrollierten Dialoggenerierung bei, insbesondere bei der Verarbeitung mehrerer Attribute.</sample>
    <sample id="218">Die Autoren gehören der Google Translate-Gruppe an, also nicht einer Universität.</sample>
    <sample id="219">Jia-Huei Ju präsentierte die Arbeit „A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports“, die von Yu-Shiang Huang, Cheng-Wei Lin sowie Professoren Che Lin und Chuan-Ju Wang durchgeführt wurde. Ziel der Studie ist es, relevante finanzielle Informationen aus jährlichen Form 10-K-Berichten automatisiert zu extrahieren. Da diese Berichte sich jährlich stark ähneln, wurde ein Highlighting-Task entwickelt, um Unterschiede und Ähnlichkeiten zwischen Berichten aufzudecken. Der Ansatz besteht aus einem mehrstufigen Pipeline, der Relationserkennung und feine Tuning-Phasen umfasst. Zunächst werden die Berichte segmentiert, dann werden Relationen zwischen dem aktuellen (Target) und dem vorherigen Jahr (Reference) klassifiziert. Anschließend wird das Modell mit externen Daten (z. B. eSNLI) und domänenspezifischen Daten (FINAL-Dataset) weitertrainiert. Die Evaluierung zeigt, dass das entwickelte Modell sowohl auf dem FINAL-Dataset als auch auf dem eSNLI-Dataset beste Ergebnisse erzielt. Besonders gut schneidet es bei der Erkennung von Mismatched-Paaren ab, die nicht im Training verwendet wurden. Die Arbeit bietet eine neue Methode zur Analyse finanzieller Berichte und legt den Grundstein für zukünftige Erweiterungen im Bereich Information Retrieval.</sample>
    <sample id="220">The authors are affiliated with Stony Brook University.</sample>
    <sample id="221">In der Arbeit wurden die Sprachpaare Deutsch-Englisch untersucht.</sample>
    <sample id="222">In diesem Werk wird untersucht, wie Open-Domain-Question-Answering-Modelle auf neue Domänen wie das Biomedizinische Feld generalisieren können. Die Modelle, trainiert auf allgemeinen Daten wie Wikipedia, haben Schwierigkeiten, wenn sie auf spezialisierte Domänen angewendet werden. Durch Dateninterventionen wie few-shot und zero-shot-Methoden wird versucht, diese Generalisierung zu verbessern. Bei few-shot werden wenige Beispiele aus dem Zielbereich verwendet, um mehr Daten zu generieren, während bei zero-shot keine Zielbeispiele vorhanden sind. Die Ergebnisse zeigen, dass Retriever- und Reader-Modelle durch solche Interventionen um bis zu 8 bzw. 11 Prozent verbessert werden können. Zudem wird ein Rahmen zur Identifizierung des Datenshifts zwischen Quell- und Zielbereich vorgestellt, wobei vier Typen unterschieden werden: keine, konzeptionelle, kovariante und vollständige Shifts. Mittels einer Kompatibilitätsmessung lassen sich Ziel-Datensätze auf diesem 2D-Grid einordnen. Die Analyse zeigt, dass few-shot-Interventionen in fast allen Fällen nützlich sind, während zero-shot besonders bei konzeptionellen und kovarianten Shifts wirksam ist. Die Studie unterstreicht, dass gezielte Dateninterventionen je nach Art des Datenshifts unterschiedlich effektiv sind und die Generalisierungsfähigkeit von QA-Modellen erheblich verbessern können.</sample>
    <sample id="223">Der Referent heißt Shangbin.</sample>
    <sample id="224">long-mBART und base mBART</sample>
    <sample id="225">53 Aufgaben werden für das Training verwendet und 5 zusätzliche Aufgaben werden für die Tests verwendet.</sample>
    <sample id="226">Zwei Autoren sind an der Arbeit beteiligt: Regina Stodden und Omar.</sample>
    <sample id="227">Die aktuelle Forschung zu Sprachmodellen hat zwar bedeutende Fortschritte gemacht, doch fehlt es ihnen oft an einer sogenannten „grounded language understanding“ – der Fähigkeit, natürliche Sprache in eine umsetzbare Aktion oder einen Plan innerhalb eines spezifischen Umfelds zu übersetzen. Dies ist besonders in Anwendungen wie intelligenten Assistenten, medizinischen Datenbankabfragen oder Robotik relevant. Das Problem liegt darin, dass die meisten Modelle während der Vortrainierung nicht mit konkreten Umgebungen in Verbindung stehen. Die gängige Herangehensweise, Sprachmodelle zur direkten Generierung von Programmen oder Plänen zu nutzen, führt oft zu fehlerhaften oder ungültigen Ergebnissen. Die Studie präsentiert ein neues Framework namens Pangu, das Sprachmodelle nicht zur Generierung, sondern zur Diskriminierung von Kandidatenplänen einsetzt, die von einem symbolischen Agenten vorgeschlagen werden. Dies reduziert das Risiko von Fehlern und verbessert die Validität. Pangu erzielt in verschiedenen Settings, einschließlich der feinabgestimmten Lernmethoden und in-context Learning, überzeugende Ergebnisse, insbesondere bei der Fragebeantwortung in Wissensbasen. Die Forscher betonen, dass Diskriminierung eine effektivere Strategie für „grounded language understanding“ sein könnte als Generierung.</sample>
    <sample id="228">Die Autoren haben an den Datensätzen AG News, MIND, SST2 und Enron Spam experimentiert.</sample>
    <sample id="229">Gabriella Skitalinskaya and Henning Wachsmuth present research on detecting improvable claims in argumentative writing. They introduce two tasks: Suboptimal-Claim detection, which identifies whether a claim needs revision, and Claim Improvement Suggestion, which identifies what aspects of the claim should be improved. The study explores using revision histories from collaborative debate platforms like Kialo to model argument quality based on implicit revision patterns. They identify four main challenges: ensuring the representativity and reliability of revision data, choosing the right model architecture sensitive to small changes, handling contextual dependencies in argument quality, and addressing topical and user biases in revision histories. Their experiments show that revision-based data can effectively support the tasks, and modeling the distance between claim versions helps detect suboptimal claims. The impact of context depends on the specific task and quality issues. The paper provides a detailed analysis and comparison of approaches, offering insights for improving argumentative writing support systems.</sample>
    <sample id="231">NACHOS ist ein Datensatz medizinischer Daten, der aus dem Web gesammelt wurde und als Grundlage für das Training des Modells DrBERT dient.</sample>
    <sample id="232">Der Referent heißt David Vilar.</sample>
    <sample id="233">**Abstract:**  
This paper introduces EDAtt, a novel strategy for simultaneous speech translation (SimulST) that leverages the cross-attention mechanism of pre-trained offline speech translation models without requiring retraining or specialized architectures. Current SimulST approaches often involve complex training procedures, multiple models for different latency regimes, and additional modules, which complicate deployment. EDAtt simplifies this by using a single model for all latency settings and controlling output timing through attention-based thresholds. The method emits partial translations when the cross-attention weights over the last λ speech frames fall below a threshold α, indicating sufficient stability of the input information. Experimental results on German-to-English translation show that EDAtt outperforms existing SimulST strategies and offline-based methods like Wait-k and Local Agreement, achieving higher BLEU scores and lower latency. The approach also demonstrates superior performance in computational-aware latency measures. The code, models, and outputs are publicly available to support reproducibility.</sample>
    <sample id="234">Die Prompt-Strategie hat einen erheblichen Einfluss auf die Ergebnisse der Übersetzung. Unterschiedliche Prompting-Methoden können die Leistung von Large Language Models (LLMs) wie PaLM deutlich beeinflussen, insbesondere bei zero- und one-shot Prompting. Die Qualität der Beispiele in der Prompting-Strategie ist entscheidender als die Ähnlichkeit zum Quelltext. Eine gute Prompt-Strategie, wie die Verwendung von hochwertigen Beispielübersetzungen aus dem dev-Datensatz, kann die Übersetzungsqualität erheblich verbessern.</sample>
    <sample id="235">Die Autoren gehören der Carnegie Mellon University an.</sample>
    <sample id="236">Die 5 Anweisungen der Expert*innen sind in der Präsentation nicht explizit genannt. Es wird jedoch erwähnt, dass jede Aufgabe in der MultiInstruct-Datenbank mit fünf von Expert*innen verfassten Anweisungen ausgestattet ist. Die genauen Formulierungen der Anweisungen sind nicht im gegebenen Text enthalten.</sample>
    <sample id="237">Die Autoren schlagen vor, Modelle mit einem diagnostischen Test-Suit namens KITMUS zu testen, der speziell entwickelt wurde, um die Fähigkeit zur Integration von Wissen aus verschiedenen Quellen (vorabtrainiertes Wissen und Wissen zur Laufzeit) zu prüfen.</sample>
    <sample id="238">Yebowen Hu von der University of Central Florida präsentiert MeetingBank, ein neues Benchmark-Datenset für die Entwicklung von Zusammenfassungstechnologien. Das Datenset besteht aus 1.366 Stadtratsversammlungen mit Transkripten, Referenzzusammenfassungen und weiteren Ressourcen. Die Erstellung war mit zwei Herausforderungen verbunden: die Erzielung hochwertiger Zusammenfassungen und die Identifizierung vertrauenswürdiger öffentlicher Quellen. MeetingBank wurde mithilfe von Speechmatics API erstellt und enthält statistische Daten zu Dauer, Anzahl der Sprecher und Token pro Meeting. Für die Evaluation wurden verschiedene Extraktive und abstraktive Modelle getestet, darunter BART-Large, DialogLM und GPT-3. Obwohl GPT-3 in der menschlichen Bewertung in Flüssigkeit und Kohärenz am besten abschnitt, lag es in Bezug auf Informativität und Faktenhaltigkeit hinter anderen Modellen. Die Studie unterstreicht die Notwendigkeit besserer automatischer Bewertungsmetriken, die besser mit menschlichen Präferenzen übereinstimmen. MeetingBank soll Forschern helfen, fortschrittliche Meeting-Zusammenfassungssysteme zu entwickeln und bietet Einblicke in Entscheidungsprozesse von Stadträten.</sample>
    <sample id="239">Hallo zusammen, mein Name ist David Vilar, und ich werde eine kurze Zusammenfassung des Papers „Prompting PaLM für die Übersetzung: Bewertung von Strategien und Leistung“ geben. Dieses Werk entstand in Zusammenarbeit mit Kollegen aus Google Translate. PaLM ist ein großes Sprachmodell mit 540 Milliarden Parametern, das letztes Jahr, 2022, vorgestellt wurde. Es wurde auf einer großen Textsammlung mit 780 Milliarden Tokens trainiert. Zu Veröffentlichungszeit erreichte es die state-of-the-art-Leistung in hunderten NLP-Aufgaben. In dieser Arbeit präsentieren wir die erste systematische Untersuchung von Prompting-Strategien für große Sprachmodelle im Bereich der maschinellen Übersetzung. Wir haben die Übersetzungsfähigkeit solcher Modelle mithilfe der besten Praktiken der MT-Community getestet. Dazu gehört, die neuesten Testdaten zu verwenden, um eine Überlappung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden. Zudem haben wir die Ergebnisse mit state-of-the-art-Systemen verglichen, also den besten Systemen, die im WMT-Evaluationstest bewertet werden. Wir haben moderne, neuronale MT-Metriken verwendet und zusätzlich auch Ergebnisse aus menschlichen Evaluierungen präsentiert. Schließlich geben wir einige Empfehlungen für die Auswahl von Prompting-Strategien. Das Prompting hat einen großen Einfluss auf die Leistung der LLMs bei der Übersetzung, wie wir in einem einfachen Experiment sehen können, bei dem wir ein one-shot-Prompting verwendet haben und zwei unterschiedliche Prompts für jeden Satz bereitgestellt haben. Bei 516 von 1.000 Sätzen wurde ein Unterschied von mehr als einem BLEURT-Punkt beobachtet. In extremen Fällen können diese Unterschiede sogar bis zu 40 BLEURT-Punkte erreichen. Daher ist es wichtig, eine gute Prompting-Strategie zu wählen. In unseren Experimenten haben wir uns für eine 5-shot-Prompting-Strategie entschieden, bei der wir jeden Satz, den wir dem System geben, mit der Sprache markiert haben, in der er vorliegt. In diesem Beispiel, bei dem wir eine Übersetzung von Deutsch ins Englische durchführen, sind die Quellsätze mit einem „Deutsch:“-Zeichen markiert und die englischen Übersetzungen mit einem „English:“-Zeichen. Wir haben festgestellt, dass die tatsächliche Form des Promptings bei mehreren kurzen Promptings kaum einen großen Einfluss hat. Es ist jedoch entscheidend bei zero- und one-shot-Prompting. Wenn wir, wie in unserem Fall, zu einem 5-shot-Prompting übergehen, gibt es praktisch keinen Unterschied zur tatsächlichen Form des Promptings. Die Beispiele tragen den größten Teil des Gewichts. Die Zusammenfassung unserer experimentellen Ergebnisse lautet, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit zum Quellsatz. Daher ist es wichtig, Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die Auswahl von Prompts aus dem Trainingsdatensatz für die WMT-Evaluierungen anhand der Dev-Daten. Die Dev-Daten sind deutlich besser curatiert und von höherer Qualität als die Trainingsdaten, die tendenziell lauter sind. Die Ergebnisse zeigen, dass die Leistung besser ist, wenn man die Dev-Daten verwendet. Dennoch haben spezialisierte, state-of-the-art-Systeme einen erheblichen Vorteil gegenüber den PaLM-Übersetzungen. PaLM kommt jedoch ziemlich nahe an ein kommerzielles System heran. In unserem Fall haben wir uns entschieden, Google Translate als Referenz zu verwenden. Die Erkenntnisse aus der menschlichen Evaluation, die wir mit dem MQM-Framework durchgeführt haben, zeigten, dass die Fluenz von PaLM mit den state-of-the-art-Systemen vergleichbar ist, doch der Hauptunterschied liegt in der Genauigkeit. Insbesondere sind die häufigsten Fehler Omissionen. Es scheint, dass PaLM eine besser klingende Übersetzung produziert, indem es manchmal Teile des Quellsatzes weglässt, die im Übersetzungsschritt nicht berücksichtigt werden. Die Kategorie „Stil/Unbeholfen“ bei PaLM ist niedriger als bei den state-of-the-art-Systemen, was ein weiteres Zeichen dafür ist, dass PaLM wirklich flüssige Ausgaben liefert, aber dennoch Probleme mit der Genauigkeit hat. Das war die kurze Zusammenfassung. Für weitere Details kommen Sie bitte zur vollständigen Präsentation des Papers. Vielen Dank.</sample>
    <sample id="240">Hallo, ich bin Dawei, ein Doktorand an der Universität des Saarlandes in Deutschland. In diesem Video möchte ich unsere kürzliche Arbeit „Weaker Than You Think: A Critical Look at Weakly Supervised Learning“ vorstellen. Diese Arbeit entstand gemeinsam mit Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow. Ich möchte mit einer kurzen Einführung in das schwache Labeling und das schwach überwachte Lernen beginnen. Bei schwachem Labeling werden die Daten nicht manuell annotiert. Stattdessen verwenden wir schwache Labeling-Quellen, wie einfache heuristische Regeln, Wissensbasen oder niedrigwertige Crowdsourcing-Annotationen, wie in der Abbildung rechts dargestellt. Im Vergleich zu menschlichen Annotationen sind diese schwächeren Annotationen zwar viel günstiger, doch sie sind auch lauter, was bedeutet, dass ein Teil der Annotationen falsch ist. Wenn man neuronale Netzwerke direkt mit schwach annotierten Daten trainiert, neigen diese dazu, die Label-Rauscherei zu memorieren und nicht allgemein zu lernen. In schwach überwachten Lernmethoden werden Trainingsalgorithmen vorgeschlagen, um neuronale Netzwerke unter solchem Label-Rauschen robust zu trainieren, sodass die trainierten Modelle dennoch gut generalisieren. In jüngeren Arbeiten zum schwach überwachten Lernen (WSL) wird häufig behauptet, dass Modelle nur mit schwach annotierten Daten trainiert werden und dennoch eine hohe Leistung auf sauberen Testdaten erzielen. Technisch gesehen ist diese Aussage nicht falsch, doch es gibt eine Falle, nämlich die Annahme, dass zusätzlich eine saubere Validierungsdatenmenge für die Modellauswahl vorhanden ist. Wir können uns nicht damit begnügen, aber dies bedeutet, dass zusätzliche manuelle Annotationen in der schwach überwachten Lernmethode erforderlich sind. Doch wie ein Elefant im Raum wird diese Notwendigkeit oft übersehen. Die oben genannte Skepsis führt zu drei Forschungsfragen. Erstens: Ist eine saubere Validierungsdatenmenge für WSL notwendig, oder könnte man stattdessen eine laute Validierungsdatenmenge verwenden? Zweitens: Wenn saubere Daten erforderlich sind, oder wenn saubere Daten für WSL zwingend notwendig sind, wie viele saubere Stichproben benötigen wir dann? Drittens: Sollten wir die sauberen Stichproben nur zur Validierung verwenden, oder gibt es bessere Wege, sie zu nutzen? In unserer Arbeit haben wir diese Forschungsfragen beantwortet, und unsere Ergebnisse sind wie folgt. Erstens haben wir festgestellt, dass interessanterweise moderne WSL-Methoden tatsächlich saubere Validierungsbeispiele benötigen, um richtig zu funktionieren. Andernfalls kommt es zu einem großen Leistungsverlust. Wie in dieser Abbildung gezeigt, wenn keine sauberen Validierungsbeispiele vorhanden sind, können die trainierten Modelle nicht über die ursprünglichen schwachen Labels hinaus generalisieren, was bedeutet, dass das Training sinnlos ist. Dies zeigt, dass WSL-Methoden tatsächlich sauber annotierte Daten benötigen, um richtig zu funktionieren, und die Annotierungskosten für die Erzielung sauberer Validierungsbeispiele sollten nicht übersehen werden. Unsere zweite Erkenntnis ist, dass das Erhöhen der Anzahl an sauberen Validierungsbeispielen die Leistung von WSL-Methoden verbessert, wie in der Abbildung links gezeigt. Typischerweise benötigen wir nur 20 Beispiele pro Klasse, um eine hohe Leistung zu erzielen. Doch das ist nicht das Ende der Geschichte, denn wenn wir dennoch entscheiden, saubere Beispiele zu verwenden, dann wird das direkte Training darauf sogar noch bessere Leistungen erzielen. Die Abbildung rechts zeigt den Leistungsunterschied zwischen Feinabstimmungsansätzen, die direkt auf sauberen Daten angewendet werden, und WSL-Methoden, die saubere Daten nur zur Validierung verwenden. Wie wir sehen können, beginnt Feinabstimmung mit 10 Beispielen pro Klasse bereits die WSL-Methoden zu übertreffen. Schließlich kann der Leistungsgewinn, der in früheren WSL-Methoden behauptet wurde, leicht erreicht werden, indem man die Feinabstimmung auf sauberen Validierungsbeispielen fortsetzt. Wie aus den Abbildungen hervorgeht, leistet das Standardmodell, bezeichnet als FTw, ursprünglich schlechter als komplexere WSL-Methoden wie COSINE. Wenn wir jedoch erlauben, die Feinabstimmung auf sauberen Beispielen fortzusetzen, dann leistet FTw genauso gut wie andere Methoden. In der Praxis gibt es also keinen Grund, komplexere WSL-Methoden zu wählen, die mehr Rechenzeit und Speicherplatz benötigen. Zusammengefasst haben wir gezeigt, dass moderne WSL-Methoden saubere, manuell annotierte Stichproben benötigen, um richtig zu funktionieren. Ihre Leistungssteigerung und Praktikabilität werden stark überschätzt. Unsere konkreten Empfehlungen für zukünftige Arbeiten sind wie folgt. Erstens: Die Kriterien für die Modellauswahl sollten berichtet werden. Zum Beispiel, ob die Modellauswahl über saubere Validierungsdaten erfolgt. Zweitens: WSL-Methoden sollten mit Baselines des Few-Shot-Lernens verglichen werden, da beide auf sauberen Daten arbeiten. Drittens: Die kontinuierliche Feinabstimmung ist eine einfache, aber starke Baseline, die in zukünftigen Arbeiten zum WSL berücksichtigt werden sollte. Schließlich haben wir unseren Code open-sourced. Sie können ihn über den QR-Code auf dieser Folie finden. Bitte prüfen Sie ihn gerne. Vielen Dank und viel Spaß auf dem Kongress.</sample>
    <sample id="241">Ethan präsentiert das Paper „Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments“, das in Zusammenarbeit mit Yang Chen, Wei Xu und Alan Ritter an der Georgia Tech entstand. Es thematisiert die Schwächen bestehender Systeme zur automatischen Erkennung von Fehlinformationen, insbesondere die unrealistische Bewertung durch retrospektive Datensätze und das Risiko von geleaktem Gegenbeweis. Zudem fehlt oft die Einbindung von Menschen in den Prozess. Das Paper schlägt ein human-in-the-loop Framework vor, das Systeme realistischer bewertet, indem es menschliche Rückmeldungen während des gesamten Prozesses einbezieht. Das entwickelte System besteht aus zwei Komponenten: Der erste Schritt beinhaltet die Erkennung von irreführenden Behauptungen durch Keyword-Filter und ein T5-Modell zur Extraktion von Behauptungen, die von Menschen verifiziert werden. Der zweite Schritt prüft, ob die Behauptungen gegen soziale Medien-Richtlinien verstoßen, wofür ein BERT-basiertes Modell zur Stellungnahme-Erkennung genutzt wird. Die Evaluation zeigt, dass das System 65 % der Richtlinienverstöße erkennt und 124,2 Verstöße pro Stunde menschlicher Arbeit identifiziert. Das Framework bietet eine realistische Bewertung von Systemen, die menschliche Moderatoren einbeziehen, und motiviert zukünftige Forschung in diesem Bereich.</sample>
    <sample id="242">Gängige Bewertungsmethoden für Dialogsysteme sind menschliche Bewertungen, wie z. B. die Auswahl des besseren Dialogs durch menschliche Gutachter oder die Bewertung von Dialogen mit einer Likert-Skala. Zudem werden pauschale Vergleiche auf Dialogebene durchgeführt.</sample>
    <sample id="243">An der Arbeit sind 6 Autoren beteiligt.</sample>
    <sample id="244">Das benötigte Hintergrundwissen im Beispiel mit Servin und Kea ist: "Juristen entscheiden in Gerichten über Fälle."</sample>
    <sample id="245">In ihrer Präsentation stellt Lining Zhang die Arbeit „A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization“ vor. Ziel war es, eine effiziente Methode zur Identifizierung von MTurk-Arbeitern mit hoher Übereinstimmung (IAA) bei der Textzusammenfassung zu entwickeln. Dazu wurde ein zweistufiger Pipeline-Ansatz eingesetzt: Zuerst eine Qualifikationsaufgabe, um die Fähigkeit der Arbeiter zu beurteilen, und anschließend eine Ausdauerprüfung, um die Arbeitsbelastungstoleranz zu testen. Aus 200 Teilnehmern wurden letztendlich 12 Arbeiter (4 Gold, 8 Silber) ausgewählt, die eine höhere IAA als Experten erzielten. Im Vergleich zu Baseline- und CloudResearch-Methode zeigte der Pipeline-Ansatz eine ähnliche Qualität bei geringerem Aufwand. Die Analyse zeigte zudem, dass Pipeline- und CloudResearch-Arbeiter eine signifikante Korrelation in der Richtigkeit ihrer Annotationen aufwiesen, während der Pipeline keine Garantie für Richtigkeit bietet. Die Arbeit bietet eine kosteneffiziente Lösung für großskalige Annotationen mit hoher Übereinstimmung, hat jedoch Grenzen, wie beispielsweise die Begrenzung auf englische Zusammenfassungen und fehlende Garantie für Richtigkeit. Zukünftige Forschung wird sich auf die Erweiterung auf andere Sprachen, Aufgaben und Plattformen konzentrieren.</sample>
    <sample id="246">Ja, der Code ist verfügbar und kann auf GitHub heruntergeladen werden.</sample>
    <sample id="247">Jiho Kim von KAIST AI präsentiert das Paper „FACTKG: Fact Verification via Reasoning on Knowledge Graphs“. Bisherige Fact-Verification-Datasets wie FEVER oder TabFact nutzen Text oder Tabellen als Evidenz, nicht jedoch Knowledge Graphs. Das Team stellt daher eine neue Aufgabe vor: Fact-Verification basierend auf Knowledge Graphs. KGs sind besonders nützlich, da sie direkte, zuverlässige Beziehungen zwischen Entitäten darstellen und in praktischen Anwendungen wie Dialogsystemen eingesetzt werden können. Dazu wurde das Dataset FactKG erstellt, das auf DBpedia basiert und zwei Stile von Aussagen (geschrieben und umgangssprachlich) beinhaltet. Es umfasst fünf Arten von Schlussfolgerungen: one-hop, conjunction, existence, multi-hop und negation. Die Evaluation zeigt, dass Modelle, die Graph-Evidenz nutzen, wie der GEAR-Modell, die Leistung deutlich verbessern. Die Baselines, die nur die Aussagen verwenden, schneiden schlechter ab. Das Paper bietet somit eine neue Richtung für Fact-Verification mit Knowledge Graphs und demonstriert die Effektivität der Methode.</sample>
    <sample id="248">Nein, die Annotatoren für NLPositionality sind nicht in Bezug auf jede demographische Gruppe (wie Land, Geschlecht usw.) vollständig ausgewogen. Die Studie zeigt, dass bestimmte Gruppen, wie z. B. Menschen mit Hochschulbildung oder aus englischsprachigen Ländern, stärker vertreten sind, während andere Gruppen, wie nicht-binäre Personen, unterrepräsentiert sind.</sample>
    <sample id="249">Die Sätze innerhalb der akzeptablen Domain wurden durcheinander gebracht, indem relevante Strukturen beibehalten und Störungen (Noise) hinzugefügt wurden.</sample>
    <sample id="250">Eine dimensionale Bewertung bedeutet, dass verschiedene Aspekte der Qualität eines Dialogmodells einzeln untersucht werden, um dessen Stärken und Schwächen detaillierter zu bewerten, anstatt nur eine allgemeine Gesamtbewertung vorzunehmen.</sample>
    <sample id="251">Die Autoren gehören der University of Science and Technology of China an.</sample>
    <sample id="252">In dieser Präsentation stellen Sai Kiran Tanikella und seine Kollegen U-CREAT vor, ein unsupervisedes Retrieval-System für vorherige Gerichtsurteile (Prior Case Retrieval) im rechtlichen Bereich. Das System basiert auf der Extraktion von Ereignissen aus juristischen Dokumenten und zielt darauf ab, relevante vorherige Fälle effizient zu identifizieren. Die Arbeit bringt zwei bedeutende Beiträge mit: den IL-PCR-Datensatz, einen neuen Benchmark für Prior Case Retrieval mit 7.070 indischen Rechtsfällen, und den U-CREAT-Pipeline, der ein ereignisbasiertes Retrieval-Verfahren ohne spezifische Anpassung an Rechtsgebiete oder Demografie ermöglicht. Die Ereignisse werden durch Abhängigkeitsparsen extrahiert und als Subjekt-Verb-Objekt-Tripel dargestellt. Die Ergebnisse zeigen, dass ereignisbasierte Modelle, insbesondere das Event Filtered Docs-Modell, die Baseline-Methoden deutlich übertreffen. U-CREAT erzielt auch auf dem COLIEE-Datensatz bessere Ergebnisse als bisherige Ansätze, einschließlich des neuesten überwachten MTFT-BERT-Modells. Die Arbeit unterstreicht die Vorteile eines ereignisbasierten Ansatzes für die Prior Case Retrieval-Aufgabe und eröffnet neue Möglichkeiten für zukünftige Forschung in diesem Bereich.</sample>
    <sample id="253">Mario Ezra Aragón präsentiert die Arbeit „DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media“. Das Projekt, ein Gemeinschaftsprojekt mexikanischer und spanischer Forscher, zielt darauf ab, mentale Störungen anhand von sozialen Medieninhalten zu erkennen. Dazu wird ein Modell vorgeschlagen, das durch Doppel-Domain-Adaptation und geleitete Maskierung trainiert wird, um soziale Medien-Sprache zu verstehen und sich auf das Thema psychischer Erkrankungen zu spezialisieren. DisorBERT nutzt Wissen aus allgemeinen Sprachmodellen wie BERT und passt es an, um spezifische Begriffe und Kontexte im Bereich psychischer Gesundheit zu erkennen. Die Ergebnisse zeigen, dass DisorBERT im Vergleich zu Basismodellen eine bessere Balance zwischen Präzision und Erkennung erzielt. Die Analyse von Textmustern aus dem Beck Depression Inventory zeigt, dass DisorBERT eher negative oder psychologisch relevante Wörter vorhersagt. Die Visualisierung von Aufmerksamkeitswerten bestätigt, dass Begriffe wie „angst“ und „Medikamente“ als besonders wichtig identifiziert werden. Die Studie unterstreicht die Effektivität von DisorBERT und weist den Weg für zukünftige Arbeiten mit klinischen Daten und weiteren Lexika.</sample>
    <sample id="254">In dieser Arbeit wird ein Framework für die Dokumentebene-Relationsextraktion vorgestellt, das durch Unsicherheitsgesteuerte Etikettendenoisierung (Uncertainty-Guided Label Denoising) die Qualität von weit verbreiteten, aber lauten Daten verbessert. Traditionelle Methoden zur Relationsextraktion benötigen große Mengen an manuell annotierten Daten, was zeitaufwendig ist. Stattdessen werden oft weit überwachte Daten (distant supervision) verwendet, die jedoch viele Fehler enthalten. Um dies zu reduzieren, wird eine Vor-Entfernungsmethode mit pseudo-Labels eingesetzt, die jedoch ebenfalls Fehler einführen können. Die Arbeit präsentiert eine Methode zur Unsicherheitsabschätzung, um die Vertrauenswürdigkeit von Modellvorhersagen zu bestimmen. Dabei wird eine Instanz-basierte Unsicherheitsbewertung für überlappende Relationen eingeführt, um falsch positive Etiketten besser zu erkennen. Zusätzlich wird eine dynamische Klassenunsicherheitsschwelle und eine mehrphasige Trainingsstrategie verwendet, um die Leistung weiter zu steigern. Die Ergebnisse zeigen, dass das Framework auf öffentlichen Datensätzen die Leistung der Baselines deutlich übertrifft. Die Hauptbeiträge sind: (1) das Framework zur Etikettendenoisierung, (2) die Instanz-basierte Unsicherheitsbewertung für überlappende Relationen, (3) die iterative Re-Etikettierung mit dynamischen Schwellenwerten und (4) die deutliche Leistungsverbesserung im Vergleich zu bestehenden Methoden.</sample>
    <sample id="255">Die Form des Prompts ist wichtig bei zero- und one-shot Prompting. Bei five-shot Prompting spielt die Form kaum eine Rolle.</sample>
    <sample id="257">Die Autoren haben vier state-of-the-art-Dialogmodelle evaluiert.</sample>
    <sample id="258">In this video, Chiang Cheng-Han presents a new study titled "Can Large Language Models Be an Alternative to Human Evaluation?" The research explores using large language models (LLMs) to evaluate the quality of natural language text, such as stories generated by GPT-2 or written by humans. The LLMs are given instructions and samples to rate based on attributes like grammar, coherence, likability, and relevance. The team compared LLM ratings with human evaluations conducted by English teachers, who are considered experts in scoring essays. While some smaller LLMs did not show a clear preference for human-written stories, larger models like Davinci and ChatGPT demonstrated preferences similar to humans. This suggests that certain LLMs can serve as viable alternatives to human evaluation. The paper addresses various questions, including agreement between LLMs and humans, the impact of instruction wording, and the benefits and costs of using LLM evaluation. The full findings are detailed in the paper, and viewers are encouraged to read it or visit the poster at ACL.</sample>
    <sample id="259">Yusen Zhang von der Penn State University präsentiert XSemPLR, ein neues Benchmarking-System für das cross-linguale Semantic Parsing. Ziel ist es, Abfragen in verschiedenen natürlichen Sprachen in mehrere Bedeutungsrepräsentationen wie SQL oder Lambda-Kalkül zu übersetzen. Bisherige Modelle sind oft auf spezifische Sprachen oder Repräsentationen beschränkt, wodurch Lücken wie fehlende Unterstützung für Chinesisch oder Lambda-Kalkül entstehen. XSemPLR adressiert dies mit einem umfassenden Datensatz, der 22 Sprachen, 8 Bedeutungsrepräsentationen und 9 Datensätze umfasst. Zur Bewertung werden sechs verschiedene Trainings- und Evaluierungssettings getestet, darunter monolinguale, multilinguale Modelle sowie Zero- und Few-shot-Übertragungen. Die Ergebnisse zeigen, dass Encoder-Decoder-Modelle wie mT5 die besten Leistungen erbringen. Zudem wird der „Curse of Multilinguality“-Effekt beobachtet, bei dem die Leistung in englischen Datensätzen bei multilingualem Training sinkt. Cross-linguale Few-shot-Übertragungen reduzieren den Leistungsabstand erheblich. Multilinguale Modelle wie Codex oder BLOOM erweisen sich als unzureichend. XSemPLR bietet somit eine umfassende Plattform zur Bewertung von Modellen im cross-lingualen Semantic Parsing.</sample>
    <sample id="260">Die Anzahl der Autoren wird im Text nicht genannt.</sample>
    <sample id="261">Ein guter Planer sollte Skripte erstellen, die sowohl sinnvoll sind als auch die gegebenen Einschränkungen忠实ly (treu) befolgen.</sample>
    <sample id="262">Die Anzahl der Autoren wird im Text nicht genannt.</sample>
    <sample id="263">In diesem Werk untersuchen die Autoren die Problemstellung von Label-Bias in der In-Context Learning (ICL) bei großen Sprachmodellen. Sie identifizieren drei Arten von Label-Bias: Vanilla-Label-Bias, Context-Label-Bias und den neu eingeführten Domain-Label-Bias, der durch das Task-Korpus beeinflusst wird. Mittels Experimenten zeigen sie, dass das Erscheinungsbild von in-domain-Wörtern starken Bias in den Vorhersagen der Modelle verursachen kann. Aufgaben mit hohem Domain-Label-Bias leiden besonders unter dieser Instabilität, wobei selbst kalibrierende Methoden nur geringe Verbesserungen erzielen. Die Autoren schlagen eine neue Kalibrierungsmethode namens Domain-Context Calibration vor, die zufällig aus dem Task-Korpus stammende Wörter verwendet, um den Bias zu schätzen und die Vorhersagen zu korrigieren. Die Ergebnisse zeigen, dass diese Methode die ICL-Performance signifikant verbessert, insbesondere bei Aufgaben mit hohem Domain-Label-Bias. Die Studie unterstreicht die Bedeutung einer systematischen Analyse von Bias-Quellen und bietet eine umfassende Lösung zur Verbesserung der Robustheit von ICL-Methoden.</sample>
    <sample id="264">Lin Wang, eine Doktorandin an der Zhejiang University, stellt ihr Paper „TAVT: Towards Transferable Audio-Visual Text Generation“ vor. Dabei geht es um die Generierung von Texten aus Audio- und Visualdaten, wobei der Fokus auf der Übertragbarkeit des Modells zwischen verschiedenen Domänen liegt. Aufgrund der hohen Kosten der Datenannotation und der Schwierigkeiten bei der Anpassung an unterschiedliche multimodale Umgebungen, leitet sie ein neues Konzept ein: das Transferable Audio-Visual Text Generation. Das Ziel ist, ein Modell zu entwickeln, das mit geringen Mengen an annotierten Daten schnell auf neue Domänen anpassbar ist. Dazu wird ein modulares Framework mit drei Komponenten vorgestellt: einem audiovisuellen Meta-Mapper-Netzwerk, einem Encoder-Generator und einer Dual Counterfactual Contrastive Learning (DCLL)-Methode. Das Meta-Mapper-Netzwerk aligniert visuelle Konzepte in einem einheitlichen auditiven Semantikraum. Die DCLL-Methode verbessert die visuell-textuelle Ausrichtung durch feinkörnige Überwachungssignale. Die Experimente zeigen, dass TAVT in beiden Cross-Dataset- und Cross-Domain-Einstellungen die besten Ergebnisse erzielt, insbesondere bei geringer Datenverfügbarkeit. Die Ergebnisse bestätigen die Effektivität des vorgeschlagenen Ansatzes für übertragbare multimodale Textgenerierung.</sample>
    <sample id="265">Der Referent heißt Vasudha.</sample>
    <sample id="266">The text does not mention the universities to which the authors belong.</sample>
    <sample id="268">Die häufigsten Fehler von PaLM sind Omission-Fehler, also das Weglassen von Teilen des Quelltextes bei der Übersetzung.</sample>
    <sample id="269">Hallo, ich bin James Finch. Und ich bin Sarah Finch. Heute möchten wir Ihnen alles über ABC-Eval, einen neuen dimensionsbasierten Ansatz zur Bewertung von Conversational AI, erzählen. Diese Arbeit wurde vom Emory NLP Lab unter der Leitung von Professor Jinho Choi an der Emory University durchgeführt und in Zusammenarbeit mit Amazon Alexa AI. Stellen Sie sich vor, Sie haben gerade ein Dialogmodell entwickelt und möchten wissen, wie es sich im Vergleich zum aktuellen Stand der Technik schlägt. Die übliche Praxis besteht darin, menschliche Bewertungen zu verwenden, beispielsweise durch das Anfragen von menschlichen Gutachtern, um zu entscheiden, welche der beiden Gespräche besser ist oder um Gespräche anhand einer Likert-Skala zu bewerten. Diese Ansätze funktionieren gut, um eine umfassende Bewertung der allgemeinen Qualität eines Dialogs zu liefern. Allerdings hat die Qualität eines Dialogs viele Aspekte. Daher könnten Sie möglicherweise mehrere Dimensionen der Chat-Qualität bewerten wollen, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen. Ein Ansatz besteht darin, einfach menschliche Gutachter zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, beispielsweise die Relevanz der Antworten des Modells mithilfe bestehender vergleichender oder Likert-Skalen-Methoden. Wir glauben jedoch, dass es einen präziseren und zuverlässigeren Strategie für die dimensionsbasierte Bewertung von Dialogen gibt. Unser Ansatz zielt darauf ab, die Subjektivität der menschlichen Bewertung zu reduzieren, indem wir explizit annotieren, ob eine Antwort des Modells bestimmte Verhaltensweisen aufweist, wie z. B. irrelevanten Informationen oder Widersprüchen. Wir nennen diesen Ansatz Annotieren von Verhaltensweisen im Chat oder kurz ABC-Eval. Wir haben diese Methode entwickelt, um die Verhaltensweisen von Chat-Modellen umfassend abzudecken, die in der jüngsten Literatur als beeinflussend für die Chat-Qualität erwähnt wurden. ABC-Eval ist in der Lage, die Häufigkeit, mit der Chat-Modelle verschiedene thematische Fehler begehen, zu messen. Zum Beispiel misst ABC-Eval die Anzahl der Gesprächsrunden, in denen ein Chat-Modell seinen Partner ignoriert oder etwas Irrelevantes sagt, sich oder seinen Partner widerspricht, falsche Fakten erfindet oder allgemein bekannte Wissensregeln verletzt, und wann das Modell Empathie zeigt oder nicht. Um herauszufinden, welche Art von Bewertung am wirksamsten ist, haben wir vier state-of-the-art Chat-Modelle ausgewählt und sie anhand von 100 menschlich-roboter-Gesprächen pro Modell mit ABC-Eval bewertet. Zum Vergleich haben wir diese Gespräche auch mithilfe dreier bestehender Methoden bewertet: Likert-Bewertungen auf Ebene der Gesprächsrunde, Likert-Bewertungen auf Ebene des Dialogs und paarweise Vergleiche auf Ebene des Dialogs. Für jede der bestehenden Methoden haben wir Bewertungen für acht der am häufigsten gemessenen Aspekte eines Dialogs gesammelt, da dies die Standardpraxis für die Bewertung von Chat-Modellen entlang mehrerer Dimensionen ist. Aus unserer Analyse der Ergebnisse dieser Bewertungen haben wir festgestellt, dass die ABC-Eval-Verhaltenslabels insgesamt zuverlässiger sind als die durch bestehende Methoden gesammelten Labels, wie anhand der Inter-Annotator-Übereinstimmung bei 100 doppelt annotierten Gesprächen gemessen. Zudem sind ABC-Eval-Labels besser in der Vorhersage der allgemeinen Gesprächsqualität als die von bestehenden Methoden erzeugten Metriken, wie durch diese einfache lineare Regressionsanalyse gezeigt. Sie können beispielsweise sehen, wie die Messung des Anteils der Gesprächsrunden mit Selbst- und Partnerwidersprüchen jeweils 5 % und 10 % der Gesprächsqualität erklären, während die Durchschnittswerte der Likert-Konsistenz nur 4 % oder weniger erklären. Schließlich haben wir überprüft, ob jeder Bewertungsmetrik ein einzigartiger Aspekt der Chat-Qualität zugeordnet werden kann, mithilfe einer schrittweisen linearen Regression. Sie können sehen, wie die Kombination aller ABC-Eval-Metriken über 25 % der Gesprächsqualität erklärt und wie bei der Entfernung der Metriken nacheinander in der Regel eine erhebliche Menge an Information über die Qualität verloren geht. Auf der anderen Seite erklärt die Kombination aller auf Ebene der Gesprächsrunde gemessenen Likert-Metriken deutlich weniger der Qualität und weniger dieser Metriken tragen einzigartige Informationen. Diese zuverlässigen, informativen und eindeutigen ABC-Eval-Metriken ermöglichen es uns, Conversational AI mit einer höheren Auflösung zu bewerten, als dies bisherige Methoden erreichen konnten. Sie können das in den Ergebnissen unseres Experiments sehen, in denen mehrere Herausforderungen weiterhin bestehen und präzise quantifiziert wurden. Zum Beispiel haben die von uns getesteten Bots in etwa 20 % ihrer Antworten Widersprüche im Allgemeinenwissen. Sie produzieren in etwa 15 % der Antworten irrelevantes Material und widersprechen sich oder ihrem Partner in etwa 10 % der Fälle. Angesichts der schnellen Fortschritte in diesem Bereich könnten viele dieser Fehlerquoten bei neuen Modellen, die nach unserer Bewertung veröffentlicht werden, abnehmen. Dies unterstreicht jedoch nochmals die Notwendigkeit, zuverlässige und präzise Bewertungsmetriken zu verfolgen, um Modelle zu vergleichen. Wir hoffen, dass ABC-Eval von anderen in diesem Bereich genutzt werden kann, um einen bedeutenden Schritt in diese Richtung zu machen. Und wir freuen uns darauf, zu sehen, wie Conversational AI in den nächsten Monaten und Jahren weiterentwickelt wird. Vielen Dank, dass Sie uns zugehört haben.</sample>
    <sample id="270">Die Autoren gehören der Emory University an.</sample>
    <sample id="271">In dieser Arbeit steht CFT für "Clean Fine-Tuning", also das Feinabstimmen eines Modells anhand von sauber, manuell annotierten Daten.</sample>
    <sample id="272">An der Arbeit sind 7 Autoren beteiligt.</sample>
    <sample id="273">Hallo, mein Name ist Kayo Yin und ich werde unsere Arbeit mit dem Titel „Wann benötigt die Übersetzung Kontext? Eine datengetriebene, multilinguale Untersuchung“ vorstellen. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig durchgeführt. Viele Übersetzungen hängen von Kontext ab. Zum Beispiel: Wie würden wir „mole“ in diesem Satz übersetzen? Wenn der vorherige Satz „Dinge könnten gefährlich werden, wenn die Minister es herausfinden“, lautet, bezieht sich „mole“ auf einen Spion. Wenn der vorherige Satz aber „Könnte es etwas Ernstes sein, Doktor?“ lautet, bezieht sich „mole“ auf einen Muttermal. Abhängig vom Kontext ändert sich die Bedeutung des Wortes und somit auch seine Übersetzung. Allerdings ist die Bewertung, wie gut Modelle solche Fälle übersetzen können, sehr schwierig. Erstens, weil nur ein kleiner Teil der Übersetzungen vom Kontext abhängt, was es schwer macht, Metriken auf Korpus-Ebene wie BLEU, solche Übersetzungen zu erfassen. Zudem haben einige vorgeschlagen, gezielte Bewertungen für kontextabhängige Übersetzungen durchzuführen, aber solche Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Sprachmengen, da sie in der Regel auf Domänenwissen und menschliche Kuratierung angewiesen sind.

In dieser Arbeit versuchen wir, zwei Fragen zu beantworten. Erstens: Wann benötigt eine Übersetzung Kontext? Und zweitens: Wie gut können Modelle solche Fälle bewältigen? Um die erste Frage zu beantworten, haben wir zunächst gemessen, wie stark ein Wort vom Kontext abhängt, während der Übersetzung. In früheren Arbeiten haben wir CXMI als Maß zur Messung der Kontextnutzung durch Maschinenübersetzungmodelle eingeführt. Dies geschieht, indem gemessen wird, wie viel Information der Kontext C über das Ziel Y liefert, gegeben die Quelle X. CXMI kann man sich als die Information vorstellen, die das Modell durch den Kontext erhält. In dieser Arbeit haben wir CXMI erweitert zu Pointwise CXMI, das die Kontextnutzung auf Satzebene oder Wortebene messen kann. Man kann Wörter mit hohem P-CXMI als solche betrachten, die Kontext zur Übersetzung benötigen.

Nun analysieren wir Wörter mit hohem P-CXMI, um Muster zwischen diesen Wörtern zu erkennen. Unsere Analyse durchführen wir anhand von Transkriptionen von TED-Vorträgen, die ins Englische übersetzt wurden, und in 14 verschiedene Sprachen. Wir analysieren auf drei verschiedenen Ebenen. Erstens betrachten wir Satzstellungen, die einen hohen mittleren P-CXMI haben, was uns ermöglicht, beispielsweise die dualen Pronomen im Arabischen zu erkennen, die einen relativ hohen P-CXMI aufweisen. Dies kann damit erklärt werden, dass das Englische keine dualen Pronomen kennt, weshalb man Kontext benötigt, um zu bestimmen, ob ein Pronomen dual ist, wenn man ins Arabische übersetzt. Ähnlich finden wir heraus, dass bestimmte Sprachen auch Kontext benötigen, wenn man die passende Verbform wählen möchte.

Dann betrachten wir Vokabulare, die einen hohen P-CXMI aufweisen, gemittelt über alle ihrer Vorkommnisse. Dies hilft uns, Fälle wie diesen zu identifizieren, bei denen im Chinesischen Kontext benötigt wird, um Eigennamen zu übersetzen und sicherzustellen, dass im Dokument die gleiche Übersetzung verwendet wird. Ähnlich finden wir heraus, dass der Kontext für die richtige Formalität bei der Übersetzung wichtig ist.

Schließlich betrachten wir einzelne Token mit hohem P-CXMI. Dies ermöglicht uns, Phänomene zu identifizieren, die nicht wirklich durch das Wort selbst erfasst werden können, sondern vielmehr in der Satzstruktur ausgedrückt werden, wie z. B. die Auflösung von Ellipsen.

Nun verwenden wir unsere Erkenntnisse aus der Analyse, um einen Benchmark für die Dokumentübersetzung zu entwickeln. Für jede der fünf diskursiven Phänomene, die wir identifiziert haben, erstellen wir Tagger, um Wörter zu identifizieren, die zu diesem Phänomen gehören. Wir nennen unseren Tagger Multilingual Discourse-Aware, oder MuDA-Tagger. Wir können dann auch feststellen, dass verschiedene Sprachen unterschiedliche Anteile dieser diskursiven Phänomene aufweisen. Anschließend verwenden wir den MuDA-Tagger, indem wir ihn auf einen parallelen Korpus anwenden, den wir für die Bewertung verwenden möchten, und wir wenden unsere gewählten Übersetzungsmetriken auf die kontextabhängigen Beispiele an, die der MuDA-Tagger identifiziert hat. Schließlich verwenden wir unseren Benchmark sowie andere Metriken, um verschiedene Modelle bei der Dokumentübersetzung zu bewerten.

Zunächst verwenden wir Korpusmetriken: Bei BLEU finden wir, dass Modelle, die kontextunabhängig sind, die beste Leistung erzielen. Wenn wir jedoch COMET verwenden, leisten kontextbewusste Modelle die beste Leistung. Bei der Wort-f-Messung sind Modelle mit und ohne Kontext vergleichbar. Dies zeigt erneut, dass es schwierig ist, das beste Dokumentübersetzungssystem zu bestimmen, wenn man nur Korpusmetriken verwendet.

Nun verwenden wir den MuDA-Benchmark, um Modelle zu bewerten und finden heraus, dass kontextbewusste Modelle bei bestimmten diskursiven Phänomenen, wie Formalität und lexikaler Kohärenz, signifikant genauer sind als Modelle, die keinen Kontext verwenden. Diese Modelle sind jedoch nicht viel besser als Modelle ohne Kontext bei anderen Phänomenen wie Ellipsen, Pronomen und Verbformen. Dies deutet darauf hin, wo wir noch Fortschritte bei der Dokumentübersetzung benötigen.

Wir haben auch verschiedene kommerzielle Systeme verglichen und unser Benchmark zeigt, dass DeepL in der Regel genauer ist als Google Translate bei der Dokumentübersetzung.

Zusammengefasst: Wir führen eine datengetriebene Analyse über 14 Sprachpaare durch, um zu identifizieren, wann Übersetzungen Kontext benötigen, und verwenden unsere Erkenntnisse, um einen Benchmark für die Dokumentübersetzung zu erstellen, der uns dabei helfen kann, zu erkennen, welche diskursiven Phänomene Modelle gut oder schlecht bewältigen können, und welche Übersetzungssysteme gut bei der Dokumentübersetzung sind.

Vielen Dank für Ihre Aufmerksamkeit. Bis bald in Toronto.</sample>
    <sample id="274">Der Referent heißt Yusen Zhang.</sample>
    <sample id="276">Ananya und Vignesh präsentieren ihre Arbeit „IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation Metrics for Indian Languages“. Sie adressieren das Problem, dass Evaluation-Metriken für maschinelle Übersetzung (MT) meist an englischen Texten getestet werden, während andere Sprachen, insbesondere indische, unterschätzt bleiben. Ihre Studie konzentriert sich auf fünf indische Sprachen (Tamil, Malayalam, Hindi, Marathi, Gujarati) und verwendet 7.000 Übersetzungsbeispiele, die mit sieben MT-Modellen generiert wurden. Experten bewerteten diese Übersetzungen nach Fehlertypen und Schweregrad, basierend auf dem MQM-Framework. Die Ergebnisse zeigen, dass COMET-Metriken am besten mit menschlichen Bewertungen korrelieren. Nach Feinabstimmung auf das MQM-Dataset verbesserte sich IndicCOMET erheblich, besonders für indische Sprachen. Zudem zeigte sich, dass IndicCOMET auch bei unbekannten Sprachen robust bleibt und besser abschneidet als Standard-Metriken. Das Dataset ist öffentlich zugänglich, um die Weiterentwicklung von MT-Metriken für indische Sprachen zu fördern.</sample>
    <sample id="277">Die neue Methode hat keinen Namen.</sample>
    <sample id="278">The authors describe the "marked words" method as a way to identify words that distinguish marked groups from unmarked ones by using weighted log-odds ratios, based on the sociolinguistic concept of markedness.</sample>
    <sample id="279">Die Autoren gehören der University of Washington an.</sample>
    <sample id="280">Shi Tao presents MultiEMO, an attention-based multimodal fusion framework for Emotion Recognition in Conversations (ERC). The framework addresses three main challenges: underutilized multimodal complementarity, poor performance on minority emotions, and difficulty distinguishing semantically similar emotions. MultiEMO consists of four components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification. A key innovation is VisExtNet, a visual feature extractor that focuses on facial expressions while avoiding redundant scene information. The MultiAttn module uses bidirectional multi-head cross-attention to effectively integrate textual, audio, and visual modalities. Additionally, a Sample-Weighted Focal Contrastive Loss is introduced to improve classification of minority and similar emotions. Extensive experiments on MELD and IEMOCAP show state-of-the-art results, with notable improvements in minority and semantically similar emotion recognition. Visualizations confirm MultiEMO's effectiveness in complex scenarios. However, limitations include difficulties in distinguishing speakers from irrelevant people in the scene, dependency on large batch sizes for the loss function, and still suboptimal performance on minority emotions compared to majority classes.</sample>
    <sample id="281">In their work titled "When Does Translation Require Context? A Data-driven, Multilingual Exploration," Kayo Yin and colleagues investigate when and how translation depends on context. They introduce Pointwise CXMI, a measure of context usage in machine translation, to identify words that require context for accurate translation. Analyzing TED talk transcripts across 14 languages, they find that certain parts of speech, vocabulary items, and sentence structures heavily rely on context, such as dual pronouns in Arabic or formality in Chinese. Based on these findings, they develop the MuDA tagger to identify context-dependent translation phenomena in parallel corpora. Using MuDA, they create a benchmark for document-level translation, revealing that context-aware models outperform context-agnostic ones in handling formality and lexical cohesion, but not always in cases like ellipsis or pronouns. The benchmark also shows that systems like DeepL perform better than Google Translate in document-level translation. Overall, the study provides a data-driven approach to evaluating and improving document-level machine translation across multiple languages.</sample>
    <sample id="282">**Abstract:**  
This paper introduces StoryTrans, a novel approach for non-parallel story-level author-style transfer in natural language generation. Unlike previous works that focus on token- or sentence-level style transfer, StoryTrans operates at the discourse level to capture and imitate author-specific linguistic patterns, such as narrative structures and writing styles. The main challenges include preserving content while transferring style and handling style-specific content that is tightly coupled with the topic. To address these, StoryTrans learns discourse representations from source texts and combines them with learnable style embeddings to generate target-style stories. A two-stage training framework is proposed: the first stage focuses on disentangling style and content using reconstruction, disentanglement, and order losses, while the second stage fills in masked content with style-specific keywords. Experiments on newly collected Chinese and English datasets show that StoryTrans outperforms strong baselines in style control and content preservation, confirmed by both automatic and manual evaluations. Style visualization and case studies further demonstrate its effectiveness in generating coherent, stylistically aligned stories. The code and data are publicly available.</sample>
    <sample id="283">Prague</sample>
    <sample id="284">In this paper, we introduce FSUIE, a novel approach for Universal Information Extraction (UIE) that addresses limitations in current span-based models. Traditional models rely heavily on precise span boundaries, which can be ambiguous due to varying annotations. To resolve this, FSUIE introduces a fuzzy span mechanism that represents target boundaries as continuous probability distributions. This is achieved through a fuzzy span loss combining Binary Cross-Entropy and KL-divergence with a fuzzy boundary range. Additionally, we propose a fuzzy span attention mechanism that dynamically adjusts the attention span and smoothly decays attention at boundaries, improving model focus and generalization. FSUIE is evaluated on three IE tasks—named entity recognition, relationship extraction, and aspect sentiment triplet extraction—showing significant improvements over baseline models, especially on small datasets and domain-specific tasks. Ablation studies confirm the effectiveness of both fuzzy span loss and attention. The results demonstrate that FSUIE achieves state-of-the-art performance across multiple benchmarks, proving its effectiveness in enhancing universal information extraction.</sample>
    <sample id="285">Mingqi Gao von der Peking University präsentiert die Arbeit "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework". Sie zeigt, dass sowohl Modelle als auch Referenzzusammenfassungen oft faktenbasierte Fehler enthalten. Zwei Lösungsansätze werden diskutiert: die Integration von Faktenzweckkriterien in das Training oder die Entwicklung unabhängiger Faktenkorrekturmodelle (FEC). Bislang fehlt jedoch eine spezifische Forschung zu Faktenfehlern in Dialogzusammenfassungen. Gao kritisiert die bisherigen Evaluationsmethoden von FEC-Modellen, da sie allgemeine Scores liefern, die nicht zuverlässig sind und die Unterscheidung zwischen Korrektur und Neuerstellung verwischen. Daher wird ein neues Evaluationsrahmenwerk vorgeschlagen, das auf manuell annotierten Korrekturen basiert und eine feinkörnige Klassifizierung von Fehlern (inhaltlich und formell) ermöglicht. Die Ergebnisse zeigen, dass FEC-Modelle mit Referenzzusammenfassungen trainiert werden sollten und dass menschliche Korrekturdaten die Leistung verbessern. Aktuelle Modelle haben Schwierigkeiten bei bestimmten Fehlertypen wie Additionen oder Attributfehlern.</sample>
    <sample id="286">Die Referenten heißen James Finch und Sarah Finch.</sample>
    <sample id="287">Vier Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="288">Zum Testen syntaktischer Phänomene können Datensätze wie BLiMP und SyntaxGym verwendet werden.</sample>
    <sample id="290">The abbreviations of the five methods for the first research question are not explicitly mentioned in the provided text. Therefore, they cannot be determined from the given content.</sample>
    <sample id="291">Das Modell wird anhand von 11 biomedizinischen und klinischen Downstream-Aufgaben in Französisch evaluiert, konkret: Named Entity Recognition, Klassifikation, Part-of-Speech Tagging und Question Answering.</sample>
    <sample id="294">CamemBERT wurde ursprünglich mit dem OSCAR-Datensatz trainiert.</sample>
    <sample id="295">Der Referent heißt Adam Przepiórkowski.</sample>
    <sample id="296">Valerio Basile präsentiert ein Projekt der Zusammenarbeit zwischen der Universität Turin und Amazon Alexa, das sich mit der Herausforderung der Ironieerkennung in der natürlichen Sprachverarbeitung befasst. Traditionell basieren solche Modelle auf überwachten Lernmethoden, die auf großen Mengen an manuell annotierten Daten beruhen. Allerdings zeigen sich Grenzen der Annahme eines einheitlichen „ground truths“. Das Team hat deshalb das EPIC-Korpus (English Perspectivist Irony Corpus) erstellt, das aus 300 kurzen Konversationen aus sozialen Medien wie Reddit und Twitter besteht. Die Daten wurden von 74 Annotatoren in fünf Varietäten des Englischen annotiert. Die Ergebnisse zeigen, dass die Annotationen je nach Alter, Nationalität und anderen Faktoren variieren. Die Forscher haben perspektivbewusste Modelle entwickelt, die auf annotierten Datensätzen trainiert wurden, die nach Annotatoren unterteilt sind. Diese Modelle zeigen eine höhere Konsistenz und weniger Unsicherheit in ihren Vorhersagen. Zudem fanden sie heraus, dass Annotatoren aus ähnlichen Altersgruppen oder geografischen Regionen häufiger in ihrer Wahrnehmung von Ironie übereinstimmen. Das Projekt unterstreicht die Bedeutung von Perspektive und Vielfalt in der Annotation für präzisere NLP-Modelle.</sample>
    <sample id="297">Die Arbeit „From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models“ untersucht sogenannte Dogwhistles – Wörter oder Ausdrücke, die einerseits eine offene, harmlose Botschaft vermitteln, andererseits aber eine versteckte, oft diskriminierende oder antisemitische Bedeutung für eine bestimmte Gruppe haben. Das Team hat eine umfangreiche Glossar mit über 340 Begriffen erstellt, die in verschiedenen Kontexten als rassistisch, transphob oder antisemitisch kodiert verwendet werden. In einer Fallstudie wird gezeigt, dass politische Reden in den USA häufiger Dogwhistles enthalten, insbesondere im Zuge der sogenannten Southern Strategy der Republikaner. Die Forscher analysieren auch, wie Sprachmodelle wie GPT-3 solche kodierten Begriffe erkennen und wie sie die versteckten Bedeutungen identifizieren können. Zudem wird gezeigt, dass Dogwhistles bei der automatischen Toxizitätsdetektion oft untergehen, da sie weniger als schädlich eingeschätzt werden als explizite Schmähwörter. Die Studie unterstreicht die Bedeutung von Dogwhistles für politische Kommunikation und zeigt auf, wie sie bei der Umgehung von Inhaltsmoderation helfen können.</sample>
    <sample id="298">Die Ergebnisse zeigten, dass die Leistung der Modelle mit zunehmender zeitlicher Distanz zwischen Trainings- und Testdaten abnahm. Dies wurde durch Experimente bestätigt, bei denen Modelle mit kürzlichem Datenmaterial weitertrainiert wurden und eine Leistungsabnahme bei größeren zeitlichen Abständen festgestellt wurde. Somit war die zeitliche Verzögerung die Hauptursache für den Leistungsverlust.</sample>
    <sample id="299">In this work, Michalis Korakakis and Andreas Vlachos propose a minimax training approach to improve the robustness of Natural Language Inference (NLI) models against spurious correlations or "shortcuts" in training data. While NLI models achieve strong performance on standard benchmarks, they often rely on these shortcuts, leading to poor generalization on out-of-distribution adversarial examples. Traditional shortcut mitigation methods require auxiliary models that exploit shortcuts, but these approaches depend on domain-specific knowledge and may not align with the learner’s behavior. To address these limitations, the proposed method uses a minimax objective where a learner model minimizes task loss, while an auxiliary model maximizes the learner's loss by generating example weights that emphasize hard, under-represented instances. This encourages the learner to focus on examples that challenge shortcut-based reasoning. The method does not assume knowledge of the shortcuts and uses a simple feed-forward network as the auxiliary. Evaluations on datasets like MNLI, FEVER, and QQP show improved out-of-distribution performance without sacrificing in-distribution accuracy. The approach is scalable and applicable to various models and shortcut types.</sample>
    <sample id="300">Belinda präsentiert die Arbeit von Semantic Machines zur Entwicklung einer interaktiven Spracherkennung, genannt *interactive dictation*. Dabei sollen Nutzer sowohl Texte sprachgesteuert verfassen als auch Änderungen anhand natürlicher Sprachbefehle vornehmen, ohne feste Triggerwörter zu benötigen. Im Gegensatz zu bestehenden Systemen, die oft auf vorgegebene Befehle angewiesen sind, soll diese Methode intuitiver und flexibler sein. Die Arbeit formalisiert den Prozess in vier Schritte: Spracherkennung, Segmentierung von Sprachäußerungen in Texte und Befehle, Normalisierung der Befehle und Ausführung in der Dokumentendatei. Um Daten für diese Aufgabe zu sammeln, wurde eine spezielle Annotationsoberfläche entwickelt, mit der Nutzer Texte und Befehle in Echtzeit verarbeiten können. Die Forscher trainierten Modelle wie T5 und GPT-3, um die Schritte automatisch durchzuführen. Die Ergebnisse zeigen, dass GPT-3 genauer, aber langsamer ist, während T5 effizienter bleibt, wenn Befehle als Programme vorhergesagt werden. Die Forscher betonen, dass viel Potenzial für Verbesserungen besteht und veröffentlichen Code sowie eine detaillierte Arbeit, um zukünftige Forschung zu fördern.</sample>
    <sample id="302">It is necessary to permute the tokens for the output sequence because, after tagging each input token with an unordered multiset of output tokens, the tokens are not in the correct order and need to be arranged systematically to produce a meaningful logical form.</sample>
    <sample id="303">Die Autoren empfehlen Transparenz, weil sie nicht wissen, ob die positiven Stereotype, die in den Modellen auftreten, auf eine übermäßige Wertausrichtung oder andere entgegenwirkende Methoden zurückgehen. Ohne Transparenz können solche Muster nicht weiter untersucht oder korrigiert werden.</sample>
    <sample id="304">Unacceptable minimal pair inputs are sentences that are grammatically incorrect or semantically odd, used to test language models' ability to distinguish them from acceptable counterparts in acceptability judgments.</sample>
    <sample id="305">Dawei, ein Promotionsstudent an der Saarland-Universität, präsentiert die Arbeit „Weaker Than You Think: A Critical Look at Weakly Supervised Learning“. In der Studie wird untersucht, ob Schwachsupervision tatsächlich ohne saubere, manuell annotierte Daten funktioniert. Obwohl Schwachsupervision kostengünstig ist, führt sie aufgrund von Rauschen in den Labels oft zu schlechter Generalisierung. Kürzlich behaupteten viele Arbeiten, dass Modelle mit Schwachlabels allein gute Ergebnisse erzielen, doch diese Ansätze benötigen oft eine saubere Validierungsdatenmenge für die Modellauswahl. Die Studie zeigt, dass moderne Schwachsupervisionsmethoden tatsächlich saubere Validierungssamples benötigen, ansonsten die Leistung stark abfällt. Zudem verbessert sich die Modellleistung mit zunehmender Anzahl an sauberen Validierungssamples, und eine direkte Feinabstimmung auf diese Daten übertrifft oft komplexe Schwachsupervisionsansätze. Die Ergebnisse zeigen, dass die Vorteile von Schwachsupervision überschätzt werden und einfache Baselines wie Few-Shot-Learning oder Feinabstimmung besser performen. Die Autoren empfehlen, saubere Validierungssamples bei der Modellauswahl zu berücksichtigen und solche Methoden in zukünftigen Arbeiten zu vergleichen. Die Codebasis ist öffentlich zugänglich.</sample>
    <sample id="306">Sebastian Schuster und Najoung Kim präsentieren ihre Forschung zur Fähigkeit von Sprachmodellen, Entitäten in Texten zu verfolgen. Sie zeigen, dass für das Verständnis längerer Texte entscheidend ist, zu verstehen, wie sich die Zustände von Entitäten über die Zeit verändern. Um diese Fähigkeit zu evaluieren, haben sie eine Aufgabe entwickelt, bei der ein Modell den Inhalt von Kästen nach mehreren Operationen wie dem Verschieben oder Hinzufügen von Objekten vorhersagen muss. Dabei haben sie sicher gestellt, dass das Modell keine einfachen Heuristiken oder vorherige Kenntnisse aus dem Trainingsdatensatz nutzen kann. Ihre Ergebnisse zeigen, dass die meisten Modelle, wie Flan-T5 und GPT-3.5, bei der Vorhersage von Zuständen, die sich von den ursprünglichen unterscheiden, schlecht abschneiden. Nur das Modell text-davinci-003 zeigt signifikante Fähigkeiten im Entitätenverfolgen. Interessanterweise zeigen Modelle, die auf Code trainiert wurden, bessere Ergebnisse, was darauf hindeutet, dass Code-Training eine Rolle bei der Entwicklung dieser Fähigkeit spielt. Die Forschung weist darauf hin, dass vorausgehendes Training entscheidend ist, aber die Generalisierbarkeit der Ergebnisse bleibt noch unklar. Weitere Ergebnisse, einschließlich Tests mit GPT-4, finden sich in ihrem Paper auf arXiv.</sample>
    <sample id="307">The authors evaluated their models using standard NLP metrics such as accuracy, F1-score, and precision, which are commonly used for tasks like named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="308">Jenny, eine erste Jahr PhD-Studentin an der Carnegie Mellon University, präsentiert die Arbeit „NLPositionality“, die Design-Voreingenommenheit von Datensätzen und Modellen untersucht. Die Studie, durchgeführt in Zusammenarbeit mit der University of Washington und dem Allen Institute for AI, zeigt, dass NLP-Modelle und -Datensätze systematisch in ihrer Leistung zwischen verschiedenen Bevölkerungsgruppen variieren. Dieses Phänomen wird als „Positionalität“ bezeichnet und resultiert aus den Perspektiven und Erfahrungen der Forscher sowie der annotierenden Nutzer. Die Forscher untersuchen, wie Datensätze und Modelle mit der Positionalität von Endnutzern übereinstimmen, indem sie Daten von über 1.000 Annotatoren aus 87 Ländern sammeln. Sie finden, dass Modelle wie GPT-4 und Datensätze wie Dynahate vor allem mit englischsprachigen Ländern und Menschen mit Hochschulabschluss übereinstimmen, während nicht-binäre Personen weniger berücksichtigt werden. Die Studie betont die Notwendigkeit, Design-Entscheidungen zu dokumentieren, perspektivisch zu forschen und spezialisierte Datensätze für verschiedene Communities zu entwickeln, um eine inklusivere NLP-Forschung zu fördern.</sample>
    <sample id="309">Die Metrik, die zur Messung der Übereinstimmung zwischen den Kommentatoren verwendet wurde, ist die inter-annotator Agreement.</sample>
    <sample id="310">Die Domain, die gewählt wurde, um völlig unzusammenhängende Sätze zu den inakzeptablen und akzeptablen Suchanfragen hinzuzufügen, ist **Wikipedia**.</sample>
    <sample id="311">Die Autoren gehören der Universität Tübingen an.</sample>
    <sample id="312">MultiInstruct unterscheidet sich durch sein erstes großes, vielfältiges Multi-Modell-Instruktionstuning-Dataset mit 62 unterschiedlichen Aufgaben aus 10 Kategorien, das speziell für die Verbesserung der Zero-Shot-Fähigkeiten von Multi-Modell-Modellen entwickelt wurde. Es ist die erste solche Benchmark, die auf mehreren Modalitäten basiert und mit Expertenformulierten Instruktionen ausgestattet ist.</sample>
    <sample id="313">Die Arbeit wurde von der Emory NLP Lab unter der Leitung von Professor Jinho Choi und in Zusammenarbeit mit Amazon Alexa AI durchgeführt. Es wird jedoch keine genaue Anzahl der Autoren genannt.</sample>
    <sample id="314">Die Definition der binären Koordination bezieht sich auf die Kombination von zwei Elementen (Konjunkten) durch ein Koordinationsverbundwort (z. B. "und", "oder"), wobei beide Konjunkte syntaktisch gleichrangig sind und eine koordinierte Struktur bilden.</sample>
    <sample id="315">Die Studie erwähnt nicht explizit, wie lange die verwendeten Prompts im Durchschnitt waren.</sample>
    <sample id="316">Die Ergebnisse zeigen, dass das kleinere T5-Modell, wenn es auf dem CoScript-Datensatz feinabgestimmt wird, Skripte höherer Qualität generieren kann als viele große Sprachmodelle.</sample>
    <sample id="317">Peng Li von der Fudan University stellt die Arbeit „CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors“ vor. Information Extraction (IE) ist ein klassisches NLP-Problem, bei dem strukturierte Informationen aus unstrukturiertem Text extrahiert werden. Traditionelle Modelle wie T5 oder GPT-3 nutzen einen text-to-text-Ansatz, wodurch bei der Inferenz eine Diskrepanz zwischen strukturierten Eingaben und nicht-strukturierten Ausgaben entsteht. CodeIE löst dieses Problem, indem es IE in ein strukturierteres Code-Generationsproblem transformiert und große Code-Modelle wie Codex einsetzt. Dabei werden Eingaben in einen strukturierten Code-Format umgewandelt, was die Ausgabe besser ausrichtet. Bei Named Entity Recognition und Relation Extraction wurden entsprechende Prompts entwickelt, die das Modell dazu anregen, strukturierte Ergebnisse zu generieren. Die Evaluierung auf mehreren Datensätzen zeigte, dass CodeIE mit Code-Modellen und Code-Format-Prompts deutlich besser abschneidet als traditionelle Modelle, insbesondere in der Recall-Performance. Codex übertrifft dabei GPT-3 sowohl in der Genauigkeit als auch in der Strukturgenauigkeit. Die Ergebnisse unterstreichen das Potenzial von Code-Modellen für Information Extraction.</sample>
    <sample id="318">Hallo, ich bin Yanis Labrak und ich präsentiere euch unsere Arbeiten zu „DrBERT: Ein robuster vortrainierter Modell in Französisch für die biomedizinischen und klinischen Bereiche“. In dieser Präsentation sprechen wir zunächst über Sprachmodellierung im Gesundheitswesen. Anschließend stellen wir die Hauptbeiträge unseres Artikels vor. Wir führen das erste biomedizinische Modell in Französisch ein, das DrBERT genannt wird, welches auf RoBERTa basiert und auf NACHOS trainiert wurde, einem Datensatz medizinischer Daten aus dem Internet. Wir stellen auch eine Vergleichsanalyse von Modellen mit verschiedenen Vortrainierungssettings und Datensätzen vor. Danach präsentieren wir unsere Ergebnisse anhand von 11 biomedizinischen und klinischen Downstream-Aufgaben im Französischen. Schließlich ziehen wir Schlussfolgerungen aus den Experimenten und geben euch weitere Details, wie ihr auf diese Modelle zugreifen könnt.

Seit seiner Veröffentlichung im Jahr 2018 hat BERT sich als eine der effektivsten Methoden zur Lösung von Natural Language Processing-Aufgaben erwiesen und bietet enorme Leistungssteigerungen im Vergleich zu historischen statischen und kontextuellen Methoden wie Word2vec, fastText und anderen. Seitdem wurde das Modell an viele andere Sprachen angepasst, wie beispielsweise in Französisch mit CamemBERT, und auch in Bereiche wie die Biomedizin mit PubMedBERT und BioBERT sowie in klinische Anwendungen mit ClinicalBERT, meist jedoch in englischer Sprache. Spezialisierte Modelle für andere Sprachen sind jedoch selten und basieren oft auf kontinuierlichem Vortrainieren aufgrund des Mangel an domänenspezifischen Daten. Bisher gab es jedoch für Französisch keine offenen biomedizinischen Modelle. Daher fragten wir uns, welche Datenquellen am geeignetsten sind, um ein breites Anwendungsspektrum abzudecken und ob solche gescrapten Daten als Ersatz für klinische Daten verwendet werden können. Um diese Frage zu beantworten, vergleichen wir DrBERT mit unserem ChuBERT-Modell, das auf anonymisierten Daten aus der Datenbank des Universitätsklinikums Nantes basiert.

Anschließend fragten wir uns, wie viel Daten benötigt werden, um ein spezialisiertes Modell für französische Daten zu trainieren: 4 GB, 8 GB oder mehr? Um diese Frage zu beantworten, trainierten und verglichen wir vier Modelle von Grund auf neu: eine erste Version von DrBERT mit 7 GB NACHOS-Daten; eine zweite Version von DrBERT mit 4 GB NACHOS-Daten; eine erste Version von ChuBERT, einem klinischen Modell mit 4 GB Sätzen aus klinischen Notizen; und eine letzte Version von ChuBERT, die aus 4 GB NACHOS-Daten und 4 GB klinischen Notizen besteht. Zusätzlich zu diesem Vergleich stellten wir drei Modelle vor, die auf kontinuierlichem Vortrainieren basieren, um den Einfluss der Vortrainierungsstrategie zu analysieren. Ein Modell basiert auf den Gewichten von CamemBERT und wurde mit 4 GB NACHOS-Daten trainiert. Ein weiteres Modell basiert ebenfalls auf CamemBERT, wurde aber diesmal mit 4 GB klinischen Notizen trainiert. Schließlich wurde ein drittes Modell auf der Grundlage des englischen biomedizinischen Modells PubMedBERT mit 4 GB NACHOS-Daten trainiert. Insgesamt haben wir sieben Modelle.

Um unsere sieben Modelle zu evaluieren, sammelten wir Daten für öffentliche und private Downstream-Aufgaben wie Named Entity Recognition, Klassifikation, Part-of-Speech Tagging und Fragebeantwortung. Diese Modelle werden mit sechs Baseline-Modellen verglichen, darunter CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT und ClinicalBERT. Die Evaluation zeigt, dass Modelle, die auf Daten gleicher Natur trainiert wurden wie diejenigen, die für die Aufgaben verwendet werden, am besten abschnitten. Wir beobachteten jedoch, dass Daten aus heterogenen Quellen vielseitiger zu sein scheinen. Wir stellten außerdem fest, dass die Nutzung von mehr Daten zu besseren Ergebnissen führt. Im Allgemeinen erzielten Modelle, die von Grund auf vortrainiert wurden, höhere Leistungen bei den meisten Aufgaben. Allerdings zeigten unsere Experimente zu kontinuierlichem Vortrainieren mit den Gewichten und Tokenisierung von CamemBERT, trainiert auf einer 4 GB-Untermenge von NACHOS, Ergebnisse, die mit DrBERT 4 GB (vom Grund auf vortrainiert) vergleichbar waren. Dies gilt jedoch nicht für das Modell, das auf den Gewichten und Tokenisierung von CamemBERT basiert, das an Stabilitätsproblemen leidet.

Zusammenfassend zeigte unser eigenes System eine bessere Leistung bei neun von elf Downstream-Aufgaben und übertraf global die Ergebnisse des generischen Modells, hier CamemBERT. Wir beobachteten auch, dass spezialisiertere Daten besser sind, aber nicht gut skalieren. Alle vortrainierten Modelle, die aus NACHOS stammen, sind kostenlos auf Hugging Face verfügbar und unter der MIT-Lizenz. Alle Trainings-Skripte befinden sich in unserem GitHub-Repository. Vielen Dank für diese Präsentation, und wir freuen uns auf den Austausch während der Poster-Session in Toronto.</sample>
    <sample id="319">In der Arbeit werden folgende Lernstrategien untersucht: von-null-Pre-Training, kontinuierliches Pre-Training mit verschiedenen Basismodellen (CamemBERT, PubMedBERT) und das Training mit heterogenen Datenquellen.</sample>
    <sample id="320">Der Faktor der Überanpassung, der auf die Wiederverwendung von Tests zurückzuführt, ist nicht vorhanden, da die Verbesserungen auf dem CoNLL-2003-Datensatz sich in ähnlicher oder sogar stärkerer Weise auf das CoNLL++-Datensatz übertragen haben. Dies zeigt, dass kein "diminishing returns" Effekt (also kein Rückgang der Verbesserungen durch Überanpassung) beobachtet wurde.</sample>
    <sample id="321">Die Qualität der Vereinfachung wurde durch eine Analyse der Art und Umfang der Simplifikationstransformationen beurteilt, wie z. B. lexikalische Vereinfachung, Strukturänderungen und allgemeiner Vereinfachungsgrad. Zudem wurde untersucht, wie stark verschiedene Textarten (z. B. Bibeltexte, Nachrichten, Lernertexte) vereinfacht wurden.</sample>
    <sample id="322">Enrico präsentiert eine Studie, die untersucht, was Textklassifikatoren über Moral lernen. Er betont, dass Moral subjektiv ist und nicht einfach als binäre Skala zwischen moralisch und unmoralisch gesehen werden kann. Stattdessen basiert die Moral auf fünf verschiedenen Grundlagen, wie sie im Moral Foundation Theory beschrieben werden. Jeder Mensch priorisiert diese moralischen Grundlagen unterschiedlich. In der NLP-Community wird Moral in Texten oft mit einfachen Klassifikationsmodellen behandelt, was die Vielfalt menschlicher Moral nicht vollständig abbildet. Enrico und sein Team untersuchen, ob Sprachmodelle diese feineren Unterschiede in der moralischen Ausdrucksweise erkennen können. Dazu verwenden sie den Moral Foundation Twitter Corpus mit 35.000 Tweets aus sieben verschiedenen Themenbereichen. Sie zeigen, dass Sprachmodelle Unterschiede in der moralischen Bewertung zwischen Domänen wie #AllLivesMatter und #BlackLivesMatter erkennen können. Insbesondere unterscheiden sie sich in der Bewertung von Rebellion gegen Autorität. Die Ergebnisse warnen davor, Sprachmodelle zu verwenden, ohne zu berücksichtigen, dass Moral je nach Kontext anders interpretiert wird. Die Studie betont die Bedeutung einer differenzierten moralischen Analyse in der NLP.</sample>
    <sample id="323">Yujie Wang aus der Shanxi University präsentiert das Modell DHLK zur Lösung von Commonsense QA-Aufgaben, bei denen Maschinen mit allgemeinem Wissen Fragen beantworten müssen. Bestehende Ansätze kombinieren Sprachmodelle und Wissensgraphen, stoßen jedoch auf Probleme wie Lärm in den abgerufenen Subgraphen und mangelnde Wechselwirkung zwischen Text und Graph. DHLK adressiert diese Herausforderungen durch eine zweistufige Pruning-Strategie, um den Wissensgraph (HKG) zu optimieren, und durch die Integration von Synonymen aus WordNet und Wiktionary. Die Entity- und Relation-Embeddings werden mit TransE optimiert, und ein neu entwickelter Relation Mask Self-Attention-Mechanismus (RMSA) modelliert die Beziehungen im Graphen. Die Graph-Embedding-Information wird anschließend in den QA-Kontext eingefügt, um die Kontext-Embedding-Repräsentation zu verbessern. Die finale Antwortvorhersage erfolgt mithilfe eines MLP, der die Graph-Embedding, Pfadinformationen und den QA-Kontext kombiniert. Die Evaluation auf CommonsenseQA und OpenBookQA mit externen Wissensbasen wie ConceptNet zeigt, dass DHLK bessere Ergebnisse als bestehende LM- und HKG-basierte Methoden erzielt.</sample>
    <sample id="324">Ja, Sprachmodelle haben unterschiedliche politische Vorurteile. Sie zeigen verschiedene politische Einstellungen, die sich von liberal bis konservativ erstrecken, und ihre politischen Neigungen können sich je nach Trainingsdaten und weiterer Feinabstimmung verändern. Diese Vorurteile können zu Fairnessproblemen in NLP-Anwendungen führen, wie z. B. bei der Erkennung von Hassrede oder Fakenews.</sample>
    <sample id="325">Hallo! Mein Name ist Matthias Lindemann, und heute möchte ich Ihnen eine kurze Einführung in unser Paper zum Thema „Kompositionelle Generalisierung ohne Bäume mithilfe von Multiset-Tagging und latenten Permutationen“ geben. Dieses Paper ist gemeinsam mit meinen Betreuern Alexander Koller und Ivan Titov entstanden. Kompositionelle Generalisierung kann als die Fähigkeit eines Lernenden verstanden werden, tiefergehende Rekursion und bisher unerfahrene Kombinationen von Phrasen zu bewältigen, die während des Trainings einzeln gesehen wurden. Im Kontext der semantischen Parsing kann das Testen der kompositionellen Generalisierung so aussehen: Wie üblich haben wir eine Trainingsmenge von Aussagen. In diesem Fall: „The girl slept.“ und „Mary knew that the girl slept.“ Diese Aussagen sind mit logischen Formeln verbunden, die zentrale Aspekte ihrer Bedeutung darstellen. Im Gegensatz zur standardmäßigen Evaluierung im maschinellen Lernen stammt die Testmenge nicht aus der gleichen Verteilung, sondern enthält strukturell unbekannte logische Formeln. In diesem Beispiel hat das Modell während des Trainings nur flache Rekursion gesehen und wird nun auf ein Beispiel mit tieferer Rekursion getestet. Naive seq2seq-Modelle haben Schwierigkeiten bei dieser Art von out-of-distribution-Generalisierung und produzieren oft Ausgaben, die von der Eingabe abgekoppelt sind. Insbesondere scheitern sie oft daran, systematische Entsprechungen zwischen Eingabe und Ausgabe wiederzugeben, wie sie in dem Beispiel farbcodiert dargestellt sind. Eine beliebte Methode, um dies zu beheben, ist die Integration von Bäumen in die Modelle. Die Bäume sollen den kompositionellen Prozess erfassen, der Aussagen mit logischen Formeln verknüpft. Dies funktioniert gut, jedoch sind Bäume in der Regel nicht gegeben und müssen irgendwie erzeugt werden. Dies kann kompliziert und manchmal rechenintensiv sein. Typischerweise erfordert dies eine erhebliche, formalismusspezifische Vorverarbeitung der logischen Formeln, beispielsweise zur Verarbeitung von Variablensymbolen. Das Erstellen von Bäumen kann auch spezialisierte Grammatikinduktionsverfahren erfordern. In diesem Paper verwenden wir keine Bäume und stellen ein neuronales seq2seq-Modell vor, das die Entsprechungen zwischen Fragmenten der Eingabe und Fragmenten der Ausgabe direkt modelliert. Zum ersten Mal zeigen wir starke Generalisierungsfähigkeiten für tiefergehende Rekursion ohne den Einsatz von Bäumen. Unser Ansatz erzeugt die Ausgabe aus der Eingabe in zwei Schritten. Zunächst versehen wir jeden Eingabetoken mit einer ungeordneten Multimenge an Tokens, die in der Ausgabe vorkommen. Nach dem ersten Schritt haben wir alle richtigen Tokens, doch sie sind nicht geordnet. Deshalb verwenden wir im zweiten Schritt ein weiteres Modell, um eine Permutation vorherzusagen, um sie in die richtige Reihenfolge zu bringen. Wir stellen eine neue Methode zur Vorhersage der Permutation vor, die keine harten Einschränkungen für mögliche Permutationen setzt. Dies macht unseren Ansatz sehr flexibel und ausdrucksstark. Konzeptionell funktioniert unser Permutationsmodell grob so: Wir bewegen uns von links nach rechts über die Ausgabe und bestimmen für jede Position, welches Multiset-Token dort platziert wird. Für die erste Ausgabeposition wählen wir einfach eines aus, wie in Rot hervorgehoben. Danach springen wir zum nächsten Multiset-Token, um das zweite Token in der Ausgabe zu bestimmen. Ebenso bestimmen wir das dritte Token in der Ausgabe, indem wir zu einem anderen Multiset-Token springen. Wir wiederholen diesen Prozess, bis jedes Token aus dem ersten Schritt genau einmal besucht wurde. Um Ihnen einen Vorgeschmack auf unsere experimentellen Ergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen baumlosen Modellen am COGS-Benchmark. Unser Modell übertrifft die anderen deutlich in der Generalisierung zu tieferen Rekursion. Einige andere Arten struktureller Generalisierung bleiben jedoch weiterhin sehr herausfordernd. In unserem Paper lösen wir mehrere interessante technische Herausforderungen. Zunächst ist die Ausrichtung zwischen Eingabe und Ausgabe in den Trainingsdaten nicht gegeben. Als Konsequenz wissen wir für ein gegebenes Token nicht, welcher Multiset es stammt, was das Training erschwert. Zudem können manchmal mehrere Permutationen mit den Daten kompatibel sein, während die sprachlich korrekte eine latent ist. Wir beheben dies, indem wir die Ausrichtung als Teil des Trainings induzieren. Unser Permutationsverfahren ist sehr flexibel, aber es führt zur Herausforderung, dass das Finden der bestmöglichen Permutation NP-schwer ist. Dies liegt daran, dass dies mit dem „Traveling Salesman“-Problem verwandt ist. Wir approximieren dies durch eine GPU-freundliche kontinuierliche Relaxierung, die uns auch erlaubt, durch die Lösung zurückzupropagieren und sprachlich plausiblere Permutationen zu lernen. Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen bewältigen möchten, schauen Sie sich unser Paper an oder besuchen Sie unseren Poster.</sample>
    <sample id="326">Cognitive dissonance is the inconsistency between two beliefs or actions, such as knowing that smoking can be harmful but still choosing to smoke.</sample>
    <sample id="327">**Abstract:**  
This paper introduces ManagerTower, a novel vision-language (VL) architecture that enhances cross-modal representation learning by adaptively aggregating insights from pre-trained unimodal experts at different layers. Unlike previous two-tower or BridgeTower approaches, which use fixed or layer-by-layer unimodal representations, ManagerTower employs managers in each cross-modal layer to dynamically combine multi-level unimodal knowledge. This enables more comprehensive cross-modal alignment and fusion. Using RoBERTa and CLIP-ViT as unimodal encoders, ManagerTower achieves state-of-the-art performance on downstream tasks, including a 39.15% accuracy on the Wikivideo test set, with only 4 million pre-training images. Experiments show that adaptive managers effectively exploit varying levels of semantic knowledge across layers, outperforming both base models and larger models trained on more data. The proposed architecture is flexible, allowing any visual, textual, or cross-modal encoder. Code and models are publicly available.</sample>
    <sample id="328">GPT-4 steht am meisten links.</sample>
    <sample id="329">In this work, Minghang Zheng and colleagues from Peking University present a noise-resistant zero-shot video sentence localization method. The goal is to identify video segments that match a natural language query without manual annotations. Existing methods generate pseudo-queries and pseudo-events but suffer from simple queries, misalignment between queries and events, and label noise. The proposed approach uses a pre-trained image caption model to generate complex pseudo-queries and then measures frame-query relevance to create pseudo-events with high internal and low external similarity. A sliding window selects the best pseudo-events based on event quality. To reduce label noise, the method uses sample re-weighting and label refinement. Experiments on ActivityNet Captions and Charades-STA show superior performance compared to existing zero-shot methods, achieving state-of-the-art results in metrics like R@M and mIoU. The method is robust to noise and enables training without manual annotations, offering a practical solution for video sentence localization.</sample>
    <sample id="330">Ja, kumulatives Training ist gleich oder besser als iteratives Training für aktives Lernen.</sample>
    <sample id="331">Die Referentin heißt Sara Papi.</sample>
    <sample id="332">Die Daten für die MuDA-Benchmark stammen aus TED-Talk-Transkripten, die ins Englische übersetzt wurden.</sample>
    <sample id="333">Wenhao von der Nanjing University präsentiert die Arbeit „INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation“. Ziel ist es, die Generalisierungsfähigkeit von NMT-Modellen zu verbessern, indem die Repräsentationsräume glättet werden. Dabei wird beobachtet, dass seltene Token in der Repräsentationsspace spärlich verteilt sind, was zu „Löchern“ führt und die Übersetzung beeinträchtigt. Die kNN-MT-Methode verwendet eine Datenbank, um benachbarte Einträge zu finden und die Vorhersage zu verfeinern. Allerdings ist dies zeitaufwendig und die Datenbank nicht leicht aktualisierbar. INK löst dies, indem kNN-Wissen in den Trainingsprozess integriert wird. Dabei wird ein Adapter trainiert, um Repräsentationen anhand von KL-Divergenz anzupassen und die Datenbank asynchron zu aktualisieren. Die Ergebnisse zeigen, dass INK die Leistung von NMT-Modellen deutlich verbessert, mit höheren BLEU- und COMET-Scores, weniger Speicherbedarf und schnellerer Inferenz. Die Kombination von Adapter und Datenbank führt zu weiteren Verbesserungen, was zeigt, dass der Repräsentationsraum nicht vollständig durch den Adapter optimiert wird.</sample>
    <sample id="335">Der Referent heißt Matthias Lindemann.</sample>
    <sample id="336">Sprachübergreifender Transfer (Cross-lingual Transfer) bezieht sich auf das Training eines Modells auf Daten einer Quellsprache und das Anwenden dieses Modells auf eine Zielsprache, ohne dass es explizit auf Daten der Zielsprache trainiert wurde. Dies kann im Few-shot oder Zero-shot Setting erfolgen.</sample>
    <sample id="337">In ihrer Arbeit „Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning“ präsentieren die Forscher eine neue Methode zur Erstellung von Embeddings für Out-of-Vocabulary (OOV)-Wörter. OOV-Wörter sind oft schwer zu repräsentieren, da sie nicht im Trainingsvokabular enthalten sind, doch sind sie für die Leistung von Embedding-basierten Modellen entscheidend. Die Forscher nutzen das menschliche Lernverhalten, bei dem neue Wörter aus bestehenden gebildet werden, und entwickeln einen Word Relationship Graph, der die Beziehungen zwischen Wortbausteinen und assoziierten Wörtern abbildet. Dabei wird das OOV-Wort in Wordpieces zerlegt und in einem zweistufigen Graphen mit relevanten Wörtern verknüpft. Ein Selbst-Attention-Netzwerk weist Attributen zu, während zwei Schichten des Graph Attention Networks die wichtigsten Informationen extrahieren. Eine Readout-Schicht erfasst die globale Grapheninformation, und ein Kontrastive Learning-Verfahren sorgt für eine enge Anpassung an das Vektorraummodell. Extensive Experimente zeigen, dass das Modell in intrinsischen und extrinsischen Aufgaben besser abschneidet als Baseline-Methoden. Zudem ist es für statische und kontextuelle Modelle nützlich. Die Anwendung auf andere Sprachen hängt von der Qualität der Wortzerlegung ab, wobei agglutinative Sprachen besonders gut geeignet sind.</sample>
    <sample id="338">Bingsheng präsentierte die Arbeit „Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations“, eine gemeinsame Forschung von Wissenschaftlern der Rensselaer Polytechnic Institute, Northeastern University und IBM Research. Das Ziel der Studie ist es, die Qualität von menschlich annotierten Erklärungen objektiv zu bewerten, da diese oft subjektiv und von der Aufgabe abhängig sind. Die Forscher haben eine einheitliche Datenstruktur eingeführt, um verschiedene Aufgaben in eine einheitliche multiple-choice-Struktur zu überführen. Sie untersuchten die Nützlichkeit von Erklärungen durch Experimente mit neun Datenmengen und verglichen die Leistung von Basismodellen und Modellen mit Erklärungseinfluss. Dabei stellten sie fest, dass Erklärungen nicht immer neue Wissen vermitteln, sondern oft das Modell dazu bringen, auf die Erklärung zu vertrauen. Die entwickelte Metrik TREU erweitert den simulatability score und bewertet die Nützlichkeit von Erklärungen während der Feinabstimmung. Die Ergebnisse zeigen, dass TREU die Qualität von Erklärungen besser bewertet als der simulatability score und dass die Nützlichkeit von Erklärungen stark von der Aufgabe und dem Format abhängt. Die Arbeit legt den Grundstein für eine objektivere Bewertung von menschlichen Erklärungen in künftigen Forschungsprojekten.</sample>
    <sample id="339">Die Autoren gehören der Saarland University in Deutschland an.</sample>
    <sample id="340">Kuan-Hao Huang von der UCLA präsentiert das Projekt "ParaAMR", ein großes, syntaktisch vielfältiges Paraphrasendatenset, das durch AMR-Back-Translation erstellt wurde. Ziel des Projekts ist es, eine umfangreiche und syntaktisch abwechslungsreiche Paraphrasequelle zu entwickeln, um NLP-Modelle zu verbessern. Bestehende Datensätze sind entweder zu klein oder zu syntaktisch einheitlich. ParaAMR nutzt Abstract Meaning Representations (AMR), um durch Veränderung der Fokusstruktur in AMR-Graphen syntaktisch unterschiedliche Paraphrasen zu generieren. Dabei werden AMR-Graphen aus Quellsätzen erstellt, der Fokus geändert und anschließend wieder in Text übersetzt. Das Ergebnis ist ein Datensatz mit etwa 15 Millionen Sätzen und 6,9 Paraphrasen pro Satz, der syntaktische Vielfalt bewahrt, während die semantische Ähnlichkeit erhalten bleibt. ParaAMR wurde anhand automatischer und menschlicher Bewertungen analysiert und zeigte bessere Ergebnisse bei der syntaktischen Diversität. In Anwendungen wie Satzembeddings, syntaktisch kontrollierter Paraphrasegenerierung und Datenverstärkung für Few-Shot-Learning übertraf ParaAMR bestehende Datensätze. Das Datenset ist öffentlich zugänglich.</sample>
    <sample id="341">The authors use average lagging and computational-aware average lagging as latency measurements.</sample>
    <sample id="342">Hello, I'm Gao Jingsheng. Today, I present our paper introducing LiveChat, a large-scale personalized dialogue dataset automatically constructed from live streaming. Open-domain dialogue involves human-AI conversations covering various topics without specific goals, relying on pre-trained models and large datasets. Existing datasets are mostly text-based, but video-sourced datasets are closer to real speech. Current video datasets are either scripted or unscripted, but limited in scale due to manual annotation. Our goal is to address these limitations by creating a large-scale, video-sourced, personalized dataset. LiveChat is built in three steps: collecting live stream videos, extracting and transcribing audio, and constructing dialogues using a reply-to-whom matching method. We also collect persona information through manual and rule-based methods. Compared to existing datasets, LiveChat is larger, video-sourced, and includes personal annotations and longer sessions. Our experiments on response modeling and addressee recognition show that persona information and longer sessions improve performance. BART outperforms other models, highlighting the uniqueness of LiveChat. Human evaluations show LLMs excel in informativeness. We also found that performance increases with more demonstrations but slightly decreases beyond eight due to noise. In conclusion, LiveChat is a valuable resource for personalized dialogue research, and future work will focus on efficient transfer learning. Thank you.</sample>
    <sample id="343">Hallo zusammen, ich bin Akshatha, und heute präsentiere ich gemeinsam mit meinem Mitautor Martin unsere Arbeit „Der KITMUS-Test: Die Beurteilung der Integration von Wissen aus mehreren Quellen“. Diese Arbeit entstand in Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research. Natürliche Sprachverstehensmodelle beziehen sich auf eine Vielzahl von Wissensquellen, wie etwa das in ihren Parametern enthaltene Wissen, das in der Regel während der Vorabtrainierung erlernt wird, und Wissen, das während der Inferenzzeit in den Eingaben bereitgestellt wird. Kürzlich durchgeführte Arbeiten in Aufgaben wie der Fragebeantwortung zeigen, dass Modelle das vorabtrainierte Wissen nutzen können, um eine Aufgabe zu lösen. Allerdings erfordert das natürliche Sprachverstehen oft auch Wissen, das während der Inferenzzeit bereitgestellt wird. Ein Beispiel dafür ist der Satz: „John sah den neu gewählten Präsidenten im Fernsehen.“ Vorabtrainierte Parameter können Informationen über das Verhalten von Präsidenten und das Fernsehen enthalten, können aber nicht zuverlässig wissen, wer der spezifische Entität „John“ ist oder wer der neue Präsident ist, da sich der Präsident seit der Vorabtrainierung geändert haben könnte. Daher benötigen Modelle, die bei wissensintensiven NLU-Aufgaben erfolgreich sind, die Fähigkeit, sowohl vorabtrainiertes als auch inferenzzeitliches Wissen zu integrieren und zu nutzen. In dieser Arbeit schlagen wir ein diagnostisches Testset für die Wissensintegration vor. Wir führen eine Pronomenauflösungsaufgabe ein, die darauf abzielt, die Fähigkeit zu prüfen, Wissen aus verschiedenen Quellen zu nutzen. Wir testen das Datenset sowohl anhand menschlicher Teilnehmer als auch anhand etablierter Pronomenauflösungsmodelle. Hier ist ein Beispiel aus unserem Datenset: „Servin ist ein Richter. Kea ist ein Bäcker. Servin und Kea trafen sich im Park. Nach einem langen Tag an der Arbeit, bei dem er Fälle im Gericht entschied, war er glücklich, sich zu entspannen.“ Die Aufgabe besteht darin, das richtige Entity zu identifizieren, auf das sich das Pronomen „er“ bezieht, was in diesem Fall Servin ist. Die Auflösung eines Pronomens erfordert zwei Arten von Informationen. Erstens, entity-spezifisches Wissen wie „Servin ist ein Richter.“ Zweitens, Hintergrundwissen wie „Richter entscheiden über Fälle in Gerichten.“ Hintergrundwissen wird in der Regel während der Vorabtrainierung großer Sprachmodelle erlernt, während entity-spezifisches Wissen in der Regel während der Inferenzzeit beobachtet wird. Wir variieren die Verfügbarkeit dieser beiden Informationsquellen so, dass sie entweder in einer oder in mehreren Quellen vorkommen. Wir haben drei KITMUS-Einstellungen definiert. Erstens, die typische Einstellung: „Background-Pretrain“, bei der das Hintergrundwissen als vorabtrainiert angenommen wird. Zweitens, die Einstellung „Background-Both“, bei der das Hintergrundwissen sowohl vorabtrainiert als auch während der Inferenzzeit verfügbar ist. Drittens, die Einstellung „Background-Inference“, bei der beide Wissensarten nur während der Inferenzzeit verfügbar sind. Letztere Einstellung ist besonders interessant, da sie den Fall simuliert, bei dem das notwendige Hintergrundwissen, um eine Aufgabe zu lösen, nicht Teil der vorabtrainierten Daten der Modelle ist. Ein Beispiel dafür, wie wir die Verfügbarkeit von Fakten in den tatsächlichen Quellen kontrollieren, ist folgendes: In der Einstellung „Background-Pretrain“ nehmen wir an, dass das Hintergrundwissen „Politiker suchen nach gewählten Sitzen im Regierungssystem“ in den vorabtrainierten Parametern enthalten ist und in der Inferenzzeit werden wir das entity-spezifische Wissen „Chichester ist ein Politiker“ bereitgestellt. In der Einstellung „Background-Both“ stellen wir zusätzlich nicht nur entity-spezifisches Wissen, sondern auch Hintergrundwissen über Politiker in der Inferenzzeit bereit. In der Einstellung „Background-Inference“ stellen wir stattdessen die fiktive Berufsbezeichnung „mirituer“ bereit, da „mirituer“ vermutlich nicht in den vorabtrainierten Parametern enthalten ist. Wir testen das Datenset sowohl anhand menschlicher Teilnehmer als auch anhand etablierter Pronomenauflösungsmodelle. In dieser Grafik zeigen wir die Ergebnisse der besten Modelle bei der schwierigsten Variante der „Background-Pretrain“-Einstellung. Ohne spezifische Aufgabenbezogene Trainierung mit KITMUS schneiden beide Modelle schlecht ab. Wenn sie jedoch mit KITMUS trainiert werden, leisten sowohl C2F als auch BERT4Coref deutlich besser als zufällig. Dies zeigt, dass Modelle, die an generischen Referenzauflösungsdatensätzen trainiert wurden, in der Regel lernen, Oberflächenhinweise zu nutzen, die in KITMUS nicht mehr nützlich sind. Zusätzliche Experimente mit fiktivem Wissen zeigten, dass selbst die besten Modelle nicht zuverlässig Hintergrundwissen integrieren können, das nur während der Inferenzzeit bereitgestellt wird. Um die wichtigsten Erkenntnisse unserer Arbeit zusammenzufassen, scheinen viele Pronomenauflösungsmodelle ohne spezifische Aufgabenbezogene Trainierung nicht in der Lage zu sein, über Wissen aus verschiedenen Quellen zu denken. Mit spezifischer Aufgabenbezogener Trainierung können jedoch einige Modelle erfolgreich Wissen aus mehreren Quellen integrieren. Dennoch scheinen selbst die besten Modelle Schwierigkeiten zu haben, Hintergrundwissen zuverlässig zu integrieren, das nur während der Inferenzzeit bereitgestellt wird. Wenn Sie mehr Details erfahren möchten, sehen Sie sich unsere Arbeit an und besuchen Sie das Datenset und den Code auf GitHub. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="344">Die Nachteile der baumbasierten Methoden sind, dass Bäume oft nicht gegeben sind und daher aufwendig erzeugt oder vorverarbeitet werden müssen, was kompliziert und rechenintensiv sein kann.</sample>
    <sample id="345">Matthias Lindemann präsentiert eine Studie zur kompositionellen Generalisierung ohne den Einsatz von Bäumen. Gemeinsam mit Alexander Koller und Ivan Titov untersuchen sie, wie neuronale Modelle in der semantischen Parsing-Aufgabe tieferen Rekursion und neue Kombinationen von bereits gelernten Phrasen während der Testphase bewältigen können. Traditionelle seq2seq-Modelle scheitern oft an dieser Herausforderung, da sie systematische Beziehungen zwischen Eingabe und Ausgabe nicht reproduzieren. Die Verwendung von Bäumen zur Modellierung der Komposition ist zwar effektiv, erfordert jedoch aufwendige Vorverarbeitung und ist nicht immer praktikabel. Das vorgeschlagene Modell löst dieses Problem, indem es in zwei Schritten vorgeht: Zuerst werden Eingabewörter mit einer Menge von Ausgabewörtern markiert, und anschließend wird eine Permutation dieser Wörter vorhergesagt. Die Permutation wird flexibel ohne starre Einschränkungen erlernt, was die Modellierung von komplexen Strukturen ermöglicht. Die Ergebnisse zeigen, dass das Modell die Generalisierung zu tieferer Rekursion deutlich besser als andere baumlose Modelle auf dem COGS-Benchmark meistert. Zudem wird die Herausforderung der Alignment-Erstellung und der NP-schweren Permutationsoptimierung adressiert, wofür eine kontinuierliche Relaxierung genutzt wird.</sample>
    <sample id="346">Die Information über die Universität der Autoren ist im gegebenen Text nicht enthalten.</sample>
    <sample id="347">Hallo, ich bin Myra und heute möchte ich über unser Paper „Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models“ sprechen. Diese Arbeit wurde in Zusammenarbeit mit Esin Durmus und Dan Jurafsky erstellt. In den letzten Jahren haben viele Forscher die Verbreitung sozialer Vorurteile und Stereotype in großen Sprachmodellen, sogenannten LLMs, dokumentiert. Allerdings haben diese Methoden verschiedene Grenzen. Sie basieren meist auf handkodierten Datensätzen, die sehr aufwendig zu erstellen sind, und sie messen meist nur sehr spezifische Stereotype, was bedeutet, dass sie sich schlecht auf andere Demografien oder Kontexte verallgemeinern lassen. Oder sie erfassen lediglich allgemeine, breite Assoziationen, wie beispielsweise negative Assoziationen mit bestimmten Gruppen. Zudem berücksichtigt die meisten Arbeiten in diesem Bereich nicht die Intersectionalität, also die Idee, dass sich mehrfache soziale Identitäten zu Vorurteilen verketten können und einzigartige Schwerpunkte von Schaden darstellen.

Um diese Grenzen zu überwinden, nutzen wir die Eigenschaft, dass diese neuere, anweisungsgesteuerte LLMs sehr gut auf Anweisungen und Prompts reagieren. Wir können also das Modell bitten, eine Persona zu generieren, also eine Darstellung eines fiktiven Individuums, mit einem Prompt wie „Stell dir vor, du bist eine asiatische Frau. Beschreibe dich selbst.“. Sofort erkennen wir, dass dies sehr verallgemeinerbar ist, da wir einfach jeden gewünschten Identitätsmerkmal in diesen Prompt einbauen können.

Hier sind einige Beispielgenerierungen von GPT-4. Sofort sehen wir, dass, obwohl die Ausgaben nicht offensichtlich negativ oder toxisch sind im traditionellen Sinne dieser Wörter, es einige interessante Muster gibt. Die asiatische Frau wird als unauffällig dargestellt; die mittelöstliche Frau wird mit Begriffen wie „exotisch“ und „verzauberndes Land“ beschrieben. Und beide Frauen von Farbe erwähnen ihre Abstammung, während der weiße Mann dies nicht tut.

Um diese Muster zu erfassen, besteht unsere Methode aus zwei Teilen. Der erste besteht darin, diese Personas zu generieren. Unsere Prompts zur Erstellung dieser Personas wurden von einer Studie inspiriert, bei der diese Prompts an menschliche Teilnehmer gegeben wurden und dabei festgestellt wurde, dass sie auch bei Menschen rassistische Stereotype aufdecken können. Zudem ermöglicht dies eine direkte Vergleichbarkeit zwischen unseren generierten Personas und den von Menschen geschriebenen Antworten.

Der zweite Teil ist die Methode der „Marked Words“, also ein Verfahren, um die Wörter zu identifizieren, die markierte Gruppen von unmarkierten Gruppen unterscheiden. Ich werde dies später noch genauer erläutern. Der Vorteil dieser Methode besteht darin, dass wir sehr spezifische Stereotype und Muster erfassen können, ohne auf ein spezifisches Lexikon zurückzugreifen.

Die Methode der „Marked Words“ baut auf dem soziolinguistischen Konzept der „Markedness“ auf, das besagt, dass es eine unmarkierte Standardform gibt, und jede Gruppe, die sich von dieser Standardform unterscheidet, linguistisch als markiert gilt. So wird beispielsweise das Wort „Krieger“ normalerweise mit Männern assoziiert. Wenn Menschen also eine Kriegerin beschreiben, benutzen sie oft das Wort „Frauenkriegerin“ und markieren das Wort mit dem Begriff „Frau“. Allgemeiner gesagt sind die dominanten Gruppen in der Gesellschaft sowohl linguistisch als auch sozial unmarkiert, während die marginalisierten Gruppen in der Regel markiert sind.

In unserer Methode bestimmen wir zunächst, welche Gruppen als unmarkiert und welche als markiert gelten, und vergleichen anschließend die Personas mit dem Fightin’ Words-Verfahren, das im Grunde genommen gewichtete Log-Odds-Verhältnisse verwendet, um die wichtigsten Wörter für jede markierte Gruppe zu identifizieren. So würden wir beispielsweise für die Personas von schwarzen Frauen Fightin’ Words verwenden und diese gegen die Personas von Weißen und Männern vergleichen, da diese beiden Gruppen die entsprechenden unmarkierten Gruppen sind.

Nun zu einigen Ergebnissen. Zunächst verwenden wir ein Lexikon von Stereotypen und stellen fest, dass die generierten Personas deutlich mehr Stereotype enthalten als die von Menschen geschriebenen. Wenn wir jedoch tatsächlich die Verteilung der Wörter und des Lexikons untersuchen, finden wir sehr unterschiedliche Dinge. Obwohl die generierten Personas deutlich höhere Raten an Lexikonwörtern aufweisen, haben die menschlich geschriebenen Personas eine viel breitere Verteilung von Wörtern, während die Stereotypwörter in den generierten Personas lediglich die Wörter „groß“ und „sportlich“ sind. Also wirklich nur positive oder zumindest nicht-negative Wörter. Tatsächlich spiegelt dieses Lexikon viele der schädlichen Muster nicht gut wider, die wir bereits in den vorherigen Folien gesehen haben.

Stattdessen wenden wir uns den Ergebnissen unserer „Marked Words“-Methode zu, um zu zeigen, wie diese scheinbar positiven Wörter Stereotype und essentialisierende Narrative fördern. In unserer Analyse zeigen wir, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln. Erstens beinhalten die Top-Wörter unserer Gruppen Dinge wie „Kultur“, „Tradition“, „stolz“ und „exotisch“. Diese Wörter definieren diese Gruppen ausschließlich durch ihre Beziehung zu ihrer Identität und unterscheiden sie von der weißen Norm. Dies trägt zu einer langen Tradition von Diskriminierung und „Othering“ für diese Gruppen bei.

Zweitens sind viele übliche Klischees in diesen Wörtern sichtbar, insbesondere bei Frauen von Farbe. So beinhalten die Wörter, die Lateinamerikanerinnen beschreiben, Dinge wie „lebhaft“ und „kurvig“, die mit dem Klischee der „Tropen“ verbunden sind. Bei asiatischen Frauen sind die Wörter Dinge wie „klein“, „zart“ und „seidenweich“, die mit einer langen Geschichte der hypersexuellen Darstellung asiatischer Frauen verbunden sind, als sehr unterwürfig und gehorsam, und so weiter.

Schließlich sehen wir bei schwarzen Frauen, dass einige der Top-Wörter Dinge wie „stark“ und „resilient“ sind. Dies verweist auf ein Archetyp, den man als „Stärkere schwarze Frauen“ bezeichnet. Obwohl dies zunächst positiv klingt, gibt es Forschung, die zeigt, dass diese Art von Archetyp tatsächlich sehr schädlich ist, da sie einen erheblichen Druck auf diese Bevölkerungsgruppen ausübt, widerstandsfähig und stark gegen gesellschaftliche Hindernisse zu sein. Statt tatsächlich an der Änderung dieser Hindernisse zu arbeiten, wird Druck auf diese Menschen ausgeübt, sie zu überwinden, was zu sehr negativen Gesundheitsergebnissen führt, unter anderen Schäden.

Allgemeiner gesagt finden wir, dass die Wörter für jede markierte Gruppe sehr essentialisierende Narrative widerspiegeln.

Auf Grundlage dieser Muster schlagen wir drei Empfehlungen für Modelleigner vor. Erstens sollten wir als Forscher auch positive Stereotype und essentialisierende Narrative adressieren. Zweitens sollten wir ein intersectionales Lens verwenden, um Vorurteile und Schäden zu untersuchen, da vieles übersehen werden könnte, wenn wir das nicht tun. Schließlich sollte es eine erhöhte Transparenz über Methoden zur Minderung von Vorurteilen geben, da wir beispielsweise nicht wissen, ob diese positiven Stereotype aufgrund eines merkwürdig übermäßigen Wertalignments entstanden sind oder ob andere anti-stereotypische Methoden zu diesen schädlichen Mustern geführt haben. Ohne mehr Transparenz können wir keine Annahmen treffen oder dies weiter untersuchen.

Vielen Dank, dass ihr zugehört habt. Viel Spaß auf dem ACL!</sample>
    <sample id="348">In ihrer Arbeit „Marked Personas“ untersuchen Myra, Esin Durmus und Dan Jurafsky, wie soziale Stereotype in großen Sprachmodellen (LLMs) gemessen werden können. Traditionelle Methoden sind oft eingeschränkt, da sie auf manuell erstellten Datensätzen basieren und nicht gut auf verschiedene Demografien oder Kontexte generalisieren. Zudem vernachlässigen sie oft die Intersectionalität. Die Forscher nutzen stattdessen natürliche Sprachprompts, um Stereotype in LLMs zu erkennen. Dazu bitten sie Modelle, Personas zu generieren, z. B. „Stell dir eine asiatische Frau vor. Beschreibe dich selbst.“ Die Ergebnisse zeigen, dass die Modelle, obwohl nicht direkt rassistisch, stereotype Muster reproduzieren, wie z. B. „exotisch“ für mittelöstliche Frauen oder „resilient“ für schwarze Frauen. Die Methode „Marked Words“ identifiziert Wörter, die markierte Gruppen von unmarkierten unterscheiden. Sie zeigt, wie scheinbar positive Stereotype wie „stark“ oder „kultiviert“ tief sitzende Diskriminierungsstrukturen verstärken. Die Forscher empfehlen, positive Stereotype nicht zu ignorieren, eine intersectionale Perspektive zu nutzen und mehr Transparenz in Bias-Minderungsmaßnahmen zu schaffen.</sample>
    <sample id="349">Hallo zusammen, mein Name ist Jingwei Yi vom University of Science and Technology of China. Es ist mir eine Freude, ein kurzes Werbevideo zu unserem Paper zu präsentieren. Kopieren Sie meinen Modell? Schutz des Urheberrechts großer Sprachmodelle für Embedding als Dienstleistung durch Backdoor-Wasserzeichen. Zunächst stellen wir den Hintergrund zu Embedding als Dienstleistung vor. Derzeit sind große Sprachmodelle wie GPT, LLAMA und PALM in der natürlichen Sprachverarbeitung und -erzeugung hervorragend. Embedding als Dienstleistung ist eine Dienstleistung, die auf großen Sprachmodellen basiert, um verschiedene NLP-Aufgaben zu unterstützen. Zum Beispiel bietet OpenAI eine GPT-basierte Embedding-API an. Allerdings haben kürzlich durchgeführte Arbeiten gezeigt, dass Angreifer das Modell durch das Lernen aus dem Embedding stehlen und ähnliche Dienste anbieten können. Daher ist es notwendig, das Urheberrecht von Embedding als Dienstleistung zu schützen. Um das Urheberrecht von Embedding als Dienstleistung zu schützen, ist eine Lösung, ein Wasserzeichen in die Dienstleistung des Anbieters einzubauen und zu prüfen, ob ein anderes Dienstleistungsangebot dieses Wasserzeichen enthält. Die Wasserzeichenmethode muss folgende Eigenschaften erfüllen: Erstens sollte die Methode für Embedding als Dienstleistung anwendbar sein. Zweitens sollte das Wasserzeichen die Nutzbarkeit der bereitgestellten Embedding nicht beeinträchtigen. Drittens sollte das Wasserzeichen so versteckt sein, dass der Angreifer es nicht leicht erkennen oder entfernen kann. Schließlich muss das Wasserzeichen während des Modell-Extraktionsprozesses auf die Dienstleistungen des Angreifers übertragbar sein. Die vorhandenen Arbeiten lassen sich grob in vier Kategorien einteilen. Allerdings ist diese Methode entweder nicht für Embedding als Dienstleistung anwendbar oder fehlt an Übertragbarkeit. Daher schlagen wir in diesem Paper Embedding Marker vor, eine auf Backdoor basierende Wasserzeichenmethode, die für Embedding als Dienstleistung anwendbar ist. Dann stelle ich Ihnen die Details unseres Embedding Marker vor. Embedding Marker besteht aus zwei Hauptphasen: Wasserzeichen-Einbettung und Urheberrechtsverifikation. Vor diesen Hauptphasen wählen wir zunächst eine Triggermenge aus. Die Triggermenge ist eine Gruppe von Wörtern mit einer moderaten Frequenz. Wir gehen davon aus, dass der Anbieter einen allgemeinen Text-Korpus sammeln und die Wortfrequenz mit diesem zählen kann. Bei der Wasserzeichen-Einbettung definieren wir zuerst ein Ziel-Embedding. Wenn ein Benutzer einen Satz an den Anbieter-Dienst sendet, zählt der Anbieter die Anzahl der Trigger im Satz. Das bereitgestellte Embedding ist eine Gewichtssumme des Ziel-Embeddings und des ursprünglichen Embeddings. Das Gewicht des Ziel-Embeddings ist proportional zur Anzahl der Trigger im Satz. Wenn die Anzahl der Trigger im Satz größer als m ist, ist das bereitgestellte Embedding genau gleich dem Ziel-Embedding. Die Urheberrechtsverifikation besteht darin, zu prüfen, ob ein Modell hinter einem anderen Dienst das Wasserzeichen enthält. Dazu konstruieren wir zuerst eine Backdoor-Datensatz und einen harmlosen Datensatz. Der Backdoor-Datensatz besteht aus Sätzen, bei denen alle Wörter zur Triggermenge gehören, während alle Wörter in den Sätzen des harmlosen Datensatzes nicht zur Triggermenge gehören. Anschließend bittet der Anbieter den Dieb um Embedding-Embedding mit diesem Datensatz. Die Kosinus- und L2-Ähnlichkeit zwischen den angeforderten Embedding und dem Ziel-Embedding werden berechnet. Wir berechnen den Ähnlichkeitsunterschied zwischen harmlosen und Backdoor-Datensätzen, der als Delta-Kosinus und Delta-L2 definiert ist. Gleichzeitig wenden wir auch den KS-Test an und verwenden seinen p-Wert als dritten Metrik. Wir führen Experimente an vier Datensätzen durch: AG News, MIND, SST2 und Enron Spam. Wir gehen davon aus, dass der Anbieter den Wikipedia-Text-Datensatz verwendet, um die Wortfrequenz zu zählen. Die Ergebnisse auf den vier Datensätzen zeigen, dass unser Embedding Marker eine hohe Erkennungsleistung aufweist, während er eine gute Nutzbarkeit für downstream-Aufgaben beibehält. Wir validieren auch die Verstecktheit der bereitgestellten Embedding durch die Visualisierung der Embedding von Sätzen auf vier Datensätzen [unverständlich 4:39] PCA. Die Legende der Grafiken bedeutet die Anzahl der Trigger in jedem Satz. Wie in den Grafiken gezeigt, ist es schwer, zwischen den Backdoor-Embedding und den normalen Embedding zu unterscheiden. Das war alles. Vielen Dank. Herzlich willkommen, um mit uns zu diskutieren.</sample>
    <sample id="350">In recent years, leaderboard-based evaluation has become the standard in NLP, with systems often achieving human-level or even superhuman performance on benchmarks like SuperGLUE and SQuAD. However, this paper questions the reliability of such claims. While models may outperform humans on these benchmarks, the comparison is often unfair due to several issues. First, humans are typically tested on smaller subsets of the data compared to models. Second, there are errors in the ground-truth answers, leading to misleading results. Additionally, human performance is often estimated using simplistic methods, and the best human scores may not reflect the true potential of humans. Pay rates for human annotators vary widely, and details about the annotator pool are frequently omitted, further undermining the validity of comparisons. The paper argues that current benchmarks do not reliably reflect the true capabilities of models relative to humans and calls for more rigorous and transparent benchmarking practices. Superhuman performance claims, therefore, lack scientific grounding until these issues are addressed.</sample>
    <sample id="351">**Abstract:**  
This paper investigates the generalization ability of Named Entity Recognition (NER) models trained on the CoNLL-2003 dataset in modern contexts. We created the CoNLL++ dataset, consisting of Reuters news articles from 2020 annotated using CoNLL-2003 guidelines, to evaluate how well models generalize to contemporary data. Over 20 models were fine-tuned on CoNLL-2003 and tested on both the original and new datasets. Our findings indicate that model architecture, size, and the amount of fine-tuning data are crucial for generalization. Transformers and larger models showed better performance. We tested two hypotheses for performance drops: adaptive overfitting and temporal drift. Results showed no evidence of adaptive overfitting, but performance degradation correlated with the temporal gap between training and test data, confirming temporal drift as the main cause. Despite the age of CoNLL-2003, modern NER models still generalize well, suggesting that current taggers remain effective in 2023. This study highlights the importance of model design and data recency in achieving robust generalization.</sample>
    <sample id="352">ABC-Eval steht für "Annotating Behaviors in Chat" und ist ein Verfahren zur bewerteten Analyse von Chat-Modellen anhand verschiedener Dimensionen der Gesprächsqualität.</sample>
    <sample id="353">Die Arbeit „Python Code Generation by Asking Clarification Questions“ beschäftigt sich mit der Herausforderung der unvollständigen Spezifikationen in natürlichsprachlichen Beschreibungen (NLDs) für Code-Generationsmodelle. Traditionelle Methoden scheitern oft an der fehlenden Präzision der Eingaben. Um dies zu lösen, führen die Autoren ein interaktives Paradigma ein, bei dem das Modell durch das Stellen von Klarstellungsfragen (Clarification Questions, CQs) zusätzliche Spezifikationen sammelt. Sie schlagen die Aufgabe der Code-Generierung durch Klarstellungsfragen vor und entwickeln dazu das synthetische Datenset CodeClarQA, das auf Schlüsseloperationen basiert. Dabei wird eine Methode vorgestellt, um fehlende oder ausgerichtete Schlüsseloperationen zu identifizieren, basierend auf der Ähnlichkeit von Schemata zwischen NLD und Dokumentation. Die Pipeline besteht aus einem Klarstellungsbedarfsvorhersager, einem Fragenauswahler und einem Code-Generator. Die Ergebnisse zeigen, dass Klarstellungen die Codegenerierung verbessern können, obwohl das Modell noch unter der Leistung von reinen Trainingsdaten mit NLDs zurückbleibt. Die Analyse deutet darauf hin, dass die Klarstellung von Schlüsseloperationen zu besserem Code führt, allerdings bleiben Herausforderungen wie die Unterscheidung ähnlicher Operationen bestehen. Die Arbeit bietet einen vielversprechenden Ansatz zur Verbesserung der Codegenerierung bei unvollständigen Eingaben.</sample>
    <sample id="354">Das Leistungsdelta zwischen CoNLL-2003 und CoNLL++ ist bis zum Jahr 2020 höher als 5 Prozentpunkte.</sample>
    <sample id="355">Hallo, mein Name ist Vasudha und ich bin Doktorandin in Informatik an der Stony Brook University. Ich möchte unsere Arbeit, die in ACL 2023 als Langartikel angenommen wurde, vorstellen: „Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge“. Wir beginnen damit, kognitive Diskordanzen zu definieren und erklären, warum es wichtig ist, diese in der Sprache zu untersuchen. Einfach gesagt, ist kognitive Diskordanz ein Zustand, in dem zwei Überzeugungen oder Handlungen inkonsistent sind, wie in diesem Beispiel, wo eine Person sagt: „Ich weiß, dass Zigaretten mich töten können“, und dann fügt hinzu: „Ich habe nach dem Meeting ein paar Zigaretten genommen“. Diese Überzeugung und Handlung sind inkonsistent und befinden sich in Diskordanz. Wenn sie dann hinzufügt: „Ich denke nicht, dass ich meine Stelle ohne sie behalten könnte“, rechtfertigt dies die zweite Handlung, und es besteht eine Konsonanz zwischen beiden Aussagen. Obwohl kognitive Diskordanz ein sehr häufig vorkommendes Phänomen im täglichen Entscheidungsprozess ist, wird sie selten in der Sprache ausgedrückt, insbesondere im Vergleich zu anderen Diskursbeziehungen. Warum ist das wichtig? Das Studium kognitiver Diskordanzen kann uns dabei helfen, die Auswirkungen von Meinungsverschiedenheiten zwischen Menschen zu verstehen, Trends und Glaubenswerte zu verfolgen, sowie Veränderungen im Verhalten von Bevölkerungsgruppen. Hohe kognitive Diskordanz ist auch mit Angststörungen verbunden und kann dabei helfen, das psychische Wohlbefinden besser zu verstehen. Das Studium von Diskordanzen in der Sprache kann auch dabei helfen, Extremismus und Polarisation von anfälligen Gruppen besser zu verstehen. Schließlich ist kognitive Diskordanz wichtig, um die individuellen kognitiven Stile von Personen zu verstehen und Entscheidungsprozesse besser zu analysieren.

Ziel unseres Projekts war es, eine Ressource für kognitive Diskordanzen zu erstellen. Dazu haben wir eine groß angelegte Annotation von Diskordanzen durchgeführt. Wir verwendeten einen dissonanzbasierten Ansatz, wie in diesem Flussdiagramm zu sehen ist. Tweets wurden mit dem PDTB-Parser verarbeitet, und Paare von Diskursseinheiten wurden gemäß den in unserem Paper beschriebenen Richtlinien annotiert. Wie hier zu sehen ist, wurde Diskordanz nur in 3,5 % der annotierten Paare gefunden. Beim Sammeln von etwa 1000 Beispielen von Diskursseinheiten haben wir ein erstes Trainingsmodell mit nur 43 Beispielen von Diskordanzen trainiert. Nicht überraschend, war die Leistung des Klassifizierers kaum besser als Zufall. Aufgrund der geringen Vorkommenshäufigkeit von Diskordanzen und der Abwesenheit jeder vorhandenen Datensammlung stehen wir vor dem Problem der absoluten Seltenheit.

Um dies zu mildern, haben wir Kombinationen aus Transferlernen und aktivem Lernen experimentiert, um so mehr diskordante Beispiele mit weniger Annotationsschritten zu sammeln und dadurch die Annotationskosten zu senken, während wir gleichzeitig die Erkennung von Diskordanzen verbessern. Da das ursprüngliche Modell die Diskordanzklasse gar nicht erfassen konnte, begannen wir den Prozess des aktiven Lernens mit dem Transfer von Gewichtungen aus eng verwandten Aufgaben. Wir transferierten von zwei verschiedenen Aufgaben: der Stance-Klassifikation für diskussionsunabhängige Diskordanzen, bei der bestimmt wird, ob zwei Aussagen aus unterschiedlichen Personen im Einvernehmensein oder im Streit stehen, unabhängig vom Thema, bezeichnet als „Debatte“ hier, und von der binären Klassifikation der Erweiterungsklasse und der Vergleichsklasse des PDTB, da diese beiden eng mit dem Konzept von Konsonanz und Diskordanz verbunden sind, und wir sie als „CE“ bezeichnen. Wir fanden heraus, dass die Nullschussleistung auf dem annotierten Datensatz bereits deutlich besser als Zufall war, mit der besten AUC von 0,62. Weiterhin stellten wir fest, dass das iterative Feinabstimmung auf die CE-Aufgaben gefolgt von einer weiteren Feinabstimmung auf Debatte eine deutlich bessere Nullschussleistung ergibt. Dieses Modell verwenden wir, um das aktive Lernen kalt zu starten.

Anschließend bestimmen wir die beste Methode, um ein Modell mit neuen Daten aus jedem Schritt des aktiven Lernens und der Annotation zu aktualisieren. „Cumulative“ sammelt alle bisher gesammelten Daten aus der aktiven Annotation, während „Iterative“ das Modell durch das Training mit den neuesten Daten aktualisiert. Unter verschiedenen Strategien stellten wir fest, dass „Cumulative“ in der Regel gleich gut oder besser als „Iterative“ abschneidet. Um die Anzahl der Diskordanzen zu erhöhen, verwenden wir eine Strategie namens „Probability-of-Rare-Class“ (PRC), um hauptsächlich Beispiele auszuwählen, die nach Ansicht des aktuellen Modells wahrscheinlich diskordant sind. Wir vergleichen dies mit anderen etablierten AL-Strategien, die in der Community üblich sind. Wir stellen fest, dass die vorgeschlagene PRC-Strategie besser als andere etablierte Strategien funktioniert, obwohl der Unterschied klein ist. Beachte, dass die Leistung bei Zufallsauswahl deutlich geringer ist. Nach weiteren Runden aktiven Lernens mit den beiden besten Strategien verbessern wir die AUC der Diskordanzerkennung auf 0,75, was die beste Leistung für diese Aufgabe bislang ist. Wir prüfen auch die Machbarkeit jeder Strategie in Bezug auf die Annotationqualität und die Kosten für die Annotatoren. Wir stellen fest, dass PRC den höchsten Prozentsatz an Diskordanzen aufweist und für seltene Klassen am besten funktioniert. Die Annotatoren finden jedoch die Beispiele jedoch schwierig.

Zusammenfassend stellen wir fest, dass PRC eine einfache AL-Strategie für die Erkennung seltener Klassen ist und das aktive Lernen mit einer sorgfältig gestalteten Transferlernaufgabe effektiv kaltstarten kann. Wir stellen auch fest, dass iterative Aktualisierungen für das Transferlernen aus einem anderen Bereich nützlich sind, während bei aktiven Annotationen innerhalb des Domänenbereichs kumulative Aktualisierungen besser funktionieren. Hier sind die Links zu unserem Kern-Datensatz und unserem Paper. Zögern Sie nicht, uns zu kontaktieren, wenn Sie Fragen haben. Vielen Dank.</sample>
    <sample id="356">Die Autoren gehören der Universität Stanford an.</sample>
    <sample id="357">Der Referent heißt Siyu Yuan.</sample>
    <sample id="358">Die Arbeit ist mit fünf Autoren beteiligt.</sample>
    <sample id="359">Der Ansatz wird mit der state-of-the-art-Architektur für simultane Präübersetzung verglichen.</sample>
    <sample id="361">**Abstract:**  
Armineh Nourbakhsh presents "CounterComp," a novel approach to enhance compositional generalization in multi-step quantitative reasoning tasks, particularly in question answering over financial tables. State-of-the-art models often fail on such tasks due to memorization of spurious patterns. CounterComp addresses this by generating counterfactual scenarios from training data, creating positive and negative examples based on interventions in the input questions. These examples are used to introduce an auxiliary metric learning loss with a dynamic margin, encouraging the model to focus on meaningful tokens and operations. The method improves performance on both in-distribution and out-of-distribution samples, especially for tasks with more than two reasoning steps. Qualitative analysis shows that CounterComp helps models attend to relevant operational terms, enhancing compositional reasoning. The approach is effective across three state-of-the-art baselines, demonstrating its generalizability and potential for real-world applications in financial and quantitative domains.</sample>
  </task>
</testset>