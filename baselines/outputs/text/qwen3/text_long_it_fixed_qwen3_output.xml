<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">Le principali fonti di dati per i modelli linguistici sono i dati di crawling del web, in particolare corpus come il C4 Corpus, che includono testate giornalistiche come New York Times, Los Angeles Times, The Guardian, Huffington Post e altre fonti di media e social media.</sample>
    <sample id="1">Le affiliazioni degli autori dell'articolo sono McGill University, Mila e Microsoft Research.</sample>
    <sample id="2">**Abstract**  
In questo lavoro, presentiamo LayoutMask, un nuovo modello pre-addestrato per la comprensione dei documenti visivamente ricchi (Visually-rich Document Understanding, VrDU), progettato per affrontare i problemi legati all'ordine di lettura nei documenti complessi. Molti modelli pre-addestrati attuali utilizzano un ordine globale 1D per rappresentare l'ordine di lettura dei token, ma questo approccio presenta limitazioni quando i documenti presentano layout complessi o non lineari. LayoutMask introduce l'uso di un ordine 1D locale, basato sull'ordine dei token all'interno di segmenti specifici, combinato con informazioni di posizione 2D e semantiche per inferire l'ordine di lettura globale. Per migliorare l'interazione tra testo e layout, il modello propone due nuove strategie di mascheramento nel compito di Masked Language Modeling: Whole Word Masking, che maschera intere parole anziché singoli token, e Layout-Aware Masking, che privilegia il mascheramento delle prime e ultime parole dei segmenti. Inoltre, introduciamo un nuovo obiettivo di pre-addestramento, Masked Position Modeling, per recuperare le posizioni 2D dei token. Gli esperimenti su dataset come FUNSD, SROIE e CORD mostrano che LayoutMask, con l'uso di Local-1D, supera i modelli basati su Global-1D, specialmente in casi con layout misti e numeri ambigui. Questi risultati dimostrano l'efficacia dell'approccio proposto nel migliorare l'interazione testo-layout e la rappresentazione del layout nei documenti complessi.</sample>
    <sample id="3">Ciao! Benvenuti alla presentazione di DEPLAIN, un nuovo corpus per l'identificazione di testi in tedesco a livello di documento e a livello di frase. Il mio nome è Regina Stodden, e vi guiderò attraverso la prima parte della presentazione. Iniziamo definendo la semplificazione del testo. La semplificazione del testo è un processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo specifico di destinatari, ad esempio persone con problemi di lettura o parlanti non madrelingua. Per addestrare un modello di semplificazione del testo abbiamo bisogno di coppie parallele di testi, ad esempio documenti o frasi. Nell'esempio qui sotto, potete vedere una coppia di frasi allineate in modo parallelo: una frase tedesca complessa e la sua traduzione in linguaggio semplice. Per semplificare la frase, è possibile utilizzare diverse tecniche, come si vede nell'esempio, ad esempio sostituzione lessicale, eliminazione di clausole, riassegnamento di ordine o inserimento di parole. Proponiamo ora il nostro nuovo corpus, DEPLAIN, perché negli ultimi anni sono emersi alcuni problemi nei corpus esistenti. Ad esempio, questi corpus sono troppo piccoli per addestrare un modello di semplificazione del testo. Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere error-proni negli allineamenti. Per questo motivo, proponiamo il nostro nuovo corpus DEPLAIN, che è suddiviso in due sottocorpus: DEPLAIN-apa e DEPLAIN-web. DEPLAIN-apa è basato su testi di notizie. In DEPLAIN-apa, abbiamo allineato manualmente 483 documenti, il che ha portato a circa 13.000 coppie parallele di frasi. Per DEPLAIN-web, questo corpus include diversi domini e abbiamo allineato manualmente e con metodi di allineamento automatici tutti i 750 documenti. In totale otteniamo 30.450 coppie di frasi. Abbiamo analizzato un po' di più le nostre coppie di frasi, ad esempio in base al tipo di semplificazione. Come si può vedere qui, i testi biblici sono semplificati molto di più rispetto, ad esempio, ai testi di notizie o ai testi per apprendenti della lingua. A tutti i livelli, riguardo ad esempio alla semplificazione lessicale, alla semplificazione strutturale e al livello complessivo di semplificazione. Inoltre, potete vedere che il nostro corpus DEPLAIN presenta una grande varietà di trasformazioni di semplificazione. Ad esempio, nel corpus DEPLAIN-apa abbiamo molte più riassegnazioni di ordine e aggiunte di parole rispetto a DEPLAIN-web. Dall'altra parte, nel corpus web abbiamo molte più riformulazioni. Ora vediamo cosa possiamo fare con questo corpus. Ciao, sono Omar e ora parlerò dei casi d'uso per il nostro dataset DEPLAIN. Per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico. Negli ultimi anni ci sono stati molti metodi di allineamento, ma nel contesto delle traduzioni automatiche, dove abbiamo due documenti paralleli scritti in due lingue diverse e vogliamo estrarre gli allineamenti delle frasi in entrambi i documenti. Tuttavia, nel nostro caso d'uso, cerchiamo di estrarre allineamenti tra frasi di due documenti paralleli scritti nella stessa lingua, con lo stesso contenuto, ma a diversi livelli di complessità. Ora che abbiamo il nostro dataset DEPLAIN, che contiene frasi allineate manualmente, possiamo utilizzare queste frasi come standard d'oro per valutare alcuni dei metodi di allineamento proposti. Abbiamo apportato alcune modifiche ai metodi proposti e abbiamo pubblicato tutte queste modifiche e i codici per eseguire i nostri esperimenti nel paper. Alla fine, abbiamo concluso che il miglior metodo di allineamento automatico da utilizzare per la semplificazione del testo in tedesco è il metodo MASSalign. Potete trovare anche il codice per eseguire questo metodo sui vostri documenti nel paper. Il secondo caso d'uso che abbiamo mostrato nel nostro paper è un caso di semplificazione automatica del testo mediante fine-tuning di modelli linguistici per produrre testi semplificati a partire da testi complessi. Abbiamo fine-tunato due modelli diversi. Abbiamo fine-tunato il modello long-mBART per produrre semplificazioni a livello di documento e abbiamo anche fine-tunato il normale base mBART per produrre semplificazioni a livello di frase. Potete trovare anche tutti i checkpoint e potete esaminare in dettaglio i punteggi e le metriche di valutazione dei nostri esperimenti nel paper. Abbiamo concluso che questo fine-tuning di base può produrre punteggi migliori rispetto ai punteggi di base e abbiamo proposto questi risultati come punto di riferimento iniziale per il problema della semplificazione automatica del testo nel futuro. Grazie molto per l'attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie.</sample>
    <sample id="4">Il nome della relatrice è Kayo Yin.</sample>
    <sample id="5">Il modello utilizzato per ottenere l'accuratezza dell'82%-87% è T5 XL.</sample>
    <sample id="6">**Abstract:**  
In questo lavoro, Jiaan e i suoi colleghi presentano un approccio innovativo per unificare le tecniche di sommario multilingue e cross-lingue in un framework più generale chiamato "many-to-many summarization". Questo approccio mira a sviluppare un unico modello di sommario in grado di processare documenti in qualsiasi lingua di origine e generare un riassunto in qualsiasi lingua di destinazione. L'analisi preliminare condotta sui dati WikiLingua mostra che il modello many-to-many è in grado di trasferire meglio le conoscenze tra diverse lingue rispetto alle strategie tradizionali. Per supportare questa nuova impostazione, i ricercatori hanno proposto PISCES, un modello pre-addestrato su tre fasi: meta-pre-addestramento, pre-addestramento cross-lingue e pre-addestramento specifico del compito. I risultati sperimentali dimostrano che PISCES supera diversi baselines, tra cui mBART-50 e mT5, in termini di performance. Inoltre, studi di ablazione e test umani confermano l'efficacia e la superiorità del modello. Questo lavoro rappresenta un passo significativo verso un modello di sommario universale in grado di gestire qualsiasi combinazione di lingue di origine e destinazione, aprendo la strada a nuove applicazioni multilingue e cross-lingue.</sample>
    <sample id="7">Sì, i tagger CoNLL-2003 funzionano ancora bene nel 2023, purché si utilizzino architetture moderne, modelli di grandi dimensioni e un numero sufficiente di esempi per il fine-tuning. Il calo di prestazioni osservato è principalmente dovuto al "temporal drift" (degrado temporale) e non all'overfitting adattivo.</sample>
    <sample id="8">La novità del metodo di valutazione umana proposto, ABC-Eval, consiste nell'annotare esplicitamente i comportamenti specifici dei modelli di chat, come la irrilevanza, le contraddizioni, le hallucinazioni e la mancanza di empatia, al fine di ridurre la soggettività e fornire una valutazione più precisa e dettagliata delle prestazioni dei modelli conversazionali.</sample>
    <sample id="9">Il successo dell'attuale approccio scarsamente supervisionato (WSL) si basa in larga misura sull'utilizzo di dati di validazione puliti, anche se spesso questa necessità viene trascurata.</sample>
    <sample id="10">Per migliorare il punteggio, possono essere fatti progressi fornendo ai modelli linguistici un accesso più completo e accurato alle conoscenze di fondo, simile a quelle degli annotatori, e migliorando la capacità dei modelli di comprendere e utilizzare espressioni riferimenti indirette. Inoltre, potrebbe essere utile migliorare la generalizzazione tra domini e rafforzare la comprensione del contesto.</sample>
    <sample id="11">**Abstract:**  
Jack Hessel presenta uno studio sull'abilità dei modelli linguistici di grandi dimensioni (LLM) di comprendere l'umorismo, utilizzando i dati del "New Yorker Caption Contest" come benchmark. Il lavoro, realizzato in collaborazione con diversi istituti, valuta tre compiti: matching (riconoscere la caption corretta per un disegno), quality ranking (valutare la qualità delle caption) e explanation generation (spiegare perché una caption è divertente). I risultati mostrano che i modelli, anche quelli avanzati come GPT-4, non raggiungono le prestazioni umane: ad esempio, un modello fine-tuned su CLIP ottiene circa il 62% di accuratezza nel matching, rispetto al 94% degli umani. Anche con descrizioni testuali dell'immagine, GPT-4 presenta errori significativi nelle spiegazioni degli scherzi, come evidenziato da test umani. Lo studio mette in luce una significativa lacuna tra l'abilità dei modelli di comprendere l'umorismo e quella umana, sottolineando la complessità del senso dell'umorismo come capacità cognitiva. Il dataset creato è reso disponibile con un leaderboard per ulteriori ricerche. L'opera sottolinea la necessità di migliorare l'understanding dell'umorismo nei modelli linguistici, un aspetto cruciale per applicazioni avanzate come l'interazione umano-macchina.</sample>
    <sample id="12">L'articolo è stato realizzato da 5 autori: Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow.</sample>
    <sample id="13">**Abstract:**  
Daniel Rotem presenta il lavoro "Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings", sviluppato nel laboratorio di Professor Roy Schwartz all'Università Ebraica di Gerusalemme. L'obiettivo è analizzare e migliorare le tecniche di inferenza adattiva per ridurre il tempo e i costi di inferenza nei modelli linguistici di grandi dimensioni. Due approcci comuni sono Multi Model e Early Exit: il primo utilizza diversi modelli separati, mentre il secondo applica classificatori intermedi all'interno dello stesso modello. Tuttavia, l'Early Exit soffre di "conflicting gradients", dove i segnali di gradiente da diversi classificatori interferiscono, riducendo le prestazioni. Per risolvere questo problema, l'articolo introduce SWEET (Separating Weights in Early Exit Transformers), un nuovo metodo di sintonizzazione che evita i conflitti di gradiente limitando gli aggiornamenti dei parametri ai soli classificatori successivi. I risultati mostrano che SWEET riduce il gap tra Early Exit e Multi Model, migliorando l'accuratezza, specialmente a velocità elevate. Inoltre, SWEET supera entrambi i metodi nel trade-off velocità-accuratezza, soprattutto per modelli BERT-Large. Il lavoro evidenzia l'esistenza di conflitti di gradiente nell'addestramento di Early Exit e propone una soluzione innovativa per migliorare l'efficienza dell'inferenza adattiva in contesti a risorse limitate.</sample>
    <sample id="14">Ciao, mi chiamo Adam Przepiórkowski e questo intervento è sulle Strutture di Dipendenza della Coordinatione. Come potreste sapere, esistono diverse strutture di dipendenza assunte da diverse teorie e approcci corpus. Per esempio, nelle Universal Dependencies, la struttura della coordinazione, come "Lisa, Bart e Maggie", è tale che il primo congiunto è il capo dell'intera struttura coordinata. In questo caso, Lisa. Un approccio simile è adottato da Igor Mel'čuk nella sua teoria del significato e del testo, dove anche qui l'intera struttura coordinata è guidata dal primo congiunto. Questi due approcci sono asimmetrici. Sì, essi distinguono uno dei congiunti. Ora, questi sono approcci asimmetrici alle strutture coordinate, come l'approccio di Praga. L'approccio "congiunzione come capo" adottato nei Praga Dependency Treebanks, dove le strutture coordinate sono guidate dalla congiunzione stessa. Otteniamo quindi alcune dipendenze da essa a tutti i congiunti. Infine, c'è anche un approccio multi-capo che viene utilizzato, per esempio, nella Word Grammar di Hudson, dove dicono che tutti i congiunti sono capi della struttura coordinata. Otteniamo quindi dipendenze dal governatore, qui "lo ama" a tutti i congiunti separatamente: Lisa, Bart e Maggie. L'obiettivo di questo articolo è produrre un nuovo argomento a favore delle strutture simmetriche della coordinazione, come queste due, e contro le strutture asimmetriche della coordinazione, come queste due. Bene. L'argomento si basa sul principio della minimizzazione della lunghezza delle dipendenze che spiegherò in base a questi esempi. In inglese, come potreste sapere, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli adjunct possono essere più lontani. Quindi "Marge read it yesterday" è corretto perché l'oggetto diretto è vicino al verbo, mentre "Marge read yesterday it" è molto peggio. Sì? Perché qui tra il verbo e l'oggetto diretto c'è un adjunct: "yesterday". Tuttavia, questo effetto può essere attenuato quando l'oggetto diretto è molto pesante e lungo. Perché allora può essere spostato in una posizione dopo l'adjunct. Questo è illustrato qui. Entrambi questi enunciati sono accettabili. "Marge read this absolutely fascinating book about bees yesterday." È okay, invece di "it", abbiamo questo lungo NP. Ma è anche okay dire "Marge read yesterday this absolutely fascinating book about bees." La ragione qui è che questo è possibile perché anche se questa frase viola il principio generale che gli oggetti diretti dovrebbero essere vicini al verbo, soddisfa il principio di minimizzazione della lunghezza delle dipendenze, che dice che sono preferite le dipendenze più corte. Questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quelle che non sono costanti tra queste due strutture. Qui abbiamo una dipendenza da "read" all'adjunct di lunghezza 7 misurata in parole e da "read" a "book" di lunghezza 4, quindi in totale 11. Quando scambiamo questi due costituenti, la somma di queste due dipendenze diventa 6. Invece di 11, 6 è molto più breve. Per questo motivo suona abbastanza bene. Sì? Violano un principio, ma soddisfano un altro. Bene. Quindi ciò che abbiamo fatto è estratto diverse statistiche sulla coordinazione dalla versione migliorata del Penn Treebank e vedere l'articolo "Why wouldn't you use universal dependencies" e queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti a sinistra tendano ad essere più brevi. Per esempio, "sale e pepe" e non "pepe e sale", misurati in sillabe. Inoltre, l'osservazione fatta nell'analisi sintattica che questa tendenza cresce con la differenza di lunghezza. Quindi, quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto più breve preferisce essere il primo, più forte, giusto? Quindi la proporzione è maggiore del congiunto breve a sinistra. Ma ciò che è nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando il governatore è a sinistra o assente. Giusto? Il governatore è a sinistra in questo esempio "I saw Bart and Lisa" quindi il governatore è a sinistra. È assente nell'esempio successivo "Homer came and sneezed." Qui abbiamo una coordinazione di due verbi e non c'è un governatore esterno. In questi casi, il congiunto a sinistra preferisce essere più breve; la maggior parte della differenza più grande tra i due congiunti. Tuttavia, quando il governatore è a destra, come qui, "laughed" governa la coordinazione Ted e Ned, questo effetto scompare. Abbiamo mostrato questo misurando la lunghezza in caratteri, la prima colonna, in sillabe la colonna centrale, e in parole la colonna di destra. Mi concentrerò su quest'ultima. Quello che vediamo qui è che quando il governatore è a sinistra, la tendenza per il congiunto a sinistra di essere più breve cresce costantemente, con la differenza assoluta in parole, e lo stesso si osserva quando non c'è un governatore, come nella coordinazione di frasi. Ma quando il governatore è a destra, questa tendenza scompare. E mostriamo nell'articolo come questo fornisce un argomento contro le strutture asimmetriche della coordinazione, come queste due, e per le strutture simmetriche, come queste due. Consultate l'articolo per gli argomenti completi. Parlate con noi durante la sessione del poster. Grazie.</sample>
    <sample id="15">L'articolo è stato scritto da Matthias Lindemann, Alexander Koller e Ivan Titov, quindi sono coinvolti 3 autori.</sample>
    <sample id="16">I testi biblici risultano più semplificati rispetto ai testi di notizie o ai testi per apprendenti della lingua.</sample>
    <sample id="17">**Abstract**  
Shengqiong Wu introduce un nuovo approccio per l'estrazione relazionale multimodale, un compito che mira a identificare relazioni semantiche tra entità utilizzando informazioni testuali e visive. Nelle applicazioni reali, come i social media, i dati spesso includono diversi modelli, rendendo insufficiente l'analisi basata solo sul testo. Per affrontare questo problema, l'approccio proposto integra informazioni visive, come immagini che mostrano un "bachelor", "gown" o "cap", per migliorare la comprensione contestuale. Tuttavia, esistono due sfide principali: l'overutilizzo di informazioni interne e l'underutilizzo di informazioni esterne. Per risolverle, l'approccio propone un principio di *Graph Information Bottleneck* per filtrare e raffinare le informazioni interne, e un modello di argomenti multimodali per integrare informazioni esterne, come parole chiave tematiche. Il framework proposto include la costruzione di un grafo cross-modale unificato (CMG), il raffinamento fine-grained del CMG, e l'enrichimento con caratteristiche tematiche multimodali. Gli esperimenti su dataset MRE dimostrano che il metodo migliora significativamente le prestazioni rispetto ai metodi basati solo sul testo e ai baselines multimodali. Lo studio di ablazione rivela che entrambi i moduli—screening interno ed estrazione esterna—contribuiscono positivamente. Inoltre, l'approccio mostra maggiore efficacia in contesti con bassa rilevanza testo-immagine, dove l'informazione esterna è più utile. Questo lavoro introduce un'idea innovativa per l'estrazione relazionale multimodale, combinando la riduzione e l'arricchimento delle informazioni.</sample>
    <sample id="18">L'esempio della preferenza per i congiunti a sinistra più brevi è "salt and pepper" rispetto a "pepper and salt", dove il congiunto a sinistra ("salt") è più breve del congiunto a destra ("pepper"). Questa tendenza è rafforzata quando la differenza di lunghezza tra i due congiunti aumenta.</sample>
    <sample id="19">**Abstract:**  
Zhang Qin, una studentessa del master presso l'Università di Shenzhen, presenta il lavoro "A Survey for Efficient Open Domain Question Answering", accettato all'ACL 2023. Lo studio si concentra sui sistemi di risposta a domande in dominio aperto, che tipicamente utilizzano un framework a due stadi: un modulo di recupero e un modulo lettore. Tuttavia, tali sistemi affrontano sfide significative, come la gestione di un corpus di Wikipedia molto grande, la dimensione elevata degli indici e l'uso di modelli linguistici pesanti, che rendono complessa l'applicazione in dispositivi con risorse limitate. L'obiettivo del lavoro è sviluppare sistemi efficienti, con minori costi di memoria, inferenza più veloce e prestazioni paragonabili. Vengono presentate diverse strategie per migliorare l'efficienza: tecniche di ricerca approssimata, lettura selettiva, riduzione della dimensione degli indici e ottimizzazione del modello attraverso modelli leggeri o distillazione del conoscimento. Vengono confrontati diversi approcci, tra cui sistemi a un solo stadio, come quelli basati esclusivamente sul recupero o sulla generazione. Le conclusioni suggeriscono che, a seconda delle risorse disponibili e delle esigenze di prestazioni, si possa optare per sistemi a un solo stadio o per soluzioni ibride. Infine, vengono proposti due ambiti futuri di ricerca: l'adattamento ai dispositivi a basso consumo energetico e l'introduzione di nuovi metriche di valutazione.</sample>
    <sample id="20">Sì, i modelli sono disponibili gratuitamente su Hugging Face e sono rilasciati con la licenza MIT, quindi puoi usarli per la tua ricerca.</sample>
    <sample id="21">DEPLAIN-apa contiene testi basati su notizie.</sample>
    <sample id="22">I fattori che contribuiscono a una buona generalizzazione sono:  
1. Architettura del modello (i modelli Transformer generalizzano meglio).  
2. Dimensione del modello (modelli più grandi tendono a generalizzare meglio).  
3. Numero di esempi di fine-tuning (più esempi portano a una migliore generalizzazione).</sample>
    <sample id="23">In questo lavoro, Dan Garrette e il team analizzano le limitazioni dei modelli di generazione immagini basati su testo, in particolare il modello Imagen, che utilizza un encoder T5-XXL per tradurre il testo in rappresentazioni utilizzabili da un modello a diffusione. Pur essendo in grado di produrre immagini di alta qualità, tali modelli mostrano una scarsa capacità di rappresentare correttamente il testo all'interno delle immagini. L'analisi rivela che il problema risiede nell'encoder T5, che utilizza la tokenizzazione SentencePiece, decomponendo le parole in sottoparole e non in caratteri singoli, rendendo difficile per il modello ricostruire correttamente la grafia delle parole. Test su diversi modelli mostrano che T5 ha una bassa accuratezza nella "spelling" (fino al 70% per la versione XXL), mentre modelli come ByT5, che operano a livello di byte, riescono a replicare con alta precisione i caratteri di input. Per migliorare la capacità di generare testo visivo, i ricercatori hanno integrato nel modello Imagen una rappresentazione aggiuntiva proveniente da ByT5-small, aumentando di poco il numero di parametri ma migliorando notevolmente la capacità di generare testo. Il lavoro introduce inoltre due nuovi benchmark, WikiSpell e DrawText, e propone una strategia efficiente per migliorare la capacità di spelling nei modelli di generazione immagini.</sample>
    <sample id="24">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata in base alla lunghezza in parole, sillabe e caratteri, analizzando i dati estratti dall'Enhanced Penn Treebank. I risultati mostrano che quando il governatore è a sinistra o assente, il congiunto a sinistra tende a essere più breve, e questa tendenza aumenta con la differenza di lunghezza tra i due congiunti.</sample>
    <sample id="25">Gli esperimenti sono stati progettati analizzando statistiche estratte dall'enhanced version of the Penn Treebank, focalizzandosi sulla posizione del governatore rispetto ai congiunti. Sono state misurate le lunghezze dei congiunti in caratteri, sillabe e parole, confrontando i casi in cui il governatore era a sinistra, assente o a destra. Questo ha permesso di osservare come la tendenza del congiunto di sinistra a essere più breve dipenda dalla posizione del governatore.</sample>
    <sample id="26">Un classificatore base addestrato solo su 43 esempi di dissonanza (su un dataset non bilanciato) non performa significativamente meglio del caso casuale.</sample>
    <sample id="27">L'articolo non specifica il numero di autori coinvolti.</sample>
    <sample id="28">I nomi dei personaggi nella conversazione presa a esempio sono Bob e Alice.</sample>
    <sample id="29">I modelli di traduzione sensibili al contesto migliorano rispetto a quelli indipendenti nel gestire fenomeni del discorso come la formalità e la coesione lessicale.</sample>
    <sample id="30">**Abstract:**  
L'articolo presenta LLM-Blender, un semplice ma efficace framework per l'apprendimento ensemble di modelli linguistici di grandi dimensioni. L'idea chiave si basa su una fusione generativa e su un ranking pairwise, che permette di selezionare i modelli più adatti per ogni input specifico. Molti modelli linguistici, pur mostrando buone prestazioni medie, non si comportano sempre in modo ottimale su singoli esempi. LLM-Blender propone un approccio a due fasi: nella prima fase, un modulo chiamato PairRanker confronta le risposte di diversi modelli per ogni input, utilizzando un'architettura di cross-attention (ad esempio RoBERTa) per determinare quale risposta è più adatta. Questo modulo genera una matrice di confronti, da cui si ottiene un ordine finale dei modelli. Nella seconda fase, i top K modelli vengono fusi in un modello generativo (GenFuser) che produce l'output finale. I risultati sperimentali mostrano che LLM-Blender supera in modo significativo i modelli singoli e altri metodi di ranking, anche su metriche automatiche e giudizi umani. Inoltre, è stato creato un nuovo dataset, MixInstruct, per valutare tali framework. LLM-Blender dimostra quindi un elevato potenziale come soluzione semplice e performante per l'ensemble learning nei modelli linguistici di grandi dimensioni.</sample>
    <sample id="31">Gli autori dell'articolo sono affiliati a istituzioni accademiche e di ricerca, tra cui l'Università di Stanford, il MIT, l'Università di Washington e altre istituzioni di ricerca in ambito linguistico e informatico. Tuttavia, nel testo fornito non vengono specificate le affiliazioni esatte degli autori.</sample>
    <sample id="33">Il framework NLPositionality quantifica la posizionalità confrontando le annotazioni di utenti diversi (raccolte attraverso Lab in the Wild) con quelle dei dataset e dei modelli NLP, utilizzando il coefficiente di correlazione di Pearson (Pearson's R). Questo permette di valutare quanto i modelli e i dataset siano allineati con le opinioni di diverse popolazioni in base a caratteristiche demografiche come paese di origine, livello di istruzione, genere, ecc.</sample>
    <sample id="34">**Abstract:**  
In questo lavoro, presentiamo CREST, un framework congiunto per la razionalizzazione selettiva e la generazione di testi contrapposizionali. CREST combina due approcci complementari: la razionalizzazione, che identifica i token rilevanti per la decisione di un classificatore, e la generazione di contrapposizioni, che modifica specifiche parti dell'input per produrre esempi alternativi. Il framework utilizza un modello razionalizzatore con un componente maschera addestrabile per generare spiegazioni significative e un editor basato su un masked language model per creare esempi contrapposizionali. I risultati sperimentali mostrano che i contrapposizioni generati da CREST sono validi e naturali, come confermato da valutazioni umane. Inoltre, CREST-Rationalization, che utilizza sia esempi fattuali che contrapposizionali durante l'addestramento, migliora le prestazioni dei modelli downstream su dati in-domain, contrastive e out-of-domain. Un'analisi della spiegabilità rivela che le razionalizzazioni prodotte da CREST sono più plausibili e simularle in modo contrappositivo modifica efficacemente le decisioni del classificatore. Questo dimostra che CREST genera spiegazioni interpretabili e contrapposizioni utili per migliorare la comprensione e l'addestramento dei modelli di NLP.</sample>
    <sample id="36">**Abstract:**  
Nel lavoro "Learning Language-Specific Layers for Multilingual Machine Translation", Telmo Pessoa Pires e i suoi collaboratori propongono un approccio innovativo per migliorare la traduzione multilingue senza aumentare i costi di inferenza. L'idea centrale è l'introduzione di strati specifici per ciascuna lingua (Language-Specific Layers, o LSL), che permettono di allocare capacità aggiuntive solo dove necessario. Durante l'inferenza, solo lo strato relativo alla lingua corrente (sorgente o target) viene utilizzato, mantenendo così un costo computazionale costante. Per determinare la posizione ottimale degli LSL all'interno del modello, i ricercatori hanno adottato un approccio basato sull'apprendimento automatico, valutando i pesi dei parametri durante la fase di addestramento. I risultati sperimentali mostrano che l'architettura appresa migliora significativamente le prestazioni rispetto ai modelli tradizionali e agli adattatori linguistici, soprattutto per le lingue a risorse limitate. I test sono stati condotti su un insieme di 10 lingue, includendo lingue europee, asiatiche e swahili, con valutazioni su metriche come chrF, spBLEU e COMET. L'approccio proposto si dimostra statisticamente significativo in 84 su 90 direzioni di traduzione, dimostrando un'efficacia generale e un miglioramento particolarmente marcato per le lingue a basso risorse. Questo lavoro apre la strada a modelli multilingue più efficienti e adattabili.</sample>
    <sample id="37">Lo studio precedente ha mostrato che i soggetti umani, quando dati gli stessi prompt per generare una persona, hanno espresso stereotipi razziali, permettendo un confronto diretto tra le risposte umane e quelle generate dai modelli.</sample>
    <sample id="38">L'unico dato citato è il Penn Treebank.</sample>
    <sample id="39">L'autore menzionato è Adam Przepiórkowski, quindi è coinvolto solo un autore nell'articolo.</sample>
    <sample id="40">Le attività strettamente correlate alla dissonanza cognitiva sono:

1. **Classificazione dello stato di dissonanza indipendente dal tema** (determinare se due affermazioni di dibattito sono in accordo o in disaccordo, indipendentemente dal tema).
2. **Classificazione binaria delle classi di espansione e confronto del PDTB** (che sono strettamente correlate alle concezioni di consonanza e dissonanza).</sample>
    <sample id="41">**Abstract:**  
In questo lavoro, presentiamo PeaCoK, un Grafo della Conoscenza Commonsense basato sulle Persone, progettato per rappresentare conoscenze di mondo rilevanti per la generazione di narrazioni coerenti e coinvolgenti. La comprensione delle personalità dei partecipanti (come parlanti, ascoltatori o personaggi) è cruciale per creare dialoghi e storie realistiche. Tuttavia, i sistemi di elaborazione del linguaggio naturale non hanno ancora appreso rappresentazioni efficaci delle personalità reali, che coinvolgono conoscenze complesse e interconnesse. PeaCoK include circa 3.800 personalità e 40.000 attributi, con oltre 100.000 fatti inferenziali e interconnessioni tra personalità. Costruito in tre fasi, il grafo integra conoscenze da grafi commonsense esistenti, modelli linguistici pre-addestrati e annotazioni umane. I test mostrano che l'addestramento di modelli come Comet-BART su PeaCoK migliora significativamente le prestazioni automatiche e umane rispetto a modelli basici come GPT-3. Inoltre, l'applicazione di PeaCoK a compiti di generazione dialogica (es. su ConvAI2) dimostra miglioramenti in fluency, coerenza, coinvolgimento e espressione delle personalità. Rispetto a grafi commonsense generali, PeaCoK ha un impatto più positivo grazie alla sua natura centrica sulle personalità. L'analisi mostra che un maggiore numero di attributi condivisi tra interlocutori porta a conversazioni più coerenti e coinvolgenti, sottolineando l'importanza della conoscenza interconnessa delle personalità per la narrazione. PeaCoK offre così una risorsa utile per addestrare modelli di generazione della conoscenza e migliorare la modellazione narrativa.</sample>
    <sample id="42">L'articolo non specifica il numero di autori coinvolti.</sample>
    <sample id="43">L'informazione sul numero di autori non è specificata nel testo fornito.</sample>
    <sample id="44">Il framework NLPositionality differisce dai lavori precedenti confrontando le annotazioni degli utenti finali con quelle dei modelli e dei dataset, anziché concentrarsi solo sull'accordo tra annotatori o sulla modellazione delle loro distribuzioni.</sample>
    <sample id="45">La configurazione che si sovrappone maggiormente al lessico degli stereotipi è quella delle persone generate (generated personas).</sample>
    <sample id="46">I sistemi commerciali messi a confronto sono DeepL e Google Translate.</sample>
    <sample id="47">Ciao, sono Shangbin, dottorando all'Università di Washington. Oggi sto presentando il nostro lavoro intitolato "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models" (Da dati di preaddestramento a modelli linguistici a compiti downstream: tracciare le tracce dei pregiudizi politici che portano a modelli NLP ingiusti). I modelli linguistici vengono addestrati su grandi quantità di dati provenienti da web crawls. I media di notizie politiche sono ben rappresentati nei dati di preaddestramento. Secondo un sondaggio del corpus C4, possiamo vedere che il New York Times, il Los Angeles Times, The Guardian, Huffington Post e così via sono ben coperti nei dati di addestramento dei modelli linguistici. Questo ha creato una situazione a doppio taglio per le applicazioni dei modelli linguistici. Da un lato, essi sono riusciti a imparare da prospettive diverse, celebrazione della democrazia e della pluralità di idee. Dall'altro lato, queste diverse opinioni politiche sono intrinsecamente socialmente pregiudicate e potrebbero portare a problemi di equità nelle applicazioni dei compiti downstream. A questo proposito, proponiamo di investigare il percorso di propagazione dei pregiudizi politici dai dati di preaddestramento ai modelli linguistici ai compiti downstream, specificamente ponendo le seguenti domande: in primo luogo, come valutiamo l'inclinazione politica dei modelli linguistici e qual è il ruolo che i dati di preaddestramento potrebbe avere su tali pregiudizi politici? In secondo luogo, come si comportano i modelli linguistici con diverse inclinazioni politiche nei compiti downstream e se ciò potrebbe portare a problemi di equità nelle applicazioni NLP? Specificamente, abbiamo prima proposto di interrogare i modelli linguistici con diversi formati di prompt utilizzando questionari politici, come il test sulle conferenze politiche. Questo ci permette di effettuare una valutazione automatica ben fondata sulla letteratura scientifica politica. Alcuni risultati preliminari dimostrano che, in primo luogo, i modelli linguistici hanno inclinazioni politiche variabili. Occupano tutti e quattro i quadranti del campus politico. Possiamo anche vedere che GPT-4 è il modello linguistico più liberale tra tutti, e la serie GPT è in generale più socialmente liberale rispetto alla serie BART e alle sue varianti. In secondo luogo, intendiamo investigare in che misura i pregiudizi politici dei modelli linguistici siano effettivamente acquisiti dai dati di addestramento. Possiamo condurre un esperimento controllato addestrando ulteriormente i checkpoint dei modelli linguistici su 6 diversi corpus partisani, divisi in notizie e social media, ulteriormente divisi per inclinazione politica. Addestrando ulteriormente i modelli linguistici su tali corpus partisani, possiamo vedere che le coordinate ideologiche del modello linguistico si spostano corrispondentemente. Ad esempio, per RoBERTa addestrata ulteriormente su un corpus Reddit di sinistra, possiamo notare un significativo spostamento a sinistra in termini di pregiudizi politici. Abbiamo anche cercato di investigare se i modelli linguistici possano acquisire la polarizzazione che è preponderante nella nostra società moderna. Dividiamo quindi i corpus di preaddestramento in quelli precedenti e successivi alla 45esima presidenza degli Stati Uniti. Addestriamo separatamente i modelli linguistici su due diversi corpus temporali. Possiamo vedere che i modelli linguistici in generale avevano un'inclinazione politica più lontana dal centro dopo il 2017. Questo indica che i modelli linguistici possono anche acquisire la polarizzazione della nostra società. Ultimo ma non meno importante, valutiamo i modelli linguistici con diverse inclinazioni politiche in compiti come la rilevazione dello speech d'odio e la rilevazione di notizie false, che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative. Vediamo che, se investighiamo le prestazioni per categoria, ovvero se dividiamo le prestazioni in diverse demografie o inclinazioni politiche dei media, possiamo notare un pattern. Ad esempio, per la rilevazione dello speech d'odio, i modelli linguistici di sinistra sono più bravi a rilevare lo speech d'odio mirato a gruppi sociali minoritari, ma peggio a rilevare lo speech d'odio mirato a gruppi più potenti nella società. Al contrario, i modelli linguistici di destra sono più bravi a rilevare lo speech d'odio mirato a bianchi e uomini, ma peggio a rilevare lo speech d'odio mirato a neri, LGBTQ+ e altre comunità minoritarie. Tendenze simili si verificano anche per la rilevazione di notizie false, dove vediamo che i modelli linguistici di sinistra sono più bravi a rilevare disinformazione proveniente da un'opinione politica opposta e viceversa. Mostriamo inoltre molti esempi qualitativi per mostrare che i modelli linguistici con diverse inclinazioni politiche danno effettivamente previsioni diverse per esempi di speech d'odio e disinformazione basati sulle loro categorie sociali. Ci sono molti altri esempi nell'appendice per sottolineare ulteriormente che ciò indica che esiste un problema di equità molto urgente legato ai pregiudizi politici dei modelli linguistici. Ad esempio, se i modelli linguistici di destra vengono sintonizzati su speech d'odio o disinformazione o altro e vengono distribuiti su una piattaforma sociale popolare, ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere marginalizzate e lo speech d'odio mirato a gruppi minoritari potrebbe semplicemente proliferare senza controllo. Questo ha suonato l'allarme affinché riconoscessimo e affrontassimo i problemi di equità derivanti dalle inclinazioni politiche dei modelli linguistici. Un breve momento di discussione. Vorremmo anche sottolineare che esponiamo un dilemma unico riguardo ai pregiudizi politici dei modelli linguistici. È come stare tra Scilla e Cariddi. Se non sanificiamo le opinioni politiche nei dati di addestramento dei modelli linguistici, i pregiudizi si propagheranno dai dati di preaddestramento ai modelli linguistici ai compiti downstream, creando alla fine problemi di equità. Se cerchiamo di sanificare in qualche modo, rischiamo anche la censura o l'esclusione. È incredibilmente difficile determinare cosa sia effettivamente neutrale e debba essere mantenuto nei dati di monitoraggio linguistico. È un po' come il problema del trolley elettrico. Okay, molto bene. Penso che questo sia tutto per oggi. Grazie per il vostro tempo.</sample>
    <sample id="48">L'autore menziona di lavorare con i suoi colleghi da Google Translate, ma non specifica il numero esatto di autori coinvolti nell'articolo. Pertanto, non è possibile determinare con certezza quanti autori sono coinvolti.</sample>
    <sample id="49">Le valutazioni MPP sono state eseguite fino a 1024 token di lunghezza del contesto.</sample>
    <sample id="50">**Abstract:**  
Il lavoro presenta DEPLAIN, un nuovo corpus per la semplificazione del testo in tedesco, disponibile a livello di documento e di frase. DEPLAIN è stato creato per rispondere alle limitazioni dei corpus esistenti, come la scarsa dimensione e l’affidabilità delle allineamenti automatici. Il corpus è suddiviso in due sottocorpi: DEPLAIN-apa, basato su testi giornalistici con 13.000 coppie di frasi allineate manualmente, e DEPLAIN-web, che include diversi domini con 30.450 coppie di frasi, allineate sia manualmente che automaticamente. L’analisi ha rivelato variazioni significative nel tipo e nel grado di semplificazione, con testi biblici semplificati in modo più marcato rispetto a quelli giornalistici. Il corpus è stato utilizzato per valutare metodi di allineamento automatico, con il miglior risultato ottenuto con MASSalign. Inoltre, DEPLAIN è stato impiegato per addestrare modelli linguistici (come long-mBART e mBART) per la semplificazione automatica del testo, ottenendo risultati superiori ai benchmark esistenti. Questi risultati costituiscono un punto di riferimento per futuri studi sulla semplificazione automatica del tedesco. Il lavoro offre un risorsa utile per la ricerca e lo sviluppo di strumenti NLP mirati a migliorare la comprensione del testo da parte di specifici gruppi di destinatari.</sample>
    <sample id="51">I domini inclusi nel loro set di dati sono musica, libri e ricette.</sample>
    <sample id="52">La posizionalità è definita come l'insieme di prospettive che una persona possiede in base alle sue caratteristiche demografiche, identità e esperienze di vita.</sample>
    <sample id="53">Il nome del relatore è Dawei.</sample>
    <sample id="54">**Abstract:**  
In questo lavoro, presentiamo un approccio basato sul transfer learning e sull'active learning per affrontare il problema della rara classe nella rilevazione della dissonanza cognitiva in testi naturali. La dissonanza cognitiva si verifica quando due credenze o azioni sono inconsistenti, e rappresenta un fenomeno rilevante per comprendere decisioni, cambiamenti di atteggiamento e salute mentale. Tuttavia, l'espressione della dissonanza nei testi è estremamente rara, rendendo difficile la creazione di un dataset adeguato. Per superare questa sfida, abbiamo annotato un dataset di circa 1.000 coppie di unità discorsive, in cui la dissonanza è presente in soli il 3,5%. Utilizzando un modello iniziale addestrato su un numero limitato di esempi, abbiamo osservato prestazioni inferiori al caso. Per migliorare il rilevamento, abbiamo combinato transfer learning da compiti correlati (come classificazione di posizione in dibattiti e relazioni di espansione e confronto) con active learning. Abbiamo proposto una strategia di selezione degli esempi basata sulla probabilità della classe rara (PRC), che ha dimostrato di superare altre strategie esistenti. Con l'iterazione dell'active learning, il modello ha raggiunto un AUC di 0,75, il miglior risultato ottenuto finora. I risultati mostrano che il transfer learning ben progettato e la strategia PRC sono efficaci per il rilevamento della dissonanza, anche in contesti con dati rari. Questo lavoro apre la strada a future applicazioni in analisi del linguaggio e salute mentale.</sample>
    <sample id="55">Sì, EDAtt adatta un modello ST offline esistente senza doverlo riallineare o adottare un'architettura specifica per la SimulST.</sample>
    <sample id="56">Non è possibile determinare il numero di autori dall'estratto fornito. L'informazione non è inclusa nel testo.</sample>
    <sample id="57">No, il modello testato non funziona bene sulla suite di test senza addestramento specifico su KITMUS. Tuttavia, con un addestramento specifico, alcuni modelli riescono a migliorare significativamente le prestazioni.</sample>
    <sample id="58">Le tre varianti di KITMUS sono:

1. **Background-Pretrain**: la conoscenza di fondo è disponibile solo durante la preformazione.
2. **Background-Both**: la conoscenza di fondo è disponibile sia durante la preformazione che all'inferenza.
3. **Background-Inference**: la conoscenza di fondo è disponibile solo all'inferenza.</sample>
    <sample id="59">**Abstract:**  
In questo lavoro, presentiamo DrBERT, il primo modello pre-addestrato in francese specializzato per ambiti biomedici e clinici, basato su RoBERTa e addestrato sul dataset NACHOS, composto da dati medici estratti dal web. Il nostro studio valuta l’efficacia di diversi approcci di pre-addestramento, confrontando DrBERT con modelli come ChuBERT, basati su dati clinici anonimizzati, e modelli pre-addestrati su dati eterogenei o in inglese (PubMedBERT, BioBERT, ClinicalBERT). Sono state analizzate sette versioni di modelli, tra cui quattro addestrati da zero e tre basati su pre-addestramento continuo. I risultati mostrano che i modelli addestrati da zero, in particolare DrBERT, superano i modelli generici come CamemBERT su nove su undici compiti downstream, tra cui riconoscimento di entità nominate, classificazione e risposta a domande. I dati eterogenei sembrano offrire maggiore versatilità, mentre l’uso di dati specializzati non si scalano bene. Inoltre, DrBERT addestrato su 4 GB di NACHOS ha prestazioni paragonabili a quelle di un modello addestrato da zero, ma non è il caso per i modelli basati su CamemBERT. Tutti i modelli sono disponibili gratuitamente su Hugging Face, con licenza MIT, e i codici di addestramento sono accessibili su GitHub. Questo lavoro apre la strada a modelli linguistici avanzati in francese per domini medici.</sample>
    <sample id="60">Gli autori dell'articolo sono Javad Hosseini, Filip Radlinski, Silvia Pareti e Annie Louis. Non sono specificate le loro affiliazioni nell'estratto fornito.</sample>
    <sample id="61">L'ultima domanda di ricerca è: "Should we only use the clean samples for validation, or there are better ways to utilize them?" (Dovremmo utilizzare solo i campioni puliti per la validazione, oppure esistono modi migliori per utilizzarli?)</sample>
    <sample id="62">**Abstract:**  
Nitay Calderon e i suoi collaboratori presentano uno studio sistematico sull'applicazione della distillazione del knowledge per la generazione del linguaggio naturale (NLG), con l'obiettivo di comprimere modelli linguistici grandi e complessi senza comprometterne le prestazioni. L'approccio proposto si concentra sull'utilizzo di pseudo-target generati da un modello "insegnante" per addestrare un modello "studente" più piccolo. Lo studio si distingue per il suo focus su contesti realistici, caratterizzati da set di dati con risorse limitate, grandi quantità di dati non etichettati e modelli di dimensioni moderate, mirando a un'efficienza computazionale elevata. Vengono analizzate diverse strategie di distillazione, tra cui la generazione di più pseudo-target anziché uno solo, il campionamento con alta temperatura per aumentare la diversità, e una tecnica innovativa chiamata *joint-teaching*, che combina distillazione a livello di parole e pseudo-target generati sia dall'insegnante che dallo studente, per ridurre il bias di esposizione e migliorare la capacità correttiva dello studente. Lo studio include quattro compiti NLG (sommarizzazione, generazione di domande, ragionamento comune e trasferimento di stile), e mostra come l'uso di dati non etichettati e l'adeguata selezione delle strategie di distillazione possano significativamente migliorare le prestazioni del modello compresso. Il lavoro offre una ricetta pratica per l'applicazione della distillazione nel contesto della NLG, fornendo un contributo significativo alla compressione dei modelli linguistici.</sample>
    <sample id="63">La sensibilità della metrica misura la capacità del modello di produrre output coerenti per lo stesso compito, indipendentemente da lievi variazioni nel testo delle istruzioni.</sample>
    <sample id="64">Il nome della relatrice è Jingwei Yi.</sample>
    <sample id="65">Una maggiore sensibilità indica che il modello produce output meno coerenti per lo stesso compito in base a variazioni lievi nelle istruzioni, suggerendo una performance peggiore. Quindi, una minore sensibilità è desiderabile e indica una performance migliore.</sample>
    <sample id="66">**Abstract**  
This paper presents a comprehensive survey on the application of deep learning techniques to mathematical reasoning, a crucial component of human intelligence involving the comprehension and decision-making based on numerical and linguistic data. The study explores various aspects of mathematical reasoning, including text-based problems, visual and tabular contexts, and geometric reasoning, which require the integration of symbolic knowledge and neural networks. It highlights neuro-symbolic approaches for solving geometric problems and discusses automated theorem proving as another key area. Recent advances in neural architectures, such as sequence-to-sequence and sequence-to-tree models, have enabled more structured representations of mathematical expressions. The emergence of large language models (LLMs) has further expanded the possibilities, with techniques like chain-of-thought prompting and self-consistency decoding improving reasoning capabilities. However, LLMs still face challenges in precise mathematical reasoning, prompting the development of augmented models, such as program-aided LLMs and Chameleon, which integrate external tools to enhance performance. While progress has been made, mathematical reasoning in low-resource languages and specialized domains remains underexplored. Additionally, generalization and robustness issues persist, particularly with large numbers and consistency in reasoning. This survey aims to provide a holistic overview of current research trends and challenges in the field of deep learning for mathematical reasoning.</sample>
    <sample id="67">**Abstract:**  
In this work, we investigate the phenomenon of interference in multilingual translation models, where training on one language pair can either enhance or hinder performance on another. While previous studies have proposed methods to mitigate interference, their effectiveness is often limited, particularly in small models. Our analysis identifies key factors influencing interference, such as model size, data scale, and language similarity. We find that severe interference occurs primarily when the model is under-scaled relative to the data size, and that tuning the sampling temperature plays a critical role in improving performance. Through extensive experiments with Transformer-based models across 15 languages from the WMT dataset, we show that language similarity has a limited impact on interference, and that increasing the model and data size significantly reduces interference. Additionally, we demonstrate that adjusting the sampling temperature—especially using values around 5—can effectively balance training across language pairs, reducing the negative effects of interference without requiring specialized algorithms. Our results suggest that interference is mainly a problem of scale and calibration, and that modest increases in model size combined with tuned temperature sampling are sufficient to achieve strong multilingual translation performance. These findings offer practical insights for training efficient and effective multilingual models.</sample>
    <sample id="68">Durante il pre-addestramento, ai modelli vengono messi a disposizione contesti linguistici diversi, tra cui frasi grammaticali e non grammaticali da dataset specifici (come BLiMP o SyntaxGym), nonché contesti estratti da fonti esterne come Wikipedia, per valutare come i modelli giudicano l'accettabilità linguistica in contesti sempre più lunghi e vari.</sample>
    <sample id="69">In genere, sono necessari circa 20 campioni per classe per raggiungere buone prestazioni in WSL.</sample>
    <sample id="70">Gli autori dell'articolo sono Myra, Esin Durmus e Dan Jurafsky.</sample>
    <sample id="71">**Abstract:**  
Javad Hosseini e i suoi collaboratori presentano il lavoro "Resolving Indirect Referring Expressions for Entity Selection", in cui introducono il corpus AltEntities, un dataset pubblico creato per studiare come gli utenti utilizzino riferimenti indiretti per selezionare entità in contesti conversazionali. L'obiettivo è comprendere meglio le strategie linguistiche usate quando gli utenti non riescono o non vogliono fare riferimento diretto a un'entità, come un brano musicale, un libro o una ricetta. Il dataset, raccolto tramite annotazione di massa, copre tre domini e include 6.000 domande alternative e 42.000 espressioni riferitorie indirette. La metodologia prevede un setup informale basato su un cartone animato con tre bolle di dialogo: la prima stabilisce il contesto, la seconda presenta due opzioni, e la terza chiede all'annotatore di selezionarne una usando un riferimento indiretto. Gli annotatori vengono forniti di informazioni contestuali (come link di ricerca o testi da Wikipedia) per aiutarli a comprendere le entità. I risultati mostrano che i modelli linguistici, come T5 XL, raggiungono un'accuratezza elevata (92-95%) quando hanno accesso alla stessa conoscenza contestuale degli annotatori, ma solo intorno al 60% quando si basano solo sui nomi delle entità. Questo sottolinea l'importanza di migliorare la capacità dei modelli di comprendere riferimenti indiretti in contesti reali. Il dataset è disponibile per il download e rappresenta un utile benchmark per valutare l'intelligenza artificiale nel comprendere il linguaggio naturale.</sample>
    <sample id="72">È necessario sviluppare nuovi metodi per misurare i bias dell'informazione perché i modelli linguistici, addestrati su dati web con inclinazioni politiche, possono propagare questi bias nei compiti downstream, causando problemi di equità. I bias politici possono influenzare in modo distorto la rilevazione di discorsi d'odio o notizie false, marginalizzando determinati gruppi e compromettendo l'obiettività dei risultati. Senza strumenti adeguati per identificare e valutare tali bias, è difficile garantire un uso giusto e imparziale dei modelli linguistici.</sample>
    <sample id="73">Il nome della relatrice è Akshatha.</sample>
    <sample id="74">**Abstract**  
In this paper, we introduce *Dense-ATOMIC*, a densely-connected extension of the ATOMIC commonsense knowledge base, designed to enhance knowledge coverage and support multi-hop reasoning. While ATOMIC provides high-quality, event-centered inferential knowledge, it suffers from sparse connectivity due to the absence of B-to-B, A-to-B, and A-to-A links, limiting the number of multi-hop paths. To address this, we propose a three-step construction process: normalizing tail events, training a relation prediction model, and building Dense-ATOMIC. Our model, *Rel-CSKGC*, predicts relations between head and tail events using a pre-trained language model (RoBERTa) and MaxPooling, avoiding reliance on the sparse graph structure while leveraging semantic information. We also introduce an Intra- and Inter-Cluster Completion Strategy to efficiently infer missing links by grouping events into clusters. Evaluation results show that Rel-CSKGC outperforms both relation prediction and translation-based methods on automatic and human assessments. Dense-ATOMIC demonstrates significantly higher knowledge coverage and a larger number of 1-hop, 2-hop, and 3-hop paths compared to ATOMIC. Furthermore, it improves the performance of commonsense generation models like COMET, enabling more diverse and contextually rich outputs. Our experiments on multi-hop paths confirm the effectiveness of Dense-ATOMIC in supporting complex reasoning tasks. This work provides a valuable resource for advancing commonsense reasoning in AI systems.</sample>
    <sample id="75">**Abstract:**  
Zheng Yandan presents JointProp, a joint semi-supervised learning framework for Named Entity Recognition (NER) and Relation Extraction (RE) tasks. While supervised models have achieved good performance, they require extensive labeled data, which is costly and domain-specific. Semi-supervised approaches offer a more efficient alternative but often neglect the interdependencies between NER and RE. JointProp addresses this by leveraging the relationships between entities and relations through a heterogeneous graph structure. The framework consists of four components: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. Span features are generated from contextualized token representations, and a graph is built using k-Nearest Neighbor (kNN) to model relationships between labeled and unlabeled data. Label propagation is performed across this graph, allowing pseudo-labels to be iteratively refined until convergence. These pseudo-labels are then filtered based on confidence and used to retrain the model. Experiments on four datasets—both joint and single-task—show that JointProp significantly outperforms baseline models, especially in semi-supervised settings. The framework demonstrates the benefits of jointly modeling NER and RE, improving performance by exploiting the interconnections between tasks and unlabeled data. The results highlight the effectiveness of integrating heterogeneous graph-based label propagation in semi-supervised information extraction.</sample>
    <sample id="76">L'infrastruttura di propagazione dei bias politici ha un aspetto a catena, che va dal dati di pre-addestramento ai modelli linguistici e poi alle applicazioni downstream. I bias politici presenti nei dati di pre-addestramento (come articoli di stampa e social media) vengono acquisiti e rafforzati nei modelli linguistici, che a loro volta possono influenzare in modo non equo le prestazioni in compiti come la rilevazione dello speech d'odio e delle notizie false. Questo crea un percorso di propagazione che può portare a problemi di equità e giustizia sociale.</sample>
    <sample id="77">L'articolo presenta il lavoro "On Improving Summarization Factual Consistency from Natural Language Feedback", un progetto congiunto tra Yale University e Microsoft Research. L'obiettivo principale è migliorare la coerenza fattuale nei modelli di sintesi testuale attraverso l'uso di feedback umano. Per questo scopo, i ricercatori hanno creato il dataset DeFacto, che include dimostrazioni umane e feedback per correggere errori fatti in sintesi. Il dataset è stato raccolto utilizzando il set di dati XSum e le uscite iniziali generate dal modello Pegasus. Ogni annotazione include un'etichetta di coerenza fattuale, una versione corretta della sintesi e un feedback dettagliato con istruzioni, spiegazioni e evidenze provenienti dal testo sorgente. Il lavoro introduce tre nuovi compiti di NLG: editing della sintesi, generazione del feedback e correzione automatica degli errori fatti. I risultati mostrano che i modelli fine-tuned e i grandi modelli linguistici possono sfruttare efficacemente il feedback umano per l'editing, mentre la generazione del feedback rimane una sfida. Inoltre, l'incorporazione di spiegazioni durante l'addestramento migliora le prestazioni del modello. Il dataset DeFacto, disponibile su GitHub, offre un'importante risorsa per il training e l'analisi di metriche di coerenza fattuale e per la valutazione meta-fattuale.</sample>
    <sample id="78">Sì, il processo di semplificazione differisce tra DEPLAIN-apa e DEPLAIN-web. DEPLAIN-apa presenta più riassegnazioni e aggiunte di parole, mentre DEPLAIN-web include più riformulazioni.</sample>
    <sample id="79">No, CoScript non è specificamente menzionato come disponibile pubblicamente nel testo. L'autore invita a consultare il loro lavoro per ulteriori dettagli, ma non indica esplicitamente una disponibilità pubblica del dataset.</sample>
    <sample id="80">La filigrana viene inserita nel testo modificando l'embedding fornito dal servizio. Quando un utente invia una frase, il servizio calcola il numero di trigger (parole selezionate) presenti nella frase. L'embedding fornito è una somma ponderata tra l'embedding originale e un embedding target. Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Se il numero di trigger supera un valore m, l'embedding fornito è esattamente uguale all'embedding target.</sample>
    <sample id="81">Gli autori dell'articolo sono affiliati all'Università di Penn State.</sample>
    <sample id="82">**Abstract:**  
In questo lavoro, proponiamo ULRA (Unsupervised Learning via Rank Aggregation), un nuovo framework per il punteggio automatico di saggi (AES) in un contesto non supervisionato. Gli AES tradizionali richiedono grandi dataset etichettati, ma la raccolta di tali dati è onerosa e limitata. Per ovviare a questo problema, ULRA sfrutta segnali euristici di qualità per generare supervisione pseudo-esperta. Il framework introduce un modulo di ranking euristico (HER), che utilizza diversi segnali di qualità (ad esempio, numero di termini unici, lunghezza del testo) per generare liste di ranking e coppie parziali di ordine. Queste coppie vengono poi aggregate da un modulo Deep Pairwise Rank Aggregation (DPRA), che apprende a combinare le informazioni incoerenti provenienti da diversi segnali euristici attraverso una funzione di perdita personalizzata con pesi di confidenza apprendibili. In fase di inferenza, i punteggi previsti vengono trasformati in un range predefinito tramite una strategia di scoring. Gli esperimenti su setting transductivi e induttivi mostrano che ULRA supera significativamente gli approcci non supervisionati esistenti, anche se il suo rendimento è inferiore rispetto ai metodi supervisionati a causa della mancanza di etichette precise. In sintesi, ULRA offre un'alternativa promettente per l'AES non supervisionato, sfruttando in modo efficace segnali euristici multipli per migliorare la qualità del punteggio automatico.</sample>
    <sample id="83">Sì, i modelli Encoder-Decoder come mT5 possono migliorare con l'addestramento su una combinazione di lingue, come dimostrato dagli esperimenti in XSemPLR. Tuttavia, l'addestramento multilingue può causare un calo delle prestazioni in alcune lingue, come l'inglese, un fenomeno noto come "Curse of Multilinguality".</sample>
    <sample id="84">**Abstract**  
In this paper, we introduce PAD-Net, an efficient framework for dynamic networks, aiming to address the issue of excessive parameter usage in fully dynamic architectures. Traditional static networks use fixed parameters, while dynamic networks adapt their architecture or parameters based on input, offering better performance but at the cost of increased parameter count and computational load. Fully dynamic networks, such as those using Mixture of Experts or Dynamic Convolution, often lead to significant model size growth, limiting their practical application. To overcome this, we propose PAD-Net, which partitions parameters into static and dynamic components, allowing for a more efficient balance between flexibility and efficiency. By identifying and converting redundant dynamic parameters into static ones, PAD-Net maintains or even improves the model's representation power while significantly reducing the number of parameters and computations. Our experiments show that PAD-Net outperforms both static and fully dynamic networks in terms of performance, while using fewer resources. Additionally, we explore optimal dynamic ratios and scale factors, demonstrating the importance of balancing dynamic and static components. Compared to network pruning, PAD-Net achieves better results by preserving static parameters. We also observe that PAD-Net enhances output discriminability, contributing to its superior performance. Future work includes extending PAD-Net to other architectures, exploring hardware-friendly structures, and introducing more diverse modes of parameter interaction.</sample>
    <sample id="85">Un esempio di pianificazione linguistica vincolata è "fare un dolce al cioccolato", dove il vincolo specifico ("cioccolato") richiede che lo script generato includa passaggi rilevanti per la preparazione di un dolce al cioccolato, rispetto a un obiettivo più astratto come "fare un dolce".</sample>
    <sample id="86">Gli autori verificano la segretezza del loro metodo visualizzando gli embedding delle frasi sui quattro dataset utilizzando PCA. Il legenda delle figure indica il numero di trigger in ciascuna frase, e i risultati mostrano che gli embedding con backdoor non sono distinguibili dagli embedding normali.</sample>
    <sample id="87">Il lavoro utilizza i PLM esistenti, come CamemBERT e PubMedBERT, come punto di partenza per il pre-addestramento continuo, addestrandoli su dati specifici del dominio (NACHOS o note cliniche) per costruire modelli specializzati come DrBERT e ChuBERT. Inoltre, confronta i risultati di questi modelli con quelli di modelli addestrati da zero (from-scratch) per valutare l'impatto delle diverse strategie di pre-addestramento.</sample>
    <sample id="88">GPT-4 è meno allineato con i paesi non anglofoni e con le persone non binarie rispetto ai paesi anglofoni e ai sessi maschile e femminile.</sample>
    <sample id="89">La relatrice mostra il modo in cui il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione nell'esempio in cui spiega che, quando il modello predice la traduzione in tedesco del frammento "I'm going to talk about...", l'attenzione si concentra sui frame audio ricevuti, permettendo di decidere quando emettere una parola tradotta in base alla stabilità dell'attenzione.</sample>
    <sample id="90">**Abstract**  
In questo lavoro, Haneul Yoo e i colleghi interrogano la necessità di reclutare parlanti nativi per l'annotazione dei dati NLP, proponendo un'alternativa basata su apprendenti di lingue straniere. L'obiettivo è valutare la fattibilità di utilizzare apprendenti come annotatori, specialmente per lingue con pochi parlanti nativi, come l'irlandese. L'analisi si concentra su tre lingue (inglese, coreano e indonesiano) e su quattro compiti del benchmark GLUE: analisi del sentiment, NLI, NER e MRC. Gli apprendenti sono classificati in tre livelli di competenza (base, intermedio, avanzato) e confrontati con parlanti nativi. I partecipanti svolgono un pre-test, un'annotazione di 10 esempi con risorse aggiuntive (come dizionari o sistemi di traduzione automatica), e un post-test per valutare l'impatto sull'apprendimento. I risultati mostrano che le etichette annotate dagli apprendenti sono quasi accurate, in particolare per compiti semplici, e che, attraverso il voto maggioritario, possono raggiungere prestazioni paragonabili a quelle dei nativi. Inoltre, l'annotazione ha un effetto positivo sull'apprendimento linguistico. Simulazioni di addestramento con etichette degli apprendenti mostrano che i modelli raggiungono il 95% delle prestazioni del ground truth. Questo studio apre la strada a nuove strategie per costruire dataset per lingue a risorse limitate, superando barriere geografiche e tecnologiche.</sample>
    <sample id="91">La quantità di attività influisce positivamente sulla performance del modello: all'aumentare del numero di compiti, il modello raggiunge prestazioni migliori e una sensibilità ridotta.</sample>
    <sample id="92">1. Modelli seq2seq tradizionali (naive seq2seq)  
2. Metodi che integrano alberi (tree-based models)  
3. Altri modelli senza alberi (treeless models)</sample>
    <sample id="93">I due coautori, Alexander Koller e Ivan Titov, sono i mentori (advisor) del primo autore, Matthias Lindemann.</sample>
    <sample id="94">**Abstract:**  
Jingwei Yi, dell'Università Cinese della Scienza e Tecnologia, presenta un lavoro intitolato *Protecting the Copyright of Large Language Models for Embedding as Services via Backdoor Watermark*. L'articolo affronta il problema della protezione della proprietà intellettuale nei servizi di embedding basati su modelli linguistici di grandi dimensioni, come GPT o LLAMA. Questi servizi, come l'API di embedding di OpenAI, sono vulnerabili a furto di modello da parte di attaccanti che possono apprendere e replicare le funzionalità del modello. Per contrastare questo rischio, l'articolo propone *Embedding Marker*, un metodo basato su backdoor per inserire un watermark negli embedding forniti. Il watermark è progettato per essere invisibile, non compromettere l'utilità degli embedding e rimanere trasferibile durante l'estrazione del modello. Il metodo prevede due fasi principali: l'iniezione del watermark e la verifica del copyright. Durante l'iniezione, il numero di trigger (parole selezionate in base alla frequenza) in una frase determina il peso dell'embedding target. Nella fase di verifica, si confrontano le similarità cosine e L2 tra gli embedding richiesti e l'embedding target, utilizzando anche il test KS. Gli esperimenti su quattro dataset (AG News, MIND, SST2, Enron Spam) mostrano un'alta capacità di rilevamento senza compromettere le prestazioni downstream. Inoltre, l'analisi visiva conferma la covertness del watermark. L'approccio proposto offre una soluzione promettente per proteggere la proprietà intellettuale nei servizi di embedding.</sample>
    <sample id="95">Il testo non menziona chi sia l'autore di PaLM. David Vilar è l'autore del lavoro presentato, non dell'architettura PaLM.</sample>
    <sample id="96">Ciao a tutti. Mi chiamo Jenny, sono una studentessa di primo anno del dottorato all'Università Carnegie Mellon e oggi presenterò il nostro lavoro NLPositionality, che caratterizza le bias di progettazione nei dataset e nei modelli. Questo lavoro è stato realizzato in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, in particolare con Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap. Cominciamo immaginando che tu stia lavorando per un giornale e stia esaminando i commenti sotto un articolo di notizie cercando di rimuovere il contenuto tossico. Potresti rivolgerti a un API popolare come il Prospective API per la rilevazione della tossicità, e questo funziona molto bene se sei Carl Jones. Infatti, il Prospective API è in grado di rilevare correttamente gli esempi tossici. Ma non è affatto il caso di Aditya Sharma, dove il Prospective API non è così sensibile a termini offensivi più comuni in contesti indiani. Questo è un esempio di bias di progettazione, dove vediamo differenze sistematiche nel rendimento della tecnologia tra diverse popolazioni. I bias di progettazione come quello che abbiamo appena visto possono verificarsi a causa della posizionalità degli studiosi di NLP e dei sviluppatori dei modelli. La posizionalità è semplicemente le prospettive che le persone hanno in base alla loro demografia, identità e esperienze di vita. Questo concetto è ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femministi e queer. Come ricercatrice, la posizionalità può influenzare il processo di ricerca e i risultati perché può cambiare le decisioni che i ricercatori prendono. E così una domanda che potrebbe sorgere è: i dataset e i modelli hanno una posizionalità? Non stiamo dicendo che i modelli stessi o i dataset abbiano identità demografiche o esperienze di vita, ma accumulano giudizi e opinioni di persone reali e quindi possono rappresentare certe posizionalità rispetto ad altre. Il lavoro precedente ha suggerito alcune prove aneddotiche di posizionalità, come i divari culturali nei modelli e nei dataset, nonché definizioni teoriche della posizionalità dei modelli. Tuttavia, questi lavori non analizzano effettivamente gli utenti finali rispetto ai dataset e ai modelli stessi, e studiare la posizionalità dei modelli e dei dataset è sempre più importante man mano che le attività NLP diventano più soggettive e orientate alla società, e è difficile caratterizzare come queste posizionalità siano distorti perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API. Per studiare la posizionalità dei dataset e dei modelli, confrontiamo effettivamente le annotazioni di utenti reali con i dataset e i modelli esistenti. Lo facciamo attraverso il nostro framework NLPositionality. Il nostro framework funziona in due passaggi principali. Il primo passo è riannotare i dataset con annotatori diversi. Dobbiamo farlo guardando alle demografie degli annotatori originali dei dataset, perché di solito solo pochi annotatori annotano ciascun esempio e perché le demografie sono raramente raccolte e condivise. Pertanto, optiamo per riannotare i dati per ottenere molte annotazioni per esempio e per ottenere un insieme ricco di dati demografici. Poi confrontiamo le annotazioni per demografia con i modelli e i dataset utilizzando un punteggio di correlazione di Pearson's R. In questo modo, il nostro framework si differenzia dalla letteratura sull'incertezza degli annotatori confrontando gli utenti finali con i modelli e i dataset, le previsioni e le etichette, anziché guardare solo all'accordo tra gli annotatori o modellare le distribuzioni degli annotatori. Il nostro framework è in gran parte abilitato da Lab in the Wild, una piattaforma online di crowdsourcing per collaborazioni in ambito HCI. Lab in the Wild è una piattaforma sperimentale online dove possiamo reclutare volontari diversificati. Rispetto a piattaforme come MTurk che hanno in gran parte partecipanti provenienti dagli Stati Uniti o dall'India, Lab in the Wild è comunque in grado di ottenere dati di alta qualità. Abbiamo ospitato due compiti su Lab in the Wild, uno dei quali era sull'accettabilità sociale, e il modo in cui funziona è che i partecipanti leggeranno una situazione dal dataset Social Chemistry e poi scriveranno quanto siano socialmente accettabili quelle situazioni. Dopo di che, per mantenere il loro interesse nello studio, possono confrontare le loro risposte con un AI e con altre persone. Abbiamo poi confrontato queste annotazioni con Social Chemistry, Delphi e GPT 4. Abbiamo replicato un setup molto simile per il compito di rilevamento della tossicità e del discorso d'odio, dove i partecipanti leggeranno un'istanza da Dynahate e scriveranno se pensano che si tratti di un'istanza di discorso d'odio. Abbiamo poi confrontato queste annotazioni con Dynahate, Perspective API, Rewire API, Hate Roberta e GPT 4. Lo studio ha alla fine accumulato oltre 16.000 annotazioni da oltre 1000 annotatori provenienti da 87 paesi. Ora siamo meglio equipaggiati per rispondere a chi si allinea di più i dataset e i modelli NLP. Troviamo che esiste una posizionalità nell'NLP. Ad esempio, troviamo che i dataset e i modelli si allineano di più ai paesi parlanti inglese. Per l'analisi sull'accettabilità sociale di GPT 4, troviamo che si allinea di più ai paesi confuciani e parlanti inglese. Troviamo che Dynahate si allinea anche di più ai paesi parlanti inglese. Troviamo inoltre un allineamento aggiuntivo con le persone che hanno un'istruzione universitaria. Per GPT 4, nel compito sull'accettabilità sociale, troviamo che si allinea di più alle persone con un'istruzione universitaria o di scuola superiore e troviamo lo stesso per Dynahate, dove si allinea di più alle persone con un'istruzione universitaria. Tuttavia, quando i modelli e i dataset si allineano a specifiche popolazioni, alcune sono inevitabilmente lasciate indietro. Un esempio di questo è che i dataset e i modelli sono meno allineati alle persone non binarie rispetto ai loro controparti uomini e donne. Troviamo questo anche nell'analisi sull'accettabilità sociale di GPT 4, nonché nell'analisi del compito Dynahate. Dato che esiste una posizionalità nell'NLP, cosa possiamo fare al riguardo? Abbiamo alcune raccomandazioni per questo. La prima è mantenere un registro di tutte le scelte di progettazione rilevanti durante tutto il processo di ricerca. La seconda raccomandazione è condurre la ricerca NLP con una prospettiva perspectivistica. La terza raccomandazione è costruire dataset e modelli specializzati all'interno di 4 comunità specifiche. Un buon esempio di questo è l'iniziativa Masakhani. Vogliamo sottolineare che l'NLP inclusivo non è solo far sì che tutte le tecnologie funzionino per tutti. Questo conclude la mia presentazione. Se volete saperne di più, non esitate a visitare il nostro dashboard per i risultati più aggiornati dell'analisi e il nostro articolo. Grazie.</sample>
    <sample id="97">La relatrice menziona due problemi associati a SimulST: l'uso di architetture specifiche che richiedono ottimizzazione aggiuntiva e i lunghi e complessi procedimenti di addestramento, inclusi diversi obiettivi di ottimizzazione e la necessità di addestrare e mantenere diversi modelli per raggiungere diversi regimi di latenza.</sample>
    <sample id="98">Un modo efficace per mitigare i bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP potrebbe essere l'adozione di tecniche di pre-addestramento controllato su corpora diversificati e rappresentativi, unitamente a un'attenta selezione e sanitizzazione dei dati per ridurre le distorsioni ideologiche, senza incorrere in censura o esclusione.</sample>
    <sample id="99">Ciao, sono Siyu Yuan dell'Università di Fudan. Sono qui per presentare il nostro lavoro intitolato "Distillare la conoscenza degli script dai Large Language Models per la pianificazione linguistica vincolata". Nella vita quotidiana, gli umani spesso pianificano le loro azioni seguendo istruzioni passo dopo passo in forma di script orientati agli obiettivi. I lavori precedenti hanno sfruttato i modelli linguistici per pianificare obiettivi astratti di attività stereotipate come "fare una torta". Hanno mostrato che i Large Language Models possono efficacemente decomporre gli obiettivi in passaggi. Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione per gli obiettivi astratti di attività stereotipate. La pianificazione per gli obiettivi con vincoli specifici, come "fare una torta al cioccolato", rimane poco studiata. In questo articolo, definiamo il problema della pianificazione linguistica vincolata, che impone diversi vincoli sugli obiettivi della pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multifacetati. Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli. In questo articolo, valutiamo e miglioriamo inizialmente la capacità di pianificazione linguistica vincolata dei Large Language Models. Poiché non esiste un dataset di obiettivi specifici per supportare il nostro studio, dobbiamo acquisire questi obiettivi inizialmente. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multifacetati per l'acquisizione di dati con l'intervento umano utilizzando InstructGPT. Campioniamo 100 obiettivi specifici e valutiamo gli script generati dai Large Language Models. Questa tabella riporta l'accuratezza complessiva dei risultati. Abbiamo trovato che tutti i modelli linguistici ottengono risultati insoddisfacenti nella pianificazione per gli obiettivi specifici. Poi conduciamo un'analisi dettagliata per investigare il motivo per cui i modelli di apprendimento falliscono. I risultati nella figura mostrano che la completezza semantica negli script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita. Approfondiamo ulteriormente le categorie di vincoli definite in wikiHow. Il calore mappa nella figura mostra che le prestazioni di pianificazione di InstructGPT variano notevolmente per gli obiettivi di diverse categorie. Studi precedenti hanno mostrato che la qualità dell'output dei modelli linguistici presenta una varianza elevata, portando a prestazioni scadenti. Pertanto, adottiamo l'idea di "over-generate-then-filter" per migliorare la qualità della generazione. Iniziamo mostrando i tipi di vincoli con esempi a InstructGPT e otteniamo obiettivi specifici basati sugli obiettivi astratti di partenza. Poi, InstructGPT genera in modo eccessivo K script per gli obiettivi specifici. Successivamente, viene sviluppato un modello filtro per selezionare gli script fedeli. Convertiamo gli script e gli obiettivi in embedding InstructGPT e calcoliamo la similarità coseno come punteggio di similarità per misurare la similarità semantica. Inoltre, premiamo lo script che contiene le parole chiave del vincolo obiettivo. Manteniamo lo script solo se l'obiettivo mirato ha il punteggio più alto nell'insieme degli obiettivi. Con il nostro metodo, InstructGPT può generare script di maggiore qualità. Il nostro metodo migliora notevolmente la capacità di pianificazione sia in termini di completezza semantica che di fedeltà ai vincoli. Poiché i Large Language Models sono costosi da distribuire, è essenziale abilitare la capacità di pianificazione linguistica nei modelli più piccoli e specializzati. La creazione del dataset è un passo essenziale per questo scopo. Tuttavia, gli studi precedenti non abilitano la pianificazione per obiettivi specifici e l'annotazione manuale del dataset è costosa. Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica per distillare dataset di pianificazione linguistica vincolata dai Large Language Models. Applichiamo il nostro metodo per creare un dataset di pianificazione linguistica vincolata, chiamato CoScript. In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità dell'insieme di validazione e test, chiediamo a lavoratori crowdsourced di trovare e correggere i campioni errati. Questa figura mostra la distribuzione dei vincoli di CoScript. Abbiamo scoperto che CoScript mostra un'alta pluralità negli obiettivi specifici generati. Con CoScript possiamo provare modelli più piccoli ma specializzati per la pianificazione linguistica vincolata. Abbiamo scoperto che T5 sintonizzato su CoScript può generare script di qualità superiore rispetto alla maggior parte dei Large Language Models, indicando che i modelli più piccoli possono superare i modelli più grandi quando vengono opportunamente addestrati su dataset adatti. In sintesi, stabiliamo il problema della pianificazione linguistica vincolata. Valutiamo la capacità di pianificazione linguistica vincolata dei Large Language Models e sviluppiamo un metodo "over-generate-then-filter" per i Large Language Models. Utilizziamo i Large Language Models per generare un dataset di script di alta qualità, CoScript, per la pianificazione linguistica vincolata. Speriamo che il dataset CoScript possa essere una risorsa preziosa per avanzare la ricerca sulla pianificazione linguistica. Grazie per il vostro tempo. Troverete maggiori dettagli su CoScript nel nostro articolo.</sample>
    <sample id="100">**Abstract:**  
L'articolo presenta PromptRank, un approccio innovativo e data-efficient per il retrieval multi-hop nei sistemi di QA (Question Answering). A differenza dei metodi tradizionali che richiedono migliaia di esempi per addestrarsi, PromptRank utilizza solo 128 esempi, rendendolo particolarmente utile per domini a basso risorsivo o che richiedono competenze specialistiche. L'approccio combina un metodo di retrieval non supervisionato (basato su TF-IDF e hyperlink traversal) con un reranker a base di modelli linguistici di grandi dimensioni, addestrato in modo few-shot. I candidati di chain vengono valutati in base alla probabilità della domanda data la chain, calcolata attraverso un modello linguistico. Per migliorare le performance, vengono esplorati diversi strumenti, come l'ottimizzazione delle istruzioni, il campionamento delle istruzioni e la scalatura della temperatura. Gli esperimenti su HotpotQA mostrano che PromptRank supera sistemi completamente supervisionati come DrKit e si confronta positivamente con i migliori retriever multi-hop densi. Inoltre, quando integrato con un modello lettore (ELECTRA-Large), PromptRank dimostra un'ottima performance nei task di QA multi-hop, distanziando MDR solo di pochi punti di EM (Exact Match). L'articolo evidenzia l'efficacia dell'uso di modelli linguistici per il ranking few-shot di percorsi multi-hop, la rilevanza delle istruzioni ottimizzate e la superiorità della probabilità della domanda data la chain come funzione di scoring.</sample>
    <sample id="101">La fluidità di PaLM è paragonabile a quella dei sistemi di traduzione all'avanguardia, ma presenta alcuni problemi di accuratezza.</sample>
    <sample id="102">Le proprietà importanti di un metodo di filigrana sono:

1. Deve essere applicabile ai servizi di embedding.
2. Non deve degradare l'utilità degli embedding forniti.
3. Deve essere sufficientemente nascosto per evitare che l'attaccante lo riconosca facilmente o lo rimuova.
4. Deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello.</sample>
    <sample id="103">Il testo non specifica quali siano le 14 lingue in cui sono stati tradotti i discorsi TED in inglese.</sample>
    <sample id="104">Il testo non specifica il numero esatto di istanze campionate da un set di dati per la riannotazione.</sample>
    <sample id="105">Le metriche di distanza utilizzate per misurare la differenza tra set di dati benigni e backdoor sono il **cosine similarity**, l'**L2 similarity** e il **test KS (Kolmogorov-Smirnov)** con il suo **p-value**.</sample>
    <sample id="106">**Abstract:**  
In questo lavoro, presentiamo QUEST, un dataset di retrieval per query che contengono vincoli impliciti basati su operazioni insiemistiche, come intersezione e differenza. Questo dataset è stato costruito per affrontare le esigenze informative complesse, come quelle di Jane, una zoologa che cerca di identificare una specie di rettile in base a caratteristiche specifiche, o di Austin, un lettore che cerca libri di fiction storica ambientati in Francia. QUEST include oltre 3.000 query che richiedono l'identificazione di entità rilevanti, con risposte verificate e spazi di testo attribuibili a diversi vincoli della query. La costruzione del dataset si basa su categorie di Wikipedia in quattro domini (film, libri, piante, animali), combinandole attraverso operazioni insiemistiche e successivamente parafrazandole in modo naturale grazie all'annotazione umana. Gli esperimenti mostrano che i sistemi di retrieval attuali, sia basati su rappresentazioni sparse che dense, non riescono a gestire efficacemente tali query complesse, con performance F1 basse, in particolare per le query che coinvolgono intersezioni e differenze insiemistiche. Il dataset QUEST mira a supportare futuri studi e miglioramenti nei sistemi di retrieval per scenari di ricerca con vincoli selettivi, contribuendo a un'efficace comprensione e soddisfazione di esigenze informative complesse.</sample>
    <sample id="107">I modelli basati su codificatori multilingue (Encoder-PTR) sono stati utilizzati con decodificatori basati su puntatori (PTR), come XLM-R + PTR e mBERT + PTR, per valutare le prestazioni nei compiti di parsing semantico cross-linguale. Sono stati confrontati con modelli Encoder-Decoder multilingue (come mBART e mT5), che hanno mostrato prestazioni superiori su tutti i dataset. Inoltre, l'addestramento misto di diversi linguaggi ha migliorato le prestazioni dei modelli Encoder-Decoder e Encoder-PTR, anche se ha causato una riduzione delle prestazioni in inglese in alcuni casi (Curse of Multilinguality).</sample>
    <sample id="108">**Abstract:**  
Nel presente lavoro, gli autori analizzano la capacità dei modelli linguistici di giudicare l'accettabilità di frasi in contesti di lunghezza crescente, rivisitando il paradigma delle coppie minimali (MPP). Tradizionalmente, gli MPP valutano i modelli confrontando frasi accettabili e non accettabili, ma non considerano l'impatto del contesto esterno. Gli autori estendono questo approccio, creando frasi lunghe aggiungendo prefissi estratti da dataset come BLiMP o SyntaxGym, sia accettabili che non accettabili, e analizzano come i modelli reagiscono. I risultati mostrano che i giudizi MPP rimangono stabili quando il contesto è irrilevante (ad esempio, estratto da Wikipedia), ma vengono significativamente influenzati quando i prefissi provengono dallo stesso fenomeno linguistico del test. Questo suggerisce che i modelli sono sensibili a caratteristiche sintattiche e semantiche condivise, e che i giudizi MPP tradizionali, basati su input brevi, non catturano appieno la complessità del loro know-how linguistico in contesti estesi. L'analisi ulteriore rivela che le perturbazioni sintattiche non alterano significativamente i giudizi, confermando la sensibilità dei modelli a strutture linguistiche nascoste. Questo lavoro mette in luce l'importanza di valutare i modelli in contesti più realistici e lunghi, soprattutto con l'aumento delle capacità di contesto dei grandi modelli linguistici.</sample>
    <sample id="109">**Abstract:**  
L'articolo presenta *Unnatural Instructions*, un dataset di istruzioni naturali per compiti linguistici creati senza lavoro umano. L'obiettivo è generare un insieme di dati eterogenei, creativi e diversificati per l'addestramento di modelli linguistici, evitando l'uso di annotazioni manuali o benchmark accademici esistenti. Il dataset è creato automaticamente utilizzando un modello linguistico pre-addestrato (GPT-3) che genera istruzioni, input e output corrispondenti. Per aumentare la diversità, vengono anche generate parafrasi delle istruzioni. Il risultato è un dataset composto da 64.000 esempi, con circa 240.000 esempi inclusi le parafrasi. L'analisi mostra che più del 50% degli esempi generati è corretto, e anche gli errori contengono informazioni utili per l'addestramento. Il dataset include compiti creativi, come verificare un esperimento scientifico o inventare una nuova parola. I test dimostrano che un modello T5 da 11 miliardi di parametri addestrato su *Unnatural Instructions* supera baselines come T0++ e Tk-instruct su diversi benchmark. Il lavoro sottolinea la capacità dei modelli linguistici di generare dati diversificati e creativi in modo efficiente, senza la necessità di annotazioni umane, offrendo un'alternativa economica e scalabile rispetto ai metodi tradizionali.</sample>
    <sample id="111">Gli autori decidono le parole a frequenza moderata selezionando un insieme di parole che si trovano in un intervallo di frequenza moderata, utilizzando un corpus di testo generale raccolto dal provider per contare la frequenza delle parole.</sample>
    <sample id="112">Ciao a tutti, il mio nome è Shuheng. Oggi presenterò il nostro articolo intitolato "I tagger per l'entità nominata del CoNLL-2003 funzionano ancora bene nel 2023?" Cominciamo. Il nostro articolo ha investigato il problema della generalizzazione utilizzando il compito di Riconoscimento delle Entità Nominata (NER). Abbiamo osservato che i modelli utilizzati nel CoNLL-2003 sono stati impiegati per sviluppare il NER per quasi 20 anni, e questo solleva naturalmente diversi problemi. In primo luogo, questi modelli possono generalizzare ai dati moderni? E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione? Allo stesso tempo, se notiamo una scarsa generalizzazione, cosa causa il calo delle prestazioni di questi modelli? Per investigare questi problemi, abbiamo sviluppato il dataset CoNLL++. Si tratta di un dataset che abbiamo raccolto da Reuters News del 2020, e poi annotato con le stesse linee guida di annotazione del CoNLL-2003. Abbiamo poi fine-tunato oltre 20 modelli su CoNLL-2003. Li abbiamo valutati sui set di test del CoNLL-03 e sul CoNLL++. E infine, abbiamo calcolato la percentuale di variazione in F1 per valutare la generalizzazione di ciascun modello. Quindi, cosa è necessario per una buona generalizzazione? Attraverso gli esperimenti, abbiamo trovato che ci sono tre ingredienti principali necessari. Il primo è l'architettura del modello. Attraverso i nostri esperimenti, abbiamo scoperto che i modelli basati su transformer generalmente si generalizzano meglio ai nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che di solito i modelli più grandi portano a una migliore generalizzazione. E infine, sappiamo tutti che il numero di esempi utilizzati per il fine-tuning influisce direttamente sulle prestazioni del compito downstream. Qui abbiamo anche scoperto che più esempi di fine-tuning portano a una migliore generalizzazione. Per la nostra prossima domanda, cosa causa il calo delle prestazioni di alcuni modelli? Abbiamo due ipotesi. La prima è l'overfitting adattivo, che è un overfitting causato dal riutilizzo ripetuto dello stesso set di test, e questo si manifesta di solito come un ritorno decrescente su un nuovo set di test. La seconda ipotesi è il drift temporale, che è la degradazione delle prestazioni causata dall'aumento del divario temporale tra i dati di addestramento e quelli di test. Per l'overfitting adattivo, abbiamo visto che dal grafico a destra, la linea di best fit rossa ha un coefficiente angolare maggiore di uno. Questo significa che ogni unità di miglioramento che abbiamo ottenuto sul CoNLL-2003 si traduce in un miglioramento maggiore di una unità sul CoNLL++. Questo indica che non c'è un ritorno decrescente, e mostra che in questo caso non osserviamo un overfitting adattivo. E cosa ne diciamo del drift temporale? Per il drift temporale, abbiamo effettuato un esperimento per riallineare o proseguire l'addestramento di alcuni modelli con dati più recenti e abbiamo visto che le prestazioni si degradano con un divario temporale maggiore, confermando la nostra ipotesi che la causa principale del calo delle prestazioni sia il drift temporale. La nostra conclusione è che, per una buona generalizzazione, abbiamo bisogno di un'architettura di modello migliore, di una dimensione di modello più grande e di più esempi di fine-tuning. Questi vanno di pari passo: non possiamo avere solo un ingrediente e ignorare gli altri. Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni è causato da drift temporale e, sorprendentemente, non è causato da overfitting adattivo, anche se il CoNLL-2003 è stato utilizzato per oltre 20 anni. Tornando alla domanda che abbiamo posto nel titolo del nostro articolo: "I tagger del CoNLL-2003 funzionano ancora nel 2023?" Abbiamo scoperto che la risposta è in realtà un sì assolutamente. Speriamo che il nostro articolo stimoli ulteriori ricerche su come migliorare la generalizzazione dei modelli. Infine, non dimenticate di controllare il nostro articolo, il nostro dataset e, se avete domande, non esitate a contattarmi. Grazie mille.</sample>
    <sample id="114">**Abstract:**  
L'articolo presenta un lavoro presentato all'ACL 2023 intitolato "Finding the Pillars of Strength for Multi-Head Attention", sviluppato da ricercatori dell'NTU Singapore. L'obiettivo è affrontare il problema della ridondanza nei modelli linguistici di grandi dimensioni, in particolare il numero elevato di parametri. I modelli di grandi dimensioni, sebbene rivoluzionari, presentano limitazioni come il costo computazionale elevato, il tempo di addestramento lungo e la necessità di grandi quantità di dati. L'attenzione multi-testa, un componente chiave di tali modelli, mostra una certa ridondanza, poiché alcuni head possono essere rimossi senza perdere prestazioni. Lo studio propone un'architettura chiamata "Grouped Head Attention" che utilizza una strategia "divide and conquer" per comprimere l'attenzione multi-testa. Questo approccio include due fasi: un addestramento vincolato per raggruppare i head e un algoritmo "Voting-to-Stay" per prunare i head ridondanti. I risultati sperimentali mostrano miglioramenti significativi su compiti come la traduzione automatica, il modeling linguistico e la sintesi di testi, con riduzioni del 32.1% nei parametri senza compromettere le prestazioni. Inoltre, l'analisi dell'efficienza rivela un'accelerazione del 62% e una riduzione del 80% nei FLOPs. L'approccio apre la strada a una prunatura automatica specifica per compiti, ispirata all'ipotesi del "Lottery Ticket", permettendo di ottenere modelli più leggeri e efficienti per applicazioni reali.</sample>
    <sample id="115">L'approccio utilizza un segmento parlato definito da un numero fissato di frame audio, rappresentato dal parametro lambda. La decisione di emettere una traduzione parziale dipende dall'attenzione concentrata su questi ultimi lambda frame. Tuttavia, il testo non specifica la dimensione esatta di lambda in termini di frame o tempo.</sample>
    <sample id="116">Nell'esempio con Servin e Kea, la conoscenza specifica dell'entità necessaria è "Servin è un giudice".</sample>
    <sample id="117">Il fattore più importante è la qualità dell'esempio.</sample>
    <sample id="118">**Abstract:**  
In questo lavoro, proponiamo miglioramenti alle tecniche di pre-addestramento per affrontare il problema del code-switching, un fenomeno comune in contesti linguistici diversificati come l'India. I modelli multilingue pre-addestrati, come mBERT e XLM-R, mostrano prestazioni limitate su compiti code-switching come l'analisi del sentiment e il ragionamento. Per affrontare questa sfida, introduciamo **SwitchMLM**, un'obiettivo di pre-addestramento personalizzato che si concentra sui *switch-point*, ovvero le transizioni tra lingue in una frase code-switch. Questi punti vengono mascherati in modo mirato, migliorando la capacità del modello di gestire il code-switching. In assenza di dati con tag di identificazione linguistica (LID), proponiamo **FrequencyMLM**, un metodo alternativo basato sulla frequenza delle parole nei corpus monolingue. Inoltre, suggeriamo modifiche architettoniche, come connessioni residuali da strati intermedi al layer finale, e un'ulteriore perdita ausiliaria per migliorare l'apprendimento delle informazioni linguistiche. I risultati sperimentali mostrano che il nostro approccio combinato (Switch/FrequencyMLM + ResBERT + loss ausiliaria) supera gli stati dell'arte su compiti di analisi del sentiment. Gli esperimenti di probing confermano che i nostri metodi aumentano l'informazione sui *switch-point* nei layer intermedi e finali. Questo lavoro apre la strada a modelli più efficaci per il code-switching in contesti multilingue.</sample>
    <sample id="119">L'articolo si concentra sui modelli linguistici GPT-4, GPT series e BART series negli esperimenti estesi.</sample>
    <sample id="120">Il modello utilizza i punteggi di attenzione di un livello specifico, precisamente il cross-attention mechanism tra input audio e output testuale.</sample>
    <sample id="121">Gli esempi di inferenza diretta menzionati nel testo includono l'uso del nome dell'entità, ad esempio "Easy on Me", o la menzione della posizione, come "il primo".</sample>
    <sample id="122">L'autore dell'articolo, Siyu Yuan, è affiliato all'Università di Fudan.</sample>
    <sample id="123">**Abstract**  
In this work, we introduce *MultiInstruct*, the first large-scale multi-modal instruction tuning benchmark dataset comprising 62 diverse tasks across 10 categories, derived from 21 open-source datasets. Each task includes five expert-written instructions to facilitate instruction-based learning. Motivated by the lack of publicly available multi-modal instruction datasets, we aim to explore whether instruction tuning can enhance the zero-shot generalization of multi-modal pre-trained models, such as OFA, to unseen tasks. We conduct experiments using OFA as a base model, training on 53 tasks with 10,000 instances each, and evaluating on a separate test set. Our results show that instruction tuning significantly improves performance on both seen and unseen multi-modal tasks, with benefits further enhanced by transfer learning from natural instruction datasets. We introduce a novel evaluation metric, *sensitivity*, to measure the model's consistency in outputs under slight variations in instruction phrasing. Using multiple instructions per task reduces sensitivity and boosts performance. Our findings highlight the effectiveness of instruction tuning in multi-modal settings and demonstrate the value of our dataset in advancing research in this area. We also plan to release an extended version of MultiInstruct with 150 additional vision-language tasks.</sample>
    <sample id="124">**Abstract:**  
In questo lavoro, Tan Qingyu e il team presentano un'analisi approfondita delle capacità di ragionamento temporale nei Large Language Models (LLMs), identificando tre livelli distinti: time-to-time, time-to-event e event-to-event. I precedenti studi hanno spesso enfatizzato il secondo livello, mentre l'approccio proposto mira a una valutazione più completa. Per questo, i ricercatori introducono il dataset TempReason, che copre tutti e tre i livelli e include un ampio intervallo temporale. L'analisi sperimentale rivela che modelli come ChatGPT mostrano una performance significativamente inferiore in compiti di previsione del mese rispetto all'anno, evidenziando bias temporali legati alla frequenza dei dati di addestramento. Per migliorare il ragionamento temporale, i ricercatori propongono una strategia di addestramento composta da due componenti: un pre-addestramento per l'estrazione di span temporali e un apprendimento rinforzato sensibile al tempo. Il modello risultante, TempT5, mostra un miglioramento significativo rispetto ai modelli di base in diversi scenari di QA (Closed Book, Open Book e Reasoning QA). Nonostante i risultati promettenti, persistono alcune fluttuazioni di performance in base al periodo temporale, suggerendo la necessità di ulteriori ricerche per mitigare i bias. In sintesi, il lavoro contribuisce con un benchmark completo, un'analisi critica delle debolezze dei modelli e una strategia innovativa per migliorare il ragionamento temporale nei LLM.</sample>
    <sample id="125">L'articolo non specifica il numero di autori coinvolti.</sample>
    <sample id="126">No, la traduzione della query in linguaggio naturale utilizzando un modello di traduzione automatica prima del parsing semantico non è stato considerato un approccio standard. È stato utilizzato come uno dei sei setting di valutazione (Translate-Test) per testare il benchmark XSemPLR, ma non è l'approccio principale adottato nei modelli di parsing semantico cross-linguistico.</sample>
    <sample id="127">**Abstract:**  
In questo lavoro, Namgyu Ho e i suoi collaboratori presentano un approccio innovativo per trasferire le capacità di ragionamento di modelli linguistici di grandi dimensioni a modelli più piccoli, riducendo i costi computazionali e di memoria. L'idea chiave è utilizzare i grandi modelli come "insegnanti" per generare soluzioni dettagliate (chain-of-thought) per compiti complessi, che vengono poi utilizzate come dati di addestramento per modelli più piccoli. Per migliorare l'efficacia dell'apprendimento, i ricercatori introducono una tecnica chiamata "Diverse Reasoning", che prevede la generazione di molteplici soluzioni diverse da parte del modello insegnante, aumentando la varietà e la robustezza del training. I risultati mostrano che i modelli studenti addestrati con questa metodologia riescono a risolvere compiti complessi, come problemi matematici e comprensione del testo, con prestazioni significativamente superiori rispetto ai metodi di base. Inoltre, l'approccio è altamente scalabile e flessibile, permettendo di bilanciare i costi di sviluppo e di inferenza. Questo lavoro apre la strada a una distillazione efficace di capacità emergenti, rendendo accessibili le prestazioni avanzate anche a modelli di piccole dimensioni. I dati e il codice degli esperimenti sono resi disponibili al pubblico.</sample>
    <sample id="128">**Abstract:**  
In questo lavoro, Akshatha e Martin presentano il test KITMUS, un insieme di prove diagnostiche per valutare l'integrazione di conoscenza da fonti multiple nei modelli di comprensione del linguaggio naturale (NLU). I modelli linguistici di grandi dimensioni utilizzano conoscenza acquisita durante il pre-addestramento e conoscenza fornita durante l'inferenza. Tuttavia, per compiti che richiedono una comprensione approfondita, è essenziale integrare entrambe le fonti. Il KITMUS introduce un compito di risoluzione della coreferenza per valutare la capacità dei modelli di utilizzare conoscenze specifiche degli enti (disponibili all'inferenza) e conoscenze di background (acquisite durante il pre-addestramento). Il test include tre scenari: "Background-Pretrain", "Background-Both" e "Background-Inference", variando la disponibilità delle informazioni. Gli esperimenti con modelli esistenti e partecipanti umani mostrano che, senza addestramento specifico su KITMUS, i modelli non riescono a integrare efficacemente conoscenze da fonti diverse. L'addestramento specifico migliora le prestazioni, ma anche i modelli migliori hanno difficoltà nell'utilizzare la conoscenza di background fornita solo all'inferenza. Questo lavoro sottolinea l'importanza di sviluppare modelli in grado di integrare dinamicamente conoscenze da fonti multiple, un aspetto cruciale per compiti NLU complessi.</sample>
    <sample id="129">Gli autori hanno fornito come esempio di gruppo contrassegnato le donne di colore, specificando che per le donne nere i termini più comuni sono "strong" e "resilient", collegati all'archetipo della "Strong Black Woman".</sample>
    <sample id="130">Le architetture dei modelli non basate su transformer non generalizzano in modo adeguato.</sample>
    <sample id="131">Il testo non menziona specificamente i nomi dei set di dati di test utilizzati nello studio.</sample>
    <sample id="132">L'articolo è stato scritto da due autori: Akshatha e Martin.</sample>
    <sample id="133">L'autore opera con più modalità, non solo con il testo. Il lavoro si concentra su un'addestramento istruzionale multi-modale, coinvolgendo testo, immagini e bounding box.</sample>
    <sample id="135">**Abstract:**  
James e Sarah Finch presentano ABC-Eval, un nuovo approccio multidimensionale per valutare l'AI conversazionale, sviluppato dal laboratorio NLP di Emory University in collaborazione con Amazon Alexa AI. L'obiettivo è migliorare la valutazione dettagliata delle capacità dei modelli di dialogo, andando oltre le valutazioni tradizionali basate su giudizi umani generali. ABC-Eval si concentra sull'annotazione esplicita di comportamenti specifici nei risposte dei modelli, come irrilevanza, contraddizioni, allucinazioni e mancanza di empatia. Questo metodo mira a ridurre la soggettività e a fornire metriche più precise e informative. I ricercatori hanno testato ABC-Eval su quattro modelli di chat di ultima generazione, confrontandolo con metodi esistenti come valutazioni Likert a livello di turno e di dialogo, e confronti parziali. I risultati mostrano che ABC-Eval presenta un'alta concordanza tra annotatori e fornisce metriche più predittive della qualità complessiva del dialogo. Inoltre, le metriche di ABC-Eval spiegano un significativo livello di varianza nella qualità delle conversazioni, mentre i metodi tradizionali mostrano una minore capacità di discriminazione. Nonostante i progressi, i modelli testati commettono errori comuni (come violazioni del senso comune e irrilevanza) in una percentuale significativa. L'approccio ABC-Eval offre una valutazione più fine e attendibile, auspicando un utilizzo diffuso per migliorare la comparazione e lo sviluppo futuri degli assistenti conversazionali.</sample>
    <sample id="136">**Abstract**  
In questo lavoro, Jasivan e Nafise presentano FERMAT, un nuovo insieme di valutazione per l'analisi della capacità di ragionamento numerico dei modelli linguistici. L'obiettivo è affrontare le limitazioni dei benchmark esistenti, che spesso si limitano a misure di accuratezza poco informative. FERMAT si concentra su tre aspetti chiave: comprensione numerica, operazioni matematiche e dipendenza dal training. L'insieme è costituito da domande di matematica estratte da CommonCore e Illinois, con numeri modificati per testare diversi range (interi piccoli, grandi e decimali). I risultati mostrano che i modelli linguistici, anche di grandi dimensioni, presentano prestazioni insufficienti in compiti di ragionamento numerico, soprattutto in contesti diversi da quelli di training. L'analisi rivela che l'esposizione a espressioni esatte durante il training non garantisce un miglioramento significativo, suggerendo che i modelli non memorizzano semplicemente le risposte ma necessitano di un'apprendimento più profondo. Inoltre, l'introduzione di diversità linguistica e matematica, attraverso template diversificati e dataset come GSM8K e AQUA, porta a un miglioramento delle prestazioni. Questo studio sottolinea l'importanza di valutazioni più dettagliate per comprendere i punti di forza e debolezza dei modelli linguistici nel ragionamento numerico, proponendo FERMAT come alternativa più informativa ai benchmark tradizionali.</sample>
    <sample id="137">**Abstract:**  
In questo lavoro, presentiamo "Tell2Design", un dataset per la generazione di piani di piano guidati da linguaggio, pubblicato all'ACL 2023. Mentre i modelli generativi basati su testo producono immagini realistiche e creative, esiste una forte domanda di generare disegni che soddisfino specifiche richieste linguistiche, come quelle presenti nel processo di progettazione architettonica. Il nostro obiettivo è permettere agli utenti non esperti di partecipare al processo di progettazione fornendo istruzioni verbali. Definiamo un compito di apprendimento automatico nuovo, dove il modello genera piani di piano strutturati a partire da istruzioni linguistiche che descrivono semanticamente, geometricamente e topologicamente gli elementi del piano. Il dataset Tell2Design include 5,051 istruzioni annotate da utenti e 76,000 generate artificialmente, offrendo una vasta varietà di input. Per affrontare le sfide del compito, proponiamo un modello sequence-to-sequence basato su Transformer, che converte le istruzioni in una sequenza strutturata di bounding box. Risultati sperimentali mostrano che il modello T2D supera significativamente i baselines basati su generazione di immagini condizionate da testo, raggiungendo un Micro IoU del 54. Questo lavoro apre la strada a nuove ricerche nella generazione di disegni guidati da linguaggio, con un focus iniziale sul dominio dei piani di piano.</sample>
    <sample id="138">Secondo gli autori, l'area della NLU poco studiata è l'integrazione di conoscenza proveniente da fonti multiple, in particolare la capacità di combinare conoscenza pre-addestrata e conoscenza fornita durante l'elaborazione.</sample>
    <sample id="139">I nomi dei relatori sono Ying e Zhiyang.</sample>
    <sample id="140">Sì, CoScript è stato sottoposto a controlli di qualità attraverso il coinvolgimento di lavoratori esterni per identificare e correggere gli errori nei campioni.</sample>
    <sample id="141">I limiti delle risorse esistenti per la traduzione dipendente dal contesto includono il supporto limitato a determinati tipi di traduzioni contestuali e a un insieme ristretto di lingue, spesso a causa della dipendenza da conoscenza di dominio e curatela umana.</sample>
    <sample id="142">Ciao! Sto per parlare del nostro lavoro intitolato "Resolving Indirect Referring Expressions for Entity Selection", in cui presentiamo il corpus AltEntities. Il mio nome è Javad Hosseini e questo è un lavoro congiunto con Filip Radlinski, Silvia Pareti e Annie Louis. Il nostro obiettivo è comprendere il linguaggio degli utenti quando vogliono fare una scelta. Consideriamo questa domanda alternativa: "Hai inteso 'Easy on Me' o 'I Gotta Feeling'?" In questo caso, l'utente vuole selezionare tra queste due canzoni. La cosa più ovvia è utilizzare un riferimento diretto, ad esempio citando il nome della canzone "Easy on Me" o la sua posizione, "la prima". Tuttavia, a volte un riferimento indiretto è più appropriato per un dialogo più naturale. Questo può accadere quando l'utente non ricorda il nome della canzone. Oppure quando le pronunce sono molto simili e quindi difficili da distinguere. Oppure quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di riferimenti indiretti: "la più recente" o "la canzone che non è energica". Questo è un problema importante nei sistemi conversazionali e anche per il benchmarking della comprensione degli enti nei modelli linguistici di grandi dimensioni. Non sappiamo di un dataset pubblico più ampio per questa attività, quindi ne abbiamo raccolto uno utilizzando l'annotazione di massa. Il nostro dataset copre tre diversi domini: musica, libri e ricette. La metodologia di raccolta del dataset si concentra sull'informalità utilizzando un setup di completamento di un fumetto. Il fumetto ha tre bolle di dialogo. Nella prima bolla, Bob dice: "Ricordi quella canzone che ascoltavamo ieri?" E così Bob stabilisce il contesto del dialogo. Nella seconda bolla di dialogo, Alice dice: "Intendi 'Easy on Me' o 'I Gotta Feeling'?" Questa è la domanda alternativa. Nella terza bolla di dialogo, Bob usa un riferimento indiretto per selezionare uno degli enti, ad esempio: "la più recente". Forniamo automaticamente le prime due bolle di dialogo, ma la terza è compilata dall'annotatore. La prima bolla di dialogo è scelta da alcuni prompt manuali per ogni dominio. La seconda, che è la domanda alternativa, è generata in questo modo. Usiamo sempre un semplice modello: "Intendi A o B?" Dove A e B sono campioni da Wikipedia. Ecco i diversi metodi di campionamento che abbiamo utilizzato. Mentre ci spostiamo più in alto nell'elenco, gli enti diventano più simili tra loro e spesso è più difficile fare la disambiguazione. Il primo è un campionamento casuale uniforme. Il secondo è quando gli enti hanno titoli simili, ad esempio due libri con il nome "The Return". Il terzo è quando hanno descrizioni simili su Wikipedia. E infine, quando hanno informazioni simili o attributi simili su Wikipedia. Ad esempio, lo stesso genere o lo stesso artista per una canzone. Quando mostriamo questa domanda alternativa agli annotatori, sanno i nomi di questi enti, ma non necessariamente ne conoscono i dettagli. Quindi, ciò che facciamo è mostrare loro alcune informazioni di background sui due enti. Per le canzoni, mostriamo semplicemente un link di ricerca su Google per ciascuna canzone e chiediamo agli annotatori di ascoltare almeno parte di ciascuna canzone e di leggere informazioni su ciascuna. Ecco ad esempio i risultati della ricerca su Google per la canzone "Easy on Me". Per i domini ricette e libri, mostriamo alcuni testi di background da Wikipedia. Per le ricette, mostriamo anche le loro immagini, sempre da Wikipedia, in modo che gli annotatori sappiano com'è l'aspetto. Poi, chiediamo agli annotatori di selezionare uno di questi enti, ad esempio il primo, e di descriverli utilizzando tre a cinque espressioni di riferimento indiretto. Ad esempio, "quello senza parole", "non quello con il ragazzino di 12 anni", o "il fittizio", o "proviene dall'Azerbaigian", e così via. Il corpus AltEntities ha 6.000 domande alternative in tre domini, e contiene 42.000 espressioni di riferimento indiretto. I risultati con il modello T5 XL sono riassunti di seguito. Se il modello linguistico ha accesso alla stessa conoscenza di background degli annotatori, l'accuratezza è molto alta, intorno al 92-95%. Ma non è realistico. Se il modello linguistico ha accesso a una conoscenza di background parzialmente sovrapposta, l'accuratezza è tra l'82 e l'87%, che è più realistica. Ad esempio, quando il modello linguistico recupera la conoscenza di background. Se il modello linguistico ha accesso solo ai nomi degli enti, l'accuratezza è solo del 60%, quindi c'è molto spazio per migliorare. Abbiamo anche mostrato che i modelli sono generalizzabili su diversi domini. Ecco il link al nostro dataset. Grazie.</sample>
    <sample id="143">L'approccio EDAtt viene confrontato con le politiche SimulST esistenti Wait-k e Local Agreement, nonché con l'architettura all'avanguardia specificamente progettata per la traduzione simultanea.</sample>
    <sample id="144">L'affiliazione degli autori non è specificata nel testo fornito.</sample>
    <sample id="145">Il nome della relatrice è Jenny.</sample>
    <sample id="146">**Abstract:**  
Yicheng presenta un lavoro che analizza il problema dell'omissione nei sistemi di sintesi di dialoghi, un aspetto critico che limita la qualità delle sommari generate da modelli linguistici pre-addestrati. Nonostante i progressi recenti, le sommari generate spesso presentano errori fatti, tra cui l'omissione di informazioni chiave, che rende le sommari incomplete e poco utili in contesti reali. L'analisi condotta su cinque domini e sei modelli pre-addestrati rivela che circa il 70% delle sommari presenta omissis, con informazioni omesse distribuite in modo casuale nei dialoghi. Per affrontare questo problema, l'equipe ha creato il dataset OLDS, il primo dataset di riferimento per l'analisi dell'omissione, basato su cinque benchmark esistenti e arricchito con etichette di omissione generate automaticamente e validate da umani. L'equipe ha inoltre esplorato tre architetture di base per la rilevazione dell'omissione, ottenendo un F1-score intorno al 50%, che sottolinea la complessità del compito. Infine, i risultati mostrano che l'uso dell'informazione omessa per raffinare le sommari mediante un metodo di post-editing porta a un miglioramento significativo della qualità. Questo lavoro evidenzia l'importanza della rilevazione dell'omissione come passo fondamentale per migliorare la sintesi di dialoghi.</sample>
    <sample id="147">L'articolo è stato realizzato in collaborazione con tre autori: Myra, Esin Durmus e Dan Jurafsky.</sample>
    <sample id="148">Ciao, sono Sara Papi dell'Università di Trento e della Fondazione Bruno Kessler, e brevemente presenterò il lavoro "Attention as a Guide for Simultaneous Speech Translation", un lavoro congiunto con Matteo Negri e Marco Turchi. Che cos'è la traduzione simultanea del parlato? La traduzione simultanea del parlato, o SimulST, è il processo di tradurre un linguaggio parlato in un testo in un'altra lingua in tempo reale, permettendo la comunicazione tra lingue diverse. E quali sono i problemi dei modelli attuali di SimulST? Solitamente si adottano architetture specifiche, introducendo moduli aggiuntivi da ottimizzare. Procedure di addestramento lunghe e complicate, ad esempio addestramenti che coinvolgono diversi obiettivi di ottimizzazione. Inoltre, addestrare e mantenere diversi modelli per raggiungere diversi regimi di latenza. Ad esempio, addestrare un modello con una latenza media di un secondo e un altro con due secondi, e così via. Quale è la nostra soluzione? Prima di tutto, utilizzare modelli di traduzione offline già esistenti senza riallineamento o adozione di architetture specifiche per la SimulST. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici. Sfruttare inoltre le conoscenze già acquisite dal modello attraverso il meccanismo di attenzione tra input audio e output testuale, ovvero il meccanismo di attenzione incrociata, che puoi vedere nell'esempio a destra. La nostra soluzione è proporre EDAtt, ovvero Encoder-Decoder Attention, una strategia in base alla quale decidiamo se emettere o meno una traduzione parziale in base a dove si concentra l'attenzione. Una parola viene emessa se l'attenzione non è concentrata, ovvero se la somma è al di sotto di un certo threshold alpha verso gli ultimi lambda frame di parlato, il che significa che l'informazione ricevuta è abbastanza stabile. Ad esempio, se riceviamo un frammento di parlato contenente "I'm going to talk about..." e il nostro modello predice la traduzione in tedesco, osserveremo i pesi dell'attenzione incrociata e vedremo che le prime due parole si riferiscono ai primi frame di parlato ricevuti, mentre l'ultima parola si riferisce agli ultimi frame ricevuti, ovvero ai lambda frame di parlato. Questo significa che le prime due parole verranno emesse, mentre, poiché la somma dell'attenzione incrociata è al di sopra di un certo threshold alpha, non emetteremo l'ultima parola e aspetteremo un altro frammento di parlato. Se procediamo e riceviamo un altro frammento di parlato e il nostro modello prevede altre tre parole, osserveremo i pesi dell'attenzione incrociata e vedremo che nessuna parola si riferisce agli ultimi lambda frame di parlato. Questo significa che queste tre parole verranno emesse. Se guardiamo ai principali risultati di EDAtt, tracceremo i risultati della traduzione simultanea del parlato in grafici in cui abbiamo BLEU su un lato che misura la qualità della traduzione, e il ritardo medio che è una misura della latenza, e consideriamo anche il ritardo medio consapevole del calcolo che tiene conto del tempo computazionale del modello per prevedere l'output. Vogliamo che le nostre curve siano il più alte possibile in questo grafico. Ma vogliamo anche che siano spostate a sinistra. Confrontiamo con strategie popolari che vengono applicate anche ai modelli offline, come la strategia Wait-k e l'Accordo Locale. Confrontiamo anche con l'architettura di stato dell'arte specificamente adattata per la traduzione simultanea. Questi sono tutti i risultati della strategia di traduzione simultanea del parlato nel tedesco. Vediamo che supera tutte le strategie applicate ai modelli offline, poiché le curve sono spostate a sinistra. Vediamo anche che se consideriamo il tempo effettivo trascorso o il tempo consapevole del calcolo, ovvero la strategia più veloce. Se desideri scoprire ulteriori risultati, leggi il nostro articolo. Abbiamo anche rilasciato in open source il codice e i modelli, nonché l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per l'attenzione.</sample>
    <sample id="149">Sì, il set di dati CoNLL++ è disponibile pubblicamente.</sample>
    <sample id="150">**Abstract**  
In questo lavoro, presentiamo MeetingQA, un nuovo dataset per il question answering estrattivo basato su trascrizioni di riunioni. Le riunioni rappresentano un dominio ricco di informazioni e spesso contengono domande aperte che generano discussioni dettagliate tra i partecipanti. Tuttavia, i lavori precedenti hanno trascurato questo aspetto, concentrandosi principalmente sulla sintesi e sull'estrazione di azioni. MeetingQA colma questa lacuna, offrendo 7.700 domande, tra cui il 30% non rispondibili, con risposte che possono includere più speaker, più frasi discontinue o domande retoriche. Il dataset è stato creato a partire dal corpus AMI, con un alto accordo inter-annotatore (Krippendorff’s alpha 0.73). Le domande sono prevalentemente di tipo sì/no o di opinione, e il 70% delle risposte multiple include disaccordi. Le risposte medie hanno una lunghezza di circa 35 parole. I nostri esperimenti mostrano una significativa gap tra le prestazioni dei modelli finetunati (F1 84.6) e quelle umane, con un gap di oltre 25 punti F1. Inoltre, i modelli hanno difficoltà a identificare domande retoriche e a distinguere i parlanti delle risposte, specialmente in setting zero-shot. MeetingQA costituisce un nuovo e sfidante dominio per il QA, con potenziale per migliorare modelli esistenti attraverso data augmentation e addestramento su contesti complessi.</sample>
    <sample id="151">Ciao a tutti, il mio nome è Ying e il mio collega Zhiyang e presenteremo la nostra ricerca su MultiInstruct, che migliora l'apprendimento zero-shot multi-modale tramite tuning delle istruzioni. Con gli sviluppi nei modelli linguistici di grandi dimensioni, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento che riutilizzano i modelli linguistici pre-addestrati per diversi compiti downstream in modo efficiente dal punto di vista dei parametri e dei dati. Recentemente, molti studi hanno mostrato che il tuning delle istruzioni permette ai modelli linguistici di grandi dimensioni di eseguire compiti non visti in modo zero-shot seguendo istruzioni naturali. Tuttavia, la maggior parte dei lavori precedenti sul tuning delle istruzioni si è concentrata sull'incremento delle prestazioni zero-shot su compiti linguistici soltanto, mentre i compiti di computer vision e multi-modale sono stati trascurati. Pertanto, in questo lavoro vogliamo investigare se il tuning delle istruzioni su modelli pre-addestrati multi-modali possa effettivamente migliorare la generalizzazione su compiti multi-modali non visti. Inoltre, durante la nostra ricerca, abbiamo scoperto una notevole discrepanza nella disponibilità di dataset di istruzioni tra NLP e multi-modale. Esistono più di 1600 compiti linguistici soltanto con istruzioni. Tuttavia, non esiste un dataset di grandi dimensioni e pubblicamente disponibile per compiti multi-modali. Questo motiva il nostro lavoro di creare un dataset di tuning delle istruzioni multi-modale. Presentiamo quindi MultiInstruct, il primo benchmark dataset di tuning delle istruzioni multi-modale che comprende 62 compiti multi-modali diversi coprendo 10 categorie ampie. Questi compiti provengono da 21 dataset open-source esistenti e ogni compito è dotato di cinque istruzioni scritte da esperti. Per investigare il tuning delle istruzioni multi-modale sul nostro dataset proposto, utilizziamo OFA, un modello pre-addestrato multi-modale unificato, come modello base. OFA utilizza un vocabolario unificato per testo, token immagine e coordinate di una scatola delimitatrice. Mostriamo alcuni esempi di istanze dal nostro dataset MultiInstruct, per unificare il trattamento di diversi tipi di dati di input e output. Seguiamo il metodo di OFA e formuliamo tutti i compiti in un formato unificato sequenza-a-sequenza. In questo formato, il testo di input, le immagini, le istruzioni e le scatole delimitatrici sono rappresentati nello stesso spazio dei token. Ora parlerò del tuning delle istruzioni multi-modale. Per il dataset di addestramento, utilizziamo 53 compiti provenienti da 9 gruppi per l'addestramento e campioniamo 10.000 istanze per compito. Per i test, riserviamo l'intero gruppo di ragionamento comune per i test, e selezioniamo in aggiunta 5 compiti dai gruppi VQ e Miscellaneous. Utilizziamo tutte le istanze nel set di test per ogni compito. Inoltre, campioniamo casualmente 20 compiti dal set di test delle istruzioni naturali come compiti non visti per NLP. Utilizziamo il modello OFA pre-addestrato di grandi dimensioni come modello base. Durante l'addestramento, mescoliamo tutte le istanze per tutti i compiti. Ogni istanza viene casualmente combinata con una delle cinque istruzioni. Durante i test, per ogni compito, conduciamo un totale di 5 esperimenti valutando il modello utilizzando una delle cinque istruzioni. In ogni esperimento, riferiamo la prestazione minima e massima e la deviazione standard delle prestazioni su tutti e 5 gli esperimenti. Se il compito è un compito di classificazione multi-modale, riferiamo l'accuratezza. Se è un compito di generazione multi-modale, riferiamo il Rouge-L. Per i compiti NLP, riferiamo anche il Rouge-L. Introduciamo inoltre un ulteriore metrica di valutazione chiamata sensibilità. Questa misura la capacità del modello di produrre sempre gli stessi output per lo stesso compito indipendentemente dalle lievi variazioni nella formulazione dell'istruzione. Ecco i nostri risultati principali. Come possiamo vedere, il tuning delle istruzioni può migliorare significativamente le prestazioni di OFA sui compiti multi-modali visti. Inoltre, l'apprendimento trasferibile da dataset di istruzioni naturali può beneficiare del tuning delle istruzioni. Come possiamo vedere, all'aumentare del numero di compiti, il modello raggiunge prestazioni migliori e al tempo stesso una sensibilità minore. Abbiamo anche condotto un esperimento. Abbiamo utilizzato una sola istruzione contro cinque istruzioni. Come possiamo vedere, l'uso di più istruzioni può migliorare le prestazioni complessive del modello e ridurre notevolmente la sua sensibilità. Questo mostra l'effetto di diverse strategie di fine-tuning sulla sensibilità del modello. Come possiamo vedere, l'apprendimento trasferibile da dataset di istruzioni naturali permette al modello di raggiungere una sensibilità molto migliore rispetto al modello OFA originale. Possiamo anche vedere che l'apprendimento trasferibile da dataset di istruzioni naturali può aiutare OFA a raggiungere prestazioni molto migliori sul dataset di istruzioni naturali. Complessivamente, proponiamo il primo dataset di grandi dimensioni per il tuning delle istruzioni multi-modale, con un miglioramento significativo delle capacità di OFA, e esploriamo diverse tecniche di apprendimento trasferibile mostrando i loro benefici. Abbiamo progettato una nuova metrica chiamata sensibilità. Un'ultima cosa, stiamo raccogliendo un dataset molto più grande per il tuning delle istruzioni multi-modale, con circa 150 compiti aggiuntivi di visione linguistica e li rilasceremo. Questo è il codice QR per i nostri dati e modello. Grazie.</sample>
    <sample id="152">**Abstract**  
In this presentation, Frederick Riemenschneider introduces new language models specifically designed for classical philology, focusing on Ancient Greek and Latin. Building on existing models such as Latin BERT and Ancient Greek BERT, the authors highlight the limitations of current approaches, including their monolingual nature and lack of comprehensive evaluation. To address these challenges, they developed two monolingual models—GreBERTa (based on RoBERTa) and GreTa (based on T5)—as well as their multilingual counterparts, PhilBERTa and PhilTa, pre-trained on Ancient Greek, Latin, and English data. A key innovation was the creation of a high-quality pre-training corpus for Ancient Greek, leveraging the Internet Archive by identifying Greek texts through incorrectly transcribed stop words and re-OCR-ing them with appropriate settings. The models were benchmarked on tasks such as part-of-speech tagging, dependency parsing, and lemmatization, where they outperformed existing state-of-the-art models. Notably, GreTa demonstrated superior performance in lemmatization, achieving a 5 percentage point improvement for Ancient Greek. The study also explored the behavior of T5 encoders and investigated semantic and world knowledge capabilities, finding no significant difference in performance between multilingual and monolingual models. Overall, the research presents a significant advancement in NLP for classical philology, offering robust, multilingual models and a new pre-training dataset that open new possibilities for the field.</sample>
    <sample id="153">**Abstract:**  
Ninareh Mehrabi e il team di Responsible AI di Amazon Alexa AI presentano un lavoro intitolato "Resolving Ambiguities in Text-to-Image Generative Models", che si concentra sull'analisi e la risoluzione delle ambiguità nei prompt utilizzati per generare immagini a partire da testo. I prompt ambigui, come "The girl enters the room with flowers", possono essere interpretati in modi diversi, rendendo difficile per i modelli generativi produrre immagini fedeli all'intenzione dell'utente. Per affrontare questo problema, il team ha creato un dataset benchmark basato su LAVA, che copre diversi tipi di ambiguità. Hanno proposto due framework per la disambiguazione: uno che genera domande per chiarire l'intenzione dell'utente e un altro che propone diverse interpretazioni visive, da cui l'utente può scegliere. Dopo la disambiguazione, le immagini generate vengono valutate utilizzando un framework automatico basato su un modello VQA, che confronta l'immagine con l'intenzione dell'utente. I risultati mostrano che i framework proposti migliorano la fedeltà delle immagini generate e che l'approccio automatico è in accordo con le valutazioni umane. Questo lavoro contribuisce a migliorare la comprensione e la gestione delle ambiguità nei modelli text-to-image, con applicazioni significative per l'AI responsabile e l'interazione uomo-macchina.</sample>
    <sample id="154">Gli autori dell'articolo sono affiliati all'Università di Trento e alla Fondazione Bruno Kessler.</sample>
    <sample id="155">Il nome del relatore è Javad Hosseini.</sample>
    <sample id="157">**Abstract:**  
Shen Gao da Shandong University presenta il lavoro "Dialogue Summarization with Static-Dynamic Structure Fusion Graph", un approccio innovativo per la sintesi di dialoghi. L'obiettivo è estrarre informazioni salienti da contesti dialogici complessi e multi-partecipanti, fornendo un riassunto conciso. I metodi esistenti si basano su strutture grafiche statiche pre-calcolate, come il parsing del discorso o il tracciamento dello stato del dialogo, ma presentano due limiti principali: la dipendenza da strumenti linguistici esterni, che possono introdurre errori, e la mancanza di adattamento dinamico alle esigenze del compito di sintesi. Per risolvere questi problemi, l'approccio proposto, SDDS, integra strutture grafiche statiche e dinamiche. Prima, un encoder trasforma gli utterances in rappresentazioni vettoriali, mentre quattro metodi heuristici (come il KeyCo-occ e il parsing del discorso) costruiscono il grafico statico. Successivamente, un modulo grafico dinamico, basato su multi-head attention, cattura relazioni semantiche tra utterances. Le informazioni statiche e dinamiche vengono fuse in un unico grafico, Gᵘ, e un meccanismo di cross-attention doppio è integrato nel generatore di riassunti basato su un modello linguistico pre-addestrato. Questo permette di catturare sia la struttura del dialogo che le relazioni semantiche, migliorando la qualità del riassunto. Il codice e i dati sono disponibili su GitHub.</sample>
    <sample id="158">**Abstract:**  
Qipeng Guo presenta il lavoro "Dual Cache for Long Document Neural Coreference Resolution", un approccio innovativo per migliorare la risoluzione della coreferenza in documenti lunghi. La coreferenza consiste nell'identificare e raggruppare le menzioni che si riferiscono allo stesso entità all'interno di un testo. Metodi tradizionali, basati sull'enumerazione di tutte le coppie di menzioni, hanno una complessità quadratica, mentre i metodi basati su cache riducono questa complessità a livello lineare. Tuttavia, in documenti lunghi, dove gli argomenti possono cambiare frequentemente, le politiche di evizione comuni, come LRU, possono generare un alto numero di mancate corrispondenze (cache miss), specialmente per entità di alta frequenza. Per affrontare questo problema, l'approccio proposto introduce un sistema a doppia cache: una cache locale, che utilizza LRU per gestire entità locali, e una cache globale, che utilizza LFU per gestire entità frequenti. Questo sistema permette di ridurre significativamente i cache miss, migliorando l'efficienza e le prestazioni. I test su benchmark pubblici e su un testo di 30.000 parole mostrano che il Dual Cache supera i metodi basati su singola cache, anche in assenza di dati di training, e offre un miglior rapporto tra prestazioni e costo. In sintesi, il Dual Cache rappresenta un avanzamento significativo per la risoluzione della coreferenza in documenti lunghi, combinando efficienza e accuratezza.</sample>
    <sample id="159">Ciao a tutti. Sono Koustav Sinha, e sono lieto di darvi il benvenuto nel nostro talk sul nostro articolo presentato al ACL 2023. I giudizi di accettabilità dei modelli linguistici non sono sempre robusti rispetto al contesto. Questo lavoro è un contributo congiunto con John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy e Adina Williams. In questo lavoro, rivediamo i paradigmi delle coppie minimali. Il paradigma delle coppie minimali valuta i modelli linguistici in base ai giudizi di accettabilità, che possono includere anche la grammaticalità come in BLiMP, SyntaxGym, o l'accettabilità in termini di stereotipi come in CrowS pairs. In questi paradigmi, il modo tipico per valutare i modelli linguistici è mostrare una frase accettabile o grammaticale e poi una frase non accettabile o non grammaticale, sperando che il modello assegni una probabilità maggiore alla frase accettabile. Il pipeline attuale delle coppie minimali non permette di valutare l'accettabilità del modello su frasi più lunghe. Oggi i modelli linguistici di grandi dimensioni stanno emergendo con finestre di contesto sempre più lunghe. È quindi fondamentale valutare l'accettabilità del modello lungo l'intera finestra di contesto, e questo è l'obiettivo che stiamo perseguendo in questo lavoro. Stiamo rivedendo il pipeline delle coppie minimali chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe. L'approccio che adottiamo è simulare queste sequenze più lunghe rivedendo i dataset stessi e ricreando frasi selezionando quelle accettabili o non accettabili da quei dataset. Ad esempio, abbiamo scelto una coppia tipica di grammaticalità dal dataset BLiMP, nel caso degli "Adjunct Island". Quello che facciamo è ricreare sequenze più lunghe, accettabili e con la stessa struttura grammaticale. Estraiamo frasi grammaticali dagli "Adjunct Island" e le aggiungiamo come prefisso alle frasi accettabili e non accettabili. Possiamo fare lo stesso scegliendo frasi non accettabili dallo stesso fenomeno, e così possiamo testare l'accettabilità del modello. Possiamo anche farlo scegliendo frasi da un sottoinsieme diverso o da un dataset diverso. Questo è ciò che chiamiamo "scenario di mismatch". In questo caso, le frasi provengono da dataset rilevanti, ma non da quelli utilizzati per l'valutazione. Possiamo fare lo stesso anche nel caso delle frasi non accettabili. Infine, possiamo scegliere frasi da un dominio completamente diverso, come Wikipedia. Questo ci permetterà di capire se i giudizi di accettabilità del modello sono effettivamente influenzati da un contesto, se il contesto proviene da un sottoinsieme diverso del dataset o se è completamente irrilevante rispetto alla frase che stiamo analizzando. Come si comporta il modello? Prima di tutto, analizziamo le frasi di Wikipedia, completamente irrilevanti rispetto alla coppia di query attuale, e lì troviamo che i giudizi MPP sono in gran parte robusti per qualsiasi lunghezza di contesto. Aumentiamo la lunghezza del contesto fino a 1024 per massimizzare i modelli OPT e GPT-2. Qui, come mostrato dalla linea tratteggiata arancione, i giudizi MPP rimangono relativamente stabili. Cosa accade quando scegliamo frasi dallo stesso dataset? Qui stiamo creando frasi accettabili e non accettabili dallo stesso dataset BLiMP o SyntaxGym. Lì notiamo che i giudizi MPP aumentano o diminuiscono significativamente quando aggiungiamo prefissi accettabili o non accettabili. Tuttavia, quando il prefisso è coerente con la struttura, cioè quando scegliamo frasi dallo stesso fenomeno in BLiMP o SyntaxGym, notiamo un aumento o una diminuzione massiva dei giudizi MPP, a seconda che il prefisso scelto sia accettabile o non accettabile. Questo effetto è molto significativo e aumenta con la lunghezza del contesto, e probabilmente influenzerà anche i nuovi modelli linguistici con finestre di contesto molto ampie. Perché i prefissi coerenti influenzano così tanto i giudizi del modello linguistico? Abbiamo eseguito una serie di analisi in cui abbiamo cercato di perturbare la frase di input, preservando la struttura rilevante ma aggiungendo del rumore all'input. Dopo aver eseguito diverse di queste perturbazioni, abbiamo scoperto che nessun rumore ha effettivamente cambiato il comportamento del modello in termini di giudizi MPP. In sostanza, abbiamo notato che i modelli sono sensibili alle frasi perturbate in modo simile. Ovvero, quando perturbiamo le frasi nell'ambito delle frasi accettabili, vediamo un aumento simile in tutte le perturbazioni, e quando perturbiamo le frasi nell'ambito delle frasi non accettabili, vediamo una diminuzione simile dei giudizi MPP. I principali risultati del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti che sono condivise tra le frasi. L'attuale valutazione MPP, effettuata con input di breve lunghezza e singole frasi, potrebbe non catturare appieno la conoscenza astratta dei modelli linguistici lungo l'intera finestra di contesto. Per ulteriori dettagli sugli esperimenti, vi invitiamo a leggere il nostro articolo. Grazie per l'attenzione.</sample>
    <sample id="160">Il primo passaggio del metodo mappa i token di input in token di output.</sample>
    <sample id="161">CoScript rappresenta 55.000 script.</sample>
    <sample id="163">Il metodo di allineamento migliore per DEPLAIN è MASSalign.</sample>
    <sample id="164">Il vantaggio dell'apprendimento scarsamente supervisionato è che utilizza etichette deboli, che sono molto più economiche rispetto alle annotazioni manuali, anche se sono rumorose e non sempre accurate.</sample>
    <sample id="165">**Abstract:**  
Wenting Zhao presenta un lavoro intitolato "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations", in cui si affronta il problema dell'abductive reasoning in un contesto di senso comune. L'obiettivo è identificare spiegazioni plausibili che collegano un contesto e un risultato, come nel caso in cui Emily, bloccata nel traffico, riesca comunque a prendere il volo. L'approccio tradizionale richiede annotazioni supervisionate, ma queste possono essere soggettive e rumorose. Zhao propone LiPoR (Likelihood Learning with Posterior Regularization), un metodo non supervisionato che tratta le spiegazioni come variabili latenti e massimizza la verosimiglianza marginale del risultato dato il contesto. Per migliorare la selezione delle spiegazioni, LiPoR introduce un regolarizzatore che sfrutta la caratteristica di esclusività mutua tra le spiegazioni: non possono essere tutte vere contemporaneamente. Questo regolarizzatore favorisce l'identificazione di un subset di spiegazioni plausibili, riducendo l'entropia della distribuzione delle spiegazioni dato contesto e risultato. I risultati su AlphaNLI, il dataset più utilizzato per l'abductive reasoning, mostrano che LiPoR supera i modelli a zero-shot e l'approccio non supervisionato precedente, migliorando l'accuratezza di oltre 4 punti assoluti. Questo lavoro apre la strada a un'approccio non supervisionato per l'abductive reasoning, riducendo la dipendenza da annotazioni esplicite.</sample>
    <sample id="166">**Abstract:**  
Yunxin da Harbin Institute of Technology, Shenzhen, presenta un nuovo lavoro intitolato "A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text". Questo compito, che consiste nel recuperare immagini a partire da testi linguistici complessi, è particolarmente difficile a causa della similitudine elevata tra le immagini e della complessità delle descrizioni. I modelli visivo-linguistici pre-addestrati, pur performando bene su compiti di retrieval immagine-frase, mostrano un calo significativo di prestazioni quando affrontano testi complessi. Per risolvere questo problema, l'approccio proposto si ispira alla strategia Divide-and-Conquer e alla Dual-Process Theory. La strategia Divide-and-Conquer suddivide il problema complesso in sottoproblemi più semplici, mentre la Dual-Process Theory distingue due sistemi di pensiero: il Sistema 1, basato su ragionamento analogico, e il Sistema 2, specializzato in ragionamento logico. Il framework proposto, chiamato NDCR, combina questi due approcci. Il modello include un Proposition Generator per decomporre il testo complesso, un Visual-Linguistic Interactor (Sistema 1) per l'interazione visivo-linguistica, e un Neural-Symbolic Reasoner (Sistema 2) per integrare i risultati logici. Gli esperimenti mostrano che NDCR supera gli approcci esistenti, confermando l'efficacia del metodo. Inoltre, il framework permette un ragionamento interoperabile, mostrando gli stati e i risultati intermedi. Questo lavoro apre nuove prospettive per migliorare il ragionamento composto nei modelli linguistici di grandi dimensioni.</sample>
    <sample id="167">I documenti in DEPLAIN-web sono stati allineati sia manualmente che con metodi automatici. In particolare, 750 documenti sono stati allineati utilizzando entrambi i metodi, mentre i documenti di DEPLAIN-apa (483 documenti) sono stati allineati solo manualmente.</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato raccogliendo notizie dal Reuters News del 2020 e annotandole seguendo le stesse linee guida di annotazione del CoNLL-2003.</sample>
    <sample id="169">**Abstract:**  
David Vilar presenta una panoramica del lavoro "Prompting PaLM for Translation: Assessing Strategies and Performance", realizzato in collaborazione con Google Translate. L'articolo analizza per la prima volta sistematicamente l'uso di prompt in modelli linguistici di grandi dimensioni, come PaLM, per la traduzione automatica. Il modello PaLM, con 540 miliardi di parametri, è stato valutato utilizzando le migliori pratiche del campo, evitando sovrapposizioni tra dati di test e dati di addestramento. I risultati sono stati confrontati con sistemi di traduzione neurale di alta qualità (come quelli valutati in WMT) e valutati con metriche automatiche (come BLEURT) e valutazioni umane (MQM). L'esperienza mostra che la scelta del prompt ha un impatto significativo sulle prestazioni, con differenze di oltre un punto BLEURT e, in casi estremi, fino a 40 punti. La strategia più efficace risulta essere il prompt a 5 esempi, dove la qualità degli esempi è più importante della somiglianza con la frase sorgente. Gli esempi selezionati da dataset di alta qualità (come i dati di validazione WMT) portano a risultati migliori. Tuttavia, i sistemi specializzati mantengono un vantaggio sostanziale su PaLM, anche se quest'ultimo si avvicina a sistemi commerciali come Google Translate. La traduzione di PaLM è fluente, ma presenta errori di omissione, riducendo l'accuratezza complessiva.</sample>
    <sample id="170">Ciao a tutti, mi chiamo Yusen Zhang dell'Università di Pennsylvania State. Oggi presenterò il nostro lavoro intitolato "XSemPLR: Parsing Semantico Cross-Lingua in Multiple Lingue Naturali e Rappresentazioni di Significato". Il parsing semantico è un compito che mira a costruire rappresentazioni semantiche delle query degli utenti, come ad esempio SQL e Lambda Calculus. Il parsing semantico cross-lingua è il compito di tradurre query in diverse lingue naturali in diverse rappresentazioni di significato. Come mostrato in questa figura, dobbiamo tradurre le query in diverse lingue naturali utilizzando modelli neurali in SQL, Lambda o FunQL, eccetera. I modelli esistenti per il parsing semantico cross-lingua sono stati proposti e valutati separatamente su dataset limitati per compiti e applicazioni specifiche. Ad esempio, c'è una copertura molto elevata per alcune lingue naturali, ma manca la copertura per la lingua cinese e per certe rappresentazioni di significato, come il calcolo lambda. Inoltre, vengono valutati solo su un singolo modello neurale. Per questo motivo, proponiamo XSemPLR. Offriamo un dataset uniforme XSemPLR per il parsing semantico cross-lingua in molte lingue naturali e rappresentazioni di significato. Contiene 9 dataset in diversi domini, 5 compiti di parsing semantico, 8 rappresentazioni di significato e 22 lingue naturali in 15 famiglie linguistiche. Per valutare meglio il nostro benchmark, consideriamo sei impostazioni per l'addestramento e l'analisi. La prima è "Translate-Test". Utilizziamo l'API di Google Translate per tradurre la lingua di origine in quella target, quindi utilizziamo un modello monolingua per addestramento e valutazione. Ad esempio, addestriamo un modello in inglese su query in inglese, e durante l'inferenza traduciamo le query in tedesco tramite API in inglese e utilizziamo il modello addestrato per prevedere SQL. Valutiamo anche i modelli monolingua. In questa impostazione, la lingua di origine è la stessa della lingua target, ad esempio tedesco a tedesco o inglese a inglese. Valutiamo anche un'impostazione monolingua a pochi esempi, addestrando i modelli monolingua con solo il 10% dei dati di addestramento. Valutiamo inoltre i modelli multilingua, addestrando un unico modello multilingua per tutte le lingue. Ad esempio, uniamo le query in tedesco, inglese e cinese per addestrare un modello multilingua. Durante l'inferenza, possiamo utilizzare questo modello per tradurre query in tedesco o in cinese, eccetera. Consideriamo anche il trasferimento cross-lingua a zero esempi e a pochi esempi. Addestriamo su una lingua di origine e trasferiamo su un'altra lingua. Durante l'addestramento, addestriamo su query in inglese o su un mix di query in inglese e tedesco a pochi esempi per addestrare un modello multilingua che preveda l'output SQL. Abbiamo anche trovato diversi risultati interessanti. Riguardo all'analisi dei modelli monolingua, valutiamo due gruppi di modelli: Encoder-PTR, che rappresenta Encoder multilingua pre-addestrati con Decoder basati su Pointer, come XLM-R + PTR e mBERT + PTR. Valutiamo anche i modelli Encoder-Decoder, che sono modelli Encoder-Decoder multilingua pre-addestrati, come mBART e mT5. Abbiamo scoperto che i modelli Encoder-Decoder ottengono le prestazioni migliori su tutti e nove i dataset. Valutiamo su mT5 e XLM-R + PTR nell'impostazione multilingua. Abbiamo scoperto che Encoder-Decoder o Encoder-PTR possono essere migliorati addestrandoli su un mix di diverse lingue. Abbiamo scoperto che ciò accade perché la maggior parte delle principali lingue naturali ottiene un guadagno di prestazioni, eccezion fatta per l'inglese, il cui rendimento cala in sette dataset e migliora solo in tre. Penso che ciò sia noto come "Curse of Multilinguality". Confrontiamo anche il gap di prestazioni cross-lingua. In questa figura, la linea blu rappresenta il trasferimento cross-lingua a pochi esempi. La linea arancione rappresenta il trasferimento cross-lingua a zero esempi. La linea verde rappresenta l'impostazione monolingua. Abbiamo scoperto che, confrontando la linea verde e arancione, abbiamo notato che nel caso di zero esempi, il gap di prestazioni del trasferimento cross-lingua è significativo, e confrontando le linee blu e arancione, abbiamo notato che con l'impostazione a pochi esempi, il gap del trasferimento si riduce rapidamente. Abbiamo anche trovato altre interessanti osservazioni. Ad esempio, i modelli Encoder-Decoder superano il lavoro precedente o raggiungono risultati comparabili. L'addestramento su lingua naturale inglese può migliorare significativamente le prestazioni a pochi esempi nelle lingue target naturali, e abbiamo scoperto che i modelli linguistici multilingua come Codex e BLOOM sono ancora insufficienti per i compiti di parsing semantico cross-lingua. In sintesi, abbiamo costruito XSemPLR, un benchmark unificato per il parsing semantico cross-lingua con molte lingue naturali e rappresentazioni di significato. Abbiamo condotto uno studio completo del benchmark su tre tipi rappresentativi di modelli linguistici multilingua. I nostri risultati mostrano molte interessanti osservazioni. Etcetera. Benvenuti a visitare il nostro articolo e il codice. Grazie per l'attenzione.</sample>
    <sample id="171">I lavori connessi in tal senso sono classificabili in quattro categorie, ma presentano limiti: non sono applicabili agli embedding come servizio o mancano di trasferibilità.</sample>
    <sample id="172">No, gli LLM multilingue come Codex o Bloom non sono sufficienti per il Cross-Lingual Semantic Parsing (CLSP).</sample>
    <sample id="174">**Abstract:**  
Thea, co-autore del dataset "ArgAnalysis35K: A large-scale dataset for Argument Quality Analysis", presenta un'analisi sui punti di forza unici di questo dataset rispetto ad altri esistenti nel campo. L'obiettivo principale è valutare la qualità degli argomenti su una scala da 0 a 1, distinguendo tra argomenti deboli e persuasivi. Il dataset si distingue per la sua vastità (35.000 coppie argomento-analisi), la qualità elevata degli argomenti (85% provenienti da tornei e debattitori esperti), e la diversità tematica, basata su 24 argomenti selezionati da fonti esperte. Un aspetto innovativo è l'introduzione del concetto di "analisi", che combina premesse, affermazioni e logiche, fornendo un'interpretazione più completa degli argomenti rispetto ai dati tradizionali. Inoltre, il dataset implementa un modello di rilevanza, che assegna un punteggio per la rilevanza di ogni argomento rispetto a diversi temi, e utilizza un'annotazione basata sull'istanza per migliorare l'affidabilità, eliminando solo le annotazioni potenzialmente biasate. Queste caratteristiche rendono ArgAnalysis35K un dataset più diversificato, rappresentativo e affidabile, utile per applicazioni in NLP e analisi argomentativa. Per ulteriori dettagli, si consiglia di consultare l'articolo e il poster presentati.</sample>
    <sample id="175">Il metodo affronta l'ambiguità delle permutazioni utilizzando una relazione continua e amichevole per GPU, che permette di approssimare la soluzione del problema NP-hard legato alle permutazioni, consentendo anche il backpropagation per imparare le permutazioni linguisticamente plausibili.</sample>
    <sample id="176">L'equità di un modello NLP a valle viene definita in termini di capacità del modello di trattare equamente diverse categorie demografiche e opinioni politiche, senza favorire o discriminare gruppi specifici, soprattutto in compiti come la rilevazione dell'odio o delle notizie false. L'equità è messa in discussione quando i modelli, a causa di bias politici, mostrano prestazioni diverse in base all'appartenenza politica o sociale dei gruppi coinvolti.</sample>
    <sample id="177">Il nome del relatore è Yanis Labrak.</sample>
    <sample id="178">Il nome del relatore è Koustav Sinha.</sample>
    <sample id="179">**Abstract:**  
Melanie Sclar presenta "SymbolicToM", un metodo plug-and-play per migliorare le capacità di Theory of Mind (ToM) nei Large Language Models (LLMs). La ToM è la capacità di ragionare sulle menti altrui, spesso testata attraverso domande su false-belief, come nel classico esperimento Sally-Anne. Nonostante i modelli linguistici di grandi dimensioni (es. GPT-3) mostrino una performance scarsa in questi compiti, SymbolicToM utilizza rappresentazioni grafiche simboliche per rappresentare le credenze di diversi personaggi in una storia. Queste rappresentazioni, generate in tempo di inferenza con modelli NLI e OpenIE, permettono di rispondere a domande complesse, come quelle di second-order belief, in modo più preciso. I test su diversi LLM, inclusi GPT-3 e GPT-4, mostrano miglioramenti significativi: ad esempio, un aumento di 65 punti di accuratezza per GPT-3-Davinci. Inoltre, SymbolicToM è robusto a variazioni di struttura narrativa e diversità linguistica, superando modelli supervisionati in compiti out-of-domain. Il metodo evita il rischio di overfitting, offre ragionamenti più interpretabili e si dimostra vantaggioso su dataset come ParaphrasedToMi, con una maggiore tolleranza alle variazioni linguistiche. In sintesi, SymbolicToM rappresenta un passo avanti per migliorare la comprensione delle menti altrui nei modelli linguistici, aprendo nuove possibilità per applicazioni in ambito narrativo e sociale.</sample>
    <sample id="180">Il nome della relatrice è Myra.</sample>
    <sample id="181">**Abstract**  
Siyu Yuan da Fudan University presenta il lavoro "Distilling Script Knowledge from Large Language Models for Constrained Language Planning", che affronta il problema della pianificazione linguistica vincolata, in cui gli obiettivi di pianificazione sono soggetti a diversi vincoli specifici. Sebbene i modelli linguistici di grandi dimensioni siano efficaci nella decomposizione di obiettivi astratti, come "fare una torta", la loro capacità di pianificare obiettivi specifici, come "fare una torta al cioccolato", è scarsa. L'articolo valuta questa capacità e identifica carenze nella fedeltà ai vincoli. Per migliorare i risultati, viene proposto un metodo "over-generate-then-filter", che genera molteplici script e li filtra in base alla fedeltà ai vincoli e alla similarità semantica. Inoltre, viene creato il dataset CoScript, composto da 55.000 obiettivi specifici con script associati, validati da lavoratori crowdsourced. Il dataset mostra alta pluralità e permette di addestrare modelli più piccoli e specializzati, come T5, che superano molti modelli di grandi dimensioni. Questo lavoro introduce il problema della pianificazione linguistica vincolata, sviluppa un metodo per migliorare la generazione di script e presenta CoScript come risorsa utile per futuri studi nel campo della pianificazione linguistica.</sample>
    <sample id="182">Nel contesto dell'articolo, il tropicalismo indica un tratto stereotipato associato alle donne di colore, in particolare alle donne latine, descritte con parole come "vibrante" e "curvacea". Questo riflesse un'immagine stereotipata e riduttiva che collega queste donne a un'idea esotica e tropicale, contribuendo a narrativi essenzializzanti e potenzialmente dannosi.</sample>
    <sample id="183">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target utilizzando promemoria ispirati a uno studio in cui venivano dati ai partecipanti umani promemoria simili a quelli utilizzati per generare le persone nei modelli linguistici. Questo ha permesso di confrontare le rappresentazioni generate dai modelli con quelle scritte dagli umani, evidenziando stereotipi razziali e altre caratteristiche.</sample>
    <sample id="184">Nel lavoro, è stato utilizzato il Pointwise CXMI (Contextual Mutual Information) per misurare l'utilizzo del contesto a livello di frase o parola, identificando quelle che richiedono contesto per una corretta traduzione.</sample>
    <sample id="185">La differenza principale tra DrBERT e ChuBERT risiede nei dati utilizzati per l'addestramento. DrBERT è stato addestrato su dati web medicali estratti (NACHOS), mentre ChuBERT è stato addestrato su dati clinici anonimizzati provenienti da un data warehouse ospedaliero (Nantes University Hospital). Inoltre, DrBERT è il primo modello open source specializzato in ambito biomedico in francese, mentre ChuBERT è un modello clinico basato su note cliniche.</sample>
    <sample id="187">L'articolo è presentato da due autori: Ying e Zhiyang.</sample>
    <sample id="188">Il trasferimento iterativo dell'apprendimento è un processo in cui un modello viene aggiornato progressivamente, addestrandolo su nuovi dati raccolti in ogni round di apprendimento attivo, anziché accumulare tutti i dati precedentemente raccolti. Questo approccio permette di migliorare gradualmente le prestazioni del modello utilizzando i dati più recenti.</sample>
    <sample id="189">L'obiettivo del set di dati AltEntities Corpus è comprendere e analizzare come gli utenti utilizzano riferimenti indiretti per selezionare un'entità (come canzoni, libri o ricette) in contesti conversazionali, fornendo un dataset per valutare la capacità dei modelli linguistici di comprendere tali riferimenti.</sample>
    <sample id="190">Un utente malintenzionato può estrarre i parametri del modello attraverso un EaaS (Embedding as a Service) imparando dagli embedding restituiti dal servizio e utilizzando tecniche di estrazione del modello, fornendo così un servizio simile a quello originale.</sample>
    <sample id="191">L'articolo è un lavoro congiunto di Sara Papi, Matteo Negri e Marco Turchi, quindi sono coinvolti 3 autori.</sample>
    <sample id="192">**Abstract:**  
In this presentation, Yang Luo introduces CAME, a Confidence-guided Adaptive Memory Efficient optimizer designed to address the memory and performance trade-off in training large language models. Traditional adaptive optimizers like Adam require significant memory to store first and second moment estimates, while memory-efficient alternatives like Adafactor suffer from performance degradation. CAME aims to combine the fast convergence of adaptive methods with the low memory footprint of efficient ones. Inspired by Non-negative Matrix Factorization (NMF) and the limitations of Adafactor, CAME introduces a novel approach to reduce the instability caused by erroneous updates in memory-efficient optimizers. It leverages the residual between predicted and actual updates to guide adaptive confidence-based optimization, using this residual as a denominator to stabilize the update direction. Experimental results on models like BERT, GPT-2, and T5 demonstrate that CAME outperforms both Adam and Adafactor in terms of validation accuracy and memory efficiency, particularly for large batch sizes. The optimizer significantly reduces memory usage while maintaining or improving training performance, making it a promising advancement for large-scale language model training. The proposed method not only enhances the effectiveness of memory-efficient optimization but also extends its applicability to large-batch training scenarios.</sample>
    <sample id="193">Il testo non specifica il numero di annotatori impiegati per creare il set di dati iniziale.</sample>
    <sample id="194">Gli autori dell'articolo sono affiliati all'Università di Washington e all'Allen Institute for AI. I collaboratori menzionati sono Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap.</sample>
    <sample id="195">**Abstract:**  
L'articolo presenta RoHT, un framework innovativo per il Reasoning over Hierarchical Question Decomposition Tree (RoHT), progettato per migliorare l'efficacia dell'Explainable Question Answering (XQA). L'obiettivo è rispondere a domande complesse fornendo spiegazioni basate su fonti eterogenee, come basi di conoscenza strutturate e corpora testuali. I metodi esistenti presentano limiti: i metodi neuro-simbolici si basano su rappresentazioni formali che richiedono basi di conoscenza complete, mentre i metodi basati sulla decomposizione utilizzano solo corpora testuali, limitandone l'efficacia. RoHT affronta queste sfide decomponendo le domande in un albero gerarchico di sottodomande (HQDT), permettendo di selezionare fonti di conoscenza appropriate a ogni livello. Il framework opera in due fasi: nella prima, viene costruito l'HQDT tramite decomposizione e generazione di domande intermedie; nella seconda, si esegue un ragionamento probabilistico per integrare informazioni da KB e testo, valutando la certezza di ogni nodo. I risultati su dataset complessi come KQA Pro e Musique mostrano che RoHT supera i metodi esistenti, soprattutto quando combina conoscenze da KB e testo, dimostrando l'efficacia della decomposizione esplicita e dell'integrazione eterogenea.</sample>
    <sample id="196">L'esempio in cui il governatore è a sinistra è: "I saw Bart and Lisa".</sample>
    <sample id="197">I modelli all'avanguardia nei sistemi di dialogo menzionati nel testo sono quattro modelli di chat state-of-the-art, non specificati per nome, che sono stati valutati utilizzando l'ABC-Eval.</sample>
    <sample id="198">La valutazione dell'accettabilità dei modelli nell'intera finestra di contesto è necessaria perché i modelli linguistici moderni hanno contesti di input sempre più lunghi, e quindi è cruciale comprendere come le loro giudizi sull'accettabilità siano influenzati da contesti estesi, non solo da frasi singole.</sample>
    <sample id="199">Sì, la formazione multilingue ha causato un calo delle prestazioni rispetto al modello monolingue inglese in sette dataset, un fenomeno noto come "Curse of Multilinguality".</sample>
    <sample id="200">No, gli annotatori non conoscono l'entità in anticipo, ma vengono forniti di alcuni dati di background per comprendere le entità.</sample>
    <sample id="201">Le metriche di MT utilizzate per la valutazione sono state le metriche neurali di MT di stato dell'arte e i risultati della valutazione umana basata su esperti (MQM framework).</sample>
    <sample id="202">No, il regresso nella generalizzazione non influisce su specifici tipi di NER, ma è causato principalmente dal "temporal drift", ovvero il divario temporale tra i dati di addestramento e quelli di test. Questo fenomeno colpisce in modo generale i modelli, non specifici tipi di entità nominate.</sample>
    <sample id="203">La posizionalità nella NLP è importante perché i dati e i modelli possono riflettere le prospettive di gruppi specifici, portando a prestazioni sistematicamente diverse tra diverse popolazioni. Questo può portare a bias nei risultati e a una mancata rappresentazione di certi gruppi, rendendo cruciale comprendere e mitigare tali bias per garantire equità e inclusione.</sample>
    <sample id="204">Gli LLM multilingue come BLOOM non sono stati affinati mediante adattatori o con una messa a punto integrale per i compiti di semantic parsing cross-linguistico, poiché il loro rendimento su tali compiti è risultato insufficiente.</sample>
    <sample id="205">**Abstract**  
This study investigates the propagation of political biases in language models, tracing their origins from pretraining data through model training to downstream NLP applications. Language models are trained on large-scale web-crawled data, which includes a diverse range of political news sources, such as the New York Times and The Guardian. While this diversity can support democratic values, it also introduces socially biased perspectives that may lead to fairness issues. The research evaluates the political leanings of various language models using political questionnaires and demonstrates that models like GPT-4 exhibit distinct ideological positions, with GPT models generally leaning more liberal than BART variants. Further pretraining on partisan corpora shows that models’ ideological positions shift accordingly, reflecting the influence of training data. The study also finds that models trained on data post-2016 exhibit more polarized political leanings, mirroring societal trends. In downstream tasks like hate speech and fake news detection, politically biased models display uneven performance: left-leaning models are better at detecting hate speech against minorities but worse against dominant groups, and vice versa for right-leaning models. These findings highlight significant fairness concerns, as biased models deployed in real-world applications could marginalize certain groups or allow harmful content to spread. The paper underscores the complex dilemma between bias propagation and censorship, emphasizing the need for careful handling of political biases in language model training.</sample>
    <sample id="206">Per il trasferimento dell'apprendimento, i ricercatori fanno ricorso a due compiti specifici: il classificatore di posizione (stance) di dissonanza indipendente dal tema (chiamato "debate") e la classificazione binaria delle relazioni di espansione e confronto del PDTB (chiamata "CE"). In particolare, il modello migliore utilizzato per avviare il processo di apprendimento attivo è stato ottenuto addestrando prima sul compito CE e successivamente sul compito debate.</sample>
    <sample id="207">I recenti set di test utilizzati per valutare le capacità di PaLM sono i set di test più aggiornati, selezionati per evitare sovrapposizioni con i dati di addestramento del modello. Inoltre, sono stati utilizzati i set di test standard del WMT (Workshop on Machine Translation) per confrontare le prestazioni con i sistemi di traduzione automatica di alta qualità.</sample>
    <sample id="208">Gli autori hanno proposto tre suggerimenti alla fine.</sample>
    <sample id="209">Il metodo proposto migliora significativamente la capacità di pianificazione linguistica vincolata, aumentando sia la completezza semantica che la fedeltà ai vincoli rispetto al metodo di riferimento. Inoltre, permette di generare un dataset di alta qualità (CoScript) che consente a modelli più piccoli e specializzati di superare i modelli linguistici di grandi dimensioni quando addestrati correttamente.</sample>
    <sample id="210">Il nome del relatore è Shuheng.</sample>
    <sample id="211">Sì, i risultati e il set di dati nell'articolo possono essere utilizzati come parametri di riferimento per futuri lavori sull'automatizzazione della semplificazione del testo.</sample>
    <sample id="212">Nell'articolo, viene utilizzato un solo modello più piccolo, specificamente T5, addestrato sul dataset CoScript.</sample>
    <sample id="213">Il modello utilizzato come modello di base per analizzare l'ottimizzazione delle istruzioni multimodali è OFA.</sample>
    <sample id="215">**Abstract:**  
In this talk, Adam Przepiórkowski explores the dependency structure of coordination, comparing asymmetric and symmetric approaches. Asymmetric models, such as those in Universal Dependencies and Meaning-Text Theory, designate the first conjunct as the head of the coordinate structure, while the Prague approach identifies the conjunction itself as the head. In contrast, symmetric models, like those in Hudson’s Word Grammar, treat all conjuncts as heads. The paper presents a novel argument in favor of symmetric coordination structures and against asymmetric ones, based on the principle of dependency length minimization. This principle suggests that shorter syntactic dependencies are preferred. Przepiórkowski analyzes data from the Penn Treebank, showing that left conjuncts tend to be shorter when the governor is on the left or absent, but this tendency disappears when the governor is on the right. This observation challenges asymmetric models, which assume a single head, and supports symmetric models, where all conjuncts are equally head-like. The findings are further supported by statistical analysis of coordination structures in terms of length differences between conjuncts, measured in words, syllables, and characters. The results suggest that dependency length minimization provides a strong argument for symmetric coordination structures, offering new insights into the syntactic and cognitive principles underlying coordination.</sample>
    <sample id="217">**Abstract:**  
In questo lavoro, presentiamo "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation", in cui affrontiamo il problema della generazione di dialoghi controllabili con più attributi, un aspetto spesso trascurato dagli approcci esistenti che si concentrano su attributi singoli. Introduciamo DCG (Disentangled Controllable Generation), un modello basato su DialoGPT che utilizza un modulo di prompt compositivo per guidare la generazione di risposte dialogiche in base a diversi attributi. Proponiamo due tipi di prompt: uno orientato agli attributi e uno orientato al compito, che vengono concatenati per creare un embedding completo. Per migliorare la capacità di generalizzazione compositiva, introduciamo combinazioni pseudo-attrattive e una perdita di disentanglement. Inoltre, sviluppiamo MAE (Multi-Attribute Evaluation), un framework di valutazione unificato e senza riferimenti, che non richiede dati annotati aggiuntivi. I nostri esperimenti su benchmark come DailyDialog-CG dimostrano che DCG supera i metodi baseline in termini di controllabilità e qualità testuale, anche per combinazioni di attributi non viste. I risultati mostrano che i prompt orientati agli attributi migliorano la capacità di controllo, mentre il disentanglement migliora la generalizzazione compositiva. La valutazione con coefficienti di correlazione conferma l'efficacia di MAE rispetto a metriche classiche, specialmente per attributi continui. Questo lavoro apre la strada a un approccio più flessibile e generalizzabile per la generazione di dialoghi controllabili multi-attributo.</sample>
    <sample id="218">Gli autori dell'articolo sono affiliati a Google Translate.</sample>
    <sample id="219">**Abstract:**  
In questo lavoro, presentiamo un pipeline multistadio basato su confronto e contrapposizione per estrarre segnali finanziari da report annuali (Form 10-K), richiesti dalla SEC. L'analisi di tali documenti richiede un notevole sforzo umano a causa della loro similarità testuale tra anni consecutivi e della dipendenza temporale del contenuto. Per affrontare questa sfida, proponiamo un compito di "highlighting" che identifica le parole rilevanti nel confronto tra un report (target) e quello dell'anno precedente (reference). Il pipeline include tre fasi principali: segmentazione del documento, riconoscimento delle relazioni tra testi e sintonizzazione del modello in due stadi (out-of-domain e in-domain). Utilizziamo il dataset eSNLI per la sintonizzazione out-of-domain e un dataset interno (FINAL) per la valutazione. Il modello è addestrato a riconoscere tre tipi di relazioni tra coppie di testi: simili sintatticamente e semantica, rivisitate (simili in forma ma diverse nel significato) e non corrispondenti (nuove informazioni). I risultati mostrano che il modello proposto ottiene prestazioni superiori rispetto ad altre baselines su entrambi i dataset, dimostrando una buona capacità di generalizzazione. Questo lavoro apre la strada a future applicazioni nell'estratto automatico di informazioni da report finanziari, con potenziali miglioramenti nell'efficacia e nell'integrazione di ulteriori tecniche di recupero dell'informazione.</sample>
    <sample id="220">Gli autori dell'articolo sono affiliati all'Università di Stony Brook.</sample>
    <sample id="221">L'articolo non specifica le coppie linguistiche esattamente analizzate, ma menziona un esempio di traduzione dal tedesco all'inglese. Per informazioni dettagliate sulle coppie linguistiche, si consiglia di consultare il full presentation del paper.</sample>
    <sample id="222">**Abstract:**  
This work addresses the challenges of domain adaptation in open-domain question answering (QA), focusing on improving model performance when moving from a general-purpose domain (e.g., Wikipedia) to specialized domains like biomedical or news. The study investigates how to adapt both retriever and reader models to new domains through data interventions. It introduces two main approaches: few-shot and zero-shot methods. In few-shot adaptation, a small number of target-domain examples are used to prompt large language models to generate additional training data, which is then converted into cloze-style questions for model fine-tuning. This approach leads to significant performance improvements, with retriever and reader models showing average gains of 8% and 11%, respectively. For zero-shot adaptation, the study manipulates question, answer, and context distributions to better align with the target domain, finding that uniform answer distributions and unsupervised retrievers like BM25 perform best. The research also identifies the nature of dataset shifts (concept, covariate, or full shift) using compatibility measures based on likelihood scores from source models. By mapping datasets onto a 2D compatibility grid, the authors determine the type of shift and recommend suitable interventions. Results show that few-shot methods are broadly effective, while zero-shot approaches also benefit certain shifts. Overall, the work provides insights into how domain adaptation can be systematically improved through targeted data interventions, achieving up to a 24% improvement in reader performance.</sample>
    <sample id="223">Il nome del relatore è Shangbin.</sample>
    <sample id="224">I modelli studiati durante gli esperimenti sono long-mBART e base mBART.</sample>
    <sample id="225">Di 62 attività totali, 53 vengono utilizzate per l'addestramento e 9 vengono riservate per il test.</sample>
    <sample id="226">Due autori sono coinvolti nell'articolo: Regina Stodden e Omar.</sample>
    <sample id="227">L'abstract presenta un'analisi delle limitazioni attuali dei modelli linguistici nell'ambito della comprensione linguistica fondata (grounded language understanding), un'area che richiede la capacità di mappare espressioni naturali in rappresentazioni eseguibili all'interno di un ambiente specifico, come query SQL o sequenze di azioni per robot. Il problema principale risiede nella mancanza di grounding durante la pre-training, che rende complesso il passaggio da input testuali a rappresentazioni eseguibili. L'approccio tradizionale basato sulla generazione di piani o programmi da parte dei modelli linguistici presenta limiti in termini di validità e grammaticalità. Per affrontare questa sfida, l'articolo propone un nuovo framework, chiamato Pangu, che separa il ruolo del modello linguistico da quello dell'agente simbolico. Il modello linguistico si concentra sulla discriminazione e valutazione dei piani proposti, anziché sulla generazione. Questo approccio migliora la robustezza e l'efficienza del modello, specialmente in scenari non-i.i.d. I risultati sperimentali, condotti su diversi modelli linguistici e setting (fine-tuning e in-context learning), mostrano che Pangu supera consistentemente le baselines in termini di accuratezza e efficienza campionaria. L'articolo conclude sostenendo che, per la comprensione linguistica fondata, la discriminazione potrebbe essere una strategia più efficace rispetto alla generazione.</sample>
    <sample id="228">Gli autori hanno effettuato i test sui seguenti set di dati: AG News, MIND, SST2 e Enron Spam.</sample>
    <sample id="229">**Abstract**  
In questo lavoro, Gabriella Skitalinskaya e Henning Wachsmuth presentano un'analisi sull'identificazione di affermazioni argomentative che possono essere migliorate, con l'obiettivo di supportare gli scrittori nella revisione dei loro testi. L'argomentazione efficace richiede una formulazione ottimale delle affermazioni, che influisce direttamente sull'impatto del messaggio sull'audience. Per affrontare questa sfida, l'articolo introduce due compiti principali: il rilevamento di affermazioni non ottimali e la proposta di miglioramenti specifici. L'approccio si basa su dati derivati da revisioni umane, ottenuti da piattaforme di dibattito online come Kialo, dove le versioni finali delle affermazioni sono considerate ottimali. L'analisi esplora quattro principali sfide: la rappresentatività e attendibilità dei dati, la complessità e l'architettura dei modelli, la dipendenza dal contesto e i bias tematici e degli utenti. I risultati mostrano che i dati basati sulle revisioni possono essere utilizzati efficacemente per entrambi i compiti, con un'attenzione particolare al ruolo della distanza tra versioni di affermazioni e all'importanza del contesto nel valutare la qualità argomentativa. L'articolo conclude con una valutazione sistematica delle strategie adottate, offrendo una guida per futuri sviluppi nell'analisi e supporto della scrittura argomentativa.</sample>
    <sample id="231">NACHOS è un dataset di dati medici estratti dal web, utilizzato per addestrare il modello DrBERT, il primo modello pre-addestrato in francese per domini biomedici e clinici.</sample>
    <sample id="232">Il nome del relatore è David Vilar.</sample>
    <sample id="233">**Abstract:**  
Sara Papi, in collaborazione con Matteo Negri e Marco Turchi, presenta il lavoro "Attention as a Guide for Simultaneous Speech Translation", che propone una soluzione innovativa per la traduzione simultanea (SimulST), il processo di traduzione in tempo reale di discorsi orali in un'altra lingua. I modelli esistenti per la SimulST richiedono architetture specifiche, complessi procedimenti di addestramento e diversi modelli per gestire diverse regimi di latenza, aumentando il costo computazionale e la complessità. L'approccio proposto, EDAtt (Encoder-Decoder Attention), sfrutta il meccanismo di attenzione cross-attention già presente nei modelli offline di traduzione automatica, senza doverli rieducare o adattare specificamente per la SimulST. La strategia si basa sull'analisi dei pesi di attenzione per decidere quando emettere una traduzione parziale: una parola viene emessa solo quando l'attenzione non è concentrata sui frame vocali più recenti, indicando che l'informazione ricevuta è sufficientemente stabile. I risultati mostrano che EDAtt supera le strategie tradizionali come Wait-k e Local Agreement, nonché architetture specifiche per la SimulST, offrendo un miglior equilibrio tra qualità della traduzione, latenza e tempo computazionale. Il codice, i modelli e i risultati sono stati resi open source per favorire la riproducibilità.</sample>
    <sample id="234">La strategia del prompting influisce in modo significativo sui risultati. Sperimentazioni hanno mostrato che una differenza di prompting può portare a variazioni di oltre un punto BLEURT, e in alcuni casi fino a 40 punti. Inoltre, la qualità degli esempi utilizzati nel prompting è più importante della somiglianza con la frase originale.</sample>
    <sample id="235">Gli autori dell'articolo sono affiliati alle seguenti istituzioni: Kayo Yin, Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig. Tuttavia, nel testo non vengono specificate le affiliazioni istituzionali precise degli autori.</sample>
    <sample id="236">Le 5 istruzioni sono scritte dagli esperti per ogni compito del dataset MultiInstruct, ma il testo non specifica il contenuto esatto di ciascuna istruzione.</sample>
    <sample id="237">Gli autori propongono il "KITMUS Test", un insieme di test diagnostici progettato per valutare la capacità dei modelli di integrare conoscenza proveniente da diverse fonti, come la conoscenza pre-addestrata e quella fornita durante l'inferenza.</sample>
    <sample id="238">In questo video, Yebowen Hu presenta MeetingBank, un nuovo benchmark dataset creato per supportare lo sviluppo di tecnologie di sintesi per riunioni. Il dataset include 1.366 riunioni del Consiglio Cittadino, con trascrizioni audio, sintesi di riferimento e ulteriori risorse. La raccolta dati ha affrontato due sfide principali: la creazione di sintesi di alta qualità e l'accesso a fonti attendibili. MeetingBank è strutturato in base a ID unici per ogni riunione, permettendo l'allineamento tra trascrizioni e sintesi. L'analisi statistica mostra che le sintesi dei consigli comunali presentano un livello di astrazione medio, con punteggi di copertura tra 0,7 e 0,9. Per l'analisi dei modelli, sono stati testati sistemi extractive e abstrattivi, tra cui BART-Large, DialogLM e GPT-3. Sebbene GPT-3 abbia ottenuto buoni risultati in termini di fluidità e coerenza, ha mostrato limitazioni in informatività e fatti. L'assessment umano ha rilevato che i sistemi extractive come Extr-Oracle e DialogLM si sono distinti. L'obiettivo principale di MeetingBank è fornire un utile strumento per la ricerca, offrendo anche spunti sull'approccio decisionale dei consigli comunali. Il dataset è disponibile per il download e l'uso da parte della comunità accademica.</sample>
    <sample id="239">Ciao a tutti, mi chiamo David Vilar e oggi darò un breve riassunto del paper "Prompting PaLM for Translation: Assessing Strategies and Performance." Questo lavoro è stato realizzato in collaborazione con i miei colleghi di Google Translate. PaLM è un modello linguistico di grandi dimensioni con 540 miliardi di parametri, presentato l'anno scorso nel 2022. È stato addestrato su un grande insieme di testi, che comprende 780 miliardi di token. All'epoca della pubblicazione, ha raggiunto i risultati di stato dell'arte in centinaia di compiti NLP. In questo lavoro, presentiamo la prima analisi sistematica dell'uso di prompt per i modelli linguistici di grandi dimensioni nell'ambito della traduzione automatica. Abbiamo valutato la capacità di traduzione di questi modelli utilizzando le migliori pratiche della comunità di traduzione automatica. Questo include l'uso dei set di test più recenti per evitare sovrapposizioni tra i dati di test e quelli di addestramento del modello linguistico. Abbiamo confrontato i risultati con i sistemi di stato dell'arte, in particolare con i risultati del WMT. Abbiamo utilizzato metriche di traduzione automatica di stato dell'arte e, inoltre, abbiamo mostrato i risultati di valutazioni umane basate su esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione degli prompt. Il prompt ha un grande impatto sulle prestazioni dei modelli linguistici di grandi dimensioni per la traduzione, come possiamo vedere in un semplice esperimento in cui abbiamo utilizzato un prompt a un esempio e fornito due diversi prompt per ciascuna frase. Per la maggior parte delle frasi, su 1.000, la differenza osservata è superiore a un punto BLEURT. E in casi estremi, può arrivare fino a 40 punti BLEURT. È quindi importante selezionare una buona strategia di prompt. Nei nostri esperimenti, abbiamo adottato una strategia di prompt a cinque esempi, in cui abbiamo semplicemente segnato ciascuna frase fornita al sistema con la lingua in cui è scritta. In questo esempio, dove effettuiamo la traduzione dal tedesco all'inglese, le frasi in tedesco, le frasi sorgente, sono contrassegnate con due punti e un "German" e le traduzioni in inglese con due punti e un "English". Abbiamo visto che la forma effettiva del prompt non ha un grande impatto nel caso di prompt brevi. È cruciale per i prompt a zero e a un esempio. Quando, come nel nostro caso, passiamo a un prompt a cinque esempi, quasi non c'è differenza nella forma effettiva del prompt. È invece il contenuto degli esempi che ha il maggiore peso. In sintesi, i nostri risultati sperimentali indicano che la qualità degli esempi è più importante della somiglianza con la frase sorgente. È quindi importante selezionare esempi provenienti da traduzioni di alta qualità. In particolare, confrontiamo i prompt selezionati dai dati di addestramento per le valutazioni WMT sui dati di sviluppo. I dati di sviluppo sono molto più curati e di qualità superiore rispetto ai dati di addestramento, che sono più rumorosi. I loro risultati sono quindi migliori quando si utilizzano i dati di sviluppo. Tuttavia, i sistemi specializzati di stato dell'arte mantengono un vantaggio sostanziale rispetto alle traduzioni di PaLM. Tuttavia, PaLM si avvicina abbastanza a un sistema commerciale. Nel nostro caso, abbiamo scelto di valutare con Google Translate. Le ispezioni ottenute dalla valutazione umana che abbiamo effettuato utilizzando il framework MQM hanno rivelato che la fluidità di PaLM è paragonabile a quella dei sistemi di stato dell'arte, ma la differenza principale deriva dall'accuratezza. In particolare, gli errori più comuni sono gli errori di omissione. Sembrerebbe quindi che PaLM scelga di produrre una traduzione che suona meglio, talvolta omettendo parti della frase sorgente che potrebbero risultare difficili da tradurre. Tuttavia, la categoria "Stile/Incongruo" per PaLM è inferiore rispetto a quella dei sistemi di stato dell'arte, un ulteriore segnale che PaLM fornisce un output molto fluido, ma comunque con alcuni problemi di accuratezza. Ecco, questa è una breve panoramica. Per ulteriori dettagli, vi invitiamo a partecipare alla presentazione completa del paper. Grazie molto.</sample>
    <sample id="240">Ciao, mi chiamo Dawei, sono un dottorando di ricerca all'Università di Saarland in Germania. In questo video, vorrei presentare il nostro lavoro recente intitolato "Weaker Than You Think: A Critical Look at Weakly Supervised Learning". Questo lavoro è stato realizzato in collaborazione con Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow. Vorrei iniziare con una breve introduzione alla supervisione debole e all'apprendimento supervisionato debole. Nella supervisione debole non si etichettano i dati in modo manuale. Invece, si etichettano i dati utilizzando fonti di etichettatura debole, come semplici regole euristiche, basi di conoscenza o crowdsourcing di bassa qualità, come illustrato nella figura a destra. Rispetto alle annotazioni umane, le annotazioni più deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni è errata. Se addestriamo direttamente le reti neurali su dati etichettati in modo debole, le reti neurali tendono a memorizzare il rumore delle etichette e non generalizzano. Nell'apprendimento supervisionato debole, vengono proposti algoritmi di addestramento per addestrare robustamente le reti neurali in presenza di tali rumori di etichetta, in modo che i modelli addestrati generalizzino comunque bene. Nei lavori recenti sull'apprendimento supervisionato debole (WSL), una pretesa comune è che si addestrino i modelli solo sui dati etichettati in modo debole e si raggiunga un alto rendimento sui set di test puliti. Dal punto di vista tecnico, questa affermazione non è sbagliata, ma c'è un problema, che è l'assunzione che esista un ulteriore set di validazione pulito disponibile per la selezione del modello. Non possiamo fermarci a questo problema, ma ciò implica che siano necessarie ulteriori annotazioni manuali nell'apprendimento supervisionato debole. Tuttavia, come un elefante nella stanza, questa necessità è spesso trascurata. Le domande precedentemente sollevate si basano su tre domande di ricerca. Prima di tutto, i dati di validazione puliti sono necessari per l'WSL o possiamo utilizzare un set di validazione rumoroso invece? Secondo, se i dati puliti sono necessari, o se i dati puliti sono obbligatori per il funzionamento dell'WSL, quanti campioni puliti abbiamo bisogno? Infine, dovremmo utilizzare solo i campioni puliti per la validazione, oppure esistono modi migliori per sfruttarli? Abbiamo risposto a queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti. Prima di tutto, abbiamo scoperto che, interessantemente, i metodi recenti di WSL richiedono effettivamente campioni di validazione puliti per funzionare correttamente. Altrimenti, c'è un notevole calo delle prestazioni. Come mostrato in questa figura, se non ci sono campioni di validazione puliti, i modelli addestrati non riescono a generalizzare al di là delle etichette originali deboli, il che significa che l'addestramento è inutile. Questo indica che i metodi di WSL richiedono effettivamente dati etichettati correttamente per funzionare correttamente e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato. Il nostro secondo risultato è che aumentare il numero di campioni di validazione puliti aiuta i metodi di WSL a raggiungere prestazioni migliori, come mostrato nella figura a sinistra. Tipicamente, si necessita solo di 20 campioni per classe per raggiungere un'alta performance. Ma non finisce qui, perché se comunque decidiamo di accedere a campioni puliti, addestrarli direttamente porterà a prestazioni ancora migliori. La figura a destra mostra la differenza di performance tra approcci di fine-tuning, che vengono direttamente applicati sui dati puliti, e approcci di WSL, che utilizzano i dati puliti solo per la validazione. Come possiamo vedere, se abbiamo 10 campioni per classe, il fine-tuning diretto inizia a superare gli approcci di WSL. Infine, l'aumento delle prestazioni dichiarato nei precedenti approcci di WSL può essere facilmente ottenuto permettendo il fine-tuning continuo sui campioni di validazione puliti. Come possiamo vedere dalle figure, il modello base, denominato FTw, inizialmente sottoperforma rispetto a metodi più complessi di WSL, come COSINE. Tuttavia, se permettiamo il fine-tuning continuo sui campioni puliti, FTw si comporta altrettanto bene rispetto ad altri metodi. Quindi, in pratica, non c'è motivo per scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio su disco. In sintesi, abbiamo dimostrato che gli approcci recenti di WSL richiedono campioni manualmente annotati per funzionare correttamente. Il loro guadagno di prestazioni e praticità sono fortemente sopravvalutati. Le nostre raccomandazioni concrete per il futuro lavoro sono le seguenti. Prima di tutto, segnalare i criteri di selezione del modello. Per esempio, segnalare se la selezione del modello è stata effettuata utilizzando campioni di validazione puliti. Secondo, gli approcci di WSL dovrebbero essere confrontati con baselines di few-shot learning, poiché entrambi operano su campioni puliti. Terzo, il fine-tuning continuo è un baseline semplice ma potente che dovrebbe essere considerato nei futuri lavori su WSL. Infine, abbiamo reso open-source il nostro codice. Lo potete trovare attraverso il codice QR in questa diapositiva. Vi preghiamo di controllarlo. Grazie e buon convegno.</sample>
    <sample id="241">**Abstract**  
Ethan presenta un lavoro congiunto con Yang Chen, Wei Xu e Alan Ritter, intitolato "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments". Lo studio si concentra sui limiti delle attuali metodologie di rilevamento automatico di disinformazione, in particolare nell'uso di dati retrospecitivi e la possibilità di "leak" di evidenze contrarie. Questi approcci non riescono a catturare la realtà dei social media, dove la disinformazione si diffonde rapidamente e richiede interventi tempestivi da parte di umani. Per affrontare questi problemi, i ricercatori propongono un framework end-to-end che integra feedback umani in diversi stadi del processo. Il sistema utilizza un modello T5 per l'estrazione di affermazioni potenzialmente false da tweet, seguito da un ranking basato sulla popolarità e una verifica umana. Un secondo componente, basato su BERT, valuta la posizione dell'autore verso trattamenti non approvati, segnalando i tweet violatori. L'efficacia del sistema è valutata in termini di rilevamento precoce di disinformazione (prima che venga smentita) e capacità di identificare violazioni delle policy di Twitter. Risultati mostrano un tasso di rilevamento del 65% per le violazioni, con 124.2 segnalazioni rilevate per ora di lavoro umano. L'approccio offre una visione realistica dell'interazione tra sistemi automatizzati e moderatori umani, promuovendo lo sviluppo di futuri sistemi di rilevamento della disinformazione con un feedback umano integrato.</sample>
    <sample id="242">I metodi di valutazione comuni per i sistemi di dialogo includono l'utilizzo di valutazioni umane, come chiedere a giudici umani di selezionare quale tra due conversazioni è migliore o di valutare le conversazioni su una scala Likert. Inoltre, vengono utilizzate valutazioni a livello di turno e a livello di dialogo con scale Likert, nonché confronti a coppie a livello di dialogo.</sample>
    <sample id="243">L'articolo è stato realizzato in collaborazione con un totale di 6 autori: Sebastian Santy, Ronan Le Bras, Katharina Reinecke, Maarten Sap, Jenny (l'autrice) e un altro collaboratore non specificato. Tuttavia, solo cinque nomi sono menzionati esplicitamente.</sample>
    <sample id="244">Nell'esempio con Servin e Kea, la conoscenza di base necessaria è che "i giudici decidono casi in tribunali".</sample>
    <sample id="245">**Abstract**  
In this work, we present a two-step pipeline for identifying high-agreement annotators on Amazon Mechanical Turk (MTurk) for summarization tasks. Our approach addresses the challenges of unreliable automatic metrics and unclear best practices for worker recruitment. The pipeline includes a "Qualification Task" and an "Endurance Task" to filter workers based on their annotation accuracy and workload capacity. In the first step, 200 participants are evaluated on their ability to assess six summary dimensions across three documents, resulting in 26 qualified workers (8 gold and 18 silver). The second step tests endurance with 10 HITs, retaining 12 workers (4 gold and 8 silver), who demonstrate higher inter-annotator agreement (IAA) than experts, with Krippendorff’s Alpha reaching 0.443. In a reference-based task, the pipeline workers achieved a Krippendorff’s Alpha of 0.534, outperforming a baseline method (MACE) and CloudResearch workers (0.513). A correctness analysis showed strong correlation between pipeline workers and real GPT models, though training correctness is not guaranteed. The pipeline reduces annotation costs and resource waste while achieving high-quality results comparable to CloudResearch. However, it is limited to English summarization and does not ensure full correctness training. Future work will explore broader applications and improvements in worker quality. This study provides a scalable, cost-effective method for recruiting high-agreement annotators on MTurk.</sample>
    <sample id="246">Sì, il codice è disponibile su GitHub.</sample>
    <sample id="247">**Abstract:**  
In questo lavoro, presentiamo FactKG, un nuovo dataset per la verifica dei fatti basata su ragionamenti su grafi di conoscenza (KG). A differenza dei dataset esistenti, come FEVER, TabFact o VitaminC, che utilizzano testi o tabelle come fonti di evidenza, FactKG sfrutta il grafo di conoscenza DBpedia, offrendo una nuova sfida per la verifica dei fatti. Il dataset include affermazioni in due stili – scritto e colloquiale – e due etichette: "SUPPORTED" e "REFUTED". Le affermazioni richiedono diversi tipi di ragionamento, tra cui one-hop, conjunction, existence, multi-hop e negation, rendendo FactKG un dataset versatile per testare modelli di ragionamento. Per generare affermazioni colloquiali, abbiamo utilizzato un modello di trasferimento di stile e template basati sulla presupposizione. I risultati mostrano che i modelli che sfruttano l'evidenza del grafo di conoscenza, come il GEAR, superano significativamente le baselines che utilizzano solo le affermazioni. Questo lavoro apre la strada a nuove applicazioni pratiche, come la verifica della coerenza tra dialoghi e grafi di conoscenza in sistemi di intelligenza artificiale. FactKG è disponibile per il download e può essere utilizzato per ulteriori ricerche nel campo della verifica dei fatti basata su KG.</sample>
    <sample id="248">No, gli annotatori per NLPositionality non sono bilanciati rispetto a ciascun gruppo demografico. Il lavoro si basa su un campione diversificato, ma non equamente distribuito, proveniente da 87 paesi e include informazioni demografiche come livello educativo, ma non è specificato che ogni gruppo demografico sia rappresentato in modo bilanciato.</sample>
    <sample id="249">Le frasi nel dominio accettabile sono state perturbate cercando di preservare la struttura rilevante ma aggiungendo rumore all'input. Tuttavia, queste perturbazioni non hanno modificato in modo significativo il giudizio di accettabilità (MPP) del modello, che ha mostrato sensibilità simile alle frasi perturbate.</sample>
    <sample id="250">Avere una valutazione dimensionale significa analizzare e valutare diversi aspetti specifici della qualità del dialogo (come rilevanza, coerenza, empatia, ecc.) in modo dettagliato, piuttosto che fornire una valutazione generale o globale. Questo permette di comprendere meglio i punti di forza e di debolezza di un modello di AI conversazionale.</sample>
    <sample id="251">L'autore, Jingwei Yi, è affiliato all'Università Cinese della Scienza e Tecnologia (University of Science and Technology of China).</sample>
    <sample id="252">**Abstract**  
In questo lavoro, presentiamo U-CREAT, un pipeline per il recupero di casi precedenti (Prior Case Retrieval) basato su un approccio non supervisionato che utilizza l'estrazione degli eventi. Il nostro contributo principale è la creazione del dataset IL-PCR, un nuovo benchmark per il compito di recupero di casi legali indiani, composto da 7.070 casi con un elevato numero medio di citazioni. Inoltre, proponiamo il pipeline U-CREAT, che estrae eventi (triplette soggetto-verbo-oggetto) dai documenti legali e li utilizza per confrontare query e candidati, migliorando l'efficienza e la generalizzazione tra sistemi legali diversi. Abbiamo testato diversi modelli, tra cui basati su conteggio, trasformatori e eventi, e i risultati mostrano che i modelli basati sugli eventi, in particolare il modello "Event Filtered Docs", superano significativamente i metodi di baseline come BM25. U-CREAT ha anche superato i metodi supervisionati recenti su dati COLIEE, raggiungendo lo stato dell'arte nel compito di recupero documentale legale. Questo lavoro apre nuove possibilità per l'applicazione di tecniche di elaborazione del linguaggio naturale nel dominio legale, migliorando l'accesso e l'uso di precedenti giuridici.</sample>
    <sample id="253">**Abstract:**  
Mario Ezra Aragón presenta il lavoro "DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media", un progetto collaborativo tra ricercatori messicani e spagnoli. Lo studio mira a rilevare segni di disturbi mentali analizzando i post sui social media, sfruttando l'abbondanza di dati disponibili online. Il modello proposto, DisorBERT, utilizza un approccio di adattamento a dominio doppio, combinando un modello linguistico di base (come BERT) con dati specifici di Reddit e un lessico relativo alla salute mentale. Questo permette al modello di adattarsi al linguaggio specifico dei social media e ai sintomi dei disturbi mentali. Per migliorare l'apprendimento, DisorBERT incorpora un meccanismo di mascheramento guidato, che aiuta il modello a concentrarsi su parole rilevanti durante l'addestramento. I risultati sull'insieme di dati eRisk mostrano un buon equilibrio tra precisione e recall, superando modelli esistenti come MentalBERT. L'analisi qualitativa rivela che DisorBERT genera risposte più orientate alla salute mentale, identificando parole associate a disturbi come la depressione. Inoltre, l'uso di strumenti di visualizzazione dimostra che il modello focalizza l'attenzione su termini chiave come "ansioso" e "medicazione". Il lavoro dimostra l'efficacia dell'adattamento a dominio doppio e del mascheramento guidato nel rilevare segnali di disturbi mentali nei testi sociali. Future linee di ricerca includono l'uso di risorse lessicali diverse e dati clinici.</sample>
    <sample id="254">**Abstract**  
In this paper, we present "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction," a novel framework aimed at improving the quality of pseudo labels in document-level relation extraction (DocRE) using distant supervision (DS) data. DS data, while useful for pretraining DocRE models, often contain significant noise, leading to false positives and missed relations. To address this issue, we introduce an uncertainty-guided label denoising approach that leverages uncertainty estimation to filter unreliable pseudo labels. Our method first trains a pre-denoising DocRE model using both DS and human-annotated data. Then, we apply Monte Carlo dropout to estimate model uncertainty, enabling the identification of uncertain predictions. To handle overlapping relations between entity pairs, we propose an instance-level uncertainty estimation method that assigns uncertainty scores to each positive pseudo label. Based on the observation that different relation classes have distinct uncertainty distributions, we introduce dynamic class uncertainty thresholds to filter out high-uncertainty pseudo labels. Furthermore, we design a multi-phase training strategy to iteratively refine the pseudo labels and enhance model performance. Experimental results on public datasets demonstrate that our framework outperforms existing baselines, achieving significant improvements in relation extraction accuracy. The key contributions include uncertainty-guided label denoising, instance-level uncertainty estimation for overlapping relations, dynamic class uncertainty thresholds, and an effective multi-phase training strategy. Overall, our approach effectively mitigates noise in DS data and enhances the robustness and accuracy of document-level relation extraction.</sample>
    <sample id="255">La forma del prompting si rivela importante nei casi di zero-shot e one-shot prompting. In questi scenari, la struttura e la formulazione del prompt hanno un impatto significativo sulle prestazioni del modello. Al contrario, quando si utilizza un approccio a 5-shot, la forma del prompt ha un'influenza quasi trascurabile, poiché gli esempi forniti giocano il ruolo principale.</sample>
    <sample id="257">Gli autori hanno valutato quattro modelli di dialogo all'avanguardia.</sample>
    <sample id="258">**Abstract:**  
In questo lavoro, viene proposto l'uso dei Large Language Models (LLM) come alternativa all'assessment umano nella valutazione della qualità del testo in compiti di Natural Language Processing. L'idea principale è di fornire agli LLM istruzioni specifiche e campioni di testo da valutare, in modo che possano restituire valutazioni simili a quelle degli umani. Nonostante l'approccio sembri intuitivo, al momento della presentazione del lavoro all'ACL non esistevano precedenti studi che esplorassero questa idea. L'obiettivo è trovare un metodo alternativo all'assessment umano, spesso instabile e difficile da riprodurre. Per verificare l'efficacia degli LLM, sono stati utilizzati modelli come T0, InstructGPT (Curie e Davinci) e ChatGPT per valutare storie generate da GPT-2 o scritte da umani, basandosi su quattro criteri: grammatica, coerenza, piacevolezza e rilevanza. I risultati mostrano che alcuni LLM, in particolare Davinci e ChatGPT, mostrano una preferenza chiara per i testi umani, simile a quella degli insegnanti di inglese, considerati esperti. Questo suggerisce che alcuni LLM possono essere utilizzati efficacemente come alternativa all'assessment umano. Il lavoro affronta inoltre diverse domande relative alla validità, alla sensibilità alle istruzioni e ai costi rispetto all'assessment umano, fornendo un'analisi completa nel paper.</sample>
    <sample id="259">**Abstract**  
In questo lavoro presentiamo XSemPLR, un benchmark unificato per il parsing semantico cross-linguistico, che supporta diversi linguaggi naturali e rappresentazioni semantiche. Il dataset include 9 dataset in diversi domini, 5 compiti di parsing semantico, 8 rappresentazioni semantiche e 22 linguaggi appartenenti a 15 famiglie linguistiche. Per valutare il benchmark, proponiamo sei impostazioni di addestramento ed esecuzione, tra cui Translate-Test, modelli monolingue, modelli multilingue e transfer zero-shot e few-shot cross-linguistico. I nostri esperimenti confrontano diversi tipi di modelli multilingue, tra cui Encoder-PTR e Encoder-Decoder, come XLM-R + PTR, mBERT + PTR, mBART e mT5. Risultati sperimentali mostrano che i modelli Encoder-Decoder ottengono le prestazioni migliori su tutti i dataset. Inoltre, l'addestramento multilingue migliora le prestazioni complessive, anche se porta a un calo delle performance in alcuni casi (Curse of Multilinguality). Il transfer few-shot riduce significativamente il gap di performance rispetto al transfer zero-shot. Inoltre, i modelli pre-addestrati su inglese migliorano le prestazioni in few-shot su linguaggi target, mentre modelli come Codex e BLOOM risultano insufficienti per compiti di parsing semantico cross-linguistico. XSemPLR fornisce un riferimento unificato per valutare e sviluppare modelli multilingue in ambito di parsing semantico.</sample>
    <sample id="260">La presentazione non specifica il numero di autori coinvolti nell'articolo.</sample>
    <sample id="261">Un buon pianificatore dovrebbe produrre script che siano ragionevoli e fedeli ai vincoli imposti sui goal specifici.</sample>
    <sample id="262">L'articolo non specifica il numero di autori coinvolti.</sample>
    <sample id="263">In questo lavoro, analizziamo i problemi di bias nei label durante l'apprendimento in contesto (in-context learning) nei modelli linguistici di grandi dimensioni. L'apprendimento in contesto è un paradigma popolare, ma risulta instabile a causa di scelte di progettazione come l'ordine e la selezione degli esempi. I bias nei label, in particolare, possono influenzare in modo significativo le previsioni del modello. Proponiamo una classificazione sistematica dei diversi tipi di bias, tra cui il *vanilla-label bias*, il *context-label bias* e un nuovo tipo, il *domain-label bias*, che riflette l'impatto del corpus del compito sulle predizioni del modello. Per mitigare questi bias, introduciamo un metodo di calibrazione innovativo, chiamato *domain-context calibration*, che utilizza testi privi di contenuto, ma specifici del dominio, per stimare e correggere i bias del modello. Sperimentiamo su diversi compiti di classificazione e modelli, dimostrando che la calibrazione proposta migliora significativamente le prestazioni, in particolare nei compiti con alto *domain-label bias*. I risultati mostrano che l'uso di parole casuali del dominio, anziché token predefiniti, porta a miglioramenti maggiori, confermando l'efficacia del nostro approccio. Questo lavoro fornisce una comprensione più approfondita dei bias nei label e una soluzione promettente per migliorare l'apprendimento in contesto nei grandi modelli linguistici.</sample>
    <sample id="264">**Abstract:**  
In this paper, we introduce TAVT (Transferable Audio-Visual Text Generation), a novel framework aimed at addressing the challenges of generating text from audio-visual inputs across different domains. Unlike uni-modal text generation, which benefits from large-scale pre-training, multimodal tasks face significant hurdles due to domain shifts in visual and audio modalities, as well as the high cost of data annotation. To overcome these challenges, TAVT proposes a modular architecture composed of an audio-visual meta-mapper network, an encoder-generator based on transformers, and a Dual Counterfactual Contrastive Learning (DCLL) mechanism. The meta-mapper aligns visual concepts across domains by mapping them into a unified audio semantic space, leveraging audio clusters derived from the Flickr dataset. The transformer-based encoder and generator incorporate an attention-based mechanism to evaluate the contribution of each modality to text generation. DCLL enhances visual-text alignment by generating fine-grained supervision signals from counterfactual examples, reducing reliance on randomly selected negative samples. The framework is trained using a meta-learning approach, enabling rapid adaptation to new domains with limited labeled data. Extensive experiments on MSVD and MSR-VTT benchmarks demonstrate that TAVT significantly outperforms state-of-the-art models in both cross-dataset and cross-domain settings, particularly excelling in low-resource domains. Our results highlight the effectiveness of TAVT in achieving robust and transferable audio-visual text generation.</sample>
    <sample id="265">Il nome della relatrice è Vasudha.</sample>
    <sample id="266">L'affiliazione degli autori non è menzionata nel testo fornito.</sample>
    <sample id="268">Gli errori più comuni di PaLM sono gli errori di omissione, ovvero l'omissione di parti della frase originale durante la traduzione.</sample>
    <sample id="269">Ciao, sono James Finch. E sono Sarah Finch. Oggi ti parleremo di ABC-Eval, un nuovo approccio multidimensionale per valutare gli assistenti AI per conversazioni. Questo lavoro è stato realizzato dal laboratorio Emory NLP, guidato dal professor Jinho Choi presso l'Emory University, in collaborazione con Amazon Alexa AI. Immagina che tu abbia appena sviluppato un modello di dialogo e vorresti capire quanto è performante rispetto agli standard attuali. La pratica comune è utilizzare valutazioni umane, ad esempio chiedendo ai giudici umani di selezionare quale tra due conversazioni è migliore o di valutare le conversazioni con una scala di Likert. Questi approcci funzionano bene per fornire valutazioni complete della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto, potresti voler valutare diverse dimensioni della qualità del chat per comprendere meglio i punti di forza e le debolezze del modello a un livello più dettagliato. Un approccio potrebbe essere chiedere semplicemente ai giudici umani di valutare diverse dimensioni della qualità del dialogo, ad esempio la rilevanza delle risposte del modello utilizzando metodi esistenti di confronto o scale Likert. Tuttavia, crediamo che esista una strategia più precisa e affidabile per la valutazione multidimensionale del dialogo. Il nostro approccio tenta di ridurre la soggettività della valutazione umana, annotando esplicitamente se ogni risposta del modello esprime determinati comportamenti, ad esempio rispondere con informazioni irrilevanti o contraddire se stesso. Chiamiamo questo approccio "annotare i comportamenti nel chat" o ABC-Eval in breve. Abbiamo sviluppato questo metodo per coprire in modo completo i comportamenti dei modelli di chat che sono stati suggeriti recentemente come influenti sulla qualità del chat. ABC-Eval è in grado di misurare le frequenze con cui i modelli di chat commettono diversi errori tematici. Ad esempio, ABC-Eval misura il numero di turni in cui un modello di chat ignora il partner o dice qualcosa di irrilevante, si contraddice o si contraddice con il partner, inventa fatti errati o viola la conoscenza comune, e quando il modello riesce o non riesce a mostrare empatia. Per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su 100 conversazioni uomo-bot per modello utilizzando ABC-Eval. Per confronto, abbiamo valutato queste conversazioni anche con tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti a coppie a livello di dialogo. Per ciascun metodo esistente, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questo è lo standard per valutare i modelli di chat su diverse dimensioni. Dall'analisi dei risultati di queste valutazioni, abbiamo scoperto che le etichette di comportamento di ABC-Eval sono in generale più affidabili rispetto alle etichette raccolte con metodi esistenti, come misurato dall'accordo tra annotatori su 100 conversazioni doppie annotate. Inoltre, le etichette di ABC-Eval sono più predictive della qualità complessiva della conversazione rispetto alle metriche prodotte da metodi esistenti, come mostrato da un semplice analisi di regressione lineare. Ad esempio, puoi vedere come misurare la proporzione di turni con contraddizioni con se stesso o con il partner spiega rispettivamente il 5% e il 10% della qualità della conversazione, mentre le punteggi medi di coerenza Likert spiegano solo il 4% o meno. Infine, abbiamo controllato se ciascuna metrica di valutazione cattura un aspetto unico della qualità del chat utilizzando una regressione lineare a passi. Puoi vedere come la combinazione di tutte le metriche di ABC-Eval spiega oltre il 25% della qualità della conversazione, e mentre rimuovi una metrica alla volta, la maggior parte di esse comporta una perdita significativa di informazioni sulla qualità. Dall'altro lato, la combinazione di tutte le metriche Likert a livello di turno spiega molto meno della qualità e poche di queste metriche portano informazioni uniche. Queste metriche ABC-Eval affidabili, informative e distinte ci permettono di valutare l'AI per conversazioni con una risoluzione maggiore rispetto a quanto i metodi precedenti siano riusciti a ottenere. Puoi vedere che nei risultati del nostro esperimento ci sono ancora diverse sfide da affrontare e sono state quantificate con precisione. Ad esempio, i bot che abbiamo testato violano la conoscenza comune in circa il 20% delle loro risposte. Producono informazioni irrilevanti in circa il 15% delle risposte, e si contraddicono con se stessi o con il partner circa il 10% delle volte. Con il rapido ritmo di progressi nel campo, molte di queste percentuali di errori potrebbero diminuire nei nuovi modelli rilasciati da quando abbiamo effettuato la nostra valutazione. Tuttavia, questo è motivo ancora più forte per perseguire metriche di valutazione affidabili e precise per confrontare i modelli. Speriamo che ABC-Eval possa essere utilizzato da altri nel settore come un passo significativo in questa direzione. E ci aspettiamo con interesse di vedere come l'AI per conversazioni continuerà a progredire nei prossimi mesi e anni. Grazie per aver guardato.</sample>
    <sample id="270">Gli autori dell'articolo appartengono al Emory NLP Lab, guidato dal Professor Jinho Choi all'Emory University, e collaborano con Amazon Alexa AI.</sample>
    <sample id="271">In questo articolo, CFT sta per "Fine-tuning" (affinamento), un approccio in cui si addestra ulteriormente un modello su dati puliti per migliorare le sue prestazioni. Specificamente, il modello FTw (che probabilmente sta per "Fine-tuning with weak labels") inizialmente underperforma rispetto ad approcci WSL più complessi, ma raggiunge prestazioni paragonabili quando si permette un ulteriore fine-tuning sui dati puliti.</sample>
    <sample id="272">L'articolo è un lavoro congiunto di 7 autori: John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy e Adina Williams, oltre all'autore principale Koustav Sinha. In totale, sono coinvolti 8 autori.</sample>
    <sample id="273">Ciao, mi chiamo Kayo Yin e presenterò il nostro lavoro intitolato "Quando la traduzione richiede contesto? Un'indagine multilingue basata sui dati". Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig. Molte traduzioni dipendono dal contesto. Ad esempio, come tradurrebbero "mole" in questa frase? Se la frase precedente fosse "Le cose potrebbero diventare pericolose se i ministri lo scoprissero", "mole" si riferirebbe a un informatore. Ma se la frase precedente fosse "Potrebbe essere qualcosa di grave, dottore?", "mole" si riferirebbe a una macchia di nascita. Quindi, in base al contesto, il significato della parola cambia e, di conseguenza, anche la sua traduzione. Tuttavia, valutare quanto bene i modelli riescono a tradurre casi come questo è abbastanza difficile. In primo luogo perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende impossibile per metriche a livello di corpus, come BLEU, catturare queste traduzioni. Alcune persone hanno proposto valutazioni mirate per le traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e set linguistici limitati, poiché di solito si basano su conoscenze di dominio e curatela umana.

In questo lavoro cerchiamo di rispondere a due domande. Prima, quando la traduzione richiede contesto? E seconda, quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipende dal contesto durante la traduzione. Nel lavoro precedente, abbiamo introdotto CXMI come misura dell'uso del contesto da parte dei modelli di traduzione automatica. Questo viene fatto misurando quanto informazione il contesto C fornisce sul target Y, dato il source X. Si può pensare a CXMI come all'informazione guadagnata dal fornire contesto al modello. In questo lavoro, estendiamo CXMI a Pointwise CXMI, che può misurare l'uso del contesto a livello di frase o a livello di parola. Si può pensare alle parole con alto P-CXMI come a quelle che richiedono contesto per la traduzione. Ora analizziamo le parole con alto P-CXMI per cercare schemi tra queste parole. Eseguiamo l'analisi sui testi di discorsi TED tradotti dall'inglese in 14 diversi linguaggi. Eseguiamo l'analisi a tre diversi livelli. In primo luogo, osserviamo i tag di parte del discorso con alto P-CXMI medio. Questo ci permette, ad esempio, di trovare i pronomi duali nell'arabo che hanno un P-CXMI relativamente alto. Questo può essere spiegato perché l'inglese non ha pronomi duali, quindi hai bisogno di contesto per determinare se un pronome è duale quando lo traduci in arabo. In modo simile, scopriamo che certi linguaggi richiedono contesto quando vogliamo scegliere la forma corretta del verbo. Poi osserviamo gli elementi del vocabolario con alto P-CXMI medio su tutte le loro diverse occorrenze. Questo ci aiuta a identificare casi come quelli qui, dove in cinese hai bisogno di contesto per tradurre i nomi propri per assicurarti di usare la stessa traduzione all'interno del documento. In modo simile, scopriamo che il contesto è importante per tradurre nel livello di formalità corretto. Infine, osserviamo diversi token individuali con alto P-CXMI. Questo ci permette di identificare fenomeni che non possono essere realmente catturati dalla parola stessa, ma che vengono piuttosto espressi nella struttura della frase, ad esempio la risoluzione degli ellissi.

Ora utilizziamo i nostri risultati dall'analisi per progettare un benchmark per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni discorsivi che abbiamo identificato, creiamo taggatori per identificare automaticamente le parole che appartengono al fenomeno. Chiamiamo il nostro tagger MuDA (Multilingual Discourse-Aware). Poi possiamo notare che diversi linguaggi hanno proporzioni diverse di questi fenomeni discorsivi. Utilizziamo quindi il tagger MuDA applicandolo su un corpus parallelo che vogliamo usare per l'valutazione e applichiamo le metriche di traduzione scelte sugli esempi dipendenti dal contesto che il tagger MuDA ha identificato. Infine, utilizziamo il nostro benchmark nonché altre metriche per valutare diversi modelli per la traduzione a livello di documento. In primo luogo, quando utilizziamo le metriche a livello di corpus: per BLEU, troviamo che i modelli non dipendenti dal contesto hanno la migliore prestazione. Ma se utilizziamo COMET, i modelli dipendenti dal contesto danno i migliori risultati. E se utilizziamo la misura f-word, i modelli con e senza contesto hanno prestazioni paragonabili. Questo dimostra nuovamente che è difficile determinare il miglior sistema di traduzione a livello di documento se utilizziamo solo le metriche a livello di corpus.

Ora utilizziamo il benchmark MuDA per valutare i modelli e troviamo che i modelli dipendenti dal contesto sono significativamente più precisi di quelli che non utilizzano contesto per certi fenomeni discorsivi come la formalità e la coesione lessicale. Tuttavia, questi modelli non sono molto migliori di quelli che non utilizzano contesto per altri fenomeni come le ellissi, i pronomi e la forma del verbo. Questo suggerisce dove dovremmo vedere ulteriori progressi per la traduzione a livello di documento. Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che DeepL è generalmente più preciso di Google Translate per la traduzione a livello di documento.

In sintesi, eseguiamo un'analisi basata sui dati su 14 coppie di linguaggi per identificare quando le traduzioni richiedono contesto e utilizziamo i nostri risultati per costruire un benchmark per la traduzione automatica a livello di documento che può aiutarci a identificare quali fenomeni discorsivi i modelli riescono a gestire bene o meno, e quali sistemi di traduzione sono bravi per la traduzione a livello di documento. Grazie molto per l'attenzione. Ci vediamo a Toronto.</sample>
    <sample id="274">Il nome del relatore è Yusen Zhang.</sample>
    <sample id="276">**Abstract:**  
In questo lavoro, presentiamo *IndicMT Eval*, un dataset progettato per la meta-evaluzione dei metric per la traduzione automatica in lingue indiane. Mentre molte metric sono state sviluppate e valutate per la traduzione in inglese, la traduzione in altre direzioni, come quelle delle lingue indiane, è poco studiata. Per colmare questa lacuna, abbiamo creato un dataset basato su cinque lingue indiane (Tamil, Malayalam, Hindi, Marathi e Gujarati), appartenenti a due famiglie linguistiche diverse. Utilizzando il dataset Flores, abbiamo generato 7.000 candidati di traduzione da sette sistemi di traduzione automatica, poi abbiamo raccolto annotazioni umane dettagliate su errori di precisione, fluidità e altre categorie, ispirandoci al framework MQM. I risultati mostrano che metric basate su sovrapposizione (come chrF) hanno alta correlazione con le valutazioni umane, ma si dimostrano inadeguate complessivamente. Le metric basate su embedding, come BERTscore e LabSE, mostrano correlazioni migliori, mentre COMET e le sue varianti si rivelano le più performanti. Abbiamo fine-tunato COMET sul nostro dataset, ottenendo un modello, IndicCOMET MQM, che supera le baselines su diverse lingue e mostra una maggiore robustezza in test esterni. Questo dataset e i risultati offrono un importante contributo per migliorare l'accuratezza della valutazione automatica in contesti multilingue.</sample>
    <sample id="277">Il nuovo metodo non ha un nome specifico menzionato nel testo.</sample>
    <sample id="278">L'autore descrive il metodo delle "parole contrassegnate" come un approccio per identificare le parole che distinguono i gruppi contrassegnati (marginalizzati) da quelli non contrassegnati (dominanti), basandosi sul concetto sociolinguistico di "contrassegnatura". Questo metodo utilizza il rapporto log-odds ponderato (Fightin' Words) per individuare le parole chiave associate a ciascun gruppo contrassegnato, rivelando stereotipi e narrazioni essenzialiste nascosti nei dati generati dai modelli linguistici.</sample>
    <sample id="279">L'autore, Shangbin, è un dottorando presso l'University of Washington.</sample>
    <sample id="280">**Abstract:**  
In this work, we present MultiEMO, an attention-based correlation-aware multimodal fusion framework for Emotion Recognition in Conversations (ERC). The goal of ERC is to predict the emotional label of each utterance in a dialogue, leveraging textual, audio, and visual modalities. Despite progress in the field, challenges remain, such as underutilization of multimodal complementarity, poor performance on minority emotions, and difficulty in distinguishing semantically similar emotions. To address these issues, MultiEMO introduces four key components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification. A novel visual feature extractor, VisExtNet, is proposed to focus on facial expressions while avoiding redundant scene information. For multimodal fusion, we design MultiAttn, a network based on bidirectional multi-head cross-attention layers that effectively integrates information across modalities. Additionally, a Sample-Weighted Focal Contrastive loss is introduced to enhance classification of minority and semantically similar emotions. Extensive experiments on MELD and IEMOCAP datasets demonstrate that MultiEMO achieves state-of-the-art performance, particularly in minority and ambiguous emotion classes. Visualizations further highlight its effectiveness in complex scenarios where modalities are asynchronous. Despite its strengths, MultiEMO has limitations, including challenges in distinguishing between speakers and irrelevant individuals in visual data and the need for large batch sizes for optimal performance. Overall, this work advances the field of multimodal emotion recognition by addressing key challenges through novel architectural and loss design choices.</sample>
    <sample id="281">**Abstract**  
Il lavoro presenta un'analisi data-driven multilingue per identificare quando la traduzione richiede contesto e valutare quanto i modelli di traduzione lo gestiscono. La traduzione di parole ambigue, come "mole", dipende spesso dal contesto, rendendo difficile valutare le prestazioni dei modelli con metriche tradizionali come BLEU, che non catturano bene i casi contestuali. Per affrontare questa sfida, gli autori introducono il Pointwise CXMI, un'estensione di CXMI, per misurare il grado in cui una parola o una frase dipende dal contesto durante la traduzione. L'analisi su 14 coppie di lingue, basata su trascrizioni di discorsi TED, rivela che alcune categorie grammaticali, vocaboli e fenomeni linguistici richiedono contesto per essere tradotti correttamente, ad esempio i pronomi duali in arabo o la coesione lessicale. Gli autori sviluppano un benchmark, il MuDA tagger, per identificare automaticamente i casi contestuali in corpus paralleli. I risultati mostrano che i modelli a contesto sono più accurati in fenomeni come formalità e coesione, ma non sempre superiori in casi come ellissi o verbi. Inoltre, DeepL supera Google Translate in traduzione a livello documentale. Questo lavoro fornisce un quadro per valutare i modelli di traduzione a livello documentale e individuare le aree in cui è necessario migliorare.</sample>
    <sample id="282">**Abstract**  
In this work, we present StoryTrans, a novel approach for non-parallel story-level author-style transfer in natural language generation. Unlike previous methods that focus on token- or sentence-level style transfer, our model operates at the discourse level, addressing the challenge of capturing and imitating complex author-specific linguistic patterns, such as narrative structures and discourse features. The primary challenges include preserving content while transferring style and handling the association between style and topic, which makes direct style transfer difficult. To address these, StoryTrans learns discourse representations from source texts and combines them with learnable style embeddings to generate text in the target style. A new training objective is introduced to disentangle style from content by aligning discourse representations across different styles in the latent space. The generation process is divided into two stages: first, transferring style while masking content keywords, and then incorporating these keywords to enhance content preservation. Extensive experiments on newly collected Chinese and English datasets demonstrate that StoryTrans outperforms strong baselines in terms of style control and content fidelity. Both automatic and manual evaluations confirm the effectiveness of our approach, while style visualization shows alignment with target styles in the feature space. StoryTrans successfully enriches storylines with relevant content and maintains the original semantics, making it a promising solution for author-style transfer at the story level.</sample>
    <sample id="283">La prima struttura di dipendenza simmetrica menzionata è quella di Hudson's Word Grammar, che include il nome della città "Lisa, Bart, and Maggie".</sample>
    <sample id="284">**Abstract:**  
In this paper, we introduce FSUIE, a novel approach for Universal Information Extraction (UIE) that addresses the limitations of traditional span-based models. These models often rely heavily on precise span boundaries, which can be ambiguous due to varying annotation practices. To overcome this, FSUIE introduces a *fuzzy span mechanism*, allowing the model to represent target boundaries as continuous probability distributions rather than fixed positions. This is achieved through a *fuzzy span loss* that combines Binary Cross-Entropy with the golden boundary and KL-divergence with a fuzzy boundary distribution. Additionally, we propose a *fuzzy span attention* mechanism, which dynamically adjusts the attention range and smoothly decays attention weights at the boundaries, enhancing the model's ability to focus on relevant semantic information. The architecture integrates these components without compromising the text encoding capabilities of the underlying transformer model. Experiments on three major IE tasks—named entity recognition, relationship extraction, and aspect sentiment triplet extraction—demonstrate that FSUIE achieves state-of-the-art results across multiple datasets, including ACE2004, ADE, and AST-V2. Ablation studies confirm the effectiveness of both the fuzzy span loss and attention mechanisms in improving convergence and generalization. Overall, FSUIE offers a more robust and flexible framework for universal information extraction.</sample>
    <sample id="285">**Abstract:**  
Mingqi Gao da Peking University presenta il lavoro "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework", che affronta il problema degli errori fatti in sintesi di dialoghi generati da modelli. Gli errori fatti possono essere corretti in due modi: integrando obiettivi di fatti durante l'addestramento o utilizzando modelli dedicati (FEC) che correggono le sintesi esistenti. Tuttavia, gli studi precedenti non hanno affrontato specificamente i dialoghi e hanno utilizzato metriche generali come FactCC e DAE, che presentano due limiti principali: forniscono punteggi generali poco precisi e non distinguono chiaramente tra correzioni effettive e generazione di nuove sintesi. Per migliorare l'accuratezza, l'articolo propone un framework di valutazione fine-grained basato su ERRANT, che include allineamento, classificazione e confronto degli errori. Gli errori vengono classificati in base a contenuto (parte del discorso, dipendenze) e forma (aggiunta, cancellazione, sostituzione). L'uso di correzioni manuali durante l'addestramento migliora le prestazioni dei FEC, mentre i modelli attuali faticano a correggere errori come aggiunte o errori di attributi. L'introduzione di dati annotati da umani e sintetici è una prospettiva promettente. L'articolo sottolinea l'urgenza di rivedere i metodi di valutazione per FEC.</sample>
    <sample id="286">Il nome della relatrice o del relatore è James Finch e Sarah Finch.</sample>
    <sample id="287">L'articolo è stato realizzato da 4 autori: Javad Hosseini, Filip Radlinski, Silvia Pareti e Annie Louis.</sample>
    <sample id="288">I set di dati che possono essere utilizzati per testare i fenomeni sintattici includono BLiMP, SyntaxGym e CrowS pairs.</sample>
    <sample id="290">Le abbreviazioni dei cinque metodi per la prima domanda di ricerca non sono specificate nel testo fornito. Il testo menziona metodi come "COSINE" e "FTw" (probabilmente "Fine-Tuning with weak labels"), ma non elenca cinque metodi con le loro abbreviazioni. Pertanto, non è possibile rispondere con le abbreviazioni dei cinque metodi.</sample>
    <sample id="291">Il modello viene valutato su 11 attività downstream biomedicalhe e cliniche in francese, tra cui riconoscimento di entità nominate, classificazione, tagging delle parti del discorso e risposte a domande.</sample>
    <sample id="294">CamemBERT viene inizialmente addestrato sui dati OSCAR, un corpus di testi in francese estratti dal web.</sample>
    <sample id="295">Il nome del relatore è Adam Przepiórkowski.</sample>
    <sample id="296">**Abstract:**  
Valerio Basile presenta un lavoro frutto della collaborazione tra l'Università di Torino e Amazon Alexa, focalizzato sull'analisi del riconoscimento dell'ironia nel linguaggio naturale. L'approccio tradizionale del machine learning supervisionato si basa sull'uso di dati annotati, presupponendo un'unica "verità" (ground truth). Tuttavia, l'ironia, fenomeno pragmatico e latente, presenta sfide particolari per i modelli NLP, poiché la sua interpretazione dipende dal contesto e dalla prospettiva dell'annotatore. Per affrontare questo problema, i ricercatori hanno creato il corpus EPIC (English Perspectivist Irony Corpus), costituito da circa 300 conversazioni brevi raccolte da Reddit e Twitter, in cinque varianti dell'inglese. I dati sono stati annotati da 74 annotatori tramite la piattaforma Prolific, con 5 annotazioni per ogni conversazione. I risultati mostrano differenze nell'accordo tra annotatori in base a caratteristiche come sesso, età e nazionalità. I modelli "perspective-aware", addestrati su sottocampioni dei dati, mostrano una maggiore confidenza nelle previsioni rispetto ai modelli basati su una "verità" unica. Inoltre, si osserva che annotatori di generazioni simili e provenienti da aree geografiche vicine (es. Regno Unito e Irlanda) tendono a mostrare maggiori divergenze nell'interpretazione dell'ironia. Questo studio sottolinea l'importanza della prospettiva umana nell'annotazione e nell'addestramento dei modelli NLP.</sample>
    <sample id="297">**Abstract:**  
Il lavoro "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models" analizza l'uso di dogwhistles — termini che trasmettono messaggi diversi a gruppi diversi — nel discorso politico e pubblico, con un focus particolare su quelle che possono essere interpretate come retoriche anti-ebraiche, razziste o transfobiche. Attraverso la creazione di una glossario di oltre 340 termini, la ricerca propone una tipologia basata su registro, tipo e persona del messaggio, permettendo di identificare il significato nascosto di tali espressioni. Un caso studio su discorsi politici storici negli Stati Uniti mostra una correlazione tra l'uso di dogwhistles razziali e la strategia del Sud del Partito Repubblicano, evidenziando come siano diventati uno strumento di influenza politica indiretta. L'articolo valuta anche la capacità dei modelli linguistici, come GPT-3, di riconoscere e interpretare i dogwhistles, dimostrando una performance variabile, soprattutto per quelli informali o transfobici. Infine, viene mostrato come i dogwhistles possano evadere i sistemi di moderazione del contenuto online, come il Prospective API, poiché frasi che contengono tali termini vengono percepiti come meno tossiche rispetto a quelle con epiteti espliciti. Questo studio sottolinea l'importanza di comprendere e rilevare i messaggi codificati per affrontare l'odio e la discriminazione nel linguaggio digitale.</sample>
    <sample id="298">I risultati che hanno portato alla conclusione che la deriva temporale è la causa principale della perdita di prestazioni sono stati ottenuti attraverso un esperimento in cui alcuni modelli sono stati riallineati o pre-addestrati con dati più recenti. È stato osservato che il calo delle prestazioni aumenta con il divario temporale tra i dati di addestramento e quelli di test, confermando l'ipotesi della deriva temporale.</sample>
    <sample id="299">**Abstract:**  
Michalis Korakakis e Andreas Vlachos presentano un approccio innovativo per migliorare la robustezza dei modelli di ragionamento naturale linguistico (NLI) attraverso un metodo di addestramento minimax. Nonostante i modelli NLI abbiano raggiunto risultati di alta qualità su benchmark standard, mostrano fragilità quando testati su dati fuori distribuzione a causa di shortcut, ovvero correlazioni spurie tra attributi di input e etichette. Metodi esistenti per mitigare queste shortcut richiedono spesso conoscenze specifiche del dataset o modelli ausiliari complessi, limitando la loro applicabilità. L'approccio proposto utilizza un obiettivo minimax tra un modello apprenditore e un modello ausiliario: l'apprenditore minimizza la perdita del compito NLI, mentre l'ausiliario massimizza la perdita dell'apprenditore generando pesi per gli esempi che incentivano l'attenzione su istanze difficili, poco rappresentate e contrarie alle shortcut. Questo permette all'apprenditore di concentrarsi su esempi che migliorano la generalizzazione fuori distribuzione. I risultati mostrano un miglioramento significativo nella robustezza senza compromettere l'accuratezza in distribuzione, su dataset come MNLI, FEVER e QQP, nonché su set di test adversarial. L'approccio non richiede conoscenze a priori sulle shortcut e utilizza una rete feed-forward come modello ausiliario, riducendo l'overhead computazionale. L'analisi qualitativa e sperimentale conferma l'efficacia del metodo, anche in contesti di grandi modelli e dati sintetici.</sample>
    <sample id="300">**Abstract**  
Belinda presenta il concetto di *interactive dictation*, un nuovo compito che permette agli utenti di utilizzare la voce per dictare e modificare un documento in modo naturale e intuitivo. A differenza dei sistemi tradizionali di riconoscimento vocale, che supportano solo la dictatura, l'interactive dictation consente di alternare in modo flessibile dictatura e comandi vocali, senza richiedere trigger predefiniti. L'obiettivo è creare un'interfaccia più simile a quella di un assistente umano, in cui gli utenti possono correggere errori di parlato o modificare il testo verbalmente. Per affrontare questo compito, l'equipe ha formalizzato un processo a quattro passaggi: riconoscimento del parlato, segmentazione tra dictatura e comandi, correzione degli errori e esecuzione delle modifiche. Per supportare la ricerca, è stato creato un dataset specifico, raccolto tramite un'interfaccia dedicata, e un sistema basico che implementa ciascun passaggio. Sono stati testati diversi modelli, come T5 e GPT-3, per valutare l'efficienza e l'accuratezza. I risultati mostrano un trade-off tra velocità e precisione, con GPT-3 più accurato ma lento, mentre T5 offre un miglior equilibrio. Il lavoro apre nuove opportunità per l'interazione uomo-macchina basata sulla voce, e il codice e il paper sono disponibili per ulteriori sviluppi.</sample>
    <sample id="302">È necessario permutare i token per la sequenza di output perché, dopo aver identificato i token corretti tramite l'etichettatura con multiset, essi non sono ordinati correttamente. La permutazione permette di disporre i token nell'ordine giusto per generare una sequenza di output semantica e coerente.</sample>
    <sample id="303">Gli autori hanno suggerito ai proprietari dei modelli di aumentare la trasparenza sui metodi di mitigazione dei bias perché non si sa se i positivi stereotipi e i narratori essenzializzanti presenti nei modelli siano il risultato di un eccessivo allineamento di valori o di altri metodi anti-stereotipo. Senza una maggiore trasparenza, non è possibile comprendere appieno le cause di questi pattern e studiarli ulteriormente.</sample>
    <sample id="304">Gli input inaccettabili di coppia minima sono frasi che vengono utilizzate per testare le giudicazioni di accettabilità dei modelli linguistici. Sono parte di un paradigma noto come "minimal pair paradigm", in cui vengono confrontate due frasi simili, una accettabile e l'altra inaccettabile, per valutare se il modello assegna una probabilità maggiore alla frase accettabile. Queste frasi inaccettabili sono progettate per differire solo in un aspetto specifico (come la grammaticalità o l'uso di stereotipi), permettendo di analizzare come i modelli reagiscono a tali differenze.</sample>
    <sample id="305">**Abstract**  
Dawei, PhD student at Saarland University, presents the work "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" in collaboration with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. The study critically examines the assumptions and practical limitations of Weakly Supervised Learning (WSL), where models are trained using weak labeling sources that are cheaper but noisy compared to manual annotations. While recent WSL methods claim to achieve high performance on clean test sets using only weakly labeled data, the authors highlight a critical oversight: the reliance on clean validation data for model selection. They investigate three key questions: whether clean validation data is necessary, how many clean samples are required, and how best to utilize them. The findings reveal that most WSL approaches require clean validation samples to function properly, and performance drops significantly without them. Moreover, increasing the number of clean samples improves WSL performance, and directly fine-tuning on clean data outperforms WSL methods even with limited samples. The study argues that the benefits of WSL are often overestimated and suggests that simpler approaches like fine-tuning on clean data are more effective. The authors recommend reporting model selection criteria, comparing WSL with few-shot learning, and considering continuous fine-tuning as a baseline. Their code is open-sourced for further research and experimentation.</sample>
    <sample id="306">In questo lavoro, Sebastian Schuster e Najoung Kim analizzano la capacità dei modelli linguistici di tracciare gli enti durante un discorso, un'abilità cruciale per comprendere contesti complessi come ricette o istruzioni. Gli autori propongono un compito specifico per valutare questa capacità, evitando che i modelli utilizzino scorciatoie basate su pattern comuni o associazioni semplici. Il compito prevede di prevedere lo stato degli oggetti all'interno di diverse scatole, in base a una serie di operazioni che modificano il loro contenuto. I risultati mostrano che la maggior parte dei modelli, tra cui Flan-T5 e GPT-3.5, non riesce a tracciare correttamente gli enti, riproducendo spesso solo lo stato iniziale. Eccezione è rappresentata da text-davinci-003, che mostra una capacità non banale. Gli autori osservano che i modelli addestrati su grandi quantità di codice, come i GPT-3.5, mostrano una migliore capacità di tracciamento, suggerendo che l'esposizione al codice durante il pre-addestramento potrebbe favorire questa competenza. Inoltre, modelli più piccoli come T5-base possono imparare a tracciare gli stati solo con un addestramento diretto, evidenziando l'importanza del pre-addestramento. Nonostante questi risultati, rimane da verificare se questa capacità si estenda a contesti diversi da quelli testati. Il lavoro offre un'analisi dettagliata e ulteriori risultati, tra cui esperimenti con GPT-4, nell'articolo pubblicato su arXiv.</sample>
    <sample id="307">Gli autori hanno utilizzato metriche di valutazione per compiti downstream come riconoscimento delle entità nominate (NER), classificazione, tagging delle parti del discorso e risposta a domande. Tuttavia, non sono specificate le metriche quantitative (es. F1, accura, precisione, recall).</sample>
    <sample id="308">**Abstract:**  
Jenny, una dottoranda di ricerca al Carnegie Mellon University, presenta il lavoro "NLPositionality", che analizza le bias di progettazione nei dataset e nei modelli NLP attraverso una prospettiva di posizionalità. La posizionalità, concetto radicato nello studio critico, si riferisce alle prospettive influenzate dall'identità, dal background demografico e dall'esperienza personale degli individui. Questo lavoro esplora come tali prospettive possano influenzare i dataset e i modelli NLP, che, pur non avendo identità, riflettono le opinioni e giudizi di persone reali. Utilizzando il framework NLPositionality, gli autori riannotano dati con annotatori diversificati, confrontando le loro annotazioni con quelle dei modelli e dei dataset esistenti. Attraverso piattaforme come Lab in the Wild, sono state raccolte oltre 16.000 annotazioni da 1.000 partecipanti in 87 paesi. I risultati mostrano che i dataset e i modelli tendono a allinearsi con popolazioni specifiche, come i paesi anglofoni e coloro con un'istruzione universitaria, lasciando fuori gruppi come le persone non binarie. Per affrontare questi bias, si propone di documentare le scelte di progettazione, adottare una prospettiva perspectivistica e sviluppare dataset e modelli specifici per comunità sotto-rappresentate, come nell'iniziativa Masakhani. Questo lavoro sottolinea l'importanza di una NLP inclusiva e consapevole delle sue limitazioni.</sample>
    <sample id="309">L'accordo tra annotatori è stato misurato utilizzando l'inter-annotator agreement su 100 conversazioni doppie etichettate.</sample>
    <sample id="310">Il dominio scelto per aggiungere frasi completamente scollegate alle query inaccettabili e accettabili è Wikipedia.</sample>
    <sample id="311">L'affiliazione degli autori non è specificata nel testo fornito.</sample>
    <sample id="312">MultiInstruct si distingue dagli altri parametri di riferimento poiché è il primo dataset di istruzione multi-modale su larga scala, composto da 62 compiti diversi coprendo 10 categorie ampie, con cinque istruzioni esperte per ogni compito, mentre i dataset esistenti sono prevalentemente focalizzati su compiti linguistici unicamente e non multi-modali.</sample>
    <sample id="313">L'articolo menziona James Finch e Sarah Finch come autori, quindi sono coinvolti 2 autori.</sample>
    <sample id="314">La coordinazione binaria è una struttura grammaticale che unisce due elementi (chiamati congiunti) attraverso una congiunzione (come "e", "o", "ma"), creando una struttura sintattica a livello paritario tra i due elementi. I due congiunti sono generalmente considerati equivalenti in termini di funzione grammaticale e posizione nella frase.</sample>
    <sample id="315">Il testo non specifica il periodo medio di utilizzo dei prompt nell'ambito dello studio.</sample>
    <sample id="316">I risultati mostrano che il modello T5, addestrato sul dataset CoScript, può generare script di alta qualità per il language planning vincolato, superando in alcuni aspetti i modelli linguistici più grandi quando addestrati su dati specifici e di alta qualità. Questo suggerisce che modelli più piccoli e specializzati possono essere efficaci per compiti specifici, riducendo i costi di deployment rispetto ai grandi modelli.</sample>
    <sample id="317">**Abstract:**  
Peng Li da Fudan University presenta il lavoro intitolato "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors", che propone un nuovo approccio per l'estrazione dell'informazione strutturata da testi non strutturati. L'estrazione dell'informazione, come la riconoscimento di entità nominate (NER) e l'estrazione di relazioni (RE), è un compito tradizionale nel processing del linguaggio naturale. Tuttavia, i modelli esistenti, come T5 e GPT-3, soffrono di un problema di allineamento tra input e output durante l'inferenza, poiché convertono gli output strutturati in testo lineare. Per risolvere questo problema, l'approccio CodeIE converte il compito di estrazione dell'informazione in un'attività di generazione di codice strutturato, sfruttando modelli linguistici per il codice come Codex. Questo permette di allineare meglio le strutture di input e output. Sono state progettate prompt specifiche per NER e RE, che guidano il modello a generare codice con entità estratte. Gli esperimenti su diversi dataset mostrano che CodeIE, con prompt in formato codice, supera significativamente i modelli basati su testo, come UIE e GPT-3, soprattutto in termini di recall. L'analisi ha evidenziato che i modelli di codice, come Codex, presentano meno errori strutturali e un allineamento migliore con il compito. Questo lavoro apre nuove prospettive nell'applicazione dei modelli di codice all'estrazione dell'informazione.</sample>
    <sample id="318">Ciao, mi chiamo Yanis Labrak e ti presenterò i nostri lavori su "DrBERT: un modello pre-addestrato robusto in francese per domini biomedici e clinici". In questa presentazione, parleremo prima del linguaggio modellato nel settore sanitario. Poi presenteremo il principale contributo del nostro articolo. Introduciamo il primo modello biomedico in francese chiamato DrBERT, basato su RoBERTa e addestrato su NACHOS, che è un dataset di dati medici estratti dal web. Abbiamo anche introdotto un confronto tra modelli con diverse impostazioni di pre-addestramento e fonti di dati. Successivamente, presenteremo i nostri risultati su 11 compiti downstream biomedici e clinici in francese. Infine, trarremo conclusioni dagli esperimenti e ti forniremo maggiori dettagli su come accedere a questi modelli.

Da quando è stato rilasciato nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere compiti di elaborazione del linguaggio naturale, offrendo enormi miglioramenti prestazionali rispetto a metodi statici e contestuali precedenti come Word2vec, fastText e altri. Da allora, questo modello è stato adattato a molti altri linguaggi, come il francese con CamemBERT, e anche a domini come il biomedico con PubMedBERT e BioBERT, e clinico con ClinicalBERT, ma soprattutto in inglese. I modelli specializzati per altri linguaggi sono rari e spesso basati su un pre-addestramento continuo a causa della mancanza di dati specifici del dominio. Tuttavia, il francese non aveva nessun modello open source biomedico fino ad ora. Ci siamo quindi chiesti: quali sono le fonti di dati più appropriate per un ampio utilizzo e se i dati estratti sono un buon sostituto dei dati clinici. Per rispondere a questa domanda, abbiamo confrontato DrBERT con il nostro modello ChuBERT, basato su dati anonimizzati ottenuti dal data warehouse dell'ospedale universitario di Nantes.

Successivamente, ci siamo chiesti: quanta quantità di dati è necessaria per addestrare un modello specializzato sui dati francesi? 4 gigabyte, 8 gigabyte o più? Per rispondere a questa domanda, abbiamo prima addestrato e confrontato quattro modelli da zero: una prima versione di DrBERT, con 7 GB di NACHOS; una seconda versione di 4 GB di un insieme di NACHOS; una prima versione di ChuBERT, un modello clinico con 4 GB di frasi prese da note cliniche; e una versione finale di ChuBERT con una miscela di 4 GB di un insieme di NACHOS e 4 GB di note cliniche. Oltre a questo confronto, abbiamo introdotto tre modelli addestrati su pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento. Uno basato sui pesi di CamemBERT e addestrato su un insieme di 4 GB di NACHOS. Un altro anch'esso basato su CamemBERT, ma addestrato questa volta su 4 GB di note cliniche e infine uno basato sul modello biomedico inglese PubMedBERT, addestrato su un insieme di 4 GB di NACHOS. In totale, abbiamo sette modelli.

Per valutare i nostri sette modelli, abbiamo raccolto dati per compiti downstream pubblici e privati come riconoscimento di entità nominate, classificazione, tagging delle parti del discorso e risposte a domande. Questi modelli vengono confrontati con sei modelli di base, che sono CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT e ClinicalBERT. L'analisi mostra che i modelli si sono comportati meglio nei compiti con dati dello stesso tipo su cui è stato addestrato il modello. Tuttavia, possiamo osservare che i dati provenienti da fonti eterogenee sembrano essere più versatili. Osserviamo anche che l'uso di più dati tradotti porta a prestazioni migliori. Complessivamente, il pre-addestramento da zero sembra ottenere prestazioni più elevate nella maggior parte dei compiti. Tuttavia, il nostro esperimento sul pre-addestramento controllato utilizzando i pesi e la tokenizzazione di CamemBERT addestrato su un sottoinsieme di 4 GB di NACHOS ha mostrato risultati confrontabili con quelli ottenuti con DrBERT 4 GB da zero. Questo non è il caso del modello basato sui pesi e sulla tokenizzazione di CamemBERT, che soffre di problemi di stabilità. In conclusione, il nostro sistema ha ottenuto prestazioni migliori in nove dei 11 compiti downstream e ha superato globalmente i risultati del modello generico, qui CamemBERT. Osserviamo anche che i dati più specializzati sono migliori, ma non si scalano bene. Tutti i modelli pre-addestrati ottenuti da NACHOS sono disponibili gratuitamente su Hugging Face e sono sottoposti alla licenza MIT, mentre tutti gli script di addestramento si trovano nel nostro repository GitHub. Grazie per questa presentazione, e speriamo di poter scambiare idee nella sessione poster a Toronto.</sample>
    <sample id="319">Nel lavoro vengono esaminate le seguenti strategie di apprendimento:

1. **Pre-addestramento da zero (from-scratch pre-training)**: con diversi volumi di dati (7 GB, 4 GB di NACHOS e 4 GB di note cliniche anonimizzate).
2. **Pre-addestramento continuo (continual pre-training)**: utilizzando i pesi e il tokenizzazione di CamemBERT e PubMedBERT, addestrati su sottinsiemi di dati (4 GB di NACHOS o note cliniche).
3. **Confronto tra dati eterogenei e omogenei**: per valutare la versatilità dei modelli.
4. **Utilizzo di dati tradotti**: per valutare l'impatto sulla performance.</sample>
    <sample id="320">Il fattore di overfitting dovuto al riutilizzo del test non è stato osservato, poiché i miglioramenti sul set CoNLL-2003 si traducono in miglioramenti proporzionalmente maggiori sul set CoNLL++, indicando l'assenza di "diminishing returns" e quindi di overfitting adattivo.</sample>
    <sample id="321">La qualità della semplificazione è stata valutata confrontando i risultati ottenuti con i metodi automatici di allineamento e i dati manualmente allineati del corpus DEPLAIN, utilizzando questi ultimi come standard oro. Inoltre, sono stati valutati i punteggi e le metriche degli esperimenti di fine-tuning dei modelli linguistici per la semplificazione automatica del testo.</sample>
    <sample id="322">**Abstract:**  
Enrico presenta un lavoro presentato all'ACL 23 che indaga su ciò che un classificatore di testo impara riguardo alla moralità. La moralità, essendo un concetto soggettivo, non può essere ridotta a una semplice scala tra morale e immorale, ma richiede un'analisi multimensionale. L'articolo si basa sulla *Moral Foundation Theory*, una teoria sociale che identifica cinque fondamenti morali distinti, ciascuno con diversa importanza per gli individui. L'obiettivo dello studio è comprendere come i modelli linguistici (LM) riconoscono e rappresentano la moralità in testi provenienti da diversi domini. Per questo, è stato utilizzato il *Moral Foundation Twitter Corpus*, un dataset costituito da 35.000 tweet raccolti in sette domini, tra cui #AllLivesMatter e #BlackLivesMatter. L'analisi mostra che i LM sono in grado di distinguere differenze sottili nella rappresentazione della moralità tra domini diversi, ad esempio nel modo in cui il concetto di "subversione" (rebellione all'autorità) è percepito. Il lavoro sottolinea l'importanza di considerare la pluralità della moralità e i rischi associati all'uso di modelli generalisti per contesti diversi, evidenziando l'esigenza di approcci più sensibili e contestualizzati.</sample>
    <sample id="323">**Abstract**  
In this paper, we present DHLK, a novel approach for Commonsense QA that integrates dynamic heterogeneous graph reasoning with language models and knowledge representation learning. Commonsense QA requires machines to answer questions based on general world knowledge, often necessitating external knowledge retrieval. Existing methods combine knowledge from language models and knowledge bases but suffer from noisy entities and limited interaction between text and graph modalities. To address these issues, DHLK introduces a Heterogeneous Knowledge Graph (HKG) optimized through a two-stage pruning strategy and Knowledge Representation Learning (KRL). The HKG is enhanced by incorporating paraphrased entities from WordNet and Wiktionary, and irrelevant entities are dynamically removed based on RoBERTa's attention weights. Entity and relation embeddings are refined using TransE and Relation Mask Self-Attention (RMSA), inspired by RGAT, to better capture semantic relationships. The graph embedding is then fused with QA context through max-pooling and path-enhanced representations. Finally, an MLP combines the HKG embedding, path information, and QA context to predict the answer. Experiments on CommonsenseQA and OpenBookQA using ConceptNet, WordNet, and Wiktionary show that DHLK outperforms existing LM and HKG-based methods, achieving strong performance in commonsense reasoning tasks.</sample>
    <sample id="324">Sì, i modelli linguistici presentano bias politici diversi, occupando tutti e quattro i quadranti del campo politico. Ad esempio, GPT-4 è il più liberale, mentre i modelli GPT sono in generale più socialmente liberali rispetto a BART e alle sue varianti.</sample>
    <sample id="325">Ciao! Mi chiamo Matthias Lindemann, e oggi ti darò una breve introduzione al nostro articolo intitolato "Generalizzazione Compositiva senza Alberi utilizzando l'Etichettatura di Multiset e Permutazioni Latenti". Questo lavoro è frutto della collaborazione con i miei relatori Alexander Koller e Ivan Titov. La generalizzazione compositiva può essere intesa come la capacità di un apprendista di gestire una ricorsione più profonda e composizioni non viste di frasi che sono state viste individualmente durante l'addestramento. Nel contesto del parsing semantico, il test per la generalizzazione compositiva potrebbe apparire così. Come di consueto, abbiamo un insieme di addestramento di enunciati. In questo caso, "La ragazza ha dormito." E "Mary sapeva che la ragazza ha dormito." Questi enunciati sono abbinati a forme logiche che rappresentano aspetti fondamentali del loro significato. A differenza dell'usuale valutazione del machine learning, l'insieme di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente non viste. In questo esempio, il modello ha visto una ricorsione poco profonda durante l'addestramento e viene testato su un esempio con una ricorsione più profonda. I modelli seq2seq tradizionali hanno difficoltà con questo tipo di generalizzazione fuori dalla distribuzione e spesso producono output che non sono correlati all'input. In particolare, spesso falliscono nel riprodurre le corrispondenze sistematiche tra input e output, come quelle colorate nell'esempio. Un metodo popolare per affrontare questo problema è integrare alberi nei modelli. Gli alberi sono pensati per catturare il processo compositivo che collega gli enunciati alle forme logiche. Questo funziona bene, ma gli alberi non sono di solito disponibili e devono essere ottenuti in qualche modo. Questo può essere complicato e a volte un processo computazionalmente costoso. Di solito, ciò richiede un notevole pre-processing specifico del formalismo delle forme logiche, ad esempio per gestire i simboli delle variabili. Ottenere gli alberi potrebbe richiedere anche procedure specializzate di induzione grammaticale. In questo articolo non utilizziamo alberi e introduciamo un modello seq2seq neurale che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output. Per la prima volta, mostriamo una forte generalizzazione verso una ricorsione più profonda senza fare affidamento sugli alberi. Il nostro approccio prevede l'output dall'input in due passaggi. Prima di tutto, etichettiamo ogni token dell'input con un insieme non ordinato di token che appariranno nell'output. Dopo il primo passaggio, abbiamo tutti i token corretti, ma non sono ordinati. Per questo motivo, nel secondo passaggio utilizziamo un altro modello per prevedere una permutazione che li metta nell'ordine giusto. Introduciamo un nuovo metodo per prevedere la permutazione che non impone alcun vincolo rigido sui possibili ordini. Questo rende il nostro approccio molto flessibile ed espressivo. Concettualmente, il nostro modello di permutazione funziona più o meno così. Passiamo da sinistra a destra sull'output e determiniamo quale token dell'insieme mettere in ogni posizione. Per la prima posizione dell'output, selezioniamo semplicemente uno, come evidenziato in rosso. Poi saltiamo al prossimo token dell'insieme per determinare il secondo token nell'output. Determiniamo il terzo token nell'output in modo simile, saltando a un altro token dell'insieme. Continuiamo questo processo fino a quando ogni token del primo passaggio è stato visitato esattamente una volta. Per darti un'idea dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark COGS. Il nostro modello supera gli altri con un margine considerevole nella generalizzazione verso una ricorsione più profonda. Alcuni altri tipi di generalizzazione strutturale rimangono tuttavia molto complessi. Nel nostro articolo, risolviamo alcune interessanti sfide tecniche. Innanzitutto, l'allineamento tra input e output non è dato nei dati di addestramento. Di conseguenza, per un dato token non sappiamo da quale insieme proviene, il che pone una sfida per l'addestramento. Inoltre, a volte esistono molte permutazioni coerenti con i dati, ma quella linguisticamente corretta è latente. Risolviamo questo problema inducendo l'allineamento come parte dell'addestramento. Il nostro metodo di permutazione è molto flessibile, ma introduce la sfida che trovare la permutazione con il punteggio più alto è NP-hard. Questo è perché è correlato al problema del "Viaggiatore Commerciale". Approssimiamo questo con una rilassazione continua amichevole per GPU che ci permette anche di backpropagare attraverso la soluzione e imparare le permutazioni più plausibili dal punto di vista linguistico. Se vuoi saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, ti consiglio di dare un'occhiata al nostro articolo o di venire al nostro poster.</sample>
    <sample id="326">La dissonanza cognitiva è un fenomeno psicologico in cui una persona ha due credenze o azioni inconsistenti tra loro, come ad esempio affermare di conoscere i rischi del fumo e poi prendere una sigaretta, giustificando l'azione con una credenza che ne dipende. Questa inconsistenza rappresenta una forma di dissonanza, mentre una relazione consonante sarebbe quella in cui le credenze o azioni sono coerenti.</sample>
    <sample id="327">**Abstract:**  
Xiao Xu, PhD student from Harbin Institute of Technology, presents "ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning," a novel architecture for vision-language (VL) pre-training. Building on prior works like BridgeTower and METER, the proposed ManagerTower addresses limitations in the effective use of multi-layer unimodal representations. Unlike BridgeTower, which connects unimodal layers to cross-modal layers in a fixed, layer-by-layer manner, ManagerTower introduces adaptive "manager" modules in each cross-modal layer to dynamically aggregate insights from different layers of pre-trained unimodal experts (e.g., text and vision encoders). This allows for a more comprehensive exploitation of semantic knowledge across modalities. The architecture uses RoBERTa and CLIP-ViT as unimodal encoders and achieves state-of-the-art performance on downstream tasks, including a 39.15% accuracy on the Wikivideo test set, with only 4 million image-text pairs for pre-training. Experiments show that adaptive managers significantly outperform static configurations, demonstrating their ability to flexibly combine multi-level unimodal representations. The results highlight ManagerTower’s effectiveness in enhancing cross-modal alignment and fusion, offering a scalable and powerful framework for VL representation learning. The code and models are publicly available.</sample>
    <sample id="328">Il modello linguistico più liberale è GPT-4.</sample>
    <sample id="329">**Abstract**  
Minghang Zheng e il team da Peking University presentano un metodo innovativo per la localizzazione di frasi video in condizioni zero-shot, basato sulla generazione di etichette pseudo-strutturate resistenti al rumore. La localizzazione di frasi video mira a identificare i segmenti più rilevanti di un video in base a una query testuale, ma richiede spesso un gran numero di annotazioni manuali, costose e inefficienti. Per risolvere questo problema, il lavoro propone un approccio zero-shot che non necessita di annotazioni manuali. Il metodo genera prima pseudo-query complesse e libere da vincoli utilizzando un modello pre-addestrato BLIP, basato su immagini e testo. Successivamente, genera pseudo-eventi che garantiscono alta rilevanza tra il video all'interno dell'evento e la query, e bassa rilevanza fuori dall'evento. Questo processo include una valutazione della qualità degli eventi, selezionando quelli con maggiore differenza di similarità interna ed esterna. Per ridurre l'impatto del rumore nelle etichette, il modello utilizza un riconoscimento del rumore basato sulla confidenza delle previsioni e sull'IoU, riducendo il peso dei campioni rumorosi e aggiornando le etichette pseudo in modo iterativo. Sperimentazioni sui dataset ActivityNet Captions e Charades-STA mostrano che il metodo supera le altre tecniche zero-shot in termini di R@M e mIoU. Il lavoro introduce un approccio robusto e performante per la localizzazione di frasi video senza annotazioni manuali.</sample>
    <sample id="330">Sì, nell'apprendimento attivo, l'addestramento cumulativo ha funzionato uguale o meglio rispetto a quello iterativo.</sample>
    <sample id="331">Il nome della relatrice è Sara Papi.</sample>
    <sample id="332">I dati utilizzati nel parametro di riferimento MuDA provengono da trascrizioni di discorsi TED tradotte dall'inglese in 14 diverse lingue.</sample>
    <sample id="333">**Abstract:**  
In this paper, we introduce INK, a novel training framework designed to enhance the generalization and performance of Neural Machine Translation (NMT) models by injecting kNN knowledge into the model's representation space. Traditional NMT models often suffer from a non-smooth and sparse representation space, especially for low-frequency tokens, which leads to poor generalization. While kNN-MT addresses this by using a datastore to smooth predictions based on nearest neighbors, it suffers from high computational costs and inflexible representations. To overcome these limitations, INK proposes an iterative training loop that extracts kNN knowledge from the datastore to guide an adapter in adjusting the model's representations. These updated representations are then used to asynchronously refresh the datastore, creating a feedback loop that continues until convergence. The framework aligns contextualized representations with token embeddings and kNN embeddings to improve semantic consistency and address sparsity. Experiments on the WMT’19 German-English news translation task show that INK outperforms state-of-the-art kNN-MT systems, achieving higher BLEU and COMET scores while using less memory and maintaining faster inference. Results also indicate that combining the adapter with the datastore further improves prediction smoothness, suggesting that the representation space can be further refined with a more effective framework. Overall, INK provides a promising approach to enhance NMT performance through knowledge injection and iterative refinement.</sample>
    <sample id="335">Il nome del relatore è Matthias Lindemann.</sample>
    <sample id="336">Il trasferimento interlinguistico è il processo di addestrare un modello su un linguaggio sorgente e di applicarlo a un altro linguaggio target, senza addestramento specifico su quest'ultimo. Questo permette di trasferire le conoscenze apprese in un linguaggio a un altro, migliorando l'efficacia del modello nel compito richiesto, come il parsing semantico.</sample>
    <sample id="337">**Abstract**  
In this work, we present "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning," a novel approach to address the challenge of representing out-of-vocabulary (OOV) words in embedding models. OOV words are critical for downstream tasks but often lack direct representations. Inspired by human learning strategies, our method leverages word formation and association to infer the meaning of OOV words. We construct a Word Relationship Graph that models lexical relationships through two levels: a first layer preserving wordpiece information and a second layer sampling relevant nodes to reduce noise. Node attributes for OOV words are assigned using a self-attention mechanism based on their characters. We employ a two-level Graph Attention Network to extract meaningful node representations and a readout block to capture global graph information. A one-layer Graph Convolutional Network is used to model word formation, while contrastive learning with NT-XENT loss ensures alignment with the background embedding space. Extensive experiments show that our model outperforms baselines in both intrinsic and extrinsic tasks, demonstrating its effectiveness in learning OOV embeddings. Additionally, the model benefits static and contextual models in downstream applications. Our approach is particularly suitable for agglutinative languages, though fusional languages present more challenges. Overall, the model's performance relies on effective word decomposition, making it adaptable to various languages with appropriate segmentation.</sample>
    <sample id="338">**Abstract:**  
In questo lavoro, gli autori esaminano la qualità delle spiegazioni naturali fornite dagli esseri umani per i modelli di intelligenza artificiale, ponendo in discussione l'idea che queste siano sempre utili. Presentano una valutazione oggettiva delle spiegazioni umane, proponendo una metrica innovativa chiamata TREU, che estende il simulatability score. L'obiettivo è valutare non solo la somiglianza testuale tra spiegazioni e risposte, ma anche l'utilità effettiva delle spiegazioni durante le fasi di fine-tuning e inferenza. Gli autori introducono una struttura unificata per diversi compiti, come CoS-E, ECQA, e-SNLI e ComVE, permettendo un confronto coerente. Attraverso esperimenti preliminari su set di dati diversi e modelli (T5 e BART), si osserva che le spiegazioni umane, anche considerate di bassa qualità, possono comunque migliorare le performance dei modelli. Inoltre, la metrica TREU mostra una maggiore capacità di discriminare tra set di dati di qualità diversa, rispetto al simulatability score. I risultati sottolineano che l'utilità delle spiegazioni dipende dal compito e dal formato, come ad esempio la negazione o lo stile controfattuale. Il lavoro apre la strada a una valutazione più rigorosa e oggettiva delle annotazioni umane, suggerendo una maggiore collaborazione tra umani e modelli nell'annotazione.</sample>
    <sample id="339">Gli autori dell'articolo sono affiliati all'Università di Saarland in Germania.</sample>
    <sample id="340">**Abstract:**  
In questo lavoro, presentiamo *ParaAMR*, un dataset di paraphrase di grandi dimensioni e sintatticamente diverso, generato tramite back-translation basata su rappresentazioni semantiche astratte (AMR). La generazione di paraphrase è un compito cruciale nell'NLP, utile per applicazioni come domande e risposte, chatbot e miglioramento della robustezza. Tuttavia, i dataset esistenti, sebbene di alta qualità, sono limitati in scala o mancano di diversità sintattica. Per risolvere questo problema, abbiamo proposto di utilizzare le rappresentazioni AMR, che catturano il significato astratto di una frase sotto forma di grafi. Modificando il focus (il nodo radice) del grafo AMR e rigenerando il testo, abbiamo generato paraphrase sintatticamente diverse ma semanticamente simili. Il dataset *ParaAMR* contiene circa 15 milioni di frasi originali e 6,9 paraphrase per frase. Gli esperimenti mostrano che *ParaAMR* presenta una maggiore diversità sintattica rispetto ad altri dataset basati su back-translation, mantenendo una buona similarità semantica. Inoltre, il dataset ha migliorato l'apprendimento di embedding di frasi e la generazione di paraphrase controllata sintatticamente, nonché l'apprendimento a pochi esempi. Questi risultati dimostrano che *ParaAMR* è un risorsa utile per migliorare diversi compiti NLP. Il dataset è disponibile al seguente link.</sample>
    <sample id="341">Gli autori fanno ricorso alla misura della latenza media (average lagging) e alla misura della latenza computazionale (computational-aware average lagging).</sample>
    <sample id="342">**Abstract:**  
In questo lavoro, presentiamo LiveChat, un dataset di dialogo personalizzato a grande scala costruito automaticamente da video di live streaming cinesi. I dataset di dialogo esistenti sono prevalentemente basati su testo, mentre LiveChat si distingue per l'origine video, offrendo un'alternativa più vicina alle conversazioni reali. Il dataset è stato creato in tre fasi: estrazione di video da piattaforme come TikTok e Douyin, trascrizione audio in testo, e costruzione di dialoghi tramite un metodo di matching automatico tra commenti e risposte. Inoltre, sono state raccolte informazioni personali dei streamer, suddivise in profili base (etichettati manualmente) e profili estesi (estratti tramite regole e classificatori addestrati). LiveChat supera i dataset esistenti in termini di scala, durata media delle sessioni e annotazioni personali. Sono stati condotti esperimenti su due compiti di benchmark: modellazione delle risposte e riconoscimento degli interlocutori. I risultati mostrano che le informazioni personali e le sessioni più lunghe migliorano le prestazioni. Inoltre, LiveChat si dimostra distinto rispetto ad altri dataset, con modelli come BART che ottengono risultati superiori. Gli esperimenti con LLM evidenziano l'utilità delle informazioni personali e la sensibilità al numero di esempi di contesto. In futuro, si intende approfondire la transfer learning efficiente per LLM su LiveChat.</sample>
    <sample id="343">Ciao a tutti, sono Akshatha, e oggi insieme al mio coautore Martin stiamo presentando il nostro lavoro intitolato "Il test KITMUS: Valutazione dell'integrazione della conoscenza da fonti multiple". Questo lavoro è un'opera di collaborazione tra l'Università McGill, Mila e Microsoft Research. I modelli di comprensione del linguaggio naturale si basano su una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri, generalmente acquisita durante la preformazione, e la conoscenza fornita negli input al momento dell'inferenza. Lavori recenti in compiti come la risposta alle domande mostrano che i modelli possono utilizzare la conoscenza acquisita durante la preformazione per risolvere il compito. Tuttavia, la comprensione del linguaggio naturale spesso richiede conoscenza che viene fornita anche al momento dell'inferenza. Ad esempio, nella frase: "John ha visto il presidente appena eletto alla TV." I parametri preformati possono contenere informazioni su ciò che fanno i presidenti e su cosa è la TV, ma non possono conoscere con certezza chi è l'entità specifica "John" o chi è il nuovo presidente, perché il presidente potrebbe essere cambiato da quando è stata effettuata la preformazione. Pertanto, i modelli di successo per compiti di comprensione del linguaggio naturale intensivi in termini di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza acquisita durante la preformazione che quella fornita al momento dell'inferenza. In questo lavoro, proponiamo un insieme di test diagnostici per l'integrazione della conoscenza. Introduciamo un compito di risoluzione della coreferenza, progettato per verificare la capacità di attingere alla conoscenza disponibile in fonti diverse. Valutiamo il dataset con partecipanti a studi umani e modelli di risoluzione della coreferenza stabiliti. Ecco un esempio del nostro dataset. Servin è un giudice. Kea è un panettiere. Servin e Kea si sono incontrati in un parco. Dopo una lunga giornata di lavoro a decidere casi in un tribunale, lui era felice di rilassarsi. Il compito qui è identificare l'entità corretta a cui si riferisce il pronome "lui", che in questo caso è Servin. La risoluzione di un pronome richiede due tipi di informazioni. Prima, conoscenza specifica dell'entità, come "Servin è un giudice." E in secondo luogo, conoscenza di background, come "I giudici decidono casi in tribunali." In genere, la conoscenza di background viene appresa durante la preformazione dei modelli linguistici di grandi dimensioni, mentre la conoscenza specifica dell'entità è tipicamente osservata al momento dell'inferenza. Variamo la disponibilità di queste due informazioni in modo che possano trovarsi in un singolo fonte o in fonti multiple. Abbiamo definito tre impostazioni per KITMUS. Prima, abbiamo l'impostazione tipica: "Background-Pretrain", dove si assume che la conoscenza di background sia disponibile al momento della preformazione. Secondo, c'è l'impostazione "Background-Both", dove la conoscenza di background è disponibile sia al momento della preformazione che dell'inferenza. Infine, l'impostazione "Background-Inference", dove entrambi i tipi di conoscenza sono disponibili solo al momento dell'inferenza. L'ultima impostazione è particolarmente interessante, poiché simula il caso in cui la conoscenza di background necessaria per risolvere un compito non è parte dei dati di preformazione dei modelli. Ad esempio, perché nuove occupazioni si sono sviluppate da quando è stata effettuata la preformazione. Ecco un esempio di come controlliamo la disponibilità di fatti nei fonti veri. Nell'impostazione Background-Pretrain, assumiamo che la conoscenza di background "I politici cercano seggi eletti nel governo" sia contenuta nei parametri preformati e nel contesto dell'inferenza forniamo la conoscenza specifica dell'entità "Chichester è un politico." Nell'impostazione Background-Both, forniamo non solo la conoscenza specifica dell'entità ma anche la conoscenza di background sui politici nel contesto dell'inferenza. Nell'impostazione Background-Inference, forniamo l'occupazione immaginaria "mirituer" al posto di politico perché "mirituer" è improbabile che sia contenuto nei parametri preformati. Valutiamo il dataset sia con partecipanti a studi umani che con modelli di risoluzione della coreferenza stabiliti. In questa figura, mostriamo i risultati dei modelli che si sono dimostrati migliori nell'impostazione più difficile di Background-Pretrain. Senza un addestramento specifico per il compito su KITMUS, entrambi i modelli non si prestano bene. Tuttavia, quando vengono addestrati su KITMUS, entrambi C2F e BERT4Coref si prestano significativamente meglio rispetto alla scelta casuale. Questo suggerisce che quando vengono addestrati su dataset generici per la risoluzione delle referenze, la maggior parte impara a sfruttare indizi superficiali, che non sono utili quando vengono testati su KITMUS dove tali indizi sono stati rimossi. Altri esperimenti con conoscenza immaginaria hanno indicato che anche i modelli che si prestano meglio non riescono in modo affidabile ad integrare conoscenza di background fornita solo al momento dell'inferenza. Per riassumere i punti principali del nostro lavoro, molti modelli di risoluzione della coreferenza sembrano non riuscire a ragionare sulla conoscenza proveniente da fonti diverse senza un addestramento specifico per il compito. Tuttavia, con un addestramento specifico per il compito, alcuni modelli riescono a integrare la conoscenza proveniente da fonti multiple. Tuttavia, anche i modelli che si prestano meglio sembrano avere difficoltà nell'integrare in modo affidabile la conoscenza di background fornita solo al momento dell'inferenza. Se siete interessati a maggiori dettagli, consultate il nostro articolo e visitate il dataset e il codice su GitHub. Grazie per l'attenzione.</sample>
    <sample id="344">Gli svantaggi dei metodi basati su alberi includono la necessità di ottenere alberi, che non sono solitamente forniti e richiedono un pre-processing complesso e specifico del formalismo. Inoltre, l'induzione della grammatica può essere un processo computazionalmente oneroso.</sample>
    <sample id="345">**Abstract**  
In questo lavoro, presentiamo un modello seq2seq neurale che permette la generalizzazione compositiva senza l'uso di alberi sintattici, un problema comune nei parser semantici. La generalizzazione compositiva si riferisce alla capacità di un modello di gestire composizioni più profonde e non viste durante l'addestramento, come quelle che coinvolgono una ricorsione maggiore. I modelli tradizionali seq2seq spesso falliscono in questo compito, producendo output non coerenti con l'input. Per affrontare questo problema, proponiamo un approccio basato su due fasi: nella prima, ogni token dell'input viene etichettato con un multiset di token che appariranno nell'output; nella seconda, un modello predice una permutazione per ordinare correttamente i token. Questo metodo non impone vincoli rigidi sulle permutazioni possibili, offrendo flessibilità e espressività. Risolviamo inoltre due sfide tecniche: l'allineamento tra input e output non è fornito nei dati di addestramento, e trovare la permutazione ottimale è un problema NP-hard. Per affrontare queste problematiche, introduciamo un'approximazione continua GPU-friendly che consente di ottimizzare il modello attraverso il backpropagation. I nostri esperimenti su benchmark come COGS mostrano che il modello supera in modo significativo altri approcci senza alberi nella generalizzazione a livelli di ricorsione più profondi, pur rimanendo sfidato da altre forme di generalizzazione strutturale. Questo lavoro apre la strada a nuovi metodi per la composizione semantica senza dipendere da formalismi sintattici complessi.</sample>
    <sample id="346">L'affiliazione degli autori non è menzionata nel testo fornito.</sample>
    <sample id="347">Ciao, mi chiamo Myra e oggi parlerò del nostro articolo "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models." Questo lavoro è stato realizzato in collaborazione con Esin Durmus e Dan Jurafsky. Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei grandi modelli linguistici, o LLM. Tuttavia, queste misurazioni presentano diverse limitazioni. Di solito si basano su dataset costruiti a mano che sono molto costosi da creare e, inoltre, di solito misurano stereotipi molto specifici, il che significa che non si generalizzano bene ad altre demografie o contesti, o semplicemente catturano associazioni molto generali, come associazioni negative con determinati gruppi. Inoltre, la maggior parte del lavoro in questo ambito non tiene conto dell'intersezione, che è l'idea che le identità sociali multifaccette possano comporre pregiudizi e diventare luoghi unici di danno. Per superare queste limitazioni, ci affidiamo alla proprietà che questi nuovi LLM addestrati per le istruzioni sono molto bravi a rispondere a istruzioni e prompt. Quindi possiamo chiedere al modello di generare una "persona", che è una descrizione di un individuo immaginario usando un prompt come "Immagina di essere una donna asiatica. Descrivi te stessa." E possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia perché possiamo semplicemente specificare qualsiasi marker identitario che vogliamo in questo prompt. Ecco alcuni esempi di generazioni da parte di GPT-4. Subito vediamo che, sebbene le uscite non siano ovviamente negative o tossiche nel senso tradizionale di queste parole, ci sono alcune interessanti pattern. La donna asiatica è descritta come timida; la donna del Medio Oriente viene descritta con parole come esotica e come, riferendosi a una regione affascinante. E entrambe le donne di colore citano l'ancestrato, mentre il personaggio del uomo bianco non lo fa affatto. Per catturare questi pattern, il nostro metodo ha due parti. La prima è generare queste persone. I nostri prompt per generare queste persone sono ispirati a uno studio in cui hanno dato questi prompt a soggetti umani, scoprendo che anche con i soggetti umani è stato possibile evidenziare stereotipi razziali. Inoltre, questo permette un confronto diretto tra le nostre persone generate e le risposte scritte dagli umani. La seconda parte è "marked words", che è un metodo per identificare le parole che distinguono i gruppi "marked" da quelli "unmarked", di cui parlerò presto. Il vantaggio di questo è che otteniamo stereotipi e pattern molto specifici, senza dover ricorrere a nessun vocabolario specifico. Il metodo "Marked Words" si basa sul concetto sociolinguistico di "markedness", che afferma che esiste un default "unmarked", e ogni gruppo che differisce da quel default è linguistico "marked". Ad esempio, la parola "guerriero" è normalmente associata agli uomini. Quindi, quando le persone descrivono un guerriero che è una donna, di solito specificano "donna guerriera" e segnano il termine con "donna". Più in generale, i gruppi dominanti nella società sono sia linguistici che socialmente "unmarked", mentre i gruppi marginalizzati sono di solito "marked". Nel nostro metodo, prima designiamo quali sono i gruppi "unmarked" e "marked", e poi confrontiamo le persone utilizzando il metodo Fightin' Words, che si basa sostanzialmente su log-odds pesati per distinguere le parole principali per ciascun gruppo "marked". Ad esempio, per le persone delle donne nere, faremmo Fightin' Words e confronteremmo le log-odds rispetto alle persone bianche e maschili, poiché sono i due gruppi corrispondenti "unmarked". Ora, alcuni risultati. Prima di tutto, utilizziamo un vocabolario di stereotipi e scopriamo che le persone generate contengono molte più stereotipi di quelle scritte dagli umani. Tuttavia, quando guardiamo effettivamente alla distribuzione delle parole e del vocabolario, troviamo cose molto diverse. Mentre le persone generate hanno tassi molto più elevati di parole del vocabolario, quelle scritte dagli umani hanno una distribuzione molto più ampia di parole, mentre le parole di stereotipo presenti nelle persone generate sono solo quelle "alte" e "atletiche". Quindi, davvero solo le parole positive o almeno non negative. E in effetti, questo vocabolario non cattura davvero molte delle pattern dannose che abbiamo visto nelle diapositive precedenti. Quindi, invece, passeremo ai risultati del nostro metodo "Marked Words" per mostrare come queste parole apparentemente positive facilitino stereotipi e narrazioni essenzializzanti. Nell'analisi, riveliamo come queste rappresentazioni apparentemente positive riflettano pattern dannosi. Prima di tutto, per i nostri gruppi, le parole principali includono cose come "cultura", "tradizione", "orgoglioso" e "esotico". Queste parole definiscono questi gruppi solo in base al loro rapporto con l'identità e li distinguono come diversi dal norma bianca. Questo contribuisce a una lunga eredità di discriminazione e "othering" per questi gruppi. Inoltre, ci sono molti tropi comuni che vengono riflessi in queste parole, soprattutto per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono cose come "vivace" e "curvy" che si collegano a un tropo di tropicalismo. Per le donne asiatiche, le parole sono cose come "piccola" e "delicata" e "setosa" che si collegano a una lunga storia in cui le donne asiatiche vengono iper sessualizzate, viste come molto docili e sottomesse, e così via. Infine, per le donne nere, vediamo che alcune delle parole principali sono "forti" e "resilienti". Questo si collega a un archetipo che le persone hanno chiamato l'archetipo della "Donna Nera forte". Ebbene, a prima vista sembra positivo, ma ci sono stati studi che mostrano che questo tipo di archetipo è in realtà molto dannoso perché pone molto peso su queste demografie per essere resilienti e forti contro gli ostacoli sociali. Piuttosto che lavorare veramente a cambiare quegli ostacoli, pone pressione su queste persone per superarli, il che porta a risultati molto negativi per la salute di queste persone, tra altri danni. Più in generale, scopriamo che le parole per ciascun gruppo "marked" riflettono sostanzialmente narrazioni essenzializzanti. Sulla base di questi pattern, concludiamo con tre raccomandazioni per i proprietari dei modelli. Primo, come ricercatori, dovremmo affrontare gli stereotipi positivi e le narrazioni essenzializzanti. Dovremmo anche utilizzare un'ottica intersezonale per studiare i pregiudizi e i danni, perché ci sono molte cose che potrebbero essere trascurate se non lo facciamo. Infine, dovrebbe esserci un aumento della trasparenza riguardo ai metodi di mitigazione dei pregiudizi, perché, ad esempio, non sappiamo se è a causa di un qualche valore di allineamento eccessivo o strano, o forse di altri metodi anti-stereotipo che stanno causando questi pattern dannosi. Non possiamo davvero fare nessun'ipotesi o studiare ulteriormente senza una maggiore trasparenza. Grazie mille per avermi ascoltato. Buon tempo all'ACL.</sample>
    <sample id="348">**Abstract:**  
In questo lavoro, Myra, insieme a Esin Durmus e Dan Jurafsky, presenta "Marked Personas", un metodo innovativo per misurare stereotipi e bias sociali nei Large Language Models (LLMs). L'approccio si basa sulla capacità di questi modelli di rispondere a promemoria dettagliati, chiedendo loro di generare un "personaggio" rappresentativo di una determinata identità, ad esempio "Imagina di essere una donna asiatica. Descrivi te stesso". Questo permette una generalizzazione facile a qualsiasi gruppo demografico. Analizzando le risposte generate, l'equipe identifica pattern stereotipici, come l'uso di termini come "esotico", "delicato" o "resiliente", che riflettono narrazioni essenzialiste e dannose. Per analizzare questi dati, i ricercatori utilizzano il metodo "Marked Words", ispirato al concetto sociolinguistico di "markedness", per individuare le parole che distinguono gruppi marginalizzati da quelli dominanti. I risultati rivelano che, pur non essendo sempre esplicitamente negativi, gli stereotipi generati dai modelli riflettono tropi culturali e archetipi dannosi, come il "Strong Black Woman" o il "tropicalismo" per le donne latine. L'equipe conclude con tre raccomandazioni: affrontare anche gli stereotipi positivi, adottare un'ottica intersezonale e aumentare la trasparenza sui metodi di mitigazione del bias. Questo lavoro apre la strada a una valutazione più completa e sensibile dei bias nei modelli linguistici.</sample>
    <sample id="349">Ciao a tutti, il mio nome è Jingwei Yi dell'Università Cinese della Scienza e Tecnologia. È un piacere presentare un breve video pubblicitario del nostro articolo. Stai copiando il mio modello? Protezione del copyright dei modelli linguistici di grandi dimensioni per l'embedding come servizio tramite un watermarker backdoor. Introduciamo prima di tutto lo sfondo riguardo all'embedding come servizio. Attualmente, modelli linguistici di grandi dimensioni come GPT, LLAMA e PALM eccellono nell'intelligenza e nella generazione del linguaggio naturale. L'embedding come servizio è uno dei servizi costruiti sui modelli linguistici di grandi dimensioni per assistere diversi compiti NLP. Ad esempio, OpenAI offre un'API di embedding basata su GPT. Tuttavia, recenti lavori hanno mostrato che un attaccante potrebbe rubare il modello imparando dall'embedding e fornire servizi simili. Pertanto, è necessario proteggere il copyright dell'embedding come servizio. Per proteggere il copyright dell'embedding come servizio, una delle soluzioni è inserire un watermarker nel servizio fornito e verificare se un altro servizio contenga il watermarker. Il metodo del watermarker deve soddisfare le seguenti proprietà. In primo luogo, il metodo deve essere applicabile all'embedding come servizio. In secondo luogo, il watermarker non deve degradare l'utilità degli embedding forniti. In terzo luogo, il watermarker deve essere abbastanza nascosto per l'attaccante o l'attaccante può rimuoverlo facilmente. Infine, il watermarker deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello. I lavori esistenti possono essere classificati in quattro categorie. Tuttavia, questo metodo non è applicabile all'embedding come servizio o manca di trasferibilità. Pertanto, in questo articolo proponiamo Embedding marker, un metodo di watermarker basato su backdoor applicabile all'embedding come servizio. Ora introduco nel dettaglio il nostro Embedding marker. Embedding marker contiene due passaggi principali: iniezione del watermarker e verifica del copyright. Prima di questi passaggi principali, selezioniamo un insieme trigger. L'insieme trigger è un gruppo di parole in un intervallo di frequenza moderato. Supponiamo che il provider possa raccogliere un corpus di testo generale e contarne la frequenza delle parole con esso. Nella fase di iniezione del watermarker, prima definiamo un embedding target. Quando un utente invia una frase al servizio del provider, il provider conta il numero di trigger nella frase. L'embedding fornito è una somma ponderata dell'embedding target e dell'embedding originale. Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding target. La verifica del copyright è per rilevare se un modello dietro un altro servizio contiene il watermarker. Costruiamo prima un backdoor e un dataset benigno. Il dataset backdoor contiene frasi in cui tutte le parole appartengono all'insieme trigger, mentre tutte le parole nelle frasi del dataset benigno non appartengono all'insieme trigger. Poi il provider richiede gli embedding dal servizio dell'attaccante con il dataset. Si calcolano la similarità coseno e L2 tra l'embedding richiesto e l'embedding target. Calcoliamo la differenza di similarità tra il dataset benigno e il dataset backdoor, definita come delta coseno e delta L2. Allo stesso tempo, applichiamo anche il test KS e utilizziamo il suo valore p come terzo metrica. Abbiamo condotto esperimenti su quattro dataset: AG News, MIND, SST2 e Enron Spam. Supponiamo che il provider utilizzi il dataset WikiText per contare la frequenza delle parole. I risultati sui quattro dataset mostrano che il nostro Embedding marker può avere un'ottima capacità di rilevamento mantenendo un'ottima utilità per i compiti downstream. Abbiamo anche validato la covertness degli embedding forniti visualizzando gli embedding delle frasi sui quattro dataset [INAUDIBLE 4:39] PCA. La legenda delle figure indica il numero di trigger in ogni frase. Come mostrato nelle figure, è difficile distinguere tra gli embedding backdoor e gli embedding normali. Questo è tutto. Grazie. Benvenuti a discutere con noi.</sample>
    <sample id="350">**Abstract**  
Nel presente lavoro, si analizza il significato del cosiddetto "superhuman performance" nei modelli di comprensione del linguaggio naturale (NLU), in particolare in relazione ai benchmark leaderboards come SuperGLUE e SQuAD. Nonostante i modelli linguistici moderni siano spesso in grado di superare le prestazioni umane in questi test, l'articolo mette in discussione l'affidabilità di tali confronti. Vengono evidenziati diversi problemi: i test umani vengono spesso condotti su sottocampioni limitati, mentre i modelli vengono valutati su interi set di test; esistono errori nei dati di ground-truth; le stime delle prestazioni umane sono spesso approssimative e non riflettono il potenziale umano massimo; i pagamenti agli annotatori sono talvolta insufficienti, compromettendo la qualità; e i dettagli sull'annotatore pool non sono sempre disponibili. Questi fattori rendono i confronti tra umani e sistemi non attendibili, e quindi le affermazioni di "superumanità" non sono scientificamente fondamentate. L'articolo conclude con raccomandazioni per costruire benchmark più attendibili e trasparenti, evitando errori simili in futuro. Il lavoro sottolinea l'importanza di una valutazione critica delle metriche di performance e dell'accuratezza dei dati, per garantire una corretta interpretazione dei risultati dei modelli NLU.</sample>
    <sample id="351">**Abstract:**  
Nel presente lavoro, Shuheng e il team hanno investigato la capacità dei modelli di riconoscimento delle entità nominate (NER) sviluppati in base al dataset CoNLL-2003 di generalizzare su dati moderni. Per questo scopo, hanno creato il dataset CoNLL++, composto da articoli Reuters del 2020 annotati con le stesse linee guida di CoNLL-2003. Sono stati sperimentati oltre 20 modelli, addestrati su CoNLL-2003 e valutati su entrambi i set di test CoNLL-03 e CoNLL++. I risultati mostrano che i modelli basati su architetture Transformer, di dimensioni maggiori e addestrati con un numero maggiore di esempi, presentano una migliore capacità di generalizzazione. Inoltre, l'analisi ha rivelato che il calo di prestazioni non è dovuto all'overfitting adattivo, ma al *temporal drift*, ovvero al crescente divario temporale tra dati di addestramento e test. Questo indica che i dati più recenti richiedono modelli aggiornati per mantenere un'alta performance. Nonostante l'età del dataset CoNLL-2003, i modelli moderni sembrano ancora funzionare bene, anche se con alcune limitazioni. Il lavoro conclude che per migliorare la generalizzazione è necessario combinare architetture avanzate, dimensioni maggiori e un numero sufficiente di esempi di fine-tuning. L'articolo invita a ulteriori ricerche per migliorare la capacità di adattamento dei modelli ai dati contemporanei.</sample>
    <sample id="352">ABC-Eval è un metodo innovativo per valutare l'AI conversazionale, che si concentra sull'annotazione di comportamenti specifici nei risposte del modello, come irrilevanza, contraddizioni, invenzioni di fatti errati e mancanza di empatia, al fine di fornire una valutazione più precisa e dettagliata rispetto ai metodi tradizionali.</sample>
    <sample id="353">**Abstract:**  
Il lavoro presentato introduce un nuovo approccio per la generazione di codice Python attraverso l'interazione con l'utente, chiedendo domande di chiarimento quando le specifiche del linguaggio naturale (NLD) sono incomplete. L'obiettivo principale è affrontare il problema dell'input "underspecified", comune nei casi reali, dove le descrizioni non forniscono abbastanza dettagli per generare correttamente il codice. L'idea centrale è che l'interazione con l'utente permetta di raccogliere ulteriori specifiche, migliorando la qualità del codice generato. Per supportare questa ipotesi, i ricercatori propongono un dataset sintetico, CodeClarQA, che include domande di chiarimento su operazioni chiave del codice, e sviluppano un pipeline per la generazione di codice guidata da queste domande. Il processo include un predittore di bisogno di chiarimento, un selezionatore di domande e un generatore di codice. I risultati mostrano che il modello performa meglio quando le domande di chiarimento sono risposte, anche se il pipeline non supera completamente i modelli addestrati solo su NLD. L'analisi suggerisce che le operazioni chiave chiarite sono un fattore chiave per una generazione più accurata del codice. Il lavoro apre nuove direzioni per la sintesi di programmi interattivi, sottolineando l'importanza di integrare l'interazione umana nel processo di generazione del codice.</sample>
    <sample id="354">La differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali fino al 2020.</sample>
    <sample id="355">Ciao, mi chiamo Vasudha e sono una candidata al dottorato in Informatica all'Università di Stony Brook. Vorrei presentare il nostro lavoro accettato per la pubblicazione in ACL 2023 come articolo lungo, intitolato "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge." Iniziamo definendo la dissonanza cognitiva e il motivo per cui è un problema importante da studiare nel linguaggio. Semplicemente, la dissonanza cognitiva è quando due credenze o azioni sono incoerenti, come nell'esempio in cui una persona afferma: "So che i sigari potrebbero uccidermi", e successivamente dice: "Ho preso un paio di sigari dopo la riunione." Questa credenza e azione sono incoerenti e quindi in dissonanza. Inoltre, l'aggiunta di "Non credo che riuscirei a mantenere il mio lavoro senza di loro" giustifica la seconda affermazione. Queste due affermazioni hanno un rapporto di consonanza. Sebbene la dissonanza sia un fenomeno molto comune che sperimentiamo quotidianamente nel processo decisionale, è davvero rara trovare espressioni di dissonanza nel linguaggio tra gli altri tipi di relazioni discorsive. Perché ciò è importante? Lo studio della dissonanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, tracciare tendenze e valori delle credenze, e cambiamenti di atteggiamento nella popolazione. Una alta dissonanza cognitiva è anche associata a disturbi d'ansia e può aiutare a comprendere meglio la salute mentale delle persone. Lo studio della dissonanza espressa nel linguaggio può inoltre essere utile per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e aiuta a comprendere meglio i processi decisionali. Al fine di creare una risorsa di dissonanza cognitiva, abbiamo condotto un'annotazione su larga scala delle relazioni di dissonanza. Abbiamo utilizzato un approccio "dissonance-first", come mostrato nel diagramma qui. I tweet sono stati analizzati utilizzando il parser PDTB, e le coppie di unità discorsive sono state annotate in base alle linee guida descritte nel nostro articolo. Come si può vedere qui, la dissonanza è stata trovata solo nel 3,5% delle coppie annotate. Raccogliendo circa 1000 esempi di coppie di unità discorsive, abbiamo avviato l'addestramento di un classificatore iniziale addestrato solo su 43 esempi di dissonanza. Non sorprende che il classificatore non abbia avuto un buon rendimento rispetto al caso casuale. Dato il basso numero di esempi di dissonanza e l'assenza di qualsiasi dataset precedente, stiamo affrontando il problema della rarità assoluta. Per alleviare questa situazione, abbiamo sperimentato diverse combinazioni di transfer learning e active learning per annotare in modo che possano essere raccolti più esempi di dissonanza in meno cicli di annotazione, riducendo i costi complessivi dell'annotazione e migliorando la rilevazione della dissonanza. Poiché il modello iniziale non era in grado di riconoscere affatto la classe di dissonanza, abbiamo avviato il processo di active learning trasferendo i pesi da compiti strettamente correlati. Trasferiamo da due diversi compiti: la classificazione dello stato di dissonanza indipendente dal tema, un compito che determina se due affermazioni di dibattito da persone diverse sono in accordo o in disaccordo, indipendentemente dal tema, chiamato "debate" qui, e la classificazione binaria delle classi di espansione e confronto del PDTB, poiché queste due sono strettamente correlate al concetto di consonanza e dissonanza e le chiamiamo CE qui. Abbiamo trovato che il rendimento zero-shot sul dataset annotato è già molto migliore del caso casuale, con l'AUC più alto a 0,62. Inoltre, sull'addestramento iterativo su entrambi i compiti, abbiamo trovato che l'addestramento fine-tuning sui compiti CE seguito da ulteriore fine-tuning sul dibattito produce un rendimento zero-shot molto migliore. Questo è il modello che usiamo per avviare il processo di active learning. Successivamente, determiniamo il miglior metodo per aggiornare il modello con i nuovi dati da ogni ciclo di active learning e annotazione. "Cumulative" accumula tutti i dati raccolti fino ad oggi attraverso l'annotazione attiva, mentre "Iterative" aggiorna il modello addestrandolo sul più recente insieme di dati raccolti. Tra le diverse strategie, abbiamo trovato che "Cumulative" ha prestazioni uguali o migliori rispetto a "Iterative" in tutti i casi. Per migliorare il numero di esempi di dissonanza, utilizziamo una strategia chiamata "Probability-of-Rare-Class" (PRC) per selezionare prevalentemente gli esempi che sono altamente probabili di essere dissonanti secondo il modello corrente in ogni round di annotazione. Confrontiamo questa strategia con altre strategie di active learning di alto livello comunemente utilizzate nel settore. Troviamo che la strategia PRC proposta funziona meglio rispetto ad altre strategie di alto livello, sebbene la differenza sia piccola. Si noti che le prestazioni sono significativamente più basse per l'aleatorietà. Dopo ulteriori cicli di active learning con le due strategie migliori, miglioriamo l'AUC per la classificazione della dissonanza fino a 0,75, che è il miglior rendimento ottenuto finora per questa attività. Controlliamo anche la fattibilità di ogni strategia in termini di qualità dell'annotazione e costi per gli annotatori. Troviamo che PRC ha la percentuale più alta di dissonanza e funziona meglio per le classi rare. Tuttavia, gli annotatori trovano gli esempi difficili. In sintesi, troviamo che PRC è una semplice strategia di active learning per l'acquisizione delle classi rare e l'avvio del processo di active learning con un compito di transfer learning opportunamente progettato, che aiuta significativamente. Troviamo anche che l'aggiornamento iterativo è utile per il transfer learning da un dominio diverso, mentre le annotazioni attive nel dominio beneficiano dell'aggiornamento cumulativo. Questi sono i link al nostro dataset principale e al nostro articolo. Non esitate a contattarci se avete domande. Grazie.</sample>
    <sample id="356">Gli autori dell'articolo sono Matthias Lindemann, Alexander Koller e Ivan Titov. Matthias Lindemann è il relatore, mentre Alexander Koller e Ivan Titov sono i suoi advisor.</sample>
    <sample id="357">Il nome della relatrice è Siyu Yuan.</sample>
    <sample id="358">L'articolo è stato realizzato in collaborazione con 5 autori: Patrick Fernandes, Emmy Liu, André F. T. Martins, Graham Neubig e Kayo Yin.</sample>
    <sample id="359">L'approccio EDAtt viene confrontato con l'architettura state-of-the-art specificamente progettata per la traduzione simultanea, nota come SimulST.</sample>
    <sample id="361">Armineh Nourbakhsh, dottoranda presso l'Institute of Language Technologies dell'Università Carnegie Mellon e direttrice di ricerca presso l'AI Research di JP Morgan, presenta il lavoro "CounterComp", un approccio innovativo per migliorare la generalizzazione composizionale nei compiti di ragionamento quantitativo multi-step. I modelli neurali di ultima generazione mostrano una scarsa performance su questi compiti, soprattutto quando richiedono più di due passaggi logici, a causa di pattern spuri appresi durante l'addestramento. Per risolvere questo problema, CounterComp introduce un'auxiliary loss basata su scenari contrari (counterfactual), estratti dal dataset di addestramento. Questi scenari vengono utilizzati per generare esempi positivi e negativi, dove un intervento nel testo della domanda non modifica o modifica l'output, rispettivamente. L'obiettivo è guidare il modello a focalizzarsi su token rilevanti per le operazioni matematiche. L'approccio migliora significativamente le prestazioni su dati in-distribution e out-of-distribution, dimostrando una maggiore capacità di generalizzazione composizionale. Inoltre, i risultati qualitativi mostrano che il modello impara a concentrarsi su token più significativi durante la generazione delle operazioni. Il lavoro è stato presentato in un poster e offre un promettente contributo per migliorare la comprensione e l'elaborazione del ragionamento numerico nei modelli di linguaggio.</sample>
  </task>
</testset>