<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">Le principali fonti di dati per i modelli linguistici sono i dati di crawling del web, come il corpus C4, che includono testate giornalistiche come New York Times, Los Angeles Times, The Guardian e Huffington Post, nonché contenuti da social media.</sample>
    <sample id="1">Gli autori dell'articolo sono affiliati all'Università di McGill, a Mila e a Microsoft Research.</sample>
    <sample id="2">Tu Yi da Ant Group presenta il lavoro del team su un modello pre-addestrato per l'Understanding dei Documenti Ricchi di Immagini (Visually-rich Document Understanding - VrDU). Il modello proposto, chiamato LayoutMask, mira a risolvere i problemi legati all'ordine di lettura nei documenti, un aspetto critico nei modelli esistenti che utilizzano posizioni globali 1D. LayoutMask introduce invece posizioni locali 1D, combinando informazioni di layout e testo per migliorare l'interazione tra testo e layout. Il modello utilizza due nuove strategie di mascheramento (Whole Word Masking e Layout-Aware Masking) e un nuovo obiettivo di pre-addestramento, Masked Position Modeling, per promuovere l'apprendimento delle relazioni spaziali e semantiche. Gli esperimenti mostrano che l'uso di posizioni locali 1D migliora le prestazioni su dataset come FUNSD e SROIE, anche se presenta alcune limitazioni su CORD. In particolare, il modello riesce meglio a riconoscere entità come "Total" in layout complessi, dove l'ordine di lettura globale non è sufficiente. Il lavoro sottolinea l'importanza di integrare informazioni spaziali e semantiche per migliorare l'interpretabilità dei documenti.</sample>
    <sample id="3">Ciao! Benvenuti alla nostra presentazione di DEPLAIN, un nuovo corpus per l'identificazione di testi in tedesco a livello di documento e a livello di frase. Il mio nome è Regina Stodden, e vi guiderò attraverso la prima parte della presentazione. Iniziamo definendo la semplificazione del testo. La semplificazione del testo è un processo di adattamento di un testo per migliorare la sua comprensione da parte di un gruppo specifico, come ad esempio persone con problemi di lettura o parlanti non madrelingua. Per addestrare un modello di semplificazione del testo, abbiamo bisogno di coppie parallele di testi, ad esempio documenti o frasi. Nell'esempio qui sopra, potete vedere una coppia di frasi allineate in modo parallelo: una frase tedesca complessa e la sua traduzione in linguaggio semplice. Per semplificare la frase, sono possibili diverse tecniche, come si può vedere nell'esempio, ad esempio sostituzione lessicale, eliminazione di clausole, riassegnamento di ordine o inserimento di parole. Ora proponiamo il nostro nuovo corpus, DEPLAIN, poiché negli ultimi anni ci sono state alcune problematiche con i corpus esistenti. Ad esempio, questi corpus sono troppo piccoli per addestrare un modello di semplificazione del testo. Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono presentare errori nell'allineamento. Per questo motivo proponiamo il nostro nuovo corpus, DEPLAIN, che è diviso in due sottocorpus: DEPLAIN-apa e DEPLAIN-web. DEPLAIN-apa si basa su testi di notizie. In DEPLAIN-apa, abbiamo allineato manualmente 483 documenti, ottenendo circa 13.000 coppie di frasi parallele. Per quanto riguarda DEPLAIN-web, questo corpus include diversi domini e abbiamo allineato manualmente e con metodi automatici tutti i 750 documenti. In totale, otteniamo 30.450 coppie di frasi. Abbiamo analizzato un po' di più le nostre coppie di frasi, ad esempio in base al tipo di semplificazione. Come si può vedere qui, i testi biblici sono semplificati molto di più rispetto, ad esempio, ai testi di notizie o ai testi per apprendenti della lingua. A tutti i livelli, riguardo ad esempio alla semplificazione lessicale, strutturale o al livello generale di semplificazione. Inoltre, si può notare che il nostro corpus DEPLAIN presenta un'alta varietà di trasformazioni di semplificazione. Ad esempio, nel sottocorpus DEPLAIN-apa abbiamo molte più riassegnazioni di ordine e aggiunte di parole rispetto a quanto non accade nel sottocorpus DEPLAIN-web. Dall'altra parte, nel corpus web abbiamo molte più riformulazioni. Ora vediamo cosa possiamo fare con questo corpus. Ciao, sono Omar e ora parlerò dei casi d'uso per il nostro dataset DEPLAIN. Il primo caso d'uso è valutare i metodi di allineamento automatico. Negli ultimi anni sono stati proposti molti metodi di allineamento, ma nel contesto della traduzione automatica, dove abbiamo due documenti paralleli scritti in due lingue diverse e vogliamo estrarre gli allineamenti delle frasi in entrambi i documenti. Tuttavia, nel nostro caso d'uso, stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli scritti nella stessa lingua, con lo stesso contenuto, ma a diversi livelli di complessità. Ora che abbiamo il nostro dataset DEPLAIN, con frasi allineate manualmente, possiamo utilizzare queste frasi come standard d'oro per valutare alcuni dei metodi proposti di allineamento. Abbiamo apportato alcune modifiche ai metodi proposti e abbiamo pubblicato tutte queste modifiche e i codici per eseguire i nostri esperimenti nell'articolo. Alla fine, abbiamo concluso che il miglior metodo automatico di allineamento da utilizzare per la semplificazione del testo in tedesco è il metodo MASSalign. Potete trovare anche il codice per utilizzare questo metodo sui vostri documenti nell'articolo. Il secondo caso d'uso che abbiamo presentato nel nostro articolo è un caso di semplificazione automatica del testo tramite il fine-tuning di modelli linguistici per produrre testi semplificati a partire da testi complessi. Abbiamo fine-tunato due diversi modelli. Abbiamo fine-tunato il modello long-mBART per produrre semplificazioni a livello di documento e abbiamo anche fine-tunato il modello base mBART per produrre semplificazioni a livello di frase. Potete trovare anche tutti i checkpoint e potete approfondire i dettagli sui punteggi e sulle metriche di valutazione dei nostri esperimenti nell'articolo. Abbiamo concluso che questo fine-tuning base può produrre punteggi migliori rispetto ai punteggi di base e abbiamo proposto questi risultati come benchmark iniziale per il problema della semplificazione automatica del testo in futuro. Grazie mille per l'attenzione e speriamo di incontrarvi tutti durante il convegno. Grazie.</sample>
    <sample id="4">Il nome della relatrice è Kayo Yin.</sample>
    <sample id="5">Il modello utilizzato per ottenere un'accuratezza dell'82%-87% è il T5 XL.</sample>
    <sample id="6">In questo lavoro, Jiaan e il suo team presentano un approccio innovativo per l'estratto di testi in più lingue, unificando i concetti di summarization multilingue e cross-lingue in un'impostazione più generale chiamata "many-to-many summarization". Questo approccio mira a sviluppare un unico modello in grado di elaborare documenti in qualsiasi lingua di origine e generare il loro riassunto in qualsiasi lingua di destinazione. I ricercatori hanno condotto esperimenti preliminari sull'insieme di dati WikiLingua, confrontando diversi modelli, tra cui mBART-50, e hanno dimostrato che l'approccio many-to-many permette un miglior trasferimento di conoscenze tra lingue rispetto ai metodi precedenti. Inoltre, hanno proposto PISCES, un modello pre-addestrato per la summarization many-to-many, addestrato in tre fasi: meta pre-training, cross-lingual pre-training e pre-training specifico del compito. I risultati sperimentali mostrano che PISCES supera diversi baselines, come mBART-50 e mT5, e gli studi umani confermano la sua superiorità. Questo lavoro apre la strada a nuove possibilità per l'elaborazione del linguaggio naturale in contesti multilingue.</sample>
    <sample id="7">Sì, i tagger CoNLL-2003 funzionano ancora bene nel 2023, soprattutto se si utilizzano architetture moderne come i transformer, modelli più grandi e un adeguato numero di esempi per il fine-tuning. Il calo di prestazioni osservato è principalmente dovuto al "temporal drift" (degradazione temporale), non all'overfitting adattivo.</sample>
    <sample id="8">The novelty of the proposed human evaluation method, ABC-Eval, is that it reduces subjectivity by explicitly annotating whether model responses express certain behaviors, such as irrelevant information or contradictions, rather than relying on holistic judgments. This approach allows for a more precise and reliable assessment of chat models by measuring specific thematic errors and capturing distinct aspects of chat quality.</sample>
    <sample id="9">Il successo dell'attuale approccio scarsamente supervisionato (WSL) si basa in larga misura sull'utilizzo di dati di validazione puliti, anche se spesso questa necessità viene trascurata.</sample>
    <sample id="10">Per migliorare il punteggio, possono essere sviluppati modelli in grado di recuperare e utilizzare in modo efficace il contesto e le informazioni di background, simili a quelle disponibili per gli annotatori, e migliorare la comprensione delle espressioni di riferimento indirette attraverso l'apprendimento di relazioni semantiche e contestuali più precise.</sample>
    <sample id="11">Jack Hessel, un ricercatore presso l'AI2, presenta un lavoro congiunto con collaboratori di diversi istituti, che esplora la capacità dei modelli linguistici di grandi dimensioni di comprendere l'umorismo. L'obiettivo è valutare se questi modelli siano in grado di capire e spiegare gli scherzi, in particolare attraverso il contesto delle didascalie dei cartoni di *The New Yorker*. Il team ha creato tre compiti: matching (identificare la didascalia corretta), quality ranking (valutare la qualità delle didascalie) e explanation generation (spiegare perché una didascalia è divertente). I risultati mostrano che i modelli, anche quelli avanzati come GPT-4, non raggiungono l'efficacia umana, con un'accuratezza intorno al 62% nel compito di matching, rispetto al 94% degli umani. Anche con descrizioni umane degli immagini, i modelli non riescono a spiegare correttamente gli scherzi, come evidenziato da test umani. Il lavoro include un dataset annotato e un leaderboard per incoraggiare ulteriori ricerche in questo campo. L'obiettivo è stimolare una migliore comprensione dell'umorismo da parte degli AI, un tema cruciale per lo sviluppo di modelli più intelligenti e sensibili.</sample>
    <sample id="12">L'articolo è stato realizzato da 5 autori: Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow.</sample>
    <sample id="13">In his presentation, Daniel Rotem introduces his research titled "Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings," conducted at the Hebrew University under Professor Roy Schwartz. The work explores adaptive inference methods—specifically Multi Model and Early Exit—used to reduce inference costs in large language models. Multi Model uses separate models with classifiers to decide when to stop computation, while Early Exit integrates classifiers at different transformer layers, sharing model parameters. However, Early Exit suffers from conflicting gradients, where different classifiers interfere with each other during training, leading to lower performance. Rotem’s experiments show that Multi Model classifiers outperform Early Exit ones by up to 2.3% on average, especially in early layers. To address this issue, the team proposes SWEET (Separating Weights in Early Exit Transformers), a novel fine-tuning method that prevents conflicting gradients by allowing each layer to be updated only by its corresponding classifier. Results show that SWEET significantly closes the performance gap between Early Exit and Multi Model, especially at high inference speeds. The study highlights the existence of conflicting gradients in Early Exit, presents the first fair comparison between adaptive methods, and introduces SWEET as a promising approach for improving Early Exit architectures.</sample>
    <sample id="14">Ciao, il mio nome è Adam Przepiórkowski e questo intervento è sulla struttura dipendente della coordinazione. Come probabilmente sapete, esistono diverse strutture dipendenti assunte da diverse teorie e approcci corpus. Ad esempio, nelle Universal Dependencies, la struttura della coordinazione, come "Lisa, Bart e Maggie", è tale che il primo congiunto è il capo dell'intera struttura coordinata. In questo caso, Lisa. Un approccio simile è adottato da Igor Mel'čuk nella sua teoria del significato e del testo, dove anche qui l'intera struttura coordinata è guidata dal primo congiunto. Questi due approcci sono asimmetrici. Sì, ne selezionano uno dei congiunti. Dall'altro lato, ci sono approcci asimmetrici alla struttura coordinata, come l'approccio praghese. L'approccio "congiunzione come capo" adottato nei treebank di dipendenza praghese, dove le strutture coordinate sono guidate dalla congiunzione stessa. Otteniamo così alcune dipendenze che vanno dalla congiunzione a tutti i congiunti. Infine, c'è anche un approccio a più capi, utilizzato ad esempio nella Word Grammar di Hudson, dove si afferma che tutti i congiunti siano capi della struttura coordinata. Otteniamo quindi dipendenze dal governatore, qui "lo ama" a tutti i congiunti separatamente: Lisa, Bart e Maggie. L'obiettivo di questo lavoro è presentare un nuovo argomento a favore delle strutture simmetriche della coordinazione, come queste due, e contro le strutture asimmetriche della coordinazione, come queste due. L'argomento si basa sul principio di minimizzazione della lunghezza delle dipendenze che spiegherò attraverso questi esempi. In inglese, come sai, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli adjunct possono essere più lontani. Quindi "Marge ha letto it ieri" è accettabile perché l'oggetto diretto è vicino al verbo, mentre "Marge ha letto ieri it" è molto peggio. Giusto? Perché qui tra il verbo e l'oggetto diretto c'è un adjunct: "ieri". Tuttavia, questo effetto può essere attenuato quando l'oggetto diretto è molto pesante e molto lungo. In quel caso, può essere spostato dopo l'adjunct. Questo è illustrato qui. Entrambi questi esempi sono accettabili. "Marge ha letto questo libro assolutamente affascinante sulle api ieri." È ok invece di "it", abbiamo questo NP lungo. Ma è anche accettabile dire "Marge ha letto ieri questo libro assolutamente affascinante sulle api." La ragione qui è che è possibile perché, pur se questa frase viola il principio grammaticale generale per cui gli oggetti diretti dovrebbero essere vicini al verbo, soddisfa il principio di minimizzazione della lunghezza delle dipendenze, che afferma che si preferiscono dipendenze più corte. Questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quelle che non sono costanti in queste due strutture. Qui abbiamo una dipendenza da "read" all'adjunct di lunghezza 7 misurata in parole e da "read" a "book" di lunghezza 4, quindi in totale 11. Quando scambiamo questi due costituenti, la somma di queste due dipendenze diventa 6. Invece di 11, 6 è molto più breve. Per questo motivo suona abbastanza bene. Giusto? Viola un principio, ma soddisfa un altro. Okay. Quindi, ciò che abbiamo fatto è estratto varie statistiche sulla coordinazione dalla versione migliorata del Penn Treebank e vedi il paper "Perché non usi le Universal Dependencies" e queste statistiche confermano un'osservazione fatta molte volte prima, ovvero che i congiunti a sinistra tendono a essere più brevi. Ad esempio, "sale e pepe" e non "pepe e sale", misurati in sillabe. Inoltre, l'osservazione fatta nell'analisi sintattica che questa tendenza cresce con la differenza di lunghezza. Quindi, quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto più breve preferisce essere il primo, in modo più marcato, giusto? Quindi la proporzione è maggiore del congiunto breve a sinistra. Ma ciò che è nuovo in questo lavoro è che abbiamo osservato che questa tendenza si verifica solo quando il governatore è a sinistra o assente. Giusto? Il governatore è a sinistra in questo esempio "I saw Bart and Lisa", quindi il governatore è a sinistra. È assente nell'esempio successivo "Homer came and sneezed." Qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno. In questi casi, il congiunto a sinistra preferisce essere più breve; il più grande differenza tra i due congiunti. Tuttavia, quando il governatore è a destra, come qui, "laughed" governa la coordinazione "Ted and Ned", questo effetto scompare. Lo abbiamo mostrato misurando la lunghezza in caratteri, la prima colonna, in sillabe la colonna centrale, e in parole la colonna destra. Io concentrerò l'attenzione sulla colonna destra. Quello che vediamo qui è che quando il governatore è a sinistra, la tendenza del congiunto a sinistra ad essere più breve cresce costantemente con la differenza assoluta in parole, e lo stesso è osservato quando non c'è un governatore, come nella coordinazione di frasi. Ma quando il governatore è a destra, questa tendenza scompare. E mostriamo nel paper come questo fornisce un argomento contro le strutture asimmetriche della coordinazione, come queste due, e a favore delle strutture simmetriche, come queste due. Per ulteriori argomenti, consulta il paper. Parla con noi durante la sessione del poster. Grazie.</sample>
    <sample id="15">L'articolo è stato scritto da tre autori: Matthias Lindemann, Alexander Koller e Ivan Titov.</sample>
    <sample id="16">I testi biblici risultano più semplificati rispetto ai testi di notizie o ai testi per apprendenti della lingua.</sample>
    <sample id="17">Shengqiong Wu, una dottoranda all'NUS, presenta un lavoro su estrazione relazionale multimodale. L'obiettivo è determinare le relazioni semantiche tra entità utilizzando non solo testo ma anche informazioni visive, come in ambienti reali come i social media. Tuttavia, l'uso di informazioni visive presenta due problemi: sovroutilizzo delle informazioni interne e sottoutilizzo di quelle esterne. Per risolverli, l'approccio proposto utilizza un principio di "Graph Information Bottleneck" per filtrare e ottimizzare le informazioni interne, e integra informazioni topic multimediali per arricchire il contesto. Il framework proposto include la rappresentazione di testo e immagini tramite grafi di scena, la creazione di un grafo cross-modale unificato (CMG), il raffinamento del CMG attraverso filtraggio fine-grained, e l'integrazione di feature topic multimediali. Gli esperimenti su dataset MRE mostrano miglioramenti significativi rispetto ai metodi basati solo sul testo e ai baselines multimodali. Lo studio mostra che il filtraggio interno è utile per dati ad alta rilevanza testo-immagine, mentre l'uso di informazioni esterne è più efficace quando la rilevanza è bassa. In sintesi, il lavoro introduce un nuovo approccio di sottrazione e aggiunta simultanea di informazioni per migliorare l'estrazione relazionale multimodale.</sample>
    <sample id="18">L'esempio della preferenza per i congiunti a sinistra più brevi è "salt and pepper" rispetto a "pepper and salt", dove il congiunto a sinistra ("salt") è più breve del congiunto a destra ("pepper"). Questa tendenza è confermata da statistiche linguistiche che mostrano che, quando la differenza di lunghezza tra i congiunti aumenta, il congiunto più breve preferisce essere a sinistra.</sample>
    <sample id="19">Zhang Qin, una studentessa del master presso l'Università di Shenzhen, presenta il lavoro "A Survey for Efficient Open Domain Question Answering", accettato all'ACL 2023. L'articolo si concentra sui sistemi di risposta a domande in dominio aperto, che tipicamente utilizzano un framework a due stadi: un sistema di recupero che estrae contesti da un corpus di Wikipedia e un lettore che genera la risposta. Tuttavia, tali sistemi affrontano sfide come la dimensione elevata del corpus (26 milioni di documenti), la velocità di ricerca dell'indice e l'uso di modelli linguistici pesanti. Lo scopo del lavoro è sviluppare sistemi più efficienti, con minori costi di memoria, inferenza più veloce e prestazioni comparabili. Vengono esaminati diversi approcci, tra cui sistemi a un solo stadio (recupero solo o generatore solo), tecniche per accelerare il recupero (come la ricerca approssimata), ridurre la lettura (con skip reading) e comprimere l'indice. Vengono anche confrontati diversi modelli esistenti in termini di velocità, memoria e prestazioni. Le conclusioni suggeriscono che, a seconda delle risorse disponibili, si possa optare per sistemi a un solo stadio o per modelli più compatti. Infine, vengono proposti due possibili futuri sviluppi: l'implementazione su dispositivi a basso consumo energetico e l'uso di metriche di valutazione più complete.</sample>
    <sample id="20">Sì, puoi usare i modelli per la tua ricerca. Tutti i modelli pre-addestrati ottenuti da NACHOS sono disponibili gratuitamente su Hugging Face, sotto la licenza MIT, e gli script di addestramento sono disponibili sul repository GitHub.</sample>
    <sample id="21">DEPLAIN-apa contiene testi di notizie (news texts).</sample>
    <sample id="22">I fattori che contribuiscono a una buona generalizzazione sono:  
1. **Architettura del modello** (i modelli Transformer generalizzano meglio).  
2. **Dimensione del modello** (modelli più grandi generalizzano meglio).  
3. **Numero di esempi di fine-tuning** (più esempi portano a una migliore generalizzazione).</sample>
    <sample id="23">Dan Garrette discute i problemi che i modelli di generazione immagini basati su testo, come Imagen, incontrano nel rappresentare correttamente il testo all'interno delle immagini. Nonostante tali modelli siano in grado di produrre immagini di alta qualità, spesso falliscono nel riprodurre testo, soprattutto parole semplici. Questo problema è legato all'uso del T5-XXL come encoder del testo, che utilizza il tokenizzazione SentencePiece, riducendo il testo in token subword. Questo rende difficile per il modello ricostruire correttamente le parole, poiché deve decomporre i token in singoli caratteri. Sperimentazioni mostrano che T5 non è bravo a "spellare" le parole, anche se i modelli più grandi come PaLM lo fanno meglio, ma sono troppo grandi per essere pratici. ByT5, che utilizza tokenizzazione a livello di byte, riesce invece a rappresentare correttamente il testo in modo efficiente. Per migliorare Imagen, i ricercatori hanno aggiunto al testo codificato da T5 una rappresentazione aggiuntiva generata da ByT5, ottenendo un miglioramento significativo nella capacità di generare immagini con testo leggibile. Il lavoro introduce anche due nuovi benchmark, WikiSpell e DrawText, e una strategia efficiente per migliorare la capacità di spelling dei modelli.</sample>
    <sample id="24">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata in base alla lunghezza in parole, sillabe e caratteri, analizzando i dati estratti dall'Enhanced Penn Treebank. I risultati mostrano che quando il governatore è a sinistra o assente, il congiunto a sinistra tende a essere più breve, e questa tendenza aumenta con la differenza di lunghezza tra i due congiunti.</sample>
    <sample id="25">Gli esperimenti sono stati progettati analizzando statistiche estratte dall'enhanced version of the Penn Treebank, focalizzandosi sulla posizione del governatore (a sinistra, assente o a destra) e misurando la lunghezza dei congiunti in caratteri, sillabe e parole. Questo ha permesso di osservare come la tendenza del congiunto di sinistra a essere più breve vari in base alla posizione del governatore.</sample>
    <sample id="26">Un classificatore base addestrato su dati non bilanciati, come nel caso della dissonanza cognitiva (presente solo in 3.5% dei casi), non è molto efficace e performa appena meglio del caso casuale.</sample>
    <sample id="27">L'articolo non specifica il numero di autori coinvolti.</sample>
    <sample id="28">I nomi dei personaggi nella conversazione presa a esempio sono Bob e Alice.</sample>
    <sample id="29">I modelli di traduzione sensibili al contesto migliorano rispetto a quelli indipendenti nel gestire fenomeni del discorso come la formalità e la coesione lessicale.</sample>
    <sample id="30">Il paper "LLM-Blender" presenta un framework semplice ed efficace per l'apprendimento ensemble di modelli linguistici di grandi dimensioni, basato su confronti a coppie e fusione generativa. Il team, proveniente da AI2 e USC, osserva che, sebbene alcuni modelli siano superiori in termini di prestazioni medie, il modello migliore può variare in base all'input specifico. Per affrontare questo problema, propongono LLM-Blender, un framework a due stadi: nel primo, un modulo chiamato PairRanker confronta a coppie le risposte di diversi modelli, utilizzando un'architettura di attenzione incrociata per determinare quale risposta è migliore per un dato input. I risultati vengono aggregati in una matrice di confronti, da cui si ottiene un ordine finale. Nel secondo stadio, i top K risultati vengono fusi da un modello generativo, chiamato GenFuser, che produce l'output finale. Il PairRanker supera metodi precedenti grazie al confronto diretto tra candidati, migliorando la correlazione con il ranking ideale. Per valutare il framework, i ricercatori hanno creato il dataset MixInstruct, utilizzando metriche automatiche come BERTScore e BLUERT, nonché giudizi di ChatGPT. I risultati mostrano che LLM-Blender supera modelli come Vicuna e Open Assistant in una percentuale significativa di casi. In sintesi, LLM-Blender è un framework promettente per migliorare le prestazioni degli LLM attraverso un'ensemble semplice ma efficace.</sample>
    <sample id="31">Gli autori dell'articolo sono affiliati all'Università di Harvard, al MIT, all'Università di Stanford e all'Università di Washington.</sample>
    <sample id="33">Il framework NLPositionality quantifica la posizionalità confrontando le annotazioni di utenti diversi (considerando le loro caratteristiche demografiche) con quelle dei modelli e dei dataset, utilizzando il coefficiente di correlazione di Pearson (Pearson's R) per misurare il grado di allineamento tra le valutazioni umane e quelle dei sistemi NLP.</sample>
    <sample id="34">Marcos Treviso presenta il lavoro "CREST: A Joint Framework for Rationalization and Counterfactual Text Generation", sviluppato in collaborazione con Alexis Ross, Nuno Guerreiro e André Martins. L'obiettivo è combinare due approcci per l'interpretazione dei modelli di classificazione: la razionalizzazione selettiva e la generazione di esempi contrari (counterfactuals). CREST utilizza un modello razionalizzatore che genera un'esplicazione (rationale) per l'input originale e, successivamente, un editor che modifica parte dell'input per creare un esempio contrariante. Questi esempi vengono valutati da umani e automaticamente, mostrando che CREST genera esempi contrari più validi e naturali rispetto ad altri metodi. Inoltre, CREST viene utilizzato per migliorare le performance dei modelli downstream attraverso l'augmentazione dei dati e una nuova tecnica di razionalizzazione che sfrutta sia esempi fattuali che contrari. Gli esperimenti su dataset come IMDB e SNLI dimostrano che CREST-Rationalization supera gli altri metodi, specialmente su dati fuori dal dominio. Infine, le razionalizzazioni generate da CREST sono più plausibili e simulabili, specialmente in termini di capacità di influenzare la decisione del classificatore. Il lavoro propone quindi un framework efficace per generare esempi contrari e spiegazioni interpretabili.</sample>
    <sample id="36">**Abstract:**  
Nel lavoro "Learning Language-Specific Layers for Multilingual Machine Translation", Telmo Pessoa Pires e i suoi collaboratori propongono un approccio innovativo per migliorare la traduzione multilingue mantenendo costi di inferenza ridotti. L’obiettivo è aumentare la capacità per ogni lingua solo dove è necessario, senza aumentare il costo complessivo. La soluzione proposta è l’uso di "Language-Specific Layers" (LSLs), strati del modello dedicati a una singola lingua, selezionati in base alla lingua sorgente o target durante l’inferenza. Questo permette di ignorare i pesi non rilevanti, riducendo il tempo di calcolo. Per decidere dove posizionare le LSLs, il modello impara automaticamente la struttura ottimale attraverso l’analisi dei pesi durante l’addestramento, selezionando i componenti con i pesi più elevati. I risultati sperimentali mostrano significativi miglioramenti rispetto ai modelli tradizionali e agli adattatori linguistici, specialmente per le lingue a risorse limitate. I test sono stati condotti su diversi dataset, tra cui WMT21 e Flores-101, utilizzando metriche come chrF, spBLEU e COMET. L’architettura proposta, basata su un encoder profondo e un decoder superficiale, dimostra un’efficacia elevata, con miglioramenti statisticamente significativi in 84 su 90 direzioni di traduzione. Questo lavoro apre la strada a modelli multilingue più efficienti e adatti a contesti con risorse linguistiche limitate.</sample>
    <sample id="37">The previous study found that when human subjects were given the same persona prompts, they also surfaced racial stereotypes.</sample>
    <sample id="38">L'unico dato utilizzato nello studio è il Penn Treebank.</sample>
    <sample id="39">L'autore dell'articolo è Adam Przepiórkowski. Non vengono menzionati altri autori.</sample>
    <sample id="40">Le attività strettamente correlate alla dissonanza cognitiva sono: 

1. **Classificazione della posizione di dissonanza indipendente dal tema (topic-independent dissonance stance classification)** – determina se due affermazioni da persone diverse sono in accordo o in disaccordo, indipendentemente dal tema.  
2. **Classificazione binaria delle relazioni di espansione e confronto del PDTB (CE)** – queste relazioni sono strettamente correlate alle concezioni di consonanza e dissonanza.</sample>
    <sample id="41">Silin, dell'EPFL Natural Language Processing Lab, presenta il lavoro "PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives", sviluppato in collaborazione con Sony. L'obiettivo è migliorare la coerenza e l'interesse nei dialoghi e nelle narrazioni, integrando conoscenze comuni basate su personaggi e ruoli. PeaCoK è un grafo del sapere che rappresenta circa 3.800 personaggi e 40.000 attributi, con 100.000 fatti e interconnessioni tra personaggi. Costruito in tre fasi, utilizza grafi esistenti, modelli linguistici pre-addestrati e annotazioni umane con l'aiuto di un modello AI. Lo studio dimostra che PeaCoK migliora la capacità dei modelli linguistici di generare conoscenze personali e di produrre dialoghi più coesi e coinvolgenti. Rispetto ad altri grafi, PeaCoK si distingue per il focus su personaggi specifici e le loro relazioni, portando a un miglioramento significativo in termini di fluidezza, coerenza e espressione personale nei dialoghi. I risultati suggeriscono che PeaCoK è una risorsa utile per addestrare modelli di generazione del linguaggio e migliorare l'interazione narrativa. Il lavoro è pubblicato e disponibile online.</sample>
    <sample id="42">The text does not specify the number of authors involved in the paper.</sample>
    <sample id="43">Il testo non specifica il numero di autori coinvolti nell'articolo.</sample>
    <sample id="44">Il framework NLPositionality differisce dai lavori precedenti confrontando le annotazioni degli utenti finali con quelle dei modelli e dei dataset, anziché concentrarsi solo sull'accordo tra annotatori o sulla modellazione delle loro distribuzioni.</sample>
    <sample id="45">La configurazione che si sovrappone maggiormente al lessico degli stereotipi è quella delle **persone generate** (generated personas), poiché contengono un tasso molto più elevato di parole stereotipate rispetto alle risposte umane.</sample>
    <sample id="46">I sistemi commerciali messi a confronto sono DeepL e Google Translate.</sample>
    <sample id="47">Ciao, mi chiamo Shangbin, dottorando all'Università di Washington. Oggi sto presentando il nostro lavoro intitolato "Da dati di preaddestramento a modelli linguistici a compiti downstream: tracciare le tracce delle bias politici che portano a modelli NLP non equi". I modelli linguistici vengono addestrati su grandi quantità di dati estratti da internet. Le notizie politiche di media come il New York Times, il Los Angeles Times, The Guardian, Huffington Post e così via, sono ben rappresentate nei dati di preaddestramento. Secondo un'indagine sul corpus C4, possiamo vedere che questi media sono ampiamente coperti nei dati di addestramento dei modelli linguistici. Questo ha creato una situazione a doppio taglio per le applicazioni dei modelli linguistici. Da un lato, sono in grado di apprendere da prospettive diverse, celebrando la democrazia e la pluralità di idee. Dall'altro lato, queste diverse opinioni politiche sono inherentemente socialmente biasate e potrebbero portare a problemi di equità in applicazioni downstream.

A questo scopo, proponiamo di investigare la propagazione del bias politico dal dati di preaddestramento ai modelli linguistici e ai compiti downstream, ponendo specificamente le seguenti domande: Prima, come possiamo valutare l'inclinazione politica dei modelli linguistici e qual è il ruolo che i dati di preaddestramento potrebbero avere in tali bias politici? Secondo, come si comportano effettivamente i modelli linguistici con diverse inclinazioni politiche nei compiti downstream e se questo potrebbe generare problemi di equità nelle applicazioni NLP?

Specificamente, abbiamo prima proposto di promuovere i modelli linguistici con diversi formati di prompt utilizzando questionari politici come il test di conferenza politica. Questo ci permette di effettuare valutazioni automatiche ben fondate sulla letteratura scientifica politica. Alcuni risultati preliminari dimostrano che i modelli linguistici hanno diverse inclinazioni politiche. Occupano tutti e quattro i quadranti del campus politico. Possiamo anche notare che GPT-4 è il modello linguistico più liberale tra tutti, e la serie GPT è in generale più socialmente liberale rispetto alla serie BART e alle sue varianti.

In secondo luogo, miriamo a investigare fino a che punto i bias politici dei modelli linguistici vengono effettivamente acquisiti dai dati di addestramento. Possiamo condurre un esperimento controllato addestrando ulteriormente i checkpoint dei modelli linguistici su sei diversi corpus partisani, suddivisi in notizie e media sociali, ulteriormente divisi per inclinazione politica. Addestrando ulteriormente i modelli linguistici su tali corpus partisani, possiamo vedere che anche le coordinate ideologiche del modello si spostano corrispondentemente. Per esempio, per RoBERTa addestrata ulteriormente su un corpus di Reddit di orientamento di sinistra, possiamo notare un notevole spostamento verso sinistra in termini di bias politici.

Inoltre, cerchiamo di investigare se i modelli linguistici possano acquisire la polarizzazione che è diffusa nella nostra società moderna. Dividiamo i corpus di preaddestramento in due periodi: prima e dopo la presidenza del 45° presidente degli Stati Uniti. Addestriamo separatamente i modelli linguistici sui due diversi corpus temporali. Possiamo notare che i modelli linguistici in generale avevano un orientamento politico più distante dal centro dopo il 2017. Questo indica che i modelli linguistici possono anche acquisire la polarizzazione nella nostra società.

Infine, valutiamo i modelli linguistici con diverse inclinazioni politiche per la rilevazione dell'odio e della notizia falsa, applicazioni NLP che spesso coinvolgono modelli linguistici e possono avere implicazioni molto significative. Possiamo vedere che se investighiamo le prestazioni per categoria, cioè se separiamo le prestazioni in diverse demografie o inclinazioni politiche dei media, possiamo notare un pattern. Per esempio, per la rilevazione dell'odio, i modelli linguistici di sinistra sono meglio in grado di rilevare l'odio rivolto a gruppi sociali minoritari, ma peggio in grado di rilevare l'odio rivolto a gruppi più potenti nella società. Al contrario, i modelli linguistici di destra sono meglio in grado di rilevare l'odio rivolto a bianchi e uomini, ma peggio in grado di rilevare l'odio rivolto a neri, LGBTQ+ e altre comunità minoritarie. Trend simili si verificano anche per la rilevazione delle notizie false, dove vediamo che i modelli linguistici di sinistra sono meglio in grado di rilevare disinformazione proveniente dall'opposto orientamento politico e viceversa.

Mostriamo inoltre molti esempi qualitativi per dimostrare che i modelli linguistici con diverse inclinazioni politiche danno effettivamente previsioni diverse per esempi di odio e disinformazione in base alle loro categorie sociali. Ci sono molti altri esempi nell'appendice per sottolineare ulteriormente che ciò indica un problema di equità molto urgente riguardante i bias politici dei modelli linguistici. Per esempio, se i modelli linguistici di destra venissero sottoutilizzati per la rilevazione dell'odio o della disinformazione e poi implementati su una piattaforma sociale popolare, ciò potrebbe significare che le persone con opinioni politiche opposte potrebbero essere marginalizzate e l'odio rivolto a gruppi minoritari potrebbe diffondersi senza controllo.

Questo ha suonato l'allarme per noi affinché riconoscessimo e affrontassimo i problemi di equità causati dagli orientamenti politici dei modelli linguistici.

Un po' di discussione. Vorremmo anche sottolineare che esponiamo un dilemma unico riguardo ai bias politici dei modelli linguistici. È come stare tra Scilla e Cariddi. Se non saniamo le opinioni politiche nei dati di addestramento dei modelli linguistici, il bias si propagherà dai dati di preaddestramento ai modelli linguistici e ai compiti downstream, creando infine problemi di equità. Se cerchiamo di sanare in qualche modo, rischiamo anche la censura o l'esclusione. E' incredibilmente difficile determinare cosa sia effettivamente neutrale e debba essere conservato nei dati di monitoraggio linguistico. È un po' come il problema del trolley elettrico.

Bene, credo che questo sia tutto per oggi. Grazie per il vostro tempo.</sample>
    <sample id="48">L'autore menziona di lavorare con i colleghi di Google Translate, ma non specifica il numero esatto di autori coinvolti nell'articolo. Pertanto, non è possibile determinare con precisione quanti autori siano coinvolti.</sample>
    <sample id="49">Le valutazioni MPP sono state eseguite fino a un massimo di 1024 token di lunghezza del contesto.</sample>
    <sample id="50">DEPLAIN è un nuovo corpus per la semplificazione del testo in tedesco, suddiviso in due sottocorpi: DEPLAIN-apa, basato su testi giornalistici con 13.000 coppie di frasi allineate manualmente, e DEPLAIN-web, che include diversi domini e 30.450 coppie di frasi, ottenute sia manualmente che con metodi automatici. Il corpus è stato creato per risolvere i limiti dei dati esistenti, come la piccola dimensione e l’errore nei metodi di allineamento automatico. DEPLAIN mostra una varietà di tecniche di semplificazione, con differenze tra i sottocorpi, ad esempio più riorientamenti e aggiunte di parole in DEPLAIN-apa e più riformulazioni in DEPLAIN-web. Il corpus è stato utilizzato per valutare metodi di allineamento automatico, con la conclusione che MASSalign è il migliore per il tedesco. Inoltre, è stato impiegato per addestrare modelli linguistici (come long-mBART e mBART) per la semplificazione automatica, ottenendo risultati superiori ai benchmark. Questo lavoro fornisce un riferimento per futuri studi sulla semplificazione del testo in tedesco.</sample>
    <sample id="51">I domini inclusi nel loro set di dati sono musica, libri e ricette.</sample>
    <sample id="52">La posizionalità è definita come l'insieme di prospettive che una persona possiede a causa della sua demografia, identità e esperienze di vita.</sample>
    <sample id="53">Il nome del relatore è Dawei.</sample>
    <sample id="54">This paper presents a study on cognitive dissonance detection in language, focusing on the challenges posed by the rarity of dissonance examples in discourse. Cognitive dissonance occurs when a person holds conflicting beliefs or actions, and detecting it can help understand decision-making, mental health, and societal polarization. However, dissonance is extremely rare in text, making it difficult to train effective classifiers. The authors created a large-scale annotated dataset of discourse unit pairs, finding dissonance in only 3.5% of cases. To address the rare-class problem, they combine transfer learning with active learning (AL) strategies. They transfer knowledge from related tasks—such as stance classification and PDTB discourse relations—and use AL to iteratively improve the model with minimal annotation effort. A novel Probability-of-Rare-Class (PRC) strategy is proposed to select the most promising dissonance examples for annotation, outperforming other AL methods. Using this approach, they achieve an AUC of 0.75, the best performance reported so far. The study highlights the effectiveness of transfer learning for cold-starting AL and shows that cumulative updates work better for domain adaptation, while iterative updates are more suitable for transfer learning. The paper provides a valuable resource for studying dissonance in language and offers practical insights for handling rare-class classification tasks.</sample>
    <sample id="55">Sì, EDAtt adatta un modello ST offline esistente senza riallineamento o adozione di architetture specifiche per la SimulST.</sample>
    <sample id="56">L'informazione sul numero di autori non è specificata nel contenuto fornito.</sample>
    <sample id="57">No, il modello testato non funziona bene sulla suite di test senza addestramento specifico su KITMUS. Tuttavia, con un addestramento specifico, alcuni modelli riescono a integrare conoscenze da fonti multiple, anche se continuano a incontrare difficoltà nell'utilizzare conoscenze fornite solo all'infereza.</sample>
    <sample id="58">Le tre varianti di KITMUS sono: "Background-Pretrain", "Background-Both" e "Background-Inference".</sample>
    <sample id="59">Yanis Labrak presenta DrBERT, il primo modello pre-addestrato in francese per domini biomedici e clinici, basato su RoBERTa e addestrato su NACHOS, un dataset di dati medici estratti dal web. L'obiettivo è valutare l'efficacia dei dati estratti rispetto a dati clinici anonimizzati e analizzare l'impatto della quantità e della natura dei dati sull'addestramento. Sono stati confrontati sette modelli, tra cui DrBERT e ChuBERT, con diverse dimensioni di dati (da 4 a 7 GB) e strategie di pre-addestramento, inclusi modelli basati su CamemBERT e PubMedBERT. I risultati mostrano che i modelli addestrati da zero (from-scratch) offrono prestazioni migliori in compiti downstream come riconoscimento di entità nominate, classificazione e question answering. I dati eterogenei sembrano più versatili, mentre i dati specializzati non scalano bene. DrBERT ha superato i modelli generici come CamemBERT in 9 su 11 compiti. Tutti i modelli sono disponibili gratuitamente su Hugging Face con licenza MIT e i codici sorgente su GitHub. L'obiettivo è fornire uno strumento open source per il francese nel settore biomedico, con potenziale applicazione in ambiti clinici.</sample>
    <sample id="60">Gli autori dell'articolo sono Javad Hosseini, Filip Radlinski, Silvia Pareti e Annie Louis. Non vengono specificate le loro affiliazioni nell'estratto fornito.</sample>
    <sample id="61">The last research question is: Should we only use the clean samples for validation, or are there better ways to utilize them?</sample>
    <sample id="62">Nitay Calderon, autore principale di un paper ACL, presenta uno studio sistematico sulla distillazione del knowledge per la Generazione del Linguaggio Naturale (NLG), con il supporto di collaboratori da Microsoft e il suo advisor, Roi. Lo studio mira a comprimere i modelli linguistici di grandi dimensioni, mantenendo il loro rendimento, un obiettivo cruciale per l'industria. La distillazione del knowledge trasferisce conoscenza da un modello "insegnante" a uno "studente", utilizzando due approcci principali: distillazione a livello di parola e distillazione a livello di sequenza, quest'ultima basata su pseudo-target generati dal modello insegnante. Lo studio si distingue per l'uso di set di dati realistici, con pochi dati etichettati e molti non etichettati, e per il focus su quattro compiti NLG: sommario, generazione di domande, ragionamento comune e trasferimento di stile. L'approccio principale proposto è "joint-teaching", una tecnica innovativa che combina distillazione a livello di parola e pseudo-target generati da entrambi insegnante e studente, migliorando l'apprendimento e riducendo il bias di esposizione. Lo studio suggerisce un "ricettario" per la distillazione NLG, con risultati promettenti per l'efficienza computazionale e il mantenimento delle prestazioni.</sample>
    <sample id="63">La sensibilità della metrica misura la capacità del modello di produrre output coerenti per lo stesso compito indipendentemente da leggere variazioni nel testo delle istruzioni.</sample>
    <sample id="64">Il nome della relatrice è Jingwei Yi.</sample>
    <sample id="65">Una maggiore sensibilità indica che il modello produce output inconsistenti per lo stesso compito a causa di variazioni lievi nelle istruzioni, quindi suggerisce una performance peggiore.</sample>
    <sample id="66">L'articolo presenta una panoramica sui progressi del deep learning per il ragionamento matematico, un aspetto fondamentale dell'intelligenza umana. Il ragionamento matematico include la capacità di risolvere problemi con operazioni aritmetiche, interpretare diagrammi geometrici e dimostrare teoremi. I metodi di deep learning, come i modelli sequence-to-sequence e sequence-to-tree, sono stati utilizzati per formalizzare queste attività come compiti di generazione di sequenze o strutture ad albero. I modelli linguistici di grandi dimensioni (LLMs) sono stati applicati al ragionamento matematico, con tecniche come il chain-of-thought per guidare il modello attraverso passaggi intermedi. Tuttavia, gli LLMs mostrano limitazioni, come la mancanza di precisione nel ragionamento matematico, che possono essere mitigate con strategie come la self-consistency. Altri approcci, come i program-aided LLMs e Chameleon, integrano strumenti esterni per migliorare le prestazioni. Nonostante i progressi, i modelli continuano a incontrare difficoltà nella generalizzazione, soprattutto con numeri grandi e in contesti non inglesi o di dominio specifico. Il ragionamento matematico in ambienti a basso risorse e in settori come finanza, scienza e medicina rimane una sfida aperta.</sample>
    <sample id="67">Uri discute l'interferenza nei modelli di traduzione multilingue, fenomeno in cui l'apprendimento di una lingua può influenzare positivamente o negativamente altre. L'interferenza è più grave quando i modelli sono piccoli rispetto alla dimensione dei dati, ma si riduce con la scala. L'analisi mostra che la similarità linguistica e il numero totale di lingue non influiscono significativamente sull'interferenza. Gli esperimenti con diversi modelli Transformer e set di dati WMT rivelano che l'interferenza è meno problematica quando si utilizza una quantità sufficiente di dati o si regola correttamente la temperatura di campionamento. Quest'ultima, infatti, permette di migliorare la performance dei modelli multilingue, soprattutto per le lingue a basso risorse. In sintesi, l'interferenza può essere mitigata aumentando la scala del modello e ottimizzando il parametro di temperatura, senza la necessità di algoritmi specializzati. I risultati suggeriscono che la gestione dell'interferenza è più semplice di quanto si pensi, e che piccoli accorgimenti possono portare a significativi miglioramenti nella traduzione multilingue.</sample>
    <sample id="68">Durante il pre-addestramento, ai modelli linguistici vengono messi a disposizione contesti linguistici diversi, tra cui frasi grammaticali e non grammaticali da dataset specifici come BLiMP e SyntaxGym, nonché frasi da domini completamente irrilevanti come Wikipedia. Questi contesti vengono utilizzati per valutare come i modelli giudicano l'accettabilità linguistica in base a sequenze di lunghezza crescente.</sample>
    <sample id="69">In genere, sono necessari circa 20 campioni per classe per raggiungere buone prestazioni in WSL.</sample>
    <sample id="70">Gli autori dell'articolo sono Myra, Esin Durmus e Dan Jurafsky.</sample>
    <sample id="71">Javad Hosseini e i suoi collaboratori hanno presentato il lavoro "Resolving Indirect Referring Expressions for Entity Selection", introducendo il corpus AltEntities. L'obiettivo è comprendere come gli utenti usino riferimenti indiretti per selezionare entità, come canzoni, libri o ricette, in contesti conversazionali. Spesso gli utenti non usano nomi diretti, ma espressioni come "la più recente" o "quella non energetica". Per affrontare questo problema, i ricercatori hanno creato un dataset di grandi dimensioni, con 6.000 domande alternative e 42.000 riferimenti indiretti, raccolti tramite annotazione di massa. Le annotazioni sono state realizzate in un contesto informale, presentato attraverso un cartone animato con tre bolle di dialogo. Gli annotatori ricevono informazioni di background sugli entità e devono selezionarne una usando riferimenti indiretti. I risultati mostrano che i modelli linguistici, come T5 XL, raggiungono un'accuratezza elevata (92-95%) con informazioni complete, ma solo intorno al 60% con solo nomi delle entità. Questo sottolinea l'importanza di migliorare la comprensione delle referenze indirette in sistemi conversazionali e LLM. Il dataset è disponibile online.</sample>
    <sample id="72">È necessario sviluppare nuovi metodi per misurare i bias dell'informazione perché i modelli linguistici, addestrati su dati web molto vasti, possono acquisire e propagare bias politici presenti nei dati di addestramento. Questi bias possono influenzare negativamente le applicazioni downstream, causando problemi di equità, come una maggiore capacità di rilevare hate speech o fake news in base all'orientamento politico, con conseguenti discriminazioni o marginalizzazione di gruppi sociali. Riconoscere e quantificare tali bias è essenziale per affrontare le sfide etiche e garantire un utilizzo più equo e responsabile dei modelli linguistici.</sample>
    <sample id="73">Il nome della relatrice è Akshatha.</sample>
    <sample id="74">Il paper presenta Dense-ATOMIC, un'estensione del knowledge graph ATOMIC, che migliora la copertura delle conoscenze e il numero di percorsi multi-hop. ATOMIC, pur essendo di alta qualità, presenta limitazioni come la mancanza di link B-to-B, A-to-B e A-to-A, riducendo i percorsi multi-hop. Dense-ATOMIC completa questi link attraverso un processo che include la normalizzazione degli eventi, il training del modello Rel-CSKGC e una strategia di completamento intra e inter-clusters. Rel-CSKGC, basato su RoBERTa, predice relazioni tra eventi senza dipendere dalla struttura del grafo, sfruttando informazioni semantiche. I risultati mostrano che Dense-ATOMIC migliora la copertura delle conoscenze e supporta meglio il ragionamento multi-hop, beneficiando anche il modello COMET. L'analisi sperimentale conferma l'efficacia del metodo, con prestazioni superiori rispetto ad approcci basati su traduzione e predizione delle relazioni. Il lavoro apre nuove opportunità per il ragionamento comune-senso in ambiti NLP.</sample>
    <sample id="75">In this presentation, Zheng Yandan introduces Jointprop, a joint semi-supervised learning framework for Named Entity Recognition (NER) and Relation Extraction (RE). Traditional supervised models require extensive labeled data, which is costly and domain-specific. While semi-supervised approaches reduce annotation costs, they often neglect the interdependencies between NER and RE tasks. Jointprop addresses this by modeling the tasks together, leveraging both labeled and unlabeled data through a heterogeneous graph structure. The framework includes four components: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. Span features are derived from contextualized token representations, and a graph is built to capture similarities between data points. Label propagation is performed across the graph, refining pseudo-labels iteratively until convergence. Low-confidence pseudo-labels are filtered out, and the remaining are combined with true labels to retrain the model. Experiments on four datasets show that Jointprop significantly outperforms baseline models in both joint and single-task settings, demonstrating the benefits of integrating NER and RE tasks and exploiting unlabeled data effectively. The results highlight the effectiveness of the proposed joint semi-supervised approach in improving performance with limited labeled data.</sample>
    <sample id="76">L'infrastruttura di propagazione dei bias politici ha un aspetto a tre livelli: dal dati di preaddestramento ai modelli linguistici e poi alle applicazioni downstream. I dati di preaddestramento, ricchi di fonti giornalistiche e social media con opinioni politiche diverse, influenzano il bias nei modelli. Questi bias si riflettono poi nel comportamento dei modelli in compiti come la rilevazione dell'odio e delle notizie false, dove modelli con inclinazioni politiche diverse mostrano prestazioni non equitative. Questo crea problemi di giustizia sociale nelle applicazioni NLP.</sample>
    <sample id="77">This work, a joint effort between Yale University and Microsoft Research, introduces DeFacto, a new dataset designed to enhance the factual consistency of summarization models through human demonstrations and feedback. The dataset is built on the XSum corpus, using summaries generated by the Pegasus model, and includes annotations where human annotators label summaries as factually consistent or not, provide corrected versions, and offer detailed feedback with explanations and supporting evidence. The study proposes three novel natural language generation (NLG) tasks: summary editing, feedback generation, and automatic factual error correction. Experimental results show that models can effectively use human feedback for summary editing, but feedback generation remains a challenge. The automatic correction task benefits from explanation generation, achieving strong performance with limited data. The dataset's fine-grained annotations make it useful not only for evaluating and improving summarization models but also for training factuality metrics and meta-evaluation systems. The DeFacto dataset is publicly available on GitHub, offering a valuable resource for future research on factual consistency in natural language generation.</sample>
    <sample id="78">Sì, il processo di semplificazione differisce tra DEPLAIN-apa e DEPLAIN-web. Nel corpus DEPLAIN-apa, si osservano più riordini e aggiunte di parole, mentre nel corpus DEPLAIN-web ci sono più riformulazioni. Inoltre, i testi della Bibbia sono semplificati in modo più marcato rispetto ai testi di notizie o ai testi per apprendenti.</sample>
    <sample id="79">Yes, CoScript is made publicly available as a high-quality dataset for constrained language planning.</sample>
    <sample id="80">La filigrana viene inserita nel testo modificando gli embedding in base al numero di trigger presenti nella frase. L'embedding fornito è una somma ponderata tra l'embedding originale e un embedding target, dove il peso dell'embedding target è proporzionale al numero di trigger. Quando il numero di trigger supera un certo valore (m), l'embedding fornito è esattamente uguale all'embedding target.</sample>
    <sample id="81">Gli autori dell'articolo sono affiliati all'Università dello Stato di Pennsylvania (Penn State University).</sample>
    <sample id="82">In questo video, viene presentato il lavoro intitolato "Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring", che propone un nuovo framework per la valutazione automatica degli scritti (AES) in un contesto non supervisionato. I modelli AES tradizionali richiedono grandi quantità di dati etichettati, un processo costoso e laborioso. Per ovviare a questo problema, l'approccio proposto, chiamato ULRA (Unsupervised Learning from Rank Aggregation), utilizza diversi segnali euristici per generare informazioni di qualità come supervisione pseudo-vero. Questi segnali vengono utilizzati per creare liste di ranking parziali, che vengono poi aggregate in un'unica supervisione tramite un modulo DPRA (Deep Pairwise Rank Aggregation). Questo modulo include una funzione di perdita progettata per gestire le contraddizioni tra segnali diversi e assegnare pesi di confidenza. Inoltre, viene proposta una strategia di punteggio per adattare i risultati del modello ai range predefiniti. Gli esperimenti dimostrano che ULRA supera i metodi non supervisionati esistenti, anche se il suo rendimento è inferiore rispetto ai metodi supervisionati a causa della mancanza di supervisione forte. In sintesi, ULRA rappresenta un passo avanti nella valutazione automatica degli scritti senza l'uso di dati etichettati.</sample>
    <sample id="83">Sì, i modelli codificatore-decodificatore come mT5 possono migliorare con l'addestramento su una combinazione di lingue, ottenendo prestazioni migliori rispetto all'addestramento monolingue, anche se l'inglese può mostrare una riduzione in alcuni casi (Curse of Multilinguality).</sample>
    <sample id="84">Shwai He presenta il lavoro "PAD-Net: An Efficient Framework for Dynamic Networks" presentato all'ACL 2023. I network dinamici, a differenza dei tradizionali network statici, possono adattare la loro architettura o i parametri in base all'input, offrendo maggiore flessibilità. Tuttavia, i network dinamici completi utilizzano troppi parametri, aumentando notevolmente la dimensione del modello, come nel caso di BERT-Base con Mixture of Experts. Questo limita la loro applicabilità. Per risolvere questo problema, Shwai propone PAD-Net, un framework che introduce un mix di parametri statici e dinamici, riducendo il numero di parametri dinamici ridondanti. Utilizzando un metodo iterativo per suddividere i parametri in modo dinamico o statico, PAD-Net mantiene il potere rappresentativo del modello originale, migliorando l'efficienza computazionale e riducendo la complessità. Gli esperimenti mostrano che PAD-Net supera sia i network statici che i network dinamici completi in termini di prestazioni, con un numero inferiore di parametri e operazioni. Inoltre, studi di ablation e confronti con tecniche di pruning confermano l'efficacia del metodo. Shwai conclude suggerendo futuri lavori, come l'estensione del framework a network diversi e a configurazioni hardware-friendly, nonché l'introduzione di ulteriori modi per combinare parametri statici, dinamici e zero.</sample>
    <sample id="85">Un esempio di pianificazione linguistica vincolata è "preparare una torta al cioccolato", dove il vincolo specifico ("al cioccolato") richiede che lo script includa passaggi rilevanti per l'uso del cioccolato, diversamente da un obiettivo astratto come "preparare una torta".</sample>
    <sample id="86">Gli autori verificano la segretezza del loro metodo visualizzando gli embedding delle frasi sui quattro dataset utilizzando PCA e osservando che è difficile distinguere gli embedding con il backdoor da quelli normali.</sample>
    <sample id="87">Il lavoro utilizza i PLM esistenti, come CamemBERT e PubMedBERT, come punto di partenza per il pre-addestramento continuo o per confrontare diverse strategie di addestramento. In particolare, alcuni modelli sono basati sui pesi e sulla tokenizzazione di CamemBERT, addestrati su dati medici estratti dal web (NACHOS) o da note cliniche anonimizzate. Questo permette di valutare l'impatto delle diverse fonti di dati e strategie di pre-addestramento nel costruire un nuovo modello, DrBERT, specifico per il dominio biomedico e clinico in francese.</sample>
    <sample id="88">GPT-4 è meno allineato con i paesi non anglofoni e con le persone non binarie rispetto ai paesi anglofoni e alle persone maschi e femmine.</sample>
    <sample id="89">La relatrice mostra il modo in cui il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione nell'esempio in cui il modello traduce il frammento "I'm going to talk about..." in tedesco, analizzando i pesi dell'attenzione incrociata per decidere quando emettere le parole tradotte.</sample>
    <sample id="90">**Abstract**  
In questo studio, Haneul Yoo e i collaboratori interrogano la necessità di reclutare parlanti nativi per l'annotazione dei dati NLP, proponendo l'uso di apprendenti linguistici come alternativa. Considerando la scarsità di parlanti nativi per molte lingue, l'articolo esplora la fattibilità di utilizzare apprendenti, soprattutto per lingue come l'irlandese, dove i parlanti nativi sono limitati ma il numero di apprendenti è elevato. L'indagine si basa su un esperimento controllato su tre lingue (inglese, coreano e indonesiano) e quattro compiti del benchmark GLUE, tra cui analisi del sentiment, NLI, NER e MRC. Gli apprendenti sono classificati in livelli di competenza (base, intermedio, avanzato) e confrontati con parlanti nativi. I partecipanti svolgono un pre-test, un'annotazione assistita da risorse aggiuntive (come dizionari o sistemi di traduzione automatica), e un post-test per valutare l'impatto sull'apprendimento linguistico. I risultati mostrano che le etichette annotate dagli apprendenti sono altamente accurate, specialmente per compiti semplici, e che l'aggregazione delle loro etichette mediante voto della maggioranza li rende quasi equivalenti ai nativi. Inoltre, i modelli linguistici addestrati con le etichette degli apprendenti raggiungono performance vicine al 95% rispetto a quelle reali. L'esperienza dimostra anche un miglioramento della competenza linguistica degli apprendenti. Questo lavoro apre la strada a nuove strategie per la costruzione di dati NLP, soprattutto per lingue a risorse limitate, superando barriere geografiche e tecnologiche.</sample>
    <sample id="91">La quantità di attività influisce positivamente sulla performance del modello: aumentando il numero di compiti, il modello raggiunge prestazioni migliori e una sensibilità più bassa.</sample>
    <sample id="92">Gli autori confrontano il loro metodo con tre approcci di riferimento, che includono modelli seq2seq tradizionali, modelli che utilizzano alberi per catturare la composizione, e altri modelli senza alberi (treeless models) esistenti.</sample>
    <sample id="93">I due coautori, Alexander Koller e Ivan Titov, sono i consulenti (advisor) del primo autore, Matthias Lindemann.</sample>
    <sample id="94">Jingwei Yi, dell'Università Cinese della Scienza e Tecnologia, presenta un lavoro intitolato "Protecting the copyright of large language models for embedding as services via backdoor watermark". L'embedding come servizio, offerto da modelli come GPT o LLAMA, è utilizzato per compiti NLP, ma è vulnerabile al furto di modelli tramite l'apprendimento dagli embedding. Per proteggere i diritti d'autore, propongono "Embedding Marker", un metodo basato su backdoor per inserire un watermark negli embedding. Il processo include due fasi: iniezione del watermark e verifica del copyright. Viene selezionato un trigger set di parole con frequenza moderata. Durante l'iniezione, gli embedding vengono modificati in base al numero di trigger presenti nel testo. Per la verifica, si confrontano gli embedding richiesti da un servizio sospetto con un embedding target, calcolando similarità coseno, L2 e test KS. Gli esperimenti su dataset come AG News, MIND, SST2 e Enron Spam mostrano una buona performance di rilevamento senza degradare l'utilità degli embedding. La covertà è verificata tramite visualizzazione PCA, dove gli embedding con watermark non sono distinguibili da quelli normali. L'approccio offre una soluzione promettente per proteggere i diritti d'autore negli embedding come servizio.</sample>
    <sample id="95">Il primo autore di PaLM non è menzionato nel testo fornito. Il testo si concentra su David Vilar e il suo lavoro di revisione del paper, ma non indica chi sia l'autore originale di PaLM.</sample>
    <sample id="96">Ciao a tutti. Mi chiamo Jenny, sono una dottoranda di primo anno all'Università Carnegie Mellon e oggi presenterò il nostro lavoro "NLPositionality: caratterizzazione dei bias di progettazione nei dataset e nei modelli". Questo lavoro è stato realizzato in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, in particolare Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap.

Iniziamo immaginando di lavorare per un quotidiano e di dover filtrare i commenti sotto un articolo per rimuovere il contenuto tossico. Potresti rivolgerti ad un API popolare come il Prospective API per la rilevazione della tossicità, e funziona molto bene se sei Carl Jones, dove il Prospective API riesce a rilevare correttamente i casi tossici. Tuttavia, non è realmente il caso di Aditya Sharma, dove il Prospective API non è altrettanto sensibile a termini offensivi più comuni in contesti indiani. Questo è un esempio di bias di progettazione, dove vediamo differenze sistematiche nel rendimento della tecnologia tra diverse popolazioni.

I bias di progettazione come quello appena visto possono verificarsi a causa della posizionalità degli ricercatori NLP e sviluppatori di modelli. La posizionalità è semplicemente le prospettive che le persone tengono come risultato della loro demografia, identità e esperienze di vita. Questo concetto è ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femministi e queer. Come ricercatrice, la posizionalità può influenzare il processo di ricerca e i suoi risultati, poiché può cambiare le decisioni che i ricercatori prendono.

Una domanda che potrebbe sorgere è: i dataset e i modelli hanno una posizionalità? Non stiamo dicendo che i modelli stessi o i dataset hanno identità demografiche e esperienze di vita, ma aggregano giudizi e opinioni di persone reali, e quindi possono rappresentare certe posizionalità rispetto ad altre.

Lavori precedenti hanno suggerito alcune prove aneddotiche di posizionalità, come divari culturali nei modelli e nei dataset, nonché definizioni teoriche della posizionalità dei modelli. Tuttavia, questi lavori non confrontano effettivamente gli utenti finali con i dataset e i modelli stessi, e lo studio della posizionalità dei modelli e dei dataset è sempre più importante man mano che i compiti NLP diventano più soggettivi e socialmente orientati. È difficile caratterizzare come queste posizionalità siano distorti perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API.

Per studiare la posizionalità dei dataset e dei modelli, confrontiamo le annotazioni realizzate da utenti reali con dataset e modelli esistenti. Lo facciamo attraverso il nostro framework NLPositionality. Il nostro framework funziona in due passaggi principali. Il primo passo è riannotare i dataset con annotatori diversi. Dobbiamo farlo guardando alla demografia degli annotatori originali dei dataset, poiché di solito solo pochi annotatori annotano ciascuna istanza e la demografia raramente viene raccolta e condivisa. Optiamo quindi per riannotare i dati per ottenere molte annotazioni per istanza e un insieme ricco di dati demografici. Poi prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i dataset usando un punteggio di correlazione Pearson R. Il nostro framework si differenzia dalla letteratura sul disaccordo tra annotatori confrontando gli utenti finali con i modelli e i dataset, previsioni e etichette, anziché guardare solo all'accordo tra annotatori o modellare le distribuzioni degli annotatori.

Il nostro framework è in gran parte abilitato da Lab in the Wild, una piattaforma online per la raccolta di dati di crowdsourcing in collaborazione con esperti in HCI. Lab in the Wild è una piattaforma di sperimentazione online dove possiamo reclutare volontari diversificati. Rispetto a piattaforme come M Turk che hanno principalmente partecipanti dagli Stati Uniti o dall'India, Lab in the Wild riesce comunque a ottenere dati di alta qualità.

Abbiamo ospitato due compiti su Lab in the Wild, uno riguardante l'accettabilità sociale, in cui i partecipanti leggeranno una situazione dal dataset Social Chemistry e scriveranno quanto ritengono che una situazione sia socialmente accettabile. Successivamente, per rimanere coinvolti nello studio, possono confrontare le loro risposte con quelle di un AI e di altri. Abbiamo poi confrontato queste annotazioni con Social Chemistry, Delphi e GPT 4.

Abbiamo replicato un setup molto simile per il compito di rilevazione della tossicità e del discorso d'odio, dove i partecipanti leggeranno un'istanza da Dynahate e scriveranno se ritengono che si tratti di un caso di discorso d'odio. Abbiamo poi confrontato queste annotazioni con Dynahate, Perspective API, Rewire API, Hate Roberta e GPT 4.

Il nostro studio alla fine ha raccolto oltre 16.000 annotazioni da oltre 1.000 annotatori provenienti da 87 paesi.

Ora siamo meglio equipaggiati per rispondere a chi si allineano di più i dataset e i modelli NLP. Troviamo che esiste una posizionalità in NLP. Ad esempio, troviamo che i dataset e i modelli sono più allineati ai paesi parlanti inglese. Per l'analisi dell'accettabilità sociale di GPT 4, troviamo che è più allineato ai paesi confuciani e parlanti inglese. Troviamo che anche Dynahate è più allineato ai paesi parlanti inglese. Troviamo inoltre un ulteriore allineamento con le persone che hanno un'istruzione universitaria. Per GPT 4, nel compito di accettabilità sociale, troviamo che è più allineato alle persone con un'istruzione universitaria o di scuola superiore e troviamo lo stesso per Dynahate, dove è più allineato alle persone con un'istruzione universitaria.

Tuttavia, quando i modelli e i dataset sono allineati a specifiche popolazioni, alcune vengono inevitabilmente lasciate indietro. Un esempio di questo è che i dataset e i modelli sono meno allineati alle persone non binarie rispetto ai loro omologhi uomini e donne. Troviamo questo nel compito di accettabilità sociale di GPT 4 nonché nell'analisi del compito Dynahate.

Dato che esiste una posizionalità in NLP, cosa possiamo fare al riguardo? Abbiamo alcune raccomandazioni per questo. La prima è tenere traccia di tutte le scelte di progettazione rilevanti durante tutto il processo di ricerca. La seconda è svolgere la ricerca NLP con la lente del perspectivismo. La terza raccomandazione è costruire dataset e modelli specializzati all'interno di 4 comunità specifiche. Un buon esempio di questo è l'iniziativa Masakhani. Vogliamo sottolineare che l'NLP inclusivo non è solo fare in modo che tutte le tecnologie funzionino per tutti. Questo conclude la mia presentazione. Se volete saperne di più, non esitate a visitare il nostro dashboard per gli ultimi risultati dell'analisi e il nostro articolo. Grazie.</sample>
    <sample id="97">La relatrice menziona due problemi associati a SimulST: l'uso di architetture specifiche che richiedono ottimizzazione aggiuntiva e i lunghi e complessi procedimenti di addestramento, nonché la necessità di addestrare e mantenere diversi modelli per raggiungere diversi regimi di latenza.</sample>
    <sample id="98">Un modo efficace per mitigare i bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP potrebbe essere l'adozione di tecniche di sanitizzazione dei dati, come l'equilibratura delle fonti politiche rappresentate, la rimozione o la riduzione di contenuti estremisti, e l'inclusione di dati provenienti da fonti diverse e non polarizzate. Tuttavia, questo richiede un attento equilibrio per evitare censura o esclusione di opinioni legittime.</sample>
    <sample id="99">Ciao, sono Siyu Yuan dell'Università di Fudan. Sono qui per presentare il nostro lavoro intitolato "Distillare la conoscenza degli script dai Large Language Models per la pianificazione linguistica vincolata". Nella vita quotidiana, gli umani spesso pianificano le proprie azioni seguendo istruzioni passo dopo passo nella forma di script orientati agli obiettivi. I lavori precedenti hanno sfruttato i modelli linguistici per pianificare obiettivi astratti di attività stereotipate, come "fare una torta". Hanno mostrato che i Large Language Models possono efficacemente decomporre gli obiettivi in passaggi. Tuttavia, il lavoro precedente si è concentrato principalmente sulla pianificazione per obiettivi astratti di attività stereotipate. La pianificazione per obiettivi con vincoli specifici, come "fare una torta al cioccolato", rimane poco studiata. In questo articolo, definiamo il problema della pianificazione linguistica vincolata, che impone diversi vincoli sugli obiettivi della pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multifacetati. Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli. In questo lavoro, valutiamo e miglioriamo innanzitutto la capacità di pianificazione linguistica vincolata dei Large Language Models. Poiché non esiste un dataset di obiettivi specifici per supportare il nostro studio, dobbiamo acquisire questi obiettivi per primi. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multifacetati per l'acquisizione di dati con l'intervento umano tramite InstructGPT. Campioniamo 100 obiettivi specifici e valutiamo gli script generati dai Large Language Models. Questa tabella riporta l'accuratezza complessiva dei risultati. Abbiamo scoperto che tutti i modelli linguistici raggiungono risultati insoddisfacenti nella pianificazione per obiettivi specifici. Successivamente conduciamo un'analisi dettagliata per investigare il motivo per cui i modelli di apprendimento falliscono. I risultati nella figura mostrano che la completezza semantica negli script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita. Approfondiamo ulteriormente le categorie più fini di vincoli definite in wikiHow. Il calore della mappa mostrata nella figura indica che le prestazioni di pianificazione degli InstructGPT variano notevolmente a seconda delle categorie degli obiettivi. Studi precedenti hanno dimostrato che la qualità dell'output dei modelli linguistici presenta una grande varianza, portando a prestazioni scadenti. Pertanto, adottiamo l'idea di "over-generare e poi filtrare" per migliorare la qualità della generazione. Prima mostriamo i tipi di vincoli con esempi per InstructGPT e otteniamo obiettivi specifici a partire dagli obiettivi astratti di base. Successivamente, InstructGPT genera K script per gli obiettivi specifici. Poi sviluppiamo un modello filtro per selezionare gli script fedeli. Convertiamo gli script e gli obiettivi in embedding InstructGPT e calcoliamo la similarità coseno come punteggio di similarità semantica. Inoltre, premiamo gli script che contengono le parole chiave del vincolo obiettivo. Manteniamo l'script solo se l'obiettivo mirato ha il punteggio più alto nell'insieme di obiettivi. Con il nostro metodo, InstructGPT può generare script di maggiore qualità. Il nostro metodo migliora notevolmente la capacità di pianificazione sia in termini di completezza semantica che di fedeltà ai vincoli. Poiché i Large Language Models sono costosi da implementare, è essenziale abilitare la capacità di pianificazione linguistica nei modelli più piccoli e specializzati. La creazione di un dataset è un passo fondamentale in questo senso. Tuttavia, gli studi precedenti non abilitano la pianificazione per obiettivi specifici e l'annotazione manuale dei dataset è costosa. Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica per distillare dataset di pianificazione linguistica vincolata da Large Language Models. Applichiamo il nostro metodo per costruire un dataset di pianificazione linguistica vincolata, chiamato CoScript. In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità dell'insieme di validazione e test, chiediamo a lavoratori crowdsourced di trovare e correggere gli esempi errati. Questa figura mostra la distribuzione dei vincoli di CoScript. Abbiamo scoperto che CoScript mostra una grande pluralità negli obiettivi specifici generati. Con CoScript possiamo provare modelli più piccoli ma specializzati per la pianificazione linguistica vincolata. Abbiamo scoperto che un T5 sottoposto a fine-tuning su CoScript può generare script di qualità superiore a quelli di molti Large Language Models, indicando che i modelli più piccoli possono superare i modelli più grandi quando addestrati correttamente su dataset adatti. In sintesi, abbiamo stabilito il problema della pianificazione linguistica vincolata. Abbiamo valutato la capacità di pianificazione linguistica vincolata dei Large Language Models e sviluppato un metodo "over-generate-then-filter" per i Large Language Models. Utilizziamo i Large Language Models per generare un dataset di script di alta qualità, CoScript, per la pianificazione linguistica vincolata. Speriamo che il dataset CoScript possa rappresentare una risorsa preziosa per avanzare la ricerca sulla pianificazione linguistica. Grazie per il vostro tempo. Per ulteriori dettagli su CoScript, consultate il nostro articolo.</sample>
    <sample id="100">Il lavoro presenta PromptRank, un approccio innovativo per il multi-hop QA (Question Answering) che combina un metodo di retrieval non supervisionato (TF-IDF) con un reranker basato su linguaggio modello per migliorare l'efficienza dati. In ambito multi-hop, le risposte richiedono una catena di documenti (chain) per essere correttamente elaborate. Gli approcci esistenti necessitano di migliaia di esempi per ottenere buone prestazioni, ma PromptRank si distingue per utilizzare solo 128 esempi, rendendolo adatto a domini a basso risorsa. Il processo prevede due fasi: la raccolta iniziale di candidati tramite TF-IDF e hyperlink traversal, seguita da un reranking basato sulla probabilità del quesito data la chain, calcolata tramite un linguaggio modello. Per migliorare i risultati, vengono utilizzate tecniche come la ricerca di istruzioni ottimali, il campionamento di istruzioni multiple e la scalatura della temperatura. I test su HotpotQA mostrano che PromptRank supera sistemi completamente supervisionati come DrKit e si confronta positivamente con i migliori retriever multi-hop. Inoltre, quando integrato con un reader model (ELECTRA-Large), mostra prestazioni competitive in QA multi-hop. Il lavoro sottolinea l'importanza dell'istruzione nel guidare il ragionamento del linguaggio modello e l'efficacia della probabilità del quesito data la chain come funzione di scoring. PromptRank rappresenta quindi un'alternativa promettente per il QA multi-hop in contesti con risorse limitate.</sample>
    <sample id="101">La fluidità di PaLM è paragonabile a quella dei sistemi di traduzione all'avanguardia, ma presenta alcuni problemi di accuratezza.</sample>
    <sample id="102">Le proprietà importanti di un metodo di filigrana sono:  
1. Deve essere applicabile ai servizi di embedding.  
2. Non deve degradare l'utilità degli embedding forniti.  
3. Deve essere sufficientemente celato per evitare la sua rimozione facile da parte di un attaccante.  
4. Deve essere trasferibile durante il processo di estrazione del modello.</sample>
    <sample id="103">Il testo non specifica quali siano le 14 lingue in cui sono stati tradotti i discorsi TED in inglese.</sample>
    <sample id="104">Il testo non specifica il numero esatto di istanze campionate da un set di dati per la riannotazione. Tuttavia, si menziona che vengono raccolte oltre 16.000 annotazioni da parte di più di 1.000 annotatori, suggerendo che ogni istanza è stata annotata da diversi annotatori, ma non il numero preciso di istanze originali.</sample>
    <sample id="105">Le metriche di distanza utilizzate per misurare la differenza tra set di dati benigni e backdoor sono il **cosine similarity**, l'**L2 similarity** e il **test KS (Kolmogorov-Smirnov)**, con il suo **p-value** come terza metrica.</sample>
    <sample id="106">Chaitanya presenta il lavoro "QUEST", un dataset per il retrieval basato su query che includono vincoli impliciti di tipo insiemistico. L'idea nasce dall'osservazione che gli utenti spesso esprimono bisogni informativi con più preferenze o vincoli, come nel caso di Jane, una zoologa che cerca di identificare una specie di rettile, o Austin, un lettore che cerca libri storici ambientati in Francia. Queste query implicano operazioni come intersezione e complemento tra insiemi. Per costruire QUEST, gli autori utilizzano categorie da Wikipedia in quattro domini (film, libri, piante, animali), applicando operazioni insiemistiche e chiedendo agli annotatori di paraphrasare le query e verificare la rilevanza delle entità. Il dataset include oltre 3.000 query, con documenti contrassegnati da span di testo rilevanti per ciascun vincolo. Gli esperimenti mostrano che i sistemi attuali hanno difficoltà a gestire queste query complesse, con bassi punteggi di F1, specialmente per le query con intersezione e differenza insiemistica. L'obiettivo è fornire un dataset utile per migliorare i sistemi di retrieval per scenari di ricerca selettiva.</sample>
    <sample id="107">I modelli basati su codificatori multilingue sono stati utilizzati per creare un'unica rappresentazione semantica delle query in diverse lingue naturali, permettendo il parsing semantico cross-linguistico in diverse rappresentazioni significative. Sono stati valutati in diversi scenari, tra cui monolingua, multilingua, zero-shot e few-shot, dimostrando che i modelli Encoder-Decoder ottengono prestazioni superiori rispetto ai modelli Encoder-PTR. Inoltre, l'addestramento su un mix di lingue diversificate migliora le prestazioni, anche se può causare un calo delle prestazioni in inglese (Curse of Multilinguality).</sample>
    <sample id="108">Il lavoro presenta un'analisi sull'efficacia dei giudizi di accettabilità linguistica nei modelli di linguaggio, utilizzando il paradigma delle coppie minimali (MPP). Gli autori osservano che i modelli linguistici non sono sempre robusti al contesto, soprattutto quando si valutano frasi più lunghe, un aspetto cruciale con i modelli a grandi finestre di contesto. Per testare questa ipotesi, i ricercatori hanno esteso le coppie minimali a sequenze più lunghe, aggiungendo prefissi da diversi insiemi di dati (come BLiMP o Wikipedia) alle frasi di riferimento. I risultati mostrano che i giudizi MPP rimangono stabili quando il contesto è irrilevante, ma variano significativamente quando il prefisso è coerente con la struttura grammaticale o semantica delle frasi. Questo suggerisce che i modelli siano sensibili a caratteristiche sintattiche e semantiche condivise, e che i test tradizionali, basati su frasi brevi, non riescano a catturare appieno la loro comprensione contestuale. L'analisi ha inoltre rivelato che le perturbazioni delle frasi non alterano in modo significativo i giudizi, confermando la sensibilità dei modelli a tali caratteristiche. Il lavoro evidenzia la necessità di valutazioni più complete per comprendere appieno le capacità dei modelli linguistici.</sample>
    <sample id="109">L'articolo presenta "Unnatural Instructions", un dataset di istruzioni naturali per compiti linguistici creati automaticamente senza lavoro umano. L'obiettivo è generare un ampio insieme di istruzioni diverse, creative e utili per l'addestramento di modelli linguistici. Il dataset è stato creato utilizzando un modello linguistico pre-addestrato (GPT-3) per generare istruzioni e output corrispondenti, e poi per creare parafrase automatiche. Il risultato è un dataset di 64.000 esempi, con circa 240.000 esempi inclusi le parafrase. L'analisi mostra che più del 50% degli esempi sono corretti e anche quelli errati possono essere utili per l'addestramento. Il dataset include compiti creativi diversi dai classici problemi NLP, come verificare un esperimento scientifico o inventare una nuova parola. L'addestramento di un modello T5 su Unnatural Instructions ha mostrato prestazioni superiori a T0++ e Tk-instruct su diversi benchmark. Il dataset dimostra che i modelli linguistici possono generare dati creativi e diversificati in modo più efficiente e a basso costo rispetto agli annotatori umani.</sample>
    <sample id="111">Gli autori decidono le parole a frequenza moderata analizzando un corpus di testi generale e contando la frequenza delle parole al suo interno, selezionando quelle che si trovano in un intervallo di frequenza moderato.</sample>
    <sample id="112">Ciao a tutti, il mio nome è Shuheng. Oggi presenterò il nostro lavoro: "I tagger per l'entità nomiata CoNLL-2003 funzionano ancora bene nel 2023?" Iniziamo. Il nostro articolo ha investigato il problema della generalizzazione nell'ambito del compito di Riconoscimento delle Entità Nome (NER). Abbiamo osservato che i modelli sono stati utilizzati nel CoNLL-2003 per sviluppare il NER per quasi 20 anni, e questo solleva naturalmente diversi problemi. In primo luogo, questi modelli possono generalizzare ai dati moderni? E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione? Allo stesso tempo, se notiamo una scarsa generalizzazione, cosa causa il calo delle prestazioni di questi modelli? Per investigare questi problemi, abbiamo sviluppato il dataset CoNLL++. Questo è un dataset che abbiamo raccolto da Reuters News del 2020, e che abbiamo annotato utilizzando le stesse linee guida di annotazione del CoNLL-2003. Abbiamo poi fine-tunato oltre 20 modelli su CoNLL-2003. Li abbiamo valutati su entrambi i set di test CoNLL-03 e CoNLL++. E infine, abbiamo calcolato il cambiamento percentuale in F1 per valutare la generalizzazione di ciascun modello.

Quindi, cosa è necessario per una buona generalizzazione? Durante gli esperimenti, abbiamo trovato che ci sono tre ingredienti principali necessari. Il primo è l'architettura del modello. Dal nostro esperimento, abbiamo visto che i modelli Transformer generalmente si generalizzano meglio sui nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che di solito i modelli più grandi portano a una migliore generalizzazione. E infine, tutti sappiamo che il numero di esempi di fine-tuning influisce direttamente sulle prestazioni di un compito downstream. Qui abbiamo anche visto che più esempi di fine-tuning portano a una migliore generalizzazione.

Passiamo alla prossima domanda: cosa causa il calo delle prestazioni di alcuni modelli? Abbiamo due ipotesi. La prima è l'overfitting adattivo, che è l'overfitting causato dal riutilizzo continuo dello stesso set di test e si manifesta spesso come il "diminishing returns" su un nuovo set di test. La seconda ipotesi è il drift temporale, che è il degrado delle prestazioni causato dall'aumento del divario temporale tra i dati di addestramento e quelli di test.

Per l'overfitting adattivo, abbiamo visto che nel grafico a destra, la linea di fit migliore rossa ha un gradiente maggiore di uno. Questo significa che ogni unità di miglioramento che abbiamo ottenuto su CoNLL-2003 si traduce in più di un'unità di miglioramento su CoNLL++. Questo indica che non c'è un "diminishing returns" e mostra che in questo caso non è osservato l'overfitting adattivo.

E allora cosa riguarda il drift temporale? Per il drift temporale, abbiamo eseguito un esperimento in cui abbiamo riallineato o continuato l'addestramento di alcuni modelli con dati più recenti e abbiamo visto che le prestazioni peggiorano con un divario temporale più ampio. Questo conferma la nostra ipotesi che la causa principale del calo delle prestazioni è il drift temporale.

La nostra conclusione è che, per una buona generalizzazione, è necessario un'architettura del modello migliore, una dimensione del modello più grande e un numero maggiore di esempi di fine-tuning. Questi vanno di pari passo: non possiamo avere solo uno di questi ingredienti e ignorare gli altri. Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni è causato da drift temporale e, sorprendentemente, non è causato da overfitting adattivo, anche se il CoNLL-2003 è stato utilizzato per più di 20 anni.

Tornando alla domanda che abbiamo posto nel titolo del nostro articolo: "I tagger CoNLL-2003 funzionano ancora bene nel 2023?" Abbiamo scoperto che la risposta è un "sì" molto chiaro. Speriamo che il nostro articolo stimoli ulteriori ricerche su come migliorare la generalizzazione dei modelli. Infine, non dimenticate di controllare il nostro articolo, il nostro dataset e, se avete domande, non esitate a contattarmi. Grazie mille.</sample>
    <sample id="114">Il lavoro presentato all'ACL 2023, realizzato da ricercatori dell'NTU Singapore, si concentra sull'ottimizzazione dei parametri nei modelli linguistici di grandi dimensioni, in particolare sul ridurre la ridondanza del meccanismo di attenzione multi-head. L'approccio proposto, denominato "Grouped Head Attention", utilizza una strategia di "divide and conquer" con due fasi principali: la "group-constrained training", che raggruppa le teste di attenzione per rendere quelle all'interno di un gruppo più simili e quelle tra gruppi più diverse, e l'algoritmo "Voting-to-Stay", che pruned le teste ridondanti, mantenendo una sola per gruppo. Questo permette una riduzione significativa dei parametri, fino al 90% in casi estremi, senza compromettere le prestazioni. I test su compiti come traduzione automatica, modellazione linguistica e sintesi abstrattiva mostrano miglioramenti del 3.8-7% rispetto ai baseline, con una compressione del 16.9-32.1%. Inoltre, il modello LITE ottiene un'efficienza inferenziale superiore. Il lavoro suggerisce che i modelli linguistici possono essere pruned in modo mirato per applicazioni specifiche, riducendo il carico computazionale e migliorando l'efficienza, simile a disinstallare app non utilizzate su un dispositivo mobile.</sample>
    <sample id="115">L'approccio utilizza un segmento parlato definito da un numero fissato di frame audio, rappresentato dal parametro lambda, per determinare quando emettere una traduzione parziale in base alla stabilità dell'attenzione. La dimensione esatta non è specificata in termini di millisecondi o parole, ma è gestita attraverso questo parametro.</sample>
    <sample id="116">Nell'esempio con Servin e Kea, la conoscenza specifica dell'entità necessaria è "Servin è un giudice".</sample>
    <sample id="117">Il fattore più importante è la qualità dell'esempio.</sample>
    <sample id="118">Il lavoro presentato al ACL 2023 si concentra sull'adattamento delle tecniche di pre-addestramento per gestire il code-switching, un fenomeno comune in contesti linguistici diversificati come l'India. I modelli multilingue pre-addestrati come mBERT e XLM-R mostrano prestazioni limitate su compiti come l'analisi del sentiment e la risposta alle domande in testi code-switched. Per migliorare queste prestazioni, gli autori propongono **SwitchMLM**, un'alternativa all'obiettivo MLM standard. SwitchMLM si basa sull'identificazione dei "switch-point", ovvero i punti di transizione tra lingue in una frase, e maschera solo tali token. Per superare la necessità di dati con tag LID, viene introdotto **FrequencyMLM**, un metodo alternativo basato sulla frequenza dei token in corpora monolingue. Inoltre, vengono proposte modifiche architetturali, come connessioni residuali da strati intermedi al layer finale e una loss ausiliaria per migliorare l'apprendimento delle informazioni linguistiche. Gli esperimenti mostrano che l'approccio combinato (Switch/FrequencyMLM + ResBERT + loss ausiliaria) migliora significativamente le prestazioni su compiti di analisi del sentiment. Gli esperimenti di probing confermano che i metodi proposti aumentano la quantità di informazioni sui switch-point nei layer intermedi e finali, supportando l'efficacia dell'architettura proposta.</sample>
    <sample id="119">L'articolo si concentra su modelli linguistici come GPT-4, GPT series, BART series e RoBERTa negli esperimenti estesi.</sample>
    <sample id="120">Il modello utilizza i punteggi di attenzione di un livello specifico, precisamente il cross-attention mechanism tra l'input audio e l'output testuale.</sample>
    <sample id="121">Gli esempi di inferenza diretta includono il nome dell'entità, come "Easy on Me", o la posizione, come "il primo".</sample>
    <sample id="122">L'autore dell'articolo, Siyu Yuan, è affiliato all'Università di Fudan.</sample>
    <sample id="123">Ying e Zhiyang presentano il loro lavoro su MultiInstruct, un dataset innovativo per il tuning delle istruzioni in ambito multi-modale, mirato a migliorare l'apprendimento zero-shot in compiti che combinano testo, immagini e bounding box. Con l'avanzamento dei modelli linguistici di grandi dimensioni, l'uso di istruzioni per migliorare le prestazioni su compiti non visti è diventato un'area promettente, soprattutto nel campo NLP. Tuttavia, mancavano dataset multi-modali di grandi dimensioni per questo scopo. Per colmare questa lacuna, gli autori hanno creato MultiInstruct, il primo benchmark di questo tipo, che include 62 compiti diversi derivati da 21 dataset open-source, ciascuno con cinque istruzioni scritte da esperti. Hanno utilizzato OFA, un modello pre-addestrato multi-modale, come base per il tuning. I risultati mostrano che il tuning delle istruzioni migliora significativamente le prestazioni su compiti multi-modali già visti e aumenta la capacità di generalizzazione. Inoltre, l'apprendimento trasferito da dataset di istruzioni naturali riduce la sensibilità del modello alle variazioni nell'input. Gli autori introducono anche una nuova metrica, la "sensitività", per valutare la coerenza delle risposte del modello. Infine, stanno raccogliendo un dataset più grande e lo renderanno disponibile al pubblico.</sample>
    <sample id="124">Tan Qingyu presenta il lavoro "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models", in cui si analizza e migliora la capacità di ragionamento temporale nei modelli linguistici di grandi dimensioni. L'articolo suddivide il ragionamento temporale in tre livelli: time-to-time, time-to-event e event-to-event. I precedenti studi si concentravano principalmente sul secondo livello, mentre il team propone un'analisi più completa. È stato creato il dataset TempReason, che copre tutti e tre i livelli e include un'ampia gamma temporale. Sono state valutate diverse strategie di QA (Closed Book, Open Book e Reasoning QA), e i risultati mostrano che i modelli come ChatGPT soffrono in particolare nel ragionamento temporale complesso. Per migliorare le capacità di ragionamento, i ricercatori hanno proposto due strategie di addestramento: l'estrazione di span temporali e un'apprendimento rinforzato sensibile al tempo. Il modello finale, TempT5, mostra un miglioramento significativo rispetto ai modelli baseline, anche se persistono alcune fluttuazioni legate all'equilibrio dei dati di addestramento. L'obiettivo futuro è superare questi bias di ragionamento temporale. In sintesi, lo studio introduce un benchmark completo e propone un paradigma di addestramento per migliorare il ragionamento temporale nei modelli linguistici.</sample>
    <sample id="125">L'articolo non specifica il numero di autori coinvolti.</sample>
    <sample id="126">No, la traduzione della query in linguaggio naturale utilizzando un modello di traduzione automatica prima del parsing semantico non è stato considerato un approccio standard. Questo metodo è stato utilizzato come uno dei sei setting di valutazione (Translate-Test) nell'analisi del benchmark XSemPLR, ma non è l'approccio principale o standard adottato nei modelli di parsing semantico cross-linguistico.</sample>
    <sample id="127">In their paper "Large Language Models Are Reasoning Teachers," Namgyu Ho, Laura Schmid, and Se-Young Yun propose a method to transfer reasoning capabilities from large language models (LLMs) to smaller, more deployable models. Chain-of-thought (CoT) prompting has been shown to improve complex reasoning in large models, but is ineffective for smaller ones. The authors address this by using large models as "reasoning teachers" to generate step-by-step solutions, which are then used to fine-tune smaller student models. To enhance this process, they introduce "Diverse Reasoning," a technique that generates multiple reasoning paths from the teacher model using stochastic sampling, leading to more robust training data. Their experiments on 12 tasks show that this approach significantly outperforms standard fine-tuning and prompt-based methods, especially in text-based reasoning tasks. The method is highly scalable and adaptable, offering trade-offs between development costs (e.g., teacher model quality, dataset size) and inference costs (e.g., student model size). The results suggest that reasoning abilities can be effectively distilled from large models to smaller ones, opening up possibilities for deploying complex reasoning in resource-constrained environments. The paper provides detailed analysis, code, and data for further exploration.</sample>
    <sample id="128">Akshatha e Martin presentano il lavoro "KITMUS Test: Evaluating Knowledge Integration from Multiple Sources", un progetto collaborativo tra McGill University, Mila e Microsoft Research. L'obiettivo è valutare la capacità dei modelli di comprensione del linguaggio naturale di integrare conoscenze provenienti da fonti diverse, sia pre-addestrate che fornite durante l'inferenza. I modelli linguistici, infatti, spesso devono combinare informazioni generali (apprese durante il pre-addestramento) con dati specifici (disponibili al momento dell'uso). Il test KITMUS include compiti di risoluzione della coreferenza, in cui i modelli devono collegare pronomi a entità specifiche, basandosi su conoscenze di contesto e background. I ricercatori hanno definito tre scenari: uno in cui la conoscenza di background è disponibile solo durante il pre-addestramento, uno in cui è presente in entrambi i momenti e uno in cui è fornita solo all'inferenza. I risultati mostrano che i modelli, senza addestramento specifico, non riescono a integrare efficacemente le conoscenze da fonti multiple. Solo con un addestramento mirato, alcuni modelli riescono a migliorare, anche se continuano a mostrare limiti nell'utilizzo della conoscenza fornita esclusivamente all'inferenza. Il lavoro sottolinea l'importanza di migliorare la capacità dei modelli di integrare conoscenze eterogenee per compiti complessi.</sample>
    <sample id="129">The authors provided the example of Black women as a marked group, highlighting the top words associated with them such as "strong" and "resilient," which reflect the "Strong Black Women" archetype.</sample>
    <sample id="130">Le architetture dei modelli che non generalizzano in modo adeguato sono quelle non basate su transformer. Gli esperimenti hanno mostrato che i modelli transformer generalizzano generalmente meglio ai dati moderni rispetto ad altre architetture.</sample>
    <sample id="131">Nel contenuto fornito non vengono menzionati specifici nomi di set di dati di test.</sample>
    <sample id="132">The article involves two authors: Akshatha and Martin.</sample>
    <sample id="133">L'autore opera con più modalità, non solo con il testo. Il lavoro si concentra su un'addestramento a istruzioni multi-modalità, coinvolgendo sia il testo che le immagini.</sample>
    <sample id="135">ABC-Eval è un nuovo metodo per valutare l'AI conversazionale in modo multidimensionale, sviluppato dal laboratorio NLP dell'Emory University e in collaborazione con Amazon Alexa AI. A differenza delle valutazioni tradizionali basate su giudizi umani (come Likert o confronti tra conversazioni), ABC-Eval si concentra su comportamenti specifici dei modelli, come irrilevanza, contraddizioni, errori di logica o mancanza di empatia. Questo approccio riduce la soggettività e fornisce una valutazione più precisa e dettagliata. I ricercatori hanno testato quattro modelli di chat su 100 conversazioni, confrontando ABC-Eval con metodi esistenti. I risultati mostrano che ABC-Eval ha un'alta affidabilità (misurata attraverso l'accordo tra annotatori) e fornisce metriche più predictive della qualità complessiva della conversazione. Inoltre, le metriche di ABC-Eval sono distinte e forniscono informazioni uniche, spiegando insieme oltre il 25% della qualità della conversazione. Nonostante i progressi, i modelli testati commettono errori significativi, come violazioni del senso comune (20%) o risposte irrilevanti (15%). L'obiettivo è promuovere metriche più affidabili per confrontare i modelli futuri. ABC-Eval rappresenta un passo importante verso valutazioni più accurate del comportamento dell'AI conversazionale.</sample>
    <sample id="136">Jasivan presenta il lavoro svolto con la supervisione di Nafise all'Università di Sheffield, intitolato "FERMAT: An Alternative to Accuracy for Numerical Reasoning". L'obiettivo è migliorare l'analisi delle capacità di ragionamento numerico dei modelli linguistici, spesso insufficienti, specialmente nei modelli più piccoli. I test attuali, come CommonCore e Illinois, non forniscono informazioni dettagliate sulle forze e debolezze dei modelli. FERMAT introduce un insieme di valutazione flessibile basato su operazioni aritmetiche, con domande estratte da fonti reali e modificate per testare diversi aspetti: comprensione numerica, operazioni matematiche e dipendenza dal training. I risultati mostrano che i modelli, anche dopo il fine-tuning, non riescono a risolvere correttamente i problemi numerici, anche quando l'espressione esatta è stata vista in fase di training. L'analisi rivela l'importanza della diversità linguistica e matematica, con l'uso di template diversi e di dataset come GSM8K e AQUA che migliorano le prestazioni. FERMAT mira a fornire un'alternativa più informativa rispetto all'accuratezza, evidenziando la necessità di benchmark più rappresentativi. L'approccio proposto sottolinea l'importanza della diversità e della corretta rappresentazione numerica per migliorare il ragionamento numerico nei modelli linguistici.</sample>
    <sample id="137">Sicong presenta il lavoro "Tell2Design: A Dataset for Language-Guided Floor Plan Generation", pubblicato all'ACL 2023. Questo studio introduce un nuovo compito di apprendimento automatico: generare piani di piano in base a istruzioni linguistiche. A differenza della generazione di immagini artistiche, questa attività richiede un'adeguata comprensione di vincoli, semantica, geometria e topologia. Il dataset Tell2Design include 5.051 istruzioni umane e 76.000 generate artificialmente, associate a piani di piano. Il modello proposto, basato su un framework encoder-decoder con transformer, converte le istruzioni in una sequenza di bounding box strutturata. Rispetto ad altri approcci, il modello ottiene risultati migliori in termini di IoU, grazie alla capacità di gestire istruzioni complesse e vincolanti. Tuttavia, esiste un divario tra istruzioni artificiali e umane, che può essere mitigato con un addestramento iniziale su dati artificiali. Il lavoro costituisce una base per futuri studi sulla generazione di progetti guidata dal linguaggio, con applicazioni potenziali in ambiti di design e architettura.</sample>
    <sample id="138">Secondo gli autori, l'area della NLU che è poco studiata è l'integrazione di conoscenza da fonti multiple, in particolare la capacità di modelli di utilizzare sia la conoscenza acquisita durante il pre-addestramento che la conoscenza fornita durante l'addestramento.</sample>
    <sample id="139">I nomi dei relatori sono Ying e Zhiyang.</sample>
    <sample id="140">Yes, CoScript was subjected to quality checks. Crowd-sourced workers were asked to find and revise incorrect samples to ensure the quality of the validation and test sets.</sample>
    <sample id="141">Le risorse esistenti per la traduzione dipendente dal contesto hanno limiti in termini di tipi di contesti supportati e di lingue coperte, spesso a causa della dipendenza da conoscenza di dominio e curatela umana.</sample>
    <sample id="142">Ciao! Sto per parlare del nostro lavoro intitolato "Resolving Indirect Referring Expressions for Entity Selection", in cui introduciamo il corpus AltEntities. Il mio nome è Javad Hosseini e si tratta di un lavoro congiunto con Filip Radlinski, Silvia Pareti e Annie Louis. Il nostro obiettivo è comprendere il linguaggio degli utenti quando intendono fare una scelta. Consideriamo questa domanda alternativa: "Hai inteso 'Easy on Me' o 'I Gotta Feeling'?" In questo caso, l'utente vuole selezionare tra due brani musicali. La cosa più ovvia è utilizzare un riferimento diretto, ad esempio citando il nome del brano, "Easy on Me", o la sua posizione, "il primo". Tuttavia, a volte un riferimento indiretto è più naturale e appropriato. Questo può accadere quando l'utente non ricorda il nome del brano, quando le pronunce sono molto simili e quindi difficili da distinguere, o quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di riferimenti indiretti, ad esempio "il più recente" o "la canzone che non è energica". Questo è un problema importante nei sistemi conversazionali e anche per il benchmarking della comprensione delle entità da parte degli LLM. Non sappiamo di un dataset pubblico più ampio per questo compito, quindi ne abbiamo creato uno attraverso un'annotazione di massa. Il nostro dataset copre tre diversi domini: musica, libri e ricette. Il metodo di raccolta del dataset si concentra sull'informalità utilizzando un setup di completamento di un fumetto. Il fumetto ha tre bolle di dialogo. Nella prima bolla, Bob dice: "Ricordi quella canzone che ascoltavamo ieri?" Con questa frase, Bob stabilisce il contesto del dialogo. Nella seconda bolla, Alice chiede: "Hai inteso 'Easy on Me' o 'I Gotta Feeling'?", che è la domanda alternativa. Nella terza bolla, Bob utilizza un riferimento indiretto per selezionare una delle entità, ad esempio "il più recente". Forniamo automaticamente le prime due bolle, mentre la terza è compilata dall'annotatore. La prima bolla è scelta da alcuni prompt manuali per ciascun dominio. La seconda, che costituisce la domanda alternativa, viene generata utilizzando un semplice modello: "Hai inteso A o B?", dove A e B sono campioni da Wikipedia. Ecco i diversi metodi di campionamento che abbiamo utilizzato. Man mano che si sale nella lista, le entità diventano più simili tra loro e di solito è più difficile fare la disambiguazione. Il primo metodo è il campionamento casuale uniforme. Il secondo metodo è quando le entità hanno titoli simili, ad esempio due libri con il titolo "The Return". Il terzo metodo è quando hanno descrizioni simili su Wikipedia. Infine, quando hanno informazioni simili o attributi simili su Wikipedia, ad esempio lo stesso genere o lo stesso artista per una canzone. Quando mostriamo questa domanda alternativa agli annotatori, sanno i nomi delle entità, ma non necessariamente ne conoscono i dettagli. Quello che facciamo è mostrare loro alcune informazioni di background sulle due entità. Per le canzoni, mostriamo semplicemente un link di ricerca su Google per ciascuna canzone e chiediamo agli annotatori di ascoltare almeno parte di ciascuna canzone e di leggerne alcune informazioni. Ecco, ad esempio, i risultati della ricerca di Google per la canzone "Easy on Me". Per i domini di ricette e libri, mostriamo invece del testo di background tratto da Wikipedia. Per le ricette, mostriamo inoltre le loro immagini, anch'esse prese da Wikipedia, in modo che gli annotatori possano capire come appaiono. Poi chiediamo agli annotatori di selezionare una delle entità, ad esempio la prima, e di descriverla utilizzando tre a cinque espressioni referenziali indirette. Ad esempio, "quella senza parole", "non quella con il ragazzo di 12 anni", "la fictionale" o "quella proveniente dall'Azerbaijan", e così via. Il corpus AltEntities contiene 6.000 domande alternative su tre domini e comprende 42.000 espressioni referenziali indirette. I risultati con il modello T5 XL sono riassunti qui sotto. Se il modello linguistico ha accesso esattamente alla stessa conoscenza di background degli annotatori, l'accuratezza è molto alta, intorno al 92-95%. Ma non è realistico. Se il modello linguistico ha accesso a una conoscenza di background parzialmente sovrapposta, l'accuratezza è compresa tra l'82 e l'87%, che è più realistica. Ad esempio, quando il modello linguistico recupera la conoscenza di background. Se il modello linguistico ha accesso solo ai nomi delle entità, l'accuratezza è solo del 60%, quindi c'è molto spazio per migliorare. Abbiamo anche dimostrato che i modelli sono generalizzabili tra i domini. Ecco il link al nostro dataset. Grazie.</sample>
    <sample id="143">L'approccio EDAtt viene confrontato con le politiche SimulST esistenti Wait-k e Local Agreement, nonché con l'architettura di stato dell'arte specificamente progettata per la traduzione simultanea.</sample>
    <sample id="144">L'affiliazione degli autori non è specificata nel testo fornito.</sample>
    <sample id="145">Il nome della relatrice è Jenny.</sample>
    <sample id="146">**Abstract:**  
Yicheng presenta un lavoro sull'analisi e la gestione dell'omissione nei sistemi di sintesi di dialoghi. La sintesi dei dialoghi, un sottotask della sintesi testuale, presenta sfide significative, tra cui l'omissione di informazioni critiche nei riassunti generati, che ne limita l'applicabilità pratica. Lo studio rivela che anche i modelli di ultima generazione presentano un tasso di omissione elevato, con circa il 70% dei riassunti che non includono informazioni essenziali. L'omissione è distribuita in modo casuale all'interno dei dialoghi, rendendo difficile per i modelli identificare le informazioni chiave. Per affrontare il problema, l'equipe ha creato il dataset OLDS, il primo dataset dedicato all'analisi dell'omissione, costruito su cinque benchmark esistenti e annotato con etichette di omissione a livello di utterance. Sono stati valutati diversi framework per la rilevazione dell'omissione, con risultati che evidenziano la difficoltà del compito. Inoltre, l'uso delle informazioni omesse per raffinare i riassunti ha portato a un miglioramento significativo delle performance, dimostrando che la rilevazione e l'utilizzo dell'omissione possono essere strategie promettenti per migliorare la qualità dei riassunti di dialogo. Questo lavoro apre la strada a nuove linee di ricerca per affrontare le limitazioni attuali dei modelli di sintesi dei dialoghi.</sample>
    <sample id="147">L'articolo è stato realizzato in collaborazione con tre autori: Myra, Esin Durmus e Dan Jurafsky.</sample>
    <sample id="148">Ciao, sono Sara Papi dell'Università di Trento e della Fondazione Bruno Kessler, e brevemente introdurrò il paper "Attention as a Guide for Simultaneous Speech Translation", che è un lavoro congiunto con Matteo Negri e Marco Turchi. Cosa è la traduzione simultanea del parlato? La traduzione simultanea del parlato, o SimulST, è il processo di tradurre un linguaggio parlato in un testo in un'altra lingua in tempo reale, permettendo la comunicazione tra lingue diverse. E quali sono i problemi dei modelli attuali di SimulST? Solitamente vengono utilizzate architetture specifiche, introducendo moduli aggiuntivi da ottimizzare. Procedure di allenamento lunghe e complicate, ad esempio, allenamenti che coinvolgono diversi obiettivi di ottimizzazione. E l'allenamento e la manutenzione di diversi modelli per raggiungere diversi regimi di latenza. Ad esempio, allenare un modello con una latenza media di un secondo e un altro con due secondi, e così via. Quale è la nostra soluzione? Prima di tutto, utilizzare i modelli esistenti per la traduzione offline senza riallenamento o adottare un'architettura specifica per la SimulST. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici. E sfruttare la conoscenza già acquisita dal modello attraverso il meccanismo di attenzione tra l'input audio e l'output testuale. Questo è il meccanismo di attenzione incrociata, e puoi vedere un esempio a destra. La nostra soluzione è proporre EDAtt, o Encoder-Decoder Attention, che è una strategia in cui decidiamo se emettere o meno una traduzione parziale in base a dove si concentra l'attenzione. Una parola viene emessa se l'attenzione non è concentrata, cioè se la somma è inferiore a un certo阈值 alpha verso gli ultimi lambda frame di parlato, il che significa che l'informazione ricevuta è abbastanza stabile. Ad esempio, se riceviamo un frammento di parlato contenente "I'm going to talk about..." e il nostro modello predice la traduzione in tedesco, guarderemo i pesi dell'attenzione incrociata e vedremo che le prime due parole si riferiscono ai primi frame di parlato ricevuti, mentre l'ultima parola si riferisce agli ultimi frame ricevuti, come lambda frame di parlato. Questo significa che le prime due parole verranno emesse, mentre poiché la somma dell'attenzione incrociata è superiore a un certo阈值 alpha, non emetteremo l'ultima parola e aspetteremo un altro frammento di parlato. Se andiamo avanti e riceviamo un altro frammento di parlato, e il nostro modello prevede altre tre parole, guarderemo quei pesi dell'attenzione incrociata e vedremo che nessuna parola si riferisce agli ultimi lambda frame di parlato. Questo significa che queste tre parole verranno emesse. Se guardiamo ai principali risultati di EDAtt, tracceremo i risultati della traduzione simultanea del parlato in grafici in cui abbiamo il BLEU su un lato che misura la qualità della traduzione, e la media del ritardo che è la misura della latenza, e consideriamo anche la media del ritardo consapevole del calcolo che tiene conto del tempo computazionale del modello per prevedere l'output. Vogliamo che le nostre curve siano il più alte possibile in questo grafico. Ma vogliamo anche che siano spostate a sinistra. Confrontiamo con strategie popolari che sono applicate anche ai modelli offline, che sono la strategia Wait-k e l'Accordo Locale. Confrontiamo anche con l'architettura all'avanguardia specificamente adattata per la traduzione simultanea. Questi sono tutti i risultati della strategia di traduzione simultanea del parlato in tedesco. E vediamo che supera tutte le strategie applicate ai modelli offline poiché le curve sono spostate a sinistra. E vediamo anche che se consideriamo il tempo effettivo trascorso o il tempo consapevole del calcolo, cioè la strategia più veloce. Se vuoi scoprire altri risultati, leggi il nostro paper. Abbiamo anche rilasciato il codice e i modelli e l'output simultaneo open source per facilitare la riproducibilità del nostro lavoro. Grazie per l'attenzione.</sample>
    <sample id="149">Sì, il set di dati CoNLL++ è disponibile pubblicamente.</sample>
    <sample id="150">Archiki presenta il lavoro "MEETINGQA: Extractive Question-Answering on Meeting Transcripts", un nuovo dataset per il QA estrattivo basato su trascrizioni di riunioni. Il dataset è stato creato per sfruttare il potenziale delle domande e risposte nei meeting, un aspetto poco esplorato finora. Utilizzando il corpus AMI, sono state selezionate e annotate 7.700 domande, tra cui il 30% non rispondibili, con risposte che spesso coinvolgono più speaker o frasi discontinue. Il dataset include diversi tipi di domande, tra cui quelle di tipo sì/no, opinioni e domande retoriche. I risultati mostrano una significativa differenza tra le prestazioni umane (F1 84,6) e i modelli finetunati (gap di 25 punti F1), con modelli a contesto breve che spesso superano quelli a lungo contesto. L'analisi degli errori rivela difficoltà nel riconoscere domande retoriche e identificare i parlanti. L'uso di dati silver annotati automaticamente migliora le prestazioni in setting zero-shot. In sintesi, MEETINGQA rappresenta un nuovo domino per il QA, con sfide significative per i modelli esistenti, sia in setting finetunati che zero-shot.</sample>
    <sample id="151">Ciao a tutti, mi chiamo Ying e insieme al mio collega Zhiyang presenteremo i nostri risultati sulla ricerca intitolata MultiInstruct: miglioramento del learning zero-shot multi-modale tramite tuning delle istruzioni. Con i progressi nei modelli linguistici di grandi dimensioni, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento che permettono di riusare i modelli linguistici pre-addestrati in modo efficiente in termini di parametri e dati per compiti downstream diversi. Recentemente, diversi studi hanno dimostrato che il tuning delle istruzioni permette ai modelli linguistici di grandi dimensioni di eseguire compiti non visti in modo zero-shot seguendo istruzioni naturali. Tuttavia, la maggior parte dei lavori precedenti sul tuning delle istruzioni si è concentrata sull'incremento delle prestazioni zero-shot per compiti linguistici soltanto, mentre i compiti di computer vision e multi-modale sono stati trascurati. Pertanto, in questo lavoro vogliamo investigare se il tuning delle istruzioni su modelli multi-modali pre-addestrati possa effettivamente migliorare la generalizzazione verso compiti multi-modali non visti. Inoltre, durante la nostra ricerca, abbiamo scoperto una considerevole discrepanza nella disponibilità di dataset di istruzioni tra NLP e multi-modale. Esistono più di 1600 compiti linguistici soltanto con istruzioni. Tuttavia, non esiste un grande dataset multi-modale pubblicamente disponibile. Questo motiva noi a costruire un dataset di tuning delle istruzioni multi-modale. Presentiamo qui MultiInstruct, il primo benchmark per il tuning delle istruzioni multi-modale, che comprende 62 compiti multi-modali diversi coprendo 10 categorie ampie. Questi compiti provengono da 21 dataset open-source esistenti e ogni compito è dotato di cinque istruzioni scritte da esperti. Per investigare il tuning delle istruzioni multi-modali sul nostro dataset proposto, prendiamo OFA, un modello pre-addestrato multi-modale unificato, come modello base. OFA utilizza un vocabolario unificato per il linguaggio, i token immagine e le coordinate di un bounding box. Ecco alcuni esempi di istanze dal nostro dataset MultiInstruct, per unificare il processamento di diversi tipi di dati di input e output. Seguiamo il metodo di OFA e formuliamo tutti i compiti in un formato unificato sequenza-a-sequenza. In questo formato, il testo di input, le immagini, le istruzioni e i bounding box sono rappresentati nello stesso spazio di token. Ora parlerò del tuning delle istruzioni multi-modali. Per il dataset di addestramento, utilizziamo 53 compiti provenienti da 9 gruppi per l'addestramento e campioniamo 10.000 istanze per compito. Per i test, riserviamo l'intero gruppo di ragionamento comune per i test e selezioniamo inoltre 5 compiti dagli gruppi VQ e Miscellaneous. Utilizziamo tutte le istanze nella divisione di test per ciascun compito. Inoltre, campioniamo casualmente 20 compiti dalla divisione di test di istruzioni naturali come compiti non visti per NLP. Utilizziamo il modello OFA pre-addestrato come modello base. Durante l'addestramento, mescoliamo tutte le istanze per tutti i compiti. Ogni istanza è casualmente combinata con una delle cinque istruzioni. Durante i test, per ciascun compito, conduciamo un totale di 5 esperimenti valutando il modello utilizzando una delle cinque istruzioni. In ciascun esperimento, riferiamo la prestazione minima e massima e la deviazione standard delle prestazioni su tutti e 5 gli esperimenti. Se il compito è un compito di classificazione multi-modale, riferiamo l'accuratezza. Se è un compito di generazione multi-modale, riferiamo Rouge-L. Per i compiti NLP, riferiamo anche Rouge-L. Introduciamo inoltre un ulteriore metrica di valutazione chiamata sensibilità. Questa misura la capacità del modello di produrre sempre gli stessi output per lo stesso compito indipendentemente da lievi variazioni nel linguaggio delle istruzioni. Ecco i nostri risultati principali. Come possiamo vedere, il tuning delle istruzioni può migliorare significativamente le prestazioni di OFA sui compiti multi-modali visti. Inoltre, l'apprendimento transfer da dataset di istruzioni naturali può beneficiare del tuning delle istruzioni. Come possiamo vedere, con l'aumentare del numero di compiti, il modello raggiunge prestazioni migliori e al contempo una sensibilità inferiore. Abbiamo anche condotto un esperimento: utilizzo di una sola istruzione rispetto a cinque istruzioni. Come possiamo vedere, l'uso di più istruzioni può migliorare significativamente le prestazioni complessive del modello e ridurre molto la sua sensibilità. Questo mostra l'effetto di diverse strategie di fine-tuning sulla sensibilità del modello. Come possiamo vedere, l'apprendimento transfer da dataset di istruzioni naturali permette al modello di raggiungere una sensibilità molto migliore rispetto al modello OFA originale. Possiamo anche vedere che l'apprendimento transfer da dataset di istruzioni naturali può aiutare OFA a raggiungere prestazioni molto migliori sul dataset di istruzioni naturali. Complessivamente, proponiamo il primo dataset di grandi dimensioni per il tuning delle istruzioni multi-modale, che ha migliorato significativamente le capacità di OFA nel raggiungere compiti brevi. Esploriamo diverse tecniche di transfer learning e mostriamo i loro benefici. Proponiamo inoltre una nuova metrica chiamata sensibilità. Un'ultima cosa, stiamo raccogliendo un dataset di tuning delle istruzioni multi-modale molto più grande, con circa 150 ulteriori compiti visione-linguaggio, e li rilasceremo. Questo è il codice QR per i nostri dati e modello. Grazie.</sample>
    <sample id="152">Frederick Riemenschneider presenta un lavoro sull'intersezione tra NLP e filologia classica, introducendo nuovi modelli linguistici per il greco antico e il latino. Pur esistendo modelli come Latin BERT e Ancient Greek BERT, essi sono monolingui e basati su architetture specifiche, limitando la loro applicabilità. Per rispondere a queste sfide, l'équipe ha sviluppato GreBERTa e GreTa (monolingui) e PhilBERTa e PhilTa (multilingui), pre-addestrati su dati di alta qualità, inclusi nuovi corpus da Internet Archive. I modelli sono stati testati su compiti come tagging POS, parsing dipendente e lemmatizzazione, superando il state-of-the-art. L’analisi ha rivelato differenze significative tra encoder di T5 e modelli encoder-only. Sebbene i modelli multilingui non siano superiori in termini di prestazioni rispetto a quelli monolingui, offrono maggiore flessibilità. Il lavoro include un dataset di alta qualità per il greco antico e un'analisi approfondita delle capacità semantiche e del knowledge. In sintesi, il progetto offre nuove risorse per la filologia classica, con modelli più performanti e multilingui, disponibili in un articolo dettagliato.</sample>
    <sample id="153">Ninareh Mehrabi, postdoc alla Amazon Alexa AI, presenta il lavoro "Resolving Ambiguities in Text-to-Image Generative Models", che si concentra sull'analisi e la risoluzione delle ambiguità nei prompt utilizzati per generare immagini da testo. I prompt ambigui, come "The girl enters the room with flowers", possono essere interpretati in modi diversi, causando immagini non fedeli all'intenzione dell'utente. Per affrontare questo problema, l'equipe ha creato un dataset benchmark basato su LAVA, che include diversi tipi di ambiguità. Hanno proposto due framework per la disambiguazione: uno che genera domande per chiarire l'intenzione dell'utente, e un altro che suggerisce diverse interpretazioni visive. Dopo la disambiguazione, le immagini vengono generate e valutate tramite un framework automatico che utilizza un modello VQA per verificare se corrispondono all'intenzione dell'utente. I risultati mostrano che la disambiguazione migliora la fedeltà delle immagini e che il framework di valutazione automatica è in accordo con le valutazioni umane. In conclusione, lo studio propone metodi per ridurre le ambiguità nei prompt e valutare la fedeltà delle immagini generate.</sample>
    <sample id="154">Gli autori dell'articolo sono affiliati all'Università di Trento e alla Fondazione Bruno Kessler.</sample>
    <sample id="155">Il nome del relatore è Javad Hosseini.</sample>
    <sample id="157">Shen Gao da Shandong University presenta il lavoro "Dialogue Summarization with Static-Dynamic Structure Fusion Graph", realizzato in collaborazione con altri ricercatori. L'obiettivo è creare un riassunto conciso di un dialogo, estratto dalle informazioni più rilevanti. I metodi esistenti si basano su strutture grafiche statiche pre-calcolate, ma presentano due limiti: dipendenza da strumenti esterni non sempre affidabili e mancanza di adattamento dinamico al compito di riassunto. Per risolvere questi problemi, l'equipe ha sviluppato il modello SDDS, che unisce informazioni statiche e dinamiche. Il modello include un Utterance Encoder per rappresentare i discorsi in vettori, un modulo Static-Dynamic Graph che combina grafi statici (come Discourse Parsing e Key Co-occurrence) con grafi dinamici basati sull'attenzione multi-head, e un Summary Generator basato su un modello linguistico pre-addestrato. Il modulo Static-Dynamic Graph integra informazioni strutturali e semantiche, migliorando la rappresentazione del dialogo. Il modello utilizza anche una matrice di interazione tra speaker e una fusione di grafi tramite convoluzione 1x1. La generazione del riassunto incorpora informazioni strutturali tramite un meccanismo di cross-attenzione doppio. Il codice e i dati sono disponibili su GitHub.</sample>
    <sample id="158">Qipeng Guo da AWS presenta il lavoro "Dual Cache for Long Document Neural Coreference Resolution", che affronta il problema della risoluzione della coreferenza in documenti lunghi. La coreferenza consiste nell'identificare e raggruppare le menzioni che si riferiscono allo stesso entità. I metodi tradizionali sono complessi e richiedono risorse elevate, mentre i metodi basati su cache riducono la complessità a livello lineare. Tuttavia, nell'elaborazione di documenti lunghi, i cambiamenti di argomento possono causare un alto tasso di mancato accesso alla cache, specialmente con politiche come LRU. Per migliorare questa situazione, l'approccio "Dual Cache" propone due cache: una locale (LRU) per entità locali e una globale (LFU) per entità frequenti. Questo sistema classifica le entità e le inserisce nella cache appropriata, riducendo i mancato accessi. L'analisi su benchmark pubblici e un libro di 30.000 parole mostra che Dual Cache supera i metodi baseline in termini di prestazioni e efficienza, anche senza dati di addestramento. Inoltre, riduce significativamente i mancato accessi rispetto a una singola cache. In sintesi, Dual Cache offre un miglior rapporto tra prestazioni e costo, rendendolo un'opzione più efficiente per la risoluzione della coreferenza in documenti lunghi.</sample>
    <sample id="159">Ciao a tutti. Sono Koustav Sinha, e sono lieto di darvi il benvenuto nel nostro talk relativo al nostro articolo presentato all'ACL 2023. I giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto. Questo è un lavoro congiunto con John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy e Adina Williams. In questo lavoro, riconsideriamo i paradigmi delle coppie minimali. Il paradigma delle coppie minimali valuta i modelli linguistici attraverso giudizi di accettabilità, che possono includere anche giudizi di grammaticalità come BLiMP, SyntaxGym, o accettabilità in termini di stereotipi come CrowS pairs. In questo paradigma, il modo tipico per valutare i modelli linguistici è mostrare una frase accettabile o grammaticale, e poi mostrare un’altra frase accettabile o non grammaticale. L’idea è che il modello assegni una probabilità maggiore alla frase accettabile. Tuttavia, la pipeline attuale dei paradigmi delle coppie minimali non permette di valutare l’accettabilità del modello su frasi più lunghe. Oggi i modelli linguistici di grandi dimensioni hanno contesti sempre più lunghi, quindi è cruciale valutare l’accettabilità dei modelli lungo l’intero contesto. Questo è l’obiettivo del nostro lavoro: riconsiderare la pipeline delle coppie minimali chiedendo ai modelli di valutare l’accettabilità su sequenze sempre più lunghe. Per simulare queste sequenze più lunghe, riconsideriamo i dataset stessi e ricreiamo frasi scegliendo frasi accettabili o non accettabili da quei dataset. Ad esempio, abbiamo scelto una coppia tipica di grammaticalità dal dataset BLiMP relativo al caso degli "Adjunct Island". Cosa facciamo? Ricostruiamo sequenze più lunghe, che siano accettabili o non accettabili, mantenendo la stessa struttura grammaticale. Estraiamo frasi grammaticali dagli "Adjunct Island" e le aggiungiamo come prefisso sia alla query accettabile che a quella non accettabile. Possiamo fare lo stesso scegliendo frasi non accettabili dallo stesso fenomeno, e questo può essere utilizzato per testare i giudizi di accettabilità del modello. Possiamo anche farlo scegliendo frasi da un sottogruppo diverso o da un diverso dataset. Questo è ciò che chiamiamo scenario di "mismatch". In questo caso, le frasi provengono da dataset rilevanti, ma non dallo stesso dataset utilizzato per l'analisi. Possiamo fare lo stesso anche per il caso non accettabile. Infine, possiamo scegliere frasi da un dominio completamente diverso, ad esempio Wikipedia. Questo ci permette di capire se i giudizi di accettabilità dei modelli sono effettivamente influenzati dal contesto, se il contesto proviene da un sottogruppo diverso del dataset, oppure se è completamente irrilevante rispetto alla frase che stiamo analizzando. Come si comporta il modello? Innanzitutto, esaminiamo le frasi di Wikipedia, che sono completamente irrilevanti per la coppia di query attuale. In questo caso, i giudizi MPP risultano per lo più robusti a qualsiasi lunghezza di contesto. Aumentiamo la lunghezza del contesto fino a 1024 per raggiungere il limite massimo di modelli come OPT e GPT-2. Qui, come si vede nella linea tratteggiata arancione, i giudizi MPP rimangono relativamente stabili. Cosa succede quando scegliamo frasi dallo stesso dataset? In questo caso, creiamo frasi accettabili e non accettabili dallo stesso dataset BLiMP o SyntaxGym. Qui notiamo che i giudizi MPP aumentano o diminuiscono in modo significativo quando aggiungiamo prefissi accettabili o non accettabili. Tuttavia, quando i prefissi corrispondono alla stessa struttura grammaticale, ossia quando scegliamo frasi dallo stesso fenomeno in BLiMP o SyntaxGym, vediamo un aumento o una diminuzione massiva dei giudizi MPP, a seconda che il prefisso scelto sia accettabile o non accettabile. Questo effetto è molto significativo e aumenta con la lunghezza del contesto, e potrebbe influenzare i nuovi modelli linguistici con contesti molto più lunghi. Perché i prefissi che corrispondono influenzano così tanto i giudizi dei modelli linguistici? Abbiamo condotto una serie di analisi in cui abbiamo cercato di perturbare le frasi di input, mantenendo la struttura rilevante ma aggiungendo rumore. Dopo diverse perturbazioni, non abbiamo visto che nessun rumore modificasse effettivamente il comportamento del modello in termini di giudizi MPP. In sintesi, i modelli sono sensibili alle perturbazioni in modo simile: quando perturbiamo frasi accettabili, vediamo un aumento simile in tutti i casi; quando perturbiamo frasi non accettabili, vediamo una diminuzione simile nei giudizi MPP. I punti chiave del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti che sono condivise tra le frasi. La valutazione MPP, come la conduciamo attualmente con input di breve lunghezza e singole frasi, potrebbe non catturare completamente la conoscenza astratta del modello lungo l’intero contesto. Per ulteriori dettagli sugli esperimenti, vi consigliamo di leggere il nostro articolo. Grazie per l'attenzione.</sample>
    <sample id="160">Il primo passaggio del metodo mappa i token di input in token di output attraverso un'etichettatura con un insieme non ordinato (multiset) di token che appariranno nell'output.</sample>
    <sample id="161">CoScript rappresenta 55,000 script.</sample>
    <sample id="163">Il metodo di allineamento migliore per DEPLAIN è MASSalign.</sample>
    <sample id="164">Il vantaggio dell'apprendimento scarsamente supervisionato è che permette di addestrare modelli utilizzando etichette generate da fonti deboli (come regole semplici, basi di conoscenza o crowdsourcing di bassa qualità), che sono molto più economiche rispetto alle annotazioni manuali effettuate da esperti.</sample>
    <sample id="165">Wenting Zhao, PhD student at Cornell University, presents a paper titled "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations." Abductive reasoning involves finding a plausible explanation that connects a given context to an outcome. Traditional methods rely on supervised learning, requiring annotated explanations, which can be subjective and inconsistent. Zhao introduces LiPoR (Likelihood Learning with Posterior Regularization), an unsupervised approach that eliminates the need for such annotations. LiPoR treats explanations as latent variables and maximizes the likelihood of the outcome given the context, while incorporating a regularizer to enforce mutual exclusivity among explanations—only one explanation can be true at a time. This regularizer encourages a preference for a subset of explanations, improving the model's ability to identify the most plausible one. On the AlphaNLI dataset, LiPoR outperforms existing zero-shot and unsupervised models, including GPT-3, by over 4 points in accuracy. The approach represents a significant advancement in learning abductive reasoning without relying on labeled data.</sample>
    <sample id="166">**Abstract:**  
In questo lavoro, presentiamo un framework neurale basato sulla strategia "Divide-and-Conquer" e sulla Dual-Process Theory per migliorare il compito di recupero immagini da testi linguistici complessi. Questo compito è particolarmente difficile a causa della similarità elevata tra le immagini e della complessità delle descrizioni testuali. I modelli visuo-linguistici pre-addestrati, sebbene efficaci in compiti di recupero immagini-testo standard, mostrano un calo significativo di prestazioni quando affrontano testi complessi. Per risolvere questo problema, proponiamo un sistema a due livelli ispirato alla Dual-Process Theory: il primo livello (System 1) si occupa di ragionamento analogico attraverso un interagente visuo-linguistico, mentre il secondo livello (System 2) esegue un ragionamento logico simbolico. Il framework include un Generatore di Proposizioni per decomporre il testo complesso in proposizioni semplici, un Neural-Symbolic Reasoner per integrare i risultati del ragionamento, e operazioni come negazione e congiunzione per gestire logiche complesse. I risultati sperimentali mostrano che il nostro metodo, NDCR, supera gli approcci baseline, confermando l'efficacia del design modulare e della combinazione di ragionamento analogico e logico. Questo lavoro apre la strada all'integrazione di ragionamento simbolico nei modelli linguistici per compiti di comprensione e pianificazione complessi.</sample>
    <sample id="167">I documenti in DEPLAIN-web sono stati allineati in parte in modo manuale e in parte con metodi automatici.</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato raccogliendo notizie dal Reuters News del 2020 e annotandole seguendo le stesse linee guida di annotazione del CoNLL-2003.</sample>
    <sample id="169">David Vilar presenta una breve recensione del lavoro "Prompting PaLM for Translation: Assessing Strategies and Performance", realizzato in collaborazione con Google Translate. L'articolo analizza per la prima volta sistematicamente l'uso di prompt per migliorare la traduzione con modelli linguistici di grandi dimensioni, come PaLM, un modello con 540 miliardi di parametri addestrato su 780 miliardi di token. L'obiettivo è valutare la capacità di traduzione di tali modelli utilizzando i migliori standard del settore, come i test set più recenti e i sistemi di riferimento WMT. Gli autori confrontano le performance di PaLM con quelle dei sistemi di traduzione neurali di ultima generazione, utilizzando metriche automatizzate e valutazioni umane. Risultati sperimentali mostrano che la scelta degli esempi di prompt è cruciale: una strategia a 5 esempi, con marcatura linguistica, ottiene buoni risultati. Tuttavia, anche se PaLM si avvicina a sistemi commerciali come Google Translate, presenta errori di omissione e minori accuratezza, anche se la fluidezza è paragonabile a sistemi di alto livello. In sintesi, la qualità degli esempi e la scelta del prompt influenzano significativamente la traduzione, ma i sistemi specializzati mantengono un vantaggio sostanziale.</sample>
    <sample id="170">Ciao a tutti, mi chiamo Yusen Zhang dell'Università di Penn State. Oggi presenterò il nostro lavoro intitolato "XSemPLR: Parsing Semantico Cross-Linguistico in Multiple Lingue Naturali e Rappresentazioni di Significato". Il parsing semantico è un compito che mira a costruire rappresentazioni semantiche delle query degli utenti, come SQL o Lambda Calculus. Il parsing semantico cross-linguistico è il compito di tradurre query in diverse lingue naturali in diverse rappresentazioni di significato. Come mostrato in questa figura, dobbiamo tradurre le query in diverse lingue naturali attraverso modelli neurali in SQL, Lambda o FunQL, e così via. I modelli esistenti per il parsing semantico cross-linguistico sono stati proposti e valutati separatamente su dataset limitati in termini di compiti e applicazioni. Per esempio, ci sono molte coperture su certe lingue naturali, ma manca la copertura per la lingua cinese. Inoltre, mancano certe rappresentazioni di significato, come il calcolo lambda, o vengono valutati solo su certi modelli neurali. Ad esempio, viene utilizzato un solo modello per valutarli. A questo punto, proponiamo XSemPLR. Forniamo un dataset uniforme XSemPLR per il parsing semantico cross-linguistico in diverse lingue naturali e rappresentazioni di significato. Contiene 9 dataset in diversi domini, 5 compiti di parsing semantico, 8 rappresentazioni di significato e 22 lingue naturali appartenenti a 15 famiglie linguistiche. Per valutare meglio il nostro benchmark, consideriamo sei impostazioni per l'addestramento e l'analisi. La prima è Translate-Test. Utilizziamo l'API di Google Translate per tradurre la lingua di origine in quella target, quindi addestriamo e valutiamo un modello monolingue. Per esempio, addestriamo il modello inglese su query in inglese e durante l'inferenza traduciamo le query tedesche tramite API in inglese e quindi utilizziamo il modello addestrato per prevedere l'SQL. Valutiamo anche il modello monolingue. In questa impostazione, la lingua di origine è la stessa della lingua target, ad esempio tedesco a tedesco o inglese a inglese. Valutiamo anche l'impostazione monolingue a pochi esempi addestrando i modelli monolingue con solo il 10% dei dati di addestramento. Valutiamo anche il modello multilingue, addestrando un unico modello multilingue per tutte le lingue. Per esempio, mettiamo insieme le query tedesche, inglesi e cinesi per addestrare un modello multilingue. Durante l'inferenza possiamo utilizzare questo modello per tradurre le query tedesche o cinesi, ecc. Consideriamo anche il trasferimento cross-linguistico a zero esempi e a pochi esempi. Addestriamo su una lingua di origine e trasferiamo a un'altra lingua. Durante l'addestramento, addestriamo su query inglesi o su un mix di query inglesi e tedesche a pochi esempi per addestrare un modello multilingue per prevedere l'output SQL. Abbiamo anche trovato molti risultati interessanti. Riguardo all'analisi dei modelli monolingue, valutiamo due gruppi di modelli, tra cui Encoder-PTR, che sta per Encoder Multilingue Pre-addestrato con Decodificatori basati su Pointer, come XLM-R + PTR e mBERT + PTR. Valutiamo anche i modelli Encoder-Decoder, che sono modelli encoder-decoder multilingue pre-addestrati, come mBART e mT5. Abbiamo scoperto che Encoder-Decoder ottiene le prestazioni migliori su tutti i nove dataset. Valutiamo mT5 e XLM-R + PTR in impostazione multilingue. Abbiamo scoperto che Encoder-Decoder o Encoder-PTR possono essere migliorati addestrandoli in un mix di varie lingue. Abbiamo scoperto che questo è dovuto al fatto che la maggior parte delle principali lingue naturali ottiene un miglioramento delle prestazioni, eccezion fatta per l'inglese, le cui prestazioni calano in sette dataset e migliorano solo in tre dataset. Penso che questo sia noto come "Curse of Multilinguality". Confrontiamo anche il divario di prestazioni cross-linguistiche. In questa figura, la linea blu rappresenta il trasferimento cross-linguistico a pochi esempi. La linea arancione rappresenta il trasferimento cross-linguistico a zero esempi. La linea verde rappresenta l'impostazione monolingue. Abbiamo scoperto che, confrontando la linea verde e arancione, abbiamo trovato che nel setting a zero esempi, il divario di prestazioni nel trasferimento cross-linguistico è significativo, e confrontando la linea blu e arancione, abbiamo scoperto che con l'impostazione a pochi esempi il divario di trasferimento si riduce rapidamente. Abbiamo anche scoperto alcuni altri risultati interessanti. Ad esempio, Encoder-Decoder supera il lavoro precedente o raggiunge risultati comparabili. L'addestramento su lingua naturale inglese può migliorare significativamente le prestazioni a pochi esempi nelle lingue naturali target, e abbiamo scoperto che i modelli linguistici multilingue come Codex e BLOOM sono tuttavia ancora insufficienti per i compiti di parsing semantico cross-linguistico. In sintesi, abbiamo costruito XSemPLR, un benchmark unificato per il parsing semantico cross-linguistico con diverse lingue naturali e rappresentazioni di significato. Abbiamo condotto uno studio completo del benchmark su tre tipi rappresentativi di modelli linguistici multilingue. I nostri risultati mostrano molti interessanti risultati. E così via. Benvenuti a visitare il nostro articolo e il codice. Grazie per l'ascolto.</sample>
    <sample id="171">I lavori connessi in tal senso possono essere classificati in quattro categorie, ma spesso non sono applicabili agli embedding come servizi o mancano di trasferibilità.</sample>
    <sample id="172">No, gli LLM multilingue come Codex o Bloom non sono sufficienti per il Cross-Lingual Semantic Parsing (CLSP).</sample>
    <sample id="174">Thea, co-autrice del dataset "ArgAnalysis35K", spiega le caratteristiche uniche di questo dataset per l'analisi della qualità degli argomenti. Il dataset si distingue per la sua grande dimensione (35.000 coppie argomento-analisi), la qualità elevata degli argomenti, che provengono da tornei di alto livello e da debattitori esperti, e la diversità tematica, basata su 24 argomenti selezionati da fonti esperte. Un aspetto innovativo è l'introduzione del concetto di "analisi", che combina premesse, affermazioni e argomenti in modo coerente, migliorando la comprensione degli argomenti stessi. Inoltre, il dataset utilizza un modello di rilevanza per valutare quanto un argomento sia rilevante per un determinato tema, permettendo una maggiore flessibilità. L'analisi della affidabilità degli annotatori è basata sull'istanza, eliminando solo le valutazioni potenzialmente biasate e non i giudizi complessi. Queste caratteristiche rendono ArgAnalysis35K un dataset più affidabile, diversificato e completo rispetto ad altri esistenti. Thea invita a consultare il paper e il poster per ulteriori dettagli.</sample>
    <sample id="175">Il metodo affronta l'ambiguità delle permutazioni utilizzando un'approximazione continua GPU-friendly, che permette di backpropagare attraverso la soluzione e imparare le permutazioni più plausibili dal punto di vista linguistico. Questo approccio risolve il problema NP-hard associato alla ricerca della permutazione ottimale, simile al problema del "Traveling Salesman".</sample>
    <sample id="176">L'equità di un modello NLP a valle viene definita in base al suo comportamento su compiti specifici come la rilevazione di discorsi d'odio e notizie false, analizzando se il modello mostra bias politici che possono portare a risultati ingiusti o distorti per gruppi sociali diversi.</sample>
    <sample id="177">The name of the presenter is Yanis Labrak.</sample>
    <sample id="178">Il nome del relatore è Koustav Sinha.</sample>
    <sample id="179">Melanie Sclar presenta "SymbolicToM", un metodo plug-and-play per migliorare le capacità di "Theory of Mind" nei Large Language Models (LLM). La Theory of Mind è la capacità di comprendere gli stati mentali altrui, spesso testata con domande su false credenze, come nel classico test Sally-Anne. I modelli linguistici di grandi dimensioni, come GPT-3, mostrano una scarsa performance in questi test. SymbolicToM utilizza rappresentazioni grafiche esplicite per modellare le credenze dei personaggi in una storia, distinguendo tra credenze dirette e credenze su credenze (first-order e second-order). Queste rappresentazioni, generate in tempo di inferenza con modelli NLI e OpenIE, permettono di rispondere a domande complesse mediante ricorsione su grafi. I test mostrano miglioramenti significativi: ad esempio, un incremento di 65 punti di accuratezza per GPT-3. Inoltre, SymbolicToM è efficace anche su dataset fuori dal dominio, come D₁ e D₂, dove i modelli supervisionati decadono, mentre SymbolicToM mantiene performance elevate. Il metodo è interpretabile, evita l'overfitting e si dimostra superiore rispetto alle soluzioni tradizionali, anche su dataset con maggiore diversità linguistica. In sintesi, SymbolicToM rappresenta un approccio promettente per migliorare la comprensione dei modelli linguistici riguardo agli stati mentali altrui.</sample>
    <sample id="180">The name of the speaker is Myra.</sample>
    <sample id="181">**Abstract**  
Siyu Yuan da Fudan University presenta il lavoro "Distilling Script Knowledge from Large Language Models for Constrained Language Planning", che affronta il problema della pianificazione linguistica vincolata, dove gli obiettivi di pianificazione sono soggetti a diversi vincoli specifici. Sebbene i modelli linguistici di grandi dimensioni siano in grado di decomporre obiettivi astratti in passaggi logici, la loro capacità di pianificare obiettivi specifici con vincoli è scarsa. L'articolo valuta questa capacità e rivela che i modelli non riescono a soddisfare i vincoli, pur generando script semanticamente completi. Per migliorare i risultati, l'approccio "over-generate-then-filter" è adottato: InstructGPT genera un gran numero di script, che vengono filtrati utilizzando similarità semantica e keyword rilevanti. Questo metodo migliora significativamente la completezza semantica e la fedeltà ai vincoli. Inoltre, l'articolo introduce CoScript, un dataset di alta qualità di 55.000 obiettivi specifici con script, creato distillando conoscenza da modelli linguistici di grandi dimensioni. CoScript mostra una grande varietà di vincoli e permette di addestrare modelli più piccoli e specializzati, come T5, che superano i grandi modelli quando addestrati su dati appropriati. Questo lavoro apre la strada per ricerche future sulla pianificazione linguistica vincolata e offre un risorsa preziosa per la comunità accademica.</sample>
    <sample id="182">Nel contesto dell'articolo, il tropicalismo indica un'immagine stereotipata e riduttiva associata alle donne di origine latina, che le descrive come "vibranti" e "curvacee", collegandole a tratti esotici e legati a una visione stereotipata del tropico. Questo contribuisce a narrativi essenzializzanti e dannosi che riducono le persone a caratteristiche culturali o fisiche superficiali.</sample>
    <sample id="183">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target utilizzando promemoria ispirati a uno studio in cui i partecipanti umani erano stati invitati a descrivere se stessi in base a specifiche identità, permettendo così un confronto diretto tra le risposte generate dai modelli linguistici e quelle scritte dagli umani.</sample>
    <sample id="184">In questo lavoro, è stato utilizzato l'indice CXMI (Contextual Mutual Information) e la sua estensione, il Pointwise CXMI, per misurare l'utilizzo del contesto durante la traduzione.</sample>
    <sample id="185">La differenza principale tra DrBERT e ChuBERT risiede nei dati utilizzati per l'addestramento: DrBERT è addestrato su dati web medici estratti (NACHOS), mentre ChuBERT è basato su dati clinici anonimizzati provenienti da un magazzino dati dell'ospedale di Nantes. Inoltre, DrBERT è il primo modello open source specializzato per il francese nel campo biomedico, mentre ChuBERT è un modello clinico.</sample>
    <sample id="187">L'articolo è stato presentato da Ying e dal suo collega Zhiyang, quindi sono coinvolti 2 autori.</sample>
    <sample id="188">L'iterative transfer learning (trasferimento iterativo dell'apprendimento) è un approccio in cui un modello viene addestrato su compiti correlati in modo sequenziale, aggiornando progressivamente i parametri del modello con nuovi dati. Questo permette di migliorare gradualmente le prestazioni su un compito specifico, come la rilevazione della dissonanza cognitiva, sfruttando conoscenze apprese da compiti simili.</sample>
    <sample id="189">The goal of the dataset is to understand users' language when they want to make a choice, specifically focusing on resolving indirect referring expressions for entity selection in conversational systems and for benchmarking LLMs' entity understanding.</sample>
    <sample id="190">Un utente malintenzionato può estrarre i parametri del modello attraverso un EaaS (Embedding as a Service) imparando dagli embedding restituiti dal servizio e utilizzando tecniche di estrazione del modello per ricostruire un modello simile.</sample>
    <sample id="191">L'articolo è un lavoro congiunto di tre autori: Sara Papi, Matteo Negri e Marco Turchi.</sample>
    <sample id="192">L'audio presenta il lavoro di Yang Luo sul nuovo ottimizzatore "CAME: Confidence-guided Adaptive Memory Efficient Optimization", progettato per migliorare l'efficienza della memoria durante l'addestramento dei modelli linguistici di grandi dimensioni. Oggi, gli ottimizzatori adattivi come Adam richiedono un uso elevato di memoria, mentre alternative più efficienti come Adafactor sacrificano le prestazioni per ridurre il consumo di memoria. CAME mira a combinare le prestazioni elevate degli ottimizzatori tradizionali con un basso utilizzo di memoria. L'approccio si basa sull'analisi degli errori di aggiornamento presenti in ottimizzatori esistenti, introducendo un meccanismo che utilizza la differenza tra aggiornamenti previsti e generati per guidare gli aggiornamenti in modo più stabile e adattivo. Gli esperimenti su modelli come BERT, GPT-2 e T5 mostrano che CAME migliora significativamente l'accuratezza di validazione rispetto a Adam e Adafactor, riducendo notevolmente il consumo di memoria, soprattutto con grandi dimensioni dei batch. Inoltre, CAME dimostra una buona efficienza su compiti downstream e si rivela particolarmente adatto all'addestramento su grandi lotti, rappresentando un'importante estensione degli ottimizzatori a bassa memoria esistenti.</sample>
    <sample id="193">The text does not specify the number of annotators used to create the initial dataset.</sample>
    <sample id="194">Gli autori dell'articolo sono affiliati all'Università di Washington e all'Allen Institute for AI. I nomi degli autori menzionati sono Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap.</sample>
    <sample id="195">Il lavoro presenta RoHT, un framework innovativo per l'Answering Esplicabile (XQA), che combina decomposizione gerarchica delle domande e ragionamento probabilistico. XQA richiede di rispondere a domande complesse e fornire spiegazioni. I metodi neuro-simbolici e basati sulla decomposizione hanno limiti: i primi dipendono da KB incomplete, i secondi da corpus di testo non sempre sufficienti. RoHT affronta queste sfide decomponendo le domande in un albero gerarchico (HQDT), dove ogni nodo rappresenta una sottodomanda, e applicando un ragionamento probabilistico per integrare KB e testo. Il framework include un scheduler per selezionare fonti di conoscenza, esecutori per ottenere risposte con probabilità e un aggregatore per combinare i risultati. RoHT è stato testato su KQA Pro e Musique, mostrando miglioramenti significativi rispetto ad altri metodi, grazie all'integrazione di conoscenze eterogenee. Il risultato dimostra l'efficacia della decomposizione esplicita e della fusione di fonti diverse per rispondere a domande complesse in modo più preciso ed esplicabile.</sample>
    <sample id="196">L'esempio in cui il governatore è a sinistra è "I saw Bart and Lisa", dove il verbo "saw" (governatore) è posizionato a sinistra rispetto alla coordinazione "Bart and Lisa".</sample>
    <sample id="197">I modelli all'avanguardia nei sistemi di dialogo menzionati nel testo sono quattro modelli di chat di ultima generazione, pur non essendo specificamente nominati con i loro nomi. Tuttavia, il testo indica che sono stati selezionati per essere valutati con ABC-Eval e confrontati tra loro.</sample>
    <sample id="198">La valutazione dell'accettabilità dei modelli nell'intera finestra di contesto è necessaria perché i modelli linguistici moderni gestiscono contesti sempre più lunghi, e è cruciale comprendere come le loro giudizi di accettabilità siano influenzati da contesti estesi, non solo da frasi singole. Questo aiuta a valutare in modo più completo la capacità dei modelli di mantenere giudizi coerenti e robusti anche in presenza di contesti complessi e diversi.</sample>
    <sample id="199">Sì, la formazione multilingue ha causato un calo delle prestazioni rispetto al modello monolingue inglese in sette dei nove dataset, un fenomeno noto come "Curse of Multilinguality".</sample>
    <sample id="200">No, gli annotatori non conoscono l'entità in anticipo. Ricevono informazioni di background sulle entità (come link di ricerca o testi da Wikipedia) per poterle comprendere e selezionare correttamente.</sample>
    <sample id="201">Le metriche di MT utilizzate per la valutazione sono state le metriche neurali di traduzione automatica (come BLEURT) e i risultati della valutazione umana basata sul framework MQM.</sample>
    <sample id="202">No, il regresso nella generalizzazione non influisce su specifici tipi di NER. L'analisi ha mostrato che il calo di prestazioni è dovuto al "temporal drift" (spostamento temporale), cioè all'incremento del divario temporale tra i dati di addestramento e quelli di test, e non a un declino specifico di alcuni tipi di entità nominate.</sample>
    <sample id="203">La posizionalità nella NLP è importante perché i dati e i modelli riflettono le prospettive e le esperienze di coloro che li creano, portando a bias sistematici che possono escludere o discriminare certe popolazioni. Questo influisce sulla capacità dei sistemi NLP di funzionare equamente per tutti gli utenti.</sample>
    <sample id="204">Gli LLM multilingue come BLOOM non sono stati affinati mediante adattatori o con una messa a punto integrale per i compiti di semantic parsing cross-linguistico.</sample>
    <sample id="205">Shangbin, PhD student alla University of Washington, presenta un lavoro che analizza come i pregiudizi politici si propaghino dai dati di pre-addestramento ai modelli linguistici e alle applicazioni downstream. I modelli linguistici, addestrati su grandi quantità di dati web, includono una vasta gamma di fonti giornalistiche, come New York Times e Huffington Post, che portano sia diversità di opinioni che potenziali bias. L'analisi mostra che i modelli linguistici presentano inclinazioni politiche diverse, con GPT-4 più liberale rispetto a BART. I pre-addestramenti su corpora partigiani evidenziano come i bias si adattino ai dati. Inoltre, i modelli addestrati su dati post-2017 mostrano una maggiore polarizzazione. Test su rilevamento di hate speech e fake news rivelano che modelli con inclinazioni politiche diverse hanno prestazioni non equitative: ad esempio, modelli di sinistra rilevano meglio hate speech contro minoranze, mentre quelli di destra lo fanno contro gruppi dominanti. Questo crea problemi di equità, soprattutto se i modelli vengono utilizzati in piattaforme sociali. L'articolo solleva un dilemma etico: eliminare i bias rischia di introdurre censura, mentre mantenerli porta a discriminazioni. Un problema complesso, simile al dilemma del trolley.</sample>
    <sample id="206">Per il trasferimento dell'apprendimento, il modello fa ricorso ai compiti di classificazione della posizione di dissonanza indipendente dal tema (debate) e alla classificazione binaria delle classi di espansione e confronto del PDTB (CE).</sample>
    <sample id="207">I recenti set di test utilizzati per valutare le capacità di PaLM sono i migliori set di test della comunità di traduzione automatica (MT), inclusi i set di valutazione WMT. Questi set sono stati selezionati per evitare sovrapposizioni tra i dati di test e i dati di addestramento del modello.</sample>
    <sample id="208">Gli autori hanno proposto tre suggerimenti alla fine.</sample>
    <sample id="209">Il guadagno del metodo proposto rispetto al metodo di riferimento è un miglioramento significativo nella qualità dei piani linguistici, sia in termini di completezza semantica che di fedeltà ai vincoli. Inoltre, il metodo consente di ottenere un dataset di alta qualità (CoScript) che permette a modelli più piccoli e specializzati di superare i grandi modelli linguistici quando addestrati su dati appropriati.</sample>
    <sample id="210">Il nome del relatore è Shuheng.</sample>
    <sample id="211">Sì, i risultati e il set di dati DEPLAIN possono essere utilizzati come parametri di riferimento per futuri lavori sull'automatizzazione della semplificazione del testo.</sample>
    <sample id="212">Nell'articolo non vengono specificati il numero esatto di modelli più piccoli utilizzati, ma si menziona che viene utilizzato un modello specializzato (T5) addestrato sul dataset CoScript.</sample>
    <sample id="213">Il modello utilizzato come modello di base per analizzare l'ottimizzazione delle istruzioni multimodali è OFA.</sample>
    <sample id="215">Il discorso di Adam Przepiórkowski si concentra sulla struttura dipendente della coordinazione, confrontando diversi approcci teorici. Alcune teorie, come Universal Dependencies e la teoria del significato-testo di Igor Mel'čuk, assumono una struttura asimmetrica, in cui il primo congiunto è il nucleo della struttura coordinata. Altri approcci, come quelli della Prague Dependency Treebank, considerano il congiuntivo come nucleo, mentre il Word Grammar di Hudson propone una struttura simmetrica, con tutti i congiunti come nuclei. L'obiettivo del lavoro è sostenere una struttura simmetrica attraverso il principio di minimizzazione della lunghezza delle dipendenze.

L'argomentazione si basa sull'osservazione che, in inglese, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli adjunct possono essere più distanti. Quando gli oggetti diretti sono lunghi, possono essere spostati dopo gli adjunct, riducendo la lunghezza totale delle dipendenze. Questo principio è stato applicato all'analisi di dati estratti dal Penn Treebank, che mostra che i congiunti a sinistra tendono a essere più brevi, soprattutto quando il governatore è a sinistra o assente. Questa tendenza scompare quando il governatore è a destra, suggerendo che le strutture asimmetriche non riescono a spiegare i dati. Questo supporta le strutture simmetriche come quelle del Word Grammar.</sample>
    <sample id="217">Il lavoro presentato, "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation", affronta il problema della generazione di dialoghi controllabili con più attributi, un aspetto spesso trascurato nei metodi esistenti che si concentrano su attributi singoli. Gli autori propongono DCG (Disentangled Controllable Generation), un modello che impara concetti di attributi da valori osservati e utilizza una perdita di disentanglement per separare combinazioni di attributi diverse. Introducono anche MAE (Multi-Attribute Evaluation), un framework di valutazione unificato e senza riferimenti, capace di valutare attributi discreti e continui senza dati annotati. Il modello si basa su DialoGPT e utilizza due tipi di prompt: uno orientato agli attributi e uno orientato al compito, combinati per migliorare la generazione e la capacità di generalizzazione. Gli esperimenti su benchmark come DailyDialog-CG mostrano che DCG supera i baselines in termini di controllabilità e qualità del testo, anche per combinazioni di attributi non viste. La valutazione con coefficienti di correlazione conferma l'efficacia di MAE, che si dimostra più affidabile rispetto a metriche classiche. Il lavoro dimostra che il metodo proposto permette una generalizzazione complessiva dagli attributi visti a quelli non visti, aprendo nuove possibilità per la generazione di dialoghi controllabili in contesti multi-attributo.</sample>
    <sample id="218">Gli autori dell'articolo sono affiliati a Google Translate.</sample>
    <sample id="219">Jia-Huei Ju presenta un lavoro sull'analisi dei rapporti finanziari, focalizzato sull'estrazione di segnali rilevanti tramite un pipeline multistadio. Il lavoro utilizza il Form 10-K, un rapporto annuale obbligatorio per le aziende statunitensi, e si basa sull'osservazione che i testi annuali sono molto simili tra loro, con circa l'80% dei token uguali. Per migliorare l'estrazione di informazioni utili, l'equipe ha introdotto un compito di "highlighting", che identifica le parole più rilevanti confrontando i rapporti di due anni consecutivi. Il pipeline proposto include tre fasi: segmentazione del documento, riconoscimento delle relazioni tra testi e fine-tuning del modello con dati esterni e interni. I dati utilizzati comprendono l'insieme eSNLI e il dataset interno FINAL. I risultati mostrano che il modello adattato al dominio ha ottime prestazioni su FINAL e mantiene una buona generalizzazione su eSNLI. Inoltre, il metodo si dimostra efficace anche su dati non utilizzati durante l'addestramento. Il lavoro apre la strada a futuri miglioramenti, come l'integrazione di ulteriori funzionalità e tecniche di recupero informazioni.</sample>
    <sample id="220">L'autore dell'articolo, Vasudha, è una candidata al dottorato in Computer Science all'Università di Stony Brook.</sample>
    <sample id="221">L'articolo non specifica le coppie linguistiche esattamente analizzate, ma menziona un esempio di traduzione dal tedesco all'inglese. Pertanto, almeno la coppia tedesco-inglese è stata analizzata.</sample>
    <sample id="222">**Abstract:**  
Questo lavoro indaga le sfide e le strategie di intervento per l'adattamento di dominio nell'ambito del Question Answering (QA) aperto. Utilizzando un esempio, si illustra come modelli pre-addestrati su Wikipedia, come retriever e reader, possano fallire quando applicati a domini specifici, come il biomedico, a causa di un'incapacità di discriminare tra contesti diversi. L'obiettivo è migliorare la generalizzazione di tali modelli attraverso interventi dati. L'approccio proposto include metodi a zero-shot e few-shot: nel primo, si controllano le interazioni tra questioni, contesti e risposte variando le loro distribuzioni; nel secondo, si utilizzano esempi del dominio target per generare nuovi dati. I risultati mostrano miglioramenti significativi, con un incremento medio del 11% per il reader e dell'8% per il retriever. Inoltre, si propone una misura di compatibilità tra modello e dominio, basata sulla probabilità assegnata al contesto e alle risposte da parte del modello sorgente. Questo permette di mappare i dataset target su una griglia 2D, identificando il tipo di shift (nessuno, concettuale, covariato o completo). I dati con shift concettuale o covariato rispondono positivamente agli interventi zero-shot, mentre quelli con shift completo beneficiano maggiormente delle strategie few-shot. In conclusione, il lavoro dimostra che l'efficacia degli interventi dati dipende dal tipo di shift e suggerisce strategie mirate per migliorare la performance in domini diversi.</sample>
    <sample id="223">Il nome del relatore è Shangbin.</sample>
    <sample id="224">Durante gli esperimenti sono stati studiati i modelli long-mBART e base mBART.</sample>
    <sample id="225">Out of the 62 diverse tasks used in MultiInstruct, 53 tasks from 9 groups are used for training, and additional tasks are reserved for testing, including the entire common sense reasoning group and 5 tasks from VQ and Miscellaneous groups.</sample>
    <sample id="226">Due autori sono coinvolti nell'articolo: Regina Stodden e Omar.</sample>
    <sample id="227">Il testo presenta un'analisi sulle limitazioni attuali dei modelli linguistici nell'ambito della "grounded language understanding", ovvero la capacità di collegare espressioni naturali a rappresentazioni eseguibili in un ambiente specifico, come query SQL o sequenze di azioni per robot. L'approccio tradizionale si basa sulla generazione di piani o programmi, ma spesso produce risultati non validi o grammaticalmente errati. Per risolvere questo problema, gli autori propongono un nuovo framework chiamato Pangu, che utilizza i modelli linguistici non per generare, ma per discriminare e valutare candidati piani proposti da un agente simbolico. Questo permette di evitare errori di validità e grammatica. I test su diversi modelli linguistici, come BERT, T5 e Codex, mostrano che Pangu supera i metodi esistenti in termini di efficienza e generalizzazione, specialmente in contesti non i.i.d. Il lavoro suggerisce che, per compiti di grounded language understanding, la discriminazione potrebbe essere una strategia più efficace rispetto alla generazione.</sample>
    <sample id="228">Gli autori hanno effettuato i test sui seguenti set di dati: AG News, MIND, SST2 e Enron Spam.</sample>
    <sample id="229">Gabriella Skitalinskaya presenta un lavoro congiunto con Henning Wachsmuth sull'identificazione di affermazioni migliorabili per supportare la scrittura argomentativa. La revisione del testo è un processo ricorsivo cruciale per raggiungere una formulazione ottimale, soprattutto in contesti argomentativi dove la chiarezza e la precisione influenzano l'effetto sul pubblico. Il lavoro introduce due compiti: la detezione di affermazioni non ottimali e la proposta di miglioramenti. L'obiettivo è aiutare gli autori a capire quando un'asserzione è sufficientemente ben formulata. L'approccio si basa sui dati di revisione impliciti, estratti da piattaforme come Kialo, dove le versioni finali sono considerate ottimali. Tuttavia, l'uso di tali dati presenta sfide come rappresentatività, complessità dei modelli, dipendenza dal contesto e bias topici o degli utenti. L'analisi mostra che i dati di revisione possono essere utilizzati efficacemente, e che modellare la distanza tra versioni di un'asserzione aiuta a rilevare quelle non ottimali. Il contesto influisce in modo diverso a seconda del compito e delle problematiche di qualità del testo. Per ulteriori dettagli, si rimanda al paper presentato.</sample>
    <sample id="231">NACHOS è un dataset di dati medici raccolti dal web, utilizzato per l'addestramento del modello DrBERT, il primo modello pre-addestrato in francese per domini biomedicali e clinici.</sample>
    <sample id="232">Il nome del relatore è David Vilar.</sample>
    <sample id="233">Sara Papi, dell'Università di Trento e della Fondazione Bruno Kessler, presenta il lavoro "Attention as a Guide for Simultaneous Speech Translation", realizzato con Matteo Negri e Marco Turchi. La traduzione simultanea (SimulST) converte in tempo reale parlato in testo in un'altra lingua, facilitando la comunicazione tra lingue. I modelli attuali richiedono architetture specifiche, ottimizzazioni complesse e diversi modelli per gestire differenti livelli di latenza. La soluzione proposta utilizza modelli offline pre-addestrati, senza modifiche architetturali, e gestisce la latenza tramite parametri specifici. L'attenzione tra input audio e output testuale è usata per decidere quando emettere traduzioni parziali. Il metodo EDAtt (Encoder-Decoder Attention) emette una parola solo quando l'attenzione non è concentrata sui frammenti audio più recenti, garantendo stabilità. I risultati mostrano che EDAtt supera le strategie tradizionali e quelle specifiche per SimulST, offrendo alta qualità traduttiva e bassa latenza. Il codice, i modelli e i dati sono disponibili open source per favorire la riproducibilità.</sample>
    <sample id="234">La strategia del prompting influisce significativamente sui risultati: nel test con 1,000 frasi, una differenza di oltre 1 punto BLEURT è stata osservata tra due prompt diversi, con casi estremi che raggiungono un divario di 40 punti BLEURT. Inoltre, è stata dimostrata l'importanza della qualità degli esempi forniti, con una strategia a 5 esempi che ha mostrato risultati quasi indipendenti dalla forma specifica del prompt.</sample>
    <sample id="235">Gli autori dell'articolo sono affiliati a diversi istituti: Kayo Yin, Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig. Tuttavia, nel testo non vengono specificate le loro affiliazioni istituzionali.</sample>
    <sample id="236">Le 5 istruzioni scritte da esperti sono parte del dataset MultiInstruct, ma nel testo non vengono specificate le loro esatte formulazioni. Il documento menziona solo che ogni task è equipaggiato con cinque istruzioni esperte, senza elencarle.</sample>
    <sample id="237">Gli autori propongono un test chiamato KITMUS, un insieme di prove diagnostiche progettate per valutare la capacità dei modelli di integrare conoscenza proveniente da fonti diverse, come la conoscenza pre-addestrata e quella fornita durante l'inferenza. Il test si concentra su compiti di risoluzione della coerenza referenziale, dove i modelli devono utilizzare informazioni specifiche sugli entità e conoscenza di fondo per risolvere correttamente i riferimenti pronominiali.</sample>
    <sample id="238">In questo video, Yebowen Hu presenta MeetingBank, un nuovo dataset benchmark per la sommariizzazione di riunioni. Il dataset è stato creato per rispondere all'esigenza di tecnologie di sommariizzazione adatte a diversi contesti. MeetingBank include trascrizioni di riunioni del Consiglio Cittadino, sommari esperti e ulteriori risorse. Il processo di raccolta dati ha coinvolto la conversione audio in testo, l'identificazione unica delle riunioni e l'allineamento dei timestamp per ottenere sommari accurati. Il dataset comprende 1.366 riunioni e circa 7.000 istanze, con statistiche dettagliate su durata, numero di token e speaker. L'analisi ha valutato il livello di astrazione dei sommari, mostrando che i sommari dei Consigli di Seattle e Boston sono più densi, mentre quelli di Denver sono meno editati. Diversi sistemi di sommariizzazione, tra cui modelli estrattivi e abstrattivi, sono stati testati. Pur non ottenendo buoni risultati automatici, GPT-3 ha ottenuto punteggi elevati in termini di fluency e coherence durante l'assessment umano. L'obiettivo principale è fornire un utile strumento per la ricerca e offrire isight sulla decisione politica. L'invito finale è a scaricare e utilizzare MeetingBank per ulteriori studi.</sample>
    <sample id="239">Ciao a tutti, il mio nome è David Vilar e oggi darò un breve resoconto del paper "Prompting PaLM for Translation: Assessing Strategies and Performance". Questo lavoro è stato realizzato in collaborazione con i miei colleghi del team Google Translate. PaLM è un modello linguistico di grandi dimensioni con 540 miliardi di parametri, presentato l'anno scorso nel 2022. È stato addestrato su un vasto insieme di testi, comprendenti 780 miliardi di token. Al momento della pubblicazione, ha raggiunto lo stato dell'arte in centinaia di compiti di NLP. In questo lavoro, presentiamo il primo studio sistematico sull'uso di prompt per modelli linguistici di grandi dimensioni nel contesto della traduzione automatica. Abbiamo valutato la capacità di traduzione di tali modelli utilizzando le migliori pratiche della comunità di traduzione automatica. Questo include l'uso dei set di test più recenti per evitare sovrapposizioni tra i dati di test e quelli di addestramento del modello linguistico. Abbiamo confrontato i risultati con i sistemi all'avanguardia, in particolare con i risultati del WMT. Abbiamo utilizzato metriche avanzate di traduzione automatica neurale e inoltre mostriamo i risultati di valutazioni umane basate su esperti. Alla fine, forniamo alcune raccomandazioni per le strategie di selezione degli prompt. L'uso degli prompt ha un grande impatto sulle prestazioni dei modelli linguistici di grandi dimensioni per la traduzione, come si può vedere in un semplice esperimento in cui abbiamo utilizzato un prompt a un solo esempio e fornito due diversi prompt per ciascuna frase. Per la maggior parte delle frasi, 516 su 1000, la differenza osservata è superiore di un punto BLEURT. In casi estremi, questa differenza può arrivare fino a 40 punti BLEURT. Quindi, è importante selezionare una buona strategia di prompting. Nei nostri esperimenti, abbiamo scelto una strategia di prompting a 5 esempi, dove abbiamo semplicemente segnalato ad ogni frase fornita al sistema la lingua in cui era scritta. In questo esempio, dove traduciamo dal tedesco all'inglese, le frasi tedesche, quelle di origine, sono contrassegnate con due punti e un colonne tedesca e le traduzioni in inglese con due punti e una colonne inglese. Abbiamo visto che la forma effettiva del prompt non ha un grande impatto nel caso di prompt brevi. È cruciale per i prompt a zero e a un esempio. Quando, come nel nostro caso, utilizziamo un prompt a cinque esempi, non c'è quasi differenza nella forma effettiva del prompt. Sono gli esempi stessi a portare il maggior peso. In sintesi, i nostri risultati sperimentali indicano che la qualità degli esempi è più importante della somiglianza con la frase originale. È quindi importante selezionare esempi provenienti da traduzioni di alta qualità. In particolare, confrontiamo la selezione di prompt utilizzando i dati di addestramento per le valutazioni WMT sui dati di sviluppo. I dati di sviluppo sono molto più curati e di alta qualità rispetto ai dati di addestramento, che sono più rumorosi. I loro risultati mostrano un miglioramento delle prestazioni quando si utilizzano i dati di sviluppo. Tuttavia, i sistemi specializzati all'avanguardia hanno un vantaggio sostanziale rispetto alle traduzioni di PaLM. Ma PaLM si avvicina abbastanza a un sistema commerciale. Nel nostro caso, abbiamo scelto di valutare con Google Translate. Le informazioni che abbiamo ottenuto dalla valutazione umana che abbiamo condotto utilizzando il framework MQM indicano che la fluidità di PaLM è paragonabile ai sistemi all'avanguardia, ma la differenza principale risiede nell'accuratezza. In particolare, gli errori più comuni sono gli errori di omissione. Sembra che PaLM scelga di produrre una traduzione che suona meglio, talvolta eliminando parti della frase originale durante la traduzione. Tuttavia, la categoria "Stile/Awkward" per PaLM è inferiore rispetto ai sistemi all'avanguardia, un ulteriore segnale che PaLM produce output molto fluidi, ma comunque con alcuni problemi di accuratezza. Questo conclude il mio breve resoconto. Per maggiori dettagli, vi invitiamo a partecipare alla presentazione completa del paper. Grazie molto.</sample>
    <sample id="240">Ciao, mi chiamo Dawei, sono un dottorando di ricerca all'Università di Saarland in Germania. In questo video, vorrei presentare il nostro lavoro recente intitolato "Weaker Than You Think: A Critical Look at Weakly Supervised Learning". Questo lavoro è stato realizzato in collaborazione con Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow. Vorrei cominciare con una breve introduzione al concetto di supervisione debole e apprendimento supervisionato debole. Nella supervisione debole, non si etichetta manualmente i dati. Invece, si etichetta i dati utilizzando fonti di etichettatura debole, come semplici regole euristiche, basi di conoscenza o crowdsourcing di bassa qualità, come illustrato nella figura a destra. Rispetto all'annotazione umana, queste etichette sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di etichette è errata. Se addestriamo direttamente le reti neurali sui dati etichettati in modo debole, le reti tendono a memorizzare il rumore delle etichette e non generalizzano. Nell'apprendimento supervisionato debole, vengono proposti algoritmi di addestramento per addestrare robustamente le reti neurali in presenza di tali rumori di etichetta in modo che i modelli addestrati generalizzino comunque bene. Nei lavori recenti sull'WSL (Weakly Supervised Learning), un'affermazione comune è che si addestrino i modelli solo sui dati etichettati in modo debole e si raggiunga un'alta performance su insiemi di test puliti. Dal punto di vista tecnico, questa affermazione non è errata, ma c'è un problema, che è che si assume che esista un insieme di validazione pulito disponibile per la selezione del modello. Non possiamo fermarci a questo setting del problema, ma ciò implica che siano necessarie ulteriori annotazioni manuali nell'apprendimento supervisionato debole. Tuttavia, questa necessità è spesso ignorata, come un elefante nella stanza. Le domande sollevate sopra portano a tre domande di ricerca. Prima, è necessario un insieme di validazione pulito per l'WSL, oppure si potrebbe utilizzare un insieme di validazione rumoroso? Secondo, se i dati puliti sono necessari o obbligatori affinché l'WSL funzioni, quanti campioni puliti sono necessari? Infine, dovremmo utilizzare solo i campioni puliti per la validazione, oppure esistono modi migliori per sfruttarli? Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti. Primo, abbiamo scoperto che, in modo interessante, i metodi recenti di WSL richiedono effettivamente campioni di validazione puliti per funzionare correttamente. Altrimenti, si verifica un grande calo di prestazioni. Come si vede in questa figura, se non ci sono campioni di validazione puliti, i modelli addestrati non riescono a generalizzare al di là delle etichette originali deboli, il che significa che l'addestramento è inutile. Questo indica che i metodi WSL richiedono effettivamente dati etichettati correttamente per funzionare correttamente, e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato. Il nostro secondo risultato è che aumentare il numero di campioni di validazione puliti aiuta i metodi WSL a raggiungere prestazioni migliori, come mostrato nella figura a sinistra. Di solito, si necessita di circa 20 campioni per classe per raggiungere prestazioni elevate. Ma non finisce qui, perché se comunque decidiamo di ottenere campioni puliti, l'addestramento su di essi direttamente porterà a prestazioni ancora migliori. La figura a destra mostra la differenza di prestazioni tra approcci di fine-tuning, che vengono applicati direttamente sui dati puliti, e approcci WSL, che utilizzano i dati puliti solo per la validazione. Come possiamo vedere, se abbiamo 10 campioni per classe, il fine-tuning diretto inizia a superare gli approcci WSL. Infine, l'aumento delle prestazioni dichiarato negli approcci WSL precedenti può essere facilmente ottenuto permettendo di continuare il fine-tuning sui campioni di validazione puliti. Come possiamo vedere dalle figure, il modello base, denominato FTw, inizialmente underperform rispetto a metodi WSL più complessi, come COSINE. Tuttavia, se permettiamo di continuare il fine-tuning sui campioni puliti, FTw mostra prestazioni pari ad altri metodi. Quindi, in pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio su disco. Per sintetizzare, abbiamo dimostrato che gli approcci recenti di WSL richiedono campioni manualmente annotati e puliti per funzionare correttamente. Il loro guadagno di prestazioni e praticità vengono pesantemente sopravvalutati. Le nostre raccomandazioni concrete per i lavori futuri sono le seguenti. Prima, riferire i criteri per la selezione del modello. Ad esempio, riferire se la selezione del modello è fatta utilizzando campioni di validazione puliti. Secondo, gli approcci WSL dovrebbero essere confrontati con baselines di few-shot learning, poiché entrambi lavorano su campioni puliti. Terzo, il fine-tuning continuo è un baseline semplice ma potente che dovrebbe essere considerato nei futuri lavori sull'WSL. Infine, abbiamo open-sourced il nostro codice. Potete trovarlo tramite il codice QR presente in questa slide. Vi prego di controllarlo. Grazie e buona conferenza.</sample>
    <sample id="241">**Abstract:**  
In questo lavoro, Ethan e i suoi colleghi presentano un framework per l'valutazione di sistemi di rilevamento dell'fake news basati sull'interazione uomo-sistema, applicato al contesto delle fake news sui trattamenti del COVID-19. L'approccio proposto mira a risolvere due limitazioni principali dei sistemi esistenti: l'uso di dati retrospecitivi e la mancanza di coinvolgimento umano durante il processo di rilevamento. Il framework proposto è end-to-end, integrando feedback umano in diversi stadi del processo, dal rilevamento iniziale di affermazioni potenzialmente false alla verifica delle violazioni delle policy. Il sistema utilizza un modello T5 per l'estrazione di affermazioni e un modello BERT per la classificazione della posizione dell'autore rispetto a trattamenti non approvati. L'efficacia del sistema è valutata in termini di capacità di rilevare affermazioni errate prima che vengano smentite da fonti attendibili, nonché di identificare violazioni delle policy sociali. I risultati mostrano che il sistema riesce a rilevare il 65% delle violazioni e a identificare 124.2 violazioni per ora di lavoro umano. L'approccio proposto offre una valutazione più realistica e uman-centric del processo di rilevamento dell'fake news, fornendo un modello replicabile per futuri sistemi di rilevamento. Il lavoro sottolinea l'importanza di integrare feedback umano per migliorare l'efficacia e l'utilità pratica dei sistemi automatizzati.</sample>
    <sample id="242">I metodi di valutazione comuni per i sistemi di dialogo includono l'utilizzo di valutazioni umane, come quelle in cui giudici umani selezionano quale tra due conversazioni è migliore o assegnano un punteggio utilizzando una scala di Likert. Questi approcci forniscono una valutazione globale della qualità del dialogo, ma non sempre riescono a catturare aspetti specifici e dettagliati.</sample>
    <sample id="243">L'articolo è stato realizzato in collaborazione con un totale di 6 autori: Sebastian Santy, Ronan Le Bras, Katharina Reinecke, Maarten Sap, Jenny (l'autrice) e un altro collaboratore non specificato. Tuttavia, sulla base del testo, vengono menzionati chiaramente 5 nomi oltre a Jenny. Pertanto, il numero totale di autori coinvolti nell'articolo è **6**.</sample>
    <sample id="244">Nell'esempio con Servin e Kea, la conoscenza di base necessaria è che "i giudici decidono casi in corti di giustizia".</sample>
    <sample id="245">L'articolo presentato da Lining Zhang e i suoi colleghi analizza un pipeline per identificare lavoratori ad alta concordanza su MTurk per compiti di sintesi. Il metodo proposto prevede due fasi: una "Qualification Task" e un "Endurance Task", che valutano rispettivamente l'abilità di valutazione multidimensionale e la capacità di gestire carichi di lavoro pesanti. Dopo una selezione iniziale basata su qualifiche predefinite, vengono selezionati 26 lavoratori, di cui 8 "gold" e 18 "silver", che passano la prima fase. Dopo la seconda fase, ne rimangono 12, con 4 gold e 8 silver, che mostrano un alto livello di concordanza (IAA) superiore agli esperti. Il test basato su riferimenti dimostra una buona performance, con un Krippendorff's Alpha di 0.534. Confronti con metodi baseline e CloudResearch mostrano che il pipeline è efficace e economico, con risultati simili a quelli di annotatori di alta qualità. Tuttavia, esistono limiti: il lavoro si concentra solo su inglese, le domande non sono soluzioni universali e non è garantita la correttezza dell'annotazione. Lo studio suggerisce un modello promettente per annotazioni di alta qualità su larga scala a basso costo, con futuri sviluppi su diversi compiti, lingue e piattaforme.</sample>
    <sample id="246">Sì, il codice è disponibile su GitHub.</sample>
    <sample id="247">Jiho Kim da KAIST AI presenta il lavoro "FACTKG: Fact Verification via Reasoning on Knowledge Graphs", in cui propone un nuovo compito di verifica della verità basato su grafi di conoscenza. Finora i dataset esistenti come FEVER, VitaminC, TabFact e InfoTabs utilizzano testo o tabelle come fonte di evidenza, ma non grafi di conoscenza. L'obiettivo di FACTKG è colmare questa lacuna, sfruttando il potenziale dei grafi di conoscenza per una verifica più affidabile e diretta. Il dataset utilizza DBpedia e include affermazioni in due stili: scritto e colloquiale, per maggiore applicabilità pratica. Le affermazioni sono etichettate come "SUPPORTED" o "REFUTED", e richiedono diversi tipi di ragionamento, tra cui one-hop, conjunction, existence, multi-hop e negation. Per creare le affermazioni colloquiali, sono state utilizzate tecniche di trasferimento di stile e template basati sulla presupposizione. I risultati mostrano che i modelli che utilizzano evidenze da grafi di conoscenza, come il GEAR, superano significativamente le baselines che si basano solo sulle affermazioni. Il dataset FACTKG rappresenta un passo avanti per la verifica fatti in contesti che richiedono una coerenza tra grafi di conoscenza e linguaggio naturale.</sample>
    <sample id="248">No, gli annotatori per NLPositionality non sono bilanciati rispetto a ciascun gruppo demografico. Il lavoro mostra che i dati e i modelli NLP sono allineati principalmente con popolazioni specifiche, come i paesi anglofoni e le persone con istruzione universitaria, lasciando fuori gruppi come le persone non binarie. L'obiettivo dello studio era invece rivelare queste discrepanze, non necessariamente ottenere un campione bilanciato.</sample>
    <sample id="249">Le frasi nel dominio accettabile sono state perturbate mantenendo intatta la struttura rilevante, ma aggiungendo rumore all'input. Queste perturbazioni non hanno modificato in modo significativo il giudizio di accettabilità del modello, che ha mostrato una risposta simile in tutti i casi.</sample>
    <sample id="250">Avere una valutazione dimensionale significa analizzare e valutare diversi aspetti specifici della qualità di un dialogo (come rilevanza, coerenza, empatia, ecc.) in modo dettagliato, piuttosto che fornire un giudizio globale. Questo permette di comprendere meglio i punti di forza e di debolezza di un modello di AI conversazionale.</sample>
    <sample id="251">Gli autori dell'articolo sono affiliati all'University of Science and Technology of China.</sample>
    <sample id="252">Sai Kiran Tanikella e il suo team presentano U-CREAT, un approccio non supervisionato per il recupero di casi legali precedenti. Il lavoro si concentra sulle sfide del recupero di casi legali rilevanti, un compito cruciale per avvocati e giudici, reso complesso dall'enorme quantità di documenti legali. Per affrontare questa sfida, i ricercatori hanno creato il dataset IL-PCR, un nuovo benchmark per il recupero di casi legali indiano, composto da 7.070 casi con un elevato numero di citazioni. Inoltre, hanno sviluppato il pipeline U-CREAT, che utilizza un approccio basato sugli eventi estratti dai testi legali, ottenuti tramite analisi delle dipendenze grammaticali e triplette soggetto-verbo-oggetto. Questo metodo permette un recupero efficiente e generalizzabile, senza la necessità di addestramento specifico per il diritto o la demografia. Gli esperimenti mostrano che i modelli basati sugli eventi, in particolare il modello Event Filtered Docs, superano significativamente i metodi di baseline e i modelli basati su trasformatori. U-CREAT ha dimostrato prestazioni eccellenti anche su dataset canadese (COLIEE’21), superando metodi supervisionati recenti. Questo lavoro apre nuove possibilità per lo sviluppo di tecnologie di supporto alla ricerca legale.</sample>
    <sample id="253">Mario Ezra Aragón presenta il lavoro "DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media", realizzato da un gruppo di ricerca messicano e spagnolo. Lo studio mira a rilevare segni di disturbi mentali analizzando i post sui social media. L'obiettivo è sviluppare una tecnologia in grado di rilevare precocemente i disturbi e fornire supporto. Per affrontare la scarsità di dati annotati, l'approccio proposto utilizza l'adattamento di dominio, addestrando un modello linguistico (BERT) su dati specifici di Reddit e disturbi mentali. DisorBERT integra anche un lessico per guidare il processo di mascheramento, migliorando la capacità del modello di focalizzarsi su parole rilevanti. I risultati mostrano un buon equilibrio tra precisione e recall, superando modelli come MentalBERT. L'analisi di esempi del Beck Depression Inventory rivela che DisorBERT genera risposte con un significato più negativo e psicologico. L'uso di strumenti di visualizzazione conferma che parole come "anxious" e "medication" sono particolarmente rilevanti. In conclusione, l'adattamento doppio e il mascheramento guidato permettono di catturare efficacemente segnali di disturbi mentali, con risultati promettenti per futuri sviluppi.</sample>
    <sample id="254">In questo lavoro, Sun Qi e il team presentano un framework innovativo per l'estrazione di relazioni a livello documentale (DocRE), chiamato "Uncertainty Guided Label Denoising". L'obiettivo è migliorare la qualità delle etichette generate da dati distantly supervised (DS), spesso affetti da rumore. I metodi precedenti utilizzavano etichette pseudo-annotate, ma correvano il rischio di introdurre errori. Per mitigare questo problema, l'approccio proposto introduce una stima dell'incertezza per valutare la fiducia nei predizioni del modello. Utilizzando la tecnica Monte Carlo dropout, si calcolano le score di incertezza a livello di istanza, soprattutto per relazioni sovrapposte. Inoltre, vengono introdotti threshold dinamici per le classi di relazione, per filtrare le etichette poco affidabili. Un'ulteriore innovazione è una strategia di addestramento multi-fase, che permette di iterativamente rietichettare i dati DS. I risultati sperimentali mostrano un miglioramento significativo rispetto ai metodi di baseline su due dataset pubblici. Le principali contribuzioni includono: un framework per la denoising delle etichette basato sull'incertezza, una stima dell'incertezza a livello di istanza, una strategia di rietichettatura iterativa con threshold dinamici, e un notevole miglioramento delle prestazioni nel DocRE.</sample>
    <sample id="255">La forma del prompting si rivela importante nei casi di zero-shot e one-shot prompting. In questi scenari, la struttura e la formulazione del prompt hanno un impatto significativo sulle prestazioni del modello. Al contrario, quando si utilizza un approccio a 5-shot, la forma del prompting ha un'influenza trascurabile, poiché gli esempi forniti rappresentano il fattore determinante.</sample>
    <sample id="257">Gli autori hanno valutato quattro modelli di dialogo all'avanguardia.</sample>
    <sample id="258">Chiang Cheng-Han presenta un lavoro in cui si propone l'uso dei Large Language Models (LLM) come alternativa all'valutazione umana nella valutazione della qualità del testo. L'idea è di fornire agli LLM istruzioni specifiche e campioni da valutare, in modo che possano assegnare punteggi basati su attributi come grammatica, coerenza, piacevolezza e rilevanza. Questo approccio mira a superare i limiti dell'valutazione umana, come l'instabilità e la difficoltà di riproducibilità. Per verificare l'efficacia degli LLM, i ricercatori hanno confrontato le loro valutazioni con quelle di insegnanti di inglese, considerati esperti. I risultati mostrano che alcuni LLM, come Davinci e ChatGPT, riescono a discriminare tra testi scritti da umani e da GPT-2, mostrando una preferenza simile a quella degli insegnanti. Questo suggerisce che alcuni LLM possono essere utilizzati come alternativa all'valutazione umana. Il lavoro risponde a diverse domande, come l'effetto delle istruzioni, la riproducibilità e i vantaggi e i costi rispetto all'valutazione umana, e fornisce risultati su altre task. Il lavoro è stato presentato al convegno ACL e i dettagli sono disponibili nel paper.</sample>
    <sample id="259">Yusen Zhang, dell'Università di Penn State, presenta XSemPLR, un benchmark unificato per il parsing semantico cross-linguistico in diverse lingue naturali e rappresentazioni semantiche. Il dataset include 9 dataset di diversi domini, 5 compiti di parsing semantico, 8 rappresentazioni semantiche e 22 lingue appartenenti a 15 famiglie linguistiche. XSemPLR valuta sei scenari di addestramento ed esecuzione, tra cui Translate-Test, modelli monolingue, multilingue, e transfer zero-shot e few-shot. Gli esperimenti mostrano che i modelli Encoder-Decoder (come mT5) ottengono prestazioni migliori rispetto a quelli basati su encoder e pointer (come XLM-R + PTR). L'addestramento multilingue migliora le prestazioni in molte lingue, ma causa una riduzione delle performance in inglese, un fenomeno chiamato "Curse of Multilinguality". Inoltre, il transfer few-shot riduce il gap di performance rispetto al transfer zero-shot. I risultati evidenziano che i modelli multilingue come Codex e BLOOM non sono ancora sufficientemente adatti per compiti di parsing semantico cross-linguistico. XSemPLR offre un'analisi approfondita e un riferimento per futuri studi in questo campo.</sample>
    <sample id="260">La domanda non fornisce informazioni specifiche sul numero di autori coinvolti nell'articolo. Pertanto, non è possibile determinare il numero di autori sulla base del contenuto fornito.</sample>
    <sample id="261">Un buon pianificatore dovrebbe produrre script che siano **ragionevoli** e **fedeli ai vincoli** imposti sui goal specifici.</sample>
    <sample id="262">L'articolo non specifica il numero di autori coinvolti.</sample>
    <sample id="263">L'articolo presenta un'analisi sistematica dei bias dei label nell'apprendimento in contesto (in-context learning) nei modelli linguistici di grandi dimensioni. L'apprendimento in contesto è noto per essere instabile a causa di scelte di design come l'ordine e la selezione degli esempi di contesto, che introducono bias nelle previsioni del modello. L'opera introduce una tipologia di bias dei label, identificando un nuovo tipo, il "domain-label bias", che si verifica quando il corpus del compito influisce sulle previsioni del modello. Per mitigare questi bias, i ricercatori propongono un metodo di calibrazione innovativo chiamato "domain-context calibration", che utilizza testi "content-free" casuali estratti dal corpus del compito per stimare e correggere i bias. Gli esperimenti mostrano che il metodo migliora significativamente le prestazioni dell'apprendimento in contesto, soprattutto nei compiti con un alto livello di domain-label bias. L'approccio è efficace anche su modelli più grandi come GPT-3. L'articolo sottolinea l'importanza di considerare i diversi tipi di bias per migliorare la stabilità e l'affidabilità dell'apprendimento in contesto.</sample>
    <sample id="264">Lin Wang, una studentessa del dottorato all'Università di Zhejiang, ha presentato il lavoro intitolato "TAVT: Towards Transferable Audio-Visual Text Generation". Il lavoro affronta il problema della generazione di testo multimodale (audio-visual) in contesti diversi, dove la mancanza di dati annotati e le variazioni tra domini causano degradazione del modello. Per risolvere questo problema, Wang propone un nuovo compito: la generazione di testo audio-visual trasferibile, che mira a creare modelli in grado di adattarsi rapidamente a nuovi domini con pochi dati. Il framework proposto include una rete meta-mapper per allineare i concetti visivi in uno spazio semantico audio unificato, un encoder e generatore basati su Transformer, e un meccanismo di apprendimento contrastivo counterfactual per migliorare l’allineamento tra visivo e testo. I test su benchmark come MSVD e MSR-VTT mostrano che TAVT supera i metodi esistenti, specialmente in scenari a basso risorsa, dove altri modelli subiscono un calo di prestazioni. Gli esperimenti di ablazione confermano l’importanza delle caratteristiche audio nel migliorare le performance del modello. In sintesi, TAVT rappresenta un passo avanti nella generazione multimodale trasferibile, con un framework modulare e adattabile a diversi contesti.</sample>
    <sample id="265">Il nome della relatrice è Vasudha.</sample>
    <sample id="266">L'articolo non menziona le affiliazioni degli autori.</sample>
    <sample id="268">Gli errori più comuni di PaLM sono gli omissioni, ovvero la mancata traduzione di parti del testo sorgente.</sample>
    <sample id="269">Ciao, sono James Finch. E sono Sarah Finch. Oggi ti parleremo di ABC-Eval, un nuovo approccio multidimensionale per valutare l'AI conversazionale. Questo lavoro è stato realizzato dal laboratorio NLP dell'Emory University, guidato dal professor Jinho Choi, e in collaborazione con Amazon Alexa AI. Immagina di aver sviluppato un modello di dialogo e di voler capire quanto bene si confronta con lo stato dell'arte. La pratica comune è utilizzare valutazioni umane, ad esempio chiedendo a giudici umani di selezionare quale tra due conversazioni è migliore o di valutare le conversazioni su una scala di Likert. Questi approcci funzionano bene per fornire valutazioni complessive della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto, potresti voler valutare diversi aspetti della qualità del chat per comprendere i punti di forza e di debolezza del modello a un livello più fine. Un approccio potrebbe essere semplicemente chiedere a giudici umani di valutare diversi aspetti della qualità del dialogo, ad esempio la rilevanza delle risposte del modello utilizzando metodi esistenti di confronto o scale di Likert. Tuttavia, crediamo che esista una strategia più precisa e affidabile per la valutazione multidimensionale del dialogo. Il nostro approccio tenta di ridurre la soggettività della valutazione umana, annotando esplicitamente se ogni risposta del modello esprime certi comportamenti, ad esempio rispondere con informazioni irrilevanti o contraddire se stesso. Chiamiamo questo approccio "annotazione dei comportamenti nel chat" o ABC-Eval, per breve. Abbiamo sviluppato questo metodo per coprire in modo completo i comportamenti dei modelli di chat che recentemente sono stati suggeriti come influenzatori della qualità del chat. ABC-Eval è in grado di misurare le percentuali in cui i modelli di chat commettono diversi tipi di errori tematici. Ad esempio, ABC-Eval misura il numero di turni in cui un modello di chat ignora il partner o dice qualcosa di irrilevante, si contraddice o si contraddice il partner, inventa fatti errati o viola la conoscenza comune, e quando il modello riesce o fallisce nel mostrare empatia. Per determinare che tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su 100 conversazioni umano-bot per modello utilizzando ABC-Eval. Per confronto, abbiamo valutato queste conversazioni anche utilizzando tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti a coppie a livello di dialogo. Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché è la pratica standard per valutare i modelli di chat lungo diverse dimensioni. Dall'analisi dei risultati di queste valutazioni, abbiamo scoperto che le etichette di comportamento di ABC-Eval sono in generale più affidabili rispetto alle etichette raccolte con metodi esistenti, come misurato dall'accordo tra annotatori su 100 conversazioni doppie. Inoltre, le etichette di ABC-Eval sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte da metodi esistenti, come mostrato da questa semplice analisi di regressione lineare. Ad esempio, puoi vedere come la misurazione della proporzione di turni con contraddizioni tra il modello e il partner spiega rispettivamente il 5% e il 10% della qualità della conversazione, mentre le valutazioni medie Likert spiegano solo il 4% o meno. Infine, abbiamo verificato se ciascuna metrica di valutazione cattura un aspetto unico della qualità del chat utilizzando una regressione lineare stepwise. Puoi vedere come la combinazione di tutte le metriche ABC-Eval spiega più del 25% della qualità della conversazione, e quando rimuovi una metrica alla volta, la maggior parte di esse comporta la perdita di una quantità significativa di informazioni sulla qualità. Dall'altra parte, la combinazione di tutte le metriche Likert a livello di turno spiega molto meno della qualità, e poche di queste metriche portano informazioni uniche. Queste metriche ABC-Eval affidabili, informative e distinte ci permettono di valutare l'AI conversazionale con una risoluzione più elevata rispetto a quanto i metodi precedenti riescono a raggiungere. Puoi vedere che nei risultati del nostro esperimento rimangono diverse sfide, che sono state quantificate in modo preciso. Ad esempio, i bot che abbiamo testato violano la conoscenza comune in circa il 20% delle loro risposte. Producono informazioni irrilevanti in circa il 15% delle risposte e si contraddicono o si contraddicono il partner circa il 10% delle volte. Con il rapido ritmo di progresso nel campo, molte di queste percentuali di errori potrebbero diminuire nei nuovi modelli rilasciati da quando abbiamo condotto la nostra valutazione. Tuttavia, questo è motivo ancora più forte per perseguire metriche di valutazione affidabili e precise per confrontare i modelli. Speriamo che ABC-Eval possa essere utilizzato da altri nel campo come un passo significativo in questa direzione. E aspettiamo con interesse di vedere come l'AI conversazionale avanzrà nei prossimi mesi e anni. Grazie per aver guardato.</sample>
    <sample id="270">The authors of the article are affiliated with the Emory NLP Lab led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI.</sample>
    <sample id="271">In questo articolo, **CFT** sta per **Clean Fine-Tuning**, ovvero l'addestramento fine-tuning effettuato su dati puliti (manualmente annotati). L'articolo confronta questo approccio semplice con metodi più complessi di apprendimento supervisionato debole (WSL), mostrando che il CFT può raggiungere prestazioni paragonabili o superiori con un costo computazionale inferiore.</sample>
    <sample id="272">L'articolo è un lavoro congiunto di 7 autori: John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy e Adina Williams, oltre all'autore principale Koustav Sinha. In totale, sono coinvolti **7 autori**.</sample>
    <sample id="273">Ciao, mi chiamo Kayo Yin e presenterò il nostro lavoro intitolato "Quando la traduzione richiede contesto? Un'indagine multilingue basata sui dati". Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig. Molte traduzioni dipendono dal contesto. Ad esempio, come tradurrebbero "mole" in questa frase? Se la frase precedente fosse "Le cose potrebbero diventare pericolose se i ministri lo scoprissero", "mole" si riferisce a un informatore. Ma se la frase precedente fosse "Potrebbe essere qualcosa di grave, dottore?", "mole" si riferisce a una macchia di nascita. Quindi, a seconda del contesto, il significato della parola cambia, e di conseguenza anche la sua traduzione. Tuttavia, valutare quanto bene i modelli riescono a tradurre casi come questo è abbastanza difficile. In primo luogo, perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende metriche a livello di corpus come BLEU inadeguate a catturare tali traduzioni. Inoltre, alcuni hanno suggerito valutazioni mirate per traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e limitati insiemi di lingue, poiché spesso si basano su conoscenze di dominio e curazione umana. In questo lavoro, cerchiamo di rispondere a queste due domande. Prima, quando la traduzione richiede contesto? E seconda, quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipende dal contesto durante la traduzione. Nel lavoro precedente, abbiamo introdotto CXMI come misura dell'uso del contesto da parte dei modelli di traduzione automatica. Questo si fa misurando quanto informazione il contesto C fornisce sul target Y, dato il source X. Si può pensare a CXMI come all'informazione guadagnata dal fornire contesto al modello. In questo lavoro, estendiamo CXMI a Pointwise CXMI, che può misurare l'uso del contesto a livello di frase o a livello di parola. Si può pensare che le parole con un alto P-CXMI siano quelle che richiedono contesto per la traduzione. Ora analizziamo le parole con alto P-CXMI per cercare pattern tra queste parole. Eseguiamo l'analisi sui trascritti di discorsi TED tradotti dall'inglese in 14 diverse lingue. Eseguiamo l'analisi a tre diversi livelli. In primo luogo, guardiamo ai tag di parte del discorso che hanno un alto valore medio di P-CXMI. Questo ci permette di trovare, ad esempio, i pronomi duali nell'arabo che hanno un P-CXMI relativamente alto. Questo può essere spiegato perché l'inglese non ha pronomi duali, quindi è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo. Allo stesso modo, troviamo che certe lingue richiedono anche contesto quando si desidera scegliere la forma appropriata del verbo. Poi guardiamo agli elementi lessicali che hanno un alto P-CXMI in media su tutti i loro diversi usi. Questo ci aiuta a identificare casi come questo, dove in cinese si ha bisogno di contesto per tradurre i nomi propri, per assicurarsi di utilizzare la stessa traduzione all'interno del documento. Allo stesso modo, troviamo che il contesto è importante per tradurre nel livello di formalità giusto. Infine, guardiamo a diversi token individuali che hanno un alto P-CXMI. Questo ci permette di identificare fenomeni che non possono essere realmente catturati dalla parola stessa, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione delle ellissi. Ora utilizziamo i nostri risultati dall'analisi per progettare un benchmark per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni discorsivi che abbiamo identificato, creiamo dei tagger per identificare automaticamente le parole che appartengono al fenomeno. Abbiamo chiamato il nostro tagger MuDA tagger, che sta per Multilingual Discourse-Aware. Possiamo quindi anche notare che diverse lingue hanno proporzioni diverse di questi fenomeni discorsivi. Utilizziamo quindi il MuDA tagger applicandolo su un corpus parallelo che vogliamo utilizzare per l'valutazione e applichiamo le nostre metriche di traduzione scelte sugli esempi dipendenti dal contesto che il MuDA tagger ha identificato. Infine, utilizziamo il nostro benchmark nonché altre metriche per valutare diversi modelli sulla traduzione a livello di documento. Innanzitutto, quando utilizziamo metriche a livello di corpus: per BLEU, troviamo che i modelli non dipendenti dal contesto hanno le prestazioni migliori. Ma se utilizziamo COMET, i modelli a conoscenza del contesto hanno le prestazioni migliori. E se utilizziamo la misura f-sintattica a livello di parola, i modelli con e senza contesto hanno prestazioni comparabili. Questo dimostra nuovamente che è difficile determinare il miglior sistema di traduzione a livello di documento se si utilizzano solo metriche a livello di corpus. Ora utilizziamo il benchmark MuDA per valutare i modelli e troviamo che i modelli a conoscenza del contesto sono significativamente più accurati dei modelli che non utilizzano il contesto per certi fenomeni discorsivi come la formalità e la coesione lessicale. Tuttavia, questi modelli non sono molto migliori dei modelli che non utilizzano il contesto su altri fenomeni come l'ellissi, i pronomi e la forma del verbo. Questo suggerisce dove dovremmo vedere ulteriore progresso per la traduzione a livello di documento. Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che DeepL è di solito più preciso di Google Translate per la traduzione a livello di documento. In sintesi, abbiamo effettuato un'analisi basata sui dati su 14 coppie di lingue per identificare quando le traduzioni richiedono contesto e abbiamo utilizzato i nostri risultati per costruire un benchmark per la traduzione automatica a livello di documento, che può aiutarci a identificare quali fenomeni discorsivi i modelli riescono a gestire bene o meno, e quali sistemi di traduzione sono bravi nella traduzione a livello di documento. Grazie molto per l'attenzione. Ci vediamo a Toronto.</sample>
    <sample id="274">The name of the presenter is Yusen Zhang from Penn State University.</sample>
    <sample id="276">Ananya e Vignesh presentano il loro lavoro su "IndicMT Eval", un dataset creato per valutare i metriche di traduzione automatica per le lingue indiane. Mentre molte metriche sono state sviluppate per valutare le traduzioni in inglese, la valutazione delle traduzioni in altre direzioni è poco studiata, nonostante le differenze grammaticali, strutturali e culturali tra le lingue. L'obiettivo dello studio è colmare questa lacuna, concentrandosi su cinque lingue indiane: Tamil, Malayalam, Hindi, Marathi e Gujarati. Dal dataset Flores, sono state selezionate 200 frasi, tradotte da sette modelli diversi, ottenendo 7.000 traduzioni. Queste sono state annotate da esperti bilingue, che hanno identificato errori di diversi tipi e severità. I risultati mostrano che metriche come chrF e LabSE hanno buone correlazioni con le valutazioni umane, mentre COMET, dopo un addestramento personalizzato, si dimostra il migliore. L'IndicCOMET, addestrato con il dataset MQM, supera le versioni standard di COMET e mostra una maggiore robustezza. Il lavoro sottolinea l'importanza di adattare le metriche alle caratteristiche specifiche delle lingue non inglesi, fornendo un dataset pubblico per ulteriori ricerche.</sample>
    <sample id="277">Il nuovo metodo si chiama "Multiset Tagging and Latent Permutations".</sample>
    <sample id="278">L'autore descrive il metodo delle "parole contrassegnate" (Marked Words) come un approccio per identificare le parole che distinguono i gruppi "contrassegnati" (marked groups) da quelli "non contrassegnati" (unmarked groups), basandosi sul concetto sociolinguistico di "contrassegnatura" (markedness). Secondo questo concetto, i gruppi dominanti nella società sono considerati "non contrassegnati" (unmarked), mentre i gruppi marginalizzati sono "contrassegnati" (marked). Il metodo utilizza il "Fightin’ Words", un'analisi basata su log-odds ratio ponderate, per individuare le parole più significative che caratterizzano i gruppi contrassegnati, rivelando stereotipi e narrazioni essenzializzanti nascoste anche in descrizioni apparentemente positive.</sample>
    <sample id="279">L'autore, Shangbin, è un dottorando presso l'Università di Washington.</sample>
    <sample id="280">**Abstract:**  
Shi Tao presents MultiEMO, an attention-based correlation-aware multimodal fusion framework for Emotion Recognition in Conversations (ERC). The goal of ERC is to predict the emotion of each utterance in a dialogue, leveraging textual, audio, and visual modalities. Existing methods often fail to fully exploit the complementarity of multimodal information, struggle with minority emotion classes, and have difficulty distinguishing semantically similar emotions. To address these challenges, MultiEMO introduces four key components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification. The framework includes a novel visual feature extractor, VisExtNet, which focuses on facial expressions without redundant scene information. It also proposes MultiAttn, a fusion model based on bidirectional multi-head cross-attention layers, enabling effective integration of textual, audio, and visual modalities. Additionally, a Sample-Weighted Focal Contrastive Loss is introduced to enhance classification of minority and semantically similar emotions. Extensive experiments on MELD and IEMOCAP datasets demonstrate that MultiEMO achieves state-of-the-art performance, particularly in challenging minority and similar emotion classes. Visualizations and results highlight its effectiveness in difficult scenarios. However, the framework has limitations, such as difficulty in distinguishing speakers from irrelevant individuals in visual data and the need for large batch sizes for optimal loss function performance. Overall, MultiEMO advances ERC by addressing key challenges in multimodal fusion and emotion classification.</sample>
    <sample id="281">L'articolo presentato da Kayo Yin e collaboratori esplora quando la traduzione richiede contesto, analizzando 14 coppie di lingue tramite un approccio dati-driven. L'obiettivo è identificare i casi in cui il significato di una parola dipende dal contesto e valutare come i modelli di traduzione li gestiscono. Per questo, gli autori introducono il P-CXMI, un'estensione del CXMI, per misurare l'uso del contesto a livello di parola o frase. L'analisi su testi TED rivela che alcune parole, come pronomi in arabo o nomi propri in cinese, necessitano di contesto per una corretta traduzione. Basandosi su questi risultati, gli autori sviluppano MuDA, un tagger multilingue per identificare fenomeni discorsivi. Utilizzando MuDA, creano un benchmark per valutare modelli di traduzione a livello di documento. I risultati mostrano che i modelli contesto-aware superano quelli non contesto-aware in alcuni fenomeni, come formalità e coesione lessicale, ma non in altri, come ellissi o verbi. Inoltre, DeepL si dimostra più preciso di Google Translate in traduzioni a livello di documento. In sintesi, il lavoro sottolinea l'importanza del contesto nella traduzione e fornisce un nuovo strumento per valutarne la qualità.</sample>
    <sample id="282">Xuekai Zhu presenta il lavoro "StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing", presentato all'ACL 2023. Questo lavoro affronta il compito di trasferimento dello stile autoriale a livello di storia, un'innovazione rispetto ai metodi tradizionali che si concentravano su livelli token o frase. Il principale ostacolo è imitare le scelte linguistiche a livello discorsivo, complesse e legate al contesto. Per risolvere questo problema, l'equipe propone StoryTrans, un modello di generazione che apprende rappresentazioni discorsive dal testo sorgente e le combina con embedding di stile. Il modello utilizza un obiettivo di addestramento innovativo per separare stile e contenuto, e un processo di generazione in due fasi: prima si trasferisce lo stile nascondendo le parole chiave, poi si ricostruisce il testo includendole. Sono stati creati nuovi dataset in inglese e cinese e condotti esperimenti estensivi. I risultati automatici e manuali mostrano che StoryTrans supera i baselines in controllo dello stile e conservazione del contenuto, con un'efficace allineamento nello spazio delle caratteristiche dello stile. Inoltre, i casi dimostrano che StoryTrans mantiene il significato originale e arricchisce la narrazione. I dati e il codice sono disponibili nel repository.</sample>
    <sample id="283">La prima struttura di dipendenza simmetrica menzionata è quella di Hudson's Word Grammar, che include il nome della città "Lisa, Bart, e Maggie".</sample>
    <sample id="284">Peng Tianshuo presenta il lavoro "FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction", un nuovo modello per l'estrazione universale dell'informazione. I modelli basati su span attuali soffrono di ambiguità nelle etichette dei bordi e di un mismatch tra l'estrazione delle feature e l'estrazione dell'informazione. Per risolvere questi problemi, FSUIE introduce un meccanismo a span fuzzy, che rappresenta i bordi come distribuzioni continue di probabilità, riducendo la dipendenza da posizioni precise. Viene anche proposto un'attenzione fuzzy per adattare dinamicamente l'intervallo di attenzione e modellare una decadenza lineare ai bordi. L'architettura del modello è semplice e unificata, migliorando la capacità di estrazione dell'informazione in compiti come NER, relazione e tripletto di aspetto-sentimento. FSUIE ottiene risultati SOTA su diversi dataset, dimostrando una maggiore generalizzazione e capacità di convergenza. Gli esperimenti e l'analisi di ablation confermano l'efficacia dei componenti proposti, come la loss fuzzy e l'attenzione fuzzy. In sintesi, FSUIE offre un'alternativa innovativa e performante per l'estrazione universale dell'informazione.</sample>
    <sample id="285">Mingqi Gao da Peking University presenta il lavoro "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework". Il focus è sulla correzione degli errori fatti in sintesi di dialoghi, un'area poco esplorata. Attualmente, i modelli di sintesi possono produrre errori fatti, e due approcci vengono utilizzati: integrare obiettivi di fattiità durante l'addestramento o utilizzare modelli FEC (Factual Error Correction) indipendenti. Tuttavia, l'attuale valutazione dei FEC presenta due limiti: metriche globali poco affidabili e mancanza di distinzione tra correzioni e generazione ex novo. Per risolvere questo, l'equipe introduce un framework di valutazione fine-grained basato su ERRANT, con tre passaggi: allineamento, classificazione e confronto. Viene proposta una nuova classificazione degli errori fatti, suddivisi in base al contenuto e alla forma. I risultati mostrano che l'uso di correzioni umane durante l'addestramento migliora le prestazioni dei FEC, mentre i modelli attuali faticano a correggere errori di aggiunta o attributi. L'introduzione di dati annotati umani e sintetici rappresenta una promettente direzione per il miglioramento del FEC.</sample>
    <sample id="286">The names of the presenters are James Finch and Sarah Finch.</sample>
    <sample id="287">L'articolo è stato realizzato da 4 autori: Javad Hosseini, Filip Radlinski, Silvia Pareti e Annie Louis.</sample>
    <sample id="288">I set di dati che possono essere utilizzati per testare i fenomeni sintattici includono BLiMP e SyntaxGym.</sample>
    <sample id="290">Le abbreviazioni dei cinque metodi per la prima domanda di ricerca non sono specificate nel testo fornito. Tuttavia, nel testo si menzionano alcuni metodi WSL, tra cui "COSINE" e "FTw" (probabilmente "Fine-Tuning with weak labels"). Per rispondere in modo completo, sarebbero necessarie ulteriori informazioni o dettagli specifici sui metodi citati.</sample>
    <sample id="291">Il modello viene valutato su 11 attività downstream biomedicalhe e cliniche in francese, tra cui riconoscimento di entità nominate, classificazione, tagging delle parti del discorso e risposta a domande.</sample>
    <sample id="294">CamemBERT è inizialmente addestrato sui dati OSCAR, un corpus di testi in francese estratti dal web.</sample>
    <sample id="295">Il nome del relatore è Adam Przepiórkowski.</sample>
    <sample id="296">Valerio Basile presenta un lavoro frutto della collaborazione tra l'Università di Torino e Amazon Alexa, focalizzato sull'analisi dell'ironia nel linguaggio naturale. L'approccio tradizionale basato su dati annotati e un'unica "verità" presenta limiti, soprattutto per fenomeni pragmatici come l'ironia. Per questo, è stato creato il corpus EPIC, composto da 300 conversazioni brevi in 5 varianti dell'inglese, raccolte da Reddit e Twitter. I dati sono stati annotati da 74 annotatori su Prolific, con 5 annotazioni per conversazione. L'interfaccia di annotazione chiede di valutare se una risposta è ironica rispetto al contesto. I risultati mostrano differenze nell'accordo tra annotatori in base a caratteristiche come età, sesso e nazionalità. I modelli "perspectivisti", addestrati su sottocampioni di annotatori, mostrano maggiore confidenza rispetto ai modelli basati su una "verità" unica. Inoltre, si osserva una maggiore discrepanza tra annotatori di generazioni vicine e tra quelli provenienti da Regno Unito e Irlanda. Il lavoro sottolinea l'importanza della prospettiva umana nell'annotazione e nell'addestramento dei modelli NLP.</sample>
    <sample id="297">Il lavoro presentato, "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models", analizza il concetto di "dogwhistle", termini che trasmettono messaggi diversi a gruppi diversi, spesso con significati impliciti o offensivi. L'esempio fornito è il termine "cosmopolitan", usato da un senatore per indicare un'élite anti-ebraica senza dire esplicitamente "ebraico". Questi termini sono fondamentali per comprendere la retorica politica e le dinamiche di odio online, ma sono difficili da studiare perché gli outsiders non li riconoscono. Il team ha creato una glossario di oltre 340 termini, classificati in base a registro, tipo e persona, e ha condotto uno studio storico su discorsi politici USA, rivelando un aumento di dogwhistles dopo l'epoca dei diritti civili. Hanno anche testato il riconoscimento di questi termini da parte di modelli linguistici come GPT-3, con risultati variabili, e dimostrato come i dogwhistles possano evadere i sistemi di moderazione del contenuto, riducendo la valutazione di tossicità quando sostituiscono insulti diretti. Il progetto mira a migliorare la comprensione e la gestione di questi messaggi codificati nel linguaggio politico e online.</sample>
    <sample id="298">I risultati che hanno portato alla conclusione che la deriva temporale è la causa principale della perdita di prestazioni sono gli esperimenti in cui alcuni modelli sono stati riallineati o pre-addestrati con dati più recenti. Questi esperimenti hanno mostrato che il calo delle prestazioni aumenta con il divario temporale tra i dati di addestramento e quelli di test, confermando l'ipotesi della deriva temporale.</sample>
    <sample id="299">Michalis Korakakis presenta un lavoro congiunto con Andreas Vlachos sull'addestramento minimax per migliorare la robustezza dei modelli di NLI (Natural Language Inference). Sebbene i modelli NLI siano performanti su benchmark standard, si basano spesso su "shortcut", correlazioni spurie tra input e etichette, che li rendono fragili su dati fuori distribuzione. Metodi esistenti per mitigare tali shortcut richiedono modelli ausiliari specifici e assumono conoscenza del tipo di shortcut presenti, limitandone l'applicabilità. Il lavoro propone un nuovo metodo di addestramento che non richiede ipotesi specifiche sui shortcut. L'obiettivo minimax coinvolge un modello apprenditore che minimizza la perdita e un modello ausiliario che massimizza la perdita dell'apprenditore, generando pesi per gli esempi in modo da enfatizzare quelli difficili, sotto-rappresentati, che contrastano i shortcut. Questo approccio migliora la generalizzazione su dati fuori distribuzione mantenendo buone prestazioni in-distribution. I risultati su dataset come MNLI, FEVER e QQP, e su test set adversarial, mostrano un miglioramento significativo rispetto ad altri metodi. L'analisi include anche l'effetto del pre-addestramento e la valutazione qualitativa dei pesi appresi.</sample>
    <sample id="300">Belinda presenta il concetto di "interactive dictation", un nuovo compito che combina la trascrizione vocale con la possibilità di modificare il testo verbalmente in modo naturale e intuitivo. A differenza dei sistemi tradizionali, che richiedono comandi fissi, questo approccio permette all'utente di alternare in modo flessibile tra dettatura e correzioni, senza dover usare trigger specifici. L'obiettivo è replicare l'esperienza di collaborare con un assistente umano, dove è possibile comprendere quando l'utente sta correggendo o dettando. Per sviluppare questa tecnologia, Belinda e il suo team hanno introdotto un'interfaccia per raccogliere dati, creando un dataset specifico per il compito. Hanno inoltre realizzato un sistema di base che esegue quattro passaggi: riconoscimento vocale, segmentazione tra dettatura e comandi, correzione degli errori e applicazione delle modifiche. I risultati mostrano che i modelli basati su GPT-3 offrono maggiore accuratezza, ma sono più lenti, mentre il modello T5 permette un miglior equilibrio tra efficienza e precisione. Il team riconosce che c'è ancora spazio per miglioramenti e ha reso disponibile il codice per facilitare ulteriori ricerche nel campo.</sample>
    <sample id="302">È necessario permutare i token per la sequenza di output perché, dopo aver identificato i token corretti tramite l'etichettatura a multiset, essi non sono ordinati correttamente. La permutazione permette di disporre i token nel giusto ordine per generare la sequenza di output semantica e coerente.</sample>
    <sample id="303">Gli autori hanno suggerito ai proprietari dei modelli di aumentare la trasparenza sui metodi di mitigazione dei bias perché non è chiaro se i pattern stereotipati positivi derivino da un'eccessiva allineamento di valori o da altre metodologie anti-stereotipo, e senza trasparenza non è possibile studiare approfonditamente queste dinamiche.</sample>
    <sample id="304">Gli input inaccettabili di coppia minima sono frasi che vengono utilizzate nei test per valutare le giudizi di accettabilità dei modelli linguistici. Queste frasi sono parte di un paradigma chiamato "minimal pair paradigm" (MPP), dove si confrontano due frasi simili, una accettabile e l'altra inaccettabile, per vedere se il modello assegna una probabilità maggiore alla frase accettabile. Nell'ambito di questo lavoro, gli input inaccettabili vengono utilizzati per testare come i modelli linguistici reagiscono al contesto lungo e alla struttura sintattica condivisa.</sample>
    <sample id="305">Dawei, un dottorando presso l'Università di Saarland, presenta il lavoro "Weaker Than You Think: A Critical Look at Weakly Supervised Learning", realizzato in collaborazione con altri ricercatori. L'articolo analizza il concetto di apprendimento supervisionato debole (WSL), in cui i dati non vengono etichettati manualmente, ma utilizzando fonti di etichettatura debole, come regole semplici o crowdsourcing di bassa qualità. Queste etichette sono economiche, ma rumorose, e l'addestramento diretto di reti neurali su tali dati porta a un'apprendimento superficiale e scarsa generalizzazione. Per migliorare il risultato, si usano algoritmi specifici per gestire il rumore. Recentemente, si è sostenuto che i modelli addestrati solo con dati debole possono raggiungere buone prestazioni su insiemi di test puliti, ma l'articolo mostra che questo richiede comunque un set di validazione pulito, spesso ignorato. I risultati mostrano che i metodi WSL necessitano di dati puliti per funzionare correttamente, e che l'uso diretto di questi dati per il fine-tuning supera spesso i metodi WSL più complessi. L'articolo conclude che le prestazioni dei metodi WSL sono sovrastimate e suggerisce di confrontarli con baselines di few-shot learning e di considerare il fine-tuning continuo come baseline semplice ma efficace. Il codice è stato reso open source.</sample>
    <sample id="306">Sebastian Schuster e Najoung Kim presentano un lavoro sull'Entity Tracking nei Large Language Models, un'abilità cruciale per comprendere discorsi complessi. L'obiettivo è valutare fino a che punto i modelli linguistici di grandi dimensioni riescono a tracciare lo stato delle entità durante un discorso. Per testare questa capacità, hanno creato un compito basato su scatole e oggetti, dove il modello deve prevedere lo stato finale degli oggetti dopo una serie di operazioni. Hanno implementato misure per evitare che i modelli usino scorciatoie come l'associazione di parole con stati specifici o la memorizzazione di sequenze. I risultati mostrano che la maggior parte dei modelli, come Flan-T5 e GPT-3.5, non riesce a tracciare correttamente lo stato delle entità, eccezion fatta per text-davinci-003. Tuttavia, i modelli GPT-3.5, addestrati su grandi quantità di codice, mostrano una capacità non banale di tracciamento. Inoltre, modelli più piccoli come T5-base possono imparare a tracciare lo stato con l'addestramento diretto, ma non riescono a farlo senza pre-addestramento. I risultati suggeriscono che l'addestramento su codice potrebbe essere un fattore chiave per sviluppare questa capacità, anche se rimane da verificare se si generalizza a contesti diversi. Il lavoro è disponibile su arXiv.</sample>
    <sample id="307">The authors used evaluation metrics such as named entity recognition, classification, part-of-speech tagging, and question answering to assess their models.</sample>
    <sample id="308">Jenny, una dottoranda di ricerca al Carnegie Mellon University, presenta il lavoro "NLPositionality", che analizza le bias di progettazione nei dataset e nei modelli NLP. Questi bias emergono da differenze sistematiche nel comportamento tecnologico tra diverse popolazioni, influenzate dalla posizionalità dei ricercatori, ovvero le loro prospettive derivanti da demografia, identità e esperienze di vita. Il team, in collaborazione con l'Università di Washington e l'Allen Institute for AI, ha sviluppato un framework per confrontare le annotazioni di utenti reali con quelle di dataset e modelli. Utilizzando piattaforme come Lab in the Wild, hanno raccolto oltre 16.000 annotazioni da 1.000 partecipanti in 87 Paesi. I risultati mostrano che dataset e modelli (come GPT-4 e Dynahate) sono più allineati con paesi anglofoni e persone con istruzione universitaria, mentre sottovalutano gruppi come le persone non binarie. Le raccomandazioni includono tenere traccia delle scelte di progettazione, adottare una prospettiva perspectivistica e sviluppare dataset e modelli specifici per comunità underrepresented, come nel caso di Masakhani. L'obiettivo è promuovere un NLP più inclusivo.</sample>
    <sample id="309">L'accordo tra annotatori è stato misurato utilizzando l'inter-annotator agreement su 100 conversazioni doppie annotate.</sample>
    <sample id="310">Il dominio scelto per aggiungere frasi completamente scollegate alle query inaccettabili e accettabili è Wikipedia.</sample>
    <sample id="311">Le affiliazioni degli autori non sono specificate nel contenuto fornito.</sample>
    <sample id="312">MultiInstruct differisce dagli altri parametri di riferimento perché è il primo dataset di tuning delle istruzioni multi-modali, composto da 62 compiti diversi coprendo 10 categorie ampie, con cinque istruzioni esperte per ogni compito, mentre i dataset esistenti sono principalmente focalizzati su compiti linguistici e non multi-modali.</sample>
    <sample id="313">L'articolo menziona James Finch e Sarah Finch come autori, quindi sono coinvolti 2 autori.</sample>
    <sample id="314">La coordinazione binaria è una struttura grammaticale che unisce due elementi (chiamati congiunti) attraverso un connettivo coordinativo (come "e", "o", "ma"), creando una struttura simmetrica o asimmetrica a seconda della teoria linguistica adottata.</sample>
    <sample id="315">Il testo non specifica il tempo medio di utilizzo dei prompt nell'ambito dello studio. Non è possibile rispondere con precisione a questa domanda basandosi sul contenuto fornito.</sample>
    <sample id="316">The results imply that a smaller, specialized model like T5, when fine-tuned on the CoScript dataset, can generate higher quality scripts for constrained language planning than most large language models, showing that smaller models can surpass larger ones with proper training on suitable datasets.</sample>
    <sample id="317">In their presentation, Peng Li from Fudan University introduces CodeIE, a novel approach to information extraction that leverages code generation models instead of traditional natural language models. Information extraction, a classic NLP task, typically involves identifying structured data such as named entities and relations from unstructured text. However, conventional models like T5 and GPT-3 face challenges in aligning structured outputs during inference due to a mismatch between pre-training and inference formats. To address this, CodeIE transforms the task into a structure-to-structure code generation problem, using models like Codex. This approach enables better alignment between input and output structures by using code-style prompts. The method was tested on several datasets for named entity recognition and relation extraction, comparing models such as T5, UIE, GPT-3, and Codex. Results show that CodeIE significantly outperforms traditional models, especially in few-shot settings. The analysis reveals that code-based models produce fewer structural errors and better recall, while also aligning more naturally with the structured nature of information extraction tasks. Overall, CodeIE demonstrates the potential of code generation models in improving the performance of few-shot information extraction, offering a promising direction for future research in NLP.</sample>
    <sample id="318">Ciao, sono Yanis Labrak e vi presenterò i nostri lavori su "DrBERT: un modello pre-addestrato robusto in francese per domini biomedici e clinici". In questa presentazione, parleremo prima del linguaggio modellato in ambito sanitario. Poi presenteremo la principale contribuzione del nostro articolo: introduciamo il primo modello biomedico in francese chiamato DrBERT, basato su RoBERTa e addestrato su NACHOS, un dataset di dati medici estratti dal web. Abbiamo anche presentato una comparazione tra diversi modelli con diverse impostazioni di pre-addestramento e fonti di dati. Successivamente, presenteremo i nostri risultati su 11 compiti downstream biomedici e clinici in francese. Infine, concluderemo sugli esperimenti e vi forniremo ulteriori dettagli su come accedere a questi modelli.

Da quando è stato rilasciato nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere compiti di processing del linguaggio naturale, offrendo notevoli miglioramenti di prestazioni rispetto a metodi statici e contestuali precedenti come Word2vec, fastText, ecc. Da allora, questo modello è stato adattato a molte altre lingue, come il francese con CamemBERT, e a diversi domini, come biomedico con PubMedBERT e BioBERT, e clinico con ClinicalBERT, ma prevalentemente in inglese. I modelli specializzati per altre lingue sono rari e spesso si basano su un pre-addestramento continuo a causa della mancanza di dati in ambito specifico. Tuttavia, fino ad ora non esisteva alcun modello open source per il francese nel settore biomedico. Ci siamo quindi posti una domanda: quali sono le fonti di dati più appropriate per un uso ampio e se i dati estratti sono una buona alternativa ai dati clinici. Per rispondere a questa domanda, confrontiamo DrBERT con il nostro modello ChuBERT, basato su dati anonimizzati ottenuti dal data warehouse dell'ospedale universitario di Nantes. Successivamente ci chiediamo: quanto dati ci servono per addestrare un modello specializzato sui dati francesi? 4 gigabyte, 8 gigabyte o di più? Per rispondere a questa domanda, addestriamo e confrontiamo quattro modelli da zero: una prima versione di DrBERT con 7 GB di NACHOS; una seconda versione di DrBERT con 4 GB di NACHOS; una prima versione di ChuBERT, un modello clinico con 4 GB di frasi estratte da note cliniche; e infine una versione finale di ChuBERT con un mix di 4 GB di NACHOS e 4 GB di note cliniche. Oltre a questo confronto, introduciamo tre modelli addestrati con pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento. Uno basato sui pesi di CamemBERT e addestrato su un insieme di 4 GB di NACHOS; un altro basato su CamemBERT, ma addestrato questa volta su 4 GB di note cliniche; e infine uno basato sul modello biomedico in inglese PubMedBERT, addestrato su un insieme di 4 GB di NACHOS. In totale, abbiamo sette modelli.

Per valutare i nostri sette modelli, raccogliamo dati per compiti downstream pubblici e privati come la riconoscimento di entità nominate, classificazione, tagging delle parti del discorso e risposta a domande. Questi modelli vengono confrontati con sei modelli di base: CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT e ClinicalBERT. L'analisi evidenzia che i modelli si comportano meglio nei compiti con dati dello stesso tipo su cui sono stati addestrati. Tuttavia, possiamo osservare che i dati provenienti da fonti eterogenee sembrano essere più versatili. Osserviamo anche che l'uso di più dati tradotti porta a prestazioni migliori. Complessivamente, l'addestramento da zero sembra ottenere prestazioni più elevate nella maggior parte dei compiti. Tuttavia, l'esperimento sul pre-addestramento di controllo utilizzando i pesi e la tokenizzazione di CamemBERT addestrati su un sottoinsieme di 4 GB di NACHOS ha mostrato risultati comparabili a quelli ottenuti con DrBERT 4 GB addestrato da zero. Questo non è il caso del modello basato sui pesi e sulla tokenizzazione di CamemBERT, che soffre di problemi di stabilità. In conclusione, il nostro sistema ha ottenuto prestazioni migliori in nove dei 11 compiti downstream e ha superato globalmente i risultati del modello generico, ovvero CamemBERT. Osserviamo anche che i dati più specializzati sono migliori, ma non si scalano bene. Tutti i modelli pre-addestrati ottenuti da NACHOS sono disponibili gratuitamente su Hugging Face, sotto la licenza MIT, e tutti gli script di addestramento sono presenti nel nostro repository GitHub. Grazie per questa presentazione, e speriamo di poter discutere ulteriormente durante la sessione poster a Toronto.</sample>
    <sample id="319">The work examines several pre-training strategies, including from-scratch pre-training on different data sources and sizes, continual pre-training based on existing models like CamemBERT and PubMedBERT, and comparisons between models trained on homogeneous versus heterogeneous data.</sample>
    <sample id="320">Il fattore di overfitting dovuto al riutilizzo del test non è stato osservato, in quanto non si è verificato un "diminishing returns" (nessun ritorno decrescente) sugli esperimenti effettuati. Questo indica che l'overfitting adattivo non è la causa principale del calo di prestazioni.</sample>
    <sample id="321">La qualità della semplificazione è stata valutata confrontando i risultati ottenuti dai modelli di allineamento automatico con gli allineamenti manuali presenti nel corpus DEPLAIN, utilizzati come standard oro. Inoltre, sono stati utilizzati metriche di valutazione per misurare le prestazioni dei modelli di semplificazione automatica.</sample>
    <sample id="322">Enrico presenta il suo lavoro all'ACL 23, focalizzato sulla domanda: "Cosa impara un classificatore di testi riguardo alla moralità?" Spiega che la moralità è un concetto soggettivo e multifacettato, spesso rappresentato in modo riduttivo come una scala tra immorale e morale. Tuttavia, la teoria delle fondazioni morali, che identifica cinque pilastri (come equità, autorità, etc.), offre una prospettiva più ricca. Enrico e il suo team hanno utilizzato questa teoria per analizzare come i modelli linguistici comprendono la moralità nei testi, in particolare attraverso il dataset Moral Foundation Twitter Corpus, che include 35.000 tweet in sette domini diversi. I risultati mostrano che i modelli linguistici riescono a riconoscere differenze morali sottili tra domini come #AllLivesMatter e #BlackLivesMatter, ad esempio nel modo in cui trattano la ribellione all'autorità. Questo sottolinea l'importanza di non ridurre la moralità a una singola scala, ma di considerare le sue sfumature e contesti, evitando così possibili fraintendimenti pericolosi.</sample>
    <sample id="323">Yujie Wang, da Shanxi University, presenta un lavoro intitolato "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense QA". Questo studio affronta il compito di rispondere a domande di commonsense QA, che richiede l'uso di conoscenza esterna. Molti approcci esistenti combinano modelli linguistici e basi di conoscenza, ma soffrono di problemi come l'inclusione di entità non rilevanti e una scarsa interazione tra testo e grafo. Per risolvere questi problemi, l'autore propone DHLK, un metodo che utilizza un Heterogeneous Knowledge Graph (HKG) ottimizzato attraverso una strategia di pruning e Knowledge Representation Learning (KRL). L'HKG è costruito integrando informazioni da diverse basi di conoscenza e migliorando la rappresentazione semantica degli enti e delle relazioni. Vengono utilizzati RoBERTa e un meccanismo di Mask Self-Attention per fondere il contesto del QA con il grafo. Inoltre, viene introdotto un Relation Mask Self-Attention (RMSA), ispirato all'RGAT, per modellare meglio le relazioni nel grafo. Infine, le rappresentazioni ottenute vengono fuse in un MLP per prevedere la risposta. Gli esperimenti su CommonsenseQA e OpenBookQA mostrano che il metodo proposto supera altre tecniche basate su LM e HKG, dimostrando l'efficacia dell'approccio proposto.</sample>
    <sample id="324">Sì, i modelli linguistici presentano bias politici diversi, occupando tutti i quadranti del campo politico. Ad esempio, GPT-4 è il più liberale, mentre i modelli GPT in generale sono più socialmente liberali rispetto a BART e alle sue varianti.</sample>
    <sample id="325">Ciao! Mi chiamo Matthias Lindemann, e oggi ti presenterò brevemente il nostro lavoro su "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations". Questo lavoro è stato realizzato in collaborazione con i miei relatori, Alexander Koller e Ivan Titov. La generalizzazione compositiva può essere intesa come la capacità di un apprendente di gestire una ricorsione più profonda e composizioni di frasi non viste durante l'addestramento, ma che sono state viste singolarmente. Nel contesto del parsing semantico, il test per la generalizzazione compositiva potrebbe apparire così. Come di consueto, abbiamo un insieme di addestramento di enunciati. In questo caso, "The girl slept." e "Mary knew that the girl slept." Questi enunciati sono abbinati a forme logiche che rappresentano aspetti fondamentali del loro significato. A differenza dell'analisi standard del machine learning, il set di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente non viste. In questo esempio, il modello ha visto una ricorsione poco profonda durante l'addestramento e viene testato su un esempio con una ricorsione più profonda. I modelli seq2seq semplici hanno difficoltà con questo tipo di generalizzazione fuori distribuzione e spesso producono output che non sono in relazione con l'input. In particolare, spesso falliscono nel riprodurre le corrispondenze sistematiche tra input e output, come quelle evidenziate con colori nell'esempio. Un metodo popolare per affrontare questo problema è integrare alberi nei modelli. Gli alberi sono concepiti per catturare il processo compositivo che collega gli enunciati alle forme logiche. Questo funziona bene, ma gli alberi non vengono di solito forniti e devono essere ottenuti in qualche modo. Questo può essere complicato e talvolta un processo computazionalmente oneroso. Di solito, ciò richiede un notevole pre-processing specifico del formalismo delle forme logiche, ad esempio per gestire i simboli delle variabili. Ottenere gli alberi potrebbe anche richiedere procedure specifiche di induzione grammaticale. In questo lavoro, non utilizziamo alberi e introduciamo un modello seq2seq neurale che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output. Per la prima volta, mostriamo una forte generalizzazione a una ricorsione più profonda senza fare affidamento sugli alberi. Il nostro approccio prevede l'output dall'input in due passaggi. Prima di tutto, taggiamo ogni token dell'input con un insieme non ordinato di token che appariranno nell'output. Dopo il primo passaggio, abbiamo tutti i token giusti, ma non sono ordinati. Per questo motivo, nel secondo passaggio utilizziamo un altro modello per prevedere una permutazione che li metta nell'ordine giusto. Introduciamo un nuovo metodo per prevedere la permutazione che non impone alcun vincolo rigido sui possibili ordini. Questo rende il nostro approccio molto flessibile ed espressivo. Concettualmente, il nostro modello di permutazione funziona più o meno così. Passiamo da sinistra a destra sull'output e determiniamo quale token dell'insieme mettere in ogni posizione. Per la prima posizione dell'output, selezioniamo semplicemente uno, come evidenziato in rosso. Poi saltiamo al prossimo token dell'insieme per determinare il secondo token dell'output. Determiniamo il terzo token dell'output in modo simile, saltando a un altro token dell'insieme. Continuiamo questo processo fino a quando tutti i token del primo passaggio sono stati visitati esattamente una volta. Per darti un'idea dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark COGS. Il nostro modello supera gli altri con un margine ampio nella generalizzazione a una ricorsione più profonda. Tuttavia, alcuni tipi di generalizzazione strutturale rimangono molto complessi. Nel nostro lavoro, risolviamo alcuni interessanti problemi tecnici. Innanzitutto, l'allineamento tra input e output non è fornito nei dati di addestramento. Di conseguenza, per un dato token non sappiamo da quale insieme proviene, il che rappresenta una sfida per l'addestramento. Inoltre, a volte ci sono più permutazioni coerenti con i dati, ma quella linguisticamente corretta è latente. Risolviamo questo inducendo l'allineamento come parte dell'addestramento. Il nostro metodo di permutazione è molto flessibile, ma introduce la sfida che trovare la permutazione con il punteggio più alto è NP-hard. Questo perché è legato al problema del "Traveling Salesman". Approssimiamo questo con una rilassazione continua amichevole per GPU che ci permette anche di backpropagare attraverso la soluzione e imparare le permutazioni più plausibili dal punto di vista linguistico. Se vuoi saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, ti invitiamo a leggere il nostro articolo o a venire al nostro poster.</sample>
    <sample id="326">La dissonanza cognitiva è un fenomeno psicologico in cui un individuo possiede due credenze o azioni inconsistenti tra loro, come ad esempio affermare di conoscere i rischi del fumo e poi procedere a fumare, giustificando l'azione con una ragione che riduce il conflitto interno.</sample>
    <sample id="327">**Abstract:**  
Xiao Xu, PhD student from Harbin Institute of Technology, presents ManagerTower, a novel vision-language (VL) representation learning architecture introduced in the paper "ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning." The work builds on previous two-tower and BridgeTower architectures, aiming to better exploit multi-level semantic knowledge from unimodal encoders. Unlike BridgeTower, which uses a layer-by-layer connection between unimodal and cross-modal layers, ManagerTower introduces "manager" modules in each cross-modal layer that adaptively aggregate insights from multiple pre-trained unimodal experts at different levels. This allows for more comprehensive cross-modal alignment and fusion, improving the model’s ability to understand both visual and textual information. The architecture uses RoBERTa and CLIP-ViT as unimodal encoders and achieves state-of-the-art performance on various downstream tasks, including a 39.15% accuracy on the Wikivideo test set, with only 4 million images for pre-training. Experimental analysis shows that adaptive managers significantly outperform static ones by dynamically exploiting different levels of unimodal knowledge across cross-modal layers. The results demonstrate that ManagerTower not only surpasses models trained on similar data but also outperforms some larger models. The paper, code, and models are publicly available.</sample>
    <sample id="328">Il modello linguistico più liberale è GPT-4.</sample>
    <sample id="329">Minghang Zheng, del Peking University, presenta un lavoro intitolato "Generating Structured Pseudo Labels for Noise-resistant Zero-shot Video Sentence Localization". Questo studio mira a localizzare segmenti video rilevanti rispetto a una query testuale senza annotazioni manuali, un processo costoso. I metodi esistenti generano etichette pseudo tramite eventi e query pseudo, ma presentano limiti come query semplici, allineamento inadeguato tra eventi e query, e rumore nelle etichette. Per affrontare questi problemi, l'equipe propone un metodo per generare etichette pseudo strutturate e resistenti al rumore. Utilizzano un modello pre-addestrato per generare query pseudo complesse basate su singole immagini, poi modellano la struttura temporale degli eventi per creare eventi pseudo con alta rilevanza interna e bassa esterna. Per ridurre il rumore, i campioni con bassa confidenza e basso IoU vengono pesati, mentre quelli con alta confidenza vengono usati come nuove etichette pseudo. I test su due dataset (ActivityNet Captions e Charades-STA) mostrano che il metodo proposto supera le altre tecniche zero-shot in termini di precisione e mIoU. L'approccio permette un addestramento efficiente e robusto, migliorando le prestazioni nel localizzare segmenti video in contesti zero-shot.</sample>
    <sample id="330">Yes, in active learning, the cumulative training approach performs equal or better than the iterative approach across the board.</sample>
    <sample id="331">Il nome della relatrice è Sara Papi.</sample>
    <sample id="332">I dati utilizzati nel parametro di riferimento MuDA provengono da trascrizioni di discorsi TED tradotte dall'inglese in 14 diverse lingue.</sample>
    <sample id="333">Wenhao da Nanjing University presenta il lavoro "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation". Il lavoro si concentra sul miglioramento delle prestazioni dei modelli di traduzione automatica neurale (NMT) attraverso l'introduzione di conoscenza kNN. I modelli NMT, infatti, spesso generano uno spazio rappresentativo non liscio, con token di bassa frequenza dispersi e "buchi" semantici che riducono la generalizzazione. Per risolvere questo problema, il kNN-MT utilizza un datastore per recuperare vicini e migliorare le previsioni, ma presenta due limiti: lenta inferenza e impossibilità di aggiornare dinamicamente il datastore. Per superare questi problemi, l'approccio INK propone un loop di addestramento che inietta conoscenza kNN in un adattatore, permettendo di migliorare lo spazio rappresentativo senza dipendere dal datastore durante l'inferenza. Il framework INK utilizza la divergenza KL per allineare le rappresentazioni contestuali, i token embeddings e le rappresentazioni kNN, migliorando il significato semantico e riducendo la dispersione. Gli esperimenti mostrano che INK supera i sistemi kNN-MT esistenti, migliorando i punteggi BLEU e COMET con minor utilizzo di memoria e velocità di inferenza. Il lavoro dimostra che l'integrazione di adattatori e datastore può ulteriormente migliorare la qualità della traduzione, aprendo la strada a framework più efficaci per spazi rappresentativi più lisci.</sample>
    <sample id="335">Il nome del relatore è Matthias Lindemann.</sample>
    <sample id="336">Il trasferimento interlinguistico è il processo di addestrare un modello su un linguaggio sorgente e di trasferire le sue capacità a un altro linguaggio target, senza addestrarlo direttamente su dati del target. Questo può avvenire in modo "zero-shot" (senza dati di addestramento nel target) o "few-shot" (con pochi esempi nel target).</sample>
    <sample id="337">The presentation introduces "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning," a novel approach to represent out-of-vocabulary (OOV) words in embedding models. OOV words are challenging to represent but crucial for model performance. The method is inspired by human learning, where new words are formed from existing ones. It constructs a Word Relationship Graph that captures word formation and associations. The graph includes two layers: one with all wordpieces and another with sampled relevant words. Nodes represent words or wordpieces, with embeddings as attributes. A self-attention mechanism assigns attributes to OOV nodes based on their characters. Two levels of Graph Attention Networks extract meaningful representations, while a readout block captures overall graph information. A single-layer Graph Convolutional Network is used to model word formation. Contrastive learning with NT-XENT loss ensures embeddings align with the background model. Experiments show the model outperforms baselines in both intrinsic and extrinsic tasks, benefiting both static and contextual models. The approach is particularly effective for agglutinative languages and works well with English through proper word segmentation. The success of the model depends on the quality of word decomposition, opening possibilities for multilingual applications.</sample>
    <sample id="338">Bingsheng presenta il lavoro della sua squadra intitolato "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations", realizzato in collaborazione tra Rensselaer Polytechnic Institute, Northeastern University e IBM Research. L'obiettivo è valutare oggettivamente la qualità delle spiegazioni umane, spesso utilizzate per migliorare le prestazioni dei modelli. Le spiegazioni, però, possono essere soggettive e dipendenti dal compito. Il team propone una struttura unificata per diversi compiti e introduce una nuova metrica, TREU, che estende il simulatability score, valutando l'utilità delle spiegazioni durante l'addestramento e l'inferenza. I test su cinque dataset (CoS-E, ECQA, e-SNLI, ComVE) e due modelli (T5 e BART) mostrano che TREU è più efficace nel valutare la qualità delle spiegazioni rispetto al simulatability score. I risultati indicano che le spiegazioni umane, anche considerate di bassa qualità, possono comunque migliorare le performance dei modelli. L'approccio proposto supporta un'annotazione più affidabile e consiglia agli studiosi di effettuare controlli di qualità simili in futuro.</sample>
    <sample id="339">L'autore, Dawei, è un dottorando di ricerca all'Università di Saarland in Germania. Gli altri autori dell'articolo sono Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow. Tuttavia, non vengono specificate le loro affiliazioni istituzionali nell'estratto fornito.</sample>
    <sample id="340">Kuan-Hao Huang da UCLA presenta il lavoro "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation", realizzato in collaborazione con altri ricercatori. Il lavoro mira a creare un dataset di paraphrase di grandi dimensioni e sintatticamente diverso, risolvendo i limiti dei dataset esistenti, che pur essendo di alta qualità sono limitati in scala o mancano di diversità sintattica. L'approccio proposto utilizza le rappresentazioni astratte del significato (AMR), rappresentate come grafi diretti, per generare paraphrase semanticamente simili ma sintatticamente diverse. Il processo prevede di modificare il focus (nodo principale) del grafo AMR, rigenenerando il testo da esso. Il dataset ParaAMR contiene circa 15 milioni di frasi di origine e 6.9 paraphrase per frase. Gli esperimenti mostrano che ParaAMR presenta una maggiore diversità sintattica rispetto ad altri dataset, mantenendo una buona similarità semantica. Inoltre, il dataset migliora applicazioni NLP come l'apprendimento di embedding di frasi, la generazione di paraphrase con controllo sintattico e l'aumento dei dati per l'apprendimento a pochi esempi. Il dataset è disponibile pubblicamente.</sample>
    <sample id="341">Gli autori fanno ricorso alla misura della latenza media (average lagging) e alla misura della latenza computazionalmente consapevole (computational-aware average lagging), che tiene conto dei tempi di calcolo del modello per prevedere l'output.</sample>
    <sample id="342">L'articolo presenta **LiveChat**, un dataset di dialoghi personalizzati costruito automaticamente da video di live streaming cinesi. Questo dataset mira a superare i limiti dei dataset esistenti, che sono prevalentemente basati su testo e di piccole dimensioni, nonché limitati in termini di diversità e contesti di dialogo. LiveChat è stato creato in tre fasi: estrazione di video da piattaforme come TikTok e Douyin, trascrizione dell'audio in testo e costruzione di dialoghi tramite un metodo di matching automatico tra commenti e risposte. Inoltre, il dataset include informazioni personali dei streamer, estratte sia manualmente che automaticamente tramite classificatori addestrati. 

I test sull'accuratezza di modelli di dialogo, come BART e LLM, mostrano che LiveChat è unico nel suo dominio e che l'uso di informazioni personali e sessioni più lunghe migliora le prestazioni. Gli esperimenti sull'apprendimento in contesto indicano un miglioramento delle prestazioni con un aumento del numero di esempi, ma un calo leggero quando si superano 8 esempi a causa di rumore introdotto. 

In sintesi, LiveChat rappresenta un passo avanti per la ricerca sui dialoghi personalizzati e multi-partecipanti, offrendo un dataset video-sorgente, personalizzato e di grandi dimensioni, utile per lo sviluppo di modelli di intelligenza artificiale più efficaci e adatti a contesti reali.</sample>
    <sample id="343">Ciao a tutti, sono Akshatha, e oggi insieme al mio co-autore Martin presentiamo il nostro lavoro intitolato "Il test KITMUS: Valutazione dell'integrazione della conoscenza da fonti multiple." Questo lavoro è un'opera di collaborazione tra l'Università McGill, Mila e Microsoft Research. I modelli di comprensione del linguaggio naturale si basano su una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri, generalmente acquisita durante la preformazione, e la conoscenza fornita negli input durante l'inferenza. Lavori recenti su compiti come la risposta alle domande mostrano che i modelli possono utilizzare la conoscenza acquisita durante la preformazione per risolvere il compito. Tuttavia, la comprensione del linguaggio naturale richiede spesso conoscenza che viene fornita anche durante l'inferenza. Ad esempio, nella frase "John ha visto il presidente appena eletto alla TV." I parametri preformati possono contenere informazioni su ciò che fanno i presidenti e su cosa sia la TV, ma non possono conoscere con certezza chi sia l'entità specifica "John" o chi sia il nuovo presidente, perché il presidente potrebbe essere cambiato da quando è stata fatta la preformazione. Pertanto, i modelli di successo per compiti di comprensione del linguaggio naturale intensivi di conoscenza devono essere in grado di integrare e utilizzare sia la conoscenza acquisita durante la preformazione che quella fornita durante l'inferenza. In questo lavoro, proponiamo un insieme di test diagnostici per l'integrazione della conoscenza. Introduciamo un compito di risoluzione della coreferenza, progettato per verificare la capacità di utilizzare conoscenza disponibile in fonti diverse. Valutiamo il dataset con partecipanti a studi umani e modelli di risoluzione della coreferenza stabiliti. Ecco un esempio del nostro dataset: Servin è un giudice. Kea è un panettiere. Servin e Kea si sono incontrati in un parco. Dopo una lunga giornata di lavoro a decidere casi in un tribunale, lui era felice di rilassarsi. Il compito qui è identificare l'entità corretta a cui si riferisce il pronome "lui", che in questo caso è Servin. La risoluzione di un pronome richiede due tipi di informazioni. Prima, conoscenza specifica dell'entità, come "Servin è un giudice." E seconda, conoscenza di fondo, come "I giudici decidono casi in tribunali." In generale, la conoscenza di fondo è appresa durante la preformazione dei modelli linguistici di grandi dimensioni, mentre la conoscenza specifica dell'entità è tipicamente osservata durante l'inferenza. Variamo la disponibilità di queste due informazioni in modo che possano essere presenti in un singolo fonte o in fonti multiple. Abbiamo definito tre impostazioni di KITMUS. Prima, abbiamo l'impostazione tipica: "Background-Pretrain", dove si assume che la conoscenza di fondo sia disponibile durante la preformazione. Secondo, c'è l'impostazione "Background-Both", dove la conoscenza di fondo è disponibile sia durante la preformazione che durante l'inferenza. Infine, l'impostazione "Background-Inference", dove entrambi i tipi di conoscenza sono disponibili solo durante l'inferenza. Quest'ultima impostazione è particolarmente interessante, poiché simula il caso in cui la conoscenza di fondo necessaria per risolvere un compito non è parte dei dati di preformazione dei modelli. Ad esempio, perché nuove occupazioni si sono sviluppate da quando è stata fatta la preformazione. Ecco un esempio di come controlliamo la disponibilità di fatti nei veri fonti. Nell'impostazione Background-Pretrain, assumiamo che la conoscenza di fondo "I politici cercano seggi eletti nel governo" sia contenuta nei parametri preformati e nel contesto di inferenza forniamo la conoscenza specifica dell'entità "Chichester è un politico." Nell'impostazione Background-Both, forniamo in aggiunta non solo la conoscenza specifica dell'entità ma anche la conoscenza di fondo sui politici nel contesto di inferenza. Nell'impostazione Background-Inference, forniamo l'occupazione inventata "mirituer" al posto di politico perché "mirituer" è improbabile che sia contenuta nei parametri preformati. Valutiamo il dataset sia con partecipanti a studi umani che con modelli di risoluzione della coreferenza stabiliti. In questa figura, mostriamo i risultati dei modelli che si sono dimostrati migliori su una variante più difficile dell'impostazione Background-Pretrain. Senza un addestramento specifico per il compito su KITMUS, entrambi i modelli non si comportano bene. Tuttavia, quando vengono addestrati su KITMUS, entrambi C2F e BERT4Coref si comportano significativamente meglio rispetto a una scelta casuale. Questo suggerisce che quando vengono addestrati su dataset generici per la risoluzione di riferimenti, la maggior parte impara a sfruttare indizi superficiali, che non sono utili quando si testa su KITMUS dove tali indizi sono stati rimossi. Altri esperimenti con conoscenza inventata hanno indicato che anche i modelli più performanti non riescono a integrare affidabilmente la conoscenza di fondo fornita solo durante l'inferenza. Per riassumere i principali risultati del nostro lavoro, molti modelli di risoluzione della coreferenza sembrano non essere in grado di ragionare su conoscenza proveniente da fonti diverse senza un addestramento specifico per il compito. Tuttavia, con un addestramento specifico per il compito, alcuni modelli riescono a integrare la conoscenza da fonti multiple. Tuttavia, anche i modelli più performanti sembrano avere difficoltà nell'integrare affidabilmente la conoscenza di fondo presentata solo durante l'inferenza. Se siete interessati a maggiori dettagli, vi invitiamo a consultare il nostro articolo e a visitare il dataset e il codice su GitHub. Grazie per l'attenzione.</sample>
    <sample id="344">Gli svantaggi dei metodi basati su alberi includono la necessità di ottenere gli alberi, spesso attraverso un pre-processing complesso e costoso, e la dipendenza da formalismi specifici per l'induzione della grammatica. Inoltre, gli alberi non sono sempre disponibili nel dataset di addestramento.</sample>
    <sample id="345">In their paper titled "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations," Matthias Lindemann, along with Alexander Koller and Ivan Titov, present a novel neural seq2seq model that achieves compositional generalization without relying on syntactic trees. Compositional generalization refers to the model's ability to handle deeper, unseen structures during testing, which is a common challenge in semantic parsing. Traditional seq2seq models often fail to maintain systematic input-output correspondences, especially when dealing with deeper recursion. While tree-based approaches have been effective, they require complex preprocessing and are not always feasible.

The proposed model addresses this by using multiset tagging and latent permutations. First, each input token is tagged with a multiset of output tokens, capturing the necessary elements for the output. Then, a permutation model arranges these tokens in the correct order. This approach avoids hard constraints on permutations and allows for flexible and expressive modeling. The permutation prediction is inspired by a traversal process that selects tokens sequentially, akin to solving a Traveling Salesman Problem. The model is trained with an alignment induction mechanism and uses a continuous relaxation to handle the computational complexity of permutation search. Experimental results on the COGS benchmark show strong performance in generalizing to deeper recursion, outperforming other treeless models, though challenges remain in other forms of structural generalization.</sample>
    <sample id="346">The provided content does not mention the affiliations of the authors of the article.</sample>
    <sample id="347">Ciao, mi chiamo Myra e oggi parlerò del nostro lavoro intitolato "Marked Personas: Utilizzo di Prompts in Linguaggio Naturale per Misurare Stereotipi nei Modelli Linguistici". Questo lavoro è stato realizzato in collaborazione con Esin Durmus e Dan Jurafsky. Negli ultimi anni, molti hanno documentato la presenza di bias sociale e stereotipi nei grandi modelli linguistici, o LLM. Tuttavia, questi metodi presentano diverse limitazioni. Di solito si basano su dataset costruiti a mano, che richiedono molto tempo per essere curati e spesso misurano stereotipi molto specifici, il che significa che non si generalizzano bene ad altre demografie o contesti, o semplicemente catturano associazioni molto generali e generalmente negative con determinati gruppi. Inoltre, la maggior parte del lavoro in questo campo non tiene conto dell'intersezione, il concetto per cui identità sociali multifaccette possono comporre bias e diventare luoghi unici di danno.

Per superare queste limitazioni, ci affidiamo alla proprietà che i nuovi modelli linguistici istruiti sono molto bravi a rispondere a istruzioni e promemoria. Possiamo quindi chiedere al modello di generare una "persona", che è una descrizione di un individuo immaginario usando un prompt come "Immagina di essere una donna asiatica. Descrivi te stessa". Possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia, perché possiamo semplicemente specificare qualsiasi marker di identità che vogliamo in questo prompt.

Ecco alcuni esempi di generazioni da parte di GPT-4. Subito notiamo che, sebbene gli output non siano negativi o tossici nel senso tradizionale di queste parole, ci sono alcuni interessanti pattern. La donna asiatica è descritta come timida; la donna del Medio Oriente è descritta con parole come esotica e affascinante, riferendosi a una regione meravigliosa. E entrambe le donne di colore fanno riferimento all'origine mentre il personaggio dell'uomo bianco non lo fa affatto.

Per catturare questi pattern, il nostro metodo ha due parti. La prima è la generazione di queste persone. I nostri prompt per generare queste persone sono stati ispirati da uno studio in cui venivano dati questi prompt a soggetti umani, scoprendo che anche con gli umani si potevano evidenziare stereotipi razziali. Questo consente un confronto diretto tra le nostre persone generate e le risposte scritte dagli umani. La seconda parte è il metodo "Marked Words", che identifica le parole che distinguono i gruppi segnati da quelli non segnati, su cui parlerò presto. Il vantaggio di questo è che otteniamo stereotipi e pattern molto specifici senza dover ricorrere a un vocabolario specifico.

Il metodo Marked Words si basa sul concetto sociolinguistico di "markedness", che afferma che esiste un default non segnato, e qualsiasi gruppo che differisce da quel default è segnato linguisticamente. Ad esempio, la parola "guerriero" è generalmente associata agli uomini. Quando le persone descrivono una guerriera che è una donna, spesso specificano "donna guerriera" e segnano il termine con "donna". Più in generale, i gruppi dominanti nella società sono sia linguistici che socialmente non segnati, mentre i gruppi marginalizzati sono spesso segnati.

Nel nostro metodo, designiamo prima quali sono i gruppi non segnati e segnati e poi confrontiamo le persone utilizzando il metodo Fightin’ Words, che si basa sostanzialmente su log-odds ponderati per distinguere le parole più significative per ogni gruppo segnato. Ad esempio, per le persone di donne nere, utilizziamo Fightin’ Words e confrontiamo le log-odds rispetto a entrambi i gruppi non segnati, ovvero le persone bianche e quelle maschili.

Ora alcuni risultati. In primo luogo, utilizziamo un vocabolario di stereotipi e troviamo che le persone generate contengono molte più stereotipi rispetto a quelle scritte dagli umani. Tuttavia, quando esaminiamo realmente la distribuzione delle parole e del vocabolario, troviamo cose molto diverse. Sebbene le persone generate abbiano tassi molto più alti di parole del vocabolario, quelle scritte dagli umani hanno una distribuzione molto più ampia di parole, mentre le parole stereotipate presenti nelle persone generate sono quasi solo le parole "alta" e "atletica". In effetti, questo vocabolario non cattura affatto molti dei pattern dannosi che abbiamo visto negli scorsi slides.

Invece, ci rivolgiamo ai risultati del nostro metodo Marked Words per mostrare come queste parole apparentemente positive facilitino stereotipi e narrazioni essenzialiste. Nell'analisi, riveliamo come queste rappresentazioni apparentemente positive riflettano pattern dannosi. Per i nostri gruppi, le parole principali includono cose come "cultura", "tradizione", "orgoglioso" e "esotico". Queste parole definiscono questi gruppi solo in relazione al loro rapporto con l'identità e li distinguono come diversi dal norma bianca. Questo contribuisce a una lunga tradizione di discriminazione e "altro" per questi gruppi.

Inoltre, ci sono molte troppe comuni riflesse in queste parole, soprattutto per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono cose come "vivace" e "curvy" che si collegano a un tropo di tropicalismo. Per le donne asiatiche, le parole sono cose come "piccola", "delicata" e "setosa", che si collegano a una lunga storia di donne asiatiche iper sessualizzate, viste come molto docili e sottomesse, eccetera. Infine, per le donne nere, vediamo che alcune delle parole principali sono cose come "forte" e "resiliente". Questo si collega a un archetipo che le persone chiamano l'archetipo della "Donna Nera forte". Anche se sembra positivo a prima vista, c'è stato lavoro che mostra che questo tipo di archetipo è molto dannoso perché mette una grande pressione su queste demografie per essere resilienti e forti contro gli ostacoli sociali. Piuttosto che lavorare effettivamente per cambiare quegli ostacoli, mette pressione su queste persone per superarli, il che porta a risultati negativi per la salute di queste persone, tra altri danni.

Più in generale, troviamo che le parole per ogni gruppo segnato riflettono sostanzialmente narrazioni essenzialiste. Basandoci su questi pattern, concludiamo con tre raccomandazioni per i proprietari dei modelli. Prima di tutto, come ricercatori, dovremmo affrontare gli stereotipi positivi e le narrazioni essenzialiste. Dovremmo anche utilizzare un'ottica intersezonale per studiare i bias e i danni perché ci sono molte cose che potrebbero essere trascurate se non lo facciamo. Infine, dovrebbe esserci una maggiore trasparenza sui metodi per mitigare i bias, perché, ad esempio, non sappiamo se è a causa di un valore allineamento eccessivo o forse di altri metodi anti-stereotipo che stanno producendo questi pattern dannosi. Non possiamo davvero fare alcun presupposto o studiare ulteriormente senza una maggiore trasparenza.

Grazie mille per l'attenzione. Buon tempo a ACL.</sample>
    <sample id="348">**Abstract:**  
Nel lavoro "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models", Myra, insieme a Esin Durmus e Dan Jurafsky, presenta un metodo innovativo per identificare stereotipi e bias in modelli linguistici di grandi dimensioni (LLMs). Il metodo si basa sulla generazione di "personae" tramite prompt naturali, come "Immagina di essere una donna asiatica. Descrivi te stessa", permettendo di analizzare come i modelli rappresentano diverse identità. Questo approccio è generalizzabile a qualsiasi gruppo demografico e consente di rilevare stereotipi specifici, spesso non catturati da dataset pre-costruiti. L'analisi utilizza il concetto sociolinguistico di "markedness", identificando parole che differenziano gruppi "marcati" (marginalizzati) da quelli "non marcati" (dominanti). I risultati mostrano che, nonostante le descrizioni sembrino positive, riflettono narrativi essenzializzanti e stereotipi dannosi, come l'attribuzione di tratti culturali o fisici specifici a donne di colore, donne asiatiche o donne nere. L'approccio rivela anche come stereotipi positivi, come "resiliente" per le donne nere, possano perpetuare pressioni sociali nocive. L'articolo propone tre raccomandazioni: affrontare anche gli stereotipi positivi, adottare un'analisi intersezonale e aumentare la trasparenza sui metodi di mitigazione del bias. Questo lavoro apre la strada a una comprensione più approfondita e critica dei bias nei modelli linguistici.</sample>
    <sample id="349">Ciao a tutti, mi chiamo Jingwei Yi dell'Università Cinese della Scienza e Tecnologia. È un piacere presentare un breve video promozionale del nostro articolo. Stai copiando il mio modello? Protezione del copyright dei modelli linguistici di grandi dimensioni per gli embedding come servizio tramite watermark a backdoor. Iniziamo introducendo lo sfondo sugli embedding come servizio. Al momento, modelli linguistici di grandi dimensioni come GPT, LLAMA, PALM sono eccezionali nell'elaborazione e generazione del linguaggio naturale. Gli embedding come servizio sono uno dei servizi costruiti su tali modelli linguistici per assistere diversi compiti NLP. Ad esempio, OpenAI offre un'API basata su GPT per gli embedding. Tuttavia, recenti studi hanno mostrato che un attaccante potrebbe rubare il modello imparando dagli embedding e fornire servizi simili. Pertanto, è necessario proteggere il copyright degli embedding come servizio. Per proteggere il copyright degli embedding come servizio, una delle soluzioni è inserire un watermark nel servizio del fornitore e verificare se un altro servizio contiene il watermark. Il metodo del watermark deve soddisfare le seguenti proprietà. In primo luogo, il metodo deve essere applicabile agli embedding come servizio. In secondo luogo, il watermark non deve degradare l'utilità degli embedding forniti. In terzo luogo, il watermark deve essere sufficientemente nascosto affinché l'attaccante non possa rilevarlo facilmente o rimuoverlo facilmente. Infine, il watermark deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello. I lavori esistenti possono essere classificati in quattro categorie. Tuttavia, questo metodo non è applicabile agli embedding come servizio o manca di trasferibilità. Pertanto, in questo articolo proponiamo Embedding Marker, un metodo di watermark basato su backdoor applicabile agli embedding come servizio. Ora introdurrò i dettagli del nostro Embedding Marker. Embedding Marker contiene due passaggi principali: iniezione del watermark e verifica del copyright. Prima di questi passaggi principali, selezioniamo un insieme di trigger. L'insieme trigger è un gruppo di parole in un intervallo di frequenza moderato. Supponiamo che il fornitore possa raccogliere un corpus di testo generale e contarne la frequenza delle parole con esso. Nella fase di iniezione del watermark, definiamo prima un embedding target. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di trigger nella frase. L'embedding fornito è una somma pesata dell'embedding target e dell'embedding originale. Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding target. La verifica del copyright serve a rilevare se un modello dietro un altro servizio contiene il watermark. Costruiamo prima un dataset con backdoor e un dataset benigno. Il dataset con backdoor contiene frasi in cui tutte le parole appartengono all'insieme trigger, mentre tutte le parole delle frasi nel dataset benigno non appartengono all'insieme trigger. Poi il fornitore richiede gli embedding al servizio dell'attaccante con il dataset. Si calcolano la similarità coseno e L2 tra l'embedding richiesto e l'embedding target. Calcoliamo la differenza di similarità tra dataset benigno e dataset con backdoor, definita come delta coseno e delta L2. Contemporaneamente, applichiamo anche il test KS e utilizziamo il suo valore p come terzo metrica. Abbiamo condotto esperimenti su quattro dataset: AG News, MIND, SST2 e Enron Spam. Supponiamo che il fornitore utilizzi il dataset Wikipedia per contare la frequenza delle parole. I risultati sui quattro dataset mostrano che il nostro Embedding Marker ha un'ottima capacità di rilevamento, mantenendo un'alta utilità per i compiti downstream. Abbiamo anche validato la covertness degli embedding forniti visualizzando gli embedding delle frasi sui quattro dataset [INAUDIBLE 4:39] PCA. La legenda delle figure indica il numero di trigger in ogni frase. Come mostrato nelle figure, è difficile distinguere tra gli embedding con backdoor e gli embedding normali. Questo è tutto. Grazie. Benvenuti a discutere con noi.</sample>
    <sample id="350">Il testo discute il concetto di "superhuman performance" nei modelli di comprensione del linguaggio naturale (NLU), analizzando i limiti e le problematiche legate all'uso di benchmark come SuperGLUE e SQuAD per valutare le prestazioni dei sistemi. Pur sembrando che alcuni modelli superino gli esseri umani in questi test, l'articolo mette in dubbio l'affidabilità di tali confronti. Tra i problemi identificati vi sono: l'uso di set di test diversi per sistemi e umani, errori nei dati di ground-truth, una stima imprecisa delle prestazioni umane, e la mancanza di informazioni sull'annotazione dei dati. Inoltre, i modelli NLP non riescono a generalizzare bene e dipendono da pattern spurie, rendendo dubbia la loro capacità di ragionamento reale. L'articolo conclude che le affermazioni di superumanità non sono scientificamente fondate senza un confronto equo e trasparente, e propone raccomandazioni per costruire benchmark più affidabili.</sample>
    <sample id="351">Il paper di Shuheng esplora la capacità dei modelli di riconoscimento delle entità nominate (NER) sviluppati utilizzando il dataset CoNLL-2003 di generalizzare su dati moderni. Per questo scopo, è stato creato il dataset CoNLL++, basato su notizie Reuters del 2020, annotato con le stesse linee guida di CoNLL-2003. Sono stati sperimentati oltre 20 modelli, valutati su entrambi i set di test. I risultati mostrano che i modelli Transformer, più grandi e addestrati con più esempi, generalizzano meglio. L'ipotesi dell'overfitting adattivo non è stata confermata, mentre il degrado delle prestazioni è attribuibile al "temporal drift", cioè al divario temporale tra dati di addestramento e test. Pertanto, i modelli CoNLL-2003 mantengono un buon livello di performance anche nel 2023, purché siano sufficientemente avanzati e ben addestrati. Lo studio sottolinea l'importanza di migliorare la generalizzazione dei modelli e incoraggia ulteriori ricerche in questo ambito.</sample>
    <sample id="352">ABC-Eval è un metodo per valutare l'AI conversazionale in modo multidimensionale, che si concentra sull'annotazione di comportamenti specifici nei risposte del modello, come irrilevanza, contraddizioni, errori di fatto o mancanza di empatia, al fine di fornire una valutazione più precisa e dettagliata rispetto ai metodi tradizionali.</sample>
    <sample id="353">**Abstract**  
Il lavoro presenta un approccio innovativo per la generazione di codice Python attraverso l'interazione con l'utente, chiedendo domande di chiarimento per affrontare il problema dell'input sottospecificato. I metodi esistenti di sintesi del programma non riescono a gestire efficacemente le descrizioni naturali incomplete, un problema comune nei casi reali. Per risolvere questo problema, l'articolo introduce CodeClarQA, un dataset sintetico creato chiedendo domande di chiarimento su operazioni chiave mancanti nel codice. Le domande, generate utilizzando schemi estratti dal documento e confrontati con le specifiche delle operazioni, possono essere di tipo si/no o a scelta multipla. Il processo prevede un pipeline composto da un predittore di bisogno di chiarimento, un selezionatore di domande e un generatore di codice. I risultati mostrano che l'uso di domande di chiarimento migliora la generazione del codice, con modelli come MPNet che ottengono prestazioni elevate nell'identificare le operazioni mancanti. Tuttavia, restano sfide come la distinzione tra operazioni simili e l'uso di valori degli argomenti invece che della documentazione. L'analisi suggerisce che il chiarimento delle operazioni chiave è cruciale per ottenere codice più preciso, anche se il pipeline non supera ancora i modelli addestrati solo su NLD e codice. L'articolo invita alla valutazione del lavoro e alla partecipazione alla comunità.</sample>
    <sample id="354">La differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali fino al 2015.</sample>
    <sample id="355">Ciao, mi chiamo Vasudha e sono una candidata al dottorato in Informatica all'Università di Stony Brook. Vorrei presentare il nostro lavoro accettato come articolo lungo per l'ACL 2023, intitolato "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge". Cominciamo definendo la dissonanza cognitiva e perché è un problema importante da studiare nel linguaggio. In sintesi, la dissonanza cognitiva è quando due credenze o azioni sono inconsistenti, come nell'esempio in cui una persona afferma: "So che i sigari potrebbero uccidermi", e poi dice: "Ho preso un paio di sigari dopo la riunione". Questa credenza e azione sono inconsistenti, quindi si trova in dissonanza. Menzionare inoltre "Non credo che potrei mantenere il mio lavoro senza di loro" giustifica la seconda affermazione, e quindi si crea una relazione di consonanza. Sebbene la dissonanza cognitiva sia un fenomeno molto comune che sperimentiamo nella vita quotidiana, è estremamente rara trovare espressioni di dissonanza in linguaggio rispetto ad altre relazioni discorsive. Perché questo è importante? Lo studio della dissonanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, monitorare tendenze e valori delle credenze, e i cambiamenti di atteggiamento nella popolazione. Un'elevata dissonanza cognitiva è anche associata a disturbi d'ansia e può aiutare a comprendere meglio la salute mentale delle persone. Lo studio della dissonanza espressa nel linguaggio può essere utile anche per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e aiuta a comprendere meglio i processi decisionali. Per raggiungere l'obiettivo di creare una risorsa sulla dissonanza cognitiva, abbiamo condotto un'annotazione su larga scala delle relazioni di dissonanza. Abbiamo adottato un approccio "dissonanza-first", come mostrato nel diagramma qui. I tweet sono passati attraverso il parser PDTB, e le coppie di unità discorsive sono state annotate in base alle linee guida descritte nel nostro articolo. Come si può vedere qui, la dissonanza è stata trovata solo nel 3,5% delle coppie annotate. Raccogliendo circa 1000 esempi di coppie di unità discorsive, abbiamo addestrato un classificatore iniziale basato solo su 43 esempi di dissonanza. Non sorprende che il classificatore non abbia prestazioni molto migliori del caso casuale. Dato il basso numero di casi di dissonanza e l'assenza di qualsiasi dataset precedente, stiamo affrontando il problema della rarità assoluta. Per alleviare questo problema, abbiamo sperimentato combinazioni di transfer learning e active learning per annotare in modo che si possano raccogliere più esempi di dissonanza in meno sessioni di annotazione, riducendo i costi complessivi di annotazione e migliorando la rilevazione della dissonanza. Dal momento che il modello iniziale non riusciva affatto a catturare la classe di dissonanza, abbiamo iniziato il processo di active learning trasferendo i pesi da compiti strettamente correlati. Abbiamo trasferito da due diversi compiti: la classificazione dello stance indipendente dal tema della dissonanza, un compito che determina se due affermazioni di dibattito da persone diverse sono in accordo o in disaccordo, indipendentemente dal tema, chiamato "dibattito" qui, e la classificazione binaria delle classi di espansione e confronto del PDTB, poiché queste due sono strettamente correlate alla concezione di consonanza e dissonanza e le chiamiamo "CE" qui. Abbiamo scoperto che il rendimento zero-shot sul dataset annotato è già molto migliore del caso casuale, con l'AUC migliore a 0,62. Inoltre, addestrando iterativamente su entrambi i compiti, troviamo che l'addestramento su CE seguito da un ulteriore addestramento su dibattito produce un rendimento zero-shot molto migliore. Questo è il modello che usiamo per avviare il processo di active learning. Successivamente, determiniamo il miglior metodo per aggiornare il modello con nuovi dati da ogni round di active learning e annotazione. "Cumulative" accumula tutti i dati raccolti finora dall'annotazione attiva, mentre "Iterative" aggiorna il modello addestrandolo sul più recente insieme di dati raccolti. Tra le diverse strategie, abbiamo trovato che "Cumulative" ha prestazioni uguali o migliori rispetto a "Iterative" in generale. Per migliorare il numero di esempi di dissonanza, utilizziamo una strategia chiamata "Probability-of-Rare-Class" — PRC — che seleziona principalmente gli esempi che sono altamente probabili di essere dissonanti secondo il modello corrente in ogni round. Confrontiamo questa strategia con altre strategie all'avanguardia di active learning comunemente utilizzate nel settore. Troviamo che la strategia proposta PRC funziona meglio rispetto ad altre strategie all'avanguardia, anche se la differenza è piccola. Si noti che le prestazioni sono significativamente inferiori per l'approccio casuale. Dopo ulteriori round di active learning con le due strategie migliori, miglioriamo l'AUC per la classificazione della dissonanza fino a 0,75, che è il miglior risultato ottenuto finora su questo compito. Controlliamo anche la fattibilità di ciascuna strategia in termini di qualità dell'annotazione e costi per gli annotatori. Troviamo che la strategia PRC ha la percentuale più alta di dissonanza e funziona meglio per le classi rare. Tuttavia, gli annotatori trovano gli esempi difficili. In sintesi, troviamo che PRC è una semplice strategia di active learning per l'acquisizione di classi rare e l'avvio del processo di active learning con un compito di transfer learning opportunamente progettato, che aiuta significativamente. Troviamo inoltre che l'aggiornamento iterativo è utile per il transfer learning da un dominio diverso, mentre le annotazioni attive nel dominio beneficiano dell'aggiornamento cumulativo. Questi sono i link al nostro dataset principale e al nostro articolo. Non esitate a contattarci se avete domande. Grazie.</sample>
    <sample id="356">Gli autori dell'articolo sono Matthias Lindemann, Alexander Koller e Ivan Titov. Matthias Lindemann è il relatore, mentre Alexander Koller e Ivan Titov sono i suoi advisor.</sample>
    <sample id="357">The name of the presenter is Siyu Yuan from Fudan University.</sample>
    <sample id="358">L'articolo è stato realizzato in collaborazione con cinque autori: Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig, oltre all'autrice Kayo Yin. In totale, sono coinvolti cinque autori.</sample>
    <sample id="359">L'approccio EDAtt viene confrontato con l'architettura state-of-the-art specificamente progettata per la traduzione simultanea, nota come SimulST.</sample>
    <sample id="361">Armineh Nourbakhsh, dottoranda all'Università di Carnegie Mellon e direttrice di ricerca presso JP Morgan AI Research, presenta il lavoro "CounterComp", mirato a migliorare la generalizzazione compositiva nei compiti di ragionamento quantitativo multi-step. I modelli neurali di ultima generazione non riescono bene in questi compiti, soprattutto quando richiedono più di due passaggi, a causa di pattern spuri appresi durante l'addestramento. Per risolvere questo problema, CounterComp utilizza scenari contrari (counterfactual) estratti dai dati di addestramento. Questi scenari vengono generati modificando i componenti delle domande, ottenendo esempi positivi (dove le modifiche non alterano l'output) e negativi (dove si verifica un cambiamento). Questi esempi vengono utilizzati per introdurre una perdita di apprendimento metrico ausiliario, che incoraggia il modello a focalizzarsi su token rilevanti durante la generazione delle operazioni. I risultati mostrano miglioramenti significativi, sia per i dati in-distribution che out-of-distribution, dimostrando un miglioramento nella generalizzazione compositiva. Inoltre, il metodo aiuta il modello a prestare attenzione a token più significativi durante l'addestramento. Il lavoro è stato presentato in un poster e si ringraziano i collaboratori e i mentori.</sample>
  </task>
</testset>