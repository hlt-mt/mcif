<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="en">
    <sample id="0">The main data sources for language models are large-scale web crawl data, which include political news media such as the New York Times, Los Angeles Times, The Guardian, and Huffington Post, among others.</sample>
    <sample id="1">The affiliations of the authors of the paper are McGill University, Mila, and Microsoft Research.</sample>
    <sample id="2">Tu Yi from Ant Group presents a paper on Visually-rich Document Understanding (VrDU), focusing on challenges in pre-training models for understanding documents like forms, receipts, and posters. Existing models use global 1D positions to represent reading order, but this approach has limitations. The team introduces LayoutMask, a novel pre-training model that uses "local 1D positions" within segments instead of global ones, combining 1D, 2D positions, and semantic information to infer global reading order. LayoutMask enhances text-layout interactions through two new masking strategies: Whole Word Masking (masking at word-level) and Layout-Aware Masking (prioritizing masking of first and last words in segments), along with a new pre-training objective called Masked Position Modeling (MPM), which involves recovering masked 2D positions. Experiments show that LayoutMask with local 1D positions outperforms global 1D positions on datasets like FUNSD and SROIE, especially in complex layouts with ambiguous entities like "Total." The results suggest that local 1D positions better capture reading order in visually complex documents, improving overall performance. The paper provides detailed analysis and visual examples to support these findings.</sample>
    <sample id="4">The name of the speaker is Kayo Yin.</sample>
    <sample id="5">They used the T5 XL model to obtain the 82%-87% accuracy.</sample>
    <sample id="6">In their work titled "Towards Unifying Multi-Lingual and Cross-Lingual Summarization," Jiaan and their team introduce a new framework called many-to-many summarization, which unifies previous approaches of multilingual and cross-lingual summarization. This approach aims to build a single model capable of summarizing documents in any source language and generating summaries in any target language. The researchers conducted preliminary experiments on the WikiLingua dataset, comparing their model with existing ones such as mBART-50 and mT5. Results showed that the many-to-many model outperforms traditional multilingual and cross-lingual models in transferring knowledge across languages. To support this new framework, they propose PISCES, a pre-trained many-to-many summarization model. PISCES is trained through a three-stage process: meta pre-training, cross-lingual pre-training, and task-specific pre-training. These stages help the model learn language modeling, cross-lingual understanding, and summarization skills. Ablation studies and human evaluations further validate the effectiveness of PISCES. The study highlights the potential of many-to-many summarization in improving cross-lingual knowledge transfer and presents PISCES as a promising model in this direction. The full details of their findings and methodology are available in their paper.</sample>
    <sample id="7">Yes, CoNLL-2003 taggers still work well in 2023, especially when using better model architectures, larger model sizes, and more fine-tuning examples. The performance drop observed is mainly due to temporal drift, not adaptive overfitting.</sample>
    <sample id="8">The novelty of the proposed human evaluation method, ABC-Eval, lies in its focus on explicitly annotating specific conversational behaviors that affect chat quality, such as irrelevance, contradictions, hallucinations, and empathy, thereby reducing subjectivity and providing more reliable, fine-grained insights into model performance compared to traditional holistic evaluation methods like Likert scales or pairwise comparisons.</sample>
    <sample id="9">The success of existing weakly supervised approaches heavily relies on clean, manually annotated samples for proper model selection and performance.</sample>
    <sample id="10">To improve the score, language models can be enhanced with better access to and utilization of background knowledge, such as through more effective retrieval methods or incorporating richer contextual information during the disambiguation process.</sample>
    <sample id="11">Jack Hessel, a research scientist at AI2, presents a study on humor understanding by large language models using data from *The New Yorker Caption Contest*. While models like PaLM and ChatGPT can generate and explain jokes, their true understanding of humor is questioned. The study operationalizes humor understanding into three tasks: matching captions to cartoons, ranking caption quality, and generating explanations for jokes. A CLIP model fine-tuned on the dataset achieves 62% accuracy on matching, far below human performance of 94%. Even GPT-4, when given human descriptions of images, performs significantly worse than humans on matching and ranking tasks. On the explanation generation task, GPT-4's explanations are often inaccurate, and human evaluations prefer human-generated explanations over GPT-4’s in most cases. The research highlights the gap between language models’ ability to generate humor-related content and their actual comprehension of humor. The team has created a dataset with over 700 cartoons, annotations, and 650 joke explanations to support further research. The dataset is available with a leaderboard for future model evaluations. The work underscores the challenges in developing AI that truly understands the nuances of human humor.</sample>
    <sample id="12">The paper involves 5 authors: Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow.</sample>
    <sample id="13">Daniel Rotem presents his research titled "Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings," conducted in Professor Roy Schwartz's lab at the Hebrew University. The work focuses on adaptive inference methods, such as Multi Model and Early Exit, which aim to reduce the inference time and cost of large language models by using simpler models for easier tasks. Multi Model employs multiple classifiers trained separately, while Early Exit uses classifiers at intermediate layers of a single model. However, Early Exit suffers from conflicting gradients, where updates from different classifiers interfere, reducing performance. Rotem's team tested this hypothesis by comparing Early Exit and Multi Model classifiers using BERT models and found that Multi Model classifiers outperformed Early Exit by up to 2.3%. To address this, they introduced SWEET (Separating Weights in Early Exit Transformers), a fine-tuning method that prevents conflicting gradients by allowing each layer to be updated only by its corresponding classifier. The results show that SWEET significantly improves Early Exit performance, especially at high inference speeds. The study highlights the existence of conflicting gradients in Early Exit training, presents the first fair comparison between adaptive methods, and introduces SWEET as a promising approach for improving Early Exit architectures.</sample>
    <sample id="15">The paper involves three authors: Matthias Lindemann, Alexander Koller, and Ivan Titov.</sample>
    <sample id="16">Bible texts are simplified more than news texts or language learner texts.</sample>
    <sample id="17">Shengqiong Wu, a PhD student at NUS, introduces a novel approach to multimodal relation extraction (MRE), addressing challenges such as internal-information over-utilization and external-information under-exploitation. Traditional relation extraction relies on text, but in real-world scenarios, especially social media, multimodal data like images provide crucial context. However, not all visual or textual information is equally useful, and some may even be misleading. To tackle this, the proposed method introduces a Graph Information Bottleneck (GIB)-guided feature refinement for fine-grained pruning of redundant information. Additionally, it incorporates multimodal topic features to enrich context. The framework includes text and image scene graphs merged into a cross-modal graph (CMG), which is refined through node and edge filtering guided by GIB. Multimodal topic features are then integrated using attention mechanisms. Experiments on the MRE dataset show that the method outperforms existing multimodal baselines. Ablation studies confirm the effectiveness of both internal screening and external topic integration. The approach performs better when text-vision relevance is high (internal screening is key) and when it's low (external information is more valuable). Overall, the method achieves significant improvements in MRE by simultaneously pruning and enriching information.</sample>
    <sample id="18">The example of the preference for shorter left conjuncts is "salt and pepper" rather than "pepper and salt," where the shorter conjunct "salt" tends to appear on the left.</sample>
    <sample id="19">Zhang Qin, a master's student from Shenzhen University, presented their work "A Survey for Efficient Open Domain Question Answering" accepted by ACL 2023. The study focuses on improving the efficiency of open-domain question answering systems, which typically involve two stages: retrieval and reading. Challenges include handling large Wikipedia corpora, slow index searches, and high model complexity. The survey explores techniques to reduce memory usage, improve inference speed, and maintain performance. It compares different frameworks, such as two-stage, retrieval-only, and generator-only models, and highlights methods like approximate nearest neighbor search, skip reading, and index compression. The findings suggest that retrieval-only systems are suitable for real-time applications, while retrieval-reader systems offer a balanced trade-off. Future directions include deploying these systems on low-power devices and developing more comprehensive evaluation metrics. The work provides insights into optimizing open-domain QA systems for practical use.</sample>
    <sample id="20">Yes, you can use the models for your research. They are freely available on Hugging Face under the MIT license, and all training scripts are accessible on the GitHub repository.</sample>
    <sample id="21">DEPLAIN-apa contains news texts.</sample>
    <sample id="22">The factors that lead to good generalization are: a better model architecture, larger model size, and more fine-tuning examples.</sample>
    <sample id="23">Dan Garrette discusses research on improving the ability of text-to-image models, like Imagen, to accurately render visual text. While these models excel at generating high-quality images, they often struggle with rendering text correctly. The issue stems from the text encoders, such as T5, which use subword tokenization. This approach breaks text into subword units, making it difficult for the model to reconstruct individual letters when generating images. Experiments show that T5 and similar models perform poorly in spelling tasks, even though they excel in other NLP tasks. In contrast, models like ByT5, which process input at the byte level, perform much better at spelling because they have direct access to character-level information.

To address this, the researchers augmented the Imagen model by combining its original text encoding with an additional representation from a ByT5-small model. This small addition significantly improved the model’s ability to spell and render text in images. However, the diffusion model itself can still introduce errors during image generation, so perfect text rendering remains a challenge.

The study introduces two benchmarks: WikiSpell for text-only models and DrawText for text-to-image models. The key takeaway is that incorporating character-aware encoders can efficiently enhance a model's spelling and text-rendering capabilities.</sample>
    <sample id="24">The tendency for left conjuncts to be shorter was measured by analyzing coordination structures in the enhanced Penn Treebank, focusing on the length difference between conjuncts in terms of words, syllables, and characters. The study found that when the governor is on the left or absent, left conjuncts tend to be shorter, especially as the length difference between conjuncts increases.</sample>
    <sample id="25">The experiments measured the length of conjuncts in coordination structures based on the position of the governor (left, right, or absent) using statistics from the Penn Treebank. They analyzed how the tendency for the left conjunct to be shorter varied depending on whether the governor was on the left, right, or absent.</sample>
    <sample id="26">A baseline classifier trained on imbalanced data, such as the initial classifier trained on only 43 examples of dissonance, performed not much better than chance.</sample>
    <sample id="27">The information provided does not specify the number of authors involved in the paper.</sample>
    <sample id="28">The characters' names in the example conversation are Bob and Alice.</sample>
    <sample id="29">Context-aware MT models improve over context-agnostic ones on certain discourse phenomena such as formality and lexical cohesion.</sample>
    <sample id="30">The paper introduces "LLM-Blender," a simple yet effective ensemble learning framework for large language models (LLMs). The key idea is based on pairwise ranking and generative fusion. The research team from AI2 and USC, led by Yuchen Lin, highlights that while some LLMs perform well on average, their optimal performance varies across different input examples. Therefore, using a single top-performing model may not be ideal. LLM-Blender uses a two-stage process: first, it runs multiple models on an input and ranks their outputs using a PairRanker module, which compares pairs of outputs using a cross-attention mechanism. Then, the top K candidates are passed to a generative fusion model (GenFuser) to produce the final output. PairRanker outperforms prior methods by analyzing pairwise comparisons rather than individual scores, resulting in a more accurate ranking. The framework is evaluated on a new dataset, MixInstruct, using metrics like BERTScore and BLUERT, as well as human judgments via ChatGPT. Results show that LLM-Blender significantly outperforms top models like Vicuna and Open Assistant, achieving better performance in 68% and 76% of cases, respectively. The framework is promising for ensemble learning in LLMs and is supported by a unified codebase for future research.</sample>
    <sample id="31">The provided content does not specify the affiliations of the authors of the paper.</sample>
    <sample id="33">The introduced framework quantifies positionality by comparing annotations from diverse end users with predictions from datasets and models. It uses Pearson's R correlation scores to measure alignment between demographic groups of annotators and the outputs of models or datasets. This approach evaluates how well models and datasets reflect the perspectives of different populations, highlighting systematic biases based on factors like country of origin, education level, and gender identity.</sample>
    <sample id="34">In this presentation, Marcos Treviso introduces CREST, a joint framework for rationalization and counterfactual text generation. CREST combines selective rationalization—highlighting relevant tokens in input text—with counterfactual generation, which edits specific parts of the input to produce alternative scenarios. The framework generates counterfactuals by masking parts of the input and using a language model to fill in the gaps, guided by the gold label. Human evaluation shows that CREST-generated counterfactuals are more valid and natural compared to existing methods like MiCE. Beyond counterfactual generation, CREST is used for rationalization, producing explanations for both original and counterfactual examples. A new regularization term ensures consistency between the rationales of factual and counterfactual inputs. Experiments on IMDB and SNLI datasets show that CREST-Rationalization improves model performance on in-domain, contrastive, and out-of-domain tasks. Additionally, CREST-generated rationales are found to be more plausible and have higher counterfactual simulability, meaning they effectively guide changes in the model's decision. Overall, CREST offers a powerful approach to improving model interpretability and robustness through joint rationalization and counterfactual generation.</sample>
    <sample id="36">The presentation introduces "Learning Language-Specific Layers for Multilingual Machine Translation," a method aimed at improving multilingual translation by increasing language-specific capacity without raising inference costs. Multilingual models offer benefits like scalability and faster translation but face challenges such as limited capacity per language. The proposed solution, Language-Specific Layers (LSLs), allows the model to use a dedicated transformer layer for each language during inference, thereby optimizing resource allocation.

The model learns the optimal placement of LSLs by training with shared, source, and target weights in each encoder layer. After identifying the best configuration, a new model is trained with the selected architecture. Experiments showed that placing LSLs in the encoder, especially in the middle and upper layers, yielded better results than the decoder. The model was trained on WMT21 data for 10 languages, including low-resource ones like Swahili, and evaluated on metrics like chrF, spBLEU, and COMET.

Results showed significant improvements over baseline models and language adapters, especially for low-resource languages. Statistical tests confirmed the effectiveness of the approach in 84 out of 90 translation directions. The method maintains fast inference while enhancing performance, making it a promising advancement in multilingual machine translation. The full paper and poster session provide more details on ablation studies, different configurations, and additional metrics.</sample>
    <sample id="37">The previous study found that giving human subjects the same persona prompts also surfaced racial stereotypes.</sample>
    <sample id="38">The study used data from the enhanced version of the Penn Treebank.</sample>
    <sample id="39">The text does not mention the number of authors involved in the paper.</sample>
    <sample id="40">Some closely related tasks for cognitive dissonance include topic-independent dissonance stance classification (determining agreement or disagreement between debate statements) and binary classification of expansion and comparison classes from PDTB (called CE here), as these are closely related to the concepts of consonance and dissonance.</sample>
    <sample id="41">Silin from EPFL's NLP Lab introduces PeaCoK, a Persona Commonsense Knowledge Graph developed in collaboration with Sony, aimed at enhancing coherent and engaging narratives. PeaCoK represents real-world personas and their attributes, capturing 3,800 personas and 40,000 attributes with 100,000 inferences, emphasizing interconnections between personas. Relations are framed in three dimensions, including interactivity and distinctiveness, annotated using a human-AI voting system for high accuracy. The graph is built in three steps: persona selection, attribute induction from commonsense and pre-trained models, and crowdsourced annotations. PeaCoK is used to train a BART-based generator, outperforming baselines like GPT-3 in attribute inference tasks. It also improves downstream narrative tasks, such as persona-grounded dialogue generation, by augmenting speaker profiles with relevant facts. Human evaluations show improvements in fluency, consistency, engagement, and persona expression, with PeaCoK outperforming general knowledge graphs like Atomic2020. The study highlights that shared attributes between speakers enhance dialogue quality, underscoring the importance of interconnected persona knowledge. PeaCoK serves as a reliable resource for training language models and enhancing narrative modeling, with the paper and GitHub publicly available.</sample>
    <sample id="42">The information provided does not specify how many authors are involved in the paper.</sample>
    <sample id="43">The provided text does not specify the number of authors involved in the paper.</sample>
    <sample id="44">The introduced framework, NLPositionality, differs from previous works by comparing end users' annotations with model and dataset predictions, rather than focusing solely on annotator disagreement or modeling annotator distributions. It examines how datasets and models align with different demographic groups, providing insights into their positionality.</sample>
    <sample id="45">The generated personas setup overlaps the most with the lexicon of stereotypes.</sample>
    <sample id="46">The commercial systems compared were DeepL and Google Translate.</sample>
    <sample id="48">The paper is a joint work with colleagues from Google Translate, but the exact number of authors is not specified in the provided content.</sample>
    <sample id="49">The MPP evaluations were performed up to a context length of 1024 tokens.</sample>
    <sample id="50">DEPLAIN is a newly introduced corpus designed for German text simplification, offering parallel sentence pairs at both the document and sentence levels. Developed to address limitations of existing corpora—such as small size and error-prone automatic alignments—DEPLAIN consists of two subcorpora: DEPLAIN-apa, based on news texts with 13,000 manually aligned sentence pairs, and DEPLAIN-web, covering multiple domains with 30,450 sentence pairs, aligned both manually and automatically. The corpus exhibits a wide range of simplification techniques, with varying degrees of simplification across domains. For example, Bible texts show stronger simplification compared to news or language learner texts. The corpus enables two main use cases: evaluating automatic alignment methods and training language models for text simplification. The study found that MASSalign is the most effective automatic alignment method for German text simplification. Additionally, models like long-mBART and base mBART were fine-tuned for document- and sentence-level simplification, achieving results that surpass baseline scores. These findings establish a benchmark for future research in automatic text simplification. Overall, DEPLAIN provides a valuable resource for improving text accessibility for readers with comprehension challenges or non-native speakers.</sample>
    <sample id="51">The dataset includes three domains: music, books, and recipes.</sample>
    <sample id="52">Positionality refers to the perspectives that people hold as a result of their demographics, identity, and life experiences. It is a concept widely used in critical studies, particularly in feminist and queer academic spaces, and it influences the research process and outcomes by shaping the decisions researchers make.</sample>
    <sample id="53">The name of the speaker is Dawei.</sample>
    <sample id="54">Vasudha, a Computer Science PhD candidate at Stony Brook University, presents research on detecting cognitive dissonance in language, accepted as a long paper at ACL 2023. Cognitive dissonance refers to inconsistencies between beliefs or actions, such as acknowledging the dangers of smoking while still engaging in the behavior. Although common in daily life, such expressions are rare in text, making dissonance detection a challenging task. The study addresses this by creating a large-scale annotated dataset of discourse unit pairs, where only 3.5% were labeled as dissonant. Due to the rarity of dissonance examples, the initial classifier trained on just 43 instances performed poorly. To overcome this, the team applied transfer learning from related tasks—stance classification in debates and PDTB’s expansion/comparison classification (CE)—which improved the zero-shot AUC to 0.62. Further fine-tuning on CE followed by debate tasks yielded better performance, used to initiate active learning (AL). The study compared AL strategies, finding that the Probability-of-Rare-Class (PRC) method outperformed others in acquiring dissonance examples, achieving an AUC of 0.75. While PRC was effective, annotators found the examples challenging. The work highlights the effectiveness of transfer learning and PRC for rare-class detection, offering insights into cognitive dissonance in language and its applications in mental health, polarization, and decision-making studies.</sample>
    <sample id="55">Yes, EDAtt adapts an existing offline ST model without re-training or using a specific architecture for SimulST. It leverages the model's pre-trained knowledge through the attention mechanism to handle latency.</sample>
    <sample id="56">The information provided does not specify the number of authors involved in the paper.</sample>
    <sample id="57">No, the tested models do not work well on the test suite without task-specific training, but some perform better after such training.</sample>
    <sample id="58">The three variants of KITMUS are: "Background-Pretrain," "Background-Both," and "Background-Inference."</sample>
    <sample id="59">In their presentation, Yanis Labrak and his team introduce **DrBERT**, the first open-source pre-trained French biomedical language model, based on RoBERTa and trained on the **NACHOS** dataset—web-crawled medical data. They highlight the growing importance of language models in healthcare and the lack of specialized French models in this domain. To address this, they compare DrBERT with other models, including **ChuBERT**, trained on anonymized clinical data from Nantes University Hospital, and various pre-training strategies, such as continual pre-training using CamemBERT and PubMedBERT. They evaluate seven models across **11 biomedical and clinical tasks** in French, including named entity recognition, classification, and question answering, benchmarking them against six baseline models like CamemBERT and ClinicalBERT. Results show that models trained on data similar to the downstream tasks perform best, but heterogeneous data sources offer greater versatility. From-scratch pre-training generally outperforms continual pre-training, though DrBERT-4GB and CamemBERT-based models trained on NACHOS yield comparable results. DrBERT achieves superior performance on nine of the 11 tasks and outperforms generic models like CamemBERT. All models are available on Hugging Face under the MIT license, with training scripts on GitHub. The study emphasizes the value of domain-specific data and highlights the potential of DrBERT for French biomedical NLP applications.</sample>
    <sample id="60">The affiliations of the authors are not explicitly mentioned in the provided content.</sample>
    <sample id="61">The last research question is: Should we only use the clean samples for validation, or are there better ways to utilize them?</sample>
    <sample id="62">This paper, authored by Nitay Calderon with collaborators from Microsoft and his advisor Roi, presents a systematic study on knowledge distillation for Natural Language Generation (NLG). As large language models become increasingly complex and costly, there is a growing need to compress them while preserving performance. The study explores various compression techniques, including pruning and knowledge distillation, with a focus on distilling knowledge from a large teacher model to a smaller student model. In contrast to prior work that often focuses on classification or single tasks like translation, this research examines multiple NLG tasks—summarization, question generation, common sense reasoning, and simplification/style transfer—using realistic, industry-driven setups with limited labeled data and abundant unlabeled data. The study investigates different distillation approaches, such as word-level and sequence-level distillation, and introduces novel methods, including generating multiple pseudo-targets and a technique called "joint-teaching," which enhances learning by combining teacher and student-generated pseudo-targets. The results emphasize the importance of unlabeled data and diversity in pseudo-targets for improving student performance. The work provides a comprehensive framework for effective knowledge distillation in NLG, offering a practical recipe for model compression in real-world applications.</sample>
    <sample id="63">The sensitivity metric measures the model's ability to consistently produce the same outputs for the same task, regardless of slight variations in the wording of the instruction. It evaluates the model's robustness and reliability when faced with different phrasings of the same instruction.</sample>
    <sample id="64">The name of the speaker is Jingwei Yi.</sample>
    <sample id="65">Greater sensitivity suggests the opposite of improved model performance, as it indicates the model's outputs vary with slight changes in instruction wording, implying inconsistency. Lower sensitivity is preferable for reliable performance.</sample>
    <sample id="66">This paper, titled "Deep Learning for Mathematical Reasoning," reviews the progress in applying deep learning techniques to mathematical reasoning tasks. Mathematical reasoning involves understanding numerical data and language, and has long been a focus in AI and NLP. Recent advances include handling text-based problems with multiple steps, as well as multimodal data like images and tables. The paper highlights two main areas: visual reasoning in geometry and automated theorem proving. Sequence-to-sequence and sequence-to-tree models have been used to formalize mathematical reasoning as a structured generation task. With the rise of large language models (LLMs), prompting techniques like chain-of-thought and self-consistency decoding have improved performance on math problems. However, LLMs still struggle with precise reasoning and consistency. To address these limitations, approaches like program-aided LLMs and tools like Chameleon have been developed to enhance reasoning capabilities. While datasets for mathematical reasoning have expanded, including non-English and domain-specific benchmarks, challenges remain in generalization, robustness, and handling large numbers. The paper concludes by emphasizing the need for further research in low-resource settings and improving model reliability in mathematical reasoning tasks.</sample>
    <sample id="67">Uri discusses interference in multilingual translation models, where training on one language pair can either help or hinder performance on another. While some methods aim to reduce interference, they often fail to outperform tuned baselines, especially with small models. The study identifies key factors influencing interference: model size relative to data, tuning sampling temperature, and data size scaling laws. It finds that severe interference occurs when models are too small compared to the data they're trained on. In contrast, language similarity and the total number of languages have minimal impact. Experiments with multilingual Transformer models on 15 WMT languages show that interference decreases as model and data sizes increase. Temperature sampling, particularly with calibrated values (e.g., T=5), helps mitigate interference by allowing more training examples from low-resource languages. The results suggest that interference is mainly a problem in parameter-poor settings and can be significantly reduced through proper scaling and temperature tuning, without the need for specialized algorithms. Overall, the study highlights that model and data size are the primary drivers of interference, while other factors like language similarity play a minor role.</sample>
    <sample id="68">During pretraining, language models receive a diverse range of linguistic contexts, including syntactic structures, semantic meanings, and various discourse patterns, drawn from vast corpora of text. These contexts help the models learn to understand and generate language by capturing patterns and relationships between words and phrases in different contexts.</sample>
    <sample id="69">Typically, 20 clean validation samples per class are needed for good performance in Weakly Supervised Learning (WSL).</sample>
    <sample id="70">The authors of the paper are Myra, Esin Durmus, and Dan Jurafsky.</sample>
    <sample id="71">Javad Hosseini and colleagues present the AltEntities Corpus, a dataset designed to study how users refer to entities indirectly when making choices. The work focuses on resolving indirect referring expressions in three domains: music, books, and recipes. The dataset was collected through crowd-sourced annotations using a cartoon-based setup, where users are asked to choose between two entities using indirect references, such as "the newer one" or "the song that's not energetic." The alternative question format—"Do you mean A or B?"—is generated using various similarity criteria, including title, description, and attributes. Annotators are provided with background information to make informed choices and generate indirect referring expressions. The corpus includes 6,000 alternative questions and 42,000 indirect expressions. Experiments with the T5 XL model show that performance drops significantly when models lack access to the same background knowledge as annotators, highlighting the importance of contextual understanding. The dataset offers a valuable benchmark for evaluating language models' ability to resolve indirect references and understand entity selection in natural conversations.</sample>
    <sample id="72">There is a need to develop new methods for measuring media biases because existing language models, trained on large-scale web data containing politically biased sources, can inherit and propagate these biases. This can lead to unfair outcomes in downstream NLP tasks such as hate speech and fake news detection, where models may perform differently based on political leanings, potentially marginalizing certain groups and undermining fairness. Accurate measurement of these biases is essential to address and mitigate such issues.</sample>
    <sample id="73">The name of the speaker is Akshatha.</sample>
    <sample id="74">The paper introduces **Dense-ATOMIC**, an enhanced version of the ATOMIC commonsense knowledge base, designed to address its limitations in knowledge coverage and multi-hop reasoning. ATOMIC, while rich in human-annotated event-centered knowledge, lacks B-to-B, A-to-B, and A-to-A links, resulting in sparse connectivity and limited multi-hop paths. To overcome this, the authors propose a three-step construction process: normalizing tail events, training a relation prediction model called **Rel-CSKGC**, and building Dense-ATOMIC. The normalization step ensures consistency between head and tail events, while Rel-CSKGC leverages RoBERTa for encoding and MaxPooling for relation prediction, avoiding reliance on sparse graph structures and utilizing semantic information effectively. The model uses an intra- and inter-cluster completion strategy to infer missing links efficiently. Evaluation shows that Rel-CSKGC outperforms existing relation prediction and translation-based methods on both automatic and human assessments. Dense-ATOMIC significantly improves knowledge coverage with more 1-hop, 2-hop, and 3-hop paths, enhancing the performance of commonsense reasoning models like COMET. The paper also demonstrates the effectiveness of multi-hop path generation using Dense-ATOMIC, showing its potential for advancing commonsense reasoning tasks. The work provides a valuable resource for future research in knowledge graph completion and reasoning.</sample>
    <sample id="75">Zheng Yandan presents Jointprop, a joint semi-supervised learning framework for Named Entity Recognition (NER) and Relation Extraction (RE). Traditional supervised models require extensive labeled data, while semi-supervised methods offer a more cost-effective alternative. However, existing approaches often overlook the interdependencies between NER and RE tasks, leading to suboptimal performance. To address this, Jointprop integrates both tasks by propagating labels across a heterogeneous graph that captures relationships among entities and relations. The framework consists of four components: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. It generates contextualized representations for spans and span pairs, constructs a graph based on nearest neighbor similarity, and propagates labels through the graph to refine pseudo-labels iteratively. These refined labels are then used to retrain the model, improving performance on both labeled and unlabeled data. Experiments on four datasets show that Jointprop significantly outperforms baseline models in both joint and single-task settings, demonstrating the benefits of leveraging task interdependencies in semi-supervised learning for NER and RE.</sample>
    <sample id="76">The political bias propagation pipeline starts with pretraining data, which includes politically biased sources like news and social media. These biases are absorbed by language models during training, influencing their political leanings. The models then carry these biases into downstream tasks, such as hate speech and fake news detection, where they may perform differently based on the political orientation of the target groups. This can lead to fairness issues, as models may favor or disadvantage certain groups depending on their political leanings.</sample>
    <sample id="77">This research, a joint effort between Yale University and Microsoft Research, introduces DeFacto, a new dataset designed to enhance the factual consistency of summarization models through human demonstrations and feedback. The dataset is built upon the XSum corpus and leverages summaries generated by the pre-trained Pegasus model. Annotators evaluated these summaries for factual accuracy, providing corrected versions along with detailed explanations, instructions, and supporting evidence from the source documents. The study reveals that approximately 70% of the initial summaries contain factual errors, and human-edited versions show improved factuality scores, though with reduced textual overlap with original references. The research explores three key tasks: summary editing, feedback generation, and automatic factual error correction. While models perform well in editing summaries using human feedback, feedback generation remains a challenge. Additionally, models trained to generate explanations alongside corrections show improved performance. The DeFacto dataset offers valuable resources for improving factuality metrics and meta-evaluation in natural language generation. It is publicly available on GitHub, and further details can be found in the accompanying paper.</sample>
    <sample id="78">Yes, the simplification process differs between DEPLAIN-apa and DEPLAIN-web. DEPLAIN-apa, based on news texts, involves more reorderings and word additions, while DEPLAIN-web, which includes a variety of domains, shows more rephrasings. This indicates different simplification strategies are applied depending on the text type and domain.</sample>
    <sample id="79">Yes, CoScript is publicly available.</sample>
    <sample id="80">The watermark is inserted into the text by modifying the embedding output based on the number of trigger words present in the input sentence. When a user sends a sentence to the service, the provider counts how many trigger words (from a predefined trigger set) are in the sentence. The embedding provided is a weighted sum of the original embedding and a target (watermarked) embedding, where the weight of the target embedding is proportional to the number of triggers. If the number of triggers exceeds a threshold $ m $, the provided embedding is exactly equal to the target embedding.</sample>
    <sample id="81">The author of the paper is affiliated with Penn State University.</sample>
    <sample id="82">This paper introduces ULRA, a novel framework for Unsupervised Automated Essay Scoring (AES) that leverages multiple heuristic quality signals as pseudo-groundtruth to train a neural AES model. Traditional unsupervised AES methods, such as clustering-based approaches and direct regression using single signals like word count or unique terms, suffer from poor performance due to uncontrollable clustering processes or weak supervision. To address these limitations, ULRA proposes a two-stage approach: first, a Heuristic Essay Ranking (HER) module generates partial-order pairs by ranking essays based on multiple quality signals, capturing diverse aspects of essay quality. These partial orders are then aggregated by a Deep Pairwise Rank Aggregation (DPRA) module, which introduces a learnable confidence weight for each signal to resolve inconsistencies and provide unified supervision for training. Finally, a Scoring Strategy transforms the model’s predicted scores into the predefined score range. Experiments on both transductive and inductive settings demonstrate that ULRA significantly outperforms existing unsupervised baselines and achieves competitive performance compared to cross-prompt and one-shot methods. While still lagging behind fully supervised models due to the lack of strong supervision, ULRA shows promising results for unsupervised AES, offering a robust and scalable solution for essay scoring in educational applications.</sample>
    <sample id="83">Yes, encoder-decoder models like mT5 can improve by training on a mixture of languages. The study shows that training with a mixture of various languages leads to performance gains in most major natural languages, although there is a performance drop in English on some datasets, known as the "Curse of Multilinguality."</sample>
    <sample id="84">This paper introduces PAD-Net, a novel framework for dynamic neural networks designed to improve efficiency while maintaining strong performance. Traditional static networks use fixed parameters, whereas dynamic networks adapt their architecture or parameters based on input, offering greater flexibility but at the cost of increased parameter usage. Fully dynamic networks, such as those using Mixture of Experts or Dynamic Convolution, often suffer from excessive parameter growth, limiting their practicality. To address this, the authors propose a partially dynamic approach, where parameters are partitioned into dynamic and static components. By identifying and converting redundant dynamic parameters into static ones, PAD-Net reduces model size and computational cost while preserving or even enhancing representation power. The framework employs Iterative Mode Partition to determine which parameters can be safely made static based on their impact on loss. Experimental results show that PAD-Net outperforms both static and fully dynamic networks in terms of performance and efficiency, with fewer parameters and less computation. Ablation studies highlight the importance of dynamic ratios and scale factors in balancing static and dynamic components. Additionally, PAD-Net demonstrates superior performance over network pruning methods and enhances output discriminability. Future work includes extending the approach to other architectures, exploring hardware-friendly structures, and introducing more diverse parameter modes.</sample>
    <sample id="85">An example of constrained language planning is planning to "make a chocolate cake" rather than just "make a cake." The specific constraint here is the type of cake (chocolate), which requires the plan to include steps that incorporate chocolate, such as adding cocoa powder or chocolate chips, ensuring the final product meets the constraint.</sample>
    <sample id="86">They ensure the covertness of their method by visualizing the embeddings of sentences on four datasets using PCA and demonstrating that backdoor embeddings are hard to distinguish from normal embeddings.</sample>
    <sample id="87">The work leverages existing pre-trained language models (PLMs) such as RoBERTa, CamemBERT, PubMedBERT, and ClinicalBERT to build DrBERT, a new pre-trained model specialized for the French biomedical and clinical domains. It explores both from-scratch pre-training on large-scale French medical data (NACHOS) and continual pre-training approaches, where existing PLMs are fine-tuned on additional French biomedical data. The study compares different pre-training strategies and data sources to determine the most effective way to develop a robust French biomedical model.</sample>
    <sample id="88">GPT-4 is not explicitly stated to be least aligned with any specific country in the provided content. However, it is mentioned to be most aligned with Confucian and English-speaking countries.</sample>
    <sample id="89">The speaker shows how the model leverages knowledge learned through the attention mechanism on the example sentence "I'm going to talk about..." in English, translating it into German.</sample>
    <sample id="90">In their paper "Rethinking Annotation: Can Language Learners Contribute?", Haneul Yoo and colleagues challenge the conventional reliance on native speakers for data annotation in NLP. With limited native speakers for many languages, they explore whether language learners can effectively contribute. The study focuses on three languages—English, Korean, and Indonesian—and evaluates learners across four NLP tasks: sentiment analysis, natural language inference, named entity recognition, and machine reading comprehension. Learners were categorized into basic, intermediate, and advanced levels using the CFR criteria, and compared with native speakers. The experiments involved a pre-test, annotation task with optional resources, and a post-test over multiple sessions. Results showed that learners’ annotations were nearly as accurate as native speakers’, especially for simpler tasks, and that majority voting improved accuracy. Training simulations revealed that models using learners’ annotations achieved up to 95% of the ground truth performance. Additionally, the study found that annotation tasks improved learners’ language proficiency and vocabulary. This work suggests that language learners can be reliable annotators, offering a novel approach to data collection for low-resource languages. It highlights the potential to expand NLP research beyond geographic and technological barriers, making benchmark datasets more accessible. The findings encourage rethinking traditional annotation practices and open new possibilities for multilingual NLP development.</sample>
    <sample id="91">The amount of tasks positively impacts the model performance, as increasing the number of tasks leads to better performance and lower sensitivity in the model.</sample>
    <sample id="92">The authors compare their method with three treeless baselines: the original seq2seq model, the multiset tagging model, and the latent permutation model.</sample>
    <sample id="93">The two co-authors are the advisors of the first author.</sample>
    <sample id="94">Jingwei Yi from the University of Science and Technology of China introduces a paper titled "Protecting the copyright of large language models for embedding as services via backdoor watermark." The paper addresses the issue of model theft in embedding-as-a-service scenarios, where attackers may infer and replicate models by learning from their outputs. To protect the copyright of these services, the authors propose Embedding Marker, a backdoor-based watermarking method specifically designed for embedding services. The method involves two key steps: watermark injection and copyright verification. During injection, a target embedding is subtly combined with the original embedding based on the number of trigger words in the input, ensuring the watermark remains covert and does not degrade utility. For verification, the method compares embeddings from the suspect service against the target embedding using cosine similarity, L2 similarity, and statistical tests like the KS test. Experiments on datasets such as AG News, MIND, SST2, and Enron Spam demonstrate that Embedding Marker achieves high detection accuracy while maintaining strong performance on downstream tasks. Visual analysis also confirms the watermark's effectiveness and covert nature, as backdoor and normal embeddings are visually indistinguishable. This work provides a novel and practical approach to securing embedding-as-a-service models against unauthorized replication.</sample>
    <sample id="95">The first author of PaLM is not mentioned in the provided content.</sample>
    <sample id="97">The speaker mentions three problems of SimulST:

1. Specific architectures are usually trained, introducing additional modules to be optimized.
2. Long and complicated training procedures, involving different optimization objectives.
3. Training and maintaining several models to reach different latency regimes.</sample>
    <sample id="98">An effective way to mitigate social and political biases in datasets when training NLP models is to carefully curate and balance the pretraining data, ensuring diverse and representative sources across political spectra, while also implementing controlled experiments and post-training evaluations to detect and adjust for emerging biases in model outputs.</sample>
    <sample id="100">PromptRank is a data-efficient approach for multi-hop question answering (QA), which requires answering questions by reasoning across multiple documents. Traditional multi-hop retrievers need thousands of training examples, making them costly and less effective in low-resource domains. PromptRank addresses this by combining an unsupervised retrieval method with a few-shot language model-based reranker, achieving strong performance with as few as 128 training examples. The method involves two main steps: first, retrieving candidate document chains using TF-IDF and hyperlinks, and second, reranking these chains using a language model that scores the likelihood of the question given the chain. This scoring function is more effective than the reverse and is enhanced by carefully designed instructions that prompt the model to reason over the chain documents. The instructions are optimized through techniques like instruction search and sampling. PromptRank is evaluated on the HotpotQA dataset using metrics like R@K and AR@K, and it performs comparably to state-of-the-art dense retrievers while outperforming fully supervised systems like DrKit. When combined with a downstream reader model (ELECTRA-Large), PromptRank achieves strong multi-hop QA performance, slightly underperforming MDR by about four exact match points. Overall, PromptRank demonstrates the effectiveness of using language models for few-shot retrieval in multi-hop QA, highlighting the importance of instruction design and scoring functions in eliciting reasoning capabilities.</sample>
    <sample id="101">The fluency of PaLM is comparable to state-of-the-art systems, as indicated by human evaluations using the MQM framework. However, while PaLM produces fluent output, it tends to have accuracy issues, often omitting parts of the source sentence in favor of a more natural-sounding translation.</sample>
    <sample id="102">The important properties of a watermarking method, as discussed in the presentation, are:

1. **Applicability**: The method should be suitable for embedding as a service.
2. **Utility Preservation**: The watermark should not degrade the quality or utility of the provided embeddings.
3. **Covertness**: The watermark should be difficult for attackers to detect or remove easily.
4. **Transferability**: The watermark should remain detectable even after the model is extracted and used in an attacker's service.</sample>
    <sample id="103">The English TED talks have been translated into 14 different languages, but the specific languages are not listed in the provided content.</sample>
    <sample id="104">The presentation does not specify the exact number of instances sampled from one dataset for reannotating.</sample>
    <sample id="105">The distance metrics used for measuring the difference between benign and backdoor datasets are cosine similarity, L2 similarity, and the KS test p-value.</sample>
    <sample id="106">The audio presents the QUEST dataset, developed by researchers from Google DeepMind, to address information retrieval challenges involving implicit set constraints in queries. Using examples like Jane, a zoologist seeking a reptile species, and Austin, a reader looking for historical fiction novels set in France, the talk highlights how users often express complex, multi-constraint information needs. QUEST includes over 3,000 entity-seeking queries with implicit set operations, verified answers, and annotated documents showing where each query constraint is satisfied. The dataset is constructed using Wikipedia categories across four domains, with human-generated paraphrased queries and validated annotations. Evaluation shows that current retrieval systems struggle with these queries, particularly those involving set intersections and differences, as evidenced by low F1 scores and recall metrics. The dataset aims to help future research improve systems for handling selective information needs, offering a benchmark for evaluating retrieval models that can handle complex, multi-constraint queries. The researchers invite the audience to explore their paper and presentation at ACL.</sample>
    <sample id="107">The multilingual encoder-based models, such as Encoder-PTR (e.g., XLM-R + PTR and mBERT + PTR) and Encoder-Decoder models (e.g., mBART and mT5), were used to perform cross-lingual semantic parsing by training on multiple languages either in a monolingual, multilingual, or few-shot setting. These models were evaluated for their ability to translate queries in various natural languages into different meaning representations like SQL and Lambda Calculus. The results showed that Encoder-Decoder models achieved the best performance overall, and training with a mixture of languages improved performance, except for English in some cases.</sample>
    <sample id="108">The talk by Koustav Sinha presents research on evaluating language models' acceptability judgments using the Minimal Pair Paradigm (MPP) with longer contexts. Traditional MPP assessments focus on short, isolated sentences, but modern large language models can process longer contexts. The study revisits MPP by extending sentence lengths and testing how context influences acceptability judgments. Researchers recreated sentence pairs from datasets like BLiMP and SyntaxGym, adding prefixes from the same or different datasets, or even unrelated sources like Wikipedia. Results showed that when prefixes matched the syntactic structure of the target sentence, MPP judgments changed significantly, indicating that models are influenced by shared syntactic and semantic features in the context. In contrast, irrelevant contexts had minimal impact. The findings suggest that current MPP evaluations, which use short sentences, may not fully capture how models process language across longer contexts. The work highlights the importance of re-evaluating language models with extended contexts to better understand their linguistic knowledge and robustness.</sample>
    <sample id="109">**Abstract:**  
This paper introduces *Unnatural Instructions*, a large-scale dataset of natural language instructions and corresponding input-output pairs, generated entirely without human labor. Traditional instruction tuning relies on either reformulating existing benchmarks or collecting and annotating user-generated prompts, both of which are labor-intensive or limited in scope. In contrast, *Unnatural Instructions* leverages a pre-trained language model (e.g., GPT-3) to automatically generate diverse instruction examples by prompting it with a small set of manually crafted seed examples. The process involves generating instructions, their inputs, and outputs in two steps, followed by creating paraphrased versions of the instructions to enhance diversity. The final dataset contains 64,000 instruction examples and over 240,000 variants when including paraphrases. Analysis shows that over 50% of the generated examples are correct, with even incorrect examples providing useful signals for instruction tuning. The dataset includes highly creative and varied tasks, such as verifying scientific experiments or inventing new words. Fine-tuning an 11-billion-parameter T5 model on *Unnatural Instructions* yields superior performance compared to models trained on existing datasets like Super-Natural Instructions, T0++, and Tk-instruct across multiple benchmarks. The work demonstrates that language models can efficiently generate high-quality, diverse instruction data at scale, offering a cost-effective and scalable alternative to human annotation.</sample>
    <sample id="111">The authors decide what moderate-frequency words are by collecting a general text corpus and counting the word frequency within it. They then select words that fall within a moderate frequency interval as the trigger set.</sample>
    <sample id="114">The paper "Finding the Pillars of Strength for Multi-Head Attention" presented at ACL 2023 by researchers from Nanyang Technological University of Singapore addresses the issue of parameter redundancy in large language models, particularly focusing on the multi-head attention mechanism. While large language models have revolutionized NLP by handling multiple tasks in one model, they suffer from high parameter counts, long training times, and large data requirements. The authors propose a novel approach called grouped head attention, which uses a two-stage strategy to compress the multi-head attention layer. The first stage involves group-constrained training, which clusters attention heads into groups, making intra-group heads more similar and inter-group heads more distinct. The second stage, Voting-to-Stay, prunes redundant heads, retaining only one per group. This leads to significant parameter compression without sacrificing performance. The models, GHT and GHT-PS, demonstrate improvements in BLEU scores for machine translation, ROUGE scores for summarization, and performance on language modeling tasks, along with substantial compression rates. The work also highlights the potential for task-specific pruning, inspired by the Lottery Ticket Hypothesis, suggesting that only necessary parameters should be retained for specific applications, much like removing unused apps on a smartphone. This approach offers a promising path toward more efficient and deployable large language models.</sample>
    <sample id="115">The approach does not specify a fixed speech segment size. Instead, it uses a parameter called lambda, which refers to the number of speech frames, to determine when to emit a partial translation based on the attention mechanism. The actual segment size can vary depending on the speech input and the model's attention distribution.</sample>
    <sample id="116">The entity-specific knowledge needed in the example with Servin and Kea is "Servin is a judge."</sample>
    <sample id="117">The most important factor is the example quality.</sample>
    <sample id="118">The presentation introduces "SwitchMLM," a novel pretraining technique designed to improve performance on code-switched NLP tasks, such as sentiment analysis and question answering. Code-switching involves mixing words from multiple languages, common in multilingual communities like India. Traditional multilingual models like mBERT and XLM-R perform poorly on such tasks. To address this, the authors propose two key innovations: SwitchMLM and FrequencyMLM. SwitchMLM focuses on masking only switch-points—pairs of tokens that mark language transitions—rather than all tokens as in standard MLM. However, this requires language identification (LID) tags, which are not always available. To overcome this, FrequencyMLM uses monolingual corpora to estimate LID tags based on word likelihoods. Additionally, the authors suggest architectural improvements, such as residual connections from intermediate layers with higher switch-point information to the final layer, and an auxiliary LID-based loss to enhance language encoding. Probing experiments using linear and conditional probing confirm that these methods increase switch-point information in model representations. Results show that combining SwitchMLM/FrequencyMLM with residual connections and auxiliary loss yields the best performance on sentiment analysis across multiple language pairs. Overall, the work advances pretraining techniques tailored for code-switched NLP, improving model understanding of mixed-language inputs.</sample>
    <sample id="119">The paper focuses on RoBERTa and GPT series models in the extended experiments.</sample>
    <sample id="120">The model uses attention scores from the cross-attention mechanism, typically from a specific layer in the encoder-decoder architecture, rather than combining scores from multiple layers. This allows the model to make decisions on emitting partial translations based on the stability of attention over the most recent speech frames.</sample>
    <sample id="121">Examples of direct inference include using the exact name of an entity, such as saying "Easy on Me," or referring to the entity by its position, like "the first one."</sample>
    <sample id="122">The author of the paper is affiliated with Fudan University.</sample>
    <sample id="123">Ying and Zhiyang present their research on **MultiInstruct**, a novel multi-modal instruction tuning benchmark designed to enhance zero-shot learning in multi-modal tasks. While instruction tuning has been widely explored in natural language processing, its application to multi-modal settings has been limited due to the lack of large-scale multi-modal instruction datasets. To address this, the researchers created **MultiInstruct**, the first multi-modal instruction tuning dataset containing 62 diverse tasks across 10 categories, derived from 21 open-source datasets, each with five expert-written instructions. The study uses **OFA**, a unified multi-modal pre-trained model, as the base model and reformulates all tasks into a unified sequence-to-sequence format. The training involves 53 tasks with 10,000 instances each, while testing includes unseen tasks from various groups. Performance is evaluated using metrics like accuracy, Rouge-L, and a newly introduced **sensitivity** metric, which measures the model's consistency in outputs under varying instruction phrasings. Results show that instruction tuning significantly improves OFA’s performance on seen and unseen tasks, and transfer learning from natural instruction datasets further enhances performance and reduces sensitivity. The researchers also plan to expand the dataset with 150 additional vision-language tasks. Overall, the work advances multi-modal zero-shot learning through instruction tuning and introduces valuable new evaluation metrics.</sample>
    <sample id="124">Tan Qingyu from NUS and Alibaba presents a study on benchmarking and improving the temporal reasoning capabilities of large language models (LLMs). The research categorizes temporal reasoning into three levels: time-to-time, time-to-event, and event-to-event. Prior work has focused mainly on the second level, but the study aims for a more comprehensive analysis. The team introduces the TempReason dataset, covering all three reasoning levels with extensive time coverage. They evaluate LLMs in three QA settings: Closed Book, Open Book, and a new Reasoning QA setup. To enhance temporal reasoning, they propose a training strategy involving temporal span extraction pre-training and time-sensitive reinforcement learning. Their model, TempT5, outperforms existing models like FLAN-T5-L and ChatGPT on the TempReason benchmark, especially in Open Book and Reasoning QA tasks. However, ChatGPT shows significant performance drops, especially in month prediction and across different time periods, revealing temporal reasoning biases. The study highlights the need for more balanced training data and better reasoning strategies. Overall, the work contributes a new benchmark, exposes LLM limitations in temporal reasoning, and proposes effective training methods to enhance this capability.</sample>
    <sample id="125">The information provided does not specify the number of authors involved in the paper.</sample>
    <sample id="126">Yes, translating the natural language query using a machine translation model (such as Google Translate API) before semantic parsing was considered as a baseline in the "Translate-Test" setting.</sample>
    <sample id="127">Namgyu Ho, a master's student at KAIST AI, introduces the paper "Large Language Models Are Reasoning Teachers," a collaborative work with Laura Schmid and Se-Young Yun. The paper addresses the challenge that chain-of-thought (CoT) prompting, a technique enabling large language models to solve complex tasks, is only effective for very large models like GPT-3 due to their high computational demands. To overcome this, the authors propose using these large models as "reasoning teachers" to train smaller models. They generate step-by-step reasoning solutions using CoT prompting and use them as training data to fine-tune smaller models. The team introduces a novel technique called "Diverse Reasoning," which involves generating multiple reasoning paths from the teacher model using stochastic sampling, thereby improving the training quality for the student models. Their experiments show that this method significantly enhances the performance of small models on complex reasoning tasks, with notable improvements on benchmarks like Multi Arith. The approach is scalable, and the trade-offs between development and inference costs are discussed. The paper provides detailed analysis, code, and data, encouraging further research and application of this distillation method. The work demonstrates that reasoning abilities from large models can be effectively transferred to smaller, more deployable models.</sample>
    <sample id="128">In their work "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources," Akshatha and Martin present a diagnostic test suite designed to assess how natural language understanding models integrate knowledge from different sources—pretraining data and inference-time inputs. They focus on coreference resolution, a task requiring the model to link pronouns to their referents using both entity-specific and background knowledge. The KITMUS test introduces three settings: "Background-Pretrain," where background knowledge is available during pretraining; "Background-Both," where both types of knowledge are available during inference; and "Background-Inference," where only inference-time knowledge is provided. The latter is particularly challenging, as it simulates scenarios where background knowledge is not present in the model's pretraining data. The study evaluates models and human participants, showing that without task-specific training, models struggle to integrate knowledge from multiple sources. However, with such training, some models like C2F and BERT4Coref improve significantly. Nevertheless, even the best models face challenges in reliably using inference-time-only background knowledge. The work highlights the importance of training models to effectively combine knowledge from different sources for better performance in knowledge-intensive NLU tasks.</sample>
    <sample id="129">The authors gave the example of a "woman warrior" as a marked group, noting that the term is usually specified as "woman warrior" to mark the gender difference from the default, unmarked group (men).</sample>
    <sample id="130">The English content does not explicitly mention which model architectures do not generalize well. However, it states that transformer models generally generalize better to new data, implying that non-transformer architectures (such as traditional RNNs or CRFs) may not generalize as well.</sample>
    <sample id="131">The names of the testing datasets are not mentioned in the provided content.</sample>
    <sample id="132">The paper involves two authors: Akshatha and Martin.</sample>
    <sample id="133">The author works with multiple modalities, including text, images, and bounding boxes, as the research focuses on multi-modal instruction tuning.</sample>
    <sample id="135">ABC-Eval is a new method for evaluating conversational AI models introduced by the Emory NLP Lab in collaboration with Amazon Alexa AI. Unlike traditional evaluation methods that rely on human judges to rate overall conversation quality using Likert scales or pairwise comparisons, ABC-Eval focuses on identifying specific behaviors that affect chat quality. It measures how often chat models commit thematic errors, such as being irrelevant, contradicting themselves or their partner, hallucinating facts, violating common sense, or failing to show empathy. The approach uses explicit annotations to label these behaviors, reducing subjectivity in evaluation.

The researchers tested ABC-Eval on four state-of-the-art chat models across 100 human-bot conversations each. Results showed that ABC-Eval labels are more reliable and predictive of conversation quality compared to traditional methods, as demonstrated by higher inter-annotator agreement and better performance in linear regression analyses. ABC-Eval also captures unique aspects of chat quality, explaining over 25% of conversation quality when combined, compared to much less from traditional metrics.

The study highlights that current models still have significant issues, such as common sense violations and irrelevant responses, but ABC-Eval provides a more precise and informative way to assess and compare conversational AI systems, offering a valuable tool for future research and development in the field.</sample>
    <sample id="136">In this presentation, Jasivan introduces FERMAT, an alternative evaluation framework for numerical reasoning in language models, developed in collaboration with Nafise at the University of Sheffield. The motivation stems from the limitations of current benchmarks, which primarily rely on accuracy scores and fail to capture the nuanced strengths and weaknesses of models in mathematical reasoning. FERMAT aims to address this by providing a flexible, arithmetic-based evaluation set that considers number understanding, mathematical operations, and training dependency. The framework includes math word problems adapted from CommonCore and Illinois, with variations in number representations such as decimals and large integers to test model performance across different scenarios.

The study reveals that even large language models perform poorly on numerical reasoning tasks, especially when tested with real-world variations. Fine-tuning with diverse templates and examples improves performance, highlighting the importance of training data diversity. The research also investigates how training data influences model accuracy, showing that linguistic cues and mathematical diversity significantly impact results. By incorporating diverse datasets like GSM8K and AQUA, FERMAT demonstrates promising improvements in model performance. Ultimately, FERMAT provides a more informative and representative evaluation method for assessing numerical reasoning in language models, emphasizing the need for better benchmarking tools in this domain.</sample>
    <sample id="137">In their ACL 2023 paper, Sicong from the Singapore University of Technology and Design introduces "Tell2Design," a novel dataset and task for language-guided floor plan generation. Unlike traditional text-to-image models that focus on artistic generation, this work targets structured design tasks where floor plans must adhere to specific semantic, geometric, and topological constraints derived from natural language instructions. The Tell2Design dataset includes 5,051 human-annotated and 76,000 artificially generated instructions paired with real-world floor plans, offering rich, multi-sentence descriptions for each design. The key challenges include strict design constraints, interpreting unstructured text, and handling ambiguous or incomplete user input. To address these, the authors propose a sequence-to-sequence model under the encoder-decoder framework, using a pre-trained T5 model to translate instructions into structured bounding box sequences representing room layouts. The model outperforms text-conditional image generation baselines on the T2D dataset, achieving a Micro IoU of 54 and a Macro IoU of 53. The study also highlights the importance of bridging the language gap between artificial and human instructions, showing that combining both types of data improves performance. This work lays the foundation for future research in language-guided design generation, aiming to democratize the design process by enabling users to create floor plans through natural language instructions.</sample>
    <sample id="138">The authors claim that the integration of knowledge from multiple sources, specifically combining pretrain-time knowledge with inference-time knowledge, is an understudied area in natural language understanding (NLU).</sample>
    <sample id="139">The names of the speakers are Ying and Zhiyang.</sample>
    <sample id="140">Yes, CoScript underwent quality checks through crowd-sourced workers who identified and revised incorrect samples to ensure the quality of the validation and test sets.</sample>
    <sample id="141">The limits of existing resources for context-dependent translation include their support for only limited types of context-dependent translations and limited sets of languages. These resources often rely on domain knowledge and human curation, making them less scalable and comprehensive.</sample>
    <sample id="143">The approach is compared to the Wait-k strategy and the Local Agreement, which are also applied to offline models, as well as to the state-of-the-art architecture specifically tailored for simultaneous pre-translation.</sample>
    <sample id="144">The affiliations of the authors of the paper are not mentioned in the provided text.</sample>
    <sample id="145">The name of the speaker is Jenny.</sample>
    <sample id="146">Yicheng, a PhD student from Fudan University, presents a paper on the analysis of omission in dialogue summarization. Dialogue summarization, a subtask of text summarization, aims to extract key information from dialogues. Despite advances using pre-trained language models, generated summaries often suffer from factual errors, with omission being a major issue that leads to incomplete summaries. Yicheng's study reveals that up to 70% of generated summaries from state-of-the-art models contain omissions, and omitted information is randomly distributed across dialogues, making it difficult for models to identify key content. To address this, the researchers introduce the OLDS dataset, which provides high-quality omission labels for dialogue summarization. They construct the dataset using five existing benchmarks and generate multiple candidate summaries per dialogue. An automatic labeling method, validated through human evaluation, is used to annotate omissions. The study explores three baseline models for omission detection and finds that the task is highly challenging due to label imbalance. However, using detected omissions to refine summaries through post-editing significantly improves summary quality, highlighting the potential of omission detection as a valuable step in enhancing dialogue summarization. The dataset and findings offer a foundation for future research in this area.</sample>
    <sample id="147">The paper involves three authors: Myra, Esin Durmus, and Dan Jurafsky.</sample>
    <sample id="149">Yes, the dataset is publicly available.</sample>
    <sample id="150">In their ACL paper titled "MEETINGQA: Extractive Question-Answering on Meeting Transcripts," Archiki and collaborators introduce MeetingQA, a novel dataset for extractive question answering based on real-life meeting transcripts. The dataset addresses the underutilized QA aspect of meetings, where participants ask open-ended questions that elicit detailed responses and discussions. MeetingQA includes 7,700 questions, with 30% being unanswerable, and features complex answer structures such as multi-span answers and multiple speakers. The dataset was collected from the AMI corpus and annotated with high inter-annotator agreement (Krippendorff’s alpha of 0.73). It includes diverse question types, such as yes/no and rhetorical questions, and highlights challenges like identifying answer spans and speaker roles. The paper explores various models, including short- and long-context approaches, single- and multi-span extractors, and data augmentation techniques. Results show a significant gap between model performance and human performance, especially in zero-shot settings, with models struggling with rhetorical questions and speaker attribution. Overall, MeetingQA presents a challenging and rich resource for QA research, emphasizing the need for better models that can handle the complexity of real-world meeting discussions.</sample>
    <sample id="152">Frederick Riemenschneider presents research on the application of large language models to classical philology, focusing on Ancient Greek and Latin. While existing models like Latin BERT and Ancient Greek BERT have been developed, they are monolingual and based on BERT, limiting their utility for multilingual tasks. The team introduces new models—GreBERTa and GreTa for Ancient Greek, and PhilBERTa and PhilTa for multilingual use—to address these gaps. These models are pre-trained on diverse datasets, including newly curated resources from the Internet Archive, overcoming OCR limitations for Greek texts. The models are evaluated on tasks like part-of-speech tagging, dependency parsing, and lemmatization, where they significantly outperform existing state-of-the-art models. The encoder-decoder architecture, especially GreTa, excels in lemmatization, improving performance by 5 percentage points for Ancient Greek. The study also explores the behavior of T5 encoders and investigates whether multilingual models offer advantages in semantic and world knowledge. While multilingual models perform similarly to monolingual ones, they provide a unified approach for processing Latin and Greek. The research contributes high-quality pre-training data and rigorously benchmarks models, offering valuable tools for classical philology. For more details, the audience is directed to the full paper.</sample>
    <sample id="153">In this presentation, Ninareh Mehrabi from Amazon Alexa AI's Responsible AI team discusses the work titled "Resolving Ambiguities in Text-to-Image Generative Models." The research addresses the challenge of ambiguous prompts in text-to-image models, which hinder the generation of images that accurately reflect user intent. The team introduces a benchmark dataset, an extended version of the LAVA corpus, covering various types of ambiguities. They propose two frameworks for disambiguating prompts: one that generates clarifying questions for users to resolve ambiguity, and another that generates multiple visual interpretations for user selection. After disambiguation, the refined prompts are input into text-to-image models to generate images, which are then evaluated for faithfulness to the user’s intention. The evaluation framework uses a VQA model to assess whether the generated images align with the user's stated intention. The study reveals that resolving ambiguities improves image generation fidelity and that the proposed automatic evaluation framework aligns well with human evaluations. The work contributes to improving the reliability and accuracy of text-to-image systems by addressing prompt ambiguities and providing tools for their evaluation. This research highlights the importance of understanding and mitigating ambiguity in natural language prompts to enhance the performance of AI-driven image generation.</sample>
    <sample id="154">The authors of the paper are affiliated with the University of Trento and the Foundazione Bruno Kessler.</sample>
    <sample id="155">The name of the speaker is Javad Hosseini.</sample>
    <sample id="157">Shen Gao from Shandong University introduces the work "Dialogue Summarization with Static-Dynamic Structure Fusion Graph" (SDDS), a joint effort with several collaborators. Dialogue summarization aims to extract key information from multi-participant conversations into concise summaries. Existing methods rely on pre-computed static graphs using external linguistic tools, but these approaches face two issues: dependence on unreliable tools and the inability of static graphs to adapt dynamically to the summarization task. The proposed SDDS model addresses these challenges with four main components: an Utterance Encoder, Static-Dynamic Graph module, and a pre-trained language model as the Summary Generator. The Static-Dynamic Graph module combines pre-built static graphs (like discourse parsing and speaker interaction graphs) with a dynamic graph capturing semantic relationships through multi-head attention. The model integrates static and dynamic structures using a fusion method and a dual cross-attention mechanism to enhance the summarization process. The approach captures both structural and semantic relationships between utterances, leading to more accurate summaries. The code and data are publicly available on GitHub.</sample>
    <sample id="158">In this presentation, Qipeng Guo from AWS introduces "Dual Cache for Long Document Neural Coreference Resolution," a novel approach to improving coreference resolution in long texts. Coreference resolution involves identifying and grouping mentions of the same entity across a document. Traditional methods suffer from quadratic complexity due to pairwise comparisons, while recent cache-based methods reduce this to linear complexity. However, these methods often struggle with long documents where topics change frequently, leading to high cache misses. The proposed dual cache system addresses this by using two caches: a local cache with LRU (Least Recently Used) eviction policy for local entities and a global cache with LFU (Least Frequently Used) policy for global entities. This dual approach allows the model to better manage both short-term and long-term entity mentions, reducing cache misses and improving efficiency. The method was evaluated on multiple benchmarks, showing superior performance compared to baseline models, especially on long documents. It also demonstrated significant improvements in cache efficiency and a favorable performance-to-cost ratio. The results confirm that the dual cache system is more effective and efficient than single-cache approaches, making it a promising solution for coreference resolution in large-scale, real-world texts.</sample>
    <sample id="160">The first step of the method maps input tokens to unordered multiset tokens that will appear in the output.</sample>
    <sample id="161">CoScript represents 55,000 specific goals with scripts.</sample>
    <sample id="163">The best alignment method for DEPLAIN is MASSalign.</sample>
    <sample id="164">The benefit of weakly supervised learning is that it allows training models using weak labeling sources, such as heuristic rules or low-quality crowdsourcing, which are much cheaper than manual annotations, even though the labels are noisy. This makes the learning process more cost-effective compared to fully supervised learning.</sample>
    <sample id="165">Wenting Zhao, a PhD student at Cornell University, presents a paper titled *"Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations."* The work addresses the challenge of abductive reasoning, which involves identifying plausible explanations that connect a given context to an outcome. Using the example of Emily being stuck in traffic but still making her flight, the paper highlights how explanations like "Her flight was delayed" bridge the gap between context and outcome. Traditional approaches rely on supervised learning, which requires annotated explanations—often subjective and noisy. Zhao introduces LiPoR, an unsupervised method that learns abductive reasoning without such annotations. LiPoR treats explanations as latent variables and maximizes the marginal likelihood of outcomes given contexts, while also incorporating a regularizer that enforces mutual exclusivity among explanations. This ensures that only a subset of explanations is favored, reflecting real-world reasoning where explanations are typically mutually exclusive. Evaluated on the AlphaNLI dataset, LiPoR outperforms existing zero-shot and unsupervised models by over 4 absolute points in accuracy. The method demonstrates that abductive reasoning can be effectively learned without explicit supervision, opening new avenues for commonsense reasoning research.</sample>
    <sample id="166">Yunxin from Harbin Institute of Technology, Shenzhen, introduces a new image retrieval framework called NDCR, designed to handle linguistically complex text. Traditional visual language models struggle with such tasks due to high image similarity and long descriptions. Inspired by the Divide-and-Conquer strategy and Dual-Process Theory, the NDCR framework combines analogical reasoning (System 1) and logical reasoning (System 2). The framework includes a Proposition Generator to break down complex text into simple propositions, a Visual-Linguistic Interactor for matching propositions with images, and a Neural-Symbolic Reasoner to integrate reasoning states and results. This system enables logical operations like negation and conjunction, improving retrieval accuracy. Experimental results show that NDCR outperforms existing methods, and ablation studies confirm the effectiveness of each component. Case studies demonstrate the model's ability to process and explain intermediate reasoning steps, showcasing its interoperability. The approach highlights the potential of neural-symbolic reasoning in enhancing compositional reasoning and planning in large language models, suggesting that combining Divide-and-Conquer with Dual-Process Theory can be a powerful strategy for complex reasoning tasks.</sample>
    <sample id="167">The documents in DEPLAIN-web were aligned using both manual and automatic alignment methods. Specifically, 750 documents were aligned, with some alignments done manually and others using automatic alignment techniques, resulting in a total of 30,450 parallel sentence pairs.</sample>
    <sample id="168">The CoNLL++ dataset was created by collecting data from Reuters News from 2020 and annotating it using the same CoNLL-2003 annotation guidelines.</sample>
    <sample id="169">David Vilar presents a review of the paper "Prompting PaLM for Translation: Assessing Strategies and Performance," a collaborative work with Google Translate researchers. The study evaluates the translation capabilities of PaLM, a 540-billion-parameter language model, using best practices from the machine translation (MT) community. The research systematically examines prompting strategies for large language models (LLMs) in translation tasks, comparing PaLM’s performance with state-of-the-art MT systems using WMT benchmarks and neural metrics like BLEURT, as well as human evaluations via the MQM framework.

The results show that prompting significantly affects translation performance, with a 5-shot prompting strategy yielding the best results. Marking examples with their source and target languages improves model performance. However, the quality of the examples is more critical than their similarity to the source sentence. Using high-quality examples from curated data, such as the WMT dev set, leads to better performance than using noisy training data.

While PaLM’s translations are fluent and comparable to commercial systems like Google Translate, they suffer from accuracy issues, particularly omission errors. PaLM tends to produce more natural-sounding translations by omitting parts of the source text. Overall, specialized MT systems still outperform PaLM, but the model shows strong potential in translation tasks. The study highlights the importance of prompt selection and example quality in leveraging LLMs for translation.</sample>
    <sample id="171">The existing works on protecting the copyright of embedding-as-a-service can be broadly classified into four categories. However, these methods either are not applicable to embedding-as-a-service or lack transferability during model extraction. This limitation motivates the proposed Embedding Marker method in the paper.</sample>
    <sample id="172">No, multilingual LLMs such as Codex or Bloom are not sufficient for Cross-Lingual Semantic Parsing (CLSP), as found in the study. They are still inadequate for this specific task.</sample>
    <sample id="174">The paper introduces ArgAnalysis35K, a large-scale dataset designed for argument quality analysis, offering several unique features that distinguish it from existing datasets. Unlike many current datasets, which are often sourced from crowdsourcing platforms and limited in scope, ArgAnalysis35K contains 35,000 high-quality argument-analysis pairs, with 85% of arguments sourced from expert and intermediate debaters, ensuring a higher standard of argument quality. The dataset includes a diverse range of 24 themes, expanding beyond the typical 30–40 pre-selected motions, thereby reflecting the variety of topics encountered in parliamentary debates. A key innovation is the inclusion of "analysis," a new concept in NLP, which combines claims and premises to provide deeper explanations of arguments. Additionally, the dataset employs instance-based annotator reliability, allowing for more accurate and fair annotations by considering annotator biases on a per-argument basis. It also introduces a relevance model that scores how relevant each argument is to specific themes, enabling broader applicability of arguments across various topics. Overall, ArgAnalysis35K offers a more diverse, high-quality, and reliable resource for argument quality analysis, with enhanced relevance modeling and a novel analytical framework. Researchers are encouraged to explore the full paper and provide feedback for further improvements.</sample>
    <sample id="175">The method deals with permutation ambiguity by inducing the alignment between input and output during training and using a flexible permutation prediction model. Instead of enforcing strict constraints, it learns to select the most linguistically plausible permutation through a continuous relaxation approach that approximates the NP-hard permutation problem. This allows the model to explore multiple valid permutations and backpropagate errors to learn the correct one.</sample>
    <sample id="176">The fairness of a downstream NLP model is defined by its ability to perform consistently and justly across different demographic or political groups, without favoring or disadvantaging any particular group. In the context of political biases, it involves ensuring that the model does not disproportionately detect or misdetect hate speech or misinformation based on the social category or political leaning of the target, thus avoiding marginalization of certain groups.</sample>
    <sample id="177">The name of the speaker is Yanis Labrak.</sample>
    <sample id="178">The name of the speaker is Koustav Sinha.</sample>
    <sample id="179">In her talk, Melanie Sclar introduces **SymbolicToM**, a plug-and-play method to enhance the **Theory of Mind (ToM)** reasoning capabilities of large language models (LLMs). ToM refers to the ability to understand and reason about others' mental states, often tested using false-belief scenarios like the Sally-Anne test. Despite their capabilities, LLMs like GPT-3 and ChatGPT struggle with these tasks. SymbolicToM addresses this by using **explicit graphical representations** to model characters’ beliefs and their beliefs about others’ beliefs (e.g., BBob and BBob,Alice). These graphs are computed during inference using existing NLI and OpenIE models, enabling the model to answer ToM questions by translating them into factual queries over the graphs.

The method was tested on various LLMs, showing significant performance improvements—up to 67 accuracy points for some models—compared to both baseline models and supervised approaches. SymbolicToM also demonstrated robustness in out-of-domain scenarios, including modified story structures and linguistic diversity. It outperformed supervised models in generalization tasks and enabled even strong models like GPT-4 to fully solve new datasets. Overall, SymbolicToM provides a scalable, interpretable, and effective way to improve LLMs’ understanding of mental states, enhancing their ability to reason about others' beliefs and intentions.</sample>
    <sample id="180">The name of the speaker is Myra.</sample>
    <sample id="181">This paper introduces the problem of *constrained language planning*, where the goal is to generate step-by-step scripts that adhere to specific constraints, such as "make a chocolate cake" instead of the general "make a cake." While previous work has shown that large language models (LLMs) can effectively plan for abstract goals, their performance on constrained goals remains limited. The study evaluates LLMs on constrained planning tasks and finds that they often fail to maintain faithfulness to constraints, despite acceptable semantic completeness. To address this, the authors propose an *over-generate-then-filter* approach, where multiple scripts are generated and filtered based on semantic similarity and keyword matching. This method significantly improves the quality of generated scripts in terms of both semantic completeness and constraint adherence. Furthermore, the paper introduces *CoScript*, a large-scale dataset of constrained language planning tasks, containing 55,000 specific goals with corresponding scripts. The dataset is validated by human annotators and demonstrates high diversity in constraint types. Experiments show that models fine-tuned on CoScript, such as T5, can outperform many LLMs in constrained planning tasks. The work establishes a foundation for future research on constrained language planning and provides a valuable resource through the CoScript dataset.</sample>
    <sample id="182">In the context of this paper, tropicalism indicates a stereotype or trope that associates Latina women with characteristics like "vibrant" and "curvaceous," which are linked to imagery and cultural associations typically connected with tropical regions. This contributes to essentializing and reductive narratives about women of color.</sample>
    <sample id="183">The authors created the human-written portrayals of target groups by giving similar prompts to human subjects, inspired by a study where participants were asked to imagine themselves as members of specific demographic groups and describe themselves. This method enabled a direct comparison between the generated personas from the language models and the human-written responses.</sample>
    <sample id="184">In this work, Pointwise CXMI was used to measure context usage, allowing for analysis at both the sentence level and the word level.</sample>
    <sample id="185">The difference between DrBERT and ChuBERT lies in their training data sources. DrBERT is trained on NACHOS, a dataset of medical crawled data from the web, while ChuBERT is trained on anonymized clinical data obtained from the Nantes University Hospital data warehouse. DrBERT is designed for a broader range of biomedical usage, whereas ChuBERT is specifically focused on clinical data.</sample>
    <sample id="187">Two authors are involved in the paper: Ying and Zhiyang.</sample>
    <sample id="188">Iterative transfer learning is a process where a model is first fine-tuned on a related task (e.g., CE or debate classification), and then further fine-tuned on another related task, improving its performance incrementally and enabling better cold-start performance in active learning for rare class detection like dissonance.</sample>
    <sample id="189">The goal of the AltEntities Corpus dataset is to understand how users use indirect referring expressions to select entities in conversational contexts, providing a benchmark for evaluating language models' ability to resolve such references across different domains.</sample>
    <sample id="190">An attacker can extract model parameters through Embedding as a Service (EaaS) by querying the service with carefully crafted inputs and analyzing the output embeddings. By collecting a sufficient number of embedding outputs corresponding to different input texts, the attacker can reverse-engineer or approximate the model's parameters using techniques such as gradient-based methods or model inversion attacks. This allows the attacker to steal the model's knowledge and potentially deploy similar services without access to the original model.</sample>
    <sample id="191">The paper involves three authors: Sara Papi, Matteo Negri, and Marco Turchi.</sample>
    <sample id="192">Yang Luo introduces CAME, a confidence-guided adaptive memory-efficient optimizer designed to address the memory and performance trade-off in training large language models. Traditional adaptive optimizers like Adam require significant memory due to storing first and second moment estimates, while memory-efficient alternatives like Adafactor suffer from performance degradation. CAME aims to combine the fast convergence of adaptive methods with the low memory usage of efficient ones.

The approach is inspired by non-negative matrix factorization (NMF) and the instability in Adafactor's updates. CAME identifies and mitigates erroneous updates by computing an instability matrix from the residual between predicted and actual updates. This instability is used as a denominator in the optimization step, leading to more stable and adaptive updates.

Experiments on BERT, GPT-2, and T5 show that CAME outperforms Adam and Adafactor in terms of validation accuracy and memory efficiency, particularly with large batch sizes. It achieves a 3.4% increase in validation accuracy over Adafactor with the same number of training steps and reduces memory usage compared to other optimizers like Adam and LAMB. CAME also performs well on downstream tasks, demonstrating its effectiveness in training large language models with reduced memory overhead. Overall, CAME offers a promising solution for efficient and effective training of large models.</sample>
    <sample id="193">The provided information does not specify the number of annotators used to create the initial dataset.</sample>
    <sample id="194">The authors of the paper are affiliated with Carnegie Mellon University, the University of Washington, and the Allen Institute for AI.</sample>
    <sample id="195">The paper introduces RoHT, a novel framework for Explainable Question Answering (XQA) that addresses the limitations of existing neuro-symbolic and decomposition-based methods. RoHT employs a Hierarchical Question Decomposition Tree (HQDT) to represent complex questions as structured trees, where each node represents a sub-question, and leaf nodes are atomic questions. The framework performs probabilistic reasoning over the HQDT, integrating knowledge from both structured knowledge bases (KBs) and unstructured text corpora. It uses a two-stage approach: first, building the HQDT by decomposing the question into sub-questions and computing certainty scores, and second, reasoning through the tree by selecting appropriate knowledge sources, retrieving answers with probabilities, and aggregating them to generate the final answer. The framework is evaluated on two datasets, KQA Pro and Musique, where it outperforms existing methods by effectively combining knowledge from heterogeneous sources. On KQA Pro, RoHT demonstrates improved recall by supplementing an incomplete KB with Wikipedia. On Musique, it achieves significant gains in F1 scores by incorporating both text and KB information. The results highlight the effectiveness of hierarchical decomposition and probabilistic reasoning in enhancing the accuracy and explainability of complex question answering.</sample>
    <sample id="196">The example where the governor is on the left is "I saw Bart and Lisa."</sample>
    <sample id="197">The state-of-the-art models in dialogue systems include several advanced conversational AI models, though specific names were not provided in the text. The evaluation described in the content compared four such models using ABC-Eval, highlighting the importance of precise and reliable metrics to assess their performance across multiple dimensions of chat quality.</sample>
    <sample id="198">We need to evaluate the models' acceptability throughout the context window because modern large language models have longer context windows, and it is crucial to understand how their acceptability judgments are affected by longer and more complex sequences of text. This helps ensure their robustness and reliability in real-world applications.</sample>
    <sample id="199">Yes, training in a multilingual fashion caused a performance drop compared to the monolingual English model in seven out of the nine datasets, which is referred to as the "Curse of Multilinguality." However, there was a performance gain in three datasets.</sample>
    <sample id="200">No, the annotators do not know about the entity in advance. They are provided with some background knowledge about the entities (such as search links, Wikipedia text, or images) to help them understand and select the correct entity based on the indirect referring expression.</sample>
    <sample id="201">The evaluation used state-of-the-art neural MT metrics, specifically BLEURT, and also included expert-based human evaluation results.</sample>
    <sample id="202">The regression in generalization does impact specific NER types. While the study found that overall performance drops due to temporal drift, certain entity types may be affected more than others depending on how language usage and naming conventions have evolved over time. However, the paper does not provide a detailed breakdown of which specific NER types are most impacted.</sample>
    <sample id="203">Positionality in NLP matters because it highlights how datasets and models may systematically favor certain groups over others, based on the perspectives and experiences of the people who created or annotated them. This can lead to biased outcomes that exclude or misrepresent underrepresented populations, affecting the fairness, inclusivity, and effectiveness of NLP technologies in real-world applications.</sample>
    <sample id="204">The provided text does not specify whether multilingual LLMs like BLOOM were fine-tuned with adapters or full fine-tuning. It only mentions that they are inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="205">Shangbin, a PhD student at the University of Washington, presents research on political bias in language models, tracing its path from pretraining data through models to downstream tasks. Large language models are trained on diverse web data, including politically biased sources like news outlets and social media. This diversity allows models to learn multiple perspectives but also introduces social biases that may affect fairness in applications. The study evaluates political leanings of models using standardized questionnaires and finds that models like GPT-4 exhibit liberal tendencies, while others, such as BART, are more conservative. Further experiments show that pretraining on partisan corpora shifts models’ ideological positions, reflecting societal polarization post-2017. The research also reveals fairness issues in downstream tasks: left-leaning models are better at detecting hate speech against minorities but less effective for dominant groups, while right-leaning models show the opposite pattern. Similar biases are observed in fake news detection. These findings highlight a pressing concern: politically biased models, if deployed without caution, may marginalize certain groups or allow harmful content to go unchecked. The study underscores a dilemma: removing bias risks censorship, while retaining it introduces fairness challenges. The research calls for awareness and careful mitigation of political biases in language models.</sample>
    <sample id="206">They use a model that first fine-tunes on the CE (comparison and expansion) tasks and then on the debate task for transfer learning.</sample>
    <sample id="207">The recent test sets used to assess the PaLM capabilities are the latest test sets from the WMT evaluation, which ensure no overlap with the training data of the language model.</sample>
    <sample id="208">The authors proposed three recommendations at last.</sample>
    <sample id="209">The proposed method achieves a significant gain over the strongest baseline, but the exact numerical value of the gain is not specified in the provided content.</sample>
    <sample id="210">The name of the speaker is Shuheng.</sample>
    <sample id="211">Yes, the results and dataset in the paper can be used as a benchmark for future research in automatic text simplification, as they provide a gold standard for alignment and serve as a base for evaluating simplification models.</sample>
    <sample id="212">The paper does not specify the exact number of smaller models experimented with, but it mentions that they find T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating they at least experimented with T5.</sample>
    <sample id="213">The OFA model is used as the base model for investigating multi-modal instruction tuning.</sample>
    <sample id="215">Adam Przepiórkowski's talk discusses the dependency structure of coordination, contrasting asymmetric and symmetric approaches. In theories like Universal Dependencies and Meaning-Text Theory, the first conjunct is considered the head, while Prague Dependency Treebanks use the conjunction as the head, and Word Grammar proposes a multi-headed structure. Przepiórkowski argues for symmetric structures, where all conjuncts are equally heads, and against asymmetric ones. His argument is based on the principle of dependency length minimization, which favors shorter dependencies. He uses examples showing that direct objects are typically closer to the verb, but when they are long, their position can be adjusted to minimize dependency length. Using data from the Penn Treebank, he finds that left conjuncts tend to be shorter, especially when the governor is on the left or absent. However, this tendency disappears when the governor is on the right. This pattern supports symmetric structures, as it suggests that no single conjunct is privileged. The findings challenge asymmetric models and reinforce the case for symmetric coordination structures. The paper provides further detailed analysis, and Przepiórkowski invites discussion at the poster session.</sample>
    <sample id="217">The paper "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation" introduces a novel approach to generating dialogue with control over multiple attributes. Previous methods focused on single attributes or used discrete labels, lacking support for continuous attributes and compositional generalization. The authors propose DCG, a Disentangled Controllable Generation model, which learns attribute concepts from seen values and uses disentanglement loss to separate different attribute combinations. They also introduce MAE, a unified reference-free evaluation framework, to assess controllability across different attribute granularities. The model is built on DialoGPT with a compositional prompt module, including attribute- and task-oriented prompts to guide generation. Pseudo attribute combinations and disentanglement loss enhance diversity and generalization. Experiments on DailyDialog-CG show that DCG outperforms baselines in controllability and text quality, especially for unseen attribute combinations. The evaluation framework MAE correlates well with human judgments and is effective for both discrete and continuous attributes. The study demonstrates that the proposed method successfully generalizes from seen to unseen attribute combinations, proving the effectiveness of prompt-based disentangled controllable dialogue generation.</sample>
    <sample id="218">The authors of the paper are affiliated with Google Translate.</sample>
    <sample id="219">Jia-Huei Ju presents a research paper titled *"A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports"*, conducted with colleagues at Academia Sinica. The study focuses on analyzing annual financial reports (Form 10-K) to extract meaningful financial signals, which is a labor-intensive task. Observing that financial reports from consecutive years share high text similarity (about 80% of tokens), the researchers propose a highlighting task to identify important words that reflect changes or relations between reports. This task involves comparing a target report with its previous year's reference report to detect key differences.

The proposed pipeline includes document segmentation, relation recognition, and two stages of fine-tuning—first using an external dataset (eSNLI) for out-of-domain training, then applying in-domain fine-tuning with pseudo labels. The model is evaluated on the eSNLI dataset and the newly released FINAL dataset, using metrics like precision and PCC (Pearson Correlation Coefficient). Results show that the domain-adaptive model performs well, even on unseen mismatched pairs, demonstrating strong generalization.

The work contributes a novel highlighting task and dataset, with potential for future improvements in effectiveness and integration with information retrieval techniques. The paper and GitHub repository provide further details.</sample>
    <sample id="220">The authors of the paper are affiliated with Stony Brook University.</sample>
    <sample id="221">The paper analyzed the translation performance of PaLM on German to English language pairs.</sample>
    <sample id="222">This work explores challenges and solutions for domain adaptation in open-domain question answering (QA). It uses a case study where a model trained on Wikipedia struggles with biomedical questions due to domain differences. The paper investigates interventions like zero-shot and few-shot learning to improve generalization. Few-shot methods use a few target domain examples to generate more data, improving retriever and reader performance by 8% and 11% on average. Zero-shot methods manipulate question, answer, and context distributions to better align with the target domain. The study also classifies dataset shifts into no shift, concept shift, covariate shift, and full shift, using compatibility measures based on likelihood scores from source models. By mapping datasets onto a 2D grid, the authors identify which interventions are most effective for each shift type. Few-shot adaptations benefit all datasets, while zero-shot methods help those with concept or covariate shifts. The results show up to 24% improvement in reader performance, highlighting the importance of matching interventions to the nature of domain shifts.</sample>
    <sample id="223">The name of the speaker is Shangbin.</sample>
    <sample id="224">The models investigated during the experiments were long-mBART for document-level simplifications and base mBART for sentence-level simplifications.</sample>
    <sample id="225">From the 62 diverse tasks used in MultiInstruct, 53 tasks are used for training and additional tasks are reserved for testing, including the entire common sense reasoning group and 5 additional tasks from the VQ and Miscellaneous groups.</sample>
    <sample id="226">The presentation mentions two individuals, Regina Stodden and Omar, who are involved in presenting the paper, but it does not explicitly state how many authors are involved in the paper itself. Therefore, based on the given information, we cannot determine the exact number of authors.</sample>
    <sample id="227">The audio discusses the limitations of current language models in grounded language understanding, which involves mapping natural language to executable plans or programs within specific environments. Traditional models, trained only on text, struggle with this due to the lack of grounding. Existing approaches often rely on language models to generate plans, which can be invalid or ungrammatical. The presented work introduces the Pangu framework, which shifts focus from generation to discrimination, using a symbolic agent to propose plans and a language model to score and rank them. This approach avoids the complexities of generation, leading to more reliable results. The framework is tested on knowledge-based question answering and shows strong performance across various models and learning settings, including fine-tuning and in-context learning. Pangu demonstrates high sample efficiency and robustness, especially under non-i.i.d. conditions, as it avoids overfitting seen structures. The key takeaway is that for grounded language understanding, discrimination is a more effective strategy than generation. The authors welcome further discussions and collaborations.</sample>
    <sample id="228">The authors experimented on the following datasets: AG News, MIND, SST2, and Enron Spam.</sample>
    <sample id="229">Gabriella Skitalinskaya presents research on detecting improvable claims in argumentative writing, conducted in collaboration with Henning Wachsmuth. The study addresses the challenge of determining when an argumentative claim is optimally phrased and what revisions might improve it. The paper introduces two tasks: Suboptimal-Claim detection, which identifies whether a claim needs revision, and Claim Improvement Suggestion, which identifies specific quality issues to address. The research explores using revision histories from collaborative debate platforms like Kialo to model argument quality based on implicit patterns of improvement. The authors highlight four key challenges: ensuring the representativeness and reliability of revision-based data, selecting appropriate models sensitive to small changes in phrasing, accounting for contextual dependencies in argument quality, and addressing topical and user biases in revision data. The study concludes that revision-based data can be effectively used for these tasks, and modeling the differences between claim versions helps in detecting suboptimal claims. Additionally, the impact of contextual information varies depending on the task and the type of quality issues present. The paper offers a detailed analysis of strategies for addressing these challenges and provides insights into improving argumentative writing through computational support.</sample>
    <sample id="231">NACHOS is a dataset of medical crawled data from the web, used for training the DrBERT model, which is a robust pre-trained model in French for biomedical and clinical domains.</sample>
    <sample id="232">The name of the speaker is David Vilar.</sample>
    <sample id="233">Sara Papi from the University of Trento and FBK introduces the paper "Attention as a Guide for Simultaneous Speech Translation" co-authored with Matteo Negri and Marco Turchi. The paper addresses the challenges of SimulST, which translates spoken language into text in real-time. Current models often require complex architectures, multiple training objectives, and several models for different latency levels. The proposed solution, EDAtt (Encoder-Decoder Attention), uses existing offline ST models without retraining, handling latency through attention mechanisms. EDAtt emits partial translations based on cross-attention weights: a word is emitted if attention is not concentrated on the last λ speech frames, indicating stable information. This strategy avoids emitting uncertain outputs, improving reliability. Results show that EDAtt outperforms other strategies like Wait-k and Local Agreement, achieving higher BLEU scores and lower latency. It also performs best in computational-aware latency, making it the fastest method. The approach is open-sourced to support reproducibility. Overall, EDAtt offers a simple, effective, and efficient method for simultaneous speech translation.</sample>
    <sample id="234">The prompting strategy has a significant impact on the results, as demonstrated by experiments showing differences of more than one BLEURT point, with extreme cases reaching up to 40 BLEURT points. Selecting a good prompting strategy is crucial for achieving better translation performance with large language models like PaLM.</sample>
    <sample id="235">The affiliations of the authors of the paper are not explicitly mentioned in the provided text. However, the authors are named as Kayo Yin, Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig. Typically, such information would be included in the full paper or presentation materials, but it is not provided here.</sample>
    <sample id="236">The 5 expert-written instructions are different phrasings or formulations of the same task, designed to guide the model in performing the multi-modal task in various ways. They are used to improve the model's generalization and robustness by exposing it to diverse instruction styles during training and evaluation.</sample>
    <sample id="237">The authors propose a diagnostic test suite called KITMUS to evaluate models' ability to integrate and use knowledge from multiple sources, such as pretrain-time knowledge and inference-time knowledge, in natural language understanding tasks.</sample>
    <sample id="238">In this video, Yebowen Hu from the University of Central Florida introduces MeetingBank, a new benchmark dataset for meeting summarization. The dataset includes 1,366 City Council meetings with transcripts, reference summaries, and related URLs. The creation process involved using Speechmatics API for transcription, identifying meetings via ItemID, and aligning timestamps with summaries. MeetingBank provides statistics on meetings, duration, tokens, speakers, and summarization instances across cities. The dataset is analyzed using coverage and density scores to measure abstraction levels, revealing that most summaries are extractive, with Seattle and Boston showing higher density than Denver. For evaluation, the team tested top summarization systems, including extractive models like LexRank and abstractive models like BART-Large and DialogLM. While DialogLM performed best in ROUGE-2, GPT-3 excelled in fluency and coherence but lagged in informativeness and factuality according to human evaluation. The study highlights the need for improved automatic metrics and emphasizes the value of MeetingBank as a resource for developing better summarization tools and understanding City Council decision-making. The dataset is available for researchers to use and explore.</sample>
    <sample id="241">Ethan presents a paper titled "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments," co-authored with colleagues at Georgia Tech. The paper addresses two major shortcomings in current misinformation detection systems: unrealistic evaluation methods and a lack of human-centric design. Many systems use outdated or leaked data, making them ineffective for real-time detection. Additionally, they often exclude or marginalize human input, which is crucial for accurate moderation. The proposed framework integrates human feedback throughout the process, from detecting misleading claims in social media posts to verifying policy violations. The system uses a T5 model for claim extraction and a BERT model for stance classification, with human verification at key stages. Evaluation shows the system can detect unapproved treatments before they are debunked in the news, achieving 65% accuracy in policy violation detection. It also demonstrates high efficiency, identifying 124.2 policy violations per human hour. The framework provides a realistic, end-to-end evaluation method for misinformation detection systems and aims to inspire future human-in-the-loop approaches. The work also offers an external perspective on the development and assessment of such systems.</sample>
    <sample id="242">Common evaluation methods for dialogue systems include human evaluations such as asking human judges to select which of two conversations is better or to rate conversations using a Likert scale. These methods provide holistic assessments of overall dialogue quality. Additionally, dimensional evaluations assess specific aspects of chat quality, such as relevance, coherence, empathy, and factual accuracy, often through behavior annotation approaches like ABC-Eval.</sample>
    <sample id="243">The paper involves 6 authors: Jenny (the presenter), Sebastian Santy, Ronan Le Bras, Katharina Reinecke, and Maarten Sap from the University of Washington and the Allen Institute for AI.</sample>
    <sample id="244">The background knowledge needed in the example with Servin and Kea is that "Judges decide cases in law courts."</sample>
    <sample id="245">In their presentation, Lining Zhang and colleagues introduce a two-step pipeline for identifying high-agreement workers on Amazon Mechanical Turk (MTurk) for summarization tasks. The motivation stems from the challenges of using automatic metrics and the lack of best practices for worker recruitment. The pipeline includes a "Qualification Task" and an "Endurance Task." The first stage evaluates annotators' ability to assess multiple dimensions of summaries, resulting in categorization into gold, silver, bronze, and blocked workers, with only gold and silver advancing. The second stage tests endurance by assigning a heavy workload, further filtering to 12 high-agreement workers. These workers achieved higher inter-annotator agreement (IAA) than experts, with Krippendorff’s Alpha reaching 0.534 in a reference-based task. The study also compares their performance with baseline MTurk workers and CloudResearch annotators, finding that the pipeline workers achieved similar quality at lower cost. However, the pipeline does not guarantee correctness, and further research is needed. The work offers a scalable, cost-effective method for recruiting reliable annotators, with potential for broader applications across tasks, languages, and platforms. The study is limited to English summarization on MTurk and lacks a guaranteed training mechanism for correctness. The research was supported by Google.</sample>
    <sample id="246">Yes, the code is available on GitHub.</sample>
    <sample id="247">Jiho Kim from KAIST AI introduces the paper "FACTKG: Fact Verification via Reasoning on Knowledge Graphs," presenting a novel fact verification task that utilizes knowledge graphs (KGs) as evidence sources. Unlike existing datasets such as FEVER or TabFact, which rely on text or tables, FACTKG is the first to leverage KGs like DBpedia for verifying natural language claims. The dataset includes claims in both written and colloquial styles, with two labels: SUPPORTED and REFUTED. It encompasses five reasoning types: one-hop, conjunction, existence, multi-hop, and negation. Verification involves retrieving relevant triples from the KG and reasoning about their connections to the claim. The paper also describes methods for generating colloquial claims using a style transfer model and presupposition templates. Experimental results show that baselines using KG evidence, particularly the GEAR model, outperform claim-only approaches and majority class baselines. The proposed dataset supports practical applications in dialogue systems and other tasks requiring consistency checks between natural language and KGs. The work contributes a new benchmark for fact verification, emphasizing the potential of KGs in reliable and interpretable reasoning. The dataset is available for download, and the authors invite further collaboration.</sample>
    <sample id="248">No, the annotators for NLPositionality are not balanced in regard to each demographic, such as country and gender. The study collected annotations from over 1000 annotators across 87 countries, but the data shows that certain groups, such as people from English-speaking countries and those with college or graduate education, were more represented, while others, such as non-binary individuals, were underrepresented.</sample>
    <sample id="249">In the acceptable domain, sentences were perturbed by altering them while preserving their relevant syntactic and semantic structure. Despite these changes, the models showed similar increases in MPP judgments across all perturbations, indicating their sensitivity to the underlying structure rather than surface-level changes.</sample>
    <sample id="250">A dimensional evaluation means assessing different aspects or dimensions of chat quality, such as relevance, consistency, empathy, and common sense, to understand the strengths and weaknesses of a conversational AI model in a more detailed and nuanced way.</sample>
    <sample id="251">The authors of the paper are affiliated with the University of Science and Technology of China.</sample>
    <sample id="252">Sai Kiran Tanikella presents "U-CREAT: Unsupervised Case Retrieval using Events extrAcTion," a joint work with Abhinav Joshi, Akshat Sharma, and Ashutosh Modi. The research addresses the challenge of Prior Case Retrieval (PCR) in legal contexts, where retrieving relevant past cases cited in a query document is essential but difficult due to the growing volume of legal data. The team introduces two key contributions: the IL-PCR dataset and the U-CREAT pipeline. The IL-PCR dataset, the first benchmark for PCR in Indian legal documents, contains 7,070 cases with an average of 6.775 citations per query, offering a comprehensive testbed. The U-CREAT pipeline employs an unsupervised, event-based approach using dependency parsing to extract subject-verb-object triplets (events) from legal texts. These events are used to compute an interaction matrix, which ranks candidate cases based on event similarity. The study compares various models—count-based, transformer-based, and event-based—and finds that event-based models, especially the Event Filtered Documents model, significantly outperform baselines like BM25 and even recent supervised approaches on the COLIEE dataset. U-CREAT demonstrates high efficiency, generalization across legal systems, and sets a new state-of-the-art in PCR, highlighting the potential of event-based methods in legal AI.</sample>
    <sample id="253">Mario Ezra Aragón presents "DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media," a collaborative effort between researchers from Mexico and Spain. The research aims to detect mental health disorders by analyzing social media content, leveraging the vast amount of data available online where individuals often discuss their mental health. The study employs domain adaptation to improve model performance when annotated data is scarce, using BERT as a base model and adapting it to the specific language of Reddit and mental health discussions. DisorBERT integrates domain-specific knowledge through guided masking, enhancing the model's ability to focus on relevant words during training. The results on the eRisk dataset show that DisorBERT achieves a better balance between precision and recall compared to baseline models. The model is tested on sentences from Beck's Depression Inventory, demonstrating that DisorBERT generates more psychologically relevant predictions than BERT. Visualization techniques reveal that DisorBERT highlights key terms like "anxious" and "medication," which are central to depression. The model outperforms MentalBERT, despite using less data, showing a strong balance between user detection and accurate labeling. Future work includes exploring additional lexical resources and incorporating clinical data to further enhance the model's capabilities.</sample>
    <sample id="254">In this research, Sun Qi from Nanjing University of Science and Technology introduces "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction," aiming to enhance the performance of document-level relation extraction (DocRE) by improving the quality of distantly supervised (DS) data. Traditional methods rely on human-annotated data, which is costly, while DS data, though abundant, are noisy and prone to false positives. To address this, the proposed framework introduces uncertainty-guided label denoising to filter out unreliable pseudo labels. It utilizes Monte Carlo dropout to estimate model uncertainty and introduces an instance-level uncertainty estimation method to handle overlapping relations. Dynamic class uncertainty thresholds are proposed to filter pseudo labels based on their uncertainty scores, particularly benefiting long-tail relation classes. A multi-phase training strategy is also designed to iteratively refine DS labels and improve model performance. Experimental results on public datasets show that the framework outperforms existing baselines. The key contributions include the uncertainty-guided denoising framework, instance-level uncertainty estimation for overlapping relations, dynamic thresholding for long-tail classes, and significant performance improvements in DocRE tasks. This work provides a robust solution to the noise problem in DS data and advances the field of document-level relation extraction.</sample>
    <sample id="255">The form of the prompting is important in the cases of zero and one-shot prompting. It is not as crucial when using a five-shot prompting strategy, where the examples provided carry most of the weight.</sample>
    <sample id="257">The authors evaluated four state-of-the-art chat models.</sample>
    <sample id="258">In this video, Chiang Cheng-Han introduces a research paper titled "Can Large Language Models Be an Alternative to Human Evaluation?" The study explores using large language models (LLMs) to evaluate the quality of text in natural language processing tasks, such as assessing stories generated by GPT-2 or written by humans. The researchers provide LLMs with instructions and samples, expecting them to rate the text based on attributes like grammar, coherence, likability, and relevance. To validate the effectiveness of LLM evaluation, the team compares the model ratings with human evaluations conducted by English teachers, who are considered experts in scoring written texts. The results show that while smaller LLMs like T0 and InstructGPT (curie) do not consistently prefer human-written stories, larger models like Davinci and ChatGPT demonstrate a clear preference similar to human evaluators. This suggests that certain LLMs can serve as viable alternatives to human evaluation. The paper addresses various questions, such as agreement between LLMs and humans, the impact of instruction wording, and the benefits and costs of LLM evaluation. The findings indicate that, under the right conditions, LLMs can provide meaningful and reliable evaluations, offering a promising alternative to traditional human-based assessments.</sample>
    <sample id="259">Yusen Zhang from Penn State University introduced XSemPLR, a new benchmark for cross-lingual semantic parsing that supports multiple natural languages and meaning representations. Semantic parsing involves translating user queries into formal representations like SQL or Lambda Calculus, and cross-lingual semantic parsing extends this to multiple languages. Existing models are limited in coverage, often missing languages like Chinese or meaning representations like Lambda Calculus. XSemPLR addresses this by providing a comprehensive dataset with 9 datasets, 5 tasks, 8 meaning representations, and 22 languages across 15 families. The benchmark includes six evaluation settings, such as Translate-Test, Monolingual, Multilingual, and Zero-shot/Few-shot transfer. The analysis shows that Encoder-Decoder models (e.g., mT5) outperform Encoder-PTR models (e.g., XLM-R + PTR) across all datasets. Training on a multilingual mix improves performance, though English performance sometimes declines, highlighting the "Curse of Multilinguality." Cross-lingual transfer gaps are significant in zero-shot settings but reduce with few-shot training. The study also finds that pretraining on English helps few-shot learning in other languages, while models like Codex and BLOOM are insufficient for the task. Overall, XSemPLR offers a unified benchmark to evaluate and advance cross-lingual semantic parsing across diverse languages and representations.</sample>
    <sample id="260">The provided content does not mention the number of authors involved in the paper. Therefore, based on the given information, it is not possible to determine how many authors are involved.</sample>
    <sample id="261">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="262">The provided content does not specify the number of authors involved in the paper.</sample>
    <sample id="263">This work addresses the issue of label biases in in-context learning (ICL), a key paradigm for leveraging large language models. The study identifies three types of label biases: vanilla-label bias (model preference for certain labels), context-label bias (bias from in-context examples), and a newly identified domain-label bias (bias from task-specific vocabulary). Experiments show that domain-label bias significantly affects model predictions, especially when random in-domain words are present, leading to poor performance on some tasks. The authors propose a novel calibration method called domain-context calibration, which uses random in-domain words as content-free text to estimate and correct biases across all label types. This approach outperforms prior methods that rely on fixed tokens like "not available," particularly on tasks with high domain-label bias. Comprehensive experiments across multiple datasets and models, including GPT-3, demonstrate that domain-context calibration improves ICL performance, especially for tasks with large domain-label bias. The method enhances decision boundaries and reduces prediction errors, showing its effectiveness in mitigating various label biases. Overall, the work provides a systematic analysis of label bias in ICL and introduces a practical calibration technique to improve model reliability and performance.</sample>
    <sample id="264">Lin Wang from Zhejiang University introduces TAVT, a novel framework for Transferable Audio-Visual Text Generation. While uni-modal text generation has advanced significantly due to large-scale pre-training, multimodal tasks like audio-visual text generation face challenges due to domain shifts and high annotation costs. TAVT aims to address these by enabling models to adapt quickly to new domains with limited labeled data. The framework includes three key components: an audio-visual meta-mapper network, a transformer-based encoder-generator, and Dual Counterfactual Contrastive Learning (DCLL). The meta-mapper aligns visual concepts across domains into a unified audio semantic space using audio clustering and learnable tokens. The encoder-generator assigns modality contributions to each word using cross-attention. DCLL enhances visual-text alignment by generating fine-grained supervision signals from counterfactual examples, avoiding reliance on negative samples. The model is trained using a meta-learning approach similar to MAML, with support and query sets for adaptation. Experiments on MSVD and MSR-VTT benchmarks show TAVT outperforms state-of-the-art methods in both cross-dataset and cross-domain settings, especially excelling in low-resource domains. The results highlight TAVT’s effectiveness in handling domain shifts and improving transferability in audio-visual text generation.</sample>
    <sample id="265">The name of the speaker is Vasudha.</sample>
    <sample id="266">The provided text does not mention the affiliations of the authors of the paper.</sample>
    <sample id="268">The most common errors of PaLM are omission errors, where parts of the source sentence are dropped in the translation to produce a better-sounding output.</sample>
    <sample id="270">The authors of the paper are affiliated with the Emory NLP Lab at Emory University, led by Professor Jinho Choi, and in collaboration with Amazon Alexa AI.</sample>
    <sample id="271">In this paper, CFT stands for "Continuously Fine-Tuning."</sample>
    <sample id="272">The paper involves 7 authors.</sample>
    <sample id="274">The name of the speaker is Yusen Zhang.</sample>
    <sample id="276">Ananya and Vignesh present "IndicMT Eval," a dataset aimed at meta-evaluating machine translation (MT) metrics for Indian languages. While English-centric MT metrics are well-studied, their applicability to other languages, especially Indian ones, remains underexplored. The study focuses on five Indian languages—Tamil, Malayalam (Dravidian), and Hindi, Marathi, Gujarati (Indo-Aryan)—using 200 sentences from the Flores dataset. These are translated into English by seven models, resulting in 7,000 candidate translations. Human annotators evaluate these translations, identifying error types (accuracy, fluency, special) and severity, and assigning overall scores. The analysis reveals that overlap-based metrics like chrF have high correlation with human scores, but embedding-based metrics, especially BERTscore and MuRIL, and COMET variants show stronger overall performance. COMET variants, particularly fine-tuned IndicCOMET MQM, outperform baseline models on most languages and demonstrate better robustness on the ACES Translation Accuracy Challenge Sets. The study highlights the limitations of existing metrics, which often produce narrow score ranges, and emphasizes the importance of language-specific evaluation. The dataset is publicly available, offering a valuable resource for improving MT evaluation for Indian languages.</sample>
    <sample id="277">The new method is called "Multiset Tagging and Latent Permutations."</sample>
    <sample id="278">The author described the "marked words" method as a technique to identify words that distinguish marked (marginalized) groups from unmarked (dominant) groups by using weighted log-odds ratios to find the top distinguishing words for each marked group, drawing on the sociolinguistic concept of "markedness." This method allows for the detection of specific stereotypes and patterns without relying on a predefined lexicon.</sample>
    <sample id="279">The author, Shangbin, is a PhD student at the University of Washington.</sample>
    <sample id="280">In this work, Shi Tao introduces "MultiEMO," an attention-based, correlation-aware multimodal fusion framework for Emotion Recognition in Conversations (ERC). The goal of ERC is to predict the emotion of each utterance using textual, audio, and visual modalities. Existing methods face challenges such as inadequate exploitation of multimodal complementarity, poor performance on minority emotion classes, and difficulty distinguishing semantically similar emotions. To address these, MultiEMO includes four key components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification. The framework introduces VisExtNet, a novel visual feature extractor that avoids redundant scene information by focusing on facial expressions. It also proposes MultiAttn, a fusion model using bidirectional multi-head cross-attention to integrate multimodal information effectively. Additionally, a Sample-Weighted Focal Contrastive Loss is introduced to improve classification of minority and similar emotions. Extensive experiments on MELD and IEMOCAP datasets show MultiEMO achieves state-of-the-art performance, especially in minority and semantically similar emotion classes. However, limitations include the inability of VisExtNet to distinguish speakers from others in the scene, the need for large batch sizes with the loss function, and relatively poorer performance on minority emotions compared to majority classes.</sample>
    <sample id="281">Kayo Yin presents the work "When Does Translation Require Context? A Data-driven, Multilingual Exploration," conducted with collaborators Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig. The study investigates when translation depends on context and how well models handle such cases. Traditional metrics like BLEU fail to capture context-dependent translations due to their focus on corpus-level performance. To address this, the researchers introduced Pointwise CXMI, a measure of context usage at the word and sentence level, identifying words that require context for accurate translation. Analysis on TED talk transcripts across 14 languages revealed patterns such as dual pronouns in Arabic and formality in Chinese, where context is essential. Based on these findings, they developed the MuDA tagger, a multilingual tool to identify context-dependent discourse phenomena. Using MuDA, the team created a benchmark for document-level translation, showing that context-aware models outperform context-agnostic ones in some phenomena like formality and cohesion, but not others like ellipsis or pronouns. The benchmark also highlights that systems like DeepL perform better than Google Translate in document-level tasks. This work provides a data-driven approach to understanding and evaluating context in multilingual translation.</sample>
    <sample id="282">This paper introduces **StoryTrans**, a novel framework for **non-parallel story author-style transfer** at the **discourse level**, addressing a key challenge in natural language generation. While most prior work focuses on token- or sentence-level style transfer, StoryTrans advances the task to the **story level**, capturing complex **discourse structures** and **author-specific linguistic preferences**. The main challenges include imitating author styles at the discourse level and preserving content while transferring styles across different topics. To address these, StoryTrans learns **discourse representations** from source texts and combines them with **style embeddings** for target-style generation. A new training objective is proposed to align discourse representations in the latent space and enhance content preservation through a **two-stage generation process**: first, transferring the source text with masked content keywords, and then generating the full text by incorporating these keywords. The model is trained using a multi-objective framework, including **self-reconstruction loss**, **disentanglement loss**, **sentence order loss**, and **style classifier loss**. Extensive experiments on newly collected Chinese and English datasets show that StoryTrans outperforms strong baselines in **style control** and **content preservation**, confirmed by both **automatic and manual evaluations**. Style visualization and case studies further validate the model’s ability to generate coherent, style-consistent stories while maintaining original semantics. The code and data are publicly available.</sample>
    <sample id="283">The name of the first mentioned symmetrical dependency structure is the Prague approach.</sample>
    <sample id="284">In this presentation, Peng Tianshuo introduces FSUIE, a novel framework for Universal Information Extraction (UIE) that addresses limitations in current span-based models. Traditional models rely heavily on precise span boundaries, which can be ambiguous due to varying annotations. FSUIE introduces a *fuzzy span mechanism* to model span boundaries as continuous probability distributions, reducing overreliance on exact positions. It also proposes an *adaptive attention mechanism* to better align with the limited span length assumption, enhancing the model's ability to capture relevant context.

The framework includes a *fuzzy span loss* combining Binary Cross Entropy and KL-divergence to guide learning, along with a *fuzzy span attention* mask function that dynamically adjusts attention spans and decays attention at boundaries. These components improve the model's generalization and performance across various tasks.

Experiments on named entity recognition, relationship extraction, and aspect sentiment triplet extraction show that FSUIE outperforms existing models, achieving state-of-the-art results on multiple datasets. Ablation studies confirm the effectiveness of the fuzzy span loss and attention mechanisms in improving convergence and extraction capability. Visualizations support the model's focus on semantically relevant regions. Overall, FSUIE demonstrates strong performance and adaptability across diverse information extraction tasks.</sample>
    <sample id="285">Mingqi Gao from Peking University introduces their work, "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework." The study addresses the issue of factual errors in dialogue summaries, which can be corrected either by improving summarization models or using independent Factual Error Correction (FEC) models. However, the current evaluation methods for FEC models are flawed, relying on overall factuality scores that lack precision and fail to distinguish between error correction and summary generation. To improve evaluation, the authors propose a fine-grained framework based on human-annotated reference corrections, emphasizing minimal edits to correct errors while maintaining fluency. They introduce a new taxonomy of factual errors, categorizing them as content-based or form-based. Using this framework, they find that training FEC models with human-corrected summaries improves performance, and combining human and synthetic data is promising. However, current FEC models struggle with certain error types like additions and attribute errors. The work highlights the need for more accurate evaluation and training methods to enhance factual error correction in dialogue summarization.</sample>
    <sample id="286">The name of the speaker is James Finch.</sample>
    <sample id="287">The paper involves 4 authors: Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="288">Datasets such as BLiMP and SyntaxGym can be used to test syntactic phenomena.</sample>
    <sample id="290">The abbreviations of the five methods for the first research question are not explicitly listed in the provided text. However, one method mentioned is "FTw" (likely referring to a fine-tuning approach), and another is "COSINE." The other methods are not named in the text.</sample>
    <sample id="291">The model is evaluated on 11 biomedical and clinical downstream tasks in French, including named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="294">CamemBERT is initially trained on the OSCAR dataset, which is a large corpus of text in multiple languages, including French.</sample>
    <sample id="295">The name of the speaker is Adam Przepiórkowski.</sample>
    <sample id="296">Valerio Basile presents a collaborative study between the University of Turin and Amazon Alexa, focusing on improving natural language understanding through more nuanced models of irony detection. Traditional NLP relies on supervised learning with annotated data, but the assumption of a single "ground truth" has limitations, especially for complex phenomena like irony. The team created the EPIC corpus, a dataset of 300 short English conversations from social media platforms, annotated by 74 participants across five English varieties. Each conversation received five annotations, revealing significant inter-annotator differences based on factors like age, gender, and nationality. The researchers developed "perspective-aware" models, trained on data splits reflecting these annotator demographics, which showed increased confidence in predictions compared to standard models. Notably, younger generations and annotators from the UK and Ireland exhibited the most variation in labeling irony. The study highlights the importance of considering annotator perspectives in NLP tasks, suggesting that models should account for diverse human interpretations rather than assuming a single correct answer. This approach opens new avenues for more accurate and context-aware natural language processing systems.</sample>
    <sample id="297">The speech discusses the research project "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models," which explores how certain terms—known as dogwhistles—carry hidden, often offensive meanings that are understood by specific in-groups but remain ambiguous to outsiders. Using Senator Josh Hawley’s use of the term "cosmopolitan" as an example, the research highlights how such language can subtly convey anti-Semitic or other prejudiced messages while maintaining plausible deniability. The project develops a comprehensive glossary of over 340 dogwhistle terms, categorized by register, type, and persona, and examines their historical use in U.S. political speeches, revealing a strong correlation with the Republican Southern Strategy. The study also evaluates how language models like GPT-3 recognize dogwhistles, finding that performance varies, particularly with informal or transphobic terms. Finally, the research demonstrates that dogwhistles can evade content moderation systems by reducing perceived toxicity when slurs are replaced with coded language. This work underscores the importance of understanding dogwhistles in NLP and linguistics, as they challenge traditional notions of meaning and enable covert, harmful rhetoric online.</sample>
    <sample id="298">The findings that led to the conclusion that temporal drift is the main cause of performance loss include the experiment where models were retrained or continued to be pre-trained with more recent data. The results showed that performance degraded with a larger temporal gap between the training and test data, confirming that the increasing time difference between the data used for training (CoNLL-2003) and the newer test data (CoNLL++) is the primary reason for the performance drop.</sample>
    <sample id="299">Michalis Korakakis and Andreas Vlachos present a method to improve the robustness of Natural Language Inference (NLI) models by reducing their reliance on spurious correlations, or "shortcuts," that can lead to poor out-of-distribution performance. While NLI models excel on standard benchmarks, they often exploit dataset-specific patterns, such as high word overlap, which do not generalize well. Existing shortcut mitigation approaches require domain-specific knowledge and assume a pre-trained auxiliary model, which can be computationally expensive and may not align with the learner's behavior. To address these limitations, the authors propose a minimax training approach where a learner model minimizes task loss, while an auxiliary model maximizes the learner’s loss by generating example weights that emphasize hard, under-represented instances. This encourages the learner to focus on challenging examples that counteract shortcut-based learning. The method does not assume specific shortcut types and uses a feed-forward network for the auxiliary. Evaluated on datasets like MNLI, FEVER, and QQP, along with out-of-distribution adversarial test sets, the approach improves out-of-distribution performance without sacrificing in-distribution accuracy. The paper also explores the method’s scalability and generalization to larger models and synthetic shortcuts.</sample>
    <sample id="300">Belinda introduces the concept of "interactive dictation," a task that allows users to dictate and edit a document using voice commands in a natural and intuitive way. Unlike traditional speech-to-text systems, which only support dictation, interactive dictation enables users to make edits through open-ended natural language commands without relying on fixed trigger words. The system must distinguish between dictation and editing commands in real-time and apply the edits accordingly. The work formalizes this task as a four-step process: speech recognition, segmentation of dictation and command utterances, normalization of commands, and execution of actions to update the document. To support this task, the team developed a data collection interface and built a dataset by simulating the process of dictating and editing an email. They also created a baseline system using separate models for each step, experimenting with architectures like T5 and GPT-3. Results show that GPT-3 achieves higher accuracy but is slower, while T5 benefits from predicting intermediate programs for efficiency. The team highlights that there is significant room for improvement and has released their code and paper to encourage further research in this area.</sample>
    <sample id="302">It is necessary to permute the tokens for the output sequence because, after tagging each input token with an unordered multiset of output tokens, the tokens are not in the correct order. Permuting them ensures the output sequence is structured correctly according to the logical form.</sample>
    <sample id="303">The authors recommended increased transparency about bias mitigation methods because without it, it is difficult to determine whether positive stereotypes and essentializing narratives in language models result from excessive value alignment or other anti-stereotyping methods, making it hard to study and address these issues effectively.</sample>
    <sample id="304">Minimal-pair unacceptable inputs are sentences that are considered ungrammatical or unacceptable within a specific linguistic phenomenon, used in comparison with their acceptable counterparts in minimal pair paradigms to evaluate language models' acceptability judgments.</sample>
    <sample id="305">Dawei, a PhD student at Saarland University, presents the paper "Weaker Than You Think: A Critical Look at Weakly Supervised Learning," co-authored with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. The study critically examines the assumptions and effectiveness of weakly supervised learning (WSL), where models are trained using noisy, weakly labeled data instead of clean human annotations. While WSL claims to achieve high performance without clean data, the research reveals that clean validation data is essential for proper model selection and generalization. Without it, performance drops significantly. The study finds that even a small number of clean samples (e.g., 20 per class) can greatly improve WSL performance, and direct fine-tuning on clean data outperforms complex WSL methods. This suggests that the benefits of WSL are often overstated. The authors recommend reporting clean validation use, comparing WSL with few-shot learning, and considering fine-tuning as a strong baseline. The code is open-sourced for further exploration.</sample>
    <sample id="306">Sebastian Schuster and Najoung Kim present research on entity tracking in language models, focusing on the ability of these models to understand and track changes in entity states across a discourse. They argue that this is crucial for comprehending longer texts but has not been systematically studied. To evaluate this, they designed a task involving boxes and objects where models must predict the final contents of boxes after a series of operations. The task was carefully constructed to prevent models from relying on shortcuts like copying initial states or using heuristics. Testing with models like Flan-T5 and GPT-3 variants revealed that most models simply repeated initial states, while only text-davinci-003 showed non-trivial tracking. Further analysis indicated that models pre-trained on code, like GPT-3.5, performed better, suggesting code pre-training enhances entity tracking. Smaller models could learn tracking with fine-tuning, but randomly initialized models could not. The results highlight the importance of pre-training data and raise questions about the generalizability of entity tracking abilities. The full study, including more results and analysis, is available on arXiv.</sample>
    <sample id="307">The authors evaluated their models using standard natural language processing tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. While specific evaluation metrics (e.g., F1 score, accuracy) are not explicitly mentioned, these tasks typically involve metrics like precision, recall, F1 score, and accuracy.</sample>
    <sample id="308">Jenny, a first-year PhD student at Carnegie Mellon University, presents the work "NLPositionality," which explores design biases in NLP datasets and models by examining their alignment with different populations. The study highlights how models like Prospective API may perform differently based on cultural contexts, such as being less sensitive to offensive terms in Indian contexts. Positionality, a concept from critical studies, refers to the influence of researchers' demographics and experiences on their work. This concept is extended to datasets and models, which aggregate human judgments and may reflect certain positionalities over others. Through the framework NLPositionality, the team re-annotated data with diverse participants using Lab in the Wild, collecting over 16,000 annotations from 1,000+ participants across 87 countries. They compared these annotations with models like GPT-4, Perspective API, and Hate Roberta. The results show that datasets and models align more with English-speaking countries and individuals with higher education, but are less aligned with non-binary individuals. The team recommends keeping detailed design records, adopting a perspectivist approach, and creating specialized datasets and models for underrepresented communities. The goal is to promote inclusive NLP that acknowledges and addresses these biases.</sample>
    <sample id="309">The metric used for measuring inter-annotator agreement was not explicitly named in the provided text. However, it is implied that a standard inter-annotator agreement metric, such as Cohen's Kappa or Fleiss' Kappa, was used to assess the reliability of ABC-Eval behavior labels compared to labels collected by existing methods.</sample>
    <sample id="310">The domain chosen to add completely unrelated sentences to the unacceptable and acceptable queries was Wikipedia.</sample>
    <sample id="311">The provided text does not mention the affiliations of the authors of the paper.</sample>
    <sample id="312">MultiInstruct differs from other benchmarks by being the first large-scale multi-modal instruction tuning dataset, consisting of 62 diverse multi-modal tasks across 10 categories, each with five expert-written instructions. Unlike previous benchmarks that mainly focused on language-only tasks, MultiInstruct addresses the lack of publicly available multi-modal instruction datasets and enables instruction tuning for unseen multi-modal tasks.</sample>
    <sample id="313">The text mentions James Finch and Sarah Finch as presenters, but it does not specify the number of authors involved in the paper. It only mentions the Emory NLP Lab led by Professor Jinho Choi and collaboration with Amazon Alexa AI. Therefore, the exact number of authors is not provided in the given content.</sample>
    <sample id="314">The question asks for the definition of binary coordination, but the provided content does not explicitly define binary coordination. However, based on the context, binary coordination refers to a coordination structure involving exactly two conjuncts (e.g., "Lisa and Bart"). The discussion focuses on the dependency structures of such two-conjunct coordinations, comparing asymmetric and symmetric approaches. Therefore, while the term "binary coordination" is not directly defined in the text, it can be inferred as coordination involving two elements.</sample>
    <sample id="315">The provided text does not specify the average length of the prompts used in the study.</sample>
    <sample id="316">The findings suggest that with proper training on the CoScript dataset, smaller models like T5 can achieve higher-quality constrained language planning than most large language models. This implies that specialized, smaller models can be effectively utilized for constrained language planning tasks when trained on high-quality, constraint-aware datasets, offering a more efficient and cost-effective alternative to large language models.</sample>
    <sample id="317">In this presentation, Peng Li from Fudan University introduces CodeIE, a novel approach that leverages large code generation models for few-shot information extraction tasks. Traditional information extraction methods, such as named entity recognition (NER) and relation extraction (RE), often struggle with the mismatch between structured outputs during inference and the unstructured text learned during pre-training. CodeIE addresses this by reframing the task as a structure-to-structure code generation problem, using code language models like Codex. This approach allows for better alignment between input and output structures, improving the model's ability to generate accurate structured information. The method involves designing code-style prompts that guide the model to extract entities or relations and store them in a structured format, such as a list. The evaluation on multiple datasets shows that CodeIE significantly outperforms traditional text-based models like T5, UIE, and GPT-3, especially in few-shot settings. The analysis reveals that code format prompts reduce structural errors and improve recall, while code-pretrained models like Codex demonstrate superior performance in information extraction tasks. Overall, the study highlights the potential of using code generation models and code-style prompts to enhance few-shot learning in information extraction.</sample>
    <sample id="319">The work investigates several learning strategies, including from-scratch pre-training on different data sizes and sources (such as NACHOS and clinical notes), continual pre-training based on existing models like CamemBERT and PubMedBERT, and comparisons between models trained on homogeneous versus heterogeneous data sources.</sample>
    <sample id="320">The factor of overfitting due to test reuse, as indicated by the gradient of the best fit line in the graph, is greater than one. This suggests that improvements on the CoNLL-2003 dataset translate to more than proportional improvements on the CoNLL++ dataset, indicating that adaptive overfitting is not observed.</sample>
    <sample id="321">The quality of the simplification was evaluated by using the DEPLAIN corpus, which contains manually aligned sentence pairs, as a gold standard to assess automatic alignment methods. Additionally, the effectiveness of text simplification models was measured through evaluation metrics and scores, with results presented as a baseline benchmark for future research.</sample>
    <sample id="322">Enrico presents research on what text classifiers learn about morality, emphasizing that morality is not a singular concept but a pluralistic, subjective framework influenced by individual values. He introduces the Moral Foundations Theory, which identifies five distinct moral foundations—such as fairness and authority—that people prioritize differently. Traditional NLP approaches often treat morality as a binary scale, but this oversimplifies the complexity of moral judgments. Enrico’s work uses explainable AI techniques to explore how language models understand morality in text, particularly across different domains. He uses the Moral Foundation Twitter Corpus, containing 35,000 tweets from seven domains, to analyze how morality is expressed differently in contexts like #AllLivesMatter and #BlackLivesMatter. His findings show that language models can detect nuanced differences in moral expression, such as how subversion of authority is perceived in these domains. For instance, while ALM uses terms like "overthrow" negatively, BLM may view subversion more positively. The study highlights the importance of recognizing domain-specific moral nuances to avoid misinterpretations by language models. Enrico concludes that understanding these differences is crucial for developing more accurate and ethically aware AI systems.</sample>
    <sample id="323">Yujie Wang from Shanxi University introduces DHLK, a novel method for Commonsense QA that integrates language models and knowledge representation learning. Commonsense QA demands systems to answer questions using common knowledge, often requiring retrieval from external knowledge bases. Existing methods combine knowledge from language models and knowledge bases but suffer from noise in retrieved subgraphs and limited interaction between text and graph modalities. DHLK addresses these issues by constructing a Heterogeneous Knowledge Graph (HKG) using a two-stage pruning strategy and knowledge representation learning (KRL) to enhance structure and representation. It incorporates paraphrased entities from WordNet and Wiktionary, and dynamically prunes irrelevant entities using RoBERTa attention weights. Entity and relation embeddings are optimized using TransE, while Relation Mask Self-Attention (RMSA) is introduced to model subgraphs effectively. The method fuses graph and text information through RoBERTa and max-pooling, and integrates path information from ConceptNet for enhanced QA context. Experiments on CommonsenseQA and OpenBookQA demonstrate that DHLK outperforms existing LM and HKG-based methods, achieving strong performance by effectively combining language models with structured knowledge.</sample>
    <sample id="324">Yes, language models can exhibit different political biases. Our research shows that they occupy all four quadrants on the political spectrum, with models like GPT-4 leaning more liberal compared to others such as the BART series. These biases can be influenced by pretraining data and can affect downstream tasks like hate speech and fake news detection, leading to fairness issues in NLP applications.</sample>
    <sample id="326">Cognitive dissonance is the mental discomfort experienced by a person who holds two or more contradictory beliefs, values, or ideas at the same time, or performs an action that contradicts their beliefs or values. It is an important phenomenon to study as it can influence decision-making, mental health, and social dynamics.</sample>
    <sample id="327">Xiao Xu, a third-year PhD student from Harbin Institute of Technology, presents "ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning" at ACL 2023. This work, conducted during an internship at the MSRIC group with support from Intel, addresses the limitations of existing vision-language (VL) models like BridgeTower, which inefficiently utilize unimodal representations across different layers. ManagerTower introduces a novel architecture that employs "managers" in each cross-modal layer to adaptively aggregate insights from pre-trained unimodal experts at various levels. This allows for more effective and comprehensive cross-modal alignment and fusion by leveraging semantic knowledge from different depths of the unimodal encoders. The model uses RoBERTa and CLIP-ViT as unimodal encoders and demonstrates superior performance on downstream tasks, achieving 39.15% accuracy on the Wikivideo test set with only four million images for pre-training. It outperforms other models with similar or even larger data and parameter scales. Visual analysis of the aggregation weights reveals that adaptive managers significantly differ from static ones, showing diverse and effective utilization of unimodal knowledge across layers. The paper, code, and models are publicly available on Archive and GitHub.</sample>
    <sample id="328">The most liberal language model is GPT-4.</sample>
    <sample id="329">This paper presents a noise-resistant zero-shot video sentence localization method that eliminates the need for manual annotations. Traditional zero-shot approaches generate pseudo-queries and pseudo-events from videos but suffer from oversimplified queries, misalignment between pseudo-queries and pseudo-events, and label noise. To address these issues, the proposed method generates complex pseudo-queries using an image-text pre-trained model, such as BLIP, and then constructs pseudo-events by modeling the temporal structure of events. The pseudo-events are selected based on a quality metric that maximizes the difference between the average similarity of video frames inside and outside the event with the pseudo-queries. To reduce label noise, the method employs sample re-weighting based on model confidence and IoU with pseudo-labels, and iteratively refines pseudo-labels using high-confidence predictions. Experiments on ActivityNet Captions and Charades-STA datasets show that the proposed method, referred to as SPL, outperforms existing zero-shot methods in terms of R@M and mIoU metrics. The approach demonstrates robustness to label noise and achieves state-of-the-art performance in zero-shot video sentence localization.</sample>
    <sample id="330">Yes, cumulative training performs equal or better than iterative when doing active learning.</sample>
    <sample id="331">The name of the speaker is Sara Papi.</sample>
    <sample id="332">The data for the MuDA benchmark was taken from transcripts of TED talks that have been translated from English to 14 different languages.</sample>
    <sample id="333">In this presentation, Wenhao from Nanjing University introduces "INK," a novel framework that integrates kNN knowledge into neural machine translation (NMT) to improve generalization and performance. NMT models often suffer from non-smooth representation spaces, particularly with low-frequency tokens that disperse sparsely, creating "semantic holes." While kNN-MT addresses this by using a datastore to smooth predictions, it is computationally expensive and inflexible. INK overcomes these issues by iteratively refining representations using an adapter, guided by kNN knowledge, and updating the datastore asynchronously. The training loop involves aligning contextualized representations with token embeddings and kNN tokens to enhance semantic consistency and density. Experiments on the WMT’19 German-English task show that INK outperforms state-of-the-art kNN-MT systems, achieving higher BLEU and COMET scores with less memory and faster inference. The framework demonstrates that combining adapters and datastores can further refine the representation space, suggesting room for even more effective methods. Overall, INK provides a practical and efficient way to enhance NMT models by leveraging kNN knowledge during training.</sample>
    <sample id="335">The name of the speaker is Matthias Lindemann.</sample>
    <sample id="336">Cross-lingual transfer is the process of training a model on one language (source language) and applying it to another language (target language) without additional training on the target language. This allows the model to perform tasks, such as semantic parsing, in the target language by leveraging knowledge learned from the source language.</sample>
    <sample id="337">The research presented, titled "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning," addresses the challenge of representing out-of-vocabulary (OOV) words in embedding models. OOV words are critical for model performance but are difficult to represent due to their absence in training data. The proposed approach mimics human learning by leveraging word formation and association. It introduces a Word Relationship Graph that captures lexical relationships between OOV words and their related words. The graph is structured in two layers: the first preserves wordpiece information, while the second uses sampling to reduce noise. A self-attention mechanism assigns attributes to OOV nodes based on their characters, and a two-level Graph Attention Network extracts meaningful representations. A readout block captures overall graph information, and contrastive learning with NT-XENT loss aligns the model's embeddings with those of a background model. Experimental results show the model outperforms baselines in both intrinsic and extrinsic tasks, benefiting both static and contextual models. The approach is particularly effective for agglutinative languages and works well with English through proper word segmentation. The study concludes that the model's success depends on the rationality of word decomposition, opening possibilities for broader language applications.</sample>
    <sample id="338">The presentation by Bingsheng introduces the research titled "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations." The study, a collaboration between Rensselaer Polytechnic Institute, Northeastern University, and IBM Research, investigates the effectiveness of human-annotated explanations in improving model performance. It addresses the challenge of evaluating the quality of such explanations, which can be subjective and task-dependent. The team proposes a unified data structure that standardizes various tasks into a multiple-choice format, enabling consistent evaluation. Through preliminary experiments, they found that explanations do not always enhance model learning during fine-tuning and that their usefulness varies by task and model. The researchers introduce TREU, a new evaluation metric that extends the simulatability score by assessing explanation helpfulness during fine-tuning. TREU outperforms existing metrics in evaluating five datasets across two models (T5 and BART), revealing that even low-quality explanations can benefit models. The study highlights the importance of task-specific and format-specific factors influencing explanation utility. Overall, the work provides a foundation for more objective evaluation of human explanations and advocates for quality checks in annotation processes.</sample>
    <sample id="339">The authors of the paper are affiliated with Saarland University in Germany.</sample>
    <sample id="340">Kuan-Hao Huang from UCLA presents "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation," a collaborative work with several researchers. The paper addresses the need for large-scale, high-quality paraphrase data to train effective paraphrase generators, which are crucial for NLP applications like question answering and chatbots. Existing human-annotated datasets are high-quality but limited in scale, while automatically generated datasets like back-translation offer scale but lack syntactic diversity. To overcome this, the team proposes ParaAMR, a large-scale paraphrase dataset generated using AMR (Abstract Meaning Representation) back-translation. AMR captures the abstract meaning of a sentence in a graph structure, allowing for syntactic variation by changing the focus node and modifying the graph. This approach generates paraphrases with similar semantics but varied syntax. ParaAMR contains around 15 million source sentences, with approximately 6.9 paraphrases each. Evaluation shows that ParaAMR maintains high semantic similarity while achieving greater syntactic diversity compared to other back-translation datasets. The dataset benefits NLP tasks such as sentence embeddings, syntactic control in paraphrase generation, and few-shot learning through data augmentation. The team concludes that ParaAMR offers a valuable resource for improving NLP models by enhancing syntactic diversity without sacrificing semantic accuracy.</sample>
    <sample id="341">The authors use two latency measures: average lagging and computational-aware average lagging.</sample>
    <sample id="342">In this presentation, Gao Jingsheng introduces the paper "LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming." The paper, authored by a team from Shanghai Jiao Tong University and Xiaobing.AI, addresses the limitations of existing open-domain dialogue datasets, which are mostly text-based and lack personalization and multi-party conversation data. To overcome these challenges, the team developed LiveChat, a large-scale, video-sourced, and personalized dialogue dataset. The dataset was constructed in three steps: collecting live streaming videos, extracting and transcribing audio into utterances, and matching replies to their respective speakers using a custom method. Additionally, persona information was collected through manual labeling and automated classification to support personalized dialogue generation. The dataset is compared with existing ones, showing its larger scale, video source, and longer average sessions. The team evaluated LiveChat on two tasks—Response Modeling and Addressee Recognition—and found that persona information and session length improved model performance. Experiments with pre-trained models like BART and LLMs also highlighted the distinctiveness of LiveChat. Future work includes exploring efficient transfer learning for LiveChat. Overall, LiveChat offers a valuable resource for research in personalized and multi-party dialogue systems.</sample>
    <sample id="344">The drawbacks of tree-based methods include the need for trees to be given or obtained through complicated and computationally expensive pre-processing or grammar-induction procedures. Additionally, trees are usually not provided in the training data, making their integration challenging.</sample>
    <sample id="345">**Abstract:**  
This paper introduces a novel neural sequence-to-sequence model for achieving compositional generalization in semantic parsing without relying on syntactic trees. The model addresses the challenge of handling unseen, structurally complex logical forms during testing by directly modeling the compositional relationships between input utterances and output logical forms. Traditional approaches often require explicit syntactic trees, which are costly and complex to obtain. In contrast, the proposed method operates in two stages: first, it tags each input token with an unordered multiset of output tokens, and second, it predicts a permutation of these tokens to reconstruct the correct logical form. The permutation prediction is flexible and unconstrained, allowing for a wide range of valid orderings. A key innovation is the use of a continuous relaxation to approximate the NP-hard permutation problem, enabling efficient training via backpropagation. The model is evaluated on the COGS benchmark, where it demonstrates strong performance in generalizing to deeper recursion compared to other treeless models, despite challenges in other structural generalization tasks. The approach also handles the absence of explicit input-output alignment in training data by inducing alignment during training. This work presents a promising alternative to tree-based methods, offering a flexible and scalable solution for compositional generalization in semantic parsing.</sample>
    <sample id="346">The provided text does not mention the affiliations of the authors of the paper.</sample>
    <sample id="348">**Abstract:**  
This paper introduces *Marked Personas*, a novel method for identifying stereotypes and biases in large language models (LLMs) by leveraging natural language prompts to generate personas representing different social identities. Unlike existing approaches that rely on hand-crafted datasets or focus on specific stereotypes, this method is generalizable across demographics and accounts for intersectionality—the compounded effects of multiple social identities. By prompting LLMs to generate personas (e.g., "Imagine you are an Asian woman. Describe yourself."), the study reveals subtle yet harmful patterns in how different groups are portrayed. These patterns are analyzed using the *Marked Words* method, which identifies words that distinguish marginalized (marked) groups from dominant (unmarked) ones based on sociolinguistic "markedness." Results show that while generated personas contain fewer explicit negative stereotypes than human-written ones, they often reflect essentializing and harmful narratives, such as portraying Black women as "strong" and Asian women as "delicate." These portrayals reinforce stereotypes and contribute to systemic discrimination. The study highlights the need for researchers to address positive stereotypes and essentializing narratives, adopt an intersectional lens, and increase transparency in bias mitigation methods. Overall, *Marked Personas* offers a scalable and nuanced approach to uncovering hidden biases in LLMs.</sample>
    <sample id="350">In their paper, "What’s the Meaning of Superhuman Performance in Today’s NLU?", the authors question the validity of claims about models achieving superhuman performance on natural language understanding (NLU) benchmarks. While leaderboard-based evaluation has become standard in NLP, the paper argues that such achievements may not reflect true understanding. The authors analyze two major benchmarks, SuperGLUE and SQuAD, and find that human performance is often evaluated on small, unrepresentative subsets of the test data, while models are tested on full datasets. They also highlight errors in ground-truth answers and inconsistencies in human baseline evaluations, such as low pay rates for annotators and lack of transparency in the selection of human participants. These issues undermine the fairness and reliability of comparing human and model performance. Furthermore, the paper notes that models often exploit spurious patterns rather than demonstrating genuine understanding. The authors conclude that current benchmarks are not reliable for assessing whether models truly outperform humans in reasoning and comprehension tasks. They call for more rigorous benchmarking practices to ensure that claims of superhuman performance are scientifically meaningful. The paper emphasizes the need for transparency, better human evaluation protocols, and more representative datasets to improve the reliability of NLU research.</sample>
    <sample id="351">**Abstract:**  
This paper investigates the generalization capability of Named Entity Recognition (NER) models trained on the CoNLL-2003 dataset when applied to modern data. To evaluate this, the authors created the CoNLL++ dataset, consisting of Reuters news articles from 2020 annotated using CoNLL-2003 guidelines. Over 20 models were fine-tuned on CoNLL-2003 and evaluated on both the original CoNLL-2003 test set and the new CoNLL++ dataset. The study identifies three key factors for achieving good generalization: model architecture (with transformers performing best), model size (larger models generalize better), and the number of fine-tuning examples (more examples improve generalization). The paper also explores the causes of performance degradation. Two hypotheses were tested: adaptive overfitting (due to repeated use of the same test set) and temporal drift (performance decline due to the increasing time gap between training and test data). Results showed no evidence of adaptive overfitting, as improvements on CoNLL-2003 translated to greater improvements on CoNLL++. However, temporal drift was confirmed as the main cause of performance drop, with models pre-trained on more recent data showing better performance. The study concludes that while CoNLL-2003 taggers still work well in 2023, future research should focus on improving model generalization.</sample>
    <sample id="352">ABC-Eval stands for Annotating Behaviors in Chat.</sample>
    <sample id="353">The paper "Python Code Generation by Asking Clarification Questions" addresses the challenge of underspecification in natural language to code generation by introducing interactivity through clarification questions. The authors propose CodeClarQA, a synthetic dataset designed to identify and clarify missing key operations in code generation tasks. They use schema-based similarity scores to detect when an operation is missing or aligned with the natural language description. The dataset is created using a combination of automated methods and human annotations, and includes two types of clarification questions: yes/no and multiple-choice. The paper introduces a pipeline for code generation that involves a Clarification Need Predictor, a Question Selector, and a Code Generator. Experimental results show that clarification questions improve code generation performance, though challenges remain, such as distinguishing similar operations and using argument values effectively. The study also highlights that while the pipeline underperforms compared to models trained only on natural language descriptions, it shows improvement with more answered clarifications. Overall, the research demonstrates the potential of interactive code generation and provides insights into the challenges and future directions for improving code synthesis through clarification.</sample>
    <sample id="354">The performance delta between CoNLL-2003 and CoNLL++ is higher than 5 percentage points until the year 2015.</sample>
    <sample id="356">The affiliations of the authors of the paper are not explicitly mentioned in the provided text. However, the authors are Matthias Lindemann, Alexander Koller, and Ivan Titov. Typically, such information would be included in the full paper, such as in the authors' biographies or institutional affiliations section.</sample>
    <sample id="357">The name of the speaker is Siyu Yuan.</sample>
    <sample id="358">The paper involves five authors: Kayo Yin, Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig.</sample>
    <sample id="359">The approach is compared to the state-of-the-art architecture specifically tailored for simultaneous pre-translation.</sample>
    <sample id="361">Armineh Nourbakhsh, a PhD student at Carnegie Mellon University and research director at JP Morgan AI Research, introduces "CounterComp," a method to improve compositional generalization in multi-step quantitative reasoning tasks. Current neural models struggle with such tasks, especially when multiple arithmetic operations are involved, as they often memorize spurious patterns instead of understanding the underlying logic. To address this, CounterComp leverages counterfactual scenarios by treating each training example as an anchor and generating positive and negative examples based on interventions in the input questions. Positive examples remain unchanged in output despite question modifications, while negative examples show output changes. These triplets are used to introduce an auxiliary metric learning loss with a dynamic margin, adjusting the loss based on the degree of intervention in the question. This approach enhances the model's ability to focus on meaningful tokens related to operations, improving performance on both in-distribution and out-of-distribution samples. The method demonstrates consistent improvements across three state-of-the-art baselines, particularly for tasks requiring more than two reasoning steps. It also aids in achieving better compositional generalization, allowing models to generalize to unseen examples. The work highlights the effectiveness of counterfactual learning in refining the reasoning capabilities of language models for quantitative tasks.</sample>
  </task>
</testset>