<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Sprachmodelle sind große Web-Crawl-Daten, insbesondere politische Nachrichtenmedien wie die New York Times, Los Angeles Times, The Guardian und Huffington Post.</sample>
    <sample id="1">Die Autoren gehören der McGill University an.</sample>
    <sample id="2">**Abstract**  
This paper introduces LayoutMask, a novel pre-trained model for Visually-rich Document Understanding (VrDU) that addresses reading order issues in existing document pre-training models. Unlike previous approaches that use global 1D positions, LayoutMask employs local 1D positions within segments, combined with 2D layout and semantic information, to infer global reading order and enhance text-layout interactions. To strengthen these interactions, we propose two novel masking strategies—Whole Word Masking and Layout-Aware Masking—along with a new pre-training objective, Masked Position Modeling (MPM), which recovers masked 2D positions. Experimental results on benchmark datasets show that LayoutMask outperforms global 1D position methods, particularly in complex layouts with ambiguous entities like "Total." The model demonstrates improved performance in understanding document structures and learning robust layout representations. This work advances document understanding by integrating spatial and semantic reasoning during pre-training.</sample>
    <sample id="3">Hallo! Willkommen bei unserer Präsentation von DEPLAIN, einem neuen Korpus zur Textvereinfachung auf Dokument- und Satzebene. Mein Name ist Regina Stodden, und ich werde euch durch den ersten Teil der Präsentation führen. Zunächst definieren wir Textvereinfachung. Textvereinfachung ist ein Prozess, bei dem ein Text so angepasst wird, um die Textverständlichkeit für eine bestimmte Zielgruppe zu verbessern, beispielsweise für Menschen mit Leseschwierigkeiten oder für Nicht-Muttersprachler. Um ein Modell zur Textvereinfachung zu trainieren, benötigen wir parallele Textpaare, zum Beispiel von Dokumenten oder Sätzen. Im Beispiel seht ihr ein parallel ausgerichtetes Satzpaar eines komplexen deutschen Satzes und seiner Übersetzung in einfache Sprache. Um einen Satz zu vereinfachen, sind verschiedene Techniken möglich, wie ihr im Beispiel sehen könnt, beispielsweise lexikalische Substitution, Klauselentfernung, Umordnung oder Einfügen von Wörtern. Wir schlagen nun unser neues Korpus DEPLAIN vor, denn in den letzten Jahren gab es einige Probleme mit bestehenden Korpora. Zum Beispiel sind diese Korpora zu klein, um ein Modell zur Textvereinfachung zu trainieren. Die anderen drei Modelle, die in den letzten Jahren vorgeschlagen wurden, sind alle automatisch ausgerichtet, was bedeutet, dass ihre Ausrichtungen fehleranfällig sein können. Daher schlagen wir unser neues Korpus DEPLAIN vor, das in zwei Teilkorpora unterteilt ist: DEPLAIN-apa und DEPLAIN-web. DEPLAIN-apa basiert auf Nachrichtentexten. In DEPLAIN-apa haben wir 483 Dokumente manuell ausgerichtet. Das ergibt ungefähr 13.000 parallele Satzpaare. Bei DEPLAIN-web umfasst dieses Korpus verschiedene Domänen und wir haben auch alle 750 Dokumente auf der einen Seite manuell und auf der anderen Seite mit automatischen Ausrichtungsmethoden ausgerichtet. Insgesamt ergibt das 30.450 Satzpaare. Wir haben unsere Satzpaare etwas genauer analysiert, beispielsweise in Bezug auf die Art der Vereinfachung. Wie ihr hier sehen könnt, werden Bibeltexte deutlich stärker vereinfacht als beispielsweise Nachrichtentexte oder Texte für Sprachlernende. Auf allen Ebenen, beispielsweise in Bezug auf lexikalische Vereinfachung, Strukturvereinfachung und insgesamt Vereinfachungsgrad. Außerdem könnt ihr erkennen, dass unser DEPLAIN-Korpus eine hohe Vielfalt an verschiedenen Vereinfachungstransformationen aufweist. Beispielsweise gibt es im DEPLAIN-apa-Korpus deutlich mehr Umordnungen und Wortergänzungen als im DEPLAIN-web-Korpus. Auf der anderen Seite gibt es im Web-Korpus deutlich mehr Wiedergaben. Schauen wir uns nun an, was wir mit diesem Korpus alles tun können. Hallo, ich bin Omar und ich werde nun über die Anwendungsfälle unseres Datensatzes DEPLAIN sprechen. Als ersten Anwendungsfall können wir automatische Ausrichtungsmethoden bewerten. In den letzten Jahren gab es viele Ausrichtungsmethoden, doch im Kontext der maschinellen Übersetzung, bei der wir zwei parallele Dokumente in verschiedenen Sprachen haben und wir die Ausrichtung der Sätze in beiden Dokumenten extrahieren möchten. In unserem Anwendungsfall versuchen wir jedoch, Ausrichtungen zwischen Sätzen zweier paralleler Dokumente zu extrahieren, die in derselben Sprache geschrieben sind, denselben Inhalt haben, aber auf verschiedenen Komplexitätsniveaus. Da wir nun unseren Datensatz DEPLAIN haben, der über manuell ausgerichtete Sätze verfügt, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten. Wir haben einige Anpassungen an den vorgeschlagenen Methoden vorgenommen und haben alle diese Anpassungen sowie den Code, um unsere Experimente durchzuführen, in dem Paper veröffentlicht. Am Ende haben wir festgestellt, dass die beste automatische Ausrichtungsmethode für die Textvereinfachung im Deutschen die Methode MASSalign ist. In dem Paper findet ihr auch den Code, um diese Methode auf eure eigenen Dokumente anzuwenden. Der zweite Anwendungsfall, den wir in unserem Paper gezeigt haben, ist ein Fall der automatischen Textvereinfachung durch Feinabstimmung von Sprachmodellen, um vereinfachten Text aus dem komplexen Eingabetext zu erzeugen. Wir haben zwei verschiedene Modelle feinabgestimmt. Wir haben das Modell long-mBART feinabgestimmt, um Dokumentebene-Vereinfachungen zu erzeugen, und auch das normale Basismodell mBART, um Satzlevel-Vereinfachungen zu erzeugen. Ihr findet auch alle Checkpoints und könnt euch in dem Paper detaillierter über die Ergebnisse und die Bewertungsmetriken unserer Experimente informieren. Wir kamen zu dem Schluss, dass diese grundlegende Feinabstimmung Ergebnisse liefern kann, die besser sind als die Baseline-Ergebnisse, und schlugen diese Ergebnisse als Basisbenchmark für das Problem der automatischen Textvereinfachung in der Zukunft vor. Vielen Dank für eure Aufmerksamkeit und wir hoffen, euch alle während der Konferenz zu treffen. Danke.</sample>
    <sample id="4">Der Referent heißt Kayo Yin.</sample>
    <sample id="5">Das T5 XL Modell wurde verwendet, um die Genauigkeit von 82–87 % zu erreichen.</sample>
    <sample id="6">**Abstract:**  
This work introduces "many-to-many summarization," a unified framework that integrates multilingual and cross-lingual summarization into a more general setting, enabling a single model to summarize documents in any source language into summaries in any target language. We propose PISCES, a pre-trained many-to-many summarization model, trained through a three-stage pre-training process: meta pre-training, cross-lingual pre-training, and task-specific pre-training. Our preliminary experiments on the WikiLingua dataset show that the many-to-many approach enhances cross-lingual knowledge transfer compared to traditional multilingual and cross-lingual summarization methods. Results demonstrate that PISCES outperforms strong baselines like mBART-50 and mT5. Ablation studies and human evaluations further confirm the effectiveness of our approach. This work represents a significant step toward more flexible and powerful multilingual summarization systems.</sample>
    <sample id="7">Ja, CoNLL-2003-Tagger funktionieren noch gut in 2023, wenn sie mit modernen Architekturen, größeren Modellen und ausreichend Feinabstimmungsbeispielen verwendet werden.</sample>
    <sample id="8">Die neue Methode, ABC-Eval, unterscheidet sich dadurch, dass sie das Verhalten der Chatmodelle explizit annotiert, anstatt nur allgemeine Bewertungen vorzunehmen. Sie misst spezifische Fehlerarten wie Irrelevanz, Widersprüche oder Halluzinationen, um eine feiner gestaltete und zuverlässige Bewertung der Gesprächsqualität zu ermöglichen.</sample>
    <sample id="9">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt von der Verwendung von sauber annotierten Validierungsdaten ab.</sample>
    <sample id="10">Das Ergebnis kann durch Zugang zu mehr oder besserer Hintergrundinformationen verbessert werden.</sample>
    <sample id="11">**Abstract:**  
In this work, we explore the ability of large language models to understand humor using data from *The New Yorker Caption Contest*. We operationalize humor understanding into three tasks: caption matching, quality ranking, and explanation generation. Our results show that even state-of-the-art models like CLIP (fine-tuned on our annotated dataset) achieve only 62% accuracy on caption matching, compared to 94% for humans. Similarly, GPT-4 performs significantly worse than humans on both matching and quality ranking tasks, even when provided with human-written image descriptions. In the explanation generation task, human explanations are preferred over GPT-4's outputs in over two-thirds of cases, highlighting gaps in the models' understanding of humor. We release a dataset with over 700 cartoons, annotated with captions, descriptions, and joke explanations, along with a leaderboard for future research. This work underscores the challenges in achieving true humor understanding in AI.</sample>
    <sample id="12">Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow – insgesamt 5 Autoren.</sample>
    <sample id="13">**Abstract:**  
This work investigates adaptive inference methods for reducing the computational cost of large language models, focusing on Multi Model and Early Exit approaches. While Multi Model offers versatility and better performance, it suffers from high storage and overhead costs. Early Exit provides faster inference but faces performance degradation due to conflicting gradients from multiple classifiers. We hypothesize that these conflicting gradients, where different classifiers update shared model weights, negatively impact overall performance. To address this, we introduce SWEET (Separating Weights in Early Exit Transformers), a novel fine-tuning method that trains each transformer layer with updates from only the subsequent classifier, eliminating conflicting gradients. Experiments on BERT-base and BERT-large show that SWEET significantly closes the performance gap between Early Exit and Multi Model, achieving better accuracy at fast inference speeds. Our results highlight the importance of addressing gradient conflicts in Early Exit architectures and provide a foundation for future research on adaptive inference methods.</sample>
    <sample id="14">Hallo, mein Name ist Adam Przepiórkowski und dieser Vortrag ist über die Abhängigkeitsstruktur von Koordinationen. Wie Sie vielleicht wissen, gibt es verschiedene Abhängigkeitsstrukturen, die von unterschiedlichen Theorien und Korpusansätzen angenommen werden. Zum Beispiel geht die Universal Dependencies davon aus, dass in der Koordination, wie in „Lisa, Bart und Maggie“, der erste Konjunkt der gesamten koordinierten Struktur als Kopf fungiert. In diesem Fall also Lisa. Ein ähnlicher Ansatz wird auch in Igor Mel'čuks Theorie der Bedeutungstexte angenommen, wo auch die gesamte koordinierte Struktur durch den ersten Konjunkt geleitet wird. Diese beiden Ansätze sind asymmetrisch. Sie heben einen der Konjunkte hervor. Dagegen gibt es auch symmetrische Ansätze zu koordinierten Strukturen, wie den Prager Ansatz. Der Prager Abhängigkeitskorpus geht von einer Konjunktionsstruktur aus, die durch die Konjunktion selbst geleitet wird. Somit erhalten wir Abhängigkeiten von der Konjunktion zu allen Konjunkten. Schließlich gibt es auch einen mehrfach geköpften Ansatz, wie ihn z. B. Hudsons Word Grammar verwendet, wo alle Konjunkte als Köpfe der koordinierten Struktur gelten. Somit erhalten wir Abhängigkeiten vom Regierenden zu allen Konjunkten einzeln: Lisa, Bart und Maggie. Ziel dieses Papers ist es, ein neues Argument für symmetrische Strukturen der Koordination, wie diese beiden, und gegen asymmetrische Strukturen der Koordination, wie diese beiden, zu liefern. Das Argument basiert auf dem Prinzip der Minimierung der Abhängigkeitslänge, das ich anhand dieser Beispiele erklären werde. In der englischen Sprache wissen Sie vielleicht, dass direkte Objekte sich lieber dem Verb nähern, während Adverbien weiter weg sein können. Also ist „Marge read it yesterday“ in Ordnung, weil das direkte Objekt dem Verb nahe ist, während „Marge read yesterday it“ deutlich schlechter klingt. Das liegt daran, dass zwischen Verb und direktem Objekt ein Adverb steht: „yesterday“. Dieser Effekt kann jedoch gemildert werden, wenn das direkte Objekt sehr lang und schwer ist. Dann kann es nach dem Adverb verschoben werden. Dies zeigt das Beispiel hier. Beide Sätze sind in Ordnung. „Marge read this absolutely fascinating book about bees yesterday.“ Es ist in Ordnung, wenn statt „it“ ein langes NP steht. Aber es ist auch in Ordnung zu sagen: „Marge read yesterday this absolutely fascinating book about bees.“ Die Begründung hier ist, dass dies möglich ist, obwohl die Satzstruktur das allgemeine grammatikalische Prinzip verletzt, dass direkte Objekte dem Verb nahe sein sollten, aber das Prinzip der Minimierung der Abhängigkeitslänge erfüllt wird, das besagt, dass kürzere Abhängigkeiten bevorzugt werden. Diese beiden Bäume zeigen nur die Länge der entscheidenden Abhängigkeiten, also diejenigen, die nicht zwischen diesen beiden Strukturen konstant sind. Hier haben wir eine Abhängigkeit von „read“ zum Adverb mit einer Länge von 7 Wörtern und eine Abhängigkeit von „read“ zum „book“ mit einer Länge von 4, also insgesamt 11. Wenn wir diese beiden Konstituenten vertauschen, wird die Summe dieser beiden Abhängigkeiten 6. Stattdessen von 11 auf 6, was viel kürzer ist. Deshalb klingt das ziemlich gut. Also verletzt es ein Prinzip, erfüllt aber ein anderes. Okay. Was wir getan haben, ist, verschiedene Statistiken zur Koordination aus der erweiterten Version des Penn Treebanks zu extrahieren und im Paper „Why wouldn't you use universal dependencies“ zu untersuchen. Diese Statistiken bestätigen eine frühere Beobachtung, dass linke Konjunkte tendenziell kürzer sind. Also „Salz und Pfeffer“ und nicht „Pfeffer und Salz“, gemessen in Silben. Auch eine Beobachtung, die in der Syntaxanalyse gemacht wurde, zeigt, dass diese Tendenz mit der Längendifferenz wächst. Wenn die Differenz zwischen den Längen der beiden Konjunkte größer wird, bevorzugt das kürzere Konjunkt die linke Position, stärker. Also wächst der Prozentsatz des linken, kürzeren Konjunkts. Was neu in diesem Paper ist, ist, dass wir festgestellt haben, dass diese Tendenz nur auftritt, wenn der Regierende auf der linken Seite oder gar nicht vorhanden ist. Also ist der Regierende auf der linken Seite in diesem Beispiel „I saw Bart and Lisa“. Er ist in dem zweiten Beispiel „Homer came and sneezed“ nicht vorhanden. Hier haben wir eine Koordination zweier Verben und es gibt keinen äußeren Regierenden. In solchen Fällen bevorzugt der linke Konjunkt die kürzere Länge; der größte Unterschied zwischen den beiden Konjunkten ist auf der linken Seite. Wenn der Regierende jedoch auf der rechten Seite ist, wie hier, „laughed“ regiert die Koordination „Ted and Ned“, verschwindet dieser Effekt. Wir haben dies anhand der Längenmessung in Zeichen, in Silben und in Wörtern gezeigt. Ich werde mich auf die rechte Spalte konzentrieren. Was wir hier sehen, ist, dass wenn der Regierende auf der linken Seite ist, die Tendenz, dass der linke Konjunkt kürzer ist, mit der absoluten Differenz in Wörtern stetig wächst, und dasselbe gilt, wenn kein Regierender vorhanden ist, wie bei der Koordination von Sätzen. Wenn der Regierende jedoch auf der rechten Seite ist, verschwindet diese Tendenz. Und wir zeigen im Paper, wie dies ein Argument gegen asymmetrische Strukturen der Koordination, wie diese beiden, und für symmetrische Strukturen, wie diese beiden, liefert. Lesen Sie das Paper für die vollständigen Argumente. Und sprechen Sie mit uns während der Poster-Session. Danke.</sample>
    <sample id="15">Drei Autoren sind an der Arbeit beteiligt: Matthias Lindemann, Alexander Koller und Ivan Titov.</sample>
    <sample id="16">Die Bibeltexte werden stärker vereinfacht als Nachrichtentexte oder Texte für Sprachlerner.</sample>
    <sample id="17">**Abstract**  
Multimodal relation extraction (MRE) aims to infer semantic relations between entities using both textual and visual information. However, existing methods often suffer from internal-information over-utilization and external-information under-exploitation. To address these issues, we propose a novel framework that simultaneously performs fine-grained information pruning and external context enrichment. Our approach leverages a graph information bottleneck principle to guide feature refinement in a unified cross-modal graph (CMG), constructed by merging textual and visual scene graphs. Additionally, we incorporate multimodal topic features to supplement contextual information. Experimental results on the MRE benchmark show that our method outperforms existing multimodal baselines. Ablation studies confirm the effectiveness of both internal screening and external enrichment. Further analysis reveals that internal screening is more critical for high cross-modal relevance inputs, while external information is more beneficial for low relevance cases. Our work introduces a novel strategy for balancing information subtraction and addition, achieving significant improvements in MRE performance.</sample>
    <sample id="18">Das Beispiel ist "salt and pepper" statt "pepper and salt".</sample>
    <sample id="19">**Abstract:**  
Zhang Qin, a master's student from Shenzhen University, presents a survey on efficient open-domain question answering (ODQA) accepted at ACL 2023. The work reviews mainstream frameworks, such as two-stage models combining retrieval and reading, and one-stage alternatives like retrieval-only or generator-only systems. Challenges include handling large Wikipedia corpora, high memory usage, and slow inference. The study summarizes efficient techniques to reduce index size, improve retrieval speed, and minimize model complexity, such as approximate nearest neighbor search, skip reading, and model compression. Comparative analysis shows retrieval-reader systems balance speed, memory, and performance, while generator-only systems offer fast inference at the cost of performance. The study concludes with insights on system design based on resource constraints and proposes future directions, including deployment on low-power devices and the need for more comprehensive evaluation metrics.</sample>
    <sample id="20">Ja, die Modelle können für Ihre Forschung verwendet werden, da sie unter der MIT-Lizenz frei auf Hugging Face verfügbar sind.</sample>
    <sample id="21">DEPLAIN-apa enthält Nachrichtentexte.</sample>
    <sample id="22">Eine gute Generalisierung wird durch drei Faktoren gefördert: das Modellarchitektur (insbesondere Transformer-Modelle), die Modellgröße (größere Modelle generalisieren besser) und die Anzahl der Feinabstimmungsbeispiele (mehr Beispiele führen zu besserer Generalisierung).</sample>
    <sample id="23">This paper investigates the challenges text-to-image models face in accurately rendering text within generated images. Using the Imagen model as a case study, we find that its reliance on the T5-XXL text encoder, which uses subword tokenization, leads to poor spelling accuracy—often below 70% even for large models. This is surprising given their strong performance in other NLP tasks. In contrast, models like ByT5, which process input at the byte-level, achieve near-perfect spelling accuracy across all scales. We show that word frequency affects T5's performance, as frequent words are often encoded as single subwords, making decomposition harder. To address this, we propose a simple and efficient solution: augmenting the text encoder with a ByT5-small model, which adds only 5% to the parameter count but significantly improves spelling accuracy. This enhancement leads to better text rendering in image generation. We also introduce two new benchmarks, WikiSpell for text-only models and DrawText for text-to-image models, to evaluate spelling and text rendering capabilities. Our approach offers a practical way to improve text rendering without requiring massive increases in model size.</sample>
    <sample id="24">Die Tendenz zu kürzeren linken Konjunktionen wurde anhand der Länge in Wörtern gemessen.</sample>
    <sample id="25">Die Experimente wurden durch statistische Analysen aus dem erweiterten Penn Treebank durchgeführt, wobei die Länge der Konjunkte in verschiedenen Maßeinheiten (Zeichen, Silben, Wörter) gemessen und verglichen wurde, abhängig davon, ob der Begrenzer auf der linken oder rechten Seite stand oder fehlte.</sample>
    <sample id="26">Ein Basisklassifikator, der mit unausgewogenen Daten trainiert wird, erkennt die seltene Klasse (Dissonanz) kaum besser als zufällig.</sample>
    <sample id="27">Der Text nennt nur den Namen Shangbin, daher ist nicht eindeutig, wie viele Autoren an der Arbeit beteiligt sind. Es wird nur ein Autor erwähnt.</sample>
    <sample id="28">Die Personen im Beispielgespräch heißen Bob und Alice.</sample>
    <sample id="29">Kontextsensitive MÜ-Modelle schneiden besser ab bei Diskursphänomenen wie Formalität und lexikalischer Kohäsion.</sample>
    <sample id="30">**Abstract**  
LLM-Blender is a simple yet effective ensemble learning framework for large language models (LLMs) that leverages pairwise ranking and generative fusion to improve model performance. Given an input, LLM-Blender runs multiple LLMs, ranks their outputs using a pairwise comparison module (PairRanker), and fuses the top candidates via a generative model (GenFuser). PairRanker encodes input-output pairs and uses cross-attention to compare candidates, outperforming individual scoring methods in ranking accuracy. Experiments on the newly created MixInstruct dataset show that LLM-Blender significantly outperforms top models like Vicuna and Open Assistant across multiple metrics, achieving better results in 68% to 76% of cases. The framework demonstrates that optimal model selection varies per input, and combining models through careful ranking and fusion leads to improved performance. LLM-Blender is released with a unified codebase for future research.</sample>
    <sample id="31">Die Autoren gehören der University of California, San Diego an.</sample>
    <sample id="33">Das Framework quantifiziert die Positionalität durch die Korrelation der Annotationen nach Demografie mit den Vorhersagen von Modellen und Datensätzen, wobei der Pearson's R Korrelationskoeffizient verwendet wird.</sample>
    <sample id="34">**Abstract:**  
In this work, we introduce CREST, a joint framework for rationalization and counterfactual text generation. CREST combines selective rationalization, which highlights relevant input tokens, with counterfactual generation, which edits input to produce alternative scenarios. By integrating both approaches, CREST generates valid, fluent, and diverse counterfactuals that align with human causal reasoning. Human evaluation shows that CREST outperforms existing methods in terms of validity and naturalness. Furthermore, we demonstrate that CREST counterfactuals improve downstream models through data augmentation and rationalization. Our experiments on IMDB and SNLI datasets show that CREST-Rationalization achieves state-of-the-art performance, particularly on out-of-domain data. We also evaluate the interpretability of CREST rationales, finding them more plausible and simulable than alternatives. Overall, CREST provides a controllable and effective approach for generating explanations and counterfactuals that enhance model interpretability and robustness.</sample>
    <sample id="36">This work introduces Language-Specific Layers (LSLs) for multilingual machine translation to increase language-specific model capacity without increasing inference costs. LSLs allow the model to select and use only the relevant sublayer for a given source or target language during inference. The placement of LSLs is learned automatically by training a large model with shared, source, and target weights per encoder layer, then selecting the best architecture based on the largest weight. Experiments on WMT21 and Flores-101 show that the learned architecture significantly outperforms baselines and language adapters, especially for low-resource languages. The approach maintains fast inference and achieves statistically significant improvements in translation quality across 90 language pairs. Results highlight the effectiveness of LSLs in enhancing multilingual translation performance while keeping computational costs constant.</sample>
    <sample id="37">Die vorherige Studie zeigte, dass menschliche Teilnehmende ebenfalls rassische Stereotype in ihren Antworten widerspiegelten, was eine direkte Vergleichbarkeit mit den von LLMs generierten Personas ermöglicht.</sample>
    <sample id="38">In dieser Studie wurden Daten aus der erweiterten Version des Penn Treebanks verwendet.</sample>
    <sample id="39">Nur ein Autor ist an der Arbeit beteiligt: Adam Przepiórkowski.</sample>
    <sample id="40">Eng verwandte Aufgaben für kognitive Dissonanz sind die Stance-Klassifikation in Debatten (Agreement/Disagreement) und die Klassifikation von Expansion- und Vergleichsbeziehungen im PDTB (CE).</sample>
    <sample id="41">**Abstract:**  
This paper introduces PeaCoK, a large-scale Persona Commonsense Knowledge Graph designed to enhance coherent and engaging narrative generation by modeling real-world personas and their interconnections. PeaCoK contains 3,800 personas with 40,000 attributes and 100,000 inferred facts, capturing rich world knowledge through three dimensions of relations. Built using commonsense graphs, pre-trained language models, and human-AI annotations, PeaCoK achieves 87% F1 accuracy in relation annotation. We demonstrate its utility by training a BART-based generator, which outperforms large language models in attribute inference tasks. Furthermore, integrating PeaCoK into dialogue systems improves fluency, consistency, engagement, and persona expression in conversations. Human evaluations show that PeaCoK-augmented models perform better than those using general knowledge graphs, especially when speakers share common attributes. Our results highlight the importance of persona-centric knowledge for narrative modeling, offering a valuable resource for training lightweight models with strong knowledge generation capabilities.</sample>
    <sample id="42">Die Anzahl der Autoren wird im Text nicht genannt.</sample>
    <sample id="43">Die Anzahl der Autoren wird nicht genannt.</sample>
    <sample id="44">Das vorgestellte Framework unterscheidet sich dadurch, dass es Endnutzerannotationen mit Modellen und Datensätzen vergleicht, anstatt sich nur auf Annotatorübereinstimmung oder -verteilungen zu konzentrieren.</sample>
    <sample id="45">Die generierten Personas haben die meisten Überschneidungen mit dem Lexikon der Stereotypen.</sample>
    <sample id="46">DeepL und Google Translate wurden verglichen.</sample>
    <sample id="47">Hallo, ich bin Shangbin, Promotionsstudent an der University of Washington. Heute präsentiere ich unsere Arbeit „From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models“. Sprachmodelle werden auf großen Mengen an Web-Crawl-Daten trainiert. Politische Nachrichtenmedien sind in ihren Trainingsdaten gut vertreten. Laut einer Umfrage des C4-Korpus können wir erkennen, dass das New York Times, Los Angeles Times, The Guardian, Huffington Post und andere gut in den Trainingsdaten von Sprachmodellen vertreten sind. Dies hat sowohl Vorteile als auch Nachteile für die Anwendungen von Sprachmodellen geschaffen. Einerseits können sie aus verschiedenen Perspektiven lernen, was Demokratie und die Vielfalt von Ideen feiert. Andererseits sind diese unterschiedlichen politischen Meinungen sozial voreingenommen und könnten zu Fairnessproblemen in downstream-Anwendungen führen. Aus diesem Grund schlagen wir vor, den Verlauf der politischen Voreingenommenheit von den Trainingsdaten über Sprachmodelle bis zu downstream-Aufgaben zu untersuchen, insbesondere indem wir uns folgende Fragen stellen: Erstens, wie können wir die politische Ausrichtung von Sprachmodellen bewerten und welche Rolle spielen die Trainingsdaten dabei? Zweitens, wie performen Sprachmodelle mit unterschiedlichen politischen Ausrichtungen bei downstream-Aufgaben und führt dies zu Fairnessproblemen in NLP-Anwendungen? Konkret haben wir zunächst vorgeschlagen, Sprachmodelle mit unterschiedlichen Prompt-Formaten anhand politischer Fragebögen wie dem „political conference test“ zu evaluieren. Dies gewährleistet, dass unsere automatische Evaluation gut in der politischen Wissenschaft verankert ist. Einige vorläufige Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Ausrichtungen haben und alle vier Quadranten auf dem politischen Spektrum einnehmen. Wir können auch erkennen, dass GPT-4 das liberaleste Sprachmodell unter ihnen ist und die GPT-Reihe insgesamt sozial-liberaler als die BART-Reihe und ihre Varianten ist. Zweitens möchten wir untersuchen, inwieweit die politischen Voreingenommenheiten von Sprachmodellen tatsächlich aus den Trainingsdaten stammen. Dazu können wir ein kontrolliertes Experiment durchführen, indem wir Sprachmodell-Checkpoints weiter auf sechs unterschiedlichen parteiischen Corpora trainieren, die in Nachrichten und soziale Medien unterteilt sind, und diese wiederum nach ihrer politischen Ausrichtung unterteilen. Durch das Weitertrainieren von Sprachmodellen auf solchen parteiischen Corpora können wir erkennen, dass sich auch die ideologischen Koordinaten der Sprachmodelle entsprechend verschieben. Zum Beispiel zeigt ein RoBERTa-Modell, das auf einem linken Reddit-Korpus weiter trainiert wurde, eine erhebliche liberale Verschiebung seiner politischen Ausrichtung. Wir untersuchen zudem, ob Sprachmodelle die Polarisierung erkennen können, die in unserer heutigen Gesellschaft verbreitet ist. Dazu teilen wir die Trainingsdaten in zwei zeitliche Corpora ein: vor und nach dem 45. Präsidenten der Vereinigten Staaten. Wir trainieren Sprachmodelle jeweils separat auf diese beiden zeitlichen Corpora. Wir können erkennen, dass Sprachmodelle nach 2017 tendenziell weiter von der Mitte entfernt sind. Dies deutet darauf hin, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft erkennen können. Schließlich evaluieren wir Sprachmodelle mit unterschiedlichen politischen Ausrichtungen auf Aufgaben wie Hassrede-Erkennung und Fakenews-Erkennung, die oft Sprachmodelle verwenden und erhebliche Auswirkungen haben können. Wenn wir die Leistung pro Kategorie untersuchen, also die Leistung in verschiedene Demografiegruppen oder politische Ausrichtungen von Nachrichtenmedien unterteilen, können wir ein Muster erkennen. Zum Beispiel sind linkseitig ausgerichtete Sprachmodelle besser darin, Hassrede gegen soziale Minderheiten zu erkennen, jedoch schlechter in der Erkennung von Hassrede gegen mächtigere Gruppen in der Gesellschaft. Umgekehrt sind rechtsorientierte Sprachmodelle besser darin, Hassrede gegen Weiße und Männer zu erkennen, jedoch schlechter in der Erkennung von Hassrede gegen Afroamerikaner, LGBTQ+ und andere Minderheitsgruppen. Ähnliche Trends finden wir auch bei der Fakenews-Erkennung, wo wir erkennen, dass linkseitig ausgerichtete Sprachmodelle besser in der Erkennung von Fehlinformationen aus der entgegengesetzten politischen Richtung sind und umgekehrt. Wir zeigen auch zahlreiche qualitative Beispiele, die zeigen, dass Sprachmodelle mit unterschiedlichen politischen Ausrichtungen unterschiedliche Vorhersagen für Beispiele von Hassrede und Falschinformationen machen, basierend auf ihren sozialen Kategorien. Im Anhang sind noch mehr Beispiele enthalten, die verdeutlichen, dass es ein dringendes Fairnessproblem gibt, das durch die politischen Voreingenommenheiten der Sprachmodelle verursacht wird. Beispielsweise könnte es passieren, dass rechtsorientierte Sprachmodelle, die auf Hassrede oder Falschinformationen weiter trainiert und auf einer beliebten sozialen Medienplattform eingesetzt werden, Menschen mit entgegengesetzten politischen Auffassungen marginalisiert und Hassrede gegen Minderheitsgruppen unkontrolliert verbreitet werden. Dies hat uns gewarnt, uns der Fairnessprobleme bewusst zu werden, die durch die politischen Voreingenommenheiten von Sprachmodellen verursacht werden. Ein kurzer Diskussionspunkt: Wir möchten auch hervorheben, dass wir einzigartige Dilemmata bezüglich der politischen Voreingenommenheiten von Sprachmodellen aufzeigen. Es ist wie zwischen Skilla und Charybdis. Wenn wir politische Meinungen in den Trainingsdaten nicht sanieren, würde die Voreingenommenheit von den Trainingsdaten über Sprachmodelle bis zu downstream-Aufgaben weitergeleitet und schließlich zu Fairnessproblemen führen. Wenn wir versuchen, sie irgendwie zu sanieren, riskieren wir Zensur oder Ausschluss. Es ist unglaublich schwierig zu entscheiden, was tatsächlich neutral ist und welche Sprachdaten weiterhin in der Überwachung behalten werden sollten. Es ist etwas wie das elektrische Trolley-Problem. Okay, großartig. Ich glaube, das war alles für heute. Vielen Dank für Ihre Zeit.</sample>
    <sample id="48">David Vilar erwähnt, dass die Arbeit gemeinsam mit seinen Kollegen von Google Translate erstellt wurde. Die genaue Anzahl der Autoren wird jedoch nicht genannt.</sample>
    <sample id="49">Die MPP-Auswertungen wurden bis zu einer Kontextlänge von 1024 Token durchgeführt.</sample>
    <sample id="50">**Abstract:**  
The presentation introduces DEPLAIN, a new German text simplification corpus at the document and sentence level, designed to improve text comprehension for specific target groups. DEPLAIN consists of two subcorpora: DEPLAIN-apa, with 13,000 manually aligned sentence pairs from news texts, and DEPLAIN-web, containing 30,450 pairs from diverse domains, aligned both manually and automatically. The corpus exhibits varied simplification techniques, with stronger simplifications in Bible texts compared to news or learner texts. The corpus is used for evaluating automatic alignment methods, where MASSalign was identified as the most effective for German text simplification. Additionally, it supports training language models for automatic text simplification, with fine-tuned models achieving better-than-baseline results. DEPLAIN provides a valuable benchmark for future research in text simplification.</sample>
    <sample id="51">Die Domänen, die in den Datensatz aufgenommen wurden, sind Musik, Bücher und Rezepte.</sample>
    <sample id="52">Positionalität ist die Perspektive, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen besitzen.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">**Abstract:**  
This paper presents a novel approach to detecting cognitive dissonance in language using transfer learning and active learning to address the challenge of rare-class annotation. Cognitive dissonance, the inconsistency between beliefs or actions, is rare in discourse but crucial for understanding decision-making, mental health, and societal polarization. We created a large-scale annotated dataset, where dissonance occurred in only 3.5% of annotated pairs. Due to the scarcity of dissonance examples, we employed transfer learning from related tasks—stance classification and PDTB discourse relations—to initialize a classifier. Using active learning, we introduced the Probability-of-Rare-Class (PRC) strategy to prioritize dissonant examples, achieving an AUC of 0.75, the best performance on the task. Cumulative model updates outperformed iterative ones, and PRC proved more effective than other state-of-the-art strategies, despite annotator challenges. Our findings highlight the effectiveness of transfer learning and PRC for rare-class detection in NLP.</sample>
    <sample id="55">Ja, EDAtt passt zu einem bestehenden Offline-ST-Modell.</sample>
    <sample id="56">Die Anzahl der Autoren wird im Text nicht genannt.</sample>
    <sample id="57">Nein, das getestete Modell funktioniert nicht gut in der Testsuite, ohne spezifische Trainingsdaten aus KITMUS.</sample>
    <sample id="58">Die drei Varianten von KITMUS sind: "Background-Pretrain", "Background-Both" und "Background-Inference".</sample>
    <sample id="59">**Abstract:**  
This work introduces DrBERT, the first pre-trained biomedical language model in French, based on RoBERTa and trained on the NACHOS dataset of crawled medical data. We explore the impact of data sources and pre-training strategies by comparing DrBERT with other models, including ChuBERT (trained on clinical data) and continual pre-training variants based on CamemBERT and PubMedBERT. Our evaluation on 11 downstream tasks in French, such as named entity recognition and question answering, shows that models trained on domain-specific data perform best on related tasks, while heterogeneous data improve versatility. From-scratch pre-training on NACHOS outperforms continual pre-training approaches. DrBERT achieves superior results on nine tasks compared to generic models like CamemBERT. Although specialized data improves performance, scalability remains a challenge. All models are available on Hugging Face under the MIT license, with training scripts on GitHub. This study contributes a valuable resource for French biomedical NLP research and applications.</sample>
    <sample id="60">Die Autoren gehören der University of Wisconsin-Madison an.</sample>
    <sample id="61">Die abschließende Forschungsfrage ist: "Should we only use the clean samples for validation, or there are better ways to utilize them?"</sample>
    <sample id="62">**Abstract:**  
This paper presents a systematic study on knowledge distillation for Natural Language Generation (NLG) with a focus on practical, industry-driven scenarios. While large language models excel in NLG tasks, their size and computational cost limit real-world deployment. To address this, we explore model compression techniques, particularly knowledge distillation, to maintain performance while reducing model size. Unlike prior work that often focuses on classification or single NLG tasks with extensive labeled data, our study considers realistic setups with limited labeled data, abundant unlabeled data, and medium-sized models. We evaluate four NLG tasks—summarization, question generation, common sense reasoning, and simplification/style transfer—using a 1:4 labeled-to-unlabeled data ratio. The study investigates architectural choices, pruning effects, and knowledge distillation methods. We propose novel approaches, including generating multiple pseudo-targets and using sampling instead of beam search to enhance diversity. Additionally, we introduce *joint-teaching*, a technique that combines word-level distillation from both teacher and student pseudo-targets to reduce exposure bias and improve learning. Our findings provide a comprehensive recipe for effective NLG model compression, balancing efficiency and performance.</sample>
    <sample id="63">Die Sensitivitätsmetrik misst, wie konsistent ein Modell auf dieselbe Aufgabe reagiert, unabhängig von geringen Unterschieden in der Formulierung der Anweisung.</sample>
    <sample id="64">Der Referent heißt Jingwei Yi.</sample>
    <sample id="65">Das Gegenteil ist der Fall. Eine höhere Sensitivität bedeutet, dass das Modell weniger konsistent ist und daher eine schlechtere Leistung hat.</sample>
    <sample id="66">**Abstract**  
This paper reviews recent advances in deep learning for mathematical reasoning, covering both text-based and multimodal tasks such as solving math word problems, geometric reasoning, and automated theorem proving. Mathematical reasoning involves understanding numerical data, applying logical rules, and generating proofs, making it a key challenge in AI and NLP. Deep learning approaches, including sequence-to-sequence models and tree-based architectures, have been proposed to formalize these tasks. Pre-trained language models (LLMs) show promise but face limitations in precise reasoning. Techniques like chain-of-thought prompting and self-consistency decoding improve performance, while program-aided models enhance complex problem-solving. Despite progress, challenges remain, including generalization, robustness, and handling low-resource languages. Recent efforts have expanded datasets beyond English, covering financial, scientific, and medical domains. This survey highlights current methodologies, challenges, and future directions in deep learning for mathematical reasoning.</sample>
    <sample id="67">This work investigates interference in multilingual translation models, where training on one language pair can either improve or harm performance on another. We identify that severe interference occurs when the model is underpowered relative to the data size, and that tuning the sampling temperature is crucial for strong performance. While language similarity and the number of languages are often considered factors, our experiments show that they have limited impact. Using four Transformer variants and 15 WMT languages, we find that interference decreases with larger models and sufficient data. For low-resource language pairs, temperature sampling (T &gt; 1) helps balance training focus. Our results suggest that interference can be effectively mitigated through model scaling and calibrated temperature settings, without the need for specialized algorithms. Thus, the key to reducing interference lies in model and data size, along with properly tuned temperature.</sample>
    <sample id="68">Die Modelle erhalten während des Pre-Trainings einen breiten linguistischen Kontext, der syntaktische und semantische Merkmale umfasst, die über verschiedene Sätze hinweg gemeinsam sind.</sample>
    <sample id="69">Normalerweise werden etwa 20 saubere Validierungsbeispiele pro Klasse benötigt, um eine gute Leistung in der WSL zu erzielen.</sample>
    <sample id="70">Die Autoren gehören der Stanford University an.</sample>
    <sample id="71">**Abstract:**  
This work introduces the AltEntities Corpus, a large-scale dataset for resolving indirect referring expressions in entity selection across three domains: music, books, and recipes. The dataset contains 6,000 alternative questions paired with 42,000 indirect referring expressions, collected through crowd-sourced annotations in a cartoon-based setup. Each entry includes a context, an alternative question (e.g., "Do you mean A or B?"), and an indirect reference (e.g., "the newer one"). Annotations are informed by background knowledge provided via Google links or Wikipedia texts. Experiments with the T5 XL model show that performance varies significantly based on the model's access to background knowledge: 92–95% accuracy with full knowledge, 82–87% with partial access, and only 60% with entity names alone. The dataset provides a benchmark for evaluating language models' ability to understand and resolve indirect references in natural conversations.</sample>
    <sample id="72">Es ist notwendig, neue Methoden zur Messung von Medienverzerrungen zu entwickeln, weil bestehende Ansätze nicht in der Lage sind, die politischen Verzerrungen in Sprachmodellen adäquat zu erfassen und deren Auswirkungen auf faire NLP-Anwendungen zu bewerten.</sample>
    <sample id="73">Akshatha</sample>
    <sample id="74">**Abstract**  
This paper introduces Dense-ATOMIC, a densely-connected extension of the ATOMIC commonsense knowledge base, addressing its limited multi-hop paths and incomplete link types. By normalizing tail events and employing the Rel-CSKGC model, which leverages RoBERTa and MaxPooling for relation prediction without relying on graph structure, we infer missing B-to-A, A-to-B, and A-to-A links. Our approach uses intra- and inter-cluster completion strategies to efficiently generate new triplets. Evaluation shows that Dense-ATOMIC significantly improves knowledge coverage with more 1-hop, 2-hop, and 3-hop paths and enhances commonsense reasoning performance, as demonstrated by improved COMET results and diverse multi-hop path generation. The proposed method outperforms existing relation prediction and translation-based approaches in both automatic and human evaluations, showcasing the potential of Dense-ATOMIC for advanced commonsense reasoning tasks.</sample>
    <sample id="75">**Abstract**  
This paper introduces JointProp, a joint semi-supervised learning framework for Named Entity Recognition (NER) and Relation Extraction (RE). While supervised models require extensive labeled data, semi-supervised approaches reduce annotation costs but often neglect the interdependencies between NER and RE. JointProp addresses this by modeling the tasks on a heterogeneous graph, propagating labels across both labeled and unlabeled data to capture intra- and inter-task relationships. The framework consists of span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. Label propagation refines pseudo-labels iteratively, leveraging high-density regions in the graph. Experimental results on four datasets show that JointProp significantly outperforms baseline models in both joint and single-task settings, demonstrating the benefits of integrating NER and RE through semi-supervised learning. The approach effectively exploits label correlations and improves performance with minimal labeled data.</sample>
    <sample id="76">Die Pipeline für die Verbreitung politischer Vorurteile beginnt mit der Pretraining-Daten, geht über die Sprachmodelle und endet bei den Downstream-Aufgaben. Politische Vorurteile in den Trainingsdaten werden in die Sprachmodelle übertragen und führen zu unfairer Leistung in Anwendungen wie Hassrede- und Fakenews-Erkennung.</sample>
    <sample id="77">This work introduces DeFacto, a new dataset for improving factual consistency in summarization, created through human demonstrations and feedback. Developed by Yale University and Microsoft Research, the dataset includes 2.5K annotated examples from the XSum corpus, with 70% containing factual errors. Human annotators provided labels, corrected summaries, and detailed feedback, including explanations, instructions, and supporting evidence. The study proposes three NLG tasks: summary editing, feedback generation, and automatic factual error correction. While fine-tuned and zero-shot models perform well in editing, feedback generation remains challenging. Editor models achieve strong results with less training data, especially when generating explanations. The dataset's fine-grained annotations support training factuality metrics and meta-evaluation. DeFacto is publicly available on GitHub, offering a valuable resource for enhancing summarization factuality.</sample>
    <sample id="78">Ja, der Vereinfachungsprozess unterscheidet sich: DEPLAIN-apa hat mehr Reihenfolgeänderungen und Wortergänzungen, während DEPLAIN-web häufiger Umschreibungen enthält.</sample>
    <sample id="79">Ja, CoScript ist öffentlich verfügbar.</sample>
    <sample id="80">Das Wasserzeichen wird eingebettet, indem bei jeder Anfrage die Anzahl der Triggerwörter gezählt und das Ergebnis als Gewicht für eine Ziel-Embedding-Summe verwendet wird. Wenn die Anzahl der Triggerwörter einen Schwellenwert überschreitet, wird das Ziel-Embedding direkt zurückgegeben.</sample>
    <sample id="81">Die Autoren gehören der Penn State University an.</sample>
    <sample id="82">**Abstract**  
This paper introduces ULRA, a novel framework for unsupervised automated essay scoring (AES) that leverages multiple heuristic quality signals as pseudo-groundtruth supervision. Traditional unsupervised AES approaches, such as clustering-based methods or single-signal weak supervision, suffer from poor performance due to limited or inconsistent signals. To address this, ULRA introduces a heuristic essay ranking module (HER) that generates partial-order pairs by ranking essays based on multiple quality signals, such as term uniqueness and word count. These pairs are then aggregated using a Deep Pairwise Rank Aggregation (DPRA) module, which incorporates a learnable confidence weight for each signal to resolve inconsistencies and provide unified supervision. Additionally, a scoring strategy transforms predicted scores into a predefined range. Experiments on both transductive and inductive settings show that ULRA significantly outperforms existing unsupervised baselines and achieves competitive results compared to cross-prompt and one-shot methods, though it still lags behind fully supervised approaches due to the lack of strong supervision. The proposed method demonstrates the effectiveness of aggregating multiple heuristic signals for robust unsupervised AES.</sample>
    <sample id="83">Ja.</sample>
    <sample id="84">**Abstract:**  
This paper introduces PAD-Net, an efficient framework for dynamic networks that addresses the issue of excessive parameter usage in fully dynamic models. Traditional networks use static parameters, while dynamic networks adapt their architecture or parameters based on input. However, fully dynamic networks often lead to significant parameter growth, as seen when replacing BERT-Base feed-forward layers with Mixture of Experts. To mitigate this, PAD-Net partitions parameters into static and dynamic components, reducing redundancy while maintaining performance. The framework employs Iterative Mode Partition to identify and convert redundant dynamic parameters into static ones, minimizing loss impact. Experiments show that PAD-Net outperforms both static and fully dynamic networks in terms of accuracy, while using fewer parameters and computations. Ablation studies highlight the importance of dynamic ratios and scale factors in balancing performance and efficiency. Compared to pruning methods, PAD-Net achieves better results by preserving static parameters. Future work includes extending the approach to other architectures and hardware-friendly structures, as well as exploring combinations of zero elements, static, and dynamic parameters.</sample>
    <sample id="85">Ein Beispiel für eingeschränkte Sprachplanung ist das Planen eines spezifischen Ziels wie "eine Schokoladentorte backen", das zusätzliche Einschränkungen enthält, im Gegensatz zu einem abstrakten Ziel wie einfach "eine Torte backen".</sample>
    <sample id="86">Sie stellen die Opazität ihrer Methode sicher, indem sie die Embeddings visuell auf vier Datensätzen mittels PCA darstellen und zeigen, dass die Backdoor-Embeddings und normale Embeddings schwer voneinander zu unterscheiden sind.</sample>
    <sample id="87">Die Arbeit nutzt bestehende PLMs wie CamemBERT und PubMedBERT als Ausgangspunkt für das kontinuierliche Vortraining auf französischen medizinischen Daten.</sample>
    <sample id="88">GPT-4 ist am wenigsten auf nicht-binäre Menschen ausgerichtet.</sample>
    <sample id="89">Der Beispielsatz lautet: "I'm going to talk about..."</sample>
    <sample id="90">**Abstract:**  
This paper investigates whether language learners can effectively contribute to NLP data annotation, challenging the traditional reliance on native speakers. We conducted a proof-of-concept study involving 120 annotated samples across three languages (English, Korean, Indonesian) and four GLUE benchmark tasks. Language learners were categorized into three proficiency levels using the CFR criteria and compared with native speakers. Participants completed pre- and post-tests to assess language proficiency and learning effects. Results show that learners' annotations are nearly as accurate as native speakers', especially for simpler tasks, and majority voting improves their performance. Training simulations demonstrated that models using learners' annotations achieved up to 95% of ground truth performance, sometimes surpassing native-labeled models. Additionally, annotation tasks improved learners’ language skills. This study suggests a novel, scalable approach for data annotation in low-resource languages, reducing reliance on native speakers and enabling broader NLP research.</sample>
    <sample id="91">Die Leistung des Modells verbessert sich mit zunehmender Anzahl der Aufgaben.</sample>
    <sample id="92">Die Autoren vergleichen ihre Methode mit drei baumlosen Baselines: seq2seq, CopyNet und Transformer.</sample>
    <sample id="93">Die beiden Co-Autoren sind die Berater (Advisors) des ersten Autors.</sample>
    <sample id="94">**Abstract:**  
This paper introduces *Embedding Marker*, a backdoor-based watermarking method designed to protect the copyright of embedding-as-a-service (EaaS) models. As large language models like GPT and LLAMA are increasingly used for embedding services, the risk of model theft through embedding outputs has grown. To address this, Embedding Marker injects a covert watermark into embeddings by leveraging a trigger set of moderately frequent words. During watermark injection, the provider blends a target embedding with the original embedding based on the number of triggers in a query, ensuring the watermark remains imperceptible and does not degrade utility. For copyright verification, the method compares embeddings from the suspect service against the target embedding using cosine and L2 similarity, along with a KS test. Experiments on datasets like AG News and Enron Spam show high detection accuracy with minimal impact on downstream tasks. Visual analysis confirms the watermark's stealthiness. Embedding Marker offers a practical, transferable, and effective solution for securing EaaS models against unauthorized replication.</sample>
    <sample id="95">Der erste Autor von PaLM wird in dem Text nicht genannt.</sample>
    <sample id="96">Hallo alle miteinander. Ich bin Jenny, eine erste Jahr PhD-Studentin an der Carnegie Mellon University, und heute werde ich die Arbeit NLPositionality präsentieren, die sich mit der Charakterisierung von Designbias in Datensätzen und Modellen beschäftigt. Diese Arbeit wurde in Zusammenarbeit mit einigen Kollegen von der University of Washington und dem Allen Institute for AI durchgeführt, nämlich Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap. Fangen wir also an, indem wir uns vorstellen, dass wir für eine Zeitung arbeiten und Kommentare unter einem Nachrichtenartikel durchsuchen, um giftigen Inhalt zu entfernen. Wir könnten uns dann einem beliebten API wie dem Prospective API für Toxizitätsdetektion zuwenden. Dies funktioniert sehr gut, wenn man Carl Jones ist, bei dem das Prospective API die toxischen Instanzen richtig erkennt. Das ist jedoch nicht der Fall bei Aditya Sharma, bei dem das Prospective API nicht so empfindlich auf offensive Begriffe reagiert, die in indischen Kontexten häufiger vorkommen. Dies ist ein Beispiel für einen Designbias, bei dem wir systematische Leistungsunterschiede von Technologien zwischen Bevölkerungsgruppen beobachten. Solche Designbias wie das oben genannte können aufgrund der Positionalität von NLP-Forschern und Modellentwicklern entstehen. Positionalität ist einfach die Perspektiven, die Menschen haben, als Ergebnis ihrer Demografie, Identität und Lebenserfahrungen. Dies ist ein Begriff, der weit verbreitet in kritischen Studien, insbesondere in feministischen und queer akademischen Räumen, verwendet wird. Als Forscher kann die Positionalität den Forschungsprozess und dessen Ergebnisse beeinflussen, da sie die Entscheidungen der Forscher verändern kann. Eine Frage, die man sich stellen könnte, lautet: Haben Datensätze und Modelle eine Positionalität? Wir sagen nicht, dass Modelle oder Datensätze selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen echter Menschen und können somit bestimmte Positionalitäten über andere darstellen. Frühere Arbeiten haben anekdotische Beweise für Positionalität vorgeschlagen, wie kulturelle Lücken in Modellen und Datensätzen sowie theoretische Definitionen von Modell-Positionalität. Diese Arbeiten untersuchen jedoch nicht wirklich den Vergleich zwischen Endnutzern und Datensätzen oder Modellen selbst. Das Studieren der Positionalität von Modellen und Datensätzen wird zunehmend wichtig, da NLP-Aufgaben subjektiver und sozialer werden, und es schwierig ist, zu charakterisieren, wie diese Positionalitäten verzerrt sind, da nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter APIs verborgen sind. Um die Positionalität von Datensätzen und Modellen zu untersuchen, vergleichen wir tatsächlich die Annotationen echter Nutzer mit bestehenden Datensätzen und Modellen. Wir tun dies durch unser Framework NLPositionality. Unser Framework arbeitet in zwei Hauptschritten. Der erste Schritt besteht darin, Datensätze mit diversen Annotatoren erneut zu annotieren. Dabei müssen wir uns bewusst sein, die Demografie der ursprünglichen Annotatoren der Datensätze zu betrachten, da normalerweise nur wenige Annotatoren pro Instanz annotieren und die Demografie selten gesammelt und geteilt wird. Daher entscheiden wir uns, die Daten erneut zu annotieren, um viele Annotatoren pro Instanz zu erhalten und eine reiche Menge an demografischen Daten zu sammeln. Wir vergleichen anschließend diese Annotationen nach demografischen Kriterien mit Modellen und Datensätzen mithilfe eines Pearson-R-Korrelationskoeffizienten. Unser Framework unterscheidet sich also von der Literatur zur Annotator-Disagreement, indem wir Endnutzer mit Modellen und Datensätzen, Vorhersagen und Labels vergleichen, anstatt nur auf Annotator-Übereinstimmung oder Modellierung von Annotator-Verteilungen zu achten. Unser Framework ist hauptsächlich durch Lab in the Wild und eine Online-Crowdsourcing-Plattform ermöglicht, die von HCI-Kollegen genutzt wird. Lab in the Wild ist eine Online-Experimentierplattform, mit der wir diverse Freiwillige rekrutieren können. Im Vergleich zu Plattformen wie MTurk, die hauptsächlich Teilnehmer aus den USA oder Indien haben, ermöglicht Lab in the Wild weiterhin die Erhebung hoher Qualität Daten. Wir haben auf Lab in the Wild zwei Aufgaben platziert, eine davon ist die soziale Akzeptabilität. Dabei lesen die Teilnehmer eine Situation aus dem Social Chemistry Datensatz und schreiben dann, wie sozial akzeptabel diese Situation ist. Danach können sie, um im Studium engagiert zu bleiben, ihre Antworten mit denen eines KI-Systems und anderen vergleichen. Wir haben anschließend diese Annotationen mit Social Chemistry, Delphi und GPT 4 verglichen. Wir haben dann eine sehr ähnliche Aufgabenstruktur für die Toxizitäts- und Hassrede-Erkennung reproduziert, bei der die Teilnehmer eine Instanz aus Dynahate lesen und anschließend feststellen, ob sie dies als eine Instanz von Hassrede betrachten. Wir verglichen diese Annotationen mit Dynahate, dem Prospective API, dem Rewire API, Hate Roberta und GPT 4. Unser Studium sammelte letztendlich über 16.000 Annotationen von über 1000 Annotatoren aus 87 Ländern. Jetzt sind wir besser ausgestattet, um die Frage zu beantworten, mit wem NLP-Datensätze und Modelle am besten übereinstimmen. Wir stellen fest, dass es Positionalität in NLP gibt. Zum Beispiel stellen wir fest, dass Datensätze und Modelle am besten mit englischsprachigen Ländern übereinstimmen. Bei der sozialen Akzeptabilität von GPT 4 finden wir heraus, dass es am besten mit konfuzianischen und englischsprachigen Ländern übereinstimmt. Wir stellen auch fest, dass Dynahate am besten mit englischsprachigen Ländern übereinstimmt. Wir finden zudem eine weitere Übereinstimmung mit Menschen, die eine Hochschulbildung haben. Bei GPT 4 in der sozialen Akzeptabilitätsaufgabe finden wir heraus, dass es am besten mit Personen übereinstimmt, die eine Hochschul- oder Graduate School-Bildung haben, und wir finden dasselbe für Dynahate, das am besten mit Personen übereinstimmt, die eine Hochschulbildung haben. Allerdings, wenn Modelle und Datensätze mit bestimmten Bevölkerungsgruppen übereinstimmen, bleiben einige unweigerlich zurück. Ein Beispiel dafür ist, dass Datensätze und Modelle weniger mit nicht-binären Personen übereinstimmen als mit ihren männlichen und weiblichen Gegenstücken. Wir finden dies sowohl in der sozialen Akzeptabilitätsaufgabe von GPT 4 als auch in der Analyse der Dynahate-Aufgabe. Angesichts der Tatsache, dass es Positionalität in NLP gibt, was können wir also tun? Wir haben einige Empfehlungen hierzu. Erstens ist es wichtig, alle relevanten Designentscheidungen während des gesamten Forschungsprozesses zu dokumentieren. Zweitens ist es ratsam, NLP-Forschung mit dem Blickwinkel der Perspektivität durchzuführen. Unsere dritte Empfehlung ist, spezialisierte Datensätze und Modelle innerhalb von vier spezifischen Communities zu erstellen. Ein gutes Beispiel dafür ist das Masakhani-Initiativ. Wir möchten betonen, dass inklusives NLP nicht einfach bedeutet, dass alle Technologien für jeden funktionieren. Damit endet meine Präsentation. Wenn Sie mehr erfahren möchten, können Sie gerne unsere Dashboard für die aktuellsten Analyseergebnisse und unser Paper besuchen. Vielen Dank.</sample>
    <sample id="97">Die Referentin geht auf drei Probleme von SimulST ein.</sample>
    <sample id="98">Soziale und politische Verzerrungen in NLP-Modellen können nicht vollständig eliminiert werden, da die Sanierung von Trainingsdaten zwischen Zensur und Ausgrenzung schwierig abwägt. Eine mögliche Strategie besteht darin, diverse und ausgewogene Datensätze zu verwenden und die Verzerrungen bewusst zu analysieren und zu kompensieren.</sample>
    <sample id="99">Hallo, ich bin Siyu Yuan von der Fudan University. Ich möchte unsere Arbeit „Distilling Script Knowledge from Large Language Models for Constrained Language Planning“ vorstellen. Im Alltag planen Menschen oft ihre Handlungen, indem sie Schritt-für-Schritt-Anweisungen in Form von zielorientierten Skripten folgen. Frühere Arbeiten haben Sprachmodelle genutzt, um für abstrakte Ziele stereotypischer Aktivitäten wie „Kuchen backen“ zu planen und zeigten, dass große Sprachmodelle Ziele effektiv in Schritte zerlegen können. Allerdings konzentrierte sich die frühere Forschung hauptsächlich auf das Planen von abstrakten Zielen stereotypischer Aktivitäten. Das Planen von Zielen mit spezifischen Einschränkungen, wie z. B. „einen Schokoladenkuchen backen“, bleibt jedoch noch weitgehend unerforscht.

In diesem Paper definieren wir das Problem der eingeschränkten Sprachplanung, bei dem unterschiedliche Einschränkungen auf die Planungsziele angewendet werden. Ein abstraktes Ziel kann von verschiedenen realen, spezifischen Zielen mit mehrfachen Einschränkungen geerbt werden. Ein guter Planer sollte Skripte erstellen, die sinnvoll und den Einschränkungen treu sind. In diesem Paper bewerten wir zunächst die Fähigkeit großer Sprachmodelle zur eingeschränkten Sprachplanung und verbessern sie. Da keine Datenbank spezifischer Ziele existiert, um unsere Studie zu unterstützen, müssen wir diese Ziele zunächst erwerben. Wie in der Tabelle gezeigt, erweitern wir die abstrakten Ziele mithilfe von InstructGPT mit mehrfachen Einschränkungen, um eine menschliche Datenakquise zu ermöglichen. Wir sammeln 100 spezifische Ziele und bewerten die von Sprachmodellen generierten Skripte. Diese Tabelle berichtet über die Gesamtgenauigkeit der Ergebnisse. Wir stellen fest, dass alle Sprachmodelle bei der Planung spezifischer Ziele unzufriedenstellende Ergebnisse liefern.

Dann führen wir eine detaillierte Analyse durch, um zu untersuchen, warum die Lernmodelle scheitern. Die Ergebnisse in der Abbildung zeigen, dass die semantische Vollständigkeit der generierten Skripte akzeptabel ist, aber die Treue zu den Einschränkungen nicht gewährleistet werden kann. Wir untersuchen detaillierter die in wikiHow definierten Kategorien von Einschränkungen. Das Wärmekartendiagramm in der Abbildung zeigt, dass die Planungsleistung der InstructGPTs je nach Kategorie der Ziele stark variiert. Frühere Studien haben gezeigt, dass die Ausgabegüte von Sprachmodellen stark variiert und dadurch schlechte Leistungen erzielt wird. Daher setzen wir das Konzept „übererzeugen und dann filtern“ ein, um die Generationsqualität zu verbessern. Zuerst zeigen wir InstructGPT die Arten von Einschränkungen mit Beispielen und erhalten spezifische Ziele auf der Grundlage der Seed-Abstraktziele. Anschließend generiert InstructGPT K Skripte für spezifische Ziele. Dann entwickeln wir ein Filtermodell, um die treuen Skripte auszuwählen. Wir konvertieren Skripte und Ziele in InstructGPT-Embeddings und berechnen die Kosinusähnlichkeit als Ähnlichkeitswerte, um die semantische Ähnlichkeit zu messen. Darüber hinaus belohnen wir Skripte, die Schlüsselwörter der Ziel-Einschränkung enthalten. Wir behalten ein Skript nur dann, wenn es in der Zielmenge den höchsten Wert erzielt.

Mit unserem Verfahren kann InstructGPT Skripte höherer Qualität generieren. Unser Verfahren verbessert die Planungsfähigkeit sowohl in der semantischen Vollständigkeit als auch in der Treue zu den Einschränkungen. Da große Sprachmodelle teuer in der Anwendung sind, ist es entscheidend, die Sprachplanungsfähigkeit kleinerer, spezialisierter Modelle zu ermöglichen. Die Erstellung einer Datenbank ist ein entscheidender Schritt hierfür. Allerdings können frühere Studien keine Planung für spezifische Ziele ermöglichen und manuelle Datenbankannotation ist kostspielig. Daher folgen wir dem Ansatz symbolischer Wissensdistillation, um Datenbanken zur eingeschränkten Sprachplanung aus großen Sprachmodellen zu extrahieren. Wir wenden unser Verfahren an, um eine Datenbank für eingeschränkte Sprachplanung, genannt CoScript, zu erstellen. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten. Um die Qualität der Validierungs- und Testdaten sicherzustellen, bitten wir Crowdsourcing-Arbeiter, falsche Beispiele zu finden und zu korrigieren. Diese Abbildung zeigt die Einschränkungsverteilung von CoScript. Wir stellen fest, dass CoScript eine hohe Vielfalt in den generierten spezifischen Zielen aufweist. Mit CoScript können wir kleinere, aber spezialisierte Modelle für die eingeschränkte Sprachplanung testen. Wir stellen fest, dass T5, das auf CoScript feinabgestimmt wurde, Skripte höherer Qualität generiert als die meisten großen Sprachmodelle, was zeigt, dass kleinere Modelle größere Modelle übertreffen können, wenn sie richtig auf geeignete Datensätze trainiert werden.

Zusammenfassend haben wir das Problem der eingeschränkten Sprachplanung etabliert. Wir haben die Fähigkeit großer Sprachmodelle zur eingeschränkten Sprachplanung bewertet und entwickeln eine Methode zur „übererzeugen-und-dann-filtern“ für große Sprachmodelle. Wir nutzen große Sprachmodelle, um eine hochwertige Skriptdatenbank, CoScript, für die eingeschränkte Sprachplanung zu generieren. Wir hoffen, dass die CoScript-Datenbank ein wertvolles Ressourcenmaterial für die Forschung zur Sprachplanung sein wird. Vielen Dank für Ihre Aufmerksamkeit. Weitere Details zu CoScript finden Sie in unserem Paper.</sample>
    <sample id="100">Multi-hop Frage-Antwort-Systeme erfordern oft mehrere Dokumente, um eine Frage zu beantworten. PromptRank ist ein dateneffizienter Ansatz, der mit nur 128 Beispielen gute Leistungen erzielt. Er kombiniert TF-IDF-Retrieval und eine few-shot Sprachmodell-basierte Re-Ranking-Strategie. Dabei wird die Wahrscheinlichkeit der Frage unter Berücksichtigung der Ketten-Dokumente als Bewertungsfunktion genutzt. Die Ketten werden in Prompts kodiert, und Anweisungen werden verwendet, um das Schlussfolgern des Modells zu fördern. Experimente zeigen, dass PromptRank staatliche Systeme wie DrKit übertreffen und sich mit modernen Retrievers vergleichen kann. Die Ergebnisse deuten darauf hin, dass die Wahl der Anweisung und die Likelihood-basierte Bewertung entscheidend für die Leistung sind. PromptRank erzielt gute Ergebnisse in der nachgelagerten QA-Performance und unterliegt MDR nur geringfügig. Dies zeigt, dass Sprachmodelle effektiv für Few-Shot-Ranking in der Multi-Hop-QA genutzt werden können.</sample>
    <sample id="101">Die Sprachgewandtheit von PaLM ist vergleichbar mit den besten Systemen der Branche.</sample>
    <sample id="102">Die wichtigsten Eigenschaften eines Wasserzeichenverfahrens sind: Anwendbarkeit auf Embedding-Dienste, keine Beeinträchtigung der Nutzbarkeit der Embeddings, ausreichende Verstecktheit und Übertragbarkeit auf den Angreiferdienst.</sample>
    <sample id="103">Die englischen TED Talks wurden in 14 verschiedene Sprachen übersetzt, jedoch werden die spezifischen Sprachen im Text nicht genannt.</sample>
    <sample id="104">Die Anzahl der aus einem Datensatz extrahierten Instanzen für die erneute Annotierung wird im Text nicht genannt.</sample>
    <sample id="105">Die Distanzmetriken, die verwendet werden, sind Kosinusähnlichkeit und L2-Norm (euklidische Distanz).</sample>
    <sample id="106">**Abstract:**  
This paper introduces QUEST, a retrieval dataset designed to address information needs expressed through implicit set constraints. Using scenarios like a zoologist identifying a reptile or a reader seeking specific books, we highlight how users often formulate queries involving set operations such as intersection and complement. QUEST contains over 3,000 entity-seeking queries derived from Wikipedia categories, with human-annotated paraphrased queries, verified answer entities, and document spans attributed to specific query constraints. The dataset challenges retrieval systems to find multi-answer sets where evidence for different constraints may be distributed across a document. Evaluation shows that existing retrieval and reranking systems perform poorly, especially on queries involving set intersections and differences. QUEST provides a benchmark for improving systems handling selective information needs, offering opportunities for future research in natural language understanding and information retrieval.</sample>
    <sample id="107">Modelle, die auf einem mehrsprachigen Encoder basieren, wurden verwendet, um ein einziges mehrsprachiges Modell für alle Sprachen zu trainieren, das dann bei der Inferenz sowohl deutsche als auch chinesische Abfragen übersetzen kann.</sample>
    <sample id="108">This paper investigates how language models perform on acceptability judgments when presented with longer context sequences, revisiting the minimal pair paradigm (MPP) used in datasets like BLiMP and SyntaxGym. While current MPP evaluations focus on short, isolated sentences, this study extends the paradigm to longer sequences by adding context prefixes from the same or different datasets, or even unrelated domains like Wikipedia. Results show that MPP judgments remain stable with irrelevant contexts, but are significantly influenced by context from the same dataset, especially when the prefix matches the syntactic phenomenon under evaluation. This effect grows with increasing context length, suggesting that models’ acceptability judgments are sensitive to shared syntactic and semantic features across sentences. The findings highlight the limitations of current MPP evaluations in capturing models’ abstract knowledge over extended contexts and emphasize the need for more comprehensive assessment methods as language models continue to support longer context windows.</sample>
    <sample id="109">**Abstract:**  
This work introduces *Unnatural Instructions*, a large-scale dataset of natural language instructions, inputs, and outputs generated automatically without human labor. By prompting a pre-trained language model (e.g., GPT-3) with examples from the Super-Natural Instructions dataset, the model generates novel instruction-response pairs. The dataset is further diversified through automatic paraphrasing, resulting in 64,000 primary examples and 240,000 total examples when including paraphrases. Analysis shows that over 50% of generated examples are correct, with even incorrect examples offering useful training signals. The dataset includes highly creative and diverse tasks, such as verifying scientific experiments or inventing new words. Fine-tuning an 11-billion-parameter T5 model on Unnatural Instructions outperforms models trained on Super-Natural Instructions and other benchmarks like T0++ and Tk-instruct. The results highlight the potential of language models to generate high-quality, diverse instruction data efficiently and cost-effectively, surpassing the limitations of human annotators.</sample>
    <sample id="111">Die Autoren entscheiden, was Wörter mit mittlerer Häufigkeit sind, indem sie einen allgemeinen Textkorpus sammeln und die Wortfrequenzen darin zählen.</sample>
    <sample id="112">Hallo zusammen, mein Name ist Shuheng. Heute möchte ich unsere Arbeit präsentieren: „Funktionieren die Named-Entity-Tagger von CoNLL-2003 immer noch gut im Jahr 2023?“ Los geht’s. In unserem Paper untersuchen wir das Problem der Generalisierung im Rahmen der Named-Entity-Recognition (NER) Aufgabe. Wir beobachten, dass Modelle über fast 20 Jahre in CoNLL-2003 zur Entwicklung von NER verwendet wurden, was mehrere Probleme aufwirft. Erstens: Können diese Modelle auf moderne Daten generalisieren? Und wenn wir neue Tagger entwickeln, was ist für eine gute Generalisierung notwendig? Gleichzeitig: Wenn wir eine schlechte Generalisierung beobachten, was verursacht dann den Leistungsverlust dieser Modelle? Um diese Fragen zu untersuchen, haben wir das CoNLL++ Datenset entwickelt. Dieses Datenset haben wir aus Reuters News von 2020 gesammelt und mit den gleichen Annotationen wie im CoNLL-2003 annotiert. Danach haben wir über 20 Modelle an CoNLL-2003 feinabgestimmt und sie sowohl auf den CoNLL-03 Testdaten als auch auf dem CoNLL++ bewertet. Zuletzt haben wir den prozentualen F1-Änderungsgrad berechnet, um die Generalisierung jedes Modells zu beurteilen. Was ist für eine gute Generalisierung erforderlich? Während der Experimente haben wir festgestellt, dass drei Hauptelemente erforderlich sind. Das erste ist die Modellarchitektur. Durch unsere Experimente haben wir festgestellt, dass Transformer-Modelle normalerweise besser auf neue Daten generalisieren. Das zweite Element ist die Modellgröße. Wir fanden heraus, dass größere Modelle normalerweise zu einer besseren Generalisierung führen. Und schließlich wissen wir alle, dass die Anzahl der Feinabstimmungsbeispiele den Leistungsgrad einer nachgelagerten Aufgabe direkt beeinflusst. Hier haben wir ebenfalls festgestellt, dass mehr Feinabstimmungsbeispiele tatsächlich zu einer besseren Generalisierung führen. Zu unserer nächsten Frage: Was verursacht den Leistungsverlust einiger Modelle? Wir hatten zwei Hypothesen. Die erste Hypothese ist die adaptive Überanpassung, also die Überanpassung, die entsteht, wenn dasselbe Testset immer wieder verwendet wird, und dies zeigt sich in der Regel als abnehmender Nutzen auf einem neuen Testset. Die zweite Hypothese ist der zeitliche Drift, also der Leistungsverlust, der durch den zunehmenden zeitlichen Abstand zwischen Trainings- und Testdaten verursacht wird. Für die Überanpassung haben wir im Diagramm auf der rechten Seite gesehen, dass die rote beste Anpassungslinie eine Steigung hat, die größer als 1 ist. Das bedeutet, dass jede Verbesserungseinheit, die wir auf CoNLL-2003 erzielen, sich in mehr als eine Verbesserungseinheit auf CoNLL++ übersetzt. Das bedeutet, dass es keinen abnehmenden Nutzen gibt und zeigt uns, dass in diesem Fall keine adaptive Überanpassung beobachtet wird. Was ist mit dem zeitlichen Drift? Für den zeitlichen Drift haben wir ein Experiment durchgeführt, bei dem wir einige Modelle mit kürzlichem Datenmaterial erneut trainiert oder weiter trainiert haben. Dabei haben wir festgestellt, dass die Leistung mit einem größeren zeitlichen Abstand abnimmt und dies bestätigt unsere Hypothese, dass der Hauptgrund für den Leistungsverlust der zeitliche Drift ist. Unsere Schlussfolgerung lautet: Für eine gute Generalisierung benötigen wir eine bessere Modellarchitektur, eine größere Modellgröße sowie mehr Feinabstimmungsbeispiele. Diese drei Faktoren hängen eng zusammen – wir können nicht einfach nur eines dieser Elemente haben und die anderen weglassen. Gleichzeitig haben wir auch festgestellt, dass der Leistungsverlust hier durch den zeitlichen Drift verursacht wird. Und überraschend ist, dass er nicht durch adaptive Überanpassung verursacht wird, obwohl CoNLL-2003 bereits seit über 20 Jahren genutzt wird. Zurück zu der Frage, die wir in der Überschrift unseres Papers gestellt haben: „Funktionieren die CoNLL-2003 Tagger immer noch gut im Jahr 2023?“ Und wir haben festgestellt, dass die Antwort tatsächlich ein klares „Ja“ ist. Wir hoffen, dass unser Paper weitere Forschung zur Verbesserung der Generalisierungsfähigkeit von Modellen inspiriert. Schließlich, schaut unbedingt in unser Paper und unser Datenset, und wenn ihr Fragen habt, zögert nicht, mich zu kontaktieren. Vielen Dank.</sample>
    <sample id="114">**Abstract:**  
This work introduces "Finding the Pillars of Strength for Multi-Head Attention," presented at ACL 2023, focusing on optimizing the parameter redundancy in large language models (LLMs). While LLMs are revolutionary, they suffer from excessive parameters, long training times, and high data requirements. We address the parameter-heavy issue by proposing a grouped head attention mechanism with two stages: group-constrained training and the Voting-to-Stay algorithm. The first stage clusters attention heads into groups, promoting intra-group similarity and inter-group diversity. The second stage prunes redundant heads, retaining only one per group. Our models, GHT and GHT-PS, achieve significant parameter compression (up to 32.1%) while maintaining performance on tasks like machine translation, language modeling, and abstractive summarization. The LITE model further demonstrates 90% parameter pruning, 62% faster inference, and 80% fewer FLOPs. Future work explores task-specific pruning inspired by the Lottery Ticket Hypothesis, enabling efficient, application-tailored LLMs.</sample>
    <sample id="115">Bei dem Ansatz wird die Sprachsegmentgröße durch den Parameter lambda bestimmt, der die Anzahl der letzten Sprachframes angibt, auf die sich die Aufmerksamkeit konzentrieren muss, bevor eine Übersetzung ausgestoßen wird.</sample>
    <sample id="116">Das entitätsspezifische Wissen, das im Beispiel mit Servin und Kea benötigt wird, ist: "Servin ist ein Richter."</sample>
    <sample id="117">Der wichtigste Faktor ist die Qualität des Beispiels.</sample>
    <sample id="118">**Abstract**  
This paper introduces *SwitchMLM*, a novel masked language modeling (MLM) approach tailored for code-switched natural language processing (NLP). Code-switching, common in multilingual communities like India, poses challenges for existing multilingual models such as mBERT and XLM-R. To address this, we propose *SwitchMLM*, which focuses on switch-points—language transitions in code-switched text—and restricts masking to these positions. When LID-tagged data is unavailable, we introduce *FrequencyMLM*, a surrogate method that uses monolingual corpora to infer language identities. Additionally, we enhance model architecture with residual connections from intermediate layers rich in switch-point information to the final layer, and introduce an auxiliary LID-based loss to encourage language-aware representations. Our experiments on sentiment analysis show that combining *SwitchMLM* or *FrequencyMLM* with ResBERT and the auxiliary loss achieves state-of-the-art results across multiple language pairs. Probing experiments confirm that our methods increase switch-point information in model representations, validating our approach's effectiveness in capturing code-switching patterns.</sample>
    <sample id="119">Die Arbeiten in den erweiterten Experimenten konzentrieren sich auf RoBERTa und andere Modelle der BART-Serie sowie die GPT-Serie.</sample>
    <sample id="120">Das Modell verwendet Aufmerksamkeitswerte aus einer bestimmten Ebene.</sample>
    <sample id="121">Direkte Inferenzen sind Beispiele wie das Nennen des Namens eines Objekts, z. B. "Easy on Me", oder das Verweisen auf die Position, z. B. "das erste".</sample>
    <sample id="122">Die Autoren gehören der Fudan University an.</sample>
    <sample id="123">**Abstract:**  
This work introduces MultiInstruct, the first large-scale multi-modal instruction tuning benchmark comprising 62 diverse tasks across 10 categories, derived from 21 open-source datasets. Each task includes five expert-written instructions, addressing the lack of multi-modal instructional data. Using OFA, a unified multi-modal pre-trained model, we investigate instruction tuning for zero-shot generalization to unseen multi-modal tasks. Our experiments show that instruction tuning significantly improves performance on seen tasks and enhances transferability. We also introduce a new metric, sensitivity, to measure the model’s consistency across instruction variations. Transfer learning from natural instruction datasets further boosts performance and reduces sensitivity. Our results demonstrate the effectiveness of instruction tuning in multi-modal settings and highlight the benefits of diverse instruction templates. We also present plans to expand MultiInstruct with 150 additional vision-language tasks. This work paves the way for more efficient and versatile multi-modal zero-shot learning.</sample>
    <sample id="124">This work presents TempReason, a comprehensive benchmark for evaluating and enhancing the temporal reasoning capabilities of large language models (LLMs). We categorize temporal reasoning into three levels: time-to-time, time-to-event, and event-to-event. Previous studies have overemphasized the second level, but our dataset covers all three with extensive temporal coverage. We introduce a new QA setting called Reasoning QA, where models are provided with temporal knowledge and must reason to find answers. To improve temporal reasoning, we propose a training strategy combining temporal span extraction pre-training and time-sensitive reinforcement learning. Our experiments show that while models like ChatGPT struggle with temporal tasks, especially in month prediction and across different time periods, our proposed model, TempT5, significantly outperforms existing models on TempReason, especially in open-book and reasoning QA settings. This study highlights the need for more balanced training data and better temporal reasoning strategies in LLMs.</sample>
    <sample id="125">Die Anzahl der Autoren wird im Text nicht genannt.</sample>
    <sample id="126">Ja.</sample>
    <sample id="127">**Abstract:**  
This paper introduces "Large Language Models Are Reasoning Teachers," a method that transfers reasoning capabilities from large language models to smaller, more deployable ones. While chain-of-thought prompting enables large models to solve complex tasks, it is computationally expensive and limited to very large models. To address this, we propose using large models as "reasoning teachers" to generate step-by-step solutions, which are then used to fine-tune smaller student models. Our key contribution is the introduction of *Diverse Reasoning*, a novel technique that generates multiple reasoning paths for each question via stochastic sampling, enhancing training diversity and student performance. Experiments on 12 tasks show that our method significantly outperforms baselines, with performance improvements up to 55% on Multi Arithmetic. Even small models (0.3B parameters) achieve notable results, demonstrating the effectiveness and scalability of our approach. The method balances development and inference costs, offering a practical pathway for deploying reasoning capabilities in smaller models. The paper provides extensive analysis, code, and data, encouraging further research and application.</sample>
    <sample id="128">**Abstract:**  
In this work, we introduce the KITMUS test suite to evaluate models' ability to integrate knowledge from multiple sources in natural language understanding tasks. We focus on coreference resolution, requiring both pretrain-time background knowledge and inference-time entity-specific knowledge. Our dataset includes scenarios where knowledge is available in one or multiple sources, simulating different settings: "Background-Pretrain," "Background-Both," and "Background-Inference." Human and model evaluations show that without task-specific training, most models fail to perform well on KITMUS, relying instead on surface cues. While training on KITMUS improves performance, even top models struggle to reliably integrate inference-only background knowledge. This highlights the challenge of knowledge integration from diverse sources and the need for further research in this area. The dataset and code are available on GitHub.</sample>
    <sample id="129">Ein Beispiel für eine markierte Gruppe, das die Autoren gegeben haben, ist die "Strong Black Women"-Archetyp.</sample>
    <sample id="130">Modellarchitekturen, die nicht auf Transformer basieren, generalisieren nicht gut.</sample>
    <sample id="131">Die Testdatensätze werden als "clean test sets" bezeichnet.</sample>
    <sample id="132">Zwei Autoren sind an der Arbeit beteiligt: Akshatha und Martin.</sample>
    <sample id="133">Die Autoren arbeiten mit mehreren Modalitäten.</sample>
    <sample id="135">**Abstract:**  
In this work, we introduce ABC-Eval, a novel dimensional approach for evaluating conversational AI models. Unlike traditional human evaluations that focus on holistic quality, ABC-Eval assesses specific behavioral aspects of chat responses, such as relevance, consistency, empathy, and factual accuracy. By explicitly annotating whether responses exhibit errors like contradictions, hallucinations, or irrelevant content, ABC-Eval reduces subjectivity and provides a more precise evaluation. We compared ABC-Eval with existing methods, including Likert ratings and pairwise comparisons, across 100 conversations per model for four state-of-the-art chatbots. Our results show that ABC-Eval achieves higher inter-annotator agreement and better predictive power for overall conversation quality. It captures distinct aspects of chat quality, explaining over 25% of quality variance when combined, compared to significantly less for traditional methods. Despite the progress in conversational AI, current models still exhibit notable errors, such as common-sense violations and contradictions. ABC-Eval offers a reliable, fine-grained evaluation framework, enabling more accurate model comparisons and driving future advancements in the field.</sample>
    <sample id="136">**Abstract:**  
This work introduces FERMAT, an alternative evaluation framework for assessing numerical reasoning in language models, addressing the limitations of existing benchmarks that rely solely on accuracy scores. Motivated by the poor performance of models, especially smaller ones, on real-world numerical tasks like fact-checking, FERMAT focuses on number understanding, mathematical operations, and training dependency. The dataset, derived from CommonCore and Illinois, includes varied numerical representations (integers, decimals) and operation types. Baseline evaluations reveal significant shortcomings in models’ numerical reasoning, even when fine-tuned with teacher-generated templates. Training dependency analysis shows that models do not reliably memorize expressions, highlighting the importance of linguistic context. Further experiments demonstrate that increasing language and mathematical diversity in training—via additional templates from GSM8K and AQUA—significantly improves performance. FERMAT provides a more nuanced evaluation of models’ mathematical capabilities, emphasizing the need for diverse and representative benchmarks.</sample>
    <sample id="137">**Abstract**  
This paper introduces *Tell2Design*, a large-scale dataset for language-guided floor plan generation, addressing the need for AI systems that can translate natural language instructions into structured design layouts. Unlike text-to-image models focused on artistic outputs, *Tell2Design* emphasizes generating floor plans that comply with semantic, geometric, and topological constraints specified in user instructions. The dataset includes 5,051 human-annotated and 76,000 artificially generated instructions, paired with real-world floor plans. We propose a sequence-to-sequence model under the encoder-decoder framework, using a pre-trained T5 model to understand instructions and generate structured room bounding boxes. Our approach outperforms text-conditional image generation baselines, achieving a Micro IoU of 54 and a Macro IoU of 53 on the test set. The study highlights challenges such as handling ambiguous instructions and bridging the language distribution gap between human and artificial data. Results demonstrate that combining both data types improves performance, suggesting mutual benefits during training. This work lays the foundation for future research on language-guided design generation, particularly in the floor plan domain.</sample>
    <sample id="138">Die Integration von Wissen aus verschiedenen Quellen (vor dem Training und während der Inferenz) in NLU-Modellen.</sample>
    <sample id="139">Die Referenten heißen Ying und Zhiyang.</sample>
    <sample id="140">Ja, Coscript durchlief eine Qualitätskontrolle durch Crowdsourcing-Arbeiter, die fehlerhafte Beispiele fanden und korrigierten.</sample>
    <sample id="141">Bestehende Ressourcen für kontextbasierte Übersetzung sind auf begrenzte Typen von Kontextabhängigkeit und begrenzte Sprachmengen beschränkt, da sie oft auf domain-spezifischem Wissen und menschlicher Curatorship basieren.</sample>
    <sample id="142">Hallo! Ich möchte über unsere Arbeit zum Thema „Auflösen indirekter Bezugspassagen zur Entitätswahl“ sprechen, bei der wir den AltEntities-Korpus eingeführt haben. Mein Name ist Javad Hosseini und dies ist eine gemeinsame Arbeit mit Filip Radlinski, Silvia Pareti und Annie Louis. Unser Ziel ist es, das Sprachverhalten der Nutzer zu verstehen, wenn sie eine Wahl treffen möchten. Betrachten Sie diese alternative Frage: „Meinst du 'Easy on Me' oder 'I Gotta Feeling'?“ Hier möchte der Nutzer zwischen diesen beiden Liedern wählen. Die offensichtlichste Methode ist die direkte Referenz, beispielsweise durch das Nennen des Liednamens „Easy on Me“ oder durch die Angabe der Position, „das erste“. Aber manchmal ist eine indirekte Referenz angemessener, um einen natürlicheren Dialog zu ermöglichen. Dies kann geschehen, wenn der Nutzer den Namen des Liedes nicht mehr weiß, wenn die Aussprache der Lieder zu ähnlich ist und dadurch schwer zu unterscheiden ist, oder wenn der Nutzer eine Präferenz ausdrücken möchte. Hier sind einige Beispiele für indirekte Referenzen, wie zum Beispiel „das neuere“ oder „das Lied, das nicht energiegeladen ist.“ Dies ist ein wichtiges Problem in konversationellen Systemen und auch für die Bewertung von LLMs (Large Language Models) in Bezug auf das Verständnis von Entitäten. Wir sind uns nicht bewusst, dass es einen größeren öffentlichen Datensatz für diese Aufgabe gibt, deshalb haben wir einen solchen mithilfe von Crowdsourcing erstellt. Unser Datensatz umfasst drei verschiedene Bereiche: Musik, Bücher und Rezepte. Unsere Datensammlungsmethode betont die Alltagssprache mithilfe eines Cartoons, in dem eine Dialogsituation dargestellt wird. Der Cartoon besteht aus drei Sprechblasen. In der ersten Sprechblase sagt Bob: „Erinnerst du dich an das Lied, das wir gestern gehört haben?“ Damit legt Bob den Dialogkontext fest. In der zweiten Sprechblase sagt Alice: „Meinst du 'Easy on Me' oder 'I Gotta Feeling'?“ Das ist die alternative Frage. Und in der dritten Sprechblase verwendet Bob eine indirekte Referenz, um eine der Entitäten auszuwählen, beispielsweise: „das neuere.“ Wir füllen die ersten beiden Sprechblasen automatisch, während die dritte vom Annotator ausgefüllt wird. Die erste Sprechblase wird aus einigen manuell erstellten Prompts pro Bereich gewählt. Die zweite Sprechblase, also die alternative Frage, wird wie folgt generiert: Wir verwenden immer eine einfache Vorlage: „Meinst du A oder B?“ wobei A und B Beispiele aus Wikipedia sind. Hier sind die verschiedenen Samplingmethoden, die wir verwendet haben. Je weiter oben wir in der Liste sind, desto ähnlicher sind die Entitäten zueinander, und desto schwieriger ist es in der Regel, sie zu unterscheiden. Die erste Methode ist eine zufällige Auswahl. Die zweite Methode ist, wenn die Entitäten ähnliche Titel haben, beispielsweise zwei Bücher mit dem Namen „The Return“. Die dritte Methode ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben. Und schließlich, wenn sie ähnliche Info-Boxen oder Attribute auf Wikipedia haben, beispielsweise dieselbe Genre oder denselben Künstler für ein Lied. Wenn wir diese alternative Frage den Annotatoren zeigen, wissen sie zwar die Namen der Entitäten, aber nicht unbedingt etwas über die Entitäten selbst. Daher zeigen wir den Annotatoren einige Hintergrundinformationen zu den beiden Entitäten. Bei Liedern zeigen wir einfach einen Google-Suchlink zu jedem Lied und bitten die Annotatoren, mindestens einige Teile jedes Liedes anzuhören und über jedes Lied zu lesen. Hier ist beispielsweise das Google-Suchergebnis für das Lied „Easy on Me“. Für die Bereiche Rezepte und Bücher zeigen wir einige Hintergrundtexte aus Wikipedia. Bei Rezepten zeigen wir zusätzlich Bilder aus Wikipedia, damit die Annotatoren wissen, wie die Rezepte aussehen. Anschließend bitten wir die Annotatoren, eine der Entitäten auszuwählen, beispielsweise die erste, und sie mit drei bis fünf indirekten Bezugspassagen zu beschreiben. Ein Beispiel könnte sein: „diejenige ohne Worte“, „nicht die mit dem 12-jährigen Jungen“ oder „die fiktive“, „die aus Aserbaidschan kommt“ usw. Der AltEntities-Korpus enthält 6.000 alternative Fragen über drei Bereiche und umfasst 42.000 indirekte Bezugspassagen. Die Ergebnisse mit dem T5 XL-Modell sind unten zusammengefasst. Wenn das Sprachmodell Zugriff auf dieselben Hintergrundinformationen wie die Annotatoren hat, ist die Genauigkeit sehr hoch, etwa 92 bis 95 %. Das ist jedoch nicht realistisch. Wenn das Sprachmodell Zugriff auf teilweise überlappendes Hintergrundwissen hat, ist die Genauigkeit zwischen 82 und 87 %, was realistischer ist. Ein Beispiel dafür ist, wenn das Sprachmodell das Hintergrundwissen abruft. Wenn das Sprachmodell nur Zugriff auf Entitätsnamen hat, ist die Genauigkeit nur 60 %, was bedeutet, dass viel Raum für Verbesserungen besteht. Wir haben außerdem gezeigt, dass die Modelle allgemein auf verschiedene Bereiche anwendbar sind. Hier ist ein Link zu unserem Datensatz. Vielen Dank.</sample>
    <sample id="143">Der Ansatz wird mit den bestehenden SimulST-Richtlinien Wait-k und Local Agreement verglichen.</sample>
    <sample id="144">Die Autoren gehören der Universität Nantes an.</sample>
    <sample id="145">Der Referent heißt Jenny.</sample>
    <sample id="146">**Abstract:**  
Dialogue summarization, though advanced with pre-trained language models, still suffers from significant omission errors, leading to incomplete summaries. This paper systematically analyzes the omission problem across five domains and six models, revealing that up to 70% of summaries omit critical information. Omitted content is randomly distributed, highlighting the challenge of identifying key utterances. To address this, we introduce the OLDS dataset, the first high-quality resource for omission detection in dialogue summarization. The dataset includes multiple candidate summaries and omission labels generated through automatic and human evaluation. We explore three baseline models for omission detection, achieving an average F1-score of around 50%, indicating the task's difficulty. Furthermore, we propose a post-editing method that incorporates detected omissions into the refinement process, significantly improving summary quality. This work underscores the importance of omission detection as a crucial step toward enhancing the accuracy and completeness of dialogue summarization.</sample>
    <sample id="147">Die Arbeit ist an drei Autoren beteiligt: Myra, Esin Durmus und Dan Jurafsky.</sample>
    <sample id="148">Hallo, ich bin Sara Papi von der Universität Trento und der Fondazione Bruno Kessler. Ich werde das Papier „Attention as a Guide for Simultaneous Speech Translation“ kurz vorstellen, das ein gemeinsames Werk mit Matteo Negri und Marco Turchi ist.

Was ist simultane Sprachübersetzung? Simultane Sprachübersetzung, auch SimulST genannt, ist der Prozess, bei dem gesprochene Sprache in Echtzeit in Text einer anderen Sprache übersetzt wird, wodurch Kommunikation über Sprachgrenzen ermöglicht wird. Welche Probleme haben aktuelle SimulST-Modelle? Typischerweise werden spezifische Architekturen trainiert, wobei zusätzliche Module optimiert werden müssen. Längere und kompliziertere Trainingsprozesse, beispielsweise das Training mit unterschiedlichen Optimierungszielen, und das Training und Unterhalten mehrerer Modelle, um verschiedene Latenzregime zu erreichen. Zum Beispiel ein Modell mit einer durchschnittlichen Latenz von einer Sekunde und ein weiteres mit zwei Sekunden, und so weiter.

Was ist unsere Lösung? Zunächst verwenden wir bestehende Offline-ST-Modelle, ohne sie erneut zu trainieren oder spezifische Architekturen für SimulST zu verwenden. Wir verwenden nur ein Modell pro Latenzregime und behandeln die Latenz durch spezifische Parameter. Wir nutzen das bereits erlernte Wissen des Modells über die Aufmerksamkeitsmechanismen zwischen Audiowiedergabe und Textausgabe, genauer gesagt den Kreuz-Aufmerksamkeitsmechanismus, den Sie rechts sehen können. Unsere Lösung ist, EDAtt, den Encoder-Decoder-Aufmerksamkeitsmechanismus, vorzuschlagen. Dabei entscheiden wir, ob wir eine Teilübersetzung ausgeben oder nicht, basierend darauf, wohin die Aufmerksamkeit gerichtet ist. Ein Wort wird ausgegeben, wenn die Aufmerksamkeit nicht konzentriert ist, also wenn ihre Summe unter einen bestimmten Schwellenwert alpha gegenüber den letzten lambda Sprachfassungen liegt, was bedeutet, dass die empfangene Information bereits stabil genug ist.

Beispielsweise, wenn wir ein Sprachsegment mit „I’m going to talk about...“ empfangen und das Modell die Übersetzung ins Deutsche vorhersagt, schauen wir uns die Kreuz-Aufmerksamkeitsgewichte an. Die ersten beiden Wörter weisen auf die frühesten empfangenen Sprachfassungen hin, während das letzte Wort auf die letzten empfangenen Sprachfassungen, also die letzten lambda Sprachfassungen, weist. Dies bedeutet, dass die ersten beiden Wörter ausgegeben werden, da die Summe der Kreuz-Aufmerksamkeit über einen bestimmten Schwellenwert alpha liegt, während das letzte Wort nicht ausgegeben wird und auf ein weiteres Sprachsegment gewartet wird.

Wenn wir ein weiteres Sprachsegment empfangen und das Modell drei weitere Wörter vorhersagt, schauen wir uns erneut die Kreuz-Aufmerksamkeitsgewichte an. Wir stellen fest, dass keine Wörter auf die letzten lambda Sprachfassungen verweisen. Dies bedeutet, dass diese drei Wörter ausgegeben werden.

Wenn wir die Hauptergebnisse von EDAtt betrachten, zeigen wir die Ergebnisse der simultanen Sprachübersetzung in Diagrammen, bei denen wir auf der einen Seite BLEU haben, der die Übersetzungsgenauigkeit misst, und auf der anderen Seite den durchschnittlichen Verzögerungszeitraum, der die Latenz misst. Wir berücksichtigen auch den rechenzeitbewussten durchschnittlichen Verzögerungszeitraum, der die Rechenzeiten des Modells zur Vorhersage des Outputs berücksichtigt. Wir möchten, dass unsere Kurven so hoch wie möglich in diesem Diagramm sind. Gleichzeitig möchten wir, dass sie nach links verschoben sind. Wir vergleichen dies mit beliebten Strategien, die auch auf Offline-Modellen angewandt werden, nämlich der Wait-k-Strategie und der Lokalen Übereinstimmung. Wir vergleichen auch mit der state-of-the-art-Architektur, die speziell für simultane vorherige Übersetzung entwickelt wurde.

Dies sind alle Ergebnisse der simultanen Sprachübersetzungstrategie auf Deutsch. Wir sehen, dass sie alle Strategien übertrifft, die auf Offline-Modellen angewandt werden, da die Kurven nach links verschoben sind. Wir sehen auch, dass, wenn wir die tatsächliche vergangene Zeit oder die rechenzeitbewusste Zeit berücksichtigen, dies die schnellste Strategie ist.

Wenn Sie weitere Ergebnisse kennenlernen möchten, lesen Sie unser Papier. Wir haben außerdem den Code, die Modelle und die simultane Ausgabe open source veröffentlicht, um die Wiederholbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="149">Ja, der Datensatz ist öffentlich zugänglich.</sample>
    <sample id="150">**Abstract:**  
In this work, we introduce MeetingQA, an extractive question-answering dataset derived from real-world meeting transcripts, aiming to address the underexplored QA aspect of meetings. MeetingQA contains 7.7K questions, including open-ended and rhetorical queries, with answers spanning multiple sentences and speakers. The dataset exhibits a high inter-annotator agreement (Krippendorff’s alpha = 0.73) and includes 30% unanswerable questions, 40% with multi-span answers, and 48% with multi-speaker contributions. We evaluate various models, including short- and long-context approaches, single- and multi-span variants, and data augmentation using silver annotations. Results show a significant gap between human performance (F1 = 84.6) and fine-tuned models (over 25 F1 points), with zero-shot models performing even worse (nearly 50 F1 points lower). Error analysis reveals challenges in identifying rhetorical questions and speaker roles. MeetingQA presents a novel and challenging benchmark for QA systems in the meeting domain, highlighting the need for further research in this underexplored area.</sample>
    <sample id="151">Hallo zusammen, mein Name ist Ying und mein Kollege Zhiyang und wir werden unsere Forschung zu MultiInstruct präsentieren, die das Multi-Modal Zero-Shot Learning durch Instruction Tuning verbessert. Mit den Fortschritten in den großen Sprachmodellen begannen viele Arbeiten, neue Lernparadigmen zu erforschen, bei denen vortrainierte Sprachmodelle in einer parameter- und dateneffizienten Weise für verschiedene downstream Tasks wiederverwendet werden. In jüngster Zeit haben viele Studien gezeigt, dass Instruction Tuning es großen Sprachmodellen ermöglicht, in einem zero-shot-Modus auf unbekannte Aufgaben zu performen, indem sie natürliche Anweisungen folgen. Allerdings konzentrierten sich die meisten früheren Arbeiten zu Instruction Tuning darauf, die zero-shot-Performance auf reinen Sprachtasks zu verbessern, während computer vision- und multi-modal Tasks ausgelassen wurden. Daher möchten wir in dieser Arbeit untersuchen, ob das Instruction Tuning von multi-modal vortrainierten Modellen tatsächlich die Generalisierung auf unbekannte multi-modal Tasks verbessern kann. Zudem stellten wir während unserer Forschung eine erhebliche Diskrepanz in der Verfügbarkeit von instructional Datasets zwischen NLP und multi-modal fest. Es existieren über 1600 Sprach-only instruction Tasks. Allerdings gibt es keinen großskaligen öffentlich verfügbaren multi-modal instruction Task. Dies motiviert uns, ein multi-modal instruction tuning Dataset zu erstellen. Hier stellen wir MultiInstruct vor, das erste multi-modal instruction tuning Benchmark Dataset, das aus 62 verschiedenen multi-modal Tasks besteht, die 10 breite Kategorien abdecken. Diese Tasks stammen aus 21 bestehenden Open-Source-Datasets und jeder Task ist mit fünf Expertenformulierten Anweisungen ausgestattet. Um das multi-modal instruction tuning auf unserem vorgeschlagenen Dataset zu untersuchen, verwenden wir OFA als Grundmodell, ein einheitliches multi-modal vortrainiertes Modell. OFA verwendet ein einheitliches Vokabular für Sprache, Bildtokens und die Koordinaten eines Bounding Boxes. Hier zeigen wir einige Beispielinstanzen aus unserem MultiInstruct Dataset, um die Verarbeitung unterschiedlicher Eingabe- und Ausgabedatentypen zu vereinheitlichen. Wir folgen dem Verfahren von OFA und formulieren alle Tasks in einem einheitlichen sequenz-zu-sequenz Format. Dabei werden Eingabetext, Bilder, Anweisungen und Bounding Boxes im gleichen Tokenraum dargestellt. Jetzt werde ich über multi-modal instruction tuning sprechen. Für das Trainingsdataset verwenden wir 53 Tasks aus 9 Gruppen für das Training und wir sampeln 10.000 Instanzen pro Task. Für die Testung reservieren wir die gesamte Common Sense Reasoning Gruppe für die Testung und wählen zusätzlich 5 Tasks aus den VQ und Miscellaneous Gruppen aus. Wir verwenden alle Instanzen in der Testaufteilung für jeden Task. Zusätzlich sampeln wir zufällig 20 Tasks aus der Testaufteilung der natürlichen Anweisungen als unbekannte Aufgaben für NLP. Wir verwenden das vortrainierte OFA Large Modell als Grundmodell. Während des Trainings mischen wir alle Instanzen aller Tasks. Jede Instanz wird zufällig mit einer ihrer fünf Anweisungsvorlagen kombiniert. Während der Testung führen wir für jeden Task insgesamt 5 Experimente durch, indem wir das Modell mit einer der fünf Anweisungen bewerten. In jedem Experiment berichten wir über die Mindest- und Höchstleistung sowie die Standardabweichung der Leistung über alle 5 Experimente. Wenn die Aufgabe eine multi-modal Klassifizierungsaufgabe ist, berichten wir über die Genauigkeit. Wenn es sich um eine multi-modal Generierungsaufgabe handelt, berichten wir über Rouge-L. Für NLP-Aufgaben berichten wir ebenfalls über Rouge-L. Wir führen auch eine zusätzliche Bewertungsmetrik namens Sensitivität ein. Diese misst die Fähigkeit des Modells, für dieselbe Aufgabe unabhängig von geringfügigen Unterschieden in der Formulierung der Anweisung dieselben Ausgaben zu produzieren. Hier sind unsere Hauptergebnisse. Wie wir sehen können, verbessert Instruction Tuning die Leistung von OFA erheblich bei gesehenen multi-modal Tasks. Auch das Transferlernen von natürlichen instruction Datasets kann das Instruction Tuning fördern. Hier können wir sehen, dass mit zunehmender Anzahl an Tasks das Modell eine bessere Leistung erzielt und gleichzeitig eine geringere Sensitivität aufweist. Wir haben auch ein Experiment durchgeführt, bei dem wir eine Anweisung versus fünf Anweisungen verwenden. Wie wir sehen können, verbessert das Verwenden mehrerer Anweisungen die Gesamtleistung des Modells erheblich und reduziert seine Sensitivität stark. Dies zeigt den Effekt unterschiedlicher Feinabstimmungsstrategien auf die Modellsensitivität. Wie wir sehen können, ermöglicht das Transferlernen von natürlichen instruction Datasets dem Modell eine viel bessere Sensitivität im Vergleich zum ursprünglichen OFA-Modell. Wir können auch sehen, dass das Transferlernen von natürlichen instruction Datasets OFA dabei hilft, eine viel bessere Leistung auf dem natürlichen instruct Dataset zu erzielen. Insgesamt schlagen wir das erste großskalige multi-modal instruction tuning Dataset vor, das die kurzsichtige Fähigkeit von OFA erheblich verbessert, und wir untersuchen verschiedene Transferlernmethoden und zeigen ihre Vorteile. Wir entwickeln eine neue Metrik namens Sensitivität. Ein weiteres Projekt ist die Erstellung eines viel größeren multi-modal instruction tuning Datasets mit etwa 150 zusätzlichen vision language Tasks, das wir bald veröffentlichen werden. Hier ist ein QR-Code für unsere Daten und Modelle. Vielen Dank.</sample>
    <sample id="152">**Abstract:**  
In this presentation, Frederick Riemenschneider introduces new language models tailored for classical philology, focusing on Ancient Greek and Latin. Building on existing monolingual BERT-based models, the team developed GreBERTa (RoBERTa for Greek) and GreTa (T5-based encoder-decoder for Greek), as well as their multilingual counterparts PhilBERTa and PhilTa, trained on Greek, Latin, and English texts. To enhance pre-training data, they leveraged the Internet Archive, identifying Greek texts via OCR errors and creating a high-quality corpus. Rigorous benchmarking on tasks like part-of-speech tagging, dependency parsing, and lemmatization showed significant improvements over previous state-of-the-art models. GreTa's encoder initially underperformed but improved with training, highlighting differences between T5 and native encoder-only models. Encoder-decoder models excelled in lemmatization, achieving a 5-point gain for Greek. While multilingual models outperformed earlier ones, no significant difference was found between multilingual and monolingual models in semantic and world knowledge tasks. The work provides new, robust resources for classical studies, emphasizing the potential of modern NLP in philological research.</sample>
    <sample id="153">**Abstract:**  
This work addresses ambiguities in prompts given to text-to-image generative models, which hinder faithful image generation. We introduce a benchmark dataset, an extended version of LAVA, covering various ambiguity types. Our framework proposes two approaches to disambiguate prompts: generating clarifying questions for user feedback or suggesting multiple visual interpretations for user selection. After disambiguation, we evaluate the generated images for faithfulness to user intention using an automatic evaluation framework based on a VQA model. Results show that our disambiguation methods improve generation quality and that our evaluation framework aligns with human judgments. This study provides tools to both resolve and assess ambiguities in text-to-image generation, enhancing model reliability and user satisfaction.</sample>
    <sample id="154">Die Autoren gehören der University of Trento und der Foundazione Bruno Kessler an.</sample>
    <sample id="155">Javad Hosseini</sample>
    <sample id="157">**Abstract:**  
This paper introduces SDDS, a novel dialogue summarization model that integrates static and dynamic graph structures to enhance the capture of dialogue context. Traditional methods rely on pre-computed static graphs from external tools, which suffer from error propagation and lack of adaptability. SDDS addresses these issues by combining four heuristic static graph structures—such as discourse parsing and speaker interaction frequency—with a dynamic graph module that learns semantic relationships through multi-head attention. The model fuses static and dynamic graph representations using a unified graph and incorporates them into the summarization process via a dual cross-attention mechanism. Experimental results demonstrate that SDDS effectively captures both structural and semantic information, leading to more accurate and coherent summaries. The code and data are publicly available on GitHub.</sample>
    <sample id="158">**Abstract:**  
This paper introduces *Dual Cache*, a novel approach for long document neural coreference resolution. Traditional methods suffer from quadratic complexity, while recent cache-based approaches reduce this to linear but face challenges in long documents with frequent topic shifts. The proposed dual cache combines a local cache (using LRU eviction) for local entities and a global cache (using LFU eviction) for globally frequent entities. By classifying mentions and dynamically assigning them to the appropriate cache, Dual Cache reduces cache misses and improves efficiency. Evaluations on four benchmarks show that Dual Cache outperforms baselines, even with unbounded memory, and achieves better performance-cost ratios. On a 30,000-word book, the performance gap between Dual Cache and baselines is significant. The results demonstrate that Dual Cache effectively balances model efficiency and performance, making it a cost-effective solution for coreference resolution in long documents.</sample>
    <sample id="159">Hallo zusammen. Ich bin Koustav Sinha und begrüße euch herzlich zu unserem Vortrag zu unserem ACL 2023 Paper. Die Akzeptabilitätsurteile von Sprachmodellen sind nicht immer robust gegenüber dem Kontext. Dies ist eine gemeinsame Arbeit mit John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy und Adina Williams. In dieser Arbeit widmen wir uns erneut den minimalen Paar-Paradigmen. Das minimale Paar-Paradigma bewertet Sprachmodelle anhand von Akzeptabilitätsurteilen, was Grammatikalität umfassen kann, wie beispielsweise bei BLiMP oder SyntaxGym, oder Akzeptabilität in Bezug auf Stereotype, wie bei CrowS pairs. Im Rahmen dieses minimalen Paar-Paradigmas wird typischerweise ein akzeptables oder grammatisch korrektes Satzpaar präsentiert, wobei das Modell erwartet wird, dem akzeptablen Satz eine höhere Wahrscheinlichkeit zuzuordnen. Das aktuelle MPP-Pipeline-Verfahren erlaubt uns jedoch nicht, die Akzeptabilität von Modellen in Bezug auf längere Sätze zu bewerten. Heutzutage haben große Sprachmodelle immer größere Kontextfenster, sodass es entscheidend ist, die Akzeptabilität der Modelle über das gesamte Kontextfenster hinweg zu bewerten – das ist unser Ziel in dieser Arbeit. Wir überarbeiten das MPP-Pipeline-Verfahren, indem wir das Modell bitten, Akzeptabilitätsurteile für immer längere Sequenzen zu treffen. Dazu simulieren wir diese längeren Sequenzen, indem wir die Datensätze erneut betrachten und Sätze auswählen, die entweder akzeptabel oder unakzeptabel sind. Hier haben wir beispielsweise ein typisches Paar aus der BLiMP-Datensammlung aus dem Bereich der Adjunkt-Island-Fälle ausgewählt. Um längere Sequenzen zu erstellen, extrahieren wir grammatisch korrekte Sätze aus der Adjunkt-Island-Datensammlung und fügen diese als Präfix sowohl zu den akzeptablen als auch zu den unakzeptablen Anfragen hinzu. Ebenso können wir dies mit unakzeptablen Sätzen aus dem gleichen Phänomen durchführen, um die Akzeptabilität des Modells zu testen. Wir können dies auch durch Auswahl von Sätzen aus einem anderen Datensatz oder einem anderen Subset durchführen, was wir als „Mismatch-Szenario“ bezeichnen. In diesem Fall stammen die Sätze zwar von relevanten Datensätzen, doch nicht vom gleichen Datensatz, mit dem wir bewerten. Ebenso können wir dies für den Fall von Unakzeptabilität durchführen. Schließlich können wir Sätze aus einem vollkommen unverknüpften Bereich wie Wikipedia auswählen. Dies zeigt uns, ob die Akzeptabilitätsurteile des Modells vom Kontext beeinflusst werden – ob der Kontext aus einem anderen Subset des Datensatzes stammt oder vollkommen irrelevant ist. Wie schneidet das Modell ab? Zunächst betrachten wir Wikipedia-Sätze, die vollkommen irrelevant zu den aktuellen Abfragen sind. Hier stellen wir fest, dass die MPP-Urteile weitgehend robust gegenüber beliebiger Kontextlänge sind. Wir erhöhen die Kontextlänge bis zu 1024, um die Modelle OPT und GPT 2 maximal auszulösen. Hier zeigt die gestrichelte orangene Linie, dass die MPP-Urteile relativ stabil bleiben. Was passiert jedoch, wenn wir Sätze aus dem gleichen Datensatz wählen? Hier erstellen wir Sätze aus akzeptablen und unakzeptablen Domänen aus dem gleichen BLiMP- oder SyntaxGym-Datensatz. Hier sehen wir, dass die MPP-Urteile sich erheblich erhöhen oder verringern, sobald entweder akzeptable oder unakzeptable Präfixe hinzugefügt werden. Wenn wir jedoch die Struktur abgleichen, also Sätze aus dem gleichen Phänomen in BLiMP oder SyntaxGym wählen, beobachten wir eine massive Erhöhung oder Verringerung der MPP-Urteile, abhängig davon, ob der gewählte Präfix akzeptabel oder unakzeptabel ist. Dieser Effekt ist sehr groß und wird mit zunehmender Kontextlänge noch stärker. Dies könnte auch neuere Sprachmodelle mit großen Kontextfenstern beeinflussen. Warum wirkt sich der passende Präfix so stark auf die Urteile der Sprachmodelle aus? Wir haben eine Reihe von Analysen durchgeführt, bei denen wir den Eingabetext durch Störungen beeinflussten, wobei wir die relevanten Strukturen beibehielten, aber Störungen hinzufügten. Nach mehreren dieser Störungen stellten wir fest, dass keine dieser Störungen den Modellen das Verhalten verändert hat, was uns die MPP-Urteile zeigt. Im Grunde genommen stellten wir fest, dass die Modelle auf die gestörten Sätze in ähnlicher Weise reagieren. Das heißt, wenn wir Sätze im akzeptablen Bereich stören, sehen wir in allen Störungen einen ähnlichen Anstieg, und wenn wir Sätze im unakzeptablen Bereich stören, sehen wir eine ähnliche Verringerung der MPP-Urteile. Die zentralen Erkenntnisse unserer Arbeit sind, dass Sprachmodelle empfindlich gegenüber latenten syntaktischen und semantischen Merkmalen sind, die über verschiedene Sätze hinweg gemeinsam sind. Die aktuelle MPP-Bewertung, bei der wir uns auf kurze und einzelne Satzeingaben beschränken, könnte die abstrakten Kenntnisse der Sprachmodelle über das gesamte Kontextfenster nicht vollständig erfassen. Lesen Sie unser Paper für weitere Einzelheiten zu unseren Experimenten. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="160">Im ersten Schritt der Methode werden die Input-Token mit einem ungeordneten Multiset von Output-Tokens zugeordnet.</sample>
    <sample id="161">In CoScript sind insgesamt 55.000 Skripte vertreten.</sample>
    <sample id="163">Die beste Ausrichtungsmethode für DEPLAIN ist MASSalign.</sample>
    <sample id="164">Der Vorteil von schwach überwachtem Lernen ist, dass es billige, wenn auch geratene Annotationen verwendet, anstatt teure manuelle Labels.</sample>
    <sample id="165">**Abstract:**  
This paper introduces LiPoR, an unsupervised method for abductive reasoning that learns plausible explanations without supervision. Abductive reasoning involves finding a plausible explanation that connects a given context to an outcome. Traditional approaches rely on annotated explanations, which are often subjective and noisy. LiPoR treats explanations as latent variables and maximizes the marginal likelihood of the outcome given the context, while enforcing mutual exclusivity among explanations through a posterior regularization term. This ensures that only a subset of explanations is favored, reflecting real-world reasoning where explanations are typically mutually exclusive. Evaluated on the AlphaNLI dataset, LiPoR outperforms existing unsupervised and zero-shot models by over 4 absolute points in accuracy, demonstrating its effectiveness in learning abductive reasoning without labeled data. Our work provides a novel, scalable approach to abductive reasoning in a closed-world setting.</sample>
    <sample id="166">**Abstract:**  
This paper introduces NDCR, a Neural Divide-and-Conquer Reasoning framework for image retrieval from linguistically complex text. Traditional visual language models struggle with complex descriptions due to their focus on analogical reasoning, akin to System 1 in the Dual-Process Theory. To address this, NDCR integrates System 1 (analogical reasoning) and System 2 (logical reasoning) using the Divide-and-Conquer strategy. The framework consists of three main components: a Proposition Generator that decomposes complex text into simple propositions, a Visual-Linguistic Interactor (System 1) that matches propositions with images, and a Neural-Symbolic Reasoner (System 2) that combines reasoning states via negation and conjunction operations. Experimental results show that NDCR outperforms existing methods, with ablation studies confirming the effectiveness of each module. The approach enables transparent, step-by-step reasoning, demonstrating the potential of combining neural and symbolic reasoning for complex image-text tasks. This work highlights the value of integrating Dual-Process Theory and Divide-and-Conquer strategies in enhancing compositional reasoning in large language models.</sample>
    <sample id="167">In DEPLAIN-web wurden 750 Dokumente sowohl manuell als auch mit automatischen Alignmentmethoden ausgerichtet.</sample>
    <sample id="168">Der CoNLL++-Datensatz wurde durch die Sammlung von Nachrichten aus Reuters News aus dem Jahr 2020 und deren Annotation gemäß den CoNLL-2003-Richtlinien erstellt.</sample>
    <sample id="169">**Abstract:**  
In this work, David Vilar and colleagues from Google Translate present the first systematic study on prompting large language models (LLMs) for machine translation, focusing on PaLM, a 540-billion-parameter model. The study evaluates translation performance using state-of-the-art metrics and human evaluations, ensuring no overlap between test data and model training data. Results show that prompting strategies significantly affect translation quality, with a 5-shot approach yielding the best results. The quality of example translations is more critical than prompt similarity to the source sentence. While PaLM achieves performance close to commercial systems like Google Translate, it still lags behind in accuracy, particularly due to omission errors. Human evaluations using the MQM framework indicate that PaLM produces fluent translations but struggles with accuracy. Overall, the study highlights the importance of prompt selection and example quality, while emphasizing that specialized MT systems maintain an advantage over LLM-based translation.</sample>
    <sample id="170">Hallo alle, mein Name ist Yusen Zhang vom Penn State University. Heute möchte ich unsere Arbeit „XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations“ vorstellen. Semantische Parsing ist eine Aufgabe, bei der semantische Darstellungen von Benutzeranfragen erstellt werden, wie z. B. SQL und Lambda-Kalkül. Das Cross-Lingual Semantic Parsing ist die Aufgabe, Anfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsrepräsentationen zu übersetzen. Wie in diesem Bild gezeigt, müssen wir Anfragen in mehreren natürlichen Sprachen mithilfe neuronaler Modelle in SQL, Lambda oder FunQL usw. übersetzen. Bestehende Modelle für cross-linguales semantisches Parsing werden getrennt vorgeschlagen und auf Datensätzen begrenzter Aufgaben und Anwendungen bewertet. Zum Beispiel gibt es viel Abdeckung für bestimmte natürliche Sprachen, aber Chinesisch fehlt und es fehlt eine Abdeckung für bestimmte Bedeutungsrepräsentationen, wie Lambda-Kalkül. Oder sie werden nur auf bestimmten neuronalen Modellen bewertet. Zum Beispiel gibt es nur ein einzelnes Modell, um sie zu bewerten. Aus diesem Grund schlagen wir XSemPLR vor. Wir stellen einen einheitlichen Datensatz XSemPLR für das cross-linguale semantische Parsing in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen bereit. Er enthält 9 Datensätze aus verschiedenen Bereichen, 5 semantische Parsing-Aufgaben, 8 Bedeutungsrepräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien. Um unseren Benchmark besser zu bewerten, betrachten wir sechs Trainings- und Bewertungsszenarien. Das erste ist Translate-Test. Wir verwenden die Google Translate API, um die Quelle in die ZielSprache zu übersetzen, und trainieren und bewerten dann ein monolinguales Modell. Zum Beispiel trainieren wir ein englisches Modell mit englischen Anfragen und während der Inferenz übersetzen wir deutsche Anfragen mit der API ins Englische und verwenden dann das trainierte Modell, um SQL vorherzusagen. Wir testen auch monolinguale Modelle. In diesem Szenario ist die Quellsprache die gleiche wie die Zielsprache, z. B. Deutsch zu Deutsch oder Englisch zu Englisch. Wir testen auch das Monolinguale Few-shot-Szenario, bei dem monolinguale Modelle nur mit 10 % der Trainingsdaten trainiert werden. Wir testen auch ein Multilingual-Modell, bei dem wir ein einziges multilinguales Modell für alle Sprachen trainieren. Zum Beispiel kombinieren wir deutsche, englische und chinesische Anfragen, um ein multilinguales Modell zu trainieren. Während der Inferenz können wir dieses Modell verwenden, um deutsche oder chinesische Anfragen zu übersetzen. Wir betrachten auch Cross-linguale Zero-shot- und Few-shot-Übertragungen. Wir trainieren auf einer Quellsprache und übertragen auf eine andere Sprache. Während des Trainings trainieren wir das Modell auf englischen Anfragen oder auf einer Kombination aus englischen und deutschen Few-shot-Anfragen, um ein multilinguales Modell zu trainieren, das SQL-Ausgaben vorhersagt. Wir haben auch viele interessante Ergebnisse gefunden. Bei der Analyse der monolingualen Modelle bewerten wir zwei Gruppen von Modellen, darunter Encoder-PTR, das für Multilinguale vorgetrainierte Encoder mit pointerbasierten Decodern steht, wie z. B. XLM-R + PTR und mBERT + PTR. Wir bewerten auch Encoder-Decoder-Modelle, die Multilinguale vorgetrainierte Encoder-Decoder-Modelle sind, wie mBART und mT5. Wir fanden heraus, dass Encoder-Decoder-Modelle die beste Leistung auf allen neun Datensätzen erzielten. Wir bewerten mT5 und XLM-R + PTR im multilingualen Szenario. Wir fanden heraus, dass Encoder-Decoder- oder Encoder-PTR-Modelle durch das Training mit einer Mischung aus verschiedenen Sprachen verbessert werden können. Wir fanden heraus, dass dies daran liegt, dass die meisten Hauptnatursprachen eine Leistungssteigerung erzielen, außer dass die Leistung im Englischen auf sieben Datensätzen abnimmt und nur auf drei Datensätzen steigt. Ich denke, das wird als „Fluch der Multilingualität“ bezeichnet. Wir vergleichen auch die Leistungsunterschiede bei der Sprachübersetzung. In diesem Bild ist die blaue Linie die cross-linguale Few-shot-Übertragung. Die orange Linie ist die cross-linguale Zero-shot-Übertragung. Die grüne Linie ist das monolinguale Szenario. Wir fanden heraus, dass durch den Vergleich der grünen und orangen Linie die Leistungsunterschiede im Zero-shot-Szenario erheblich sind, und durch den Vergleich der blauen und orangen Linie fanden wir heraus, dass sich der Transfer-Unterschied mit dem Few-shot-Szenario schnell verringert. Wir fanden auch andere interessante Erkenntnisse. Zum Beispiel übertrifft Encoder-Decoder frühere Arbeiten oder erzielt vergleichbare Ergebnisse. Das Vortraining auf der englischen natürlichen Sprache kann die Leistung bei Few-shot auf Zielnatursprachen erheblich steigern, und wir fanden heraus, dass multilinguale Sprachmodelle wie Codex und BLOOM immer noch unzureichend für cross-linguale semantische Parsing-Aufgaben sind. Zusammenfassend haben wir XSemPLR erstellt, einen einheitlichen Benchmark für das cross-linguale semantische Parsing mit mehreren natürlichen Sprachen und Bedeutungsrepräsentationen. Wir führen eine umfassende Benchmark-Studie an drei repräsentativen Typen von multilingualen Sprachmodellen durch. Unsere Ergebnisse zeigen viele interessante Erkenntnisse. Und so weiter. Besuchen Sie gerne unser Paper und Code. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="171">Bestehende Arbeiten lassen sich grob in vier Kategorien einteilen, jedoch fehlt vielen davon entweder die Anwendbarkeit auf Embedding-as-a-Service oder die Übertragbarkeit.</sample>
    <sample id="172">Nein, Codex und Bloom sind für CLSP (Cross-Lingual Semantic Parsing) noch nicht ausreichend.</sample>
    <sample id="174">**Abstract:**  
The ArgAnalysis35K dataset introduces a novel approach to argument quality analysis by offering the largest collection of high-quality argument-analysis pairs (35,000) in the field. Unlike existing datasets, which often suffer from low quality, limited diversity, and lack of depth, ArgAnalysis35K sources arguments from expert and intermediate debaters, ensuring higher quality and broader coverage. It includes 24 diverse themes, capturing a wide range of motions relevant to parliamentary debates. A key innovation is the introduction of "analysis," combining claims and premises into a coherent explanation, enhancing the depth of argument evaluation. The dataset also incorporates instance-based annotator reliability, allowing for more accurate and fair scoring by accounting for annotator biases on a per-argument basis. Additionally, a relevance model assigns scores to indicate how relevant each argument is to different themes, enabling more flexible use in various contexts. Overall, ArgAnalysis35K provides a more diverse, reliable, and nuanced resource for argument quality analysis, with detailed insights available in the accompanying paper and poster.</sample>
    <sample id="175">Die Methode approximiert die Suche nach der besten Permutation durch eine GPU-freundliche kontinuierliche Relaxierung und lernt dadurch linguistisch plausiblere Permutationen.</sample>
    <sample id="176">Die Fairness eines nachgeschalteten NLP-Modells wird definiert durch die Gleichmäßigkeit seiner Leistung über verschiedene demografische Gruppen oder politische Einstellungen hinweg, insbesondere bei Aufgaben wie Hassrede- und Fake-News-Erkennung.</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Der Referent heißt Koustav Sinha.</sample>
    <sample id="179">**Abstract:**  
Melanie Sclar presents SymbolicToM, an inference-time method to enhance Theory of Mind (ToM) reasoning in large language models (LLMs). ToM involves understanding others’ mental states, often tested via false-belief scenarios like the Sally-Anne test. Despite their capabilities, LLMs struggle with such tasks. SymbolicToM uses explicit graphical representations to model characters’ beliefs, enabling accurate reasoning about nested mental states. The method computes belief graphs (e.g., BBob and BBob,Alice) for all character combinations up to a predefined ToM level, leveraging NLI and OpenIE models. This approach allows efficient question-answering by translating queries into factual graph-based questions. Experiments show significant performance gains across multiple LLMs, including GPT-3 and Flan-T5-XXL, with improvements of up to 67 accuracy points. SymbolicToM outperforms supervised baselines, particularly in out-of-domain scenarios like modified ToMi datasets and linguistically diverse ParaphrasedToMi. It avoids overfitting and provides interpretable reasoning, making it a plug-and-play solution for improving ToM reasoning in LLMs.</sample>
    <sample id="180">Myra</sample>
    <sample id="181">**Abstract:** This paper introduces constrained language planning, a task where goals are accompanied by specific constraints, requiring planners to generate faithful and complete scripts. While large language models (LLMs) excel at abstract goal planning, they struggle with constrained goals like "make a chocolate cake." We evaluate LLMs on this task and find that although semantic completeness is acceptable, constraint faithfulness is lacking. To address this, we propose an over-generate-then-filter method, which improves script quality by generating multiple candidates and filtering them using semantic similarity and keyword matching. Furthermore, we create CoScript, a large-scale dataset of 55,000 constrained goals with scripts, validated by crowd-sourced workers. Using CoScript, we fine-tune smaller models like T5, achieving performance comparable to or better than LLMs. Our work establishes constrained language planning as a new direction and provides a valuable resource for future research.</sample>
    <sample id="182">Tropikalismus bezeichnet in dieser Arbeit stereotype, oft verallgemeinernde Beschreibungen, die mit bestimmten ethnischen oder kulturellen Gruppen assoziiert werden, wie beispielsweise "vibrant" und "curvaceous" bei Latina Frauen, die auf tropische Vorstellungen verweisen.</sample>
    <sample id="183">Die Autoren haben die von Menschen verfassten Beschreibungen der Zielgruppen durch eine Studie erstellt, bei der sie ähnliche Prompts wie bei den Modellen an Menschen weitergaben.</sample>
    <sample id="184">In dieser Arbeit wurde CXMI (Contextual Mutual Information) zur Messung der Kontextnutzung verwendet.</sample>
    <sample id="185">Der Unterschied zwischen DrBERT und ChuBERT besteht darin, dass DrBERT auf dem Datensatz NACHOS (Web-crawled Daten) trainiert wird, während ChuBERT auf anonymisierten klinischen Daten aus dem Nantes University Hospital trainiert wird.</sample>
    <sample id="187">Zwei Autoren sind an der Arbeit beteiligt: Ying und Zhiyang.</sample>
    <sample id="188">Iteratives Transferlernen ist der Prozess, bei dem ein Modell schrittweise anhand von Daten aus einem verwandten Aufgabenbereich (z. B. CE- oder Debattenaufgaben) feinabgestimmt wird, um anschließend besser auf eine seltene Zielklasse (wie kognitive Dissonanz) zu lernen.</sample>
    <sample id="189">Das Ziel des Datensatzes ist, indirekte Referenzen in der Sprache von Nutzern zu verstehen, um bei der Auswahl zwischen Entities (z. B. Songs, Büchern, Rezepten) zu helfen.</sample>
    <sample id="190">Ein Angreifer kann Modellparameter über einen EaaS (Embedding as a Service) extrahieren, indem er die Embeddings des Dienstes lernt und dadurch ein ähnliches Modell rekonstruiert.</sample>
    <sample id="191">Die Arbeit ist von drei Autoren gemeinsam verfasst worden.</sample>
    <sample id="192">**Abstract:**  
This work introduces CAME, a confidence-guided adaptive memory-efficient optimizer for large language model training. Traditional adaptive methods like Adam require high memory due to storing first and second moment estimates, while memory-efficient alternatives like Adafactor suffer from performance degradation. CAME addresses this by combining fast convergence with low memory usage. Inspired by non-negative matrix factorization and the errors in Adafactor's updates, CAME introduces an instability matrix derived from the residual between predicted and actual updates. This instability is used as a denominator to adaptively adjust the optimization step, improving stability and convergence. Extensive experiments on BERT, GPT-2, and T5 show that CAME outperforms Adam and Adafactor in validation accuracy while significantly reducing memory consumption, especially for large batch sizes. CAME also demonstrates comparable performance to baselines on downstream tasks with lower memory footprint, making it a promising extension for memory-efficient optimization in large-scale training.</sample>
    <sample id="193">Die Anzahl der Annotatoren wurde nicht genannt.</sample>
    <sample id="194">Die Autoren gehören zur Carnegie Mellon University und der University of Washington sowie dem Allen Institute for AI.</sample>
    <sample id="195">**Abstract:**  
This paper introduces RoHT, a novel framework for explainable question answering (XQA) that leverages hierarchical question decomposition. RoHT addresses the limitations of existing neuro-symbolic and decomposition-based methods by integrating knowledge from both structured knowledge bases (KBs) and text corpora. It constructs a Hierarchical Question Decomposition Tree (HQDT) to decompose complex questions into sub-questions, allowing flexible selection of knowledge sources for each node. Probabilistic reasoning over the HQDT enables the fusion of KB and text-based knowledge while considering generation and answering probabilities. RoHT is evaluated on KQA Pro and Musique datasets, demonstrating significant improvements over existing methods. On KQA Pro, RoHT outperforms KB-only models when combined with Wikipedia, and on Musique, it achieves a 11.9 F1 improvement over state-of-the-art text-based methods. The results highlight the effectiveness of hierarchical decomposition and multi-source knowledge integration for complex XQA tasks.</sample>
    <sample id="196">Das Beispiel mit dem Begrenzer auf der linken Seite ist: "I saw Bart and Lisa."</sample>
    <sample id="197">Der Stand der Technik für Dialogsysteme beinhaltet die Verwendung von menschlicher Bewertung, wie z. B. die Auswahl des besseren Dialogs durch menschliche Gutachter oder die Bewertung von Dialogen anhand einer Likert-Skala, um die Gesamtqualität zu beurteilen.</sample>
    <sample id="198">Weil moderne Sprachmodelle immer längere Kontextfenster haben und ihre Akzeptanzurteile daher über das gesamte Kontextfenster hinweg bewertet werden müssen, um deren Robustheit und Verständnis zu prüfen.</sample>
    <sample id="199">Ja, bei sieben Datensätzen gab es einen Leistungsabfall beim mehrsprachigen Training im Vergleich zum einsprachigen englischen Modell.</sample>
    <sample id="200">Ja, die Annotatoren kennen die Entitäten im Voraus.</sample>
    <sample id="201">Für die Bewertung wurden state-of-the-art-neurale MT-Metriken sowie expertenbasierte menschliche Evaluationsresultate verwendet.</sample>
    <sample id="202">Nein, die Regression wirkt sich nicht auf bestimmte NER-Typen aus.</sample>
    <sample id="203">Positionalität ist für NLP wichtig, weil sie systematische Unterschiede in der Leistung von Technologien zwischen Bevölkerungsgruppen aufzeigt und zeigt, dass Daten und Modelle bestimmte Perspektiven bevorzugen können, was zu Ungleichheiten führen kann.</sample>
    <sample id="204">Nein, sie wurden weder durch Adapter noch durch vollständige Feinabstimmung angepasst.</sample>
    <sample id="205">This study investigates how political biases in pretraining data propagate through language models and affect downstream NLP tasks. We evaluate the political leanings of various language models using political questionnaires and find that models exhibit diverse ideological positions, with GPT-4 being the most liberal. Further pretraining on partisan corpora shifts models’ ideological coordinates, indicating that political biases are largely captured from training data. We also observe that language models trained on post-2017 data show increased polarization. When applied to tasks like hate speech and fake news detection, models with different political leanings demonstrate biased performance—left-leaning models are more effective at detecting hate speech against minorities but less so against dominant groups, and vice versa for right-leaning models. These findings highlight significant fairness concerns in NLP applications. Addressing political bias in language models presents a dilemma: removing bias risks censorship, while retaining it may perpetuate unfairness. This study underscores the urgent need for careful consideration of political bias in the development and deployment of language models.</sample>
    <sample id="206">Für das Transferlernen verwenden wir ein Modell, das zunächst auf den CE-Aufgaben (Expansion und Comparison der PDTB) feinabgestimmt wird, gefolgt von einer weiteren Feinabstimmung auf der Debattenaufgabe.</sample>
    <sample id="207">Zur Bewertung der PaLM-Fähigkeiten wurden die aktuellen Testsets der WMT-Evaluation verwendet.</sample>
    <sample id="208">Die Autoren haben drei Empfehlungen vorgeschlagen.</sample>
    <sample id="209">Die genaue Höhe des Gewinns gegenüber der stärksten Baseline wird nicht quantitativ genannt, aber es wird angegeben, dass die vorgeschlagene Methode die Planungsfähigkeit sowohl in der semantischen Vollständigkeit als auch in der Treue zu den Einschränkungen deutlich verbessert.</sample>
    <sample id="210">Der Referent heißt Shuheng.</sample>
    <sample id="211">Ja, die Ergebnisse und der Datensatz können als Benchmark für die automatische Textvereinfachung verwendet werden.</sample>
    <sample id="212">In der Arbeit wird mit einem kleineren Modell, dem T5, experimentiert.</sample>
    <sample id="213">OFA wird als Basismodell für die Untersuchung der multimodalen Unterrichtsabstimmung verwendet.</sample>
    <sample id="215">This paper investigates the dependency structure of coordination, focusing on the debate between asymmetric and symmetric approaches. While frameworks like Universal Dependencies and Meaning-Text Theory assume the first conjunct as the head, the Prague approach and Word Grammar propose symmetric structures with either the conjunction or all conjuncts as heads. The study argues for symmetric structures by applying the principle of dependency length minimization. Analysis of the Penn Treebank reveals that left conjuncts tend to be shorter, especially when the governor is on the left or absent. This pattern disappears when the governor is on the right, suggesting that asymmetric structures cannot account for these findings. The results support symmetric coordination models, as they better align with dependency length preferences. The study provides new empirical evidence against traditional asymmetric views, offering a novel argument for symmetric dependency structures in coordination.</sample>
    <sample id="217">**Abstract:**  
This work addresses the challenge of compositional generalization in multi-attribute controllable dialogue generation (CDG). Existing methods focus on single attributes or lack support for continuous attributes, limiting controllability and evaluation. We propose DCG, a Disentangled Controllable Generation model that learns attribute concepts from seen values and uses disentanglement loss to separate attribute combinations. A unified reference-free evaluation framework, MAE, is introduced to assess controllability across different attribute granularities without requiring labeled data. Experiments on two benchmarks show that DCG outperforms baselines in attribute controllability and text quality, especially for unseen combinations. Our approach combines attribute- and task-oriented prompts, enhancing both control and fluency. Visualizations and correlation analyses confirm the effectiveness of our method in disentangling attributes and generalizing from seen to unseen combinations. This study advances multi-attribute CDG by enabling flexible, controllable, and generalizable dialogue generation.</sample>
    <sample id="218">Die Autoren gehören der Google Translate-Abteilung an.</sample>
    <sample id="219">This paper introduces a compare-and-contrast multistage pipeline for uncovering financial signals in annual reports (Form 10-K). Motivated by the high textual similarity between consecutive reports and the need for automated financial analysis, we propose a highlighting task to identify important words that reflect changes or relations between a target report and its previous year's reference. Our approach includes document segmentation, relation classification into three types (β, revised, mismatched), and a two-stage fine-tuning process using external (eSNLI) and domain-specific data. The model is evaluated on the eSNLI and our newly released FINAL dataset using precision and PCC metrics. Results show that our domain-adaptive highlighting model achieves state-of-the-art performance on both datasets, demonstrating strong generalization and effectiveness in identifying financial signals, especially in mismatched pairs not seen during training. This work provides a novel framework for automated financial report analysis and opens avenues for future improvements in information retrieval and financial signal extraction.</sample>
    <sample id="220">Die Autoren gehören der Stony Brook University an.</sample>
    <sample id="221">In der Arbeit wurden die Sprachpaare Deutsch-Englisch untersucht.</sample>
    <sample id="222">This work investigates challenges and interventions for domain adaptation in open-domain question answering (QA). When applying general-purpose models trained on Wikipedia to new domains, such as biomedical, performance often degrades due to shifts in data distribution. We explore data interventions, including zero-shot and few-shot methods, to enhance model generalization. Few-shot approaches leverage target-domain examples to generate additional training data, improving retriever and reader performance by 8% and 11% on average. Zero-shot methods manipulate question, answer, and context distributions to better align with target domains. We also analyze the nature of dataset shifts—concept, covariate, and full shift—using compatibility measures based on likelihood scores from source models. By mapping datasets onto a 2D shift grid, we identify that full-shift datasets benefit most from few-shot adaptations, while concept and covariate shifts also respond well to zero-shot methods. Our results demonstrate that targeted data interventions can improve reader performance by up to 24%, highlighting the importance of aligning model training with domain-specific characteristics.</sample>
    <sample id="223">Der Referent heißt Shangbin.</sample>
    <sample id="224">Während der Experimente wurden das Modell long-mBART für die Dokumentebene und das Modell base mBART für die Satzenebene untersucht.</sample>
    <sample id="225">Für das Training werden 53 Aufgaben verwendet, und für die Tests werden zusätzliche 5 Aufgaben sowie die gesamte Common Sense Reasoning-Gruppe verwendet.</sample>
    <sample id="226">Zwei Autoren sind an der Arbeit beteiligt: Regina Stodden und Omar.</sample>
    <sample id="227">Recent advances in language models have enabled progress across various NLP tasks, yet grounded language understanding—mapping natural language to executable plans or programs in specific environments—remains a challenge. This is largely due to the lack of grounding during pre-training. Current approaches often rely on language models to directly generate plans, which can lead to invalid or non-executable outputs. In this work, we propose Pangu, a novel framework that shifts the focus from generation to discrimination, leveraging language models to score and rank candidate plans generated by a symbolic agent. This approach avoids the complexities of plan generation and improves robustness and validity. We evaluate Pangu on knowledge-based question answering, a representative grounded task, using models like BERT, T5, and Codex with both fine-tuning and in-context learning. Pangu demonstrates strong performance, high sample efficiency, and robustness under non-i.i.d. settings. Our results suggest that discrimination, rather than generation, may be a more effective strategy for grounded language understanding.</sample>
    <sample id="228">Die Autoren haben an den Datensätzen AG News, MIND, SST2 und Enron Spam experimentiert.</sample>
    <sample id="229">**Abstract:**  
Gabriella Skitalinskaya und Henning Wachsmuth präsentieren eine Studie zur Erkennung von verbesserbaren Argumenten in argumentativer Schreibunterstützung. Die Arbeit untersucht zwei Aufgaben: die Erkennung von suboptimalen Aussagen und die Vorschläge zur Verbesserung. Basierend auf revisionsgestützten Daten aus Online-Debattenebenen wie Kialo wird analysiert, wie Qualität von Argumenten modelliert werden kann. Die Forscher identifizieren vier Hauptausforderungen: Repräsentativität und Zuverlässigkeit der Daten, Modellkomplexität, Kontextabhängigkeit von Qualitätseigenschaften und thematische sowie Nutzerbias. Die Ergebnisse zeigen, dass revisionsgestützte Daten effektiv für die gestellten Aufgaben genutzt werden können, wobei die Modellierung der Distanz zwischen Aussagenversionen hilfreich ist. Zudem hängt der Einfluss von Kontextinformationen von Aufgabe und Qualitätsschwächen ab. Die Studie bietet eine systematische Analyse und Vergleich von Ansätzen und unterstreicht die Relevanz von revisionbasierten Ansätzen in der argumentativen Textverarbeitung.</sample>
    <sample id="231">NACHOS ist ein Datensatz medizinischer Web-Scraping-Daten.</sample>
    <sample id="232">Der Referent heißt David Vilar.</sample>
    <sample id="233">**Abstract:**  
This paper introduces EDAtt, a novel strategy for simultaneous speech translation (SimulST) that leverages the cross-attention mechanism of pre-trained offline speech translation models without requiring retraining or specialized architectures. Current SimulST approaches often involve complex training procedures and multiple models for different latency regimes. EDAtt addresses these challenges by using a single model and controlling latency through attention-based decisions. It emits partial translations when the attention weights over the last λ speech frames fall below a threshold α, indicating sufficient information stability. Evaluation on German shows that EDAtt outperforms existing SimulST methods and strategies adapted from offline models, achieving higher BLEU scores and lower latency, including computational-aware latency. The approach is computationally efficient and facilitates real-time translation with minimal delay. The code, models, and outputs are publicly available to support reproducibility.</sample>
    <sample id="234">Die Prompt-Strategie hat einen großen Einfluss auf die Ergebnisse der Übersetzung durch große Sprachmodelle. Unterschiedliche Prompt-Strategien können die Leistung um mehrere BLEURT-Punkte beeinflussen, wobei die Qualität der Beispiele wichtiger ist als die Ähnlichkeit zum Quelltext.</sample>
    <sample id="235">Die Autoren gehören der University of Edinburgh an.</sample>
    <sample id="236">Die 5 Anweisungen der Expert*innen sind nicht explizit genannt, aber jede Aufgabe im MultiInstruct-Datensatz ist mit fünf von Expert*innen verfassten Anweisungen ausgestattet.</sample>
    <sample id="237">Die Autoren schlagen den KITMUS-Test vor, ein diagnostisches Testset zur Evaluierung der Integration von Wissen aus verschiedenen Quellen.</sample>
    <sample id="238">**Abstract:**  
In this video, Yebowen Hu introduces MeetingBank, a new benchmark dataset for meeting summarization, comprising 1,366 City Council meetings with transcripts, reference summaries, and related URLs. The dataset addresses the challenges of obtaining high-quality summaries and reliable public meeting data. It includes detailed statistics on meeting duration, speaker count, and summarization instances per city. The study evaluates both extractive and abstractive summarization systems, including Oracle, LEAD, LexRank, TextRank, BART-Large, and GPT-3. While extractive methods like Oracle achieve high ROUGE-2 scores, abstractive models such as DialogLM perform best in automatic metrics. Human evaluations reveal that GPT-3 excels in fluency and coherence but falls short in informativeness and factuality. MeetingBank provides valuable resources for researchers and offers insights into City Council decision-making processes. The dataset encourages further development of summarization technologies and alignment of automatic evaluation metrics with human preferences.</sample>
    <sample id="239">Hallo zusammen, mein Name ist David Vilar, und ich werde ein kurzes Review des Papers „Prompting PaLM for Translation: Assessing Strategies and Performance“ geben. Dieses Paper ist gemeinsam mit meinen Kollegen von Google Translate entstanden. PaLM ist ein großes Sprachmodell mit 540 Milliarden Parametern, das letztes Jahr, 2022, vorgestellt wurde. Es wurde auf einer großen Menge an Text trainiert, bestehend aus 780 Milliarden Token. Zu Veröffentlichungszeit erreichte es den Stand der Technik in hunderten NLP-Aufgaben. In dieser Arbeit präsentieren wir die erste systematische Studie zur Prompting-Strategie großer Sprachmodelle für maschinelle Übersetzung. Wir haben die Übersetzungsfähigkeit solcher Modelle mit den besten Praktiken der MT-Community getestet. Dazu gehörte die Verwendung der neuesten Testsets, um eine Überlappung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden. Wir verglichen zudem mit den besten Systemen, also mit den WMT-Evaluierungen. Wir verwendeten state-of-the-art-Metriken für neuronale Übersetzungssysteme und zeigten zudem Ergebnisse von menschlichen Bewertungen durch Experten. Schließlich geben wir einige Empfehlungen für Strategien zur Prompt-Auswahl. Das Prompting hat einen großen Einfluss auf die Leistung der LLMs bei der Übersetzung, wie wir in einem einfachen Experiment sahen, bei dem wir ein One-Shot-Prompting mit zwei unterschiedlichen Prompts pro Satz verwendeten. Bei 516 von 1.000 Sätzen stellten wir einen Unterschied von mehr als einem BLEURT-Punkt fest. In Extremfällen können solche Unterschiede bis zu 40 BLEURT-Punkte betragen. Daher ist es wichtig, eine gute Prompting-Strategie zu wählen. In unseren Experimenten haben wir uns für ein Five-Shot-Prompting entschieden, bei dem wir einfach jeden Satz mit der Sprache markiert haben, in der er vorliegt. In diesem Beispiel, bei dem wir eine Übersetzung von Deutsch ins Englische durchführen, sind die Quellsätze mit „Deutsch:“ und die englischen Übersetzungen mit „Englisch:“ markiert. Wir stellten fest, dass die konkrete Form des Promptings bei mehreren kurzen Promptings kaum einen Einfluss hat. Sie ist jedoch entscheidend für Zero- und One-Shot-Prompting. Wenn wir, wie in unserem Fall, zu Five-Shot-Prompting übergehen, ist der Unterschied zur konkreten Form des Promptings nahezu vernachlässigbar. Die Beispiele tragen in diesem Fall die meiste Gewichtung. Insgesamt ergaben unsere experimentellen Ergebnisse, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit zum Quellsatz. Es ist daher wichtig, Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere verglichen wir die Auswahl von Prompts aus dem Trainingsdatensatz für die WMT-Evaluierungen anhand der Dev-Daten. Die Dev-Daten sind deutlich besser curatiert und von höherer Qualität als die Trainingsdaten, die tendenziell lauter und ungenauer sind. Die Ergebnisse zeigten, dass die Leistung bei der Verwendung der Dev-Daten besser war. Dennoch haben spezialisierte state-of-the-art-Systeme einen erheblichen Vorteil gegenüber den PaLM-Übersetzungen. PaLM kommt jedoch ziemlich nahe an ein kommerzielles System heran. In unserem Fall haben wir uns entschieden, Google Translate als Vergleichssystem zu verwenden. Die Erkenntnisse aus der menschlichen Bewertung, die wir mit dem MQM-Framework durchgeführt haben, zeigten, dass die Flüssigkeit von PaLM vergleichbar mit state-of-the-art-Systemen ist. Der Hauptunterschied ergibt sich jedoch aus der Genauigkeit. Besonders häufige Fehler sind Omissionen, also das Weglassen von Teilen des Quellsatzes. Es scheint, dass PaLM gelegentlich bessere klingende Übersetzungen produziert, indem es Teile des Quellsatzes, die bei der Übersetzung schwierig sind, einfach weglässt. Die Kategorie „Stil/Unbeholfenheit“ für PaLM ist jedoch niedriger als bei den state-of-the-art-Systemen, was ein weiteres Signal dafür ist, dass PaLM wirklich flüssige Ausgaben produziert, aber immer noch mit Problemen der Genauigkeit zu kämpfen hat. Das war eine kurze Zusammenfassung. Für weitere Details besuchen Sie bitte die vollständige Präsentation des Papers. Vielen Dank.</sample>
    <sample id="240">Hallo, ich bin Dawei, ein Doktorand an der Universität des Saarlandes in Deutschland. In diesem Video möchte ich unsere kürzliche Arbeit „Weaker Than You Think: A Critical Look at Weakly Supervised Learning“ vorstellen. Dies ist eine gemeinsame Arbeit mit Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow. Ich möchte mit einer kurzen Einführung in das schwache Labeling und das schwach überwachte Lernen beginnen. Bei schwachem Labeling werden die Daten nicht manuell annotiert. Stattdessen werden sie mithilfe von schwachen Labeling-Quellen annotiert, wie etwa einfachen Heuristikregeln, Wissensbasen oder niedrigwertigen Crowdsourcing-Annotationen, wie in der Abbildung rechts dargestellt. Im Vergleich zu menschlichen Annotationen sind diese schwächeren Annotationen zwar viel günstiger, sind jedoch auch rauschig, was bedeutet, dass ein gewisser Anteil der Annotationen falsch ist. Wenn man neuronale Netzwerke direkt mit schwach annotierten Daten trainiert, neigen diese dazu, den Label-Rausch zu memorieren und sich nicht zu verallgemeinern. Im schwach überwachten Lernen werden Trainingsalgorithmen vorgeschlagen, um neuronale Netzwerke unter solchem Label-Rausch robust zu trainieren, so dass die trainierten Modelle dennoch gut generalisieren. In jüngeren Arbeiten im Bereich des schwach überwachten Lernens, also WSL für Weakly Supervised Learning, wird häufig behauptet, dass man Modelle nur mit schwach annotierten Daten trainiert und dennoch eine hohe Leistung auf sauberen Testdaten erzielt. Technisch gesehen ist diese Aussage nicht falsch, aber es gibt einen Haken: Es wird angenommen, dass ein zusätzliches sauberes Validierungsset zur Modellselektion vorhanden ist. Wir können uns nicht damit zufrieden geben, aber dies impliziert, dass zusätzliche manuelle Annotationen im schwach überwachten Lernen erforderlich sind. Doch diese Notwendigkeit wird oft übersehen, wie ein Elefant im Raum. Die oben genannte Zweifel führen zu drei Forschungsfragen. Erstens: Ist ein sauberes Validierungsset für WSL erforderlich oder könnte man stattdessen ein rauschiges Validierungsset verwenden? Zweitens: Wenn saubere Daten erforderlich sind, oder wenn saubere Daten für WSL unerlässlich sind, wie viele saubere Beispiele benötigen wir dann? Drittens: Sollten wir die sauberen Beispiele nur für die Validierung verwenden oder gibt es bessere Möglichkeiten, sie zu nutzen? Wir haben diese Forschungsfragen in unserer Arbeit untersucht und unsere Ergebnisse sind wie folgt. Erstens stellen wir fest, dass interessanterweise die neuesten WSL-Methoden tatsächlich saubere Validierungsbeispiele benötigen, um ordnungsgemäß zu funktionieren. Andernfalls gibt es einen großen Leistungsverlust. Wie in dieser Abbildung gezeigt, wenn keine sauberen Validierungsbeispiele vorhanden sind, können die trainierten Modelle nicht über die ursprünglichen schwachen Labels hinaus generalisieren, was bedeutet, dass das Training sinnlos ist. Dies zeigt, dass WSL-Methoden tatsächlich sauber annotierte Daten benötigen, um ordnungsgemäß zu funktionieren, und die Annotationskosten für die Erzielung von sauberen Validierungsbeispielen sollten nicht übersehen werden. Unsere zweite Erkenntnis ist, dass das Erhöhen der Anzahl der sauberen Validierungsbeispiele die Leistung der WSL-Methoden verbessert, wie in der linken Abbildung gezeigt. Typischerweise benötigen wir nur 20 Beispiele pro Klasse, um eine hohe Leistung zu erzielen. Doch das ist nicht das Ende der Geschichte, denn wenn wir dennoch beschließen, saubere Beispiele zu verwenden, dann wird das direkte Training auf diesen Beispielen sogar noch bessere Ergebnisse liefern. Die rechte Abbildung zeigt den Leistungsunterschied zwischen Feinabstimmungsansätzen, die direkt auf sauberen Daten angewendet werden, und WSL-Methoden, die saubere Daten nur zur Validierung verwenden. Wie wir sehen können, wenn wir 10 Beispiele pro Klasse haben, beginnt die direkte Feinabstimmung, WSL-Methoden zu übertreffen. Schließlich können die Leistungsverbesserungen, die in früheren WSL-Methoden behauptet werden, leicht durch das Erlauben der Weiterfeinabstimmung auf sauberen Validierungsbeispielen erzielt werden. Wie aus den Abbildungen ersichtlich ist, unterperformt das Standardmodell, genannt FTw, zunächst komplexere WSL-Methoden wie COSINE. Wenn wir jedoch die Weiterfeinabstimmung auf sauberen Beispielen erlauben, leistet sich FTw gleich gut wie andere Methoden. In der Praxis gibt es also keinen Grund, komplexere WSL-Methoden zu wählen, die mehr Rechenzeit und Speicherplatz benötigen. Zusammengefasst haben wir gezeigt, dass aktuelle WSL-Methoden saubere, manuell annotierte Beispiele benötigen, um ordnungsgemäß zu funktionieren. Ihre Leistungsverbesserungen und Praktikabilität werden stark überbewertet. Unsere konkreten Empfehlungen für zukünftige Arbeiten sind wie folgt. Erstens: Die Kriterien für die Modellselektion sollten berichtet werden. Zum Beispiel, ob die Modellselektion über saubere Validierungsbeispiele erfolgt. Zweitens sollten WSL-Methoden mit Baselines der Few-Shot-Learning-Methode verglichen werden, da beide auf sauberen Beispielen arbeiten. Drittens ist die kontinuierliche Feinabstimmung eine einfache, aber starke Baseline, die in zukünftigen Arbeiten im Bereich WSL berücksichtigt werden sollte. Schließlich haben wir unseren Code freigegeben. Sie können ihn über den QR-Code auf dieser Folie finden. Bitte fühlen Sie sich frei, ihn zu prüfen. Vielen Dank und viel Spaß auf dem Konferenz.</sample>
    <sample id="241">**Abstract**  
This paper introduces a human-in-the-loop evaluation framework for early detection of misinformation, focusing on claims about COVID-19 treatments. Existing systems often suffer from unrealistic evaluations and limited human involvement. We propose an end-to-end system that integrates human feedback throughout the detection process, from raw social media data to actionable outputs. The system comprises two main components: a T5-based model for identifying check-worthy claims and a BERT-based model for stance classification to flag policy violations. Human moderators verify claims and assess policy breaches using a Likert scale. Our evaluation shows that the system can detect unapproved treatments before they are debunked in news articles, achieving 65% accuracy in policy violation detection and identifying 124.2 policy violations per human hour. This work provides a realistic, human-centric approach to misinformation detection, offering insights for future systems and demonstrating the value of involving humans in the detection process.</sample>
    <sample id="242">Gängige Bewertungsmethoden für Dialogsysteme sind humanoide Bewertungen wie die Auswahl des besseren Dialogs oder die Bewertung mit einer Likert-Skala, sowie dimensionsbasierte Ansätze, bei denen verschiedene Aspekte der Dialogqualität einzeln bewertet werden.</sample>
    <sample id="243">An der Arbeit sind fünf Autoren beteiligt.</sample>
    <sample id="244">Das Hintergrundwissen, dass Richter Fälle in Gerichten entscheiden, wird benötigt.</sample>
    <sample id="245">**Abstract**  
This study presents a two-step pipeline for identifying high-agreement workers on Amazon Mechanical Turk (MTurk) for summarization tasks. The pipeline includes a qualification task to assess annotators' ability to evaluate multiple dimensions and an endurance task to test their capacity for handling heavy workloads. From 200 participants, 12 workers (4 gold, 8 silver) passed both stages, achieving higher inter-annotator agreement (IAA) than experts, with Krippendorff’s Alpha reaching 0.534 in a reference-based task. Compared to baselines like MACE and CloudResearch, the pipeline offers similar quality at lower cost and higher task completion rates. However, it does not guarantee correctness, as shown by correlation analysis with expert judgments and GPT models. The approach provides a scalable, cost-effective method for recruiting reliable annotators, though limitations include a focus on English summarization and the lack of a universal solution for correctness training. Future work will explore broader applications and improvements in annotator quality.</sample>
    <sample id="246">Ja, der Code ist auf GitHub verfügbar.</sample>
    <sample id="247">**Abstract:**  
This paper introduces **FactKG**, a novel dataset for **Knowledge Graph-Based Fact Verification**, addressing the gap in existing fact verification datasets that rely on text or tables rather than knowledge graphs (KGs). FactKG leverages **DBpedia** as the knowledge source and includes claims in both **written** and **colloquial** styles for practical applicability. The dataset supports five reasoning types: **one-hop, conjunction, existence, multi-hop**, and **negation**, enabling comprehensive evaluation of fact verification systems. The task involves retrieving evidence from the KG and verifying claims based on the relationships between entities. To enhance practicality, we employ a **colloquial style transfer model** and **presupposition templates** to generate diverse claims. Experimental results show that models utilizing KG evidence, such as the **GEAR model**, outperform baselines that rely solely on textual claims. FactKG provides a valuable resource for research on fact verification and consistency checks between natural language and structured knowledge, with potential applications in dialogue systems and information validation.</sample>
    <sample id="248">Nein, die Annotatoren sind nicht in Bezug auf jede demographische Gruppe ausgewogen.</sample>
    <sample id="249">Durch das Hinzufügen von Störungen, die die relevanten Strukturen beibehielten, aber den Input veränderten.</sample>
    <sample id="250">Eine dimensionale Bewertung bedeutet, verschiedene Aspekte der Qualität eines Dialogmodells einzeln zu bewerten, um seine Stärken und Schwächen genauer zu verstehen.</sample>
    <sample id="251">Die Autoren gehören der University of Science and Technology of China an.</sample>
    <sample id="252">**Abstract:**  
This work introduces U-CREAT, an unsupervised pipeline for Prior Case Retrieval (PCR) in legal documents, along with the IL-PCR dataset, a new benchmark for PCR tasks in Indian law. Legal professionals often rely on cited precedents, but the growing volume of cases necessitates automated retrieval. U-CREAT employs an event-based approach, extracting subject-verb-object triplets from documents using dependency parsing. These events are then used to compute an interaction matrix, enabling efficient and accurate retrieval of relevant cases. Experiments on the IL-PCR and COLIEE datasets show that U-CREAT outperforms existing methods, including transformer-based models like BERT and supervised approaches. Event-based models, especially the Event Filtered Documents approach, achieve the highest F1 scores with lower inference times. The results highlight the effectiveness of event extraction in capturing factual similarities, demonstrating U-CREAT's generalization across legal systems. This work advances the field of legal information retrieval by providing a robust, unsupervised framework and a comprehensive dataset for future research.</sample>
    <sample id="253">**Abstract:**  
This paper introduces *DisorBERT*, a double domain adaptation model designed to detect signs of mental disorders in social media content. Mental disorders, such as depression and PTSD, are increasingly discussed online, offering opportunities for early detection through automated analysis. To address the challenge of limited annotated data, DisorBERT leverages domain adaptation, using a pre-trained language model (BERT) and adapting it to the specific language of Reddit and mental health discussions. Additionally, guided masking is employed to focus the model on clinically relevant terms. The model demonstrates superior performance on the eRisk dataset, achieving a balanced precision and recall compared to baseline models. Analysis shows that DisorBERT generates more psychologically oriented predictions, such as words related to mental health symptoms, and highlights key terms like "anxious" and "medication" in attention maps. Results indicate that DisorBERT outperforms MentalBERT, even with less training data, offering a promising approach for mental health monitoring. Future work includes exploring additional lexical resources and integrating clinical data.</sample>
    <sample id="254">**Abstract:**  
This paper presents an uncertainty-guided label denoising framework for document-level distant relation extraction (DocRE) to address the issue of noisy pseudo labels in distantly supervised data. Existing methods rely on pseudo labels generated by pre-trained models, which often introduce false positives and degrade performance. To mitigate this, we propose an uncertainty estimation approach based on Monte Carlo dropout to assess the reliability of model predictions. We further introduce an instance-level uncertainty estimation method to handle overlapping relations, which previous methods fail to address. A dynamic class uncertainty threshold strategy is designed to filter out unreliable pseudo labels, especially for long-tail relation classes. Additionally, a multi-phase training strategy is developed to iteratively refine the pseudo labels and enhance model performance. Experimental results on public datasets show that our framework outperforms existing baselines, achieving significant improvements in relation extraction accuracy. The contributions include a novel label denoising approach, instance-level uncertainty estimation for overlapping relations, dynamic thresholding for long-tail classes, and an effective multi-phase training strategy.</sample>
    <sample id="255">Die Form des Prompts ist wichtig bei zero- und one-shot Prompting, nicht jedoch bei five-shot Prompting.</sample>
    <sample id="257">Die Autoren haben vier state-of-the-art-Dialogmodelle evaluiert.</sample>
    <sample id="258">**Abstract:**  
This paper explores the feasibility of using large language models (LLMs) as an alternative to human evaluation in natural language processing tasks. While previous work, such as G-Eval, has used LLMs for evaluation, this study is among the first to systematically investigate their effectiveness. We evaluate whether LLMs can reliably rate the quality of text samples based on instructions, similar to human evaluators. Our experiments compare human-written and GPT-2-generated stories, rated on grammar, coherence, likability, and relevance. Human raters (English teachers) prefer human-written texts, and we find that certain advanced LLMs, such as Davinci and ChatGPT, show similar preferences, indicating their potential as evaluators. However, smaller models fail to capture this distinction. The study addresses questions about agreement between LLMs and humans, instruction wording, sampling methods, and cost-benefit analysis. Results suggest that while LLM evaluation is promising, its reliability and consistency depend on model size and instruction design. This work provides a foundation for future research on automated evaluation using large language models.</sample>
    <sample id="259">**Abstract:**  
This paper introduces XSemPLR, a unified benchmark for cross-lingual semantic parsing, supporting multiple natural languages and meaning representations. XSemPLR includes nine datasets across five tasks, eight semantic formalisms, and 22 languages from 15 language families, addressing gaps in existing benchmarks like limited language and representation coverage. We evaluate models under six settings, including monolingual, multilingual, and cross-lingual few- and zero-shot transfer. Encoder-Decoder models (e.g., mT5) outperform Encoder-PTR models (e.g., XLM-R + PTR) across all datasets. Training with multilingual data improves performance, though English performance sometimes declines—a phenomenon termed the "Curse of Multilinguality." Cross-lingual transfer gaps are significant in zero-shot settings but reduced with few-shot data. Pretraining on English boosts few-shot performance, while models like Codex and BLOOM remain inadequate for cross-lingual semantic parsing. XSemPLR provides a comprehensive benchmark for advancing multilingual semantic parsing research.</sample>
    <sample id="260">Die Anzahl der Autoren wird im Text nicht genannt.</sample>
    <sample id="261">Ein guter Planer sollte Skripte erstellen, die sowohl sinnvoll als auch den gegebenen Einschränkungen treu sind.</sample>
    <sample id="262">Die Anzahl der Autoren wird im Text nicht genannt.</sample>
    <sample id="263">In this work, we investigate label biases in in-context learning, a key paradigm for utilizing large language models. We introduce a typology of label biases, identifying a novel type called domain-label bias, which arises from the influence of task-specific vocabulary on model predictions. Through experiments, we show that domain-label bias significantly affects model performance, especially in tasks where in-context learning struggles to outperform random guessing. To address these biases, we propose domain-context calibration, a method that uses random in-domain words as content-free text to estimate and correct label biases. Our experiments across multiple datasets and models demonstrate that domain-context calibration significantly improves in-context learning performance, particularly for tasks with high domain-label bias. Furthermore, we show that using diverse, domain-specific content-free text leads to better calibration than traditional approaches. This work provides a systematic understanding of label biases and offers an effective method to enhance the reliability of in-context learning in large language models.</sample>
    <sample id="264">**Abstract:**  
This paper introduces TAVT, a novel framework for Transferable Audio-Visual Text Generation, aiming to address the challenges of domain shifts in multimodal text generation. Unlike traditional uni-modal approaches, TAVT enables models to adapt quickly to new domains with limited labeled data by leveraging a unified audio semantic space. The framework consists of three components: an audio-visual meta-mapper network, a transformer-based encoder-generator, and Dual Counterfactual Contrastive Learning (DCLL). The meta-mapper aligns visual concepts across domains using a pre-clustered audio semantic space, while DCLL enhances visual-text alignment through fine-grained supervision without relying on negative samples. Experiments on MSVD and MSR-VTT benchmarks demonstrate that TAVT outperforms state-of-the-art methods in both cross-dataset and cross-domain settings, especially in low-resource scenarios. The results highlight TAVT’s effectiveness in achieving robust and transferable audio-visual text generation across diverse domains.</sample>
    <sample id="265">Der Referent heißt Vasudha.</sample>
    <sample id="266">Die Autoren gehören der Universität Cambridge an.</sample>
    <sample id="268">Die häufigsten Fehler von PaLM sind Omission-Fehler, bei denen Teile des Quelltextes in der Übersetzung weggelassen werden.</sample>
    <sample id="269">Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und heute werden wir Ihnen alles über ABC-Eval, einen neuen dimensionsbasierten Ansatz zur Bewertung von Conversational AI, erzählen. Diese Arbeit wurde vom Emory NLP Lab unter der Leitung von Professor Jinho Choi an der Emory University und in Zusammenarbeit mit Amazon Alexa AI durchgeführt. Stellen Sie sich vor, Sie haben gerade ein Dialogmodell entwickelt und möchten wissen, wie gut es sich mit dem aktuellen Stand der Technik vergleichen lässt. Die übliche Praxis besteht darin, menschliche Bewertungen zu verwenden, beispielsweise indem menschliche Gutachter entscheiden, welcher der beiden Dialoge besser ist, oder Dialoge anhand einer Likert-Skala bewerten. Diese Methoden funktionieren gut, um eine umfassende Bewertung der allgemeinen Qualität eines Dialogs zu liefern. Allerdings hat die Qualität eines Dialogs viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chat-Qualität bewerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen. Ein Ansatz besteht darin, einfach menschliche Gutachter zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, wie beispielsweise die Relevanz der Antwort des Modells, wobei bestehende vergleichende oder Likert-Skala-Methoden verwendet werden. Wir glauben jedoch, dass es einen präziseren und zuverlässigeren Strategie für die dimensionsbasierte Bewertung von Dialogen gibt. Unserer Ansatz versucht, die Subjektivität menschlicher Bewertungen zu verringern, indem er explizit annotiert, ob die Antwort des Modells bestimmte Verhaltensweisen ausdrückt, wie z. B. irrelevantes Information zu liefern oder sich selbst zu widersprechen. Wir bezeichnen diesen Ansatz als Annotation von Verhaltensweisen im Chat oder kurz ABC-Eval. Wir haben diese Methode entwickelt, um die in der neuesten Literatur erwähnten Verhaltensweisen von Chat-Modellen umfassend abzudecken, die die Chat-Qualität beeinflussen. ABC-Eval ist in der Lage, die Häufigkeit, mit der Chat-Modelle verschiedene thematische Fehler begehen, zu messen. Zum Beispiel misst ABC-Eval die Anzahl der Dialoge, in denen ein Chat-Modell seinen Gesprächspartner ignoriert oder etwas Irrelevantes sagt, sich selbst oder seinen Gesprächspartner widerspricht, falsche Fakten erfindet oder allgemein bekannte Wissensregeln verletzt, und in denen das Modell Empathie zeigt oder nicht. Um herauszufinden, welche Art von Bewertung am effektivsten ist, haben wir vier etablierte Chat-Modelle ausgewählt und sie jeweils anhand von 100 menschlich-roboter-Dialogen mit ABC-Eval bewertet. Zum Vergleich haben wir diese Dialoge auch mit drei bestehenden Methoden bewertet: Likert-Bewertungen auf der Ebene einzelner Dialoge, Likert-Bewertungen auf der Ebene des gesamten Dialogs und Paarvergleiche auf der Ebene des gesamten Dialogs. Für jede dieser bestehenden Methoden haben wir Bewertungen für acht der am häufigsten gemessenen Aspekte des Dialogs gesammelt, da dies die Standardpraxis ist, um Chat-Modelle anhand mehrerer Dimensionen zu bewerten. Aus der Analyse dieser Bewertungsergebnisse haben wir festgestellt, dass die ABC-Eval-Verhaltenslabels insgesamt zuverlässiger sind als Labels, die durch bestehende Methoden gesammelt wurden, wie anhand der Inter-Annotator-Übereinstimmung bei 100 doppelt annotierten Dialogen gemessen. Darüber hinaus sind die ABC-Eval-Labels besser in der Vorhersage der allgemeinen Gesprächsqualität als die Metriken, die durch bestehende Methoden erzeugt werden, wie anhand dieser einfachen linearen Regression gezeigt. Zum Beispiel können Sie sehen, wie das Messen des Anteils der Dialoge mit Selbst- und Partnerwidersprüchen jeweils 5 % und 10 % der Gesprächsqualität erklären, während die durchschnittlichen Likert-Konsistenzbewertungen lediglich 4 % oder weniger erklären. Schließlich haben wir geprüft, ob jeder Bewertungsmaßstab einen einzigartigen Aspekt der Chat-Qualität erfasst, indem wir eine schrittweise lineare Regression durchgeführt haben. Sie können sehen, wie die Kombination aller ABC-Eval-Metriken über 25 % der Gesprächsqualität erklärt, und wenn Sie die Metriken nacheinander entfernen, verlieren Sie in den meisten Fällen eine beträchtliche Menge an Informationen über die Qualität. Auf der anderen Seite erklärt die Kombination aller Likert-Metriken auf der Ebene eines Dialogs deutlich weniger der Qualität, und weniger dieser Metriken tragen einzigartige Informationen bei. Diese zuverlässigen, informativen und unterschiedlichen ABC-Eval-Metriken ermöglichen es uns, Conversational AI mit einer höheren Auflösung zu bewerten, als es bisherige Methoden schaffen. Sie können das an den Ergebnissen unseres Experiments sehen, bei dem immer noch einige Herausforderungen bestehen und präzise quantifiziert wurden. Zum Beispiel haben die von uns getesteten Bots in etwa 20 % ihrer Antworten Verstöße gegen das allgemeine Wissen. Sie produzieren in etwa 15 % der Antworten irrelevantes Material, und sie widersprechen sich selbst oder ihrem Gesprächspartner in etwa 10 % der Fälle. Angesichts der schnellen Fortschritte im Bereich der Conversational AI könnten viele dieser Fehlerquoten in neuen Modellen, die seit unserem Evaluationszeitpunkt veröffentlicht wurden, abnehmen. Allerdings ist dies umso mehr ein Grund, zuverlässige und präzise Bewertungskriterien zu verfolgen, um Modelle miteinander zu vergleichen. Wir hoffen, dass ABC-Eval von anderen im Bereich genutzt werden kann, als bedeutender Schritt in diese Richtung. Wir freuen uns darauf, zu sehen, wie Conversational AI in den nächsten Monaten und Jahren weiterentwickelt wird. Vielen Dank fürs Zuschauen.</sample>
    <sample id="270">Die Autoren gehören der Emory University an.</sample>
    <sample id="271">CFT steht für "Fine-tuning" (Feinabstimmung) in dieser Arbeit.</sample>
    <sample id="272">An der Arbeit sind 7 Autoren beteiligt.</sample>
    <sample id="273">Hallo, mein Name ist Kayo Yin und ich werde unsere Arbeit mit dem Titel „Wann benötigt Übersetzung Kontext? Eine datengetriebene, multilinguale Untersuchung“ vorstellen. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig erstellt. Viele Übersetzungen hängen von Kontext ab. Zum Beispiel: Wie würden wir das Wort „Mole“ in diesem Satz übersetzen? Wenn der vorherige Satz lautete: „Dinge könnten gefährlich werden, wenn die Minister es herausfinden“, bezieht sich „Mole“ auf einen Spion. Wenn der vorherige Satz jedoch lautete: „Könnte es etwas Ernstes sein, Doktor?“, bezieht sich „Mole“ auf einen Muttermal. Abhängig vom Kontext ändert sich die Bedeutung des Wortes und somit auch die Übersetzung. Allerdings ist es schwierig, zu bewerten, wie gut Modelle solche Fälle übersetzen. Erstens, weil nur ein kleiner Teil der Übersetzungen vom Kontext abhängt, wodurch metrische Maße wie BLEU auf Korpus-Ebene diese Übersetzungen nicht erfassen können. Zudem haben einige Forscher vorgeschlagen, gezielte Evaluierungen für kontextabhängige Übersetzungen durchzuführen, doch solche Ressourcen unterstützen nur begrenzte Arten kontextabhängiger Übersetzungen und begrenzte Sprachmengen, da sie in der Regel auf Fachwissen und menschliche Kuratierung zurückgreifen. In dieser Arbeit versuchen wir, zwei Fragen zu beantworten: Erstens: Wann benötigt eine Übersetzung Kontext? Zweitens: Wie gut können Modelle solche Fälle behandeln? Um die erste Frage zu beantworten, haben wir begonnen, zu messen, wie stark ein Wort in der Übersetzung vom Kontext abhängt. In früheren Arbeiten haben wir CXMI als Maß für die Nutzung von Kontext durch maschinelle Übersetzungssysteme eingeführt. Dabei wird gemessen, wie viel Information der Kontext C über das Ziel Y liefert, gegeben die Quelle X. Man kann CXMI als die Information betrachten, die das Modell durch den Kontext gewinnt. In dieser Arbeit haben wir CXMI auf Pointwise CXMI erweitert, das die Nutzung von Kontext auf Satz- oder Wortebene messen kann. Wörter mit hohem P-CXMI sind solche, die für die Übersetzung Kontext benötigen. Jetzt analysieren wir Wörter mit hohem P-CXMI, um Muster zwischen diesen Wörtern zu erkennen. Unsere Analyse führen wir anhand von TED-Transkripten durch, die aus dem Englischen in 14 verschiedene Sprachen übersetzt wurden. Wir analysieren auf drei unterschiedlichen Ebenen. Zunächst betrachten wir Wortarten mit hohem mittlerem P-CXMI. Dadurch können wir beispielsweise die dualen Pronomen im Arabischen erkennen, die einen relativ hohen P-CXMI aufweisen. Dies lässt sich erklären, da das Englische keine dualen Pronomen hat, weshalb man beim Übersetzen ins Arabische Kontext benötigt, um zu erkennen, ob ein Pronomen dual ist. Ähnlich finden wir heraus, dass in bestimmten Sprachen auch Kontext benötigt wird, um die passende Verbform zu wählen. Dann betrachten wir Vokabellisten, bei denen das durchschnittliche P-CXMI über alle Vorkommnisse hinweg hoch ist. Dies hilft uns, Fälle wie diesen zu identifizieren, bei denen im Chinesischen Kontext benötigt wird, um eigennamen korrekt zu übersetzen und sicherzustellen, dass im Dokument die gleiche Übersetzung verwendet wird. Ähnlich ist Kontext für die korrekte Formularität bei der Übersetzung wichtig. Schließlich betrachten wir einzelne Token mit hohem P-CXMI. Dadurch können wir Phänomene erkennen, die nicht allein durch das Wort selbst, sondern vielmehr durch die Satzstruktur ausgedrückt werden, wie zum Beispiel die Auflösung von Ellipsen. Jetzt nutzen wir unsere Erkenntnisse aus der Analyse, um einen Benchmark für die Dokumentübersetzung zu erstellen. Für jede der fünf diskursiven Phänomene, die wir identifiziert haben, erstellen wir Tagger, um Wörter zu identifizieren, die sich auf das Phänomen beziehen. Wir nennen unseren Tagger „Multilingual Discourse-Aware“ oder MuDA-Tagger. Wir können auch feststellen, dass verschiedene Sprachen unterschiedliche Proportionen dieser diskursiven Phänomene aufweisen. Danach verwenden wir den MuDA-Tagger, indem wir ihn auf ein paralleles Korpus anwenden, das wir für die Evaluation verwenden möchten, und wenden anschließend unsere gewählten Übersetzungsmetriken auf die kontextabhängigen Beispiele an, die der MuDA-Tagger identifiziert hat. Schließlich verwenden wir unseren Benchmark sowie andere Metriken, um verschiedene Modelle bei der Dokumentübersetzung zu evaluieren. Zunächst verwenden wir corpusbasierte Metriken: Bei BLEU erzielen Modelle, die nicht kontextsensitiv sind, die beste Leistung. Wenn wir jedoch COMET verwenden, leisten kontextsensitive Modelle die beste Leistung. Wenn wir die Wort-f-Maßzahl verwenden, sind Modelle mit und ohne Kontext vergleichbar. Dies zeigt erneut, wie schwierig es ist, das beste Dokumentübersetzungssystem zu bestimmen, wenn man sich allein auf corpusbasierte Metriken verlässt. Jetzt verwenden wir den MuDA-Benchmark, um Modelle zu evaluieren, und stellen fest, dass kontextsensitive Modelle in bestimmten diskursiven Phänomenen, wie Formalität und lexikalische Kohäsion, signifikant genauer sind als Modelle, die keinen Kontext verwenden. Diese Modelle sind jedoch nicht viel besser als Modelle ohne Kontext bei anderen Phänomenen wie Ellipsen, Pronomen und Verbformen. Dies deutet darauf hin, in welchen Bereichen wir noch Fortschritte bei der Dokumentübersetzung benötigen. Wir haben auch verschiedene kommerzielle Systeme verglichen, und unser Benchmark zeigt, dass DeepL in der Regel genauer ist als Google Translate bei der Dokumentübersetzung. Zusammenfassend haben wir eine datengetriebene Analyse über 14 Sprachpaare durchgeführt, um zu erkennen, wann Übersetzungen Kontext benötigen, und nutzen unsere Erkenntnisse, um einen Benchmark für die Dokumentübersetzung zu erstellen, der uns helfen kann, zu erkennen, welche diskursiven Phänomene Modelle gut oder schlecht bewältigen und welche Übersetzungssysteme gut bei der Dokumentübersetzung sind. Vielen Dank für Ihre Aufmerksamkeit. Bis nach Toronto.</sample>
    <sample id="274">Der Referent heißt Yusen Zhang.</sample>
    <sample id="276">**Abstract:**  
This work introduces *IndicMT Eval*, a dataset for meta-evaluating machine translation (MT) metrics for Indian languages. While English-centric MT metrics are widely studied, their applicability to other languages—especially Indian languages—is understudied. We focus on five Indian languages from two language families (Dravidian and Indo-Aryan) and generate 7,000 candidate translations using seven MT systems. Human annotators evaluate these translations, identifying error types and severity using the MQM framework. Our analysis shows that overlap-based metrics like chrF have high correlation with human scores but perform poorly overall. Embedding-based metrics, particularly BERTScore with MuRIL embeddings, and COMET variants show better performance. Fine-tuning COMET on our dataset results in *IndicCOMET MQM*, which outperforms the baseline COMET on most languages and demonstrates strong zero-shot generalization. The dataset and findings highlight the need for language-specific evaluation metrics and provide a valuable resource for future research in MT evaluation for Indian languages.</sample>
    <sample id="277">Die neue Methode hat keinen Namen.</sample>
    <sample id="278">Die Autoren beschreiben die Methode der „markierten Wörter“ als eine Technik, um Wörter zu identifizieren, die markierte Gruppen von unmarkierten Gruppen abheben, indem sie gewichtete Log-Odds-Verhältnisse verwenden, um die charakteristischen Wörter jeder Gruppe zu unterscheiden.</sample>
    <sample id="279">Die Autoren gehören der University of Washington an.</sample>
    <sample id="280">**Abstract:**  
This paper introduces MultiEMO, an attention-based correlation-aware multimodal fusion framework for Emotion Recognition in Conversations (ERC). Existing methods often fail to effectively exploit multimodal complementarity, struggle with minority emotion classes, and face challenges in distinguishing semantically similar emotions. To address these issues, MultiEMO comprises four components: unimodal feature extraction, context modeling, multimodal fusion via MultiAttn, and emotion classification with a Sample-Weighted Focal Contrastive loss. The proposed VisExtNet extracts visual features by focusing on facial expressions without redundant scene information. MultiAttn integrates modalities through bidirectional multi-head cross-attention layers, enabling effective fusion of textual, audio, and visual cues. The Sample-Weighted Focal Contrastive loss enhances classification of minority and similar emotions by assigning higher weights and maximizing inter-class distances. Extensive experiments on MELD and IEMOCAP datasets demonstrate state-of-the-art performance, particularly in minority and semantically similar emotion classes. Visualizations confirm MultiEMO's effectiveness in challenging scenarios. Despite its strengths, limitations include speaker-irrelevant visual processing and dependency on large batch sizes.</sample>
    <sample id="281">**Abstract:**  
This work investigates when translation requires context and how well models handle such cases across 14 language pairs. Using a data-driven approach, we introduce Pointwise CXMI to measure context dependency at the word and sentence levels, identifying words that require contextual information for accurate translation. Analysis of TED talk transcripts reveals patterns in context-dependent translation, such as dual pronouns in Arabic, proper nouns in Chinese, and formality in various languages. Based on these findings, we develop the MuDA tagger, a multilingual tool to identify discourse phenomena requiring context. Using MuDA, we create a benchmark for document-level translation, evaluating models with both corpus-level and context-aware metrics. Results show that context-aware models outperform context-agnostic ones in phenomena like formality and lexical cohesion, but perform similarly on others like ellipsis and pronouns. The benchmark also highlights that DeepL often outperforms Google Translate in document-level translation. This work provides insights into context-dependent translation challenges and offers a valuable resource for evaluating document-level machine translation systems.</sample>
    <sample id="282">**Abstract:**  
This paper introduces StoryTrans, a novel approach for non-parallel story-level author-style transfer at the discourse level. Unlike prior work focusing on token- or sentence-level style transfer, StoryTrans addresses the challenge of capturing and imitating complex author-specific discourse structures and linguistic preferences in long texts. The method learns discourse representations from source texts and combines them with learnable style embeddings to generate target-style stories. To enhance content preservation and style alignment, StoryTrans employs a two-stage training framework: first, transferring style while masking content keywords, and second, filling in the masked content. Experimental results on newly collected Chinese and English datasets demonstrate that StoryTrans outperforms strong baselines in style control and content preservation. Both automatic and manual evaluations confirm its effectiveness, while style visualization shows alignment with target styles in the feature space. StoryTrans successfully enriches storylines with relevant content and maintains original semantics, offering a significant advancement in author-style transfer at the discourse level.</sample>
    <sample id="283">Prague</sample>
    <sample id="284">**Abstract**  
This paper introduces FSUIE, a novel framework for Universal Information Extraction (UIE) that addresses the limitations of traditional span-based models. Current models heavily rely on precise span boundaries, which are ambiguous in practice. To resolve this, FSUIE introduces a *fuzzy span loss* that represents target boundaries as continuous probability distributions, allowing for more flexible and robust extraction. Additionally, we propose *fuzzy span attention* to adaptively adjust the attention span during extraction, enhancing model generalization and reducing over-reliance on exact boundary positions. The attention mechanism dynamically adjusts its range and smoothly decays at the boundaries, improving the model's ability to capture relevant context. Experiments on three IE tasks—named entity recognition, relation extraction, and aspect sentiment triplet extraction—show that FSUIE achieves state-of-the-art results across multiple benchmarks, including ACE, ADE, and AST-V2 datasets. Ablation studies confirm the effectiveness of both the fuzzy span loss and attention mechanisms. FSUIE demonstrates strong performance and generalization, particularly on small-scale data, making it a promising approach for universal information extraction.</sample>
    <sample id="285">**Abstract:**  
This work addresses factual error correction (FEC) in dialogue summarization, a largely unexplored area. While existing FEC models for general summarization rely on factuality metrics like FactCC and DAE, we identify critical flaws in their evaluation: vague overall scores and the inability to distinguish between error correction and summary generation. To overcome these issues, we introduce manually annotated reference corrections, enabling precise evaluation of FEC models. We propose a new taxonomy of factual errors, distinguishing between content-based and form-based categories. Our evaluation framework, built on ERRANT, includes alignment, classification, and comparison steps. Experiments reveal that training FEC models with reference summaries from dialogue datasets improves performance, and combining human-annotated with synthetic data shows promise. However, current models struggle with specific error types like additions, attribute, modality, and link errors. This study highlights the need for refined evaluation methods and more targeted FEC approaches in dialogue summarization.</sample>
    <sample id="286">James Finch und Sarah Finch</sample>
    <sample id="287">Vier Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="288">BLiMP und SyntaxGym.</sample>
    <sample id="290">Die Abkürzungen der fünf Methoden für die erste Forschungsfrage sind nicht explizit genannt.</sample>
    <sample id="291">Das Modell wird anhand von 11 biomedizinischen und klinischen Downstream-Aufgaben in Französisch evaluiert, darunter Named Entity Recognition, Klassifikation, Part-of-Speech Tagging und Fragebeantwortung.</sample>
    <sample id="294">CamemBERT wurde ursprünglich mit OSCAR-Daten trainiert.</sample>
    <sample id="295">Der Referent heißt Adam Przepiórkowski.</sample>
    <sample id="296">This work explores the challenges of natural language understanding through a collaborative study between the University of Turin and Amazon Alexa, focusing on irony detection. Traditional approaches rely on supervised learning with manually annotated data, assuming a single "ground truth." However, this study highlights the limitations of this assumption by analyzing inter-annotator variability in labeling irony. The EPIC corpus, consisting of 300 short English conversations from social media, was annotated by 74 participants across five English varieties. Results show significant differences in annotations based on annotator demographics, such as age and nationality. The study introduces "perspective-aware" models, trained on data split by annotator groups, which demonstrate higher confidence in predictions compared to aggregated "gold standard" models. Notably, younger generations and annotators from the UK and Ireland showed the highest disagreement. These findings suggest that irony perception is subjective and influenced by cultural and generational factors, calling for more nuanced approaches in natural language processing.</sample>
    <sample id="297">**Abstract:**  
This work explores the concept of dogwhistles—coded terms that convey hidden, often offensive meanings to specific in-groups while appearing neutral to outsiders. Using a glossary of over 340 terms, including racist, transphobic, and anti-Semitic examples, the study analyzes historical U.S. political speeches, revealing a strong correlation between the use of racial dogwhistles and the Republican Southern Strategy. The research also evaluates how language models like GPT-3 recognize and interpret dogwhistles, finding that performance varies significantly depending on the term’s register and context. Additionally, the study demonstrates that replacing explicit slurs with dogwhistles in hate speech can reduce automated toxicity detection scores, highlighting how such coded language evades content moderation. By developing a typology and contextual analysis, this work advances understanding of dogwhistles as a mechanism of political influence and a challenge for NLP systems.</sample>
    <sample id="298">Die Ergebnisse zeigten, dass die Leistung von Modellen mit zunehmender zeitlicher Differenz zwischen Trainings- und Testdaten abnimmt, was die Hypothese der zeitlichen Verzögerung bestätigte.</sample>
    <sample id="299">**Abstract:**  
This work introduces a minimax training approach to improve the robustness of Natural Language Inference (NLI) models against spurious correlations (shortcuts) in training data. While NLI models achieve strong performance on standard benchmarks, they often rely on shortcuts that hinder out-of-distribution generalization. Existing shortcut mitigation methods typically require domain-specific knowledge or pre-trained auxiliary models, which may not align with the learner's behavior. To address this, we propose a minimax framework where a learner model minimizes task loss, while an auxiliary model maximizes the learner's loss by generating example weights that emphasize under-represented, hard instances. This encourages the learner to focus on examples that counteract shortcut dependencies. The method does not assume prior knowledge of shortcuts and uses a feed-forward network as the auxiliary. Evaluated on MNLI, FEVER, QQP, and out-of-distribution test sets, our approach improves out-of-distribution performance while maintaining in-distribution accuracy. We also analyze generalization to larger models and synthetic shortcuts, demonstrating the method's versatility and effectiveness.</sample>
    <sample id="300">**Abstract:**  
This work introduces *interactive dictation*, a novel task enabling users to naturally dictate and edit documents using voice commands without fixed triggers. Unlike conventional speech-to-text systems, which support only dictation, interactive dictation allows seamless interleaving of dictation and editing via intuitive natural language utterances. The task involves four key steps: speech recognition, segmentation of dictation and commands, normalization of commands, and execution of actions to update the document. To advance this task, the authors present a custom data collection interface and dataset, capturing real-world interactions. A baseline system is developed using separate models for segmentation, ASR repair, and interpretation, with experiments comparing T5 and GPT-3 architectures. Results show a trade-off between runtime and accuracy, with GPT-3 achieving higher accuracy but slower performance. The work opens new research directions in natural, flexible voice-based document editing and provides code and a dataset to support future studies.</sample>
    <sample id="302">Es ist notwendig, die Token für die Ausgabesequenz zu permutieren, weil sie nach der ersten Schritt zwar alle vorhanden sind, aber nicht in der richtigen Reihenfolge stehen.</sample>
    <sample id="303">Die Autoren empfehlen mehr Transparenz, weil sie nicht wissen, ob die positiven Stereotype durch übermäßige Wertausrichtung oder andere Anti-Stereotype-Methoden entstehen, und ohne Transparenz können solche Muster nicht weiter untersucht werden.</sample>
    <sample id="304">Inakzeptable Minimalpaareingaben sind Sätze in einem Minimalpaar-Test, die als ungrammatikalisch oder unakzeptabel gelten und mit ihren akzeptablen Gegenstücken verglichen werden, um die Fähigkeit von Sprachmodellen zu bewerten.</sample>
    <sample id="305">**Abstract:**  
This work critically examines the assumptions and practicality of Weakly Supervised Learning (WSL), highlighting that recent methods rely on clean validation data for proper functioning. While WSL claims to train models solely on weakly labeled data, our findings reveal that performance drops significantly without clean validation samples. We show that even a small number of clean samples per class (e.g., 20) can greatly improve WSL performance. Moreover, directly fine-tuning on these clean samples outperforms complex WSL approaches, suggesting that simpler methods like fine-tuning are more efficient. Our results challenge the overestimation of WSL's benefits and emphasize the need for clean data in model selection. We recommend reporting validation criteria, comparing WSL with few-shot learning, and considering fine-tuning as a baseline. Our findings urge the community to reassess the practicality and cost-effectiveness of WSL approaches.</sample>
    <sample id="306">In this work, Sebastian Schuster and Najoung Kim investigate the ability of large language models to track entities across discourse. They present a task where models must predict the contents of boxes after a sequence of operations, requiring them to maintain and update entity states. The task is designed to prevent reliance on shortcuts such as copying initial states or using heuristics based on word associations. Experiments with models like Flan-T5 and GPT-3/3.5 show that most models fail to track entity states beyond simple repetition of initial conditions. However, GPT-3.5 models, which were pre-trained on substantial code, demonstrate non-trivial tracking abilities. Smaller models like T5-base can learn the task with fine-tuning, but randomly initialized models cannot. The findings suggest that pre-training on code may be crucial for developing entity tracking capabilities in language models. While the results are promising, further research is needed to determine the generalizability of these abilities beyond the specific task setup.</sample>
    <sample id="307">Die Autoren haben keine spezifischen Bewertungsmetriken namentlich genannt, erwähnen aber verschiedene downstream Tasks wie Named Entity Recognition, Klassifikation, Part-of-Speech Tagging und Fragebeantwortung, auf denen die Modelle evaluiert wurden.</sample>
    <sample id="308">**Abstract:**  
This work introduces *NLPositionality*, a framework to characterize design biases in NLP datasets and models by examining their alignment with diverse user populations. Through re-annotation of datasets by a global, demographically diverse group of over 1,000 annotators from 87 countries, we compare human judgments with model predictions and dataset labels using Pearson’s R correlation. Our findings reveal that NLP datasets and models exhibit positionality, aligning more closely with English-speaking countries and individuals with higher education, while underrepresenting non-binary individuals and other marginalized groups. We argue that positionality stems from the demographics and perspectives of annotators and developers, influencing model outcomes. To address these biases, we recommend documenting design choices, adopting a perspectivist research approach, and creating community-specific datasets and models. This work underscores the importance of inclusivity in NLP, highlighting the need for more representative and socially aware AI systems.</sample>
    <sample id="309">Die Übereinstimmung zwischen den Kommentatoren wurde mit der Inter-Annotator-Agreement-Metrik gemessen.</sample>
    <sample id="310">Wikipedia wurde als Domain gewählt, um völlig unzusammenhängende Sätze zu den inakzeptablen und akzeptablen Suchanfragen hinzuzufügen.</sample>
    <sample id="311">Die Autoren gehören der Universität zu Köln an.</sample>
    <sample id="312">MultiInstruct unterscheidet sich durch sein erstes großes, vielfältiges Multi-Modell-Instruktionstuning-Dataset mit 62 unterschiedlichen Aufgaben aus 10 Kategorien und fünf Experteninstruktionen pro Aufgabe.</sample>
    <sample id="313">Die Anzahl der Autoren wird nicht explizit genannt, aber die Arbeit wurde vom Emory NLP Lab unter der Leitung von Professor Jinho Choi und in Zusammenarbeit mit Amazon Alexa AI durchgeführt. Es wird jedoch nicht angegeben, wie viele Autoren genau beteiligt waren.</sample>
    <sample id="314">Die Definition der binären Koordination bezieht sich auf eine Koordination, bei der zwei Konjunkte durch ein Koordinationsmittel (wie "und") verbunden sind.</sample>
    <sample id="315">Die Studie erwähnt nicht, wie lange die verwendeten Prompts im Durchschnitt waren.</sample>
    <sample id="316">Die Ergebnisse zeigen, dass das kleinere T5-Modell, nach der Feinabstimmung auf CoScript, Skripte höherer Qualität generieren kann als viele große Sprachmodelle.</sample>
    <sample id="317">**Abstract**  
This paper introduces CodeIE, a novel approach that leverages large code generation models for few-shot information extraction. Traditional information extraction models, such as T5 and GPT-3, face challenges due to the mismatch between structured outputs during inference and unstructured text during pre-training. To address this, CodeIE transforms information extraction into a structure-to-structure code generation task, using code language models like Codex. By employing code-style prompts, the model can generate structured outputs more effectively. Experiments on three named entity recognition and four relation extraction datasets show that CodeIE significantly outperforms traditional text-based models, especially in few-shot settings. The analysis reveals that code models better align with the structure of information extraction tasks, reduce structural errors, and achieve higher recall compared to text-based prompts. Additionally, Codex consistently outperforms GPT-3 in both accuracy and label consistency. This work highlights the potential of code generation models in enhancing few-shot information extraction tasks.</sample>
    <sample id="318">Hallo, ich bin Yanis Labrak und ich werde Ihnen unsere Arbeiten zu „DrBERT: Ein robuster vortrainierter Modell in Französisch für die biomedizinischen und klinischen Bereiche“ präsentieren. In dieser Präsentation sprechen wir zunächst über das Sprachmodellieren im Gesundheitswesen. Anschließend stellen wir die Hauptbeiträge unseres Artikels vor. Wir führen das erste biomedizinische Modell in Französisch ein, das DrBERT, das auf RoBERTa basiert und auf NACHOS trainiert wurde, einem Datensatz medizinischer Daten, die aus dem Web heruntergeladen wurden. Wir stellen auch eine Vergleichsanalyse von Modellen mit verschiedenen Vortrainierungssettings und Datensätzen vor. Danach präsentieren wir unsere Ergebnisse auf 11 biomedizinischen und klinischen Downstream-Aufgaben in Französisch. Schließlich ziehen wir Schlussfolgerungen aus den Experimenten und geben Ihnen weitere Details darüber, wie Sie auf diese Modelle zugreifen können.

Seit seiner Veröffentlichung im Jahr 2018 hat BERT sich zu einem der effektivsten Ansätze zur Lösung von Aufgaben im Bereich des natürlichen Sprachverstehens entwickelt und bietet im Vergleich zu historischen statischen und kontextuellen Methoden wie Word2vec, fastText und anderen enorme Leistungssteigerungen. Seitdem wurde dieses Modell auf viele andere Sprachen angepasst, wie beispielsweise auf Französisch mit CamemBERT, und auch auf Bereiche wie die Biomedizin mit PubMedBERT und BioBERT sowie auf klinische Anwendungen mit ClinicalBERT – meist jedoch in englischer Sprache. Spezialisierte Modelle für andere Sprachen sind jedoch selten und basieren oft auf kontinuierlichem Vortrainieren aufgrund des Mangels an in-domain-Daten. Bislang gab es jedoch keine offensichtliche, öffentlich verfügbare Modell für die Biomedizin auf Französisch.

Daher fragten wir uns, welche Datenquellen am besten für eine breite Anwendung geeignet sind und ob diese gescraped Daten eine gute Alternative für klinische Daten darstellen. Um diese Frage zu beantworten, vergleichen wir DrBERT mit unserem ChuBERT-Modell, das auf anonymisierten Daten aus dem Datenlager des Universitätsklinikums Nantes basiert. Anschließend fragten wir uns, wie viel Daten benötigt werden, um ein spezialisiertes Modell auf französischen Daten zu trainieren: 4 GB, 8 GB oder mehr?

Um diese Frage zu beantworten, trainieren und vergleichen wir vier von-Null-Modell: eine erste Version von DrBERT, trainiert auf 7 GB NACHOS-Daten; eine zweite Version von DrBERT, trainiert auf 4 GB NACHOS-Daten; eine erste Version von ChuBERT, ein klinisches Modell, trainiert auf 4 GB Sätzen aus klinischen Notizen; und eine letzte Version von ChuBERT, trainiert auf einer Mischung aus 4 GB NACHOS-Daten und 4 GB klinischen Notizen.

Zusätzlich zu diesem Vergleich stellen wir drei Modelle vor, die auf kontinuierlichem Vortrainieren basieren, um den Einfluss der Vortrainierungsstrategie zu analysieren. Ein Modell basiert auf den Gewichten von CamemBERT und wurde auf einem 4 GB Datensatz aus NACHOS trainiert. Ein weiteres Modell basiert ebenfalls auf CamemBERT, wurde jedoch diesmal auf 4 GB klinischen Notizen trainiert. Schließlich ein Modell, das auf dem englischen biomedizinischen Modell PubMedBERT basiert und auf 4 GB NACHOS-Daten trainiert wurde. Insgesamt haben wir sieben Modelle.

Um unsere sieben Modelle zu bewerten, sammeln wir Daten für öffentliche und private Downstream-Aufgaben wie Named Entity Recognition, Klassifikation, Part-of-Speech Tagging und Fragebeantwortung. Diese Modelle werden mit sechs Basismodellen verglichen: CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT und ClinicalBERT.

Die Bewertung zeigt, dass die Modelle am besten in den Aufgaben abschneiden, die mit Daten gleicher Natur wie diejenigen trainiert wurden, auf denen das Modell trainiert wurde. Wir beobachten jedoch, dass Daten aus heterogenen Quellen vielseitiger sind. Wir stellen auch fest, dass das Verwenden von mehr Daten zu besseren Leistungen führt. Im Allgemeinen scheint das von-Null-Vortrainieren bei den meisten Aufgaben höhere Leistungen zu erzielen. Allerdings zeigte unser Experiment mit kontrolliertem Vortrainieren, bei dem die Gewichte und Tokenisierung von CamemBERT mit einem 4 GB Untermenge von NACHOS-Daten trainiert wurden, Ergebnisse, die mit DrBERT 4 GB von-Null-Vortrainierung vergleichbar sind. Dies ist jedoch nicht der Fall für das Modell, das auf CamemBERT-Gewichten und -Tokenisierung basiert, das von Stabilitätsproblemen betroffen ist.

Zur Schlussfolgerung zeigte unser eigenes System bessere Leistungen in neun der 11 Downstream-Aufgaben und übertraf insgesamt die Ergebnisse des generischen Modells, hier CamemBERT. Wir beobachten auch, dass spezialisierte Daten besser sind, aber nicht gut skalieren. Alle vortrainierten Modelle, die aus NACHOS stammen, sind kostenlos auf Hugging Face verfügbar und unter der MIT-Lizenz. Alle Trainings-Skripte befinden sich in unserem GitHub-Repository. Vielen Dank für diese Präsentation, und wir freuen uns auf den Austausch während der Poster-Session in Toronto.</sample>
    <sample id="319">In der Arbeit werden verschiedene Lernstrategien untersucht, darunter from-scratch-Pre-Training, kontinuierliches Pre-Training basierend auf CamemBERT und PubMedBERT sowie Vergleiche zwischen Modellen mit unterschiedlichen Datenquellen und -mengen.</sample>
    <sample id="320">Der Faktor der Überanpassung ist nicht vorhanden, da keine abnehmenden Renditen beobachtet wurden.</sample>
    <sample id="321">Die Qualität der Vereinfachung wurde durch die Analyse der Art der Vereinfachung und durch Evaluierung der Ergebnisse von automatischen Alignment-Methoden beurteilt.</sample>
    <sample id="322">**Abstract:**  
In this paper, we investigate what text classifiers learn about morality, focusing on the Moral Foundation Theory, which identifies five distinct moral foundations that shape human moral judgments. While previous NLP research often reduces morality to a single binary scale, we argue that morality is inherently pluralistic and context-dependent. Using the Moral Foundation Twitter Corpus, which contains 35,000 tweets across seven domains, we analyze how language models perceive moral expressions in different contexts. Our experiments reveal that models can detect domain-specific moral nuances, such as the differing associations of "subversion" in #AllLivesMatter and #BlackLivesMatter. While models recognize these variations, we caution that relying on a single classifier across diverse domains may lead to oversimplified or misleading moral interpretations. This work highlights the importance of domain-aware moral understanding in language models and underscores the need for explainable AI techniques to uncover how morality is expressed and interpreted in text.</sample>
    <sample id="323">**Abstract**  
This paper introduces DHLK, a novel approach for Commonsense QA that integrates language models with knowledge representation learning. Existing methods suffer from noisy entity retrieval and limited interaction between text and subgraph modalities. DHLK addresses these issues by constructing a Heterogeneous Knowledge Graph (HKG) using a two-stage pruning strategy, knowledge representation learning (KRL), and paraphrase retrieval from WordNet and Wiktionary. The HKG is optimized with TransE and modeled using Relation Mask Self-Attention (RMSA), inspired by RGAT, to capture semantic relationships. Entity and relation embeddings are refined through RMSA iterations, and graph embeddings are obtained via max-pooling. The QA context is enhanced with HKG path information and fused with language model embeddings through an MLP for final answer prediction. Experiments on CommonsenseQA and OpenBookQA using ConceptNet, WordNet, and Wiktionary show that DHLK outperforms existing LM and HKG-based methods, demonstrating its effectiveness in leveraging structured knowledge and language understanding for commonsense reasoning.</sample>
    <sample id="324">Ja, Sprachmodelle haben unterschiedliche politische Vorurteile.</sample>
    <sample id="325">Hallo! Mein Name ist Matthias Lindemann, und heute möchte ich Ihnen eine kurze Einführung in unser Paper zu „Compositional Generalization without Trees using Multiset Tagging and Latent Permutations“ geben. Dieses Paper entstand in Zusammenarbeit mit meinen Betreuern Alexander Koller und Ivan Titov. Compositional Generalization kann als die Fähigkeit eines Lernenden verstanden werden, tiefere Rekursion und bisher nicht gesehene Kombinationen von Phrasen zu bewältigen, die während des Trainings individuell gesehen wurden. Im Kontext der semantischen Parsing könnte ein Test auf compositional Generalization so aussehen: Wie üblich haben wir eine Trainingsmenge an Aussagen. In diesem Fall: „The girl slept.“ und „Mary knew that the girl slept.“ Diese Aussagen sind mit logischen Formeln verbunden, die zentrale Aspekte ihrer Bedeutung darstellen. Im Gegensatz zur Standardbewertung im maschinellen Lernen stammt die Testmenge nicht aus der gleichen Verteilung, sondern enthält logische Formeln mit strukturell neuen Elementen. In diesem Beispiel hat das Modell während des Trainings nur flache Rekursion gesehen und wird nun auf ein Beispiel mit tieferer Rekursion getestet. Naive seq2seq-Modelle haben Schwierigkeiten mit dieser Art von Generalisierung außerhalb der Verteilung und produzieren oft Ausgaben, die vom Eingabetext abgekoppelt sind. Insbesondere scheitern sie oft daran, systematische Korrespondenzen zwischen Eingabe und Ausgabe wie in dem Beispiel farbcodiert dargestellt, wiederzugeben. Eine beliebte Methode, um dies zu adressieren, ist die Integration von Bäumen in die Modelle. Die Bäume sollen den kompositionellen Prozess erfassen, der Aussagen mit logischen Formeln verknüpft. Dies funktioniert gut, aber Bäume werden normalerweise nicht vorgegeben und müssen auf irgendeine Weise erzeugt werden. Dies kann kompliziert und manchmal ein rechenintensiver Prozess sein. Typischerweise erfordert dies eine erhebliche vorab bearbeitende Schritte, die spezifisch für die Formalismen sind, z. B. um Variablenzeichen zu behandeln. Das Erzeugen von Bäumen kann auch spezialisierte Grammatikinduktionsverfahren erfordern. In diesem Paper verwenden wir keine Bäume und stellen ein neuronales seq2seq-Modell vor, das die Korrespondenzen zwischen Fragmenten der Eingabe und Fragmenten der Ausgabe direkt modelliert. Zum ersten Mal zeigen wir eine starke Generalisierung zu tieferer Rekursion ohne Abhängigkeit von Bäumen. Unser Ansatz erzeugt die Ausgabe aus der Eingabe in zwei Schritten. Zunächst wird jedem Eingabetoken eine ungeordnete Multimenge von Tokens zugewiesen, die in der Ausgabe auftreten. Nach dem ersten Schritt haben wir alle richtigen Tokens, doch sie sind nicht geordnet. Deshalb verwenden wir im zweiten Schritt ein weiteres Modell, um eine Permutation vorherzusagen, um sie in die richtige Reihenfolge zu bringen. Wir stellen eine neue Methode zur Vorhersage der Permutation vor, die keine harten Einschränkungen für die möglichen Permutationen vorgibt. Dies macht unseren Ansatz sehr flexibel und druckfähig. Konzeptionell funktioniert unser Permutationsmodell ungefähr so: Wir bewegen uns von links nach rechts über die Ausgabe und bestimmen, welches Multiset-Token in jeder Position platziert wird. Für die erste Ausgabeposition wählen wir einfach eines aus, wie hier in Rot hervorgehoben. Danach springen wir zu dem nächsten Multiset-Token, um das zweite Token in der Ausgabe zu bestimmen. Das dritte Token in der Ausgabe bestimmen wir auf ähnliche Weise, indem wir zu einem anderen Multiset-Token springen. Wir wiederholen diesen Prozess, bis jedes Token aus dem ersten Schritt genau einmal besucht wurde. Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen baumlosen Modellen am COGS-Benchmark. Unser Modell übertrifft die anderen deutlich bei der Generalisierung zu tieferer Rekursion. Einige andere Arten struktureller Generalisierung bleiben jedoch weiterhin sehr herausfordernd. In unserem Paper lösen wir einige interessante technische Herausforderungen. Zunächst ist die Ausrichtung zwischen Eingabe und Ausgabe in den Trainingsdaten nicht vorgegeben. Als Folge davon wissen wir für ein gegebenes Token nicht, aus welcher Multimenge es stammt, was ein Problem für das Training darstellt. Zudem können manchmal mehrere Permutationen mit den Daten kompatibel sein, wobei die sprachlich korrekte Permutation latent ist. Wir adressieren dies, indem wir die Ausrichtung als Teil des Trainings induzieren. Unsere Permutationsmethode ist sehr flexibel, bringt aber die Herausforderung mit sich, dass das Finden der höchstrangigen Permutation NP-schwer ist. Dies liegt daran, dass dies mit dem „Traveling Salesman“-Problem verwandt ist. Wir approximieren dies mit einer GPU-freundlichen kontinuierlichen Relaxation, die es uns auch ermöglicht, durch die Lösung zu backpropagieren und sprachlich plausible Permutationen zu lernen. Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen bewältigen möchten, schauen Sie sich bitte unser Paper an oder besuchen Sie unseren Poster.</sample>
    <sample id="326">Kognitive Dissonanz ist der Zustand von zwei widersprüchlichen Überzeugungen oder Handlungen, wie z. B. das Wissen, dass Zigaretten gefährlich sind, und dennoch das Rauchen.</sample>
    <sample id="327">**Abstract**  
This paper introduces ManagerTower, a novel vision-language (VL) architecture that enhances cross-modal representation learning by adaptively aggregating insights from pre-trained unimodal experts at different layers. Unlike previous two-tower and BridgeTower approaches, which either ignore or inefficiently utilize multi-layer unimodal representations, ManagerTower employs managers in each cross-modal layer to dynamically combine semantic knowledge from various depths of visual and textual encoders. This design enables more comprehensive cross-modal alignment and fusion. Using RoBERTa and CLIP-ViT as unimodal encoders, ManagerTower achieves superior performance on downstream tasks with only 4 million image-text pairs for pre-training. It outperforms models like METER and BridgeTower, including those trained on larger datasets or with more parameters. Visualizations of aggregation weights reveal that adaptive managers effectively exploit varying levels of unimodal knowledge across cross-modal layers. Our results demonstrate the effectiveness of adaptive aggregation in improving VL representation learning. The code and models are publicly available.</sample>
    <sample id="328">GPT-4 steht am meisten links.</sample>
    <sample id="329">**Abstract:**  
This paper presents a noise-resistant zero-shot video sentence localization method that eliminates the need for manual annotations. Existing methods generate simple pseudo-queries and pseudo-events, leading to misalignment and label noise. We propose a structured pseudo-label generation approach that first uses an image-text pre-trained model to generate complex free-form pseudo-queries. Then, we model event temporal structure to generate pseudo-events with high relevance within and low relevance outside the event. To reduce label noise, we apply sample re-weighting based on prediction confidence and IoU, and iteratively refine pseudo-labels. Experiments on ActivityNet Captions and Charades-STA show that our method, SPL, outperforms existing zero-shot approaches in terms of R@M and mIoU metrics. Our approach effectively handles label noise and achieves state-of-the-art performance in zero-shot video sentence localization.</sample>
    <sample id="330">Ja, kumulatives Training ist besser oder gleich gut wie iteratives Training für aktives Lernen.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">Die Daten für die MuDA-Benchmark stammen aus Transkripten von TED-Vorträgen, die ins Englische übersetzt wurden.</sample>
    <sample id="333">**Abstract:**  
In this work, we introduce INK, a novel training framework that injects kNN knowledge into neural machine translation (NMT) to improve generalization and performance. NMT models often suffer from sparse, non-smooth representation spaces, especially for low-frequency tokens, leading to poor predictions. While kNN-MT addresses this by leveraging nearest neighbors during decoding, it is computationally expensive and inflexible. To overcome these issues, INK employs an adapter to iteratively refine representations using kNN knowledge, aligning contextualized representations with token embeddings, kNN embeddings, and same-target tokens via KL-divergence. The updated representations are used to asynchronously refresh the datastore, enabling a self-improving loop. Experiments on the WMT’19 German-English task show that INK outperforms state-of-the-art kNN-MT systems, achieving higher BLEU and COMET scores with lower memory usage and faster inference. Results indicate that combining adapters and datastores further enhances representation smoothing, demonstrating the effectiveness of our approach in refining NMT models.</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">Sprachübergreifender Transfer ist das Training eines Modells auf einer Quellsprache und das Anwenden dieses Modells auf eine ZielSprache, ohne dass es explizit für diese ZielSprache trainiert wurde.</sample>
    <sample id="337">**Abstract:**  
This work introduces a graph-based approach for learning context-free out-of-vocabulary (OOV) word embeddings by leveraging word formation and association. Traditional embedding models struggle with OOV words, which are critical for downstream tasks. Our method constructs a Word Relationship Graph, where nodes represent words or wordpieces, and edges capture lexical relationships. By tokenizing OOV words into subunits and associating them with relevant words, we form a two-level graph structure. A self-attention mechanism assigns attributes to OOV nodes based on their characters, while a two-layer Graph Attention Network extracts meaningful representations. A readout block summarizes the graph-level information, and contrastive learning with NT-XENT loss aligns graph embeddings with background models. Experimental results show that our model outperforms baselines in both intrinsic and extrinsic tasks, demonstrating its effectiveness in learning OOV words. The approach is particularly suitable for agglutinative languages and performs well with English through proper word segmentation. The model's success hinges on the rationality of word decomposition, opening possibilities for multilingual applications.</sample>
    <sample id="338">**Abstract:**  
This work investigates the quality and utility of human-annotated natural language explanations in machine learning models. We challenge the assumption that human explanations are inherently beneficial and propose a systematic evaluation framework. Our contributions include a unified data structure that standardizes diverse tasks into a multiple-choice format, enabling consistent evaluation. Through preliminary experiments, we find that explanations do not always enhance model learning during fine-tuning and that their usefulness depends on task type and format. We introduce TREU, a novel metric extending the simulatability score, which evaluates explanation helpfulness during both fine-tuning and inference. Evaluating five datasets with T5 and BART, TREU outperforms existing metrics, revealing that even "low-quality" explanations can aid models. Our results highlight the task- and format-dependent nature of explanation utility, emphasizing the need for objective evaluation methods. This work provides a foundation for improving human–AI collaboration in explanation generation and model training.</sample>
    <sample id="339">Die Autoren gehören der Saarland University in Deutschland an.</sample>
    <sample id="340">**Abstract:**  
This paper introduces *ParaAMR*, a large-scale, syntactically diverse paraphrase dataset generated via AMR back-translation. Existing paraphrase datasets are either limited in scale or lack syntactic diversity. To address this, we leverage Abstract Meaning Representations (AMR), modifying their focus and structure to generate semantically equivalent but syntactically varied paraphrases. ParaAMR contains 15 million source sentences with approximately 6.9 paraphrases each, significantly outperforming other back-translation-based datasets in syntactic diversity while maintaining comparable semantic similarity. We evaluate ParaAMR on several NLP tasks, including sentence embeddings, syntactically controlled paraphrase generation, and few-shot learning. Results show that models trained on ParaAMR achieve better performance on the STS benchmark, demonstrate improved syntactic control, and enhance few-shot learning outcomes. Our findings highlight the value of syntactic diversity in paraphrase data for improving NLP applications. The dataset is publicly available.</sample>
    <sample id="341">Die Autoren verwenden die durchschnittliche Verzögerung (average lagging) und die computergestützte durchschnittliche Verzögerung (computational-aware average lagging) als Latenzmessungen.</sample>
    <sample id="342">**Abstract**  
This paper introduces LiveChat, a large-scale, personalized, and video-sourced dialogue dataset automatically constructed from live streaming content. Unlike existing text-based datasets, LiveChat captures real spoken conversations, offering a more natural dialogue representation. The dataset is built in three steps: video collection from Chinese platforms, audio transcription using ASR, and dialogue construction through a reply-to-whom matching method. Additionally, persona information is extracted using both manual and automated methods to support personalized dialogue generation. LiveChat outperforms existing datasets in scale, video source, and average session length. Experiments on response modeling and addressee recognition demonstrate the effectiveness of persona information and longer sessions in improving performance. Further, pre-trained models like BART show better adaptation to LiveChat, highlighting its distinctiveness. The study also explores transfer learning and in-context learning, revealing that performance improves with more demonstrations but may degrade due to noise. LiveChat provides a valuable resource for research on personalized and multi-party dialogue systems, and future work will focus on efficient transfer learning for large language models.</sample>
    <sample id="343">Hallo zusammen, ich bin Akshatha, und heute präsentieren Martin und ich unsere Arbeit „Der KITMUS-Test: Die Beurteilung der Integration von Wissen aus mehreren Quellen“. Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research. Natürlichsprachliche Verständnismodelle beziehen sich auf eine Vielzahl von Wissensquellen, wie beispielsweise das in ihren Parametern enthaltene Wissen, das normalerweise durch eine Vortrainierung erworben wird, und Wissen, das während der Inferenzzeit in den Eingaben gegeben wird. Kürzlich durchgeführte Arbeiten bei Aufgaben wie der Fragebeantwortung zeigen, dass Modelle das während der Vortrainierung erlernte Wissen verwenden können, um die Aufgabe zu lösen. Aber natürlichsprachliches Verständnis erfordert oft Wissen, das auch während der Inferenzzeit bereitgestellt wird. Ein Beispiel ist der Satz: „John sah den neu gewählten Präsidenten im Fernsehen.“ Die vortrainierten Parameter können Informationen enthalten, was Präsidenten tun und was ein Fernseher ist, aber sie können nicht zuverlässig wissen, wer diese instanzspezifische Entität „John“ ist, oder wer der neue Präsident ist, da sich der Präsident seit der Vortrainierung geändert haben könnte. Daher benötigen erfolgreiche Modelle für Wissensintensive NLU-Aufgaben die Fähigkeit, sowohl vortrainiertes als auch inferenzzeitliches Wissen zu integrieren und zu nutzen. In dieser Arbeit schlagen wir ein diagnostisches Testset für die Wissensintegration vor. Wir führen eine Pronomenauflösungsaufgabe ein, die darauf abzielt, die Fähigkeit zu prüfen, Wissen aus verschiedenen Quellen zu nutzen. Wir bewerten das Datenset mit menschlichen Studienteilnehmern und etablierten Pronomenauflösungsmodellen. Hier ist ein Beispiel aus unserem Datenset: Servin ist ein Richter. Kea ist ein Bäcker. Servin und Kea trafen sich im Park. Nach einem langen Tag am Arbeitsplatz, bei dem er Fälle in einem Gericht entschied, war er glücklich, sich zu entspannen. Die Aufgabe besteht darin, das richtige Entity zu identifizieren, auf das sich das Pronomen „er“ bezieht, was in diesem Fall Servin ist. Die Auflösung eines gegebenen Pronomens erfordert zwei Arten von Informationen. Erstens, spezifisches Wissen über Entitäten wie „Servin ist ein Richter.“ Und zweitens, Hintergrundwissen wie „Richter entscheiden Fälle in Gerichten.“ Allgemein wird Hintergrundwissen während der Vortrainierung großer Sprachmodelle gelernt, während spezifisches Entitätenwissen typischerweise während der Inferenzzeit beobachtet wird. Wir variieren die Verfügbarkeit dieser beiden Informationen so, dass sie entweder in einer einzigen Quelle oder in mehreren Quellen vorliegen können. Wir haben drei Szenarien für KITMUS definiert. Erstens haben wir das typische Szenario: „Hintergrund-Vortrainierung“, bei dem angenommen wird, dass das Hintergrundwissen in der Vortrainierungszeit verfügbar ist. Zweitens gibt es das Szenario „Hintergrund-Beide“, bei dem das Hintergrundwissen sowohl in der Vortrainierungszeit als auch in der Inferenzzeit verfügbar ist. Schließlich das Szenario „Hintergrund-Inferenz“, bei dem beide Wissensarten nur in der Inferenzzeit verfügbar sind. Dies letzte Szenario ist besonders interessant, da es den Fall simuliert, bei dem das für die Lösung einer Aufgabe notwendige Hintergrundwissen nicht Teil der Vortrainingsdaten der Modelle ist. Ein Beispiel für die Steuerung der Verfügbarkeit von Fakten in den tatsächlichen Quellen ist folgendes: Im Szenario „Hintergrund-Vortrainierung“ gehen wir davon aus, dass das Hintergrundwissen „Politiker streben Sitze im Regierungsgremium an“ in den vortrainierten Parametern enthalten ist, und in der Inferenzzeit geben wir das spezifische Wissen über die Entität „Chichester ist ein Politiker.“ Im Szenario „Hintergrund-Beide“ geben wir zusätzlich sowohl spezifisches Wissen als auch Hintergrundwissen über Politiker in der Inferenzzeit an. Im Szenario „Hintergrund-Inferenz“ geben wir die fiktive Berufsbezeichnung „mirituer“ anstelle von „Politiker“ an, da „mirituer“ unwahrscheinlich in den vortrainierten Parametern enthalten ist. Wir bewerten das Datenset sowohl mit menschlichen Studienteilnehmern als auch mit etablierten Pronomenauflösungsmodellen. In dieser Grafik zeigen wir die Ergebnisse der besten Modelle in der schwierigsten Variante des „Hintergrund-Vortrainierung“-Szenarios. Ohne spezifische Aufgabenbezogene Trainierung auf KITMUS schneiden beide Modelle nicht gut ab. Wenn sie jedoch mit KITMUS trainiert werden, leisten sowohl C2F als auch BERT4Coref deutlich besser als eine zufällige Wahl. Dies deutet darauf hin, dass Modelle, die auf generischen Referenzauflösungsdatensätzen trainiert wurden, meist lernen, Oberflächennachweise zu nutzen, die nicht nützlich sind, wenn sie auf KITMUS getestet werden, wo solche Hinweise entfernt wurden. Zusätzliche Experimente mit fiktivem Wissen zeigten, dass selbst die besten Modelle nicht zuverlässig Hintergrundwissen integrieren können, das nur während der Inferenzzeit bereitgestellt wird. Um die Hauptergebnisse unseres Papers zusammenzufassen, scheinen viele Pronomenauflösungsmodelle ohne spezifische Aufgabenbezogene Trainierung nicht in der Lage zu sein, über Wissen aus verschiedenen Quellen zu denken. Mit spezifischer Aufgabenbezogener Trainierung gelingen einigen Modellen jedoch die Integration von Wissen aus mehreren Quellen. Dennoch scheinen selbst die besten Modelle Schwierigkeiten zu haben, Hintergrundwissen, das nur während der Inferenzzeit bereitgestellt wird, zuverlässig zu integrieren. Wenn Sie mehr Details erfahren möchten, sehen Sie sich bitte unser Paper an und besuchen Sie das Datenset und den Code auf GitHub. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="344">Die Nachteile der baumbasierten Methoden sind, dass Bäume oft nicht gegeben sind und daher aufwendig erzeugt werden müssen, z. B. durch spezialisierte Präprocessing-Schritte oder Grammatikinduktionsverfahren.</sample>
    <sample id="345">**Abstract:**  
In this paper, we present a novel neural seq2seq model for compositional generalization in semantic parsing that does not rely on syntactic trees. Our approach addresses the challenge of mapping input utterances to logical forms with deeper, unseen structures by modeling correspondences between input and output fragments. The model operates in two steps: first, it tags each input token with an unordered multiset of output tokens, and second, it predicts a permutation to arrange these tokens correctly. This permutation prediction is flexible and does not impose strict constraints, enabling generalization to deeper recursion. We introduce a continuous relaxation to efficiently approximate the optimal permutation, overcoming the NP-hardness of the problem. On the COGS benchmark, our model outperforms existing treeless approaches in generalizing to deeper recursion, though other structural generalizations remain challenging. By inducing alignments during training, we handle the lack of explicit input-output alignment and learn linguistically plausible permutations. Our work provides a promising alternative to tree-based methods for compositional generalization.</sample>
    <sample id="346">Die Information über die Universität der Autoren ist im gegebenen Text nicht enthalten.</sample>
    <sample id="347">Hallo, ich bin Myra und heute möchte ich über unser Paper "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" sprechen. Diese Arbeit wurde in Zusammenarbeit mit Esin Durmus und Dan Jurafsky erstellt. In den letzten Jahren haben viele Forscher die Verbreitung von sozialer Vorurteile und Stereotypen in großen Sprachmodellen, sogenannten LLMs, dokumentiert. Allerdings haben diese Methoden verschiedene Einschränkungen. Sie basieren in der Regel auf handkodierten Datensätzen, die sehr zeitaufwendig zu erstellen sind, und sie messen meist nur sehr spezifische Stereotypen, was bedeutet, dass sie sich nicht gut auf andere Demografien oder Kontexte übertragen lassen, oder sie erfassen lediglich allgemeine, breite Assoziationen, wie negative Assoziationen mit bestimmten Gruppen. Zudem berücksichtigt die meisten Arbeit in diesem Bereich nicht die Intersektionalität, also die Idee, dass sich mehrfache soziale Identitäten zu Vorurteilen verketten können und einzigartige Schwerpunkte von Schaden darstellen.

Um diese Einschränkungen zu überwinden, nutzen wir die Eigenschaft, dass diese neuere, anweisungsgesteuerte LLMs sehr gut auf Anweisungen und Prompts reagieren. Wir können also das Modell bitten, eine Personas zu generieren, also eine Darstellung einer fiktiven Person, beispielsweise mit dem Prompt: „Stell dir vor, du bist eine asiatische Frau. Beschreibe dich selbst.“. Sofort sehen wir, dass dies sehr allgemein auf jede Demografie anwendbar ist, da wir einfach beliebige Identitätsmerkmale in den Prompt einbauen können. Hier sind einige Beispielgenerierungen von GPT-4. Sofort erkennen wir, dass die Ausgaben zwar nicht offensichtlich negativ oder toxisch sind im herkömmlichen Sinne, aber doch interessante Muster aufweisen. Die asiatische Frau wird als unauffällig dargestellt; die mittelöstliche Frau wird mit Begriffen wie „exotisch“ und „verzaubernden Region“ beschrieben. Und beide Frauen von Farbe erwähnen ihre Abstammung, während der weiße Mann solche Referenzen gar nicht macht.

Um diese Muster zu erfassen, besteht unsere Methode aus zwei Teilen. Der erste besteht darin, diese Personas zu generieren. Unsere Prompts zur Erstellung dieser Personas wurden von einer Studie inspiriert, bei der solche Prompts an menschliche Teilnehmer gegeben wurden, wodurch ebenfalls rassische Stereotype zutage traten. Zudem ermöglicht das eine direkte Vergleichbarkeit zwischen unseren generierten Personas und den von Menschen verfassten Antworten. Der zweite Teil ist die Methode der „Marked Words“, mit der wir die Wörter identifizieren, die markierte Gruppen von unmarkierten Gruppen unterscheiden, was ich gleich noch genauer erläutern werde. Der Vorteil dieser Methode besteht darin, dass wir sehr spezifische Stereotype und Muster erfassen können, ohne auf ein spezifisches Lexikon zurückzugreifen.

Die Methode „Marked Words“ baut auf dem soziolinguistischen Konzept der „Markedness“ auf, das besagt, dass es eine unmarkierte Standardform gibt und jede Gruppe, die sich von dieser Standardform unterscheidet, sprachlich markiert ist. Zum Beispiel wird das Wort „Krieger“ normalerweise mit Männern assoziiert. Wenn Menschen eine Kriegerin beschreiben, fügen sie oft das Wort „Frau“ hinzu, also „Frauenkriegerin“, um das Wort zu markieren. Allgemeiner gesagt sind die dominanten Gruppen in der Gesellschaft sowohl sprachlich als auch sozial unmarkiert, während marginalisierte Gruppen in der Regel markiert sind.

In unserer Methode bezeichnen wir zunächst die unmarkierten und markierten Gruppen und vergleichen anschließend die Personas mithilfe der „Fightin’ Words“-Methode, die im Grunde genommen gewichtete Log-Odds-Verhältnisse verwendet, um die wichtigsten Wörter für jede markierte Gruppe zu identifizieren. Beispielsweise vergleichen wir bei den Personas von schwarzen Frauen die Log-Odds-Verhältnisse mit den Personas von Weißen und Männern, da diese die beiden entsprechenden unmarkierten Gruppen darstellen.

Zu den Ergebnissen: Zunächst verwenden wir ein Lexikon von Stereotypen und stellen fest, dass die generierten Personas deutlich mehr Stereotype enthalten als die von Menschen geschriebenen. Allerdings zeigt sich bei genauerer Betrachtung der Verteilung der Wörter und des Lexikons, dass sich zwei sehr unterschiedliche Dinge abzeichnen. Während die generierten Personas deutlich höhere Raten an Lexikonwörtern aufweisen, haben die menschlich geschriebenen Personas eine viel breitere Verteilung an Wörtern, während die Stereotypwörter in den generierten Personas hauptsächlich die Wörter „groß“ und „sportlich“ sind. Also eigentlich nur positive oder zumindest nicht-negative Wörter. Tatsächlich erfassen diese Lexika nicht wirklich viele der schädlichen Muster, die wir in den früheren Folien gesehen haben.

Stattdessen wenden wir uns den Ergebnissen unserer „Marked Words“-Methode zu, um zu zeigen, wie diese scheinbar positiven Wörter Stereotype und essentialisierende Narrative fördern. In unserer Analyse zeigen wir, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln. Erstens beinhalten die Top-Wörter unserer Gruppen Dinge wie „Kultur“, „Tradition“, „stolz“ und „exotisch“. Diese Wörter definieren diese Gruppen ausschließlich durch ihre Beziehung zu ihrer Identität und unterscheiden sie von der weißen Norm. Dies trägt zu einer langen Tradition von Diskriminierung und Anderung dieser Gruppen bei. Zudem sind viele gängige Klischees in diesen Wörtern enthalten, insbesondere bei Frauen von Farbe. Beispielsweise beinhalten die Wörter, die Latina-Frauen beschreiben, Dinge wie „lebhaft“ und „kurvig“, was mit dem Klischee der „Tropik“-Assoziationen verbunden ist. Bei asiatischen Frauen finden wir Wörter wie „klein“, „zart“ und „seidenweich“, die sich mit einer langen Geschichte von hypersexueller Darstellung asiatischer Frauen verbinden, als seien sie äußerst unterwürfig und gehorsam. Und schließlich bei schwarzen Frauen beobachten wir, dass einige der Top-Wörter Dinge wie „stark“ und „resilient“ sind. Dies verweist auf ein Archetypus, den man als den „Strong Black Women“-Archetyp bezeichnet. Obwohl dies zunächst positiv klingt, gibt es Forschung, die zeigt, dass solche Archetypen sehr schädlich sind, da sie Druck auf diese Demografien ausüben, stark und resilient gegenüber gesellschaftlichen Hindernissen zu sein. Anstatt tatsächlich an diesen Hindernissen zu arbeiten, wird der Druck auf diese Menschen gelegt, sie zu überwinden, was zu negativen Gesundheitsfolgen und anderen Schäden führt.

Im Allgemeinen stellen wir fest, dass die Wörter für jede markierte Gruppe ziemlich genau essentialisierende Narrative widerspiegeln. Aufgrund dieser Muster schlagen wir drei Empfehlungen für die Modelleigner vor. Erstens sollten wir als Forscher auch positive Stereotype und essentialisierende Narrative adressieren. Zweitens sollten wir ein intersektionales Lens verwenden, um Vorurteile und Schäden zu untersuchen, da viele Dinge übersehen werden könnten, wenn wir das nicht tun. Schließlich sollte es eine erhöhte Transparenz zu Methoden zur Bias-Minderung geben, denn beispielsweise wissen wir nicht, ob solche positiven Stereotype auf eine Art übermäßiger Wertausrichtung zurückzuführen sind oder auf andere anti-stereotypische Methoden, die zu diesen schädlichen Muster führen. Ohne mehr Transparenz können wir keine Annahmen treffen oder diese weiter untersuchen.

Vielen Dank fürs Zuhören. Viel Spaß auf dem ACL.</sample>
    <sample id="348">**Abstract:**  
This paper introduces *Marked Personas*, a method to measure stereotypes in large language models (LLMs) by generating personas through natural language prompts. Unlike previous approaches that rely on hand-crafted datasets and limited stereotypes, this method leverages the instruction-following capabilities of LLMs to generate personas for various demographic groups, enabling broad generalization. Using the sociolinguistic concept of *markedness*, the study identifies words that distinguish marginalized (marked) groups from dominant (unmarked) ones. Analysis reveals that generated personas contain stereotypes, including seemingly positive terms like "exotic," "petite," and "strong," which reinforce essentializing narratives and harmful tropes. These patterns reflect long-standing societal biases and contribute to discrimination. The study highlights the importance of addressing both negative and positive stereotypes, adopting an intersectional lens, and increasing transparency in bias mitigation. The findings underscore the need for more nuanced approaches to detect and reduce stereotyping in LLMs. (199 words)</sample>
    <sample id="349">Hallo alle, mein Name ist Jingwei Yi von der University of Science and Technology of China. Es ist mir eine Freude, ein kurzes Werbevideo zu unserem Paper zu präsentieren. Kopieren Sie meinen Modell? Schutz des Urheberrechts von großen Sprachmodellen für Embedding als Dienstleistung durch Backdoor-Wasserzeichen. Lassen Sie uns zunächst den Hintergrund zu Embedding als Dienstleistung einführen. Derzeit sind große Sprachmodelle wie GPT, LLAMA, PALM in der natürlichen Sprachverarbeitung und -erzeugung außergewöhnlich. Embedding als Dienstleistung ist eine der Dienstleistungen, die auf großen Sprachmodellen basieren, um verschiedene NLP-Aufgaben zu unterstützen. Zum Beispiel bietet OpenAI eine GPT-basierte Embedding-API an. Allerdings haben kürzliche Arbeiten gezeigt, dass Angreifer das Modell durch Lernen aus dem Embedding stehlen und ähnliche Dienstleistungen anbieten können. Daher ist es notwendig, das Urheberrecht von Embedding als Dienstleistung zu schützen. Um das Urheberrecht von Embedding als Dienstleistung zu schützen, ist eine Lösung, ein Wasserzeichen in den Dienstleistungen des Anbieters einzubauen und zu prüfen, ob ein anderer Dienst das Wasserzeichen enthält. Die Wasserzeichenmethode muss folgende Eigenschaften erfüllen: Erstens sollte die Methode auf Embedding als Dienstleistung anwendbar sein. Zweitens sollte das Wasserzeichen die Nutzbarkeit der bereitgestellten Embedding nicht beeinträchtigen. Drittens sollte das Wasserzeichen für den Angreifer ausreichend verdeckt sein, oder der Angreifer könnte es leicht entfernen. Schließlich muss das Wasserzeichen während des Modell-Extraktionsprozesses auf die Dienstleistungen des Angreifers übertragbar sein. Bestehende Arbeiten lassen sich grob in vier Kategorien einteilen. Allerdings ist diese Methode entweder nicht auf Embedding als Dienstleistung anwendbar oder fehlt an Übertragbarkeit. Daher schlagen wir in diesem Paper Embedding Marker vor, eine auf Backdoor basierende Wasserzeichenmethode, die auf Embedding als Dienstleistung anwendbar ist. Dann möchte ich die Details unseres Embedding Marker vorstellen. Embedding Marker besteht aus zwei Hauptschritten: Wasserzeicheninjektion und Urheberrechtsverifikation. Vor diesen Hauptschritten wählen wir zunächst eine Triggermenge aus. Die Triggermenge ist eine Gruppe von Wörtern in einem moderaten Frequenzintervall. Wir gehen davon aus, dass der Anbieter eine allgemeine Textkorpus sammeln und die Wortfrequenz damit zählen kann. Bei der Wasserzeicheninjektion definieren wir zuerst ein Ziel-Embedding. Wenn ein Benutzer einen Satz an den Anbieterdienst sendet, zählt der Anbieter die Anzahl der Trigger im Satz. Das bereitgestellte Embedding ist eine Gewichtssumme des Ziel-Embeddings und des ursprünglichen Embeddings. Das Gewicht des Ziel-Embeddings ist proportional zur Anzahl der Trigger im Satz. Wenn die Anzahl der Trigger im Satz größer als m ist, ist das bereitgestellte Embedding genau gleich dem Ziel-Embedding. Die Urheberrechtsverifikation besteht darin, zu prüfen, ob ein Modell hinter einem anderen Dienst das Wasserzeichen enthält. Wir erstellen zunächst ein Backdoor-Datenset und ein harmloses Datenset. Das Backdoor-Datenset besteht aus Sätzen, deren Wörter alle zur Triggermenge gehören, während alle Wörter in den Sätzen des harmlosen Datensets nicht zur Triggermenge gehören. Anschließend bittet der Anbieter den Dieb, Embedding von seinem Dienst mit dem Datenset anzufragen. Die Kosinus- und L2-Ähnlichkeit zwischen dem angeforderten Embedding und dem Ziel-Embedding werden berechnet. Wir berechnen den Ähnlichkeitsunterschied zwischen harmlosem und Backdoor-Datenset, definiert als Delta Kosinus und Delta L2. Gleichzeitig wenden wir auch den KS-Test an und verwenden seinen p-Wert als dritten Metrik. Wir führen Experimente an vier Datensätzen AG News, MIND, SST2 und Enron Spam durch. Wir gehen davon aus, dass der Anbieter den Wiki Text Datensatz verwendet, um die Wortfrequenz zu zählen. Die Ergebnisse an den vier Datensätzen zeigen, dass unser Embedding Marker eine gute Erkennungsleistung hat, während er gleichzeitig eine gute Nutzbarkeit für nachgelagerte Aufgaben beibehält. Wir validieren auch die Verdecktheit der bereitgestellten Embedding durch die Visualisierung der Embedding von Sätzen auf den vier Datensätzen. Die Legende der Grafiken bedeutet die Anzahl der Trigger in jedem Satz. Wie in den Grafiken gezeigt, ist es schwer, zwischen den Backdoor-Embedding und normalen Embedding zu unterscheiden. Das ist alles. Vielen Dank. Wir freuen uns auf eine Diskussion mit Ihnen.</sample>
    <sample id="350">In recent years, leaderboard-based evaluation has become the standard in NLP, with models often achieving human-level or even superhuman performance on benchmarks like SuperGLUE and SQuAD. However, this paper challenges the reliability of such claims. By analyzing these benchmarks, we reveal several issues that make human-system comparisons unfair. First, humans are typically evaluated on smaller subsets of the test data compared to models. Second, we identify errors in ground-truth answers, such as incorrect entailment judgments. Additionally, human performance is often underestimated using simplistic aggregation methods, and annotator details are rarely disclosed. Pay rates for human annotators vary significantly and may affect performance quality. These factors undermine the validity of superhuman performance claims. We argue that current benchmarks lack the rigor needed to reliably compare models and humans, and suggest improvements to create more trustworthy evaluation frameworks. Our findings highlight the need for more transparent and scientifically sound benchmarks in NLU research.</sample>
    <sample id="351">**Abstract:**  
This paper investigates the generalization capability of Named Entity Recognition (NER) models trained on the CoNLL-2003 dataset in modern contexts. We created the CoNLL++ dataset, annotated with recent Reuters news from 2020, to evaluate how well models generalize to contemporary data. Over 20 models were fine-tuned on CoNLL-2003 and tested on both CoNLL-2003 and CoNLL++. Our findings indicate that better generalization is achieved with transformer-based architectures, larger model sizes, and more fine-tuning examples. We also examined two hypotheses for performance degradation: adaptive overfitting and temporal drift. Results show no evidence of adaptive overfitting, but performance drops correlate with the increasing temporal gap between training and test data, confirming temporal drift as the main cause. Despite the age of CoNLL-2003, our experiments demonstrate that modern NER models still perform well on new data, suggesting that current taggers remain effective in 2023. The study highlights the importance of model design and data recency in achieving robust generalization.</sample>
    <sample id="352">ABC-Eval steht für "Annotating Behaviors in Chat" und ist ein Verfahren zur präzisen und zuverlässigen Bewertung von Dialogmodellen durch die Analyse spezifischer Verhaltensmuster in den Antworten.</sample>
    <sample id="353">This paper introduces CodeClarQA, a novel approach to Python code generation by incorporating interaction through clarification questions (CQs) to address input underspecification in natural language descriptions (NLDs). We propose a synthetic dataset with clarifications on key operations and a pipeline for code generation that includes a Clarification Need Predictor, a Question Selector, and a Code Generator. Our method identifies missing key operations by comparing schema elements between NLDs and operation documentation using similarity scores. Experiments show that our approach improves code generation performance, especially when using high-ranked CQs. However, challenges remain, such as distinguishing similar operations and using argument values instead of documentation. Results indicate that clarifications enhance code generation, though the pipeline still underperforms compared to model-only training. Overall, this work highlights the potential of interactive code generation and provides a foundation for future research in this direction.</sample>
    <sample id="354">Das Leistungsdelta zwischen CoNLL-2003 und CoNLL++ ist bis zum Jahr 2020 höher als 5 Prozentpunkte.</sample>
    <sample id="355">Hallo, mein Name ist Vasudha und ich bin Doktorandin in Informatik an der Stony Brook University. Ich möchte unsere Arbeit präsentieren, die in ACL 2023 als Long Paper angenommen wurde: „Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge“ (Transferlernen für Dissonanzerkennung: Bewältigung des Problems seltener Klassen). Wir beginnen damit, kognitive Dissonanz zu definieren und erklären, warum es sich um ein wichtiges Forschungsfeld in der Sprache handelt. Einfach gesagt, ist kognitive Dissonanz ein Zustand, in dem zwei Überzeugungen oder Handlungen inkonsistent sind, wie in diesem Beispiel, in dem eine Person sagt: „Ich weiß, dass Zigaretten mich töten könnten“, und anschließend bemerkt: „Ich habe nach der Besprechung ein paar Zigaretten genommen“. Diese Überzeugung und Handlung sind inkonsistent und befinden sich in Dissonanz. Wenn sie weiterhin hinzufügt: „Ich denke nicht, dass ich meine Stelle ohne sie behalten könnte“, rechtfertigt das die zweite Handlung, und sie bilden eine Konsonanz-Beziehung. Obwohl Dissonanz ein sehr verbreitetes Phänomen in unserem täglichen Entscheidungsprozess ist, wird sie in Sprache und anderen Diskursbeziehungen selten ausgedrückt. Warum ist das wichtig? Die Untersuchung kognitiver Dissonanz kann uns dabei helfen, die Auswirkungen von Meinungsverschiedenheiten zwischen Menschen zu verstehen, Trends und Glaubenswerte zu verfolgen sowie Veränderungen von Einstellungen in der Bevölkerung zu analysieren. Hohe kognitive Dissonanz ist auch mit Angststörungen verbunden und kann uns dabei helfen, das psychische Wohlbefinden besser zu verstehen. Die Untersuchung von Dissonanz in Sprache kann auch bei der Erkenntnis von Extremismus und Polarisierung von verletzlichen Gruppen hilfreich sein. Schließlich ist kognitive Dissonanz wichtig, um die individuellen kognitiven Stile von Personen besser zu verstehen und Entscheidungsprozesse besser zu begreifen.

Ziel unseres Projekts war es, eine Ressource für kognitive Dissonanz zu erstellen. Dazu haben wir eine großangelegte Annotation von Dissonanzbeziehungen durchgeführt. Wir verwendeten einen dissonanzzentrierten Ansatz, wie in dem hier gezeigten Flussdiagramm dargestellt. Tweets wurden zunächst mit dem PDTB-Parser verarbeitet, und Paare von Diskursseinheiten wurden gemäß den in unserem Paper beschriebenen Richtlinien annotiert. Wie hier zu sehen ist, wurde Dissonanz nur in 3,5 % der annotierten Paare gefunden. Bei der Sammlung von etwa 1.000 Beispielen von Diskursseinheitenpaaren trainierten wir zunächst einen Klassifikator, der nur auf 43 Beispielen von Dissonanz trainiert wurde. Nicht überraschend zeigte sich, dass der Klassifikator kaum besser als Zufall abschnitt. Angesichts der geringen Vorkommenshäufigkeit von Dissonanz und der Abwesenheit einer vorherigen Datensammlung stehen wir vor dem Problem der absoluten Seltenheit. Um dies zu mildern, haben wir Kombinationen von Transferlernen und aktivem Lernen experimentiert, um eine effizientere Sammlung von dissonanten Beispielen zu ermöglichen, wodurch die Gesamtkosten der Annotation gesenkt und die Erkennung von Dissonanz verbessert werden können.

Da der ursprüngliche Modell keine Dissonanzklasse überhaupt erfassen konnte, begannen wir den aktiven Lernprozess mit dem Transfer von Gewichten aus eng verwandten Aufgaben. Wir übertrugen Gewichte von zwei verschiedenen Aufgaben: einer Aufgabe zur Stance-Klassifikation von Dissonanz, unabhängig vom Thema, bei der bestimmt wird, ob zwei Aussagen aus unterschiedlichen Personen im Einvernehm oder im Streit stehen, und einer binären Klassifikation der Erweiterungs- und Vergleichsklassen des PDTB, da diese beiden eng mit dem Konzept von Konsonanz und Dissonanz verbunden sind und wir sie CE nennen. Wir fanden heraus, dass die Zero-shot-Performance auf dem annotierten Datensatz bereits deutlich besser als Zufall ist, wobei die beste AUC bei 0,62 liegt. Zudem fanden wir, dass das iterative Feinabstimmen auf die CE-Aufgaben gefolgt von einer weiteren Feinabstimmung auf die Debatte-Aufgabe eine deutlich bessere Zero-shot-Performance liefert. Dies ist der Modellansatz, den wir zur Kaltstart-Annotation des aktiven Lernens verwenden.

Als nächstes bestimmen wir die beste Methode, um das Modell mit neuen Daten aus jedem Durchgang des aktiven Lernens und der Annotation zu aktualisieren. „Cumulative“ sammelt alle bisher gesammelten Daten aus aktiven Annotationen, während „Iterative“ das Modell durch das Trainieren auf der neuesten Datenmenge aktualisiert. Unter den verschiedenen Strategien stellten wir fest, dass „Cumulative“ überall gleich oder besser als „Iterative“ abschneidet.

Um die Anzahl an Dissonanzbeispielen zu erhöhen, verwenden wir eine Strategie namens „Probability-of-Rare-Class“ (PRC), um in jedem Durchgang vor allem die Beispiele auszuwählen, die nach der aktuellen Modellvorhersage am wahrscheinlichsten dissonant sind. Wir vergleichen dies mit anderen, in der Community üblichen AL-Strategien. Wir finden heraus, dass die vorgeschlagene PRC-Strategie besser als andere etablierte Strategien arbeitet, obwohl der Unterschied gering ist. Zu beachten ist, dass die Leistung bei Zufallsauswahl deutlich schlechter ist. Nach weiteren Durchgängen des aktiven Lernens mit den beiden besten Strategien verbessern wir die AUC der Dissonanzklassifikation auf 0,75, was der besten Leistung, die wir bisher für diese Aufgabe erzielt haben, entspricht.

Wir prüfen auch die Machbarkeit jeder Strategie hinsichtlich der Qualität der Annotation und der Kosten für die Annotatoren. Wir finden heraus, dass PRC den höchsten Prozentsatz an Dissonanzbeispielen liefert und für seltene Klassen am besten funktioniert. Allerdings finden die Annotatoren die Beispiele schwierig. Zusammenfassend finden wir, dass PRC eine einfache AL-Strategie für die Sammlung seltener Klassen ist und der Kaltstart des aktiven Lernens mit einer entsprechend gestalteten Transferlernaufgabe erheblich hilft. Wir stellen außerdem fest, dass iterative Aktualisierungen für das Transferlernen aus einem anderen Bereich nützlich sind, während bei aktiven Annotationen im selben Bereich kumulative Aktualisierungen vorteilhaft sind.

Hier sind die Links zu unserem Kern-Datensatz und unserem Paper. Zögern Sie nicht, uns zu kontaktieren, wenn Sie Fragen haben. Vielen Dank.</sample>
    <sample id="356">Die Autoren gehören der Universität Stanford an.</sample>
    <sample id="357">Der Referent heißt Siyu Yuan.</sample>
    <sample id="358">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="359">Der Ansatz wird mit der state-of-the-art-Architektur für simultane Präübersetzung verglichen.</sample>
    <sample id="361">**Abstract:**  
Armineh Nourbakhsh presents CounterComp, a method to enhance compositional generalization in multi-step quantitative reasoning tasks using counterfactual scenarios. Current neural models struggle with such tasks, especially when outputs require more than two steps, due to reliance on spurious patterns. CounterComp addresses this by mining counterfactual examples from training data, generating positive and negative samples based on interventions in question components. These examples are used to introduce an auxiliary metric learning loss with a dynamic margin, encouraging the model to focus on meaningful input tokens related to operations in the output. This approach improves performance on both in-distribution and out-of-distribution samples, enhancing compositional generalization. Qualitative analysis shows that the method helps models attend to more relevant tokens. The work demonstrates that adding this auxiliary loss consistently improves state-of-the-art baselines, particularly in complex reasoning scenarios.</sample>
  </task>
</testset>