<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="en">
    <sample id="0">The main data sources for language models are large-scale web crawl data, which include political news media such as the New York Times, Los Angeles Times, The Guardian, and Huffington Post, among others.</sample>
    <sample id="1">The authors of the paper are affiliated with McGill University, Mila, and Microsoft Research.</sample>
    <sample id="2">This paper introduces LayoutMask, a novel pre-trained model designed to address the challenges in Visually-rich Document Understanding (VrDU) tasks such as form, receipt, and poster recognition. Unlike existing models that rely on global 1D positions to represent reading order, LayoutMask utilizes local 1D positions derived from in-segment token orders, combined with 2D spatial information and semantic context, to infer global reading order. This approach enhances text-layout interactions and improves layout representation learning. To further strengthen these interactions, LayoutMask introduces two novel masking strategies—Whole Word Masking and Layout-Aware Masking—alongside a new pre-training objective called Masked Position Modeling (MPM). Whole Word Masking increases the difficulty of prediction by masking entire words, while Layout-Aware Masking emphasizes cross-segment context learning. MPM aims to recover masked 2D positions, promoting spatial and semantic reasoning. Experimental results show that LayoutMask outperforms global 1D position methods on FUNSD and SROIE datasets, with only minor performance differences on CORD. The improved performance is attributed to LayoutMask’s adaptability in handling complex layouts and ambiguous entities like “Total.” This work provides a new direction for pre-training in document understanding by emphasizing local and spatial reasoning.</sample>
    <sample id="4">The name of the speaker is Kayo Yin.</sample>
    <sample id="5">They used the T5 XL model to obtain the 82%-87% accuracy.</sample>
    <sample id="6">This paper introduces "Towards Unifying Multi-Lingual and Cross-Lingual Summarization," a novel framework that unifies multilingual and cross-lingual summarization into a more general setting called many-to-many summarization. The goal is to develop a single model capable of processing documents in any source language and generating summaries in any target language. The authors conduct preliminary experiments on the WikiLingua dataset, comparing traditional multilingual summarization, cross-lingual summarization, and their proposed many-to-many approach. The results show that the many-to-many setting enables better cross-lingual knowledge transfer than previous methods. To support this framework, the authors propose PISCES, a pre-trained many-to-many summarization model trained through a three-stage pre-training process: meta pre-training, cross-lingual pre-training, and task-specific pre-training. PISCES outperforms existing baselines such as mBART-50 and mT5 in experimental results, and ablation and human studies confirm the effectiveness of each training stage. This work represents a significant step toward building more flexible and versatile cross-lingual summarization systems capable of handling a wide range of language pairs and tasks.</sample>
    <sample id="7">Yes, CoNLL-2003 taggers still work well in 2023, especially when using better model architectures, larger model sizes, and more fine-tuning examples. The performance drop observed is mainly due to temporal drift, not adaptive overfitting.</sample>
    <sample id="8">The novelty of the proposed human evaluation method, ABC-Eval, lies in its focus on explicitly annotating specific behaviors in chat responses, such as irrelevance, contradictions, hallucinations, and empathy, to provide a more precise, reliable, and fine-grained evaluation of conversational AI models compared to traditional holistic methods like Likert scales or pairwise comparisons.</sample>
    <sample id="9">The success of existing weakly supervised approaches heavily relies on clean, manually annotated samples for model selection and validation.</sample>
    <sample id="10">To improve the score, language models can be enhanced with better access to and utilization of background knowledge, such as through more effective retrieval mechanisms or integration of contextual information similar to what annotators use. Additionally, improving the model's ability to understand and generate indirect referring expressions could lead to significant improvements.</sample>
    <sample id="11">Jack Hessel, a research scientist at AI2, presents "Do Androids Laugh at Electric Sheep? Humor 'Understanding' Benchmarks from The New Yorker Caption Contest," a collaborative study with institutions such as the University of Utah and OpenAI. The research investigates whether large language models (LLMs) truly understand humor, despite their ability to generate and explain jokes. While models like Google's PaLM and ChatGPT can produce and explain jokes, their understanding is questioned when they fail to recognize the nuances of humor, such as in a knock-knock joke involving a pineapple. To evaluate humor understanding, the team operationalizes The New Yorker Caption Contest into three tasks: matching captions to cartoons, ranking caption quality, and generating explanations for jokes. Using a dataset of over 700 cartoons and 650 joke explanations, they find that even the best-performing model, CLIP fine-tuned on their corpus, achieves only 62% accuracy on the matching task—far below human performance of 94%. Similarly, GPT-4, when provided with human-authored image descriptions, still underperforms compared to humans in both matching and quality ranking tasks. Human evaluations also show that explanations generated by GPT-4 are less preferred than those written by humans. The study highlights a significant gap in LLMs’ humor understanding and provides a new benchmark for future research. The dataset and leaderboard are made publicly available for further exploration.</sample>
    <sample id="12">The paper involves 5 authors: Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow.</sample>
    <sample id="13">**Abstract:**  
In this work, we analyze and improve adaptive inference methods for large language models in low-resource settings. Adaptive inference leverages the varying complexity of real-world data to reduce inference costs by using low-capacity models for simpler tasks. Two common approaches are Multi Model, which employs multiple models with classifiers to decide when to stop inference, and Early Exit, which uses classifiers at intermediate layers of a single model. While Multi Model is versatile and accurate, it suffers from storage and computational overhead. Early Exit, though faster and more memory-efficient, faces performance degradation due to conflicting gradients—where multiple classifiers update shared model weights, leading to interference. To address this, we propose SWEET (Separating Weights in Early Exit Transformers), a novel fine-tuning method that trains each layer with updates only from its corresponding classifier, eliminating conflicting gradients. Our experiments show that SWEET significantly improves the accuracy of Early Exit models, closing most of the performance gap with Multi Model, while maintaining faster inference speeds. SWEET outperforms both methods in speed-accuracy trade-offs, especially for BERT-Large. This study provides the first fair comparison between Early Exit and Multi Model, highlights the issue of conflicting gradients, and introduces SWEET as a promising approach for optimizing adaptive inference.</sample>
    <sample id="15">The paper involves three authors: Matthias Lindemann, Alexander Koller, and Ivan Titov.</sample>
    <sample id="16">The Bible texts are simplified more than news texts or language learner texts.</sample>
    <sample id="17">This paper introduces a novel approach for Multimodal Relation Extraction (MRE) that addresses two key challenges: internal-information over-utilization and external-information under-exploitation. While traditional methods rely solely on text, real-world scenarios often involve multimodal data, such as text and images, which provide richer context. However, not all modalities are equally useful, and some information may be redundant or even misleading. To tackle these issues, the proposed method employs a Graph Information Bottleneck (GIB)-guided feature refinement to perform fine-grained pruning of multimodal features, ensuring only relevant information is retained. Additionally, the framework integrates multimodal topic information as an external knowledge source to compensate for potential information deficiencies. The method constructs a unified cross-modal graph (CMG) by merging textual and visual scene graphs, then refines it through GIB-based optimization. Multimodal topic features are further incorporated using attention mechanisms to enhance context modeling. Experimental results on the MRE benchmark show that the proposed approach outperforms existing multimodal baselines. Ablation studies confirm the effectiveness of both internal screening and external compensation. Furthermore, analysis reveals that internal screening is more beneficial for high-relevance inputs, while external information is more critical for low-relevance cases. Overall, this work provides a systematic framework for simultaneous information subtraction and addition in MRE, achieving state-of-the-art performance.</sample>
    <sample id="18">The example of the preference for shorter left conjuncts is "salt and pepper" rather than "pepper and salt," where the shorter conjunct "salt" is placed on the left.</sample>
    <sample id="19">This presentation introduces the work titled "A Survey for Efficient Open Domain Question Answering," accepted at ACL 2023. The paper focuses on addressing the challenges of deploying open-domain question answering (ODQA) systems efficiently, particularly in terms of memory usage, inference speed, and performance. Traditional ODQA systems, such as the two-stage framework introduced by Danqi Chen in 2017, involve a retrieval stage to find relevant evidence from a large corpus like Wikipedia and a reading stage to generate an answer. However, these systems face significant challenges due to the massive size of the corpus, large index files, and the use of complex language models. To overcome these issues, the paper explores various efficient techniques, such as approximate nearest neighbor search, skip reading, index compression, and model size reduction through methods like parameter sharing and knowledge distillation. The study compares different system architectures, including retrieval-only and generator-only models, and identifies trade-offs between speed, memory, and performance. The findings suggest that retrieval-only systems are suitable for real-time applications, while combined retrieval-reader systems offer a balanced approach. The paper concludes with insights on optimizing ODQA systems and outlines future directions, such as deploying these systems on low-power devices and developing more comprehensive evaluation metrics.</sample>
    <sample id="20">Yes, you can use the models for your research as they are freely available on Hugging Face under the MIT license.</sample>
    <sample id="21">DEPLAIN-apa contains news texts.</sample>
    <sample id="22">Good generalization is achieved through three main factors: a better model architecture, larger model size, and more fine-tuning examples.</sample>
    <sample id="23">This paper investigates the limitations of text-image models in rendering visual text and proposes a method to improve their ability to accurately represent textual content. The study focuses on the Imagen model, which uses a T5-XXL encoder to process input text before feeding it to a diffusion model for image generation. Despite its success in generating high-quality images, the model struggles with rendering text, as the T5 encoder's subword tokenization makes it difficult to decompose words into individual characters. Experiments reveal that T5 variants, including the large XXL model, perform poorly on spelling tasks, achieving less than 70% accuracy. In contrast, ByT5, which processes input at the byte level, excels at spelling due to its direct access to character-level information. The research also shows that T5 struggles more with frequent words, as they are often encoded as fewer subwords, making decomposition more challenging. To address this, the authors propose augmenting the Imagen model by concatenating the output of a ByT5-small model to the existing T5 encoder. This addition, which increases the parameter count by only 5%, significantly improves the model's spelling ability and, consequently, its text rendering performance. The paper introduces the WikiSpell and DrawText benchmarks and presents an efficient strategy for enhancing text rendering in diffusion models by incorporating character-aware encoders.</sample>
    <sample id="24">The tendency for left conjuncts to be shorter was measured by analyzing statistics from the enhanced version of the Penn Treebank, focusing on the length differences between conjuncts in terms of characters, syllables, and words. The results showed that when the governor is on the left or absent, the left conjunct tends to be shorter, especially as the length difference between conjuncts increases.</sample>
    <sample id="25">The experiments measured the length of conjuncts in coordination structures based on the position of the governor (left, right, or absent) using statistics from the Penn Treebank. They analyzed how the tendency for the left conjunct to be shorter varied depending on whether the governor was on the left, on the right, or absent.</sample>
    <sample id="26">A baseline classifier trained on imbalanced data performs not much better than chance.</sample>
    <sample id="27">The information provided does not specify the number of authors involved in the paper.</sample>
    <sample id="28">The characters' names in the example conversation are Bob and Alice.</sample>
    <sample id="29">Context-aware MT models improve over context-agnostic ones on discourse phenomena such as formality and lexical cohesion.</sample>
    <sample id="30">**Abstract:**  
This paper introduces LLM-Blender, a simple yet effective ensemble learning framework for large language models (LLMs), designed to improve output quality by combining multiple models' strengths. While many LLMs achieve strong average performance, our analysis shows that the optimal model for a specific input can vary significantly. To address this, LLM-Blender employs a two-stage approach: first, a pairwise ranking module called PairRanker, which compares model outputs for a given input using cross-attention mechanisms (e.g., RoBERTa) to determine the best candidates. This module generates a comparison matrix, which is then used to rank the outputs. In the second stage, the top K candidates are fused using a generative model (GenFuser) to produce the final output. Experiments demonstrate that PairRanker outperforms traditional ranking methods in correlation with human judgments and automatic metrics. To evaluate LLM ensembles, we introduce MixInstruct, a new benchmark dataset containing outputs from 11 open-source LLMs. Results show that LLM-Blender significantly outperforms top models like Vicuna and Open Assistant on multiple metrics, achieving better performance in 68% to 76% of cases. The framework is efficient, effective, and provides a unified codebase for future research. Overall, LLM-Blender offers a promising approach to leveraging the diversity of LLMs for improved performance.</sample>
    <sample id="31">The affiliations of the authors of the paper are not explicitly mentioned in the provided content.</sample>
    <sample id="33">The introduced framework quantifies positionality by re-annotating datasets with diverse annotators, collecting demographic data, and comparing these annotations with model predictions and existing datasets using Pearson's R correlation scores. This approach evaluates how well models and datasets align with different demographic groups, highlighting systematic biases and positionalities.</sample>
    <sample id="34">**Abstract**  
In this work, we introduce CREST, a joint framework that combines rationalization and counterfactual text generation to enhance model interpretability and robustness. CREST generates counterfactual examples by masking key tokens in a rationale derived from the input, which is then edited by a masked language model to produce a modified input. These counterfactuals are evaluated for validity and naturalness through human assessments, showing that CREST outperforms existing methods like MiCE. Beyond counterfactual generation, CREST is also used for rationalization, where both factual and counterfactual examples are processed through a shared rationalizer to highlight meaningful input parts. A regularization term ensures consistency between rationales from factual and counterfactual examples. Experimental results on the IMDB dataset demonstrate that CREST-Rationalization achieves superior performance on in-domain, contrastive, and out-of-domain tasks compared to models trained with human or automatically generated counterfactuals. Furthermore, the generated rationales are evaluated for plausibility, forward simulability, and a novel metric called counterfactual simulability, where CREST produces the most plausible and effective explanations. Overall, CREST enables the generation of valid, fluent, and diverse counterfactuals that improve model interpretability and downstream performance, offering a promising approach for explainable AI.</sample>
    <sample id="36">This paper introduces Language-Specific Layers (LSLs) as a novel approach to enhance multilingual machine translation (MT) by increasing language-specific model capacity without increasing inference costs. Multilingual MT systems offer benefits like scalability and reduced error propagation but suffer from limited capacity per language. LSLs address this by incorporating one transformer layer per language, which is selectively activated during inference based on the source or target language. This selective activation ensures that only relevant layers are used, maintaining constant inference speed. The placement of LSLs within the encoder is learned through an automated process that analyzes the importance of shared, source, and target weights across layers. Based on these weights, the model determines the optimal architecture, resulting in a configuration that combines shared and language-specific layers. Experiments on WMT21 news translation data across 10 languages, including low-resource Swahili, demonstrate that the learned LSL architecture significantly outperforms both baseline models and language adapters in terms of translation quality metrics such as chrF, spBLEU, and COMET. The improvements are particularly notable for low-resource languages, and statistical significance is confirmed for 84 out of 90 translation directions. The approach is efficient and scalable, offering a promising direction for future multilingual MT systems.</sample>
    <sample id="37">The previous study found that human subjects also surfaced racial stereotypes when given the same persona prompts.</sample>
    <sample id="38">The study used data from the enhanced version of the Penn Treebank.</sample>
    <sample id="39">The text does not mention the number of authors involved in the paper.</sample>
    <sample id="40">Some closely related tasks for cognitive dissonance include topic-independent dissonance stance classification (determining agreement or disagreement between debate statements) and binary classification of expansion and comparison classes from PDTB (called CE here), as these are closely related to the concepts of consonance and dissonance.</sample>
    <sample id="41">This paper introduces **PeaCoK**, a large-scale **Persona-grounded Commonsense Knowledge Graph** designed to enhance the coherence and engagement of natural language narratives, such as dialogues and stories. PeaCoK captures the relationships between personas and their attributes, incorporating around 3,800 personas and 40,000 attributes, forming over 100,000 factual inferences. It is built through a three-step process: selecting personas from existing commonsense knowledge, inducing attributes using pre-trained language models, and annotating relations via a human-AI collaboration framework. Evaluation shows that PeaCoK enables language models, such as a BART-based generator, to achieve better performance in persona attribute inference compared to large-scale language models like GPT-3.5. Furthermore, integrating PeaCoK into dialogue systems improves fluency, consistency, engagement, and persona expression in conversations, outperforming general social knowledge graphs like Atomic2020. Human evaluation also reveals that increased overlap in shared attributes between speakers leads to more consistent and engaging dialogues, emphasizing the value of PeaCoK’s interconnected persona knowledge. Overall, PeaCoK provides a reliable knowledge base for training language models and enhancing narrative generation with real-world persona-aware commonsense reasoning.</sample>
    <sample id="42">The information provided does not specify the number of authors involved in the paper.</sample>
    <sample id="43">The information provided does not specify the number of authors involved in the paper.</sample>
    <sample id="44">The introduced framework, NLPositionality, differs from previous works by comparing end users' annotations with models and datasets' predictions and labels, rather than focusing solely on annotator agreement or modeling annotator distributions.</sample>
    <sample id="45">The generated personas overlap the most with the lexicon of stereotypes.</sample>
    <sample id="46">The commercial systems compared were DeepL and Google Translate.</sample>
    <sample id="48">The number of authors involved in the paper is not explicitly stated in the provided content.</sample>
    <sample id="49">MPP evaluations were performed up to a context length of 1024 tokens.</sample>
    <sample id="50">The presentation introduces DEPLAIN, a new German text simplification corpus designed for both document- and sentence-level analysis. Text simplification aims to improve readability for specific audiences, such as non-native speakers or individuals with reading difficulties. To train effective models, parallel text pairs are essential, and DEPLAIN addresses limitations of existing corpora, such as small size and unreliable automatic alignments. The corpus consists of two subcorpora: DEPLAIN-apa, based on news texts with 13,000 manually aligned sentence pairs, and DEPLAIN-web, covering diverse domains with 30,450 sentence pairs, aligned both manually and automatically. The corpus showcases a wide range of simplification techniques, with notable differences in simplification levels across domains. The presentation also highlights two use cases for DEPLAIN: evaluating automatic alignment methods and fine-tuning language models for text simplification. The best-performing alignment method for German text simplification was found to be MASSalign. Additionally, models like long-mBART and base mBART were fine-tuned for document- and sentence-level simplification, achieving results better than baseline benchmarks. The corpus and associated resources provide a valuable foundation for future research in German text simplification.</sample>
    <sample id="51">The domains included in their dataset are music, books, and recipes.</sample>
    <sample id="52">Positionality refers to the perspectives that people hold as a result of their demographics, identity, and life experiences, widely used in critical studies, specifically in feminist and queer academic spaces.</sample>
    <sample id="53">The name of the speaker is Dawei.</sample>
    <sample id="54">**Abstract**  
Cognitive dissonance, the inconsistency between beliefs or actions, is a critical phenomenon in language that offers insights into decision-making, mental health, and social dynamics. However, dissonance expressions are rare in natural language, making their detection a challenging task. This paper presents a novel approach to dissonance detection using transfer learning and active learning (AL) to address the rare-class problem. We first create a large-scale annotated dataset of discourse unit pairs, where dissonance occurs in only 3.5% of cases. Due to the scarcity of dissonance examples, we employ transfer learning from related tasks—stance classification in debates and PDTB discourse relations—to initialize a model capable of detecting dissonance. The best transfer strategy involves fine-tuning on CE (comparison and expansion) tasks followed by debate classification, achieving an AUC of 0.62. To further improve performance, we implement active learning with the Probability-of-Rare-Class (PRC) strategy, which selects examples most likely to be dissonant, leading to an AUC of 0.75 after multiple rounds. While PRC is effective for rare-class acquisition, it presents challenges for annotators. Our results demonstrate that combining transfer learning with PRC-based active learning significantly enhances dissonance detection, offering a scalable solution for rare-class NLP tasks. This work contributes both a valuable dataset and a practical methodology for addressing low-frequency phenomena in language.</sample>
    <sample id="55">Yes, EDAtt adapts an existing offline ST model without re-training or using specific architectures for SimulST.</sample>
    <sample id="56">The information provided does not specify the number of authors involved in the paper.</sample>
    <sample id="57">No, the tested models do not work well on the test suite without task-specific training, but they perform significantly better after being trained on KITMUS. However, they still struggle with integrating backward knowledge provided only at inference time.</sample>
    <sample id="58">The three variants of KITMUS are: "Background-Pretrain", "Background-Both", and "Background-Inference".</sample>
    <sample id="59">**Abstract:**  
This work introduces DrBERT, the first open-source pre-trained language model in French specialized for biomedical and clinical domains. Building upon RoBERTa, DrBERT is trained on NACHOS, a large dataset of crawled medical data, addressing the lack of domain-specific French models. The study evaluates DrBERT against multiple pre-training strategies and data sources, including clinical data from Nantes University Hospital (ChuBERT) and continual pre-training approaches based on CamemBERT and PubMedBERT. Seven models are compared across 11 downstream tasks such as named entity recognition, classification, and question answering, alongside six baseline models like CamemBERT and ClinicalBERT. Results show that models trained on domain-specific data perform best on related tasks, while heterogeneous data sources enhance versatility. From-scratch pre-training generally outperforms continual pre-training, though DrBERT trained on 4 GB of NACHOS achieves comparable performance to CamemBERT-based models. DrBERT surpasses generic models like CamemBERT on nine out of eleven tasks, highlighting its effectiveness. All models are freely available on Hugging Face under the MIT license, with training scripts on GitHub. This study contributes to advancing French natural language processing in healthcare by providing robust, open-source tools for biomedical and clinical applications.</sample>
    <sample id="60">The affiliations of the authors of the paper are not explicitly mentioned in the provided content.</sample>
    <sample id="61">The last research question is: Should we only use the clean samples for validation, or are there better ways to utilize them?</sample>
    <sample id="62">This paper presents a systematic study on knowledge distillation for Natural Language Generation (NLG) with a focus on compressing large language models while preserving their performance. The research explores the effectiveness of pseudo-target training as a method for distilling knowledge from a large teacher model to a smaller student model. Unlike prior work that often focuses on classification or pre-training tasks, this study addresses various industry-driven NLG tasks, including summarization, question generation, common sense reasoning, and style transfer, using realistic setups with limited labeled data and abundant unlabeled data. The study evaluates different architectural choices, pruning techniques, and distillation strategies, emphasizing the importance of unlabeled data in enhancing distillation performance. Key findings include the benefits of generating multiple pseudo-targets and using sampling methods with high temperature to increase diversity in the training data. Additionally, the paper introduces a novel technique called "joint-teaching," which combines word-level distillation from both teacher and student-generated pseudo-targets to reduce exposure bias and improve learning. The results highlight the potential of systematic knowledge distillation approaches in achieving efficient and effective model compression for NLG tasks.</sample>
    <sample id="63">The sensitivity metric measures a model's ability to consistently produce the same outputs for the same task despite slight variations in the wording of the instruction.</sample>
    <sample id="64">The name of the speaker is Jingwei Yi.</sample>
    <sample id="65">Greater sensitivity suggests the opposite of improved model performance, as it indicates the model's outputs vary with slight instruction wording changes, implying inconsistency. Lower sensitivity is desired for reliable performance.</sample>
    <sample id="66">This paper provides a comprehensive survey of deep learning approaches for mathematical reasoning, a crucial component of human intelligence that involves understanding and decision-making based on numerical and linguistic data. The study explores various aspects of mathematical reasoning, including text-based problems, visual and tabular contexts, and automated theorem proving. It highlights the importance of neuro-symbolic reasoning in solving geometric problems and the use of sequence-to-sequence and sequence-to-tree models for encoding mathematical expressions. The paper also discusses the application of large language models (LLMs) in mathematical reasoning, emphasizing the effectiveness of chain-of-thought prompting and self-consistency decoding strategies to enhance reasoning accuracy. Despite the progress, challenges such as the inability to handle large numbers and inconsistencies in mathematical reasoning remain. Additionally, the paper addresses the development of non-English datasets and domain-specific benchmarks in finance, science, and medicine. While significant advances have been made, generalization and robustness issues persist, indicating the need for further research in low-resource settings and model reliability. The survey aims to provide insights into current trends and challenges in deep learning for mathematical reasoning, offering a foundation for future work in this evolving field.</sample>
    <sample id="67">This study investigates the phenomenon of interference in multilingual translation models, where training on one language pair can either enhance or hinder performance on another. While previous methods to mitigate interference have shown limited success, especially with larger models, this work identifies key factors that influence interference, such as model size, data size, and language similarity. The research reveals that severe interference occurs primarily in small models relative to the data size, and that adjusting the sampling temperature is crucial for achieving strong performance. Language similarity and the number of languages in the model have a minimal impact on interference levels, as demonstrated through experiments with multiple language pairs and varying data sizes. By comparing bilingual and multilingual model losses, the study quantifies interference and finds that increasing model and data scale significantly reduces its impact. Additionally, the use of temperature sampling—particularly with calibrated values—effectively mitigates interference without the need for complex algorithms. The results suggest that interference is more of a scaling issue than a problem requiring specialized solutions. Overall, the study concludes that appropriately scaled models combined with tuned temperature settings can substantially reduce interference in multilingual translation, offering a practical and effective approach to improving model performance.</sample>
    <sample id="68">During pretraining, models receive a diverse range of linguistic contexts, including syntactic and semantic features from various domains and structures, which influence their acceptability judgments and sensitivity to context during evaluation.</sample>
    <sample id="69">Typically, 20 clean validation samples per class are needed for good performance in Weakly Supervised Learning (WSL).</sample>
    <sample id="70">The authors of the paper are Myra, Esin Durmus, and Dan Jurafsky.</sample>
    <sample id="71">This work introduces the AltEntities Corpus, a newly created dataset designed to study how users refer to entities indirectly when making choices. The dataset includes 6,000 alternative questions across three domains—music, books, and recipes—each paired with up to five indirect referring expressions. The data was collected using a cartoon-based annotation setup, where annotators were provided with background information about two entities and asked to select one using indirect expressions, such as “the newer one” or “the one with piano music.” The study highlights the importance of understanding indirect references in conversational systems and evaluating language models’ entity recognition capabilities. The authors tested a T5 XL model under different knowledge access scenarios: with full, partial, or no background information. Results showed high accuracy (92–95%) when the model had the same knowledge as annotators, but significantly lower performance (60%) when only entity names were available. This underscores the challenge of disambiguating entities based on indirect references alone. The dataset is valuable for benchmarking and improving language models' understanding of natural, conversational language. The AltEntities Corpus is publicly available for further research.</sample>
    <sample id="72">There is a need to develop new methods for measuring media biases because existing language models, trained on large-scale web crawl data containing politically biased sources, can inherit and propagate these biases. This can lead to unfair outcomes in downstream NLP tasks such as hate speech and fake news detection, where models may perform differently based on political leanings, potentially marginalizing certain groups and undermining fairness.</sample>
    <sample id="73">The name of the speaker is Akshatha.</sample>
    <sample id="74">This paper introduces **Dense-ATOMIC**, a densely-connected extension of the ATOMIC commonsense knowledge base, designed to improve knowledge coverage and support multi-hop reasoning. While ATOMIC provides high-quality event-centered commonsense knowledge, it suffers from sparse connectivity due to missing B-to-B, A-to-B, and A-to-A links, limiting its utility for complex reasoning tasks. To address this, we propose a three-step construction process: normalizing tail events, training a relation prediction model called **Rel-CSKGC**, and building Dense-ATOMIC. Rel-CSKGC leverages pre-trained language models (e.g., RoBERTa) and a MaxPooling-based approach to predict relations between head and tail events without relying on graph structure, thus overcoming sparsity and better utilizing semantic information. We also introduce an Intra- and Inter-Cluster Completion Strategy to efficiently infer missing links. Evaluation results show that Dense-ATOMIC significantly improves knowledge coverage with more 1-hop, 2-hop, and 3-hop paths, and enhances the performance of commonsense generation models like COMET. Additionally, multi-hop path evaluation demonstrates the effectiveness of Dense-ATOMIC in supporting complex reasoning. This work provides a valuable resource for advancing commonsense reasoning and knowledge graph completion.</sample>
    <sample id="75">This paper introduces **Jointprop**, a novel joint semi-supervised learning framework for **Named Entity Recognition (NER)** and **Relation Extraction (RE)**. Traditional supervised methods require extensive labeled data, which is costly and domain-specific. While semi-supervised approaches have shown promise, they often overlook the interdependencies between NER and RE tasks. Jointprop addresses this by modeling both tasks simultaneously through **label propagation over a heterogeneous graph**, capturing both intra- and inter-task relationships among labeled and unlabeled data. The framework consists of four components: **span feature generation**, **heterogeneous graph construction**, **joint label propagation**, and **model optimization**. It generates contextualized representations for spans and span pairs, constructs a k-Nearest Neighbor graph to model data similarity, and propagates pseudo-labels across the graph to refine predictions iteratively. The final pseudo-labels, filtered by confidence thresholds, are used to retrain the model. Experiments on four datasets, including both joint- and single-task settings, demonstrate that Jointprop significantly outperforms baseline models, particularly in semi-supervised joint-task scenarios. The results highlight the benefits of exploiting task interdependencies for improved performance in NER and RE.</sample>
    <sample id="76">The political bias propagation pipeline starts with pretraining data, which contains politically biased content from various sources like news media and social platforms. These biases are then absorbed by language models during training, influencing their political leanings. The biased language models can then propagate these biases into downstream tasks, such as hate speech detection and fake news detection, potentially leading to unfair outcomes where certain groups are disadvantaged or marginalized based on the model's political leaning.</sample>
    <sample id="77">This paper introduces DeFacto, a newly created dataset designed to improve the factual consistency of summarization models through human demonstrations and feedback. Developed as a joint effort between Yale University and Microsoft Research, DeFacto includes around 2.5K annotated data points, with 70% containing factual errors. The dataset is built upon the XSum corpus and leverages initial summaries generated by the Pegasus model. Human annotators labeled summaries for factual consistency, provided corrected versions, and included detailed feedback with instructions, explanations, and supporting evidence from the source documents. The study explores three new natural language generation (NLG) tasks: summary editing, feedback generation, and automatic factual error correction. Experimental results show that fine-tuned models and large language models can effectively use human feedback for summary editing, but feedback generation remains challenging. The editor model achieves comparable performance to baseline models with less training data, and generating explanations enhances model performance. The dataset's fine-grained annotations make it valuable for training factuality metrics and meta-evaluation. The DeFacto dataset is publicly available on GitHub, offering a resource for advancing research on factual consistency in summarization.</sample>
    <sample id="78">Yes, the simplification process differs for DEPLAIN-apa and DEPLAIN-web. DEPLAIN-apa involves more reorderings and word additions, while DEPLAIN-web includes more rephrasings.</sample>
    <sample id="79">Yes, CoScript is publicly available.</sample>
    <sample id="80">The watermark is inserted by modifying the embedding output based on the number of trigger words in the input text. When a user sends a sentence, the provider counts the number of trigger words. The provided embedding is a weighted sum of the target (watermarked) embedding and the original embedding, with the weight proportional to the number of triggers. If the number of triggers exceeds a threshold $ m $, the provided embedding is exactly the target embedding.</sample>
    <sample id="81">The author is affiliated with Penn State University.</sample>
    <sample id="82">This paper introduces ULRA, a novel framework for Unsupervised Automated Essay Scoring (AES) that leverages multiple heuristic quality signals as pseudo-groundtruth supervision. Traditional AES models rely on large labeled datasets, which are costly and impractical to obtain for new prompts or without expert scorers. Existing unsupervised approaches, such as clustering-based methods and direct regression using single signals like word count or unique term count, suffer from poor performance due to limited and inconsistent supervision. To address this, ULRA aggregates multiple heuristic signals—such as word count, readability, and lexical diversity—into partial-order pairs through a Heuristic Essay Ranking (HER) module. These pairs are then used to train a neural AES model via a Deep Pairwise Rank Aggregation (DPRA) module, which incorporates a learnable confidence weight for each signal to resolve inconsistencies and unify supervision. Finally, a scoring strategy transforms the model's output into the predefined score range. Experimental results on both transductive and inductive settings show that ULRA significantly outperforms existing unsupervised baselines and achieves competitive performance compared to cross-prompt and one-shot methods, although it still lags behind fully supervised models due to the lack of strong supervision. This work demonstrates the effectiveness of aggregating heuristic signals for robust unsupervised AES.</sample>
    <sample id="83">Yes, encoder-decoder models such as mT5 can improve by training on a mixture of languages, as the performance gains from multilingual training were observed across most major natural languages, except for English in some cases.</sample>
    <sample id="84">This paper introduces PAD-Net, a partially dynamic network framework designed to improve the efficiency of dynamic networks while maintaining strong performance. Traditional static networks use fixed parameters, whereas fully dynamic networks adapt their architecture or parameters based on input, often leading to excessive parameter usage and increased model size. The study investigates whether fully dynamic networks contain redundant dynamic parameters and whether a combination of static and dynamic parameters can yield better results. Based on this hypothesis, PAD-Net partitions parameters into static and dynamic components, using scale factors to control their contribution and an iterative mode partitioning method to identify and convert redundant dynamic parameters into static ones. Experimental results show that PAD-Net outperforms both static and fully dynamic networks in terms of performance, while significantly reducing the number of parameters and computational cost. Ablation studies highlight the importance of dynamic ratios and scale factors in achieving optimal performance. Additionally, PAD-Net demonstrates superior performance compared to network pruning techniques by preserving static parameters and enhancing model discriminability. Future directions include extending the framework to other network architectures, hardware-friendly structures, and exploring combinations of zero elements, static, and dynamic parameters.</sample>
    <sample id="85">An example of constrained language planning is "make a chocolate cake," which imposes specific constraints on the abstract goal of "making a cake."</sample>
    <sample id="86">They ensure the covertness of their method by visualizing the embeddings of sentences on four datasets using PCA and showing that it is hard to distinguish between backdoor embeddings and normal embeddings.</sample>
    <sample id="87">The work uses existing pre-trained language models (PLMs) like CamemBERT and PubMedBERT as a starting point for continual pre-training on French biomedical data. It compares models trained from scratch on NACHOS data with those fine-tuned or pre-trained on CamemBERT weights, analyzing the impact of different pre-training strategies and data sources.</sample>
    <sample id="88">The content provided does not specify which country GPT-4 is least aligned with. It mentions that GPT-4 is most aligned with English-speaking and Confucian countries, but does not provide information on the least aligned country.</sample>
    <sample id="89">The speaker shows how the model leverages knowledge learned through the attention mechanism using the example sentence "I'm going to talk about..." in English, translating it into German.</sample>
    <sample id="90">This paper investigates the feasibility of using language learners as annotators for NLP tasks, challenging the conventional reliance on native speakers. With limited native speakers for many languages, especially low-resource ones like Irish, the study explores whether learners can contribute effectively. The research involved three languages—English, Korean, and Indonesian—and four NLP tasks from the GLUE benchmark: sentiment analysis, natural language inference, named entity recognition, and machine reading comprehension. Language learners were categorized into basic, intermediate, and advanced levels using a revised CFR framework, and compared with native speakers. Participants completed pre-tests, annotation tasks with or without additional resources, and post-tests to assess both annotation accuracy and learning outcomes. Results showed that learners produced nearly accurate annotations, especially for simpler tasks, and when aggregated via majority voting, their performance matched that of native speakers. Training simulations further demonstrated that models trained on learners’ annotations achieved up to 95% of the ground-truth performance, sometimes surpassing models trained on native annotations. The study also observed improvements in learners’ language proficiency and vocabulary through annotation tasks. These findings suggest that language learners can effectively contribute to NLP data annotation, offering a novel, scalable approach for building benchmark datasets in low-resource languages. This work opens new possibilities for expanding NLP research beyond geographic and technological barriers.</sample>
    <sample id="91">The amount of tasks positively impacts the model performance, as increasing the number of tasks leads to better performance and lower sensitivity.</sample>
    <sample id="92">The authors do not explicitly name three treeless baselines in the provided content. However, they mention that they compare their method with "other treeless models" on the COGS benchmark, without specifying their names.</sample>
    <sample id="93">The two co-authors, Alexander Koller and Ivan Titov, are the advisors of the first author, Matthias Lindemann.</sample>
    <sample id="94">This paper introduces *Embedding Marker*, a novel backdoor-based watermarking method designed to protect the copyright of embedding-as-a-service (EaaS) provided by large language models. With the increasing popularity of EaaS, such as OpenAI's GPT-based embedding API, the risk of model extraction and unauthorized replication has grown. To address this, the proposed method injects a covert watermark into the embedding outputs without degrading their utility for downstream tasks. The watermark is embedded by modifying the embeddings of sentences containing predefined "trigger" words, with the degree of modification proportional to the number of triggers. When a sentence contains more than a threshold number of triggers, the embedding becomes identical to a predefined target embedding. For copyright verification, the method compares the embeddings of backdoor and benign datasets using cosine similarity, L2 distance, and statistical tests like the KS test. Experimental results on four benchmark datasets (AG News, MIND, SST2, and Enron Spam) demonstrate that *Embedding Marker* achieves high detection accuracy while maintaining strong performance on downstream tasks. Visual analysis also confirms the watermark's covert nature, as the modified embeddings are indistinguishable from normal ones. This approach provides a practical and effective solution for protecting the intellectual property of EaaS.</sample>
    <sample id="95">The first author of PaLM is not mentioned in the provided content.</sample>
    <sample id="97">The speaker mentions three problems of SimulST: specific architectures requiring additional modules, long and complicated training procedures with different optimization objectives, and training and maintaining several models for different latency regimes.</sample>
    <sample id="98">An effective way to mitigate social and political biases in datasets when training NLP models is to carefully curate and balance the training data, ensuring diverse and representative sources, while being mindful of the potential for bias propagation. Additionally, controlled experiments and further pretraining on partisan corpora can help understand and adjust for biases, but it remains a complex challenge requiring careful consideration of neutrality and fairness.</sample>
    <sample id="100">This paper introduces **PromptRank**, a data-efficient approach for multi-hop question answering (QA) that addresses the challenge of requiring large amounts of labeled training data. Traditional multi-hop retrievers rely on thousands of question-chain pairs for training, which can be costly and impractical in low-resource domains. PromptRank combines an **unsupervised retrieval method** with a **few-shot language model-based reranker**, achieving strong performance with as few as 128 training examples. The method involves two key steps: first, retrieving candidate chains using **TF-IDF retrieval and hyperlink traversal**, and second, **reranking** these candidates using a language model that scores chains based on the **likelihood of the question given the chain**. The chain prompt is constructed by inserting documents into a prompt format with an instruction to guide the language model's reasoning. The study explores techniques such as **instruction search**, **instruction sampling**, and **temperature scaling** to enhance performance. Evaluations on the **HotpotQA** dataset show that PromptRank outperforms fully supervised systems like DrKit and matches state-of-the-art dense retrievers. When combined with a downstream reader model (ELECTRA-Large), PromptRank achieves strong multi-hop QA performance, underperforming MDR by only about four exact match points. The results highlight the effectiveness of using language models for few-shot ranking and emphasize the importance of well-designed instructions in eliciting reasoning from the model.</sample>
    <sample id="101">The fluency of PaLM is comparable to state-of-the-art systems, according to the human evaluation using the MQM framework.</sample>
    <sample id="102">The important properties of a watermarking method are: 

1. Applicable to embedding as services.
2. Does not degrade the utility of the provided embeddings.
3. Covert enough to prevent easy removal by attackers.
4. Transferable to the attacker's services during the model extraction process.</sample>
    <sample id="103">The English TED talks have been translated into 14 different languages, but the specific languages are not listed in the provided content.</sample>
    <sample id="104">The text does not specify the number of instances sampled from one dataset for reannotating.</sample>
    <sample id="105">The distance metrics used for measuring the difference between benign and backdoor datasets are cosine similarity, L2 similarity, and the KS test p-value.</sample>
    <sample id="106">This paper introduces QUEST, a retrieval dataset designed to address information-seeking scenarios involving implicit set constraints. The dataset is motivated by real-world examples, such as a zoologist searching for an unknown species or a reader looking for specific types of books. These scenarios involve queries that implicitly express set operations like intersection and complement. QUEST contains over 3,000 entity-seeking queries constructed from Wikipedia categories in four domains: films, books, plants, and animals. Queries are generated by applying set operations and then paraphrased by human annotators to ensure fluency and naturalness. Each query's answer entities are verified for relevance, and documents are annotated with attributable spans corresponding to different query constraints. The dataset presents a challenging retrieval task, as systems must identify multi-answer sets where evidence for relevance comes from different parts of the document. Evaluation shows that existing retrieval systems, including sparse and dense retrievers and a T5-based reranker, perform poorly, with particularly low F1 scores for queries involving set intersections and differences. The authors hope that QUEST will aid future research in improving systems for handling selective information needs. They invite researchers to explore the dataset and attend their presentation at ACL.</sample>
    <sample id="107">The multilingual encoder-based models, such as XLM-R + PTR and mBERT + PTR, were used with pointer-based decoders for semantic parsing. They were evaluated in both monolingual and multilingual settings, and their performance was compared with encoder-decoder models like mBART and mT5. Training these models on a mixture of languages improved their performance, except for English, which showed a performance drop in some cases.</sample>
    <sample id="108">This paper investigates the robustness of language models' acceptability judgments when presented with longer context sequences, revisiting the Minimal Pair Paradigm (MPP) used in tasks such as BLiMP and SyntaxGym. While traditional MPP evaluations focus on short, isolated sentences, the authors extend this approach to evaluate models on longer sequences by prepending context from the same or different datasets. The study finds that models' acceptability judgments remain stable when irrelevant contexts (e.g., from Wikipedia) are added, but are significantly influenced when context comes from the same syntactic phenomenon in the original dataset. Specifically, adding acceptable or unacceptable prefixes from the same domain leads to substantial shifts in the model's acceptability judgments, with the effect growing as the context length increases. This suggests that models are sensitive to shared syntactic and semantic features across sentences, and that current MPP evaluations may not fully reflect the models' understanding in extended contexts. The authors also show that perturbing sentences within the same acceptability domain leads to consistent changes in judgments, reinforcing the role of latent syntactic features. The work highlights the need for evaluating language models on longer sequences to better understand their contextual reasoning capabilities.</sample>
    <sample id="109">**Abstract:**  
This paper introduces *Unnatural Instructions*, a large-scale dataset of natural language instructions and their corresponding input-output pairs, generated automatically without human labor. While instruction tuning typically relies on manually annotated data or reformulated academic benchmarks, *Unnatural Instructions* leverages a pre-trained language model (e.g., GPT-3) to generate diverse, creative tasks in a fully automated way. The process begins by prompting the model to generate an instruction and input, followed by generating a matching output. Additional paraphrases of each instruction are created to enhance diversity. The resulting dataset contains 64,000 examples, expanding to 240,000 with paraphrases. Analysis shows that over 50% of the generated examples are correct, with even incorrect examples providing useful signals for instruction tuning. The dataset includes highly creative and unconventional tasks, such as evaluating scientific experiments or inventing new words. The utility of *Unnatural Instructions* is demonstrated by fine-tuning an 11-billion-parameter T5 model, which outperforms baselines like T0++ and Tk-instruct on multiple benchmarks, including Super-Natural Instructions, T0, BIG-Bench Hard, and LMentry. The study highlights the potential of language models to generate diverse, high-quality instruction data efficiently, surpassing the limitations of crowd-sourced annotations.</sample>
    <sample id="111">The authors decide what moderate-frequency words are by collecting a general text corpus and counting the word frequency within it. They select a trigger set consisting of words that fall within a moderate frequency interval.</sample>
    <sample id="114">This paper introduces a novel approach to compress large language models by optimizing the multi-head attention mechanism, titled "Finding the Pillars of Strength for Multi-Head Attention." The work, presented at ACL 2023 by researchers from Nanyang Technological University, addresses the issue of parameter redundancy in large language models. The proposed method, called Grouped Head Attention (GHT), employs a two-stage strategy: group-constrained training and Voting-to-Stay pruning. Group-constrained training divides attention heads into groups, making intra-group heads more similar and inter-group heads more distinct, using a combination of homogenization and diversification objectives. The Voting-to-Stay algorithm then prunes redundant heads, retaining only one representative per group, achieving significant parameter compression. The models GHT and GHT-PS demonstrate performance improvements across tasks such as machine translation, language modeling, and abstractive summarization, with up to 32.1% parameter compression and comparable performance. Additionally, the LITE model achieves 90% parameter pruning, 62% faster inference, and 80% fewer FLOPs. The study highlights the potential of task-specific pruning, inspired by the Lottery Ticket Hypothesis, suggesting that large language models can be efficiently tailored for specific applications without performance loss. This approach paves the way for more efficient deployment of large models in real-world scenarios.</sample>
    <sample id="115">The approach uses a speech segment size defined by lambda speech frames, which determines how much of the audio input the model considers when deciding whether to emit a partial translation.</sample>
    <sample id="116">The entity-specific knowledge needed is "Servin is a judge."</sample>
    <sample id="117">The most important factor is example quality.</sample>
    <sample id="118">This paper introduces *SwitchMLM*, a novel masked language modeling (MLM) approach tailored for code-switched natural language processing (NLP). Code-switching, common in multilingual communities like India, involves alternating between languages within a single sentence, such as in "Laptop, mere, bag, me, rakha, hai" (English-Hindi mix). Traditional multilingual models like mBERT and XLM-R perform poorly on such tasks. To address this, the authors propose *SwitchMLM*, which focuses on maskable *switch-points*—pairs of tokens marking language transitions. This approach requires language identification (LID) tags, which may not always be available, so they also introduce *FrequencyMLM*, a surrogate method that uses negative log likelihood from monolingual corpora to infer LID tags. Additionally, the paper suggests architectural modifications, such as residual connections from intermediate layers with high switch-point information to the final layer, and an auxiliary LID-based loss to enhance language-specific encoding. Experimental results on sentiment analysis show that combining Switch/FrequencyMLM with ResBERT and auxiliary loss outperforms existing methods across language pairs. Probing experiments using linear and conditional probing confirm that the proposed methods increase switch-point information in model representations, validating the effectiveness of the architectural and training modifications. Overall, the work advances pretraining techniques for code-switched NLP by improving model awareness of language transitions and enhancing performance on downstream tasks.</sample>
    <sample id="119">The paper focuses on GPT series, BART series, and RoBERTa in the extended experiments.</sample>
    <sample id="120">The model uses attention scores from a specific layer, leveraging the cross-attention mechanism between the audio input and textual output. It does not combine scores from several layers.</sample>
    <sample id="121">Examples of direct inference include using the name of the entity, such as saying "Easy on Me," or referring to the position, like "the first one."</sample>
    <sample id="122">The authors are affiliated with Fudan University.</sample>
    <sample id="123">This research introduces **MultiInstruct**, the first large-scale multi-modal instruction tuning benchmark dataset comprising 62 diverse tasks across 10 categories, derived from 21 open-source datasets. Each task includes five expert-written instructions, addressing the lack of publicly available multi-modal instructional data. The study investigates whether instruction tuning can improve the zero-shot generalization of multi-modal pre-trained models, such as OFA, on unseen tasks. Using OFA as a base model, the researchers train on 53 tasks with 10,000 instances each and evaluate on a held-out test set, including tasks from various groups. The evaluation includes standard metrics like accuracy and ROUGE-L, as well as a newly proposed metric called **sensitivity**, which measures the model’s consistency in output across variations in instruction wording. Results show that instruction tuning significantly improves OFA’s performance on seen multi-modal tasks, with transfer learning from natural instruction datasets further enhancing performance and reducing sensitivity. The study also demonstrates that using multiple instruction templates per task improves both performance and robustness. Additionally, the team is expanding the dataset with approximately 150 new vision-language tasks. This work contributes to the growing field of multi-modal zero-shot learning by providing a benchmark and demonstrating effective instruction tuning strategies.</sample>
    <sample id="124">This paper presents "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models," which systematically categorizes temporal reasoning into three levels: time-to-time, time-to-event, and event-to-event. Prior work has focused mainly on the second level, but this study provides a more comprehensive analysis. The authors introduce TempReason, a benchmark dataset covering all three levels with extensive temporal coverage. They evaluate large language models (LLMs) on three QA settings: closed-book, open-book, and a new "Reasoning QA" setup. The results reveal that models like ChatGPT struggle with month-level time prediction and exhibit significant performance fluctuations across different time periods. To enhance temporal reasoning, the authors propose a training strategy combining temporal span extraction pre-training and time-sensitive reinforcement learning. Their final model, TempT5, outperforms existing models on TempReason, especially in open-book and reasoning QA settings. The study highlights the limitations of current LLMs in temporal reasoning and provides a framework for future improvements. The findings emphasize the need for more balanced training data and specialized training methods to reduce temporal reasoning biases in large language models.</sample>
    <sample id="125">The information provided does not specify the number of authors involved in the paper.</sample>
    <sample id="126">Yes, translating the natural language query using a machine translation model (like Google Translate API) before semantic parsing was considered as a baseline in the "Translate-Test" setting.</sample>
    <sample id="127">This paper introduces a novel approach titled "Large Language Models Are Reasoning Teachers," where large language models (LLMs) are utilized as reasoning teachers to transfer their reasoning capabilities to smaller, more deployable models. The work addresses the limitation that chain-of-thought (CoT) prompting, which enables LLMs to perform multi-step reasoning, is only effective on very large models due to their high computational and memory demands. To overcome this, the authors propose using these large models to generate step-by-step solutions for complex tasks, which are then used as training data to fine-tune smaller models. A key innovation, called "Diverse Reasoning," involves generating multiple reasoning paths from the teacher model using stochastic sampling, thereby improving the diversity and quality of training data for the student model. The method is evaluated on 12 reasoning tasks, demonstrating significant improvements over baseline approaches, with performance on tasks like Multi Arithmetic increasing from 33% to 55%. The results show that even small models with less than 1 billion parameters can achieve notable reasoning capabilities through this distillation process. The approach is scalable and offers a trade-off between development and inference costs. The paper also provides detailed analysis, code, and data, making the method accessible for further research and applications.</sample>
    <sample id="128">**Abstract:**  
In this work, we introduce the KITMUS Test, a diagnostic framework designed to evaluate the ability of natural language understanding (NLU) models to integrate knowledge from multiple sources—both pretraining data and inference-time context. Our focus is on coreference resolution, a task requiring the integration of entity-specific and background knowledge. We present a dataset with scenarios where pronouns must be resolved using knowledge that may be available in either pretraining data, inference context, or both. We define three settings—Background-Pretrain, Background-Both, and Background-Inference—to systematically assess how models utilize knowledge from different sources. Human and model evaluations reveal that without task-specific training, most models fail to perform well on KITMUS, suggesting they rely heavily on surface cues rather than deeper knowledge integration. While training on KITMUS improves performance, even the best models struggle to reliably integrate background knowledge provided solely at inference time, especially when it involves novel or fictional concepts. Our findings highlight the challenges models face in reasoning over heterogeneous knowledge sources and emphasize the need for better methods to enable robust knowledge integration in NLU tasks. The dataset and code are publicly available on GitHub.</sample>
    <sample id="129">The authors gave the example of a "woman warrior" as a marked group, noting that the term is typically marked with "woman" because "warrior" is usually associated with men.</sample>
    <sample id="130">The content does not specifically mention which model architectures do not generalize well, but it highlights that transformer models generally generalize better to new data. It implies that non-transformer models may not generalize as well, but without explicit naming, we cannot definitively state which ones.</sample>
    <sample id="131">The provided content does not mention the names of any specific testing datasets.</sample>
    <sample id="132">The paper involves two authors: Akshatha and Martin.</sample>
    <sample id="133">The author works with multiple modalities, including text, images, and bounding boxes.</sample>
    <sample id="135">ABC-Eval is a novel dimensional evaluation framework for assessing conversational AI models, developed by the Emory NLP Lab in collaboration with Amazon Alexa AI. Traditional human evaluation methods, such as Likert ratings or pairwise comparisons, provide holistic assessments of dialogue quality but lack granularity in identifying specific strengths and weaknesses. ABC-Eval addresses this by explicitly annotating behaviors that affect chat quality, such as irrelevance, contradictions, hallucinations, and empathy. This approach reduces subjectivity by focusing on measurable behaviors rather than overall judgments. The method was tested on four state-of-the-art chat models using 100 human-bot conversations each, with results compared against existing evaluation techniques. ABC-Eval demonstrated higher inter-annotator reliability and greater predictive power for overall conversation quality, as evidenced by linear regression analyses. It also revealed that combining all ABC-Eval metrics explains over 25% of conversation quality, significantly more than traditional methods. The study highlights that current models still exhibit notable issues, such as common sense violations in 20% of responses and contradictions in 10%. ABC-Eval offers a more precise and informative evaluation tool, enabling a finer-grained analysis of chat models and supporting the development of more reliable conversational AI systems.</sample>
    <sample id="136">**Abstract:**  
In this work, we introduce FERMAT, a novel evaluation framework designed to assess the numerical reasoning capabilities of language models more effectively than traditional accuracy-based benchmarks. Motivated by the limitations of existing metrics and the poor performance of many models on numerical tasks, we develop FERMAT to provide a detailed analysis of models' strengths and weaknesses in arithmetic reasoning. The framework is built using questions from CommonCore and Illinois datasets, with numbers modified to include decimals, small, and large integers, enabling a broader evaluation of model capabilities. FERMAT also examines the impact of mathematical operations and training dependency on performance. Through zero-shot and fine-tuned evaluations, we find that even larger models struggle significantly with numerical reasoning, highlighting the need for more representative benchmarks. Fine-tuning with teacher-generated templates improves performance across various numerical contexts, but models still fail to generalize well from training to testing when exact expressions are not seen. Additionally, we explore the effects of training data diversity, showing that incorporating diverse linguistic and mathematical examples from datasets like GSM8K and AQUA significantly enhances model performance. Our findings emphasize the importance of language and mathematical diversity in training, as well as the need for improved number encoding and tokenization strategies. FERMAT offers a more informative alternative to traditional metrics, aiming to bridge the gap between model evaluation and real-world numerical reasoning requirements.</sample>
    <sample id="137">This paper introduces *Tell2Design*, a novel dataset and task for language-guided floor plan generation, aiming to enable users to design floor plans through natural language instructions. Unlike traditional image generation models that focus on producing realistic artwork, this task requires generating structured 2D floor plans that strictly adhere to detailed semantic, geometric, and topological constraints specified in text. The Tell2Design dataset contains 5,051 human-annotated and 76,000 artificially generated language instructions paired with real-world floor plans. The proposed approach frames floor plan generation as a sequence-to-sequence problem, using a transformer-based encoder-decoder model initialized with a pre-trained language model (T5). This model reconstructs room bounding boxes as a structured target sequence from unstructured text, enabling it to handle varying instruction lengths and design complexities. Evaluation on the T2D dataset shows that the proposed model achieves state-of-the-art performance in terms of IoU scores, significantly outperforming text-conditional image generation baselines. The study also highlights the importance of addressing the language distribution gap between artificial and human instructions, demonstrating that combining both types of data during training improves performance. This work lays the foundation for future research on language-guided design generation, particularly in the domain of floor plan design.</sample>
    <sample id="138">The authors claim that the integration of knowledge from multiple sources (both pretraining and inference time) is an understudied area in natural language understanding (NLU).</sample>
    <sample id="139">The names of the speakers are Ying and Zhiyang.</sample>
    <sample id="140">Yes, CoScript underwent quality checks where crowd-sourced workers found and revised incorrect samples to ensure the quality of the validation and test sets.</sample>
    <sample id="141">Existing resources for context-dependent translation are limited in the types of context-dependent translations they support and the sets of languages they cover, as they usually rely on domain knowledge and human curation.</sample>
    <sample id="143">The approach is compared to the Wait-k strategy, the Local Agreement, and the state-of-the-art architecture specifically tailored for simultaneous pre-translation.</sample>
    <sample id="144">The affiliations of the authors of the paper are not mentioned in the provided content.</sample>
    <sample id="145">The name of the speaker is Jenny.</sample>
    <sample id="146">This paper presents a comprehensive analysis of the omission problem in dialogue summarization, highlighting its prevalence and impact on summary quality. Despite the advances in using pre-trained language models for dialogue summarization, generated summaries often suffer from factual omissions, leading to incomplete and inaccurate outputs. The study reveals that even state-of-the-art models exhibit a high omission rate, with approximately 70% of summaries missing critical information. Omitted content is found to be randomly distributed across dialogues, emphasizing the difficulty of identifying key information in unstructured conversations. To address this, the authors introduce the OLDS dataset, the first benchmark for omission detection in dialogue summarization. This dataset includes high-quality omission labels generated through both automatic and human evaluation methods. The paper explores three baseline models for omission detection—pair-wise classification, sequence labeling, and pointer networks—showing that the task is highly challenging due to label imbalance. Furthermore, the study demonstrates that incorporating detected omissions into a post-editing refinement process significantly improves summary quality. These findings underscore the importance of omission detection as a crucial step toward enhancing the accuracy and completeness of dialogue summaries.</sample>
    <sample id="147">The paper involves three authors: Myra, Esin Durmus, and Dan Jurafsky.</sample>
    <sample id="149">Yes, the dataset is publicly available.</sample>
    <sample id="150">**Abstract:**  
In this paper, we introduce *MeetingQA*, a novel extractive question-answering dataset derived from real-world meeting transcripts. Meeting transcripts, characterized by their length, domain specificity, and information density, present a rich yet underexplored domain for NLP research. While prior work has focused on summarization and action item extraction, we highlight the underutilized QA aspect of meetings, where participants often pose open-ended questions that elicit detailed responses and discussions. MeetingQA contains 7,700 questions, with 30% being unanswerable, 40% having multi-span answers, and 48% involving multiple speakers. The dataset includes diverse question types, such as yes/no, opinion-seeking, and rhetorical questions, with 70% of multi-speaker answers involving disagreement. We evaluate a range of QA models, including short- and long-context approaches, single- and multi-span models, and data augmentation using silver annotations. Results show a significant gap between model performance and human baseline (F1=84.6), especially in zero-shot settings, with models struggling to identify rhetorical questions and accurately attribute answers to the correct speaker. Overall, MeetingQA presents a challenging benchmark for QA models, highlighting the need for further research in handling complex, real-world dialogue structures.</sample>
    <sample id="152">This presentation introduces a series of newly developed language models tailored for classical philology, focusing on Ancient Greek and Latin. While existing models such as Latin BERT and Ancient Greek BERT have advanced the field, they are limited to monolingual, encoder-only architectures and lack comprehensive evaluation. To address these shortcomings, the research team pre-trained two monolingual models—GreBERTa (based on RoBERTa) and GreTa (based on T5)—for Ancient Greek, as well as multilingual equivalents, PhilBERTa and PhilTa, trained on Ancient Greek, Latin, and English data. A key innovation involved creating a high-quality pre-training corpus for Ancient Greek by leveraging the Internet Archive, using OCR with Greek character support after identifying Greek texts via misrecognized stop words. The models were benchmarked on tasks like part-of-speech tagging, dependency parsing, and lemmatization, outperforming previous state-of-the-art models. The study also revealed that the T5 encoder, when used in isolation, initially underperforms compared to native encoder-only models but improves with additional training. Encoder-decoder models demonstrated notable improvements in lemmatization. Despite their multilingual capabilities, no significant performance difference was found between multilingual and monolingual models in semantic and world knowledge tasks. The work provides valuable resources and insights for classical philology, offering a foundation for further research in NLP and ancient languages.</sample>
    <sample id="153">**Abstract:**  
In this work, we address the issue of ambiguities in prompts provided to text-to-image generative models, which hinder the ability of these models to generate images that align with user intention. We introduce a framework to disambiguate ambiguous prompts by leveraging external signals, either through generating clarifying questions for user feedback or by proposing multiple visual interpretations for user selection. To support this, we curate a benchmark dataset, an extended version of the LAVA corpus, that encompasses various types of ambiguities. Once prompts are disambiguated, we evaluate the faithfulness of the generated images to user intent using an automatic evaluation framework. This involves generating images from both ambiguous and disambiguated prompts and using a VQA model to assess whether the resulting images reflect the user’s intended meaning. Our experiments show that resolving ambiguities improves the faithfulness of image generation, and our automatic evaluation framework aligns well with human judgments, making it a reliable tool for assessing model performance. This work contributes both a benchmark dataset and evaluation methodologies to advance the development of more accurate and user-intention-aware text-to-image models.</sample>
    <sample id="154">The authors of the paper are affiliated with the University of Trento and Foundazione Bruno Kessler.</sample>
    <sample id="155">The name of the speaker is Javad Hosseini.</sample>
    <sample id="157">This paper introduces "Dialogue Summarization with Static-Dynamic Structure Fusion Graph" (SDDS), a novel approach to dialogue summarization that addresses the limitations of existing methods relying on pre-computed static graph structures. Traditional approaches heavily depend on external linguistic tools, which can be error-prone and lack adaptability to downstream summarization tasks. The proposed SDDS model integrates both static and dynamic graph representations to better capture the structure and semantics of dialogues. The model consists of four main components: an Utterance Encoder for vector representation, a Static Graph module using heuristic methods such as discourse parsing and speaker interaction frequency, a Dynamic Graph module that employs multi-head attention to learn semantic relationships, and a Summary Generator based on a pre-trained language model. To enhance the integration of static and dynamic information, the model uses a fusion mechanism that combines static adjacency matrices with dynamic relation matrices, followed by a dual cross-attention mechanism to incorporate graph representations during summary generation. Experimental results demonstrate the effectiveness of the proposed method in capturing both structural and semantic aspects of dialogues, leading to more accurate and coherent summaries. The code and data are publicly available on GitHub.</sample>
    <sample id="158">This paper introduces "Dual Cache for Long Document Neural Coreference Resolution," a novel approach to improving the efficiency and accuracy of coreference resolution in long documents. Coreference resolution involves identifying and clustering mentions of the same entity across a text. Traditional methods suffer from quadratic complexity due to pairwise mention comparisons, while recent cache-based methods reduce this to linear complexity but face challenges with long documents where entities may be scattered and topics change frequently. The proposed dual cache system addresses these issues by combining a local cache (using LRU eviction) for short-range, local entities and a global cache (using LFU eviction) for high-frequency, globally relevant entities. This dual structure allows the model to manage cache misses more effectively, especially in long documents. The method is evaluated on four public benchmarks and a manually annotated book with 30,000 words, showing that dual cache outperforms baseline methods, particularly in long documents, with fewer cache misses and higher performance per computational cost. The results demonstrate that dual cache achieves a superior balance between model efficiency and accuracy, making it a more cost-effective solution for coreference resolution in large-scale text processing.</sample>
    <sample id="160">The first step of the method maps input tokens to an unordered multiset of tokens that will appear in the output.</sample>
    <sample id="161">CoScript represents 55,000 scripts.</sample>
    <sample id="163">The best alignment method for DEPLAIN is MASSalign.</sample>
    <sample id="164">The benefit of weakly supervised learning is that it allows training neural networks using cheaper, albeit noisy, annotations from weak labeling sources, rather than relying on expensive manual labeling.</sample>
    <sample id="165">In this presentation, Wenting Zhao introduces a novel unsupervised approach to abductive reasoning called LiPoR (Likelihood Learning with Posterior Regularization). Abductive reasoning involves identifying a plausible explanation that connects a given context to an outcome. Traditional methods rely on supervised learning with annotated explanations, which can be subjective and inconsistent. Zhao's work addresses this by proposing an unsupervised method that does not require explicit labeling of plausible explanations. LiPoR treats explanations as latent variables and maximizes the marginal likelihood of the outcome given the context. To ensure the selection of plausible explanations, the method incorporates a regularizer based on the mutual exclusivity of explanations—only one explanation can be true at a time. This regularizer encourages a preference for a subset of explanations by minimizing the entropy of the posterior distribution over explanations. Experimental results on the AlphaNLI dataset show that LiPoR outperforms existing unsupervised and zero-shot models by over 4 absolute points in accuracy. This work demonstrates that abductive reasoning can be effectively learned without supervision, opening new possibilities for reasoning tasks in natural language processing.</sample>
    <sample id="166">This paper introduces a novel neural divide-and-conquer reasoning framework, named NDCR, for image retrieval from linguistically complex text. The task is challenging due to high image similarity and complex descriptions, where traditional visual language models struggle. Drawing inspiration from the Divide-and-Conquer strategy and the Dual-Process Theory of human cognition, the framework integrates two reasoning systems: System 1 for analogical reasoning and System 2 for logical reasoning. The proposed method includes three key components: a Proposition Generator that decomposes complex text into simple propositions, a Visual-Linguistic Interactor (System 1) that matches propositions with images, and a Neural-Symbolic Reasoner (System 2) that combines reasoning states through negation and conjunction operations. Experimental results show that NDCR outperforms existing baselines, with ablation studies confirming the effectiveness of each module. Case studies further demonstrate the framework's ability to provide interpretable inference states and results. The study suggests that integrating neural symbolic reasoning with divide-and-conquer strategies can enhance compositional reasoning and planning in large language models, offering a promising direction for tackling complex reasoning tasks.</sample>
    <sample id="167">The documents in DEPLAIN-web were aligned using both manual and automatic alignment methods.</sample>
    <sample id="168">The CoNLL++ dataset was created by collecting data from Reuters News from 2020 and annotating it using the same CoNLL-2003 annotation guidelines.</sample>
    <sample id="169">This paper presents a systematic evaluation of using large language models (LLMs) for machine translation, focusing on the effectiveness of different prompting strategies with PaLM, a 540-billion parameter model. The study assesses PaLM's translation capabilities using standard machine translation test sets and compares its performance to state-of-the-art systems, including neural MT metrics and human evaluations via the MQM framework. The results show that prompting strategies significantly influence translation quality, with a 5-shot prompting approach yielding the best results. The study finds that the quality of example translations is more critical than the similarity between the prompt and the source sentence. High-quality examples from curated datasets, such as WMT dev data, lead to better performance compared to noisy training data. While PaLM achieves fluency comparable to commercial systems like Google Translate, it lags in accuracy, often omitting parts of the source text to produce more fluent outputs. The findings highlight the importance of prompt selection and example quality in leveraging LLMs for translation tasks, while also emphasizing the current limitations in accuracy compared to specialized MT systems.</sample>
    <sample id="171">The existing works on protecting the copyright of embedding as services can be broadly classified into four categories, but they either are not applicable to embedding as services or lack transferability.</sample>
    <sample id="172">No, multilingual LLMs such as Codex or Bloom are still inadequate for cross-lingual semantic parsing (CLSP) tasks.</sample>
    <sample id="174">The paper introduces ArgAnalysis35K, a large-scale dataset designed for argument quality analysis, offering several unique features that distinguish it from existing datasets in the field. With 35,000 argument-analysis pairs, it is the largest of its kind, containing high-quality arguments sourced from expert and intermediate debaters, as well as speeches from top tournaments. The dataset features a diverse range of 24 themes, capturing a broader spectrum of motions commonly found in parliamentary debates, rather than relying on a limited set of pre-selected topics. A novel aspect is the inclusion of "analysis," which goes beyond simple claims or premises by combining them into coherent explanations, thereby enhancing the depth of argument evaluation. The dataset also introduces instance-based annotator reliability, allowing for more accurate scoring by accounting for annotator biases on a per-argument basis. Additionally, it includes a relevance model that assigns relevance scores to arguments based on their connection to specific themes, enabling a more nuanced understanding of how arguments apply across different contexts. These features collectively contribute to a more reliable, diverse, and informative dataset for argument quality analysis, with the potential to advance research in natural language processing and argumentation studies. The paper encourages further exploration and feedback on the dataset.</sample>
    <sample id="175">The method deals with permutation ambiguity by inducing the alignment between input and output as part of the training process and using a GPU-friendly continuous relaxation to approximate the highest-scoring permutation, allowing backpropagation to learn linguistically plausible permutations.</sample>
    <sample id="176">The fairness of a downstream NLP model is defined by its ability to perform consistently and justly across different demographics and political leanings, without marginalizing certain groups or favoring others, particularly in tasks like hate speech and fake news detection.</sample>
    <sample id="177">The name of the speaker is Yanis Labrak.</sample>
    <sample id="178">The name of the speaker is Koustav Sinha.</sample>
    <sample id="179">**Abstract:**  
Melanie Sclar presents *SymbolicToM*, a plug-and-play inference-time method designed to enhance the Theory of Mind (ToM) reasoning capabilities of large language models (LLMs). ToM involves understanding others' mental states, often tested through false-belief scenarios like the Sally-Anne task. Despite their capabilities, LLMs such as GPT-3 and ChatGPT struggle with such tasks. *SymbolicToM* addresses this by leveraging explicit graphical representations of characters' beliefs, including nested perspectives (e.g., what Bob believes about what Alice believes). These graphs, such as BBob and BBob,Alice, are generated using off-the-shelf NLI and OpenIE models during inference. The method then translates belief-based questions into factual queries over these graphs, enabling more accurate responses. Experimental results show significant performance improvements across multiple LLMs, including GPT-3 and Flan-T5-XXL, with gains of up to 67 accuracy points on in-domain ToM tasks. *SymbolicToM* also demonstrates robustness in out-of-domain scenarios, such as modified story structures and paraphrased questions, where supervised models like Textual Time Travel and fine-tuned GPT-3 show substantial performance drops. By avoiding overfitting and providing interpretable reasoning, *SymbolicToM* outperforms supervised approaches in both standard and novel ToM benchmarks, including the newly introduced ParaphrasedToMi dataset. This work offers a scalable, model-agnostic approach to improving ToM reasoning in LLMs.</sample>
    <sample id="180">The name of the speaker is Myra.</sample>
    <sample id="181">This paper introduces the problem of constrained language planning, where the goal is to generate step-by-step scripts that adhere to specific constraints, such as "make a chocolate cake," rather than abstract goals like "make a cake." While large language models (LLMs) have shown effectiveness in planning for abstract goals, their performance on constrained goals remains limited. The study evaluates LLMs on constrained planning tasks and finds that although generated scripts are semantically complete, they often fail to meet specific constraints. To address this, the authors propose an over-generate-then-filter approach, where multiple scripts are generated and filtered based on semantic similarity and constraint keywords. This method significantly improves the quality of generated scripts in terms of both completeness and faithfulness to constraints. Furthermore, the paper introduces CoScript, a large-scale dataset of 55,000 constrained language planning examples, created by distilling knowledge from LLMs and validated by human annotators. The dataset exhibits high diversity in constraint types and enables the training of smaller, specialized models—such as T5—that outperform many large models on constrained planning tasks. The work establishes a foundation for constrained language planning and provides a valuable resource for future research in this area.</sample>
    <sample id="182">In the context of this paper, tropicalism indicates a stereotype or trope that associates Latina women with vibrant and curvaceous traits, linking them to a perceived tropical or exotic identity, which contributes to essentializing and discriminatory narratives.</sample>
    <sample id="183">The authors created the human-written portrayals of target groups by giving similar prompts to human subjects, inspired by a study where participants were asked to imagine themselves as members of specific demographic groups and describe themselves. This enabled a direct comparison between human-generated and model-generated personas.</sample>
    <sample id="184">In this work, Pointwise CXMI was used to measure context usage in machine translation, allowing for analysis at both the sentence and word levels.</sample>
    <sample id="185">DrBERT is a biomedical model in French trained on NACHOS, a dataset of crawled medical data from the web, while ChuBERT is based on anonymized clinical data from the Nantes University Hospital data warehouse. DrBERT is designed for a wide range of usage with crawled data, whereas ChuBERT is more specialized for clinical data.</sample>
    <sample id="187">The paper involves two authors: Ying and Zhiyang.</sample>
    <sample id="188">Iterative transfer learning is a method where a model is first trained on a related task, then iteratively fine-tuned on additional related tasks to improve performance on the target task, in this case, dissonance detection.</sample>
    <sample id="189">The goal of the AltEntities Corpus dataset is to understand users' language when they want to make a choice between entities, particularly through the use of indirect referring expressions, and to benchmark language models' entity understanding in conversational systems.</sample>
    <sample id="190">An attacker can extract model parameters through an Embedding as a Service (EaaS) by learning from the embeddings provided by the service. By analyzing a sufficient number of embedding outputs, the attacker can reverse-engineer or approximate the parameters of the underlying model and provide similar services.</sample>
    <sample id="191">The paper involves three authors: Sara Papi, Matteo Negri, and Marco Turchi.</sample>
    <sample id="192">This presentation introduces **CAME**, a **Confidence-guided Adaptive Memory Efficient optimizer**, designed to address the dual challenge of achieving **fast convergence** and **low memory usage** in training large language models. Traditional adaptive optimizers like Adam suffer from high memory consumption due to storing first and second moment estimates, while memory-efficient alternatives like Adafactor, though reducing memory usage, often incur performance penalties due to erroneous updates. CAME tackles this by leveraging insights from **Non-negative Matrix Factorization (NMF)** and addressing the instability caused by erroneous updates in memory-efficient optimizers. It introduces a method to calculate an **instability matrix** based on the residual between predicted and actual updates, which is then used as a denominator to guide more adaptive optimization steps. Experimental results on large language models such as BERT, GPT-2, and T5 demonstrate that CAME significantly outperforms both Adam and Adafactor in terms of validation accuracy and memory efficiency, especially under large batch sizes. The proposed optimizer reduces memory usage while maintaining or even improving training performance, making it a promising advancement for memory-efficient optimization in large-scale model training.</sample>
    <sample id="193">The text does not specify the number of annotators used to create the initial dataset.</sample>
    <sample id="194">The authors of the paper are affiliated with Carnegie Mellon University, the University of Washington, and the Allen Institute for AI.</sample>
    <sample id="195">This paper introduces RoHT, a novel framework for Explainable Question Answering (XQA) that leverages hierarchical question decomposition to integrate knowledge from both structured knowledge bases (KBs) and unstructured text corpora. Existing XQA methods face limitations: neuro-symbolic approaches are constrained by incomplete KBs, while decomposition-based methods struggle with the diversity of natural language. RoHT addresses these challenges by constructing a Hierarchical Question Decomposition Tree (HQDT), where complex questions are decomposed into sub-questions and atomic questions. The framework performs probabilistic reasoning over the HQDT, recursively determining the optimal knowledge sources (KB, text, or child nodes) for each node and aggregating answers with probabilities. This enables the model to flexibly combine heterogeneous knowledge sources at different decomposition levels. Evaluations on KQA Pro and Musique datasets show that RoHT outperforms existing methods. On KQA Pro, integrating KB and Wikipedia improves performance significantly, while on Musique, RoHT-text and RoHT-mix models achieve substantial gains over state-of-the-art methods. The results highlight the effectiveness of hierarchical decomposition and the benefits of combining structured and unstructured knowledge for complex question answering.</sample>
    <sample id="196">The example where the governor is on the left is "I saw Bart and Lisa."</sample>
    <sample id="197">The state-of-the-art models in dialogue systems are not explicitly named in the provided content, but the text refers to "four state-of-the-art chat models" that were evaluated using ABC-Eval. Specific model names are not mentioned.</sample>
    <sample id="198">We need to evaluate the models' acceptability throughout the context window because modern large language models have increasing context windows, and it is crucial to understand how their acceptability judgments are affected by longer and more complex contexts, which may influence their syntactic and semantic understanding.</sample>
    <sample id="199">Yes, training in a multilingual fashion caused a performance drop compared to the monolingual English model in seven out of the nine datasets, a phenomenon referred to as the "Curse of Multilinguality."</sample>
    <sample id="200">No, the annotators do not know about the entity in advance. They are provided with background knowledge about the entities before selecting and describing them using indirect referring expressions.</sample>
    <sample id="201">The evaluation used state-of-the-art neural MT metrics, specifically BLEURT, and also included expert-based human evaluation results.</sample>
    <sample id="202">The regress in generalization does not specifically impact certain NER types; the performance drop is attributed to temporal drift rather than specific entity types.</sample>
    <sample id="203">Positionality in NLP matters because it highlights systematic performance differences in datasets and models across populations, which can lead to biased outcomes. Understanding these biases is crucial as NLP tasks become more subjective and socially oriented, and it helps ensure more inclusive and equitable technology development.</sample>
    <sample id="204">The content does not specify whether multilingual LLMs like BLOOM were fine-tuned with adapters or full fine-tuning.</sample>
    <sample id="205">This study investigates how political biases in pretraining data propagate through language models to affect fairness in downstream NLP applications. Language models are trained on extensive web-crawled data, which includes a significant presence of politically biased sources such as news outlets and social media. While this diversity can promote democratic discourse, it also introduces socially biased perspectives that may lead to unfair outcomes in tasks like hate speech and fake news detection. The research evaluates the political leanings of language models using political questionnaires and finds that models such as GPT-4 exhibit distinct ideological positions, with GPT models generally more liberal than BART variants. Controlled experiments show that further pretraining on partisan corpora shifts models' ideological coordinates, and models pre-trained on post-2017 data reflect increased political polarization. Evaluations on downstream tasks reveal that models with different political leanings perform unevenly across demographic and ideological groups—left-leaning models excel in detecting hate speech against minorities but struggle with speech targeting dominant groups, and vice versa for right-leaning models. These findings highlight a pressing fairness concern: biased models deployed in real-world applications may marginalize certain groups or fail to control harmful content. The study underscores the dilemma between allowing diverse perspectives and preventing harmful bias, akin to the trolley problem, emphasizing the urgent need for addressing political bias in language models.</sample>
    <sample id="206">They use a model that first fine-tunes on the CE (comparison and expansion) tasks and then further fine-tunes on the debate task for transfer learning.</sample>
    <sample id="207">The recent test sets used to assess the PaLM capabilities are the latest test sets from the WMT evaluation.</sample>
    <sample id="208">The authors proposed three recommendations at last.</sample>
    <sample id="209">The text does not provide specific numerical values for the gain of the proposed method over the strongest baseline.</sample>
    <sample id="210">The name of the speaker is Shuheng.</sample>
    <sample id="211">Yes, the results and dataset in the paper can be used as a benchmark for future automatic text simplification research.</sample>
    <sample id="212">They experiment with one smaller model, T5 fine-tuned on CoScript.</sample>
    <sample id="213">The OFA model is used as the base model for investigating multi-modal instruction tuning.</sample>
    <sample id="215">**Abstract:**  
This paper investigates the dependency structure of coordination, focusing on the debate between symmetric and asymmetric approaches. Traditional theories, such as Universal Dependencies and Meaning-Text Theory, assume an asymmetric structure where the first conjunct is the head, while the Prague Dependency Treebanks and Word Grammar propose symmetric structures, with either the conjunction or all conjuncts as heads. The study argues in favor of symmetric structures by employing the principle of dependency length minimization. This principle suggests that shorter syntactic dependencies are preferred, and it is used to analyze coordination patterns in the Penn Treebank. The analysis reveals that left conjuncts tend to be shorter when the governor is on the left or absent, a tendency that disappears when the governor is on the right. This asymmetry in conjunct length supports the idea that coordination structures are not inherently asymmetric, as the head position is not fixed. Instead, the data suggest that coordination can be better modeled with symmetric structures, where all conjuncts are treated equally. The findings challenge the assumptions of traditional asymmetric theories and provide empirical evidence in favor of symmetric dependency structures. The paper contributes to the ongoing discussion on coordination by offering a novel argument based on syntactic and statistical patterns.</sample>
    <sample id="217">This paper introduces "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation," focusing on enhancing the ability of dialogue systems to generate responses under multiple controllable attributes. Existing methods primarily target single attributes and struggle with continuous or compositional combinations. To address these limitations, the authors propose DCG (Disentangled Controllable Generation), which learns attribute concepts from seen values and employs a disentanglement loss to separate different attribute combinations. A novel unified, reference-free evaluation framework, MAE, is introduced to assess controllability across varying attribute granularities without requiring additional labeled data. The model integrates attribute-oriented and task-oriented prompts, concatenated to guide the generation process, and pseudo combinations are used to enhance diversity. Experiments on two benchmarks demonstrate that DCG outperforms existing methods in both controllability and text quality for both seen and unseen attribute combinations. The effectiveness of DCG is further supported by correlation analyses showing that MAE aligns well with human judgments. The study confirms the model's ability to generalize from seen attributes to unseen combinations, showcasing the potential of prompt-based disentangled approaches in multi-attribute controllable dialogue generation.</sample>
    <sample id="218">The authors of the paper are affiliated with Google Translate.</sample>
    <sample id="219">This paper introduces a compare-and-contrast multistage pipeline designed to uncover financial signals from 10-K annual reports. The work addresses the challenge of extracting meaningful information from highly similar and year-dependent financial reports, which typically require significant human effort. The proposed approach defines a highlighting task that identifies key words (rationales) that explain the relationship between a target report and its previous year's reference report. The pipeline consists of document segmentation, relation recognition, and two stages of fine-tuning: out-of-domain and in-domain. The out-of-domain stage leverages the eSNLI dataset for general natural language inference, while in-domain tuning uses pseudo-labeled revised pairs to improve relevance. The model is evaluated using the eSNLI dataset and the newly released FINAL dataset, with performance measured via precision and Pearson Correlation Coefficient (PCC). Results show that the domain-adaptive model achieves superior performance on the FINAL dataset and maintains strong generalization on eSNLI. The method also demonstrates effectiveness in simulating mismatched pairs not seen during training. The study contributes a novel highlighting task and a practical pipeline for financial report analysis, with potential for future improvements through additional features and techniques in information retrieval.</sample>
    <sample id="220">The authors are affiliated with Stony Brook University.</sample>
    <sample id="221">The paper analyzed translation from German into English.</sample>
    <sample id="222">**Abstract**  
This work investigates challenges and interventions for domain adaptation in open-domain question answering (QA). Using a motivating example, it highlights how models trained on general-purpose corpora like Wikipedia may fail when applied to specialized domains, such as biomedical, due to confusion caused by domain-specific terminology. The study presents three main contributions: (1) exploring data interventions to improve out-of-domain generalization, (2) identifying the type of dataset shift between source and target domains, and (3) determining which interventions are most effective for each type of shift. The research evaluates zero-shot and few-shot adaptation strategies. Few-shot methods leverage a small number of target-domain examples to prompt large language models for generating additional training data, improving retriever and reader performance by 8% and 11% on average. Zero-shot methods manipulate question, answer, and context distributions to better align with the target domain. The study also introduces a compatibility measure to classify dataset shifts—no shift, concept shift, covariate shift, and full shift—using likelihood scores from source models. Results show that few-shot adaptations benefit all target datasets, while concept and covariate shifts also respond well to zero-shot approaches. Overall, the work demonstrates that targeted data interventions can significantly enhance model performance, with up to 24% improvement in reader accuracy, depending on the nature of the domain shift.</sample>
    <sample id="223">The name of the speaker is Shangbin.</sample>
    <sample id="224">The models investigated during the experiments were long-mBART for document-level simplifications and base mBART for sentence-level simplifications.</sample>
    <sample id="225">From the 62 diverse tasks used in MultiInstruct, 53 tasks are used for training and 9 tasks are used for testing.</sample>
    <sample id="226">The presentation mentions two individuals, Regina Stodden and Omar, who are involved in presenting the paper. However, the number of authors is not explicitly stated in the provided content.</sample>
    <sample id="227">Recent advances in language models have enabled significant progress in natural language processing, yet grounded language understanding remains a critical challenge. Grounded language understanding involves mapping natural language to executable plans or programs within specific environments, such as executing SQL queries or controlling robots. The main issue lies in the lack of grounding during pre-training, leading to generated plans that are often invalid or non-executable. To address this, we propose the Pangu framework, which shifts the focus from generation to discrimination, leveraging language models for scoring and ranking candidate plans generated by a symbolic agent. This approach avoids the complexities of generating valid plans and instead capitalizes on the language model's strength in discrimination. We evaluate Pangu on knowledge-based question answering, a representative grounded language task, using various models like BERT, T5, and Codex, with both fine-tuning and in-context learning settings. Pangu demonstrates strong performance and sample efficiency, outperforming baselines like ArcaneQA. Notably, Pangu shows robustness under non-i.i.d. settings, as it does not overfit to seen structures. Our key insight is that for grounded language understanding, discrimination may be a more effective strategy than generation. This work highlights a novel direction for leveraging language models in grounded tasks.</sample>
    <sample id="228">The authors experimented on the following datasets: AG News, MIND, SST2, and Enron Spam.</sample>
    <sample id="229">This paper presents a study on detecting and improving suboptimal claims in argumentative writing, focusing on the challenges and opportunities of using revision-based data. The authors introduce two tasks: Suboptimal-Claim detection, which determines whether a claim needs revision, and Claim Improvement Suggestion, which identifies the types of quality issues that should be addressed. The research explores how to model argument quality based on implicit revision patterns found in collaborative debate platforms like Kialo. Four key challenges are identified: Representativity and Reliability of the revision data, Model Complexity and Architecture for capturing subtle changes in phrasing, the dependence of quality on contextual information, and Topical and User Bias in collaborative revision histories. The study demonstrates that revision-based data can be effectively used for these tasks, with modeling the distance between claim versions proving beneficial for detecting suboptimal claims. Additionally, the impact of contextual information varies depending on the specific task and quality issues involved. The paper provides a detailed analysis of strategies to address these challenges and offers a systematic comparison of approaches. The findings contribute to the development of tools that support novice writers in refining their argumentative claims, ultimately enhancing the clarity, persuasiveness, and effectiveness of their writing.</sample>
    <sample id="231">NACHOS is a dataset of medical crawled data from the web used for training the DrBERT model.</sample>
    <sample id="232">The name of the speaker is David Vilar.</sample>
    <sample id="233">This paper introduces EDAtt, a novel strategy for simultaneous speech translation (SimulST) that leverages the cross-attention mechanism of pre-trained offline speech translation models without requiring retraining or specialized architectures. Traditional SimulST approaches often involve complex training procedures, multiple models for different latency regimes, and additional modules, leading to increased complexity and computational costs. In contrast, EDAtt uses a single model for all latency settings and controls translation emission based on attention stability. Specifically, a partial translation is emitted when the cross-attention weights over the last λ speech frames fall below a threshold α, indicating that the model has sufficiently processed the input. This approach allows for real-time translation with adaptive latency. Experimental results on German-to-English SimulST show that EDAtt outperforms existing methods, including the Wait-k and Local Agreement strategies, as well as state-of-the-art SimulST architectures. The method achieves higher BLEU scores and lower average lagging, both in terms of time and computational cost. The proposed solution is efficient, flexible, and maintains the quality of the underlying offline model. The paper also provides open-source code and models to support further research and reproducibility.</sample>
    <sample id="234">The prompting strategy has a significant impact on the results, as demonstrated by an experiment showing a difference of more than one BLEURT point, with extreme cases reaching up to 40 BLEURT points. Selecting a good prompting strategy is crucial for achieving better performance in translation tasks.</sample>
    <sample id="235">The affiliations of the authors of the paper are not explicitly mentioned in the provided content.</sample>
    <sample id="236">The 5 expert-written instructions are not explicitly listed in the provided content. The presentation mentions that each task in the MultiInstruct dataset is equipped with five expert-written instructions, but it does not detail what those specific instructions are.</sample>
    <sample id="237">The authors propose the KITMUS test suite to evaluate models' ability to integrate and use knowledge from multiple sources, such as pretrain-time and inference-time knowledge, in a coreference resolution task.</sample>
    <sample id="238">In this video, Yebowen Hu from the University of Central Florida introduces MeetingBank, a newly created benchmark dataset for meeting summarization. The dataset includes transcripts, reference summaries, and related URLs from 1,366 City Council meetings across various cities. The data collection process involves converting audio recordings into transcripts using Speechmatics API, extracting meeting details, aligning timestamps, and pairing them with expert-written summaries. The dataset provides statistical insights into meeting durations, speaker counts, and summarization instances, along with metrics like coverage and density to evaluate the level of abstraction in summaries. For model evaluation, the study compares top-tier summarization systems, including extractive and abstractive models like BART-Large, DialogLM, and GPT-3. While extractive models like Oracle show strong ROUGE-2 scores, DialogLM performs best among abstractive models. Human evaluation highlights GPT-3's strength in fluency and coherence but its shortcomings in informativeness and factuality. The study emphasizes the need for improved evaluation metrics that better align with human judgment. MeetingBank serves as a valuable resource for researchers aiming to develop advanced meeting summarization technologies and offers insights into city council decision-making processes. The dataset is available for download and further exploration.</sample>
    <sample id="241">This paper presents a human-in-the-loop evaluation framework for early detection of misinformation, specifically focusing on claims about COVID-19 treatments. Existing automated systems for detecting misinformation on social media are often evaluated using retrospective datasets and may suffer from counter-evidence leakage, limiting their real-time effectiveness. Additionally, these systems typically exclude or marginalize human input, which is crucial for accurate and context-sensitive moderation. To address these issues, the authors propose an end-to-end system that integrates human feedback throughout the detection process. The system consists of two main components: (1) a detection module that identifies potentially misleading claims from raw tweets using keyword filtering and a T5-based model for claim extraction, followed by human verification based on trendiness; and (2) a policy violation verification module using a BERT-based stance classifier to flag tweets promoting unapproved treatments. The framework is evaluated for early detection of unapproved treatments before their debunking in news articles and for policy violation detection, with results showing 65% accuracy in identifying policy violations and 124.2 policy violations detected per human hour. The work highlights the importance of human involvement in real-time misinformation detection and provides a benchmark for future human-in-the-loop systems.</sample>
    <sample id="242">Common evaluation methods for dialogue systems include human evaluation approaches such as asking human judges to select which of two conversations is better (pairwise comparisons) or to rate conversations using a Likert scale. These methods provide holistic evaluations of overall dialogue quality. Additionally, dimensional evaluations assess specific aspects of chat quality, such as relevance, coherence, empathy, and factual accuracy, often through behavior annotation or Likert ratings on specific dimensions.</sample>
    <sample id="243">The paper involves 6 authors: Jenny (the presenter), Sebastian Santy, Ronan Le Bras, Katharina Reinecke, Maarten Sap, and presumably one more person not explicitly named in the text. However, based on the information provided, the explicit number of named collaborators is 5. If we count Jenny as an author, that makes it 6.</sample>
    <sample id="244">The background knowledge needed is that "Judges decide cases in law courts."</sample>
    <sample id="245">This study presents a two-step pipeline for identifying high-agreement workers on Amazon Mechanical Turk (MTurk) for summarization tasks, aiming to improve annotation quality while reducing costs and resource waste. The pipeline includes pre-task qualification settings, such as location, task completion rate, and number of HITs, followed by a Qualification Task and an Endurance Task. The Qualification Task evaluates annotators' ability to assess summaries across six dimensions, categorizing them into gold, silver, bronze, and blocked groups. Only gold and silver workers proceed to the Endurance Task, which tests their capacity to handle a heavier workload. The final pipeline results in 12 workers (4 gold, 8 silver), achieving higher inter-annotator agreement (IAA) than baseline methods and CloudResearch workers. In reference-based tasks, the pipeline workers demonstrated a Krippendorff’s Alpha of 0.534, outperforming the MACE-filtered baseline (0.380) and CloudResearch (0.513). Analysis of correctness across annotation sources shows that pipeline workers correlate well with expert judgments and real GPT models. However, the pipeline does not guarantee correctness training. The study concludes that the proposed method offers a scalable, cost-effective approach for recruiting high-agreement annotators, with potential for broader applications across tasks, languages, and platforms. Limitations include a focus on English summarization and the lack of a guaranteed correctness training mechanism.</sample>
    <sample id="246">Yes, the code is available on GitHub.</sample>
    <sample id="247">This paper introduces **FACTKG**, a novel dataset for **Knowledge Graph-Based Fact Verification**, addressing a gap in existing fact verification datasets that primarily rely on text or tables as evidence. Unlike prior works, FACTKG leverages **DBpedia**, a structured knowledge graph, to verify natural language claims. The dataset includes claims in both **written** and **colloquial** styles for broader practical applicability and is labeled as either **SUPPORTED** or **REFUTED**. The verification task involves retrieving relevant triples from the knowledge graph and reasoning using five types of logical inference: **one-hop**, **conjunction**, **existence**, **multi-hop**, and **negation**. To enhance practicality, colloquial claims were generated using a **style transfer model** and **presupposition templates**. The paper also evaluates baseline models, including **claim-only** approaches and a **GEAR model** that utilizes graph evidence. Results show that the GEAR model significantly outperforms other baselines, demonstrating the effectiveness of incorporating knowledge graph evidence in fact verification. This work opens new avenues for research in fact verification, particularly in tasks requiring consistency checks between natural language and structured knowledge sources, such as dialogue systems and information retrieval. The dataset is publicly available for further research.</sample>
    <sample id="248">No, the annotators for NLPositionality are not balanced in regard to each demographic. The study recruited over 1000 annotators from 87 countries, but it does not indicate that the distribution across demographics like country, gender, etc., was balanced. The focus was on gathering a diverse set of annotations rather than ensuring equal representation across all demographics.</sample>
    <sample id="249">Sentences in the acceptable domain were perturbed by preserving the relevant structure while adding noise to the input.</sample>
    <sample id="250">A dimensional evaluation means assessing different aspects or dimensions of chat quality, such as relevance, coherence, empathy, and factual accuracy, to gain a detailed understanding of a model's strengths and weaknesses, rather than just providing a general overall rating.</sample>
    <sample id="251">The authors of the paper are affiliated with the University of Science and Technology of China.</sample>
    <sample id="252">**Abstract:**  
This paper introduces *U-CREAT*, an unsupervised prior case retrieval (PCR) pipeline, and the *IL-PCR* dataset, a new benchmark for PCR tasks in the legal domain. With the growing volume of legal documents, retrieving relevant and cited past cases becomes increasingly challenging. The *IL-PCR* dataset, comprising 7,070 Indian legal cases with an average of 6.775 citations per query, provides a comprehensive test bed for PCR algorithms. It outperforms the existing COLIEE’21 dataset in terms of document length, vocabulary size, and citation density. The *U-CREAT* pipeline employs an event-based approach using unsupervised learning techniques, achieving high retrieval efficiency and generalization across legal systems without domain-specific tuning. The method extracts events from legal documents using dependency parsing to form subject-verb-object triplets, which are then used to compute an interaction matrix for ranking candidate cases. Experimental results show that event-based models significantly outperform baseline and transformer-based approaches, with the *Event Filtered Documents* model achieving the highest performance. U-CREAT also demonstrates superior results on the COLIEE’21 dataset, outperforming recent supervised methods. This work advances the field of legal case retrieval by offering a robust, unsupervised framework and a new benchmark, paving the way for further research in legal AI.</sample>
    <sample id="253">**Abstract:**  
In this work, we introduce *DisorBERT*, a double domain adaptation model designed to detect signs of mental disorders in social media content. Mental disorders, such as depression and anxiety, often manifest through online interactions, making social media a valuable source for early detection. However, the limited availability of annotated data in specific domains poses a challenge. To address this, *DisorBERT* leverages domain adaptation techniques, transferring knowledge from a general language model (BERT) to specialized domains like Reddit and mental health. The model incorporates guided masking, which enhances the focus on domain-relevant words during training. Experimental results on the eRisk dataset demonstrate that *DisorBERT* achieves a better balance between precision and recall compared to baseline models. Additionally, analysis of generated text and attention mechanisms reveals that *DisorBERT* produces more psychologically relevant responses, such as words associated with mental health symptoms, compared to BERT. Visualization techniques further highlight the model's ability to identify key terms related to depression, such as "anxious" and "medication." The proposed approach outperforms existing models like MentalBERT, showing effectiveness in capturing mental health signals from social media. Future work includes exploring additional lexical resources and clinical data integration to further enhance detection capabilities.</sample>
    <sample id="254">**Abstract:**  
This paper presents an uncertainty-guided label denoising framework for document-level distant supervision (DS) relation extraction (DocRE). Traditional DS-based DocRE models suffer from noisy pseudo labels, leading to performance degradation and false positive relations. To address this challenge, our approach introduces uncertainty estimation to filter unreliable pseudo labels and improve label quality. We first train a pre-denoising DocRE model using both DS and human-annotated data to generate pseudo labels. Then, we employ Monte Carlo dropout to estimate model uncertainty, enabling the identification of unreliable predictions. To handle overlapping relations, we propose an instance-level uncertainty estimation method that assigns uncertainty scores to each relation class. We further introduce dynamic class uncertainty thresholds to filter out pseudo labels with high uncertainty, particularly benefiting long-tail relations. A multi-phase training strategy is designed to iteratively re-label DS data, enhancing model performance through continuous refinement. Experimental results on public datasets demonstrate that our framework significantly outperforms existing baselines, achieving state-of-the-art performance. The key contributions include an uncertainty-guided label denoising framework, instance-level uncertainty estimation for overlapping relations, dynamic class uncertainty thresholds for long-tail problems, and substantial performance improvements in document-level relation extraction.</sample>
    <sample id="255">The form of the prompting is important in cases of zero and one-shot prompting. For five-shot prompting, the actual form has nearly no influence, as the examples carry most of the weight.</sample>
    <sample id="257">The authors evaluated four state-of-the-art chat models.</sample>
    <sample id="258">This video presents a study titled "Can Large Language Models Be an Alternative to Human Evaluation?" The research explores the feasibility of using large language models (LLMs) to evaluate the quality of text in natural language processing tasks, such as assessing generated stories. Traditionally, human evaluation has been the standard method, but it is often unstable and difficult to reproduce. The authors propose using LLMs as an alternative by providing them with clear instructions and samples to rate based on criteria like grammar, coherence, likability, and relevance. The study tested this approach using four LLMs—T0, InstructGPT (curie and davinci), and ChatGPT—by having them evaluate stories generated by GPT-2 and human writers. Human evaluators, specifically English teachers, were used as a benchmark for ground-truth ratings. The results showed that while some smaller models did not distinguish well between human and machine-generated texts, larger models like Davinci and ChatGPT demonstrated preferences similar to human raters, favoring human-written stories. This suggests that certain LLMs can effectively replace human evaluators in specific tasks. The paper also addresses various factors affecting LLM evaluation, such as instruction wording and response sampling, as well as the benefits and costs compared to human evaluation. The findings indicate that LLM evaluation is a promising and viable alternative in natural language processing.</sample>
    <sample id="259">**Abstract**  
This paper introduces XSemPLR, a comprehensive benchmark for cross-lingual semantic parsing, supporting multiple natural languages and meaning representations such as SQL and Lambda Calculus. Current models are limited in their language and representation coverage, often neglecting languages like Chinese and meaning representations like Lambda Calculus. XSemPLR addresses this by providing a unified dataset encompassing 9 datasets, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages from 15 language families. The benchmark is evaluated under six settings, including Translate-Test, Monolingual, Multilingual, and Cross-lingual Zero-shot and Few-shot transfer. The study compares various multilingual models, including Encoder-PTR and Encoder-Decoder architectures, such as mT5 and mBART. Results show that Encoder-Decoder models achieve the best performance across all datasets. Training with a multilingual mixture improves performance for most languages, though English performance occasionally declines, highlighting the "Curse of Multilinguality." Cross-lingual transfer gaps are significant in zero-shot settings but are reduced with few-shot training. The findings also reveal that pretraining on English enhances few-shot performance in other languages, while models like Codex and BLOOM remain insufficient for cross-lingual semantic parsing. XSemPLR offers a valuable resource for advancing research in multilingual semantic understanding and representation.</sample>
    <sample id="260">The information provided does not specify the number of authors involved in the paper.</sample>
    <sample id="261">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="262">The information provided does not specify the number of authors involved in the paper.</sample>
    <sample id="263">**Abstract:**  
In-context learning (ICL) in large language models (LLMs) is known to be unstable due to label biases introduced by various design choices, such as the selection and order of in-context examples. This work systematically investigates label biases in ICL, identifying three types: vanilla-label bias (model preference for label names), context-label bias (bias from the context examples), and a newly discovered domain-label bias (bias arising from the task corpus). Experiments show that domain-label bias significantly affects model predictions, especially when random in-domain words are present. On tasks with high domain-label bias, even advanced calibration methods fail to improve performance beyond chance. To address these biases, we propose domain-context calibration, a novel method that uses random in-domain words as content-free text to estimate and correct label biases. This approach outperforms prior methods that rely on predefined tokens like "not available," particularly in tasks with significant domain-label bias. Across multiple datasets and models, including GPT-3, domain-context calibration improves ICL performance, especially for high-bias tasks, by refining decision boundaries and reducing bias effects. Our findings highlight the importance of considering domain-specific factors in calibration strategies and provide a systematic framework for mitigating label biases in ICL. This work contributes to a better understanding of ICL instability and offers practical improvements for real-world applications.</sample>
    <sample id="264">This paper introduces TAVT, a novel framework for **Transferable Audio-Visual Text Generation**, aiming to address the challenges of domain shifts in multimodal text generation tasks. While uni-modal text generation has advanced significantly through large-scale pre-training, multimodal tasks suffer from high annotation costs and performance degradation across different domains. TAVT proposes a unified audio semantic space to align visual concepts across varying domains, leveraging the observation that audio semantics remain more stable than visual content under domain shifts. The framework consists of three key components: an **audio-visual meta-mapper network**, a **transformer-based encoder and generator**, and **Dual Counterfactual Contrastive Learning (DCLL)**. The meta-mapper maps visual concepts into a unified audio semantic space using learnable visual prefixes and audio clustering. The generator incorporates modality-aware attention weights to balance contributions from audio and visual inputs. DCLL introduces counterfactual supervision signals to directly optimize visual-text alignment without relying on negative samples. The model is trained using a meta-learning approach, enabling fast adaptation to new domains with limited labeled data. Extensive experiments on MSVD and MSR-VTT benchmarks demonstrate that TAVT significantly outperforms existing methods, especially in low-resource domains, showcasing its effectiveness in cross-dataset and cross-domain settings.</sample>
    <sample id="265">The name of the speaker is Vasudha.</sample>
    <sample id="266">The affiliations of the authors of the paper are not provided in the given content.</sample>
    <sample id="268">The most common errors of PaLM are omission errors, where parts of the source sentence are dropped in the translation.</sample>
    <sample id="270">The authors of the paper are affiliated with the Emory NLP Lab, led by Professor Jinho Choi at Emory University, and they collaborated with Amazon Alexa AI.</sample>
    <sample id="271">CFT stands for "Fine-tuning" in this paper.</sample>
    <sample id="272">The paper involves 7 authors.</sample>
    <sample id="274">The name of the speaker is Yusen Zhang.</sample>
    <sample id="276">**Abstract:**  
This paper introduces *IndicMT Eval*, a dataset designed for meta-evaluating machine translation (MT) metrics for Indian languages, addressing the lack of research on evaluation metrics for non-English translations. The study focuses on five Indian languages—Tamil, Malayalam (Dravidian), and Hindi, Marathi, Gujarati (Indo-Aryan)—selected from the Flores dataset. For each of 200 source sentences, seven translation models generate 1,400 candidate translations, resulting in 7,000 samples. These are annotated by bilingual experts using the MQM framework, capturing error types, severity, and overall scores. Analysis reveals that overlap-based metrics like chrF have high correlation with human scores but perform poorly overall. Embedding-based metrics, particularly BERTscore with multilingual embeddings, show better performance, with MuRIL and COMET variants achieving the highest correlations. However, many metrics exhibit narrow score distributions, limiting their interpretability. Fine-tuning COMET on the MQM dataset yields *IndicCOMET*, which outperforms COMET baselines on most languages and demonstrates strong zero-shot generalization. Evaluation on the ACES Translation Accuracy Challenge further validates its robustness. This work provides a valuable resource for improving MT evaluation in Indian languages and highlights the need for language-specific metric adaptation. The dataset is publicly available for future research.</sample>
    <sample id="277">The new method is called "Multiset Tagging and Latent Permutations."</sample>
    <sample id="278">The author described the "marked words" method as a technique to identify words that distinguish marked groups (those that differ from the unmarked default) from unmarked ones, using weighted log-odds ratios to find the top distinguishing words for each marked group. This method draws on the sociolinguistic concept of "markedness," where dominant, unmarked groups are linguistically and socially default, while marginalized, marked groups are distinguished by specific terms that highlight their difference.</sample>
    <sample id="279">The author is a PhD student at the University of Washington.</sample>
    <sample id="280">In this work, we present MultiEMO, an attention-based correlation-aware multimodal fusion framework designed for Emotion Recognition in Conversations (ERC). The goal of ERC is to predict the emotion label of each utterance in a dialogue, leveraging textual, audio, and visual modalities. Existing methods often fail to fully exploit the complementarity of multimodal information, perform poorly on minority emotion classes, and struggle with semantically similar emotions. To address these challenges, MultiEMO introduces four key components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification. A novel visual feature extractor, VisExtNet, is proposed to capture facial expressions without redundant scene-related information. For multimodal fusion, we design MultiAttn, a network that uses bidirectional multi-head cross-attention layers to integrate complementary information across modalities. Additionally, we introduce a Sample-Weighted Focal Contrastive Loss to enhance classification performance on minority and semantically similar emotions. Extensive experiments on MELD and IEMOCAP datasets demonstrate that MultiEMO achieves state-of-the-art results, particularly in challenging emotion categories. Despite its strengths, MultiEMO has limitations, such as not distinguishing between speakers and irrelevant individuals in visual data and requiring large batch sizes for optimal performance. Overall, MultiEMO provides a robust and effective approach for ERC by addressing key challenges in multimodal fusion and emotion classification.</sample>
    <sample id="281">This work investigates when translation requires context and how well machine translation models handle such cases across multiple languages. The study introduces Pointwise CXMI, an extension of the CXMI metric, to measure context dependence at the word and sentence levels. By analyzing TED talk transcripts translated into 14 languages, the authors identify patterns in context-dependent translation, such as dual pronouns in Arabic, formality in Chinese, and ellipsis resolution. These findings are used to develop the MuDA tagger, a multilingual discourse-aware tool that identifies context-dependent words in parallel corpora. The MuDA benchmark is then used to evaluate document-level translation models. Results show that context-aware models outperform context-agnostic ones for certain discourse phenomena like formality and lexical cohesion, but not for others like ellipsis and pronouns. Additionally, the benchmark reveals that commercial systems like DeepL perform better than Google Translate in document-level translation. Overall, the study highlights the importance of context in translation and provides a new evaluation framework to assess models’ ability to handle context-dependent phenomena across languages.</sample>
    <sample id="282">This paper introduces **StoryTrans**, a novel approach to non-parallel story-level author-style transfer in natural language generation. Unlike prior work that focuses on token- or sentence-level style transfer, StoryTrans operates at the **discourse level**, capturing higher-level author linguistic preferences such as narrative structures and styles. The key challenges addressed include preserving content while transferring style and handling the complexity of long texts with topic-specific stylistic features. To overcome these, StoryTrans learns **discourse representations** from source texts and combines them with **learnable style embeddings** to generate text in the target style. The model employs a two-stage training framework: the first stage focuses on disentangling style and content using self-reconstruction, disentanglement, sentence order, and style classifier losses; the second stage reconstructs the full text by incorporating masked style-specific keywords. Extensive experiments on newly collected Chinese and English datasets demonstrate that StoryTrans outperforms strong baselines in **style control** and **content preservation**, confirmed by both automatic and manual evaluations. Style visualization further supports the model's ability to align generated texts with target styles in the feature space. StoryTrans effectively enriches storylines with relevant content and maintains semantic coherence, showcasing its effectiveness in author-style transfer at the story level.</sample>
    <sample id="283">The name of the first mentioned symmetrical dependency structure is the Prague approach.</sample>
    <sample id="284">This paper introduces FSUIE, a novel framework for Universal Information Extraction (UIE) that addresses limitations in current span-based models by incorporating a fuzzy span mechanism. Traditional UIE models heavily rely on precise span boundary annotations, which can be ambiguous due to varying reasonable annotations. To resolve this, FSUIE introduces a fuzzy span loss that models target boundaries as continuous probability distributions rather than fixed positions, enhancing robustness. Additionally, it proposes an adaptive fuzzy span attention mechanism to align with the span's limited length assumption, improving the attention distribution used for span extraction. This mechanism dynamically adjusts attention ranges and applies linear decay at boundaries, improving model flexibility and performance. The FSUIE architecture integrates these components at the top level without affecting text encoding. Experimental results across named entity recognition, relationship extraction, and aspect sentiment triplet extraction demonstrate significant improvements over existing models, achieving state-of-the-art performance on multiple benchmark datasets. Ablation studies confirm the effectiveness of both fuzzy span loss and attention mechanisms in improving convergence and generalization. FSUIE's unified structure and domain adaptability make it a powerful solution for diverse information extraction tasks.</sample>
    <sample id="285">This paper presents "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework," which addresses the issue of factual errors in dialogue summaries. The study identifies two main approaches to addressing factual errors: integrating factuality objectives during model training or using an independent Factual Error Correction (FEC) model. However, the paper argues that existing FEC evaluation methods, such as FactCC and DAE, are flawed due to their vague overall scoring and failure to distinguish between error correction and summary generation. To address these issues, the authors propose a fine-grained evaluation framework based on ERRANT, incorporating manually annotated reference corrections. This framework enables more accurate assessment of FEC models by classifying errors into content-based and form-based categories. The study also introduces a new taxonomy for factual errors and evaluates FEC models using different training strategies. Key findings include the effectiveness of training FEC models with human-corrected summaries, the benefits of combining human and synthetic data, and the limitations of current FEC models in handling certain types of errors, such as additions and attribute errors. The work highlights the importance of improving evaluation methods to better align FEC models with their intended purpose of correcting factual errors in dialogue summaries.</sample>
    <sample id="286">The name of the speaker is James Finch.</sample>
    <sample id="287">The paper involves 4 authors: Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="288">Datasets such as BLiMP and SyntaxGym can be used to test syntactic phenomena.</sample>
    <sample id="290">The text does not explicitly mention the abbreviations of five methods for the first research question. It refers to "recent WSL methods" and specifically names "COSINE" and "FTw" (likely referring to a fine-tuning approach), but does not provide five specific abbreviations.</sample>
    <sample id="291">The model is evaluated on 11 biomedical and clinical downstream tasks in French, including named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="294">CamemBERT is initially trained on the OSCAR dataset, which is a large multilingual corpus derived from the Common Crawl web crawl.</sample>
    <sample id="295">The name of the speaker is Adam Przepiórkowski.</sample>
    <sample id="296">This work presents a collaborative effort between the University of Turin and Amazon Alexa to explore the challenges of natural language understanding, particularly in detecting irony, through a more nuanced and perspective-aware approach. Traditional natural language processing relies on annotated datasets that assume a single ground truth, but irony, being a latent and pragmatic phenomenon, presents significant challenges. To address this, the researchers developed the EPIC (English Perspectivist Irony Corpus), a dataset consisting of 300 short conversations collected from social media platforms such as Reddit and Twitter over a 1.5-year period, across five varieties of English. The data was annotated by 74 participants via the Prolific platform, with each conversation receiving five annotations. Analysis revealed variability in inter-annotator agreement based on factors such as gender, age, and nationality. The study introduced "perspective-aware" models, trained on subsets of data annotated by different groups, which demonstrated increased confidence in their predictions compared to models trained on aggregated "gold standard" annotations. Additionally, the research identified that younger generations and annotators from the UK and Ireland showed the highest disagreement in irony perception. These findings suggest that irony detection should consider the influence of annotator demographics and perspectives, rather than assuming a single objective truth. The results highlight the importance of incorporating diverse viewpoints in training models to improve the accuracy and robustness of natural language understanding systems.</sample>
    <sample id="297">This paper, "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models," explores the use of dogwhistles—coded language that conveys a dual message to different audiences—in political rhetoric and their implications for NLP and content moderation. The study identifies over 340 dogwhistle terms, primarily targeting racial, transphobic, and anti-Semitic groups, and categorizes them based on register, persona, and type. A case study of historical U.S. political speeches reveals a strong correlation between the use of racial dogwhistles and the Republican Southern Strategy, highlighting their role in covertly signaling prejudice while maintaining political plausibility. The research also evaluates language models, particularly GPT-3, in detecting and interpreting dogwhistles, finding that models perform better with formal terms but struggle with informal, social media-based language. Additionally, the study demonstrates how dogwhistles can evade content moderation systems by analyzing toxicity detection with tools like Prospective API, showing that replacing explicit slurs with dogwhistles results in lower toxicity scores. Overall, the work underscores the importance of understanding context-dependent, coded language in political discourse and its challenges for automated detection systems.</sample>
    <sample id="298">The findings that retraining or continuing to pre-train models with more recent data showed performance degradation with larger temporal gaps led to the conclusion that temporal drift is the main cause of performance loss.</sample>
    <sample id="299">This work introduces a minimax training approach to improve the robustness of Natural Language Inference (NLI) models against reliance on spurious correlations, or shortcuts, commonly found in training data. While NLI models achieve high performance on standard benchmarks, they often fail on out-of-distribution adversarial examples due to over-reliance on these shortcuts. Existing shortcut mitigation methods typically require prior knowledge of the shortcuts and use auxiliary models, which may not align with the learner’s behavior and introduce additional computational overhead. To address these limitations, the proposed method employs a minimax training framework where a learner model minimizes task loss, while an auxiliary model maximizes the learner’s loss by generating example weights that emphasize under-represented, hard examples. These hard examples, which are often ignored in standard training, are crucial for improving generalization on out-of-distribution data. The auxiliary model is modeled as a feed-forward network and optimized alternately with the learner using standard optimization techniques. Experiments on datasets such as MNLI, FEVER, and QQP, alongside adversarial test sets, demonstrate that the minimax approach consistently improves out-of-distribution performance without sacrificing in-distribution accuracy. The method is dataset-agnostic, does not assume prior knowledge of shortcuts, and is computationally efficient. The study also explores the generalization of the approach to larger models and synthetic shortcuts, showing promising transferability.</sample>
    <sample id="300">This paper introduces the concept of *interactive dictation*, a novel task that enables users to both dictate and edit text using natural language voice commands without predefined trigger words. Unlike traditional speech-to-text systems, which primarily support dictation, interactive dictation allows users to dynamically interleave dictation with edits, such as replacing text or correcting speech errors, using intuitive and open-ended commands. The task is formalized as a four-step process: automatic speech recognition (ASR), segmentation of utterances into dictation and commands, normalization of commands, and execution of actions to update the document state. To support this task, the authors developed a custom data collection interface and built a corresponding dataset. A baseline system was also created, which includes separate models for segmentation, ASR repair, and interpretation. The system was evaluated using exact match accuracy between predicted and target end states, revealing a trade-off between runtime and accuracy. While GPT-3 models achieved higher accuracy, they were slower, whereas T5 models offered a better balance between efficiency and accuracy when predicting intermediate programs. The paper concludes by releasing the code and dataset to encourage further research in this emerging area of natural language processing and human-computer interaction.</sample>
    <sample id="302">It is necessary to permute the tokens for the output sequence because, after tagging each input token with an unordered multiset of output tokens, the tokens are not in the correct order. Permuting them ensures the output sequence is structured correctly according to the desired logical form.</sample>
    <sample id="303">The authors recommended increased transparency about bias mitigation methods because without it, it is difficult to determine whether positive stereotypes and essentializing narratives in language models arise from excessive value alignment or other anti-stereotyping methods, making it hard to study and address these issues effectively.</sample>
    <sample id="304">Minimal-pair unacceptable inputs are sentences that are considered ungrammatical or unacceptable within a specific linguistic context, used in comparison to their acceptable counterparts to evaluate language models' acceptability judgments.</sample>
    <sample id="305">**Abstract:**  
In this work, we critically examine the assumptions and practical implications of weakly supervised learning (WSL), where models are trained using noisy, weakly labeled data instead of manual annotations. While recent WSL methods claim to achieve high performance on clean test sets using only weak labels, we reveal a critical dependency on clean validation data for model selection, which has been largely overlooked. Our experiments demonstrate that without clean validation samples, WSL methods fail to generalize, highlighting the necessity of manual annotations despite their cost. Furthermore, we find that increasing the number of clean validation samples improves WSL performance, but direct fine-tuning on these samples outperforms WSL approaches even with limited clean data. This suggests that the performance gains attributed to WSL can be easily achieved through simple fine-tuning, making complex WSL methods less practical. We recommend that future WSL research report model selection criteria, compare against few-shot learning baselines, and consider continuous fine-tuning as a strong alternative. Our findings challenge the current perception of WSL and emphasize the importance of clean data in achieving reliable results. The code for our experiments is publicly available.</sample>
    <sample id="306">This paper investigates the ability of large language models (LLMs) to track entities and their states across a discourse, a crucial skill for understanding complex narratives or instructions. The authors, Sebastian Schuster and Najoung Kim, present a novel task designed to evaluate this ability by simulating scenarios involving boxes and objects. The task requires models to predict the final state of each box after a sequence of operations, such as moving or adding objects. To ensure that models are not relying on shortcuts like copying initial states or exploiting lexical heuristics, the task was carefully designed. Experimental results with models such as Flan-T5 and GPT-3/3.5 show that most models fail to track entity states beyond simple repetition of initial conditions. However, the text-davinci-003 model demonstrates non-trivial tracking capabilities. Further analysis reveals that models pre-trained on substantial code data, such as GPT-3.5, are more likely to exhibit meaningful entity tracking, suggesting that exposure to code during pre-training may enhance this ability. While fine-tuning can enable smaller models like T5-base to perform the task, randomly initialized models of the same architecture cannot, emphasizing the importance of pre-training. The study highlights the need for further research to determine whether these tracking abilities generalize beyond the specific task setup. The full analysis and additional experiments, including those with GPT-4, are available in the paper.</sample>
    <sample id="307">The authors evaluated their models using downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. However, specific evaluation metrics (e.g., accuracy, F1 score, etc.) were not explicitly mentioned in the provided content.</sample>
    <sample id="308">This presentation introduces **NLPositionality**, a framework for analyzing the **positionality** of NLP datasets and models—systematic biases that reflect the perspectives and experiences of the people who create or annotate them. The work highlights how design biases can lead to unequal performance across different populations, as illustrated by examples such as the Prospective API's reduced sensitivity to offensive language in Indian contexts. Positionality, rooted in critical theory, refers to how researchers' identities and backgrounds influence their work. While datasets and models themselves do not possess identities, they reflect the aggregated judgments of human annotators, potentially privileging certain viewpoints over others. To study this, the authors re-annotate data using a diverse global sample of over 1,000 participants from 87 countries, comparing their judgments with existing models and datasets. The results reveal that NLP systems tend to align more closely with English-speaking populations and individuals with higher education levels, while underrepresenting non-binary individuals and those from non-English backgrounds. The study concludes with recommendations, including maintaining transparent design records, adopting a perspectivist approach in research, and developing community-specific datasets and models to promote inclusivity in NLP. The findings underscore the importance of recognizing and addressing positionality in the development of fair and equitable AI systems.</sample>
    <sample id="309">The metric used for measuring inter-annotator agreement was not explicitly named in the content provided, but it is typically measured using metrics such as Cohen's Kappa or Fleiss' Kappa. However, since the question asks for the metric used based on the given content and it is not specified, the answer is that the specific metric is not mentioned.</sample>
    <sample id="310">The domain chosen to add completely unrelated sentences to the unacceptable and acceptable queries was Wikipedia.</sample>
    <sample id="311">The affiliations of the authors of the paper are not mentioned in the provided content.</sample>
    <sample id="312">MultiInstruct differs from other benchmarks by being the first multi-modal instruction tuning benchmark dataset, consisting of 62 diverse multi-modal tasks across 10 categories, each with five expert-written instructions, addressing the lack of large-scale publicly available multi-modal instruction tasks.</sample>
    <sample id="313">The content does not specify the number of authors involved in the paper.</sample>
    <sample id="314">Binary coordination refers to a syntactic construction involving two conjuncts joined by a coordinating conjunction, such as "and" or "or," forming a coordinated structure.</sample>
    <sample id="315">The duration of the prompts used in the study is not specified in the provided content.</sample>
    <sample id="316">The findings imply that smaller, specialized models like T5, when fine-tuned on the CoScript dataset, can achieve higher quality script generation for constrained language planning compared to most large language models, suggesting that proper training on suitable datasets can enable smaller models to surpass larger ones in specific tasks.</sample>
    <sample id="317">This paper introduces **CodeIE**, a novel approach that leverages large code generation models to enhance few-shot information extraction (IE) tasks. Traditional IE models, such as T5 and GPT-3, often struggle with the mismatch between structured outputs during inference and unstructured text during pre-training. To address this, CodeIE transforms the IE task into a **structure-to-structure code generation** problem, using code language models like Codex. By encoding input text into structured formats and prompting the model to generate structured outputs in code, CodeIE aligns input and output representations more effectively. The method employs carefully designed code-style prompts for both named entity recognition (NER) and relation extraction (RE), enabling models to generate structured results with minimal training data. Experimental evaluations on three NER and four RE datasets show that CodeIE significantly outperforms traditional text-based models like UIE and GPT-3, especially in few-shot settings. Analysis reveals that code-format prompts reduce structural errors and improve recall, while code-pretrained models like Codex demonstrate superior performance over natural language models. Overall, this work highlights the potential of code generation models in improving few-shot IE tasks through better alignment of input-output structures.</sample>
    <sample id="319">The work investigates several learning strategies, including from-scratch pre-training, continual pre-training using existing models (CamemBERT and PubMedBERT), and comparisons of different data sources and sizes (NACHOS and clinical notes) to determine their impact on model performance.</sample>
    <sample id="320">The factor of overfitting due to test reuse is not observed, as the gradient of the best fit line was greater than one, indicating no diminishing returns.</sample>
    <sample id="321">The quality of the simplification was evaluated by using the DEPLAIN corpus as a gold standard to assess automatic alignment methods and by fine-tuning language models to produce simplified text, then comparing their performance against baseline scores.</sample>
    <sample id="322">This paper explores what text classifiers learn about morality by applying explainable AI techniques to language models trained on moral content. While traditional approaches to morality in NLP often reduce it to a binary scale of moral vs. immoral, the authors argue that morality is inherently pluralistic and subjective, as reflected in the Moral Foundations Theory. This theory posits five distinct moral foundations—such as fairness and authority—that individuals prioritize differently, shaping their moral judgments. The study investigates how language models perceive and represent morality across different domains using the Moral Foundation Twitter Corpus, a dataset of 35,000 tweets from seven domains, including #AllLivesMatter and #BlackLivesMatter. The analysis reveals that language models can detect nuanced differences in how morality is expressed, such as varying attitudes toward subversion of authority between domains. For instance, subversion is viewed negatively in #AllLivesMatter but more positively in #BlackLivesMatter. The findings caution against using a single model across diverse domains, as it may lead to misinterpretations of moral content. The research highlights the importance of domain-specific moral understanding in language models and underscores the complexity of morality as a multi-dimensional concept.</sample>
    <sample id="323">**Abstract**  
This paper introduces DHLK, a novel framework for Commonsense QA that integrates language models and knowledge representation learning to enhance reasoning with heterogeneous knowledge graphs (HKGs). Commonsense QA requires systems to leverage external knowledge, but existing methods often suffer from noisy entity retrieval and limited interaction between text and graph modalities. To address these issues, DHLK constructs an optimized HKG using a two-stage pruning strategy and knowledge representation learning (KRL) to filter irrelevant entities and enrich the graph with paraphrased entities from WordNet and Wiktionary. The framework employs RoBERTa and Mask Self-Attention to encode and fuse QA contexts with graph entities, dynamically pruning weakly relevant entities based on attention weights. Entity and relation embeddings are optimized using TransE, and Relation Mask Self-Attention (RMSA) is introduced to model the subgraph, inspired by RGAT. The final graph embedding is obtained via max-pooling over key entities, which is then combined with path-enhanced QA context embeddings in an MLP for answer prediction. Experiments on CommonsenseQA and OpenBookQA using ConceptNet, WordNet, and Wiktionary demonstrate that DHLK outperforms existing LM and HKG-based methods, achieving competitive performance on standard benchmarks.</sample>
    <sample id="324">Yes, language models have different political biases, as demonstrated by their varying leanings across the political spectrum and their performance on tasks like hate speech and fake news detection.</sample>
    <sample id="326">Cognitive dissonance is the mental discomfort experienced by a person who holds two or more contradictory beliefs, values, or ideas at the same time, or when they perform an action that contradicts their beliefs or values. It is often resolved by changing one's beliefs or justifying the behavior.</sample>
    <sample id="327">This paper introduces **ManagerTower**, a novel vision-language (VL) representation learning architecture that enhances cross-modal understanding by effectively aggregating insights from pre-trained unimodal experts at different levels. Building upon BridgeTower, which connects unimodal layers with cross-modal layers in a layer-by-layer fashion, ManagerTower addresses its limitations by introducing **manager modules** in each cross-modal layer. These managers adaptively combine semantic knowledge from multiple unimodal representations, enabling more comprehensive cross-modal alignment and fusion. The architecture is flexible, allowing any visual, textual, or cross-modal encoder to be used. Pre-trained on only 4 million image-text pairs, ManagerTower achieves state-of-the-art performance on various downstream tasks, including a 39.15% accuracy on the Wikivideo test set. Notably, it outperforms other models with similar or larger training scales. Visualization of aggregation weights reveals that adaptive managers significantly differ from static ones, showing diverse and effective exploitation of unimodal semantic knowledge across layers. This work demonstrates the importance of adaptive aggregation in improving cross-modal representation learning. The code, paper, and models are publicly available for further research and application.</sample>
    <sample id="328">GPT-4 is the most liberal language model.</sample>
    <sample id="329">This paper presents a novel zero-shot video sentence localization method that generates structured pseudo-labels to train models without manual annotations. Traditional zero-shot approaches suffer from simplistic pseudo-queries, misalignment between pseudo-queries and events, and label noise. To address these issues, the proposed method generates complex pseudo-queries using an image-text pre-trained model (BLIP) and models event temporal structure to ensure high relevance within events and low relevance outside. A sliding window approach is used to select pseudo-events with the highest quality, based on the difference in similarity between video frames and queries inside and outside the event. To reduce label noise, the method employs sample re-weighting based on model confidence and IoU, and refines pseudo-labels using high-confidence predictions. Experiments on ActivityNet Captions and Charades-STA datasets show that the proposed method, referred to as SPL, achieves superior performance in zero-shot settings across multiple evaluation metrics, including R@M and mIoU. The approach demonstrates robustness to label noise and provides a scalable solution for training video sentence localization models without manual supervision. The code is publicly available.</sample>
    <sample id="330">Yes, cumulative training performs equal or better than iterative when doing active learning.</sample>
    <sample id="331">The name of the speaker is Sara Papi.</sample>
    <sample id="332">The data for the MuDA benchmark was taken from transcripts of TED talks that have been translated from English to 14 different languages.</sample>
    <sample id="333">This paper introduces **INK**, a novel training framework designed to enhance the generalization and performance of Neural Machine Translation (NMT) models by injecting kNN knowledge into the model's representation space. Traditional NMT models often suffer from a non-smooth and sparse representation space, particularly for low-frequency tokens, leading to poor generalization. While kNN-MT addresses this by using a key-value datastore to refine predictions during decoding, it suffers from high computational costs and inflexible representations. To overcome these limitations, INK proposes an iterative training loop that extracts kNN knowledge from the datastore to guide an adapter in adjusting the model's representations. This process involves aligning contextualized representations with token embeddings, kNN token embeddings, and same-target token representations using KL-divergence. The updated representations are then used to asynchronously refresh the datastore. After convergence, the datastore can be discarded, enabling efficient inference with minimal memory usage. Experiments on the WMT’19 German-English task show that INK outperforms state-of-the-art kNN-MT systems, achieving improvements of 1.99 COMET and 1.0 BLEU scores. The framework demonstrates that combining an adapter with kNN knowledge effectively smooths the representation space, leading to better translation quality with faster inference and reduced memory consumption.</sample>
    <sample id="335">The name of the speaker is Matthias Lindemann.</sample>
    <sample id="336">Cross-lingual transfer is the process of training a model on one source language and applying it to another target language without additional training on the target language, often to evaluate how well the model can generalize across different languages.</sample>
    <sample id="337">This paper introduces a novel approach for learning context-free out-of-vocabulary (OOV) word embeddings using a graph-based relation mining framework. Traditional embedding models struggle with OOV words, which are critical for downstream tasks but often lack representation. To address this, the proposed method leverages word formation and association, inspired by human learning patterns. It constructs a Word Relationship Graph that captures lexical relationships by tokenizing OOV words into wordpieces and linking them to relevant words, forming a two-level graph structure. Nodes represent words or wordpieces, with their embeddings as attributes. A self-attention mechanism assigns attributes to OOV nodes based on their characters, while a two-level Graph Attention Network extracts meaningful representations by fusing node-level and graph-level information. A readout block captures the overall graph structure, and contrastive learning with NT-XENT loss aligns the OOV embeddings with the background embedding space. Experimental results demonstrate the model's superiority over baselines on both intrinsic and extrinsic tasks, enhancing performance in both static and contextual models. The approach is particularly suitable for agglutinative languages and shows promising results with English through proper word segmentation. The effectiveness of the model largely depends on the rationality of word decomposition, making it adaptable to various languages.</sample>
    <sample id="338">This paper investigates the effectiveness of human-generated natural language explanations in improving model performance and explores objective methods to evaluate their quality. The study addresses the challenge of assessing the utility of explanations, which can be subjective and task-dependent. The authors introduce a unified data structure that standardizes various tasks into a multiple-choice format, enabling consistent evaluation across different datasets. They conduct preliminary experiments to analyze the utility of explanations during fine-tuning and inference stages, revealing that explanations do not always convey new knowledge but can influence model behavior. Based on these insights, the paper proposes a novel evaluation metric called TREU, which extends the simulatability score by incorporating the impact of explanations during fine-tuning. TREU is evaluated on five datasets and two models (T5 and BART), demonstrating its superiority in capturing the quality and helpfulness of human explanations compared to traditional metrics. The findings highlight that the effectiveness of explanations depends on the task and format, such as negation in natural language inference and counterfactual reasoning in other tasks. The study emphasizes the importance of rigorous quality checks in human annotation processes and provides a foundation for future research in explanation evaluation.</sample>
    <sample id="339">The authors of the paper are affiliated with Saarland University in Germany.</sample>
    <sample id="340">This paper introduces *ParaAMR*, a large-scale, syntactically diverse paraphrase dataset created through AMR (Abstract Meaning Representation) back-translation. While existing paraphrase datasets such as MRPC and Quora offer high-quality data, they are limited in scale, and automatically generated datasets like back-translation lack syntactic diversity. To address this, the authors propose leveraging AMR graphs, which capture the semantic structure of sentences, to generate paraphrases with varied syntax. The method involves parsing a sentence into an AMR graph, modifying the focus (root node), and regenerating text from the altered graph. This process ensures semantic consistency while introducing syntactic variation. The resulting dataset contains approximately 15 million source sentences, with an average of 6.9 paraphrases per sentence. Evaluation shows that ParaAMR achieves comparable semantic similarity to other back-translation datasets but exhibits significantly higher syntactic diversity. The dataset is demonstrated to enhance several NLP tasks, including sentence embedding learning, syntactically controlled paraphrase generation, and few-shot learning through data augmentation. These results highlight ParaAMR’s potential to improve the robustness and versatility of paraphrase generation models. The dataset is publicly available for research purposes.</sample>
    <sample id="341">The authors use average lagging as the latency measure, and also consider computational-aware average lagging, which accounts for the model's computational times to predict the output.</sample>
    <sample id="342">This paper introduces **LiveChat**, a large-scale, personalized dialogue dataset automatically constructed from live streaming videos, aiming to address limitations in existing open-domain dialogue datasets. Unlike traditional text-based datasets, LiveChat is video-sourced, capturing more natural spoken conversations. It is designed to support both general open-domain and personalized dialogue research, as well as multi-party conversation scenarios, which are underrepresented in current datasets. The dataset is built in three steps: collecting live stream videos, transcribing audio into utterances, and constructing dialogues using a reply-to-whom matching method. Additionally, persona information is extracted using both manual labeling and rule-based classifiers to enable personalized dialogue generation. The paper evaluates LiveChat on two tasks—**Response Modeling** and **Addressee Recognition**—demonstrating that persona information and longer sessions improve performance. Experiments also show that BART outperforms other models on LiveChat, indicating its distinctiveness compared to existing datasets. Furthermore, large language models (LLMs) show strong performance in terms of informativeness, though their effectiveness slightly decreases with excessive demonstration samples due to noise. Overall, LiveChat offers a valuable resource for advancing research in personalized and multi-party dialogue systems, and future work will focus on efficient transfer learning for LLMs.</sample>
    <sample id="344">The drawbacks of tree-based methods include the need for trees, which are usually not given and require complicated and computationally expensive pre-processing or grammar-induction procedures.</sample>
    <sample id="345">This paper introduces a novel neural sequence-to-sequence model for semantic parsing that achieves compositional generalization without relying on syntactic trees. The model addresses the challenge of handling deeper and structurally unseen compositions during testing, which traditional seq2seq models often fail to generalize. Instead of using trees to capture compositional structure, the approach employs multiset tagging and latent permutations to model the relationship between input utterances and their corresponding logical forms. The model operates in two steps: first, it tags each input token with a multiset of output tokens, ensuring all necessary tokens are present, and second, it predicts a permutation of these tokens to produce the final output. This permutation prediction is flexible and unconstrained, allowing for a wide range of valid reordering strategies. The method uses a continuous relaxation to approximate the optimal permutation, enabling gradient-based training despite the NP-hard nature of the problem. Experimental results on the COGS benchmark demonstrate strong performance in generalizing to deeper recursion compared to other treeless models. While challenges remain in other types of structural generalization, the paper presents a promising approach to compositional generalization by directly modeling input-output correspondences through multiset tagging and latent permutations.</sample>
    <sample id="346">The affiliations of the authors of the paper are not mentioned in the provided content.</sample>
    <sample id="348">**Abstract:**  
In this paper, "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models," Myra, Esin Durmus, and Dan Jurafsky introduce a novel method to detect and analyze stereotypes in large language models (LLMs) by leveraging the models' ability to generate personas based on natural language prompts. Traditional methods of measuring bias in LLMs are limited by their reliance on manually curated datasets, narrow focus on specific stereotypes, and lack of intersectionality. To address these issues, the authors propose generating personas using prompts such as "Imagine you are an Asian woman. Describe yourself," which allows for generalizable and customizable analysis across diverse demographics. The method involves two components: persona generation and the identification of "marked words" that distinguish stereotyped groups from unmarked ones using log-odds ratios. Through this approach, the study reveals that generated personas often reflect essentializing and harmful stereotypes, such as linking Asian women with "delicate" and "petite" traits or Black women with "strong" and "resilient" labels, which can perpetuate harmful societal expectations. The authors argue that these patterns are not captured well by existing stereotype lexicons and highlight the need for increased transparency in bias mitigation strategies, intersectional analysis, and further research into positive stereotypes and their societal impacts. The work underscores the importance of addressing both overt and subtle forms of bias in LLMs to promote fairness and inclusivity.</sample>
    <sample id="350">In recent years, leaderboard-based evaluation has become the dominant approach in natural language understanding (NLU), often leading to claims of human-level or even superhuman performance by AI systems. This paper critically examines the validity of such claims by analyzing two widely used benchmarks, SuperGLUE and SQuAD. Despite systems outperforming humans in several tasks, the study reveals significant issues in how human performance is measured and compared. Key problems include the use of different test sets for systems and humans, errors in ground-truth answers, and insufficient motivation and transparency in human annotation processes. These factors contribute to an unfair and unreliable comparison between AI models and human performance. Additionally, the paper highlights that systems may exploit spurious patterns in data, while humans are not subject to such biases. The authors argue that current benchmarks do not accurately reflect the true capabilities of AI models or the variability in human performance. Consequently, claims of superhuman performance are not scientifically grounded. The paper concludes with recommendations for improving benchmark design to ensure more reliable and meaningful evaluations of NLU systems.</sample>
    <sample id="351">This paper investigates the generalization capabilities of Named Entity Recognition (NER) models trained on the CoNLL-2003 dataset in modern contexts. Despite being used for nearly two decades, the relevance of these models in 2023 remains uncertain. To address this, the authors created the CoNLL++ dataset, comprising news articles from Reuters (2020) annotated using CoNLL-2003 guidelines. Over 20 models were fine-tuned on CoNLL-2003 and evaluated on both CoNLL-2003 and CoNLL++ test sets to assess generalization. The study identifies three key factors for effective generalization: model architecture (transformers perform better), model size (larger models generalize better), and the number of fine-tuning examples. The analysis also explores two hypotheses for performance degradation: adaptive overfitting and temporal drift. Results show no evidence of adaptive overfitting, as improvements on CoNLL-2003 translated to greater gains on CoNLL++. However, temporal drift was confirmed as the main cause of performance drop, with larger temporal gaps between training and test data leading to worse performance. The findings suggest that modern NER models can still generalize well if designed appropriately, and the performance decline is not due to overfitting but rather to the increasing temporal gap. The study concludes that CoNLL-2003 taggers remain effective in 2023 with proper adaptation.</sample>
    <sample id="352">ABC-Eval stands for Annotating Behaviors in Chat.</sample>
    <sample id="353">This paper introduces a novel approach to code generation by incorporating interaction through clarification questions (CQs) to address the challenge of input underspecification in natural language descriptions (NLDs). Existing code generation models struggle when NLDs lack sufficient detail, especially at the operation level. To overcome this, the authors propose the CodeClarQA dataset, which includes synthetic clarifications for key operations identified via a code knowledge graph. The method involves identifying key operations, representing them in a latent space, and using schema-based similarity to determine if an NLD aligns with an operation or requires clarification. The paper also introduces a pipeline for CQ-driven code generation, consisting of a Clarification Need Predictor, a Question Selector, and a Code Generator. Experimental results show that incorporating clarifications improves code generation performance, supported by both CQ ranking and code generation metrics. However, challenges remain, including distinguishing similar operations and using argument values effectively. The analysis highlights that clarified key operations contribute to better code generation, although the top-ranked CQs sometimes miss relevant clarifications. The work presents a promising direction for improving code generation through interactive clarification, offering a new task and dataset for future research.</sample>
    <sample id="354">The performance delta between CoNLL-2003 and CoNLL++ is higher than 5 percentage points until 2020.</sample>
    <sample id="356">The authors of the paper are Matthias Lindemann, Alexander Koller, and Ivan Titov.</sample>
    <sample id="357">The name of the speaker is Siyu Yuan.</sample>
    <sample id="358">The paper involves 5 authors: Kayo Yin, Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig.</sample>
    <sample id="359">The approach is compared to the state-of-the-art architecture specifically tailored for simultaneous pre-translation.</sample>
    <sample id="361">Armineh Nourbakhsh, a PhD student at Carnegie Mellon University and research director at JP Morgan AI Research, presents "CounterComp," a method designed to enhance compositional generalization in multi-step quantitative reasoning tasks. The approach leverages counterfactual scenarios to improve the performance of neural models on question-answering tasks that involve arithmetic operations derived from financial tables. Current state-of-the-art models struggle with such tasks, especially when multiple steps are required, due to their tendency to memorize spurious patterns rather than learning meaningful relationships. CounterComp addresses this by mining counterfactual examples from the training data, generating positive and negative samples based on interventions in the input questions. These examples are used to introduce an auxiliary metric learning loss with a dynamic margin, which encourages the model to focus on relevant input tokens when generating appropriate operations. This method improves performance on both in-distribution and out-of-distribution samples, enhancing the model's ability to generalize compositionally. Qualitative analysis shows that the CounterComp loss helps the model attend to more meaningful tokens during training, aligning with the operations in the output. The approach is demonstrated to be effective across multiple state-of-the-art baselines, particularly in tasks with more than two reasoning steps.</sample>
  </task>
</testset>