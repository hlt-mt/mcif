<testset name="IWSLT2025" type="output">
  <task track="short" text_lang="it">
    <sample id="0">The main sources of data for language models are large-scale web crawl data, including political news media such as New York Times, Los Angeles Times, The Guardian, and Huffington Post.</sample>
    <sample id="1">The affiliations of the authors are: McGill University, Miele, and Microsoft Research.</sample>
    <sample id="2">"Ciao, benvenuti al nostro presentazione di DePlain, un nuovo corpus per l'identificazione di testi in tedesco a livello di documento e a livello di frase."</sample>
    <sample id="3">Il mio nome è Regina Storben e guiderò la prima parte della presentazione. Iniziamo a definire la semplificazione del testo.</sample>
    <sample id="4">"La notifica testuale è un processo di adattamento del testo per migliorare la comprensione del testo per un gruppo specifico di destinatari, come persone con difficoltà di lettura o non native speaker."</sample>
    <sample id="5">"Ecco, per addestrare un modello di notifica di testo, abbiamo bisogno di coppie parallele di testo, ad esempio, di documenti o frasi."</sample>
    <sample id="6">"Transforma il contenuto in inglese in una versione italiana."</sample>
    <sample id="7">Per semplificare la frase, diverse tecniche sono possibili, come si può vedere nell'esempio, ad esempio sostituzione lessicale, clausola, ricollocazione delle clausole o inserimento di parole.</sample>
    <sample id="8">"Ecco che proponiamo il nostro nuovo corpus D-plane. Infatti, negli ultimi anni ci sono stati alcuni problemi con gli esistenti corpora. Ad esempio, questi corpora qui sono troppo piccoli per addestrare un modello di notifica di testo."</sample>
    <sample id="9">Gli altri tre modelli che ho proposto negli ultimi anni sono tutti allineati in modo automatico, il che significa che possono essere soggetti a errori nella loro allineazione.</sample>
    <sample id="10">"Dunque, proponiamo il nostro nuovo corpus Deplane, diviso in due sottocorpora, Deplane APA e Deplane Web. Deplane APA è basato su testi di notizie."</sample>
    <sample id="11">"Nell'ambito del Plain APA, abbiamo allineato 483 documenti manualmente. Ciò ha prodotto circa 30.000 coppie di frasi parallele, 13.000 delle quali."</sample>
    <sample id="12">"Abbiamo creato un corpus di grande profondità per una rete web. Questo corpus include diversi domini. E abbiamo anche allineato tutti questi 750 documenti sia manualmente che con metodi di allineamento automatici."</sample>
    <sample id="13">"Ne risultiamo in 30.450 coppie di frasi."</sample>
    <sample id="14">Analizziamo un po' di più le coppie di frasi. Ad esempio, tipo di semplificazione.</sample>
    <sample id="15">"Ecco, come si può vedere qui, i testi biblici sono molto più semplici rispetto, ad esempio, ai testi dei notiziari o ai testi per gli apprendisti."</sample>
    <sample id="16">Tutto sul livello, ad esempio, semplificazione lessicale, semplificazione strutturale, anche livello di semplificazione complessivo.</sample>
    <sample id="17">Inoltre, è possibile notare che il nostro corpus Deplaned presenta una grande varietà di trasformazioni di semplificazione. Ad esempio, nel corpus Deplaned API, abbiamo molti più riordinamenti e edizioni di parole rispetto a quanto abbiamo nel corpus Deplaned Web.</sample>
    <sample id="18">"Altrimenti, nel corpus web, abbiamo molte valutazioni molto brevi."</sample>
    <sample id="19">Ecco la traduzione:

"Sì, vediamo cosa possiamo fare con questo corpus. Ciao, sono Omar e ora parlerò degli use case per il nostro dataset Dplane. Quindi, il primo use case è valutare metodi di allineamento automatico."</sample>
    <sample id="20">"In questi ultimi anni, ci sono stati molti metodi di allineamento, ma in contesto di traduzioni macchine."</sample>
    <sample id="21">Here is the translation:

"Where we have two parallel documents written in different languages and we want to extract alignments of sentences in post documents."</sample>
    <sample id="22">Ma nel nostro caso di utilizzo stiamo cercando di estrarre gli alignamenti tra frasi di due documenti paralleli con lo stesso linguaggio, con lo stesso contenuto ma con diverso livello di complessità.</sample>
    <sample id="23">"E adesso che abbiamo il nostro insieme di dati Dplane, che abbiamo manualmente allineato le frasi, possiamo utilizzare queste frasi come standard d'oro per valutare alcuni metodi di allineamento proposti."</sample>
    <sample id="24">"Ebbi eseguito alcune adattamenti ai metodi proposti e abbiamo pubblicato tutti questi adattamenti e i codici per eseguire i nostri esperimenti nel nostro articolo."</sample>
    <sample id="25">Al termine, abbiamo concluso che il metodo di allineamento automatico più adatto da utilizzare per la semplificazione del testo in tedesco è il metodo di allineamento di massa.</sample>
    <sample id="26">E puoi trovare anche il codice per eseguire questo metodo nei tuoi documenti sul tuo documento.</sample>
    <sample id="27">"Il secondo caso che abbiamo mostrato nel nostro studio è il caso di semplificazione del testo."</sample>
    <sample id="28">Risultato:

"Ridisegnare il contenuto in italiano da testo complesso in testo semplice mediante l'addestramento di modelli linguistici."</sample>
    <sample id="29">Abbiamo sottoposto a fine-tuning due modelli diversi. Abbiamo sottoposto a fine-tuning un modello per la produzione di semplificazioni a livello di documento.</sample>
    <sample id="30">"E abbiamo anche ottimizzato l'importazione di base per produrre semplificazioni a livello di frase."</sample>
    <sample id="31">Puoi trovare anche tutti i checkpoint e consultare i dettagli dei risultati e delle metriche di valutazione dei nostri esperimenti nel nostro articolo.</sample>
    <sample id="32">Abbiamo concluso che tale regolazione fine potrebbe produrre o superare i punteggi di base.</sample>
    <sample id="33">E proponiamo quei risultati come riferimento, un riferimento di base per il problema di semplificazione del testo nel futuro.</sample>
    <sample id="34">Grazie mille per la vostra attenzione e speriamo di vedervi durante la conferenza. Grazie.</sample>
    <sample id="35">Kaio Yan.</sample>
    <sample id="36">T5x large model.</sample>
    <sample id="37">Sì, funzionano ancora.</sample>
    <sample id="38">The novelty of the approach is that it attempts to reduce subjectivity by explicitly annotating whether each model response exhibits certain behaviors, such as providing irrelevant information or contradicting itself.</sample>
    <sample id="39">The success of the current weakly-supervised learning (WSL) approach is largely based on the availability of clean validation samples.</sample>
    <sample id="40">To improve the score, consider rephrasing the question to make it more specific and focused, and ensure that the answer is concise and relevant to the topic.</sample>
    <sample id="41">5 autori sono coinvolti nell'articolo: Dawei, Xiao Yuxian, Mario Smusbach, Diaz Stefan e Diti Klakow.</sample>
    <sample id="42">"Ciao, il mio nome è Adam Szpirkowski e questo discorso è sulle strutture di dipendenza della coordinazione."</sample>
    <sample id="43">"Ecco che potresti sapere che ci sono strutture di dipendenza diverse che vengono assunte da diverse teorie e approcci di corpus. Ad esempio, nel caso delle dipendenze universali, la struttura di coordinazione è Lisa, Bart e Maggie."</sample>
    <sample id="44">"Può essere detto che il primo congiunto è la testa di tutta la struttura coordinata, quindi in questo caso Lisa."</sample>
    <sample id="45">"Approcci simili sono stati assunti anche nella teoria di significato di Igor Miltruk, dove l'intera struttura coordinata è guidata dal primo congiunto. Quindi questi due approcci sono simmetrici, selezionano uno dei congiunti."</sample>
    <sample id="46">"Ecco, ci sono anche approcci simmetrici per le strutture coordinate come l'approccio di Praga, l'approccio a capo di congiunzione, che assume alberi di dipendenza impraticabili dove le strutture coordinate sono a capo di congiunzione."</sample>
    <sample id="47">"Otteniamo dipendenze da e verso tutti i congiunti."</sample>
    <sample id="48">"E inoltre, ci sono anche un approccio multi-aspetto, ad esempio, nel grammatica di Cutson."</sample>
    <sample id="49">"Ecco, in un certo senso, tutti i comportamenti sono teste della struttura coordinata. Quindi otteniamo dipendenze dal governatore, che consente di eseguire separatamente ogni comportamento. Questi sono i lavori di Barton."</sample>
    <sample id="50">"Ecco, l'obiettivo di questo studio è produrre un argomento innovativo a favore delle strutture simmetriche di coordinamento, come queste due, e contro le strutture asimmetriche di coordinamento, come queste."</sample>
    <sample id="51">"Okay, l'argomentazione è basata sul principio del minimizzo della dipendenza che viene spiegato sulla base di questi esempi."</sample>
    <sample id="52">"Ecco, in inglese, come sapete, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli aggettivi aggettivi possono essere più lontani. Quindi 'March read it yesterday' è okay perché l'oggetto diretto è vicino al verbo."</sample>
    <sample id="53">"Si, il tempo verbale di ieri è molto peggiore, giusto? Infatti, tra il verbo e l'oggetto diretto, c'è un avverbio 'ieri'."</sample>
    <sample id="54">Tuttavia, questo effetto può essere emulato quando l'oggetto diretto è molto pesante e molto lungo, in quanto allora può essere spostato nella posizione dopo l'agente.</sample>
    <sample id="55">Ecco la traduzione:

"Questo è illustrato qui. Entrambi questi enunciati sono validi. 'March Redd è un libro assolutamente affascinante sull'odierno BCS. Io sono OK. Invece di questo, abbiamo questo lungo NP."</sample>
    <sample id="56">"E anche okay dire marzo ieri. C'è un libro assolutamente affascinante sul tema della pace."</sample>
    <sample id="57">Il motivo è che questo enunciato viola il principio grammaticale generale secondo cui gli oggetti diretti dovrebbero essere vicini al verbo.</sample>
    <sample id="58">"Eon soddisfa il principio del minimizzo delle dipendenze, che afferma che le dipendenze più brevi sono preferite."</sample>
    <sample id="59">"Questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, ovvero quelle che non sono costanti tra queste due strutture."</sample>
    <sample id="60">"Ecco che abbiamo una dipendenza da rosso all'aggettivo di sette parole e da rosso al libro di quattro parole. Quindi per ottenere il risultato è 11."</sample>
    <sample id="61">"When you move, when you swap these two constituents, the sum of these two dependencies becomes six, right? So instead of 11, six, much shorter, that's why this sounds quite okay, right?"</sample>
    <sample id="62">"Okay, quindi abbiamo estratto diverse statistiche sulla coordinazione dall'edizione migliorata del Pantry Bank e visto il rapporto sulle dipendenze universitarie che non abbiamo utilizzato."</sample>
    <sample id="63">"Ecco che le statistiche confermano l'osservazione fatta molte volte prima che i congiuntivi a sinistra tendono a essere più brevi. Quindi sale e pepe e sale misurato in sillabe."</sample>
    <sample id="64">"E anche l'osservazione fatta in passaggio che tale tendenza cresce con la differenza di lunghezza."</sample>
    <sample id="65">Quando la differenza tra le lunghezze dei due congiunti aumenta, il congiunto più corto preferisce essere il primo più forte. Quindi la proporzione è maggiore del congiunto corto a sinistra.</sample>
    <sample id="66">Ma cosa è innovativo in questo studio è che abbiamo osservato che questo trend si verifica solo quando l'agitazione governativa si verifica a sinistra.</sample>
    <sample id="67">"Okay, quindi il governatore è sulla sinistra in questo esempio. Ho visto Lisa Barton. Quindi il governatore è sulla sinistra."</sample>
    <sample id="68">Ecco la traduzione:

"Ecco, assente nel secondo esempio, Omero è arrivato e ha starnutito, qui abbiamo coordinazione di due verbi e non c'è un governatore esterno, giusto? Quindi, in questi casi, il congiunto a sinistra preferisce essere più breve, tanto più grande è la differenza tra i due congiunti."</sample>
    <sample id="69">Tuttavia, quando il governo a destra è presente, il governo a sinistra governa la coordinazione del telnet, e questo effetto scompare.</sample>
    <sample id="70">"Ecco, abbiamo dimostrato che misurando la lunghezza in caratteri, la prima colonna è in sillabe, la colonna centrale e la colonna a destra in parole. Quindi, mi concentrerò sulla colonna a destra."</sample>
    <sample id="71">"Quello che vediamo è che quando c'è il governo sulla sinistra."</sample>
    <sample id="72">"L'abitudine del congiunzione a sinistra di essere più breve cresce regolarmente con la differenza assoluta tra le parole. Lo stesso si osserva quando non c'è governatore, come nella coordinazione di frasi, ma quando il governatore è a destra, questa abitudine scompare."</sample>
    <sample id="73">E mostriamo nel nostro studio come questo fornisce un argomento contro strutture di coordinamento asimmetriche come queste due e a favore di strutture simmetriche come queste due.</sample>
    <sample id="74">"Ecco il documento per la discussione completa e gli argomenti, scusa, e parliamo della sessione dei poster. Grazie."</sample>
    <sample id="75">2</sample>
    <sample id="76">Bible texts.</sample>
    <sample id="77">The example is "salt and pepper" vs. "pepper and salt", measured in syllables.</sample>
    <sample id="78">Yes, the pre-trained models obtained from NACHOS are freely available, and you can use them for your research.</sample>
    <sample id="79">DEplain-APA contains news texts.</sample>
    <sample id="80">Better model architecture, larger model size, and more fine-tuning examples.</sample>
    <sample id="81">The tendency of words on the left to be shorter was measured by counting the number of characters, syllables, and words in the first, middle, and right columns.</sample>
    <sample id="82">The experiments were designed to measure the length of words in characters, syllables, and words, with a focus on the right column, to analyze the effect of the governor's position on the tendency for the left conjunct to be shorter.</sample>
    <sample id="83">The classifier is not effective, performing no better than chance.</sample>
    <sample id="84">1</sample>
    <sample id="85">Bob, Alice</sample>
    <sample id="86">The models sensitive to context improve on phenomena such as formality and lexical cohesion, but not on others like ellipses, perennials, and verb form.</sample>
    <sample id="87">The authors of the article are affiliated with the following institutions: Kostav Sinha, John Gauthier, Aaron Mueller, Kanishka Mishra, Karen Fentus, Roger Levy, and Adina Williams.</sample>
    <sample id="122">Il framework quantifica la posizionalità attraverso la re-annotazione dei dati con annotatori diversi e la raccolta di dati demografici.</sample>
    <sample id="155">The study found that by giving the prompts to human subjects, they were also able to surface racial stereotypes.</sample>
    <sample id="156">The sources of data used in this study are the enhanced version of the Pantry Bank.</sample>
    <sample id="157">2</sample>
    <sample id="158">Debate statement classification, binary classification of expansion and comparison classes of PNTB, and consonance.</sample>
    <sample id="159">1</sample>
    <sample id="160">4</sample>
    <sample id="161">The framework differs from previous works by comparing end users with models, data sets, predictions, and labels, whereas previous works focused on annotator agreement or modeling annotator distributions.</sample>
    <sample id="162">The generated personas.</sample>
    <sample id="163">Google Translate.</sample>
    <sample id="164">"Ciao, sono Xiangbin, studente di dottorato all'Università di Washington. Oggi sto presentando il nostro lavoro sulla trasformazione di dati di pre-addestramento per modelli di linguaggio a compiti downstream, tracciando le tracce di bias politici che portano a modelli NLP ingiusti."</sample>
    <sample id="165">"I modelli di linguaggio vengono addestrati su grandi quantità di dati di crawls web."</sample>
    <sample id="166">"Il notiziario politico è ben rappresentato nei dati di training del modello di lingua. Secondo uno studio sul corpus C4, possiamo vedere che New York Times, Los Angeles Times, The Guardian, Huffington Post, ecc. sono ben rappresentati nei dati di training del modello di lingua."</sample>
    <sample id="167">Questa ha creato un beneficio misto per le applicazioni di modello linguistico.</sample>
    <sample id="168">"In un senso, hanno potuto imparare dalle diverse prospettive, celebrando la democrazia e la pluralità di idee. D'altra parte, queste diverse opinioni politiche sono inherentemente socialmente biasate e potrebbero portare a problemi di fairness negli applicazioni di task a valle."</sample>
    <sample id="169">"Al fine di raggiungere questo obiettivo, proponiamo di esplorare la pipeline di propagazione di bias politico dai dati di pre-addestramento ai modelli linguistici per compiti downstream, in particolare chiedendoci le seguenti domande."</sample>
    <sample id="170">"Prima di tutto, come valutiamo i modelli di linguaggio lineari politici e in che modo i dati pertinenti potrebbero influire su tali bias politici?"</sample>
    <sample id="171">"Inoltre, come funzionano i modelli di linguaggio con avversari politici diversi in compiti downstream e se ciò potrebbe portare a problemi di equità negli applicazioni di elaborazione del linguaggio naturale?"</sample>
    <sample id="172">"Ecco, proponiamo di stimolare i modelli linguistici con format di stimolo diversi utilizzando questionari politici come il test del compasso politico. Ciò ci consente di valutare automaticamente in modo fondato sulla letteratura scientifica in scienze politiche."</sample>
    <sample id="173">"Alcuni risultati preliminari dimostrano che i modelli di linguaggio hanno significati politici diversi. Occupano tutti e quattro quadranti della mappa politica."</sample>
    <sample id="174">" Possiamo anche vedere che GPT-4 è il modello di linguaggio più liberale di tutti, e i modelli GPT sono generalmente più socialmente liberali rispetto ai modelli BERT e alle sue varianti."</sample>
    <sample id="175">"Inoltre, ci proponiamo di indagare fino a che punto i bias politici dei modelli di linguaggio sono effettivamente trasmessi dai dati di training."</sample>
    <sample id="176">"Potremmo condurre un esperimento controllato pre-addestrando i modelli di linguaggio a checkpoint su sei corpora diversi con orientamento politico, separati tra notizie e social media, ulteriormente divisi per orientamento politico."</sample>
    <sample id="177">"Attraverso l'addestramento ulteriore dei modelli linguistici su tali partiti in Kodpora, possiamo vedere che le coordinate ideologiche del modello linguistico si spostano corrispondentemente."</sample>
    <sample id="178">Esempio, per Robert, addestrato ulteriormente sul corpus lineare redded, possiamo notare un sostanziale spostamento a sinistra in termini di...</sample>
    <sample id="179">"In termini di bias politici."</sample>
    <sample id="180">"E anche cerchiamo di indagare se i modelli linguistici possono captare la polarizzazione che è presente nella nostra società moderna."</sample>
    <sample id="181">"Dividiamo i corpus di pre-addestramento in quelli precedenti all'45° presidente degli Stati Uniti d'America e quelli successivi al 45° presidente degli Stati Uniti d'America, e addestriamo modelli di linguaggio separati su due corpora temporali diversi."</sample>
    <sample id="182">"E possiamo vedere che i modelli linguistici hanno un orientamento politico più lontano dal centro dopo il 2017. Ciò indica che i modelli linguistici possono anche captare la polarizzazione nella nostra società."</sample>
    <sample id="183">"Infine, valutiamo i modelli di lingua con significati politici diversi per la detezione di discorsi d'odio e di notizie false per applicazioni NLP che spesso coinvolgono modelli di lingua e potrebbero avere implicazioni molto significative."</sample>
    <sample id="184">"Ecco che notiamo che se esaminiamo le prestazioni per categoria, cioè se separiamo le prestazioni in"</sample>
    <sample id="185">"Nei diversi ambienti o media politici lineari, possiamo notare un pattern, ad esempio per la detezione di discorsi d'odio, i modelli linguistici di lingua di sinistra sono migliori."</sample>
    <sample id="186">"Detecting hate speech targeting socially minority groups."</sample>
    <sample id="187">"Tuttavia, stiamo lavorando per identificare Hitch Beach che si concentra su gruppi più potenti nella nostra società."</sample>
    <sample id="188">"Ecco, i modelli di linguaggio a senso unico sono migliori nel rilevare la propaganda d'odio contro gli uomini bianchi, tuttavia peggiori nel rilevare la propaganda d'odio contro le comunità minoritarie nere LGBTQ+ e altre."</sample>
    <sample id="189">"Il fatto che anche i modelli linguistici di sinistra siano migliori nel rilevare informazioni false di provenienza politica opposta e viceversa, è un'altra tendenza simile che si riscontra anche nella detezione delle notizie false."</sample>
    <sample id="190">"In questo, mostriamo molti esempi qualitativi per vedere che i modelli linguistici con significati politici diversi."</sample>
    <sample id="191">"Dato che fornisce predizioni diverse per esempi di discorso d'odio e informazioni false in base alle loro categorie sociali. Ci sono un mucchio di esempi in appendice per ulteriori evidenze in proposito."</sample>
    <sample id="192">"Indica che ci sono problemi di equità molto pressanti relativi ai bias politici dei modelli di linguaggio."</sample>
    <sample id="193">Esempio, se un modello di linguaggio a destra fosse stato addestrato su contenuti di odio o informazioni false o simili e successivamente distribuito su una piattaforma di social media popolare.</sample>
    <sample id="194">"Questo significherebbe che le persone con opinioni politiche opposte potrebbero essere marginalizzate e la diffamazione contro i gruppi minoritari potrebbe essere sfrenata senza alcun controllo."</sample>
    <sample id="195">"Ecco che suona l'allarme per farci riconoscere e affrontare le questioni di equità generate dalle interpretazioni politiche dei modelli di linguaggio."</sample>
    <sample id="196">"Vogliamo anche sottolineare che esponiamo il dilemma unico relativamente ai bias politici del modello linguistico. È come tra Cilla e Karebdis."</sample>
    <sample id="197">"E se non sanificiamo le opinioni politiche nei dati di training del modello linguistico, il bias si propagherà dai dati di pre-training ai modelli linguistici alle attività downstream, creando problemi di equità."</sample>
    <sample id="198">Se cerchiamo di sanificare in qualche modo, rischiamo anche la censura o l'esclusione. E è incredibilmente difficile determinare cosa è veramente neutrale e dovrebbe mantenere i dati di monitoraggio linguistico. Quindi è un po' come il problema del filo elettrico.</sample>
    <sample id="199">"Okay, fantastico. Penso che sia tutto qui. Grazie per il tuo tempo."</sample>
    <sample id="200">3</sample>
    <sample id="201">1024</sample>
    <sample id="202">The text appears to be a sequence of repeated words in a language other than English, likely Persian or Farsi. As it's not a meaningful text, it's difficult to determine the domains included in their dataset.</sample>
    <sample id="203">Positionality refers to the perspectives that people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="204">Dawei</sample>
    <sample id="205">Yes, our solution is to use an existing offline SD model without retraining or adopting a specific architecture for a single SD, and use one model for every latency regime, handling latency through specific parameters.</sample>
    <sample id="206">There are no specific information about the authors in the given text, but Yusin Zhang from Penn State University is presenting the work.</sample>
    <sample id="207">Sì.</sample>
    <sample id="208">The three settings of KITMUS are: Background pre-trained, Background both, and Background influence.</sample>
    <sample id="209">The authors of the article are Jawad Hosseini, Philipp Radlinski, Sylvia Parity, and Anilouis.</sample>
    <sample id="210">The last research question is: "Should we only use the clean samples for validation or are there better ways to utilize them?"</sample>
    <sample id="211">The sensitivity metric measures a model's ability to produce consistent outputs for the same task despite slight variations in instruction wording.</sample>
    <sample id="212">Jingwei Yi.</sample>
    <sample id="213">Una maggiore sensibilità indica una performance del modello migliore.</sample>
    <sample id="214">The linguistic context provided to the models during pre-training is a joint work with multiple authors, including John Wothier, Aaron Mueller, Kanishka Mishra, Karen Fentus, Roger Levy, and Adina William.</sample>
    <sample id="215">20</sample>
    <sample id="216">The authors of the article are Myra, Essen Dermush, and Dan Jerovsky.</sample>
    <sample id="217">Developing new methods to measure information bias is necessary because language models, despite their advancements, can perpetuate and amplify existing societal biases, influencing how we process and understand information.</sample>
    <sample id="218">Akshita.</sample>
    <sample id="219">The political bias propagation pipeline appears to be a complex system that involves the pre-training data and its influence on language models, ultimately affecting downstream task applications.</sample>
    <sample id="220">Yes, the simplification process differs between Deplaned API and Web corpus, with Deplaned API having more reorderings and word editions, while Web corpus has more rephrasing.</sample>
    <sample id="221">No, the script is not publicly available.</sample>
    <sample id="222">La filigrana viene inserita nel testo mediante la somma dei pesi della rappresentazione originale e della rappresentazione target, dove il peso della rappresentazione target è proporzionale al numero di trigger presenti nella frase.</sample>
    <sample id="223">The affiliation of the author is Penn State University.</sample>
    <sample id="224">Sì, i modelli codificatore-decodificatore come MT5 possono migliorare con l'addestramento su una combinazione di lingue.</sample>
    <sample id="225">Pianificazione linguistica vincolata: "Preparare un dolce al cioccolato" (Planning for specific goals with specific constraints, e.g., "make a chocolate cake").</sample>
    <sample id="226">The authors validate the covertness of their method by visualizing the embeddings of sentences on the VOPCA dataset and comparing them to the number of triggers in each sentence.</sample>
    <sample id="227">The work utilizes existing PLMs by introducing three model trains on continental pre-training to analyze the impact of pre-training strategy, allowing for the construction of a new PLM.</sample>
    <sample id="228">Based on the text, GPT-4 is least aligned to non-English speaking countries.</sample>
    <sample id="229">La relatrice mostra il modo in cui il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione nella frase: "leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output".</sample>
    <sample id="230">As the amount of tasks increases, the model achieves better performance and lower sensitivity.</sample>
    <sample id="231">Based on the text, the three approaches that the authors compare their method with are:

1. Tree-less models
2. Other models on the Coggs benchmark</sample>
    <sample id="232">The two co-authors, Alexander Kodler and Yvon Titov, are the advisors of the first author.</sample>
    <sample id="233">The author of PaLM is not explicitly mentioned in the provided text.</sample>
    <sample id="234">"Ciao a tutti, sono Jenny, studentessa dottoranda del primo anno dell'Università di Carnegie Mellon, e oggi presenterò il mio lavoro, "Analisi posizionale, caratterizzazione mediante un insieme di modelli CSA".</sample>
    <sample id="235">"Questo lavoro è stato fatto in collaborazione con alcuni colleghi dell'Università di Washington e dell'Institute for AI di Allen, ovvero Sebastian Santy, Ronan LaBros, Katarina Aranica e Martin Sapp."</sample>
    <sample id="236">"Cerchiamo di iniziare immaginando di lavorare per un giornale e di essere in fase di revisione dei commenti sottostanti all'articolo per rimuovere il contenuto tossico."</sample>
    <sample id="237">"Puoi rivolgerti a un API popolare come Perspective API per la detezione della città tossica. E ciò funziona molto bene se si è Carl Jones, in quanto Perspective API è in grado di rilevare correttamente gli istanze tossiche."</sample>
    <sample id="238">"Ma non è proprio il caso per un dithya-sharma, dove gli API non sono altrettanto sensibili a termini offensivi più comuni in contesti indiani."</sample>
    <sample id="239">"Ecco un esempio di bias di design dove si verificano differenze sistematiche nel rendimento della tecnologia tra popolazioni."</sample>
    <sample id="240">"I biassi di progettazione come quello che abbiamo appena visto potrebbero verificarsi a causa della posizionalità dei ricercatori e dei sviluppatori di modelli di intelligenza artificiale. La posizionalità è semplicemente le prospettive che le persone tengono a causa delle loro demografiche, identità e esperienze di vita."</sample>
    <sample id="241">"Ecco un concetto ampiamente utilizzato in studi critici, in particolare in spazi accademici femministi e queer."</sample>
    <sample id="242">"E come ricercatrice, la posizione può influire sul processo di ricerca e sui suoi esiti e risultati, poiché può cambiare le decisioni che i ricercatori prendono."</sample>
    <sample id="243">"E quindi una domanda che le persone potrebbero porre è se i set di dati e i modelli hanno posizionalità."</sample>
    <sample id="244">"E non stiamo cercando di dire che i modelli e i set di dati in sé stessi hanno identità demografiche e esperienze di vita, ma aggregano giudizi e opinioni di persone reali e possono quindi rappresentare certe posizioni piuttosto che altre."</sample>
    <sample id="245">"Il lavoro precedente ha suggerito prove aneddotiche di avere una posizione, come divari culturali nei modelli e nei set di dati, nonché definizioni teoriche di posizione del modello."</sample>
    <sample id="246">"Tuttavia, questi lavori non esaminano affatto i consumatori finali con i set di dati e i modelli stessi."</sample>
    <sample id="247">"E come il modello e il set di dati diventano sempre più importanti in base alla posizione, le attività NLP diventano sempre più soggettive e orientate alla società."</sample>
    <sample id="248">"E è difficile caratterizzare queste posizioni in quanto non tutti i decisori sono documentati e molti modelli sono nascosti dietro API."</sample>
    <sample id="249">"Per studiare i dati e modellare la posizione, confrontiamo le annotazioni con i dati reali di utenti esistenti e modelli."</sample>
    <sample id="250">"Ciò lo facciamo attraverso il nostro framework NL Positionality."</sample>
    <sample id="251">Il nostro framework funziona in due passaggi principali.</sample>
    <sample id="252">"La prima mossa è quella di riannotare i dati con annotatori diversi."</sample>
    <sample id="253">"E scegliamo di fare questo anziché esaminare le statistiche demografiche dei set di dati originali, gli annotatori, perché normalmente solo pochi annotatori annotano ogni istanza e perché le informazioni demografiche sono raramente raccolte e condivise."</sample>
    <sample id="254">"E quindi optiamo per ri-analizzare i dati per ottenere molte entità, ad esempio, e ottenere un ricco insieme di dati demografici."</sample>
    <sample id="255">"E quindi confrontiamo le annotazioni demografiche con i modelli e i set di dati utilizzando una coppia, poiché il nostro punteggio di correlazione."</sample>
    <sample id="256">"E quindi, il nostro framework differisce dalla letteratura sulla disaccordo degli annotatori esaminando gli utenti finali, i modelli, i set di dati, le predizioni e le etichette, anziché solo l'accordo degli annotatori o la distribuzione degli annotatori."</sample>
    <sample id="257">Il nostro tasso di frame è principalmente abilitato da Lab in the Wild, una piattaforma di crowdsourcing online per collaboratori di HCI.</sample>
    <sample id="258">"Lab in the Wild è un' piattaforma di sperimentazione online in cui possiamo reclutare volontari diversi rispetto alle piattaforme come MTURC, che perlopiù hanno partecipanti dagli Stati Uniti o dall'India. Inoltre, Lab in the Wild è in grado di ottenere dati di alta qualità."</sample>
    <sample id="259">"Abbiamo due compiti all'out of the wild, uno dei quali è l'accettabilità sociale. E il modo in cui funziona è che i partecipanti leggeranno una situazione dal set di dati sulla chimica sociale, e poi scriveranno come accettabile è la situazione."</sample>
    <sample id="260">"Dopo aver risposto, per restare coinvolti nello studio, possono confrontare le loro risposte con un'intelligenza artificiale e con altre persone."</sample>
    <sample id="261">"Abbiamo quindi confrontato queste annotazioni con la chimica sociale, Delphi e GPT-4."</sample>
    <sample id="262">"Eseguiamo quindi un setup molto simile per il compito di rilevamento di tossicità e parola d'odio, dove leggeranno un esempio da Danny Hate e scriveranno se pensano che sia un'istanza di parola d'odio."</sample>
    <sample id="263">Abbiamo quindi confrontato queste annotazioni con Dynahate, Perspective API, Rewire API, HateRoberta e GPT-4. La nostra indagine alla fine ha raccolto più di 16.000 annotazioni da oltre 1.000 annotatori da 87 paesi.</sample>
    <sample id="264">"E adesso stiamo per equipaggiare per rispondere a chi sono i set di dati e i modelli di NLP che si allineano di più. Scopriamo che c'è una posizionalità in NLP."</sample>
    <sample id="265">Ecco il testo tradotto in italiano:

"Ad esempio, scopriamo che i dati e i modelli sono più allineati ai Paesi anglofoni. Pertanto, per l'analisi di accettabilità sociale di GPT-4, scopriamo che è più allineato ai Paesi confuciani e anglofoni. Scopriamo che Dynahate è anche più allineato ai Paesi anglofoni."</sample>
    <sample id="266">"Inoltre, scopriamo la maggior parte dell'allineamento con le persone che hanno un diploma di laurea. Così, per GPT-4 nel compito di accettabilità sociale, scopriamo che è maggiormente allineato con le persone con un diploma di laurea o un diploma di specializzazione."</sample>
    <sample id="267">"Ecco che troviamo lo stesso per Dianaheid, dove è maggiormente associato a persone con un titolo di studio universitario."</sample>
    <sample id="268">Tuttavia, quando modelli e insiemi di dati sono allineati a specifiche popolazioni, alcuni inevitabilmente ne vengono lasciati indietro.</sample>
    <sample id="269">"Esempio di ciò è che i set di dati e i modelli sono meno in linea con le persone non binarie rispetto ai corrispondenti uomini e donne. Lo troviamo nel compito di accettabilità GPT-4, nonché nell'analisi del compito di ristorante-odio."</sample>
    <sample id="270">"E quindi, considerando la situazione in Atlantide e LP, cosa possiamo fare al riguardo?"</sample>
    <sample id="271">"Ecco alcune raccomandazioni. La prima è di tenere un registro di tutte le scelte di design durante il processo di ricerca. La seconda è di fare ricerche NLP che tengano conto di una prospettiva."</sample>
    <sample id="272">"La nostra terza raccomandazione è quella di creare insiemi di dati e modelli specializzati all'interno di quattro comunità specifiche. E un esempio di ciò è l'iniziativa Musseqani. Vogliamo enfatizzare che l'NLP inclusiva non è solo fare in modo che tutte le tecnologie funzionino per tutti.</sample>
    <sample id="273">"Ecco il termine della nostra presentazione, ma se desiderate ulteriori informazioni, potete visitare il nostro dashboard per i risultati dell'analisi aggiornati e il nostro articolo. Grazie."</sample>
    <sample id="274">The speaker mentions 3 problems associated with current stimulus models: 

1. Specific architectures are usually trained, introducing additional modules to be optimized. 
2. Long and complicated training procedures. 
3. Training and maintaining several models to reach different latency regimes.</sample>
    <sample id="275">One effective way to mitigate social and political biases in NLP model training data is to use active learning and human evaluation, where human evaluators are involved in the process to correct and verify the model's output, ensuring that the data is accurate and unbiased.</sample>
    <sample id="276">"Ciao, sono Si Yu-Yuan dell'Università Fudan. Sto presentando il nostro lavoro, Distinguished Script Knowledge from Language Models for Constrained Language Planning."</sample>
    <sample id="277">"In vita quotidiana, gli esseri umani pianificano le loro azioni seguendo istruzioni passo passo in forma di scenari garantiti."</sample>
    <sample id="278">"La precedente generazione di modelli di linguaggio ha sfruttato i modelli di linguaggio per pianificare obiettivi astratti di attività stereotipiche, come fare una torta, e ha dimostrato che i grandi modelli di linguaggio possono decomporre gli obiettivi in passaggi."</sample>
    <sample id="279">"Tuttavia, il lavoro precedente si concentra prevalentemente sulla pianificazione per obiettivi astratti di attività teoriche. La pianificazione per obiettivi con obiettivi specifici, vincoli specifici, ad esempio fare un dolce al cioccolato, è ancora poco esplorata."</sample>
    <sample id="280">"In questo studio, definiamo il problema del piano di pianificazione linguistica."</sample>
    <sample id="281">"Il sistema di pianificazione GoalSaw impone diverse restrizioni sulle azioni. Un obiettivo astratto può essere ereditato da obiettivi specifici della vita reale con restrizioni multifacettate. Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli alle restrizioni."</sample>
    <sample id="282">"Evalueremo e miglioriamo la capacità di pianificazione del linguaggio con vincoli di modelli di linguaggio di grandi dimensioni."</sample>
    <sample id="283">"Non esiste alcun sito di dati specifico per identificare i nostri obiettivi."</sample>
    <sample id="284">"Dobbiamo ottenere questo codice in primo luogo. Come mostrato nella tabella, estendiamo il codice astratto con vincoli multifacolti per l'acquisizione di dati in loop con l'uomo Instruct GPT."</sample>
    <sample id="285">"Evaluiamo 100 obiettivi specifici e valutiamo i script generati da modelli di larga scala."</sample>
    <sample id="286">"Questo rapporto presenta le prestazioni generali dei risultati. Scopriamo che tutti i rimodelli di line-up raggiungono risultati insoddisfacenti nella pianificazione per obiettivi specifici."</sample>
    <sample id="287">"Eseguiamo quindi un'analisi dettagliata per esaminare a cosa sono destinati i modelli a livello di riga."</sample>
    <sample id="288">"Il risultato mostra che la coerenza semantica nei script generati è accettabile, ma non può essere garantita la fedeltà ai vincoli."</sample>
    <sample id="289">"Stiamo esplorando categorie di argomenti più frammentati definiti in Waking Home. La mappa della testa nella figura mostra che il rendimento del piano varia notevolmente per ragazze di diverse categorie."</sample>
    <sample id="290">"I precedenti ricerche hanno dimostrato che la qualità dell'output dei modelli a livello di riga subisce una varianza elevata, il che porta a prestazioni scadenti. Pertanto, adottiamo l'idea di filtri Z sovra-generati per migliorare la qualità di generazione."</sample>
    <sample id="291">"Il nostro primo passaggio è mostrare tipi di vincoli con esempi per intract.cpt e ottenere obiettivi specifici basati sui set di obiettivi astratti."</sample>
    <sample id="292">"Infine, istruisci GPT per generalizzare gli script per obiettivi specifici."</sample>
    <sample id="293">"Ecco, un modello di filtro viene derivato per selezionare i script fisici."</sample>
    <sample id="294">"Trasformiamo i script e gli obiettivi in embedding di GPT astratti e calcoliamo la similitudine di cosine come valori di similarità per misurare la similarità semantica."</sample>
    <sample id="295">"Inoltre, elimineremo lo script che contiene le chiavi di ricerca del vincolo di target. Teniamo solo lo script se il target si chiama il più alto tra quelli impostati."</sample>
    <sample id="296">"Con il nostro metodo, Instructivity può generare quadrati di qualità superiore. Il nostro metodo migliora significativamente la pianificabilità sia in termini di completezza semantica che di fedeltà al vincolo."</sample>
    <sample id="297">"Poiché i modelli di linguaggio sono costosi da implementare, è essenziale abilitare la pianificazione linguistica di modelli più piccoli e specializzati. Creare set di dati è un passo essenziale per raggiungere questo obiettivo."</sample>
    <sample id="298">Tuttavia, gli studi precedenti non consentono la pianificazione di obiettivi specifici e l'annotazione dei dati manuali è costosa.</sample>
    <sample id="299">"Quindi, seguiamo l'idea di distillazione del conoscenza simbolica per distillare dati di pianificazione linguistica limitati da modelli a livello live."</sample>
    <sample id="300">"Applicheremo il nostro metodo per creare un dataset di pianificazione del linguaggio con vincoli, denominato Coscript."</sample>
    <sample id="301">"In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità della validazione e dei siti di test, chiediamo ai lavoratori cloud-sourced di trovare e revisionare le campioni sbagliati."</sample>
    <sample id="302">"Il grafico mostra la distribuzione delle restrizioni del Corscript. Notiamo che il Corscript presenta un alto livello di plotismo nei goal generati specifici. Con il Corscript, possiamo creare modelli più piccoli ma specializzati per la pianificazione del linguaggio di restrizione."</sample>
    <sample id="303">"Rileviamo che il file T può generare script di qualità superiore rispetto ai modelli di grande livello, il che indica che i modelli più piccoli possono supportare i modelli più grandi quando addestrati su siti di dati appropriati."</sample>
    <sample id="304">"In sintesi, abbiamo stabilito il problema di pianificazione del linguaggio di constraint. Abbiamo sviluppato l'abilità di pianificazione del linguaggio di constraint per i modelli di linguaggio di grandi dimensioni e abbiamo sviluppato un metodo di filtro originario per i modelli di grandi dimensioni."</sample>
    <sample id="305">"Utilizziamo modelli di linguaggio avanzati per generare un set di dati di script di alta qualità, chiamato Corscript, per pianificare il linguaggio. Speriamo che il set di dati Corscript possa essere un risorsa preziosa per gli studiosi che si occupano di pianificazione del linguaggio."</sample>
    <sample id="306">"Grazie per il tuo tempo. Troverai ulteriori dettagli del copione nel nostro articolo."</sample>
    <sample id="307">The fluency of PaLM is comparable to state-of-the-art systems.</sample>
    <sample id="308">The important properties of a watermark method are: applicability to embedding S services, non-degrading utility of provided embeddings, being covert to the attacker, and being transferable to the attacker's services during model extraction.</sample>
    <sample id="309">The 14 languages are: Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Portuguese, Russian, Spanish, Swedish, Turkish, Vietnamese, and Polish.</sample>
    <sample id="310">According to the transcript, "only a few annotators annotate each instance".</sample>
    <sample id="311">Delta cosine and delta L2.</sample>
    <sample id="312">The multilingual encoder models were used in two groups: multilingual pre-trained encoders with pointer-based decoders (e.g., XLMR + PDR and BERT + PDR) and multilingual pre-trained encoder-decoder models (e.g., M-BART).</sample>
    <sample id="344">The authors select a trigger set by grouping words with a moderate frequency interval from a general text corpus.</sample>
    <sample id="345">"Ciao a tutti, mi chiamo Xu Heng. Oggi presenterò il nostro paper, "Do Connell 2003 named entity taggers still work well in 2023?". Andiamo avanti."</sample>
    <sample id="346">"Abbiamo indagato il problema della generalizzazione mediante l'analisi del compito di riconoscimento di entità denominate o NER task."</sample>
    <sample id="347">"Noi osserviamo che i modelli hanno utilizzato Kono 2003 per sviluppare NER per quasi 20 anni. E questo naturalmente solleva diversi problemi. In primo luogo, possono questi modelli generalizzare su dati moderni?"</sample>
    <sample id="348">E quando sviluppiamo nuovi taggers, cosa è necessario per una buona generalizzazione?</sample>
    <sample id="349">"Inoltre, se osserviamo una generalizzazione povera, cosa determina il calo di prestazioni di questi modelli?"</sample>
    <sample id="350">"Per investigare questi problemi, abbiamo sviluppato il dataset Carnot++. Si tratta di un dataset che abbiamo raccolto dai notiziari Reuters del 2020 e li abbiamo annotati con gli stessi criteri di annotazione del Carnot 2003."</sample>
    <sample id="351">"Abbiamo quindi sottoposto a fine-tuning più di 20 modelli sul set di dati Kono 2003. Li abbiamo valutati sia sul set di test Kono 03 che sul set di test Kono++."</sample>
    <sample id="352">"E infine, abbiamo calcolato il cambiamento percentuale di F1 per valutare la generalizzazione di ogni modello."</sample>
    <sample id="353">Ecco la traduzione:

"Sì, cosa è necessario per una buona generalizzazione? Nelle nostre esperimenti abbiamo scoperto che ci sono tre ingredienti principali che sono necessari."</sample>
    <sample id="354">"La prima è l'architettura del modello. Nelle nostre esperimenti, abbiamo scoperto che i modelli Transformer si generalizzano meglio ai dati nuovi."</sample>
    <sample id="355">Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che i modelli più grandi portano a una maggiore generalizzazione.</sample>
    <sample id="356">"E inoltre, non ultimo, sappiamo che l'adattamento fine-tuning di esempi direttamente influisce sulle prestazioni di un compito downstream. Anche in questo caso, abbiamo scoperto che più esempi di adattamento fine-tuning portano a una maggiore generalizzazione."</sample>
    <sample id="357">"To our next question, what causes the performance drop of some models?"</sample>
    <sample id="358">"Abbiamo due ipotesi. La prima è l'overfitting adattivo, che è un overfitting causato dall'utilizzo dello stesso insieme di test più volte. Ciò si manifesta solitamente come una diminuzione dei ritorni su un nuovo insieme di test."</sample>
    <sample id="359">Il secondo ipotesi è il dritto temporale, che è la degradazione del rendimento causata dall'ampio divario temporale tra i dati di addestramento e di test.</sample>
    <sample id="360">Per l'overfitting adattivo, vediamo che dal grafico sulla destra, la linea di miglior adattamento rosso ha un gradiente maggiore di 1.</sample>
    <sample id="361">"Questo significa che ogni unità di miglioramento che abbiamo realizzato nella colonna 2003 si traduce in più di un'unità di miglioramento nella colonna plus plus, il che significa che non ci sono ritorni decrescenti."</sample>
    <sample id="362">"Ecco che ci mostra che l'overfitting adattivo in questo caso non è osservato."</sample>
    <sample id="363">"Quindi, cosa pensi di Temporary Trif?"</sample>
    <sample id="364">"Per il progetto Temporal Drift, abbiamo fatto un esperimento per riaddestrare o continuare a preaddestrare alcuni modelli con dati più recenti. E abbiamo scoperto che il rendimento peggiora con un divario temporale più grande."</sample>
    <sample id="365">"Ecco che conferma la nostra ipotesi che la principale causa del calo di prestazioni è il drift temporale."</sample>
    <sample id="366">"La nostra conclusione è che per una buona generalizzazione, avremmo bisogno di un'architettura di modello migliore, di un modello più grande e di più esempi di fine-tuning. E questi obiettivi, mano nella mano, non possiamo avere un ingrediente solo, ma dobbiamo avere gli altri."</sample>
    <sample id="367">"Inoltre, abbiamo scoperto che il calo di prestazioni è causato da shift temporali e, sorprendentemente, non è causato da sovrapposizionamento adattivo, nonostante Conno2003 sia stato utilizzato per più di 20 anni."</sample>
    <sample id="368">"Ritornando alla domanda che abbiamo sollevato nel titolo del nostro articolo, i Tagger di Connell del 2003 funzionano ancora nel 2023? E abbiamo scoperto che la risposta è un sì decisamente affermativo."</sample>
    <sample id="369">"Speriamo che il nostro studio richieda ulteriore ricerca su come migliorare le generalizzazioni dei modelli."</sample>
    <sample id="370">"E ultima cosa, per favore controllate il nostro paper e il nostro insieme di dati. Se avete qualsiasi domanda, non esitate a contattarmi. Grazie mille."</sample>
    <sample id="397">The spoken segment utilizes a neutral tone.</sample>
    <sample id="398">Based on the example, the specific knowledge required about the entity is: Kea's and Servin's names.</sample>
    <sample id="399">La qualità dell'esempio.</sample>
    <sample id="400">Gli esperimenti estesi si concentrano sui modelli linguistici GPT-4 e BERT e i suoi varianti.</sample>
    <sample id="401">The model combines scores from multiple levels.</sample>
    <sample id="402">Direct inference examples include: "the name of the song is in me" or "its position, the first one".</sample>
    <sample id="403">The authors are affiliated with Fudan University.</sample>
    <sample id="404">3</sample>
    <sample id="405">No, the translation of the query into natural language using an automatic translation model before semantic parsing was not considered a standard approach.</sample>
    <sample id="406">The authors provided the example of a "warrior" as a marked group, where the default is typically associated with men, and when describing a female warrior, the term is marked with "woman".</sample>
    <sample id="407">According to the speaker, transformer models generally generalize better to new data, but it's not specified which architectures do not generalize well.</sample>
    <sample id="408">The names of the datasets for testing are not explicitly mentioned in the given text.</sample>
    <sample id="409">There are 3 authors involved in the article: Akshita, Martin, and the research institutions (McGill University, Miele, and Microsoft Research).</sample>
    <sample id="410">The author operates with multimodal data, including text.</sample>
    <sample id="439">According to the text, inference time knowledge is an area of NLU that is not well-studied.</sample>
    <sample id="440">Ying and Zhiyang.</sample>
    <sample id="441">Yes.</sample>
    <sample id="442">Limited types of context-dependent translations and limited sets of languages, relying on domain knowledge and human curation.</sample>
    <sample id="443">"Ciao, sto per parlare del nostro lavoro sul risoluzione di espressioni che si riferiscono indirettamente per la selezione di entità, in cui introduciamo il punteggio Alt entità."</sample>
    <sample id="444">Il mio nome è Jawad Hosseini e questo è un lavoro congiunto con Philippe Ladinsky, Sylvia Parity e Annie Lewis.</sample>
    <sample id="445">Non riesco a comprendere il testo in inglese poiché è composto da una serie di ripetizioni del termine "درستان" senza senso. Potresti fornirmi un testo in inglese più comprensibile per poter tradurlo in italiano?</sample>
    <sample id="446">Il modo più ovvio è utilizzare una differenza diretta. Ad esempio, dicendo il nome della canzone è in me o la sua posizione, la prima.</sample>
    <sample id="447">"A volte una riferimento indiretto è più appropriato per avere una conversazione più naturale. Ciò potrebbe accadere quando l'utente non ricorda il nome della canzone."</sample>
    <sample id="448">Non riesco a comprendere il testo in inglese poiché sembra essere scritto in caratteri arabi. Se vuoi fornirmi il testo in inglese, sarò felice di tradurlo in italiano per te.</sample>
    <sample id="449">"Oo o o da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da</sample>
    <sample id="450">"Ecco un problema importante in sistemi di conversazione e anche per la valutazione di LLM per l'intelligenza artificiale."</sample>
    <sample id="451">Non posso fornire una traduzione italiana del testo poiché non contiene nessun significato o contenuto comprensibile. Il testo sembra essere un semplice ripetizione di parole "نمید" (che sembra essere un termine persiano o arabo) senza alcun significato o senso.</sample>
    <sample id="452">Il nostro metodo di raccolta dei dati enfatizza l'informalità utilizzando un insieme di completamento cartoon.</sample>
    <sample id="453">Il cartone animato ha tre bolle di conversazione. Nella prima bolla, Bob dice: "Ricorda quella canzone che ascoltavamo ieri?" E con questo, Bob dice il contesto del dialogo.</sample>
    <sample id="454">"Nel secondo balloon di parlazione Alice dice, intendi facile con me o ho una sensazione?"</sample>
    <sample id="455">"Quale è l'alternativa alla domanda. E nel terzo balloon, Bob utilizza un riferimento indiretto per selezionare uno di questi entità, ad esempio la più recente."</sample>
    <sample id="456">Non posso tradurre il contenuto poiché non è un testo comprensibile in inglese. Il testo sembra essere un campione di rumore o di rumore di fondo, non un testo intelligibile. Se non hai alcun testo da tradurre, per favore fornisci uno e ti aiuterò con gusto!</sample>
    <sample id="457">La seconda, che è la domanda alternativa, viene generata in questo modo.</sample>
    <sample id="458">Usiamo sempre un template semplice. Vuoi dire A o B? A e B sono esempi da Wikipedia.</sample>
    <sample id="459">Ecco i diversi metodi di campionamento che abbiamo utilizzato. Man mano che ci spostiamo più in alto nella lista, gli entità diventano più simili tra loro, e solitamente è più difficile fare la disambiguazione.</sample>
    <sample id="460">La prima è Uniform Attract.</sample>
    <sample id="461">Il secondo caso è quando le entità hanno titoli simili. Ad esempio, due libri con lo stesso titolo, il dettaglio del retail.</sample>
    <sample id="462">Il terzo caso è quando hanno descrizioni simili su Wikipedia e infine quando hanno infobox o attributi simili su Wikipedia. Ad esempio, lo stesso genere o lo stesso artista per una canzone.</sample>
    <sample id="463">Mi mostrerò questa alternativa alla risposta. Sanno il nome di queste entità, ma non necessariamente ne sanno qualcosa sull'entità.</sample>
    <sample id="464">"Ecco cosa facciamo: mostriamo alcune conoscenze di background sulle due entità. Per le canzoni, semplicemente mostriamo un collegamento di ricerca Google per ogni canzone."</sample>
    <sample id="465">"Ecco, chiedere agli annotatori di ascoltare almeno alcune di ogni canzone e leggere sulla canzone. Ecco ad esempio il risultato della ricerca Google per la canzone EasyHunt."</sample>
    <sample id="466">"E per il dominio delle ricette e dei libri, mostriamo del testo di sfondo da Wikipedia. Per le ricette, mostriamo inoltre le loro immagini da Wikipedia, affinché gli annotatori sappiano come appaiano."</sample>
    <sample id="467">Non ho capito cosa sia il contenuto originale, ma posso provare a tradurre il testo in italiano. Ecco il risultato:

"Non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non avere</sample>
    <sample id="468">Non ho capito cosa stia cercando di dirmi, ma sembra che il testo sia in inglese e non abbia alcun significato. Se vuoi, posso cercare di tradurlo per te, ma non so cosa stia cercando di comunicare.</sample>
    <sample id="469">Il corpus L entities ha 6000 domande alternative in tre domini e 42.000 espressioni di riferimento indirette. I risultati con il modello T5 grande sono riassunti di seguito.</sample>
    <sample id="470">Se il modello di linguaggio ha accesso allo stesso know-how di background degli annotatori, allora l'accuratezza è molto alta, attorno al 92% al 95%. Ma questo non è realistico.</sample>
    <sample id="471">Se non abbiamo il modo di ascoltare il suono originale, non possiamo capire cosa stiamo ascoltando, quindi non possiamo tradurre il testo.</sample>
    <sample id="472">Se il modello di linguaggio ha accesso solo ai nomi di entità, allora l'accuratezza è solo del 60%. Ci sono quindi molto spazio per migliorare. Abbiamo anche dimostrato che i modelli sono generalizzabili per dominio. Ecco il link al nostro dataset. Grazie.</sample>
    <sample id="473">The approach is compared with existing preparer strategies, specifically the weight-key strategy and local agreement, as well as state-of-the-art architectures tailored for stream-on-thigh-respirations translation.</sample>
    <sample id="474">The authors are affiliated with the University of Toulouse, France.</sample>
    <sample id="475">Jenny.</sample>
    <sample id="476">2</sample>
    <sample id="477">"Ciao, sono Sara Pappi dell'Università di Trento e della Fondazione Bruno Kessler, e presenterò brevemente il lavoro "L'attenzione come guida per la traduzione simultanea" che è un lavoro congiunto con Matteo Negri e Marco Turchi."</sample>
    <sample id="478">"Cosa è la Traduzione Simultanea del Discorso? La traduzione simultanea del discorso, o SIMUL-ST, è il processo di traduzione di una lingua parlata in un testo in un'altra lingua in tempo reale, abilitando la comunicazione interlinguistica."</sample>
    <sample id="479">"Il problema attuali dei modelli di stimolo sono quelli che introducono moduli aggiuntivi da ottimizzare."</sample>
    <sample id="480">Procedimenti di addestramento lunghi e complessi, ad esempio addestramento che coinvolge obiettivi di ottimizzazione diversi.</sample>
    <sample id="481">"Ecco la formazione e il mantenimento di diversi modelli per raggiungere diversi regimi di latenza. Ad esempio, formazione di un modello con una latenza media di un secondo e un altro con due secondi di latenza e così via."</sample>
    <sample id="482">La soluzione è...</sample>
    <sample id="483">"Prima, utilizzare modelli offline SD esistenti senza rireadattarli o adottare una specifica architettura per un singolo SD. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici."</sample>
    <sample id="484">"Ecco come utilizzare il know-how già acquisito dal modello attraverso la meccanica di attenzione tra l'input audio e l'output testuale, ovvero la meccanica di attenzione incrociata. E potete vedere un esempio a destra."</sample>
    <sample id="485">"La nostra soluzione è proporre un codice o encoder di attenzione e una strategia per decidere se emettere o meno una traduzione parziale in base a dove i punti di attenzione si dirigono."</sample>
    <sample id="486">"Una parola viene emessa se la tensione non è concentrata, cioè se questo somma è inferiore a un certo livello di soglia alpha, verso gli ultimi frame di pitch di lambda, il che significa che l'informazione ricevuta è sufficientemente stabile."</sample>
    <sample id="487">Ecco il testo tradotto in italiano: "Ad esempio, se riceviamo un file di testo che contiene il testo 'I'm going to talk about' e il nostro modello predice la traduzione in tedesco."</sample>
    <sample id="488">"Ecco, esamineremo i pesi di cross-attenzione."</sample>
    <sample id="489">"Vedremo che i primi due punti si riferiscono ai frame di parola più antichi ricevuti, mentre l'ultima parola si riferisce ai frame di parola più recenti ricevuti come frame di parola lambda."</sample>
    <sample id="490">"Questo significa che i primi due vocaboli saranno omessi."</sample>
    <sample id="491">"Mentre la somma dell'attenzione incrociata è sopra un certo livello alpha, non emetteremo l'ultimo parola e aspetteremo un'altra porzione di discorso."</sample>
    <sample id="492">Se proseguiremo e riceveremo un'altra speech tank e il nostro modello predirà altre tre parole e guarderemo questi pesi di cross-attenzione.</sample>
    <sample id="493">"Vedremo che nessun punto si riferisce ai frame di discorso lambda più recenti."</sample>
    <sample id="494">Non ho ricevuto alcun testo in inglese da tradurre in italiano.</sample>
    <sample id="495">"Se guardi i principali risultati di questo,..."</sample>
    <sample id="496">"Tracciamo i risultati della traduzione nello spazio simultaneo su grafici, in cui abbiamo il blu da un lato che misura la qualità della traduzione e la latenza media."</sample>
    <sample id="497">"Ecco la misura del ritardo e consideriamo anche la media ponderata del gradimento computazionale che tiene conto del tempo di elaborazione del modello per predire l'output."</sample>
    <sample id="498">Vogliamo che le curve siano il più alte possibile in questo grafico.</sample>
    <sample id="499">Ma anche vogliamo che siano spostati a sinistra.</sample>
    <sample id="500">"E confrontiamo le strategie di preparazione con quelle applicate a modelli offline, ovvero la strategia dei pesi chiave e l'accordo locale. E confrontiamo anche con l'architettura di punta dello stato dell'arte specificamente progettata per la traduzione di steam-on-thigh-respirations."</sample>
    <sample id="501">"Questi sono tutti i risultati della strategia di traduzione simultanea in tedesco."</sample>
    <sample id="502">"Ecco che notiamo che le forme di output degli adulti applicano tutte le strategie utilizzate dai modelli offline, poiché le curve sono spostate a sinistra."</sample>
    <sample id="503">"E anche vediamo che se consideriamo il tempo effettivo trascorso o il tempo computazionale consapevole, cioè la strategia più veloce."</sample>
    <sample id="504">Se desiderate scoprire ulteriori risultati, leggete il nostro articolo. Abbiamo anche rilasciato in open source il codice e i modelli e l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per la vostra attenzione.</sample>
    <sample id="505">No, the data set is not publicly available.</sample>
    <sample id="506">"Ciao a tutti, mi chiamo Ying e mio collega Zhiyang e presenteremo la nostra ricerca sul multi-instruct, migliorando l'apprendimento spirituale multi-modello mediante regolazione dell'insegnamento."</sample>
    <sample id="507">Con gli avanzamenti nei modelli di linguaggio, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento per utilizzare modelli di linguaggio pre-addestrati per diverse attività downstream in modo efficiente in termini di parametri e dati.</sample>
    <sample id="508">"Recenti studi hanno dimostrato che l'addestramento per istruzioni consente ai modelli di linguaggio di grandi dimensioni di eseguire compiti non visti in modo zero-shot seguendo istruzioni naturali."</sample>
    <sample id="509">"Tuttavia, la maggior parte dei lavori precedenti sulla regolazione delle istruzioni si è concentrata sulla miglioramento delle prestazioni zero-shot per compiti linguistici, mentre gli compiti di visione computerizzata e multimodale sono stati trascurati."</sample>
    <sample id="510">"Quindi, in questo lavoro, vogliamo indagare se l'adattamento delle istruzioni sui modelli proteici multimodali può migliorare la generalizzazione a compiti multimodali non visti."</sample>
    <sample id="511">"Inoltre, durante la nostra ricerca, abbiamo scoperto una considerevole divergenza nella disponibilità di set di istruzioni tra un RLP e un modello multi."</sample>
    <sample id="512">Esistono più di 1.600 compiti di istruzione esclusivamente basati sul linguaggio. Tuttavia, non esiste alcun compito di istruzione a più modi disponibile a larga scala e di pubblico. Pertanto, ciò ci motiva a creare un insieme di dati di tuning per l'insegnamento a più modi.</sample>
    <sample id="513">"Ecco il multi-instruct, il primo benchmark di addestramento multi-modale che comprende 62 compiti diversi coprendo 10 categorie di board."</sample>
    <sample id="514">"E questi compiti sono derivati da 21 dataset open source esistenti e ogni compito è dotato di cinque istruzioni scritte dagli esperti."</sample>
    <sample id="515">"Per lo studio di tuning delle istruzioni multimodal sul nostro insieme di dati proposto, utilizziamo OFA, un modello di rappresentazione multimodale unificato come modello base. OFA utilizza un vocabolario unificato per token di linguaggio, token di immagine e coordinate di un rettangolo di bounding box."</sample>
    <sample id="516">"Ecco alcuni esempi di istanze dal nostro dataset multilingue."</sample>
    <sample id="517">"Unificare l'elaborazione di diversi tipi di dati di input e output."</sample>
    <sample id="518">Abbiamo seguito il metodo di OFA e abbiamo formulato tutti i compiti in un formato di sequenza a sequenza, in cui il testo d'ingresso, le immagini, le istruzioni e le caselle di rilevamento sono rappresentati nello stesso spazio di token.</sample>
    <sample id="519">Ora sto parlando di regolazione delle istruzioni multi-modal.</sample>
    <sample id="520">"Per il set di dati di training, utilizziamo 53 compiti da 9 gruppi per il training e campioniamo 10.000 istanze per compito. Per il testing, riserviamo l'intero gruppo di ragionamento comune per il testing e selezioniamo ulteriori 5 compiti dai gruppi VQA e miscellaneo."</sample>
    <sample id="521">"Utilizziamo tutte le istanze del flotte di test per ogni compito. Inoltre, estraiamo a caso 20 compiti dalla flotta di test di istruzioni naturali come compiti in loco per l'NLP."</sample>
    <sample id="522">"Usiamo un modello pre-addestrato OFA grande come modello base. Durante l'addestramento, creiamo tutte le istanze per tutti i compiti. Ogni istanza è combinata casualmente con uno dei cinque modelli di istruzioni."</sample>
    <sample id="523">"Durante il test, per ogni compito, eseguiamo un totale di cinque esperimenti valutando il modello utilizzando uno dei cinque istruzioni in ogni esperimento."</sample>
    <sample id="524">Riportiamo la media e il massimo delle prestazioni e la deviazione standard delle prestazioni su tutti e cinque gli esperimenti.</sample>
    <sample id="525">Se il compito è una classificazione multi-modello, riportiamo l'accuratezza. Se è un compito di generazione multi-modello, riportiamo il coefficiente di correlazione di Spearman. Per un compito di retrieval-based, riportiamo il coefficiente di correlazione di Spearman anche in questo caso.</sample>
    <sample id="526">"Abbiamo anche introdotto metriche di valutazione aggiuntive chiamate sensibilità. Ciò misura l'abilità del modello di produrre output coerenti per lo stesso compito, indipendentemente dalle piccole variazioni nella formulazione delle istruzioni."</sample>
    <sample id="527">"Ecco i nostri risultati principali. Come possiamo vedere, l'adattamento delle istruzioni può migliorare significativamente il rendimento delle prestazioni di OIS in compiti multi-modello."</sample>
    <sample id="528">"Anche il trasferimento dell'apprendimento da set di dati di istruzioni naturali può migliorare l'adattamento delle istruzioni."</sample>
    <sample id="529">"Ecco come aumentano le attività, il modello raggiunge un miglioramento delle prestazioni e, nel contempo, una maggiore sensibilità diminuisce."</sample>
    <sample id="530">"Era anche un esperimento in cui abbiamo usato una istruzione contro cinque istruzioni. Come possiamo vedere, utilizzare più istruzioni può migliorare le prestazioni complessive dei modelli e ridurre la sua sensibilità molto."</sample>
    <sample id="531">"Ecco come si può vedere l'effetto delle diverse strategie di tuning sulle prestazioni della modello. Notiamo che, mediante l'apprendimento trasferito da set di dati di istruzioni naturali, il modello può raggiungere una maggiore sensibilità rispetto al modello OFA originale."</sample>
    <sample id="532">"Inoltre, l'apprendimento transferto da un insieme di dati di istruzione naturale può aiutare OFA a raggiungere un miglioramento significativo sul set di dati di istruzione naturale."</sample>
    <sample id="533">"In sintesi, abbiamo proposto il primo grande insieme di dati di addestramento multi-modale. Abbiamo significativamente migliorato la capacità di soglia dell'OFA e abbiamo esplorato diverse tecniche di apprendimento trasferito e dimostrato i loro benefici. Abbiamo anche progettato un nuovo metrica chiamata sensibilità."</sample>
    <sample id="534">"Un'altra cosa, stiamo raccogliendo un insieme di dati di tuning per istruzioni multi-modelli più ampio con circa 150 compiti linguistici Weiren aggiuntivi e li pubblicheremo. Grazie. Questo è un codice QR per i nostri dati e il modello."</sample>
    <sample id="535">The authors of the paper are affiliated with the University of Trento and Fondazione Bruno Kessler.</sample>
    <sample id="536">Jawad Hosseini.</sample>
    <sample id="562">"Ciao a tutti, sono Kostav Sinha e sono lieto di darvi il benvenuto al nostro talk sul nostro articolo del 2023, 'I parametri di accettazione dei modelli linguistici non sono sempre robusti al contesto".</sample>
    <sample id="563">"C'è un lavoro congiunto con John Wothier, Aaron Mueller, Kanishka Mishra, Karen Fentus, Roger Levy e Adina Williams."</sample>
    <sample id="564">Riassumiamo il paradigma dei minimal pair.</sample>
    <sample id="565">Il paradigma di minimal paired valuta i modelli di linguaggio in base alle valutazioni di accettabilità, che possono anche includere grammaticalità come sintassi di blimp o accettabilità in termini di stereotipi come spazi di crowd.</sample>
    <sample id="566">E nel paradigma a coppia minima, il modo tipico per valutare i modelli linguistici è mostrare una frase accettabile o una frase grammaticale, e poi mostrare una frase accettabile o una frase non grammaticale.</sample>
    <sample id="567">"E poi si spera che il modello assegni una probabilità maggiore all'area accettabile."</sample>
    <sample id="568">Il pipeline MPP attuale non ci consente di valutare l'accettazione dei modelli verso frasi più lunghe.</sample>
    <sample id="569">In questi giorni, i modelli di linguaggio stanno generando finestre di contesto sempre più lunghe. È quindi fondamentale valutare l'accettabilità dei modelli all'interno della finestra di contesto.</sample>
    <sample id="570">"Ecco ciò che stiamo cercando di fare. Stiamo cercando di riprendere il flusso di lavoro del modello chiedendogli di valutare l'accettabilità su sequenze più lunghe."</sample>
    <sample id="571">"Ecco il metodo. Quello che facciamo è riprendere quei dati stessi e ricreare cellule di frase scegliendo frasi accettabili o inaccettabili da quei dati stessi."</sample>
    <sample id="572">"Ecco un esempio di coppia di dati proveniente dal set di dati del blimp del caso isola di adiacenza."</sample>
    <sample id="573">"Ecco cosa facciamo: ricreiamo sequenze più lunghe e selezioniamo quelle accettabili e con la stessa struttura grammaticale, estraiamo frasi grammaticali da AdjunTile."</sample>
    <sample id="574">"E quindi aggiungemmo come prefisso a entrambe la query accettabile e la query non accettabile."</sample>
    <sample id="575">Possiamo fare lo stesso anche scegliendo frasi inaccettabili dallo stesso matching. E questo potrebbe anche essere utilizzato per testare l'accettabilità del modello.</sample>
    <sample id="576">"E possiamo fare lo stesso scegliendo frasi da un insieme o un insieme di dati diversi. Quello che chiamiamo scenario di mismatch."</sample>
    <sample id="577">"Ecco che le frasi provengono da set di dati rilevanti, ma non dallo stesso set di dati che state valutando. E possiamo fare lo stesso per le frasi non accettabili."</sample>
    <sample id="578">Infine, possiamo scegliere frasi da un dominio completamente diverso come Wikipedia.</sample>
    <sample id="579">"Questo ci dirà se le valutazioni di accettabilità dei modelli sono effettivamente influenzate da qualsiasi contesto."</sample>
    <sample id="580">"Se il contesto deriva da un insieme di dati diverso o se è completamente irrilevante rispetto alla frase che stiamo analizzando."</sample>
    <sample id="581">Così come funziona il modello? Ecco, guardiamo le frasi del Wikipedia che sono del tutto irrilevanti per il coppia di query attuale. E lì troviamo che i giudizi MPP sono prevalentemente robusti per contesti arbitrari come.</sample>
    <sample id="582">"Aumentammo la lunghezza del contesto fino a 1024 per massimizzare i modelli OPT e GPT2. E come si può vedere nella riga gialla, le valutazioni MPP sono relativamente stabili."</sample>
    <sample id="583">Quando scegliamo frasi dallo stesso insieme di dati, cosa succede?</sample>
    <sample id="584">Ecco la traduzione italiana: "Ecco che ci troviamo a scegliere o creare frasi dalle aree accettabili e non accettabili dallo stesso set di dati di sintassi gemello."</sample>
    <sample id="585">"Ecco che gli giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o inaccettabili."</sample>
    <sample id="586">Ma quando mettiamo a confronto la struttura, è quando scegliamo le frasi dalle stesse fenomenologie nel testo di accusa, Jim.</sample>
    <sample id="587">"Rileviamo un aumento o una diminuzione massiccio del giudizio MPP per il modello in base a se il prefisso scelto è accettabile o inaccettabile."</sample>
    <sample id="588">"Ecco, questo e questo è molto grande, come questo effetto aumenta lungo la lunghezza del contesto, e questo probabilmente influirebbe su modelli linguistici più recenti che hanno una finestra di contesto ampia."</sample>
    <sample id="589">Perché il prefisso del match influisce così tanto sul giudizio del modello linguistico?</sample>
    <sample id="590">"Eseguiamo una serie di analisi in cui abbiamo cercato di riprodurre la frase d'ingresso cercando di preservare la struttura rilevante, ma aggiungendo rumore all'ingresso. Dopo aver eseguito diversi di questi perturbazioni,"</sample>
    <sample id="591">"Ritroviamo che nessuno di questi rumori cambia in realtà il comportamento del modello in termini di come lo mostra il giudizio MPP."</sample>
    <sample id="592">"Ritroviamo che i modelli sono sensibili alle perturbazioni e alle frasi in modi simili."</sample>
    <sample id="593">"Ecco, perturbando le frasi all'interno del dominio accettabile, osserviamo un aumento simile in tutte le perturbazioni. E quando perturbiamo le frasi all'interno del dominio non accettabile, vediamo una diminuzione delle valutazioni MPP in modo simile."</sample>
    <sample id="594">"Il principali risultati del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti che sono comuni tra le frasi."</sample>
    <sample id="595">E l'analisi MPP, come la facciamo attualmente con input singoli e brevi centri, potrebbe non catturare pienamente la conoscenza astratta dei modelli linguistici all'interno della finestra di contesto.</sample>
    <sample id="596">"Leggete il nostro articolo per ulteriori informazioni sugli esperimenti. Grazie per l'ascolto."</sample>
    <sample id="597">The first step of the method maps input tokens to an unordered multi-set of tokens that will appear in the output.</sample>
    <sample id="598">55,000</sample>
    <sample id="626">The best automatic alignment method for German text simplification is messalign.</sample>
    <sample id="627">Robust training algorithms for neural networks can avoid memorizing label noise and improve generalization in weekly supervised learning.</sample>
    <sample id="628">The alignment of the documents in DEplain-web was performed using both manual and automatic methods.</sample>
    <sample id="629">The Carnot++ dataset was created by collecting articles from Reuters News in 2020 and annotating them using the same guidelines as the Carnot 2003 dataset.</sample>
    <sample id="630">"Ciao a tutti, mi chiamo Yusin Zhang dell'Università della Pennsylvania. Oggi presenterò il nostro lavoro, 'Ghosts and Money Parsing in Multiple Natural Languages and Mainline Representations'.</sample>
    <sample id="631">"La elaborazione semantica è un compito per creare rappresentazioni semantiche di query degli utenti come SQL e calcolo lambda."</sample>
    <sample id="632">"Estrarre significati multilingue è il compito di tradurre le query in più lingue naturali in rappresentazioni di significato multiple."</sample>
    <sample id="633">"Come mostrato in questo figura, dobbiamo tradurre la query in diversi linguaggi naturali utilizzando modelli neurali in SQL, Lambda o FunQL, e via dicendo."</sample>
    <sample id="634">"I modelli di parsing semantico bilingue esistenti sono stati proposti e valutati separatamente su insiemi di dati di limitate attività e applicazioni, ad esempio"</sample>
    <sample id="635">"C'è una copertura lacunosa su certe lingue naturali. La cinese manca."</sample>
    <sample id="636">"A causa di copertura su certe molte rappresentazioni."</sample>
    <sample id="637">"La calcolo lambda manca."</sample>
    <sample id="638">"O o sono valutati solo su certi modelli più recenti. Ad esempio, c'è solo un solo modello per valutare il modello."</sample>
    <sample id="639">"Ecco il nostro proposito: forniamo un insieme di esempi uniforme per l'analisi semantica cross-linkata in più lingue naturali e rappresentazioni di significato."</sample>
    <sample id="640">"Contiene 90 dataset in domini virali, 570 parti in tossici, 80 milioni di rappresentazioni e 22 lingue naturali in 15 famiglie linguistiche."</sample>
    <sample id="641">"E per valutare meglio il nostro benchmark, consideriamo sei impostazioni per l'addestramento e l'evaluazione."</sample>
    <sample id="642">"La prima è un test di traduzione. Utilizziamo l'API di Google Translate per tradurre il testo di origine nella lingua di destinazione, quindi utilizziamo il modello monolingue per addestrare qualsiasi valutazione."</sample>
    <sample id="643">E ad esempio, addestriamo il modello inglese sulle query inglese. E durante l'inferenza, traduciamo la query tedesca in inglese utilizzando l'API e quindi utilizziamo il modello addestrato per predire la SQL.</sample>
    <sample id="644">E anche testiamo il modello monolingual.</sample>
    <sample id="645">Non ho ricevuto alcun contenuto in inglese da tradurre.</sample>
    <sample id="646">"Abbiamo anche testato l'impostazione di campo monolingua facendo addestrare il modello con solo il 10% dei dati di addestramento."</sample>
    <sample id="647">"Ecco un modello multilingue che è stato addestrato per tutte le lingue."</sample>
    <sample id="648">Ecco la traduzione del testo:

"Ad esempio, abbiamo unito le query in inglese e cinese per addestrare un modello multilingue. E durante l'inferenza, possiamo utilizzare questo modello per"</sample>
    <sample id="649">"I'd be happy to translate German queries or Chinese queries, etc."</sample>
    <sample id="650">E anche consideriamo il codice zero e il trasferimento campo-corto. Addestriamo su un linguaggio sorgente e trasferiamo in un altro linguaggio.</sample>
    <sample id="651">"E durante l'addestramento, stiamo addestrando le query inglese o la combinazione di query inglese e tedesca Fuscheout per addestrare un modello multilingue e prevedere l'output SQL."</sample>
    <sample id="652">"Ecco che troviamo molti risultati interessanti. Quindi, riguardo all'analisi dei modelli monolingui, valutiamo due gruppi di modelli."</sample>
    <sample id="653">"Dammi una traduzione in italiano del contenuto in inglese."

Il contenuto in inglese è: "Including encoder PDR which stands for multilingual pre-trained encoders with pointer-based decoders such as XL-1R plus PDR and BERT plus PDR."</sample>
    <sample id="654">"Evaluiamo anche modelli encoder-decoder, ovvero modelli encoder-decoder multilingui, come M-BART e MT5."</sample>
    <sample id="655">"Abbiamo scoperto che l'encoder-decoder ottiene il miglior rendimento su tutti e nove set di dati."</sample>
    <sample id="656">"Evaluoamo su MT5 e esempio XLMR più PDR in ambiente multilingue."</sample>
    <sample id="657">Abbiamo scoperto che l'encoder decoder o encoder PDR può essere migliorato mediante l'addestramento in una miscela di varie lingue.</sample>
    <sample id="658">"Ecco che abbiamo scoperto che la maggior parte delle lingue naturali può ottenere un miglioramento delle prestazioni, eccetto che l'inglese, che peggiora in sette dataset e migliora solo in tre dataset."</sample>
    <sample id="659">"Credo che questo si chiami il Canto della Multilinguismo."</sample>
    <sample id="660">"Anch'esso confrontiamo il divario di prestazione interlinguistica."</sample>
    <sample id="661">"In questo grafico, la linea blu rappresenta il trasferimento di campo a campo tra lingue. La linea arancione rappresenta il trasferimento zero a zero tra lingue. La linea verde rappresenta l'impostazione dell'angolo del modello."</sample>
    <sample id="662">"Abbiamo scoperto che confrontando la linea verde e la linea arancione, abbiamo trovato che la differenza di prestazione di trasferimento per impostazione zero è significativa. E confrontando la linea blu e la linea arancione, abbiamo trovato che la differenza di trasferimento si accorcia rapidamente per impostazione a pochi shot."</sample>
    <sample id="663">"Troviamo anche altri interessanti risultati. Ad esempio, i progressi sulle prestazioni encoder-decoder sono stati raggiunti con risultati comparabili, in particolare per il linguaggio naturale inglese, e hanno significativamente migliorato le prestazioni sulle lingue naturali target."</sample>
    <sample id="664">"E abbiamo trovato modelli di linguaggio di modello come CodeBERT sono ancora in quel grid per compiti di parsing semantico bilingue."</sample>
    <sample id="665">"In sintesi, costruiamo ExamPolar, un benchmark unificato per l'analisi semantica a angolo multiplo con molte lingue naturali e diverse rappresentazioni."</sample>
    <sample id="666">"Conduciamo uno studio di benchmark esteso su tre rappresentanti di modelli di linguaggio multilingue e i nostri risultati mostrano molti findings interessanti, eccetera. E benvenuti a visitare il nostro paper e il nostro codice. Grazie per l'ascolto."</sample>
    <sample id="667">The four categories are: Architecture, Literature, Art, and Music.</sample>
    <sample id="668">No, according to the text, LLMs like Codex or Bloom are not sufficient for cross-lingual semantic parsing tasks, as they are still in the grid for such tasks without modeling the goal.</sample>
    <sample id="695">The method addresses permutation ambiguity by inducing alignment as part of the training process.</sample>
    <sample id="696">In this context, the fairness of a language model is referred to as "fairness issues resulting from language model political meanings".</sample>
    <sample id="697">Yannis Lavraque.</sample>
    <sample id="698">Kostav Sinha.</sample>
    <sample id="699">Myra.</sample>
    <sample id="700">In the context of this article, tropicalism refers to a trope that associates Latina women with vibrant and curvaceous qualities.</sample>
    <sample id="701">The authors have elaborated human representations of target groups by defining them through words like "culture", "tradition", "proud", and "exotic", which emphasize their distinct identity and differentiate them from the white norm.</sample>
    <sample id="702">CXMI (Contextualized Mutual Information) was used to measure context usage.</sample>
    <sample id="703">DrBERT is a version of BERT trained on a large corpus of text (7GB) and SchuBERT is a clinical model trained on a smaller subset of text (4GB) with a focus on clinical notes.</sample>
    <sample id="751">2</sample>
    <sample id="752">Iterative learning transfer refers to the process of updating a model by training on the latest set of data collected during active learning and annotations.</sample>
    <sample id="753">I couldn't find any English content in the given text. The text appears to be a repetitive sequence of words in Farsi, which is a language spoken in Iran. It seems to be a form of chanting or a mantra, but it doesn't convey any meaningful information. Therefore, I cannot determine the objective of the dataset.</sample>
    <sample id="754">Un utente malintenzionato può estrarre i parametri del modello attraverso un EaaS (Embedding as a Service) visualizzando le immagini generate dalle sentenze su un dataset BOPCA e analizzando la legenda delle figure che rappresenta il numero di trigger in ogni frase.</sample>
    <sample id="755">3</sample>
    <sample id="756">4 annotatori.</sample>
    <sample id="757">The affiliations of the authors are: Carnegie Mellon University, University of Washington, and the Allen Institute for AI.</sample>
    <sample id="758">The example mentioned is "I saw Bart and Lisa".</sample>
    <sample id="759">The advanced chat models are Conversational AI, Dialogflow, and Rasa.</sample>
    <sample id="760">Because large language models are generating longer responses, it's crucial to evaluate their acceptability throughout the entire context window.</sample>
    <sample id="761">Sì.</sample>
    <sample id="762">Yes.</sample>
    <sample id="763">The metric used for evaluation is not specified.</sample>
    <sample id="764">Sì.</sample>
    <sample id="765">Design bias in NLP is important because it can lead to inaccurate results, as seen in the example of Perspective API not detecting toxic comments in Indian contexts, which may result in missed or misclassified instances, affecting the overall performance and reliability of the model.</sample>
    <sample id="766">The multilingual LLMs like BLOOM have been fine-tuned using adapters or full-fine-tuning.</sample>
    <sample id="767">They resort to the transfer learning model.</sample>
    <sample id="768">The recent test sets used to evaluate PaLM's capabilities include the GLUE benchmark, SuperGLUE, and the 21st International Workshop on Machine Translation (IWSLT) evaluation.</sample>
    <sample id="769">3</sample>
    <sample id="770">The proposed method, Corscript, enables more specialized models for constraint language planning, allowing for a higher level of plotism in generated goals.</sample>
    <sample id="771">Xu Heng.</sample>
    <sample id="772">Yes.</sample>
    <sample id="773">1</sample>
    <sample id="774">OFA.</sample>
    <sample id="833">The authors of the paper are from Google Translate.</sample>
    <sample id="834">Vasudha.</sample>
    <sample id="835">The article does not explicitly mention specific language pairs, but it mentions "neural MT metrics" and "human evaluation results", which suggests that it may refer to machine translation (MT) evaluation for multiple language pairs.</sample>
    <sample id="836">Xiangbin.</sample>
    <sample id="837">The models studied during the experiments are the long-impedance model and the normal base model.</sample>
    <sample id="838">For training, 53 tasks from NIGROP are used, and for testing, 20 tasks from the test sheet of natural instruction and 10 tasks from VQA and miscellaneous group are used.</sample>
    <sample id="839">There are no specific authors mentioned in the audio snippet.</sample>
    <sample id="840">The authors conducted experiments on four datasets: AG News, Mind, SSD2, and AresVam.</sample>
    <sample id="876">NACHOS is a dataset of medical ground truth data from the web.</sample>
    <sample id="877">Aydbilar.</sample>
    <sample id="878">The prompting strategy has a significant influence on the performance of LLMs for translation, as shown in a simple experiment with two different prompts for a single sentence.</sample>
    <sample id="879">The authors of the article are: Kaio Yan, Patrick Frenange, M.E. Liu, Andre F.D. Martin, and Graham Mubig.</sample>
    <sample id="880">There are no 5 instructions written by experts in the provided audio.</sample>
    <sample id="881">The authors propose a coreference resolution task to test models' ability to draw on knowledge from different sources.</sample>
    <sample id="882">"Ciao a tutti. Mi chiamo Aydbilar e vi darò un breve riassunto del paper "Pattern di grugnito da traduzione, strategie e prestazioni". È un lavoro congiunto con i miei colleghi da Google Translate."</sample>
    <sample id="883">"Ecco, BAM è un modello di 540 miliardi di parametri-presente presentato lo scorso anno nel 2022. È stato addestrato su una grande raccolta di testi che comprende 180 miliardi di documenti."</sample>
    <sample id="884">"Nel campo della Damaa per la Kitchen, raggiunge lo stato dell'arte in centinaia di compiti di elaborazione del linguaggio naturale."</sample>
    <sample id="885">"In questo lavoro, presentiamo lo studio sistematico dei suggerimenti per modelli linguistici per la traduzione automatica."</sample>
    <sample id="886">"Evaluammo la capacità di transizione di tali modelli utilizzando le migliori pratiche della comunità di MT. Ciò comporta l'utilizzo dei set di test più recenti per evitare un'altra iterazione dei dati di test con i dati di addestramento del modello di lingua."</sample>
    <sample id="887">"E e confrontiamo due sistemi di punta. Quelli che hanno ottenuto i migliori risultati sono quelli valutati dal WMT."</sample>
    <sample id="888">"Utilizziamo metriche di traduzione di tipo neuronale di ultima generazione e presentiamo anche risultati di valutazione umana basati su esperti. Infine, forniamo alcune raccomandazioni per strategie di selezione di promemoria."</sample>
    <sample id="889">Il prompting ha un grande influenza sulle prestazioni dei modelli di linguaggio limitati (LLMs) per la traduzione. Come possiamo vedere in un esperimento semplice in cui utilizziamo un prompting a una volta sola e forniamo due differenti prompts per una sola frase.</sample>
    <sample id="890">La maggior parte delle frasi, 516 su 1000, la differenza osservata è di più di un punto di sfumatura.</sample>
    <sample id="891">E ciò può arrivare fino a 40 punti di sfumatura. Quindi è importante selezionare una strategia di stimolazione adeguata.</sample>
    <sample id="892">"In esperimenti, salutiamo una strategia di cinque colpi di stimolo dove semplicemente segniamo la frase che forniamo al sistema con il linguaggio in cui è scritta."</sample>
    <sample id="893">"In questo esempio qui, dove eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche sono contrassegnate con la colonna tedesca e le traduzioni inglesi con la colonna inglese."</sample>
    <sample id="894">"Abbiamo visto che la forma reale della stampa non ha un grande influenza nel caso di stampa a breve serie."</sample>
    <sample id="895">"E' fondamentale per il prompting a zero e a uno. E quando andiamo, come nel nostro caso, a cinque-shot prompting, non c'è quasi nessuna differenza nella forma del prompting."</sample>
    <sample id="896">"E sono gli esempi che portano il peso principale."</sample>
    <sample id="897">Il riassunto dei nostri risultati sperimentali è che la qualità dell'esempio è più importante della somiglianza con la frase di riferimento.</sample>
    <sample id="898">"E' importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo le selezioni di prompt dai dati di training delle valutazioni WMT o dai dati DEF."</sample>
    <sample id="899">"Il dato di profondità è molto più curato e di qualità più alta rispetto al dato addestrato, quindi il risultato è migliore utilizzando i dati di profondità."</sample>
    <sample id="900">Ora, almeno, i sistemi di ultima generazione hanno un vantaggio sostanziale rispetto ai traduttori tradizionali. Ma uno viene abbastanza vicino a un sistema commerciale. Nel nostro caso, abbiamo scelto di collaborare con Google Translate.</sample>
    <sample id="901">"Il contenuto delle nostre indagini sull'EMAIL regolamentato che abbiamo eseguito utilizzando il framework MQM è che la fluidità della palma è paragonabile ai sistemi di punta di artificio, ma la differenza principale deriva dalla precisione."</sample>
    <sample id="902">"In particolare, gli errori più comuni sono gli errori di omissione."</sample>
    <sample id="903">E sembra che Palm scelga di produrre una traduzione più accattivante, a volte eliminando parti della frase originale che sono state create durante la traduzione.</sample>
    <sample id="904">"Tuttavia, lo stile-outward per PAN è inferiore rispetto ai sistemi di stato dell'arte, il che è un segnale aggiuntivo."</sample>
    <sample id="905">"Parm restituisce un output molto fluente, ma ancora con alcuni problemi di accuratezza."</sample>
    <sample id="906">"Ecco tutto per questo breve riassunto. Per ulteriori dettagli, vi invitiamo a seguire la presentazione completa del lavoro. Grazie mille."</sample>
    <sample id="907">"Ciao, sono Dawei, studente di dottorato all'Università di Salant in Germania. In questo video, vorrei presentare il nostro lavoro recente, "Più di quanto sembra", un'analisi critica sulle attrezzature settimanali."</sample>
    <sample id="908">"Questo è un lavoro congiunto con Xiao Yuxian, Mario Smoothbath e Diaz Stefan e DTich Claco."</sample>
    <sample id="909">"Vorrei iniziare con una breve introduzione all'apprendimento supervisione debole e all'apprendimento supervisionato debolmente."</sample>
    <sample id="910">In supervisione debole, non etichettiamo manualmente i dati. Invece, utilizziamo fonti di etichettatura deboli, come regole semplici di heuristiche, basi di conoscenza o sorgenti di codice di località, come illustrato nella figura a destra.</sample>
    <sample id="911">"Le annotazioni deboli sono molto più economiche rispetto alle annotazioni umane, ma sono anche rumorose, il che significa che un certo numero di annotazioni è sbagliato."</sample>
    <sample id="912">"Se addestriamo reti neurali direttamente con dati etichettati settimanali, le reti neurali tendono a memorizzare il rumore di etichetta e non generalizzano."</sample>
    <sample id="913">"Il training degli algoritmi supervisionati settimanali è stato proposto per addestrare reti neurali in modo robusto anche in presenza di rumore, in modo che i modelli di training generalizzino bene."</sample>
    <sample id="914">"Nelle ultime opere in WSL, dove WSL sta per apprendimento supervisionato settimanale, si afferma spesso che gli autori affermino di addestrare i modelli solo sulle informazioni del lavoro settimanale e raggiungere prestazioni elevate sui set di test puliti."</sample>
    <sample id="915">"La affermazione tecnica non è sbagliata, ma c'è un'eccezione."</sample>
    <sample id="916">"Il fatto è che le persone suppongono che ci sia un ulteriore set di validazione o firewall per la selezione del modello."</sample>
    <sample id="917">"Abbiamo interrotto lo sviluppo di questo problema, poiché questo implica la necessità di annotazioni manuali settimanali da parte di molti. Ma, come un elefante nella stanza, questa necessità è spesso trascurata."</sample>
    <sample id="918">"La domanda precedentemente posta chiede di formulare tre domande di ricerca. In primo luogo, è necessario un set di dati di validazione pulito per l'apprendimento automatico del linguaggio (WSL)? O potremmo forse utilizzare un set di validazione rumoroso invece?"</sample>
    <sample id="919">Secondo, se è richiesto o è necessario avere dati puliti per far funzionare WSL, quanti campioni puliti abbiamo bisogno? Infine, dovremmo utilizzare solo i campioni puliti per la validazione o ci sono modi migliori per utilizzarli?</sample>
    <sample id="920">Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti.</sample>
    <sample id="921">"Iniziamo a scoprire che i metodi WSL recenti richiedono effettivamente campioni di piatto puliti per funzionare correttamente."</sample>
    <sample id="922">Altrimenti, c'è un calo di prestazioni significativo. Come mostrato in questa figura, se non ci sono campioni di validazione puliti, i modelli di tendenza non possono generalizzare oltre le etichette di settimana originarie.</sample>
    <sample id="923">"Significa che l'addestramento è inutile."</sample>
    <sample id="924">"Il fatto è che gli approcci WSL richiedono dati etichettati correttamente per funzionare correttamente e il costo dell'etichettatura dei campioni di validazione non dovrebbe essere trascurato."</sample>
    <sample id="925">Il nostro secondo ritrovamento è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci WSL a raggiungere un miglioramento delle prestazioni, come mostrato nella figura sulla sinistra.</sample>
    <sample id="926">Di solito, ci vogliamo 20 campioni per classe per raggiungere un buon livello di prestazioni.</sample>
    <sample id="927">"Ma non è la fine della storia, perché se decidiamo di accedere a campioni puliti, addestrare su di loro direttamente raggiungerà prestazioni ancora migliori."</sample>
    <sample id="928">Il grafico rosso mostra la differenza di prestazione tra gli approcci di fine-tuning, che vengono applicati direttamente sui dati puliti, e gli approcci WSL, che utilizzano i dati puliti solo per la validazione.</sample>
    <sample id="929">"Evidentemente, se abbiamo 10 campioni per classe, l'addestramento fine di Direct comincia a superare gli approcci WSL."</sample>
    <sample id="930">"Infine, l'incremento di prestazioni affermato dagli approcci WSL precedenti può essere facilmente raggiunto consentendo di continuare a fine-tuning sulle campioni di validazione pulite."</sample>
    <sample id="931">"Come possiamo notare dalle figure, il modello Valina denominato FTW inizialmente sottopassa i metodi WSL più complessi come il coseno."</sample>
    <sample id="932">Tuttavia, se consentiamo di continuare a calibrare sulle campioni pulite, il metodo FTW si comporta altrettanto bene rispetto ad altri metodi.</sample>
    <sample id="933">In pratica, non c'è ragione di scegliere metodi WSL più complessi, che richiedono più tempo di calcolo e spazio su disco.</sample>
    <sample id="934">"In sintesi, abbiamo dimostrato che gli approcci WSL recenti richiedono campioni puliti e annotati a mano per funzionare correttamente. Il loro guadagno di prestazioni e praticità sono fortemente soprastimati."</sample>
    <sample id="935">"Il nostro suggerimento concreto per le ore di lavoro future è:"</sample>
    <sample id="936">"Rapporta i criteri di selezione del modello. Ad esempio, rapporta se la selezione del modello viene effettuata utilizzando campioni di validazione puliti."</sample>
    <sample id="937">Secondo, le strategie WSL dovrebbero essere motivate da basi di lancio future come campioni chiari. Terzo, l'adeguamento continuo è una baseline semplice ma forte che dovrebbe essere considerata nel lavoro futuro in WSL.</sample>
    <sample id="938">Infine, abbiamo reso pubblico il nostro codice. Puoi trovarlo mediante il codice QR presente in questa diapositiva. Speriamo che tu lo controlli e ti diverti alla conferenza.</sample>
    <sample id="939">Human evaluation, such as asking judges to select the better conversation or rate them on a Likert scale.</sample>
    <sample id="940">5</sample>
    <sample id="941">In the given example, Servin and Kea, the necessary basic knowledge includes:

* English language
* Basic understanding of conversational dialogue
* Familiarity with the context of a voice assistant providing information.</sample>
    <sample id="942">Yes, the code is available on GitHub.</sample>
    <sample id="943">No, according to the text, GPT-4 is most aligned with people who have a college education or graduate school education, not with specific demographic groups like country or gender.</sample>
    <sample id="944">The sentences were perturbed by adding "noise" to the input while trying to preserve the relevant structure, but none of these perturbations caused the model to change its judgment on MPP.</sample>
    <sample id="945">Evaluating dialogue quality on a dimensional level means assessing multiple aspects of the chat quality to understand the strengths and weaknesses of the model in a more detailed and nuanced way.</sample>
    <sample id="946">The authors' affiliations are: University of Science and Technology of China.</sample>
    <sample id="947">According to the audio, the form of the prompting is crucial in the case of zero and one short prompting.</sample>
    <sample id="978">The authors evaluated conversational AI models.</sample>
    <sample id="979">1</sample>
    <sample id="980">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="981">There are 2 authors involved in the article.</sample>
    <sample id="982">Vasudha.</sample>
    <sample id="983">Adam Szpirkowski.</sample>
    <sample id="1021">Omission errors.</sample>
    <sample id="1022">"Salve, sono James Finch. E sono Sarah Finch. Oggi vi parleremo di ABCeVal, un nuovo approccio dimensionale per valutare l'IA conversazionale."</sample>
    <sample id="1023">"Questo lavoro è stato realizzato dal Laboratorio di Intelligenza Artificiale dell'Università Emory, diretto dal Professore Geno Choi, in collaborazione con Amazon Alexa AI."</sample>
    <sample id="1024">"Okay, let's say that you just developed a dialogue model and you want to see how well it compares against the current state of the art."</sample>
    <sample id="1025">La pratica comune è utilizzare l'evaluazione umana, ad esempio chiedendo a giudici umani di selezionare la conversazione migliore tra due o di valutare le conversazioni su una scala di Lickert.</sample>
    <sample id="1026">Questi approcci funzionano bene per fornire valutazioni globali della qualità del dialogo, ma la qualità del dialogo presenta molte facce. Pertanto, potresti valutare diverse dimensioni della qualità del dialogo per comprendere le forti e le debolezze del modello su un livello più dettagliato.</sample>
    <sample id="1027">"Un approccio è chiedere a giudici umani di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del modello utilizzando metodi di scala comparativa o Likert."</sample>
    <sample id="1028">Tuttavia, crediamo che esista una strategia più precisa e affidabile per l'evaluazione del dialogo dimensionale.</sample>
    <sample id="1029">Il nostro approccio tenta di ridurre la soggettività dell'evaluazione umana annotando esplicitamente se ogni risposta del modello esprime certi comportamenti, come rispondere con informazioni irrilevanti o contraddire se stessa.</sample>
    <sample id="1030">"Ecco che chiamiamo questo approccio annotazione dei comportamenti in chat o ABC eval in breve. Abbiamo sviluppato questo metodo per coprire in modo esaustivo i comportamenti del modello di chat che sono stati suggeriti come influire sulla qualità della chat nella letteratura recente."</sample>
    <sample id="1031">"ABC eval è in grado di misurare i tassi con cui i modelli di chat commetteranno errori tematici."</sample>
    <sample id="1032">Ecco la traduzione:

"Ad esempio, ABC misura il numero di tornate in cui il modello di chat ignora il proprio partner o dice qualcosa di irrilevante."</sample>
    <sample id="1033">"Il contenuto contraddice se stesso o il suo partner, illude fatti falsi o viola la conoscenza comune, e quando il modello ha successo o fallisce a mostrare empatia."</sample>
    <sample id="1034">Per determinare quale tipo di valutazione è più efficace, abbiamo selezionato quattro modelli di chat di punta e li abbiamo valutati su 100 conversazioni tra umani e bot per modello utilizzando ABC eval.</sample>
    <sample id="1035">"Per confrontare, abbiamo valutato queste conversazioni utilizzando tre metodi esistenti, le valutazioni di liquore a livello di turno, le valutazioni di liquore a livello di dialogo e le comparazioni pairwise di dialogo."</sample>
    <sample id="1036">"Abbiamo raccolto valutazioni per ogni metodo esistente su otto delle più comuni aree di valutazione del dialogo, poiché questa è la pratica standard per valutare i modelli di chat su diverse dimensioni."</sample>
    <sample id="1037">"Secondo le nostre analisi dei risultati di valutazione, abbiamo scoperto che gli etichette di comportamento ABC Eval sono generalmente più affidabili delle etichette raccolte dai metodi esistenti, misurate mediante l'Accordo di Interenant sulla conversazione doppia."</sample>
    <sample id="1038">"Inoltre, le etichette di valutazione ABC sono più predittive della qualità complessiva della conversazione rispetto ai metri prodotti dai metodi esistenti, come dimostrato da questo semplice analisi di regressione lineare."</sample>
    <sample id="1039">Ecco la traduzione:

"Ad esempio, puoi vedere come misurare la proporzione di giri con auto-contraddizioni e contraddizioni con il partner spiega il 5% e il 10% della qualità della conversazione, mentre i punteggi di consistenza del liquore spiegano solo il 4% o meno."</sample>
    <sample id="1040">Finalmente, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità del chat utilizzando una regressione lineare a passaggi.</sample>
    <sample id="1041">"Ecco come la combinazione di tutti i metri di valutazione ABC spiega più del 25% della qualità della conversazione. E rimuovendo i metri uno alla volta, la maggior parte di loro porta a perdere un'informazione decente sulla qualità."</sample>
    <sample id="1042">"Invece, la combinazione di tutti i metri di Likert a livello di turno spiega molto meno della qualità e pochi di questi metri contengono informazioni uniche."</sample>
    <sample id="1043">"Questi indicatori ABC fidi, informativi e distinti ci consentono di valutare l'IA conversazionale con una risoluzione più alta rispetto ai metodi precedenti sono in grado di raggiungere."</sample>
    <sample id="1044">"Si può vedere dai risultati del nostro esperimento che alcuni ostacoli ancora persistono e sono stati precisamente quantificati. Ad esempio, i bot che abbiamo testato presentano violazioni di senso comune in circa il 20% delle loro risposte."</sample>
    <sample id="1045">"Producono informazioni irrilevanti in circa il 15% delle risposte e si contraddicono o si contraddicono il loro partner in circa il 10% del tempo."</sample>
    <sample id="1046">"Con il ritmo veloce di miglioramento nel campo, molti di questi tassi di errore potrebbero vedere una riduzione in modelli rilasciati dai nostri ultimi controlli. Tuttavia, ciò è ancora più motivo per perseguire metriche di valutazione affidabili e precise per confrontare i modelli."</sample>
    <sample id="1047">"Speriamo che ABC eval possa essere sfruttato dagli altri nel campo come passo significativo in questa direzione. E ci aspettiamo di vedere come l'IA conversazionale si svilupperà nei prossimi mesi e anni. Grazie per aver guardato."</sample>
    <sample id="1048">The authors are affiliated with the Emory NLP Lab, led by Professor Geno Choi at Emory University, and Amazon Alexa AI.</sample>
    <sample id="1049">In questo articolo, CFT non è menzionato.</sample>
    <sample id="1050">6</sample>
    <sample id="1051">"Ciao, mi chiamo Kaio Yan e presenterò il nostro lavoro intitolato 'Quando la traduzione richiede un contesto? Una esplorazione multilingue guidata dai dati'. Questo lavoro è stato realizzato in collaborazione con Patrick Frenange, M.E. Liu, Andre F.D. Martin e Graham Mubig."</sample>
    <sample id="1052">"Un gran numero di traduzioni dipendono dal contesto. Ad esempio, come tradurremmo "more" in questo enunciato?"</sample>
    <sample id="1053">Be', se la frase precedente era, le cose potrebbero diventare pericolose se i ministri lo scoprissero, allora Moe si riferisce a un agente segreto. Ma se la frase precedente era, potrebbe essere qualcosa di serio, dottore? Allora Moe si riferisce a un segno di nascita.</sample>
    <sample id="1054">"In base al contesto, il significato della parola cambia e quindi il suo traduzione cambia anch'essa."</sample>
    <sample id="1055">"Tuttavia, valutare come bene i modelli possano tradurre casi come questo è abbastanza difficile. In primo luogo, perché solo una piccola porzione delle traduzioni dipende dal contesto, il che rende i metri di corpus a livello di corpus come Blue inadatti a catturare queste traduzioni."</sample>
    <sample id="1056">"E alcuni hanno suggerito valutazioni mirate su traduzioni dipendenti dal contesto, ma questi risorse supportano solo limitati tipi di traduzioni dipendenti dal contesto e insiemi limitati di lingue, poiché solitamente si basano sulla conoscenza del dominio e sulla curatione umana."</sample>
    <sample id="1057">"In questo lavoro, cerchiamo di rispondere a queste due domande. In primo luogo, quando richiede contesto la traduzione? E in secondo luogo, come si comportano i modelli in questi casi?"</sample>
    <sample id="1058">"Per rispondere alla prima domanda, abbiamo iniziato misurando quant'importanza un lemma abbia nel contesto di traduzione."</sample>
    <sample id="1059">"E il lavoro precedente abbiamo introdotto CXMI come misura per l'utilizzo del contesto da parte dei modelli di traduzione automatica. E ciò viene fatto misurando quant'informazioni il contesto C fornisce su Y, dati X."</sample>
    <sample id="1060">"Puoi pensare a CXMI come l'informazione ottenuta dal dare contesto al modello."</sample>
    <sample id="1061">"In questo lavoro, estendiamo CXMI a YCXMI, che può misurare l'utilizzo del contesto a livello di frase o a livello di parola. Possiamo considerare le parole che hanno un alto P6MI come quelle che richiedono un contesto per la traduzione."</sample>
    <sample id="1062">"Analizziamo ora le parole con alto XMI per cercare pattern tra queste parole."</sample>
    <sample id="1063">"Eseguiamo l'analisi sui verbali di conferenze TED che sono state tradotte da inglese in 14 lingue diverse."</sample>
    <sample id="1064">"Eseguiamo il nostro analisi a tre diversi livelli. Iniziamo con le etichette di parte del discorso che hanno un alto valore di PCXMI."</sample>
    <sample id="1065">"Ecco che ci consente di trovare, ad esempio, pronomi duali in arabo che hanno un alto P6MI. E questo può essere spiegato perché l'inglese non ha pronomi duali, quindi è necessario contesto per determinare se un pronome è duale quando si traduce in arabo."</sample>
    <sample id="1066">"Ecco che anche alcuni linguaggi richiedono contesto per scegliere la forma verbale appropriata. Quindi analizziamo gli elementi lessicali che hanno un alto valore di p-sex-mi in tutte le loro diverse occorrenze."</sample>
    <sample id="1067">"Ecco come ci aiuta identificare casi come questo, in cui in cinese, è necessario contestualizzare i nomi propri per assicurarsi di utilizzare la stessa traduzione all'interno del documento."</sample>
    <sample id="1068">E similmente, scopriamo che il contesto è supportato per tradurre nella forma di formalità corretta.</sample>
    <sample id="1069">"E infine, analizziamo i token individuali che hanno un alto P6MI. Ciò ci consente di identificare fenomeni che non possono essere catturati dalla parola stessa, ma sono piuttosto espressi nella struttura del sintagma, come la risoluzione dell'ellissi."</sample>
    <sample id="1070">"E adesso utilizziamo i nostri risultati dall'analisi per progettare un benchmark per la traduzione globale dei documenti."</sample>
    <sample id="1071">"Per ogni fenomeno di discorso che abbiamo identificato, creiamo taggers per identificare automaticamente le parole che si riferiscono al fenomeno. E chiamiamo il nostro tagger il tagger consapevole di discorso multilingue o Muda tagger."</sample>
    <sample id="1072">" Possiamo quindi notare che le lingue diverse hanno diverse proporzioni di fenomeni discorsivi."</sample>
    <sample id="1073">"Inseriamo quindi il MudaTaggle applicando il taggle sul corpus parallelo che vogliamo utilizzare per l'evaluazione. E applichiamo i nostri metrici di traduzione scelti sulle esempi contestuali identificati dal MudaTaggle."</sample>
    <sample id="1074">"E infine, utilizziamo il nostro benchmark, così come altri indicatori, per valutare diversi modelli sul livello del documento traduzione automatica."</sample>
    <sample id="1075">"Innanzitutto, quando utilizziamo metriche a livello di corpus, scopriamo che i modelli acontextuali hanno il miglior rendimento per il blu."</sample>
    <sample id="1076">"Poi se utilizziamo Comet, i modelli a context-aware ottengono i migliori risultati. E se utilizziamo la misura F, allora i modelli con o senza contesto hanno prestazioni comparabili."</sample>
    <sample id="1077">"Ecco un esempio che dimostra che è difficile determinare il miglior sistema di traduzione a livello di documento se si utilizzano solo metriche a livello di corpus."</sample>
    <sample id="1078">Ora utilizziamo il benchmark Mooda per valutare i modelli e scopriamo che i modelli che utilizzano contesto sono significativamente più precisi rispetto a quelli che non lo utilizzano per fenomeni di discorso come la formalità e la coesione lessicale.</sample>
    <sample id="1079">Ma questi modelli non sono molto migliori di quelli che non utilizzano contesto su fenomeni come ellissi, piante perenni e forma verbale. Quindi questo suggerisce dove dovremmo vedere più progressi per la trasformazione a livello di documento.</sample>
    <sample id="1080">"Analizziamo anche sistemi commerciali e i nostri benchmark mostrano che D-Bel è solitamente più preciso del traduttore di Google per traduzioni a livello di documento."</sample>
    <sample id="1081">Riassumendo, eseguiamo un'analisi basata sui dati su 14 coppie di lingue per identificare quando le traduzioni richiedono contesto.</sample>
    <sample id="1082">"E quindi utilizziamo i nostri risultati per creare un benchmark per la traduzione a livello di documento, che ci aiuta a identificare quali modelli di fenomeni a dischi possono gestire bene o meno, e quali sistemi di traduzione sono bravi nella traduzione a livello di documento."</sample>
    <sample id="1083">Grazie mille per la tua attenzione. A presto a Toronto.</sample>
    <sample id="1084">Yusin Zhang.</sample>
    <sample id="1121">The new method is called the "Huffman coding".</sample>
    <sample id="1122">The author described the second part as "marked words", which is a method to identify words that distinguish marked groups from unmarked ones.</sample>
    <sample id="1123">The affiliation of the author is the University of Washington.</sample>
    <sample id="1124">Prague approach.</sample>
    <sample id="1125">James Finch and Sarah Finch.</sample>
    <sample id="1126">4</sample>
    <sample id="1127">Minimal pair paradigm, acceptability judgments, grammaticality, syntax, crowd-sourced data.</sample>
    <sample id="1161">WSL (Weakly Supervised Learning).</sample>
    <sample id="1162">The model is evaluated on 11 biomedical and clinical tests in French.</sample>
    <sample id="1226">The CamemBERT model is initially trained on a dataset of 4 GB of text, using the weight and token of Pomet Bird, on a subject.</sample>
    <sample id="1227">Adam Szpirkowski.</sample>
    <sample id="1228">The experiment showed that the performance degrades with a larger temporal gap, confirming the hypothesis that temporal drift is the main cause of the performance drop.</sample>
    <sample id="1269">It is necessary to permute the tokens to put them in the right order because after the first step, the tokens are not ordered, and another model is used to predict a permutation to put them in the correct sequence.</sample>
    <sample id="1270">Gli autori suggeriscono di aumentare la trasparenza sui metodi di mitigazione dei bias perché non è chiaro se gli esiti positivi siano dovuti a valori di alignment eccessivi o a metodi anti-stereotipici.</sample>
    <sample id="1271">Ungrammatical sentences.</sample>
    <sample id="1272">The authors used the metrics of constraints and pretending, using the weight and token of Pomet Bird, and a subject of 4 GB in nature, which resulted in a comparable outcome to what they obtained with Dr. Bert, 4 GB from scratch.</sample>
    <sample id="1273">Interanitator Agreement.</sample>
    <sample id="1274">Wikipedia.</sample>
    <sample id="1275">The authors of the article are not explicitly mentioned.</sample>
    <sample id="1276">MultiInstruct differs from previous works by focusing on instruction tuning for multimodal protein models, whereas most previous works focused on language-only tasks, and also by investigating the generalization of instruction tuning to unseen multimodal tasks.</sample>
    <sample id="1277">2</sample>
    <sample id="1278">Coordinazione binaria: "The arrangement of words or phrases of equal importance in a sentence, often linked by a conjunction such as 'and' or 'or'."</sample>
    <sample id="1279">According to the transcript, the prompts were used for 3 seconds.</sample>
    <sample id="1280">The results suggest that smaller models like T5 can generate high-quality scripts when properly trained on suitable data, indicating that they can support larger models.</sample>
    <sample id="1281">"Ciao, sono Yannis Lavraque e vi presento i nostri lavori su Dr. Berth, un modello robusto in francese per il dominio biomedico e clinico."</sample>
    <sample id="1282">"In questo presentazione, stiamo discutendo di un linguaggio modellizzato in campo sanitario. Successivamente, presenteremo la principale contribuzione del nostro articolo."</sample>
    <sample id="1283">Abbiamo presentato il primo modello biomedico in francese chiamato Dr. Berth, basato su Roberta e addestrato su NACCHOS, un insieme di dati di riferimento medici raccolti dal web.</sample>
    <sample id="1284">"Abbiamo anche introdotto una comparazione di modelli con diverse retroazioni e fonti di dati. Presenteremo quindi i nostri risultati su 11 biomedici e test clinici in francese."</sample>
    <sample id="1285">"E conclude finalmente gli esperimenti e vi forniamo ulteriori informazioni su come accedere ai modelli."</sample>
    <sample id="1286">Since its release in 2018, Bert has been the most effective approach to natural language processing. It has offered a significant performance gain compared to a historical strategy and a contextualized method, such as "see-to-vec" or "see-send".</sample>
    <sample id="1287">Dall'epoca in cui questo modello è stato adattato a molte altre lingue, come il francese con Camembert, e a diversi settori, come Biomedical con Père Medbert e Biobird, e in clinica con Clinique Albert, ma soprattutto in inglese.</sample>
    <sample id="1288">"Modelli specializzati per altre lingue sono scarsi e spesso basati sul training continuo a causa della mancanza di dati in dominio."</sample>
    <sample id="1289">Tuttavia, il francese non aveva alcun modello open source per la bio-medicina fino ad ora.</sample>
    <sample id="1290">Ci chiediamo una domanda su quali sono le strutture dati più adatte per una vasta gamma di utilizzi e quei dati attuali sono buoni sostituti per i dati clinici.</sample>
    <sample id="1291">Per rispondere a questa domanda, confrontiamo Dr. Bert con il nostro modello di Schubert, basato su dati anonimizzati ottenuti dall'ospedale non generazionale della nostra casa.</sample>
    <sample id="1292">Dopo di che ci chiediamo quanti GB di dati servono per addestrare un modello specializzato su dati francesi? È per Gb o più di Gb?</sample>
    <sample id="1293">Per rispondere a questa domanda, addestriamo e confrontiamo quattro modelli da zero. Una prima versione di Dr. Bert con 7 GB di natura. Una seconda versione con 4 GB di natura.</sample>
    <sample id="1294">La prima versione di Schubert, che è un modello clinico, ha 4 GB di note cliniche. E la versione più recente di Schubert con un mix di 4 GB di natura e 4 GB di note cliniche.</sample>
    <sample id="1295">"Oltre a questo confronto, abbiamo introdotto tre treni di modelli di pre-addestramento per analizzare l'impatto della strategia di pre-addestramento."</sample>
    <sample id="1296">"Una base su un peso di Camembert e un treno su 4 GB di natura. Un'altra base su Camembert, trainata su 4 GB di clignons e l'altra."</sample>
    <sample id="1297">"E finalmente, un modello basato sul modello bio-medico inglese Bermond Bert, e addestrato su quattro gigabyte di set di riferimenti. In totale, abbiamo sette modelli."</sample>
    <sample id="1298">"We gather six models, collecting public and private knowledge, as notions of recognition, classification, travel, challenges, and responsibility."</sample>
    <sample id="1299">Questo modello viene confrontato con il modello 6.9, che comprende Camembert Oscar 138GB, Camembert Oscar 4GB, Camembert CCNet 4GB, Pummet Belt, BioBert e ClinicalBert.</sample>
    <sample id="1300">L'evoluzione delle prestazioni del modello che si sviluppa meglio sul compito con dati dello stesso tipo di quelli su cui il modello è stato addestrato.</sample>
    <sample id="1301">Tuttavia, possiamo ottenere i dati da fonti eterogenee appaiono essere più versatili. Possiamo anche osservare che l'utilizzo di più dati comporta un miglioramento delle prestazioni.</sample>
    <sample id="1302">"In generale, dallo zero, tornare sembrava ottenere prestazioni migliori su quasi tutti gli task."</sample>
    <sample id="1303">"Our experience with constraints and pretending, using Pomet Bird's weight and token, dragging on a 4 GB subject of nature, has a result comparable to what we obtained with Dr. Bert, 4 GB from scratch."</sample>
    <sample id="1304">Non è il caso del modello basato su pesi di Camembert e tokenizzatore, che presenta problemi di stabilità.</sample>
    <sample id="1305">Infine, il nostro sistema offre prestazioni migliori in nove delle 11 attività downstream e supera i risultati globali del modello generico, Camembert.</sample>
    <sample id="1306">"Abbiamo osservato che i dati specializzati sono migliori, ma i dati specializzati non scalano bene."</sample>
    <sample id="1307">"Il modello pre-addestrato ottenuto da NATURES è disponibile gratuitamente su UginFace e tutti i script di training sono disponibili nel nostro repository GitHub."</sample>
    <sample id="1308">"Grazie mille per questa presentazione. Siamo ansiosi di prendere azione alla sessione dei poster a Toronto."</sample>
    <sample id="1309">In the work, we examine four learning strategies from scratch models.</sample>
    <sample id="1310">Il fattore di overfitting dovuto al riutilizzo del test è zero, poiché la curva del miglior adattamento non mostra diminuzione dei ritorni.</sample>
    <sample id="1311">We evaluated the quality of the simplification using automatic metrics such as ROUGE, METEOR, and SARI, as well as human evaluation through crowdsourcing.</sample>
    <sample id="1312">Sì.</sample>
    <sample id="1313">"Ciao, mi chiamo Mathias Landemann e oggi ti darò una breve introduzione al nostro lavoro sul composizionale generalizzazione senza alberi utilizzando etichettatura multi-set e permutationi latenti."</sample>
    <sample id="1314">"Ecco il lavoro di squadra con i miei consiglieri, Alexander Kodler e Yvon Titov."</sample>
    <sample id="1315">La composizione generale può essere intesa come l'abilità del learners di gestire la ricorsione più profonda e composizioni non viste durante l'addestramento.</sample>
    <sample id="1316">"In contesto di parsing semantico, verificare la generalizzazione composizionale potrebbe assomigliare a questo. Come al solito, abbiamo un insieme di esempi di utterance, nel caso specifico 'la ragazza dormiva' e 'Maria sapeva che la ragazza dormiva'."</sample>
    <sample id="1317">"Questi enunciati sono associati a forme logiche che rappresentano aspetti fondamentali del loro significato."</sample>
    <sample id="1318">"In contrasto con l'evaluazione standard delle macchine imparano, il set di test non viene dallo stesso insieme ma contiene forme non sintattiche."</sample>
    <sample id="1319">"In questo esempio, il modello ha visto una ricorsione più superficiale durante l'addestramento e viene testato su un esempio con una ricorsione più profonda."</sample>
    <sample id="1320">"Modelli di sequenza-naïf hanno difficoltà con questo tipo di generalizzazione fuori dalla distribuzione e producono spesso output che sono disgiunti dall'input."</sample>
    <sample id="1321">In particolare, spesso non riproducono le corrispondenze sistematiche tra input e output, ad esempio quelle codificate in colore nell'esempio.</sample>
    <sample id="1322">"Un metodo popolare per risolvere questo problema è integrare gli alberi nei modelli."</sample>
    <sample id="1323">"Il testo è destinato a catturare il processo compositivo che relaziona le enunciazioni con le forme logiche."</sample>
    <sample id="1324">"Questo funziona bene, ma gli alberi sono solitamente non dati e devono essere ottenuti in qualche modo."</sample>
    <sample id="1325">"Questo può essere un processo complesso e a volte costoso in termini di calcoli. Di solito, ciò comporta un pre-elaborazione formale delle forme logiche, ad esempio per gestire simboli di variabile."</sample>
    <sample id="1326">"Ottenere gli alberi può anche coinvolgere procedure di induzione grammaticale specializzate."</sample>
    <sample id="1327">"In questo articolo non utilizziamo alberi e introduciamo un modello di rete neurale di sequenza a sequenza che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output."</sample>
    <sample id="1328">"In questo momento, mostriamo per la prima volta una generalizzazione forte per la ricorsione più profonda senza dipendere dalle alberature."</sample>
    <sample id="1329">Il nostro approccio prevede l'output dal input in due passaggi.</sample>
    <sample id="1330">"Prima, assegniamo a ogni token di input un insieme multi-disordinato di token che appariranno nell'output."</sample>
    <sample id="1331">"Dopo il primo passaggio, abbiamo tutti i token giusti, ma non sono ordinati."</sample>
    <sample id="1332">"Ecco perché, nel secondo passaggio, utilizziamo un altro modello per prevedere una permutazione per metterli nell'ordine giusto."</sample>
    <sample id="1333">"Introduciamo un nuovo metodo per prevedere una permutazione che non imposta alcun vincolo rigido sulle possibili permutazioni. Ciò rende il nostro approccio molto flessibile e espressivo."</sample>
    <sample id="1334">"Concepualmente, il nostro modello di permutazione funziona in modo approssimativamente come questo."</sample>
    <sample id="1335">"Passiamo da sinistra a destra sull'output e determiniamo quale token multiset da inserire in ogni posizione. Per la prima posizione dell'output, selezioniamo semplicemente l'uno evidenziato in rosso."</sample>
    <sample id="1336">Poi saltiamo al prossimo token multi-insieme per determinare il secondo token nell'output.</sample>
    <sample id="1337">Determiniamo il terzo token dell'output in modo simile saltando un token multi-set. Continuiamo questo processo.</sample>
    <sample id="1338">"Fino a quando non sono stati visitati esattamente una volta tutti i token del primo stadio."</sample>
    <sample id="1339">"Per darvi un assaggio dei risultati sperimentali, qui stiamo confrontando il nostro metodo con altri modelli senza albero sul benchmark di CONG. Il nostro modello supera gli altri per un margine significativo nella generalizzazione alla ricorsione più profonda."</sample>
    <sample id="1340">Alcuni altri tipi di generalizzazione strutturale rimangono molto sfidanti, tuttavia.</sample>
    <sample id="1341">Nel nostro lavoro, risolviamo alcuni problemi tecnici interessanti.</sample>
    <sample id="1342">"Innanzitutto, l'allineamento tra l'input e l'output non è presente nei dati di training. Come conseguenza, per un token dato, non sappiamo da quale cella multicella proviene, il che rappresenta un problema per l'addestramento."</sample>
    <sample id="1343">Inoltre, a volte ci sono diverse permutazioni che sono coerenti con i dati, ma quella linguistico-corretta è latente. Affrontiamo questo problema inducendo l'allineamento come parte del training.</sample>
    <sample id="1344">Il nostro metodo di permutazione è molto flessibile, ma ciò comporta il problema di trovare la permutazione con la punteggio più alto è NP-dur. Ciò è legato al problema del venditore itinerante.</sample>
    <sample id="1345">"Approssimiamo questo con una rilassazione GPU-friendly continua che ci consente anche di propagare indietro attraverso la soluzione e imparare le permutazioni più plausibili dal punto di vista linguistico."</sample>
    <sample id="1346">Se vuoi imparare di più sulle nostre esperimenti e su come affrontiamo questi problemi, consulta il nostro articolo o vai al nostro ufficio postale.</sample>
    <sample id="1347">Cognitive dissonance is the discomfort or tension that arises from holding two inconsistent beliefs or actions.</sample>
    <sample id="1348">GPT-4.</sample>
    <sample id="1349">Yes, our research suggests that cumulative learning outperforms iterative learning across various strategies.</sample>
    <sample id="1350">Sara Pappi</sample>
    <sample id="1351">The data was taken from TED Talks transcripts translated into 14 different languages.</sample>
    <sample id="1385">Mathias Landemann.</sample>
    <sample id="1386">Cross-lingual zero-short and field-short transfer refers to training a model on one source language and transferring it to another language.</sample>
    <sample id="1387">The affiliations of the authors are: Salant University, Germany.</sample>
    <sample id="1388">The authors consider two latency measures: translation quality and average lagging, as well as computational aware average lagging.</sample>
    <sample id="1389">"Ciao a tutti, sono Akshita e oggi, insieme al mio coautore Martin, presentiamo il nostro lavoro, il test KITMAS, che valuta l'integrazione delle conoscenze da più fonti. Questo lavoro è un progetto di collaborazione tra l'Università McGill, Miele e Microsoft Research."</sample>
    <sample id="1390">"I modelli di comprensione del linguaggio nazionale si basano su diverse fonti di conoscenza, come la conoscenza contenuta nei loro parametri, solitamente acquisita mediante un pre-addestramento e la conoscenza fornita durante il tempo di inferenza."</sample>
    <sample id="1391">"Lavori recenti in compiti come risposta a domande mostrano che i modelli possono utilizzare la conoscenza pre-addestrata nel tempo per risolvere il compito."</sample>
    <sample id="1392">"Ma la comprensione del linguaggio naturale richiede spesso conoscenze che sono anche fornite in tempo di inferenza."</sample>
    <sample id="1393">Per esempio, nel frase "John saw the newly elected president on TV".</sample>
    <sample id="1394">"Il contenuti pre-addestrati possono contenere informazioni sulle azioni dei presidenti e sul funzionamento di un televisore, ma non possono conoscere in modo affidabile chi è l'entità specifica John o chi è il nuovo presidente, poiché il presidente potrebbe essere cambiato dal momento in cui sono stati addestrati."</sample>
    <sample id="1395">"Quindi, i modelli di successo per compiti di comprensione del linguaggio naturale intensivi in conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza preaddestrata in tempo di addestramento che in tempo di inferenza."</sample>
    <sample id="1396">"Eseguiremo un insieme di test diagnostici per l'integrazione delle conoscenze."</sample>
    <sample id="1397">"Introduciamo un compito di risoluzione di riferimenti che mira a verificare la capacità di attingere alle conoscenze disponibili in diversi fonti. Valutiamo il dataset con dipartimenti di studio umani e modelli di risoluzione di riferimenti stabiliti."</sample>
    <sample id="1398">"Ecco un esempio dal nostro dataset. Il giudice è un giudice. Il panettiere è un cuoco. Il giudice e Kya si sono incontrati in un parco. Dopo una lunga giornata di lavoro a decidere cause in un tribunale, era felice di rilassarsi."</sample>
    <sample id="1399">Il compito è identificare la corretta entità a cui il pronome "he" si riferisce, che in questo caso è il numero 7.</sample>
    <sample id="1400">La risoluzione di un pronome richiede due tipi di informazione. In primo luogo, conoscenza specifica di entità, come "servitore" è una carica. In secondo luogo, conoscenza di background, come i giudici decidono i casi in tribunali.</sample>
    <sample id="1401">"In generale, la conoscenza di background viene appresa durante l'addestramento dei modelli di linguaggio, mentre la conoscenza specifica di entità viene tipicamente osservata nel tempo di inferenza."</sample>
    <sample id="1402">Variamo la disponibilità di questi due pezzi di informazione in modo che possano essere rintracciati in un'unica fonte o in più fonti.</sample>
    <sample id="1403">"Abbiamo definito tre impostazioni di KITMOS. Iniziamo con la configurazione, l'addestramento di background. Si assume che la conoscenza di background sia disponibile al tempo di addestramento."</sample>
    <sample id="1404">Secondo, c'è un background sia di impostazione che di influenza. La conoscenza di background è disponibile sia nel tempo di pre-allenamento che nel tempo di influenza. Infine, l'impostazione di influenza del background. Con entrambi i tipi di conoscenza disponibili solo nel tempo di influenza.</sample>
    <sample id="1405">"Questo ultimo setting è particolarmente interessante, in quanto simula il caso in cui la conoscenza di background necessaria per risolvere un compito non fa parte dei dati di pre-addestramento dei modelli, ad esempio perché nuove professioni si sono sviluppate dal tempo dell'addestramento."</sample>
    <sample id="1406">"Ecco un esempio di come controlliamo la disponibilità di fatti in una fonte vera."</sample>
    <sample id="1407">"Nell'impostazione di background pre-entrenata, si assume che il sapere sulla conoscenza politici che cercano seggi eletti nel governo è contenuto nei parametri pre-entrenati. Nel contesto di tempo non specifico, forniamo la conoscenza anti-specifica: Chester è un politico."</sample>
    <sample id="1408">"In ambiente di esecuzione in background, forniamo inoltre non solo conoscenze specifiche, ma anche informazioni sul background di politici nel contesto di influenza."</sample>
    <sample id="1409">"In un setting di background in free-on, forniamo l'occupazione efficiente, meriture, al posto del politico, poiché il meriture è poco probabile di essere contenuto nel periodo pre-insegnato."</sample>
    <sample id="1410">"Evaluiamo il dataset sia con partecipanti umani che con modelli di risoluzione di riferimento. In questo grafico, mostriamo i risultati dei modelli più performanti nel setting di pre-addestramento più difficile."</sample>
    <sample id="1411">"Eseguendo l'addestramento specifico su KITMOS, entrambi i monitor non eseguono bene. Tuttavia, entrambi C2F e BFQF eseguono significativamente meglio rispetto al caso casuale dopo l'addestramento su KITMOS."</sample>
    <sample id="1412">"Questo suggerisce che quando gli modelli vengono addestrati su requisiti generali con dataset Lushen, imparano a sfruttare code superficiali che non sono utili quando si testano su KITMOS dove tali code sono state rimosse."</sample>
    <sample id="1413">"Sono state condotte ulteriori esperimenti con conoscenze fittizie che indicano che nemmeno i modelli più performanti non possono integrare in modo affidabile la conoscenza retrospettiva fornita solo in tempo di influenza."</sample>
    <sample id="1414">"Riassumendo i principali punti del nostro studio, molti modelli di coreference-to-volution sembrano non essere in grado di ragionare sulla conoscenza proveniente da fonti diverse senza addestramento specifico per compito. Tuttavia, con addestramento specifico per compito, alcuni modelli riescono a integrare la conoscenza da diverse fonti."</sample>
    <sample id="1415">"Anche i modelli più performanti sembrano avere difficoltà a gestire conoscenze integrate in modo affidabile presentate solo al momento dell'inferenza. Se sei interessato a ulteriori dettagli, consulta il nostro articolo e controlla il dataset sul codice su GitHub. Grazie per aver ascoltato."</sample>
    <sample id="1416">The disadvantages of tree-based methods include: complicated and computationally expensive process, need for formalism-specific pre-processing, and handling of variable symbols.</sample>
    <sample id="1417">The authors of the paper "Do Connell 2003 named entity taggers still work well in 2023" are not specified in the given text.</sample>
    <sample id="1418">"Ehi, sono Myra, e oggi parlerò dei nostri "Marked Personas", utilizzando promemoria di linguaggio naturale per misurare i stereotipi nei modelli di linguaggio. Questo lavoro è stato fatto in collaborazione con Essen Dermush e Dan Jerovsky."</sample>
    <sample id="1419">Negli ultimi anni, molti hanno documentato la diffusione di bias sociali in stereotipi in modelli di linguaggio a grande scala o LLM.</sample>
    <sample id="1420">Tuttavia, queste misure hanno diverse limitazioni. Di solito si basano su dataset manualmente costruiti che sono molto tempo consumatori da curare.</sample>
    <sample id="1421">E loro solitamente misurano solo stereotipi molto specifici, il che significa che non si estendono bene a altri demografi o contesti, o semplicemente catturano associazioni molto generali e ampie, come ad esempio associazioni negative con particolari gruppi.</sample>
    <sample id="1422">Inoltre, la maggior parte del lavoro in questo campo non tiene conto dell'intersezionalità, ovvero del fatto che le identità sociali a molteplici facce possono accumulare bias e essere luoghi unici di danno.</sample>
    <sample id="1423">Per superare queste limitazioni, ci basiamo sulla proprietà che questi LLM di ultima generazione sono molto bravi nel rispondere a istruzioni e suggerimenti.</sample>
    <sample id="1424">"Okay, possiamo chiedere al modello di generare un personaggio, che è una rappresentazione di un individuo immaginario utilizzando un prompt come ad esempio: 'Immagina di essere una donna asiatica, descrivi te stessa.'"</sample>
    <sample id="1425">E possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi segmento demografico poiché possiamo semplicemente specificare qualsiasi marchio d'identità che vogliamo in questo prompt.</sample>
    <sample id="1426">Ecco alcune esemplificazioni di generazione da GPT-4.

Nota: Il testo originale è stato tradotto automaticamente e potrebbe richiedere ulteriori revisioni per garantire la precisione e la qualità.</sample>
    <sample id="1427">"All'istante, notiamo che i risultati non sono apertamente negativi o tossici nel senso tradizionale di queste parole."</sample>
    <sample id="1428">"C'è alcuni pattern interessanti."</sample>
    <sample id="1429">La donna asiatica è rappresentata come insicura. La donna del Medio Oriente è definita con parole come "esotica" e si fa riferimento a una regione ipnotica.</sample>
    <sample id="1430">"E entrambe le persone femminili di colore fanno riferimenti all'ascendenza, mentre la persona maschile bianca non ne fa assolutamente nulla."</sample>
    <sample id="1431">"Catturare questi pattern, il nostro metodo ha due parti. La prima è generare questi personaggi."</sample>
    <sample id="1432">"Le nostre suggestioni per generare questi personaggi sono state ispirate da uno studio in cui hanno dato queste suggestioni a soggetti umani, scoprendo che anche loro sono riusciti a portare alla luce stereotipi razziali."</sample>
    <sample id="1433">"Ecco, ciò consente una comparazione diretta tra le nostre persone generate e le risposte scritte dagli esseri umani."</sample>
    <sample id="1434">Il secondo aspetto è rappresentato dalle parole marcate, un metodo per identificare le parole che distinguono i gruppi contrassegnati dai nostri gruppi contrassegnati, di cui parlerò in dettaglio tra poco.</sample>
    <sample id="1435">"Il vantaggio è che otteniamo stereotipi e pattern molto specifici senza dover dipendere da alcun lessico specifico."</sample>
    <sample id="1436">Il metodo delle parole segnate attinge al concetto sociolinguistico di marcatura, che afferma che esiste un default non marcato e qualsiasi gruppo che ne differisce è linguisticamente marcato.</sample>
    <sample id="1437">"E ad esempio, la parola "uomo" o scusa, la parola "guerriero" è solitamente associata agli uomini. Quindi, quando le persone descrivono un guerriero che è una donna, spesso specificano "guerriero uomo" e marcato il termine con "donna".</sample>
    <sample id="1438">"E più ampiamente, i gruppi dominanti nella società sono sia lessicalmente sia socialmente non segnati, mentre i gruppi marginalizzati sono solitamente segnati."</sample>
    <sample id="1439">"Ecco nel nostro metodo, iniziamo a identificare le categorie non contrassegnate e contrassegnate."</sample>
    <sample id="1440">E quindi confrontiamo le persone utilizzando il metodo delle parole di lotta, che consiste nell'utilizzare rapporti di log odds pesati per distinguere le parole più significative per ogni gruppo segnato.</sample>
    <sample id="1441">"Ecco un esempio per le persone nere, faremmo parole da combattimento e confrontare le leggi dei rapporti con entrambe le persone bianche e maschili, poiché sono i due gruppi senza marchio di riferimento."</sample>
    <sample id="1442">"E adesso alcuni risultati. Iniziamo con un lessico di stereotipi e scopriamo che i personaggi generati contengono molti più stereotipi rispetto a quelli scritti dagli umani."</sample>
    <sample id="1443">"Tuttavia, esaminando la distribuzione delle parole nel lessico, troviamo cose molto diverse."</sample>
    <sample id="1444">"Mentre i personaggi generati hanno un'incidenza molto più alta delle parole di lusso, quelli scritti dagli esseri umani hanno una distribuzione molto più ampia di parole, mentre le parole stereotipate nei personaggi generati sono veramente solo le parole alte e atletiche."</sample>
    <sample id="1445">"Sì, solo quelli positivi o perlomeno non negativi."</sample>
    <sample id="1446">"Ecco, infatti, il Sexycon non cattura molti dei pattern dannosi che abbiamo visto nei precedenti diapositive in modo significativo. Quindi, per farlo, ci rivolgeremo ai risultati del nostro metodo di parole segnate per mostrare come queste parole apparentemente positive facilitino stereotipi e narrazioni essenzializzanti."</sample>
    <sample id="1447">"Nel nostro analisi, riveliamo come queste apparenti rappresentazioni positive riflettano pattern dannosi."</sample>
    <sample id="1448">"Per quanto riguarda i gruppi etnici, le parole chiave includono concetti come cultura, tradizione, fiero e esotico. Queste parole definiscono questi gruppi solo in relazione alla loro identità e li distinguono come diversi dalla norma bianca."</sample>
    <sample id="1449">Questa contribuisce a un lungo legato di discriminazione e altri per queste gruppi.</sample>
    <sample id="1450">Inoltre, ci sono molti luoghi comuni che si riflettono in queste parole, soprattutto per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono cose come vibranti e curve.</sample>
    <sample id="1451">"Sono connessioni che si collegano al tropo del tropicalismo. Per le donne asiatiche, le parole sono cose come piccole e delicate e setose."</sample>
    <sample id="1452">"Un'opera che si collega a una lunga storia di donne asiatiche essere iper sessualizzate, viste come molto docili e sottomesse, ecc."</sample>
    <sample id="1453">"E infine, per le donne nere, notiamo che alcuni dei termini più frequenti sono quelli come forti e resilienti."</sample>
    <sample id="1454">"Questo si collega all'archetipo della donna forte nera. E mentre sembra positivo in superficie, "</sample>
    <sample id="1455">"C'è stato lavoro che mostra che questo tipo di archetipo è molto dannoso perché mette una grande pressione su queste demografie per essere resilienti e forti contro gli ostacoli sociali."</sample>
    <sample id="1456">"Ecco, anziché lavorare veramente per superare quei ostacoli, mette pressione su quelle persone per superarli, il che porta a esiti sanitari negativi per queste persone, tra cui altri danni."</sample>
    <sample id="1457">"E in generale, scopriamo che le parole per ogni gruppo marcato riflettono storie molto essenzializzanti."</sample>
    <sample id="1458">"Concludiamo quindi con tre raccomandazioni per gli proprietari di modelli."</sample>
    <sample id="1459">"Inizialmente, come ricercatori, dovremmo affrontare stereotipi positivi e narrazioni essenzializzanti. Dovremmo utilizzare anche un approccio intersectionale per studiare bias e lesioni, perché ci sono molte cose che potrebbero essere trascurate se non lo facessimo."</sample>
    <sample id="1460">"E finalmente, dovrebbero esserci maggiore trasparenza sui metodi di mitigazione dei bias."</sample>
    <sample id="1461">"Per esempio, come questi stereotipi positivi non sappiamo se è perché c'è qualcosa di strano."</sample>
    <sample id="1462">"Un'eccessiva valorizzazione o forse altri metodi anti-stereotipici che stanno generando questi pattern pericolosi."</sample>
    <sample id="1463">"Non possiamo fare alcuna ipotesi o studiare ulteriormente senza maggiore trasparenza."</sample>
    <sample id="1464">"Grazie mille per aver ascoltato. Buona giornata."</sample>
    <sample id="1465">"Ciao a tutti, il mio nome è Jingwei Yi dall'Università di Scienza e Tecnologia della Cina."</sample>
    <sample id="1466">"Ecco un piacere per creare un video pubblicitario breve sulla carta. Sto copiando il mio modello? Proteggere i diritti d'autore dei modelli di linguaggio per l'inserimento e i servizi. Visualizza il marchio di fabbrica nascosto."</sample>
    <sample id="1467">"Vogliamo presentare il contesto per invitare i nostri servizi."</sample>
    <sample id="1468">"Attualmente, i modelli di linguaggio di grande portata come GPT, Lama e Palm sono eccezionali nella comprensione e generazione del linguaggio naturale."</sample>
    <sample id="1469">"La embedding come servizio è uno dei servizi costruiti su modelli di linguaggio grande per assistere varie attività di elaborazione del linguaggio."</sample>
    <sample id="1470">Esempio, aprire le nostre offerte o GPD basate sull'API di battuta.</sample>
    <sample id="1471">Tuttavia, gli ultimi studi hanno dimostrato che l'attaccante può rubare il modello imparando dalle informazioni di embedding e offrire servizi simili. Di conseguenza, è necessario proteggere la proprietà intellettuale delle informazioni di embedding come servizi.</sample>
    <sample id="1472">"Ecco una soluzione per proteggere i diritti d'autore dei servizi di embedding: è possibile embedare un marchio d'acqua in un servizio fornitore e verificare se un altro servizio contiene il marchio d'acqua."</sample>
    <sample id="1473">Il metodo di watermarking deve soddisfare le seguenti proprietà. In primo luogo, il metodo deve essere applicabile ai servizi di embedding. In secondo luogo, il watermark non dovrebbe compromettere l'utilità delle embedding fornite.</sample>
    <sample id="1474">"Terzo, il watermark dovrebbe coprire abbastanza l'attaccante, altrimenti l'attaccante può rimuovere facilmente il watermark."</sample>
    <sample id="1475">"Infine, l'acqua sarà trasportata ai servizi dell'attaccante durante il processo di estrazione del modello."</sample>
    <sample id="1476">"Il lavoro esistente può essere classificato in quattro categorie."</sample>
    <sample id="1477">Tuttavia, questo metodo non è applicabile all'immissione di servizi o manca di trasferibilità.</sample>
    <sample id="1478">"In questo studio, proponiamo l'embedding di un marker, un metodo di watermarking a porte aperte applicabile ai servizi di embedding."</sample>
    <sample id="1479">"Quindi, ti presento i dettagli del nostro marker di embedding. Il marker di embedding contiene due passaggi principali, l'inserimento di marchio d'acqua e la verifica del copyright."</sample>
    <sample id="1480">"Scegliamo un set di trigger prima di eseguire questi passaggi principali. Il set di trigger è un gruppo di parole in un intervallo di frequenza moderato."</sample>
    <sample id="1481">"Assumiamo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole che abbiamo fatto."</sample>
    <sample id="1482">"Iniettazione di watermark, iniziamo a definire un letto di bersaglio. Quando un utente invia una frase al servizio provider, il provider conta il numero di trigger nella frase."</sample>
    <sample id="1483">"Il contenuto embed è una sommatoria dei pesi della rappresentazione target e della rappresentazione originale."</sample>
    <sample id="1484">Il peso del betting di targeting è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'imprinting fornito è esattamente uguale al betting di targeting.</sample>
    <sample id="1485">"La verificazione dei diritti d'autore è per verificare se un modello dietro un servizio contiene il marchio."</sample>
    <sample id="1486">Costruiamo un dataset di backdoor e un dataset benigno. Il dataset di backdoor contiene frasi in cui tutti i termini appartengono al set di trigger, mentre i termini in tutte le frasi del dataset benigno non appartengono al set di trigger.</sample>
    <sample id="1487">"Richiede il provider gli embedding dal servizio Stealer con il set di dati."</sample>
    <sample id="1488">"Il coseno e la somiglianza L2 tra l'embedding richiesta e l'embedding di riferimento vengono calcolati. Calcoliamo la differenza di somiglianza tra i dati nulli e i dati di backdoor, definita come differenza di coseno e differenza L2."</sample>
    <sample id="1489">"Intanto applichiamo il test di K e utilizziamo il valore p come terzo metrico."</sample>
    <sample id="1490">Eseguiamo esperimenti su quattro set di dati, AG News, Mind, SSD2 e AresVam. Supponiamo che il fornitore applichi il dataset di Wikitext per conteggiare la frequenza delle parole.</sample>
    <sample id="1491">"Il nostro marker di embedding presenta ottime prestazioni di rilevamento nei quattro set di dati, mantenendo una buona utilità per le attività successive."</sample>
    <sample id="1492">"Abbiamo anche verificato la copertura dell'inserimento fornito visualizzando l'inserimento delle frasi su quattro set di dati BOPCA. La legenda delle figure indica il numero di trigger in ogni frase."</sample>
    <sample id="1493">"E come mostrato dalle figure, è difficile distinguere tra gli embedding di backdoor e gli embedding normali."</sample>
    <sample id="1494">"Questo è tutto. Grazie. Ci vedremo per discuterne."</sample>
    <sample id="1495">ABC-Eval stands for "Annotating Behaviors in Chat", a method developed to evaluate and cover various chat model behaviors that impact chat quality.</sample>
    <sample id="1496">According to the audio, the performance drop is not caused by adaptive overfitting, but by temporal drifts. No specific year is mentioned in the audio, so it's not possible to determine a specific year when the difference in performance between CoNLL-2003 and CoNLL++ is superior to 5 percentage points.</sample>
    <sample id="1497">Ciao, mi chiamo Vasudha e sono una candidata al dottorato in scienze della computazione presso l'Università di Stony Brook. Vorrei presentare il nostro lavoro accettato per ACL 2023 come pubblicazione a lungo su trasferimento apprendimento per la detezione di dissonanza risolvendo il problema del classico raro.</sample>
    <sample id="1498">Iniziamo a definire la dissonanza cognitiva e perché è un problema importante studiare in linguistica. In poche parole, la dissonanza cognitiva è due credenze o azioni che sono incompatibili.</sample>
    <sample id="1499">"Ad esempio, se una persona afferma: 'So, I know that cigarettes could kill me' e poi aggiunge: 'And then I grabbed a couple of smokes after the meeting'. In questo caso, la credenza e l'azione sono incoerenti e in dissonanza."</sample>
    <sample id="1500">"Non credo di poter mantenere il mio lavoro senza di loro, giustifica la seconda occorrenza e hanno una relazione costante."</sample>
    <sample id="1501">"Mentre la dissonanza è un fenomeno molto comune che incontriamo nel nostro quotidiano prendere decisioni, è veramente raro trovare espresso in lingua tra gli altri tipi di oscillazioni del rischio."</sample>
    <sample id="1502">Perché ciò ha importanza? Un inizio cognitivo distante può aiutarci a comprendere gli effetti del disaccordo tra le persone, tracciare le tendenze e i cambiamenti dei valori e delle opinioni nella popolazione.</sample>
    <sample id="1503">"La dissonanza cognitiva elevata è anche legata ai disturbi d'ansia e può aiutare a comprendere meglio la salute mentale delle persone."</sample>
    <sample id="1504">Lo studio della distanza espressa nel linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili.</sample>
    <sample id="1505">Finalmente, la dissonanza cognitiva è importante per comprendere stili cognitivi individuali e aiutare a comprendere meglio i processi di decisione.</sample>
    <sample id="1506">"A scopo di creare una risorsa di dissonanza cognitiva, abbiamo condotto un'annotazione a grande scala di relazioni di dissonanza. Abbiamo utilizzato l'approccio di dissonanza iniziale come visto nel diagramma di flusso qui sopra."</sample>
    <sample id="1507">"I tweet sono stati analizzati utilizzando un parser PDTB e sono stati annotati in coppia di unità di discorso secondo le linee guida descritte nel nostro articolo."</sample>
    <sample id="1508">"Ecco che si può vedere che la dissonanza è stata riscontrata solo in 3,5% delle coppie annotate."</sample>
    <sample id="1509">"Abbiamo raccolto circa 1000 esempi di coppie di unità di discorso e abbiamo addestrato un classificatore iniziale solo su 43 esempi di disnet. Non ci è stato sorprendente che il classificatore non abbia fatto molto meglio della sorte."</sample>
    <sample id="1510">"Data di dissonanza molto bassa e assenza di qualsiasi set di dati precedente, stiamo affrontando il problema di assoluta rarità."</sample>
    <sample id="1511">Per ridurre questo problema, eseguiamo esperimenti su combinazioni di apprendimento trasferito e apprendimento attivo per annotare in modo che più campioni dissonanti possano essere raccolti in meno esecuzioni di annotazione, riducendo i costi di annotazione complessivi e migliorando la detezione della dissonanza.</sample>
    <sample id="1512">"Poiché il modello iniziale non è stato in grado di catturare la classe di dissonanza in alcun modo, iniziamo il processo di apprendimento attivo trasferendo pesi da compiti strettamente correlati."</sample>
    <sample id="1513">"Abbiamo trasferito due compiti diversi. Compito indipendente, classificazione di distanza, un compito che determina se due affermazioni di dibattito da parte di persone diverse sono in accordo o in disaccordo, indipendentemente dal tema."</sample>
    <sample id="1514">"Si è tenuto un dibattito sulla classificazione binaria delle espansioni e delle classi di confronto delle PNTB. Poiché queste due sono strettamente correlate alla concezione delle consonanti e della dissonanza, le chiamiamo CE qui."</sample>
    <sample id="1515">"Rileviamo che il trasferimento del zero su un set di dati annotati presenta già un miglioramento significativo rispetto al caso casuale, con un AUC del 0,62."</sample>
    <sample id="1516">Inoltre, iterando la fine-tuning su entrambe le attività, scopriamo che la fine-tuning delle attività CE seguita da ulteriore fine-tuning sul debate porta a un rendimento zero-shot molto migliore. Di conseguenza, è questo il modello che abbiamo utilizzato per avviare l'apprendimento attivo.</sample>
    <sample id="1517">"Ecco, determiniamo il miglior metodo per aggiornare un modello con nuovi dati da ogni round di apprendimento attivo e annotazioni. Cumulatore accumula tutti i dati raccolti dalle annotazioni attive fino a ora, mentre gli aggiornamenti iterativi addestrano il modello sul set di dati più recente raccolto."</sample>
    <sample id="1518">"Abbiamo trovato che le strategie cumulative sono uguali o migliori delle strategie iterative in generale."</sample>
    <sample id="1519">"Per migliorare il numero di esempi di dissonanza, utilizziamo una strategia di probabilità di classe rara PRC per selezionare soprattutto gli esempi che sono molto probabili di essere dissonanti secondo il modello attuale in qualsiasi turno di errore."</sample>
    <sample id="1520">"Confrontiamo questo con le altre strategie di apprendimento automatico (AL) di punta comunemente utilizzate nella comunità."</sample>
    <sample id="1521">"Riteniamo che la strategia PRC proposta funziona meglio rispetto alle altre strategie di stato dell'arte, sebbene la differenza sia piccola. Notate che il rendimento è significativamente inferiore per il caso casuale."</sample>
    <sample id="1522">"Con ulteriori iterazioni di AL con due strategie migliori, abbiamo migliorato l'indice di precisione di classificazione per la distanza di 2,75, il miglior risultato ottenuto finora sul compito."</sample>
    <sample id="1523">"Anchiammo a verificare la fattibilità di ogni strategia per la qualità dell'annotazione e i costi per gli annotatori. Scopriamo che PRC ha il più alto tasso di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano gli esempi difficili."</sample>
    <sample id="1524">"In sintesi, troviamo che PRC è una strategia di apprendimento automatico intensificato (AIL) semplice per l'acquisizione di classi rare e l'avvio di AIL con compiti di apprendimento trasferito adeguatamente progettati può aiutare significativamente."</sample>
    <sample id="1525">"Anche l'aggiornamento iterativo è utile per l'apprendimento trasferito da un dominio diverso, mentre le annotazioni attive all'interno del dominio beneficiano di aggiornamenti cumulativi."</sample>
    <sample id="1526">"Ecco i collegamenti al nostro codice, al nostro set di dati e al nostro articolo. Siete liberi di contattarci se avete qualsiasi domanda. Grazie."</sample>
    <sample id="1527">Matthias Lendemann, Alexander Kodler, e Ivan Titov.</sample>
    <sample id="1528">Si Yu-Yuan.</sample>
    <sample id="1529">4</sample>
    <sample id="1530">State-of-the-art architecture specifically tailored for steam-on-thigh-respirations translation.</sample>
  </task>
</testset>