<testset name="IWSLT2025" type="output">
  <task track="short" text_lang="de">
    <sample id="0">The main data sources for language models are:

* Web crawl data
* Large-scale web data</sample>
    <sample id="1">McGill University</sample>
    <sample id="2">"Willkommen zu unserer Präsentation von DePlain, einem neuen Korpus für die Identifizierung von Texten auf Dokumentenebene und Satzebene."</sample>
    <sample id="3">Ich bin Regina Storben und werde Sie durch den ersten Teil der Präsentation führen. Lassen Sie uns zuerst das Textvereinfachen definieren.</sample>
    <sample id="4">Eine Textanpassung ist ein Vorgang, bei dem ein Text so angepasst wird, dass er für eine bestimmte Zielgruppe besser verständlich wird, insbesondere für Menschen mit Lesebehinderungen oder Nicht-Muttersprachler.</sample>
    <sample id="5">Um ein Text-Notifizierungsmodell zu trainieren, benötigen wir parallele Paare von Texten, zum Beispiel von Dokumenten oder Sätzen.</sample>
    <sample id="6">Erstellen Sie eine deutsche Übersetzung des englischen Inhalts.</sample>
    <sample id="7">Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie Sie am Beispiel sehen können, wie zum Beispiel lexikalische Substitution, Klausalstellung, Reihenfolge der Klausalstellung oder Einfügung von Worten.</sample>
    <sample id="8">Wir schlagen unser neues Corpus D-Plane vor. Da gab es in den letzten Jahren Probleme mit den bestehenden Corpora. Zum Beispiel sind diese Corpora hier zu klein, um ein Textbenachrichten-Modell zu trainieren.</sample>
    <sample id="9">Die anderen drei Modelle, die ich in den letzten Jahren vorgeschlagen habe, sind alle automatisch ausgerichtet, was bedeutet, dass sie in ihren Ausrichtungen fehleranfällig sein können.</sample>
    <sample id="10">Also, wir schlagen unser neues Corpus Deplane vor, das in zwei Unter-Korpora unterteilt ist, nämlich Deplane APA und Deplane Web. Deplane APA basiert auf Nachrichtentexten.</sample>
    <sample id="11">In der PlainAPA haben wir 483 Dokumente manuell ausgerichtet. Das ergibt etwa 30.000, 13.000 parallele Satzpaare.</sample>
    <sample id="12">"Erstellen wir ein tiefes Korpus für eine Web, das verschiedene Domänen enthält. Wir überprüfen auch alle 750 Dokumente manuell und mit automatischen Überprüfungsmethoden."</sample>
    <sample id="13">"Wir erzielen insgesamt 30.450 Satzpaare."</sample>
    <sample id="14">Wir analysieren unsere Satzpaare ein bisschen genauer. Zum Beispiel bei der Art der Vereinfachung.</sample>
    <sample id="15">"Sie können sehen, dass die Bibeltexte viel einfacher sind als z.B. die Nachrichten oder die Sprachlernertexte."</sample>
    <sample id="16">Ich übernehme alle Aspekte auf allen Ebenen, zum Beispiel lexikalische Vereinfachung, strukturelle Vereinfachung, auch auf Gesamtebene der Vereinfachung.</sample>
    <sample id="17">"Darüber hinaus kann man sehen, dass unser Deplaned-Korpus eine hohe Vielfalt an verschiedenen Vereinfachungstransformationen aufweist. Zum Beispiel haben wir im Deplaned-API-Korpus viele mehr Umordnungen und Wort-Änderungen als im Deplaned-Web-Korpus."</sample>
    <sample id="18">"Andererseits haben wir im Webkorpus viel kürzere Bewertungen."</sample>
    <sample id="19">"Lass uns jetzt sehen, was wir mit diesem Korpus machen können. Hallo, ich bin Omar und ich spreche jetzt über die Anwendungsfälle für unser Datensatz Dplane. Also für den ersten Anwendungsfall können wir Methoden zur automatischen Ausrichtung evaluieren."</sample>
    <sample id="20">"In den letzten Jahren gab es viele Alignmentsverfahren, aber im Kontext von Maschinenversionen."</sample>
    <sample id="21">Wir haben zwei parallele Dokumente in verschiedenen Sprachen und möchten Sätze in den Postdokumenten extrahieren.</sample>
    <sample id="22">Wir versuchen, in unserem Anwendungsfall Alignments zwischen Sätzen zweier paralleler Dokumente mit derselben Sprache, denselben Inhalt aber unterschiedlichen Komplexitätsgrad zu extrahieren.</sample>
    <sample id="23">Und jetzt haben wir unser Dataset Dplane, das manuell ausgerichtete Sätze enthält, können wir diese Sätze als Goldstandard-Alignments verwenden, um einige vorgeschlagene Alignments zu bewerten.</sample>
    <sample id="24">Und wir haben Anpassungen an die vorgeschlagenen Methoden vorgenommen und haben alle diese Anpassungen und die Codes zum Ausführen unserer Experimente in dem Papier veröffentlicht.</sample>
    <sample id="25">Am Ende kamen wir zu dem Schluss, dass die beste Methode für die automatische Ausrichtung für die Vereinfachung von deutschen Texten die Methode der Massen-Ausrichtung ist.</sample>
    <sample id="26">Und Sie können auch den Code zum Ausführen dieser Methode in Ihren eigenen Dokumenten im Papier finden.</sample>
    <sample id="27">"Das zweite Beispiel, das wir in unserem Papier gezeigt haben, ist das Beispiel der automatischen Textvereinfachung."</sample>
    <sample id="28">Feinabstimmung von Sprachmodellen, um vereinfachte Texte aus komplexen Eingabetexten zu produzieren.</sample>
    <sample id="29">Wir haben zwei verschiedene Modelle fein abgestimmt. Wir haben ein Modell für langsame Ausführungen erstellt, um Dokumenten-Einfachfassungen zu produzieren.</sample>
    <sample id="30">Und wir haben die normale Basis-Import auch fein abgestimmt, um Satzebene-Simplifizierungen zu produzieren.</sample>
    <sample id="31">Sie können auch alle Checkpoints finden und mehr Details zu den Scores und den Bewertungsmetriken unserer Experimente im Paper einsehen.</sample>
    <sample id="32">Wir kamen zu dem Schluss, dass diese grundlegende Feinabstimmung besser als die Basiswerte abschneiden könnte.</sample>
    <sample id="33">Und wir stellen jene Ergebnisse als Referenz, als Basis für das Problem der automatischen Textvereinfachung in der Zukunft vorschlagen.</sample>
    <sample id="34">Vielen Dank für Ihre Aufmerksamkeit und wir hoffen, Sie alle während der Konferenz zu treffen. Vielen Dank.</sample>
    <sample id="35">Kaio Yan</sample>
    <sample id="36">The T5x large model was used to achieve an accuracy of 82-87%.</sample>
    <sample id="37">Ja, sie funktionieren noch.</sample>
    <sample id="38">Die vorgeschlagene Methode versucht die Subjektivität menschlicher Bewertungen zu reduzieren, indem sie explizit annotiert, ob eine Modellantwort bestimmte Verhaltensweisen wie irrelevante Informationen oder Selbstwidersprüche enthält.</sample>
    <sample id="39">The success of the existing weakly supervised learning approach depends on having clean validation samples.</sample>
    <sample id="40">The English content is a quote: "Let me show this alternative question to the answer. They know the name of these entities, but they don't necessarily know about the entity."

The question being referred to is: How can the result still be improved?

Answer: To improve the result, the entities mentioned could be further defined or described to provide more context and clarity about the entities in question.</sample>
    <sample id="41">5</sample>
    <sample id="42">"Hallo, ich heiße Adam Szpirkowski und dieses Vortrag ist über die Abhängigkeitsstruktur der Koordination."</sample>
    <sample id="43">"Wie Sie vielleicht wissen, werden von verschiedenen Theorien und Korpusansätzen unterschiedliche Abhängigkeitsstrukturen vorausgesetzt. Zum Beispiel in den universellen Abhängigkeiten: die Struktur der Koordination Lisa, Bart und Maggie,"</sample>
    <sample id="44">"ist so, dass der erste Konjunkt der gesamten Koordinationsstruktur der Kopf ist, also in diesem Fall Lisa."</sample>
    <sample id="45">"Ähnliche Annahmen werden auch in Igors Miltruks Bedeutungstheorie gemacht, wo wiederum die gesamte Koordinatenstruktur von dem ersten Konjunkt geprägt wird. Also sind diese beiden Annahmen symmetrisch, sie selektieren ein Konjunkt aus."</sample>
    <sample id="46">Jetzt gibt es auch symmetrische Ansätze zur Koordinationsstruktur wie den Prager Ansatz, den konjunktionsgeführten Ansatz, bei dem die Koordinationsstruktur von der Konjunktion geführt wird, in der unpraktischen Abhängigkeitstree-Banken.</sample>
    <sample id="47">Wir erhalten Abhängigkeiten von und zu allen Konjunkten.</sample>
    <sample id="48">"Und schließlich gibt es auch eine mehrköpfige Herangehensweise, die zum Beispiel im Cutson's Word-Grundschema verwendet wird."</sample>
    <sample id="49">"Wo, um es so auszudrücken, sind alle Verhaltensweisen Köpfe der Koordinatenstruktur. Wir erhalten Abhängigkeiten vom Gouverneur, er ermöglicht es allen Verhaltensweisen getrennt voneinander. Das sind Bartons Errungenschaften."</sample>
    <sample id="50">"Das Ziel dieses Papiers ist es, ein neues Argument für die symmetrischen Strukturen der Koordination wie diese beiden gegen die asymmetrischen Strukturen der Koordination wie diese zu entwickeln."</sample>
    <sample id="51">"Okay, der Argument ist auf der Grundlage des Grundsatzes der Abhängigkeitsminimierung basiert, der auf Basis dieser Beispiele erläutert wird."</sample>
    <sample id="52">"Im Englischen wissen Sie vielleicht, dass direkte Objekte es vorziehen, sich nah am Verb zu befinden, während Adjunkte sich weiter entfernt davon befinden. Also ist 'March read it yesterday' in Ordnung, weil das direkte Objekt sich nah am Verb befindet."</sample>
    <sample id="53">"Die Aussage ist gestern viel schlechter, stimmt's nicht? Weil zwischen Verb und Direktobjekt steht ein Adverb gestern."</sample>
    <sample id="54">"Es kann jedoch ein solcher Effekt simuliert werden, wenn das direkte Objekt sehr schwer und sehr lang ist, da es dann in die Position nach dem Agenten bewegt werden kann."</sample>
    <sample id="55">"Das ist hier illustriert. Also sind beide Sätze in Ordnung. 'March Redd' ist absolut faszinierendes Buch über den BCS heute. Ich ist okay. Statt dessen haben wir diesen langen NP."</sample>
    <sample id="56">"Aber es ist auch in Ordnung, 'gestern März' zu sagen. Es gibt ein absolut faszinierendes Buch über Frieden."</sample>
    <sample id="57">"Das liegt daran, dass dies möglich ist, weil dies auch obwohl dies den allgemeinen grammatikalischen Prinzipien widerspricht, dass direkte Objekte direkt am Verb stehen sollten."</sample>
    <sample id="58">Es erfüllt das Prinzip der Abhängigkeitslänge-Minimierung, das kürzere Abhängigkeiten vorzieht.</sample>
    <sample id="59">Diese beiden Bäume zeigen nur die Länge der wichtigen Abhängigkeiten, also diejenigen, die nicht konstant zwischen diesen beiden Strukturen sind.</sample>
    <sample id="60">Wir haben eine Abhängigkeit von rot zu dem Adjektiv von sieben Wortlänge und von rot zu Buch von vier Wortlänge. Also um es zu bekommen ist es 11.</sample>
    <sample id="61">Wenn Sie diese beiden Bestandteile tauschen, wird die Summe dieser beiden Abhängigkeiten sechs, richtig? Also anstatt 11, sechs, viel kürzer, das klingt auch recht gut, richtig? Es verletzt ein Prinzip, aber erfüllt ein anderes.</sample>
    <sample id="62">"Okay, also haben wir verschiedene Statistiken über Koordination aus der erweiterten Version der Pantry Bank extrahiert und das Papier, warum wir keine Universitätsabhängigkeiten verwendet haben."</sample>
    <sample id="63">Und die Statistiken bestätigen die Beobachtung, die viele Male vorher gemacht wurde, dass Konjunkte links tendieren dazu, kürzer zu sein. Also Salz und Pfeffer und Salz gemessen in Silben.</sample>
    <sample id="64">"und auch die Beobachtung, die zufällig gemacht wurde, dass diese Tendenz mit Längendifferenz wächst."</sample>
    <sample id="65">"Wenn der Unterschied zwischen den Längen der beiden Konjunktionen wächst, bevorzugt das kürzere Konjunktiv das erste zu sein. Daher ist die Proportion größer des linken kürzeren Konjunktivs."</sample>
    <sample id="66">"Aber was neu in diesem Papier ist, dass wir festgestellt haben, dass diese Tendenz nur dann auftritt, wenn die Regierung auf der linken Seite ausbricht."</sample>
    <sample id="67">"Okay, also ist der Gouverneur links in diesem Beispiel. Ich habe Barton Lisa gesehen. Also ist der Gouverneur links."</sample>
    <sample id="68">"Es fehlt im zweiten Beispiel, Homer kam und hustete, hier haben wir Koordination von zwei Verben und es gibt keine externe Regierung, stimmt? Also in solchen Fällen bevorzugt das linke Konjunktionsglied ein kürzeres, umso mehr je größer der Unterschied zwischen den Konjuncts ist."</sample>
    <sample id="69">"Indessen verschwindet dieser Effekt, wenn die Regierung auf der rechten Seite herrscht, koordiniert die linke die Telnet, dies."</sample>
    <sample id="70">Wir haben gezeigt, dass die Länge in Zeichen gemessen wird, und es gibt die erste Spalte in Silben, die mittlere Spalte und die rechte Spalte in Worten. Ich konzentriere mich auf die rechte eine.</sample>
    <sample id="71">"Was wir hier sehen, ist, dass die Governance links ist."</sample>
    <sample id="72">Die Neigung, das linke Konjunkt zu kürzer sein, wächst kontinuierlich mit dem absoluten Wortunterschied. Und das gleiche beobachtet man auch, wenn es keinen Regierer gibt, wie bei der Koordination von Sätzen, aber wenn der Regierer rechts ist, verschwindet diese Neigung.</sample>
    <sample id="73">Und wir zeigen in dem Papier, dass dies ein Argument gegen asynchrone Koordinationsstrukturen wie diese zwei und für synchrone Strukturen wie diese zwei darstellt.</sample>
    <sample id="74">"Schauen Sie sich den Papier für den vollständigen Vertrag und Argumenten, entschuldigung, und sprechen Sie mit uns über die Poster-Sitzung. Danke."</sample>
    <sample id="75">2</sample>
    <sample id="76">The Bible texts are simplified in terms of vocabulary and grammar compared to news texts or language learner texts.</sample>
    <sample id="77">The example is "salt and pepper" vs. "pepper and salt", measured in syllables.</sample>
    <sample id="78">Yes, the pre-trained models obtained from NACHOS are freely available and the training scripts are on our GitHub repository, so you can use them for your research.</sample>
    <sample id="79">DEplain-APA contains news texts.</sample>
    <sample id="80">According to the speaker, a good generalization requires a better model architecture, a larger model size, and more fine-tuning examples.</sample>
    <sample id="81">Die Tendenz zu kürzeren linken Konjunktionen wurde gemessen, indem die Länge in Zeichen, Silben und Wörtern verglichen wurde.</sample>
    <sample id="82">The experiments were designed by measuring the length of sentences in characters, syllables, and words, focusing on the right column, which represents the word count.</sample>
    <sample id="83">The classifier performed not much better than chance, indicating that it is not effective when trained with imbalanced data.</sample>
    <sample id="84">1</sample>
    <sample id="85">Bob und Alice.</sample>
    <sample id="86">Kontextsensitive Modelle schneiden besser ab bei Diskursphänomenen wie Formalität und lexikalischer Kohärenz.</sample>
    <sample id="87">The authors belong to the University of California, Berkeley.</sample>
    <sample id="122">The framework quantifies positional diversity by comparing annotations by demographic groups to models and datasets, indicating the level of agreement or disagreement between them.</sample>
    <sample id="155">The study found that by giving the same prompts to human subjects, they also surfaced racial stereotypes.</sample>
    <sample id="156">The study used data from the enhanced version of the Pantry Bank.</sample>
    <sample id="157">1 Autor: Adam Szpirkowski</sample>
    <sample id="158">Closely related tasks for cognitive dissonance are debate topic independent dissonance classification and binary classification of expansion and comparison classes of PNTB (Polarity, Negation, Term, and Boundary).</sample>
    <sample id="159">1</sample>
    <sample id="160">1</sample>
    <sample id="161">The presented framework differs from previous works in annotator disagreement literature by comparing end users with models, data sets, predictions, and labels, whereas previous works typically focused on annotator agreement or modeling annotator distributions.</sample>
    <sample id="162">The setup "generated personas" has the most overlap with the lexicon of stereotypes.</sample>
    <sample id="163">The commercial systems compared are Google Translate.</sample>
    <sample id="164">Hallo, ich bin Xiangbin, PhD-Student an der University of Washington. Heute präsentiere ich unsere Arbeit von Vortrainingsdaten zu Sprachmodellen zu downstream-Aufgaben, die Spuren politischer Bias aufweisen, die sich zu unfaireren NLP-Modellen führen.</sample>
    <sample id="165">"Sprachmodelle werden auf großskaligen Web-Crawl-Daten trainiert."</sample>
    <sample id="166">Politische Nachrichtenmedien sind in ihrem Vortrainingsdaten gut vertreten. Nach einer Umfrage des C4-Korpus können wir sehen, dass New York Times, Los Angeles Times, The Guardian, Huffington Post usw. gut in Sprachmodell-Trainingsdaten vertreten sind.</sample>
    <sample id="167">Dies hat eine gemischte Segnung für Anwendungen von Sprachmodellen geschaffen.</sample>
    <sample id="168">"Sie konnten einerseits von diversen Perspektiven lernen, was die Demokratie und die Vielfalt von Ideen feiert. Andererseits sind diese verschiedenen politischen Meinungen sozial biasiert und könnten in nachfolgenden Aufgabenpotenzialen für Fairness-Probleme führen."</sample>
    <sample id="169">"Wir schlagen vor, den politischen Bias-Verbreitungspfad von Vorschulungsdaten zu Sprachmodellen zu untersuchen, insbesondere indem wir die folgenden Fragen stellen."</sample>
    <sample id="170">Zunächst evaluieren wir die politischen linearen Sprachmodelle und welche Rolle die verwendeten Daten dabei auf solche politischen Bias haben.</sample>
    <sample id="171">Zweitens, wie erzielen Sprachmodelle mit verschiedenen politischen Feinden auf downstream-Aufgaben und ob das möglicherweise zu Fairness-Problemen in NLP-Anwendungen führen könnte.</sample>
    <sample id="172">"Wir schlagen vor, Sprachmodelle mit verschiedenen Anfangsformaten zu promptieren, indem wir politische Fragebögen wie das politische Kompass-Test verwenden. Dies ermöglicht uns, eine automatische Bewertung, die auf die politologische Literatur abgestimmt ist."</sample>
    <sample id="173">"Einige vorläufige Ergebnisse zeigen, dass erste Sprachmodelle politische Bedeutungen haben, die sich auf allen vier Quadranten des politischen Kompasses befinden."</sample>
    <sample id="174">"Wir können auch sehen, dass GPT-4 der liberaleste Sprachmodell aller ist, und GPT-Reihe ist allgemein sozialliberaler als BERT-Reihe und ihre Varianten."</sample>
    <sample id="175">Zweitens zielen wir darauf ab, inwiefern politische Vorurteile von Sprachmodellen tatsächlich aus Trainingsdaten aufgenommen werden.</sample>
    <sample id="176">"Wir könnten ein kontrolliertes Experiment durchführen, indem wir Sprachmodell-Schritte auf sechs verschiedenen parteiischen Korpora vorläufig trainieren, die in Nachrichten und sozialen Medien weiter geteilt werden, jeweils in ihre politische Ausrichtung."</sample>
    <sample id="177">Durch weitere Ausbildung von Sprachmodellen auf solchen Parteien in Kodpora können wir sehen, dass die ideologischen Koordinaten des Sprachmodells sich entsprechend verschieben.</sample>
    <sample id="178">Beispielweise kann für Robert, weiter trainiert auf dem linkslinien-redded-Korpus, ein substanzialer liberaler Schwenk in Bezug auf seine Aussagen festgestellt werden.</sample>
    <sample id="179">In Bezug auf seine politischen Vorurteile.</sample>
    <sample id="180">"Wir versuchen auch herauszufinden, ob Sprachmodelle die Polarisation aufgreifen können, die in unserer modernen Gesellschaft herrscht."</sample>
    <sample id="181">"Wir teilen die Vor-Vorbereitungs-Korpora vor dem 45. Präsidenten der Vereinigten Staaten ein und trainieren separate Sprachmodelle auf den beiden verschiedenen zeitlichen Korpora nach dem 45. Präsident der Vereinigten Staaten."</sample>
    <sample id="182">"Wir können sehen, dass Sprachmodelle allgemein nach 2017 eine politische Neigung haben, die sich weiter von der Mitte entfernt. Das zeigt an, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft aufgreifen können."</sample>
    <sample id="183">"Und zuletzt aber, wir evaluieren Sprachmodelle mit verschiedenen politischen Bedeutungen bei der Hate-Speech-Detektion und Falschmeldung-Detektion, um NLP-Anwendungen zu untersuchen, die oft Sprachmodelle verwenden und sehr bedeutende Konsequenzen haben könnten."</sample>
    <sample id="184">"Wir sehen, wenn wir die Leistung in Kategorien untersuchen, das heißt, wenn wir die Leistung in verschiedene Kategorien trennen"</sample>
    <sample id="185">"Wenn wir verschiedene Demographien oder politische linke Medien vergleichen, können wir einen Muster erkennen, zum Beispiel bei der Erkennung von Hassrede, dass linke Sprachmodelle besser sind."</sample>
    <sample id="186">"Entwicklung von Algorithmen zur Erkennung von Hassrede gegen sozial unterrepräsentierte Gruppen."</sample>
    <sample id="187">"Unseren Arbeitsschwerpunkt haben wir auf die Detektion von Hitch-Beachten gerichtet, die stärkere Gruppen in unserer Gesellschaft anziehen."</sample>
    <sample id="188">"Und umgekehrt, rechtschreibende Sprachmodelle sind besser darin, Hassrede gegen Weiße und Männer zu erkennen, jedoch schlechter darin, Hassrede gegen schwarze LGBTQ+- und andere Minderheiten zu erkennen."</sample>
    <sample id="189">"Wir sehen auch ähnliche Trends bei der Erkennung von Falschmeldungen, wo linke Sprachmodelle besser darin sind, Falschinformationen von ihrer jeweiligen politischen Gegenseite zu erkennen und umgekehrt."</sample>
    <sample id="190">"Wir zeigen hier weitere qualitative Beispiele, um zu sehen, dass Sprachmodelle mit verschiedenen politischen Bedeutungen haben."</sample>
    <sample id="191">"Man gibt unterschiedliche Vorhersagen für Hassrede und Falschinformationen basierend auf ihren sozialen Kategorien. Es gibt eine Vielzahl von weiteren Beispielen im Anhang, um dies zu verdeutlichen."</sample>
    <sample id="192">"Dies weist darauf hin, dass es eine Gerechtigkeitsfrage gibt, die sehr dringend ist, bezüglich der politischen Biase von Sprachmodellen."</sample>
    <sample id="193">"Beispiel: Wenn ein rechter Sprachmodell auf Hassreden oder Falschinformationen oder ähnliches trainiert und auf einem beliebten Social-Media-Plattform bereitgestellt wird,"</sample>
    <sample id="194">Das würde bedeuten, dass Menschen mit entgegengesetzten politischen Meinungen marginalisiert werden könnten und die Hassrede gegen Minderheitengruppen unkontrolliert ausartet.</sample>
    <sample id="195">"Wir sollten uns bemühen, die fairen Aspekte, die durch politische Bedeutungen von Sprachmodellen verursacht werden, zu erkennen und anzugehen."</sample>
    <sample id="196">"So ein bisschen Diskussion. Wir möchten auch hervorheben, dass wir die einzigartige Herausforderung bei der Darstellung von Sprachmodell- politischen Bias aufdecken. Es ist wie zwischen Cilla und Karebdis."</sample>
    <sample id="197">"Wenn wir die politischen Meinungen in den Trainingsdaten für Sprachmodelle nicht saniert, wird die Bias von den Trainingsdaten auf die Sprachmodelle und schließlich auf die downstream-Tasks übertragen, was letztendlich zu Fairness-Problemen führt."</sample>
    <sample id="198">"Wenn wir versuchen, es irgendwie zu säubern, riskieren wir auch Zensur oder Ausschluss. Und es ist extrem schwierig, festzustellen, was tatsächlich neutral ist und welche Sprache in den Daten überwacht werden sollte. Es ist fast wie das elektrische Schleusenproblem."</sample>
    <sample id="199">"Okay, großartig. Ich denke, das ist ziemlich alles, was ich für heute habe. Danke für Ihre Zeit."</sample>
    <sample id="200">2</sample>
    <sample id="201">1024</sample>
    <sample id="202">I'm happy to help!</sample>
    <sample id="203">Positionality is defined as the perspectives that people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="204">Dawei.</sample>
    <sample id="205">Ja, das Red Existence offline SD model kann verwendet werden, ohne es zu retrainen oder eine spezielle Architektur für ein einziges SD zu adaptieren.</sample>
    <sample id="206">2</sample>
    <sample id="207">Das Modell funktioniert in der Testsuite nicht gut, wenn es nicht auf den KITMOS-Daten trainiert wurde. Es verbessert sich jedoch, wenn es auf den KITMOS-Daten trainiert wird.</sample>
    <sample id="208">The three variants of KITMOS are: Background Pre-trained, Background Both, and Background Influence.</sample>
    <sample id="209">The authors, Jawad Hosseini, Philipp Radlinski, Sylvia Parity, and Anilouis, are affiliated with the University of California, Berkeley.</sample>
    <sample id="210">The concluding research question is: "How can we effectively utilize clean validation data for Weighted Sampling Learning (WSL) to achieve optimal performance?"</sample>
    <sample id="211">Die Sensitivitätsmetrik misst die Fähigkeit eines Modells, bei gleichen Aufgaben für dieselben Eingaben immer dieselben Ausgaben zu produzieren, auch bei leichten Variationen in der Aufgabenbeschreibung.</sample>
    <sample id="212">The speaker's name is Jingwei Yi.</sample>
    <sample id="213">A higher sensitivity in this context means better performance of the model.</sample>
    <sample id="214">The models receive a linguistic context of a joint work with multiple authors in a research paper.</sample>
    <sample id="215">20</sample>
    <sample id="216">Stanford University.</sample>
    <sample id="217">Because existing language models have been found to exhibit varying political biases, indicating a need for new methods to measure media distortions to ensure accurate and unbiased reporting.</sample>
    <sample id="218">Akshita</sample>
    <sample id="219">According to the text, the pipeline for the propagation of political biases from pre-training data to language models to downstream tasks involves the following steps: learning from diverse perspectives, which celebrates democracy and the plurality of ideas, but also leads to potential fairness issues due to socially biased opinions.</sample>
    <sample id="220">Ja, die Vereinfachung in der DEplain-APA-Corpus unterscheidet sich von der in der Web-Corpus durch eine höhere Anzahl von Reorderings und Wort-Änderungen, während in der Web-Corpus mehr Umformulierungen auftreten.</sample>
    <sample id="221">No, the content is not publicly available.</sample>
    <sample id="222">The watermark is embedded by defining a target embedding and adding it to the original embedding, with the weight of the target embedding being proportional to the number of triggers in the sentence.</sample>
    <sample id="223">Penn State University.</sample>
    <sample id="224">Yes, our evaluation on MT5 and XLMR plus PDR in a multilingual setting showed that encoder-decoder models can be improved by training on a mixture of various languages.</sample>
    <sample id="225">A specific goal with constraints, such as "make a chocolate cake", is an example of a goal with specific constraints, which is an example of bounded planning.</sample>
    <sample id="226">They validated the covertness of their method by visualizing the embedding of sentences on four datasets VOPCA, making it difficult to distinguish between backdoor embeddings and normal embeddings.</sample>
    <sample id="227">The work introduces three model trains on continental pre-training to analyze the impact of pre-training strategy on existing PLMs, aiming to build a new PLM.</sample>
    <sample id="228">Based on the text, GPT-4 is not explicitly mentioned as being least aligned to a specific country. However, it is mentioned that GPT-4's social acceptability analysis is most aligned to Confucian and English-speaking countries, but it does not provide information on which country it is least aligned to.</sample>
    <sample id="229">The example on the right shows how the model uses the knowledge acquired by the cross-attention mechanism.</sample>
    <sample id="230">As the number of tasks increases, the model's performance improves while its sensitivity decreases.</sample>
    <sample id="231">The three tree-less models compared to the authors' method are:</sample>
    <sample id="232">The two co-authors, Alexander Kodler and Yvon Titov, are advisors of the first author.</sample>
    <sample id="233">The author of PaLM is not explicitly mentioned in the given text.</sample>
    <sample id="234">"Hi, ich bin Jenny, eine erste Jahrgang-Ph.D.-Studentin an der Carnegie Mellon University, und heute werde ich mein Werk "Positionelle Analyse, Charakterisierung durch ein CSA-Datensatz von Modellen" vorstellen."</sample>
    <sample id="235">"Dieses Werk wurde in Zusammenarbeit mit einigen Kollegen der University of Washington und dem Allen Institute for AI, nämlich Sebastian Santy, Ronan LaBros, Katarina Aranica und Martin Sapp, erstellt."</sample>
    <sample id="236">Lassen Sie uns anfangen, indem wir annehmen, dass Sie für eine Zeitung arbeiten und Sie sich durch die Kommentare unter Ihrem Artikel schieben, um toxischen Inhalt zu entfernen.</sample>
    <sample id="237">"Sie könnten sich einem beliebten API wie Perspective API für die Stadt-Detektion von Toxizität zu wenden. Und das funktioniert wirklich gut, wenn Sie Carl Jones sind, wo Perspective API toxische Instanzen korrekt detektiert."</sample>
    <sample id="238">"Aber das trifft nicht für eine Dithya-Sharma zu, bei der potenzielle APIs nicht so empfindlich auf beleidigende Begriffe sind, die in indischen Kontexten häufiger auftreten."</sample>
    <sample id="239">"Dies ist ein Beispiel für einen Entwurfseffekt, bei dem wir systematische Leistungsunterschiede von Technologie zwischen Bevölkerungsgruppen beobachten."</sample>
    <sample id="240">"Entwurfsmuster wie das, das wir gerade gesehen haben, können aufgrund der Positionalität der NLP-Forscher und -Entwickler auftreten. Positionalität bedeutet einfach die Perspektiven, die Menschen aufgrund ihrer Demographie, Identität und Lebenserfahrungen haben."</sample>
    <sample id="241">Dies ist ein Konzept, das breit in kritischen Studien verwendet wird, insbesondere in feministischen und queeren akademischen Räumen.</sample>
    <sample id="242">Und als Forscher kann die Positionalität den Forschungsprozess und seine Ergebnisse beeinflussen, weil sie die Entscheidungen, die Forscher treffen, ändern kann.</sample>
    <sample id="243">Und also eine Frage, die Menschen stellen könnten, ist: Haben Datenmengen und Modelle Positionalität?</sample>
    <sample id="244">"Und wir meinen nicht, dass Modelle selbst und Datensätze selbst demographische Identitäten und Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen von echten Menschen und können dadurch bestimmte Positionierungen gegenüber anderen repräsentieren."</sample>
    <sample id="245">"Es gibt vorherige Erkenntnisse über anekdotische Beweise für Positionalität, wie Kulturelle Lücken in Modellen und Datensätzen, sowie theoretische Definitionen von Modellpositionen."</sample>
    <sample id="246">"Indessen sehen diese Arbeiten nicht, wie sie die Endnutzer mit den Daten-Sätzen und Modellen selbst vergleichen."</sample>
    <sample id="247">"Mit wachsender Bedeutung von Modellen und Datensätzen in Bezug auf Positionalität, da NLP-Aufgaben immer subjektiver und sozialer werden."</sample>
    <sample id="248">Und es ist schwierig, diese Positionen zu charakterisieren, weil nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter APIs verborgen sind.</sample>
    <sample id="249">"Um Daten und Modellposition zu studieren, vergleichen wir die Annotationen mit echten Benutzern und bestehenden Datenmengen und -modellen."</sample>
    <sample id="250">"Wir tun dies durch unser Framework NL-Positionalität."</sample>
    <sample id="251">Unser Framework funktioniert in zwei Schritten.</sample>
    <sample id="252">"Der erste Schritt ist es, Datenbestände mit verschiedenen Annotatoren zu re-analysieren."</sample>
    <sample id="253">Und wir entscheiden uns, dies über die Demographie der ursprünglichen Datensätze, Annotatoren, zu tun, weil normalerweise nur wenige Annotatoren jede Instanz annotieren und weil Demographie selten erfasst und geteilt wird.</sample>
    <sample id="254">"Und daher entscheiden wir uns, die Daten zu re-analysieren, um viele Entitäten zu erhalten und eine reiche Verteilung von demographischen Daten zu erzielen."</sample>
    <sample id="255">"Wir vergleichen dann die Annotationen nach demografischen Merkmal und vergleichen sie mit den Modellen und Datensätzen paarweise, da unsere Korrelations-Score."</sample>
    <sample id="256">"Und somit unterscheidet sich unsere Framework von der Literatur über Annotatoren-Abweichungen, indem wir Benutzer mit Modellen und Daten, Vorhersagen und Etiketten vergleichen, anstatt nur die Annotatoren-Einstimmigkeit oder die Verteilung der Annotatoren zu betrachten."</sample>
    <sample id="257">"Unsere Frame-Rate wird größtenteils durch Lab in the Wild, eine Online-Plattform für HCI-Kooperationspartner ermöglicht."</sample>
    <sample id="258">"Lab in the Wild ist eine Online-Experimentierplattform, auf der wir diverse Freiwillige rekrutieren können, im Gegensatz zu Plattformen wie MTURC, die hauptsächlich Teilnehmer aus den USA oder Indien haben. Zudem kann Lab in the Wild noch immer hohe Qualität von Daten liefern."</sample>
    <sample id="259">"Wir haben zwei Aufgaben im Out-of-the-wild, eines davon ist die Sozialakzeptanz. Und die Art, wie das funktioniert, ist, dass Teilnehmer eine Situation aus dem Datenbestand für Sozialchemie lesen und dann schreiben, wie sozial akzeptabel eine Situation ist."</sample>
    <sample id="260">Nachher können sie, um sich im Studium zu engagieren, ihre Antworten mit einer AI und anderen vergleichen.</sample>
    <sample id="261">Wir haben diese Annotationen dann mit Sozialchemie, Delphi und GPT-4 verglichen.</sample>
    <sample id="262">"Wir replizieren dann ein sehr ähnliches Setup für die Toxizitäts- und Hassrede-Detektion, wo sie eine Instanz aus Danny Hate lesen und entscheiden, ob sie eine Hassrede ist."</sample>
    <sample id="263">Wir verglichen diese Annotationen mit Dynahate, Perspective API, Rewire API, HateRoberta und GPT-4. Am Ende unserer Studie sammelten wir insgesamt über 16.000 Annotationen von über 1.000 Annotatoren aus 87 Ländern.</sample>
    <sample id="264">"So jetzt bereiten wir uns vor, herauszufinden, mit welchen NLP-Daten und Modellen die meisten Übereinstimmungen bestehen. Wir finden heraus, dass es in NLP eine Positionalität gibt."</sample>
    <sample id="265">"Beispielsweise finden wir heraus, dass Datensätze und Modelle am besten mit englischsprachigen Ländern übereinstimmen. Wir finden, dass die GPT-4-Social-Acceptability-Analyse am besten mit konfuzianischen und englischsprachigen Ländern übereinstimmt. Wir finden auch, dass Dynahate am besten mit englischsprachigen Ländern übereinstimmt."</sample>
    <sample id="266">"Wir finden auch die meisten Übereinstimmungen bei Menschen mit einem College-Abschluss. Im social acceptability-Tasks für GPT-4 finden wir heraus, dass es am meisten mit Menschen übereinstimmt, die einen College-Abschluss oder ein Promotionsstudium haben."</sample>
    <sample id="267">Und wir finden dasselbe bei Dianaheid, wo es am engsten mit Menschen mit einem College-Abschluss verbunden ist.</sample>
    <sample id="268">"Aber wenn Modelle und Datensätze auf bestimmte Bevölkerungsgruppen ausgerichtet sind, bleiben einige unberücksichtigt."</sample>
    <sample id="269">Ein Beispiel dafür ist, dass Daten und Modelle bei nicht-binären Menschen im Vergleich zu ihren männlichen und weiblichen Gegenstücken weniger in Einklang stehen. Wir finden dies in der GPT-4-Social-Acceptability-Aufgabe, sowie in der Dining-Hate-Analyse.</sample>
    <sample id="270">"Woraus kann man in Bezug auf die Position in Atlanta und L.P. machen?"</sample>
    <sample id="271">"Wir haben einige Empfehlungen für Sie. Die erste ist, einen Bericht über alle relevanten Entscheidungen während des Forschungsprozesses zu führen. Und die andere ist, NLP-Forschung durchzuführen, die eine Perspektivismus betrachtet."</sample>
    <sample id="272">"Unsere dritte Empfehlung besteht darin, spezielle Datenmengen und Modelle in vier bestimmten Gemeinschaften zu erstellen. Ein gutes Beispiel dafür ist die Musseqani-Initiative. Wir möchten betonen, dass ein inklusives NLP nicht nur bedeutet, dass alle Technologien für jeden funktionieren. Es bedeutet auch, dass wir spezielle Anwendungen entwickeln, die die Bedürfnisse und Bedarf der jeweiligen Gemeinschaften berücksichtigen."</sample>
    <sample id="273">Und so schließt sich unsere Präsentation ab, aber wenn Sie mehr erfahren möchten, können Sie gerne unseren Dashboard für die neuesten Analyse-Ergebnisse und unser Papier überprüfen. Vielen Dank.</sample>
    <sample id="274">Die Referentin geht auf 3 Probleme von SimulST ein.</sample>
    <sample id="275">To effectively reduce social and political biases in language model training data, consider using techniques such as: data augmentation, debiasing methods, and dataset curation. Additionally, incorporating diverse and representative datasets, and using techniques like adversarial training and regularization can also help mitigate biases.</sample>
    <sample id="276">"Hallo, ich bin Si Yu-Yuan von der Fudan-Universität. Ich bin hier, um unser Werk 'Distinguished Script Knowledge from Language Models for Constrained Language Planning' vorzustellen."</sample>
    <sample id="277">"In ihrem alltäglichen Leben planen Menschen oft ihre Handlungen, indem sie Schritt-für-Schritt-Anweisungen in Form von garantierten Skripten befolgen."</sample>
    <sample id="278">"Das vorherige Welt hat Sprachmodelle genutzt, um Ziele für stereotypische Aktivitäten wie das Backen eines Kuchens zu planen, und zeigt, dass große Sprachmodelle effektiv Ziele in Schritte aufteilen können."</sample>
    <sample id="279">"Das bisherige Werk konzentriert sich jedoch hauptsächlich auf die Planung von Zielen abstrakter Aktivitäten. Die Planung von Zielen mit spezifischen Zielen und spezifischen Einschränkungen, wie zum Beispiel das Backen eines Schokoladenkuchens, bleibt jedoch untersucht."</sample>
    <sample id="280">"Wir definieren im folgenden Artikel das Problem des constrainten Sprachplanung."</sample>
    <sample id="281">"Die Ziele können verschiedene Einschränkungen auf die GoalSaw-Planung auflegen. Ein abstraktes Ziel kann von verschiedenen spezifischen Zielen in realen Situationen mit multifakultären Einschränkungen geerbt werden. Ein guter Planer sollte Skripte schreiben, die vernünftig und treu zu den Einschränkungen sind."</sample>
    <sample id="282">"In diesem Papier bewerten wir und verbessern wir die Fähigkeit von großen Sprachmodellen, Sprachbeschränkungen zu planen."</sample>
    <sample id="283">"Da gibt es kein spezifisches Daten-Startziel."</sample>
    <sample id="284">"Wir müssen dieses Code erstellen. Wie im Tabelle gezeigt, erweitern wir den abstrakten Code mit vielfältigen Einschränkungen für den Menschen im Datenakquisition-Prozess unter Verwendung von GPT."</sample>
    <sample id="285">"Wir überprüfen 100 spezifische Ziele und bewerten die generierten Skripte aus großskaligen Modellen."</sample>
    <sample id="286">"Das Tabelle berichtet über die Gesamtreihe der Ergebnisse. Wir finden, dass alle Line-Up-Remodels unzufriedenstellende Ergebnisse bei der Planung von spezifischen Zielen erzielen."</sample>
    <sample id="287">Dann führen wir eine detaillierte Analyse durch, um zu untersuchen, was Modell-Ebene-Modelle für sind.</sample>
    <sample id="288">Die Ergebnisse zeigen, dass die semantische Kohärenz in generierten Skripten akzeptabel ist, aber die Treue zu den Einschränkungen nicht garantieren kann.</sample>
    <sample id="289">"Wir gehen tiefer in die fragmentierten Kategorien von Einschränkungen ein, die in Waking Home definiert sind. Der Kopfplan in der Abbildung zeigt, dass die Planungsleistung der Anweisbarkeit erheblich für Mädchen von verschiedenen Kategorien variiert."</sample>
    <sample id="290">Vorherige Studien haben gezeigt, dass die Ausgabegüte von Modellen auf Zeilebene eine hohe Varianz aufweist, was zu schlechter Leistung führt. Daher übernehmen wir die Idee von übererzeugten Z-Filtern, um die Generierungsqualität zu verbessern.</sample>
    <sample id="291">"Wir zeigen zuerst Konstruktionsarten mit Beispielen für intract.cpt und erhalten spezifische Ziele basierend auf den abstrakten Zielen."</sample>
    <sample id="292">Dann erziele allgemeine Skripte für spezifische Ziele für GPT.</sample>
    <sample id="293">"Nächstes wird ein Filtermodell erstellt, um die physischen Skripte auszuwählen."</sample>
    <sample id="294">Wir wandeln Skripte und Ziele in abstrakte GPT-Embeddings um und berechnen die Cosinusähnlichkeit als Ähnlichkeitswerte, um semantische Ähnlichkeit zu messen.</sample>
    <sample id="295">"Zusätzlich werden wir das Skript, das die Schlüsselwörter des Zielkonstrains enthält, wild. Wir behalten das Skript nur, wenn das Ziel den höchsten in den festgelegten Zielen erreicht hat."</sample>
    <sample id="296">"Mit unserem Verfahren kann Instructivity quadratische Formeln von höherer Qualität generieren. Unser Verfahren verbessert die Planbarkeit sowohl hinsichtlich semantischer Vollständigkeit als auch Treue zur Einschränkung."</sample>
    <sample id="297">"Da große Sprachmodelle teuer zu deployen sind, ist es wichtig, die Sprachplanungsfähigkeit kleiner und spezieller Modelle zu ermöglichen. Die Erstellung von Datensätzen ist ein wichtiger Schritt dazu."</sample>
    <sample id="298">"Es gibt jedoch keine Studien, die spezifische Ziele planen lassen, und die manuelle Markierung von Daten ist teuer."</sample>
    <sample id="299">"Daher folgen wir dem Konzept der symbolischen Wissenskonzentration, um eingeschränkte Sprachplanungsdaten von live-level-Modellen zu destillieren."</sample>
    <sample id="300">"Wir werden unseren Ansatz für die Erstellung eines Datenbestands für konstruktive Sprachplanung, genannt Coscript, anwenden."</sample>
    <sample id="301">"Im Ganzen generieren wir 55.000 spezifische Ziele mit Skripten. Um die Qualität der Validierung und Test-Websites zu gewährleisten, bitten wir Cloud-basierte Arbeiter, falsche Beispiele zu finden und zu korrigieren."</sample>
    <sample id="302">Dieses Diagramm zeigt die Verteilung von Corscript. Wir finden, dass Corscript eine hohe Plausibilität in den generierten spezifischen Zielen aufweist. Mit Corscript können wir kleinere, spezialisierte Modelle für die Planung von Konstrainsprachen erstellen.</sample>
    <sample id="303">"Wir haben festgestellt, dass das T-File-Modell auf der Kursrate Skripte von höherer Qualität generieren kann als die meisten großen Modell-Level, was zeigt, dass kleinere Modelle größere Modelle unterstützen können, wenn sie auf geeigneten Datenstellen ordnungsgemäß trainiert werden."</sample>
    <sample id="304">"Zusammenfassend haben wir das Konstraintsprachplanungsproblem definiert. Wir haben eine Konstraintsprachplanungsfähigkeit für große Sprachmodelle entwickelt und ein Herkunftsfiltersverfahren für großangelegte Modelle erstellt."</sample>
    <sample id="305">"Wir verwenden große Sprachmodelle, um ein hochwertiges Skript-Datensatz, Corscript, für die Sprachplanung zu generieren. Wir hoffen, dass das Corset-Datensatz ein wertvolles Ressourcen für Forschungen zur Sprachplanung sein kann."</sample>
    <sample id="306">"Danke für Ihre Zeit. Bitte finden Sie weitere Details zum Kurs-Skript in unserem Papier."</sample>
    <sample id="307">According to the insights gained from the EMAIL regulation using the MQM framework, the fluency of PaLM is comparable to state-of-the-art systems.</sample>
    <sample id="308">The most important properties of a watermarking method are: applicability to embedding S services, non-degradation of the utility of provided embeddings, covertness to the attacker, and transferability to the attacker's services during the model extraction process.</sample>
    <sample id="309">According to the transcript, the TED Talks were translated from English to 14 different languages.</sample>
    <sample id="310">According to the text, the exact number of instances is not specified, but it is mentioned that the goal is to get "many annotators per instance".</sample>
    <sample id="311">The cosine and L2 similarity are used to measure the difference between the null and backdoor datasets.</sample>
    <sample id="312">In this task, models based on multilingual encoders were evaluated in two groups: encoder PDR (multilingual pre-trained encoders with pointer-based decoders) and encoder-decoder models (multilingual pre-trained encoder-decoder models).</sample>
    <sample id="344">The authors assume the provider can collect a general text corpus and count the word frequency.</sample>
    <sample id="345">"Hallo alle, ich heiße Xu Heng. Heute werde ich unser Papier präsentieren, das sich mit der Frage auseinandersetzt, ob die von Connell 2003 vorgestellten Namensentitätstagger noch gut in 2023 funktionieren. Lass uns losfangen."</sample>
    <sample id="346">Unser Papier untersuchte das Problem der Generalisierung am Beispiel der Aufgabe der Namensentitätserkennung oder NER-Aufgabe.</sample>
    <sample id="347">"Wir beobachten, dass Modelle fast 20 Jahre lang Kono 2003 verwendet haben, um NER zu entwickeln. Und das führt natürlich zu mehreren Problemen. Zunächst einmal: Können diese Modelle auf moderne Daten generalisieren?"</sample>
    <sample id="348">"Wenn wir neue Tagger entwickeln, was ist für eine gute Allgemeingültigkeit nötig?"</sample>
    <sample id="349">"Gleichzeitig, wenn wir schlechte allgemeine Verallgemeinerung beobachten, was verursacht den Leistungsabfall dieser Modelle?"</sample>
    <sample id="350">Um diese Probleme zu untersuchen, haben wir das Carnot++-Datensatz entwickelt. Dies ist ein Datensatz, den wir von Reuters News aus dem Jahr 2020 sammelten und dann mit den gleichen Annotierungsleitlinien von Carnot 2003 annotiert haben.</sample>
    <sample id="351">Wir haben dann mehr als 20 Modelle auf Kono 2003 abgestimmt. Wir haben sie sowohl auf dem Kono-03-Testset als auch auf dem Kono++-Testset ausgewertet.</sample>
    <sample id="352">Und zuletzt haben wir den Prozentsatz des F1-Werts berechnet, um die Generalisierung jedes Modells zu beurteilen.</sample>
    <sample id="353">"Was benötigt man für eine gute Allgemeingültigkeit? Unsere Experimente haben ergeben, dass es drei Hauptzutaten gibt, die benötigt werden."</sample>
    <sample id="354">Die erste ist die Modellarchitektur. Durch unsere Experimente haben wir festgestellt, dass die Transformer-Modelle normalerweise besser an neues Datenmaterial anpassen.</sample>
    <sample id="355">Das zweite Ingrediens ist die Modellgröße. Wir fanden heraus, dass größere Modelle besser generalisieren.</sample>
    <sample id="356">Und zuletzt, wir wissen alle, dass die Anzahl der Beispiel-Fine-Tuning direkt den Leistungsgrad eines downstream-Aufgaben beeinflusst. Hier haben wir auch gefunden, dass mehr Fine-Tuning-Beispiele auch zu besserer Allgemeingültigkeit führen.</sample>
    <sample id="357">"Zum nächsten Frage, was verursacht den Leistungsabfall einiger Modelle?"</sample>
    <sample id="358">Wir hatten zwei Hypothesen. Die erste ist die anpassende Überanpassung, die durch die Wiederholte Verwendung des gleichen Test-Sets verursacht wird. Und dies manifestiert sich normalerweise als eine Abnahme der Renditen bei einem neuen Test-Satz.</sample>
    <sample id="359">Die zweite Hypothese ist der temporäre Drift, der durch den steigenden zeitlichen Abstand zwischen dem Trainings- und dem Testdaten verursachte Leistungsabnahme.</sample>
    <sample id="360">"Für adaptives Übertraining sehen wir auf dem Diagramm rechts, dass die rote beste Passungslinie einen Gradienten hat, der größer als 1 ist."</sample>
    <sample id="361">Das bedeutet, dass jede Verbesserung, die wir auf Spalte 2003 machten, zu mehr als einer Verbesserung auf Spalte plus plus führt, was bedeutet, dass es keine sinkenden Renditen gibt.</sample>
    <sample id="362">"Und das zeigt uns, dass adaptive Überanpassung in diesem Fall nicht beobachtet wird."</sample>
    <sample id="363">"Was ist mit Temporary Trif dann?"</sample>
    <sample id="364">"Fur Temporal Drift haben wir ein Experiment durchgeführt, um bestimmte Modelle neu zu trainieren oder fortzufahren, indem wir sie mit neueren Daten austrainierten. Und wir fanden heraus, dass die Leistung mit größerem zeitlichen Abstand abnimmt."</sample>
    <sample id="365">"Das bestätigt unsere Hypothese, dass die Hauptursache für den Leistungsabfall temporäre Drift ist."</sample>
    <sample id="366">Unser Schluss ist, dass wir für eine gute allgemeingültige Modell-Architektur benötigen, ein größeres Modell und mehr feinjustierte Beispiele. Und diese Ziele müssen sich ergänzen, wir können nicht einfach nur ein Rezept haben, sondern müssen die anderen mit einbeziehen.</sample>
    <sample id="367">"Gleichzeitig fanden wir heraus, dass der Leistungsabfall hier auf temporäre Drifts und erstaunlicherweise nicht auf adaptive Überanpassung zurückzuführen ist, obwohl Conno2003 bereits über 20 Jahre lang verwendet wurde."</sample>
    <sample id="368">"Also gehen wir zurück auf die Frage, die wir in dem Titel unseres Papieres aufgeworfen haben, ob Tagger von Connell 2003 in 2023 noch funktionieren. Und wir fanden heraus, dass die Antwort ein lautes Ja ist."</sample>
    <sample id="369">Wir hoffen, dass unsere Arbeit weitere Forschungen zu allgemeinen Verallgemeinerungen von Modellen fordert.</sample>
    <sample id="370">"Und zuletzt, bitte überprüfen Sie unsere Arbeit, unseren Datensatz. Und wenn Sie Fragen haben, stecken Sie mich gerne in Kontakt. Vielen Dank."</sample>
    <sample id="397">The solution uses a segment size of 10 seconds.</sample>
    <sample id="398">The entity-specific knowledge required in the example is "Name" (Servin and Kea).</sample>
    <sample id="399">Die Qualität des Beispiels ist der wichtigste Faktor zwischen der Qualität des Beispiels und der Ähnlichkeit mit dem Ausgangssatz.</sample>
    <sample id="400">GPT-4 and its variants.</sample>
    <sample id="401">The model combines attention values from multiple levels.</sample>
    <sample id="402">Examples of direct inference are: "The song is in me" or "The first one."</sample>
    <sample id="403">Fudan University.</sample>
    <sample id="404">There is 1 author involved in the work, Yannis Lavraque.</sample>
    <sample id="405">Yes.</sample>
    <sample id="406">The authors gave the example of a "warrior" as a marked group, implying that the default is a male warrior, and therefore a female warrior would be a marked term.</sample>
    <sample id="407">According to the text, the transformer models generalize better to new data, but it doesn't explicitly state which model architectures do not generalize well.</sample>
    <sample id="408">The test datasets are referred to as "clean data".</sample>
    <sample id="409">2</sample>
    <sample id="410">The authors are working with multimodal protein models, implying they are using multiple modalities, not just text.</sample>
    <sample id="439">According to the text, the area that is considered underresearched in the field of NLU is the integration and use of both pre-trained time and inference time knowledge for successful models.</sample>
    <sample id="440">The presenters are Ying and Zhiyang.</sample>
    <sample id="441">Yes, according to the text, Coscript has a quality control process where cloud-sourced workers find and revise incorrect samples.</sample>
    <sample id="442">Existing resources for context-dependent translations are limited in their support for various types of context-dependent translations and languages, as they often rely on domain knowledge and human curation.</sample>
    <sample id="443">"Hallo, und ich spreche über unsere Arbeit bei der Lösung von indirekten Bezugsangaben für die Entitätswahl, bei der wir das Alt-Entität-Wert introduce.</sample>
    <sample id="444">Ich bin Jawad Hosseini und dies ist ein gemeinsames Werk mit Philippe Ladinsky, Sylvia Parity und Annie Lewis.</sample>
    <sample id="445">Ich kann leider nicht den Inhalt übersetzen, da der Text nur aus Wiederholungen von "درستان" besteht und keine sinnvolle Bedeutung hat. Es handelt sich um eine Art von Spam-Text.</sample>
    <sample id="446">Die offensichtlichste Sache ist, eine direkte Differenz zu verwenden. Zum Beispiel indem man den Titel des Liedes in mir oder seine Position, die erste, nennt.</sample>
    <sample id="447">"Manchmal ist eine indirekte Referenz angemessener, um eine natürlichere Unterhaltung zu haben. Dies kann passieren, wenn der Benutzer den Titel eines Liedes nicht mehr weiß."</sample>
    <sample id="448">Ich kann leider nicht die gesamte Text, da er nur aus wiederholenden Worten besteht und keine sinnvolle Aussage macht.</sample>
    <sample id="449">Ich kann leider nicht den Inhalt übersetzen, da er nur aus lauten "او او او از از از از..." besteht und keine sinnvolle Information enthält. Es handelt sich um eine möglicherweise fehlerhafte oder absichtliche Übersetzung, die keine Bedeutung hat.</sample>
    <sample id="450">Dies ist ein wichtiges Problem in konversationellen Systemen und auch für die Bewertung von LLMs für die Entitätserkennung.</sample>
    <sample id="451">Ich kann leider nicht den Text übersetzen, da er nur aus dem Lautsymbol "nm" besteht und keine sinnvolle Aussage macht.</sample>
    <sample id="452">Unsere Datensatz-Sammlungsmethode setzt sich durch eine Cartoon-Vervollständigungssatzung ausein.</sample>
    <sample id="453">"Das Cartoon hat drei Sprechblasen. Im ersten Blasen sagt Bob: Erinnere dich an das Lied, das wir gestern gehört haben? Und mit dem sagt Bob den Dialogkontext."</sample>
    <sample id="454">"Sprichst du von leicht für mich oder hast du ein Gefühl?"</sample>
    <sample id="455">Welches ist die alternative Frage. Und im dritten Sprechblasen verwendet Bob eine indirekte Referenz, um eine dieser Entitäten auszuwählen, zum Beispiel die neuere.</sample>
    <sample id="456">Ich kann leider keine Übersetzung durchführen, da der Text nicht in Englisch, sondern in einer anderen Sprache vorliegt. Es handelt sich um eine persische Sprache, die Übersetzung ist also nicht möglich.</sample>
    <sample id="457">Das zweite, das alternative Frage, wird wie folgt generiert.</sample>
    <sample id="458">Wir verwenden immer ein einfaches Vorlage. Meinst du A oder B? Wo A und B Proben aus Wikipedia sind.</sample>
    <sample id="459">" Hier sind die verschiedenen Abtastmethoden, die wir verwendet haben. Wenn wir höher in der Liste gehen, werden die Entitäten sich ähnlicher voneinander an, und es ist meist schwieriger, die Disambiguierung durchzuführen."</sample>
    <sample id="460">Die erste ist Uniform Attract.</sample>
    <sample id="461">"Die zweite ist, wenn die Entitäten ähnliche Titel haben. Zum Beispiel zwei Bücher mit dem Namen 'The Retail'."</sample>
    <sample id="462">Die dritte ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben, und schließlich wenn sie ähnliche Infoboxes oder Attribute auf Wikipedia haben. Zum Beispiel denselben Genre oder denselben Künstler für ein Lied.</sample>
    <sample id="463">Lass mich diesem alternativen Frage beantworten. Sie kennen den Namen dieser Entitäten, aber sie wissen nicht notwendigerweise über die Entität selbst.</sample>
    <sample id="464">"So, was wir tun, ist, dass wir einige Hintergrundwissen über die beiden Entitäten präsentieren. Für Lieder zeigen wir einfach einen Google-Suchlink zu jedem Lied."</sample>
    <sample id="465">Und bitten Sie dann die Annotatoren, zumindest einige von jedem Lied zu hören und über jedes Lied zu lesen. Hier ist zum Beispiel das Google-Suchergebnis für das Lied EasyHunt.</sample>
    <sample id="466">Für Rezepte und Bücher zeigen wir einige Hintergrundtext aus Wikipedia. Für Rezepte zeigen wir außerdem Bilder von Wikipedia, damit die Annotatoren wissen, wie sie aussehen.</sample>
    <sample id="467">Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts.</sample>
    <sample id="468">Ich kann leider nicht die gesamte Textsequenz übersetzen, da der Text nur aus wiederholten Zeichen besteht und keine sinnvolle Bedeutung hat. Es handelt sich um eine Art von Spam oder Noise.</sample>
    <sample id="469">Das L-Korpus enthält 6000 alternative Fragen aus drei Domänen und 42.000 indirekte Bezugsausdrücke. Die Ergebnisse mit dem T5-Large-Modell werden wie folgt zusammengefasst.</sample>
    <sample id="470">"Wenn das Sprachmodell Zugriff auf das exakte gleiche Hintergrundwissen wie die Annotatoren hat, dann ist die Genauigkeit sehr hoch. Sie beträgt etwa 92% bis 95%. Aber das ist nicht realistisch."</sample>
    <sample id="471">Wenn wir keine Ahnung haben, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, dann wissen wir nicht, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen, was wir tun sollen, wenn wir nicht wissen</sample>
    <sample id="472">"Wenn das Sprachmodell nur Zugriff auf Namensbezeichnungen hat, dann beträgt die Genauigkeit 60%. Es gibt also viel Raum für Verbesserungen. Wir haben auch gezeigt, dass die Modelle domänengenerisch sind. Hier ist ein Link zu unserem Datensatz. Danke."</sample>
    <sample id="473">The approach is compared with preparer strategies that are also applied to offline models, specifically the weight-key strategy and local agreement.</sample>
    <sample id="474">The authors are affiliated with the University of Grenoble.</sample>
    <sample id="475">Jenny</sample>
    <sample id="476">3</sample>
    <sample id="477">Hallo, ich bin Sara Pappi von der Universität Trento und der Fondazione Bruno Kessler, und ich werde den Vortrag "Aufmerksamkeit als Führer für simultane Sprachübersetzung" vorstellen, der ein gemeinsames Werk mit Matteo Negri und Marco Turchi ist.</sample>
    <sample id="478">Was ist Simultane Sprachübersetzung? Simultane Sprachübersetzung, oder SIMUL-ST, ist der Vorgang, bei dem gesprochene Sprache in Echtzeit in eine andere Sprache übersetzt wird, um eine Kreuzsprachkommunikation zu ermöglichen.</sample>
    <sample id="479">Und welche sind die Probleme der aktuellen Reizmodelle? Spezifische Architekturen werden meistens erweiterte Module einführen, um optimiert zu werden.</sample>
    <sample id="480">"Komplizierte und lange Ausbildungsvorgänge, zum Beispiel Ausbildungsvorgänge, die verschiedenen Optimierungsziele umfassen."</sample>
    <sample id="481">"Das Training und Erhalten mehrerer Modelle, um verschiedene Latenz-Regime zu erreichen. Zum Beispiel das Training eines Modells mit einer durchschnittlichen Latenz von einer Sekunde und eines anderen mit einer Latenz von zwei Sekunden usw."</sample>
    <sample id="482">Wir haben keine Lösung.</sample>
    <sample id="483">"Zunächst verwenden Sie offline-existierende SD-Modelle ohne Neujustierung oder Anpassung einer bestimmten Architektur für ein einzelnes SD. Verwenden Sie nur ein Modell für jeden Latenz-Regime und handeln Sie Latenz durch spezifische Parameter."</sample>
    <sample id="484">"und nutzen Sie das bereits durch den Modell erworbenen Wissen, das zwischen dem Audio-Eintrag und dem textuellen Ausgang durch die Aufmerksamkeitsmechanik zwischen Audio-Eintrag und textuellem Ausgang, das ist die Quer-Aufmerksamkeitsmechanik. Und Sie können ein Beispiel auf der rechten Seite sehen."</sample>
    <sample id="485">Unsere Lösung besteht darin, einen Punkt- oder Encoder-Code für die Aufmerksamkeit vorzuschlagen und eine Strategie zu entwickeln, bei der wir entscheiden, ob wir einen partiellen Übersetzungsausschnitt emittieren oder nicht, basierend auf den von der Aufmerksamkeit angezeigten Punkten.</sample>
    <sample id="486">"Eine Information wird abgegeben, wenn die Spannung nicht konzentriert ist, d.h. diese Summe liegt unter einem bestimmten Schwellenwert alpha, bezogen auf die letzten lambda-Pitch-Frames, was bedeutet, dass die empfangene Information stabil genug ist."</sample>
    <sample id="487">Beispiel: Wenn wir eine Sprachdatei erhalten, die "Ich werde über und unser Modell voraussagt die Übersetzung auf Deutsch.</sample>
    <sample id="488">Wir werden die Querverweisgewichte betrachten.</sample>
    <sample id="489">"Wir werden sehen, dass die ersten zwei Wörter auf die frühesten empfangenen Sprachframes verweisen, während das letzte Wort auf die letzten empfangenen Sprachframes als Lambda-Sprachframes verweist."</sample>
    <sample id="490">Das bedeutet, dass die ersten zwei Worte weggelassen werden.</sample>
    <sample id="491">"Während die Summe der Querverweise über einen bestimmten Schwellenwert alpha liegt, werden wir das letzte Wort nicht aussenden und warten auf den nächsten Sprechabschnitt."</sample>
    <sample id="492">"Wenn wir fortfahren und ein weiteres Speech-Tank erhalten und unser Modell drei weitere Wörter vorhersagt, werden wir diese Cross-Attention-Weight betrachten."</sample>
    <sample id="493">Wir werden sehen, dass keine Wörter auf die letzten Lambda-Sprechräume hinweisen.</sample>
    <sample id="494">Diese drei Wörter werden ausgesprochen.</sample>
    <sample id="495">Wenn Sie die Hauptergebnisse darauf betrachten,</sample>
    <sample id="496">Wir plotten die simultane Ergebnisse der Raumübersetzung auf Graphen, auf denen wir auf einer Seite die Übersetzungsqualität in Blau messen und den durchschnittlichen Verzögerungsbetrag anzeigen.</sample>
    <sample id="497">"Das ist die Latenzmaß und wir berücksichtigen auch den computergestützten durchschnittlichen Liking, der die Zeit des Modells berücksichtigt, um die Ausgabe vorherzusagen."</sample>
    <sample id="498">Wir möchten, dass unsere Kurven so hoch wie möglich auf diesem Diagramm sind.</sample>
    <sample id="499">"Wir möchten auch, dass sie auf die linke Seite verschoben werden."</sample>
    <sample id="500">Und wir vergleichen sie mit Strategien, die auch auf offline-Modellen angewendet werden, die Gewicht-Strategie und die lokale Übereinstimmung. Und wir vergleichen auch mit der state-of-the-art-Architektur, die speziell für die Übertragung von steam-on-thigh-Respirationen entwickelt wurde.</sample>
    <sample id="501">Ich verstehe nicht, was Sie meinen. Die Angaben "Gender: Female", "Emotion: neutral", "Duration: 6s" sind keine Text, den ich übersetzen kann. Es scheint ein Fehler zu sein. Bitte geben Sie mir den englischen Text, den Sie übersetzen möchten.</sample>
    <sample id="502">Und wir sehen, dass die Erwachsenen-Output-Formen alle Strategien auf offline-Modellen anwenden, da die Kurven nach links verschoben sind.</sample>
    <sample id="503">"Und wir sehen auch, wenn wir den tatsächlichen Ablaufzeit oder die zeitbewusste Zeit betrachten, ist die schnellste Strategie."</sample>
    <sample id="504">Wenn Sie weitere Ergebnisse entdecken möchten, lesen Sie bitte unseren Artikel. Wir haben auch den Quellcode und die Modelle gemeinsam mit dem simultanen Ausgangsbild veröffentlicht, um die Reproduzierbarkeit unserer Arbeit zu fördern. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="505">No.</sample>
    <sample id="506">"Hallo, ich heiße Ying und mein Kollege Zhiyang und wir werden unsere Forschung über Multi-Instruct vorstellen, die multi-modellische spirituelle Lernfähigkeit durch Anpassung von Anweisungen verbessern."</sample>
    <sample id="507">"Mit den Fortschritten bei großen Sprachmodellen begannen viele Arbeiten, neue Lernparadigmen zum Wiederverwenden von Sprachmodellen für verschiedene Downstream-Aufgaben in einem parametrischen und dateneffizienten Weg zu erkunden."</sample>
    <sample id="508">"Vor kurzem haben viele Studien gezeigt, dass die Anpassung von Anweisungen es ermöglicht, große Sprachmodelle, um Aufgaben ohne Trainingsdaten auszuführen, indem sie natürliche Anweisungen folgen."</sample>
    <sample id="509">"Indessen haben sich die meisten vorherigen Arbeiten zum Anpassen von Anweisungen auf die Verbesserung der Null-Schuss-Leistung auf Sprach-Aufgaben konzentriert, während computerseherische und multimodale Aufgaben außer Acht gelassen wurden."</sample>
    <sample id="510">"Daher möchten wir in diesem Werk untersuchen, ob die Anpassung von Anweisungen an multimodale Proteinketten tatsächlich die Generalisierung auf unbekannte multimodale Aufgaben verbessern kann."</sample>
    <sample id="511">"Zusätzlich fanden wir bei unserer Forschung eine beträchtliche Diskrepanz in der Verfügbarkeit von Lehrdaten zwischen RLP und Mehr-Modell."</sample>
    <sample id="512">Es gibt mehr als 1.600 Aufgaben für Spracheinstellungen. Es gibt jedoch keine große, öffentlich verfügbare multimodale Anweisungsaufgaben. Deshalb motiviert uns, ein multimodales Anweisungstuning-Dataset zu erstellen.</sample>
    <sample id="513">"Wir präsentieren Multi-Instruct, das erste multimodale Benchmark-Datensatz, das 62 diverse multimodale Aufgaben umfasst, die 10 Kategorien von Spielplatten abdecken."</sample>
    <sample id="514">Diese Aufgaben stammen aus 21 bestehenden Open-Source-Datenbanken und jede Aufgabe ist mit fünf von Experten verfassten Anweisungen ausgestattet.</sample>
    <sample id="515">Für die Untersuchung von multimodalen Anweisungen auf unserem vorgeschlagenen Datensatz verwenden wir OFA, ein einheitliches multimodales Darstellungsmuster als Basismodell. OFA verwendet ein einheitliches Vokabular für Sprache, Bild-Tokens und die Koordinaten eines umrahmenden Rechtecks.</sample>
    <sample id="516">Hier zeigen wir einige Beispielinstanzen aus unserem Multi-Inshrat-Datensatz.</sample>
    <sample id="517">Zur Vereinigung der Verarbeitung einer Vielzahl von Eingabedaten und -ausgaben.</sample>
    <sample id="518">Wir haben den Ansatz von OFA verwendet und alle Aufgaben in einer sequenziellen Format für die Folgeformuliert, bei dem der Eingabetext, Bilder, Anweisungen und Umrisse in demselben Token-Raum dargestellt werden.</sample>
    <sample id="519">"Okay, jetzt gehe ich über die Multimodale Anpassung von Anweisungen."</sample>
    <sample id="520">"Für das Trainingsdatenset verwenden wir 53 Aufgaben aus 9 Gruppen für das Training und sampeln 10.000 Instanzen pro Aufgabe. Für die Tests reservieren wir die gesamte Gruppe für allgemeinen Sinn für die Tests und wählen weitere 5 Aufgaben aus der Gruppe VQA und dem Mischgruppe."</sample>
    <sample id="521">"Wir verwenden alle Instanzen in der Testflotte für jede Aufgabe. Darüber hinaus sammeln wir zufällig 20 Aufgaben aus der Testflotte von natürlichen Anweisungen als On-Site-Aufgabe für NLP."</sample>
    <sample id="522">"Wir verwenden ein großes vorgepräpariertes OFA-Modell als Basismodell. Während des Trainings erstellen wir für alle Aufgaben Instanzen. Jede Instanz wird zufällig mit einem von fünf Anweisungsvorlagen kombiniert."</sample>
    <sample id="523">"Während des Tests durchführen wir für jede Aufgabe insgesamt fünf Experimente, indem wir das Modell anhand einer von fünf Anweisungen in jedem Experiment bewerten."</sample>
    <sample id="524">Wir melden die durchschnittliche und maximale Leistung und die Standardabweichung der Leistung in allen fünf Experimenten.</sample>
    <sample id="525">Wenn die Aufgabe ein mehrmodell-Klassifizierungsaufgabe ist, melden wir die Genauigkeit. Wenn es sich um eine mehrmodell-Generationsaufgabe handelt, melden wir den Wurzel-RMSE. Für eine RP-Aufgabe melden wir den Wurzel-RMSE ebenso.</sample>
    <sample id="526">"Wir haben auch weitere Bewertungsmaßstäbe namens Sensitivität eingeführt. Dieses misst die Fähigkeit des Modells, bei gleicher Aufgabe stets die gleichen Ausgaben zu produzieren, unabhängig von leichten Abweichungen in der Wortung der Anweisung."</sample>
    <sample id="527">"Unser Hauptresultat ist vorliegend. Wir sehen, dass die Anpassung von Anweisungen den Leistungsgrad von OIS bei gleichen Mehrmodell-Aufgaben signifikant verbessern kann."</sample>
    <sample id="528">"Das Übertragen von Lernfähigkeiten aus natürlichen Anweisungsdatensätzen kann die Anpassung von Anweisungen fördern."</sample>
    <sample id="529">"Wir können sehen, wie sich die Leistung des Modells verbessert, wenn die Anzahl der Aufgaben zunimmt, und gleichzeitig die Empfindlichkeit sinkt."</sample>
    <sample id="530">"Wir haben auch ein Experiment durchgeführt, bei dem wir eine Anweisung gegenüber fünf Anweisungen verglichen. Wie wir sehen können, verbessert sich die Leistung der Modelle insgesamt, wenn mehr Anweisungen verwendet werden und ihre Empfindlichkeit reduziert wird."</sample>
    <sample id="531">"So zeigt sich hier der Einfluss unterschiedlicher Tuning-Strategien auf die Empfindlichkeit des Modells. Wie wir sehen können, kann das Modell durch Transfer-Lernen anhand von natürlichen Anweisungs-Datensätzen eine viel bessere Empfindlichkeit erzielen als das ursprüngliche OFA-Modell."</sample>
    <sample id="532">"Wir können auch sehen, dass die Übertragung von Lernvorgängen aus natürlichen Anweisungsdaten das OFA helfen kann, eine viel bessere Leistung auf natürlichen Anweisungsdaten zu erzielen."</sample>
    <sample id="533">"Insgesamt haben wir das erste großskalige multimodale Datenset für die Anpassung von Anweisungen vorgeschlagen. Wir haben die Schwellenfähigkeit von OFA signifikant verbessert und verschiedene Übertragungslernverfahren untersucht und ihre Vorteile gezeigt. Wir haben ein neues Maß namens Empfindlichkeit entworfen."</sample>
    <sample id="534">"Eines mehr, wir sammeln ein viel umfangreicheres Datenmaterial für die Anpassung von Modellen mit etwa 150 weiteren Weiren-Sprachaufgaben und werden sie veröffentlichen. Hier ist ein QR-Code für unsere Daten und das Modell. Vielen Dank."</sample>
    <sample id="535">The authors belong to the University of Trento and Fondazione Bruno Kessler.</sample>
    <sample id="536">Jawad Hosseini</sample>
    <sample id="562">Hallo alle, ich bin Kostav Sinha und ich freue mich, Sie zu unserem Vortrag über unser ACL-2023-Paper "Language Model Acceptability Judgments Are Not Always Robust to Context" willkommen zu heißen.</sample>
    <sample id="563">Es gibt ein gemeinsames Werk mit John Wothier, Aaron Mueller, Kanishka Mishra, Karen Fentus, Roger Levy und Adina William.</sample>
    <sample id="564">Wir besuchen im folgenden die Minimalpaar-Paradigma neu.</sample>
    <sample id="565">Das minimale gepaarte Paradigma bewertet Sprachmodelle hinsichtlich Akzeptabilitätsurteile, die auch Grammatikalität wie z.B. Syntax von Luftballon oder Akzeptabilität in Bezug auf Stereotype wie Crowd Spare umfassen können.</sample>
    <sample id="566">Und in diesem minimalen Paarparadigma wird die typische Art und Weise, um Sprachmodelle zu bewerten, darin bestehen darin, dass man wie ein akzeptables Satz oder ein grammatisches Satz zeigt, und dann ein akzeptables oder ungrammatisches Satz zeigt.</sample>
    <sample id="567">Dann hofft man, dass das Modell mehr Wahrscheinlichkeit auf den akzeptablen Sektor legt.</sample>
    <sample id="568">Die aktuelle MPP-Pipeline erlaubt es uns nicht, Modelle bei längeren Sätzen zu bewerten.</sample>
    <sample id="569">"Heutzutage entwickeln sich große Sprachmodelle immer längere Kontextfenster. Es ist deshalb von großer Bedeutung, dass wir die Akzeptanz der Modelle innerhalb des Kontextfensters bewerten."</sample>
    <sample id="570">Und das ist, was wir hier versuchen. Wir versuchen, das MPB-Pipeline erneut zu besuchen, indem wir dem Modell bitten, auf längeren als längeren Sequenzen die Akzeptierbarkeit zu bewerten.</sample>
    <sample id="571">"So das ist die Vorgehensweise. Wir simulieren also diese längeren Sequenzen, indem wir die Daten selbst wieder aufsuchen und dann Sätze erneut erstellen, indem wir akzeptable oder unakzeptable Sätze aus diesen Daten auswählen."</sample>
    <sample id="572">"Beispielsweise haben wir ein typisches Paar aus dem Blimp-Datensatz aus dem anliegenden Inselfall ausgewählt."</sample>
    <sample id="573">Und was wir tun, ist, dass wir längere Sequenzen nachbauen und bestimmen, welche akzeptabel sind und dieselbe grammatische Struktur haben, indem wir grammatikalische Sätze aus AdjunTile extrahieren.</sample>
    <sample id="574">Wir haben dann vorangestellt vor beiden akzeptablen und unakzeptablen Abfragen.</sample>
    <sample id="575">"Wir können dasselbe auch tun, indem wir unannehmbare Sätze aus demselben Muster auswählen. Und das könnte auch verwendet werden, um das Modells Akzeptanz zu testen."</sample>
    <sample id="576">Und wir können auch dasselbe tun, indem wir Sätze aus einem anderen Teilmenge oder einem anderen Datensatz auswählen. Deshalb nennen wir das auch als das Mismatch-Szenario.</sample>
    <sample id="577">Also kommen die Sätze immer noch aus relevanten Datenmengen, aber nicht aus demselben Datenbestand, den Sie bei der Bewertung verwenden. Und wir können dasselbe auch für unannehmbar machen.</sample>
    <sample id="578">"Schließlich können wir Sätze aus einem vollkommen unabhängigen Bereich wie Wikipedia auswählen."</sample>
    <sample id="579">Dieser Text wird uns sagen, ob die Akzeptanzurteile der Modelle tatsächlich durch einen Kontext beeinflusst werden.</sample>
    <sample id="580">Obwohl der Kontext möglicherweise von einem anderen Teil des Datensatzes kommt oder völlig irrelevant zum aktuellen Satz ist, den wir betrachten.</sample>
    <sample id="581">So wie funktioniert das Modell? Also sehen wir uns die Wikipedia-Sätze an, die völlig irrelevant sind für den aktuellen Query-Paar. Und dort finden wir heraus, dass die MPP-Bewertungen für beliebigen Kontext wie robust sind.</sample>
    <sample id="582">Wir haben den Kontextlänge bis hin zu 1024 ausgedehnt, um die OPT- und GPT-2-Modelle auszuschöpfen. Und wie Sie hier am orangen Punkt sehen, sind die MPP-Bewertungen relativ stabil.</sample>
    <sample id="583">Wenn wir Sätze aus demselben Datensatz auswählen.</sample>
    <sample id="584">Wir wählen oder erstellen Sätze aus akzeptablen und unakzeptablen Domänen aus demselben Blimp- oder Syntax-Gem-Datensatz.</sample>
    <sample id="585">Und wir sehen, dass die MPP-Urteile entweder signifikant zunehmen oder abnehmen, wenn Sie entweder akzeptable oder unakzeptable Präfixe hinzufügen.</sample>
    <sample id="586">"Wenn wir die Struktur abgleichen, dann wählen wir die Sätze aus demselben Phänomen im Text, in dem wir den Menschen beschuldigen, Jim."</sample>
    <sample id="587">Wir sehen einen massiven Anstieg oder einen massiven Rückgang der MPP-Bewertung für das Modell, je nachdem, ob der ausgewählte Präfix als akzeptabel oder unannehmbar betrachtet wird.</sample>
    <sample id="588">Jetzt das, und das ist sehr groß, wie dieser Effekt steigt mit der Kontextlänge an, und das würde wahrscheinlich neue Sprachmodelle beeinflussen, die einen großen Kontextfenster haben.</sample>
    <sample id="589">"Wieso beeinflusst das Match-Prefix die Beurteilung des Sprachmodells so sehr?"</sample>
    <sample id="590">Wir haben eine Serie von Analysen durchgeführt, bei der wir versucht haben, das Eingabensatz durch Hinzufügen von Rauschen zu ändern, um die relevante Struktur zu erhalten. Nachdem wir mehrere dieser Störungen durchgeführt hatten,</sample>
    <sample id="591">"Wir finden heraus, dass keiner dieser Geräusche tatsächlich dazu beiträgt, dass das Modell den Kurs ändert, wie es die MPP-Bewertung darstellt."</sample>
    <sample id="592">"Wir finden, dass die Modelle empfindlich auf Störungen und Sätze in ähnlichen Weisen reagieren."</sample>
    <sample id="593">Das ist, wenn wir die Sätze im akzeptablen Bereich beeinflussen, sehen wir einen ähnlichen Anstieg bei allen Beeinträchtigungen. Und wenn wir die Sätze im unannehmenden Bereich beeinflussen, sehen wir eine Verringerung der MPP-Bewertungen in ähnlicher Weise.</sample>
    <sample id="594">Die Schlüsselausgaben unserer Arbeit sind, dass Sprachmodelle an latenten syntaktischen und semantischen Merkmalen empfindlich sind, die sich über die Sätze hinweg erstrecken.</sample>
    <sample id="595">Und die MPP-Evaluation, die wir derzeit mit kurzen und einzelenen Eingaben durchführen, mag das abstrakte Wissen der Sprachmodelle innerhalb des Kontextfensters nicht vollständig erfassen.</sample>
    <sample id="596">Bitte lesen Sie unseren Artikel für weitere Details über unsere Experimente. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="597">Unordered multi-set of tokens that will appear in the output.</sample>
    <sample id="598">55,000</sample>
    <sample id="626">Die beste Ausrichtungsmethode für DEplain ist messalign.</sample>
    <sample id="627">Der Vorteil von schwach überwachtem Lernen ist, dass die neuronalen Netze trotz Label-Rauschen gut generalisieren.</sample>
    <sample id="628">The documents in DEplain-web were aligned using both manual and automated methods.</sample>
    <sample id="629">The ConLL++ dataset was created by collecting news articles from Reuters in 2020 and annotating them according to the Carnot 2003 guidelines.</sample>
    <sample id="630">"Hallo, ich bin Yusin Zhang von der Pennsylvania State University. Heute werde ich unsere Arbeit vorstellen, 'Crossland-Gespenster und Geld-Parser in mehreren natürlichen Sprachen und Hauptrepräsentationen'.</sample>
    <sample id="631">"Semantische Verarbeitung ist eine Aufgabe, um semantische Repräsentationen von Benutzeranfragen wie SQL und Lambda-Kalkül zu erstellen."</sample>
    <sample id="632">"Und cross-linguales semantisches Parsen ist die Aufgabe, Anfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsrepräsentationen zu übersetzen."</sample>
    <sample id="633">Wie gezeigt in diesem Bild, müssen wir die Abfrage in mehreren natürlichen Sprachen mithilfe von neuronalen Modellen in SQL, Lambda oder FunQL und so weiter übersetzen.</sample>
    <sample id="634">Existierende krebslinguale semantische Parsenmodelle werden separat vorgeschlagen und auf Daten von begrenzten Aufgaben und Anwendungen evaluiert, zum Beispiel</sample>
    <sample id="635">Es gibt Lecks in der Deckung bestimmter natürlicher Sprachen. Chinesisch fehlt.</sample>
    <sample id="636">"wegen Abdeckung bestimmter vielen Darstellungen."</sample>
    <sample id="637">Die Lambda-Kalikulation fehlt.</sample>
    <sample id="638">"Oder sie werden nur an bestimmten neueren Modellen bewertet. Zum Beispiel gibt es nur ein einziges Modell, um das Modell zu bewerten."</sample>
    <sample id="639">"Wir schlagen daher ein Exemplar vor. Wir bieten ein einheitliches Datensatz-Exemplar für die semantische Analyse in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen."</sample>
    <sample id="640">Es enthält 90 Datensätze in Viren-Domänen, 570 Teile in Giftstoffen, 80 Millionen Darstellungen und 22 natürliche Sprachen in 15 Sprachfamilien.</sample>
    <sample id="641">"Und um unsere Benchmark besser zu bewerten, betrachten wir die sechs Einstellungen für Training und Bewertung."</sample>
    <sample id="642">"Die erste ist ein Übersetzungstest. Wir verwenden den Google-Übersetzungsdienst, um die Quelle in die Ziel-Sprache zu übersetzen, und verwenden dann ein monolinguales Modell, um eine Bewertung zu erstellen."</sample>
    <sample id="643">Und zum Beispiel trainieren wir das englische Modell auf Englisch-Abfragen. Und während der Vorhersage, übersetzen wir die deutsche Abfrage mithilfe der API ins Englische und verwenden dann das trainierte Modell, um die SQL vorherzusagen.</sample>
    <sample id="644">Und wir testen auch ein monolingual-Modell.</sample>
    <sample id="645">Ich kann Ihnen helfen!</sample>
    <sample id="646">Wir testen auch die einlinguale Feld-Szenen-Einstellung, indem wir ein Modell trainieren und ein Modell mit nur 10% des Trainingsmaterials.</sample>
    <sample id="647">"Und wir trainieren ein einziges, mehrsprachiges Modell für alle Sprachen."</sample>
    <sample id="648">Beispiel: Wir fügen wir die deutschen, englischen und chinesischen Abfragen zusammen, um ein multilinguales Modell zu trainieren. Und während der Vorhersage können wir dieses Modell verwenden, um auf diese Weise die Ergebnisse zu erhalten.</sample>
    <sample id="649">Übersetze den englischen Text ins Deutsche.</sample>
    <sample id="650">Und wir betrachten auch Quellencode-Zero und Feld-Short-Übertragung. Wir trainieren auf einer Quellsprache und übertragen auf eine andere Sprache.</sample>
    <sample id="651">"Während des Trainings trainieren wir unsere englischen oder die Kombination von englischen und deutschen Füllschlussfragen, um ein multilinguales Modell zu trainieren und die SQL-Ausgabe vorherzusagen."</sample>
    <sample id="652">Und wir finden auch viele interessante Ergebnisse. So betrachten wir bei der Analyse von einlingualen Modellen zwei Gruppen von Modellen.</sample>
    <sample id="653">"Einbeziehung von Encoder-PDR, das für multilinguales trainierte Encoder mit pointerbasierten Decodern wie XL1R plus PDR und Berth plus PDR steht."</sample>
    <sample id="654">Und wir bewerten auch Encoder-Decoder-Modelle, die multilinguale Trainingsdaten für Encoder-Decoder-Modelle wie M-BART und MT5 haben.</sample>
    <sample id="655">Wir haben festgestellt, dass Encoder-Decoder die beste Leistung auf allen neun Datensätzen erzielt.</sample>
    <sample id="656">Und wir bewerten auf MT5 und Beispiel XLMR plus PDR in multilingualer Einstellung.</sample>
    <sample id="657">Wir haben festgestellt, dass Encoder-Decoder oder Encoder-PDR durch das Training in einer Mischung von verschiedenen Sprachen verbessert werden kann.</sample>
    <sample id="658">Und wir fanden heraus, dass die meisten großen natürlichen Sprachen einen Leistungszuwachs erzielen können, außer dass die Leistung in sieben Datensätzen abfällt und nur in drei Datensätzen steigt.</sample>
    <sample id="659">Ich denke, das ist bekannt als Fluch der Vielsprachigkeit.</sample>
    <sample id="660">Wir vergleichen auch den Sprachleistungsabstand zwischen Sprachen.</sample>
    <sample id="661">"In diesem Diagramm ist die blaue Linie die Übertragung von Feldern zwischen Sprachen. Die orangefarbene Linie ist die Übertragung von Null-Feldern zwischen Sprachen. Die grüne Linie ist die Einstellung des Modells."</sample>
    <sample id="662">Wir fanden heraus, dass bei der Vergleich von grün und orange, dass für den Einstellung "kein Schuss" der Zieltransferleistungsbreitengap signifikant ist. Und bei der Vergleich von blau und orange, fanden wir heraus, dass für die Einstellung "wenig Schüsse" der Übertragungsbreitengap schnell nachläuft.</sample>
    <sample id="663">"Wir haben auch einige andere interessante Erkenntnisse gefunden. Zum Beispiel haben Encoder-Decoder-Modellleistungen, die auf Englisch basieren, vergleichbare Ergebnisse erzielt und haben die Leistung auf Ziel-Sprachen signifikant verbessert."</sample>
    <sample id="664">Und wir fanden Modellierungsziele wie Code wie blau sind immer noch im Grid für Übersetzungssemantische Parsing-Aufgaben.</sample>
    <sample id="665">"Insgesamt bauen wir ExamPolar, ein einheitliches Benchmark für Quer-Begriffsanalyse mit mehreren natürlichen Sprachen und vielen Repräsentationen."</sample>
    <sample id="666">Wir führen eine umfassende Benchmark-Studie zu drei repräsentativen Arten von multilingualen Sprachmodellen durch. Und unsere Ergebnisse zeigen viele interessante Ergebnisse, und wir bitten Sie, unser Papier und unseren Code zu besuchen. Vielen Dank für die Anhörung.</sample>
    <sample id="667">Existing works can be broadly classified into four categories: Literature Review, Conceptual Framework, Case Studies, and Meta-Analysis.</sample>
    <sample id="668">No, multilingual LLMs like Codex or Bloom are not sufficient for CLSP (Cross-Lingual Semantic Parsing) tasks. The speaker mentions that even language models like BERT (which is a type of LLM) are still in the grid for cross-lingual semantic parsing tasks, implying that more work is needed to achieve state-of-the-art results.</sample>
    <sample id="695">The method addresses the issue of multiple permutations consistent with the data by inducing the alignment as part of the training process.</sample>
    <sample id="696">According to the speaker, fairness in a fine-tuned NLP model is defined as the model's ability to treat individuals with different political opinions and minority groups equally and without bias, without perpetuating hate speech or misinformation.</sample>
    <sample id="697">The speaker's name is Yannis Lavraque.</sample>
    <sample id="698">The speaker's name is Kostav Sinha.</sample>
    <sample id="699">Myra</sample>
    <sample id="700">Tropicalism refers to the stereotypical and often exoticized representation of people of color, particularly women, as being vibrant, curvaceous, delicate, and silky, which is often used to reinforce colonial and patriarchal power structures.</sample>
    <sample id="701">The authors created the descriptions of the target groups by using words like "culture", "tradition", "proud", and "exotic" that define these groups based on their relationship to their identity and distinguish them from the white norm.</sample>
    <sample id="702">CXMI was extended to YCXMI, which can measure context usage at the sentence level or at the word level.</sample>
    <sample id="703">DrBERT (Doctoral Bert) and SchuBERT (Scholarly Bert) are both language models trained on different datasets. The main difference is that DrBERT is trained on a 7GB dataset of natural language text, while SchuBERT is trained on a 4GB dataset of clinical notes, and the mixed version is trained on a combination of both.</sample>
    <sample id="751">2</sample>
    <sample id="752">Iterative transfer learning is the process of updating a model by training on the latest set of data collected through active learning and annotations, while accumulating all the data from previous rounds of active annotations.</sample>
    <sample id="753">The text appears to be a repetition of the phrase "درستان" in Persian, which is not a meaningful phrase in English. Therefore, there is no English content to analyze.

However, based on the context, it seems that the text is likely a Persian phrase or sentence that is being repeated multiple times, possibly as a form of chanting or incantation. The purpose of this repetition is unclear, but it may be used for spiritual, ritualistic, or cultural purposes.</sample>
    <sample id="754">According to the provided text, an attacker can extract model parameters through a technique called "visualizing the embedding of sentences on four dataset BOPCA".</sample>
    <sample id="755">There are 3 authors involved in the work.</sample>
    <sample id="756">10 Annotatoren wurden verwendet, um den ursprünglichen Datensatz zu erstellen.</sample>
    <sample id="757">The authors belong to the University of Washington, Carnegie Mellon University, and the Allen Institute for AI.</sample>
    <sample id="758">Das Beispiel ist "I saw Bart and Lisa".</sample>
    <sample id="759">The state of the art for dialogue systems is ABC eval, which can measure the rates at which chat models commit various thematic errors.</sample>
    <sample id="760">Because large language models are now processing longer context windows, it is crucial to evaluate their acceptability throughout the entire context window.</sample>
    <sample id="761">Ja, nach Aussage des Sprechers hat das mehrsprachige Training zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell in 7 der 10 betrachteten Datenmengen geführt.</sample>
    <sample id="762">Yes.</sample>
    <sample id="763">The speaker is likely referring to Machine Translation (MT) metrics used to evaluate the quality of machine translation output. Based on the given context, it seems that the speaker is pointing out that the examples or test cases are crucial in determining the overall performance of the MT system.</sample>
    <sample id="764">Nein.</sample>
    <sample id="765">Positionality is important for NLP because it highlights the impact of a model's training data and algorithms on its performance and decision-making, particularly in detecting biases and inaccuracies. In the given example, the Perspective API's design bias towards detecting toxic content in Western contexts led to poor performance in an Indian context, demonstrating the importance of considering positionalities in NLP to ensure fair and accurate results.</sample>
    <sample id="766">The English content suggests that the speaker is referring to large language models (LLMs) like BLOOM, which need to be adapted for multi-language translation to SQL, Lambda, or other formats. To answer the question: Yes, LLMs like BLOOM are typically fine-tuned on a specific task or domain to adapt to the required language and format.</sample>
    <sample id="767">I used a binary classification model for the task of independent dissonance stands classification, which determines whether two debate statements from different people are in agreement or in disagreement.</sample>
    <sample id="768">The actual form of the printing doesn't have a big influence in the case of serial short printing.</sample>
    <sample id="769">Three.</sample>
    <sample id="770">The text is about Corscript, a constraint language planning method. According to the text, Corscript shows high plotism in the generated specific goals, allowing for smaller but specialized models.</sample>
    <sample id="771">Xu Heng.</sample>
    <sample id="772">Yes, the proposed results and dataset can be used as a benchmark for the problem of automatic text simplification in the future.</sample>
    <sample id="773">According to the text, the experiment involves a single smaller model, as it is mentioned that "T file function on the course rate can generate scripts of higher quality than most large-level models".</sample>
    <sample id="774">The base model used for investigating multimodal instruction tuning is OFA, a unified multimodal portrayal model.</sample>
    <sample id="833">The authors belong to Google Translate.</sample>
    <sample id="834">Stony Brook University.</sample>
    <sample id="835">The language pairs used in the work are not explicitly mentioned in the given text.</sample>
    <sample id="836">Xiangbin</sample>
    <sample id="837">Die beiden Modelle, die während der Experimente untersucht wurden, sind das Modell "long impart" und das "normal base impart".</sample>
    <sample id="838">For training, 53 tasks from NIGROP are used, and for testing, 15 tasks are used (5 from VQA and miscellaneous group, and all instances from the test sheet of natural instruction).</sample>
    <sample id="839">There is one author, Regina Stotten.</sample>
    <sample id="840">The authors experimented on four datasets: AG News, Mind, SSD2, and AresVam.</sample>
    <sample id="876">NACHOS is a dataset of medical ground truth data from the web.</sample>
    <sample id="877">The speaker is AYDIBILAR, a male.</sample>
    <sample id="878">The prompt strategy has a significant impact on the performance of LLMs for translation, as demonstrated in the experiment where two different prompts for a single sentence yield different results.</sample>
    <sample id="879">The authors are affiliated with the University of California, Berkeley.</sample>
    <sample id="880">Unfortunately, the audio clip does not provide the exact instructions, but it mentions that they are collecting a larger multi-model instruction tuning dataset with around 150 additional Weiren language tasks.</sample>
    <sample id="881">The authors suggest evaluating coreference resolution models with a dataset that probes their ability to draw on knowledge from different sources, by comparing their performance with human study departments and established coreference resolution models.</sample>
    <sample id="882">"Hallo, alle. Ich heiße Aydbilar und werde Ihnen einen kurzen Überblick über das Papier 'Grunz- Muster aus der Übersetzung: Strategien und Leistung' geben. Dies ist ein gemeinsames Werk mit meinen Kollegen von Google Translate."</sample>
    <sample id="883">"Der BAM ist ein 540 Milliarden Parameter-Code-Modell, das im Jahr 2022 vorgestellt wurde. Es wurde auf einer großen Sammlung von Texten trainiert, die 180 Milliarden Dokumente umfassen."</sample>
    <sample id="884">"In der Damaa für die Küche erreicht es den Stand der Kunst in hunderten von NLP-Aufgaben."</sample>
    <sample id="885">"Wir präsentieren in diesem Werk die erste systematische Studie zu großen Sprachmodell-Prompting für die maschinelle Übersetzung."</sample>
    <sample id="886">Wir haben die Übergabefähigkeit solcher Modelle anhand der besten Praktiken der MT-Gemeinschaft bewertet. Dazu verwenden wir die neuesten Test-Sätze, um einen Kreislauf mit dem Trainingsdaten des Sprachmodells zu vermeiden.</sample>
    <sample id="887">Wir vergleichen zwei state-of-the-art-Systeme. Die besten leistungsstarken Systeme sind die WMT-Evaluation.</sample>
    <sample id="888">Wir verwenden fortschrittliche neuronale MT-Metriken und zeigen außerdem Ergebnisse der Experten-basierten menschlichen Bewertung. Schließlich bieten wir Empfehlungen für Strategien für die Auswahl von Anfragen.</sample>
    <sample id="889">Die Anleitung hat einen großen Einfluss auf die Leistung von LLMs bei der Übersetzung. Wie wir in einem einfachen Experiment sehen können, wo wir ein einmaliges Anleitungsprotokoll verwenden und zwei verschiedene Anleitungen für einen einfachen Satz bereitstellen.</sample>
    <sample id="890">Die meisten Sätze, 516 von 1000, zeigen einen Unterschied von mehr als einem Blutpunkt.</sample>
    <sample id="891">Und dies kann in extremen Fällen bis zu 40 Punkten ausweichen. Es ist also wichtig, eine gute Ansprachestrategie auszuwählen.</sample>
    <sample id="892">In unseren Experimenten feiern wir eine fünf-Schuss-Strategie, bei der wir den Satz, den wir dem System bereitstellen, mit der Sprache markieren, in der es ist.</sample>
    <sample id="893">Übersetze den englischen Inhalt ins Deutsche.</sample>
    <sample id="894">Wir sahen, dass die tatsächliche Form der Druckform bei der Serien-Druckausgabe keine große Auswirkung hat.</sample>
    <sample id="895">Es ist entscheidend für Zero-Shot- und One-Shot-Anfragen. Und wenn wir, wie in unserem Fall, zu fünffach-Abfrage kommen, gibt es fast keinen Unterschied im tatsächlichen Form der Anfrage.</sample>
    <sample id="896">Es sind die Beispiele, die am meisten Gewicht tragen.</sample>
    <sample id="897">"Die Zusammenfassung unserer experimentellen Ergebnisse lautet, dass die Beispielergebnisqualität wichtiger ist als die Ähnlichkeit zur Quelsatz."</sample>
    <sample id="898">Es ist wichtig, Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir Auswahlanfragen aus dem Trainingsdatensatz der WMT-Evaluierungen oder dem DEF-Datensatz.</sample>
    <sample id="899">Die Tiefendaten sind viel sorgfältiger und von besserer Qualität als die trainierten Daten, also sind die Ergebnisse, wenn man die Tiefendaten verwendet, besser.</sample>
    <sample id="900">Jetzt, zumindest, haben spezialisierte State-of-the-Art-Systeme einen erheblichen Vorteil gegenüber Bandübersetzungen. Aber eines kommt ziemlich nahe an ein kommerzielles System heran. In unserem Fall haben wir entschieden, mit Google Übersetzen zu kooperieren.</sample>
    <sample id="901">Die Erkenntnisse, die wir aus der E-Mail-Regelung gewannen, die wir mittels dem MQM-Framework durchführten, sind, dass die Fluideität der Palme mit den Besten-Systemen vergleichbar ist, aber die Hauptunterschiede von der Genauigkeit kommen.</sample>
    <sample id="902">"Insbesondere die am häufigsten auftretenden Fehler sind Auslassungsfehler."</sample>
    <sample id="903">Es sieht aus, als würde Palm eine bessere klingende Übersetzung produzieren, indem sie manchmal Teile des Quellensatzes entfernt, die bei der Übersetzung erstellt wurden.</sample>
    <sample id="904">"Indessen ist der Stil- Ausfall für PAN niedriger als für die state-of-the-Art-Systeme, was ein weiterer Hinweis ist."</sample>
    <sample id="905">"Parm bietet sehr fließenden Output, aber immer noch einige Probleme bei der Genauigkeit."</sample>
    <sample id="906">Und das ist's für diese kurze Zusammenfassung. Weitere Details finden Sie in der vollständigen Präsentation des Papiers. Vielen Dank.</sample>
    <sample id="907">Hallo, ich bin Dawei, ein PhD-Student an der Salant-Universität in Deutschland. In diesem Video möchte ich unsere jüngste Arbeit, "Wicker als du denkst", vorstellen, eine kritische Analyse von Woche-zu-Wege-Zubehör.</sample>
    <sample id="908">Dies ist gemeinsame Arbeit mit Xiao Yuxian, Mario Smoothbath und Diaz Stefan und DTich Claco.</sample>
    <sample id="909">Ich beginne mit einer kurzen Einführung in die schwache Überwachung und schwach überwachte Lernen.</sample>
    <sample id="910">"Bei schwacher Supervision werden wir das Datenmaterial nicht manuell markiert. Stattdessen verwenden wir schwache Markierungquellen, wie einfache Heuristikkennzeichnungen, Wissensbasen oder Lokalitätsquellen, wie in dem auf der rechten Abbildung gezeigt."</sample>
    <sample id="911">Im Vergleich zu menschlichen Annotationen sind die schwachen Annotationen viel günstiger, jedoch auch lauter, was bedeutet, dass ein bestimmter Teil der Annotationen falsch ist.</sample>
    <sample id="912">"Wenn wir neuronale Netze direkt auf wöchentlich gekennzeichneten Daten trainieren, tendieren sie, die Kennzeichnungsgeräusche zu memorieren und generalisieren nicht."</sample>
    <sample id="913">In der wöchentlichen supervisierten Lernweise werden Algorithmen vorgeschlagen, um neuronale Netze so robust gegen solche Stufenrauschen auszubilden, dass die trainierten Modelle sich noch gut generalisieren.</sample>
    <sample id="914">"Im jüngsten Forschungswerk im Bereich des WSL, wobei WSL für wöchentliches supervisierte Lernen steht, behauptet man, dass man Modelle nur anhand wöchentlicher Arbeitsdaten trainiert und hohe Leistungen auf sauberen Testmengen erzielt."</sample>
    <sample id="915">Technisch gesehen ist diese Behauptung nicht falsch, aber es gibt ein Haken.</sample>
    <sample id="916">"Die Menschen setzen voraus, dass es einen zusätzlichen sauberen Validierungssatz oder eine Firewall für die Modellauswahl gibt."</sample>
    <sample id="917">Wir haben auf diesem Problem aufgehört, als dies bedeutet, dass weitere manuelle Anmerkungen in der wöchentlichen Unterstützung erforderlich sind, wie viele. Aber, wie ein Elefant im Zimmer, wird diese Notwendigkeit oft übersehen.</sample>
    <sample id="918">"Die vorherige Frage wird drei Forschungsfragen gestellt. Erstens ist sauberes Validierungsdaten für WSL notwendig oder können wir vielleicht ein lautes Validierungsset stattdessen verwenden?"</sample>
    <sample id="919">Zweitens, wenn sauberes Datenmaterial erforderlich ist oder wenn sauberes Datenmaterial für die Funktion von WSL erforderlich ist, wie viele saubere Proben benötigen wir dann? Schließlich sollten wir nur die sauberen Proben für die Validierung verwenden oder gibt es bessere Möglichkeiten, sie zu nutzen?</sample>
    <sample id="920">Wir haben diese Forschungsfragen in unserem Werk behandelt und unsere Ergebnisse sind wie folgt.</sample>
    <sample id="921">"Zunächst finden wir interessant, dass kürzlich entwickelte WSL-Methode tatsächlich saubere, tafelfertige Proben benötigen, um ordnungsgemäß zu funktionieren."</sample>
    <sample id="922">"Ansonsten fällt die Leistung ab. Wie in diesem Bild zu sehen ist, können die Trendmodelle ohne saubere Validierungsbeispiele nicht hinausgehen und generalisieren über die ursprünglichen Wochenbezeichnungen."</sample>
    <sample id="923">Die Übersetzung lautet: "Das Training ist sinnlos."</sample>
    <sample id="924">Dies weist darauf hin, dass WSL-Ansätze saubere etikettierte Daten benötigen, um ordnungsgemäß zu funktionieren, und der Aufwand für die Erstellung sauberer Validierungsmuster sollte nicht vernachlässigt werden.</sample>
    <sample id="925">Unser zweites Ergebnis ist, dass die Zunahme von sauberen Validierungsbeispielen den Leistungsstand von WSL-Ansätzen verbessern hilft, wie in der linken Abbildung gezeigt wird.</sample>
    <sample id="926">"Typischerweise benötigen wir für eine Klasse nur 20 Proben, um eine hohe Leistung zu erzielen."</sample>
    <sample id="927">Aber das ist nicht das Ende der Geschichte, weil wir, wenn wir entscheiden, die sauberen Proben direkt zu bearbeiten, sogar bessere Leistungen erzielen werden.</sample>
    <sample id="928">Die rote Figur zeigt den Leistungsunterschied zwischen den Ansätzen zur Feinabstimmung, die direkt auf den sauberen Daten angewendet werden, und den WSL-Verfahren, die die sauberen Daten nur für die Validierung verwenden.</sample>
    <sample id="929">"Wie wir sehen können, wenn wir 10 Proben pro Klasse haben, beginnt die feine Einstellung von Direct besser als die WSL-Ansätze zu laufen."</sample>
    <sample id="930">"Endlich kann das in vorherigen WSL-Ansätzen behauptete Leistungsverbesserung erreicht werden, indem man es ermöglicht, die feinen Einstellungen anhand der sauberen Validierungsmuster fortzuführen."</sample>
    <sample id="931">"Wie wir aus den Zahlen sehen können, unterperformt das Valina-Modell FTW ursprünglich komplexere WSL-Methode wie Kosinus."</sample>
    <sample id="932">"Indessen erlauben wir, auf die sauberen Proben weiter zu justieren, dann hält FTW seinem Niveau mit anderen Methoden stand."</sample>
    <sample id="933">"In der Praxis gibt es keinen Grund, komplexere WSL-Methode auszuwählen, die mehr Rechenzeit und Speicherplatz benötigen."</sample>
    <sample id="934">"Zusammenfassend haben wir gezeigt, dass neuere WSL-Ansätze saubere, manuell annotierte Beispiele benötigen, um ordnungsgemäß zu funktionieren. Ihr Leistungsfortschritt und ihre Handhabbarkeit werden stark überschätzt."</sample>
    <sample id="935">Unserer konkreten Empfehlungen für zukünftige Arbeitsstunden folgen.</sample>
    <sample id="936">"Erste, berichten Sie die Modellauswahlkriterien. Zum Beispiel, berichten Sie, ob die Modellauswahl an sauberen Validierungsmustern durchgeführt wurde."</sample>
    <sample id="937">Zweitens sollten WSL-Ansätze mit zukünftigen Landebaselines kombiniert werden, wenn beide auf klaren Mustern arbeiten. Drittens sollte kontinuierliche Feinabstimmung als einfache, jedoch starke Basis in zukünftigen WSL-Arbeiten berücksichtigt werden.</sample>
    <sample id="938">"Endlich haben wir unser Code freigegeben. Sie finden ihn über den QR-Code auf dieser Folie. Bitte fühlen Sie sich frei, ihn auszuprobieren. Vielen Dank und genießen Sie den Kongress."</sample>
    <sample id="939">Gängige Bewertungsmethoden für Dialogsysteme sind die Verwendung von menschlichen Beurteilungen, wie zum Beispiel die Auswahl von zwei Gesprächen durch Menschen oder die Bewertung von Gesprächen nach einem Likert-Skala.</sample>
    <sample id="940">4</sample>
    <sample id="941">The background knowledge required in the example with Servin and Kea is German language proficiency.</sample>
    <sample id="942">The code is available on GitHub.</sample>
    <sample id="943">No, the annotators for NLPositionality are not balanced across demographic groups, such as country, gender, etc.</sample>
    <sample id="944">The sentences were perturbed by adding "noise" to the input while preserving the relevant structure, in order to analyze how the language model's judgment of MPP (Most Probable Parse) is affected.</sample>
    <sample id="945">A dimensional evaluation refers to the assessment of multiple aspects or attributes of a chat quality, such as fluency, coherence, relevance, engagement, and tone, to gain a more comprehensive understanding of the model's performance.</sample>
    <sample id="946">University of Science and Technology of China.</sample>
    <sample id="947">Die Form des Prompts ist wichtig in den Fällen von null oder einem kurzen Prompt.</sample>
    <sample id="978">The authors evaluated conversational AI bots.</sample>
    <sample id="979">The content is a recording of a person, Jingwei Yi, introducing a video about protecting the copyright of large language models. 

There is only 1 author involved, Jingwei Yi.</sample>
    <sample id="980">Ein guter Planer sollte scripte schreiben, die vernünftig und treu zu den Einschränkungen sind.</sample>
    <sample id="981">There is 1 author, Si Yu-Yuan, mentioned in the introduction.</sample>
    <sample id="982">Vasudha.</sample>
    <sample id="983">Adam Szpirkowski is affiliated with the University of California, Berkeley.</sample>
    <sample id="1021">Omission errors.</sample>
    <sample id="1022">" Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und heute werden wir Ihnen alles über ABCeVal, einen neuen dimensional approach zur Bewertung von konversationsbasierten AI erzählen."</sample>
    <sample id="1023">"Dieses Werk wurde von der Emory NLP-Lab, geleitet von Professor Geno Choi an der Emory University, in Zusammenarbeit mit Amazon Alexa AI erstellt."</sample>
    <sample id="1024">"Lass uns sagen, dass Sie gerade ein Dialogmodell entwickelt haben und möchten wissen, wie gut es sich im Vergleich zum aktuellen Stand der Technik abschneidet."</sample>
    <sample id="1025">Die gängige Praxis ist, die menschliche Bewertung zu verwenden, wie zum Beispiel, indem man Menschen bitten, zwischen zwei Gesprächen zu entscheiden, welches besser ist, oder Gespräche auf einer Likert-Skala zu bewerten.</sample>
    <sample id="1026">Diese Ansätze funktionieren gut, um eine umfassende Bewertung der Gesamtqualität des Dialogs zu erstellen, aber die Gesamtqualität des Dialogs hat viele Aspekte. Deshalb möchten Sie möglicherweise mehrere Dimensionen der Chat-Qualität auswerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen.</sample>
    <sample id="1027">"Eine Möglichkeit besteht darin, einfach Menschenrichter zu bitten, verschiedene Aspekte der Dialogqualität zu bewerten, wie die Relevanz von Modellantworten mithilfe von vergleichenden oder Likert-Skalen-Methoden."</sample>
    <sample id="1028">"Wir glauben jedoch, dass es eine genauer und zuverlässigere Strategie für die Beurteilung von Dimensionen gibt."</sample>
    <sample id="1029">Unsere Vorgehensweise strebt danach, die Subjektivität der Bewertung durch Menschen zu reduzieren, indem wir explizit annotieren, ob jede Antwort des Modells bestimmte Verhaltensweisen aufweist, wie zum Beispiel die Antwort mit irrelevanten Informationen oder sich selbst widersprechend.</sample>
    <sample id="1030">"Wir nennen diese Vorgehensweise Annotation von Verhaltensweisen im Chat oder ABC-Evalkurz für short. Wir haben diese Methode entwickelt, um das Verhalten von Chat-Modellen umfassend abzubilden, die in jüngster Literatur als Einflussfaktoren auf die Chat-Qualität vorgeschlagen wurden."</sample>
    <sample id="1031">"ABC-Evaluation kann Fähigkeiten messen, bei denen Chat-Modelle thematischen Fehlern ausgesetzt sind."</sample>
    <sample id="1032">"Beispiel: ABC bewertet die Anzahl der Runden, in denen ein Chat-Modell sein Partner ignoriert oder irrelevantes sagt."</sample>
    <sample id="1033">"Der Text widerspricht sich selbst oder seinem Partner, halluziniert falsche Fakten oder verletzt alltägliches Wissen und zeigt weder Mitgefühl noch Erfolg oder Misserfolg."</sample>
    <sample id="1034">"Um festzustellen, welche Bewertung am effektivsten ist, haben wir vier state-of-the-Art-Chatt-Modelle ausgewählt und sie anhand von 100 menschlichen Bot-Gesprächen pro Modell mittels ABC-Eval bewertet."</sample>
    <sample id="1035">"Für Vergleichszwecke haben wir diese Gespräche auch mit drei bestehenden Methoden evaluiert, alkoholbeurteilungen auf der Ebene der Antworten, alkoholbeurteilungen auf der Ebene des Dialogs und Vergleiche von Dialogen auf der Ebene der Paare."</sample>
    <sample id="1036">Für jede der bestehenden Methoden sammelten wir Bewertungen zu acht der am häufigsten gemessenen Aspekten des Dialogs, da dies die Standardpraxis bei der Bewertung von Chat-Modellen auf mehreren Dimensionen ist.</sample>
    <sample id="1037">"Basierend auf unseren Auswertungen dieser Bewertungsergebnisse haben wir festgestellt, dass ABC-Evaluationsbezeichnungen insgesamt zuverlässiger sind als von bestehenden Methoden gesammelte Bezeichnungen, gemessen am Interanitator-Abkommen in 100 doppelt beschrifteten Gesprächen."</sample>
    <sample id="1038">"Zusätzlich sind ABC-Evaluierungsbezeichnungen besser vorhersagend für die Gesamtqualität des Gesprächs im Vergleich zu Metriken, die von bestehenden Methoden produziert werden, wie in dieser einfachen linearen Regressionsanalyse gezeigt wird."</sample>
    <sample id="1039">"Beispielweise können Sie sehen, wie die Messung des Anteils an Selbstwidersprüchen und Widersprüchen gegenüber dem Partner 5% und 10% des Gesprächsqualitätsanteils erklärt, während die durchschnittlichen Alkoholkonsistenzwerte weniger als 4% erklären."</sample>
    <sample id="1040">"Schließlich haben wir überprüft, ob jeder Bewertungsmaßstab ein einzigartiges Aspekt der Chat-Qualität erfassen kann, indem wir eine Schritt-für-Schritt-Lineare Regression durchführten."</sample>
    <sample id="1041">"Man kann sehen, wie die Kombination aller ABC-Evaluationsmetriken mehr als 25% der Konversationsqualität erklärt. Und wenn man die Metriken nacheinander entfernt, verliert man bei fast allen Fällen einen beachtlichen Teil an Informationen über die Qualität."</sample>
    <sample id="1042">"Andererseits erklären die Kombination aller Bewertungsmaßstäbe auf Ebene der Umsetzung nur sehr wenig des Qualitätsaspekts und weniger dieser Maßstäbe tragen eindeutige Informationen."</sample>
    <sample id="1043">"Diese zuverlässigen, informierenden und distinkten ABC-Evaluationsmetriken ermöglichen es uns, das Gesprächsassistenten mit einer höheren Auflösung zu bewerten als vorhergehende Methoden dies tun konnten."</sample>
    <sample id="1044">"Sie können sehen, dass in den Ergebnissen unseres Experiments immer noch einige Herausforderungen bestehen und exakt quantifiziert sind. Zum Beispiel haben die von uns getesteten Bots in etwa 20% ihrer Antworten Common Sense-Violationen."</sample>
    <sample id="1045">Sie produzieren in etwa 15% der Antworten irrelevantes Material und widersprechen sich oder ihrem Partner etwa 10% der Zeit.</sample>
    <sample id="1046">Mit der raschen Verbesserung in diesem Bereich könnten viele dieser Fehlerraten in neuen Modellen, die seit unserer Bewertung veröffentlicht wurden, sinken. Allerdings ist dies umso mehr Grund, zuverlässige und genaue Bewertungsmaßstäbe für den Vergleich von Modellen zu verfolgen.</sample>
    <sample id="1047">"Wir hoffen, dass ABC-Evaluierung von anderen in diesem Bereich genutzt werden kann als ein bedeutender Schritt in diese Richtung. Wir freuen uns darauf, wie sich die konversationale AI in den kommenden Monaten und Jahren entwickeln wird. Vielen Dank für das Zuschauen."</sample>
    <sample id="1048">Emory University.</sample>
    <sample id="1049">WSL stands for Weakly Supervised Learning.</sample>
    <sample id="1050">6</sample>
    <sample id="1051">"Hallo, ich heiße Kaio Yan und ich werde unser Werk mit dem Titel 'Wenn Übersetzung einen Kontext benötigt? Eine datengetriebene multilinguale Erforschung' präsentieren. Dieses Werk wurde in Zusammenarbeit mit Patrick Frenange, M.E. Liu, Andre F.D. Martin und Graham Mubig erstellt."</sample>
    <sample id="1052">So viele Übersetzungen hängen vom Kontext ab. Zum Beispiel, wie würden wir "more" in diesem Satz übersetzen?</sample>
    <sample id="1053">Also, wenn das vorherige Satz war, könnte es gefährlich werden, wenn die Minister es herausfinden, dann bezieht sich Moe auf einen Spion. Wenn das vorherige Satz war, könnte es überhaupt etwas Ernstes sein, Doktor? Dann bezieht sich Moe auf ein Muttermal.</sample>
    <sample id="1054">"Je nach Kontext ändert sich der Sinn des Wortes und daher auch seine Übersetzung."</sample>
    <sample id="1055">"Es ist jedoch schwierig, wie gut Modelle solche Fälle übersetzen können. Zunächst einmal, weil nur ein kleiner Teil von Übersetzungen von Kontext abhängt, was Korpus-basierte Metriken wie Blue nicht erfassen können."</sample>
    <sample id="1056">"Und einige haben vorgeschlagen, dass man auf kontextabhängige Bewertungen für übersetzungen achtet, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Mengen an Sprachen, da sie meist auf Fachwissen und menschliche Kuratierung angewiesen sind."</sample>
    <sample id="1057">"Wir versuchen in diesem Werk diese beiden Fragen zu beantworten. Zuerst: Wann benötigt Übersetzung Kontext? Und zweitens: Wie gut können Modelle diese Fälle handhaben?"</sample>
    <sample id="1058">Um die erste Frage zu beantworten, haben wir zuerst gemessen, inwiefern ein Wort von seinem Übersetzungs Kontext abhängt.</sample>
    <sample id="1059">Und das vorherige Werk haben wir CXMI als Maß für die Kontextnutzung durch maschinelle Übersetzungsmuster vorgestellt. Und das wird erreicht, indem wir messen, wie viel Informationen der Kontext C über das Ziel Y gibt, gegebenenfalls das Quellmaterial X.</sample>
    <sample id="1060">Sie können CXMI als den Informationsgewinn betrachten, den man durch das Hinzufügen von Kontext zum Modell erzielt.</sample>
    <sample id="1061">"In dieser Arbeit erweitern wir CXMI zu YCXMI, das es ermöglicht, den Kontextgebrauch auf Satzebene oder auf Wortebene zu messen. Wir können Wörter denken, die hohe P6MI aufweisen, als solche, die für die Übersetzung Kontext benötigen."</sample>
    <sample id="1062">Jetzt analysieren wir Wörter mit hoher XMI, um Muster zwischen diesen Wörtern zu suchen.</sample>
    <sample id="1063">Und wir führen unsere Analyse auf Transkripten von TED-Vorträgen durch, die von Englisch in 14 verschiedene Sprachen übersetzt wurden.</sample>
    <sample id="1064">Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Zunächst betrachten wir Teile von Worthülsen, die hohe PCXMI-Werte haben.</sample>
    <sample id="1065">Und dies ermöglicht es uns, beispielsweise dualische Pronomen im Arabischen zu finden, die eine relativ hohe P6MI haben. Und dies kann erklärt werden, weil Englisch keine dualischen Pronomen hat, daher benötigt man Kontext, um zu bestimmen, ob ein Pronomen dualisch ist, wenn man es ins Arabische übersetzt.</sample>
    <sample id="1066">Und ähnlich finden wir, dass bestimmte Sprachen auch Kontext benötigen, wenn wir die richtige Verbform auswählen möchten. Wir sehen uns dann an Vokabeln an, die eine hohe p-Sex-Mi-Average über alle ihre verschiedenen Vorkommen haben.</sample>
    <sample id="1067">Und das hilft bei der Identifizierung von Fällen wie diesem hier, bei dem in Chinesisch, wo Sie Kontext benötigen, um Proper Nouns zu übersetzen, um sicherzustellen, dass Sie denselben Übersetzung innerhalb des Dokuments verwenden.</sample>
    <sample id="1068">Und ähnlich finden wir heraus, dass Kontext unterstützt wird, um in der richtigen Formel zu übersetzen.</sample>
    <sample id="1069">"Und schließlich betrachten wir einzelne Token, die eine hohe P6MI aufweisen. Dies ermöglicht es uns, Phänomene zu identifizieren, die nicht direkt durch das Wort selbst erfasst werden können, sondern vielmehr in der Satzstruktur ausgedrückt werden, wie zum Beispiel die Auflösung von Ellipsen."</sample>
    <sample id="1070">"So nutzen wir unsere Erkenntnisse aus unserer Analyse, um ein Benchmark für die dokumentale globale Übersetzung zu entwerfen."</sample>
    <sample id="1071">Für jede der fünf Diskursphänomene, die wir identifiziert haben, erstellen wir Tagger, um Wörter zu identifizieren, die sich auf das Phänomen beziehen. Wir nennen unseren Tagger Multilingual-Diskurs-Aware oder Muda-Tagger.</sample>
    <sample id="1072">Wir können auch feststellen, dass verschiedene Sprachen unterschiedliche Anteile dieser diskursiven Phänomene aufweisen.</sample>
    <sample id="1073">Wir verwenden dann den MudaTaggle, indem wir den Taggle auf dem parallel korrigierten Korpus anwenden, das wir für die Bewertung verwenden möchten. Wir wenden unsere gewählten Übersetzungsmetriken auf die kontextabhängigen Beispiele an, die der MudaTaggle identifiziert hat.</sample>
    <sample id="1074">"Und schließlich verwenden wir unser Benchmark sowie weitere Metriken, um verschiedene Modelle auf der Dokumentenebene auf die Maschinensprachübersetzung zu bewerten."</sample>
    <sample id="1075">"Zunächst einmal, wenn wir korpusbasierte Metriken verwenden, finden wir bei blau, dass kontextunabhängige Modelle die beste Leistung haben."</sample>
    <sample id="1076">Dann wenn wir Comet verwenden, erzielen Kontext-bewusste Modelle die besten Ergebnisse. Und wenn wir den Wort-Faktor verwenden, dann haben Modelle mit oder ohne Kontext vergleichbare Leistung.</sample>
    <sample id="1077">"Das zeigt wieder, dass es schwierig ist, das beste Dokumentenübersetzungssystem zu bestimmen, wenn man sich allein an Korpusniveaumetriken hält."</sample>
    <sample id="1078">"Wir verwenden nun die Mooda-Benchmark, um Modelle zu bewerten, und finden heraus, dass Kontext-bewusste Modelle signifikant genauer sind als Modelle, die keine Kontextinformationen verwenden, insbesondere bei bestimmten Diskursphänomenen wie Formalität und lexikalischer Kohärenz."</sample>
    <sample id="1079">"Aber diese Modelle sind nicht viel besser als Modelle, die keine Kontextinformationen für Phänomene wie Ellipsen, Perennien und Verbformen verwenden. Deshalb deutet dies an, dass wir für die Dokumenteniveau-Transformation mehr Fortschritte sehen müssen."</sample>
    <sample id="1080">"Wir vergleichen auch verschiedene kommerzielle Systeme und unsere Benchmarks zeigen, dass D-Bel in der Regel genauer als Google Translate bei Dokumentenübersetzungen ist."</sample>
    <sample id="1081">Zusammenfassend führen wir eine datengetriebene Analyse über 14 Sprachpaare durch, um festzustellen, wann Übersetzungen Kontext benötigen.</sample>
    <sample id="1082">Und dann verwenden wir unsere Erkenntnisse, um ein Benchmarks für Dokumentenübersetzung zu erstellen, was uns hilft, festzustellen, welche disk-übersetzungs-Phänomene gut oder schlecht abgehandelt werden und welche Übersetzungssysteme gut bei der Dokumentenübersetzung sind.</sample>
    <sample id="1083">Danke schön für Ihre Aufmerksamkeit. Bis Toronto.</sample>
    <sample id="1084">Yusin Zhang.</sample>
    <sample id="1121">The content is describing a stage in a process until all tokens from the first stage have been visited exactly once. 

This process does not have a specific name.</sample>
    <sample id="1122">The authors describe the "marked words" method as a way to identify words that distinguish marked groups from unmarked ones.</sample>
    <sample id="1123">University of Washington.</sample>
    <sample id="1124">Prag</sample>
    <sample id="1125">The speaker is James Finch and Sarah Finch.</sample>
    <sample id="1126">4</sample>
    <sample id="1127">According to the speaker, minimal pairs can be used to test syntactic phenomena.</sample>
    <sample id="1161">The text does not mention specific methods or their abbreviations for the first research question.</sample>
    <sample id="1162">The model is evaluated based on 11 biomedical and clinical tests in French.</sample>
    <sample id="1226">CamemBERT was originally trained on 4 GB of text data, specifically on the Pomet Bird dataset.</sample>
    <sample id="1227">Adam Szpirkowski.</sample>
    <sample id="1228">The experiment where we retrained or continued to pre-train models with more recent data showed that the performance degrades with a larger temporal gap, confirming that the main cause of the performance drop is temporal drift.</sample>
    <sample id="1269">To put the tokens in the right order.</sample>
    <sample id="1270">Die Autoren empfehlen die Transparenz über Bias-Minderungs-Methoden, um festzustellen, ob die positive Stereotype aufgrund von "weird, overly excessive value alignment" oder anderen anti-stereotypischen Methoden entstehen.</sample>
    <sample id="1271">Inacceptable minimal pairs are ungrammatical sentences, which are shown to a language model to evaluate its ability to distinguish between grammatical and ungrammatical sentences.</sample>
    <sample id="1272">The authors used the metrics of weight and token count.</sample>
    <sample id="1273">The metric used to measure the agreement between the evaluators was Interanitator Agreement.</sample>
    <sample id="1274">Wikipedia.</sample>
    <sample id="1275">The speaker is Regina Stotten, and she is affiliated with the University of Heidelberg, as stated in the presentation.</sample>
    <sample id="1276">MultiInstruct differs from other benchmarks in that it focuses on instruction tuning for multimodal protein models, whereas most previous works have focused on language-only tasks.</sample>
    <sample id="1277">2</sample>
    <sample id="1278">The binary coordination is defined as a type of poetic meter that consists of two syllables per foot.</sample>
    <sample id="1279">3 seconds</sample>
    <sample id="1280">The results suggest that the smaller T5 model can generate scripts of higher quality when properly trained on suitable data sites, indicating that it can support larger models.</sample>
    <sample id="1281">"Hallo, ich bin Yannis Lavraque und ich präsentiere unsere Arbeiten auf Dr. Berth, ein robustes Modell für die französische Biome- und Klinik-Domäne."</sample>
    <sample id="1282">"In dieser Präsentation sprechen wir über eine modellierte Sprache im Gesundheitsbereich. Dann werden wir unsere Hauptbeiträge in diesem Artikel vorstellen."</sample>
    <sample id="1283">Wir haben den ersten biomedizinischen Modell namens Dr. Berth vorgestellt, das auf Roberta basiert und auf NACCHOS trainiert wurde, einem Datensatz medizinischer Grunddaten aus dem Web.</sample>
    <sample id="1284">Wir haben auch eine Modell-Vergleich mit verschiedenen Rechenmodellen und Datenquellen durchgeführt. Wir werden unsere Ergebnisse dann auf 11 medizinischen und klinischen Tests in Französisch präsentieren.</sample>
    <sample id="1285">Und schließlich fassen wir die Experimente zusammen und geben Ihnen weitere Informationen, wie Sie auf die Modelle zugreifen können.</sample>
    <sample id="1286">Seit seiner Veröffentlichung 2018 war Bert die effektivere Vorgehensweise bei der natürlichen Sprachverarbeitung. Er hat ein großes Leistungssteigerung gegenüber historischen und kontextualisierten Strategien wie dem "Look-to-Vec" oder "Look-to-Embeddings" erzielt.</sample>
    <sample id="1287">Seitdem wurde diese Modelle an viele andere Sprachen wie Französisch mit Camembert angepasst und an andere Bereiche wie Biomedizin mit Père Medbert und Biobird und in der Klinik mit Klinik Albert, aber vor allem auf Englisch.</sample>
    <sample id="1288">Spezialisierte Modelle für andere Sprachen sind rar und werden oft kontinuierlich trainiert, da die Verfügbarkeit von Daten aus dem gleichen Kontext fehlt.</sample>
    <sample id="1289">"Es gab jedoch in Frankreich noch kein offenes Quellencode für Bio-Medizin, bis jetzt."</sample>
    <sample id="1290">Wir fragen uns, welche Datenstrukturen für eine breite Palette von Verwendungen am geeignetsten sind. Und jene aktuellen Daten sind gute Ersatz für klinische Daten.</sample>
    <sample id="1291">"Um diese Frage zu beantworten, vergleichen wir Dr. Bert mit unserem Schubert-Modell, das auf anonymisierten Daten basiert, die von unserem Haus erhalten wurden, einem nicht-generationalen Krankenhaus."</sample>
    <sample id="1292">Nachher fragen wir uns, wie viel Daten wir benötigen, um ein spezialisiertes Modell auf französischen Daten zu trainieren. Ist es für Gb bei Gb oder mehr?</sample>
    <sample id="1293">Um diese Frage zu beantworten, trainieren wir und vergleichen vier von vornherein-Modelle. Ein erster Entwurf von Dr. Bert mit 7 GB Natur. Ein zweiter Entwurf mit 4 GB Natur.</sample>
    <sample id="1294">Die erste Version von Schubert, die eine klinische Vorlage ist, hat 4 GB an klinischen Notizen. Und die letzte Version von Schubert mit einem Mix von 4 GB an Natur und 4 GB an klinischen Notizen.</sample>
    <sample id="1295">"Zusätzlich zu dieser Vergleichsanalyse haben wir drei Modellzüge auf dem europäischen Vorkurseingang eingeführt, um den Einfluss der Vorkurseingang-Strategie zu analysieren."</sample>
    <sample id="1296">Eine Basis auf dem Gewicht von Camembert und ein Zug auf einem 4-GB-Set von Natur. Eine andere, die auf Camembert basiert, aber auf 4 GB von Blinkern trainiert und die andere.</sample>
    <sample id="1297">Und schließlich, ein Modell auf Basis des englischen bio-medizinischen Modells Bermond Bert und trainiert auf vier Gigabyte eines Satzes von Schnittstellen. Insgesamt haben wir sieben Modelle.</sample>
    <sample id="1298">"Um die sechs Modelle zu bewerten, sammeln wir uns mit Publikationen und privaten Zuschreibungen, wie Nomenklaturen, Klassifizierungen, Reisen, Herausforderungen und Verantwortung."</sample>
    <sample id="1299">Dieser Modell wird im Vergleich zu Modell 6.9 getestet, das sich gegenüber Camembert Oscar 138GB, Camembert Oscar 4GB, Camembert CCNet 4GB, Pummet Belt, BioBert und ClinicalBert vergleicht.</sample>
    <sample id="1300">Die Evolution von Highlights, die am besten auf die Aufgabe abschneidet, wenn sie Daten hat, die demselben Charakter wie die, auf denen das Modell trainiert wurde.</sample>
    <sample id="1301">"Indessen können wir das Datenmaterial aus verschiedenen Quellen erhalten, scheint Datenmaterial aus heterogenen Quellen flexibler zu sein. Wir beobachten auch, dass die Verwendung von mehr Daten zu besserer Leistung führt."</sample>
    <sample id="1302">"Insgesamt erzielte das Zurücksetzen von vorne bei der meisten Aufgaben bessere Leistungen."</sample>
    <sample id="1303">"Unser Erfahrung mit Constraints und Training, indem wir das Gewicht und den Token von Pomet Bird verwenden und den Prozess auf ein Subjekt von 4 GB Größe durchführen, hat ein Ergebnis erbracht, das unserem Ergebnis mit Dr. Bert auf 4 GB Speichergröße entspricht."</sample>
    <sample id="1304">"Das ist nicht der Fall für das Modell auf Basis von Camembert-Weights und Tokenizer, das von Stabilitätsproblemen leidet."</sample>
    <sample id="1305">"Schließlich, als Fazit, bietet unser System bessere Leistung bei neun von elf Downstream-Aufgaben und übertrifft die globalen Ergebnisse des generischen Modells hier, Camembert."</sample>
    <sample id="1306">Wir haben auch festgestellt, dass spezialisiertes Datenmaterial besser ist, mehr spezialisiertes Datenmaterial ist besser, aber es skaliert nicht gut.</sample>
    <sample id="1307">"Alle vorgefertigten Modelle von NATURES sind kostenlos auf UginFace verfügbar und alle Trainings-Skripte sind auf unserem GitHub-Repository."</sample>
    <sample id="1308">"Danke für diese Präsentation. Wir freuen uns auf die Aktion bei der Poster-Sitzung in Toronto."</sample>
    <sample id="1309">The learning strategies being investigated are training and comparing four models from scratch.</sample>
    <sample id="1310">According to the graph, the gradient of the best fit line is greater than 1, which means there is no diminishing returns, and therefore, the overfitting factor is 1.</sample>
    <sample id="1311">The quality of the simplification was evaluated using automatic metrics such as ROUGE, METEOR, and SARI.</sample>
    <sample id="1312">Ja, einige Sprachmodelle haben unterschiedliche politische Vorurteile.</sample>
    <sample id="1313">"Hallo, ich heiße Mathias Landemann und heute möchte ich Ihnen einen kurzen Überblick über unser Paper zu kompositioneller Generalisierung ohne Bäume mittels Multi-Set-Tags und latenten Permutationen geben."</sample>
    <sample id="1314">Das ist gemeinsame Arbeit mit meinen Beratern Alexander Kodler und Yvon Titov.</sample>
    <sample id="1315">"Kompositionelle Generalisierung kann als die Fähigkeit eines Lernenden verstanden werden, tiefere Rekursion und unbekannte Kompositionen von Phrasen zu verarbeiten, die einzeln während des Trainings gesehen wurden."</sample>
    <sample id="1316">Im Kontext der semantischen Analyse könnte das Testen von kompositioneller Generalisierung so aussehen. Wir haben wie immer eine Ausbildungssammlung von Aussagen, in diesem Fall "Die Frau schlief" und "Mary wusste, dass die Frau schlief".</sample>
    <sample id="1317">Diese Aussagen werden mit logischen Formen verbunden, die essentielle Aspekte ihrer Bedeutung repräsentieren.</sample>
    <sample id="1318">Im Gegensatz zur standardmäßigen Bewertung von maschinellem Lernen stammt das Testset nicht aus derselben Verteilung, sondern enthält struktursynologische Formen.</sample>
    <sample id="1319">"In diesem Beispiel hat das Modell während des Trainings tieferen Rekursionen gesehen und wird auf ein Beispiel mit tiefer Rekursion getestet."</sample>
    <sample id="1320">"Naive sequenzen-zu-seqenz-modelle haben Schwierigkeiten damit, diese Art von Ausgleich außerhalb der Verteilung und erzeugen oft Ausgaben, die vom Eingang abgetrennt sind."</sample>
    <sample id="1321">"Insbesondere reproduzieren sie oft nicht die systematischen Übereinstimmungen zwischen Eingabe und Ausgabe, wie zum Beispiel jene, die in dem Beispiel farbmarkiert sind."</sample>
    <sample id="1322">Eine beliebte Methode, dies anzugehen, besteht darin, Bäume in die Modelle zu integrieren.</sample>
    <sample id="1323">Die Bäume sollen den kompositionellen Prozess erfassen, der die Äußerungen mit den logischen Formen verknüpft.</sample>
    <sample id="1324">Dies funktioniert gut, aber Bäume werden normalerweise nicht gegeben und müssen irgendwie erworben werden.</sample>
    <sample id="1325">Dies kann kompliziert und manchmal ein rechenaufwändiger Prozess sein. Typischerweise umfasst dies formale Vorverarbeitung der logischen Formen, zum Beispiel zur Behandlung von Variablen-Symbolen.</sample>
    <sample id="1326">"Der Erhalt von Bäumen kann auch spezielle Grammatik-Induktionen umfassen."</sample>
    <sample id="1327">"In diesem Papier verwenden wir keine Bäume und stellen ein neuronales Modell für die Sequenz-zu-Sequenz-Übertragung vor, das die Korrespondenzen zwischen Fragmenten des Eingangs und Fragmenten des Ausgangs direkt modelliert."</sample>
    <sample id="1328">"Zum ersten Mal zeigen wir eine starke allgemeine Verallgemeinbarkeit auf tieferer Rekursion ohne auf Bäume angewiesen zu sein."</sample>
    <sample id="1329">Unser Ansatz vorhersagt die Ausgabe aus der Eingabe in zwei Schritten.</sample>
    <sample id="1330">Zuerst kennzeichnen wir jede Eingabemarkierung mit einem unsortierten Mehrfachmengensatz von Token, der in der Ausgabe auftreten wird.</sample>
    <sample id="1331">Nach dem ersten Schritt haben wir alle richtigen Token, aber sie sind nicht geordnet.</sample>
    <sample id="1332">"Deshalb verwenden wir im zweiten Schritt ein weiteres Modell, um eine Permutation vorherzusagen, um sie in die richtige Reihenfolge zu bringen."</sample>
    <sample id="1333">Wir stellen ein neues Verfahren vor, um eine Permutation vorherzusagen, das keine harten Einschränkungen für die möglichen Permutationen aufstellt. Dies macht unsere Herangehensweise sehr flexibel und ausdrucksstark.</sample>
    <sample id="1334">Konzeptionell funktioniert unser Permutationsmodell etwa wie folgt.</sample>
    <sample id="1335">Wir gehen von links nach rechts über den Ausgang und bestimmen, welchen Multiset-Tokens wir in jede Position setzen. Bei der ersten Ausgabestelle wählen wir einfach denjenigen wie in Rot hervorgehoben.</sample>
    <sample id="1336">Dann springen wir zum nächsten Mehrfach-Token, um den zweiten Token in der Ausgabe zu bestimmen.</sample>
    <sample id="1337">Wir bestimmen den dritten Token im Ausgang in ähnlicher Weise, indem wir zu einem anderen Mehrfach-Token springen. Wir fahren fort mit diesem Prozess.</sample>
    <sample id="1338">"Bis jeder Token aus der ersten Stufe genau einmal besucht wurde."</sample>
    <sample id="1339">Um Ihnen einen Vorgeschmack von den experimentellen Ergebnissen zu geben, vergleichen wir unsere Methode mit anderen baumlosen Modellen auf dem Benchmark von CONG. Unsere Methode übertrifft die anderen um einen großen Abstand bei der Generalisierung auf tieferen Rekursionen.</sample>
    <sample id="1340">Andere Arten von strukturellen Generalisierungen bleiben jedoch sehr Herausforderungen.</sample>
    <sample id="1341">"In unserem Papier lösen wir ein paar interessante technische Herausforderungen."</sample>
    <sample id="1342">"Zuerst einmal wird die Ausrichtung zwischen Eingabe und Ausgabe in den Trainingsdaten nicht gegeben. Folglich wissen wir bei einem gegebenen Token nicht, welches Multicell es stammt, was eine Herausforderung für das Training darstellt."</sample>
    <sample id="1343">Zusätzlich gibt es manchmal mehrere Permutationen, die den Daten entsprechen, aber die linguistisch korrekte eine latente ist. Wir beheben dies, indem wir die Ausrichtung als Teil des Trainings induzieren.</sample>
    <sample id="1344">Unsere Permutationsmethode ist sehr flexibel, aber sie bringt den Herausforderung, dass das Auffinden der hochwertigsten Permutation NP-schwer ist. Das ist, weil dies mit dem Reisenden-Schmidt-Problem verbunden ist.</sample>
    <sample id="1345">Wir approximieren dies mit einer GPU-gängigen kontinuierlichen Entspannung, die uns auch erlaubt, die Lösung rückwärts zu propagieren und die linguistisch plausiblen Permutationen zu lernen.</sample>
    <sample id="1346">Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen angehen, schauen Sie bitte unsere Publikation an oder besuchen Sie unser Postamt.</sample>
    <sample id="1347">Cognitive dissonance is the discomfort or tension that occurs when two or more beliefs, values, or behaviors are inconsistent with each other.</sample>
    <sample id="1348">GPT-4.</sample>
    <sample id="1349">Yes, cumulative training performs equal or better than iterative training for active learning.</sample>
    <sample id="1350">Sara Pappi</sample>
    <sample id="1351">The data for the MuDa-Benchmark comes from transcripts of TED Talks translated from English to 14 different languages.</sample>
    <sample id="1385">Der Referent ist Mathias Landemann.</sample>
    <sample id="1386">Cross-lingual zero-short and field-short transfer refers to training a model on one source language and transferring it to another language to predict SQL output.</sample>
    <sample id="1387">Salant University</sample>
    <sample id="1388">The authors use two latency measurements: translation quality and average lagging, as well as computational aware average lagging.</sample>
    <sample id="1389">Hallo alle, ich bin Akshita und heute präsentiere mein Werk gemeinsam mit meinem Co-Autor Martin, das KITMAS-Test, das die Integration von Kenntnissen aus verschiedenen Quellen bewertet. Dieses Werk ist eine Zusammenarbeit zwischen der McGill-Universität, Miele und Microsoft-Forschung.</sample>
    <sample id="1390">"Die nationalen Sprachverständnismodelle beruhen auf einer Vielzahl von Quellen, wie z.B. auf dem in ihren Parametern enthaltenen Wissen, das meist durch Vorschulung und Wissen, das bei der Ausführungszeit bereitgestellt wird."</sample>
    <sample id="1391">"Recente Arbeiten in Aufgaben wie Fragebeantwortung zeigen, dass Modelle vorkompilierte Zeitwissen nutzen können, um die Aufgabe zu lösen."</sample>
    <sample id="1392">"Das natürliche Sprachverständnis erfordert oft Kenntnisse, die auch während der Inferenzzeit bereitgestellt werden."</sample>
    <sample id="1393">"Beispiel: John sah den neu gewählten Präsidenten auf dem Fernsehen."</sample>
    <sample id="1394">"Vortrainierte Parameter können Informationen über, was Präsidenten tun und was ein Fernseher ist, enthalten, aber sie können nicht zuverlässig wissen, wer dieser instanzspezifische John ist oder wer der neue Präsident ist, weil der Präsident möglicherweise seit dem Vortraining geändert wurde."</sample>
    <sample id="1395">"Deswegen benötigen erfolgreiche Modelle für kognitiv intensive NLU-Aufgaben die Fähigkeit, sowohl vorher trainierte als auch Echtzeit-Kennwissen zu integrieren und zu nutzen."</sample>
    <sample id="1396">"Wir schlagen ein Diagnose-Test-Suite für die Integrationskenntnisse vor."</sample>
    <sample id="1397">"Wir stellen eine ReferenzierungsAufgabe vor, die die Fähigkeit testet, auf Informationen in verschiedenen Quellen zurückzugreifen. Wir evaluieren das Dataset mit menschlichen Studienabteilungen und etablierten Referenzierungsmodellen."</sample>
    <sample id="1398">Hier ist ein Beispiel aus unserem Datensatz. Der Richter ist ein Mann. Der Bäcker ist ein Mann. Der Richter und Kya trafen sich im Park. Nach einem langen Tag am Gericht, an dem er sich um Fälle entschied, war er froh, sich zu entspannen.</sample>
    <sample id="1399">Die Aufgabe besteht darin, das korrekte Entität zu identifizieren, auf die das Pronomen "er" sich bezieht, was in diesem Fall 7 ist.</sample>
    <sample id="1400">Die Auflösung eines gegebenen Pronoms erfordert zwei Arten von Informationen. Erstens: Entity-spezifische Kenntnisse wie "Diener ist ein Dienst". Zweitens: Hintergrundwissen wie Richter fallen Urteile in Gerichtshöfen.</sample>
    <sample id="1401">"Im Allgemeinen wird Hintergrundwissen während der Vorbereitung von großen Sprachmodellen erworben, während spezifisches Wissen über Entitäten typischerweise während der Ausführungszeit beobachtet."</sample>
    <sample id="1402">"Wir variieren die Verfügbarkeit dieser beiden Informationen, so dass sie entweder in einer einzigen Quelle oder in mehreren Quellen gefunden werden kann."</sample>
    <sample id="1403">"Wir haben drei Einstellungen für KITMOS definiert. Zuerst müssen wir die Einstellung vorantreiben. Das Hintergrundwissen wird bei der Vorbereitung vermutet."</sample>
    <sample id="1404">"Zweitens gibt es einen Hintergrund, der sowohl vor der Ausbildung als auch während der Ausbildung verfügbar ist. Zuletzt das Hintergrund-Einfluss-Setting. Mit beiden Kenntnissen sind nur während der Ausbildung verfügbar."</sample>
    <sample id="1405">"Dieses letzte Setting ist insbesondere interessant, da es den Fall simuliert, in dem das notwendige Hintergrundwissen, um eine Aufgabe zu lösen, nicht Teil der Vorbild-Daten von Modellen ist, zum Beispiel weil neue Berufe entwickelt wurden, seitdem die Vorbild-Daten trainiert wurden."</sample>
    <sample id="1406">"Das ist ein Beispiel dafür, wie wir die Verfügbarkeit von Fakten in einer authentischen Quelle steuern."</sample>
    <sample id="1407">"Im Hintergrund voraus trainierten Setting setzen wir voraus, dass das Hintergrundwissen, das Politiker bei der Wahl von Regierungssitzen anstreben, in den voraus trainierten Parametern enthalten ist. Im seltenen Zeitkontext bieten wir das anti-spezifische Wissen, dass Chester ein Politiker ist."</sample>
    <sample id="1408">"In der Rücklauf-Einstellung bieten wir im Hintergrund nicht nur Anti-Spezifisches, sondern auch Hintergrundwissen über Politiker im Einfluss-Unterbereich."</sample>
    <sample id="1409">"In einem freien Setting bieten wir im Hintergrund eine effiziente Beschäftigung, anstatt Politiker, weil die Meriture wahrscheinlich nicht in der vorherigen Periode enthalten ist."</sample>
    <sample id="1410">"Wir evaluieren das Datensatz sowohl mit menschlichen Studienteilnehmern als auch mit Modellen zur Bezugsaufstellung. In dieser Figur zeigen wir die Ergebnisse der besten Modelle im schwierigsten Varianten des vorgepräparierten Settings."</sample>
    <sample id="1411">Mit unserer Aufgabenpezifischen Ausbildung auf KITMOS erzielen beide Überwachungsgeräte nicht gute Ergebnisse. Trainiert man jedoch auf KITMOS, übertrifft C2F und BFQF beide die zufällige Wahl signifikant.</sample>
    <sample id="1412">„Wenn man Modelle an allgemeinen Anforderungen mit Lushen-Daten trainiert, lernen sie, Oberflächennachfolgen auszunutzen, die bei der Überprüfung auf KITMOS nicht nützlich sind, wo solche Nachfolgen entfernt wurden.“</sample>
    <sample id="1413">"Zusätzliche Experimente mit fiktionaler Kenntnis zeigen, dass selbst die besten Modelle nicht zuverlässig rückwärtsgedruckte Kenntnisse integrieren können, wenn diese nur zu Einflusszeit bereitgestellt werden."</sample>
    <sample id="1414">"Zusammenfassend unsere Forschungsarbeit, scheinen viele Modell-architekturen für Kernreferenzen nicht in der Lage zu sein, ohne spezielle Ausbildung über Wissen aus verschiedenen Quellen zu reagieren. Jedoch können einige Modelle nach spezieller Ausbildung Wissen aus mehreren Quellen integrieren."</sample>
    <sample id="1415">"Selbst die besten Modelle haben Schwierigkeiten, rückwärts integrierte Kenntnisse zuverlässig vorherzusagen, wenn diese erst bei der Inferenzzeit bekannt werden. Wenn Sie weitere Details interessant sind, sehen Sie bitte unsere Paper und das Dataset auf GitHub. Vielen Dank für Ihre Aufmerksamkeit."</sample>
    <sample id="1416">The disadvantages of tree-based methods are the need to obtain trees, which can be a complex and computationally expensive process, involving formalism-specific preprocessing and potentially specialized grammar induction procedures.</sample>
    <sample id="1417">The authors of the paper "Do Connell 2003 named entity taggers still work well in 2023" are affiliated with the University of Colorado.</sample>
    <sample id="1418">"Hallo, ich bin Myra, und heute spreche ich über unsere Arbeit mit den markierten Personas, bei der wir natürliche Sprachanweisungen verwenden, um Stereotype in Sprachmodellen zu messen. Dieses Werk wird in Zusammenarbeit mit Essen Dermush und Dan Jerovsky durchgeführt."</sample>
    <sample id="1419">"In jüngster Zeit wurden viele die Präsenz sozialer Vorurteile in Stereotypen in großen Sprachmodellen oder LLMs dokumentiert."</sample>
    <sample id="1420">"Es gibt jedoch jedoch verschiedene Einschränkungen. Sie basieren meist auf handgefertigten Datensätzen, die sehr zeitintensiv zu kuratieren sind."</sample>
    <sample id="1421">Und sie messen auch nur sehr spezifische Stereotype, was bedeutet, dass sie sich schlecht auf andere Demographien oder Kontexte übertragen lassen, oder sie fangen allgemeine, breite Assoziationen auf, wie negative Assoziationen mit bestimmten Gruppen.</sample>
    <sample id="1422">"Darüber hinaus berücksichtigt die meisten Arbeit in diesem Bereich nicht die Intersektionalität, d.h. die Vorstellung, dass vielfältige soziale Identitäten sich kumulativ auf Vorurteile auswirken und einzigartige Orte von Schaden sein können."</sample>
    <sample id="1423">Um diese Einschränkungen zu überwinden, setzen wir uns auf die Tatsache, dass diese neueren, instruktionsangepassten LLMs sehr gut auf Anweisungen und Anregungen reagieren.</sample>
    <sample id="1424">"Wir können dem Modell ein Persona vorschlagen, das eine Darstellung eines imaginären Individuums ist, das auf einen Anlasshinweis reagiert, wie: Stellen Sie sich vor, Sie sind eine asiatische Frau, beschreiben Sie sich selbst."</sample>
    <sample id="1425">Und wir können sofort sehen, dass dies auf jeden Demografen übertragbar ist, weil wir einfach irgendeinen Identitätsmerkmal in diese Anweisung einsetzen können.</sample>
    <sample id="1426">Hier sind einige Beispiele von GPT-4-Generierungen.</sample>
    <sample id="1427">"Sogleich sehen wir, dass die Ausgaben nicht offensichtlich negativ oder toxisch im traditionellen Sinn dieser Wörter sind."</sample>
    <sample id="1428">Es gibt einige interessante Muster.</sample>
    <sample id="1429">Die asiatische Frau wird als unscheinbar dargestellt. Die mittelöstliche Frau wird mit Worten wie "exotisch" beschrieben und auf eine faszinierende Region angespielt.</sample>
    <sample id="1430">Und beide Frauen-Figuren von Farbe beziehen sich auf die Abstammung, während die weiße Mann-Figur nichts dergleichen hat.</sample>
    <sample id="1431">Um diese Muster zu erfassen, hat unsere Methode zwei Teile. Der erste Teil ist die Generierung dieser Persönlichkeiten.</sample>
    <sample id="1432">Unsere Anregungen für die Generierung dieser Charaktere wurden von einem Studie inspiriert, bei der sie diese Anregungen an menschliche Probanden gaben, wodurch sie auch rassistische Stereotype ans Licht brachten.</sample>
    <sample id="1433">Und ermöglicht auch eine direkte Vergleichbarkeit zwischen unseren generierten Personas und den von Menschen geschriebenen Antworten.</sample>
    <sample id="1434">"Der zweite Teil ist markierte Wörter, ein Verfahren, um die Wörter zu identifizieren, die markierte Gruppen von unseren markierten unterscheiden, über die ich mich im Detail später auslassen werde."</sample>
    <sample id="1435">"Der Vorteil dabei ist, dass wir sehr spezifische Stereotype und Muster erhalten, ohne auf ein bestimmtes Vokabular angewiesen zu sein."</sample>
    <sample id="1436">Das markierte Wortverfahren beruht auf dem soziolinguistischen Konzept der Markiertheit, wonach es ein unmarkiertes Standardvorbild gibt und jede Gruppe, die sich von diesem Standard abhebt, linguistisch markiert ist.</sample>
    <sample id="1437">"Beispielsweise ist das Wort 'Mann' oder entschuldigung, das Wort 'Krieger' normalerweise mit Männern verbunden. Wenn Menschen also einen weiblichen Krieger beschreiben, verwenden sie meist 'Mann-Krieger' und markieren das Wort mit 'Frau'.</sample>
    <sample id="1438">"Und breiter gesprochen sind dominante Gruppen in der Gesellschaft sowohl sprachlich als auch sozial unmarkiert, während die marginalisierten Gruppen meist markiert sind."</sample>
    <sample id="1439">"Wir beginnen in unserem Ansatz damit, dass wir die unmarkierten und markierten Gruppen festlegen."</sample>
    <sample id="1440">Und dann vergleichen wir die Personas mithilfe des Kampf-Wort-Verfahrens, das sich grundlegend auf gewichtete Log-Odds-Verhältnisse stützt, um die Top-Wörter für jede markierte Gruppe zu bestimmen.</sample>
    <sample id="1441">"So zum Beispiel für die Rollen von Schwarzen Frauen würden wir Kampf-Wörter und die Verteilung der Gesetze gegenüber beiden unmarkierten Gruppen, weißen Rollen und männlichen Rollen, vergleichen, weil diese sind die beiden korrespondierenden unmarkierten Gruppen."</sample>
    <sample id="1442">Und jetzt kommen wir zu den Ergebnissen. Wir verwenden also ein Stereotypenverzeichnis und finden heraus, dass die generierten Personas mehr Stereotype enthalten als die von Menschen geschriebenen.</sample>
    <sample id="1443">"Indessen, wenn wir den Verteilung der Wörter im Wörterbuch tatsächlich betrachten, finden wir sehr verschiedene Dinge."</sample>
    <sample id="1444">"Die generierten Personas haben eine höhere Rate an Luxuswörtern, aber die menschgeschriebenen haben eine breitere Verteilung von Wörtern, während die Stereotypwörter in den generierten Personas tatsächlich nur die Wörter 'tall' und 'athletic' sind."</sample>
    <sample id="1445">"Also nur die positiven oder zumindest nicht negativen."</sample>
    <sample id="1446">Und tatsächlich fangen die Ergebnisse des Sexycons nicht viele der schädlichen Muster ab, die wir in den vorherigen Slides gesehen haben. Also werden wir stattdessen die Ergebnisse unserer markierten Wörter-Methode verwenden, um zu zeigen, wie diese positiv erscheinenden Wörter Stereotype und essentialisierende Narrativen fördern.</sample>
    <sample id="1447">"In unserer Analyse zeigen wir, wie diese scheinbar positiven Darstellungen sich schädliche Muster widerspiegeln."</sample>
    <sample id="1448">Zuerst für Markengruppen sind die obersten Wörter wie Kultur, Tradition, stolz und exotisch. Und diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und kennzeichnen sie als unterschiedlich von der weißen Norm.</sample>
    <sample id="1449">Dies trägt zu einer langen Tradition von Diskriminierung und Ausgrenzung für diese Gruppen bei.</sample>
    <sample id="1450">"Darüber hinaus gibt es viele gängige Muster, die in diesen Worten widergespiegelt werden, insbesondere für Frauen von Farbe. Zum Beispiel beschreiben die Wörter, die Latina-Frauen betreffen, Dinge wie lebhaft und körperlich.</sample>
    <sample id="1451">"Die Wörter, die sich auf einen tropischen Topos beziehen, sind bei asiatischen Frauen Dinge wie klein und zart und seidig."</sample>
    <sample id="1452">"Die Verbindung zu einer langen Geschichte asiatischer Frauen, die hypersexualisiert, als sehr unterwürfig und so weiter gesehen werden."</sample>
    <sample id="1453">"Und schließlich sehen wir bei schwarzen Frauen, dass einige der obersten Wörter wie stark und resilient sind."</sample>
    <sample id="1454">Dies verbindet sich an ein Archetyp, den Menschen als "starken schwarzen Frauen-Archetyp" bezeichnet haben. Und obwohl es auf den ersten Blick positiv klingt,</sample>
    <sample id="1455">Es gibt Forschungen, die zeigen, dass diese Art von Archetyp sehr schädlich ist, weil sie diese Demografien dazu bringt, resilient und stark gegen soziale Hindernisse zu sein.</sample>
    <sample id="1456">"Stattdessen setzt es Druck auf diese Menschen, diese Hindernisse zu überwinden, was zu sehr negativen Gesundheitsauswirkungen für diese Menschen und anderen Schäden führt."</sample>
    <sample id="1457">Und im Großen und Ganzen finden wir, dass die Wörter für jede markierte Gruppe sehr wesentlich reflektieren.</sample>
    <sample id="1458">"So basierend auf diesen Mustern, kommen wir zu drei Empfehlungen für die Besitzer des Modells."</sample>
    <sample id="1459">Zuerst sollten wir als Forscher positive Stereotype und essentialisierende Narrative abhaken. Wir sollten auch einen intersectionellen Blickwinkel verwenden, um Vorurteile und Schäden zu untersuchen, weil es viele Dinge gibt, die übersehen werden könnten, wenn wir das nicht tun.</sample>
    <sample id="1460">"Und schließlich sollte es eine erhöhte Transparenz über Methoden zur Vermeidung von Vorurteilen geben."</sample>
    <sample id="1461">Weil zum Beispiel diese positiven Stereotypen wir nicht wissen, ob es sich um irgendeine Art von seltsamer Tatsache handelt.</sample>
    <sample id="1462">"Eine übermäßige Wertorientierung oder möglicherweise andere Anti-Stereotypen-Methoden, die zu diesen schädlichen Mustern führen."</sample>
    <sample id="1463">"Wir können einfach keine Annahmen treffen oder es weiter untersuchen, ohne mehr Transparenz."</sample>
    <sample id="1464">Vielen Dank für das Zuhören. Haben Sie einen schönen Tag.</sample>
    <sample id="1465">Hallo alle, ich heiße Jingwei Yi von der Universität für Wissenschaft und Technologie in China.</sample>
    <sample id="1466">"Es ist mir eine Freude, ein kurzes Werbevideo über Papier zu erstellen. Kopieren Sie meinen Vorbild? Schützen Sie das Urheberrecht von großen Sprachmodellen für Embedding und Dienstleistungen. Ansicht rückseitiger Wasserzeichen."</sample>
    <sample id="1467">"Lassen Sie uns zuerst das Hintergrund über die Einladung unserer Dienstleistungen vorstellen."</sample>
    <sample id="1468">"Derzeit sind große Sprachmodelle wie GPT, Lama und Palm hervorragend in der natürlichen Sprachverständnis und -generierung."</sample>
    <sample id="1469">"Das Embedding als Service ist eines der Dienstleistungen, die auf großen Sprachmodellen aufbauen, um verschiedene NLP-Aufgaben zu unterstützen."</sample>
    <sample id="1470">Beispiel: Öffnen Sie unsere Angebote oder GPD basierend auf dem Batting-API.</sample>
    <sample id="1471">"Es gibt jedoch neueren Forschungen zufolge kann der Angreifer das Modell durch das Lernen von Embeddings stehlen und ähnliche Dienstleistungen anbieten. Deshalb ist es notwendig, das Copyright von Embeddings als Dienstleistungen zu schützen."</sample>
    <sample id="1472">Um die Urheberrechte von Embedding-Diensten zu schützen. Ein Lösungsansatz besteht darin, ein Wasserzeichen in den Dienst einzubetten und festzustellen, ob ein anderer Dienst das Wasserzeichen enthält.</sample>
    <sample id="1473">Die Wassermarkenmethode muss folgende Eigenschaften erfüllen. Zunächst sollte die Methode für die Einbettung von Dienstleistungen anwendbar sein. Zweitens sollte die Wassermarkierung nicht die Funktionalität der bereitgestellten Einbettungen beeinträchtigen.</sample>
    <sample id="1474">Drittens sollte das Wasserzeichen so gut bedeckt sein, dass der Angreifer es leicht entfernen kann.</sample>
    <sample id="1475">"Schließlich wird das Wasser-Unternehmen während des Modell-Extraktionsprozesses an die Angriffsdienste übermittelt."</sample>
    <sample id="1476">"Bestehende Werke können in vier Kategorien eingeteilt werden."</sample>
    <sample id="1477">"Dieser Ansatz ist entweder nicht anwendbar für die Einbettung als Dienstleistung oder fehlt an Übertragbarkeit."</sample>
    <sample id="1478">"Also schlagen wir in diesem Papier eine Markierung vor, die eine backdoor-basierte Wasserzeichenmethode für die Einbettung von Dienstleistungen ist."</sample>
    <sample id="1479">Dann stelle ich Ihnen die Details unseres Embeddings-Markers vor. Der Embeddings-Marker enthält zwei Hauptschritte, Wasserzeicheninjektion und Urheberrechtsüberprüfung.</sample>
    <sample id="1480">"Bevor wir diese Schritte ausführen, wählen wir zuerst einen Trigger-Set. Der Trigger-Set ist ein Gruppe von Wörtern in einem moderaten Frequenzintervall."</sample>
    <sample id="1481">Wir nehmen an, dass der Anbieter ein allgemeines Textkorpus sammeln kann und den Wortfrequenzvergleich durchführen kann.</sample>
    <sample id="1482">"In der Wassermarkeninjektion definieren wir zuerst eine Zielunterlage. Wenn ein Benutzer eine Nachricht an den Dienstleister sendet, zählt der Dienstleister die Auslöserzahl in der Nachricht."</sample>
    <sample id="1483">Die vorgegebene Embedding ist eine Gewichtssummierung der Ziels-Embedding und der Original-Embedding.</sample>
    <sample id="1484">Die Last des Zielsetzens ist proportional zur Anzahl der Auslöser in der Aussage. Wenn die Anzahl der Auslöser in der Aussage größer als m ist, ist das bereitgestellte Embedding genau gleich dem Zielsetzen.</sample>
    <sample id="1485">Die Copyright-Überprüfung dient dazu, festzustellen, ob ein Modell hinter einem anderen Service ein Wortmarke enthält.</sample>
    <sample id="1486">Wir erstellen zuerst einen Backdoor und ein benignes Datensatz. Der Backdoor-Datensatz enthält Sätze, von denen alle Wörter zum Trigger-Set gehören, während alle Wörter in den Sätzen des benignen Datensatzes nicht zum Trigger-Set gehören.</sample>
    <sample id="1487">Dann fordert der Anbieter die Embeddings vom Stealer-Dienst mit dem Datensatz.</sample>
    <sample id="1488">Die Kosinus- und L2-Similarität zwischen der angeforderten Embedding und der Ziel-Embedding werden berechnet. Wir berechnen den Similaritätsunterschied zwischen den Null- und den Backdoor-Datasets, der als Delta-Kosinus und Delta-L2 definiert ist.</sample>
    <sample id="1489">"Zusätzlich fügen wir KSTest hinzu und verwenden den p-Wert als dritten Wert."</sample>
    <sample id="1490">"Wir führen Versuche an vier Datensätzen durch, nämlich AG News, Mind, SSD2 und AresVam. Wir setzen annehmen, dass der Anbieter das Wikitext-Dataset zur Zählung von Wortfrequenzen verwendet."</sample>
    <sample id="1491">Die Ergebnisse auf vier Datensätzen zeigen, dass unser Embedding-Marker eine großartige Detektionsleistung aufweist, während er auch für Downstream-Aufgaben eine großartige Nutzen bringt.</sample>
    <sample id="1492">Wir haben auch die Verborgenheit des eingeblendeten Embeddings durch die Visualisierung des Embeddings von Sätzen auf vier Datensätzen von BOPCA validiert. Die Legende der Abbildungen bedeutet die Anzahl von Auslösereizen in jedem Satz.</sample>
    <sample id="1493">"Wie die Abbildungen zeigen, ist es schwierig, zwischen den Rücktoren-Embeddings und den normalen Embeddings zu unterscheiden."</sample>
    <sample id="1494">"Das ist alles. Vielen Dank. Wir kommen uns besprechen."</sample>
    <sample id="1495">ABC-Eval steht für "annotating behaviors in chat", also die Annotation von Verhaltensweisen in Chats.</sample>
    <sample id="1496">According to the transcript, the performance drop between CoNLL-2003 and CoNLL++ is not caused by adaptive overfitting, but by temporal drift. There is no specific information provided about the exact year when the performance delta is higher than 5 percentage points.</sample>
    <sample id="1497">Hallo, ich heiße Vasudha und ich bin ein Doktoranden in Computerwissenschaften an der Stony Brook University. Ich möchte unsere Arbeit, die in ACL 2023 als Langbeitrag angenommen wurde, präsentieren - eine Arbeit zum Thema Transfer-Learning für die Erkennung von Dissonanz, die sich mit dem Herausforderung des seltenen Klassenklassements auseinandersetzt.</sample>
    <sample id="1498">Wir beginnen indem wir kognitive Dissonanz definieren und erklären, warum sie ein wichtiger Gegenstand der Forschung in Bezug auf Sprache ist. Kurz gesagt, ist kognitive Dissonanz zwei Überzeugungen oder Handlungen, die inkonsistent sind.</sample>
    <sample id="1499">"Eine solche Beobachtung, wie ein Mensch sagt, ich weiß, dass Zigaretten mich umbringen könnten, und dann sagt, ich habe nach dem Meeting ein paar Zigaretten geraucht. Dieser Glaube und diese Handlung sind inkonsistent und stehen im Widerspruch zueinander."</sample>
    <sample id="1500">"Ich denke, ich könnte ohne sie mein Job nicht behalten, rechtfertigt den zweiten Vorkommnis und sie haben eine ständige Beziehung."</sample>
    <sample id="1501">"Während Diskrepanz ein sehr alltägliches Phänomen bei Entscheidungsfindungen in unserem täglichen Leben ist, sind sie sehr selten in Sprache ausgedrückt unter anderen Arten von Risikoschwankungen."</sample>
    <sample id="1502">Also ist es wichtig, warum das zählt? Der Ausgangspunkt des Denkens kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten zwischen Menschen zu verstehen, Trends und Wertvorstellungen und Einstellungsänderungen in Bevölkerungsgruppen zu verfolgen.</sample>
    <sample id="1503">"Hohe kognitive Dissonanz ist auch mit Angststörungen verbunden und hilft bei der besseren Verständigung von Menschen im Hinblick auf ihre mentale Gesundheit."</sample>
    <sample id="1504">"Die Erforschung von Distanz in Sprache kann auch bei der Verständigung von Extremismus und Polarisierung von vulnerablen Gruppen hilfreich sein."</sample>
    <sample id="1505">"Schließlich ist die kognitive Diskrepanz wichtig, um die persönlichen kognitiven Stile von Individuen und die Entscheidungsfindungsprozesse besser zu verstehen."</sample>
    <sample id="1506">"Zur Erreichung des Ziels, ein kognitives Widerspruchressourcen zu erstellen, haben wir eine groß angelegte Annotation von Widerspruch-Beziehungen durchgeführt. Wir haben den Widerspruch-First-Ansatz verwendet, wie im Flowchart dargestellt."</sample>
    <sample id="1507">"Zitate wurden mit einem PDTB-Parser verarbeitet und Paare von Diskurs-Einheiten nach den in unserem Artikel beschriebenen Richtlinien annotiert."</sample>
    <sample id="1508">Wie man hier sehen kann, wurde Dissonanz nur in 3,5 % der markierten Paaren gefunden.</sample>
    <sample id="1509">"Wir haben bei der Sammlung von 1000 Beispielen von Diskurs-Einheiten eine Ausbildung für einen ersten Klassifikator durchgeführt, der nur auf 43 Beispielen von Disnetztraining trainiert wurde. Wie erwartet, hat der Klassifikator nicht viel besser als Zufall gefunktioniert."</sample>
    <sample id="1510">"Wir haben bei der geringen Häufigkeit von Dissonanz und der Abwesenheit jeglichen vorherigen solchen Datensatzes das Problem der absoluten Seltenheit zu bearbeiten."</sample>
    <sample id="1511">"Um dies zu erleichtern, experimentieren wir mit Kombinationen von Transferlernen und aktiven Lernen, um so dass mehr dissonante Beispiele über weniger Annotationsschleifen gesammelt werden können, was die Gesamtkosten der Annotation senkt, während die Detektion von Dissonanz verbessert wird."</sample>
    <sample id="1512">"Da das ursprüngliche Modell die Dissonanz-Klasse überhaupt nicht erfassen konnte, beginnen wir den Prozess des aktiven Lernens, indem wir Gewichte von nahe verwandten Aufgaben übertragen."</sample>
    <sample id="1513">Wir wechselten von zwei verschiedenen Aufgaben. Thema unabhängig, Distanz-Klassifizierung, eine Aufgabe, die bestimmt, ob zwei Debatten-Sätze von verschiedenen Personen einvernehmlich oder uneinvernehmlich sind, unabhängig vom Thema.</sample>
    <sample id="1514">"Wir sprechen hier über eine Debatte und die binäre Klassifizierung von Expansion und Vergleichsklassen von PNTB. Da diese beiden eng mit der Konzeption von Konsonanten und Dissonanz verbunden sind, nennen wir sie CE hier."</sample>
    <sample id="1515">"Wir finden, dass das Ergebnis bei der Übertragung der Null-Performance auf dem annotierten Datensatz bereits besser ist als das Zufallsverhalten mit einer AUC von 0,62."</sample>
    <sample id="1516">"Wir haben festgestellt, dass das weitere Feinjustieren auf beiden Aufgaben ein viel besserer Zwei-Sprach-Ergebnis erzielt, wenn wir das Feinjustieren von CE-Aufgaben mit einem weiteren Feinjustieren auf Debattenfolge ausführen. Deshalb haben wir diese Modelle verwendet, um das aktive Lernen zu starten."</sample>
    <sample id="1517">"Wir bestimmen dann die beste Methode, ein Modell mit neuen Daten aus jedem Rund von Active Learning und Annotationen zu aktualisieren. Cumulator sammelt alle bislang gesammelten Daten von Active-Annotations ein, während iterative Updates das Modell durch Training auf dem neuesten Satz von Daten aktualisiert."</sample>
    <sample id="1518">"Während wir verschiedene Strategien ausprobiert haben, haben wir festgestellt, dass kumulative Strategien mindestens genauso gut oder sogar besser als iterative Strategien erbracht haben."</sample>
    <sample id="1519">"Um das Anzahlbeispiel für Dissonanz zu verbessern, verwenden wir eine Wahrscheinlichkeit von seltenen Klassen-Strategie PRC, um vor allem Beispiele auszuwählen, die wahrscheinlich am wahrscheinlichsten dissonant sind, gemäß dem aktuellen Modell in jeder Runde des Fehlers."</sample>
    <sample id="1520">"Wir vergleichen dies mit anderen hochmodernen Strategien, die in der Community üblicherweise eingesetzt werden."</sample>
    <sample id="1521">"Wir finden, dass die vorgeschlagene PRC-Strategie besser als andere state-of-the-art-Strategien funktioniert, wenn auch der Unterschied gering ist. Beachten Sie, dass die Leistung erheblich niedriger für zufällig ist."</sample>
    <sample id="1522">"Während weiterer Runden des AL mit zwei besten Strategien haben wir den Abstandsklassifizierungsauswert 2,75 erreicht, was der beste Leistungsstand ist, den wir bislang auf dieser Aufgabe erreicht haben."</sample>
    <sample id="1523">"Wir überprüfen auch die Durchführbarkeit jeder Strategie für die Qualität der Annotation und die Kosten für die Annotatoren. Wir finden heraus, dass PRC den höchsten Anteil an Dissonanz hat und am besten für seltenen Klassen geeignet ist. Allerdings finden die Annotatoren die Beispiele auch schwierig."</sample>
    <sample id="1524">"Insgesamt finden wir, dass PRC eine einfache AIL-Strategie für die seltene Klassenakquisition und kalte AIL-Starts mit gut entworfenen Übertragungsaufgaben signifikant hilfreich sind."</sample>
    <sample id="1525">"Wir finden auch, dass iterative Update nützlich ist für das Übertragen von Lernen aus einer anderen Domäne, während in-Domain-Aktive Annotationen von kumulativen Updates profitieren."</sample>
    <sample id="1526">"Diese sind die Links zu unserem Code, unserem Datensatz und unserem Paper. Bitte melden Sie sich bei uns, wenn Sie Fragen haben. Vielen Dank."</sample>
    <sample id="1527">The authors, Matthias Lendemann, Alexander Kodler, and Ivan Titov, are affiliated with the University of Berlin.</sample>
    <sample id="1528">Si Yu-Yuan.</sample>
    <sample id="1529">There are 4 authors involved in the work: Kaio Yan, Patrick Frenange, M.E. Liu, and Andre F.D. Martin's, and Graham Mubig.</sample>
    <sample id="1530">The approach is compared with the state-of-the-art architecture specifically tailored for stream-on-thigh-respirations translation.</sample>
  </task>
</testset>