<testset name="IWSLT2025" type="output">
  <task track="short" text_lang="de">
    <sample id="0">The most important data sources for language models are large-scale web crawl data, specifically political news media outlets such as New York Times, Los Angeles Times, The Guardian, and Huffington Post.</sample>
    <sample id="1">McGill University.</sample>
    <sample id="2">Hallo, willkommen zu unserer Präsentation von DePlain, einem neuen Korpus für die Identifizierung deutscher Texte auf Dokumentenebene und auf Satzebene.</sample>
    <sample id="3">"Mein Name ist Regina Storben und ich werde Sie durch den ersten Teil der Präsentation führen. Lassen Sie uns zuerst das Textvereinfachen definieren."</sample>
    <sample id="4">"Text-Nachricht ist ein Prozess, bei dem ein Text angepasst wird, um den Textverständnis für eine bestimmte Zielgruppe zu verbessern, insbesondere für Menschen mit Leseproblemen oder Nicht-Muttersprachler."</sample>
    <sample id="5">"Um ein Text-Notification-Modell zu trainieren, benötigen wir parallele Paare von Texten, zum Beispiel von Dokumenten oder Sätzen."</sample>
    <sample id="6">Ich kann leider nicht übersetzen, da ich keine englische Inhalte erhalten habe.</sample>
    <sample id="7">"Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie Sie am Beispiel sehen können, wie z.B. lexikalische Substitution, Klausalstellung, Umordnung von Klaussätzen oder Einfügung von Worten."</sample>
    <sample id="8">Wir schlagen unseren neuen Corpus D-Plattform vor. Im letzten Jahr gab es einige Probleme mit den bestehenden Korpora. Zum Beispiel sind diese Korpora hier zu klein, um ein Text-Notifikationsmodell zu trainieren.</sample>
    <sample id="9">Die anderen drei Modelle, die ich in den letzten Jahren vorgeschlagen habe, sind alle automatisch ausgerichtet, was bedeutet, dass sie in ihrer Ausrichtung fehleranfällig sein können.</sample>
    <sample id="10">"Wir schlagen daher unser neues Korpus Deplane vor, das in zwei Unter-Korpora unterteilt ist, nämlich Deplane APA und Deplane Web. Deplane APA basiert auf Nachrichtentexten."</sample>
    <sample id="11">"In der PlainAPA haben wir 483 Dokumente manuell ausgerichtet. Es ergibt etwa 30.000, 13.000 parallele Satzpaare."</sample>
    <sample id="12">Ein tieferes Korpus für eine Web-Suche. Dieses Korpus umfasst verschiedene Domänen. Wir verknüpfen alle 750 Dokumente sowohl manuell als auch mit automatischen Verknüpfungsmethoden.</sample>
    <sample id="13">Wir erzielen insgesamt 30.450 Satzpaare.</sample>
    <sample id="14">Wir analysieren unsere Satzpaare ein bisschen genauer. Zum Beispiel bei der Art der Vereinfachung.</sample>
    <sample id="15">Sie sehen hier, dass die Bibeltexte viel simpler sind als zum Beispiel die Nachrichten oder die Texte für Lerner, für Deutsch.</sample>
    <sample id="16">"Über alle Ebenen bezüglich z.B. lexikalischer Vereinfachung, struktureller Vereinfachung, auch insgesamt Ebenen der Vereinfachung."</sample>
    <sample id="17">"Darüber hinaus kann man sehen, dass unser Deplaned-Korpus eine hohe Vielfalt an verschiedenen Einfachungs-Transformationen aufweist. Zum Beispiel haben wir in dem Deplaned-API-Korpus viel mehr Umordnungen und Wörteränderungen als in dem Deplaned-Web-Korpus."</sample>
    <sample id="18">"Andererseits haben wir im Web-Korpus viel kürzere Bewertungen."</sample>
    <sample id="19">"Lass uns sehen, was wir mit diesem Korpus tun können. Hallo, ich bin Omar und ich spreche jetzt über die Anwendungsfälle für unser Datensatz Dplane. So kann man für den ersten Anwendungsfall automatische Alignmentsverfahren evaluieren."</sample>
    <sample id="20">"In den letzten Jahren gab es viele Alignmentsverfahren, aber im Kontext von Maschinübersetzungen."</sample>
    <sample id="21">Wir haben zwei parallele Dokumente in verschiedenen Sprachen und möchten Alignements von Sätzen in den Post-Dokumenten extrahieren.</sample>
    <sample id="22">"Wir versuchen jedoch in unserem Fall, Übereinstimmungen zwischen Sätzen zweier parallel veröffentlichter Dokumente mit derselben Sprache, denselben Inhalten, aber unterschiedlichen Komplexitätsstufen zu extrahieren."</sample>
    <sample id="23">Und jetzt, da wir das Datensatz Dplane haben, der manuell gesetzte Sätze haben, können wir diese Sätze als Goldstandard-Alignments verwenden, um einige vorgeschlagene Alignment-Methoden zu bewerten.</sample>
    <sample id="24">Und wir haben Anpassungen an die vorgeschlagenen Methoden vorgenommen und alle diese Anpassungen und die Codes, um unsere Experimente auszuführen, in dem Papier veröffentlicht.</sample>
    <sample id="25">Am Ende kamen wir zu dem Schluss, dass die beste Methode für die automatische Ausrichtung für die Vereinfachung von deutschen Texten die Methode der Massenausrichtung ist.</sample>
    <sample id="26">Und Sie können auch den Code zum Ausführen dieser Methode in Ihren eigenen Dokumenten nachlesen.</sample>
    <sample id="27">"Das zweite Fallbeispiel, das wir in unserem Paper gezeigt haben, ist das Fallbeispiel der automatischen Textvereinfachung."</sample>
    <sample id="28">Feinabstimmung von Sprachmodellen, um einfachen Text aus komplexen Eingabedaten zu produzieren.</sample>
    <sample id="29">Wir haben zwei verschiedene Modelle feinjustiert. Wir haben ein Modell für die Erzeugung von Dokumenten auf einfacher Ebene entwickelt.</sample>
    <sample id="30">"Wir haben auch die normale Basis-Import-Datei so fein abgestimmt, dass wir Sätze einfacher gestalten konnten."</sample>
    <sample id="31">Sie können auch alle Checkpoints finden und weitere Details zu den Scores und Bewertungsmetriken unserer Experimente in dem Papier einsehen.</sample>
    <sample id="32">"Wir kamen zu dem Schluss, dass diese grundlegende Feinabstimmung Ergebnisse besser als die Basisscores erzielen könnte."</sample>
    <sample id="33">"Und wir legen diese Ergebnisse als Referenz, als Basis für die zukünftige automatische Textvereinfachung."</sample>
    <sample id="34">Vielen Dank für Ihre Aufmerksamkeit und wir hoffen, Sie alle während der Konferenz zu treffen. Vielen Dank.</sample>
    <sample id="35">Kaio Yan.</sample>
    <sample id="36">The T5x large model was used to achieve an accuracy of 82-87%.</sample>
    <sample id="37">Ja, sie funktionieren auch noch in 2023.</sample>
    <sample id="38">The new aspect of the proposed human evaluation method is the explicit annotation of whether a model response exhibits certain behaviors, such as providing irrelevant information or contradicting itself.</sample>
    <sample id="39">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt von sauberen Validierungsproben ab.</sample>
    <sample id="40">"Consider rephrasing the question to focus on the specific aspects of the entity that the user is unsure about, rather than asking about the entity itself."</sample>
    <sample id="41">5</sample>
    <sample id="42">"Hallo, ich heiße Adam Szpirkowski und dies ist ein Vortrag über die Abhängigkeitsstruktur der Koordination."</sample>
    <sample id="43">"Sie wissen, dass verschiedene Theorien und Korpusansätze unterschiedliche Abhängigkeitsstrukturen voraussetzen. Zum Beispiel in den Universal-Abhängigkeiten wird die Struktur von Lisa, Bart und Maggie angenommen."</sample>
    <sample id="44">"ist so, dass der erste Konjunkt der ganzen Koordinationsstruktur der Kopf ist, also in diesem Fall Lisa."</sample>
    <sample id="45">"Ähnliche Annahmen werden auch in Igors Miltruks Bedeutungstheorie für Texte gemacht, wo wieder das gesamte Koordinationsgerüst von dem ersten Konjunkt geleitet wird. Also sind diese beiden Ansätze symmetrisch, sie isolieren ein Konjunkt."</sample>
    <sample id="46">Jetzt gibt es auch symmetrische Ansätze zur Koordinationsstruktur wie den Prager Ansatz, den Konjunktionenköpfigen Ansatz, bei dem die Koordinationsstruktur von der Konjunktion dominiert wird, in unpraktischen Abhängigkeitsbaumwerken.</sample>
    <sample id="47">Wir erhalten Abhängigkeiten von und zu allen Konjunkten.</sample>
    <sample id="48">"Und schließlich gibt es auch einen mehrköpfigen Ansatz, der zum Beispiel im Cutson's-Wörterbuch verwendet wird."</sample>
    <sample id="49">"Wo, sozusagen, alle Verhaltensweisen die Kopfstruktur sind. Wir erhalten Abhängigkeiten von dem Gouverneur, er erlaubt es allen Verhaltensweisen separat. Das sind Bartons Erfindungen."</sample>
    <sample id="50">"Das Ziel dieses Beitrags ist es, ein neues Argument für symmetrische Strukturen der Koordination wie diese beiden gegen asymmetrische Strukturen der Koordination wie diese zu entwickeln."</sample>
    <sample id="51">"Okay, das Argument basiert auf dem Prinzip der Abhängigkeits-Minimierung, wie es aufgrund dieser Beispiele erläutert wird."</sample>
    <sample id="52">"Im Englischen wissen Sie möglicherweise, dass direkte Objekte es bevorzugen, sich nah am Verb zu befinden, während Adjunkte weiter weg davon sein können. Also ist March read it yesterday in Ordnung, weil das direkte Objekt sich nah am Verb befindet."</sample>
    <sample id="53">"Die Aussage 'March read yesterday' ist gestern viel schlechter, stimmt, weil zwischen dem Verb'read' und dem direkten Objekt gibt es ein Adjunkte 'yesterday'."</sample>
    <sample id="54">"Es kann jedoch auch simuliert werden, wenn das direkte Objekt sehr schwer und sehr lang ist, weil es dann an die Position nach dem Agenten verschoben werden kann."</sample>
    <sample id="55">Das ist hier illustriert. Also sind beide Sätze in Ordnung. "March Redd" ist ein absolut faszinierender Buch über den BCS heute. "I" ist okay. Stattdessen haben wir dieses lange NP.</sample>
    <sample id="56">"Aber es ist auch in Ordnung, gestern März zu sagen. Es gibt ein absolut faszinierendes Buch über Frieden."</sample>
    <sample id="57">"Die Ursache dafür ist, dass dies möglich ist, weil dies auch obwohl dies den allgemeinen grammatikalischen Grundsatz verletzt, dass direkte Objekte direkt am Verb stehen sollten."</sample>
    <sample id="58">"Es erfüllt das Prinzip der Abhängigkeitslänge-Minimierung, das sagt, dass kürzere Abhängigkeiten bevorzugt werden."</sample>
    <sample id="59">"Diese beiden Bäume zeigen nur die Länge der wichtigen Abhängigkeiten an, also diejenigen, die zwischen diesen beiden Strukturen nicht konstant sind."</sample>
    <sample id="60">"Wir haben hier eine Abhängigkeit von rot nach dem Adjektiv von sieben Wortlänge und von rot nach Buch von vier Wortlänge. Also um es zu bekommen, sind es 11."</sample>
    <sample id="61">"Wenn Sie diese beiden Konstituenten vertauschen, wird die Summe dieser beiden Abhängigkeiten sechs werden, stimmt's nicht? Also anstatt elf, sechs, viel kürzer, das klingt auch recht gut, oder? Es verletzt ein Prinzip, aber erfüllt ein anderes."</sample>
    <sample id="62">"Okay, also haben wir verschiedene Statistiken über Koordination aus der erweiterten Version des Pantry Bank extrahiert und haben uns über den Artikel bewundert, warum wir keine Universitäts-Abhängigkeiten verwendet haben."</sample>
    <sample id="63">Und die Statistiken bestätigen die Beobachtung, die oft zuvor gemacht wurde, dass Konjunktionen links tendenziell kürzer sind. Also Salz und Pfeffer und Salz gemessen in Silben.</sample>
    <sample id="64">"Auch die Bemerkung, die in Bezug auf die Tatsache gemacht wurde, dass diese Neigung mit Längendifferenz wächst."</sample>
    <sample id="65">Wenn der Unterschied zwischen den Längen der beiden Konjunkte wächst, bevorzugt der kürzere Konjunkt es zu sein, stärkerer zu sein. Daher ist die Proportion größer für den linken kürzeren Konjunkt.</sample>
    <sample id="66">"Aber was ist neu in diesem Papier ist, dass wir festgestellt haben, dass diese Tendenz nur dann auftritt, wenn die Regierung auf der linken explodiert."</sample>
    <sample id="67">"Okay, also ist der Gouverneur links in diesem Beispiel. Ich sah Barton Lisa. Also ist der Gouverneur links."</sample>
    <sample id="68">Es fehlt im zweiten Beispiel, Homer kam und schnaubte, wir haben hier Koordination von zwei Verben und es gibt kein externes Regierungsorgan, richtig? Also in solchen Fällen bevorzugt das linke Konjunktionsglied eine kürzere Form, umso mehr je größer der Unterschied zwischen den beiden Konjunkten ist.</sample>
    <sample id="69">"Indessen verschwindet dieser Effekt, wenn die Rechtsregierung hier ist und die linke die Koordination des Telnet steuert."</sample>
    <sample id="70">Wir haben gezeigt, dass wir die Länge in Zeichen messen können. Im ersten Spalte sind die Silben, in der mittleren Spalte die Silben und in der rechten Spalte die Wörter. Ich konzentriere mich auf die rechte Spalte.</sample>
    <sample id="71">Was wir hier sehen, ist, dass wenn die Governance auf der linken Seite ist.</sample>
    <sample id="72">Die Tendenz, dass das linke Konjunktum kürzer wird, wächst kontinuierlich mit dem absoluten Unterschied in Worten. Und das gleiche beobachtet man auch, wenn es kein Regierungsorgan gibt, wie bei der Koordination von Sätzen, aber wenn das Regierungsorgan rechts ist, verschwindet diese Tendenz.</sample>
    <sample id="73">Und wir zeigen in dem Papier, dass dies ein Argument gegen asynchrone Strukturen der Koordination wie diese zwei und für synchrone Strukturen wie diese zwei ist.</sample>
    <sample id="74">"Sehen Sie bitte den Papier für die vollständige Übereinkunft und Argumente, entschuldigung, und sprechen Sie mit uns über die Poster-Sitzung. Danke."</sample>
    <sample id="75">2</sample>
    <sample id="76">Bible texts.</sample>
    <sample id="77">Das Beispiel ist "salt and pepper" und "not pepper and salt", gemessen in Silben.</sample>
    <sample id="78">Yes, the pre-trained models are freely available on the Uginphase and the training scripts are on the GitHub repository, so you can use them for your research.</sample>
    <sample id="79">DEplain-APA enthält Dokumente aus News-Texten.</sample>
    <sample id="80">Ein besserer Modell-Architektur, ein größeres Modell und mehr fine-tuning-Beispiele.</sample>
    <sample id="81">The tendency to shorter left conjunctions was measured by comparing the length of characters in the right column.</sample>
    <sample id="82">Die Experimente wurden durch die Messung der Länge von Sätzen in Charaktern, Silben und Wörtern durchgeführt.</sample>
    <sample id="83">Der Basisklassifikator performs not much better than chance.</sample>
    <sample id="84">1</sample>
    <sample id="85">Bob and Alice.</sample>
    <sample id="86">Kontextsensitive Modelle schneiden besser ab bei Diskursphänomenen wie Formalkonformität und lexikalischer Kohäsion ab.</sample>
    <sample id="87">The authors belong to the University of California, Berkeley.</sample>
    <sample id="122">The framework quantifies positionalitat by comparing annotations by demographic and evaluating them against models and datasets.</sample>
    <sample id="155">Die Studie fand heraus, dass die menschlichen Teilnehmenden auch Rassismusstereotype aufsurfen konnten, wenn sie die gleichen Persona-Prompts erhielten.</sample>
    <sample id="156">The study used the enhanced version of the Pantry Bank.</sample>
    <sample id="157">1</sample>
    <sample id="158">Die eng verwandten Aufgaben für kognitive Dissonanz sind die Konsonanz und die Binär-Klassifizierung von Expansion und Vergleich von CE (Conception, Expansion).</sample>
    <sample id="159">1</sample>
    <sample id="160">There is only one author mentioned, Vasudha.</sample>
    <sample id="161">Das Framework unterscheidet sich von bisherigen Arbeiten durch die Vergleichbarkeit von Endnutzern, Modellen und Datenmengen, Vorhersagen und Etiketten, anstatt nur die Übereinstimmung von Annotatoren oder die Verteilung von Modell-Annotatoren zu betrachten.</sample>
    <sample id="162">The generated personas.</sample>
    <sample id="163">Google Translate.</sample>
    <sample id="164">"Hallo, ich bin Xiangbin, Doktorand an der University of Washington. Heute präsentiere ich unser Werk von Vortrainingsdaten zu Sprachmodellen zu downstream-Aufgaben, verfolgen wir die Spuren politischer Bias, die zu unfaireren NLP-Modellen führen."</sample>
    <sample id="165">Die Sprachmodelle werden auf groß angelegte Web-Suchdaten trainiert.</sample>
    <sample id="166">"Politische Nachrichtenmedien sind in ihren Vorlagen gut vertreten in ihrem Vorbildungsdaten. Nach einer Umfrage des C4-Korpus können wir sehen, dass New York Times, Los Angeles Times, The Guardian, Huffington Post usw. gut in Sprachmodell-Trainingsdaten vertreten sind."</sample>
    <sample id="167">Dies hat eine gemischte Segnung für Anwendungen von Sprachmodellen geschaffen.</sample>
    <sample id="168">"Aber einerseits konnten sie sich von diversen Perspektiven lernen, was Demokratie und die Vielzahl von Ideen feiert. Andererseits sind diese verschiedenen politischen Meinungen von Natur aus sozial verzerrend und könnten in downstream-Applikationen zu Fairness-Problemen führen."</sample>
    <sample id="169">"Zu diesem Zweck schlagen wir vor, die Pipeline der politischen Bias-Verbreitung von Vortrainingsdaten zu Sprachmodellen bis hin zu downstream-Aufgaben zu untersuchen, indem wir folgende Fragen stellen."</sample>
    <sample id="170">"Zuerst bewerten wir die politischen linearen Sprachmodelle und welche Rolle könnte das zugehörige Datenmaterial auf solche politischen Bias haben?"</sample>
    <sample id="171">"Zweitens, wie performieren Sprachmodelle mit verschiedenen politischen Feinden bei Abstimmungsaufgaben und ob das möglicherweise zu Fairness-Problemen in NLP-Anwendungen führen könnte?"</sample>
    <sample id="172">"Wir schlagen vor, Sprachmodelle mit verschiedenen Vorlagenformaten zu promptieren, indem wir uns an politische Fragebögen wie dem politischen Kompass-Test anlehnen. Dies ermöglicht uns eine automatisierte Bewertung, die sich auf die politologische Literatur stützt."</sample>
    <sample id="173">"Einige vorläufige Ergebnisse zeigen, dass erste Sprachmodelle unterschiedliche politische Bedeutungen haben. Sie besetzen alle vier Quadranten auf der politischen Kompass."</sample>
    <sample id="174">Wir können auch sehen, dass GPT-4 der liberaleste Sprachmodell aller ist und GPT-Reihe generell sozialliberaler als BERT-Reihe und ihre Varianten sind.</sample>
    <sample id="175">Zweitens streben wir danach, festzustellen, inwiefern politische Vorurteile von Sprachmodellen tatsächlich aus Ausbildungsdaten aufgenommen werden.</sample>
    <sample id="176">Wir könnten ein kontrolliertes Experiment durchführen, indem wir Sprachmodell-Schritte an sechs verschiedenen parteiischen Korpora anpassen, die in Nachrichten und sozialen Medien geteilt sind, jeweils geteilt nach politischer Ausrichtung.</sample>
    <sample id="177">Durch die weitere Ausbildung von Sprachmodellen auf solchen Parteien in Kodpora können wir erkennen, dass die ideologischen Koordinaten des Sprachmodells sich entsprechend verschieben.</sample>
    <sample id="178">Beispielweise kann für Robert, das weiter trainiert wurde auf dem linkslinien-farben Korpus, ein beträchtlicher liberale Verschiebung in Bezug auf sein Vokabular sichtbar werden.</sample>
    <sample id="179">In Bezug auf seine politischen Vorurteile.</sample>
    <sample id="180">Und wir versuchen auch herauszufinden, ob Sprachmodelle die Polarisation aufgreifen können, die in unserer modernen Gesellschaft herrscht.</sample>
    <sample id="181">Wir teilen die Vorkompilationskorpora vor dem 45. Präsidenten der Vereinigten Staaten ein und trainieren separate Sprachmodelle auf den beiden verschiedenen zeitlichen Korpora nach dem 45. Präsidenten der Vereinigten Staaten.</sample>
    <sample id="182">"Wir können sehen, dass Sprachmodelle generell ein politisches Aufladen haben, das sich nach 2017 weiter von der Mitte entfernt hat. Dies zeigt an, dass Sprachmodelle auch die Polarisation in unserer Gesellschaft aufnehmen können."</sample>
    <sample id="183">"Und zuletzt, wir bewerten Sprachmodelle mit verschiedenen politischen Bedeutungen auf Hassrede-Detektion und Falschmeldedetektion für NLP-Anwendungen, die oft Sprachmodelle betreffen und sehr bedeutende Implikationen haben können."</sample>
    <sample id="184">"Wir sehen, wenn wir die Leistung pro Kategorie untersuchen, d.h. wenn wir die Leistung in Kategorien trennen, dass"</sample>
    <sample id="185">"Beim Analysieren von verschiedenen Demografien oder politischen linearen Medien können wir ein Muster erkennen, dass z.B. linke Sprachmodelle besser für die Detektion von Hassrede sind."</sample>
    <sample id="186">"bei der Erkennung von Hassrede gegen sozial unterrepräsentierte Gruppen."</sample>
    <sample id="187">"Unsere Arbeit zielt darauf ab, Gruppen in unserer Gesellschaft zu identifizieren, die stärker sind."</sample>
    <sample id="188">Und umgekehrt sind rechtschreibende Sprachmodelle besser darin, Hassrede gegen Weiße und Männer zu detektieren, jedoch schlechter darin, Hassrede gegen schwarze LGBTQ+- und andere Minderheiten zu detektieren.</sample>
    <sample id="189">Ähnliche Trends werden auch bei der Erkennung von Falschmeldungen beobachtet, bei der linke Sprachmodelle besser sind, um Falschmeldungen von ihrer entgegengesetzten politischen Ausrichtung zu erkennen und umgekehrt.</sample>
    <sample id="190">"Wir zeigen hier viele qualitative Beispiele, um zu sehen, dass Sprachmodelle mit verschiedenen politischen Bedeutungen."</sample>
    <sample id="191">"Gebt unterschiedliche Vorhersagen für Beispiele von Hassrede und Falschinformationen basierend auf ihren sozialen Kategorien. Es gibt noch mehr Beispiele im Anhang, um dies zu verdeutlichen."</sample>
    <sample id="192">Dies weist darauf hin, dass es ein Fairnessproblem gibt, das sehr dringend ist, was die politischen Vorurteile von Sprachmodellen betrifft.</sample>
    <sample id="193">Beispiel: Wenn ein rechter Sprachmodell in Sprachmodellen feinjustiert wird, um Hassrede oder Falschinformationen oder ähnliches, und auf einem beliebten sozialen Medien-Plattform bereitgestellt wird,</sample>
    <sample id="194">"Dies würde bedeuten, dass Menschen mit entgegengesetzten politischen Meinungen marginalisiert werden könnten und die Hetze gegen Minderheitengruppen unkontrolliert auftreten könnte."</sample>
    <sample id="195">"Dies klingelt uns an, um die Fairness-Probleme zu erkennen und anzugehen, die durch politische Bedeutungen von Sprachmodellen verursacht werden."</sample>
    <sample id="196">"Ein bisschen Diskussion. Wir möchten auch betonen, dass wir die einzigartige Herausforderung bei der Darstellung politischer Biase von Sprachmodellen hervorheben. Es ist wie zwischen Cilla und Karebdis."</sample>
    <sample id="197">"Wenn wir die politischen Meinungen im Trainingsdaten des Sprachmodells nicht saniert, wird der Bias aus dem Vorbildungsdaten in die Sprachmodelle und schließlich in die downstream-Aufgaben weitergegeben, was letztlich zu Fairness-Problemen führt."</sample>
    <sample id="198">"Wenn wir versuchen, auf eine Weise zu saniert, werden wir auch Zensur oder Ausschluss riskieren. Und es ist extrem schwierig, festzustellen, was tatsächlich neutral ist und welches Sprachmonitoring-Daten aufbewahren sollte. Es ist also ein bisschen wie das Elektrische Schaltproblem."</sample>
    <sample id="199">"Okay, großartig. Ich denke, das ist alles, was ich für heute habe. Vielen Dank für Ihre Zeit."</sample>
    <sample id="200">2</sample>
    <sample id="201">1024</sample>
    <sample id="202">I apologize, but it seems that the input provided is not a valid question or text. The input appears to be a series of repeated words in a language that is not English. As a result, I'm unable to provide an answer based on this input. If you could rephrase your question or provide a different input, I'll do my best to assist you.</sample>
    <sample id="203">Positionalität kann allgemein als die Perspektiven, die Menschen aufgrund ihrer Demographie, Identität und Lebenserfahrungen haben, definiert werden.</sample>
    <sample id="204">Dawei</sample>
    <sample id="205">Yes, according to the text, the solution suggests using a single offline SD model for every latency regime without retraining or adopting a specific architecture for a single SD.</sample>
    <sample id="206">There is 1 author, Yusin Zhang, from Penn State University.</sample>
    <sample id="207">Nein, das Modell funktioniert nicht gut, wenn es nicht auf KITMOS trainiert wurde.</sample>
    <sample id="208">Die drei Varianten von KITMOS sind: "Background pre-trained", "Background both" und "Background influence".</sample>
    <sample id="209">Stanford University.</sample>
    <sample id="210">The concluding research question is: How can we effectively utilize clean and noisy validation data for Weighted Sensitive Learning (WSL)?</sample>
    <sample id="211">Die Sensitivitätsmetrik misst die Fähigkeit eines Modells, bei leicht abweichenden Wortungen in den Anweisungen dieselben Ausgaben zu produzieren.</sample>
    <sample id="212">Jingwei Yi.</sample>
    <sample id="213">Eine höhere Sensitivität bedeutet eine bessere Leistung des Modells.</sample>
    <sample id="214">The models receive a linguistic context of a joint work with multiple authors.</sample>
    <sample id="215">20</sample>
    <sample id="216">Essen University.</sample>
    <sample id="217">It is necessary to develop new methods to measure media bias because existing language models, including first language models, can have varying political meanings and occupy all four quadrants on the political compass, indicating a need for more accurate and nuanced measures to assess media bias.</sample>
    <sample id="218">Akshita.</sample>
    <sample id="219">Die Pipeline für die Verbreitung politischer Vorurteile besteht aus der Ausbildung von Sprachmodellen anhand von diversen Perspektiven, die jedoch sozial bias-bedingte Meinungen enthalten können, die sich dann auf downstream-Aufgaben auswirken können.</sample>
    <sample id="220">Yes, the simplification process differs between Deplaned API and Web corpus, with Deplaned API having more reorderings and word editions, while the Web corpus has more rephrasing.</sample>
    <sample id="221">No.</sample>
    <sample id="222">Das Wasserzeichen wird in den Text durch eine Gewichtssummation des Ziel-Embeddings und des Original-Embeddings eingebettet, wobei der Gewicht des Ziel-Embeddings proportional zur Anzahl der Auslöser in dem Text ist.</sample>
    <sample id="223">Penn State University.</sample>
    <sample id="224">Ja, unsere Ergebnisse zeigen, dass Encoder-Decoder-Modelle wie MT5 durch Training mit einer Mischung von Sprachen verbessert werden können.</sample>
    <sample id="225">Ein Beispiel für eingeschränkte Sprachplanung ist die Planung, um ein Schokoladenkuchen zu backen.</sample>
    <sample id="226">Die Opazität wurde durch die Visualisierung von Satz-Embeddings auf dem VOPCA-Set validiert.</sample>
    <sample id="227">The work uses existing PLMs (Pre-trained Language Models) to fine-tune a new PLM by introducing three model trains on continental pre-training to analyze the impact of pre-training strategy.</sample>
    <sample id="228">Based on the text, GPT-4 is not explicitly stated to be most aligned to any specific country. However, it is mentioned that datasets and models are most aligned to English-speaking countries. Therefore, it can be inferred that GPT-4 is likely to be less aligned to non-English speaking countries.</sample>
    <sample id="229">The example on the right shows how the model uses the knowledge acquired through the cross-attention mechanism.</sample>
    <sample id="230">The number of tasks increases the model's performance while decreasing its sensitivity.</sample>
    <sample id="231">The three tree-less models compared to the authors' method on the Coggs benchmark are not explicitly mentioned in the given text.</sample>
    <sample id="232">The two co-authors, Alexander Kodler and Yvon Titov, are advisors to the first author.</sample>
    <sample id="233">There is no information provided about the author of PaLM in the given text.</sample>
    <sample id="234">"Hallo alle, ich bin Jenny, ein erstes Jahr PhD-Student an der Carnegie Mellon University, und heute werde ich mein Werk 'Anal Positionally, characterizing design by a CSA data set of models' vorstellen."</sample>
    <sample id="235">Dieses Werk wurde in Zusammenarbeit mit einigen Leuten an der University of Washington und dem Allen Institute for AI, nämlich Sebastian Santy, Ronan LaBros, Katarina Aranica und Martin Sapp, erstellt.</sample>
    <sample id="236">"Lass uns anfangen, indem wir uns vorstellen, dass wir für eine Zeitung arbeiten und wir uns durch die Kommentare unter unserem Artikel durcharbeiten, um toxischen Inhalt zu entfernen."</sample>
    <sample id="237">"Sie könnten sich auch an ein beliebtes API wie Perspective API für die Detektion von Giftigkeit wenden. Und das funktioniert sehr gut, wenn Sie Carl Jones sind, wo Perspective API Giftigkeiten korrekt detektieren kann."</sample>
    <sample id="238">"Aber das ist nicht wirklich der Fall für dithya-sharma, wo potenzielle APIs nicht so empfindlich gegenüber offensiven Begriffen sind, die in indischen Kontexten häufiger vorkommen."</sample>
    <sample id="239">Dies ist ein Beispiel für eine Design-Biase, bei der wir systematische Leistungsunterschiede von Technologie zwischen Bevölkerungsgruppen sehen.</sample>
    <sample id="240">"Entwurfsmuster wie das, das wir gerade gesehen haben, können aufgrund der Positionalität von NLP-Forschern und Modellentwicklern auftreten. Positionalität bedeutet einfach die Perspektiven, die Menschen aufgrund ihrer Demographie, Identität und Lebenserfahrungen haben."</sample>
    <sample id="241">Dies ist ein Konzept, das breit in Kritischen Studien verwendet wird, insbesondere in feministischen und queeren akademischen Räumen.</sample>
    <sample id="242">Und als Forscher kann die Positionalität den Forschungsprozess und seine Ergebnisse beeinflussen, weil sie die Entscheidungen, die Forscher treffen, ändern kann.</sample>
    <sample id="243">Und eines der Fragen, die Menschen stellen könnten, ist, ob Datensätze und Modelle eine Position haben.</sample>
    <sample id="244">"Und wir sagen nicht, dass Modelle selbst und Datensätze selbst demographische Identitäten und Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen von echten Menschen und können somit bestimmte Positionalitäten gegenüber anderen repräsentieren."</sample>
    <sample id="245">"Vorherige Forschungen haben anekdotisches Beweismaterial für eine Positionalität gezeigt, wie zum Beispiel Kulturunterschiede in Modellen und Datensätzen, sowie theoretische Definitionen von Modellposition.</sample>
    <sample id="246">"Aber diese Arbeiten betrachten nicht, wie sich die Endbenutzer mit den Datensätzen und Modellen selbst vergleichen."</sample>
    <sample id="247">"Und die Positionierung des Modells und der Daten wird immer wichtiger, wenn NLP-Aufgaben immer subjektiver und sozial ausgerichtet werden."</sample>
    <sample id="248">Und es ist schwierig, diese Positionalitäten zu charakterisieren, da nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter APIs versteckt sind.</sample>
    <sample id="249">"Um Daten und Modellpositionierung zu studieren, vergleichen wir die Annotationen mit echten Benutzern und bestehenden Daten-Sätzen und -modellen."</sample>
    <sample id="250">"Wir tun das durch unsere Framework NL Positionality."</sample>
    <sample id="251">Unser Framework funktioniert in zwei Haupt-Schritten.</sample>
    <sample id="252">"Der erste Schritt besteht darin, Datensätze erneut mit diversen Annotatoren zu annotieren."</sample>
    <sample id="253">Und wir entscheiden uns, dies über die Demographie von Original-Daten-Sätzen, Annotatoren, zu tun, weil normalerweise nur wenige Annotatoren jedes Einzelnen anpassen und weil Demographie seltener erfasst und geteilt wird.</sample>
    <sample id="254">"Und daher entscheiden wir uns, Daten zu re-analysieren, um viele Entitäten zu erhalten und ein reiches Set an demographischen Daten."</sample>
    <sample id="255">"Wir vergleichen dann die Annotationen nach Demographie mit den Modellen und Datensätzen paarweise, basierend auf unserer Korrelations-Score."</sample>
    <sample id="256">"Und damit unterscheidet sich unser Framework von der Literatur über Annotator-Disagreement, indem wir Benutzer mit Modellen und Datensätzen, Vorhersagen und Etiketten vergleichen, anstatt nur Annotator-Einigung oder die Verteilung von Annotatoren zu betrachten."</sample>
    <sample id="257">"Unsere Frame-Rate wird größtenteils durch Lab in the Wild ermöglicht, eine Online-Plattform für Zusammenarbeiten von HCI-Kollaboratoren."</sample>
    <sample id="258">"Lab in the Wild ist eine Online-Experimentierplattform, auf der wir diverse Freiwillige rekrutieren können, im Gegensatz zu Plattformen wie MTURC, die hauptsächlich Teilnehmer aus den USA oder Indien haben. Lab in the Wild kann trotzdem hochwertige Daten erheben."</sample>
    <sample id="259">"Wir hosten zwei Aufgaben im Out of the Wild, einer davon ist die Sozialakzeptanz. Und das funktioniert so, dass Teilnehmer eine Situation aus dem Sozialchemie-Datensatz lesen und dann beschreiben, wie sozial akzeptabel eine Situation ist."</sample>
    <sample id="260">Danach können sie, um sich im Studium zu engagieren, ihre Antworten mit einem AI und anderen vergleichen.</sample>
    <sample id="261">Wir haben diese Anmerkungen dann mit Sozialchemie, Delphi und GPT-4 verglichen.</sample>
    <sample id="262">"Wenn wir dann ein ähnliches Setup für die Detektion von Toxizität und Hassrede duplizieren, lesen sie einen Vorgang von Danny Hate und schreiben, ob sie glauben, dass es eine Hassrede ist."</sample>
    <sample id="263">Wir verglichen diese Annotationen mit Dynahate, Perspective-API, Rewire-API, HateRoberta und GPT-4. Am Ende unserer Studie sammelten wir über 16.000 Annotationen von über 1.000 Annotatoren aus 87 Ländern.</sample>
    <sample id="264">"So jetzt werden wir uns bemühen, herauszufinden, mit welchen NLP-Datensätzen und Modellen wir am meisten übereinstimmen. Wir finden heraus, dass es in NLP eine Positionalität gibt."</sample>
    <sample id="265">Beispielweise finden wir heraus, dass Datensätze und Modelle am besten zu englischsprachigen Ländern passen. So finden wir bei der GPT-4-Social-Acceptability- Analyse heraus, dass sie am besten zu konfuzianischen und englischsprachigen Ländern passt. Wir finden auch heraus, dass Dynahate am besten zu englischsprachigen Ländern passt.</sample>
    <sample id="266">"Wir finden auch die meisten Übereinstimmungen bei Menschen mit einem College-Abschluss. So finden wir bei GPT-4 im Sozialakzeptanz-Task, dass es sich am meisten an Menschen mit einem College-Abschluss oder einem Abschluss an einer Hochschule orientiert."</sample>
    <sample id="267">"Und wir finden dasselbe bei Dianaheid, wo es am meisten für Menschen mit einem College-Abschluss ausgerichtet ist."</sample>
    <sample id="268">"Indessen, wenn Modelle und Datensätze auf spezifische Bevölkerungsgruppen ausgerichtet werden, bleiben einige unberücksichtigt."</sample>
    <sample id="269">Ein Beispiel dafür ist, dass Datensätze und Modelle bei nicht-binären Personen im Vergleich zu männlichen und weiblichen Gegenparteien ungenauer sind. Wir finden dies in der GPT-4-Social-Akzeptabilitätsaufgabe sowie in der Analyse der "Dining-Hate"-Task.</sample>
    <sample id="270">"Womit können wir da gegen die Position in Atlantida und LP something do?"</sample>
    <sample id="271">"Wir haben einige Empfehlungen für Sie. Die erste ist, einen Bericht über alle relevanten Gestaltungswahlen während des Forschungsprozesses zu führen. Und die andere ist, NLP-Forschung durchzuführen, die Perspektivismus berücksichtigt."</sample>
    <sample id="272">"Unsere dritte Empfehlung besteht darin, spezielle Datensätze und Modelle innerhalb von vier spezifischen Communities zu erstellen. Ein gutes Beispiel dafür ist die Musseqani-Initiative. Wir möchten betonen, dass inklusive NLP nicht nur bedeutet, dass alle Technologien für jeden funktionieren.</sample>
    <sample id="273">"Und damit schließen wir unsere Präsentation, aber wenn Sie mehr erfahren möchten, können Sie gerne unseren Dashboard für die neuesten Analyse-Ergebnisse und unser Paper überprüfen. Danke."</sample>
    <sample id="274">Die Referentin geht auf 3 Probleme von SimulST ein: 1. spezifische Architekturen, 2. lange und complicated Trainingsverfahren und 3. Trainieren und Pflegen mehrerer Modelle für verschiedene Latenz-Regimes.</sample>
    <sample id="275">According to the speaker, social and political biases in training data for NLP models can be effectively reduced by sanitizing the data, but this poses risks of censorship or exclusion, making it challenging to determine what is truly neutral.</sample>
    <sample id="276">"Hi, ich bin Si Yu-Yuan von der Fudan-Universität. Ich bin hier, um unser Werk "Distinguished Script Knowledge from Language Models for Constrained Language Planning" vorzustellen."</sample>
    <sample id="277">"Im Alltag planen Menschen ihre Aktionen, indem sie Schritt-für-Schritt-Anweisungen in Form von garantierten Skripten befolgen."</sample>
    <sample id="278">"Der vorherige Welt hat Sprachmodelle genutzt, um Ziele für stereotype Aktivitäten wie das Backen eines Kuchens zu planen, und zeigte, dass große Sprachmodelle effektiv Ziele in Schritte zerlegen können."</sample>
    <sample id="279">"Es gibt jedoch bisherige Forschungen, die sich auf das Planen von Zielen abstrakter Aktivitäten konzentrieren. Das Planen von Zielen mit spezifischen Zielen und spezifischen Einschränkungen, wie z.B. ein Schokoladenkuchen backen, bleibt jedoch noch unzureichend untersucht."</sample>
    <sample id="280">"Wir definieren in diesem Papier das Problem der Restriktionsplanung."</sample>
    <sample id="281">"Die Ziele, die verschiedene Einschränkungen auf die GoalSaw-Planung aufstellen. Ein abstraktes Ziel kann von verschiedenen realen Zielen mit vielfältigen Einschränkungen geerbt werden. Ein guter Planer sollte Skripte schreiben, die vernünftig und treu zu den Einschränkungen sind."</sample>
    <sample id="282">"Wir bewerten und verbessern in diesem Paper die Fähigkeit von großen Sprachmodellen, konstruktive Sprache zu planen."</sample>
    <sample id="283">"Da gibt es kein spezifisches Daten-Site, um unsere Anfangsziele auszumachen."</sample>
    <sample id="284">"Wir müssen dieses Code zuerst erwerben. Wie im Tabelle gezeigt, erweitern wir den abstrakten Code mit multifacetten Einschränkungen für Menschen im Datenakquisition-Prozess mit Instruct GPT."</sample>
    <sample id="285">Wir probieren 100 spezifische Ziele ab und bewerten die generierten Skripte von großskaligen Modellen.</sample>
    <sample id="286">"Dieses Tableau berichtet über die Gesamtausbeute der Ergebnisse. Wir finden, dass alle Umgestaltungen von Line-ups unzufriedenstellende Ergebnisse bei der Planung von spezifischen Zielen erzielen."</sample>
    <sample id="287">Dann führen wir eine detaillierte Analyse durch, um zu untersuchen, was Modell-Ebene-Modelle sind.</sample>
    <sample id="288">Die Ergebnisse zeigen, dass die semantische Treue in generierten Skripten akzeptabel ist, aber die Treue zu den Einschränkungen nicht garantiert werden kann.</sample>
    <sample id="289">Wir untersuchen weitere fragmentierte Themenkategorien von Einschränkungen in Waking Home. Das Kopfblatt in der Abbildung zeigt, dass die Planungsleistung von Instructivity beträchtlich für Mädchen von verschiedenen Kategorien variiert.</sample>
    <sample id="290">"Vorherige Studien haben gezeigt, dass die Ausgabequalität von Modellen auf Zeilebene in hoher Varianz liegt, was zu schlechter Leistung führt. Daher übernehmen wir die Idee von übererzeugten Z-Filtern, um die Generationsqualität zu verbessern."</sample>
    <sample id="291">"Wir zeigen zuerst Constraint-Typen mit Beispielen für intract.cpt und erhalten spezifische Ziele auf Basis der abstrakten Ziele."</sample>
    <sample id="292">Dann, geben Sie GPT allgemeine Schablonen für spezifische Ziele an.</sample>
    <sample id="293">"Nächstes wird ein Filter-Modell erstellt, um die physischen Skripte auszuwählen."</sample>
    <sample id="294">Wir konvertieren Skripte und Ziele in abstrakte GPT-Embeddings und berechnen die Kosinussimilarity als Ähnlichkeitsscores, um semantische Ähnlichkeit zu messen.</sample>
    <sample id="295">"Zusätzlich werden wir den Skript, der die Schlüsselwörter des Zielkonstrains enthält, wild. Wir behalten nur den Skript, wenn das Ziel die höchste in den festgelegten Zielen ist."</sample>
    <sample id="296">"Mit unserem Verfahren können wir Quadrate von höherer Qualität erzeugen. Unser Verfahren verbessert die Planbarkeit sowohl in semantischer Vollständigkeit als auch in Treue zum Restriktionsverlauf."</sample>
    <sample id="297">"Da große Sprachmodelle teuer zu implementieren sind, ist es wichtig, die Sprachplanungsfähigkeit von kleineren und spezialisierten Modellen zu ermöglichen. Die Erstellung von Datensätzen ist ein essentieller Schritt dazu."</sample>
    <sample id="298">"Es gibt jedoch keine Studien, die spezifische Ziele planen ermöglichen, und die manuelle Annotation von Daten ist teuer."</sample>
    <sample id="299">Daher folgen wir dem Konzept symbolischer Wissenskonzentration, um eingeschränkte Sprachplanungsdaten von live-level-Modellen zu destillieren.</sample>
    <sample id="300">Wir werden unseren Ansatz für die Erstellung eines Datensatzes für konstruierte Sprachplanung, genannt Coscript, anwenden.</sample>
    <sample id="301">"Insgesamt erstellen wir 55.000 spezifische Ziele mit Skripten. Um die Qualität der Validierung und Test-Websites zu gewährleisten, bitten wir Cloud-basierte Arbeiter, falsche Beispiele zu finden und zu korrigieren."</sample>
    <sample id="302">Dieses Diagramm zeigt die Verteilung von Corscript. Wir finden, dass Corscript eine hohe Plotism in den generierten spezifischen Zielen aufweist. Mit Corscript können wir kleinere, spezialisierte Modelle für die Planung von Konstraintsprachen erstellen.</sample>
    <sample id="303">Wir finden, dass das T-File-Funktion auf dem Kursrate Skripte von höherer Qualität erzeugen kann als die meisten großen Modell-Level, was bedeutet, dass kleinere Modelle größere Modelle unterstützen können, wenn sie ordnungsgemäß auf geeigneten Datensätzen trainiert werden.</sample>
    <sample id="304">Zusammenfassend haben wir das Konstrainten-Langage-Planungsproblem definiert. Wir haben eine Konstrainten-Langage-Planungsfähigkeit für große Sprachmodelle entwickelt und ein Entstehungsfilterverfahren für großskalige Modelle entwickelt.</sample>
    <sample id="305">"Wir verwenden große Sprachmodelle, um ein hochwertiges Skript-Datensatz, Corscript, für die Erstellung von Sprachplanungen zu generieren. Wir hoffen, dass das Corset-Datensatz ein wertvolles Ressourcen für die Forschung zu Sprachplanungen sein kann."</sample>
    <sample id="306">"Danke für Ihre Zeit. Bitte finden Sie weitere Details zum Kurs-Script in unserem Papier."</sample>
    <sample id="307">Die Sprachgewandtheit von PaLM ist vergleichbar mit den aktuellen state-of-the-art-Systemen.</sample>
    <sample id="308">Die wichtigsten Eigenschaften eines Wasserzeichenverfahrens sind: Anwendbarkeit auf S-Dienste, keine Degradierung der Embeddings, ausreichende Deckung, um den Angreifer zu täuschen, und Übertragbarkeit auf den Dienstleistungen des Angreifers während des Modell-Extraktionsprozesses.</sample>
    <sample id="309">The TED Talks were translated into 14 different languages.</sample>
    <sample id="310">According to the text, the exact number of instances extracted for re-annotation is not specified.</sample>
    <sample id="311">The cosine and L2 similarity are used to measure the difference between the null and backdoor datasets.</sample>
    <sample id="312">The models that rely on a multilingual encoder, specifically the encoder PDR, were used in this task.</sample>
    <sample id="344">The authors assume the provider can collect a general text corpus and count the word frequency.</sample>
    <sample id="345">"Hallo alle, ich heiße Xu Heng. Heute präsentiere ich unser Paper, Do Connell 2003 genannte Entitäts-Tagger funktionieren immer noch gut im Jahr 2023. Lassen Sie uns losfangen."</sample>
    <sample id="346">Unser Papier untersuchte das Problem der Generalisierung am Beispiel der Task der Namenserkennung oder des NER-Aufgaben.</sample>
    <sample id="347">"Wir beobachten, dass Modelle fast 20 Jahre lang Kono 2003 verwendet haben, um NER für fast 20 Jahre zu entwickeln. Und das erzeugt natürlich einige Probleme. Zunächst einmal: Können diese Modelle auf moderne Daten generalisieren?"</sample>
    <sample id="348">"Wenn wir neue Tagger entwickeln, was ist für eine gute allgemeine Generalisierung notwendig?"</sample>
    <sample id="349">"Währenddessen, wenn wir bei schlechter Allgemeinheit beobachten, was die Leistungsentwicklung dieser Modelle verursacht?"</sample>
    <sample id="350">"Um diese Probleme zu untersuchen, haben wir das Carnot++-Datensatz entwickelt. Dies ist ein Datensatz, den wir von Reuters News aus dem Jahr 2020 gesammelt und dann mit den gleichen Anweisungen von Carnot 2003 annotiert haben."</sample>
    <sample id="351">"Wir haben dann über 20 Modelle auf Kono 2003 feinjustiert. Wir bewerteten sie auf beiden Kono-03-Testmengen und der Kono++-Testmenge."</sample>
    <sample id="352">"Und zuletzt haben wir den prozentualen Änderungsanteil von F1 berechnet, um die Allgemeingültigkeit jeder Modelle zu beurteilen."</sample>
    <sample id="353">"Was benötigt man für eine gute Allgemeingültigkeit? Durch unsere Experimente haben wir festgestellt, dass es drei Hauptzutaten benötigt."</sample>
    <sample id="354">"Das erste ist die Modellarchitektur. Durch unsere Experimente haben wir festgestellt, dass die Transformer-Modelle besser an neue Daten generalisieren."</sample>
    <sample id="355">Das zweite Zutat ist die Modellgröße. Wir fanden heraus, dass größere Modelle zu besserer Generalisierung führen.</sample>
    <sample id="356">Und zuletzt, wir wissen alle, dass die Anzahl der Feinjustierungsbeispiele direkt auf die Leistung eines downstream-Aufgaben einwirkt. Wir fanden auch heraus, dass mehr Feinjustierungsbeispiele auch zu besserer Generalisierung führen.</sample>
    <sample id="357">"Zu unserem nächsten Frage, was verursacht den Leistungsabfall einiger Modelle?"</sample>
    <sample id="358">Wir hatten zwei Hypothesen. Die erste ist die adaptive Überanpassung, die durch Wiederholen des gleichen Test-Satzes immer wieder verursacht wird. Und dies manifestiert sich normalerweise als eine Abnahme der Renditen an einem neuen Test-Satz.</sample>
    <sample id="359">Die zweite Hypothese ist der temporäre Drift, der durch den wachsenden zeitlichen Abstand zwischen Trainings- und Testdaten verursachte Leistungsabnahme.</sample>
    <sample id="360">"Für adaptives Übertraining sahen wir an dem auf der rechten Seite abgebildeten Graphen, dass die rote beste Passungslinie einen Gradienten aufweist, der größer als 1 ist."</sample>
    <sample id="361">Dies bedeutet, dass jedes Verbesserungseinheit auf Spalte 2003 zu mehr als einer Verbesserungseinheit auf Spalte plus plus führt, was bedeutet, dass es keine sinkenden Renditen gibt.</sample>
    <sample id="362">"Das zeigt uns, dass adaptives Überanpassen in diesem Fall nicht beobachtet wird."</sample>
    <sample id="363">"Was ist mit Temporary Trif dann?"</sample>
    <sample id="364">Für Temporal Drift haben wir ein Experiment durchgeführt, um bestimmte Modelle erneut auszubilden oder fortzuführen, indem wir sie mit neueren Daten trainierten. Wir fanden heraus, dass die Leistung mit größerem zeitlichen Abstand abnimmt.</sample>
    <sample id="365">"Und dies bestätigt unsere Hypothese, dass die Hauptursache für den Leistungsabfall der zeitliche Drift ist."</sample>
    <sample id="366">Unser Schluss ist, dass wir für eine gute Allgemeingültigkeit ein besseres Modellarchitektur, ein größeres Modell und mehr feinjustierte Beispiele benötigen. Und diese Ziele müssen gemeinsam erfüllt werden, wir können nicht nur eines von ihnen haben.</sample>
    <sample id="367">"Gleichzeitig fanden wir auch heraus, dass die Leistungseinbußen hier durch zeitliche Drifts verursacht werden und überraschenderweise nicht durch adaptives Überpassen verursacht werden, obwohl Conno2003 bereits seit über 20 Jahren verwendet wird."</sample>
    <sample id="368">„Gehen wir zurück auf die Frage, die wir in unserem Titel unserer Arbeit aufgeworfen haben, funktionieren Connell-Tagger immer noch in 2023? Und wir fanden heraus, dass die Antwort ein lautes Ja ist.“</sample>
    <sample id="369">Wir hoffen, dass unsere Arbeit weitere Forschungen zu den allgemeinen Erweiterungen der Modelle anregt.</sample>
    <sample id="370">"Und zuletzt, bitte überprüfen Sie unsere Arbeit, unser Datensatz. Wenn Sie Fragen haben, können Sie mich gerne kontaktieren. Vielen Dank."</sample>
    <sample id="397">The language segment size used in this approach is 1 second.</sample>
    <sample id="398">Entity-specific knowledge: Location, Person (Servin and Kea)</sample>
    <sample id="399">Der wichtigste Faktor ist die Qualität des Beispiels.</sample>
    <sample id="400">Die Arbeiten konzentrieren sich auf GPT-4 und BERT-Modelle.</sample>
    <sample id="401">The model combines values from multiple levels.</sample>
    <sample id="402">Direct inference examples: saying the name of the song or its position, the first one.</sample>
    <sample id="403">Fudan University</sample>
    <sample id="404">1</sample>
    <sample id="405">No.</sample>
    <sample id="406">Das Beispiel für eine markierte Gruppe ist "warrior who is a woman".</sample>
    <sample id="407">The model architecture that does not generalize well is not specified in the given text.</sample>
    <sample id="408">The test data sets are referred to as "clean data".</sample>
    <sample id="409">2</sample>
    <sample id="410">The authors work with multimodal protein models, implying they are using multiple modalities, not just text.</sample>
    <sample id="439">Pre-trained time and inference time knowledge.</sample>
    <sample id="440">Ying und Zhiyang.</sample>
    <sample id="441">Yes.</sample>
    <sample id="442">The existing resources for context-dependent translations have limitations in terms of the types of context-dependent translations they support and the languages they cover, as they often rely on domain knowledge and human curation.</sample>
    <sample id="443">"Hallo, und ich spreche über unsere Arbeit am Löschen von indirekten Bezügen für die Auswahl von Entitäten, bei der wir das Alt-Entity-Score einführen."</sample>
    <sample id="444">"Mein Name ist Jawad Hosseini und dies ist ein gemeinsames Werk mit Philippe Ladinsky, Sylvia Parry und Annie Lewis."</sample>
    <sample id="445">Ich kann leider nicht den Text übersetzen, da er nicht in englischer Sprache geschrieben ist. Der Text zu übersetzen, der aus einer Reihe von Wiederholungen des Wortes "درستان" besteht, ist nicht sinnvoll, da es sich um ein persischsprachiges Wort handelt.</sample>
    <sample id="446">Die offensichtlichste Sache ist, eine direkte Differenz zu verwenden. Zum Beispiel, indem man den Namen des Liedes in mir oder seiner Position, dem ersten, sagt.</sample>
    <sample id="447">"Manchmal ist ein indirekter Hinweis besser geeignet, um eine natürlichere Konversation zu führen. Dies kann passieren, wenn der Benutzer den Titel eines Liedes nicht mehr aus dem Kopf bringt."</sample>
    <sample id="448">Ich kann leider keine Übersetzung vornehmen, da der Text keine englische Sprache enthält. Der Text scheint persisch zu sein und besteht aus Wiederholungen des Wortes "بایدی" (bāydī), was in etwa "soll" oder "müssen" bedeutet.</sample>
    <sample id="449">Ich kann leider nicht die gesamte Textsequenz übersetzen, da sie nur aus 36 Malen des Wortes "از" besteht und keine sinnvolle Bedeutung hat. Es ist nicht klar, was der ursprüngliche englische Text war, da die Eingabe nur aus dem persischen Alphabet besteht.</sample>
    <sample id="450">Dies ist ein wichtiges Problem in konversationsbasierten Systemen und auch für die Benchmarking von LLM-Entity-Verständnis.</sample>
    <sample id="451">Ich kann leider keine Übersetzung durchführen, da der Text lediglich aus wiederholten " نمید" besteht, was keine sinnvolle Aussage darstellt.</sample>
    <sample id="452">Unsere Datensatz-Sammlungsmethode setzt sich durch die Verwendung eines Cartoon-Abgeschlossenheits-Satzes auseinander.</sample>
    <sample id="453">"Das Cartoon hat drei Sprachblasen. Im ersten Blase sagt Bob, erinnere dich an das Lied, das wir gestern gehört haben? Und damit sagt Bob den Dialogkontext."</sample>
    <sample id="454">In dem zweiten Sprachblasen sagt Alice: Meinst du, ich meine leicht mit mir oder ich habe ein Gefühl?</sample>
    <sample id="455">"Das ist die alternative Frage. Und im dritten Redeblatt verwendet Bob eine indirekte Referenz, um eine dieser Entitäten auszuwählen, zum Beispiel die neuere."</sample>
    <sample id="456">Ich kann leider keine Übersetzung anbieten, da der Text nicht in Englisch, sondern in Persisch (Farsi) geschrieben ist.</sample>
    <sample id="457">"Das zweite, das alternative Frage, wird wie folgt generiert."</sample>
    <sample id="458">Wir verwenden immer ein einfaches Vorlage. Meinst du A oder B?</sample>
    <sample id="459">Hier sind die verschiedenen Abtastmethoden, die wir verwendet haben. Wenn wir höher in der Liste hinaufsteigen, werden die Entitäten einander ähnlicher und es wird schwieriger, die Verwirrungen zu lösen.</sample>
    <sample id="460">Die erste ist Uniform Attract.</sample>
    <sample id="461">Die zweite ist, wenn die Entitäten ähnliche Titel haben. Zum Beispiel zwei Bücher mit dem Namen "The Retail".</sample>
    <sample id="462">Die dritte ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben und schließlich wenn sie ähnliche Infoboxen oder Attribute auf Wikipedia haben. Zum Beispiel dieselbe Genre oder dieselbe Künstler für ein Lied.</sample>
    <sample id="463">"Lassen Sie mich dieses alternative Frage beantworten. Sie kennen den Namen dieser Entitäten, aber sie wissen nicht notwendigerweise über die Entität selbst."</sample>
    <sample id="464">"So zeigen wir einige Hintergrundinformationen über die beiden Entitäten. Bei Liedern zeigen wir einfach einen Google-Suchlink zu jedem Lied."</sample>
    <sample id="465">"Und bitten Sie die Annotatoren, zumindest einige von jeder Musik zu hören und über jede Musik zu lesen. Hier ist zum Beispiel das Google-Suchergebnis für die Single "EasyHunt".</sample>
    <sample id="466">Für das Rezepte- und Bücher-Domain zeigen wir einige Hintergrund-Texte aus Wikipedia. Für Rezepte zeigen wir ihnen außerdem Bilder von Wikipedia, damit die Annotatoren wissen, wie sie aussehen.</sample>
    <sample id="467">Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts. Wir haben nichts.</sample>
    <sample id="468">Ich kann leider nicht den Inhalt übersetzen, da es sich um eine langsame Folge von "از" handelt, die möglicherweise eine Art von Lärm oder ein Geräusch ist und nicht einen sinnvollen Text enthält.</sample>
    <sample id="469">Das L-Korpus enthält 6000 alternative Fragen in drei Domänen und 42.000 indirekte Bezugsausdrücke. Die Ergebnisse mit dem T5-Large-Modell sind wie folgt zusammengefasst.</sample>
    <sample id="470">Wenn das Sprachmodell Zugang zu dem gleichen Hintergrundwissen wie die Annotatoren hat, dann ist die Genauigkeit sehr hoch. Sie beträgt etwa 92% bis 95%. Aber das ist nicht realistisch.</sample>
    <sample id="471">Ich kann leider nicht den Inhalt übersetzen, da der Text nur aus dem Wort "اگر" (if) und verschiedenen "از" (von) besteht und keine sinnvolle Aussage macht. Es handelt sich um eine möglicherweise irrtümliche Eingabe oder eine Art von Spam.</sample>
    <sample id="472">Wenn das Sprachmodell nur Zugriff auf Entitätsnamen hat, beträgt die Genauigkeit nur 60%. Es gibt also viel Raum für Verbesserungen. Wir haben auch gezeigt, dass die Modelle domain-generalisiert sind. Hier ist ein Link zu unserem Datensatz. Vielen Dank.</sample>
    <sample id="473">The approach is compared with existing SimulST guidelines, specifically the weight-key strategy and the local agreement.</sample>
    <sample id="474">Université de Lyon.</sample>
    <sample id="475">Jenny</sample>
    <sample id="476">2</sample>
    <sample id="477">Hallo, ich bin Sara Pappi von der Universität Trento und der Fondazione Bruno Kessler, und ich werde den Vortrag "Attention as a Guide for Simultaneous Speech Translation" vorstellen, der ein gemeinsames Werk mit Matteo Negri und Marco Turchi ist.</sample>
    <sample id="478">"Was ist Simultane Sprachübersetzung? Simultane Sprachübersetzung, oder SIMUL-ST, ist der Prozess, bei dem gesprochene Sprache in Echtzeit in eine andere Sprache übersetzt wird, um eine Kreuzsprachkommunikation zu ermöglichen."</sample>
    <sample id="479">"Und was sind die Probleme der aktuellen Reizmodellen? Spezifische Architekturen werden normalerweise erweiterte Module hinzugefügt, um optimiert zu werden."</sample>
    <sample id="480">Komplizierte und lange Ausbildungsvorgänge, zum Beispiel Ausbildung mit verschiedenen Optimierungsziele,</sample>
    <sample id="481">Und das Training und Erhalten von mehreren Modellen, um verschiedene Latenzzeiten zu erreichen. Zum Beispiel das Training eines Modells mit einer durchschnittlichen Latenz von einer Sekunde und ein anderes mit einer Latenz von zwei Sekunden usw.</sample>
    <sample id="482">Welche Lösung haben wir?</sample>
    <sample id="483">Zuerst verwenden Sie ein offline-vorhandenes SD-Modell ohne Neu-Trainieren oder Anpassen einer spezifischen Architektur für ein einzelnes SD. Verwenden Sie nur ein Modell für jede Latenz-Regelung und handeln Sie Latenz durch spezifische Parameter.</sample>
    <sample id="484">"Und nutzen Sie das bereits erworbenen Wissen des Modells durch die Aufmerksamkeitsmechanik zwischen audiovisuellem Eingabe und textuellem Ausgang, das ist die Quer-Aufmerksamkeitsmechanik. Und Sie können ein Beispiel rechts sehen."</sample>
    <sample id="485">Unsere Lösung besteht darin, ein Punkt- oder Encoder-Code für die Aufmerksamkeit vorschlagen, und es ist eine Strategie, um zu entscheiden, ob wir ein partielle Übersetzung ausgeben oder nicht, basierend auf dem, wo die Aufmerksamkeit hinweist.</sample>
    <sample id="486">Ein Wort wird ausgesendet, wenn die Spannung nicht konzentriert ist, das ist, wenn diese Summe unter einem bestimmten Schwellenwert alpha liegt, in Richtung der letzten lamba's Ton-Frames, was bedeutet, dass die empfangene Information stabil genug ist.</sample>
    <sample id="487">Beispiel: Wenn wir eine Sprachdatei erhalten, die "Ich werde über das sprechen und unser Modell vorhersagt die Übersetzung auf Deutsch, dann kann unser Modell die Übersetzung wie folgt vorhersagen: "Ich werde über das sprechen und unser Modell predicted die Übersetzung auf Deutsch."</sample>
    <sample id="488">Wir werden die Kross-Beobachtungs-Weight betrachten.</sample>
    <sample id="489">"Wir werden sehen, dass die ersten zwei Wörter auf die frühste empfangenen Sprachframes hinweisen, während das letzte Wort auf die letzten empfangenen Sprachframes hinweist."</sample>
    <sample id="490">"Dies bedeutet, dass die ersten zwei Wörter weggelassen werden."</sample>
    <sample id="491">"Während die Summe der Quer-Aufmerksamkeit oberhalb eines bestimmten Schwellenwert alpha liegt, wird das letzte Wort nicht abgesetzt und wir warten auf den nächsten Sprechblock."</sample>
    <sample id="492">"Wenn wir weitergehen und wir ein weiteres Sprachtank erhalten und unser Modell drei weitere Wörter vorhersagt und wir diese Cross-Attention-Weight betrachten."</sample>
    <sample id="493">Wir werden sehen, dass keine Wörter auf die letzten Lambda-Sprechphasen verweisen.</sample>
    <sample id="494">Dies bedeutet, dass diese drei Wörter ausgesendet werden.</sample>
    <sample id="495">Wenn Sie die Hauptergebnisse davon betrachten,</sample>
    <sample id="496">Wir plotten die Ergebnisse der simultanen Raumübersetzungen auf Graphen, auf denen wir eine blaue Seite haben, die die Übersetzungsqualität und den durchschnittlichen Versatz misst.</sample>
    <sample id="497">"Das ist die Latenzmessung und wir berücksichtigen auch den computergestützten durchschnittlichen Liking, der die Zeit berücksichtigt, die das Modell benötigt, um den Ausgang zu vorhersagen."</sample>
    <sample id="498">Wir möchten, dass unsere Kurven so hoch wie möglich auf diesem Diagramm sind.</sample>
    <sample id="499">"Aber wir möchten auch, dass sie auf die linke Seite verschoben werden."</sample>
    <sample id="500">Und wir vergleichen sie mit Strategien, die auch auf offline-Modellen angewendet werden, die Gewichtsstrategie und die lokale Einigung. Und wir vergleichen auch mit dem state-of-the-art-Architektur, die speziell für die Übersetzung von steam-on-thigh-Respirationen entworfen wurde.</sample>
    <sample id="501">Ich kann leider nicht verstehe, was Sie sagen. Die Anweisung "These are all the results of the simultaneous space translation strategy on German" ist auf Englisch und ich bin eine deutsche Sprachassistentin. Ich kann jedoch die Übersetzung für Sie durchführen:

"Diese sind alle Ergebnisse der simultanen Raumübersetzung auf Deutsch."</sample>
    <sample id="502">Und wir sehen, dass die Erwachsenen-Ausgaben alle Strategien auf offline-Modellen anwenden, da die Kurven nach links verschoben sind.</sample>
    <sample id="503">"Wir sehen auch, dass die tatsächliche Laufzeit oder die zeitlich bedingte Zeit, das schnellste Verfahren ist."</sample>
    <sample id="504">"Wollen Sie mehr Ergebnisse entdecken, lesen Sie bitte unser Papier. Wir haben auch den Quellcode, die Modelle und die simultane Ausgabe freigegeben, um die Reproduzierbarkeit unseres Werks zu ermöglichen. Danke für Ihre Aufmerksamkeit."</sample>
    <sample id="505">No.</sample>
    <sample id="506">"Hallo alle, ich heiße Ying und mein Kollege Zhiyang und wir werden unsere Forschung über Multi-Instruct präsentieren, die Verbesserung von Multi-Modell-Spiritual-Lernen durch Anpassung von Anweisungen."</sample>
    <sample id="507">"Mit den Fortschritten bei großen Sprachmodellen begannen viele Arbeiten, neue Lernparadigmen zum Wiederverwenden von Sprachmodellen für verschiedene downstream-Aufgaben auf parameter- und dateneffiziente Weise zu untersuchen."</sample>
    <sample id="508">"Vor kurzem haben viele Studien gezeigt, dass die Anpassung von Anweisungen es großen Sprachmodellen ermöglicht, unerfahrene Aufgaben in einem zero-shot-Verfahren auszuführen, indem sie natürliche Anweisungen befolgen."</sample>
    <sample id="509">"Es ist jedoch bei meisten vorherigen Arbeiten zu Anpassung von Anweisungen, die Leistung bei Sprachaufgaben ohne Vorkenntnisse verbessert, während Computer-Vision- und multimodale Aufgaben ausgelassen wurden."</sample>
    <sample id="510">"Deshalb untersuchen wir in diesem Werk, ob die Anpassung von Anweisungen auf multimodale Proteinklassen tatsächlich die Allgemeingültigkeit auf unbekannte multimodale Aufgaben verbessern kann."</sample>
    <sample id="511">"Zusätzlich entdeckten wir bei unserer Forschung eine beträchtliche Diskrepanz in der Verfügbarkeit von Anweisungsdaten zwischen einem RLP und einem Mehrmodell."</sample>
    <sample id="512">Es gibt mehr als 1.600 Sprach-basierte Anweisungsaufgaben. Es gibt jedoch keine große öffentlich zugängliche multimodale Anweisungsaufgaben. Daher motiviert uns, ein multimodales Anweisungstuning-Dataset zu erstellen.</sample>
    <sample id="513">"Wir präsentieren Ihnen Multi-Instruct, das erste Mehrmodale-Instabilitätsbenchmark-Dataset, das 62 diverse Mehrmodale-Aufgaben umfasst, die 10 Kategorien von Brettspielen abdecken."</sample>
    <sample id="514">Diese Aufgaben werden aus 21 bestehenden offenen Quellcode-Datensätzen abgeleitet und jede Aufgabe ist mit fünf von Experten verfassten Anweisungen ausgestattet.</sample>
    <sample id="515">"Während wir das Multimodale Anpassen von Anweisungen an unserem vorgeschlagenen Datensatz untersuchen, verwenden wir OFA, ein einheitliches Modell für die multimodale Darstellung als Basismodell. OFA verwendet ein einheitliches Vokabular für Sprache, Bild-Tokens und die Koordinaten eines umgebenden Rechtecks."</sample>
    <sample id="516">Wir zeigen einige Beispiele aus unserem Multi-Insharat-Datensatz.</sample>
    <sample id="517">"zur Vereinigung der Verarbeitung von verschiedenen Eingabedaten- und Ausgabetypen."</sample>
    <sample id="518">Wir haben den Ansatz von OFA befolgt und alle Aufgaben in einer sequenz-zu-sequenten-Format formulierte, in dem der Eingabetext, Bilder, Anweisungen und Randboxen im gleichen Token-Raum dargestellt werden.</sample>
    <sample id="519">"Okay, jetzt spreche ich über die Anpassung von Mehrfach-Anweisungen."</sample>
    <sample id="520">"Wir verwenden für das Trainingsdatensatz 53 Aufgaben aus 9 Gruppen für das Training und Proben 10.000 Instanzen pro Aufgabe. Für das Testen reservieren wir den ganzen Gruppe für das Testen und wählen weitere 5 Aufgaben aus der VQA- und der Mischgruppe."</sample>
    <sample id="521">"Wir verwenden alle Instanzen im Test-Fleet für jede Aufgabe. Darüber hinaus wählen wir 20 Aufgaben aus dem Test-Fleet natürlicher Anweisungen als On-Site-Aufgabe für NLP aus."</sample>
    <sample id="522">"Wir verwenden ein großes OFA-Modell als Basismodell. Während des Trainings werden für alle Aufgaben alle Instanzen erstellt. Jede Instanz wird zufällig mit einem von fünf Anweisungsmustern kombiniert."</sample>
    <sample id="523">"Während des Testens führen wir für jede Aufgabe insgesamt fünf Experimente durch, indem wir das Modell anhand eines von fünf Anweisungen in jedem Experiment auswerten."</sample>
    <sample id="524">"Wir melden den Durchschnitt und den Maximalwert der Leistung und die Standardabweichung der Leistung in allen fünf Experimenten."</sample>
    <sample id="525">Wenn die Aufgabe eine mehrmodell-Klassifizierungsaufgabe ist, melden wir die Genauigkeit. Wenn es sich um eine mehrmodell-Generationsaufgabe handelt, melden wir den Wurzel-Gel als Ergebnis. Bei einer RP-Aufgabe melden wir den Wurzel-Gel ebenfalls.</sample>
    <sample id="526">"Wir haben auch weitere Auswertungsmaßstäbe eingeführt, genannt Sensitivität. Dieses misst die Fähigkeit des Modells, unabhängig von leichten Abweichungen in der Wortwahl bei der gleichen Aufgabenstellung konstante Ausgaben zu produzieren."</sample>
    <sample id="527">"Unser Hauptresultat ist das folgende. Wir sehen, dass die Anpassung von Anweisungen die Leistung von OIS bei gleichen Mehrfach-Aufgaben erheblich verbessern kann."</sample>
    <sample id="528">"Das Übertragen von Lernwissen aus natürlichen Daten für die Anweisung kann die Anpassung von Anweisungen verbessern."</sample>
    <sample id="529">"Wir sehen hier, wie die Anzahl der Aufgaben zunimmt, je besser das Modell performt und gleichzeitig weniger empfindlich wird."</sample>
    <sample id="530">"Wir haben auch ein Experiment durchgeführt, bei dem wir eine Anweisung gegenüber fünf Anweisungen verglichen haben. Wie wir sehen können, verbessert sich die Leistung des Modells insgesamt und reduziert seine Empfindlichkeit erheblich, wenn man mehr Anweisungen verwendet."</sample>
    <sample id="531">"So zeigt sich der Einfluss von verschiedenen Tuning-Strategien auf die Empfindlichkeit des Modells. Wie wir sehen, kann das Modell durch das Transfer-Lernen von Naturanweisungs-Datensätzen eine viel bessere Empfindlichkeit erreichen als das ursprüngliche OFA-Modell."</sample>
    <sample id="532">"Wir können auch sehen, dass die Transferlernen aus einem natürlichen Datenbestand für OFA, um eine bessere Leistung auf einem natürlichen Datenbestand zu erzielen."</sample>
    <sample id="533">"Im Großskalenvollständig haben wir ein multi-modales Anweisungstuning-Datensatz vorgeschlagen. Wir haben die Schwellenfähigkeit von OFA signifikant verbessert und verschiedene Übertragungslernen-Techniken untersucht und ihre Vorteile gezeigt. Wir haben ein neues Maß namens Empfindlichkeit entworfen."</sample>
    <sample id="534">"Und eines mehr, wir sammeln ein viel größeres Datenbestand für die Anpassung von Mehrfach-Modell-Instruktionen mit etwa 150 weiteren Weiren-Sprachaufgaben und werden sie veröffentlichen. Dies ist ein QR-Code für unsere Daten und das Modell. Danke."</sample>
    <sample id="535">The authors belong to the University of Trento and Fondazione Bruno Kessler.</sample>
    <sample id="536">Jawad Hosseini.</sample>
    <sample id="562">Hallo, ich bin Kostav Sinha und ich freue mich, Sie zu unserem Vortrag über unser ACL 2023-Papier "Sprachmodell-Akzeptanz-Chalets sind nicht immer robust gegenüber Kontext" willkommen.</sample>
    <sample id="563">Es gibt ein gemeinsames Werk mit John Wothier, Aaron Mueller, Kanishka Mishra, Karen Fentus, Roger Levy und Adina William.</sample>
    <sample id="564">"Wiederholen wir in diesem Werk das Minimalpaar-Paradigma."</sample>
    <sample id="565">"Das minimale Paar-Paradigma bewertet Sprachmodelle anhand von Akzeptanzurteilen, die auch Grammatikalität wie z.B. Syntax von Blimp oder Akzeptanz in Bezug auf Stereotype wie Crowd Spares umfassen."</sample>
    <sample id="566">Und in diesem minimalen Paar-Paradigma wird die übliche Methode, um Sprachmodelle zu evaluieren, darin bestehen darin, dass man wie ein akzeptables Satz oder ein grammatikalisches Satz zeigt, und dann ein akzeptables Satz oder ein ungrammatisches Satz.</sample>
    <sample id="567">"Und dann hofft man, dass das Modell mehr Wahrscheinlichkeit auf den akzeptablen Sektor legt."</sample>
    <sample id="568">Die aktuelle MPP-Pipeline erlaubt es uns nicht, Modelle auf längere Sätze zu evaluieren.</sample>
    <sample id="569">"Derzeit entwickeln sich große Sprachmodelle zu immer längeren Kontextfenstern. Es ist deshalb entscheidend, dass wir die Akzeptanz der Modelle innerhalb des Kontextfensters bewerten."</sample>
    <sample id="570">Und das ist, was wir hier versuchen. Wir versuchen, den MPB-Pipeline erneut zu besuchen, indem wir dem Modell auffordern, die Akzeptanz auf längere als längere Sequenzen zu bewerten.</sample>
    <sample id="571">"Also ist das der Ansatz. Wir simulieren also diese längeren Sequenzen, indem wir die Daten selbst wieder besuchen und dann Sätze erneut erstellen, indem wir wie akzeptable oder unakzeptable Sätze aus diesen Daten auswählen."</sample>
    <sample id="572">"So zum Beispiel haben wir hier ausgewählt, wie ein typischer Paar von Daten von Blimp aus dem Datenbestand von Adjunct Island ausgewählt."</sample>
    <sample id="573">Und was wir tun, ist, dass wir längere Sequenzen nachbilden und die annehmen, die dieselbe grammatikalische Struktur haben, wie die aus AdjunTile extrahieren wir grammatikalische Sätze.</sample>
    <sample id="574">Und dann haben wir es als Präfix zu beiden akzeptablen und nicht akzeptablen Abfragen hinzugefügt.</sample>
    <sample id="575">Wir können das Gleiche tun, indem wir unannehmbare Sätze aus demselben Matching auswählen. Und das könnte auch verwendet werden, um das Modells Akzeptierbarkeit zu testen.</sample>
    <sample id="576">Und wir können dasselbe auch dadurch erreichen, indem wir Sätze aus einer anderen Teilmenge oder einem anderen Datensatz wählen. Deshalb nennen wir das den Mismatch-Szenario.</sample>
    <sample id="577">"Die Sätze kommen immer noch aus relevanten Datensätzen, aber nicht aus demselben Datensatz, den Sie bei der Bewertung verwenden. Und wir können das auch für unannehmbar machen."</sample>
    <sample id="578">Endlich können wir Sätze aus einem vollkommen unabhängigen Bereich wie Wikipedia auswählen.</sample>
    <sample id="579">Dies wird uns erzählen, ob die Akzeptabilitätsurteile der Modelle tatsächlich von irgendeinem Kontext beeinflusst werden.</sample>
    <sample id="580">Obwohl das Kontext von einem anderen Teil des Datensatzes kommt oder völlig irrelevant zu dem Satz ist, den wir betrachten.</sample>
    <sample id="581">"Also, wie funktioniert das Modell? Also, wir schauen uns die Wikipedia-Sätze an, die vollständig irrelevant zum aktuellen Suchpaar sind. Und dort finden wir heraus, dass die MPP-Bewertungen für beliebige Kontexte wie robust sind."</sample>
    <sample id="582">Wir haben die Kontextlänge auf 1024 ausgedehnt, um die OPT- und GPT-2-Modelle auszuschöpfen. Und wie man sieht, ist die MPP-Bewertung in der orangefarbenen Linie relativ stabil.</sample>
    <sample id="583">Wenn wir Sätze aus demselben Datensatz auswählen, was passiert dann?</sample>
    <sample id="584">Wir wählen oder erstellen Sätze aus akzeptablen und unakzeptablen Domänen aus demselben Blimp- oder Syntax-Gem-Datensatz.</sample>
    <sample id="585">Und wir sehen, dass die MPP-Judements entweder signifikant zunehmen oder abnehmen, wenn Sie entweder akzeptable oder unakzeptable Präfixe hinzufügen.</sample>
    <sample id="586">"Wenn wir die Struktur abgleichen, also die Sätze aus demselben Phänomen aus dem Text, in dem man die Person beschuldigt, Jim, wählen."</sample>
    <sample id="587">"Wir sehen einen enormen Anstieg oder einen enormen Abfall der MPP-Bewertung für das Modell, je nachdem, ob der ausgewählte Präfix akzeptabel oder unakzeptabel ist."</sample>
    <sample id="588">Jetzt das und das ist sehr groß, wie dies sich innerhalb des Kontextlängen auswirkt, und dies würde wahrscheinlich neue Sprachmodelle beeinflussen, die einen großen Kontextfenster haben.</sample>
    <sample id="589">"Wieso beeinflusst das Match-Prefix die Beurteilung des Sprachmodells so viel?"</sample>
    <sample id="590">"Wir haben eine Serie von Analysen durchgeführt, bei der wir versucht haben, den Eingabensatz so zu verändern, dass wir die relevante Struktur erhalten, aber Lärm hinzufügen. Nachdem wir mehrere dieser Störungen durchgeführt hatten,"</sample>
    <sample id="591">"Wir finden heraus, dass keiner dieser Geräusche tatsächlich den Kurs des Modells ändert, wie es uns die MPP-Bewertung zeigt."</sample>
    <sample id="592">"Wir finden, dass die Modelle empfindlich auf Störungen und Sätze in ähnlichen Weisen reagieren."</sample>
    <sample id="593">"Das bedeutet, wenn wir die Sätze im akzeptablen Bereich beeinträchtigen, sehen wir einen ähnlichen Anstieg bei allen Beeinträchtigungen. Und wenn wir die Sätze im unakzeptablen Bereich beeinträchtigen, sehen wir einen Abfall der MPP-Bewertungen in ähnlicher Weise."</sample>
    <sample id="594">Die Schlüssel-Erkenntnisse unserer Arbeit sind, dass Sprachmodelle an latenten syntaktischen und semantischen Merkmalen sensibel sind, die sich über die Sätze erstrecken.</sample>
    <sample id="595">"Und die MPP-Evaluation, die wir derzeit mit kurzen und einzelenen Eingaben durchführen, mag nicht vollständig das abstrakte Wissen von Sprachmodellen innerhalb des Kontextfensters erfassen."</sample>
    <sample id="596">Bitte lesen Sie unsere Studie für weitere Details unserer Experimente. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="597">Unordered multi-set of tokens that will appear in the output.</sample>
    <sample id="598">55,000</sample>
    <sample id="626">Die beste Ausrichtungsmethode für DEplain ist die Messalign-Methode.</sample>
    <sample id="627">Robustes Trainieren von neuronalen Netzen unter Label-Rauschen, um eine gute Allgemeinheit zu erzielen.</sample>
    <sample id="628">The documents in DEplain-web were aligned using both manual and automated alignment methods.</sample>
    <sample id="629">The Carnot++ dataset was created by collecting Reuters News from 2020 and annotating them with the same guidelines as the Carnot 2003 dataset.</sample>
    <sample id="630">Hallo alle, ich heiße Yusin Zhang von der Pennsylvania State University. Heute präsentiere ich unser Werk, "Crossland Ghosts and Money Parsing in Multiple Natural Languages and Mainline Representations".</sample>
    <sample id="631">"Sémantische Verarbeitung ist eine Aufgabe, um semantische Repräsentationen von Benutzeranfragen wie SQL und Lambda-Kalikul zu erstellen."</sample>
    <sample id="632">"Das Übersetzen von Anfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsrepräsentationen ist die Aufgabe der cross-lingualen semantischen Parsierung."</sample>
    <sample id="633">Wie in diesem Bild gezeigt wird, müssen wir den Query in mehreren natürlichen Sprachen mittels neuronalen Modellen in SQL, Lambda oder FunQL und dgl. übersetzen.</sample>
    <sample id="634">"Bestehende modelliert-kreuzlinguale semantische Parsenmodelle wurden separat vorgeschlagen und auf Datenmengen von begrenzten Aufgaben und Anwendungen evaluiert, zum Beispiel"</sample>
    <sample id="635">"Es gibt Lücken in der Deckung bestimmter natürlicher Sprache. Das Chinesische fehlt."</sample>
    <sample id="636">"Wegen Abdeckung bestimmter vielen Darstellungen."</sample>
    <sample id="637">"Die Lambda-Kalkül fehlt."</sample>
    <sample id="638">"Sind sie nur an bestimmten neueren Modellen ausgewertet. Zum Beispiel gibt es nur ein einziges Modell, um das Modell zu beurteilen."</sample>
    <sample id="639">"Wir schlagen vor, ein Beispielbeispiel für das Überprüfen von semantischen Abhängigkeiten in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen. Wir bieten ein einheitliches Datenbeispiel für die Überprüfung."</sample>
    <sample id="640">Es enthält 90 Datensätze in Virus-Domänen, 570 Teile in Toxinen, 80 Millionen Repräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien.</sample>
    <sample id="641">Und um unsere Benchmark besser zu evaluieren, betrachten wir sechs Einstellungen für Training und Bewertung.</sample>
    <sample id="642">Die erste Übung ist ein Übersetzungstest. Wir verwenden den Google-Übersetzungsservice, um die Quelltext in die ZielSprache zu übersetzen, und verwenden dann ein monolinguales Modell, um die Bewertung zu trainieren.</sample>
    <sample id="643">Und wir trainieren den englischen Modell auf Englisch-Anfrage. Während der Vorhersage, übersetzen wir die deutsche Anfrage mithilfe der API in Englisch und verwenden dann das trainierte Modell, um die SQL vorherzusagen.</sample>
    <sample id="644">"Wir testen auch ein einlinguales Modell."</sample>
    <sample id="645">I cannot provide the translation of the text as it is not provided. Please provide the English text you would like me to translate.</sample>
    <sample id="646">Wir testen auch eine monolingual-Field-Setting, indem wir ein Modell trainieren und ein Modell mit nur 10% der Trainingsdaten.</sample>
    <sample id="647">"Wir haben ein Modell, das mehrere Sprachen beherrscht und ein einziges Modell für alle Sprachen trainieren."</sample>
    <sample id="648">Beispiel: Wir fügen die deutschen, englischen und chinesischen Abfragen zusammen, um ein mehrsprachiges Modell zu trainieren. Und während der Vorhersage können wir dieses Modell verwenden, um die Abfragen zu übersetzen.</sample>
    <sample id="649">Ich kann deinen englischen Text in Deutsch übersetzen. Bitte gib den Text ein!</sample>
    <sample id="650">Und wir betrachten auch die Überblendung von Code-0 und Feld-Übertragung. Wir trainieren auf einer Quellensprache und übertragen auf eine Ziel-Sprache.</sample>
    <sample id="651">"Während des Trainings trainieren wir unsere englischen oder die Kombination von englischen und deutschen Fuscheout-Anfragen, um ein mehrsprachiges Modell zu trainieren und die SQL-Ausgabe vorherzusagen."</sample>
    <sample id="652">"Wir finden auch viele interessante Ergebnisse. Wir bewerten die Analyse von monolingual-Modellen anhand zweier Gruppen von Modellen."</sample>
    <sample id="653">"Einschließlich Encoder-PDR, das für multilinguale vorgeprägte Encoder mit pointerbasierten Dekodern wie XL1R plus PDR und Berth plus PDR."</sample>
    <sample id="654">Und wir bewerten auch Encoder-Decoder-Modelle, nämlich multilinguale trainierte Encoder-Decoder-Modelle, wie M-BART und MT5.</sample>
    <sample id="655">Wir fanden heraus, dass Encoder-Decoder die beste Leistung auf allen neun Datensätzen erzielt.</sample>
    <sample id="656">Wir bewerten auf MT5 und Beispiel XLMR plus PDR in multilingualer Einstellung.</sample>
    <sample id="657">"Wir fanden heraus, dass Encoder-Decoder oder Encoder-PDR durch das Training in einer Mischung von verschiedenen Sprachen verbessert werden kann."</sample>
    <sample id="658">"Und wir fanden heraus, dass die meisten der größeren natürlichen Sprachen Leistungssteigerungen erzielen können, außer dass die Leistung in sieben Datensätzen abfällt und nur in drei Datensätzen ansteigt."</sample>
    <sample id="659">Ich denke, das wird als Fluch der Vielsprachigkeit bekannt sein.</sample>
    <sample id="660">Wir vergleichen auch den Leistungsunterschied zwischen den Sprachen.</sample>
    <sample id="661">In diesem Diagramm ist die blaue Linie die Übertragung von Feldschüssen auf mehrere Sprachen. Die orangefarbene Linie ist die Übertragung von Feldschüssen auf mehrere Sprachen ohne Vorwissen. Die grüne Linie ist die Einstellung des Modells.</sample>
    <sample id="662">Wir fanden heraus, dass bei der Vergleichsanalyse der grünen und orangefarbenen Linie festgestellt wurde, dass der Leistungsunterschied im Zieltransfer bei Null-Shot-Einstellung signifikant ist. Und bei der Vergleichsanalyse der blauen und orangefarbenen Linie fanden wir heraus, dass der Transfer-Unterschied bei wenigen Schüssen rapide abnimmt.</sample>
    <sample id="663">Wir fanden auch einige andere interessante Ergebnisse. Zum Beispiel konnten Encoder-Decoder-Modellleistungsentwicklung vergleichbare Ergebnisse erzielen, insbesondere auf Englisch, und erzielten signifikante Leistungssteigerungen bei zukünftigen Zielsprachen.</sample>
    <sample id="664">Wir fanden Modellierungsziele wie Code-Modelle wie Blue immer noch im Grid für Übersetzungssemantik-Aufgaben für mehrere Sprachen.</sample>
    <sample id="665">Zusammenfassend bauen wir ExamPolar, ein einheitliches Benchmark für die semantische Analyse von Texten in verschiedenen Naturwissenschaften und verschiedenen Repräsentationen.</sample>
    <sample id="666">Wir führen eine umfassende Benchmark-Studie zu drei repräsentativen Arten von multilingualen Sprachmodellen durch. Und unsere Ergebnisse zeigen viele interessante Erkenntnisse, und wir bitten Sie, unser Paper und unser Code zu besuchen. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="667">Existing works can be broadly classified into four categories:</sample>
    <sample id="668">No, multilingual LLMs like Codex or Bloom are not sufficient for CLSP (Cross-Lingual Semantic Parsing) tasks, as they still rely on grid-based language models and do not fully utilize the power of natural language processing.</sample>
    <sample id="695">The method addresses the ambiguity of multiple permutations by inducing the alignment as part of the training process.</sample>
    <sample id="696">According to the speaker, fairness in a fine-tuned NLP model is defined as the absence of political bias, particularly the prevention of hate speech and misinformation targeting minority groups.</sample>
    <sample id="697">Yannis Lavraque.</sample>
    <sample id="698">Kostav Sinha.</sample>
    <sample id="699">Myra</sample>
    <sample id="700">In this context, tropicalism refers to a cultural trope that associates certain characteristics, such as vibrancy and curvaceousness, with Latina women, implying a connection to the tropical regions where they may be from.</sample>
    <sample id="701">The authors created the descriptions of the target groups by analyzing the words used to describe them, specifically focusing on words like "culture", "tradition", "proud", and "exotic", which define these groups by their relationship to their identity and distinguish them from the white norm.</sample>
    <sample id="702">In dieser Arbeit wurde das CXMI (Contextualized Mutual Information) zur Messung der Kontextnutzung verwendet.</sample>
    <sample id="703">DrBERT and SchuBERT differ in their training data: DrBERT is trained on 7GB of natural language data, while SchuBERT is trained on 4GB of natural language data and 4GB of clinical notes.</sample>
    <sample id="751">2</sample>
    <sample id="752">Iteratives Transferlernen ist ein Vorgang, bei dem ein Modell nach jeder Runde aktiven Lernens und Annotationen auf den neuesten Datensatz trainiert wird.</sample>
    <sample id="753">The goal of the dataset appears to be to analyze the repetition of a phrase in Persian, likely for linguistic or phonetic research purposes.</sample>
    <sample id="754">Based on the text, it does not seem like the author is discussing how an attacker can extract model parameters from a model, but rather how they validated the covertness of an embedding on a dataset. Therefore, I cannot provide an answer to this question based on the given text.</sample>
    <sample id="755">There are 3 authors involved in the work.</sample>
    <sample id="756">2</sample>
    <sample id="757">The authors belong to the University of Washington and Carnegie Mellon University.</sample>
    <sample id="758">Das Beispiel ist "I saw Bart and Lisa".</sample>
    <sample id="759">Die Technik für Dialogsysteme, wie ABC eval, ermöglicht die Messung von Fehlern, die Chat-Modelle bei der Bearbeitung bestimmter Themen begehen.</sample>
    <sample id="760">We must evaluate the models' acceptability throughout the context window because large language models are generating longer and longer context windows.</sample>
    <sample id="761">Yes.</sample>
    <sample id="762">Ja.</sample>
    <sample id="763">The speaker mentions that "It's the examples that carry most of the weight", implying that metrics related to examples were used for evaluation.</sample>
    <sample id="764">Nein.</sample>
    <sample id="765">Design bias is important in NLP because it can lead to inaccurate results, such as Perspective API incorrectly identifying non-toxic comments as toxic, which can have serious consequences in applications like content moderation.</sample>
    <sample id="766">The multilingual LLMs like BLOOM were fine-tuned on top of a pre-trained model and then adapted using adapters.</sample>
    <sample id="767">The model used for transfer learning is not specified in the given text.</sample>
    <sample id="768">The actual form of the printing doesn't have a big influence in the case of serial short printing.</sample>
    <sample id="769">3</sample>
    <sample id="770">The paper does not provide a direct answer to this question.</sample>
    <sample id="771">Xu Heng.</sample>
    <sample id="772">Ja, die Ergebnisse und der Datensatz der Studie werden als Benchmark vorgeschlagen.</sample>
    <sample id="773">We experiment with multiple smaller models.</sample>
    <sample id="774">OFA</sample>
    <sample id="833">Google Translate.</sample>
    <sample id="834">Stony Brook University.</sample>
    <sample id="835">The study examined the following language pairs: (not specified).</sample>
    <sample id="836">Xiangbin.</sample>
    <sample id="837">Die Modelle, die während der Experimente untersucht wurden, sind das Modell für langsame Übersetzungen (long impart) und das normale Basis-Modell (normal base impart).</sample>
    <sample id="838">53 tasks are used for training and 27 tasks are used for testing.</sample>
    <sample id="839">There is only 1 author, Regina Stotten, mentioned in the given text.</sample>
    <sample id="840">The authors experimented on four datasets: AG News, Mind, SSD2, and AresVam.</sample>
    <sample id="876">NACCHOS is a dataset of medical ground data from the web.</sample>
    <sample id="877">Aydbilar.</sample>
    <sample id="878">Die Prompt-Strategie hat einen großen Einfluss auf die Ergebnisse von LLMs bei der Übersetzung.</sample>
    <sample id="879">The authors belong to the University of California, Berkeley.</sample>
    <sample id="880">There are no instructions provided in the given text.</sample>
    <sample id="881">The authors propose evaluating the dataset with human study departments and established coreference resolution models.</sample>
    <sample id="882">Hallo, alle. Ich heiße Aydbilar, und ich werde Ihnen einen kurzen Überblick über das Papier "Grunting Pattern from Translation, Assessing Strategies and Performance" geben. Dies ist ein gemeinsames Werk meiner Kollegen und mich von Google Translate.</sample>
    <sample id="883">"BAM ist ein 540 Milliarden Parameter-zu-Sprachmodell, das vor einem Jahr 2022 vorgestellt wurde. Es wurde auf einer großen Sammlung von Texten trainiert, die 180 Milliarden Dokumente umfassen."</sample>
    <sample id="884">"In der Damaa für das Küchen erreicht es den Stand der Kunst in hunderten von NLP-Aufgaben."</sample>
    <sample id="885">"Wir präsentieren in dieser Arbeit die erste systematische Studie zur großvolumigen Sprachmodell-Steuerung für die maschinelle Übersetzung."</sample>
    <sample id="886">Wir haben die Übergabefähigkeit solcher Modelle mithilfe der besten Praktiken der MT-Gemeinschaft bewertet. Dazu wurde die neuesten Testsets verwendet, um ein neues Training mit den Trainingsdaten des Sprachmodells zu vermeiden.</sample>
    <sample id="887">"Wir vergleichen zwei State-of-the-Art-Systeme. Die besten leistungsstarken Systeme sind die WMT-Evaluierung."</sample>
    <sample id="888">Wir verwenden fortschrittliche Metriken für neuronale MT und zeigen auch Ergebnisse der Experten-basierten Bewertung von Menschen. Schließlich bieten wir Empfehlungen für Strategien für die Auswahl von Anfragen.</sample>
    <sample id="889">Die Anleitung hat einen großen Einfluss auf die Leistung von LLMs bei der Übersetzung. Wie wir in einem einfachen Experiment sehen können, bei dem wir eine einmalige Anleitung und zwei verschiedene Anleitungen für eine einfache Satz verwenden.</sample>
    <sample id="890">Die meisten Sätze, 516 von 1000, weisen eine Differenz von mehr als einem Blasenpunkt auf.</sample>
    <sample id="891">Und dies kann in extremen Fällen bis zu 40 Unschärfe-Punkten reichen. Es ist deshalb wichtig, eine gute Anreize-Strategie auszuwählen.</sample>
    <sample id="892">In unseren Experimenten verwenden wir eine Fünf-Schuss-Ermunterung-Strategie, bei der wir den Satz, den wir dem System bereitstellen, mit der Sprache markieren, in der es ist.</sample>
    <sample id="893">Übersetze den englischen Inhalt nach Deutsch.</sample>
    <sample id="894">Wir haben festgestellt, dass die tatsächliche Form der Druckform keine große Auswirkung hat, wenn es sich um kurze Serien-Drucke handelt.</sample>
    <sample id="895">Es ist entscheidend für Null- und Ein-Schuss-Anstoßen. Und wenn wir, wie bei unserem Fall, zu fünf-Schuss-Anstoßen gehen, gibt es fast keinen Unterschied in der Form des Anstoßens.</sample>
    <sample id="896">"Es sind die Beispiele, die den größten Teil des Gewichts tragen."</sample>
    <sample id="897">"Die Zusammenfassung unserer experimentellen Ergebnisse lautet, dass die Beispielergebnisqualität wichtiger ist als die Ähnlichkeit zum Ausgangssatz."</sample>
    <sample id="898">Es ist wichtig, Beispiele von hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die auswählenden Anregungen aus der Trainingsdaten der WMT-Evaluierungen oder des DEF-Daten.</sample>
    <sample id="899">Die Tiefendaten sind viel besser kuratiert und von höherer Qualität als die trainierten Daten, die lauter sind, und die Ergebnisse, daher besserer Leistung bei Verwendung von Tiefendaten.</sample>
    <sample id="900">Jetzt, zumindest, haben spezialisierte State-of-the-Art-Systeme einen substantiellen Vorteil gegenüber Bandübersetzungen. Aber eines kommt ziemlich nahe an ein kommerzielles System heran. In unserem Fall haben wir entschieden, mit Google Translate zusammenzuarbeiten.</sample>
    <sample id="901">Die Erkenntnisse, die wir aus der EMAIL-Regulierung gewannen, die wir mit dem MQM-Framework durchführten, sind, dass die Fluideität der Palme vergleichbar zu den state-of-the-art-Systemen ist, aber der Hauptunterschied kommt von der Genauigkeit.</sample>
    <sample id="902">"Insbesondere die am häufigsten auftretenden Fehler sind Auslassungsfehler."</sample>
    <sample id="903">Es scheint, dass Palm entscheidet, eine bessere klingende Übersetzung zu produzieren, indem sie manchmal Teile des ursprünglichen Satzes entfernt, die in der Übersetzung erstellt werden.</sample>
    <sample id="904">"Indessen ist die Kategorie für PAN-Stile niedriger als für die aktuellen Systeme, was ein Zusatzsignal ist."</sample>
    <sample id="905">"Parm erbringt sehr flüssigen Output, aber immer noch mit Problemen bei der Genauigkeit."</sample>
    <sample id="906">"Und das ist es für diese kurze Zusammenfassung. Für weitere Details, bitte besuchen Sie die vollständige Präsentation des Papier. Vielen Dank."</sample>
    <sample id="907">Hallo, ich bin Dawei, ein Doktorand an der Salant-Universität in Deutschland. In diesem Video möchte ich unsere jüngste Arbeit, "Wicker Als Du Denkst", vorstellen, eine kritische Betrachtung von wöchentlichen Lieferungen.</sample>
    <sample id="908">"Dies ist gemeinsame Arbeit mit Xiao Yuxian, Mario Smoothbath und Diaz Stefan und DTich Claco."</sample>
    <sample id="909">"Ich möchte mit einer kurzen Einführung in das Thema schwache Überwachung und schwach überwachtes Lernen beginnen."</sample>
    <sample id="910">"In schwacher Supervision werden wir das Datenmaterial nicht manuell beschriftet. Stattdessen verwenden wir schwache Beschriftungsquellen, wie einfache Heuristikkriterien, Wissensbasen oder Lokalitätssourcing, wie in dem auf dem rechten Bild dargestellt."</sample>
    <sample id="911">Im Vergleich zu menschlichen Anmerkungen sind die schwachen Anmerkungen viel günstiger, sind jedoch auch rauschig, was bedeutet, dass ein bestimmtes Maß der Anmerkungen falsch ist.</sample>
    <sample id="912">"Wenn wir neuronale Netze direkt auf wöchentlich beschrifteten Daten trainieren, tendieren sie dazu, das Label-Rauschen zu memorieren und generalisieren."</sample>
    <sample id="913">"In der wöchentlichen supervisierten Lernung werden Algorithmen vorgeschlagen, um neuronale Netze robust gegenüber solchen Stößen zu trainieren, so dass die Trainingsmodelle sich gut generalisieren."</sample>
    <sample id="914">In jüngsten Arbeiten in WSL, wobei WSL für wöchentliches supervisierte Lernen steht, ist ein häufiger Anspruch, dass Menschen behaupten, dass sie Modelle nur anhand von wöchentlichem Arbeitsdaten ausbilden und hohe Leistungen auf sauberen Testmengen erreichen.</sample>
    <sample id="915">"Technisch gesehen ist dieser Anspruch nicht falsch, aber es gibt ein Haken."</sample>
    <sample id="916">"Dass Menschen annehmen, dass es eine weitere saubere Validierungsmenge oder -schranke für die Modellauswahl gibt."</sample>
    <sample id="917">Wir haben bei diesem Problem aufgehört, als dies nahelegt, dass weitere manuelle Anmerkungen in der wöchentlichen Unterstützung erforderlich sind, wie viele annehmen. Aber, wie ein Elefant im Zimmer, wird diese Notwendigkeit oft übersehen.</sample>
    <sample id="918">"Die vorherige Frage bezieht sich auf die Bitte, drei Forschungsfragen zu stellen. Erstens: Ist saubere Validationsdaten für WSL erforderlich? Oder können wir vielleicht einen lauten Validierungsset stattdessen verwenden?"</sample>
    <sample id="919">Zweitens, wenn sauberes Datenmaterial erforderlich oder für das Funktionieren von WSL erforderlich ist, wie viele saubere Proben benötigen wir dann? Schließlich sollten wir nur die sauberen Proben für die Validierung verwenden oder gibt es bessere Möglichkeiten, sie zu nutzen?</sample>
    <sample id="920">Wir haben diese Forschungsfragen in unserem Werk bearbeitet und unsere Ergebnisse sind wie folgt.</sample>
    <sample id="921">"Zuerst finden wir heraus, dass kürzlich entwickelte WSL-Methode tatsächlich saubere, weite, flache Proben benötigen, um ordnungsgemäß zu funktionieren."</sample>
    <sample id="922">"Sind keine sauberen Validierungsproben verfügbar, fällt die Leistung ab. Wie in diesem Diagramm gezeigt, können die Trendsmodelle sich nicht über die ursprünglichen Wochenbezeichnungen hinaus generalisieren."</sample>
    <sample id="923">"Dass die Ausbildung sinnlos ist."</sample>
    <sample id="924">Dies weist darauf hin, dass WSL-Ansätze tatsächlich sauberes etikettiertes Datenmaterial benötigen, um ordnungsgemäß zu funktionieren, und der Kosten für die Erstellung von sauberen Validierungsbeispielen sollten nicht ignoriert werden.</sample>
    <sample id="925">Unser zweites Ergebnis ist, dass ein Anstieg der Anzahl sauberer Validierungsmuster dazu beiträgt, dass WSL-Ansätze besser abschneiden, wie in dem auf der linken Abbildung gezeigt.</sample>
    <sample id="926">"Typischerweise benötigen wir für eine Klasse 20 Proben, um eine hohe Leistung zu erzielen."</sample>
    <sample id="927">"Aber das ist nicht das Ende der Geschichte, weil wir, wenn wir auch auf saubere Proben zugreifen, direkt darauf trainieren, was sogar eine bessere Leistung erzielen wird."</sample>
    <sample id="928">Die rote Figur zeigt den Leistungsunterschied zwischen den Ansätzen der Feinabstimmung, die direkt auf den sauberen Daten angewendet werden, und den Ansätzen des WSL, die nur die sauberen Daten für die Validierung verwenden.</sample>
    <sample id="929">"Wir sehen, wenn wir 10 Proben pro Klasse haben, beginnt die Feinabstimmung von Direct besser als die Ansätze von WSL zu sein."</sample>
    <sample id="930">"Schließlich kann die in vorherigen WSL-Ansätzen beanspruchte Leistungsverbesserung leicht erreicht werden, indem man weiter auf den sauberen Validierungsmustern justiert."</sample>
    <sample id="931">"Wie wir aus den Zahlen sehen können, unterleidet das Valina-Modell FTW ursprünglich bei komplexeren WSL-Methoden wie Cosinus."</sample>
    <sample id="932">"Indessen, wenn wir das Fine-Tuning auf den sauberen Mustern weiter durchführen, erzielt FTW Ergebnisse, die genauso gut wie andere Methoden sind."</sample>
    <sample id="933">"In der Praxis gibt es keinen Grund, komplexere WSL-Methode zu wählen, die mehr Rechenzeit und Speicherplatz benötigen."</sample>
    <sample id="934">Zusammenfassend haben wir gezeigt, dass neuere WSL-Ansätze saubere, manuell annotierte Proben benötigen, um ordnungsgemäß zu funktionieren. Ihr Leistungsfortschritt und ihre Praktikabilität werden stark überschätzt.</sample>
    <sample id="935">Unsere konkreten Empfehlungen für zukünftige Arbeitsstunden folgen.</sample>
    <sample id="936">"Erst berichten Sie die Modellauswahlkriterien. Zum Beispiel berichten Sie, ob die Modellauswahl an reinen Validierungsmustern durchgeführt wurde."</sample>
    <sample id="937">Zweitens sollten WSL-Abstiege mit zukünftigen Landebaselines kombiniert werden, wenn beide auf klaren Mustern arbeiten. Drittens sollte eine stetige Feinabstimmung als einfache, jedoch starke Baseline in zukünftiger WSL-Arbeit berücksichtigt werden.</sample>
    <sample id="938">"Schließlich haben wir unseren Code freigegeben. Sie können ihn über den QR-Code auf dieser Folie finden. Bitte fühlen Sie sich frei, ihn auszuprobieren. Vielen Dank und genießen Sie die Konferenz."</sample>
    <sample id="939">Gängige Bewertungsmethoden für Dialogsysteme sind die Verwendung von menschlichen Bewertungen, wie zum Beispiel die Anfrage von Menschen, um zwei Konversationen auszuwählen, die besser ist, oder die Bewertung von Konversationen auf einer Likert-Skala.</sample>
    <sample id="940">5</sample>
    <sample id="941">Based on the example, the background knowledge required is knowledge of the English language.</sample>
    <sample id="942">Yes, the code is available on GitHub.</sample>
    <sample id="943">No, die Annotatoren sind nicht ausgewogen in Bezug auf eine demographische Gruppe, da sie sich auf Menschen mit höherer Bildung konzentrieren.</sample>
    <sample id="944">The sentences were modified by adding "noise" to the input while trying to preserve the relevant structure, in order to see if this would affect the language model's judgment of MPP.</sample>
    <sample id="945">Eine dimensionale Bewertung bedeutet eine Bewertung von verschiedenen Aspekten oder Dimensionen einer Sache, um ein umfassenderes Verständnis zu erlangen.</sample>
    <sample id="946">University of Science and Technology of China.</sample>
    <sample id="947">Die Form des Prompts ist wichtig in Fällen von null oder einem kurzen Prompt.</sample>
    <sample id="978">The authors evaluated conversational AI models.</sample>
    <sample id="979">1</sample>
    <sample id="980">Ein guter Planer sollte scripts schreiben, die vernünftig und treu an den Einschränkungen sind.</sample>
    <sample id="981">There is only one author mentioned, Si Yu-Yuan from Fudan University.</sample>
    <sample id="982">Vasudha</sample>
    <sample id="983">The authors do not explicitly mention the university they belong to.</sample>
    <sample id="1021">Die häufigsten Fehler sind Omissionen.</sample>
    <sample id="1022">"Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und heute werden wir Ihnen alles über ABCeVal, einen neuen dimensionalen Ansatz zur Bewertung von Conversational AI erzählen."</sample>
    <sample id="1023">Dieses Werk wurde von der Emory-NLP-Lab, geleitet von Professor Geno Choi an der Emory University, und in Zusammenarbeit mit Amazon Alexa AI erstellt.</sample>
    <sample id="1024">"Lass uns sagen, dass Sie ein Dialogmodell entwickelt haben und sehen möchten, wie gut es sich gegen den aktuellen Stand der Technik vergleicht."</sample>
    <sample id="1025">Die übliche Praxis ist, die Bewertung durch Menschen durchzuführen, wie zum Beispiel indem man Menschen bitten, zwischen zwei Gesprächen zu entscheiden, welches besser ist, oder Gespräche anhand einer Likert-Skala zu bewerten.</sample>
    <sample id="1026">Diese Ansätze funktionieren gut, um eine umfassende Beurteilung der Gesprächsqualität zu liefern, aber die Gesprächsqualität hat viele Aspekte. Deshalb möchten Sie möglicherweise mehrere Dimensionen der Chat-Qualität auswerten, um die Stärken und Schwächen des Modells auf einer feinere Ebene zu verstehen.</sample>
    <sample id="1027">Eine Möglichkeit besteht darin, einfach Menschen-Juristen zu bitten, verschiedene Aspekte der Dialogqualität zu bewerten, wie die Relevanz von Modellantworten mithilfe von Vergleichs- oder Likert-Skalen.</sample>
    <sample id="1028">"Wir glauben jedoch, dass es ein genauer und zuverlässigerer Ansatz für die Bewertung von dialogischen Dimensionen gibt."</sample>
    <sample id="1029">Unsere Vorgehensweise zielt darauf ab, die Subjektivität der menschlichen Bewertung zu reduzieren, indem wir explizit annotieren, ob jede Modellantwort bestimmte Verhaltensweisen ausdrückt, wie zum Beispiel die Ausgabe von irrelevanten Informationen oder sich selbst widersprechen.</sample>
    <sample id="1030">Wir nennen diesen Ansatz "Verhaltensmarkierung in Chat oder ABC-Evaluation" abkürzt. Wir haben diese Methode entwickelt, um die Chat-Modell-Verhaltensweisen umfassend abzudecken, die in jüngster Literatur als Einflussfaktoren auf die Chat-Qualität vorgeschlagen wurden.</sample>
    <sample id="1031">"ABC-Evaluation kann die Geschwindigkeiten messen, bei denen Chat-Modelle verschiedene thematische Fehler begehen."</sample>
    <sample id="1032">Beispiel: ABC bewertet die Anzahl der Schritte, in denen ein Chatmodell seinen Partner ignoriert oder irrelevantes sagt.</sample>
    <sample id="1033">" widerspricht sich selbst oder seinem Partner, halluziniert falsche Fakten oder verletzt alltägliche Kenntnisse und zeigt oder vermag Mitleid nicht."</sample>
    <sample id="1034">"Um festzustellen, welche Bewertung am wirksamsten ist, haben wir vier state-of-the-Art-Chatt-Modelle ausgewählt und sie anhand von 100 menschlichen Bot-Gesprächen pro Modell mithilfe von ABC-Eval bewertet."</sample>
    <sample id="1035">Für den Vergleich haben wir diese Gespräche auch mit drei bestehenden Methoden ausgewertet, alkoholbeurteilungen auf Ebene von Wendungen, alkoholbeurteilungen auf Ebene von Dialogen und Paarweisen-Vergleichen von Dialogen.</sample>
    <sample id="1036">Wir haben für jede der bestehenden Methoden Bewertungen zu acht der am häufigsten gemessenen Aspekte des Dialogs gesammelt, da dies die Standardpraxis ist, um Chat-Modelle auf mehreren Dimensionen zu bewerten.</sample>
    <sample id="1037">"Aus unseren Analysen dieser Bewertungsergebnisse haben wir festgestellt, dass ABC-Evaluierungsbezeichnungen insgesamt zuverlässiger sind als von bestehenden Methoden gesammelte Bezeichnungen, gemessen am Interanitator-Einverständnis in 100 doppelt beschrifteten Gesprächen."</sample>
    <sample id="1038">"Darüber hinaus sind ABC-Evaluierungsbezeichnungen besser vorhersagend für die Gesamtqualität des Gesprächs im Vergleich zu Metriken, die von bestehenden Methoden produziert werden, wie dies eine einfache lineare Regression-Analyse zeigt."</sample>
    <sample id="1039">Beispiel: Sie können sehen, wie die Messung des Anteils an Selbstwidersprüchen und Widersprüchen gegenüber dem Partner 5% und 10% des Gesprächsqualitäts erklärt, während die durchschnittlichen Alkoholkonsistenz-Werte nur 4% oder weniger erklären.</sample>
    <sample id="1040">"Schließlich haben wir überprüft, ob jeder Bewertungsparameter einen einzigartigen Aspekt der Chat-Qualität abdeckt, indem wir eine schrittweise lineare Regression durchführten."</sample>
    <sample id="1041">"Man kann sehen, wie die Kombination aller ABC-Evaluationsmetriken über 25% der Gesamtqualität eines Gesprächs erklärt. Und wenn man die Metriken nacheinander entfernt, verliert man bei fast allen eine beachtliche Menge an Informationen über die Qualität."</sample>
    <sample id="1042">"Im Gegensatz dazu erklären die kombinierten Likert-Metriken auf Ebene eines Durchlaufs nur sehr viel weniger des Qualitätsaspekts und weniger dieser Metriken tragen einzigartige Informationen."</sample>
    <sample id="1043">Diese zuverlässigen, informativen und eindeutigen ABC-Evaluationsmetriken ermöglichen es uns, den Konversations-IA mit einer höheren Auflösung zu beurteilen als vorherige Methoden dies erreichen konnten.</sample>
    <sample id="1044">"Sie können im Ergebnis unseres Experiments sehen, dass sich einige Herausforderungen immer noch darstellen und genauer quantifiziert wurden. Zum Beispiel haben die getesteten Bots in etwa 20% ihrer Antworten common sense-Violationen."</sample>
    <sample id="1045">Sie produzieren irrelevanten Inhalt in etwa 15% ihrer Antworten und widersprechen sich oder ihrem Partner etwa 10% der Zeit.</sample>
    <sample id="1046">"Mit dem raschen Tempo des Fortschritts in diesem Bereich könnten viele dieser Fehlerraten bei neu veröffentlichten Modellen seit unserer Bewertung sinken. Doch das ist umso mehr Grund, um zuverlässige und genaue Bewertungsmetriken für den Vergleich von Modellen zu verfolgen."</sample>
    <sample id="1047">"Wir hoffen, dass ABC-Eval von anderen in diesem Bereich genutzt werden kann, um einen bedeutenden Schritt in diese Richtung zu setzen. Und wir freuen uns darauf, wie sich der konversationale AI in den nächsten Monaten und Jahren entwickeln wird. Vielen Dank für das Zuschauen."</sample>
    <sample id="1048">Emory University.</sample>
    <sample id="1049">CFT stands for "Clean Validation".</sample>
    <sample id="1050">6</sample>
    <sample id="1051">"Hallo, ich heiße Kaio Yan und ich werde unser Werk mit dem Titel "Wenn Übersetzung einen Kontext benötigt? Eine datengetriebene multilinguale Exploration" vorstellen. Dieses Werk wurde in Zusammenarbeit mit Patrick Frenange, M.E. Liu, Andre F.D. Martin und Graham Mubig erstellt."</sample>
    <sample id="1052">"Viele Übersetzungen hängen von Kontext ab. Zum Beispiel, wie würden wir'more' in diesem Satz übersetzen?"</sample>
    <sample id="1053">"Wenn die vorherige Aussage war, dass die Minister herausfinden, dann könnten Dinge gefährlich werden, wenn Moe einen Spion meint. Aber wenn die vorherige Aussage war, könnte es etwas Ernstes sein, Doktor? Dann meint Moe eine Narbe."</sample>
    <sample id="1054">"Je nach Kontext ändert sich die Bedeutung des Wortes und daher auch seine Übersetzung."</sample>
    <sample id="1055">"Es ist jedoch schwierig, wie gut Modelle solche Fälle wie diesen bewerten können. Zunächst einmal, weil nur ein kleiner Teil der Übersetzungen von Kontext abhängt, was Korpus-basierte Metriken wie Blu-ray nicht fassen können."</sample>
    <sample id="1056">Und einige Leute haben vorgeschlagen, Zielbewertungen auf kontextabhängigen Übersetzungen, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Mengen von Sprachen, da sie meist auf Domänenwissen und menschliche Kuratierung angewiesen sind.</sample>
    <sample id="1057">"Wir versuchen in diesem Werk diese beiden Fragen zu beantworten. Zuerst: Wann erfordert Übersetzung Kontext? Und zweitens: Wie gut können Modelle diese Fälle handhaben?"</sample>
    <sample id="1058">Um die erste Frage zu beantworten, haben wir begonnen, zu messen, inwiefern ein Wort von seinem Kontext bei der Übersetzung abhängt.</sample>
    <sample id="1059">Und wir haben zuvor CXMI als Maß für die Kontextnutzung durch maschinelle Übersetzungsmodule vorgestellt. Und dies wird erreicht, indem wir messen, wie viel Information der Kontext C über das Ziel Y gibt, wenn wir den Quelltext X betrachten.</sample>
    <sample id="1060">Sie können CXMI als den Informationsgewinn bezeichnen, den man durch die Zuschreibung von Kontext an den Modell gewinnt.</sample>
    <sample id="1061">"In diesem Werk erweitern wir CXMI zu YCXMI, das kontextuelle Verwendungsmuster auf Satzebene oder auf Wortebene messen kann. Wir können Wörter als solche betrachten, die eine hohe P6MI haben, die für die Übersetzung Kontext benötigen."</sample>
    <sample id="1062">Wir analysieren jetzt Wörter mit hohem XMI, um Muster zwischen diesen Wörtern zu suchen.</sample>
    <sample id="1063">Und wir führen unsere Analyse auf Transkripten von TED-Vorträgen durch, die aus dem Englischen in 14 verschiedene Sprachen übersetzt wurden.</sample>
    <sample id="1064">Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Zunächst überprüfen wir die Teilwort-Tags, die hohe PCXMI-Werte aufweisen.</sample>
    <sample id="1065">Und das ermöglicht es uns, beispielsweise dualische Pronomen im Arabischen zu finden, die einen relativ hohen P6MI haben. Und dies kann erklärt werden, weil Englisch keine dualischen Pronomen hat, daher benötigt man Kontext, um zu bestimmen, ob ein Pronomen dualisch ist, wenn man es ins Arabische übersetzt.</sample>
    <sample id="1066">"Wir finden auch, dass bestimmte Sprachen Kontext benötigen, wenn wir die richtige Verbform auswählen möchten. Wir sehen uns dann an Vokabeln an, die eine hohe p-Sex-MI aufweisen, die über alle ihre verschiedenen Vorkommnisse gemittelt ist."</sample>
    <sample id="1067">Und dies hilft bei der Identifizierung von Fällen wie diesem, in denen in Chinesisch Kontext notwendig ist, um Eigennamen zu übersetzen, damit Sie im Dokument die gleiche Übersetzung verwenden.</sample>
    <sample id="1068">Und ähnlich finden wir heraus, dass Kontext unterstützt wird, um in der richtigen Formellität zu übersetzen.</sample>
    <sample id="1069">Und schließlich betrachten wir einzelne Token, die einen hohen P6MI aufweisen. Dies ermöglicht es uns, Phänomene zu identifizieren, die nicht direkt durch das Wort selbst erfasst werden können, sondern vielmehr in der Satzstruktur ausgedrückt sind, wie z.B. die Auflösung von Ellipsen.</sample>
    <sample id="1070">Jetzt verwenden wir unsere Erkenntnisse aus unserer Analyse, um ein Benchmark für die dokumentarische globale Übersetzung zu entwerfen.</sample>
    <sample id="1071">Für jede der fünf Diskursphänomene, die wir identifiziert haben, erstellen wir Tagger, um Wörter zu identifizieren, die dem Phänomen zugehören. Und wir nennen unseren Tagger den multilingualen Diskursbewussten oder Muda-Tagger.</sample>
    <sample id="1072">Wir können auch feststellen, dass verschiedene Sprachen unterschiedliche Anteile dieser diskursiven Phänomene aufweisen.</sample>
    <sample id="1073">Wir verwenden dann den MudaTaggle, indem wir den Taggle auf dem parallelen Korpus anwenden, das wir für die Bewertung verwenden möchten. Wir wenden unsere Übersetzungsmetriken auf die kontextabhängigen Beispiele an, die der MudaTaggle ermittelt hat.</sample>
    <sample id="1074">"Und schließlich verwenden wir unsere Benchmarks und andere Metriken, um verschiedene Modelle auf Dokumentenebene für die maschinelle Übersetzung zu bewerten."</sample>
    <sample id="1075">"Zunächst einmal, wenn wir korpusbasierte Metriken verwenden, finden wir bei Blau, dass kontextunabhängige Modelle die beste Leistung erzielen."</sample>
    <sample id="1076">Dann verwenden wir Comet, wenn kontextbewusste Modelle am besten abgeleistet. Und wenn wir den F-Measure verwenden, dann haben Modelle mit oder ohne Kontext vergleichbare Leistung.</sample>
    <sample id="1077">Dies zeigt erneut, dass es schwierig ist, das beste Dokumentenübersetzungssystem zu bestimmen, wenn man nur Korpus-Normen verwendet.</sample>
    <sample id="1078">Jetzt verwenden wir die Mooda-Benchmark, um Modelle zu bewerten, und finden heraus, dass Kontext-bewusste Modelle signifikant genauer sind als Modelle, die keine Kontextinformationen verwenden, für bestimmte diskursive Phänomene wie Formalität und lexikalische Kohäsion.</sample>
    <sample id="1079">"Aber diese Modelle sind nicht viel besser als Modelle, die keine Kontextinformationen bei Phänomenen wie Ellipsen, Perennien und Verbformen verwenden. Das suggeriert, dass wir für die Dokumentebearbeitung weitere Fortschritte benötigen."</sample>
    <sample id="1080">"Wir vergleichen auch verschiedene kommerzielle Systeme und unsere Benchmark zeigt, dass D-Bel in der Regel genauer ist als Google Übersetzer für Dokumentenübersetzungen."</sample>
    <sample id="1081">Zusammenfassend analysieren wir über 14 Sprachpaare, um festzustellen, wann Übersetzungen Kontext benötigen.</sample>
    <sample id="1082">Und dann verwenden wir unsere Ergebnisse, um ein Benchmark für Dokumentenübersetzung zu erstellen, das uns hilft, welche Disk-Kreuz-Phänomene-Modelle gut oder schlecht handhaben können, und welche Übersetzungssysteme gut bei der Dokumentenübersetzung sind.</sample>
    <sample id="1083">Danke für deine Aufmerksamkeit. Bis Toronto.</sample>
    <sample id="1084">Yusin Zhang.</sample>
    <sample id="1121">The name of the method is "Greedy Algorithm".</sample>
    <sample id="1122">Die Autoren beschreiben die Methode der "markierten Wörter" als einen Weg, um Wörter zu identifizieren, die Gruppen von markierten Wörtern von anderen unterscheiden.</sample>
    <sample id="1123">University of Washington.</sample>
    <sample id="1124">Prag</sample>
    <sample id="1125">James Finch and Sarah Finch.</sample>
    <sample id="1126">4</sample>
    <sample id="1127">Datensätze für die Minimal Pair Paradigm zum Testen syntaktischer Phänomene.</sample>
    <sample id="1161">Die Abkürzung der Forschungsfrage ist nicht explizit genannt, aber es wird von "WSL methods" gesprochen, also wahrscheinlich "Weakly Supervised Learning" Methoden. Es gibt keine spezifischen Abkürzungen für die Methoden genannt.</sample>
    <sample id="1162">The model is evaluated based on clinical and biomedical tests.</sample>
    <sample id="1226">CamemBERT was originally trained on a dataset of 4 GB of text data.</sample>
    <sample id="1227">Adam Szpirkowski.</sample>
    <sample id="1228">Die Ergebnisse des Experiments, bei dem Modelle mit neueren Daten neu trainiert wurden, zeigten, dass die Leistung abnimmt, wenn die zeitliche Lücke größer wird, was die Hypothese bestätigt, dass die zeitliche Verzögerung die Hauptursache für den Leistungsverlust ist.</sample>
    <sample id="1269">We need to permute the tokens because after the first step, they are not in the correct order, and we need to predict the correct permutation to put them in the right sequence.</sample>
    <sample id="1270">The authors suggest that model developers should make their bias mitigation methods more transparent because they want to know whether the positive stereotypes are caused by excessive value alignment or other anti-stereotyping methods.</sample>
    <sample id="1271">Ungrammatical sentences.</sample>
    <sample id="1272">Weight and token of Pomet Bird.</sample>
    <sample id="1273">Interanitator Agreement.</sample>
    <sample id="1274">Wikipedia.</sample>
    <sample id="1275">The authors likely belong to the University of Passau.</sample>
    <sample id="1276">MultiInstruct unterscheidet sich von anderen Benchmarks dadurch, dass es sich auf die multimodale Trainingsdaten und -tasks konzentriert, während andere Arbeiten sich auf die Sprach-only-Aufgaben konzentriert haben.</sample>
    <sample id="1277">2</sample>
    <sample id="1278">The definition of binary coordination is: "Measuring length in characters, with the first column in syllables, the middle column, and in words, the right column."</sample>
    <sample id="1279">3 seconds.</sample>
    <sample id="1280">The results indicate that smaller T5 models can generate scripts of higher quality when properly trained on suitable data sites.</sample>
    <sample id="1281">"Hallo, ich bin Yannis Lavraque und ich präsentiere unsere Arbeiten auf Dr. Berth, ein robustes Modell in Französisch für den Bereich Biome und Klinik."</sample>
    <sample id="1282">"Wir sprechen in dieser Präsentation über eine modellierte Sprache im Gesundheitswesen. Dann werden wir unsere Hauptbeiträge in diesem Artikel vorstellen."</sample>
    <sample id="1283">Wir haben den ersten biomedizinischen Modell in Französisch vorgestellt, genannt Dr. Berth, das auf Roberta basiert und auf NACCHOS basiert, einem Datensatz medizinischer Bodeninformationen aus dem Web.</sample>
    <sample id="1284">Wir haben auch eine Modellvergleichung mit verschiedenen Rückkopplungen und Datenquellen durchgeführt. Wir werden unsere Ergebnisse dann auf 11 medizinische und klinische Tests in Französisch präsentieren.</sample>
    <sample id="1285">Und wir schließen schließlich die Experimente ab und geben Ihnen weitere Informationen, wie Sie Zugriff auf die Modelle erhalten.</sample>
    <sample id="1286">Seit seiner Veröffentlichung 2018 war Bert die effektivste Vorgehensweise bei der natürlichen Sprachverarbeitung. Sie hat einen großen Leistungsfortschritt gegenüber historischen Strategien und kontextualisierten Methoden wie "See-Then-Plan" oder "See-Then-Act" erzielt.</sample>
    <sample id="1287">"Seitdem wurde diese Modelle an viele andere Sprachen wie Französisch mit Camembert, andere Branchen wie Biomedizin mit Père Medbert und Biobird und in der Klinik mit Klinik Albert angepasst, vor allem Englisch."</sample>
    <sample id="1288">"Spezialisierte Modelle für andere Sprachen sind selten und werden oft kontinuierlich trainiert, da anwendungsbezogene Daten fehlen."</sample>
    <sample id="1289">"Es gab jedoch bislang in Frankreich keine offene Quellen für die Biomedizin."</sample>
    <sample id="1290">Wir fragen uns, was die geeignetsten Datenstrukturen für eine breite Palette von Anwendungen sind. Und jene aktuellen Daten sind gute Substitution für klinische Daten.</sample>
    <sample id="1291">"Um diese Frage zu beantworten, vergleichen wir Dr. Bert mit unserem Schubert-Modell, das auf anonymisierten Daten basiert, die von unserem Haus aus dem nicht-generationalen Krankenhaus stammen."</sample>
    <sample id="1292">Nachdem wir das getan haben, fragen wir uns, wie viel Daten wir benötigen, um ein spezialisiertes Modell auf französischen Daten zu trainieren. Ist es Gb bei Gb oder mehr?</sample>
    <sample id="1293">Zum Beantworten dieser Frage trainieren wir vier Modellversionen von vorneherein. Eine erste Version von Dr. Bert mit 7 GB Naturdaten. Eine zweite Version mit 4 GB Naturdaten.</sample>
    <sample id="1294">Die erste Version von Schubert, die eine klinische Vorlage ist, hat 4 GB medizinische Notizen. Und die letzte Version von Schubert mit einem Mix aus 4 GB Natur und 4 GB medizinischen Notizen.</sample>
    <sample id="1295">"Zusätzlich zu dieser Vergleichsanalyse haben wir drei Modelle auf dem Kontinent vor der Ausbildung eingesetzt, um den Einfluss der Ausbildungstrategie zu analysieren."</sample>
    <sample id="1296">Eine Basis auf Camembert-Weight und ein Zug auf 4 GB Natur. Eine andere basiert auf Camembert, aber gezogen auf 4 GB Blinken und die andere.</sample>
    <sample id="1297">Und schließlich, basierend auf einem englischen bio-medizinischen Modell, Bermond Bert, und trainiert auf vier Gigabyte eines Satzes von Schnittstellen. Insgesamt haben wir sieben Modelle.</sample>
    <sample id="1298">Wir sammeln uns sechs Modelle, indem wir öffentliche und private Quellen sammeln, wie Nomenklaturen, Klassifizierungen, Reisen, Herausforderungen und Verantwortung.</sample>
    <sample id="1299">Dieser Modell wird mit Modellen verglichen, die 6,9 ähnlich sind, nämlich Camembert Oscar 138GB, Camembert Oscar 4GB, Camembert CCNet 4GB, Pummet Belt, BioBert und ClinicalBert.</sample>
    <sample id="1300">Die Entwicklung von Highlights, die am besten auf die Aufgabe abschneidet, wenn die Daten, auf denen das Modell trainiert wurde, demselben Natur wie die Daten sind.</sample>
    <sample id="1301">"Unabhängig davon, wo wir die Daten erhalten, sehen wir, dass Daten aus heterogenen Quellen mehr Vielseitigkeit aufweisen. Wir beobachten auch, dass die Verwendung von mehr Daten zu besserer Leistung führt."</sample>
    <sample id="1302">"Im Großen und Ganzen erzielte das Zurücksetzen von vornherein bei den meisten Aufgaben bessere Leistungen."</sample>
    <sample id="1303">"Unser Erfahrung in Konstrains und Vorhersagen, indem wir das Gewicht und den Token von Pomet Bird verwenden, gezogen auf ein Subjekt von 4 GB Natur, hat ein Ergebnis, das vergleichbar ist mit dem, das wir mit Dr. Bert erreicht haben, 4 GB von Grund auf."</sample>
    <sample id="1304">"Das gilt jedoch nicht für das Modell basierend auf Camembert-Weights und -Tokenizer, das an Stabilitätsproblemen leidet."</sample>
    <sample id="1305">"Zusammenfassend, bietet unser System eine bessere Leistung bei neun von elf downstream-Aufgaben und übertrifft die Ergebnisse des generischen Modells, Camembert, global."</sample>
    <sample id="1306">Wir haben auch festgestellt, dass spezialisiertes Datenmaterial besser ist, mehr spezialisiertes Datenmaterial ist besser, aber es skaliert nicht gut.</sample>
    <sample id="1307">"Alle vorgefertigten Modelle, die von NATURES stammen, sind frei verfügbar auf UginFace und alle Trainings-Skripte sind auf unserem GitHub-Repository verfügbar."</sample>
    <sample id="1308">Danke für diese Präsentation. Wir freuen uns auf die Aktion im Poster-Sessions in Toronto.</sample>
    <sample id="1309">The learning strategies being examined in this context are training and comparing four models from scratch.</sample>
    <sample id="1310">The factor of overfitting is greater than 1, indicating that there is no adaptive overfitting.</sample>
    <sample id="1311">The quality of simplification was not explicitly mentioned in the given text.</sample>
    <sample id="1312">Ja, Sprachmodelle haben unterschiedliche politische Bedeutungen und Vorurteile.</sample>
    <sample id="1313">"Hallo, ich heiße Mathias Landemann und heute gebe ich Ihnen einen kurzen Einblick in unser Papier über kompositionelle Generalisierung ohne Bäume mit Multi-Set-Tagging und latenten Permutationen."</sample>
    <sample id="1314">Dies ist gemeinsame Arbeit mit meinen Beratern, Alexander Kodler und Yvon Titov.</sample>
    <sample id="1315">"Kompositionelle Generalisierung kann als die Fähigkeit eines Lernenden verstanden werden, tiefere Rekursion und unerforschte Kompositionen von Phrasen zu verarbeiten, die einzeln während des Trainings gesehen wurden."</sample>
    <sample id="1316">Im Kontext der semantischen Analyse könnte das Testen von kompositioneller Generalisierung wie folgt aussehen. Wir haben wie gewohnt ein Trainingskorpus von Aussagen, in diesem Fall "Die Mädchen schlief" und "Mary wusste, dass das Mädchen schlief".</sample>
    <sample id="1317">Diese Aussagen werden mit logischen Formen paarweise dargestellt, die die grundlegenden Aspekte ihrer Bedeutung repräsentieren.</sample>
    <sample id="1318">Im Gegensatz zur standardmäßigen Auswertung von Maschinellem Lernen stammt das Testset nicht aus derselben Verteilung, sondern enthält strukturell unsynthetische Formen.</sample>
    <sample id="1319">"In diesem Beispiel hat das Modell während des Trainings tieferen Rekursionen gesehen und wird an einem Beispiel getestet, das eine tiefergehende Rekursion aufweist."</sample>
    <sample id="1320">"Einfache sequenzen-sequenz-Modelle haben Schwierigkeiten mit dieser Art von Auswahlen-Auswertung und produzieren oft Ausgaben, die von den Eingaben abgetrennt sind."</sample>
    <sample id="1321">"Sie erfüllen insbesondere oft nicht die systematischen Entsprechungen zwischen Eingabe und Ausgabe, wie sie in dem Beispiel farbmarkiert sind."</sample>
    <sample id="1322">Ein populäres Verfahren, um dies zu lösen, besteht darin, Bäume in die Modelle zu integrieren.</sample>
    <sample id="1323">Die Bäume sind dazu bestimmt, den Prozess der Komposition zu erfassen, der Aussprüche mit logischen Formen verknüpft.</sample>
    <sample id="1324">Dies funktioniert gut, aber Bäume werden normalerweise nicht gegeben und müssen irgendwie erworben werden.</sample>
    <sample id="1325">Dies kann komplex und manchmal rechnerintensiv sein. Typischerweise ist dies ein formalism-spezifischer Vorgang, der die logischen Formen vorverarbeitet, zum Beispiel, um Variablen-Symbole zu verarbeiten.</sample>
    <sample id="1326">"Erhält man Bäume, kann dies auch spezielle Grammatik-Induktionsverfahren beinhalten."</sample>
    <sample id="1327">"In diesem Papier verwenden wir keine Bäume und einführen ein neuronales Modell für die Sequenz-zu-Sequenz, das die Korrespondenzen zwischen Fragmenten des Eingangs und Fragmenten des Ausgangs direkt modelliert."</sample>
    <sample id="1328">"Zum ersten Mal zeigen wir eine starke Generalisierung auf tiefe Rekursionen ohne Abhängigkeit von Bäumen."</sample>
    <sample id="1329">Unsere Vorgehensweise vorherzusagen den Ausgang aus dem Eingabe in zwei Schritten.</sample>
    <sample id="1330">Zuerst kennzeichnen wir jedes Eingabetoken mit einem unsortierten Mehrfachmengensatz von Token, die im Ausgangsort erscheinen.</sample>
    <sample id="1331">Nach dem ersten Schritt haben wir alle richtigen Token, aber sie sind nicht geordnet.</sample>
    <sample id="1332">"Deshalb verwenden wir im zweiten Schritt ein weiteres Modell, um eine Permutation vorherzusagen, um sie in die richtige Reihenfolge zu bringen."</sample>
    <sample id="1333">Wir stellen ein neues Verfahren vor, um eine Permutation vorherzusagen, das keine harten Einschränkungen an die möglichen Permutationen stellt. Dies macht unsere Ansatz sehr flexibel und ausdrucksstark.</sample>
    <sample id="1334">Konzeptionell funktioniert unser Permutationsmodell etwa wie folgt.</sample>
    <sample id="1335">Wir gehen von links nach rechts über den Ausgang und bestimmen, welches Multiset-Token wir in jede Position einsetzen. Bei der ersten Ausgabestelle wählen wir das Highlight-Element wie in Rot markiert.</sample>
    <sample id="1336">Dann springen wir zum nächsten Mehrfach-Token, um den zweiten Token im Ausgang zu bestimmen.</sample>
    <sample id="1337">Wir bestimmen den dritten Token in der Ausgabe ähnlich wie wir den ersten Token. Wir springen zu einem anderen Mehrfach-Token und fahren fort.</sample>
    <sample id="1338">"Bis alle Token der ersten Stufe genau einmal besucht wurden."</sample>
    <sample id="1339">Hier ist die Übersetzung:

"Um Ihnen einen Vorgeschmack von den experimentellen Ergebnissen zu geben, vergleichen wir unsere Methode mit anderen baumlosen Modellen auf dem Benchmark von CONG. Unsere Methode übertrifft die anderen um einen großen Abstand bei der Generalisierung auf tiefere Rekursion."</sample>
    <sample id="1340">Einige andere Arten von strukturierter Allgemeinheit bleiben jedoch sehr herausfordernd.</sample>
    <sample id="1341">In unserem Papier lösen wir ein paar interessante technische Herausforderungen.</sample>
    <sample id="1342">Zuerst einmal ist die Ausrichtung zwischen Eingabe und Ausgabe in den Trainingsdaten nicht gegeben. Als Folge davon wissen wir für einen bestimmten Token nicht, welches Multicell er stammt, was bei der Ausbildung eine Herausforderung darstellt.</sample>
    <sample id="1343">Zusätzlich gibt es manchmal mehrere Permutationen, die den Daten entsprechen, aber die linguistisch korrekte eine latent ist. Wir behandeln dies, indem wir die Ausrichtung als Teil der Ausbildung einbeziehen.</sample>
    <sample id="1344">Unsere Permutationsmethode ist sehr flexibel, aber sie bringt das Problem mit sich, dass das Auffinden der höchstbewerteten Permutation NP-schwer ist. Das ist, weil es sich mit dem Reiseverkäuferproblem verbunden ist.</sample>
    <sample id="1345">Wir approximieren dies mit einer GPU-gerechten kontinuierlichen Entspannung, die es uns auch ermöglicht, die Lösung rückwärts zu propagieren und die sprachlich plausibleren Permutationen zu lernen.</sample>
    <sample id="1346">Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen angehen, sehen Sie bitte unsere Studie oder besuchen Sie unser Postamt.</sample>
    <sample id="1347">Cognitive dissonance is the inconsistency between two beliefs or actions.</sample>
    <sample id="1348">GPT-4.</sample>
    <sample id="1349">Ja, die Forschung zeigt, dass kumulatives Training in Bezug auf iterative Strategien besser ist.</sample>
    <sample id="1350">Sara Pappi</sample>
    <sample id="1351">The data for the MuDa-Benchmark comes from transcripts of TED Talks translated from English to 14 different languages.</sample>
    <sample id="1385">Mathias Landemann.</sample>
    <sample id="1386">Cross-lingual zero-shot and field-short transfer refers to training a model on one source language and transferring it to another language to predict SQL output.</sample>
    <sample id="1387">Salant University</sample>
    <sample id="1388">The authors use two latency measurements: translation quality and average lagging.</sample>
    <sample id="1389">"Hallo alle, ich bin Akshita und heute präsentiere ich gemeinsam mit meinem Co-Autor Martin unsere Arbeit, den KITMAS-Test, der die Integration von Wissen aus mehreren Quellen evaluiert. Dieses Werk ist eine Zusammenarbeit zwischen der McGill-Universität, Miele und Microsoft-Forschung."</sample>
    <sample id="1390">"Nationale Sprachverstehungsmodelle beruhen auf einer Vielzahl von Kenntnisquellen, wie zum Beispiel auf dem in ihren Parametern enthaltenen Wissen, das durch Vorabtrainieren erworben wurde, und auf dem im Echtzeit-Eingabewissen."</sample>
    <sample id="1391">"Neuere Arbeiten in Aufgaben wie Fragenbeantwortung zeigen, dass Modelle vortrainierte Zeitwissen verwenden können, um die Aufgabe zu lösen."</sample>
    <sample id="1392">"Aber die natürliche Sprachverständigung erfordert oft Kenntnisse, die auch während der Inferenzzeit bereitgestellt werden."</sample>
    <sample id="1393">Beispiel: John sah den neu gewählten Präsidenten im Fernsehen.</sample>
    <sample id="1394">Vortrainierte Parameter können Informationen über das, was Präsidenten tun, und über was ein Fernseher ist, enthalten, aber sie können nicht zuverlässig wissen, wer dieser spezifische Einzel-Entity John ist oder wer der neue Präsident ist, weil der Präsident möglicherweise seit dem Vortrainieren gewechselt hat.</sample>
    <sample id="1395">"Deswegen benötigen erfolgreiche Modelle für kognitionsintensive NLU-Aufgaben die Fähigkeit, beides zu integrieren und zu nutzen: vorher trainierte Zeit und zeitnahes Wissen."</sample>
    <sample id="1396">"Wir schlagen ein Diagnose-Test-Suite für die Integration von Wissen vor."</sample>
    <sample id="1397">"Wir stellen ein Referenzierungsaufgabenstellung vor, die die Fähigkeit testet, auf Informationen aus verschiedenen Quellen zuzugreifen. Wir evaluieren das Dataset mit humanen Studienabteilungen und etablierten Referenzierungsmustern."</sample>
    <sample id="1398">Hier ist ein Beispiel aus unserem Datensatz. Serving ist ein Richter. Hier ist ein Bäcker. Serving und Kya trafen sich im Park. Nach einem langen Tag am Gericht, bei dem er sich mit Fällen beschäftigt hatte, war er froh, sich zu entspannen.</sample>
    <sample id="1399">Die Aufgabe besteht darin, die richtige Entität zu identifizieren, auf die sich das Pronom "er" bezieht, was in diesem Fall 7 ist.</sample>
    <sample id="1400">Die Auflösung eines gegebenen Pronomens erfordert zwei Arten von Informationen. Erstens Entitätsspezifisches Wissen wie "Diener ist ein Diener". Und zweitens Hintergrundwissen wie "Richter entscheiden Fälle in Gerichtshöfen".</sample>
    <sample id="1401">"Häufig wird allgemeine Vorwissen während des Vortrainings großer Sprachmodelle erworben, während spezifisches Wissen über Entitäten typischerweise während der Inferenzzeit beobachtet wird."</sample>
    <sample id="1402">Wir variieren die Verfügbarkeit dieser beiden Informationen, so dass sie entweder in einem einzigen Quellen oder in mehreren Quellen gefunden werden kann.</sample>
    <sample id="1403">Wir haben drei Einstellungen von KITMOS definiert. Zunächst müssen wir die Einstellung vorführen. Die Hintergrundwissen wird vorausgesetzt, wenn es zur Vorverarbeitung verfügbar ist.</sample>
    <sample id="1404">Zweitens gibt es einen Hintergrund, der sowohl vor der Ausbildung als auch während der Ausbildung verfügbar ist. Zuletzt das Hintergrund-Einfluss-Einstellung. Mit beiden Kenntypes nur während der Ausbildung verfügbar.</sample>
    <sample id="1405">Dieses letzte Setting ist insbesondere interessant, da es den Fall simuliert, in dem die notwendige Hintergrundwissen, um eine Aufgabe zu lösen, nicht Teil der Vortrainingsdaten von Modellen ist, zum Beispiel weil sich neue Berufe entwickelt haben, seitdem die Vortrainingsdaten erstellt wurden.</sample>
    <sample id="1406">Hier ist ein Beispiel dafür, wie wir den Zugang zu Fakten in einer sicheren Quelle steuern.</sample>
    <sample id="1407">Im Hintergrund vorgeprägten Setting gehen wir davon aus, dass die Hintergrundwissen, die Politiker angesichts von Wahlmandaten in Regierungen enthalten ist, in den vorgeprägten Parametern enthalten ist. Im selteneren Zeitkontext geben wir das spezifische Wissen an, dass Chester ein Politiker ist.</sample>
    <sample id="1408">"Im Back-Run-Modus bieten wir im Hintergrund nicht nur anti-spezifische, sondern auch Informationen über Politiker im Einfluss-Unterbereich."</sample>
    <sample id="1409">"Im Hintergrund in einem freien Setting bieten wir eine effiziente Besetzung, anstatt Politiker, weil Meriture unwahrscheinlich in der vortrainierten Periode enthalten ist."</sample>
    <sample id="1410">Wir bewerten das Dataset sowohl mit menschlichen Studienteilnehmern als auch mit Modellen für die Koordinierung von Referenzen. In diesem Bild zeigen wir die Ergebnisse der besten Modelle im schwierigsten Varianten des vortrainierten Szenarios.</sample>
    <sample id="1411">"Mit unserem task-spezifischen Training auf KITMOS funktionieren beide Monitore nicht gut. Trainiert man jedoch auf KITMOS, erzielen sowohl C2F als auch BFQF signifikant bessere Ergebnisse als eine zufällige Auswahl."</sample>
    <sample id="1412">Dies suggeriert, dass Modelle, wenn sie auf allgemeinen Anforderungen mit Lushen-Daten trainiert werden, lernen, Oberflächensignale auszunutzen, die bei der Überprüfung auf KITMOS, wo solche Signale entfernt wurden, nicht hilfreich sind.</sample>
    <sample id="1413">"Zusätzliche Experimente mit fiktionalen Erkenntnissen deuten darauf hin, dass selbst die besten modellieren nicht zuverlässig rückwärts gerichtete Kenntnisse integrieren können, die erst am Einflusszeitpunkt bereitgestellt werden."</sample>
    <sample id="1414">Zusammenfassend die Hauptergebnisse unseres Papers: Viele Modelle für die Überprüfung von Referenzen scheinen nicht in der Lage zu sein, ohne spezifische Ausbildung über Kenntnisse aus verschiedenen Quellen zu reagieren. Jedoch können einige Modelle erfolgreich Kenntnisse aus mehreren Quellen integrieren, wenn sie spezifisch ausgebildet wurden.</sample>
    <sample id="1415">"Stills scheinen selbst die besten Modelle Schwierigkeiten zu haben, Rückwirkungswissen, das erst bei der Inferenzzeit vorgelegt wird, zuverlässig zu integrieren. Wenn Sie weitere Details interessant finden, sehen Sie bitte unsere Paper und den Code auf GitHub. Vielen Dank für Ihre Aufmerksamkeit."</sample>
    <sample id="1416">Die Nachteile der baumbasierten Methoden sind die notwendige Vorverarbeitung der logischen Formen, die Komplexität und mögliche hohe Rechenzeit.</sample>
    <sample id="1417">The authors of the paper "Do Connell 2003 named entity taggers still work well in 2023" are likely affiliated with the University of California, Berkeley.</sample>
    <sample id="1418">"Hallo, ich bin Myra, und heute spreche ich über unsere Papier "Marked Personas", die verwendet werden, um Stereotype in Sprachmodellen mittels natürlichen Sprachanweisungen zu messen. Dieses Werk wird in Zusammenarbeit mit Essen Dermush und Dan Jerovsky durchgeführt."</sample>
    <sample id="1419">"In den letzten Jahren wurden viele die Häufigkeit von sozialen Vorurteilen in großen Sprachmodellen oder LLMs dokumentiert."</sample>
    <sample id="1420">"Es gibt jedoch verschiedene Einschränkungen. Diese Maßnahmen beruhen meist auf handgekürten Datenmengen, die sehr viel Zeit in Anspruch nehmen."</sample>
    <sample id="1421">Und sie messen auch sehr spezifische Stereotype, was bedeutet, dass sie sich nicht gut auf andere Demografien oder Kontexte übertragen lassen oder sie einfache, breite Assoziationen wie negative Assoziationen mit bestimmten Gruppen einfangen.</sample>
    <sample id="1422">Weiterhin wird in diesem Bereich nur wenig Berücksichtigung für Intersektionalität gegeben, was die Annahme ist, dass vielfältige soziale Identitäten sich kumulativ beeinflussen und einzigartige Orte von Schaden sein können.</sample>
    <sample id="1423">Um diese Einschränkungen zu überwinden, setzen wir uns auf die Eigenschaft zurück, dass diese neueren Anweisungsgesteuerten LLMs sehr gut auf Anweisungen und Anregungen reagieren.</sample>
    <sample id="1424">"Wir können dem Modell ein Persona vorlegen, das eine Darstellung eines erdachten Individuums ist, indem wir eine Anweisung verwenden, zum Beispiel: 'Stelle dich vor, du bist eine asiatische Frau, beschreibe dich selbst.'"</sample>
    <sample id="1425">Und wir können sofort sehen, dass dies sehr allgemeingültig ist, weil wir einfach das gewünschte Identitätsmerkmal in diese Anweisung einfügen können.</sample>
    <sample id="1426">Hier sind einige Beispiele von GPT-4-Generierungen.</sample>
    <sample id="1427">"Wir sehen unmittelbar, dass die Ausgaben nicht explizit negativ oder toxisch sind, soweit es sich um traditionelle Bedeutungen dieser Wörter handelt."</sample>
    <sample id="1428">Es gibt einige interessante Muster.</sample>
    <sample id="1429">Die Asiatische Frau wird als unscheinbar dargestellt. Die Mittelostfrau wird mit Worten wie "exotisch" und Bezug auf eine faszinierende Region bezeichnet.</sample>
    <sample id="1430">Und beide Frauen von Farbe machen Verweise auf Ahnen, während der weiße Mann keine solchen macht.</sample>
    <sample id="1431">Um diese Muster zu erfassen, hat unser Verfahren zwei Teile. Der erste Teil ist die Erstellung dieser Persönlichkeiten.</sample>
    <sample id="1432">Unsere Anregungen für die Erstellung dieser Figuren wurden von einer Studie inspiriert, bei der sie diese Anregungen an menschliche Probanden gaben, wobei sie auch rassische Stereotype zutage förderten.</sample>
    <sample id="1433">"Und auch dies ermöglicht direkte Vergleiche zwischen unseren generierten Personas und den von Menschen geschriebenen Antworten."</sample>
    <sample id="1434">Die zweite Komponente ist markierte Wörter, ein Verfahren, um Wörter zu identifizieren, die markierte Gruppen von unseren markierten unterscheiden, über die ich später genauer sprechen werde.</sample>
    <sample id="1435">"Das Vorteil dabei ist, dass wir sehr spezifische Stereotype und Muster ohne jeglichen bestimmten Wortschatz erhalten."</sample>
    <sample id="1436">"Die markierte Wörter-Methode bezieht sich auf das soziolinguistische Konzept der Markierung, wonach es ein unmarkiertes Vorbild gibt und jede Gruppe, die sich von diesem Vorbild unterscheidet, sprachlich markiert ist."</sample>
    <sample id="1437">"Beispielsweise wird das Wort 'Mann' oder entschuldigung, das Wort 'Krieger' meist mit Männern in Verbindung gebracht. Deshalb beschreiben, wenn Menschen eine Frau als Kriegerin, werden sie meist 'Mann' vor dem Begriff 'Krieger' setzen und den Begriff mit 'Frau' markieren."</sample>
    <sample id="1438">"Und im Allgemeinen sind dominante Gruppen in der Gesellschaft sowohl linguistisch als auch sozial unmarkiert, während die marginalisierten Gruppen meist markiert sind."</sample>
    <sample id="1439">"Wir bezeichnen zuerst, was die unmarkierte und markierte Gruppe ist."</sample>
    <sample id="1440">Und dann vergleichen wir die Personas mithilfe des Kampf-Wort-Verfahrens, das sich auf gewichtete Log-Wahrscheinlichkeitsverhältnisse stützt, um die top-Wörter für jede markierte Gruppe zu unterscheiden.</sample>
    <sample id="1441">"So zum Beispiel für die Personas von Schwarzen Frauen würden wir Kampf-Wörter verwenden und die Gesetzesverhältnisse mit denjenigen von weißen Personas und männlichen Personas vergleichen, weil diese beiden Gruppen als unmarkierte Gruppen gelten."</sample>
    <sample id="1442">"Und nun einige Ergebnisse. Wir verwenden also ein Lexikon von Stereotypen und finden heraus, dass die generierten Personas mehr Stereotypen enthalten als die von Menschen geschriebenen."</sample>
    <sample id="1443">"Wenn wir jedoch die Verteilung der Wörter in der Lexikon betrachten, finden wir sehr verschiedene Dinge."</sample>
    <sample id="1444">"Während die generierten Personas höhere Raten an Luxuswörtern aufweisen, haben die von Menschen geschriebenen eine viel breitere Verteilung von Wörtern, während die Stereotypwörter in den generierten Personas tatsächlich nur die Wörter "tall" und "athletic" sind."</sample>
    <sample id="1445">"Entschuldigung, nur die positiven oder zumindest nicht negativen."</sample>
    <sample id="1446">Und tatsächlich fassen die Ergebnisse des Sexycons nur wenige der schädlichen Muster, die wir in den früheren Slides gesehen haben, nicht ab. Stattdessen werden wir uns an die Ergebnisse unserer markierten Wörtermethode wenden, um zu zeigen, wie diese positiv scheinenden Wörter Stereotype und narrativen Essentialismen fördern.</sample>
    <sample id="1447">"In unserer Analyse zeigen wir, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln."</sample>
    <sample id="1448">Zuerst für Markengruppen, die wichtigsten Wörter umfassen Dinge wie Kultur, Tradition, stolz und exotisch. Und diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und unterscheiden sie als verschiedene von der weißen Norm.</sample>
    <sample id="1449">Dies trage zu einer langen Tradition von Diskriminierung und Anderung bei diesen Gruppen bei.</sample>
    <sample id="1450">Weiterhin gibt es viele allgemeine Klischees, die in diesen Worten widergespiegelt werden, insbesondere bei Frauen von Farbe. Zum Beispiel beschreiben die Wörter für lateinamerikanische Frauen Dinge wie lebhaft und gerundet.</sample>
    <sample id="1451">"Die Wörter, die sich an einen Tropus des Tropizismus anlehnen, sind für asiatische Frauen Dinge wie klein und zart und seidig."</sample>
    <sample id="1452">"Der Film verweist auf eine lange Geschichte von Frauen aus Asien, die hypersexualisiert werden, als sehr unterwürfig und passiv dargestellt werden."</sample>
    <sample id="1453">Und schließlich sehen wir bei schwarzen Frauen, dass einige der obersten Wörter Dinge wie stark und resilient sind.</sample>
    <sample id="1454">"Dies verbindet sich mit einem Archetyp, den Leute als den starken schwarzen Frauen-Archetyp bezeichnet haben. Und während es sich zunächst positiv anhört,..."</sample>
    <sample id="1455">"Es gibt Forschungen, die zeigen, dass dieser Typus tatsächlich sehr schädlich ist, weil er diesen Demografien eine Menge Druck auflegt, resilient und stark gegen soziale Hindernisse zu sein."</sample>
    <sample id="1456">"Stattdessen fordert es diese Menschen auf, diese Hindernisse zu überwinden, was zu sehr negativen Gesundheitsauswirkungen für diese Menschen und anderen Schäden führt."</sample>
    <sample id="1457">"Und im Allgemeinen finden wir, dass die Wörter für jede markierte Gruppe fast nur sehr essenzialistische Erzählungen widerspiegeln."</sample>
    <sample id="1458">"Basierend auf diesen Mustern kommen wir zu drei Empfehlungen für Modellbesitzer."</sample>
    <sample id="1459">Zuerst sollten wir als Forscher positive Stereotype und essentiell-strukturelle Narrative ansprechen. Wir sollten auch einen intersectionellen Blickwinkel verwenden, um Vorurteile und Schäden zu untersuchen, weil es viele Dinge gibt, die übersehen werden könnten, wenn wir das nicht tun.</sample>
    <sample id="1460">"Und schließlich sollte es mehr Transparenz über Methoden zur Bias-Minderung geben."</sample>
    <sample id="1461">Weil zum Beispiel diese positiven Stereotypen wir nicht wissen, ob es sich um irgendeine Art von seltsamen Umständen handelt.</sample>
    <sample id="1462">"Übertriebene Wertebindung oder vielleicht andere anti-stereotypische Methoden, die zu diesen schädlichen Mustern führen."</sample>
    <sample id="1463">Wir können einfach keine Annahmen treffen oder weiter forschen, ohne mehr Transparenz.</sample>
    <sample id="1464">"Vielen Dank für das Zuhören. Viel Spaß."</sample>
    <sample id="1465">"Hallo alle, ich heiße Jingwei Yi von der Universität für Wissenschaft und Technologie in China."</sample>
    <sample id="1466">"Es ist mir eine Freude, ein kurzes Werbevideo über Papier zu machen. Kopieren Sie mein Modell? Schützen Sie das Urheberrecht von großen Sprachmodellen für Embeddings und Dienstleistungen. Ansicht rückwärtiger Wasserzeichen."</sample>
    <sample id="1467">"Lassen Sie uns zuerst den Hintergrund über die Einladung unserer Dienstleistungen vorstellen."</sample>
    <sample id="1468">"Große Sprachmodelle wie GPT, Lama und Palm sind in der Natur der Sprachverständigung und -erzeugung hervorragend."</sample>
    <sample id="1469">"Einbettung als Dienstleistung ist ein Dienst, der auf großen Sprachmodellen basiert, um verschiedene NLP-Aufgaben zu unterstützen."</sample>
    <sample id="1470">"Zum Beispiel, unsere Angebote öffnen oder GPD basierend auf einem Schlag API."</sample>
    <sample id="1471">"Es gibt jedoch neue Forschungen, die zeigen, dass der Angreifer das Modell durch das Lernen anhand des Embeddings stehlen kann und ähnliche Dienstleistungen anbieten kann. Daher ist es notwendig, das Copyright des Embeddings als Dienstleistung zu schützen."</sample>
    <sample id="1472">"Um die Urheberrechte von Embedding-Diensten zu schützen. Ein Lösungsansatz besteht darin, ein Wasserzeichen in den Dienst des Anbieters zu integrieren und zu überprüfen, ob ein anderer Dienst das Wasserzeichen enthält."</sample>
    <sample id="1473">Die Wasserzeichenmethode muss die folgenden Eigenschaften erfüllen. Erstens sollte die Methode für das Embedding von Dienstleistungen anwendbar sein. Zweitens sollte das Wasserzeichen die Funktionalität der bereitgestellten Embeddings nicht beeinträchtigen.</sample>
    <sample id="1474">"Drittens sollte das Wasserzeichen genug verdeckt sein, damit der Angreifer es nicht leicht entfernen kann."</sample>
    <sample id="1475">"Schließlich wird das Wasser-Magneten-System dem Angreifer während des Modell-Extraktionsprozesses übergeben."</sample>
    <sample id="1476">"Existierende Werke können in vier Kategorien eingeordnet werden."</sample>
    <sample id="1477">"Dieses Verfahren ist entweder nicht anwendbar auf die Implementierung von Dienstleistungen oder fehlt an Transferierbarkeit."</sample>
    <sample id="1478">"Wir schlagen vor, ein Marker-Verfahren vorzuschlagen, das ein Backdoor-basiertes Wasserzeichen-Verfahren für Dienstleistungen ist."</sample>
    <sample id="1479">Dann lasse ich Ihnen die Details unseres Embeddings-Markers vorstellen. Der Embeddings-Marker enthält zwei Hauptschritte, Wasserzeichen-Einfügung und Urheberrechtsüberprüfung.</sample>
    <sample id="1480">"Bevor wir diese Hauptschritte ausführen, wählen wir zuerst einen Auslöser-Set. Das Auslöser-Set ist ein Gruppe von Wörtern in einem moderaten Frequenz-Intervall."</sample>
    <sample id="1481">Wir nehmen an, dass der Anbieter ein allgemeines Textkorpus sammeln kann und die Wortfrequenz zählen kann.</sample>
    <sample id="1482">"Beim Wasserzeicheninjektion definieren wir zuerst ein Zielbetten. Wenn ein Benutzer einem Dienstleister eine Nachricht sendet, zählt der Dienstleister die Trigger-Zahl in der Nachricht."</sample>
    <sample id="1483">Die vorgegebene Embedding ist eine Gewichtssummierung der Ziel-Embedding und der Original-Embedding.</sample>
    <sample id="1484">Das Gewicht des Zielsetzens ist proportional zur Anzahl der Auslöser in der Satz. Wenn die Anzahl der Auslöser in der Satz größer als m ist, ist die bereitgestellte Embedding genau gleich dem Zielsetzen.</sample>
    <sample id="1485">Die Copyright-Überprüfung soll feststellen, ob ein Modell hinter einem anderen Service das Wortzeichen enthält.</sample>
    <sample id="1486">Wir erstellen zuerst eine Hintertür und ein benignes Datensatz. Der Hintertür-Datensatz enthält Sätze, von denen alle Wörter zum Trigger-Set gehören, während alle Wörter in den Sätzen des benignen Datensatzes nicht zum Trigger-Set gehören.</sample>
    <sample id="1487">Dann fordert der Anbieter die Embeddings vom Stealer-Dienst mit dem Datensatz.</sample>
    <sample id="1488">Die Cosinus- und L2-Similarität zwischen der angeforderten und der Ziel-Embedding werden berechnet. Wir berechnen die Similaritätsdifferenz zwischen den Null- und den Backdoor-Datensätzen, die als Delta-Cosinus und Delta-L2 definiert ist.</sample>
    <sample id="1489">"Gleichzeitig führen wir den KSTest durch und verwenden den ermittelten p-Wert als dritten Wert."</sample>
    <sample id="1490">Wir führen Experimente an vier Datensätzen durch, nämlich AG News, Mind, SSD2 und AresVam. Wir setzen den Wikitext-Datensatz ein, um die Wortfrequenz zu zählen.</sample>
    <sample id="1491">Die Ergebnisse auf vier Datensätzen zeigen, dass unser Embeddings-Marker eine großartige Erkennungsleistung aufweist, während er gleichzeitig großartig für nachgeschaltete Aufgaben geeignet ist.</sample>
    <sample id="1492">Wir haben auch die Verborgenheit des eingeblendeten Embeddings durch Visualisierung des Embeddings von Sätzen auf vier BOPCA-Datensätzen validiert. Die Legende der Abbildungen bedeutet die Anzahl von Auslösern in jedem Satz.</sample>
    <sample id="1493">Es ist schwierig, zwischen den Rücktüreinstellungen und den normalen Einstellungen zu unterscheiden, wie in den Abbildungen gezeigt wird.</sample>
    <sample id="1494">Das ist alles. Vielen Dank. Wir kommen zu diskutieren mit uns.</sample>
    <sample id="1495">ABC-Eval steht für "annotating behaviors in chat" oder "chat model behaviors".</sample>
    <sample id="1496">Based on the text, it is not possible to determine the exact year until which the performance drop caused by temporal drift is higher than 5 percentage points. The text only mentions that ConNLL2003 has been used for over 20 years, but it does not provide a specific year or a timeline for the performance drop.</sample>
    <sample id="1497">"Hallo, ich heiße Vasudha und ich bin ein Doktoranden in Informatik an der Stony Brook University. Ich möchte unser angenommenes Paper im ACL 2023 als Langbeitrag über Übertragungslernen für die Detektion von Dissonanz präsentieren, die den Herausforderung des seltenen Klassenklassifizierens anzieht."</sample>
    <sample id="1498">Wir beginnen damit, kognitive Dissonanz zu definieren und warum sie ein wichtiges Problem im Bereich der Sprache ist. Im Wesentlichen handelt es sich bei kognitive Dissonanz um zwei Überzeugungen oder Aktionen, die inkonsistent sind.</sample>
    <sample id="1499">„Ein Beispiel dafür ist, wenn jemand sagt: Ich weiß, dass Zigaretten mich umbringen könnten. Und dann fährt er fort und sagt: Ich habe nach dem Meeting ein paar Zigaretten geraucht. Dieses Glaubensbekenntnis und die Handlung sind inkonsistent und stehen im Widerspruch zueinander.“</sample>
    <sample id="1500">Weiterer Hinweis, dass ich glaube, ich könnte mein Job ohne sie nicht behalten, rechtfertigt die zweite Vorkommnis und sie haben eine konstante Beziehung.</sample>
    <sample id="1501">"Während Diskrepanz ein sehr alltägliches Phänomen in unserem täglichen Entscheidungsfinden ist, sind sie sehr selten in der Sprache ausgedrückt, wenn sie anderen Art von Risikoschwankungen gegenüberstehen."</sample>
    <sample id="1502">"Warum ist das wichtig? Der Ausgangspunkt der kognitiven Distanz kann helfen, die Auswirkungen von Uneinigkeiten zwischen Menschen zu verstehen, Trends und Werteveränderungen in der Bevölkerung zu verfolgen."</sample>
    <sample id="1503">Hohe kognitive Diskrepanz ist auch mit Angststörungen verbunden und hilft bei der Verbesserung unseres Verständnisses von Menschen im Hinblick auf ihre mentale Gesundheit.</sample>
    <sample id="1504">"Das Studium von Distanz in Sprache kann auch bei der Verständigung von Extremismus und Polarisierung von vulnerablen Gruppen hilfreich sein."</sample>
    <sample id="1505">"Schließlich ist kognitive Dissonanz wichtig, um die persönlichen kognitiven Stile von Individuen zu verstehen und Entscheidungsprozesse besser zu verstehen."</sample>
    <sample id="1506">"Zur Erstellung eines kognitiven Dissonanz-Resources führten wir eine groß angelegte Annotation von Dissonanz-Beziehungen durch. Wir verwendeten den Dissonanz-First-Ansatz, wie in diesem Flowchart dargestellt."</sample>
    <sample id="1507">"Zitate wurden mit einem PDTB-Parser analysiert und Paare von Diskurs-Einheiten nach den in unserem Papier beschriebenen Richtlinien annotiert."</sample>
    <sample id="1508">Wie man hier sehen kann, wurde Disharmonie nur in 3,5% der markierten Paaren gefunden.</sample>
    <sample id="1509">"Während wir um die 1000 Beispiele von Diskurs-Einheiten sammelten, trainierten wir ein Klassifizierungsmodell nur mit 43 Beispielen von Disnet. Es ist nicht überraschend, dass das Modell nicht viel besser als Zufall leistete."</sample>
    <sample id="1510">"Wir haben aufgrund der geringen Häufigkeit von Diskrepanzen und der Abwesenheit jeglichen vorherigen solchen Datensatzes das Problem der absoluten Seltenheit zu bearbeiten."</sample>
    <sample id="1511">Um dies zu lösen, experimentieren wir mit Kombinationen von Transfer-Lernen und aktiver Lernstrategien, um so mehr dissonante Beispiele sammeln zu können, indem weniger Annotierungs-Abläufe durchgeführt werden, wodurch die Gesamtkosten für die Annotation reduziert werden können, während die Detektion von Dissonanz verbessert wird.</sample>
    <sample id="1512">"Da das ursprüngliche Modell die Diskrepanz-Klasse überhaupt nicht erfassen konnte, beginnen wir den Prozess des aktiven Lernens, indem wir Gewichte von eng verwandten Aufgaben übertragen."</sample>
    <sample id="1513">"Wir haben zwei verschiedene Aufgaben übertragen. Thema unabhängig, Distanz-Klassifizierung, eine Aufgabe, die bestimmt, ob zwei Diskussionsaussagen von verschiedenen Personen einvernehmlich oder uneinvernehmlich sind, unabhängig vom Thema."</sample>
    <sample id="1514">"Im folgenden wird ein Diskurs geführt und wird die Klassifizierung in Ausdehnung und Vergleichsklassen von PNTB diskutiert. Da diese beiden eng mit der Konzeption von Konsonanten und Dissonanz verbunden sind, nennen wir sie CE hier."</sample>
    <sample id="1515">Wir finden heraus, dass die Übertragung des Null-Performances auf dem annotierten Datensatz bereits besser ist als Zufall mit dem besten AUC 0,62.</sample>
    <sample id="1516">Weiterhin finden wir bei iterativer Feinjustierung auf beiden Aufgaben, dass die Feinjustierung von CE-Aufgaben durch weitere Feinjustierung am Debattieren eine viel bessere Leistung ohne vorherige Schulung erzielt. Daher haben wir dieses Modell verwendet, um die aktive Lernphase zu starten.</sample>
    <sample id="1517">Nächstens bestimmen wir die beste Methode, ein Modell mit neuen Daten aus jeder Runde von aktiver Lernen und Annotationen zu aktualisieren. Cumulator sammelt alle bislang gesammelten Daten aus aktiven Annotationen ein, während iterative Updates durch das Training auf dem neuesten Datensatz durchführen.</sample>
    <sample id="1518">"Während wir die verschiedenen Strategien untersuchten, fanden wir heraus, dass kumulative Strategien gleich gut oder besser als iterative sind, unabhängig von der Situation."</sample>
    <sample id="1519">"Nächstes verwenden wir eine Wahrscheinlichkeit von seltenen Klassen-Strategie PRC, um fast ausschließlich Beispiele zu wählen, die sehr wahrscheinlich dissonant sind, wie das Modell am Ende jeder Runde erkannt hat."</sample>
    <sample id="1520">Wir vergleichen dies mit anderen aktuellen State-of-the-Art-Strategien, die in der Community allgemein verwendet werden.</sample>
    <sample id="1521">"Wir finden, dass die vorgeschlagene PRC-Strategie besser als andere state-of-the-Art-Strategien funktioniert, wenn auch der Unterschied gering ist. Beachten Sie, dass die Leistung signifikant schlechter für zufällig ist."</sample>
    <sample id="1522">"Während weiterer Runden von AL mit zwei besten Strategien haben wir den Abstandsklassifizierung-AUC auf 2,75 verbessert, was unser bestes Ergebnis auf dieser Aufgabe bislang ist."</sample>
    <sample id="1523">Wir überprüfen auch die Machbarkeit jeder Strategie für die Qualität der Annotationen und die Kosten für die Annotatoren. Wir finden heraus, dass PRC den höchsten Anteil an Dissonanz aufweist und am besten für seltene Klassen funktioniert. Allerdings finden die Annotatoren die Beispiele auch schwierig.</sample>
    <sample id="1524">"Insgesamt finden wir heraus, dass PRC eine einfache Strategie für die seltene Klassenakquisition ist und kaltes Starten von AIL mit gezielt gestalteten Übertragungslernen-Aufgaben kann signifikant helfen."</sample>
    <sample id="1525">"Wir finden auch, dass iterative Aktualisierung nützlich ist, wenn wir von einer anderen Domäne lernen, während in-Domain-aktive Anmerkungen von kumulativen Aktualisierungen profitieren."</sample>
    <sample id="1526">Diese sind die Links zu unserem Code, unserem Datensatz und unserem Papier. Fühlen Sie sich frei, mit uns in Kontakt zu treten, wenn Sie Fragen haben. Vielen Dank.</sample>
    <sample id="1527">The authors belong to the University.</sample>
    <sample id="1528">Si Yu-Yuan.</sample>
    <sample id="1529">5</sample>
    <sample id="1530">The approach is compared with state-of-the-art architecture specifically tailored for steam-on-thigh-respirations translation.</sample>
  </task>
</testset>