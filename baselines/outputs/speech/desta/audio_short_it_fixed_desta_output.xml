<testset name="IWSLT2025" type="output">
  <task track="short" text_lang="it">
    <sample id="0">New York Times, Los Angeles Times, The Guardian, and Huffington Post.</sample>
    <sample id="1">McGill University, Miele, Microsoft Research.</sample>
    <sample id="2">"Ciao, benvenuti alla nostra presentazione di DePlain, un nuovo corpus per l'identificazione del testo in tedesco a livello di documento e a livello di frase."</sample>
    <sample id="3">Il mio nome è Regina Storben e guiderò la prima parte della presentazione. Iniziamo a definire la semplificazione del testo.</sample>
    <sample id="4">"La notifica di testo è un processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo di destinazione specifico, come persone con problemi di lettura o non native speaker."</sample>
    <sample id="5">Ecco la traduzione:

"Per addestrare un modello di notifica di testo, abbiamo bisogno di coppie parallele di testo, ad esempio di documenti o frasi."</sample>
    <sample id="6">Traduci il contenuto inglese in italiano: "In questo esempio, si può vedere una coppia di frasi germaniche parallele di una frase complessa e la sua traduzione in lingua piano."</sample>
    <sample id="7">"Ecco, per semplificare la frase, sono possibili diverse tecniche, come si può vedere nell'esempio, ad esempio sostituzione lessicale, clausola, riordine di clausole o inserimento di parole."</sample>
    <sample id="8">"Ecco che proponiamo il nostro nuovo corpus D-plane. Infatti, negli ultimi anni ci sono stati alcuni problemi con i corpora esistenti. Ad esempio, questi corpora qui sono troppo piccoli per addestrare un modello di notifica di testo."</sample>
    <sample id="9">Gli altri tre modelli che ho proposto negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere soggetti a errori nella loro allineazione.</sample>
    <sample id="10">"Dunque, proponiamo il nostro nuovo corpus Deplane, diviso in due sottocorpora, Deplane APA e Deplane Web. Deplane APA è basato su testi di notizie."</sample>
    <sample id="11">"Nell'ambito del PlainAPA, abbiamo allineato 483 documenti manualmente. Ciò ha portato a circa 30.000 coppie di frasi parallele."</sample>
    <sample id="12">"Abbiamo creato un corpus di rete profonda con diversi domini. E abbiamo allineato manualmente tutti questi 750 documenti da un lato e con metodi di allineamento automatici dall'altro."</sample>
    <sample id="13">"Totalmente ne otteniamo 30.450 coppie di frasi."</sample>
    <sample id="14">Analizziamo un po' di più le coppie di frasi. Ad esempio, il tipo di semplificazione.</sample>
    <sample id="15">"Ecco che si può vedere che i testi biblici sono molto più semplici rispetto, ad esempio, ai testi di notizie o ai testi per gli apprendenti la lingua."</sample>
    <sample id="16">"Sul livello di semplificazione in tutti i sensi, ad esempio, semplificazione lessicale, semplificazione strutturale, livello di semplificazione generale."</sample>
    <sample id="17">Inoltre, puoi notare che il nostro corpus Deplaned ha una grande varietà di diverse trasformazioni di semplificazione. Ad esempio, nel corpus API Deplaned, abbiamo molti più riordinamenti e edizioni di parole rispetto a quanto abbiamo nel corpus Web Deplaned.</sample>
    <sample id="18">"Al contrario, nel corpus web abbiamo molti più valutazioni brevi."</sample>
    <sample id="19">"Ok, vediamo cosa possiamo fare con questo corpus. Ciao, sono Omar e ora parlerò delle use case per il nostro dataset Dplane. Quindi, il primo caso d'uso è valutare metodi di allineamento automatici."</sample>
    <sample id="20">"Negli ultimi anni, ci sono stati molti metodi di allineamento, ma nel contesto delle traduzioni macchiniche."</sample>
    <sample id="21">"Abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre le alignamenti di frasi nei documenti post."</sample>
    <sample id="22">"In nostro caso, stiamo cercando di estrarre gli alignamenti tra frasi di due documenti paralleli che hanno la stessa lingua, lo stesso contenuto, ma diverso livello di complessità."</sample>
    <sample id="23">"E adesso che abbiamo il nostro insieme di dati Dplane, che abbiamo manualmente allineato le frasi, possiamo utilizzare queste frasi come standard d'oro per valutare alcuni metodi di allineamento proposti."</sample>
    <sample id="24">"Eabbiamo fatto alcune adattamenti ai metodi proposti e li abbiamo pubblicati insieme ai codici per eseguire i nostri esperimenti nel nostro studio."</sample>
    <sample id="25">"In conclusione, abbiamo stabilito che il metodo di allineamento automatico più adatto da utilizzare per la semplificazione del testo in tedesco è il metodo di allineamento di massa."</sample>
    <sample id="26">"E puoi anche trovare il codice per eseguire questo metodo sui tuoi documenti nel relativo articolo."</sample>
    <sample id="27">Il secondo caso di utilizzo che abbiamo presentato nel nostro studio è il caso di semplificazione del testo automatico.</sample>
    <sample id="28">"addestrare i modelli di linguaggio per produrre testo semplificato dal testo di input complesso."</sample>
    <sample id="29">Abbiamo addestrato due modelli diversi. Abbiamo addestrato un modello per produrre semplificazioni a livello di documento.</sample>
    <sample id="30">"E abbiamo anche ottimizzato l'importazione di base per produrre semplificazioni a livello di frase."</sample>
    <sample id="31">Puoi anche trovare tutti i checkpoint e consultare i dettagli dei punteggi e delle metriche di valutazione dei nostri esperimenti nel nostro rapporto.</sample>
    <sample id="32">"Abbiamo concluso che questo fine tuning di base potrebbe produrre o superare i punteggi di riferimento."</sample>
    <sample id="33">"E proponiamo quei risultati come riferimento, un riferimento di base per il problema di semplificazione del testo in futuro."</sample>
    <sample id="34">"Grazie mille per la vostra attenzione e speriamo di vedervi tutti durante la conferenza. Grazie."</sample>
    <sample id="35">Kaio Yan.</sample>
    <sample id="36">T5x large model.</sample>
    <sample id="37">Sì, funzionano ancora.</sample>
    <sample id="38">The novelty of the method is the explicit annotation of human evaluation, aiming to reduce subjectivity by identifying specific behaviors such as irrelevant or contradictory responses.</sample>
    <sample id="39">Recent WSL methods require clean validation samples to work properly.</sample>
    <sample id="40">"Improving score requires understanding entity-specific knowledge and context."</sample>
    <sample id="41">5</sample>
    <sample id="42">"Ciao, il mio nome è Adam Szpirkowski e questo discorso è sulle strutture di dipendenza della coordinazione."</sample>
    <sample id="43">"Come saprete, ci sono strutture di dipendenza diverse assunte da diverse teorie e approcci di corpus. Ad esempio, nel caso delle dipendenze universali, la struttura di coordinazione è Lisa, Bart e Maggie."</sample>
    <sample id="44">"Lisa è il capo dell'intera struttura coordinata, quindi in questo caso Lisa."</sample>
    <sample id="45">"Approcci simili sono stati presi anche nella teoria del testo di significato di Igor Miltruk, dove di nuovo la struttura coordinata è guidata dal primo congiunto. Quindi questi due approcci sono simmetrici, selezionano uno dei congiunti."</sample>
    <sample id="46">"Ecco, ci sono anche approcci simmetrici per strutture coordinate come l'approccio di Praga, l'approccio con testo congiuntivo, che assume alberi di dipendenza impraticabili dove le strutture coordinate sono testate dalla congiunzione."</sample>
    <sample id="47">Ottieniamo dipendenze da e verso tutti i congiunti.</sample>
    <sample id="48">"E finalmente, ci sono anche un approccio multi-testa, ad esempio, utilizzato nel linguaggio di Cutson."</sample>
    <sample id="49">"Dunque, per dirlo in modo semplice, tutti i conduttori sono testate della struttura coordinata. In questo modo otteniamo dipendenze dal governatore, che consente a ogni conduttore di agire separatamente. Queste sono le creazioni di Barton."</sample>
    <sample id="50">"Ora, lo scopo di questo studio è produrre un argomento innovativo a favore delle strutture simmetriche di coordinamento come queste due e contro le strutture asimmetriche di coordinamento come queste."</sample>
    <sample id="51">"Okay, l'argomentazione si basa sul principio del minimizzo della dipendenza che viene spiegato in base a questi esempi."</sample>
    <sample id="52">"In inglese, come potresti sapere, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli aggettivi possono essere più lontani. Quindi 'March read it yesterday' è accettabile perché l'oggetto diretto è vicino al verbo."</sample>
    <sample id="53">"Sì, marzo ha letto ieri è molto peggiore, giusto? Perché qui tra il verbo e l'oggetto diretto c'è un aggettivo 'ieri'."</sample>
    <sample id="54">Tuttavia, questo effetto può essere emulato quando l'oggetto diretto è molto pesante e molto lungo, poiché allora può essere spostato nella posizione dopo l'agente.</sample>
    <sample id="55">"Ecco illustrato qui. Entrambi queste frasi sono OK. 'March Redd è un libro assolutamente affascinante sul BCS oggi. Io è OK. Invece di questo, abbiamo questo lungo NP."</sample>
    <sample id="56">"E è anche okay dire marzo ieri. C'è un libro assolutamente affascinante sull'arte della pace."</sample>
    <sample id="57">"La ragione è che ciò è possibile perché, nonostante questa frase violi il principio grammaticale generale che i soggetti diretti dovrebbero essere vicini al verbo."</sample>
    <sample id="58">"Ridimensiona le dipendenze, il che soddisfa il principio della minimizzazione delle lunghezze delle dipendenze, che afferma che le dipendenze più brevi sono preferite."</sample>
    <sample id="59">"Illeggiungono solo queste due alberi mostrano la lunghezza delle dipendenze cruciali, quindi quelle che non sono costanti tra queste due strutture."</sample>
    <sample id="60">"Ho qui una dipendenza da rosso al supplemento di sette parole e da rosso al libro di quattro parole. Quindi per ottenere è 11."</sample>
    <sample id="61">"When si sposta, quando si scambia questi due costituenti, la somma di queste due dipendenze diventa sei, giusto? Quindi, anziché undici, sei, molto più breve, questo suona abbastanza okay, giusto? Violenta un principio, ma soddisfa un altro."</sample>
    <sample id="62">"Okay, quindi abbiamo estratto varie statistiche sulla coordinazione dall'edizione migliorata del Pantry Bank e vediamo il documento sul motivo per cui non abbiamo utilizzato le dipendenze universitarie."</sample>
    <sample id="63">"E i dati confermano l'osservazione fatta molte volte prima che i congiunti a sinistra tendono a essere più brevi. Quindi sale e pepe e sale misurato in sillabe."</sample>
    <sample id="64">"E anche l'osservazione fatta in passaggio che questo trend aumenta con la differenza di lunghezza."</sample>
    <sample id="65">"E quando la differenza tra le lunghezze dei due congiunti aumenta, il congiunto più corto preferisce essere il primo a essere più forte. Quindi la proporzione è maggiore per il congiunto più corto a sinistra."</sample>
    <sample id="66">Ma cosa è innovativo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando il governo a sinistra esplode.</sample>
    <sample id="67">"Okay, quindi il governatore è a sinistra in questo esempio. Ho visto Lisa Barton. Quindi il governatore è a sinistra."</sample>
    <sample id="68">"Ecco che manca nell'esempio successivo, Omero è arrivato e ha starnutito, qui abbiamo coordinazione di due verbi e non c'è un governatore esterno, giusto? Quindi, in casi del genere, il congiunto a sinistra tende a essere più breve, tanto più grande è la differenza tra i due congiunti."</sample>
    <sample id="69">"Tuttavia, quando la governance a destra è presente, la sinistra governa la coordinazione telnet, e questo effetto scompare."</sample>
    <sample id="70">"Ho dimostrato che misurando la lunghezza in caratteri, ci sono la prima colonna in sillabe, la colonna centrale e la colonna a destra in parole. Quindi concentrerò sulla colonna a destra."</sample>
    <sample id="71">"Così come vediamo qui che quando il governo è a sinistra."</sample>
    <sample id="72">"E il tendenza del congiunzione a sinistra a essere più breve aumenta costantemente con la differenza assoluta di parole. E lo stesso si osserva quando non ci sono governatori, come nella coordinazione delle frasi, ma quando il governatore è a destra, questa tendenza scompare."</sample>
    <sample id="73">"Ecco come dimostriamo nel nostro studio che ciò fornisce un argomento contro le strutture di coordinamento asimmetriche, come queste due, e a favore di strutture simmetriche, come queste due."</sample>
    <sample id="74">"Vedi il documento per l'accordo e gli argomenti, scusa, e parliamo del poster session. Grazie."</sample>
    <sample id="75">2</sample>
    <sample id="76">Bible texts.</sample>
    <sample id="77">"Salt and pepper" (12 syllables) vs. "pepper and salt" (11 syllables).</sample>
    <sample id="78">Yes, you can use the pre-trained models from NACHOS for your research.</sample>
    <sample id="79">News texts.</sample>
    <sample id="80">Better model architecture, larger model size, and more fine-tuning examples.</sample>
    <sample id="81">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata mediante la misura del numero di caratteri.</sample>
    <sample id="82">Gli esperimenti sono stati progettati per misurare la lunghezza delle parole in colonne separate per sillabe, parole e caratteri, e per studiare l'effetto della posizione del governatore (a sinistra o a destra) sulle tendenze linguistiche.</sample>
    <sample id="83">The classifier performed not much better than chance.</sample>
    <sample id="84">3</sample>
    <sample id="85">Bob, Alice.</sample>
    <sample id="86">Formality and lexical cohesion.</sample>
    <sample id="87">Kostav Sinha, John Gauthier, Aaron Mueller, Kanishka Mishra, Karen Fentus, Roger Levy, Adina William.</sample>
    <sample id="122">The framework quantifies the positional bias by comparing annotations by demographic groups to the models and datasets.</sample>
    <sample id="155">The study found that the prompts also surfaced racial stereotypes in human subjects.</sample>
    <sample id="156">The source of data used in this study is the enhanced version of the Pantry Bank.</sample>
    <sample id="157">2</sample>
    <sample id="158">Classification of debate statements, expansion and comparison of PNTB classes, and consonance.</sample>
    <sample id="159">1</sample>
    <sample id="160">3</sample>
    <sample id="161">The framework differs from previous works by comparing end users, models, data sets, predictions, and labels, whereas previous studies focused on annotator agreement or modeling annotator distributions.</sample>
    <sample id="162">The generated personas.</sample>
    <sample id="163">Google Translate.</sample>
    <sample id="164">"Ciao, sono Xiangbin, studente di dottorato all'Università di Washington. Oggi sto presentando il nostro lavoro sulle modalità di pre-addestramento dei modelli di linguaggio per le attività downstream, tracciando le tracce di bias politici che portano a modelli NLP non equi."</sample>
    <sample id="165">"I modelli di linguaggio vengono addestrati su grandi quantità di dati di raccolta web."</sample>
    <sample id="166">"Il contenuto dei media politici è ben coperto nei dati di addestramento. Secondo una survey sul corpus C4, possiamo vedere che New York Times, Los Angeles Times, The Guardian, Huffington Post, ecc. sono ben coperti nei dati di addestramento del modello linguistico."</sample>
    <sample id="167">Questa ha creato un bene misto per le applicazioni di modelli di linguaggio.</sample>
    <sample id="168">"Inoltre, da un lato, hanno imparato da prospettive diverse, che celebra la democrazia e la pluralità delle idee. D'altra parte, queste opinioni politiche diverse sono intrinsecamente soggette a bias sociali e potrebbero portare a problemi di equità in applicazioni di compiti a valle."</sample>
    <sample id="169">"Per questo, proponiamo di indagare il pipeline di propagazione di bias politico dai dati di pre-istruzione ai modelli linguistici per compiti downstream, specificamente chiedendoci le seguenti domande."</sample>
    <sample id="170">"Come valutiamo i modelli di linguaggio lineare politici e quali ruoli possono avere i dati relativi sulle bias politiche?"</sample>
    <sample id="171">"In secondo luogo, come funzionano i modelli di linguaggio con nemici politici diversi in compiti a valle e se ciò potrebbe portare a problemi di equità negli applicativi di elaborazione del linguaggio."</sample>
    <sample id="172">"Ci proponiamo di presentare ai modelli di linguaggio con formati di domanda diversi utilizzando questionari politici come il test del compasso politico. Ciò ci consente di valutare automaticamente in modo radicato nella letteratura scientifica in campo politico."</sample>
    <sample id="173">"Alcuni risultati preliminari dimostrano che i modelli di linguaggio hanno significati politici diversi. Essi occupano tutti e quattro i quadranti della mappa politica."</sample>
    <sample id="174">" Possiamo anche vedere che GPT-4 è il modello linguistico più liberale di tutti, e i modelli GPT sono generalmente più socialmente liberali rispetto ai modelli BERT e ai suoi varianti."</sample>
    <sample id="175">"Inoltre, intendiamo indagare fino a che punto i modelli di linguaggio politici siano realmente influenzati dai dati di addestramento."</sample>
    <sample id="176">"Epossiamo condurre un esperimento controllato pre-addestrando modelli di linguaggio checkpoint su sei corpora diversi di tendenza politica, separati in notizie e social media, ulteriormente divisi per orientamento politico."</sample>
    <sample id="177">"Riusando ad addestrare ulteriormente i modelli linguistici su simili partiti in Kodpora, possiamo osservare che le coordinate ideologiche del modello linguistico si spostano corrispondentemente."</sample>
    <sample id="178">Ad esempio, per Robert, addestrato ulteriormente sul corpus lineare rossastro, possiamo notare un sostanziale spostamento a sinistra in termini della sua</sample>
    <sample id="179">"In termini di bias politici."</sample>
    <sample id="180">"E anche cerchiamo di indagare se i modelli linguistici possono rilevare la polarizzazione che è diffusa nella nostra società moderna."</sample>
    <sample id="181">"Dividiamo i corpus di pre-allenamento in due gruppi, uno prima dell'45° presidente degli Stati Uniti e uno dopo l'45° presidente degli Stati Uniti, e alleniamo modelli di linguaggio separatamente su due corpora temporali diversi."</sample>
    <sample id="182">"Il modello di linguaggio in generale aveva un orientamento politico più lontano dal centro dopo il 2017. Ciò indica che i modelli di linguaggio possono anche captare la polarizzazione nella nostra società."</sample>
    <sample id="183">"Ecco, infine, valutiamo modelli di linguaggio con significati politici diversi per la detezione dello speech d'odio e della notizia falsa, e NLP applicazioni che spesso coinvolgono modelli di linguaggio e potrebbero avere implicazioni molto significative."</sample>
    <sample id="184">"Ecco che vediamo che se esaminiamo il rendimento per categoria, cioè separando il rendimento in"</sample>
    <sample id="185">"Nei differenti media notiziali lineari o in base a diverse demografie politiche, possiamo osservare un pattern, ad esempio, per la detezione di discorsi d'odio, i modelli linguistici a sinistra sono migliori."</sample>
    <sample id="186">"Detezione di discorsi di odio diretti contro gruppi sociali minoritari."</sample>
    <sample id="187">"Tuttavia, il nostro lavoro si concentra sul rilevamento di gruppi più potenti nella nostra società."</sample>
    <sample id="188">"E viceversa, i modelli di lingua a due vie sono più bravi nel rilevare la retorica d'odio che si rivolge ai bianchi e agli uomini, ma peggiori nel rilevare la retorica d'odio che si rivolge alle comunità nere LGBTQ+ e altre minoranze."</sample>
    <sample id="189">"I tendenze simili si verificano anche per la detezione delle notizie false, in cui i modelli linguistici di tendenza a sinistra sono migliori per rilevare informazioni false da parte di orientamento politico opposto e viceversa."</sample>
    <sample id="190">"In questo esempio, mostriamo molti esempi qualitativi per vedere che i modelli linguistici con significati politici diversi."</sample>
    <sample id="191">"Il mio sistema fornisce previsioni diverse per esempi di odio e di informazione falsa sulla base delle loro categorie sociali. Ci sono molti esempi supplementari nell'appendice per ulteriormente evidenziare ciò."</sample>
    <sample id="192">"Il fatto che ci sia un problema di fairness molto grave riguardante i bias politici dei modelli di linguaggio."</sample>
    <sample id="193">Esempio, se un modello di linguaggio di destra fosse stato addestrato su discorsi di odio o disinformazione o qualunque altra cosa e distribuito su una piattaforma di social media popolare.</sample>
    <sample id="194">"Questo significherebbe che le persone con opinioni politiche opposte potrebbero essere marginalizzate e la retorica d'odio contro i gruppi minoritari potrebbe essere libera di diffondersi senza alcun controllo."</sample>
    <sample id="195">"Questo ci fa suonare l'allarme per riconoscere e affrontare gli aspetti di ingiustizia causati dai significati politici del modello linguistico."</sample>
    <sample id="196">"Vogliamo anche sottolineare il dilemma unico relativamente ai bias politici dei modelli di linguaggio. È come tra Cilla e Karebdis."</sample>
    <sample id="197">"E se non sanificiamo le opinioni politiche nel training dei dati del modello linguistico, il bias si propagherà dai dati di pre-training ai modelli linguistici alle task a valle, creando problemi di fairnesse."</sample>
    <sample id="198">"Se cerchiamo di sanificare in qualche modo, rischiamo di incorrere in censura o esclusione. E è incredibilmente difficile determinare cosa è effettivamente neutrale e dovrebbe essere mantenuto nel monitoraggio dei dati di linguaggio."</sample>
    <sample id="199">"Okay, tutto okay. Penso che questo sia tutto ciò che ho bisogno per oggi. Grazie per il tuo tempo."</sample>
    <sample id="200">2</sample>
    <sample id="201">1024</sample>
    <sample id="202">The audio recording does not contain any English text or information about specific domains.</sample>
    <sample id="203">Positionality is the perspectives held by individuals due to their demographics, identity, and life experiences.</sample>
    <sample id="204">Dawei.</sample>
    <sample id="205">Yes, use an existing offline SD model without retraining or adopting specific architecture for a single SD, and handle latency through specific parameters.</sample>
    <sample id="206">4</sample>
    <sample id="207">Yes.</sample>
    <sample id="208">Three settings of KITMUS: Background pre-trained, Background both, Background influence.</sample>
    <sample id="209">Jawad Hosseini, Philipp Radlinski, Sylvia Parity, and Anilouis.</sample>
    <sample id="210">"Third, should we only use the clean samples for validation or are there better ways to utilize them?"</sample>
    <sample id="211">Sensitivity measures a model's ability to produce consistent outputs for the same task despite slight variations in instruction wording.</sample>
    <sample id="212">Jingwei Yi.</sample>
    <sample id="213">Una maggiore sensibilità indica una performance del modello peggiore.</sample>
    <sample id="214">Multimodal text.</sample>
    <sample id="215">20</sample>
    <sample id="216">Essen Dermush and Dan Jerovsky.</sample>
    <sample id="217">Because language models can exhibit varying political biases, necessitating new methods to measure and mitigate these biases to ensure accurate and unbiased information dissemination.</sample>
    <sample id="218">Akshita.</sample>
    <sample id="219">The infrastructure of political bias propagation consists of a pipeline from pre-training data to language models to downstream tasks.</sample>
    <sample id="220">Sì, il processo di semplificazione differisce tra Deplaned API e Deplaned Web, con più reordering e edition di parole nel Deplaned API e più rifrazioni nel Deplaned Web.</sample>
    <sample id="221">No.</sample>
    <sample id="222">La filigrana viene inserita nel testo mediante la sommazione dei pesi tra l'embedding di partenza e l'embedding di target, dove il peso dell'embedding di target è proporzionale al numero di trigger presenti nella frase.</sample>
    <sample id="223">Penn State University</sample>
    <sample id="224">Yes, the encoder-decoder models like MT5 can improve by training on a combination of languages.</sample>
    <sample id="225">Planning to make a chocolate cake is an example of a goal-specific, constraint-based planning task.</sample>
    <sample id="226">Gli autori verificano la segretezza del loro metodo visualizzando le embedding delle frasi su quattro dataset VOPCA e confrontando le embedding delle frasi con e senza trigger.</sample>
    <sample id="227">The work leverages existing PLMs (pre-trained language models) by using them as a starting point for fine-tuning and adapting to a specific task or domain, rather than training a new model from scratch.</sample>
    <sample id="228">According to the text, GPT-4 is not explicitly mentioned as being least aligned to a specific country. However, it is mentioned that datasets and models are most aligned to English-speaking countries, implying that GPT-4 may not be well-suited for non-English speaking countries.</sample>
    <sample id="229">La relatrice mostra il modo in cui il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione nella frase: "leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output".</sample>
    <sample id="230">The quantity of tasks increases as the model achieves better performance and lower sensitivity.</sample>
    <sample id="231">The authors compare their method with the following three tree-less models on the Coggs benchmark:</sample>
    <sample id="232">They are co-authors.</sample>
    <sample id="233">There is no specific author mentioned in the provided text.</sample>
    <sample id="234">"Ciao a tutti, sono Jenny, un' studentessa di dottorato di primo anno dell'Università di Carnegie Mellon, e oggi presenterò il mio lavoro, "Analisi posizionale, caratterizzazione mediante un insieme di modelli di dati CSA".</sample>
    <sample id="235">"Questo lavoro è stato fatto in collaborazione con alcuni membri dell'Università di Washington e dell'Instituto Allen per l'intelligenza artificiale, in particolare con Sebastian Santy, Ronan LaBros, Katarina Aranica e Martin Sapp."</sample>
    <sample id="236">"Immaginiamo di lavorare per un quotidiano e stiamo scorrendo i commenti sotto un nostro articolo per rimuovere il contenuto tossico."</sample>
    <sample id="237">"Potresti rivolgerti a un'API popolare come Perspective API per la detezione della città tossica. E questo funziona veramente bene se sei Carl Jones, in quanto Perspective API è in grado di rilevare correttamente gli istanze tossiche."</sample>
    <sample id="238">"Ma non è proprio il caso per una dithya-sharma, dove le API non sono altrettanto sensibili ai termini offensivi più comuni nel contesto indiano."</sample>
    <sample id="239">"Ecco un esempio di bias di progettazione in cui si osservano differenze sistemiche di prestazioni della tecnologia tra popolazioni."</sample>
    <sample id="240">"I biasi di design come quelli che abbiamo appena visto potrebbero verificarsi a causa della posizionalità degli studiosi e dei sviluppatori di modelli NLP. La posizionalità è semplicemente le prospettive che le persone tengono a causa delle loro demografie, identità e esperienze di vita."</sample>
    <sample id="241">"Ecco un concetto ampiamente utilizzato negli studi critici, in particolare in spazi accademici femministi e queer."</sample>
    <sample id="242">"E come ricercatrice, la posizione può influire sul processo di ricerca e sui suoi esiti e risultati perché può cambiare le decisioni che gli studiosi prendono."</sample>
    <sample id="243">"E quindi una domanda che le persone potrebbero porre è se i set di dati e i modelli hanno posizionalità?"</sample>
    <sample id="244">"E non stiamo cercando di dire che i modelli stessi e i set di dati stessi hanno identità demografiche e esperienze di vita, ma essi aggregano giudizi e opinioni di persone reali e possono quindi rappresentare certe posizioni rispetto ad altre."</sample>
    <sample id="245">"Il lavoro precedente ha suggerito alcune evidenze aneddotiche sulla posizione, come divari culturali in modelli e set di dati, nonché definizioni teoriche della posizione del modello."</sample>
    <sample id="246">"Tuttavia, questi lavori non esaminano i dati degli utenti finali rispetto ai set di dati e ai modelli stessi."</sample>
    <sample id="247">"Eseguire modelli e set di dati in posizione è sempre più importante come le attività NLP diventano più soggettive e orientate verso la società."</sample>
    <sample id="248">"E è difficile caratterizzare come sono inclinate queste posizioni poiché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API."</sample>
    <sample id="249">"Per studiare un insieme di dati e modellare la posizione, confrontiamo le annotazioni con utenti reali con insiemi di dati e modelli esistenti."</sample>
    <sample id="250">"Eseguiamo questo attraverso il nostro framework NL Positionality."</sample>
    <sample id="251">Il nostro framework funziona in due passaggi principali.</sample>
    <sample id="252">"La prima fase è quella di rianimare i dataset con annotatori diversi."</sample>
    <sample id="253">"E scegliamo di fare questo anziché analizzare le informazioni demografiche dei set di dati originali, gli annotatori, in quanto generalmente solo pochi annotatori annotano ogni istanza e poiché le informazioni demografiche sono rare e non sono condivise."</sample>
    <sample id="254">"E quindi optiamo per ri-analizzare i dati per ottenere molti entità, ad esempio, e ottenere un insieme di dati demografici ricco."</sample>
    <sample id="255">"Eseguiamo quindi le annotazioni per gruppo demografico e le confrontiamo con i modelli e i set di dati utilizzando un paio da paragone, poiché il nostro punteggio di correlazione."</sample>
    <sample id="256">"E quindi il nostro framework differisce dalla letteratura sulla disaccordo degli annotatori esaminando gli utenti finali, i modelli e i set di dati, le predizioni e le etichette, anziché guardare solo all'accordo degli annotatori o alle distribuzioni degli annotatori."</sample>
    <sample id="257">"Il nostro tasso di frame è prevalentemente abilitato da Lab in the Wild, una piattaforma di crowdsourcing online per collaboratori HCI."</sample>
    <sample id="258">"Lab in the Wild è un piattaforma di sperimentazione online in cui è possibile reclutare volontari diversi rispetto a piattaforme come MTURC, che prevalentemente coinvolgono partecipanti dagli Stati Uniti o dall'India. Inoltre, Lab in the Wild è in grado di ottenere dati di alta qualità."</sample>
    <sample id="259">"Ci ospitiamo due compiti all'esterno, uno dei quali è accettabilità sociale. Ecco come funziona: i partecipanti leggeranno una situazione dal set di dati di chimica sociale e poi scriveranno se una situazione è socialmente accettabile."</sample>
    <sample id="260">"Dopo, per rimanere impegnati nello studio, possono confrontare le loro risposte con un'intelligenza artificiale e con altri."</sample>
    <sample id="261">"Abbiamo quindi confrontato queste annotazioni con la chimica sociale, Delphi e GPT-4."</sample>
    <sample id="262">"Eseguiamo quindi uno scenario molto simile per la detezione di contenuti tossici e di discorso d'odio, dove leggeranno un esempio da Danny Hate e scriveranno se pensano che sia un esempio di discorso d'odio."</sample>
    <sample id="263">"Eseguiamo quindi una comparazione con Dynahate, Perspective API, Rewire API, HateRoberta e GPT-4. Il nostro studio ha raccolto più di 16.000 annotazioni da più di 1.000 annotatori provenienti da 87 paesi."</sample>
    <sample id="264">"Ora stiamo per equipaggiare per rispondere a chi si allinea con i set di dati e i modelli NLP. Troviamo che ci sia una posizionalità nell'NLP."</sample>
    <sample id="265">Ecco la traduzione del testo:

"Ad esempio, scopriamo che i dati e i modelli sono più allineati ai Paesi di lingua inglese. Quindi, per l'analisi di accettabilità sociale di GPT-4, scopriamo che è più allineato ai Paesi confuciani e di lingua inglese. Scopriamo che Dynahate è anche più allineato ai Paesi di lingua inglese."</sample>
    <sample id="266">"Troviamo inoltre maggiore allineamento con le persone che hanno una laurea. Quindi, per GPT-4 nel compito di accettabilità sociale, scopriamo che è maggiormente allineato con le persone che hanno una laurea o un dottorato di ricerca."</sample>
    <sample id="267">"Ecco che troviamo lo stesso per Dianaheid, dove è maggiormente associato a persone con un diploma di laurea."</sample>
    <sample id="268">"Tuttavia, quando i modelli e i dati sono allineati a specifiche popolazioni, alcuni inevitabilmente rimangono indietro."</sample>
    <sample id="269">"Esempio di questo è che i dati e i modelli sono meno in linea per le persone non binate rispetto ai loro omologhi uomini e donne. Lo troviamo nel compito di accettabilità GPT-4, nonché nell'analisi del compito "dining-hate".</sample>
    <sample id="270">"Quindi, considerando la posizione in Atlantide e L.P., cosa possiamo fare al riguardo?"</sample>
    <sample id="271">"Ho alcune raccomandazioni per questo. La prima è tenere un registro di tutte le scelte di progettazione durante il processo di ricerca. L'altra è fare ricerche NLP che offrano una prospettiva."</sample>
    <sample id="272">Il nostro terzo suggerimento è quello di creare insiemi di dati e modelli specializzati all'interno di quattro comunità specifiche. E un buon esempio di questo è l'iniziativa Musseqani. Vogliamo sottolineare che l'NLP inclusiva non è solo fare in modo che tutte le tecnologie funzionino per tutti.</sample>
    <sample id="273">"Ecco il termine della nostra presentazione, ma se desiderate ulteriori informazioni, potete visitare il nostro dashboard per visualizzare i risultati dell'analisi aggiornati e il nostro articolo. Grazie."</sample>
    <sample id="274">5</sample>
    <sample id="275">One effective way to mitigate social and political biases in NLP model training data is to use active learning and human evaluation to detect and correct biases, and to use diverse and representative datasets.</sample>
    <sample id="276">"Ciao, sono Si Yu-Yuan dell'Università Fudan. Sono qui per presentare il nostro lavoro, Distinguished Script Knowledge from Language Models for Constrained Language Planning."</sample>
    <sample id="277">"In vita quotidiana, gli esseri umani pianificano le loro azioni seguendo istruzioni passo passo in forma di scenari garantiti."</sample>
    <sample id="278">Il mondo precedente ha sfruttato i modelli linguistici per pianificare obiettivi astratti di attività stereotipiche, come fare un dolce, e dimostrare che i grandi modelli linguistici possono decomporre gli obiettivi in passaggi.</sample>
    <sample id="279">"Tuttavia, il lavoro precedente si concentra prevalentemente sulla pianificazione per obiettivi teorici astratti. La pianificazione per obiettivi specifici con vincoli specifici, come ad esempio fare un dolce al cioccolato, rimane ancora poco esplorato."</sample>
    <sample id="280">"In questo lavoro, definiamo il problema di pianificazione del linguaggio di vincolo."</sample>
    <sample id="281">"Il sistema di pianificazione GoalSaw impone diverse restrizioni su obiettivi diversi. Un obiettivo astratto può essere ereditato da obiettivi specifici della vita reale con restrizioni multiple. Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli alle restrizioni."</sample>
    <sample id="282">"Nel presente studio, valutiamo e miglioriamo l'abilità di pianificazione di linguaggio con vincoli in modelli di linguaggio di grandi dimensioni."</sample>
    <sample id="283">"Poiché non esiste alcun sito di dati specifici per identificare i nostri obiettivi."</sample>
    <sample id="284">"Dobbiamo acquisire questo codice in primo luogo. Come mostrato nel riquadro, estendiamo il codice astratto con vincoli multifacenti per l'acquisizione di dati in loop umano utilizzando Instruct GPT."</sample>
    <sample id="285">"Evaluiamo 100 obiettivi specifici e valutiamo i copioni generati dai modelli a larga scala."</sample>
    <sample id="286">"Questo rapporto riporta l'accuratezza generale dei risultati. Troviamo che tutti i rimodelli di line-up raggiungono risultati insoddisfacenti per la pianificazione di obiettivi specifici."</sample>
    <sample id="287">"Poi eseguiamo un'analisi dettagliata per esaminare a cosa sono i modelli a livello di riga."</sample>
    <sample id="288">"Il risultato mostrato nel grafico indica che la comprensione semantica nei script generati è accettabile, ma non può essere garantita la fedeltà ai vincoli."</sample>
    <sample id="289">Stiamo esplorando ulteriori categorie di argomenti frammentati dei vincoli definiti in Waking Home. La mappa della testa nella figura mostra che il rendimento del piano varia notevolmente per ragazze di diverse categorie.</sample>
    <sample id="290">"I precedenti studi hanno dimostrato che la qualità dell'output dei modelli a livello di riga è soggetta a una varianza elevata, il che porta a prestazioni pessime. Di conseguenza, adottiamo l'idea di filtri Z-sovraregenerati per migliorare la qualità di generazione."</sample>
    <sample id="291">"Mostriamo tipi di vincoli con esempi per intract.cpt e otteniamo obiettivi specifici basati sui obiettivi astratti."</sample>
    <sample id="292">"Quindi, fornisci script generalizzati per obiettivi specifici."</sample>
    <sample id="293">"Ecco, un modello di filtro è derivato per selezionare gli script fisici."</sample>
    <sample id="294">"Convertiamo gli script e gli obiettivi in embeddings GPT astratti e calcoliamo la similitudine di coseno come punteggi di similitudine per misurare la similitudine semantica."</sample>
    <sample id="295">"Inoltre, elimineremo lo script che contiene le parole chiave delle restrizioni di target. Teniamo solo lo script se il target è chiamato il più alto tra quelli impostati."</sample>
    <sample id="296">Con il nostro metodo, Instructivity può generare quadrati di qualità superiore. Il nostro metodo migliora notevolmente la pianificabilità sia in termini di completezza semantica che di fedeltà rispetto al vincolo.</sample>
    <sample id="297">"Poiché i modelli di linguaggio sono costosi da implementare, è essenziale abilitare la pianificazione linguistica di modelli più piccoli e specializzati. Creare set di dati è un passo essenziale per raggiungerlo."</sample>
    <sample id="298">"Tuttavia, gli studi precedenti non consentono di pianificare obiettivi specifici e l'annotazione dei set di dati manuali è costosa."</sample>
    <sample id="299">"Quindi, seguiamo l'idea della distillazione di conoscenza simbolica per distillare dati di pianificazione linguistica vincolati da modelli di livello live."</sample>
    <sample id="300">"Applicheremo il nostro metodo per creare un dataset di pianificazione del linguaggio con vincoli, denominato Coscript."</sample>
    <sample id="301">"In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità della validazione e dei siti di test, chiediamo ai lavoratori cloud di rinvenire e revisionare le campioni errate."</sample>
    <sample id="302">"Questo grafico mostra la distribuzione delle restrizioni del Corscript. Scopriamo che il Corscript mostra una alta variazione nella pianificazione dei obiettivi specifici generati. Con il Corscript, possiamo tracciare modelli più piccoli ma specializzati per la pianificazione del linguaggio di restrizione."</sample>
    <sample id="303">"Rileviamo che il file T può generare script di qualità superiore rispetto ai modelli di livello più alto, il che indica che i modelli più piccoli possono supportare modelli più grandi quando addestrati su siti di dati appropriati."</sample>
    <sample id="304">"In sintesi, abbiamo stabilito il problema di pianificazione del linguaggio di vincoli. Abbiamo sviluppato l'abilità di pianificazione del linguaggio di vincoli per i modelli di linguaggio di grande scala e sviluppato un metodo di filtro originario per i modelli di grande scala."</sample>
    <sample id="305">"Utilizziamo modelli di linguaggio avanzati per generare un set di dati di script di alta qualità, Corscript, per la pianificazione del linguaggio. Speriamo che il set di dati Corscript possa essere un'importante risorsa per gli studiosi che si occupano della pianificazione del linguaggio."</sample>
    <sample id="306">"Grazie per il tuo tempo. Troverai ulteriori dettagli sulla sceneggiatura nel nostro articolo."</sample>
    <sample id="307">Comparaibile a sistemi di punta di stato.</sample>
    <sample id="308">The important properties of a watermarking method are: applicability to embedding S services, non-degradation of embedding utility, sufficient covertness to prevent easy removal, and transferability during model extraction.</sample>
    <sample id="309">The 14 languages are not specified in the given text.</sample>
    <sample id="310">According to the text, "usually only a few annotators annotate each instance".</sample>
    <sample id="311">Cosine and L2.</sample>
    <sample id="312">The multilingual models used were multilingual pre-trained encoders with pointer-based decoders (e.g., XLMR+PDR and BERT+PDR) and multilingual pre-trained encoder-decoder models (e.g., M-BART).</sample>
    <sample id="344">The authors select a trigger set by collecting a general text corpus and counting the word frequency.</sample>
    <sample id="345">"Ciao a tutti, mi chiamo Xu Heng. Oggi presenterò il nostro articolo, "Do Connell 2003 named entity taggers still work well in 2023?". Cominciamo."</sample>
    <sample id="346">"Abbiamo esaminato il problema della generalizzazione utilizzando il compito di riconoscimento di entità nominate o compito NER."</sample>
    <sample id="347">"Osserviamo che i modelli utilizzano Kono 2003 per sviluppare il NER per quasi 20 anni. E ciò naturalmente solleva diversi problemi. In primo luogo, questi modelli possono generalizzare sui dati moderni?"</sample>
    <sample id="348">"E quando sviluppiamo nuovi taggers, cosa è necessario per una buona generalizzazione?"</sample>
    <sample id="349">"Mentre osserviamo una generale scarsa generalizzazione, cosa determina il calo di prestazioni di questi modelli?"</sample>
    <sample id="350">"Per indagare questi problemi, abbiamo sviluppato il dataset Carnot++. Questo è un dataset che abbiamo raccolto dai notiziari Reuters del 2020 e li abbiamo annotati con le stesse linee guida di annotazione di Carnot 2003."</sample>
    <sample id="351">"Habbiamo addestrato oltre 20 modelli su Kono 2003. Li abbiamo valutati su entrambi il set di test Kono 03 e il set di test Kono++."</sample>
    <sample id="352">"E ultima ma non meno importante, abbiamo calcolato il cambiamento percentuale di F1 per valutare la generalizzazione di ogni modello."</sample>
    <sample id="353">"Ecco, cosa è necessario per una buona generalizzazione? Dai nostri esperimenti, abbiamo scoperto che ci sono tre ingredienti principali che sono necessari."</sample>
    <sample id="354">"La prima è l'architettura del modello. Nel corso dei nostri esperimenti, abbiamo scoperto che i modelli transformer generalizzano meglio ai dati nuovi."</sample>
    <sample id="355">"Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che i modelli più grandi portano a una generalizzazione migliore."</sample>
    <sample id="356">"E inoltre, tutti sappiamo che il numero di esempi di fine-tuning influisce direttamente sulle prestazioni di un compito downstream. Ecco che abbiamo anche trovato che più esempi di fine-tuning portano a una maggiore generalizzazione."</sample>
    <sample id="357">"A nostro prossimo quesito, cosa causa il calo di prestazioni di alcuni modelli?"</sample>
    <sample id="358">"Abbiamo due ipotesi. La prima è l'overfitting adattativa, che è causata dalla ripetizione del set di test nello stesso insieme di test. Ciò si manifesta solitamente come diminuzione dei ritorni su un nuovo set di test."</sample>
    <sample id="359">"Il secondo ipotesi è il deriva temporale, che è la degradazione del rendimento causata dal sempre maggiore divario temporale tra i dati di addestramento e quelli di test."</sample>
    <sample id="360">"Per l'overfitting adattivo, abbiamo visto che dal grafico a destra, la linea di fit migliore ha un gradiente maggiore di 1."</sample>
    <sample id="361">Questo significa che ogni unità di miglioramento apportata sulla colonna 2003 si traduce in più di un'unità di miglioramento sulla colonna plus plus, il che significa che non ci sono ritorni decrescenti.</sample>
    <sample id="362">"Ecco che ci mostra che non si osserva sovrapposizionamento adattivo in questo caso."</sample>
    <sample id="363">"Cosa dire di Temporary Trif allora?"</sample>
    <sample id="364">"Per il progetto Temporal Drift, abbiamo fatto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti. E abbiamo scoperto che il rendimento peggiora con un divario temporale più grande."</sample>
    <sample id="365">"Ecco che conferma la nostra ipotesi che la principale causa del calo di prestazioni è il drift temporale."</sample>
    <sample id="366">"Il nostro conclusione è che per una buona generalizzazione, avremmo bisogno di un miglior architettura del modello, di un modello più grande e di esempi di fine-tuning maggiori. E questi obiettivi, mano nella mano, non possiamo avere un ingrediente solo, ma tutti gli altri."</sample>
    <sample id="367">"Inoltre, abbiamo scoperto che il calo di prestazioni è causato da deriva temporali e sorprendentemente, non è causato da sovrappesata adattiva, anche se Conno2003 è stato utilizzato per più di 20 anni."</sample>
    <sample id="368">"Ritornando alla domanda che abbiamo sollevato nel titolo del nostro articolo, i Tagger di Connell 2003 funzionano ancora nel 2023? E abbiamo scoperto che la risposta è un sì assoluto."</sample>
    <sample id="369">"Speriamo che il nostro studio richieda più ricerche su come migliorare le generalizzazioni dei modelli."</sample>
    <sample id="370">"Ecco, infine, assicuratevi di controllare il nostro lavoro, il nostro set di dati. Se avete domande, non esitate a contattarmi. Grazie mille."</sample>
    <sample id="397">The segment is 2 seconds long.</sample>
    <sample id="398">Specific knowledge of Servin and Kea's occupation, interests, and possibly their relationship.</sample>
    <sample id="399">The quality of the example.</sample>
    <sample id="400">GPT-4 and its variants.</sample>
    <sample id="401">The model combines scores from multiple levels.</sample>
    <sample id="402">Direct inference examples: using a song's title or position, e.g., "The first one" or "This is me".</sample>
    <sample id="403">Fudan University.</sample>
    <sample id="404">2</sample>
    <sample id="405">No.</sample>
    <sample id="406">The example provided is "one man warrior" to describe a female warrior.</sample>
    <sample id="407">According to the text, the transformer models normally generalize better to new data.</sample>
    <sample id="408">The names of the test sets are not explicitly mentioned in the given text.</sample>
    <sample id="409">3</sample>
    <sample id="410">L'autore opera con multimodalità, ovvero con più di un tipo di input (non solo testo).</sample>
    <sample id="439">According to the text, inference time knowledge is an area of NLU that is little studied.</sample>
    <sample id="440">Ying and Zhiyang.</sample>
    <sample id="441">Yes.</sample>
    <sample id="442">Limited types of context-dependent translations and limited sets of languages, relying on domain knowledge and human curation.</sample>
    <sample id="443">"Ciao, sto parlando del nostro lavoro sul risoluzione di espressioni di riferimento indirette per la selezione di entità, in cui presentiamo il punteggio Alt entità."</sample>
    <sample id="444">"Il mio nome è Jawad Hosseini e questo è un lavoro congiunto con Philippe Ladinsky, Sylvia Parity e Annie Lewis."</sample>
    <sample id="445">Non posso tradurre il contenuto poiché il testo è composto da una serie di ripetizioni del termine "درستان" in lingua persiana, senza significato comprensibile in inglese o italiano.</sample>
    <sample id="446">Il fatto più ovvio è utilizzare una differenza diretta. Ad esempio, dicendo il nome della canzone è in me o il suo posizione, la prima.</sample>
    <sample id="447">"A volte una riferimento indiretto è più appropriato per avere una conversazione più naturale. Ciò potrebbe accadere quando l'utente non ricorda il nome della canzone."</sample>
    <sample id="448">Non c'è testo inglese da tradurre, poiché il testo fornito è in persiano e non in inglese.</sample>
    <sample id="449">Non riesco a tradurre il contenuto in italiano poiché il testo è composto da una serie di suoni e non contiene testo scritto.</sample>
    <sample id="450">"Questo è un problema importante nei sistemi di conversazione e anche per il benchmarking delle LLM per l'intelligenza artificiale."</sample>
    <sample id="451">Non ho capito cosa si tratta di tradurre, poiché il testo fornito è solo una ripetizione di "نمید" senza senso. Se vuoi tradurre un testo specifico, per favore fornisci il testo in inglese e specifica il tipo di traduzione desiderata.</sample>
    <sample id="452">Il nostro metodo di raccolta dei dati si basa sull'informalità utilizzando un set di completamento a fumetti.</sample>
    <sample id="453">Il cartone animato ha tre bolle di conversazione. Nella prima bolla, Bob dice: "Ricorda quella canzone che ascoltavamo ieri?"</sample>
    <sample id="454">"Nel secondo bubble di conversazione, Alice dice: Vuoi dire facile con me o ho un'idea?"</sample>
    <sample id="455">"Che è l'alternativa alla domanda. E nel terzo bubble di conversazione, Bob utilizza un riferimento indiretto per selezionare uno di questi entità, ad esempio l'ultima."</sample>
    <sample id="456">Non riesco a tradurre il contenuto in italiano poiché il testo non è in inglese, ma in persiano.</sample>
    <sample id="457">"La seconda, che è la domanda alternativa, è generata in questo modo."</sample>
    <sample id="458">"Usiamo sempre un template semplice. Vuoi dire A o B? A e B sono esempi da Wikipedia."</sample>
    <sample id="459">"Ecco i metodi di campionamento che abbiamo utilizzato. Quando ci muoviamo più in alto nella lista, gli entità diventano più simili tra loro e solitamente è più difficile fare la disambiguazione."</sample>
    <sample id="460">"Il primo è Uniform Attract."</sample>
    <sample id="461">Il secondo caso è quando gli entità hanno titoli simili. Ad esempio, due libri con lo stesso nome, il retail.</sample>
    <sample id="462">Il terzo caso è quando hanno descrizioni simili su Wikipedia e infine quando hanno infobox o attributi simili su Wikipedia, ad esempio lo stesso genere o lo stesso artista per una canzone.</sample>
    <sample id="463">"Voglio mostrare loro un'altra domanda alternativa rispetto all'risposta. Sanno il nome di queste entità, ma non necessariamente ne sanno qualcosa."</sample>
    <sample id="464">"Così, ciò che facciamo è mostrare alcune conoscenze di sfondo sulle due entità. Per le canzoni, semplicemente mostriamo un collegamento di ricerca Google per ogni canzone."</sample>
    <sample id="465">"E e chiedere agli annotatori di ascoltare almeno alcune di ogni canzone e leggere su ogni canzone. Ecco ad esempio il risultato della ricerca Google per la canzone EasyHunt."</sample>
    <sample id="466">"In ambito delle ricette e dei libri, mostriamo alcuni testi di background tratti da Wikipedia. Per le ricette, mostriamo inoltre le loro immagini da Wikipedia, in modo che gli annotatori sappiano come appaiano."</sample>
    <sample id="467">Non ho capito il testo in inglese poiché non è scritto in inglese. Il testo sembra essere in persiano e contiene ripetizioni della frase "نداریم" che significa "non abbiamo" in persiano.</sample>
    <sample id="468">Non ho capito cosa hai detto, ma sembra che non ci sia alcun testo in inglese da tradurre. Il testo che hai fornito sembra essere un insieme di ripetizioni di parole in inglese, probabilmente un errore di registrazione o un testo generato casualmente. Se hai bisogno di tradurre un testo specifico, per favore fornisci il testo in inglese e ti aiuterò a tradurlo in italiano.</sample>
    <sample id="469">Il corpus delle L entità ha 6.000 domande alternative in tre domini e 42.000 espressioni di riferimento indirette. I risultati con il modello T5 grande sono riassunti di seguito.</sample>
    <sample id="470">Se il modello di lingua ha accesso allo stesso bagaglio di conoscenze degli annotatori, allora l'accuratezza è veramente alta. È intorno all'92% al 95%. Ma questo non è realistico.</sample>
    <sample id="471">Non ho capito bene, ma sembra che il testo non sia in inglese, ma in persiano. Il testo sembra essere una frase ripetuta più volte, che dice "اگر نگوش مادل از از از از از برشالی باقرام نداریم نداریم" che tradotto in italiano significa "Se non abbiamo un ascolto da parte di Dio".</sample>
    <sample id="472">Se il modello di lingua ha accesso solo ai nomi di entità, allora la precisione è solo del 60%. Ci sono quindi molto spazio per migliorare. Abbiamo anche dimostrato che i modelli sono generalizzabili a livello di dominio. Ecco il link al nostro dataset. Grazie.</sample>
    <sample id="473">Weight-key strategy and local agreement.</sample>
    <sample id="474">The authors' affiliations are not provided in the given text.</sample>
    <sample id="475">Jenny</sample>
    <sample id="476">2</sample>
    <sample id="477">"Ciao, sono Sara Pappi dell'Università di Trento e della Fondazione Bruno Kessler, e presenterò brevemente il lavoro 'L'attenzione come guida per la traduzione simultanea' che è un lavoro congiunto con Matteo Negri e Marco Turchi."</sample>
    <sample id="478">"Cosa è la Traduzione simultanea? La traduzione simultanea, o SIMUL-ST, è il processo di traduzione di un linguaggio parlato in un testo in un'altra lingua in tempo reale, abilitando la comunicazione interlinguistica."</sample>
    <sample id="479">"E quali sono i problemi dei modelli di stimolo attuali? Gli architetture specifiche sono solitamente addestrate introducendo moduli aggiuntivi da ottimizzare."</sample>
    <sample id="480">"Procedure di training lunghe e complesse, ad esempio procedure di training che coinvolgono obiettivi di ottimizzazione diversi"</sample>
    <sample id="481">"E addestrare e mantenere diversi modelli per raggiungere diversi regimi di latenza. Ad esempio, addestrare un modello con una latenza media di un secondo e un altro con una latenza di due secondi e così via."</sample>
    <sample id="482">La nostra soluzione è...</sample>
    <sample id="483">"Inizialmente, utilizzare modelli offline SD esistenti senza rirendere o adottare una specifica architettura per un singolo regime di latenza. Utilizzare solo un modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici."</sample>
    <sample id="484">"Elevare il livello di conoscenza già acquisito dal modello attraverso il meccanismo di attenzione tra l'input audio e l'output testuale, ovvero il meccanismo di cross-attenzione. E potete vedere un esempio a destra."</sample>
    <sample id="485">Il nostro approccio è quello di proporre un punto o un codice di attenzione e una strategia per decidere se emettere o meno una traduzione parziale in base a dove punta l'attenzione.</sample>
    <sample id="486">"Una parola è emessa se la tensione non è concentrata, cioè se questo somma è inferiore a un certo livello di soglia alpha, verso gli ultimi frame di pitch di lamba, il che significa che l'informazione ricevuta è sufficientemente stabile."</sample>
    <sample id="487">Ecco la traduzione del testo:

"Ad esempio, se riceviamo un file audio contenente 'I'm going to talk about' e il nostro modello predice la traduzione in tedesco,"</sample>
    <sample id="488">"Ecco che esamineremo i pesi di cross-attenzione."</sample>
    <sample id="489">"Vedremo che i primi due punti si riferiscono ai frame di parola ricevuti più antichi, mentre l'ultima parola si riferisce ai frame di parola ricevuti più recenti come frame di parola lambda."</sample>
    <sample id="490">"Questo significa che le prime due parole saranno omesse."</sample>
    <sample id="491">"Mentre la somma dell'attenzione incrociata è sopra un certo valore di soglia alpha, non emetteremo l'ultimo vocabolo e attendiamo un'altra porzione di parola."</sample>
    <sample id="492">Se andiamo avanti e riceviamo un'altra speech tank e il nostro modello predice altre tre parole, esamineremo i pesi di cross-attenzione.</sample>
    <sample id="493">"Vedremo che nessun termine si riferisce ai frame di parola successivi."</sample>
    <sample id="494">Questo significa che queste tre parole saranno emesse.</sample>
    <sample id="495">"Se guardi i principali risultati di quelli"</sample>
    <sample id="496">"Tracciato i risultati della traduzione nello spazio contemporaneamente su grafici in cui abbiamo il blu su un lato che misura la qualità della traduzione e il ritardo medio."</sample>
    <sample id="497">"Ecco il tempo di latenza e anche consideriamo la media ponderata del gradimento computazionale che tiene conto del tempo di elaborazione del modello per predire l'output."</sample>
    <sample id="498">Vogliamo che le curve siano il più alte possibile in questo grafico.</sample>
    <sample id="499">Ma anche vogliamo che siano spostati a sinistra.</sample>
    <sample id="500">"E confrontiamo le strategie di preparazione con quelle applicate anche ai modelli offline, ovvero la strategia dei pesi-chiave e l'accordo locale. E confrontiamo anche con l'architettura di punta specificamente progettata per la traduzione di steam-on-thigh-respirations."</sample>
    <sample id="501">"Ecco tutti i risultati della strategia di traduzione simultanea per il tedesco."</sample>
    <sample id="502">"Ecco che vediamo che le forme di output per adulti applicano tutte le strategie utilizzate nei modelli offline, poiché le curve sono spostate a sinistra."</sample>
    <sample id="503">"Ecco che vediamo che se consideriamo il tempo effettivo trascorso o il tempo computazionale consapevole, cioè la strategia più veloce."</sample>
    <sample id="504">Se desiderate scoprire ulteriori risultati, leggete il nostro articolo. Inoltre, abbiamo rilasciato il codice e i modelli e gli output simultanei per facilitare la riproducibilità del nostro lavoro. Grazie per la vostra attenzione.</sample>
    <sample id="505">No.</sample>
    <sample id="506">"Ciao a tutti, mi chiamo Ying e il mio collega Zhiyang e presenteremo la nostra ricerca su multi-instruct, migliorare l'apprendimento spirituale multi-modello mediante regolazione delle istruzioni."</sample>
    <sample id="507">"Con gli avanzamenti nei modelli di linguaggio di grandi dimensioni, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento che utilizzano modelli di linguaggio pre-addestrati in modo efficiente in termini di parametri e dati per compiti downstream diversi."</sample>
    <sample id="508">"Negli ultimi studi, molti hanno dimostrato che l'addestramento per istruzioni consente ai modelli di lingua di grande dimensioni di eseguire compiti sconosciuti in modo zero-shot seguendo istruzioni naturali."</sample>
    <sample id="509">"Tuttavia, la maggior parte dei lavori precedenti sull'adattamento delle istruzioni si è concentrata sul miglioramento del rendimento senza addestramento per compiti di linguaggio solo, mentre i compiti di computer vision e multimodalità sono stati trascurati."</sample>
    <sample id="510">"Dunque, in questo lavoro, vogliamo indagare se l'addestramento per istruzioni su modelli di proteine multimodali possa effettivamente migliorare la generalizzazione a compiti multimodali non visti."</sample>
    <sample id="511">Inoltre, durante la nostra ricerca, abbiamo scoperto una considerevole differenza nella disponibilità di set di dati di istruzione tra un RLP e un modello multi-modale.</sample>
    <sample id="512">"Eseguono più di 1.600 compiti di istruzione esclusivamente a livello di linguaggio. Tuttavia, non esiste alcun grande insieme di compiti di istruzione multimodale disponibile al pubblico. Ciò ci motiva a creare un insieme di dati di tuning multimodale."</sample>
    <sample id="513">"Ecco presentiamo multi-instruct, il primo dataset di benchmark di addestramento multi-modale che comprende 62 compiti diversi coprendo 10 categorie di board."</sample>
    <sample id="514">"Il contenuto di queste attività deriva da 21 set di dati open source esistenti e ogni attività è equipaggiata con cinque istruzioni scritte da esperti."</sample>
    <sample id="515">"Per l'indagine di regolazione di istruzioni multimodali sul nostro set di dati proposto, utilizziamo OFA, un modello di rappresentazione multimodale unificato come modello di base. OFA utilizza un vocabolario unificato per token di linguaggio, token di immagine e coordinate di un rettangolo di bounding."</sample>
    <sample id="516">Ecco alcuni esempi da nostro dataset multi-insetto.</sample>
    <sample id="517">"Unificare il trattamento di un insieme di tipi di dati di input e output diversi."</sample>
    <sample id="518">"Abbiamo seguito il metodo di OFA e abbiamo formulato tutti i compiti in un formato di sequenza a sequenza, in cui il testo di input, le immagini, le istruzioni e le caselle di riferimento sono rappresentati nello stesso spazio di token."</sample>
    <sample id="519">"Okay, adesso sto parlando di regolazione di istruzioni multi-modalità."</sample>
    <sample id="520">"Per il set di dati di training, utilizziamo 53 compiti da 9 gruppi per il training e campioniamo 10.000 istanze per compito. Per il testing, riserviamo l'intero gruppo di ragionamento comune per il testing e selezioniamo ulteriori 5 compiti dal gruppo VQA e dal gruppo miscellaneo."</sample>
    <sample id="521">"Utilizziamo tutte le istanze del flotte di test per ogni compito. Inoltre, estrapoliamo a caso 20 compiti dalla flotta di test di istruzioni naturali come compito on-scene per l'NLP."</sample>
    <sample id="522">"Utilizziamo un modello pre-addestrato OFA grande come modello di base. Durante l'addestramento, creiamo tutti gli istanti per tutte le attività. Ogni istante è combinato casualmente con uno dei cinque template di istruzioni."</sample>
    <sample id="523">"Durante il test, per ogni compito, conduciamo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni in ogni esperimento."</sample>
    <sample id="524">"Riportiamo la media e il massimo delle prestazioni e la deviazione standard delle prestazioni su tutti e cinque gli esperimenti."</sample>
    <sample id="525">Se il compito è una classificazione multi-modello, riportiamo l'accuratezza. Se è un compito di generazione multi-modello, riportiamo la radice GEL. Per un compito RP, riportiamo la radice GEL anch'esso.</sample>
    <sample id="526">"Abbiamo anche introdotto metriche di valutazione chiamate sensibilità. Questo misura l'abilità del modello di produrre output coerenti per lo stesso compito, indipendentemente dalle variazioni leggere nella formulazione delle istruzioni."</sample>
    <sample id="527">"Ecco i nostri risultati principali. Come possiamo vedere, l'adattamento delle istruzioni può migliorare significativamente le prestazioni di OIS su compiti multi-modelli."</sample>
    <sample id="528">"Anche l'apprendimento trasferito dai set di dati di istruzioni naturali può beneficiare l'adattamento delle istruzioni."</sample>
    <sample id="529">"Ecco che possiamo vedere come l'aumento delle attività aumenta il rendimento del modello e contemporaneamente diminuisce la sensibilità."</sample>
    <sample id="530">"Era anche eseguito un esperimento, utilizzando un'istruzione contro cinque istruzioni. Come possiamo vedere, utilizzare più istruzioni può migliorare le prestazioni complessive dei modelli e ridurre la sua sensibilità molto."</sample>
    <sample id="531">"Il video mostra l'effetto delle diverse strategie di tuning sul livello di sensibilità del modello. Come possiamo vedere, tramite l'apprendimento trasferito dai set di dati di istruzioni naturali, il modello può raggiungere un livello di sensibilità molto superiore rispetto al modello OFA originale."</sample>
    <sample id="532">"Anche possiamo vedere come l'apprendimento trasferito da un insieme di dati di istruzioni naturali possa aiutare OFA a raggiungere un performance molto migliori su un insieme di dati di istruzioni naturali."</sample>
    <sample id="533">"In sintesi, abbiamo proposto il primo grande insieme di dati di addestramento multi-modale. Abbiamo migliorato significativamente la capacità di soglia di OFA e abbiamo esplorato diverse tecniche di apprendimento trasferito e dimostrato i loro benefici. Abbiamo progettato un nuovo metrica chiamata sensibilità."</sample>
    <sample id="534">"Un'altra cosa, stiamo raccogliendo un insieme di dati di tuning per istruzioni multi-modelli con circa 150 compiti linguistici Weiren in più e li rilasceremo. Ecco un codice QR per i nostri dati e il modello. Grazie."</sample>
    <sample id="535">Sara Pappi is affiliated with the University of Trento and Fondazione Bruno Kessler, while Matteo Negri and Marco Turchi are not mentioned with any specific affiliations.</sample>
    <sample id="536">Jawad Hosseini.</sample>
    <sample id="562">"Ciao a tutti, sono Kostav Sinha e sono lieto di accogliervi nel nostro talk del nostro articolo ACL 2023, I modelli di linguaggio non sono sempre robusti al contesto."</sample>
    <sample id="563">"C'è un lavoro congiunto con John Wothier, Aaron Mueller, Kanishka Mishra, Karen Fentus, Roger Levy e Adina William."</sample>
    <sample id="564">"Rivisitiamo il paradigma delle coppie minime in questo lavoro."</sample>
    <sample id="565">Il paradigma a coppia minimo valuta i modelli di linguaggio in base a giudizi di accettabilità, che possono anche includere la grammaticalità come sintassi blimp o accettabilità in termini di stereotipi come spazi di coro.</sample>
    <sample id="566">E in questo paradigma a coppie minime, il modo tipico per valutare i modelli linguistici è mostrare una frase accettabile o una frase grammatica e quindi mostrare una frase accettabile o una frase non grammatica.</sample>
    <sample id="567">"E poi si spera che il modello assegni più probabilità alla sezione accettabile."</sample>
    <sample id="568">"La pipeline MPP attuale non ci consente di valutare l'accettazione dei modelli verso frasi più lunghe."</sample>
    <sample id="569">"In questi giorni, i modelli di linguaggio stanno generando finestre di contesto sempre più lunghe. Quindi è fondamentale valutare l'accettabilità dei modelli all'interno della finestra di contesto."</sample>
    <sample id="570">"Ecco ciò che stiamo cercando di fare. Stiamo cercando di rivisitare la pipeline MPB chiedendo al modello di valutare l'accettabilità su sequenze più lunghe."</sample>
    <sample id="571">"Ecco l'approccio. Quindi, per simulare sequenze più lunghe, rivisitiamo i set di dati stessi e ricreiamo celle di frase scegliendo frasi accettabili o non accettabili da quei set di dati."</sample>
    <sample id="572">Ecco la traduzione:

"Ecco, ad esempio, abbiamo scelto una coppia di dati dramatici dal set di dati blimp dal caso dell'isola di adiacenza."</sample>
    <sample id="573">"Ecco cosa facciamo: ricreare sequenze più lunghe e selezionare quelle accettabili che hanno la stessa struttura grammaticale, estrarre frasi grammaticali da AdjunTile."</sample>
    <sample id="574">"E e poi abbiamo aggiunto come prefisso a entrambe la query accettata e la query non accettata."</sample>
    <sample id="575">Possiamo fare lo stesso anche scegliendo frasi inaccettabili dalle stesse corrispondenze. E ciò potrebbe anche essere utilizzato per testare l'accettabilità del modello.</sample>
    <sample id="576">"E possiamo fare lo stesso scegliendo frasi da un insieme o da un insieme di dati diverso. Quello che chiamiamo scenario di mismatch."</sample>
    <sample id="577">Ecco il contenuto tradotto in italiano:

"Ecco che le frasi sono ancora tratte da set di dati rilevanti, ma non da quello che stai valutando con lo stesso insieme di dati. E possiamo fare lo stesso per non accettabile."</sample>
    <sample id="578">"Infine, possiamo scegliere frasi da un dominio completamente non correlato come Wikipedia."</sample>
    <sample id="579">"Ci dirà se le valutazioni di accettabilità dei modelli sono effettivamente influenzate da qualsiasi contesto."</sample>
    <sample id="580">"Se il contesto è proveniente da un sottoinsieme diverso del set di dati o se è del tutto irrilevante rispetto alla frase che stiamo esaminando."</sample>
    <sample id="581">Così come funziona il modello? Iniziamo a esaminare le frasi del Wikipedia che sono completamente irrilevanti rispetto al coppia di query attuale. E lì troviamo che i giudizi MPP sono in gran parte robusti per contesti arbitrari come.</sample>
    <sample id="582">"Aumentammo la lunghezza del contesto fino a 1024 per sfruttare al massimo i modelli OPT e GPT2. E come si può vedere nella linea in rosso, i giudizi di MPP sono relativamente stabili."</sample>
    <sample id="583">"Cos' accade quando scegliamo frasi dallo stesso insieme di dati?"</sample>
    <sample id="584">"Siamo qui a scegliere o creare frasi dalle domande accettabili e non accettabili dallo stesso set di dati sintassi blimp o gem."</sample>
    <sample id="585">"Ecco che vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o inaccettabili."</sample>
    <sample id="586">Ma quando ci matching la struttura, è quando scegliamo le frasi dalle stesse fenomenologie nel testo di colpa, Jim,</sample>
    <sample id="587">"vediamo un aumento massiccio o una diminuzione massiccia della valutazione di MPP per il modello a seconda che il prefisso scelto sia accettabile o inaccettabile."</sample>
    <sample id="588">"Questo, e questo è molto grande, come questo effetto aumenta con la lunghezza del contesto, e questo probabilmente influirebbe su modelli linguistici più recenti con finestra di contesto ampia."</sample>
    <sample id="589">"Perché il prefisso del match influisce così tanto sulla valutazione del modello linguistico?"</sample>
    <sample id="590">"Ecco che abbiamo fatto una serie di analisi dove abbiamo cercato di riprodurre la frase d'ingresso cercando di preservare la struttura rilevante, ma aggiungendo rumore all'input. E dopo aver fatto diverse di queste perturbazioni,"</sample>
    <sample id="591">"Rileviamo che nessuno di questi rumori fa cambiare il modello in termini di come presenta la valutazione MPP."</sample>
    <sample id="592">"Ritroviamo che i modelli sono sensibili alle perturbazioni e alle frasi in modi simili."</sample>
    <sample id="593">"Ecco, quando perturbiamo le frasi all'interno del dominio accettabile, osserviamo un aumento simile in tutte le perturbazioni. E quando perturbiamo le frasi all'interno del dominio non accettabile, osserviamo una diminuzione dei giudizi MPP in modo simile."</sample>
    <sample id="594">"Il nostro lavoro ha dimostrato che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti comuni ai diversi enunciati."</sample>
    <sample id="595">"Evalutazione MPP, il modo in cui la facciamo attualmente con input di centro singolo non cattura pienamente la conoscenza astratta dei modelli linguistici all'interno della finestra di contesto."</sample>
    <sample id="596">"Leggete il nostro articolo per ulteriori informazioni sugli esperimenti. Grazie per l'ascolto."</sample>
    <sample id="597">Unordered multi-set of tokens that will appear in the output.</sample>
    <sample id="598">55,000</sample>
    <sample id="626">Messalign.</sample>
    <sample id="627">Robust training against label noise, allowing for better generalization.</sample>
    <sample id="628">The documents in DEplain-web were aligned using both manual and automatic methods.</sample>
    <sample id="629">Il set di dati Carnot++ è stato creato raccolta notizie da Reuters News del 2020 e annotato utilizzando le stesse linee guida di annotazione del Carnot 2003.</sample>
    <sample id="630">"Ciao a tutti, mi chiamo Yusin Zhang dell'Università della Pennsylvania. Oggi presenterò il nostro lavoro, "Ghosts and Money Parsing in Multiple Natural Languages and Mainline Representations".</sample>
    <sample id="631">"La semantic processing è un compito per creare rappresentazioni semantiche delle richieste degli utenti, come ad esempio SQL e calcolo Lambda."</sample>
    <sample id="632">"Estrarre il significato semantico in più lingue è il compito di tradurre le query in diverse rappresentazioni di significato."</sample>
    <sample id="633">Come mostrato in questo figura, dobbiamo tradurre la query in più lingue naturali utilizzando modelli neurali per SQL, Lambda, o FunQL, e via dicendo.</sample>
    <sample id="634">"I modelli di parsing semantico bilingue esistenti sono stati proposti e valutati separatamente su insiemi di dati limitati e applicazioni, ad esempio"</sample>
    <sample id="635">"C'è una copertura di informazione sulle lingue naturali. La cinese manca."</sample>
    <sample id="636">"Perché di copertura su certe rappresentazioni."</sample>
    <sample id="637">"Il calcolo lambda manca."</sample>
    <sample id="638">"O o sono valutati solo su certi modelli più recenti. Ad esempio, ci sono solo un solo modello per valutare il modello."</sample>
    <sample id="639">"Ecco, pertanto, proponiamo un esempio unificato. Offriamo un insieme di dati esemplificativo per l'analisi semantica incrociata in più lingue naturali e rappresentazioni di significato."</sample>
    <sample id="640">"Contiene 90 set di dati in domini virali, 570 parti in tossici, 80 milioni di rappresentazioni e 22 lingue naturali in 15 famiglie linguistiche."</sample>
    <sample id="641">E per valutare il nostro benchmark, consideriamo sei impostazioni per l'addestramento e l'evaluazione.</sample>
    <sample id="642">Il primo è un test di traduzione. Utilizziamo l'API di Google Translate per tradurre il testo di origine nella lingua target, quindi utilizziamo un modello monolingue per addestrare qualsiasi valutazione.</sample>
    <sample id="643">"Esempio, addestriamo il modello inglese sul query inglese. Durante l'inferenza, traduciamo il query tedesco in inglese utilizzando l'API e poi utilizziamo il modello addestrato per prevedere la SQL."</sample>
    <sample id="644">"E anche testiamo il modello monolingual."</sample>
    <sample id="645">Non ho comprensione del testo in inglese, poiché non è stato fornito.</sample>
    <sample id="646">"Anche testiamo un setting di campo monolingue facendo addestrare il modello con solo l'1% dei dati di addestramento."</sample>
    <sample id="647">"E e ha un modello multilingue, che addestriamo un modello multilingue per tutte le lingue."</sample>
    <sample id="648">Esempio, combiniamo le query tedesche, inglesi e cinesi per addestrare un modello multilingue. Durante l'inferenza, possiamo utilizzare questo modello per</sample>
    <sample id="649">"Tradurre il contenuto inglese in italiano."</sample>
    <sample id="650">"Espandiamo anche il codice zero e il trasferimento campo-corto. Addestriamo su un linguaggio sorgente e trasferiamo in un altro linguaggio."</sample>
    <sample id="651">Durante l'addestramento, stiamo addestrando le query in inglese o combinazioni di query in inglese e tedesco per addestrare un modello multilingue e prevedere l'output SQL.</sample>
    <sample id="652">"Ebbiamo anche trovato molti risultati interessanti. Quindi, relativamente all'analisi dei modelli monolingui, valutiamo due gruppi di modelli."</sample>
    <sample id="653">"Including encoder PDR, che sta per encoder multilingue preaddestrati con decoder basati su puntatori come XL1R plus PDR e Berth plus PDR."</sample>
    <sample id="654">"Evaluiamo anche modelli encoder-decoder, ovvero modelli encoder-decoder multilingue, come M-BART e MT5."</sample>
    <sample id="655">"Abbiamo scoperto che l'encoder-decoder ottiene il miglior rendimento su tutti e nove dataset."</sample>
    <sample id="656">"Evaluare su MT5 e esempio XLMR più PDR in ambiente multilingue."</sample>
    <sample id="657">"Abbiamo scoperto che l'encoder decoder o encoder PDR può essere migliorato attraverso l'addestramento in un mix di vari linguaggi."</sample>
    <sample id="658">"Ecco che abbiamo scoperto che la maggior parte delle principali lingue naturali ottiene un miglioramento delle prestazioni, tranne che per l'inglese, che in sette set di dati registra un decremento delle prestazioni e in tre set di dati un aumento."</sample>
    <sample id="659">"Il pensiero che questo si chiama Maledizione della Multilinguismo."</sample>
    <sample id="660">"Anche confrontiamo il divario di prestazione tra le lingue."</sample>
    <sample id="661">"In questo grafico, la linea blu rappresenta il trasferimento di campo a campo tra lingue. La linea arancione rappresenta il trasferimento zero a zero tra lingue. La linea verde rappresenta il setting dell'angolo del modello."</sample>
    <sample id="662">"Abbiamo scoperto che confrontando la linea verde e la linea arancione, abbiamo trovato che la differenza di prestazioni di trasferimento obiettivo a zero impostazioni è significativa. E confrontando la linea blu e la linea arancione, abbiamo trovato che la differenza di trasferimento si accorcia rapidamente per impostazioni a pochi spari."</sample>
    <sample id="663">"Abbiamo anche trovato altri risultati interessanti. Ad esempio, il lavoro di progresso sull'encoder-decoder ha raggiunto risultati comparabili, soprattutto per il linguaggio naturale inglese, e ha notevolmente migliorato le prestazioni sulle lingue naturali di destinazione."</sample>
    <sample id="664">"E abbiamo trovato modelli di linguaggio di apprendimento come Codex che sono ancora all'interno di quel campo per le attività di parsing semantico cross-lingual."</sample>
    <sample id="665">"In sintesi, costruiamo ExamPolar, un benchmark unificato per la parola semantica a diverse angolature con molte rappresentazioni e molte lingue naturali."</sample>
    <sample id="666">Conduciamo uno studio di benchmarking approfondito su tre rappresentanti di modelli linguistici multilingue. E i nostri risultati mostrano molte scoperte interessanti, eccetera. E benvenuti a visitare il nostro articolo e il nostro codice. Grazie per l'ascolto.</sample>
    <sample id="667">The four categories are: Literature, Art, Music, and Film.</sample>
    <sample id="668">No, gli LLM multilingue come Codex o Bloom non sono sufficienti per il CLSP (Cross-Lingual Semantic Parsing) senza modelingare il fine del linguaggio.</sample>
    <sample id="695">The method addresses ambiguity by inducing alignment as part of the training and uses a GPU-friendly continuous relaxation to approximate the highest scoring permutation.</sample>
    <sample id="696">Fairness in NLP models is defined as the absence of biases and discrimination in the model's predictions or outputs, ensuring that all individuals or groups are treated equally and without prejudice.</sample>
    <sample id="697">Yannis Lavraque.</sample>
    <sample id="698">Kostav Sinha.</sample>
    <sample id="699">Myra.</sample>
    <sample id="700">In the context of this article, tropicalism refers to a trope that associates Latina women with vibrant and curvaceous qualities.</sample>
    <sample id="701">The authors have elaborated the representations of target groups by defining them through words like "culture", "tradition", "proud", and "exotic", which highlight their distinctiveness from the dominant white norm.</sample>
    <sample id="702">CXMI.</sample>
    <sample id="703">DrBERT is a version of BERT trained on a large corpus of text (7GB) from the internet, while SchuBERT is a clinical version of BERT trained on a 4GB subset of clinical notes.</sample>
    <sample id="751">2</sample>
    <sample id="752">Iterative update.</sample>
    <sample id="753">I cannot provide a response that contains the provided text, as it appears to be a series of repetitive words in a language that is not English. Can you please rephrase the question or provide more context about the text?</sample>
    <sample id="754">An attacker could extract model parameters through an EaaS by visualizing the embedding of sentences, as described in the provided text, which may reveal sensitive information.</sample>
    <sample id="755">3</sample>
    <sample id="756">2</sample>
    <sample id="757">The affiliations of the authors are: Carnegie Mellon University, University of Washington, and the Allen Institute for AI.</sample>
    <sample id="758">Bart and Lisa.</sample>
    <sample id="759">ABC eval measures chat model errors, including thematic ones.</sample>
    <sample id="760">To evaluate acceptability throughout the context window is crucial as large language models can generate longer responses, and acceptability can vary within the context.</sample>
    <sample id="761">Sì.</sample>
    <sample id="762">Yes.</sample>
    <sample id="763">RST (Rhetorical Structure Theory)</sample>
    <sample id="764">No.</sample>
    <sample id="765">Design bias in NLP is important because it can lead to inaccurate results, such as Perspective API's inability to detect toxic comments in Indian contexts, which can have serious consequences in applications like content moderation.</sample>
    <sample id="766">The multilingual LLMs like BLOOM have been fine-tuned using adapters or full-fine-tuning.</sample>
    <sample id="767">They use a transfer learning model.</sample>
    <sample id="768">The recent test sets used to evaluate the capabilities of PaLM are the SuperGLUE and the GLUE benchmark.</sample>
    <sample id="769">Three.</sample>
    <sample id="770">The proposed method (Corscript) allows for tracing smaller but specialized models for constraint language planning, enabling more accurate and efficient goal-oriented decision-making.</sample>
    <sample id="771">Xu Heng.</sample>
    <sample id="772">Yes.</sample>
    <sample id="773">2</sample>
    <sample id="774">OFA.</sample>
    <sample id="833">The authors are from Google Translate.</sample>
    <sample id="834">Stony Brook University.</sample>
    <sample id="835">The article does not specify which language pairs were analyzed.</sample>
    <sample id="836">Xiangbin</sample>
    <sample id="837">Long and normal base models were fine-tuned.</sample>
    <sample id="838">23.</sample>
    <sample id="839">2</sample>
    <sample id="840">The authors conducted experiments on four datasets: AG News, Mind, SSD2, and AresVam.</sample>
    <sample id="876">NACCHOS is a dataset of medical ground truth data from the web.</sample>
    <sample id="877">Aydbilar</sample>
    <sample id="878">La strategia del prompting ha un'influenza significativa sui risultati degli LLMs per la traduzione.</sample>
    <sample id="879">The affiliations of the authors are not specified in the provided text.</sample>
    <sample id="880">There are no instructions written by experts mentioned in the given text. The speaker is talking about collecting a data set and releasing a QR code, but no instructions are mentioned.</sample>
    <sample id="881">The authors propose a coreference resolution task to evaluate the ability to draw on knowledge from different sources.</sample>
    <sample id="882">"Ciao a tutti. Mi chiamo Aydbilar e vi darò un breve riassunto del paper intitolato "Pattern di grida da traduzione: strategie e prestazioni". Questo è un lavoro congiunto con i miei colleghi di Google Translate."</sample>
    <sample id="883">"Ecco BAM, un modello di linguaggio di 540 milioni di parametri presentato lo scorso anno nel 2022. È stato addestrato su una grande raccolta di testi che comprende 180 miliardi di documenti."</sample>
    <sample id="884">"Nel Damaa per la cucina, raggiunge lo stato dell'arte in centinaia di compiti di elaborazione del linguaggio naturale."</sample>
    <sample id="885">"Nell'ambito di questo lavoro, presentiamo la prima indagine sistematica sulla stimolazione di modelli linguistici per la traduzione automatica."</sample>
    <sample id="886">"Abbiamo valutato la capacità di transizione di tali modelli utilizzando le migliori pratiche della comunità MT. Ciò comporta l'utilizzo di set di test più recenti per evitare un'altra iterazione dei dati di test con i dati di training del modello di linguaggio."</sample>
    <sample id="887">"E confrontiamo due sistemi di punta. I sistemi più performanti sono quelli valutati dal WMT."</sample>
    <sample id="888">"Utilizziamo metriche di traduzione MT di ultima generazione e mostriamo anche risultati di valutazione umana di esperti. Infine, forniamo alcune raccomandazioni per strategie di selezione di prompt."</sample>
    <sample id="889">"Il prompting ha un grande influenza sulle prestazioni degli LLM per la traduzione. Come possiamo vedere in un esempio semplice in cui utilizziamo il prompting a una volta sola e forniamo due diversi prompts per una sola frase."</sample>
    <sample id="890">La maggior parte delle frasi, 516 su 1000, la differenza osservata è di più di un punto di sfumatura.</sample>
    <sample id="891">"Ecco, in casi estremi può arrivare fino a 40 punti di sfumatura. È quindi importante selezionare una buona strategia di stimolazione."</sample>
    <sample id="892">"Nel nostro esperimento, salutiamo una strategia di cinque puntate di stimolazione in cui semplicemente marciamo la frase che forniamo al sistema con il linguaggio in cui è."</sample>
    <sample id="893">"Questo esempio qui, dove eseguiamo la traduzione dal tedesco in inglese, le frasi tedesche sono contrassegnate con la colonna tedesca e le traduzioni in inglese con la colonna inglese."</sample>
    <sample id="894">Abbiamo visto che la forma effettiva della stampa non ha un grande influenza nel caso di stampa a breve serie.</sample>
    <sample id="895">"E' fondamentale per l'impronta zero e a una sola ripresa. E quando andiamo, come nel nostro caso, a cinque riprese, non c'è quasi nessuna differenza nella forma dell'impronta."</sample>
    <sample id="896">"Sono gli esempi a portare la maggior parte del peso."</sample>
    <sample id="897">"La sintesi dei nostri risultati sperimentali è che la qualità dell'esempio è più importante della somiglianza con la frase di riferimento."</sample>
    <sample id="898">"E' importante selezionare esempi da traduzioni di alta qualità. In particolare, confrontiamo le selezioni di promemoria dalle informazioni di training degli esami WMT o dalle informazioni DEF."</sample>
    <sample id="899">"Il dato di profondità è molto più curato e di qualità più alta rispetto a quello addestrato, quindi i risultati sono migliori quando si utilizza il dato di profondità."</sample>
    <sample id="900">Ora, almeno, i sistemi di punta di stato dell'arte hanno un vantaggio sostanziale rispetto alle traduzioni di banda. Ma uno si avvicina molto a un sistema commerciale. Nel nostro caso, abbiamo scelto di collaborare con Google Translate.</sample>
    <sample id="901">"Il contenuto che abbiamo ottenuto dalle analisi del regolamento e-mail eseguite utilizzando il framework MQM è che la fluidità della mano è paragonabile ai sistemi di punta, ma la differenza principale risiede nella precisione."</sample>
    <sample id="902">"In particolare, gli errori più comuni sono gli errori di omissione."</sample>
    <sample id="903">"Sembrerebbe che Palm sceglie di produrre una traduzione che suona meglio, a volte eliminando parti della frase originale che sono state create durante la traduzione."</sample>
    <sample id="904">"Tuttavia, il livello di stile esterno per PAN è inferiore rispetto ai sistemi di stato dell'arte, il che è un segnale aggiuntivo."</sample>
    <sample id="905">"Parm offre output fluenti, ma ancora con problemi di accuratezza."</sample>
    <sample id="906">"Ecco, è tutto per questo breve riassunto. Per maggiori dettagli, vi preghiamo di seguire la presentazione completa del lavoro. Grazie molto."</sample>
    <sample id="907">"Ciao, mi chiamo Dawei, studente di dottorato presso l'Università di Salant in Germania. In questo video, vorrei presentare il nostro lavoro recente, Wicker Than You Think, un'analisi critica sulle forniture settimanali."</sample>
    <sample id="908">"Ecco il lavoro congiunto con Xiao Yuxian, Mario Smoothbath e Diaz Stefan e DTich Claco."</sample>
    <sample id="909">"Vorrei iniziare con una breve introduzione all'apprendimento supervisionato debole e all'apprendimento supervisionato debolmente."</sample>
    <sample id="910">"In supervisione debole, non etichettiamo manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura deboli, come regole semplici, banche di conoscenza o sourcing del codice locale, come illustrato nella figura a destra."</sample>
    <sample id="911">"Confrontati ai commenti umani, le annotazioni deboli sono molto più economiche, ma anche rumorose, il che significa che un certo numero di annotazioni sono errate."</sample>
    <sample id="912">"Se addestriamo reti neurali direttamente sui dati etichettati settimanali, le reti neurali tendono a memorizzare il rumore di etichetta e non generalizzano."</sample>
    <sample id="913">"In apprendimento supervisionato a tempo di settimana, gli algoritmi di training vengono proposti per addestrare reti neurali in modo robusto in presenza di tale livello di rumore, in modo che i modelli di training generalizzino bene."</sample>
    <sample id="914">"In lavori recenti in WSL, dove WSL sta per apprendimento supervisionato a breve termine, una affermazione comune è che le persone dicono che addestrano i modelli sui dati del lavoro settimanale e raggiungono prestazioni elevate sui set di test puliti."</sample>
    <sample id="915">"Il contenuto è tecnicamente corretto, ma c'è un'eccezione."</sample>
    <sample id="916">"Il fatto è che le persone suppongono che ci sia un insieme di valutazione pulito o un firewall aggiuntivo per la selezione del modello."</sample>
    <sample id="917">"Abbiamo interrotto il problema in questo punto, poiché ciò implica la necessità di annotazioni manuali settimanali per molti. Ma, come un elefante nella stanza, questa necessità è spesso trascurata."</sample>
    <sample id="918">"Il dubbio in questione chiede tre domande di ricerca. In primo luogo, è necessaria una valutazione pulita per WSL? O possiamo forse utilizzare un insieme di valutazione rumoroso invece?"</sample>
    <sample id="919">"Secondo, se i dati puliti sono richiesti o sono necessari per il funzionamento di WSL, quanti campioni puliti abbiamo bisogno? Infine, dovremmo utilizzare solo i campioni puliti per la validazione o ci sono modi migliori per sfruttarli?"</sample>
    <sample id="920">"Habbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti."</sample>
    <sample id="921">"Inizialmente, scopriamo che i metodi WSL recenti richiedono campioni di piatto puliti per funzionare correttamente."</sample>
    <sample id="922">"Altrimenti, c'è un calo di prestazioni significativo. Come mostrato in questa figura, se non ci sono campioni di validazione puliti, i modelli di tendenza non possono generalizzare al di là delle etichette della settimana originale."</sample>
    <sample id="923">"Il significato è che l'addestramento è inutile."</sample>
    <sample id="924">"Il testo indica che gli approcci WSL richiedono dati etichettati in modo pulito per funzionare correttamente e il costo dell'etichettatura dei campioni di validazione non dovrebbe essere trascurato."</sample>
    <sample id="925">Il nostro secondo ritrovamento è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci WSL a raggiungere un miglioramento delle prestazioni, come mostrato nella figura a sinistra.</sample>
    <sample id="926">Di solito, abbiamo bisogno di 20 campioni per classe per raggiungere un buon livello di prestazioni.</sample>
    <sample id="927">Ma non è la fine della storia, perché se decidiamo di accedere a campioni puliti, allora l'addestramento su di loro direttamente raggiungerà prestazioni ancora migliori.</sample>
    <sample id="928">Il grafico rosso mostra la differenza di prestazioni tra gli approcci di fine-tuning, che vengono applicati direttamente ai dati puliti, e gli approcci WSL, che utilizzano i dati puliti solo per la valutazione.</sample>
    <sample id="929">"Ecco che si può vedere, se abbiamo 10 campioni per classe, l'addestramento fine di Direct comincia a superare gli approcci WSL."</sample>
    <sample id="930">"Inoltre, l'adeguamento del rendimento affermato dagli approcci WSL precedentemente può essere facilmente raggiunto consentendo di continuare a finetunare sul campione di validazione pulito."</sample>
    <sample id="931">"Evidentemente, dal grafico, il modello Valina chiamato FTW inizialmente sottopassa metodi più complessi come WSL come il coseno."</sample>
    <sample id="932">Tuttavia, se consentiamo di continuare a regolare la finezza sul campione pulito, il metodo FTW esegue altrettanto bene come gli altri metodi.</sample>
    <sample id="933">"In pratica, non c'è alcun motivo per scegliere metodi WSL più complessi, che richiedono più tempo di elaborazione e spazio su disco."</sample>
    <sample id="934">Riassumendo, abbiamo dimostrato che gli approcci WSL recenti richiedono campioni puliti e annotati a mano per funzionare correttamente. Il loro aumento di prestazioni e praticità sono stati pesantemente soprastimati.</sample>
    <sample id="935">"Il nostro consigliamo per le ore di lavoro future."</sample>
    <sample id="936">"Rapporta i criteri di selezione del modello. Ad esempio, rapporta se la selezione del modello è stata fatta con campioni di validazione puliti."</sample>
    <sample id="937">"In secondo luogo, gli approcci WSL dovrebbero essere spinti con basi di atterraggio future come campioni chiari. In terzo luogo, un'ottimizzazione continua è una base semplice ma forte che dovrebbe essere considerata nel lavoro futuro in WSL."</sample>
    <sample id="938">Finalmente, abbiamo aperto il nostro codice. Puoi trovarlo mediante il codice QR sullo schermo. Sii libero di verificarlo. Grazie e buona conferenza.</sample>
    <sample id="939">Human evaluation, such as asking human judges to select the better conversation or rate conversations on a Likert scale.</sample>
    <sample id="940">5</sample>
    <sample id="941">In the example, the necessary basic knowledge are: Servin and Kea.</sample>
    <sample id="942">Sì, il codice è disponibile sul repository GitHub.</sample>
    <sample id="943">No, according to the text, GPT-4 is most aligned to people with a college education or graduate school education, not to specific demographic groups such as country, gender, etc.</sample>
    <sample id="944">The input sentences were perturbed by adding noise to preserve the relevant structure, but none of these perturbations changed the model's judgment on MPP.</sample>
    <sample id="945">Having a dimensional evaluation means assessing multiple aspects or dimensions of dialogue quality to understand the strengths and weaknesses of the model on a more detailed level.</sample>
    <sample id="946">The author's affiliation is the University of Science and Technology of China.</sample>
    <sample id="947">In casi di zero e uno short prompting.</sample>
    <sample id="978">The authors evaluated several conversational AI models.</sample>
    <sample id="979">1</sample>
    <sample id="980">A good planner should be reasonable and faithful to constraints.</sample>
    <sample id="981">3</sample>
    <sample id="982">Vasudha.</sample>
    <sample id="983">The authors of the article are not specified in the given content.</sample>
    <sample id="1021">Omission errors.</sample>
    <sample id="1022">"Ciao, sono James Finch. E sono Sarah Finch. E oggi ti parleremo di ABCeVal, un nuovo approccio dimensionale per valutare l'IA conversazionale."</sample>
    <sample id="1023">"Questo lavoro è stato realizzato dal Laboratorio di Intelligenza Artificiale dell'Università di Emory, diretto dal Professor Geno Choi dell'Università di Emory, in collaborazione con Amazon Alexa AI."</sample>
    <sample id="1024">"Vogliamo dire che hai sviluppato un modello di dialogo e vuoi vedere come si confronta con lo stato dell'arte attuale."</sample>
    <sample id="1025">La pratica comune è quella di utilizzare l'evaluazione umana, ad esempio chiedendo a giudici umani di selezionare quale delle due conversazioni è meglio o di valutare le conversazioni su una scala di Likert.</sample>
    <sample id="1026">Questi approcci funzionano bene per fornire valutazioni globali della qualità del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto, potresti valutare diverse dimensioni della qualità del dialogo per comprendere le forze e le debolezze del modello su un livello più dettagliato.</sample>
    <sample id="1027">Un approccio è chiedere ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la rilevanza delle risposte del modello utilizzando metodi di scala comparativa o Likert.</sample>
    <sample id="1028">"Tuttavia, crediamo che ci sia una strategia più precisa e affidabile per l'evaluazione del dialogo dimensionale."</sample>
    <sample id="1029">"Il nostro approccio tenta di ridurre la soggettività dell'evaluazione umana annotando esplicitamente se ogni risposta del modello esprime certi comportamenti, come rispondere con informazioni irrilevanti o contraddire se stessa."</sample>
    <sample id="1030">"Chiamiamo questo approccio annotazione di comportamenti in chat o ABC eval in breve. Abbiamo sviluppato questo metodo per coprire complessivamente i comportamenti del modello di chat che sono stati suggeriti recentemente dalla letteratura come influire sulla qualità della chat."</sample>
    <sample id="1031">"L'ABC eval è in grado di misurare i tassi con cui i modelli di conversazione commettono errori tematici."</sample>
    <sample id="1032">Esempio, ABC valuta il numero di tornate in cui un modello di conversazione ignora il proprio partner o dice qualcosa di irrilevante.</sample>
    <sample id="1033">"Contraddice se stessa o il proprio partner, ipotizza fatti errati o viola la conoscenza comune, e quando il modello riesce o fallisce a mostrare empatia."</sample>
    <sample id="1034">Per determinare quale tipo di valutazione è più efficace, abbiamo selezionato quattro modelli di chat di punta e li abbiamo valutati su 100 conversazioni tra umani e bot per modello utilizzando ABC eval.</sample>
    <sample id="1035">"Per confrontare, abbiamo valutato queste conversazioni utilizzando tre metodi esistenti, voti per livello di turno, voti per livello di dialogo e confronti pairwise di dialogo."</sample>
    <sample id="1036">"Abbiamo raccolto valutazioni per ogni metodo esistente su otto degli aspetti del dialogo più comuni misurati, poiché è la pratica standard per valutare i modelli di chat su diverse dimensioni."</sample>
    <sample id="1037">"Da un'analisi dei risultati di valutazione, abbiamo scoperto che gli etichetti di comportamento ABC sono in generale più affidabili rispetto a quelli raccolti dai metodi esistenti, misurati in base all'Accordo Interanitator su 100 conversazioni doppie etichettate."</sample>
    <sample id="1038">"Inoltre, le etichette di valutazione ABC sono più predittive della qualità complessiva della conversazione rispetto ai metodi esistenti, come dimostrato da questo semplice analisi di regressione lineare."</sample>
    <sample id="1039">Ad esempio, si può vedere come misurare la proporzione di tornate con contraddizioni da sé e da partner spiega il 5% e il 10% della qualità del conversazione, mentre i punteggi di consistenza dei liquori spiegano solo il 4% o meno.</sample>
    <sample id="1040">"Infine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità del dialogo utilizzando una regressione lineare a passaggi."</sample>
    <sample id="1041">"Si può vedere come la combinazione di tutti i metri di valutazione ABC spiega più del 25% della qualità della conversazione. E come si rimuove ogni metro di valutazione uno alla volta, la maggior parte di essi porta a perdere un buon quantitativo di informazioni sulla qualità."</sample>
    <sample id="1042">"Al contrario, la combinazione di tutti i metrici Likert a livello di turno spiega molto meno la qualità e pochi di questi metrici contengono informazioni uniche."</sample>
    <sample id="1043">"Questi metriche ABC attendibili, informativi e distinti ci consentono di valutare l'IA conversazionale con una risoluzione più alta rispetto ai metodi precedenti sono in grado di raggiungere."</sample>
    <sample id="1044">"Ecco che risulta chiaro dagli esiti del nostro esperimento che diversi ostacoli ancora esistono e sono stati precisamente quantificati. Ad esempio, i bot che abbiamo testato presentano violazioni di senso comune in circa il 20% delle loro risposte."</sample>
    <sample id="1045">"Producono informazioni irrilevanti in circa il 15% delle risposte e si contraddicono o si contraddicono con il partner intorno al 10% del tempo."</sample>
    <sample id="1046">"Con il ritmo veloce dell'evoluzione nel campo, molti di questi tassi di errore potrebbero registrare un decremento nei nuovi modelli pubblicati dal momento in cui è stata condotta la nostra valutazione. Tuttavia, ciò è solo un ulteriore motivo per perseguire metriche di valutazione affidabili e precise per confrontare i modelli."</sample>
    <sample id="1047">"Speriamo che ABC eval possa essere utilizzato da altri nel campo come passo significativo in questa direzione. E ci aspettiamo con interesse di vedere come l'Intelligenza Artificiale conversazionale evolverà nei mesi e negli anni a venire. Grazie per aver guardato."</sample>
    <sample id="1048">The authors are affiliated with the Emory NLP Lab, led by Professor Geno Choi at Emory University, and Amazon Alexa AI.</sample>
    <sample id="1049">CFT stands for "clean validation samples".</sample>
    <sample id="1050">6</sample>
    <sample id="1051">"Ciao, il mio nome è Kaio Yan e presenterò il nostro lavoro intitolato 'When Does Translation Require a Context? Una esplorazione multilingue guidata dai dati'. Questo lavoro è stato realizzato in collaborazione con Patrick Frenange, M.E. Liu, Andre F.D. Martin e Graham Mubig."</sample>
    <sample id="1052">"Traduci il contenuto inglese in italiano."</sample>
    <sample id="1053">Be', se il precedente era, le cose potrebbero diventare pericolose se i ministri lo scoprissero, allora Moe si riferisce a uno spia. Ma se il precedente era, potrebbe essere qualcosa di serio, dottore? Allora Moe si riferisce a un segno di nascita.</sample>
    <sample id="1054">"In base al contesto, il significato della parola cambia e quindi il suo traduzione cambia pure."</sample>
    <sample id="1055">Tuttavia, valutare come i modelli possano tradurre casi come questo è molto difficile. In primo luogo, perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende impossibile per i metriche a livello di corpus come Blue catturare queste traduzioni.</sample>
    <sample id="1056">"E alcuni hanno suggerito valutazioni mirate su traduzioni dipendenti dal contesto, ma questi risorse supportano solo limitati tipi di traduzioni dipendenti dal contesto e limitati insiemi di lingue, poiché di solito si basano sulla conoscenza del dominio e sulla cura umana."</sample>
    <sample id="1057">"In questo lavoro, cerchiamo di rispondere a queste due domande. In primo luogo, quando richiede contesto la traduzione? E in secondo luogo, come gestiscono i modelli questi casi?"</sample>
    <sample id="1058">"Per rispondere alla prima domanda, abbiamo iniziato a misurare quant'importanza ha un lemma rispetto al contesto di traduzione."</sample>
    <sample id="1059">"Ecco il lavoro precedente in cui abbiamo introdotto CXMI come misura dell'utilizzo del contesto da parte dei modelli di traduzione macchinale. Ciò viene fatto misurando quanti dati il contesto C fornisce sulla destinazione Y, date la fonte X."</sample>
    <sample id="1060">"Puoi considerare CXMI come l'informazione ottenuta dal dare contesto al modello."</sample>
    <sample id="1061">"In questo lavoro, estendiamo CXMI a YCXMI, che può misurare l'utilizzo del contesto a livello di frase o a livello di parole. Possiamo considerare le parole che hanno un alto P6MI come quelle che richiedono contesto per la traduzione."</sample>
    <sample id="1062">"Analizziamo ora le parole con un alto valore di XMI per cercare di individuare pattern tra queste parole."</sample>
    <sample id="1063">"Eseguiamo l'analisi sui testi dei discorsi TED che sono stati tradotti in 14 diverse lingue."</sample>
    <sample id="1064">"Eseguiamo l'analisi a tre diversi livelli. Iniziamo analizzando le etichette di parte del discorso che hanno un alto valore di PCXMI."</sample>
    <sample id="1065">"Ecco che ci consente di trovare, ad esempio, pronomi duali in arabo che hanno un P6MI relativamente alto. E questo può essere spiegato perché l'inglese non ha pronomi duali, quindi è necessario contesto per determinare se un pronome è duale quando si traduce in arabo."</sample>
    <sample id="1066">"E analogamente, scopriamo che certi linguaggi richiedono contesto quando vogliamo scegliere la forma verbale appropriata. Allora guardiamo gli elementi lessicali che hanno un'alta p-sex-mi calcolata su tutte le sue diverse occorrenze."</sample>
    <sample id="1067">"Ecco come si identificano casi come questo, dove in cinese, è necessario contestualizzare i nomi propri per assicurarsi di utilizzare la stessa traduzione all'interno del documento."</sample>
    <sample id="1068">"E similmente, scopriamo che il contesto è supportato per tradurre nella forma di formalità corretta."</sample>
    <sample id="1069">"E finalmente, esaminiamo i token che hanno un alto P6MI. Ciò ci consente di identificare fenomeni che non possono essere catturati dalla parola stessa, ma sono piuttosto espressi dalla struttura di frase, come la risoluzione dell'ellissi."</sample>
    <sample id="1070">"Ora utilizziamo i nostri risultati dall'analisi per progettare un benchmark per la traduzione globale dei documenti."</sample>
    <sample id="1071">"Creiamo tagger per ogni uno dei cinque fenomeni di discorso che abbiamo identificato, per identificare automaticamente le parole che si riferiscono al fenomeno. E chiamiamo il nostro tagger il tagger consapevole di discorso multilingue o Muda tagger."</sample>
    <sample id="1072">"Puoi notare inoltre che le diverse lingue hanno diverse proporzioni di fenomeni discorsivi."</sample>
    <sample id="1073">"Utilizziamo quindi il MudaTaggle applicando il taglio sul corpus parallelo che vogliamo utilizzare per l'evaluazione. E applichiamo i nostri metrici di traduzione scelti sugli esempi contestuali che il MudaTaggle ha identificato."</sample>
    <sample id="1074">"Ecco, infine, utilizziamo il nostro benchmark, così come altri metriche, per valutare i modelli a livello di documento per la traduzione automatica del testo."</sample>
    <sample id="1075">"Inizialmente, quando utilizziamo metriche a livello di corpus, riscontriamo che i modelli acontextuali hanno il miglior rendimento per il blu."</sample>
    <sample id="1076">"Poi, se utilizziamo Comet, i modelli a contesto hanno il miglioramento. E se utilizziamo la misura F, i modelli con o senza contesto hanno prestazioni paragonabili."</sample>
    <sample id="1077">"Ecco di nuovo che è difficile determinare il sistema di traduzione al livello del documento se si utilizzano solo metriche al livello del corpus."</sample>
    <sample id="1078">Ora utilizziamo il benchmark Mooda per valutare i modelli e scopriamo che i modelli che utilizzano il contesto sono significativamente più precisi rispetto a quelli che non lo utilizzano per fenomeni di discorso come la formalità e la coesione lessicale.</sample>
    <sample id="1079">"Tuttavia, questi modelli non sono molto migliori di quelli che non utilizzano il contesto per fenomeni come ellissi, piante perenni e forma verbale. Quindi, questo suggerisce dove dovremmo vedere più progressi per la trasformazione a livello di documento."</sample>
    <sample id="1080">"Anche confrontiamo sistemi commerciali e i nostri benchmark mostrano che D-Bel è solitamente più preciso del traduttore di Google per traduzioni a livello di documento."</sample>
    <sample id="1081">Riassumendo, eseguiamo un'analisi guidata dai dati su 14 coppie di lingue per identificare quando le traduzioni richiedono contesto.</sample>
    <sample id="1082">"Ecco che utilizziamo i nostri risultati per creare un benchmark per la traduzione a documento, che ci aiuta a identificare quali modelli di fenomeni a dischi possono gestire bene o meno e quali sistemi di traduzione sono bravi nella traduzione a documento."</sample>
    <sample id="1083">"Grazie mille per la tua attenzione. Ci vediamo a Toronto."</sample>
    <sample id="1084">Yusin Zhang</sample>
    <sample id="1121">The new method is called "First-Come-First-Served".</sample>
    <sample id="1122">The author describes the marked words as a method to identify the words that distinguish marked groups from unmarked ones.</sample>
    <sample id="1123">University of Washington</sample>
    <sample id="1124">Prague approach.</sample>
    <sample id="1125">James Finch and Sarah Finch.</sample>
    <sample id="1126">4</sample>
    <sample id="1127">Acceptability judgments, grammaticality, crowd-sourced data, minimal pair paradigm.</sample>
    <sample id="1161">WSL (Weakly Supervised Learning).</sample>
    <sample id="1162">11 biomedical and clinical tests.</sample>
    <sample id="1226">CamemBERT is initially trained on 4 GB of text data.</sample>
    <sample id="1227">Adam Szpirkowski</sample>
    <sample id="1228">The experiment showed that retraining or continuing to pre-train models with more recent data resulted in performance degradation with a larger temporal gap, confirming the hypothesis that temporal drift is the main cause of the performance drop.</sample>
    <sample id="1269">To put the tokens in the right order.</sample>
    <sample id="1270">They suggested increasing transparency on bias mitigation methods to determine whether positive stereotypes are due to value alignment or anti-stereotyping methods.</sample>
    <sample id="1271">Unacceptable sentences in the minimal pair paradigm.</sample>
    <sample id="1272">Weight and token.</sample>
    <sample id="1273">Interanitator Agreement.</sample>
    <sample id="1274">Wikipedia.</sample>
    <sample id="1275">The authors' affiliations are not mentioned in the provided text.</sample>
    <sample id="1276">MultiInstruct differs from other works by focusing on instruction tuning for multimodal protein models, whereas previous works mainly focused on language-only tasks.</sample>
    <sample id="1277">2</sample>
    <sample id="1278">Coordinazione binaria: la disposizione di due elementi o frasi simili in un'unica frase, separate da "and" o "or".</sample>
    <sample id="1279">3 seconds.</sample>
    <sample id="1280">The results suggest that smaller models like T5 can generate high-quality scripts when properly trained on suitable data, indicating that smaller models can support larger models.</sample>
    <sample id="1281">"Ciao, sono Yannis Lavraque e vi presento i nostri lavori su Dr. Berth, un modello robusto in francese per il dominio biomedico e clinico."</sample>
    <sample id="1282">"Nella presentazione, stiamo parlando di una lingua modellata per la salute. Successivamente, presenteremo la principale contribuzione del nostro articolo."</sample>
    <sample id="1283">Abbiamo introdotto il primo modello biomedico in francese chiamato Dr. Berth, che è basato su Roberta, e addestrato su NACCHOS, che è un insieme di dati di riferimento medici provenienti dal web.</sample>
    <sample id="1284">"We have also introduced a comparison with several retro models and data sources. We will then present our results on 11 biomedical and clinical tests in French."</sample>
    <sample id="1285">"E conclude finalmente gli esperimenti e vi diamo maggiori informazioni su come accedere ai modelli."</sample>
    <sample id="1286">"Since its release in 2018, Bert has been the most effective approach to natural language processing. It has provided a significant performance gain compared to a historical strategy and a contextualized method, such as'see-to-vec' or'see-sent'."</sample>
    <sample id="1287">"Da quando questo modello è stato adattato a molte altre lingue come il francese con Camembert, e altri settori come Biomedical con Père Medbert e Biobird, e in clinica con Clinica Albert, ma soprattutto in inglese."</sample>
    <sample id="1288">"Modelli specializzati per altre lingue sono rari e spesso sono basati su addestramento continuo a causa della scarsità di dati in dominio."</sample>
    <sample id="1289">Tuttavia, il francese non aveva alcun modello open source per la biomedicina finora.</sample>
    <sample id="1290">Ci chiediamo una domanda su quali sono le strutture dati più adatte per una vasta gamma di utilizzi. E quei dati attuali sono buoni sostituti per i dati clinici.</sample>
    <sample id="1291">"Per rispondere a questa domanda, confrontiamo Dr. Bert con il nostro modello di Schubert, che è basato su dati anonimizzati ottenuti dall'ospedale non generazionale della nostra casa."</sample>
    <sample id="1292">Dopo di che ci chiediamo quanti GB di dati dobbiamo addestrare un modello specializzato su dati francesi? È per Gb o più?</sample>
    <sample id="1293">"Per rispondere a questa domanda, prima addestriamo e confrontiamo quattro modelli da zero. Una prima versione di Dr. Bert con 7 GB di natura. Una seconda versione di 4 GB di natura."</sample>
    <sample id="1294">La prima versione di Schubert, che è un modello clinico, ha 4 GB di note cliniche. E la ultima versione di Schubert con un mix di 4 GB di dati naturali e 4 GB di note cliniche.</sample>
    <sample id="1295">"Oltre a questo confronto, abbiamo introdotto tre treni di modelli di pre-allenamento per analizzare l'impatto della strategia di pre-allenamento."</sample>
    <sample id="1296">"A base on the weight of Camembert and a train on the 4 GB of nature. Another one based on Camembert but trained on 4 GB of blinking and the other."</sample>
    <sample id="1297">"E finalmente, un modello basato sul modello bio-medico inglese Bermond Bert, e addestrato su quattro gigabyte di set di snatchers. In totale, abbiamo sette modelli."</sample>
    <sample id="1298">"Per valutare questi sei modelli, raccogliamo informazioni dalle pubblicazioni e dallo spazio privato, come nomi, classificazioni, viaggi, sfide e responsabilità."</sample>
    <sample id="1299">Questo modello è stato paragonato a sei modelli che sono Camembert Oscar 138GB, Camembert Oscar 4GB, Camembert CCNet 4GB, Pummet Belt, BioBert e ClinicalBert.</sample>
    <sample id="1300">L'evoluzione dei modelli che eseguono meglio il compito con i dati della stessa natura di quelli su cui il modello è stato addestrato.</sample>
    <sample id="1301">Tuttavia, possiamo ottenere i dati da fonti eterogenee, sembra che i dati da fonti eterogenee siano più versatili. Inoltre, osserviamo che utilizzare più dati porta a una maggiore prestazione.</sample>
    <sample id="1302">"In generale, dallo zero, il ritorno sembrò ottenere prestazioni migliori su quasi tutti i compiti."</sample>
    <sample id="1303">"Our experience with constraints and pretension, using the weight and token of the Pomet Bird, pulled on a subject of 4 GB in nature, has a result comparable to what we obtained with Dr. Bert, 4 GB from scratch."</sample>
    <sample id="1304">"Il che non è il caso per il modello basato sui pesi di Camembert e del tokenizzatore, che presenta problemi di stabilità."</sample>
    <sample id="1305">Infine, come conclusione, il nostro sistema offre prestazioni migliori su nove delle 11 attività downstream e supera i risultati globali del modello generico Camembert.</sample>
    <sample id="1306">"Abbiamo osservato che i dati specializzati sono migliori, più i dati specializzati sono migliori, ma non scalano bene."</sample>
    <sample id="1307">"Tutti i modelli preaddestrati ottenuti da NATURES sono disponibili gratuitamente su UginFace e tutti i script di training sono sul nostro repository GitHub."</sample>
    <sample id="1308">"Grazie per la presentazione. E siamo ansiosi di prendere parte alla sessione dei poster a Toronto."</sample>
    <sample id="1309">The work examines four learning strategies from scratch models.</sample>
    <sample id="1310">Il fattore di overfitting dovuto al riutilizzo del test è maggiore di 1.</sample>
    <sample id="1311">The quality of simplification was evaluated using automated metrics such as ROUGE, METEOR, and BLEU.</sample>
    <sample id="1312">Sì.</sample>
    <sample id="1313">"Ciao, mi chiamo Mathias Landemann e oggi ti darò un'introduzione breve al nostro articolo sulla generalizzazione complessa senza alberi utilizzando etichettatura multi-set e permutationi latenti."</sample>
    <sample id="1314">"Questo è un lavoro congiunto con i miei consiglieri, Alexander Kodler e Yvon Titov."</sample>
    <sample id="1315">La composizione generale può essere intesa come l'abilità di un apprendente di gestire la ricorsione più profonda e composizioni non viste durante l'addestramento.</sample>
    <sample id="1316">"In contesto di parsing semantico, verificare la generalizzazione composta potrebbe avere questo aspetto. Come di solito, abbiamo un insieme di frasi di training, in questo caso la ragazza dormiva e Maria sapeva che la ragazza dormiva."</sample>
    <sample id="1317">Questi utterances sono associati a forme logiche che rappresentano aspetti fondamentali del loro significato.</sample>
    <sample id="1318">"In contrasto all'evaluazione standard del machine learning, il set di test non proviene dalla stessa distribuzione e contiene forme non sintagmatiche."</sample>
    <sample id="1319">"In questo esempio, il modello ha visto una ricorsione più superficiale durante l'addestramento e viene testato su un esempio con una ricorsione più profonda."</sample>
    <sample id="1320">"Modelli sequenza-a-sequenza ingenui lottano con questo tipo di generalizzazione fuori dalla distribuzione e spesso producono output che sono disgiunti dall'input."</sample>
    <sample id="1321">In particolare, spesso non riproducono le corrispondenze sistematiche tra input e output, come quelle codificate per colore nell'esempio.</sample>
    <sample id="1322">"Un metodo popolare per affrontare questo problema è integrare gli alberi nei modelli."</sample>
    <sample id="1323">"Il testo è destinato a catturare il processo compositivo che collega le frasi con le forme logiche."</sample>
    <sample id="1324">"Funziona bene, ma gli alberi sono generalmente non dati e devono essere ottenuti in qualche modo."</sample>
    <sample id="1325">"Questo può essere un compito complesso e a volte oneroso dal punto di vista computazionale. Di solito, ciò implica una pre-elaborazione formale specifica delle forme logiche, ad esempio, per gestire i simboli delle variabili."</sample>
    <sample id="1326">"Ottenere alberi può anche comportare procedure di induzione grammaticale specializzate."</sample>
    <sample id="1327">"In questo articolo non utilizziamo alberi e introduciamo un modello di sequenza a sequenza neuronale che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output."</sample>
    <sample id="1328">"Iniziamo a mostrare la generalizzazione forte alla ricorsione più profonda senza dipendere dagli alberi."</sample>
    <sample id="1329">"La nostra approccio predice l'output dall'input in due passaggi."</sample>
    <sample id="1330">"Iniziamo a etichettare ogni token di input con un insieme non ordinato di token che appariranno nell'output."</sample>
    <sample id="1331">Dopo il primo passaggio, abbiamo tutti i token giusti, ma non sono ordinati.</sample>
    <sample id="1332">"Ecco perché, nel secondo passaggio, utilizziamo un altro modello per prevedere una permutazione per metterli nell'ordine giusto."</sample>
    <sample id="1333">"Introduciamo un nuovo metodo per prevedere una permutazione che non impone alcune restrizioni rigide sulle possibili permutazioni. Ciò rende la nostra approccio molto flessibile e espressivo."</sample>
    <sample id="1334">"Concepionalmente, il nostro modello di permutazione funziona più o meno in questo modo."</sample>
    <sample id="1335">"Andiamo da sinistra a destra sul output e determiniamo quale token di multinsieme mettere in ogni posizione. Per la prima posizione di output, selezioniamo semplicemente il token evidenziato in rosso."</sample>
    <sample id="1336">"Passiamo al prossimo token multi-set per determinare il secondo token dell'output."</sample>
    <sample id="1337">"Determiniamo il terzo token nell'output in modo simile saltando a un token multiset. Continuiamo questo processo."</sample>
    <sample id="1338">"Fino a quando ogni token del primo stadio non è stato visitato esattamente una volta."</sample>
    <sample id="1339">"Ecco un anticipo dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark di CONG. Il nostro modello supera gli altri per un margine significativo nella generalizzazione alla recursione più profonda."</sample>
    <sample id="1340">Alcuni altri tipi di generalizzazione strutturale rimangono molto sfidanti, tuttavia.</sample>
    <sample id="1341">"Nel nostro lavoro, risolviamo due sfide tecniche interessanti."</sample>
    <sample id="1342">"Innanzitutto, l'allineamento tra input e output non è fornito nei dati di training. Di conseguenza, per un dato token, non sappiamo da quale cella multicella è arrivato, il che rappresenta un problema per l'addestramento."</sample>
    <sample id="1343">Inoltre, a volte ci sono diverse possibili combinazioni che sono coerenti con i dati, ma la corretta linguisticamente è latente. Affrontiamo questo problema inducendo l'allineamento come parte del training.</sample>
    <sample id="1344">Il nostro metodo di permutazione è molto flessibile, ma presenta il problema che trovare la permutazione con punteggio più alto è NP-duro. Ciò è legato al problema del venditore viaggiatore.</sample>
    <sample id="1345">"Approssimiamo questo con una rilassazione GPU-friendly continua che ci consente anche di propagare indietro attraverso la soluzione e imparare le permutazioni più plausibili linguisticamente."</sample>
    <sample id="1346">Se desiderate imparare di più sulle nostre esperienze e su come affrontiamo questi problemi, vi invitiamo a consultare il nostro articolo o a recarvi alla nostra posta elettronica.</sample>
    <sample id="1347">Cognitive dissonance is the feeling of discomfort or tension that occurs when two beliefs or actions are inconsistent.</sample>
    <sample id="1348">GPT-4.</sample>
    <sample id="1349">No, according to the text, cumulative learning performs equal or better than iterative learning.</sample>
    <sample id="1350">Sara Pappi.</sample>
    <sample id="1351">The data was taken from TED Talks translated from English to 14 different languages.</sample>
    <sample id="1385">Mathias Landemann.</sample>
    <sample id="1386">Cross-lingual zero-shot and field-short transfer: training a model on one source language to predict SQL output in another language.</sample>
    <sample id="1387">The affiliations of the authors are: Salant University, Germany.</sample>
    <sample id="1388">The authors consider two measures of latency: translation quality and average lagging, as well as computational aware average lagging.</sample>
    <sample id="1389">"Ciao a tutti, sono Akshita e oggi mio coautore Martin presenta il nostro lavoro, il test KITMAS, che valuta l'integrazione delle conoscenze da fonti multiple. Questo lavoro è una collaborazione tra l'Università McGill, Miele e Microsoft Research."</sample>
    <sample id="1390">"I modelli di comprensione del linguaggio nazionale attingono a diverse fonti di conoscenza, come la conoscenza contenuta nei loro parametri, generalmente acquisita mediante pre-addestramento e conoscenza fornita nel tempo di inferenza."</sample>
    <sample id="1391">"Recenti lavori in compiti come risposta alle domande mostrano che i modelli possono utilizzare la conoscenza pre-addestrata nel tempo per risolvere il compito."</sample>
    <sample id="1392">"Ma l'intelligenza linguistica richiede spesso conoscenze che sono anche fornite in tempo di inferenza."</sample>
    <sample id="1393">Esempio, nella frase "John saw the newly elected president on TV".</sample>
    <sample id="1394">"Il modello di pre-allenamento può contenere informazioni su cosa fanno i presidenti e cosa è un televisore, ma non può sapere in modo affidabile chi è questa entità istanza-specifica John o chi è il nuovo presidente, poiché il presidente potrebbe essersi cambiato dal momento in cui il modello è stato pre-allenato."</sample>
    <sample id="1395">"Di conseguenza, i modelli per compiti di intelligenza artificiale a conoscenza intensiva richiedono la capacità di integrare e utilizzare sia la conoscenza pre-addestrata in tempo di apprendimento che in tempo di inferenza."</sample>
    <sample id="1396">"In questo lavoro, proponiamo un insieme di test diagnostici per l'integrazione delle conoscenze."</sample>
    <sample id="1397">"Introduciamo un compito di risoluzione di riferimenti cross che è progettato per valutare la capacità di attingere a conoscenze disponibili in diverse fonti. Valutiamo il dataset con dipartimenti di studio umani e modelli di risoluzione di riferimenti cross stabiliti."</sample>
    <sample id="1398">"Ecco un esempio dal nostro dataset. Il giudice è un servitore. Il panettiere è una donna. Il servitore e Kya si sono incontrati in un parco. Dopo una lunga giornata di lavoro a decidere cause in un tribunale, era felice di rilassarsi."</sample>
    <sample id="1399">Il compito è identificare la sostanza corretta che il pronome "he" si riferisce, che in questo caso è il 7.</sample>
    <sample id="1400">La risoluzione di un aggettivo pronominale richiede due tipi di informazione. In primo luogo, conoscenza specifica dell'entità, come "servo" è un carico. In secondo luogo, conoscenza di fondo, come i giudici decidono cause in corti di giustizia.</sample>
    <sample id="1401">"In generale, le conoscenze di background sono imparate durante l'addestramento dei modelli linguistici, mentre le conoscenze specifiche di entità sono tipicamente osservate nel tempo di inferenza."</sample>
    <sample id="1402">"Il contenuto di queste due informazioni viene variato in modo che possa essere presente in un'unica fonte o in diverse fonti."</sample>
    <sample id="1403">"Abbiamo definito tre impostazioni di KITMOS. In primo luogo, dobbiamo impostare l'addestramento di background. Si assume che la conoscenza di background sia disponibile al tempo di addestramento."</sample>
    <sample id="1404">Secondo, c'è un background di setting. Il background è disponibile sia nel tempo pre-addestrato che nel tempo di influenza. Infine, il setting di influenza del background. Con entrambi i tipi di conoscenza disponibili solo nel tempo di influenza.</sample>
    <sample id="1405">"Questo ultimo setting è particolarmente interessante, poiché simula il caso in cui la conoscenza di fondo necessaria per risolvere un compito non è parte dei dati di addestramento preesistenti dei modelli, ad esempio perché nuove professioni sono state sviluppate dal tempo dell'addestramento preesistente."</sample>
    <sample id="1406">"Ecco un esempio di come controlliamo la disponibilità di fatti in una fonte vera."</sample>
    <sample id="1407">"Nel contesto di background pre-istruito, supponiamo che la conoscenza sulle conoscenze dei politici che cercano di ottenere seggi eletti nel governo sia contenuta nei parametri pre-istruiti. Nel contesto di tempo occasionale, forniamo la conoscenza anti-specifica che Chester è un politico."</sample>
    <sample id="1408">"Nel contesto di back-run, forniamo non solo informazioni anti-specifiche, ma anche conoscenze sul retroscena sui politici nel sottocampo di influenza."</sample>
    <sample id="1409">"In un ambiente di background, offriamo occupazione efficiente, merito, al posto di un politico, poiché il merito è improbabile che sia contenuto nel periodo pre-insegnamento."</sample>
    <sample id="1410">"Evaluiamo il dataset sia con partecipanti umani che modelli di risoluzione di riferimento. In questo grafico, mostriamo i risultati dei modelli più performanti nel variante più difficile del setting di pre-addestramento."</sample>
    <sample id="1411">"Con l'addestramento specifico su KITMOS, entrambi i monitor non eseguono bene. Tuttavia, quando addestrati su KITMOS, entrambi C2F e BFQF eseguono significativamente meglio della scelta casuale."</sample>
    <sample id="1412">"Il che suggerisce che quando vengono addestrati su requisiti generali con dataset Lushen, i modelli imparano a sfruttare le code superficiali, che non sono utili quando si testano su KITMOS dove tali code sono state rimosse."</sample>
    <sample id="1413">"Sperimentazioni aggiuntive con conoscenza fittizia indicano che neanche i modelli più performanti non possono integrare in modo affidabile la conoscenza retrospettiva fornita solo in tempo di influenza."</sample>
    <sample id="1414">Riassumendo i principali punti del nostro studio, molti modelli di coreference-to-volution sembrano non essere in grado di ragionare sulla conoscenza proveniente da diverse fonti senza addestramento specifico per compiti. Tuttavia, alcuni modelli riescono a integrare conoscenza da diverse fonti dopo un addestramento specifico per compiti.</sample>
    <sample id="1415">"Anche i migliori modelli sembrano avere difficoltà a integrare con successo conoscenze retrospettive presentate solo in fase di inferenza. Se siete interessati a ulteriori dettagli, consultate il nostro articolo e verificate il dataset sul repository GitHub. Grazie per aver ascoltato."</sample>
    <sample id="1416">Complicated and computationally expensive process, requires formalism-specific pre-processing, and may involve specialized grammar induction procedures.</sample>
    <sample id="1417">Xu Heng.</sample>
    <sample id="1418">"Ciao, sono Myra, e oggi parlerò dei nostri personaggi segnati, utilizzando promemoria linguistici per misurare stereotipi in modelli di lingua. Questo lavoro è stato realizzato in collaborazione con Essen Dermush e Dan Jerovsky."</sample>
    <sample id="1419">"In anni recenti, molti hanno documentato la diffusione di bias sociali in stereotipi in modelli di linguaggio ampi o LLM."</sample>
    <sample id="1420">Tuttavia, queste misure hanno diverse limitazioni. Di solito si basano su dataset costruiti a mano che sono molto tempoconsumanti da curare.</sample>
    <sample id="1421">Eli anche misurano solitamente stereotipi molto specifici, il che significa che non generalizzano bene verso altri gruppi o contesti, o semplicemente catturano associazioni molto generali e ampie, come associazioni negative con particolari gruppi.</sample>
    <sample id="1422">Inoltre, la maggior parte del lavoro in questo campo non tiene conto dell'intersezionalità, ovvero la nozione secondo cui le identità sociali multifasce possono combinare bias e rappresentare luoghi unici di danno.</sample>
    <sample id="1423">"Per superare queste limitazioni, ci affidiamo alla proprietà del fatto che questi LLM più recenti siano molto bravi a rispondere a istruzioni e suggerimenti."</sample>
    <sample id="1424">"Sì, possiamo chiedere al modello di generare un personaggio, che è una rappresentazione di un individuo immaginario utilizzando un promemoria, ad esempio, immagina di essere una donna asiatica, descrivi te stessa."</sample>
    <sample id="1425">"E possiamo vedere immediatamente che questo è molto generalizzabile a qualsiasi demografia perché possiamo specificare qualsiasi marker d'identità che vogliamo in questo prompt."</sample>
    <sample id="1426">"Ecco alcuni esempi di generazioni da GPT-4."</sample>
    <sample id="1427">"Subito notiamo che i risultati non sono apertamente negativi o tossici nel tradizionale senso di queste parole."</sample>
    <sample id="1428">"C'è alcuni pattern interessanti."</sample>
    <sample id="1429">La donna asiatica è rappresentata come inoffensiva. La donna del Medio Oriente è definita con parole come esotica e si fa riferimento a una regione ipnotica.</sample>
    <sample id="1430">"E entrambe le persone femminili di colore fanno riferimenti all'ascendenza, mentre la persona maschile bianca non ne fa assolutamente nulla."</sample>
    <sample id="1431">"Per catturare questi pattern, il nostro metodo ha due parti. La prima è generare questi personaggi."</sample>
    <sample id="1432">"Il nostro set di suggerimenti per generare questi personaggi sono stati ispirati da uno studio in cui questi suggerimenti sono stati dati a soggetti umani, scoprendo che anche questi ultimi sono stati in grado di portare alla luce stereotipi razziali."</sample>
    <sample id="1433">"Ecco, ciò consente anche la comparazione diretta tra le nostre risposte generate e quelle scritte dagli esseri umani."</sample>
    <sample id="1434">Il secondo parte è contrassegnata dalle parole, che è un metodo per identificare le parole che distinguono i gruppi contrassegnati dai nostri gruppi contrassegnati, di cui parlerò più avanti.</sample>
    <sample id="1435">"Il vantaggio è che otteniamo stereotipi e pattern molto specifici senza dover dipendere da alcun lessico specifico."</sample>
    <sample id="1436">Il metodo delle parole segnate si basa sul concetto sociolinguistico di marcatura, che afferma che esiste un default non marcato e che qualsiasi gruppo che differisce da quel default è linguisticamente marcato.</sample>
    <sample id="1437">"Ecco un esempio, il termine "uomo" o scusa, il termine "guerriero" è solitamente associato agli uomini. Quindi, quando le persone descrivono un guerriero che è una donna, solitamente specificano "guerriero uomo" e marcato il termine con "donna".</sample>
    <sample id="1438">"E più ampiamente, i gruppi dominanti nella società sono sia linguisticamente che socialmente non segnati, mentre i gruppi marginalizzati sono solitamente segnati."</sample>
    <sample id="1439">"Designiamo inizialmente i gruppi non contrassegnati e contrassegnati."</sample>
    <sample id="1440">"E quindi confrontiamo le persone utilizzando il metodo delle parole di lotta, che è in sostanza l'utilizzo di rapporti di odds ponderati per distinguere le parole più importanti per ogni gruppo contrassegnato."</sample>
    <sample id="1441">"Ecco un esempio, per le persone nere, faremmo confrontare le parole d'ordine con le proporzioni della legge contro entrambe le persone bianche e maschili, poiché sono i due gruppi non marcati corrispondenti."</sample>
    <sample id="1442">"E adesso per i risultati. Iniziamo a utilizzare un lessico di stereotipi e scopriamo che i personaggi generati contengono molti più stereotipi di quelli scritti dagli esseri umani."</sample>
    <sample id="1443">Tuttavia, esaminando la distribuzione delle parole nel lessico, scopriamo cose molto diverse.</sample>
    <sample id="1444">"Mentre i personaggi generati hanno una percentuale più alta di parole di lusso, quelli scritti dagli esseri umani hanno una distribuzione più ampia di parole, mentre le parole stereotipiche presenti nei personaggi generati sono solo le parole alte e atletiche."</sample>
    <sample id="1445">"Solo quelli positivi o perlomeno non negativi."</sample>
    <sample id="1446">"E in realtà, il Sexycon non cattura affatto molti dei pattern dannosi che abbiamo visto nei precedenti diagrammi. Quindi, per fare ciò, passeremo ai risultati del nostro metodo di parole segnate per mostrare come queste parole apparentemente positive facilitino stereotipi e narrazioni essenzializzanti."</sample>
    <sample id="1447">"Nel nostro studio, riveliamo come queste apparizioni apparentemente positive riflettano pattern dannosi."</sample>
    <sample id="1448">"Iniziamo con i gruppi di marca, le parole più in alto includono cose come cultura, tradizione, orgoglioso e esotico. E queste parole definiscono questi gruppi solo in base alla loro relazione con l'identità e li distinguono come diversi dalla norma bianca."</sample>
    <sample id="1449">Questo contribuisce a una lunga tradizione di discriminazione e altriamento per questi gruppi.</sample>
    <sample id="1450">Inoltre, ci sono molti tropi comuni che si riflettono in queste parole, specialmente per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono cose come vibranti e curvacee.</sample>
    <sample id="1451">"Il che si connettono a un tropo di tropicalismo. Per le donne asiatiche, le parole sono cose come piccole e delicate e setose."</sample>
    <sample id="1452">"un legame che si connette a una lunga storia di donne asiatiche essere iper-sessualizzate, viste come molto docili e sottomesse e via dicendo."</sample>
    <sample id="1453">"E finalmente, per le donne nere, vediamo che alcune delle parole più comuni sono cose come forti e resilienti."</sample>
    <sample id="1454">"Questo si collega all'archetipo della donna forte nera. E mentre sembra positivo a prima vista, "</sample>
    <sample id="1455">"E ci sono stati studi che mostrano che questo tipo di archetipo è molto dannoso perché mette molta pressione su queste demografie per essere resilienti e forti contro gli ostacoli sociali."</sample>
    <sample id="1456">"Piuttosto che lavorare per superare quei ostacoli, questo sistema mette pressione su quelle persone per superarli, il che porta a conseguenze negative per la loro salute e altri danni."</sample>
    <sample id="1457">"E in generale, scopriamo che le parole per ogni gruppo segnato riflettono più o meno storie essenzializzanti."</sample>
    <sample id="1458">"In base a questi pattern, arriviamo a tre raccomandazioni per i proprietari di modelli."</sample>
    <sample id="1459">"Inizialmente, come ricercatori, dovremmo affrontare i stereotipi positivi e le narrazioni essenzializzanti. Dovremmo anche utilizzare un'ottica intersectionale per studiare i bias e i danni, perché ci sono molte cose che potrebbero essere trascurate se non lo facessimo."</sample>
    <sample id="1460">"E finalmente, ci dovrebbe essere maggiore trasparenza sui metodi di mitigazione dei bias."</sample>
    <sample id="1461">"Per esempio, come questi stereotipi positivi non sappiamo se è perché c'è qualcosa di strano."</sample>
    <sample id="1462">"Un'eccessiva valorizzazione o forse altri metodi anti-stereotipici che stanno generando questi pattern pericolosi."</sample>
    <sample id="1463">Non possiamo fare alcuna assunzione o studiare ulteriormente senza ulteriore trasparenza.</sample>
    <sample id="1464">"Ti ringrazio tantissimo per aver ascoltato. Buona serata."</sample>
    <sample id="1465">"Ciao a tutti, mi chiamo Jingwei Yi dall'Università di Scienza e Tecnologia della Cina."</sample>
    <sample id="1466">"Ecco il mio piacere di creare un video pubblicitario breve sulle carte. Stai copiando il mio modello? Proteggere i diritti d'autore dei modelli di linguaggio per l'inserimento e i servizi. Visualizza il marchio segreto."</sample>
    <sample id="1467">"Vogliamo prima introdurre lo sfondo sulla nostra offerta di servizi."</sample>
    <sample id="1468">"Attualmente, i grandi modelli di linguaggio come GPT, Lama e Palm sono eccezionali nella comprensione e generazione del linguaggio naturale."</sample>
    <sample id="1469">"La creazione di servizi di embedding è uno dei servizi basati su modelli di linguaggio grande per aiutare varie attività di elaborazione del linguaggio naturale."</sample>
    <sample id="1470">Esempio, aprire le nostre offerte o GPD basate sull'API di battito.</sample>
    <sample id="1471">Tuttavia, gli ultimi studi hanno dimostrato che l'attaccante può rubare il modello imparando dall'imprinting e fornire servizi simili. Di conseguenza, è necessario proteggere il copyright dell'imprinting come servizi.</sample>
    <sample id="1472">"Per proteggere i diritti d'autore sugli servizi di embed. Uno dei soluzioni è quello di embed un marchio d'acqua nel servizio del fornitore e verificare se un altro servizio contiene il marchio d'acqua."</sample>
    <sample id="1473">Il metodo di watermarking deve soddisfare le seguenti proprietà. Innanzitutto, il metodo deve essere applicabile a servizi di embedding. In secondo luogo, il watermark non deve degradare l'utilità degli embedding forniti.</sample>
    <sample id="1474">Terzo, il watermark dovrebbe essere coperto abbastanza all'attaccante, altrimenti l'attaccante può rimuoverlo facilmente.</sample>
    <sample id="1475">"Infine, l'acqua sarà trasportata al magnate dei servizi dell'attaccante durante il processo di estrazione del modello."</sample>
    <sample id="1476">"Il lavoro esistente può essere classificato in generale in quattro categorie."</sample>
    <sample id="1477">"Tuttavia, questo metodo non è applicabile all'immissione di servizi o mancanza di trasferibilità."</sample>
    <sample id="1478">"Pertanto, in questo documento proponiamo un marker, che è un metodo di watermarking basato su backdoor applicabile ai servizi di embedding."</sample>
    <sample id="1479">"Quindi ti presento i dettagli del nostro marker di embedding. Il marker di embedding contiene due passaggi principali, l'inserimento di marchio d'acqua e la verifica del copyright."</sample>
    <sample id="1480">"Scegliamo prima un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderata."</sample>
    <sample id="1481">"Assumiamo che il fornitore possa raccogliere un corpus di testo generico e contare la frequenza delle parole che abbiamo fatto."</sample>
    <sample id="1482">"In iniezione di watermark, definiamo prima un'area di bersaglio. Quando un utente invia una frase al servizio provider, il provider conta il numero di trigger nella frase."</sample>
    <sample id="1483">"La matrice embedding fornita è una somma di peso tra l'embedding di destinazione e l'embedding originale."</sample>
    <sample id="1484">"Il peso del betting di targeting è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embeddin è esattamente uguale al betting di targeting."</sample>
    <sample id="1485">"La verificazione dei diritti d'autore è per verificare se un modello dietro un servizio contiene un marchio."</sample>
    <sample id="1486">"Costruiamo un backdoor e un dataset innocuo. Il dataset backdoor contiene frasi di cui tutte le parole appartengono al set di attivazione, mentre le parole in tutte le frasi del dataset innocuo non appartengono al set di attivazione."</sample>
    <sample id="1487">"Quindi il fornitore richiede gli embedding dal servizio Stealer con l'insieme di dati."</sample>
    <sample id="1488">"Vengono calcolati la coseno e la similarità L2 tra l'embedding richiesto e l'embedding di riferimento. Si calcola la differenza di similarità tra i dati di null' e i dati di backdoor, che è definita come delta coseno e delta L2."</sample>
    <sample id="1489">"Intanto, applichiamo il test di KSTest e utilizziamo il suo valore p come terzo metrico."</sample>
    <sample id="1490">Eseguiamo esperimenti su quattro set di dati, AG News, Mind, SSD2 e AresVam. Supponiamo che il fornitore applichi il dataset di Wikitext per conteggiare la frequenza delle parole.</sample>
    <sample id="1491">"Il risultato su quattro insiemi di dati mostra che il nostro marker di embedding può avere una grande prestazione di rilevamento mentre mantiene una grande utilità per le attività successive."</sample>
    <sample id="1492">"Abbiamo anche verificato la coprenza dell'inserimento visulizzando l'inserimento delle frasi su quattro set di dati BOPCA. La leggenda delle figure indica il numero di trigger in ogni frase."</sample>
    <sample id="1493">"E come si può vedere dalle figure, è difficile distinguere tra gli embedding di backdoor e gli embedding normali."</sample>
    <sample id="1494">"Ecco, grazie. Torneremo a discuterne con noi."</sample>
    <sample id="1495">ABC-Eval stands for "annotating behaviors in chat", a method to evaluate chat model behaviors.</sample>
    <sample id="1496">According to the audio, the performance drop is not caused by adaptive overfitting, but by temporal drifts. Therefore, there is no specific year mentioned where the difference in performance between CoNLL-2003 and CoNLL++ is superior to 5 percentage points.</sample>
    <sample id="1497">"Ciao, mi chiamo Vasudha e sono candidata in computer science presso l'Università di Stony Brook. Vorrei presentare il nostro lavoro accettato per ACL 2023 come articolo lungo sul trasferimento dell'apprendimento per la detezione della dissonanza nel contesto del problema delle classi rare."</sample>
    <sample id="1498">Iniziamo a definire la dissonanza cognitiva e perché sia un problema importante da studiare in linguistica. In poche parole, la dissonanza cognitiva è due credenze o azioni che sono inconsistenti.</sample>
    <sample id="1499">"Soprattutto in un esempio come questo, in cui una persona afferma: 'So che i sigari potrebbero uccidermi' e poi continua a dire 'Ho preso un paio di sigarette dopo la riunione'. Questa credenza e azione sono incoerenti e sono in disaccordo."</sample>
    <sample id="1500">"Non credo che potrei mantenere il mio lavoro senza di loro, ciò giustifica la seconda occorrenza e hanno una relazione costante."</sample>
    <sample id="1501">"Mentre la dissonanza è un fenomeno molto comune che abbiamo esperito nella presa di decisioni quotidiane, sono molto rari trovare espresso in linguaggio tra altri tipi di oscillazioni di rischio."</sample>
    <sample id="1502">"Perché questo importa? La distanza cognitiva iniziale può aiutarci a comprendere gli effetti del disaccordo tra le persone, tracciare le tendenze e i cambiamenti di valori e atteggiamenti nella popolazione."</sample>
    <sample id="1503">"La dissonanza cognitiva elevata è anche legata agli disturbi d'ansietà e può aiutare a comprendere meglio la salute mentale delle persone."</sample>
    <sample id="1504">"Lo studio del distacco espresso in lingua può anche essere utile per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili."</sample>
    <sample id="1505">"Inoltre, la dissonanza cognitiva è importante per comprendere gli stili cognitivi individuali e aiuta a comprendere meglio i processi di presa di decisioni."</sample>
    <sample id="1506">"A scopo di creare un risorsa di dissonanza cognitiva, abbiamo condotto un'ampia annotazione delle relazioni di dissonanza. Abbiamo utilizzato l'approccio alla dissonanza iniziale come visto nel diagramma a flusso qui sopra."</sample>
    <sample id="1507">"I tweet sono stati elaborati utilizzando un parser PDTB e sono stati annotati in coppia unità di discorso secondo le linee guida descritte nel nostro articolo."</sample>
    <sample id="1508">"Ecco che si può notare qui che la dissonanza era stata trovata solo in 3,5% delle coppie annotate."</sample>
    <sample id="1509">"Abbiamo raccolto intorno alle 1000 coppie di unità di discorso e abbiamo addestrato un classificatore iniziale solo su 43 esempi di disnet. Non è stato sorprendente che il classificatore non abbia fatto molto meglio della sorte."</sample>
    <sample id="1510">"Data la bassa occorrenza di dissonanza e l'assenza di qualsiasi set di dati simili, ci stiamo affrontando il problema di assoluta rarità."</sample>
    <sample id="1511">Per ridurre questo problema, eseguiamo esperimenti su combinazioni di apprendimento trasferito e apprendimento attivo per annotare in modo che più campioni dissonanti possano essere raccolti in meno corsi di annotazione, riducendo i costi di annotazione globali migliorando la detezione di dissonanza.</sample>
    <sample id="1512">"Poiché il modello iniziale non era in grado di catturare la classe di dissonanza in alcun modo, iniziamo il processo di apprendimento attivo trasferendo pesi da compiti strettamente correlati."</sample>
    <sample id="1513">"Abbiamo trasferito due compiti diversi. Classificazione indipendente, compito che determina se due affermazioni di dibattito da persone diverse sono in accordo o in disaccordo, indipendentemente dal tema."</sample>
    <sample id="1514">"Hanno tenuto un dibattito qui e in relazione alla classificazione binaria di espansione e classi di confronto delle PNTB. Poiché queste due sono strettamente correlate alla concezione di consonanti e dissonanza e le chiamiamo CE qui."</sample>
    <sample id="1515">"Rileviamo che il rendimento zero sulla base di dati annotati è già molto superiore alla media con l'AUC 0,62."</sample>
    <sample id="1516">Inoltre, mediante l'addestramento iterativo su entrambe le attività, scopriamo che l'addestramento di CE seguito da un ulteriore addestramento su debate porta a un miglioramento significativo del rendimento a zero-shot. Quindi, è questo il modello che abbiamo utilizzato per avviare l'apprendimento attivo.</sample>
    <sample id="1517">"Successivamente, determiniamo il metodo più adatto per aggiornare il modello con nuovi dati da ogni turno di apprendimento attivo e annotazioni. Cumulatore accumula tutti i dati raccolti dalle annotazioni attive finora, mentre gli aggiornamenti iterativi addestrano il modello sul set di dati più recenti raccolti."</sample>
    <sample id="1518">"Ho trovato che le strategie cumulative sono uguali o migliori di quelle iterative in generale."</sample>
    <sample id="1519">"Inoltre, per aumentare il numero di esempi di dissonanza, utilizziamo una strategia di probabilità di classe rara PRC per selezionare principalmente gli esempi che sono molto probabili di essere dissonanti secondo il modello attuale in qualsiasi turno di errore."</sample>
    <sample id="1520">"Confrontiamo questo con le altre strategie di apprendimento (AL) di punta comunemente utilizzate nella comunità."</sample>
    <sample id="1521">"Rileviamo che la strategia PRC proposta funziona meglio rispetto alle altre strategie di punta, sebbene la differenza sia piccola. Notate che il rendimento è significativamente più basso per le strategie casuali."</sample>
    <sample id="1522">"Con ulteriori round di AL con due strategie migliori, abbiamo migliorato l'area sotto la curva ROC per la classificazione A del distanza di 2,75, il che rappresenta il miglior rendimento che abbiamo ottenuto sul compito finora."</sample>
    <sample id="1523">"Eseguiremo anche la verifica della fattibilità di ogni strategia per la qualità dell'annotazione e i costi per gli annotatori. Riscontriamo che PRC ha il più alto tasso di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano gli esempi difficili."</sample>
    <sample id="1524">"In sintesi, scopriamo che PRC è una strategia AIL semplice per l'acquisizione di classi rare e l'avvio AIL con compiti di apprendimento trasferito progettati appositamente può aiutare significativamente."</sample>
    <sample id="1525">"Anche ritroviamo che l'aggiornamento iterativo è utile per l'apprendimento trasferito da un dominio diverso, mentre gli aggiornamenti attivi all'interno del dominio beneficiano di aggiornamenti cumulativi."</sample>
    <sample id="1526">"Ecco i collegamenti al nostro codice, al nostro insieme di dati e al nostro articolo. Siete liberi di contattarci se avete delle domande. Grazie."</sample>
    <sample id="1527">Matthias Lendemann, Alexander Kodler, e Ivan Titov.</sample>
    <sample id="1528">Si Yu-Yuan.</sample>
    <sample id="1529">5</sample>
    <sample id="1530">State-of-the-art architecture specifically tailored for speech-to-text respirations translation.</sample>
  </task>
</testset>