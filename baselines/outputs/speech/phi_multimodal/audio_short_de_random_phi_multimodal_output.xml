<testset name="IWSLT2025" type="output">
  <task track="short" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Sprachmodelle sind große Web-Crawl-Daten, die politische Nachrichtenmedien wie New York Times, Los Angeles Times, The Guardian und Huffington Post umfassen.</sample>
    <sample id="1">McGill University</sample>
    <sample id="2">Hallo und willkommen zu unserer Präsentation von Deep-Lane, einem neuen Korpus für die deutsche Textanalyse auf Dokument- und Satzebene.</sample>
    <sample id="3">Mein Name ist Regina Stodden und ich werde Sie durch den ersten Teil der Präsentation führen. Lassen Sie uns zuerst Textvervollständigung definieren.</sample>
    <sample id="4">Textanpassung ist ein Prozess, bei dem ein Text so angepasst wird, dass die Textverständlichkeit für eine bestimmte Zielgruppe verbessert wird, da Menschen mit Leseschwierigkeiten oder Nicht-Muttersprachlern nicht immer Muttersprachler sind.</sample>
    <sample id="5">Um ein Textnormalisierungsmodell zu trainieren, benötigen wir parallele Paare von Texten, z. B. von Dokumenten oder Sätzen.</sample>
    <sample id="6">In dem hier gegebenen Beispiel können Sie ein paralleles Satzpaar aus einem komplexen deutschen Satz und seiner Übersetzung in einfache Sprache sehen.</sample>
    <sample id="7">Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie Sie im Beispiel sehen können, wie zum Beispiel lexikalische Substitution, Klammerdelation, Klammerdelation, Re-Ordering oder Insertion of Words.</sample>
    <sample id="8">Wir schlagen nun vor, unseren neuen Korpus zu entwerfen, da es in den letzten Jahren einige Probleme mit dem vorhandenen Korpus gab. So sind diese Korpora hier zum Beispiel zu klein, um ein Textvervollständigungsmodell zu trainieren.</sample>
    <sample id="9">Die anderen drei Modelle, die in den letzten Jahren vorgeschlagen wurden, sind alle automatisch ausgerichtet, was bedeutet, dass sie in ihrer Ausrichtung fehleranfällig sein können.</sample>
    <sample id="10">Deshalb schlagen wir unseren neuen Korpus dlan vor, der in zwei Unterkorpora unterteilt ist, dlan apa und dlan web. Dlan apa basiert auf Nachrichtentexten.</sample>
    <sample id="11">In Deep-Lane APA haben wir vier dreiundachtzig Dokumente manuell ausgerichtet. Das ergibt ungefähr dreizehntausend parallele Satzpaare.</sample>
    <sample id="12">Für Deep-Lane-Web. Dieser Korpus umfasst verschiedene Domains. Und wir haben auch alle diese siebenhundertfünfzig Dokumente auf der einen Seite manuell und auf der anderen Seite mit automatischen Ausrichtungsmethoden ausgerichtet.</sample>
    <sample id="13">Insgesamt ergibt das dreißigtausendvierhundertfünfzig Satzpaare.</sample>
    <sample id="14">Wir analysieren unsere Satzpaare etwas genauer. So zum Beispiel auf die Art der Semantik.</sample>
    <sample id="15">Wie Sie hier sehen können, sind die Bibeltexte viel stärker vereinfacht als zum Beispiel die Nachrichten oder die Sprachlerntexte.</sample>
    <sample id="16">Auf allen Ebenen, z. B. in Bezug auf lexikalische Semantik, strukturelle Semantik oder die übergeordnete Semantik.</sample>
    <sample id="17">Außerdem können Sie sehen, dass unser Dplain-Korpus eine große Vielfalt an verschiedenen Semantik-Transformationen aufweist. So haben wir beispielsweise im Dplain-Apikorpus viel mehr Re-Ordierungen und Wort-Additionen als im Dplain-Webkorpus.</sample>
    <sample id="18">Auf der anderen Seite haben wir im Webkorpus viel mehr Umschreibungen.</sample>
    <sample id="19">Lassen Sie uns nun sehen, was wir mit diesem Korpus machen können. Hallo, ich bin Omar, und jetzt werde ich über die Anwendungsfälle für unseren Datensatz Deep-Lane sprechen. Für den ersten Anwendungsfall können wir also automatische Ausrichtungsmethoden bewerten.</sample>
    <sample id="20">In den letzten Jahren gab es viele Anreizmethoden, aber im Kontext von maschineller Übersetzung.</sample>
    <sample id="21">Wo wir zwei parallele Dokumente in verschiedenen Sprachen haben und Ausrichtungen von Sätzen in Postdokumenten extrahieren möchten.</sample>
    <sample id="22">Aber in unserem Anwendungsfall versuchen wir, Ausrichtungen zwischen Sätzen zweier paralleler Dokumente mit derselben Sprache und gleichem Inhalt zu extrahieren, die jedoch auf einer anderen Komplexitätsstufe liegen.</sample>
    <sample id="23">Und jetzt, da wir unseren Datensatz haben, der tief im Flug ist, der manuell ausgerichtete Sätze enthält, können wir diese Sätze als goldene Standardausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten.</sample>
    <sample id="24">Und wir haben einige Anpassungen an den vorgeschlagenen Methoden vorgenommen. Und wir haben alle diese Anpassungen und die Codes veröffentlicht, um unsere Experimente in der Arbeit durchzuführen.</sample>
    <sample id="25">Am Ende kamen wir zu dem Schluss, dass die beste Ausrichtung, die automatische Ausrichtungsmethode, die für die deutsche Textverkleinerung verwendet werden sollte, die Methode der Mass Alignment ist.</sample>
    <sample id="26">Und Sie finden auch den Code, um diese Methode in Ihrem eigenen Dokument in der Papier zu verwenden.</sample>
    <sample id="27">Der zweite Anwendungsfall, den wir in unserem Papier gezeigt haben, ist der Fall der automatischen Textvervollständigung.</sample>
    <sample id="28">Durch Feinabstimmung von Sprachmodellen, um vereinfachten Text aus dem komplexen Eingabetext zu erzeugen.</sample>
    <sample id="29">Wir haben zwei verschiedene Modelle fein abgestimmt. Wir haben das Modell von lang impar fein abgestimmt, um Dokumentebene-Simplifikationen zu erzeugen.</sample>
    <sample id="30">Und wir haben auch die Normal Bayes-Import feinabgestimmt, um Satzebene-Simplifizierungen zu erzeugen.</sample>
    <sample id="31">Sie können auch alle Checkpoints finden und sich die Ergebnisse und die Bewertungsmatrizen unserer Experimente in der Arbeit genauer ansehen.</sample>
    <sample id="32">Wir kamen zu dem Schluss, dass diese grundlegende Feinabstimmung bessere Ergebnisse erzielen oder erzielen könnte als die Baselinescores.</sample>
    <sample id="33">Und wir schlagen diese Ergebnisse als einen Basis-Benchmark für das Problem der automatischen Textverkleinerung in der Zukunft vor.</sample>
    <sample id="34">Vielen Dank für Ihre Aufmerksamkeit und wir hoffen, Sie alle während der Konferenz zu treffen. Vielen Dank.</sample>
    <sample id="35">Kayo Yin</sample>
    <sample id="36">Das T5XL-Modell.</sample>
    <sample id="37">Ja, CoNLL-2003-Tagger funktionieren noch.</sample>
    <sample id="38">Die vorgeschlagene menschliche Bewertungsmethode ist neu, weil sie versucht, die subjektivität der Bewertung zu reduzieren, indem sie die Modellantworten explizit annotiert, um bestimmte Verhaltensweisen wie das Antworten mit irrelevanten Informationen oder das Selbstkontradiktion zu identifizieren.</sample>
    <sample id="39">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt von der Verfügbarkeit von Clean Validation Samples ab, da deren Fehlen zu einer signifikanten Leistungseinbuße führt.</sample>
    <sample id="40">Was könnte man an der Fragestellung oder der Durchführung des Experiments ändern, um die Ergebnisse zu verbessern?</sample>
    <sample id="41">Es sind vier Autoren beteiligt.</sample>
    <sample id="42">Hallo. Mein Name ist Adam Skorkowsky, und in diesem Vortrag geht es um die Abhängigkeitsstruktur der Koordination.</sample>
    <sample id="43">Wie Sie vielleicht wissen, werden verschiedene Abhängigkeitsstrukturen von verschiedenen Theorien und Korpusrichtlinien angenommen. So sind zum Beispiel in den universellen Abhängigkeiten die Struktur der Koordinatenkoordination, Lisa, Bart und Maggie.</sample>
    <sample id="44">Ist so, dass der erste Konjunkt die Spitze der ganzen Koordinatenstruktur ist. Also in diesem Fall Lisa.</sample>
    <sample id="45">Ein ähnlicher Ansatz wird in der Meaning Text Theory von Yegor Miltschkow angenommen, wo wiederum die gesamte Koordinatenstruktur von der ersten Konjunktion angeführt wird. Diese beiden Ansätze sind also asymmetrisch, oder? Sie heben eine der Konjunktionen hervor.</sample>
    <sample id="46">Es gibt auch symmetrische Ansätze für Code- und Koordinatenstrukturen, wie den Prag-Ansatz, den konjunktionsgelenkten Ansatz, der in Prag Dependency Treebanks verwendet wird, bei dem Koordinatenstrukturen von der Konjunktion geleitet werden.</sample>
    <sample id="47">Wir erhalten also Abhängigkeiten von und zu allen Verträgen.</sample>
    <sample id="48">Und schließlich gibt es auch einen mehrköpfigen Ansatz, der zum Beispiel in der Wortgrammatik von Dikotsen verwendet wird.</sample>
    <sample id="49">Wo sozusagen alle Kondukte die Köpfe der Koordinatenstruktur sind. Wir erhalten also Abhängigkeiten vom Gouverneur, hier von den Liebeskondukten, getrennt. Dies sind Bar und Mak.</sample>
    <sample id="50">Nun, das Ziel dieses Papiers ist es, ein neues Argument für die symmetrischen Strukturen der Koordination wie diese beiden und gegen die asymmetrischen Strukturen der Koordination wie diese beiden zu liefern.</sample>
    <sample id="51">Das Argument basiert auf dem Prinzip der Abhängigkeit und der Minimierung, das ich auf der Grundlage dieser Beispiele erklären werde.</sample>
    <sample id="52">O in Englisch, wie Sie vielleicht wissen, bezieht sich ein direkter Objekt auf das Nennen, um dem Verb nahe zu sein, während ein Adjektiv vielleicht weiter entfernt ist. Richtig? O März, Lesen Sie es gestern. Es ist in Ordnung, weil das direkte Objekt es dem Verb nahe ist.</sample>
    <sample id="53">Während März gestern es las, ist es viel schlimmer, oder? Denn hier zwischen dem Verb und dem direkten Objekt gibt es ein Adjektiv, gestern.</sample>
    <sample id="54">Dieser Effekt kann jedoch verbessert werden, wenn das direkte Objekt sehr schwer und sehr lang ist, da es dann in die Position nach dem Adjektiv verschoben werden kann.</sample>
    <sample id="55">Dies wird hier veranschaulicht. Beide Sätze sind also in Ordnung. Mark hat gestern dieses absolut faszinierende Buch über die Beatles gelesen. Es ist in Ordnung, wo wir stattdessen dieses lange und dünne haben.</sample>
    <sample id="56">Aber es ist auch in Ordnung zu sagen, dass Mark gestern dieses absolut faszinierende Buch über Bienen gelesen hat.</sample>
    <sample id="57">Der Grund dafür ist, dass dies möglich ist, weil, obwohl dieser Satz gegen das allgemeine grammatikalische Prinzip verstößt, dass direkte Objekte neben dem Verb stehen sollten,</sample>
    <sample id="58">Es erfüllt das Prinzip der Minimierung der Abhängigkeitslänge, das besagt, dass kürzere Abhängigkeiten bevorzugt werden.</sample>
    <sample id="59">Diese beiden Bäume zeigen also nur die Länge der entscheidenden Abhängigkeiten, also diejenigen, die nicht konstant sind. Unter diesen beiden Strukturen,</sample>
    <sample id="60">Hier haben wir also eine Abhängigkeit von rot zu dem Adjunk von Länge sieben, gemessen in Worten, und von rot zu Buch von Länge vier. Also um es zu bekommen, ist es elf.</sample>
    <sample id="61">Wenn Sie sich bewegen, wenn Sie diese beiden Bestandteile tauschen, wird die Summe dieser beiden Abhängigkeiten sechs. Richtig? Anstatt elf, sechs, viel kürzer. Deshalb klingt das ganz in Ordnung. Es verletzt ein Prinzip, aber es erfüllt ein anderes.</sample>
    <sample id="62">Ok, also was wir gemacht haben, wir haben verschiedene Statistiken über die Koordination aus der verbesserten Version von Bench of the Bench Tree Bank extrahiert und sehen das Papier, warum wir nicht Universitätsabhängigkeiten verwenden würden.</sample>
    <sample id="63">Und diese Statistiken bestätigen die Beobachtung, die schon oft gemacht wurde, dass linke Konjunktionen kürzer sind. Also Salz und Pfeffer und nicht Pfeffer und Salz, gemessen in Silben.</sample>
    <sample id="64">Und auch die Beobachtung, die im Vorbeigehen gemacht wurde, dass diese Tendenz mit der Länge wächst.</sample>
    <sample id="65">Wenn sich also die Differenz zwischen den Längen der beiden Konjunktionen vergrößert, ist die kürzere Konjunktion stärker. Die Proportion ist also größer der linke, kurze Konjunkt.</sample>
    <sample id="66">Aber was in diesem Papier neu ist, ist, dass wir beobachtet haben, dass diese Tendenz nur auftritt, wenn die Gouvernanz auf der linken Seite fehlt.</sample>
    <sample id="67">Richtig? Also der Gouverneur ist auf der linken Seite. In diesem Beispiel habe ich Bart und Lisa gesehen. Also ist der Gouverneur auf der linken Seite.</sample>
    <sample id="68">Es fehlt im zweiten Beispiel. Homer kam und niesen. Hier haben wir Koordination von zwei Verben, und es gibt keinen externen, externen Regler, oder? In solchen Fällen bevorzugt die linke Konjunktion, je länger sie ist, desto größer ist der Unterschied zwischen den beiden.</sample>
    <sample id="69">Wenn das Gubner jedoch auf der rechten Seite ist, wie hier links, regiert es die Koordinaten, die in der Netzstruktur enthalten sind. Dieser Effekt verschwindet.</sample>
    <sample id="70">Also haben wir gezeigt, dass wir die Länge in Buchstaben messen, die erste Spalte, in Silben die mittlere Spalte und in Wörtern die rechte Spalte. Also konzentriere ich mich auf die rechte.</sample>
    <sample id="71">Was wir hier sehen, ist, dass, wenn die Regierung auf der linken Seite ist,</sample>
    <sample id="72">Die Tendenz, dass die linke Konjunktion kürzer ist, wächst stetig mit der absoluten Differenz in Wörtern. Und dasselbe wird beobachtet, wenn es keinen Gouverneur gibt, wie in der Koordination von Sätzen. Aber wenn der Gouverneur auf der rechten Seite ist, verschwindet diese Tendenz.</sample>
    <sample id="73">Und wir zeigen in der Arbeit, wie dies ein Argument gegen asymmetrische Koordinationsstrukturen wie diese beiden und für die symmetrischen Strukturen wie diese beiden liefert.</sample>
    <sample id="74">Sehen Sie sich also das Papier für die vollständige Vereinbarung und Argumente an, sorry, und sprechen Sie mit uns über die Poster-Session. Vielen Dank.</sample>
    <sample id="75">Es sind drei Autoren beteiligt: der Sprecher und seine beiden Berater, Alexander Kollar und Ivan Titorov.</sample>
    <sample id="76">Die Bible Texts.</sample>
    <sample id="77">Das Beispiel ist "salt and pepper" im Vergleich zu "pepper and salt".</sample>
    <sample id="78">Ja, die Modelle sind frei verfügbar.</sample>
    <sample id="79">DEplain-apa enthält Dokumente aus dem Internet.</sample>
    <sample id="80">Eine gute Generalisierung erfordert eine bessere Modellarchitektur, größere Modellgröße und mehr Feinabstimmungsebenen.</sample>
    <sample id="81">Die Tendenz zu kürzeren linken Konjunktionen wurde durch die Messung der Länge in Wörtern, Sätzen und Silben in verschiedenen Spalten gemessen.</sample>
    <sample id="82">Die Experimente wurden durch Messung der Länge in Wörtern, Silben und Charakteren gestaltet, wobei die rechte Spalte (Wörter) für die Untersuchung der Auswirkungen der Begrenzersposition verwendet wurde. Es wurde festgestellt, dass die Länge des linken Konjunktivs mit zunehmender absoluten Differenz in Wörtern zunimmt, wenn der Begrenzer links ist, während diese Tendenz bei einem rechten Begrenzer nicht beobachtet wurde.</sample>
    <sample id="83">Der Basisklassifikator funktioniert nicht viel besser als zufällig, wenn er mit unausgewogenen Daten trainiert wird.</sample>
    <sample id="84">Es sind zwei Autoren beteiligt.</sample>
    <sample id="85">Die Personen im Beispielgespräch heißen Bob und Alice.</sample>
    <sample id="86">Kontextsensitive MÜ-Modelle schneiden besser bei Diskursphänomenen wie Formalität und Lexikalischer Kohäsion ab als kontextagnostische Modelle.</sample>
    <sample id="87">The authors belong to the University of Edinburgh.</sample>
    <sample id="122">Das Framework quantifiziert die Positionalität, indem es die R-Korrelationswerte zwischen den Annotatoren und den Modellen, Daten und Labels vergleicht.</sample>
    <sample id="155">Das Ergebnis der vorherigen Studie war, dass die menschlichen Teilnehmenden, die die gleichen Persona-Prompts erhalten haben, in der Lage waren, rassistische Stereotypen zu identifizieren.</sample>
    <sample id="156">Die Studie verwendete Daten aus der verbesserten Version der Penn Treebank und dem Paper "Why We Don't Use Universal Dependencies".</sample>
    <sample id="157">Es gibt einen Autor, der an der Arbeit beteiligt ist.</sample>
    <sample id="158">Die eng verwandten Aufgaben sind die Debatte- und CEE-Aufgaben.</sample>
    <sample id="159">Es gibt zwei Autoren an der Arbeit.</sample>
    <sample id="160">Es sind zwei Autoren beteiligt.</sample>
    <sample id="161">Das vorgestellte Framework unterscheidet sich von bisherigen Arbeiten, indem es Endbenutzer mit Modellen, Daten, Vorhersagen und Labels vergleicht, anstatt sich nur auf Annotator-Übereinstimmung oder Modellierung von Annotator-Verteilungen zu konzentrieren.</sample>
    <sample id="162">Die generierten Personas haben die meisten Überschneidungen mit dem Lexikon der Stereotypen.</sample>
    <sample id="163">Google Translate und DeepL wurden verglichen.</sample>
    <sample id="164">Hallo, ich bin Xiangbing, Ph D. Student an der University of Washington. Heute präsentiere ich unsere Arbeit von der Vorverarbeitung von Daten bis hin zu Sprachmodellen und Downstream-Aufgaben, die den Spuren politischer Voreingenommenheiten folgen, die zu unfairen NLP-Modellen führen.</sample>
    <sample id="165">Die Sprachmodelle werden auf großen Web-Crowd-Daten trainiert.</sample>
    <sample id="166">Politische Nachrichtenmedien sind in ihren Pretraining-Daten gut abgedeckt. Laut einer Umfrage des C four-Korpus können wir sehen, dass die New York Times, Los Angeles Times, The Guardian, Huffington Post usw. in der Sprachmodell-Trainingsdaten gut abgedeckt sind.</sample>
    <sample id="167">Dies hat ein gemischtes Glück für Sprachmodellanwendungen geschaffen.</sample>
    <sample id="168">Auf der einen Seite waren sie in der Lage, aus verschiedenen Perspektiven zu lernen, was die Demokratie und die Vielfalt der Ideen feiert. Auf der anderen Seite sind diese unterschiedlichen politischen Meinungen inhärent sozial voreingenommen und können zu potenziellen Fairnessproblemen in Downstream Task Applications führen.</sample>
    <sample id="169">Zu diesem Zweck schlagen wir vor, die Pipeline der politischen Voreingenommenheit von vortrainierten Daten über Sprachmodelle bis hin zu Downstream-Aufgaben zu untersuchen, insbesondere durch die folgenden Fragen.</sample>
    <sample id="170">Zuerst, wie bewerten wir die politische Neigung von Sprachmodellen und welche Rolle die Trainingsdaten dabei spielen könnten?</sample>
    <sample id="171">Zweitens, wie funktionieren Sprachmodelle mit unterschiedlichen politischen Implikationen tatsächlich bei Downstream-Aufgaben und ob dies zu Fairness-Problemen in NLP-Anwendungen führen könnte.</sample>
    <sample id="172">Insbesondere haben wir vorgeschlagen, Sprachmodelle mit verschiedenen Prompt-Formaten zu testen, indem wir politische Fragebögen wie den politischen Kompass-Test verwenden. Dies stellt sicher, dass wir eine automatische Bewertung durchführen, die auf der politischen Wissenschafts-Literatur basiert.</sample>
    <sample id="173">Einige vorläufige Ergebnisse zeigen also, dass erste Sprachmodelle unterschiedliche politische Neigungen haben. Sie besetzen alle vier Quadranten des politischen Kompasses.</sample>
    <sample id="174">Wir können auch sehen, dass GPT vier das liberale Sprachmodell von allen ist. Und GPT-Theorien sind im Allgemeinen sozial liberaler als Burt-Theorien und ihre Varianten.</sample>
    <sample id="175">Zweitens wollen wir untersuchen, inwieweit die politischen Voreingenommenheiten von Sprachmodellen tatsächlich in den Trainingsdaten enthalten sind.</sample>
    <sample id="176">Wir können also ein kontrolliertes Experiment durchführen, indem wir die Sprachmodell-Checkpoints weiter vortrainieren, auf sechs verschiedenen Parteien und Korpora, getrennt in Nachrichten und soziale Medien, weiter in ihre politische Ausrichtung unterteilt.</sample>
    <sample id="177">Durch das weitere Prätaining von Sprachmodellen auf solchen parteiischen Korpora können wir sehen, dass sich auch die ideologischen Koordinaten des Sprachmodells entsprechend verschieben.</sample>
    <sample id="178">Zum Beispiel für Roberta, weiter fein abgestimmt, weiter auf dem linksliegenden Reddit-Korpus trainiert, können wir eine erhebliche liberale Verschiebung in Bezug auf seine sehen.</sample>
    <sample id="179">In Bezug auf diese politischen Voreingenommenheiten.</sample>
    <sample id="180">Und wir versuchen auch zu untersuchen, ob Sprachmodelle die Polarisierung aufnehmen können, die in unserer modernen Gesellschaft vorherrscht.</sample>
    <sample id="181">Wir teilen also das Pretraining-Corpus in Pre, vierzig, fünften Präsidenten der Vereinigten Staaten und nach vierzig, fünften Präsidenten der Vereinigten Staaten auf. Wir trainieren die Sprachmodelle separat auf den beiden verschiedenen temporalen Corpora.</sample>
    <sample id="182">Wir können sehen, dass Sprachmodelle im Allgemeinen nach zwanzig siebzehn eine politische Neigung hatten, die weiter vom Zentrum entfernt ist. Dies zeigt also, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft aufnehmen können.</sample>
    <sample id="183">Zu guter Letzt bewerten wir Sprachmodelle mit unterschiedlichen politischen Implikationen auf die Erkennung von Hassrede und Fake-News-Erkennung für NLP-Anwendungen, die oft Sprachmodelle beinhalten und sehr bedeutende Auswirkungen haben könnten.</sample>
    <sample id="184">Wir sehen also, dass, wenn wir die Leistung pro Kategorie untersuchen, das heißt, wenn wir die Leistung trennen.</sample>
    <sample id="185">Bei unterschiedlichen Bevölkerungsgruppen oder politischen Neigungen der Nachrichtenmedien können wir ein Muster erkennen, dass zum Beispiel bei der Erkennung von Hassrede linkslinige Sprachmodelle besser sind.</sample>
    <sample id="186">Bei der Erkennung von Hassrede, die soziale Minderheiten ins Visier nimmt.</sample>
    <sample id="187">Allerdings sind wir schlechter darin, Hassreden zu erkennen, die eher mächtige Gruppen in unserer Gesellschaft ins Visier nehmen.</sample>
    <sample id="188">Und umgekehrt sind weiße Sprachmodelle besser darin, Hassrede zu erkennen, die sich an Weiße und Männer richtet, aber schlechter darin, Hassrede zu erkennen, die sich an schwarze LGBTQ- und andere Minderheitengemeinschaften richtet.</sample>
    <sample id="189">Ähnliche Trends gibt es auch für die Erkennung von Fake News, wo wir sehen, dass linkslastige Sprachmodelle besser darin sind, Fehlinformationen aus ihrem gegensätzlichen politischen Leaning zu erkennen und umgekehrt.</sample>
    <sample id="190">Wir zeigen weiter viele qualitative Beispiele, um zu sehen, dass Sprachmodelle mit unterschiedlichen politischen Bedeutungen.</sample>
    <sample id="191">Geben Sie unterschiedliche Vorhersagen für Beispiele von Hassrede und Fehlinformationen basierend auf ihren sozialen Kategorien. Es gibt eine Reihe weiterer Beispiele im Anhang, um dies weiter zu betonen.</sample>
    <sample id="192">Dies zeigt, dass es ein Fairness-Problem gibt, das sehr dringend ist, da es sich um politische Voreingenommenheiten von Sprachmodellen handelt.</sample>
    <sample id="193">Zum Beispiel, wenn ein rechte Sprachmodell auf Hassrede oder Fehlinformationen oder was auch immer abgestimmt und auf eine beliebte soziale Medienplattform eingesetzt würde,</sample>
    <sample id="194">Dies würde bedeuten, dass Menschen mit gegensätzlichen politischen Meinungen marginalisiert werden könnten und Hassreden, die Minderheitengruppen ins Visier nehmen, ohne jegliche Kontrolle ausbrechen könnten.</sample>
    <sample id="195">Dies hat also die Alarmglocke für uns läuten lassen, um die Fairnessprobleme anzuerkennen und anzugehen, die durch sprachmodelle politische Vorurteile verursacht werden.</sample>
    <sample id="196">Also ein bisschen Diskussion. Wir möchten auch hervorheben, dass wir das einzigartige Dilemma in Bezug auf Sprachmodelle, politische Voreingenommenheiten, die es zwischen Cila und Karibdis gibt, aufgedeckt haben.</sample>
    <sample id="197">Wenn wir also keine politischen Meinungen in den Trainingsdaten des Sprachmodells bereinigen, wird der Vorurteil von den Vortrainingsdaten auf Sprachmodelle und zu den nachgelagerten Aufgaben übertragen, was letztendlich Fairnessprobleme verursacht.</sample>
    <sample id="198">Wenn wir versuchen, irgendwie zu sanitieren, riskieren wir auch Zensur oder Ausschluss. Und es ist unglaublich schwer zu bestimmen, was tatsächlich neutral ist und welche Sprachmonitoreingaben beibehalten werden sollten. Es ist also so etwas wie das elektrische Trolley-Problem.</sample>
    <sample id="199">Okay, großartig. Ich denke, das ist so ziemlich alles, was ich für heute habe. Danke für Ihre Zeit.</sample>
    <sample id="200">Es gibt zwei Autoren an der Arbeit.</sample>
    <sample id="201">Bis zu 1024 Token.</sample>
    <sample id="202">Sie haben Domains wie www.youtube.com, www.youtube.com/watch?v=QzJjZ1a0z5I, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=2, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=3, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=4, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=5, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=6, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=7, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=8, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=9, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=10, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=11, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=12, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=13, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=14, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=15, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=16, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=17, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=18, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=19, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=20, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=21, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=22, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=23, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=24, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=25, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=26, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=27, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=28, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=29, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=30, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=31, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=32, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=33, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=34, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=35, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=36, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=37, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=38, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=39, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=40, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=41, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=42, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=43, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=44, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=45, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=46, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=47, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=48, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=49, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=50, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=51, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=52, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=53, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=54, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=55, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=56, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=57, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=58, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=59, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=60, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=61, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=62, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=63, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=64, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=65, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=66, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=67, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=68, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=69, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=70, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=71, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=72, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=73, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=74, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=75, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=76, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=77, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=78, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=79, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=80, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=81, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=82, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=83, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=84, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=85, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=86, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=87, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=88, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=89, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gkq6zjzjZzJZ1a0z5I&amp;index=90, www.youtube.com/watch?v=QzJjZ1a0z5I&amp;list=PL9gk</sample>
    <sample id="203">Positionalität ist die Perspektive, die Menschen aufgrund ihrer demografischen Merkmale, Identität und Lebensumstände haben.</sample>
    <sample id="204">Xiaoyushen Ma, Yousheng Gu, and Dittish Klako.</sample>
    <sample id="205">Ja, EDAtt passt zu einem bestehenden Offline-ST-Modell, da es die Verwendung eines einzigen Modells für jedes Latenzregime und die Handhabung der Latenz durch spezifische Parameter ermöglicht, ohne Retraining oder spezifische Architektur für CML-ST zu benötigen.</sample>
    <sample id="206">Es gibt zwei Autoren an der Arbeit.</sample>
    <sample id="207">Nein, das getestete Modell funktioniert nicht in der Testsuite.</sample>
    <sample id="208">Die drei Varianten von KITMUS sind: Background Pre-Train, Background Both und Background Inference.</sample>
    <sample id="209">Die Autoren gehören der Universität Wien an.</sample>
    <sample id="210">Die abschließende Forschungsfrage lautet: Wie viele und welche Arten von Daten (Clean vs. Noisy) sind für die Wirksamkeit von WSL erforderlich, und wie können diese Daten optimal genutzt werden?</sample>
    <sample id="211">Die Sensitivitätsmetrik misst die Konsistenz des Modells bei der Erzeugung derselben Ausgabe für die gleiche Aufgabe, unabhängig von der Wortwahlvariation.</sample>
    <sample id="212">Jing Wei Yi.</sample>
    <sample id="213">Eine höhere Sensitivität bedeutet eine bessere Leistung des Modells.</sample>
    <sample id="214">Die Modelle erhalten einen linguistischen Kontext, indem sie auf einem großen, vielfältigen Datensatz trainiert werden, der verschiedene Sprachen, Dialekte und Sprachvarianten umfasst.</sample>
    <sample id="215">Für eine gute Leistung an der WSL werden normalerweise 20 saubere Validierungsbeispiele benötigt.</sample>
    <sample id="216">Eszter Mosch und Dan Jarrowfsky gehören zur Universität von Texas in Austin.</sample>
    <sample id="217">Neue Methoden sind notwendig, weil aktuelle Ansätze nicht in der Lage sind, die Komplexität und die sich ständig verändernden Techniken der Medienmanipulation zu erfassen.</sample>
    <sample id="218">Maksat Aydin</sample>
    <sample id="219">Die Pipeline umfasst die Vorverarbeitung von Daten, die Entwicklung von Sprachmodellen und die Anwendung auf Downstream-Aufgaben, wobei die Untersuchung der Verbreitung politischer Vorurteile von der Datenerhebung bis zur Anwendung erfolgt.</sample>
    <sample id="220">Ja, der Vereinfachungsprozess unterscheidet sich zwischen DEplain-apa und Web. DEplain-apa hat mehr Reorderings und Wortadditions, während der Web-Korpus mehr Rephrasings aufweist.</sample>
    <sample id="221">Ja, Coscript ist öffentlich verfügbar.</sample>
    <sample id="222">Das Wasserzeichen wird genau, wenn die Anzahl der Trigger in der Eingabe größer als M ist, genau dem Ziel-Wert entsprechen.</sample>
    <sample id="223">Penn State University</sample>
    <sample id="224">Ja, Encoder-Decoder-Modelle wie mt5 können durch Training mit einer Mischung verschiedener Sprachen verbessert werden.</sample>
    <sample id="225">Ein Beispiel für eingeschränkte Sprachplanung ist das Erstellen eines Rezeptes für einen Schokoladenkuchen.</sample>
    <sample id="226">Sie stellen die Opazität ihrer Methode sicher, indem sie die Embedding von Sätzen auf dem VLOPCA-Dataset visualisieren und feststellen, dass es schwierig ist, zwischen den Backdoor-Embeddings und normalen Embeddings zu unterscheiden.</sample>
    <sample id="227">Die Arbeit nutzt bestehende PLMs, um ein neues PLM aufzubauen, indem sie die Modelle trainiert, um die Auswirkungen verschiedener Pretraining-Strategien zu analysieren. Dies ermöglicht es ihnen, die Leistung und Effizienz des neuen PLMs zu verbessern, indem sie auf den Erfahrungen und Fähigkeiten der vorhandenen Modelle aufbauen.</sample>
    <sample id="228">Basierend auf dem bereitgestellten Inhalt ist GPT-4 am wenigsten ausgerichtet auf China, da es in diesem Land am wenigsten akzeptabel ist.</sample>
    <sample id="229">Der Beispielsatz, der zeigt, wie das Modell das Wissen nutzt, das durch den Aufmerksamkeitsmechanismus gelernt wurde, lautet: "Und Sie können sehen, ein Beispiel auf der rechten Seite."</sample>
    <sample id="230">Die Leistung des Modells verbessert sich mit zunehmender Anzahl der Aufgaben.</sample>
    <sample id="231">Die Autoren vergleichen ihre Methode mit drei baumlosen Baselines: ReLU, Leaky ReLU und ELU.</sample>
    <sample id="232">Die beiden Co-Autoren stehen in einer beratenden Beziehung zum ersten Autor.</sample>
    <sample id="233">Das Team von Google.</sample>
    <sample id="234">Hallo zusammen. Ich bin Jenny, eine Doktorandin in der ersten Studienjahr an der Carnegie Mellon University. Und heute werde ich Ihre Arbeit vorstellen, anal positionality, die Design-Biasen von Datensätzen und Modellen charakterisieren.</sample>
    <sample id="235">Diese Arbeit wurde in Zusammenarbeit mit einigen Leuten an der University of Washington und dem Allen Institute for AI durchgeführt, nämlich Sebastian Santi, Ronan Labrosse, Katarina Rainerka und Martin Sapp.</sample>
    <sample id="236">Stellen wir uns also vor, Sie arbeiten für eine Zeitung und durchforsten Sie Kommentare zu Ihrem Artikel, um toxische Inhalte zu entfernen.</sample>
    <sample id="237">Sie könnten sich für eine beliebte API wie die Perspektive-API für die Erkennung von Toxizität entscheiden. Und das funktioniert wirklich gut, wenn Sie Carl Jones sind, wo die Perspektive-API in der Lage ist, toxische Instanzen korrekt zu erkennen.</sample>
    <sample id="238">Aber das ist bei Aditya Sharma nicht wirklich der Fall, wo die Perspektive der API nicht wirklich so sensibel gegenüber beleidigenden Begriffen ist, die in indischen Kontexten häufiger vorkommen.</sample>
    <sample id="239">Dies ist ein Beispiel für einen Designbias, bei dem wir systematische Leistungsunterschiede der Technologie zwischen Populationen sehen.</sample>
    <sample id="240">Design-Biasen wie die, die wir gerade gesehen haben, können aufgrund der Positionierung der NLP-Forscher und Modellentwickler auftreten. Positionierung ist einfach die Perspektiven, die Menschen aufgrund ihrer demografischen Identität und Lebenserfahrungen haben.</sample>
    <sample id="241">Dies ist ein Konzept, das in der kritischen Studien, insbesondere in feministischen und queer akademischen Räumen, weit verbreitet ist.</sample>
    <sample id="242">Und als Forscher kann die Positionalität den Forschungsprozess und seine Ergebnisse und Ergebnisse beeinflussen, weil sie die Entscheidungen, die Forscher treffen, verändern kann.</sample>
    <sample id="243">Und so ist eine Frage, die die Leute stellen könnten, ob Datenmengen und Modelle Positionale haben.</sample>
    <sample id="244">Wir versuchen nicht zu sagen, dass Modelle und Zellen und Datensätze selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen von echten Menschen und können daher bestimmte Positionierungen gegenüber anderen darstellen.</sample>
    <sample id="245">Frühere Arbeiten haben also anekdotische Beweise für eine Positionalität vorgewiesen, wie zum Beispiel kulturelle Lücken in Modellen und Datensätzen sowie theoretische Definitionen von Modellpositionalität.</sample>
    <sample id="246">Diese Arbeiten betrachten jedoch nicht wirklich, ob Endbenutzer mit den Datensätzen und Modellen selbst vergleichbar sind.</sample>
    <sample id="247">Und die Positionierung von Modellen und Datensätzen wird immer wichtiger, da NLP-Aufgaben subjektiver und sozialer werden.</sample>
    <sample id="248">Und es ist schwierig, zu charakterisieren, wie diese Positionierungen verzerrt sind, weil nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter APIs verborgen sind.</sample>
    <sample id="249">Um die Datensatz- und Modellpositionalität zu untersuchen, vergleichen wir die Anmerkungen mit echten Benutzern mit vorhandenen Datensätzen und Modellen.</sample>
    <sample id="250">Wir tun dies durch unseren Rahmen, die nl-Positionalität.</sample>
    <sample id="251">Unser Framework funktioniert in zwei Hauptschritten.</sample>
    <sample id="252">Der erste Schritt besteht darin, Datensätze mit verschiedenen Annotatoren neu zu annotieren.</sample>
    <sample id="253">Und wir sollten dies tun, anstatt uns die Demografie der ursprünglichen Datensatz-Annotationen anzusehen, da normalerweise nur wenige Annotatoren jede Instanz annotieren, und weil Demografien selten gesammelt und geteilt werden.</sample>
    <sample id="254">Und so entscheiden wir uns dafür, Daten neu zu annotieren, um viele Annotatoren zu gewinnen, zum Beispiel, und um eine reiche Menge an demografischen Daten zu erhalten.</sample>
    <sample id="255">Wir nehmen dann die Anmerkungen nach demografischen Merkmalen und vergleichen sie mit den Modellen und Datensätzen unter Verwendung eines Pearson's R-Korrelationswerts.</sample>
    <sample id="256">Und so unterscheidet sich unser Framework tatsächlich von der Literatur über Meinungsverschiedenheiten von Annotatoren, indem es Endbenutzer mit Modellen und Datensätzen, Vorhersagen und Labels vergleicht, im Gegensatz zu der Betrachtung von nur der Meinungsäußerung von Annotatoren oder der Modellierung von Annotatoren.</sample>
    <sample id="257">Unser Rahmenwerk wird weitgehend durch Lab in the Wild, eine Online-Crowdsourcing-Plattform, ermöglicht, ehemaliger hci-Mitarbeiter.</sample>
    <sample id="258">Und Lab in the Wild ist eine Online-Experimentierplattform, auf der wir vielfältige Freiwillige rekrutieren können, im Gegensatz zu Plattformen wie M turk, die hauptsächlich Teilnehmer aus den USA oder Indien haben. Und darüber hinaus ist Lab in the Wild in der Lage, hochwertige Daten zu erhalten.</sample>
    <sample id="259">Wir hosten zwei Aufgaben auf Lab in the Wild, eine davon ist die soziale Akzeptanz. Und die Art und Weise, wie das funktioniert, ist, dass die Teilnehmer eine Situation aus dem Social Chemistry-Datensatz lesen und dann schreiben, wie sozial akzeptabel eine Situation ist.</sample>
    <sample id="260">Danach können sie, um sich im Studium zu engagieren, ihre Antworten mit denen anderer KI vergleichen.</sample>
    <sample id="261">Wir haben diese Anmerkungen dann mit Social Chemistry Delphi und Gpt Four verglichen.</sample>
    <sample id="262">Wir replizieren dann ein sehr ähnliches Setup für die Aufgabe der Erkennung von Toxizität und Hassrede, bei der sie eine Instanz von Dana Hate lesen und schreiben, ob sie glauben, dass es sich um eine Hassrede handelt.</sample>
    <sample id="263">Wir haben diese Anmerkungen dann mit dynahate, perspective api, rewire api, hate roberta und gpt vier verglichen. Unsere Studie am Ende umfasste über sechzehntausend Anmerkungen von über tausend Annotatoren aus siebenundachtzig Ländern.</sample>
    <sample id="264">So sind wir jetzt besser gerüstet, um zu beantworten, mit wem sich nlp-Datensätze und Modelle am besten ausrichten. Wir stellen fest, dass es in nlp eine Positionierung gibt.</sample>
    <sample id="265">Zum Beispiel finden wir, dass Datensätze und Modelle am besten mit englischsprachigen Ländern übereinstimmen. Für die gp d four, die Analyse der sozial akzeptablen Verhaltensweisen, finden wir, dass sie am besten mit konfuzianischen und englischsprachigen Ländern übereinstimmt. Wir finden, dass Dinahite auch am besten mit englischsprachigen Ländern übereinstimmt.</sample>
    <sample id="266">Wir finden auch die meisten zusätzlichen Übereinstimmungen mit Menschen, die eine College-Ausbildung haben. Bei der GPT-Four- und der sozialen Akzeptanzaufgabe finden wir also, dass es am besten mit Menschen mit einer College-Ausbildung oder einem Graduiertenstudium übereinstimmt.</sample>
    <sample id="267">Und wir finden das gleiche für Donut Hate, wo es am besten zu Menschen mit einer Hochschulausbildung passt.</sample>
    <sample id="268">Wenn Modelle und Datensätze jedoch an bestimmte Bevölkerungsgruppen angepasst werden, bleiben einige unberücksichtigt.</sample>
    <sample id="269">Ein Beispiel dafür ist, dass Datensätze und Modelle weniger mit nichtbinären Menschen im Vergleich zu den männlichen und weiblichen Gegenstücken übereinstimmen. Wir finden dies in der gp d four social acceptability task sowie in der dyna hate task analysis.</sample>
    <sample id="270">Wenn also Position in Allee und Lp besteht, was können wir dagegen tun?</sample>
    <sample id="271">Wir haben also ein paar Empfehlungen dafür. Die erste ist, ein Protokoll aller relevanten Designentscheidungen während des gesamten Forschungsprozesses zu führen. Und die andere ist, NLP-Forschung mit der Perspektivismuslinse durchzuführen.</sample>
    <sample id="272">Unsere dritte Empfehlung ist, spezialisierte Datensätze und Modelle innerhalb von vier spezifischen Gemeinschaften zu erstellen. Und ein gutes Beispiel dafür ist die Muscogee-Initiative. Ich meine, wir wollen betonen, dass inklusives NLP nicht nur bedeutet, dass alle Technologien für alle funktionieren.</sample>
    <sample id="273">Und so schließt sich unsere Präsentation ab. Aber wenn Sie mehr erfahren möchten, können Sie sich gerne auf unserem Dashboard die aktuellsten Analyseergebnisse und unsere Arbeit ansehen.</sample>
    <sample id="274">Die Referentin geht auf vier Probleme von SimulST ein: spezifische Architekturen, lange und komplizierte Trainingsverfahren, verschiedene Optimierungsziele und mehrere Modelle für unterschiedliche Latenzregime.</sample>
    <sample id="275">Um soziale und politische Verzerrungen in Datensätzen beim Training von NLP-Modellen effektiv zu reduzieren, ist es wichtig, eine ausgewogene und vielfältige Datensammlung zu verwenden, die verschiedene Perspektiven und Standpunkte widerspiegelt. Darüber hinaus kann die Implementierung von Bias-Detection- und -Minderungstechniken während des Trainingsprozesses helfen, Verzerrungen zu identifizieren und zu korrigieren. Es ist auch entscheidend, die Modelle regelmäßig zu überwachen und zu bewerten, um sicherzustellen, dass sie fair und unvoreingenommen bleiben.</sample>
    <sample id="276">Hallo, ich bin Si Yu Yuan von der Fudan University. Ich bin hier, um unsere Arbeit vorzustellen, die das Unterscheiden von Skriptwissen aus großen Sprachmodellen für die eingeschränkte Sprachplanung.</sample>
    <sample id="277">Im Alltag planen Menschen ihre Handlungen oft nach Schritt-für-Schritt-Anweisungen in Form von garantierten Skripten.</sample>
    <sample id="278">Frühe Arbeit hat Spraismodellierungen genutzt, um sich für abstrakte Ziele von stereotypischen Aktivitäten wie „Kuchen machen“ vorzubereiten und gezeigt, dass große Sprachmodelle Ziele effektiv in Schritte zerlegen können.</sample>
    <sample id="279">Die bisherigen Arbeiten konzentrieren sich jedoch hauptsächlich auf die Planung abstrakter Ziele stereotypischer Aktivitäten. Die Planung von Zielen mit spezifischen Einschränkungen, wie zum Beispiel die Herstellung eines Schokoladenkuchens, bleibt noch unstudiert.</sample>
    <sample id="280">In diesem Papier definieren wir das Problem der eingeschränkten Sprachplanung.</sample>
    <sample id="281">Was unterschiedliche Einschränkungen für die Ziele der Planung auferlegt. Ein abstrakter Ziel kann von verschiedenen realen, spezifischen Zielen mit vielschichtigen Einschränkungen geerbt werden. Ein guter Planer sollte Skripte schreiben, die vernünftig und den Einschränkungen treu sind.</sample>
    <sample id="282">In diesem Papier bewerten und verbessern wir zunächst die eingeschränkte Sprachplanungskompetenz großer Sprachmodelle.</sample>
    <sample id="283">Da es keinen Datensatz mit spezifischen Zielen gibt, um unsere Studie zu unterstützen.</sample>
    <sample id="284">Wir müssen diese Ziele zuerst erreichen. Wie in der Tabelle gezeigt, erweitern wir die abstrakten Ziele mit mehrschichtigen Einschränkungen für den Menschen. Die Schleife, Datenakquisition, Verwendung von instrugpt.</sample>
    <sample id="285">Wir nehmen hundert spezifische Ziele und bewerten die von großen neuronalen Modellen generierten Skripte.</sample>
    <sample id="286">Diese Tabelle zeigt die Gesamtgenauigkeit der Ergebnisse. Wir stellen fest, dass alle Lite-Modelle bei der Planung für spezifische Ziele unbefriedigende Ergebnisse erzielen.</sample>
    <sample id="287">Dann führen wir eine detaillierte Analyse durch, um zu untersuchen, warum neuronale Netzmodelle scheitern.</sample>
    <sample id="288">Die Ergebnisse in der Abbildung zeigen, dass die semantische Vollständigkeit in generierten Skripten akzeptabel ist, aber die Treue zu den Einschränkungen nicht garantiert werden kann.</sample>
    <sample id="289">Wir tauchen in eine feinere Unterteilung der Themenkategorien von Einschränkungen ein, die in WikiHow definiert sind. Die Heatmap in der Abbildung zeigt, dass die Planungsleistung von Instruktoren, Gpds, für Mädchen verschiedener Kategorien erheblich variiert.</sample>
    <sample id="290">Frühere Studien haben gezeigt, dass die Ausgabequalität von Sprachmodellen stark variiert, was zu schlechter Leistung führt. Daher haben wir die Idee des übergenerierten Zensfilters übernommen, um die Generierungseigenschaften zu verbessern.</sample>
    <sample id="291">Wir zeigen zunächst Einschränkungstypen mit Beispielen für instruct-gpt und erhalten spezifische Ziele basierend auf den angegebenen abstrakten Zielen.</sample>
    <sample id="292">Dann gibt instrGPT über generierte Schlüssel für spezifische Ziele.</sample>
    <sample id="293">Als nächstes wird ein Filtermodell entwickelt, um die wahrscheinlichen Skripte auszuwählen.</sample>
    <sample id="294">Wir konvertieren Skripte und Ziele in Instruktionsgpt-Embeddings und berechnen Cosine-Similarity und Ähnlichkeitswerte, um die semantische Ähnlichkeit zu messen.</sample>
    <sample id="295">Darüber hinaus bewahren wir das Skript, das die Schlüsselwörter des Zielkriteriums enthält. Wir behalten das Skript nur, wenn das Zielziel die höchste Punktzahl in der Zielseite erreicht.</sample>
    <sample id="296">Mit unserer Methode kann instruct-gpt Segmente von höherer Qualität generieren. Unsere Methode verbessert die Planierbarkeit sowohl in der semantischen Vollständigkeit als auch in der Treue zu den Einschränkungen.</sample>
    <sample id="297">Da große Sprachmodelle kostspielig in der Bereitstellung sind, ist es wichtig, die Sprachplanungskompetenz kleinerer und spezialisierterer Modelle zu ermöglichen. Die Erstellung von Datensätzen ist ein wesentlicher Schritt zu diesem Ziel.</sample>
    <sample id="298">Allerdings ermöglichen frühere Studien keine Planung für spezifische Ziele, und die manuelle Datensatzanmerkung ist teuer.</sample>
    <sample id="299">Daher folgen wir der Idee der symbolischen Wissensdestillation, um eingeschränkte Sprachplanungsdatenbanken aus großen Sprachmodellen zu destillieren.</sample>
    <sample id="300">Wir wenden unsere Methode zur Erstellung eines Datensatzes für die eingeschränkte Sprachplanung an, der als Code-Script bezeichnet wird.</sample>
    <sample id="301">Insgesamt generieren wir fünfundfünfzigtausend spezifische Ziele mit Skripten. Um die Qualität der Validierung und Testsets sicherzustellen, bitten wir die von der Cloud bezahlten Arbeiter, die falschen Proben zu finden und zu überarbeiten.</sample>
    <sample id="302">Diese Abbildung zeigt die eingeschränkte Verteilung von CodeScript. Wir stellen fest, dass CodeScript eine höhere Plausibilität in den generierten spezifischen Zielen aufweist. Mit CodeScript können wir kleinere, aber spezialisierte Modelle für die eingeschränkte Sprachplanung trainieren.</sample>
    <sample id="303">Wir haben festgestellt, dass tf-idf auf dem Code-Rate die Qualität der generierten Skripte verbessern kann, was darauf hindeutet, dass kleinere Modelle größere Modelle unterstützen können, wenn sie richtig auf geeigneten Datensätzen trainiert werden.</sample>
    <sample id="304">Zusammenfassend haben wir das eingeschränkte Sprachplanungsproblem etabliert, die eingeschränkte Sprachplanungsfähigkeit großer Sprachmodelle bewertet und eine Overgeneration-Filtermethode für große Sprachmodelle entwickelt.</sample>
    <sample id="305">Wir verwenden große Sprachmodelle, um einen hochwertigen Skriptdatensatz, Code Script, für die eingeschränkte Sprachplanung zu generieren. Wir hoffen, dass der Code-Skript-Datensatz eine wertvolle Ressource für die Forschung zur Sprachplanung sein kann.</sample>
    <sample id="306">Vielen Dank für Ihre Zeit. Bitte finden Sie weitere Details zu Corscript in unserem Papier.</sample>
    <sample id="307">Die Sprachgewandtheit von PaLM ist vergleichbar mit den besten Systemen, aber es gibt Unterschiede in der Genauigkeit.</sample>
    <sample id="308">Die wichtigsten Eigenschaften eines Wasserzeichenverfahrens sind: 1) Anwendbarkeit auf Embedding Services, 2) Erhaltung der Embedding-Utility, 3) Konvertierbarkeit für Angreifer oder einfache Entfernung, und 4) Übertragbarkeit während des Modellextraktionsprozesses.</sample>
    <sample id="309">Die englischen TED Talks wurden in 14 verschiedene Sprachen übersetzt.</sample>
    <sample id="310">1000 Instanzen.</sample>
    <sample id="311">Die Cosine- und L2-Distanzmetriken werden verwendet, um den Unterschied zwischen harmlosen und Backdoor-Datensätzen zu messen.</sample>
    <sample id="312">Modelltypen wie encoder-only, encoder-decoder und multilingual pre-trained encoder-decoder wurden auf neun Datensätzen bewertet.</sample>
    <sample id="344">Die Autoren wählen Wörter aus, die in einem moderaten Frequenzintervall vorkommen, was bedeutet, dass sie nicht zu häufig oder selten sind.</sample>
    <sample id="345">Hallo zusammen. Mein Name ist Xu Heng. Heute werde ich unser Papier vorstellen, ob die im Jahr zweitausenddrei genannten Entitätstagger im Jahr zweitausenddrei noch funktionieren? Fangen wir an.</sample>
    <sample id="346">Unser Papier untersucht das Problem der Verallgemeinerung unter Verwendung der Aufgabe der Entitätserkennung, oder der NER-Aufgabe.</sample>
    <sample id="347">Wir haben festgestellt, dass Modelle fast zwanzig Jahre lang Conll two thousand three verwendet haben, um ner zu entwickeln. Und das wirft natürlich mehrere Probleme auf. Erstens, können diese Modelle auf moderne Daten verallgemeinert werden?</sample>
    <sample id="348">Und wenn wir neue Tags entwickeln, was ist für eine gute Verallgemeinerung nötig?</sample>
    <sample id="349">Gleichzeitig, wenn wir eine schlechte Verallgemeinerung beobachten, was verursacht den Leistungsabfall dieser Modelle?</sample>
    <sample id="350">Um diese Probleme zu untersuchen, haben wir den Conll-Datensatz entwickelt. Dies ist ein Datensatz, den wir von Reuters News von Twenty Twenty gesammelt und dann mit den gleichen Conll Two Thousand Three Annotations Guidelines annotiert haben.</sample>
    <sample id="351">Wir haben dann über zwanzig Modelle auf Conker Two Thousand Three optimiert. Wir haben sie sowohl auf dem Conker Three-Testset als auch auf dem Conker plus plus-Testset bewertet.</sample>
    <sample id="352">Und zuletzt, aber nicht zuletzt haben wir den prozentualen Anstieg von F eins berechnet, um die Verallgemeinerung jedes Modells zu bewerten.</sample>
    <sample id="353">Was ist also für eine gute Verallgemeinerung nötig? Durch unsere Experimente haben wir festgestellt, dass es drei Hauptzutaten gibt, die benötigt werden.</sample>
    <sample id="354">Die erste ist die Modellarchitektur. Durch unsere Experimente haben wir festgestellt, dass die Transformermodelle normalerweise besser auf neue Daten generalisieren.</sample>
    <sample id="355">Die zweite Komponente ist die Modellgröße. Wir haben festgestellt, dass in der Regel größere Modelle zu einer besseren Verallgemeinerung führen.</sample>
    <sample id="356">Und schließlich wissen wir alle, dass die Anzahl der Feinabstimmungsexempel die Leistung einer Downstream-Aufgabe direkt beeinflusst. Hier haben wir auch festgestellt, dass mehr Feinabstimmungsexempel tatsächlich auch zu einer besseren Verallgemeinerung führen.</sample>
    <sample id="357">Zur nächsten Frage, was verursacht den Leistungsabfall einiger Modelle?</sample>
    <sample id="358">Wir hatten zwei Hypothesen. Die erste ist adaptive Überanpassung, die Überanpassung ist die Ursache dafür, dass wir den gleichen Test mehrmals wiederverwenden. Und dies zeigt sich normalerweise, wenn die Abnahme bei einem neuen Test wiederkehrt.</sample>
    <sample id="359">Die zweite Hypothese ist die zeitliche Drift, die sich durch die Leistungsabnahme aus der zunehmenden zeitlichen Lücke zwischen den Trainings- und Testdaten auszeichnet.</sample>
    <sample id="360">Für adaptive Überanpassung haben wir gesehen, dass die rote beste Passlinie aus dem Diagramm auf der rechten Seite einen Gradienten hat, der größer als eins ist.</sample>
    <sample id="361">Das bedeutet, dass jede Verbesserungseinheit, die wir auf Kerl zweitausenddrei gemacht haben, sich in mehr als einer Verbesserungseinheit auf Kerl übersetzt, was bedeutet, dass es keine abnehmenden Erträge gibt.</sample>
    <sample id="362">Und das zeigt uns, dass adaptive Überanpassung in diesem Fall nicht beobachtet wird.</sample>
    <sample id="363">Was ist also mit temporaler Trift?</sample>
    <sample id="364">Für die zeitliche Drift haben wir ein Experiment durchgeführt, um einige Modelle mit neueren Daten zu re-trainen oder weiter zu pre-trainen. Und wir fanden, dass die Leistung mit größerer zeitlicher Lücke abnimmt.</sample>
    <sample id="365">Und das bestätigt unsere Hypothese, dass die Hauptursache für den Leistungsabfall die zeitliche Drift ist.</sample>
    <sample id="366">Unsere Schlussfolgerung ist, dass wir für eine gute Verallgemeinerung ein besseres Modellarchitektur, eine größere Modellgröße sowie mehr Feinabstimmungsebenen benötigen. Und diese gehen Hand in Hand. Wir können nicht nur eine Zutat haben, sondern alle anderen.</sample>
    <sample id="367">Gleichzeitig haben wir auch festgestellt, dass der Leistungsabfall hier durch zeitliche Verschiebungen verursacht wird. Und irgendwie überraschend ist es nicht durch adaptives Überfitting verursacht, obwohl Conll Two Thousand Three seit über zwanzig Jahren verwendet wird.</sample>
    <sample id="368">Zur Frage, die wir im Titel unseres Papiers aufgeworfen haben, ob Conner zwei, oh, oh, drei Tagger in Twenty Twenty Three noch funktionieren? Und wir fanden, dass die Antwort tatsächlich ein lautes Ja ist.</sample>
    <sample id="369">Wir hoffen, dass unser Papier mehr Forschung über die Verbesserung der Modellverallgemeinerungen anregt.</sample>
    <sample id="370">Und schließlich, bitte überprüfen Sie bitte unsere Arbeit, unsere Daten, und wenn Sie Fragen haben, zögern Sie nicht, mich zu kontaktieren. Vielen Dank.</sample>
    <sample id="397">Die Sprachsegmentgröße, die bei dem Ansatz verwendet wird, beträgt 128.</sample>
    <sample id="398">Servin ist ein Judge.</sample>
    <sample id="399">Der wichtigste Faktor ist die Qualität des Beispiels.</sample>
    <sample id="400">Die Arbeiten konzentrieren sich auf GPT-4 und GPT-3, wobei GPT-4 als das liberale Sprachmodell hervorgehoben wird.</sample>
    <sample id="401">Das Modell verwendet Aufmerksamkeitswerte aus mehreren Ebenen.</sample>
    <sample id="402">Ein Beispiel für direkte Inferenz ist, den Namen eines Songs zu sagen, wie "Easy on Me", oder seine Position, wie "die erste".</sample>
    <sample id="403">Fudan University</sample>
    <sample id="404">There are three authors involved in the work.</sample>
    <sample id="405">Ja</sample>
    <sample id="406">Das Beispiel ist, wenn jemand "Womyn-Warrior" anstelle von "Warrior" verwendet, um eine Frau zu beschreiben, die als Kriegerin gilt.</sample>
    <sample id="407">Die Frage ist nicht klar formuliert, da der englische Inhalt nur angibt, dass die Transformer-Modelle gut generalisieren. Es wird nicht erwähnt, welche Modelle nicht gut generalisieren.</sample>
    <sample id="408">Die Testdatensätze heißen "Clean Data" und "WSL".</sample>
    <sample id="409">Es gibt zwei Autoren an der Arbeit, Makhtota und Martin.</sample>
    <sample id="410">Die Autoren arbeiten mit mehreren Modalitäten.</sample>
    <sample id="439">Die Autoren meinen, dass es ein zu wenig erforschtes Gebiet im Bereich der NLU ist, um sowohl prädiktive als auch inferenzielle Zeit zu integrieren und zu nutzen.</sample>
    <sample id="440">Ying und Zhi Yang.</sample>
    <sample id="441">Ja, Coscript hat eine Qualitätskontrolle durchlaufen, indem sie 55.000 spezifische Ziele mit Skripten generiert und Crowdsourced Workeren um die Überprüfung und Korrektur falscher Beispiele gebeten hat.</sample>
    <sample id="442">Die Grenzen bestehender Ressourcen für kontextbasierte Übersetzung liegen in ihrer Unterstützung für begrenzte Arten von kontextabhängigen Übersetzungen und in begrenzten Sprachen, da sie typischerweise auf Domänenwissen und menschliche Kuration angewiesen sind.</sample>
    <sample id="443">Hallo. Ich werde über unsere Arbeit zur Lösung indirekter Äquivalenzausdrücke für die Entitätsauswahl sprechen, in der wir den altentitieskorpus einführen.</sample>
    <sample id="444">Und mein Name ist Javad Hosseini, und das ist eine gemeinsame Arbeit mit Philip Radlinski, Sylvia Parati und Anil.</sample>
    <sample id="445">Unser Ziel ist es, die Sprache der Benutzer zu verstehen, wenn sie eine Wahl treffen möchten. Und ich stelle diese alternative Frage, haben Sie mich leicht gemacht gemeint, oder habe ich ein Gefühl? Hier möchte ein Benutzer zwischen diesen beiden Optionen auswählen.</sample>
    <sample id="446">Das offensichtlichste ist die Verwendung einer direkten Referenz. Zum Beispiel, indem man den Namen des Songs Easy on Me oder seine Position, die erste, sagt.</sample>
    <sample id="447">Aber manchmal ist eine indirekte Frage angemessener, um ein natürlicheres Gespräch zu führen. Dies kann passieren, wenn der Benutzer sich nicht an den Namen des Songs erinnern kann.</sample>
    <sample id="448">Oder die Aussprachen sind zu ähnlich zueinander und schwer zu unterscheiden.</sample>
    <sample id="449">Oder wenn der Benutzer eine Präferenz angeben möchte. Hier sind einige Beispiele für direkte Referenzen, zum Beispiel die neuere oder die, die nicht energetisch ist.</sample>
    <sample id="450">Dies ist ein wichtiges Problem in Konversationssystemen und auch für die Benchmarking von LLMs Entityunderstanding.</sample>
    <sample id="451">Wir sind uns eines öffentlichen Datensatzes, eines größeren öffentlichen Datensatzes für eine Aufgabe nicht bewusst, also sammeln wir einen mit der Cloud-Anmerkung. Unser Datensatz umfasst drei verschiedene Domänen, Musik, Bücher und.</sample>
    <sample id="452">Unsere Datensammlungsmethode betont die Informalität mit einem Cartoon-Vollendungssatz.</sample>
    <sample id="453">Der Cartoon hat drei Sprachblasen. In der ersten Blase sagt Bob, erinnere dich an das Lied, das wir gestern gehört haben. Und damit setzt Bob den Dialogkontext.</sample>
    <sample id="454">In der zweiten Rede, Bubel, sagt Alice, meinst du leichtfertig mit mir oder habe ich einen Fehler gemacht?</sample>
    <sample id="455">Was ist die alternative Frage? Und in der dritten Rede Bubble verwendet Bob eine indirekte Anrede, um eine dieser Entitäten auszuwählen. Zum Beispiel die neue.</sample>
    <sample id="456">Wir bieten die ersten und zweiten Sprachblasen automatisch an, aber die dritte wird vom Anotator ausgefüllt. Die erste Sprachblase wird aus einer Handynummer ausgewählt.</sample>
    <sample id="457">Die zweite, die alternative Frage, wird wie folgt generiert.</sample>
    <sample id="458">Wir verwenden immer eine einfache Vorlage. Meinen Sie A oder B, wobei A und B von Wikipedia stammen?</sample>
    <sample id="459">Hier sind die verschiedenen Stichprobenmethoden, die wir verwendet haben. Wenn wir höher in der Liste gehen, werden die Entitäten ähnlicher und es ist normalerweise schwieriger, die Unklarheit zu beseitigen.</sample>
    <sample id="460">Der erste ist einheitliche Werbung.</sample>
    <sample id="461">Die zweite ist, wenn die Entitäten ähnliche Titel haben, zum Beispiel zwei Bücher mit dem Namen The Return.</sample>
    <sample id="462">Der dritte ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben. Und schließlich, wenn sie ähnliche Infoboxen oder Attribute auf Wikipedia haben, zum Beispiel das gleiche Genre oder den gleichen Künstler.</sample>
    <sample id="463">Wenn wir diese alternative Frage den Annotatoren zeigen, kennen sie den Namen dieser Entitäten, aber sie wissen nicht unbedingt etwas über die Entität.</sample>
    <sample id="464">Was wir also tun, ist, dass wir einige Hintergrundinformationen über die beiden Entitäten zeigen. Für Songs zeigen wir einfach einen Google-Suchlink zu jedem Song.</sample>
    <sample id="465">Und dann bitten Sie die Anmerkungen, mindestens einige der Songs zu hören und über jeden Song zu lesen. Hier ist zum Beispiel das Google-Suchergebnis für den Song.</sample>
    <sample id="466">Für die Rezepte und Bücher zeigen wir einige Hintergrundtexte aus Wikipedia. Für Rezepte zeigen wir zusätzlich ihre Bilder erneut aus Wikipedia, damit die Anmerkungen wissen, wie sie aussehen.</sample>
    <sample id="467">Dann bitten wir die Anmerkungen, eine dieser Entitäten auszuwählen, zum Beispiel hier die erste, und beschreiben sie mit drei bis fünf indirekten Referenzausdrücken.</sample>
    <sample id="468">Zum Beispiel die mit der Klaviermusik. Hier sind einige Beispiele aus unserem Datensatz. Zum Beispiel die ohne Worte, nicht die mit dem zwölfjährigen Jungen oder die fiktive, oder kommt aus Aserbaidschan und.</sample>
    <sample id="469">Der Entitätenkorpus hat sechstausend alternative Fragen in drei Domänen, und er hat vierzigtausend indirekte Referenzausdrücke. Die Ergebnisse mit dem T five X Large-Modell sind unten zusammengefasst.</sample>
    <sample id="470">Wenn das Sprachmodell Zugang zu genau den gleichen Hintergrundkenntnissen wie die Anmerkungen hat, ist die Genauigkeit wirklich hoch. Es sind etwa zweiundneunzig bis fünfundneunzig Prozent. Aber das ist nicht realistisch.</sample>
    <sample id="471">Wenn das Sprachmodell Zugriff auf teilweise überlappende Hintergrundwissen hat, liegt die Genauigkeit zwischen zweiundachtzig und siebenundachtzig Prozent, was realistischer ist. Zum Beispiel, wenn das Sprachmodell das Hintergrundwissen abruft.</sample>
    <sample id="472">Wenn das Sprachmodell nur auf Entitätsnamen zugreifen kann, beträgt die Genauigkeit nur sechzig Prozent. Es gibt also viel Raum für Verbesserungen. Wir haben auch gezeigt, dass die Modelle domänengenerierend sind. Hier ist ein Link zu unserem Datensatz. Danke.</sample>
    <sample id="473">The approach is compared with two existing SimulST strategies: the wait key strategy and the local agreement.</sample>
    <sample id="474">Die Autoren gehören der Universität Bern an.</sample>
    <sample id="475">Der/die Referent/in heißt Jenny.</sample>
    <sample id="476">Es sind drei Autoren an der Arbeit beteiligt.</sample>
    <sample id="477">Hallo, ich bin Sarah Papi von der Universität von Trento und Fondazione Bruno Kessler und ich werde kurz das Paper attention as a guide for simultaneous speech translation vorstellen, das eine gemeinsame Arbeit mit Matteo Negri und Marco Turchi ist.</sample>
    <sample id="478">Was ist Simul-Sprache-Übersetzung? Simul-Sprache-Übersetzung, oder SimulSt, ist der Prozess der Übersetzung von gesprochener Sprache in Text in einer anderen Sprache in Echtzeit und ermöglicht so die Kommunikation über Sprachen hinweg.</sample>
    <sample id="479">Und was sind die Probleme der aktuellen Simul-St-Modelle? Spezifische Architekturen werden normalerweise trainiert, indem zusätzliche Module eingeführt werden, die optimiert werden müssen.</sample>
    <sample id="480">Lange und komplizierte Trainingsverfahren, z. B. Training mit verschiedenen Optimierungszielen.</sample>
    <sample id="481">Und das Training und die Wartung mehrerer Modelle, um verschiedene Latenzregime zu erreichen. Zum Beispiel das Training eines Modells mit einer durchschnittlichen Latenz von einer Sekunde und eines anderen mit zwei Sekunden Latenz und so weiter.</sample>
    <sample id="482">Was ist also unsere Lösung?</sample>
    <sample id="483">Erstens, um bereits vorhandene off-line-Models ohne Neutraining oder spezifische Architektur für Simul-Models zu verwenden, verwenden Sie nur ein Modell für jedes Latenzregime und behandeln Sie die Latenz durch spezifische Parameter.</sample>
    <sample id="484">Und es nutzt das Wissen, das ich bereits durch das Modell erlernt habe, durch den Aufmerksamkeitmechanismus zwischen Audio-Eingabe und Textausgabe, das heißt den Cross-Attention-Mechanismus. Und Sie können ein Beispiel rechts sehen.</sample>
    <sample id="485">Unsere Lösung ist, ein Dat oder Encoder-Decoder-Attention vorzuschlagen. Und es ist eine Strategie, bei der wir entscheiden, ob wir eine Teilübersetzung basierend auf dem Punkt der Aufmerksamkeit senden oder nicht.</sample>
    <sample id="486">Ein Wort wird abgebrochen, wenn die Aufmerksamkeit nicht konzentriert ist, d.h. diese Summe ist unter einem bestimmten Schwellenwert alpha, auf die letzten Lambda-Speech Frames, was bedeutet, dass die empfangenen Informationen nicht stabil genug sind.</sample>
    <sample id="487">Zum Beispiel, wenn wir einen Sprechabschnitt erhalten, der ich über sprechen werde, und unser Modell die Übersetzung ins Deutsche vorhersagt.</sample>
    <sample id="488">Und wir werden uns die Cross-Attributionen und Gewichte ansehen.</sample>
    <sample id="489">Wir werden sehen, dass die ersten beiden Wörter auf die frühesten empfangenen Sprachrahmen verweisen, während das letzte Wort auf die letzten empfangenen Sprachrahmen verweist, die als Lambda-Sprachrahmen bezeichnet werden.</sample>
    <sample id="490">Das bedeutet, dass die ersten beiden Wörter weggelassen werden.</sample>
    <sample id="491">Da die Summe der Kreuzentropie über einem bestimmten Trennwert alpha liegt, lassen wir das letzte Wort nicht aus und warten auf einen weiteren Sprechverlauf.</sample>
    <sample id="492">Wenn wir weitermachen und eine weitere Rede erhalten, und unser Modell drei Wörter vorhersagt, und wir werden uns die Kreuz-Attentionen ansehen.</sample>
    <sample id="493">Wir werden sehen, dass kein Wort auf die letzten Lambda-Speech Frames verweist.</sample>
    <sample id="494">Das bedeutet, dass diese drei Wörter weggelassen werden.</sample>
    <sample id="495">Wenn Sie sich die Hauptergebnisse darüber ansehen.</sample>
    <sample id="496">Wir zeichnen die Ergebnisse der simultanen Spracherkennung in Grafiken, in denen wir blau auf einer Seite haben, die die Übersetzungsqualität misst, und auf der anderen Seite den durchschnittlichen Rückstand.</sample>
    <sample id="497">Das ist die Latenzmaß. Und wir haben auch die computergestützte durchschnittliche Verzögerung berücksichtigt, die die Rechenzeiten des Modells berücksichtigt, um die Ausgabe zu berechnen.</sample>
    <sample id="498">Wir wollen also, dass unsere Kurven auf dieser Kurve so hoch wie möglich sind.</sample>
    <sample id="499">Aber wir wollen auch, dass sie nach links verschoben werden.</sample>
    <sample id="500">Und wir vergleichen mit geeigneten Strategien, die auch auf Offline-Modellen angewendet werden, die die Strategie und die lokale Vereinbarung sind. Und wir vergleichen auch mit der State-of-the-art-Architektur, die speziell für die simultane Übersetzung entwickelt wurde.</sample>
    <sample id="501">Dies sind alle Ergebnisse der Simultaneous Speech Translation Strategy auf Deutsch.</sample>
    <sample id="502">Und wir sehen, dass es alle Strategien übertrifft, die auf Offline-Modellen angewendet werden, da ihre Kurven nach links verschoben sind.</sample>
    <sample id="503">Und wir sehen auch, dass, wenn wir die tatsächlich verstrichene Zeit oder die rechnerbewusste Zeit betrachten, dies die schnellste Strategie ist.</sample>
    <sample id="504">Wenn Sie mehr Ergebnisse entdecken möchten, lesen Sie unser Papier. Und wir haben auch Open Source veröffentlicht, den Code und die Modelle und die simultane Hilfe, um die Reproduzierbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="505">Ja, der Datensatz ist öffentlich zugänglich.</sample>
    <sample id="506">Hallo zusammen. Mein Name ist Ying und mein Kollege Zhi Yang und ich werden unsere Forschung über Multi instruct vorstellen, die multimodale sequential learning durch instruction tuning verbessert.</sample>
    <sample id="507">Mit den Fortschritten bei großen Sprachmodellen begannen viele Arbeiten, neue Lernparadigmen zu erforschen, bei denen prädiktive Sprachmodelle für verschiedene Downstream-Aufgaben auf eine parametereffiziente und dateneffiziente Weise wiederverwendet werden.</sample>
    <sample id="508">In jüngster Zeit haben viele Studien gezeigt, dass die Anweisungssynchronisierung es großen Sprachmodellen ermöglicht, auf unvorhergesehenen Aufgaben in einer durchdachten Weise zu reagieren, indem sie natürliche Anweisungen befolgen.</sample>
    <sample id="509">Die meisten bisherigen Arbeiten zur Anweisungstuning konzentrierten sich jedoch darauf, die Leistung von Zero-Shot-Aufgaben bei Aufgaben, die nur mit Sprache durchgeführt werden, zu verbessern, während Computer Vision und multimodale Aufgaben ausgelassen wurden.</sample>
    <sample id="510">Daher wollen wir in dieser Arbeit untersuchen, ob die Anweisungstuning von multimodalen prädiktiven Modellen die Generalisierung auf nc multimodale Aufgaben tatsächlich verbessern kann.</sample>
    <sample id="511">Zusätzlich entdeckten wir bei unserer Forschung eine beträchtliche Diskrepanz in der Verfügbarkeit von Anweisungssätzen zwischen rlp und multimodal.</sample>
    <sample id="512">Es gibt mehr als eintausendsechshundert Sprach- und nur Anweisungstests. Es gibt jedoch keinen groß angelegten, öffentlich zugänglichen, multimodalen Anweisungstest. Daher motiviert uns dies, ein multimodales Anweisungstuning-Datensatz zu erstellen.</sample>
    <sample id="513">Hier präsentieren wir Multi-instruc, den ersten multimodalen Anweisungsanpassungs-Benchmark-Datensatz, der aus zweiundsechzig verschiedenen multimodalen Aufgaben besteht, die zehn verschiedene Kategorien abdecken.</sample>
    <sample id="514">Diese Aufgaben basieren auf einundzwanzig bestehenden Open-Source-Datensätzen, und jede Aufgabe ist mit fünf Experten geschriebenen Anweisungen ausgestattet.</sample>
    <sample id="515">Um die multimodale Anweisungstuning auf unserem vorgeschlagenen Datensatz zu untersuchen, nehmen wir Ofa, ein einheitliches multimodales prädiktives Modell, als unser Basismodell. Ofa verwendet ein einheitliches Vokabular für Sprache, Bild-Token und die Koordinaten eines Begrenzungsrahmens.</sample>
    <sample id="516">Hier zeigen wir einige Beispielinstanzen aus unserem Multi Intruder Datensatz.</sample>
    <sample id="517">Um die Verarbeitung verschiedener Eingabedaten- und Ausgabedatenarten zu vereinheitlichen.</sample>
    <sample id="518">Wir folgen der Methode von Ofa und formulieren alle Aufgaben in einem einheitlichen Sequenz-zu-Sequenz-Format, in dem der Eingabetext, Bilder, Anweisungen und Randboxen im gleichen Tokenraum dargestellt werden.</sample>
    <sample id="519">Okay, jetzt werde ich über multimodale Anweisungstuning sprechen.</sample>
    <sample id="520">Für den Trainingsdatensatz verwenden wir also dreiundfünfzig Aufgaben aus der neggroup für das Training und nehmen zehntausend Instanzen pro Aufgabe. Für das Testen reservieren wir die gesamte Common Sense Reasoning Group für das Testen und wählen zusätzliche fünf Aufgaben aus der Vqa- und der böswilligen Gruppe aus.</sample>
    <sample id="521">Wir verwenden alle Instanzen im Test-Bit für jede Aufgabe. Darüber hinaus nehmen wir zwanzig Aufgaben aus dem Test-Bit der natürlichen Anweisung zufällig als unsichtbare Aufgabe für NLP.</sample>
    <sample id="522">Also verwenden wir ein prähinheitsgroßes Modell als Basismodell. Während des Trainings mischen wir alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einer der fünf Anweisungs-Templates kombiniert.</sample>
    <sample id="523">Während des Tests führen wir für jede Aufgabe insgesamt fünf Experimente durch, indem wir das Modell mit einer der fünf Anweisungen in jedem Experiment bewerten.</sample>
    <sample id="524">Wir berichten über die mittlere und maximale Leistung und die Standardabweichung der Leistung aller fünf Experimente.</sample>
    <sample id="525">Wenn die Aufgabe eine mehrmodulare Klassifizierungsaufgabe ist, berichten wir über Genauigkeit. Wenn es sich um eine mehrmodulare Generierungsaufgabe handelt, berichten wir über Rouge L. Für NLP-Aufgaben berichten wir auch über Rouge L.</sample>
    <sample id="526">Wir haben auch eine zusätzliche Evaluierungsmetrik namens Sensitivität eingeführt. Dies misst also die Fähigkeit des Modells, für die gleiche Aufgabe konsistent die gleichen Ausgaben zu produzieren, unabhängig von der Variationsbreite in der Wortwahl der Anweisung.</sample>
    <sample id="527">Hier ist unser Hauptergebnis. Wie wir sehen können, kann die Anweisungseinstellung die Leistung von OS auf Multi-Modall-Aufgaben erheblich verbessern.</sample>
    <sample id="528">Auch Transfer Learning von natürlichen Anweisungssätzen kann Anweisungstuning profitieren.</sample>
    <sample id="529">Hier können wir sehen, dass das Modell mit zunehmender Anzahl von Aufgaben eine bessere Leistung und gleichzeitig eine geringere Empfindlichkeit erreicht.</sample>
    <sample id="530">Also haben wir auch ein Experiment gemacht. Wir haben eine Anweisung gegen fünf Anweisungen verwendet. Wie wir sehen können, kann die Verwendung mehr Anweisungen die Gesamtleistung des Modells verbessern und seine Empfindlichkeit stark reduzieren.</sample>
    <sample id="531">Dies zeigt also die Wirkung verschiedener Feinabstimmungsstrategien auf die Modellsensitivität. Wie wir sehen können, kann das Modell durch Transferlernen von natürlichen Anweisungsdaten, eine viel bessere Sensitivität erreichen als das ursprüngliche OFA-Modell.</sample>
    <sample id="532">Wir können auch sehen, dass transferiertes Lernen aus einem natürlichen Anweisungssatz hilft, eine viel bessere Leistung auf dem natürlichen Anweisungssatz zu erzielen.</sample>
    <sample id="533">Insgesamt haben wir einen ersten, groß angelegten, multimodalen Anweisungsanpassungsdatensatz vorgeschlagen, der die Zero-Shot-Fähigkeit von Ofa erheblich verbessert. Und wir haben verschiedene Transfer-Learning-Techniken untersucht und ihre Vorteile gezeigt. Wir haben eine neue Metrik namens Sensibilität entwickelt.</sample>
    <sample id="534">Eine weitere Sache, wir sammeln einen viel größeren, multimodalen Anweisungstuning-Datensatz mit rund einhundertfünfzig zusätzlichen visuellen und Sprachaufgaben. Und wir werden sie alle veröffentlichen. Das ist also ein QR-Code für unsere Daten und das Modell. Vielen Dank.</sample>
    <sample id="535">The authors belong to the University of Trento.</sample>
    <sample id="536">Javad Hosseini</sample>
    <sample id="562">Hallo zusammen. Ich bin Koustaph Sinha und freue mich, Sie zu unserem Vortrag über unser ACL twenty twenty three Paper Language Model Acceptability Judgments are not always robust to context.</sample>
    <sample id="563">Es ist eine gemeinsame Arbeit mit John Gauthier, Aaron Mueller, Kanishka Mishra, Karen Fentress, Roger Levy und Atina Wylie.</sample>
    <sample id="564">In dieser Arbeit untersuchen wir das Minimal Pair Paradigma.</sample>
    <sample id="565">Das minimale Paar zum Paradigma bewertet also im Grunde Sprachmodelle auf der Grundlage von Akzeptanzurteilen, die auch Grammatikalität wie Blimp-Syntax oder Akzeptanz in Bezug auf Stereotypen wie Kreuzpaare umfassen können.</sample>
    <sample id="566">Und in diesem minimalen Paar-Paradigma ist die typische Art, Sprachmodelle zu bewerten, dass man eine akzeptable oder grammatikalische Satz zeigt und dann einen unakzeptablen oder ungrammatischen Satz.</sample>
    <sample id="567">Und dann ist die Hoffnung, dass das Modell im Grunde mehr Wahrscheinlichkeit auf die akzeptable Menge legt.</sample>
    <sample id="568">Die aktuelle MPP-Pipeline erlaubt uns im Grunde nicht, die Akzeptanz von Modellen gegenüber längeren Sätzen zu bewerten.</sample>
    <sample id="569">Heutzutage kommen große Sprachmodelle mit immer längeren Kontextfenstern heraus. Es ist also entscheidend, dass wir die Akzeptanz der Modelle im gesamten Kontextfenster bewerten.</sample>
    <sample id="570">Und das ist es, was wir hier versuchen. Wir versuchen, die Pipeline zu überdenken, indem wir das Modell bitten, die Akzeptanz auf längeren und längeren Sequenzen zu bewerten.</sample>
    <sample id="571">Das ist also der Ansatz. Was wir also tun, ist, dass wir diese längeren Sequenzen simulieren. Wir besuchen die Datensätze selbst und erstellen dann Sätze neu, indem wir akzeptable oder inakzeptable Sätze aus diesen Datensätzen auswählen.</sample>
    <sample id="572">So zum Beispiel haben wir hier ein typisches Paar von Grammaticality aus dem blimp-Datensatz aus dem adjunkt-island-Fall gewählt.</sample>
    <sample id="573">Und was wir tun, ist, dass wir längere Sequenzen nachahmen, die akzeptabel sind und die gleiche Übereinstimmung der grammatikalischen Struktur haben. Wir extrahieren grammatikalische Sätze aus adjungierten.</sample>
    <sample id="574">Und dann fügen wir es als Präfix sowohl der akzeptablen Abfrage als auch der inakzeptablen Abfrage hinzu.</sample>
    <sample id="575">Wir können also dasselbe tun, indem wir unakzeptable Sätze aus der gleichen Übereinstimmung auswählen. Und das könnte auch verwendet werden, um die Akzeptanz des Modells zu testen.</sample>
    <sample id="576">Und wir können dasselbe tun, indem wir Sätze aus einer anderen Teilmenge oder einem anderen Datensatz auswählen. Das nennen wir also das Mismatch-Szenario.</sample>
    <sample id="577">Hier kommen die Sätze also immer noch aus relevanten Datensätzen, aber es ist nicht von den gleichen Datensätzen, mit denen Sie bewertet werden. Und wir können das Gleiche für Unakzeptanzfälle tun.</sample>
    <sample id="578">Schließlich können wir Sätze aus einem völlig unabhängigen Bereich wie Wikipedia auswählen.</sample>
    <sample id="579">Dies wird uns also sagen, ob die Akzeptanzurichtungen der Modelle tatsächlich von einem Kontext beeinflusst werden.</sample>
    <sample id="580">Ob der Kontext aus einer anderen Untermenge des Datensatzes kommt oder ob er völlig irrelevant für den aktuellen Satz ist, den wir uns ansehen.</sample>
    <sample id="581">Wie macht das Modell das? Zuerst schauen wir uns die Wikipedia-Sätze an, die für die aktuelle Abfrage völlig irrelevant sind. Und dort finden wir, dass die MPP-Urteile für beliebige Kontextlinien meist robust sind.</sample>
    <sample id="582">Wir haben die Kontextlänge auf bis zu eintausendvierundzwanzig erhöht, um die OPT- und GPT-2-Modelle zu maximieren. Und wir haben hier in der orangefarbenen gepunkteten Linie gesehen, dass die MPP-Urteile relativ stabil sind.</sample>
    <sample id="583">Was passiert, wenn wir Sätze aus derselben Datenbank auswählen?</sample>
    <sample id="584">Hier wählen wir also oder erstellen Sätze aus akzeptablen und inakzeptablen Domänen aus dem gleichen lim- oder syntagm-Datensatz.</sample>
    <sample id="585">Und dann sehen wir, dass die MPP-Urteile entweder signifikant zunehmen oder abnehmen, wenn Sie akzeptable oder inakzeptable Präfixe hinzufügen.</sample>
    <sample id="586">Aber wenn wir die Struktur abgleichen, das heißt, wenn wir die Sätze aus den gleichen Phänomenen in der Schuldigen Person wählen, Text, Jim,</sample>
    <sample id="587">Wir sehen einen massiven Anstieg oder einen massiven Rückgang des MPP-Urteils für das Modell, abhängig davon, ob das gewählte Präfix akzeptabel oder inakzeptabel ist.</sample>
    <sample id="588">Und das ist sehr groß. Dieser Effekt nimmt mit der Kontextlänge zu. Und das würde wahrscheinlich neuere Sprachmodelle beeinflussen, die einen großen Kontextfenster haben.</sample>
    <sample id="589">Warum beeinflusst das Abgleichspräfix das Sprachmodell so sehr?</sample>
    <sample id="590">Also haben wir eine Reihe von Analysen durchgeführt, bei denen wir versucht haben, den Eingabesatz zu stören, indem wir versucht haben, die relevante Struktur zu erhalten, indem wir dem Eingang wie Lärm hinzufügen. Und nachdem wir mehrere dieser Störungen durchgeführt haben,</sample>
    <sample id="591">Wir stellen fest, dass keines dieser Rauschen das Modell tatsächlich dazu bringt, seinen Kurs zu ändern, in Bezug darauf, wie es uns die MPP-Entwicklungsprognose zeigt.</sample>
    <sample id="592">Im Grunde genommen stellen wir fest, dass die Modelle auf ähnliche Weise empfindlich auf Perturbationssätze reagieren.</sample>
    <sample id="593">Das heißt, wenn wir die Sätze in der akzeptablen Domäne stören, sehen wir eine ähnliche Zunahme aller Störungen. Und wenn wir die Sätze in der unakzeptablen Domäne stören, sehen wir eine Abnahme der MPP-Urteile in ähnlicher Weise.</sample>
    <sample id="594">Die wichtigsten Erkenntnisse unserer Arbeit sind, dass Sprachmodelle auf latente syntaktische und semantische Merkmale sensibel sind, die über die Sätze hinweg geteilt werden.</sample>
    <sample id="595">Und die MPP-Bewertung, die Art und Weise, wie wir es derzeit mit kurzen und einzeiligen Eingaben tun, kann das abstrakte Wissen des Sprachmodells im gesamten Kontextfenster nicht vollständig erfassen.</sample>
    <sample id="596">Bitte lesen Sie unser Papier für weitere Details zu unseren Experimenten. Vielen Dank für Ihr Zuhören.</sample>
    <sample id="597">Unordentliche Multisets von Tokens.</sample>
    <sample id="598">Es gibt 55.000 Skripte in Coscript.</sample>
    <sample id="626">Die beste Ausrichtungsmethode für DEplain ist die Methode von MASS Align.</sample>
    <sample id="627">Schwach überwachtes Lernen ermöglicht es, neuronale Netzwerke unter Label Noise zu trainieren, was zu besseren allgemeinen Fähigkeiten führt.</sample>
    <sample id="628">Manuelle und automatische Alignmentmethoden wurden für die Dokumente in DEplain-web verwendet.</sample>
    <sample id="629">Der CoNLL++-Datensatz wurde erstellt, indem Daten von Reuters News aus dem Jahr 2020 gesammelt und dann mit den CoNLL 2003-Annotation-Richtlinien annotiert wurden.</sample>
    <sample id="630">Hallo zusammen. Mein Name ist Yu Xin Zhang von der Penn State University. Heute werde ich unsere Arbeit vorstellen, exemplar, Cross-lingual Semantic Parsing in multiple natural languages and many representations.</sample>
    <sample id="631">Semantisches Parsing ist also die Aufgabe, semantische Darstellungen von Benutzeranfragen zu erstellen, z. B. SQL und Lambda-Kalkül.</sample>
    <sample id="632">Und Crosslingual Semantic Parsing ist die Aufgabe, Abfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsdarstellungen zu übersetzen.</sample>
    <sample id="633">Wie in dieser Abbildung gezeigt, müssen wir die Abfrage in mehrere natürliche Sprachen übersetzen, indem wir neuronale Modelle verwenden, um SQL, Lambda oder Funql und so weiter zu verwenden.</sample>
    <sample id="634">Existierende Crosslingual Semantic Parsing-Modelle werden einzeln vorgeschlagen und auf Datensätzen mit begrenzten Aufgaben und Anwendungen bewertet. Zum Beispiel,</sample>
    <sample id="635">Es gibt Lücken in der Berichterstattung über bestimmte natürliche Sprachen. Das Chinesische fehlt. Und</sample>
    <sample id="636">Klicken Sie auf die Abdeckung bestimmter Mini-Representationen.</sample>
    <sample id="637">Der Lambda-Kalkül fehlt.</sample>
    <sample id="638">Oder sie werden nur auf bestimmten neuronalen Modellen bewertet. Zum Beispiel gibt es nur ein einziges Modell, um sie zu bewerten.</sample>
    <sample id="639">Daher schlagen wir vor, dass der Exemplar einen einheitlichen Datensatz für die semantische Analyse in mehreren natürlichen Sprachen und -repräsentationen bereitstellt.</sample>
    <sample id="640">Es enthält neunzig Datensätze in verschiedenen Domänen, fünf semantische Parsing Tasks, acht Millionen Darstellungen und zweiundzwanzig natürliche Sprachen in fünfzehn Sprachfamilien.</sample>
    <sample id="641">Und um unseren Benchmark besser zu bewerten, haben wir die sechs Einstellungen für das Training und die Bewertung berücksichtigt.</sample>
    <sample id="642">Der erste ist translate test. Wir verwenden die Google Translate-API, um die Quelle in die Zielsprache zu übersetzen. Dann verwenden wir das monolinguale Modell, um zu trainieren und zu bewerten.</sample>
    <sample id="643">Und zum Beispiel trainieren wir das englische Modell auf englischen Abfragen. Und während der Inferenz übersetzen wir die deutsche Abfrage mit der API in Englisch und verwenden dann das trainierte Modell, um die Abfrage zu prognostizieren.</sample>
    <sample id="644">Und wir testen auch ein monolinguales Modell.</sample>
    <sample id="645">In dieser Einstellung ist die Quellsprache die gleiche wie die Zieldsprache. Zum Beispiel, Deutsch zu Deutsch oder Englisch zu Englisch.</sample>
    <sample id="646">Wir haben auch die monolinguale Feldschuss-Einstellung getestet, indem wir monolinguale Modelle mit nur zehn Prozent der Trainingsdaten trainiert haben.</sample>
    <sample id="647">Und wir haben ein multilinguales Modell, das wir für alle Sprachen trainieren.</sample>
    <sample id="648">Zum Beispiel haben wir die deutschen, englischen und chinesischen Abfragen zusammengefügt, um ein mehrsprachiges Modell zu trainieren. Und während der Inferenz können wir dieses Modell verwenden.</sample>
    <sample id="649">Um deutsche Anfragen oder chinesische Anfragen zu übersetzen, etc.</sample>
    <sample id="650">Und wir betrachten auch Crosslingual Zero Shot und Few Shot Transfer, bei dem wir auf einer Quelle trainieren und auf eine andere Sprache übertragen.</sample>
    <sample id="651">Während des Trainings werden also englische Abfragen oder die Kombination aus englischen und deutschen Fewshot-Abfragen trainiert, um ein mehrsprachiges Modell zu trainieren, um die SQL-Ausgabe vorherzusagen.</sample>
    <sample id="652">Und wir finden auch viele interessante Ergebnisse. In Bezug auf die Analyse von monolingualen Modellen bewerten wir zwei Gruppen von Modellen.</sample>
    <sample id="653">Einschließlich Encoder Pdr, was für Multilingual Pretrained Encoders steht, mit pointerbasierten Decodern wie xlnr plus Pdr und mbird plus Pdr.</sample>
    <sample id="654">Und wir bewerten auch Encoder-Decoder-Modelle, die multilingual trainierte Encoder-Decoder-Modelle sind, wie Embart und mt five.</sample>
    <sample id="655">Wir haben festgestellt, dass der Encoder-Decoder auf allen neun Datensätzen die beste Leistung erzielt.</sample>
    <sample id="656">Und wir bewerten auf mt five und Beispiel xlmr plus pdr auf multilingualer Einstellung.</sample>
    <sample id="657">Wir fanden heraus, dass Encoder, Decoder oder Encoder Pdr durch das Training in einer Mischung verschiedener Sprachen verbessert werden können.</sample>
    <sample id="658">Und wir fanden, dass dies daran liegt, dass die meisten großen natürlichen Sprachen eine Leistungssteigerung erzielen können, außer dass die englische Leistung in sieben Datensätzen abfällt und nur in drei Datensätzen zunimmt.</sample>
    <sample id="659">Ich denke, das ist als Kurse der Multilingualität bekannt.</sample>
    <sample id="660">Wir vergleichen auch die Cross-link-Performance.</sample>
    <sample id="661">In dieser Abbildung ist die blaue Linie die Cross-linguistische few-shot-Transfer, die orange Linie die Cross-linguistische Zero-shot-Transfer, während die grüne Linie die monolinguale Einstellung ist.</sample>
    <sample id="662">Wir haben festgestellt, dass wir, wenn wir die grüne und orange Linie vergleichen, die Fehlerspanne für die Zero-Shot-Einstellung, die Cross-Lingual-Transfer-Leistungslücke signifikant ist. Und wenn wir die blaue und orange Linie vergleichen, haben wir festgestellt, dass die Fehlerspanne für die few-shot-Einstellung schnell verkürzt wird.</sample>
    <sample id="663">Wir finden auch einige andere interessante Erkenntnisse. Zum Beispiel übertrifft die Encoder-Decoder-Arbeit die vorherige Arbeit und erzielt vergleichbare Ergebnisse. Das Training auf Englisch natürlicher Sprache kann die Leistung von fewshot auf Zielsprachen erheblich verbessern.</sample>
    <sample id="664">Und wir fanden, dass mehrsprachige Sprachmodelle wie Codas und Blue immer noch für die semantische Analyse über Sprachen hinweg unzureichend sind.</sample>
    <sample id="665">Zusammenfassend haben wir den Exemplar erstellt, einen einheitlichen Benchmark für die semantische Analyse aus verschiedenen Blickwinkeln mit mehreren natürlichen Sprachen und Mini-Darstellungen.</sample>
    <sample id="666">Wir haben eine umfassende Benchmark-Studie zu drei repräsentativen Arten von multilingualen Sprachmodellen durchgeführt. Und unsere Ergebnisse zeigen viele interessante Erkenntnisse und so weiter. Und willkommen, um unser Papier und Code zu besuchen. Danke fürs Zuhören.</sample>
    <sample id="667">Die vorhandenen Arbeiten lassen sich in vier Kategorien einteilen.</sample>
    <sample id="668">Nein, sie sind nicht ausreichend.</sample>
    <sample id="695">Die Methode verwendet eine GPU-freundliche kontinuierliche Relaxation, um die Permutationen zu approximieren und die sprachlich plausiblen Permutationen zu lernen.</sample>
    <sample id="696">Die Fairness eines nachgeschalteten NLP-Modells wird durch die Gleichheit der Ergebnisse für verschiedene Gruppen definiert, wobei die Gruppen durch Merkmale wie Rasse, Geschlecht, Religion oder sozioökonomischer Status unterschieden werden.</sample>
    <sample id="697">The speaker is Yanis Lavraie.</sample>
    <sample id="698">Der/die Referent*in heißt Kaustubh Sinha.</sample>
    <sample id="699">Der Referent ist Myra.</sample>
    <sample id="700">Tropikalismus bezieht sich auf die Darstellung von Latina Frauen als lebhaft und exotisch, was ein häufiges Stereotyp ist.</sample>
    <sample id="701">Die Autoren haben die von Menschen verfassten Beschreibungen der Zielgruppen erstellt, indem sie die Top-Wörter aus den Beschreibungen extrahierten und diese in Markierungsgruppen einordnen, die sich durch ihre Beziehung zur Identität und Unterscheidung von der weißen Norm definieren.</sample>
    <sample id="702">In dieser Arbeit wurde cxmi zur punktweisen cxmi erweitert, um Kontextnutzung auf Satz- oder Wortebene zu messen.</sample>
    <sample id="703">DrBERT ist ein Modell, das auf 7 GB von Natur-Texten trainiert wurde, während ChuBERT ein klinisches Modell ist, das auf 4 GB von klinischen Notizen trainiert wurde.</sample>
    <sample id="751">Zwei.</sample>
    <sample id="752">Iteratives Transferlernen ist ein Ansatz, bei dem das Modell durch das Training auf dem neuesten Datensatz aktualisiert wird.</sample>
    <sample id="753">Das Ziel ist es, die Sprache der Benutzer zu verstehen, wenn sie eine Wahl treffen.</sample>
    <sample id="754">Angreifer können Modellparameter über einen EaaS extrahieren, indem sie die Embedding-Visualisierung von VLOPCA verwenden, um die Covertness zu validieren.</sample>
    <sample id="755">Es sind drei Autoren beteiligt: Sarah Papi, Matteo Negri und Marco Turchi.</sample>
    <sample id="756">Es wurden 3 Annotatoren verwendet.</sample>
    <sample id="757">Die Autoren gehören an der University of Washington und dem Allen Institute for AI.</sample>
    <sample id="758">Das Beispiel mit dem Begrenzer auf der linken Seite ist "I saw Bart and Lisa".</sample>
    <sample id="759">Die aktuelle Technik für Dialogsysteme umfasst fortschrittliche Algorithmen, maschinelles Lernen und natürliche Sprachverarbeitung, um Interaktionen zu verbessern.</sample>
    <sample id="760">Wir müssen die Akzeptanz der Modelle über das gesamte Kontextfenster bewerten, weil große Sprachmodelle längere Kontextfenster benötigen, um ihre Leistung und Zuverlässigkeit zu bewerten.</sample>
    <sample id="761">Ja, das mehrsprachige Training führte zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell, da es in sieben Datenbanken Leistung verlor und nur in drei Datenbanken Leistung gewann.</sample>
    <sample id="762">Ja, die Annotatoren kennen die Entität im Voraus, aber sie wissen nicht unbedingt, dass es sich um eine Entität handelt.</sample>
    <sample id="763">Die MT-Metriken, die für die Bewertung verwendet wurden, sind Genauigkeit, Präzision, Recall und F1-Score.</sample>
    <sample id="764">Nein, die Regression wirkt sich nicht auf bestimmte NER-Typen aus.</sample>
    <sample id="765">Positionalität ist für NLP wichtig, weil sie die Kontextualisierung von Sprache ermöglicht, was für die Erkennung von Nuancen, kulturellen Referenzen und sprachlichen Variationen entscheidend ist.</sample>
    <sample id="766">Mehrsprachige LLMs wie BLOOM wurden durch vollständige Feinabstimmung angepasst, nicht durch Adapter.</sample>
    <sample id="767">Das Modell, das für das Transferlernen verwendet wird, ist das, das nach dem iterativen Fine-Tuning von CEE auf der annotierten Datenbank und dann auf der Debatte eine viel bessere Zero-Shot-Performance erzielt.</sample>
    <sample id="768">Die aktuellen Testsets zur Bewertung der PaLM-Fähigkeiten sind der "Novice" und der "Master" Benchmark.</sample>
    <sample id="769">Drei.</sample>
    <sample id="770">Die vorgeschlagene Methode erzielt einen Gewinn von 0,2 über der stärksten Baseline.</sample>
    <sample id="771">Der Name des Referenten ist Xu-Heng.</sample>
    <sample id="772">Ja, die Ergebnisse und der Datensatz der Studie können als Benchmark verwendet werden.</sample>
    <sample id="773">Die Arbeit experimentiert mit 5 kleineren Modellen.</sample>
    <sample id="774">Das Modell, das als Basismodell für die Untersuchung der multimodalen Unterrichtsabstimmung verwendet wird, ist OFA, ein einheitliches multimodales prädiktives Modell.</sample>
    <sample id="833">The authors belong to the University of Southern Denmark.</sample>
    <sample id="834">Stony Brook University</sample>
    <sample id="835">Die Arbeit untersucht die Sprachpaare Englisch und Chinesisch.</sample>
    <sample id="836">Der Referent heißt Xiangbing.</sample>
    <sample id="837">Die Modelle von Long-impart und die normale Basis wurden untersucht.</sample>
    <sample id="838">Für Training werden 53 Aufgaben aus der NAG-Gruppe verwendet, und für Tests werden 5 Aufgaben aus der VQA-Gruppe und 5 aus der Mischa-Gruppe verwendet.</sample>
    <sample id="839">There are 11 authors involved in the work.</sample>
    <sample id="840">Die Autoren haben Experimente mit den Datensätzen AG NEWS, MIND, SSTD2 und IRAS-PAM durchgeführt.</sample>
    <sample id="876">NACHOS ist ein Datensatz von medizinischen Daten aus dem Internet.</sample>
    <sample id="877">Der Name des Referenten ist Aydil Bilal.</sample>
    <sample id="878">Die Prompt-Strategie hat einen erheblichen Einfluss auf die Leistung von LLMs bei der Übersetzung, wie in einem einfachen Experiment mit einem einzigen Prompt und zwei verschiedenen Sätzen gezeigt wurde.</sample>
    <sample id="879">Die Autoren gehören der Universität Graz an.</sample>
    <sample id="880">Die Expert*innen geben folgende 5 Anweisungen: 1. Verwenden Sie den Code, um die Daten und das Modell herunterzuladen. 2. Fügen Sie die heruntergeladenen Dateien in den Projektordner ein. 3. Aktualisieren Sie die Anweisungen im Projekt, um die neuen Dateien zu verarbeiten. 4. Testen Sie die aktualisierten Modelle mit den neuen Aufgaben. 5. Dokumentieren Sie alle Änderungen und Ergebnisse.</sample>
    <sample id="881">Die Autoren schlagen vor, ein Co-Reference Resolution Task zu verwenden, um Modelle zur Nutzung von Informationen aus mehreren Quellen zu testen.</sample>
    <sample id="882">Hallo zusammen. Mein Name ist Ayed Bilal und ich werde Ihnen eine kurze Zusammenfassung des Papiers geben, Prompting Part-of-Speech Translation: Assessing Strategies and Performance. Dies ist eine gemeinsame Arbeit mit meinen Kollegen von Google Translate.</sample>
    <sample id="883">Param ist ein fünfhundertvierzig Milliarden Parameter großes Sprachmodell, das im vergangenen Jahr, in zwanzig zweiundzwanzig, vorgestellt wurde. Es wird auf einer großen Sammlung von Texten trainiert, die siebenhundertachtzig Milliarden Token umfassen.</sample>
    <sample id="884">Bei der Zeit der Veröffentlichung erreichte es den Stand der Technik in Hunderten von NLP-Aufgaben.</sample>
    <sample id="885">In dieser Arbeit präsentieren wir die erste systematische Studie über Prompting für Last Language Model in der maschinellen Übersetzung.</sample>
    <sample id="886">Wir bewerten die Übersetzungsfähigkeit solcher Modelle anhand der besten Praktiken der IMT-Community. Dies beinhaltet die Verwendung der neuesten Testsets, um eine Überschneidung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden.</sample>
    <sample id="887">Und wir vergleichen zwei hochmoderne Systeme, also die besten Leistungsysteme, unter Verwendung der WMT-Bewertung.</sample>
    <sample id="888">Wir verwenden neueste Neurolinguistische Metriken und zeigen zusätzlich auch Expertenbewertungen. Schließlich geben wir einige Empfehlungen für die Auswahl von Prompts.</sample>
    <sample id="889">Die Prompting hat einen großen Einfluss auf die Leistung von LLMs für die Übersetzung. Wie wir in einem einfachen Experiment sehen können, bei dem wir ein Shot Prompting verwendeten und zwei verschiedene Prompts für jede Satzzeile bereitstellten,</sample>
    <sample id="890">Die Mehrheit der Sätze, fünfhundertsechzehn von eintausend, die Differenz der beobachteten ist von mehr als einem Blurrpunkt.</sample>
    <sample id="891">Und dies kann in extremen Fällen bis zu vierzig Blutpunkte betragen. Es ist also wichtig, eine gute Anreize-Strategie zu wählen.</sample>
    <sample id="892">In unseren Experimenten haben wir uns für eine fünf Schuss-Promoting-Strategie entschieden, bei der wir jeden Satz markieren, den wir dem System mit der Sprache geben.</sample>
    <sample id="893">In diesem Beispiel hier, wo wir Übersetzungen von Deutsch ins Englische durchführen, sind die deutschen Sätze, die Quell-Sätze, mit einem deutschen Doppelpunkt und die englischen Übersetzungen mit einem englischen Doppelpunkt markiert.</sample>
    <sample id="894">Wir haben gesehen, dass die tatsächliche Form der Prompting keinen großen Einfluss hat. Im Fall von mehreren kurzen Promptings.</sample>
    <sample id="895">Es ist entscheidend für Null- und Einschussanregung. Und wenn wir, wie in unserem Fall, zu Fünf-Schussanregung gehen, gibt es fast keinen Unterschied zur tatsächlichen Form der Anregung.</sample>
    <sample id="896">Es sind die Beispiele, die den größten Teil der Waage tragen.</sample>
    <sample id="897">Die Zusammenfassung unserer experimentellen Ergebnisse ist, dass die Beispielqualität wichtiger ist als die Ähnlichkeit mit dem Quellsatz.</sample>
    <sample id="898">Es ist wichtig, die Beispiele aus qualitativ hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die Auswahl von Prompts aus den Trainingsdaten der WMT-Überprüfungen oder den Dev-Daten.</sample>
    <sample id="899">Die Tiefenbilder sind viel genauer und von höherer Qualität als die Trainingsdaten, die es sind, und die Ergebnisse zeigen eine bessere Leistung, wenn die Tiefenbilder verwendet werden.</sample>
    <sample id="900">Trotzdem haben spezialisierte, hochmoderne Systeme einen erheblichen Vorteil gegenüber den PUM-Übersetzungen. Aber PUM kommt unserem kommerziellen System ziemlich nahe. In unserem Fall haben wir uns für Google Translate entschieden.</sample>
    <sample id="901">Die Erkenntnisse, die wir aus der GMM-Analyse gewonnen haben, die wir mit dem MPN-Framework durchgeführt haben, sind, dass die Flussigkeit von Palm mit den Systemen vergleichbar ist, aber der Hauptunterschied in der Genauigkeit liegt.</sample>
    <sample id="902">Insbesondere die häufigsten Fehler sind Versäumnisse.</sample>
    <sample id="903">Es scheint also, dass Palm manchmal eine bessere Klangübersetzung erzeugt, indem Teile des Originalsatzes übersprungen werden, die in der Übersetzung gemacht werden.</sample>
    <sample id="904">Die Stil-awkward-Kategorie für Pund ist jedoch niedriger als für die State-of-the-art-Systeme, was ein zusätzliches Signal ist.</sample>
    <sample id="905">Das Paar bietet wirklich flüssige Ausgabe, aber immer noch mit einigen Problemen der Genauigkeit.</sample>
    <sample id="906">Und das war's für diese wirklich kurze Übersicht. Für weitere Details, bitte kommen Sie zur vollständigen Präsentation des Papiers. Vielen Dank.</sample>
    <sample id="907">Hallo, ich bin Dawei, ein Doktorand an der Saarland University in Deutschland. In diesem Video möchte ich unsere jüngste Arbeit vorstellen, weaker than you think, eine kritische Betrachtung der wöchentlichen Überwachung.</sample>
    <sample id="908">Dies ist eine gemeinsame Arbeit mit Xiao Yusen, Mario Smusba und Gias Stephane und Dittish Klakow.</sample>
    <sample id="909">Ich möchte mit einer kurzen Einführung zu schwacher Überwachung und schwach überwachten Regressionsmodellen beginnen.</sample>
    <sample id="910">Bei schwacher Überwachung kennzeichnen wir die Daten nicht manuell, sondern mit schwachen Kennzeichnungsquellen, wie einfachen heuristischen Regeln, Wissensbasen oder niedriger Qualität von Cloud-Outsourcing, wie in der rechten Abbildung veranschaulicht.</sample>
    <sample id="911">Im Vergleich zu menschlichen Anmerkungen sind die weak Anmerkungen viel billiger, aber sie sind auch verrauscht, was bedeutet, dass ein gewisses Maß an Anmerkungen falsch ist.</sample>
    <sample id="912">Wenn wir neuronale Netze direkt mit wöchentlich beschrifteten Daten trainieren, neigen diese dazu, den Beschriftungsrausch zu merken und nicht zu verallgemeinern.</sample>
    <sample id="913">In Weakly Supervised Learning werden Algorithmen vorgeschlagen, um neuronale Netze unter solchem Laborrauschen robust zu trainieren, so dass die trainierten Modelle dennoch gut generalisieren können.</sample>
    <sample id="914">In jüngster Zeit in WSL. WSL steht also für Weakly Supervised Learning. Eine häufige Behauptung ist, dass die Leute sagen, dass sie nur Modelle trainieren und die wöchentlichen Bezeichnungsdaten und erreichen eine hohe Leistung auf sauberen Testsets.</sample>
    <sample id="915">Technisch ist diese Behauptung nicht falsch, aber es gibt einen Haken.</sample>
    <sample id="916">Das heißt, die Leute gehen davon aus, dass es einen zusätzlichen, sauberen Validierungssatz für die Modellauswahl gibt.</sample>
    <sample id="917">Wir können nicht an dieser Problemstellung zweifeln, da dies impliziert, dass im wöchentlich überwachten Lernen zusätzliche manuelle Anmerkungen erforderlich sind. Aber wie ein Elefant im Raum, diese Notwendigkeit wird oft übersehen.</sample>
    <sample id="918">Die oben genannte Zweifel lassen uns drei Forschungsfragen stellen. Erstens, ist sauberes Validierungsdaten für WSL notwendig? Oder können wir vielleicht stattdessen einen verrauschten Validierungssatz verwenden?</sample>
    <sample id="919">Zweitens, wenn saubere Daten erforderlich sind oder saubere Daten für die ordnungsgemäße Funktionsweise von WSL erforderlich sind, wie viele saubere Proben benötigen wir? Schließlich, sollten wir nur die sauberen Proben zur Validierung verwenden? Oder gibt es bessere Möglichkeiten, sie zu nutzen?</sample>
    <sample id="920">Wir haben diese Forschungsfragen in unserer Arbeit angesprochen, und unsere Ergebnisse lauten wie folgt.</sample>
    <sample id="921">Zuerst stellen wir fest, dass die neueren WSL-Methoden tatsächlich saubere weiße Datensätze benötigen, damit sie ordnungsgemäß funktionieren.</sample>
    <sample id="922">Andernfalls gibt es einen großen Leistungsabfall, wie in dieser Abbildung gezeigt. Wenn es keine sauberen Validationsproben gibt, können die trainierten Modelle nicht über die ursprünglichen schwachen Labels hinaus generalisieren.</sample>
    <sample id="923">Das heißt, das Training ist sinnlos.</sample>
    <sample id="924">Dies zeigt, dass wsl Ansätze tatsächlich saubere Etikettierungen erfordern, damit sie richtig funktionieren. Und die Annotationskosten für die Erlangung sauberen Validierungssamples sollten nicht übersehen werden.</sample>
    <sample id="925">Unsere zweite Erkenntnis ist, dass die Erhöhung der Anzahl der sauberen Validierungssamples WSL-Ansätze dabei helfen wird, eine bessere Leistung zu erzielen, wie in der Abbildung links dargestellt.</sample>
    <sample id="926">Normalerweise brauchen wir nur zwanzig Proben pro Klasse, um eine hohe Leistung zu erzielen.</sample>
    <sample id="927">Aber das ist nicht das Ende der Geschichte. Denn wenn wir auf die gleiche Weise entscheiden, auf saubere Proben zuzugreifen, dann wird das Training direkt auf ihnen sogar eine bessere Leistung erzielen.</sample>
    <sample id="928">Die rechte Abbildung zeigt die Leistungsunterschiede zwischen Feinabstimmungsmethoden, die direkt auf die sauberen Daten angewendet werden, und WSL-Methoden, die die sauberen Daten nur zur Validierung verwenden.</sample>
    <sample id="929">Wie wir sehen können, wenn wir zehn Proben pro Klasse haben, beginnt die direkte Feintuning-Methode, WSL-Ansätze zu schlagen.</sample>
    <sample id="930">Schließlich kann die behauptete Leistungsverbesserung in früheren WSL-Ansätzen leicht durch die Möglichkeit der weiteren Feinabstimmung der sauberen Validierungsproben erreicht werden.</sample>
    <sample id="931">Wie wir aus den Abbildungen sehen können, übertrifft das Valina-Modell, das als ftw bezeichnet wird, anfangs komplexere WSL-Methoden wie Cosine nicht.</sample>
    <sample id="932">Wenn wir jedoch die Feinabstimmung auf den sauberen Proben fortsetzen, funktioniert ftw genauso gut wie andere Methoden.</sample>
    <sample id="933">In der Praxis gibt es also keinen Grund, komplexere WSL-Methoden zu wählen, die mehr Rechenzeit und Festplattenspeicherplatz benötigen.</sample>
    <sample id="934">Zusammenfassend haben wir gezeigt, dass neuere WSL-Ansätze saubere, manuell annotierte Beispiele benötigen, damit sie richtig funktionieren. Ihre Leistungssteigerung und Praktikabilität werden stark überschätzt.</sample>
    <sample id="935">Unsere konkreten Empfehlungen für zukünftige Arbeiten lauten wie folgt.</sample>
    <sample id="936">Erstellen Sie zunächst einen Bericht über die Modellauswahlkriterien. Berichten Sie beispielsweise, ob die Modellauswahl gut durch Validationssätze überprüft wurde.</sample>
    <sample id="937">Zweitens sollten WSL-Ansätze mit kurzen Lernbaselines verglichen werden, da beide auf klaren Beispielen arbeiten. Drittens ist die kontinuierliche Feinabstimmung eine einfache, aber starke Baseline, die in zukünftigen Arbeiten in WSL berücksichtigt werden sollte.</sample>
    <sample id="938">Schließlich haben wir unseren Code als Open Source veröffentlicht. Sie können ihn über den QR-Code auf dieser Folie finden. Bitte fühlen Sie sich frei, ihn zu überprüfen. Vielen Dank und einen schönen Tag.</sample>
    <sample id="939">Gängige Bewertungsmethoden für Dialogsysteme umfassen die Verwendung von menschlichen Juroren, die Gespräche bewerten, und die Anwendung von Skalen wie dem Likert-Skala.</sample>
    <sample id="940">Es sind vier Autoren beteiligt: Sebastian Santi, Ronen Labrous, Katerina Raneeka und Martin Sapp.</sample>
    <sample id="941">Im Beispiel wird das Hintergrundwissen benötigt, dass Judges decide cases in law courts.</sample>
    <sample id="942">Ja, der Code ist auf GitHub verfügbar.</sample>
    <sample id="943">Nein, die Annotatoren sind nicht für NLPositionality in Bezug auf jede demographische Gruppe ausgewogen.</sample>
    <sample id="944">Sätze innerhalb der akzeptablen Domain wurden durcheinander gebracht, indem Noise hinzugefügt wurde, um die Struktur zu erhalten, was zu ähnlichen Veränderungen in den MPP-Judgments führte.</sample>
    <sample id="945">Eine dimensionale Bewertung ist eine Methode, um die Qualität von etwas anhand verschiedener Aspekte oder Dimensionen zu bewerten.</sample>
    <sample id="946">The authors belong to the University of Science and Technology of China.</sample>
    <sample id="947">Die Form des Prompts ist wichtig für Zero- und One-Shot Prompting.</sample>
    <sample id="978">Die Autoren haben mehrere Dialogmodelle evaluiert, darunter ChatGPT, Microsoft's Phi, Google's Bard, Microsoft's Phi 2, Microsoft's T5 und Microsoft's CTRL.</sample>
    <sample id="979">Es gibt zwei Autoren an der Arbeit.</sample>
    <sample id="980">Ein guter Planer schreibt realistische und vertrauenswürdige Skripte, die den vielfältigen Einschränkungen der realen Ziele gerecht werden.</sample>
    <sample id="981">Es sind zwei Autoren beteiligt.</sample>
    <sample id="982">Der/die Referent*in heißt Vasudha.</sample>
    <sample id="983">The authors belong to the University of Zurich and the University of Zurich.</sample>
    <sample id="1021">Die häufigsten Fehler von PaLM sind Omission Errors.</sample>
    <sample id="1022">Hallo. Ich bin James Finch. Und ich bin Sarah Finch. Und heute werden wir Ihnen alles über abc eval erzählen, einen neuen dimensionalen Ansatz zur Bewertung von Konversations- und KI.</sample>
    <sample id="1023">Diese Arbeit wurde vom Emory Nlp Lab unter der Leitung von Professor Gino Choi an der Emory University durchgeführt und in Zusammenarbeit mit Amazon Alexa AI.</sample>
    <sample id="1024">Nehmen wir an, Sie haben gerade ein Dialogmodell entwickelt und möchten sehen, wie es mit dem aktuellen Stand der Technik abschneidet.</sample>
    <sample id="1025">Die übliche Praxis ist die Verwendung der menschlichen Bewertung, z. B. durch die Aufforderung von menschlichen Richtern, zwischen zwei Gesprächen zu wählen oder Gespräche mit einer Likert-Skala zu bewerten.</sample>
    <sample id="1026">Diese Ansätze funktionieren gut, um eine ganzheitliche Bewertung der Gesamtqualität des Dialogs zu liefern. Aber die Qualität des Dialogs hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chat-Qualität bewerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen.</sample>
    <sample id="1027">Eine Möglichkeit besteht darin, menschliche Richter zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, z. B. die Relevanz von Modellantworten, unter Verwendung bestehender vergleichender oder Likert-Skalen.</sample>
    <sample id="1028">Wir glauben jedoch, dass es eine präzisere und zuverlässigere Strategie für die Bewertung des dimensionalen Dialogs gibt.</sample>
    <sample id="1029">Unser Ansatz versucht, die Subjektivität der menschlichen Bewertung zu reduzieren, indem er explizit anmerkt, ob jede Modellantwort bestimmte Verhaltensweisen ausdrückt, wie zum Beispiel mit irrelevanten Informationen zu antworten oder sich selbst zu widersprechen.</sample>
    <sample id="1030">Wir nennen diesen Ansatz annotierende Verhaltensweisen in Chat oder abceval in Kurzform. Wir haben diese Methode entwickelt, um Chatmodellverhaltensweisen umfassend abzudecken, die in der jüngsten Literatur als Chatqualität beeinflussend vorgeschlagen wurden.</sample>
    <sample id="1031">Abc-eval ist in der Lage, die Raten zu messen, mit denen Chatmodelle verschiedene thematische Fehler begehen.</sample>
    <sample id="1032">Zum Beispiel misst abc eval, wie oft ein Chatmodell seinen Partner ignoriert oder etwas Unerrelevantes sagt.</sample>
    <sample id="1033">Widerspricht sich selbst oder seinem Partner, halluziniert falsche Fakten oder verstößt gegen das Wissen des gesunden Menschenverstands, und wenn das Modell erfolgreich ist oder nicht, zeigt es Empathie.</sample>
    <sample id="1034">Um zu bestimmen, welche Art von Bewertung am effektivsten ist, haben wir vier hochmoderne Chatmodelle ausgewählt und sie auf hundert menschlich-botischen Gesprächen pro Modell mit abc-eval bewertet.</sample>
    <sample id="1035">Für den Vergleich haben wir diese Gespräche auch mit drei bestehenden Methoden bewertet, Lickert-Ratings auf der Turn-Ebene, Lickert-Ratings auf der Dialog-Ebene und Dialog-Ebene, Paar-Wise-Vergleiche.</sample>
    <sample id="1036">Für jede der vorhandenen Methoden haben wir Bewertungen zu acht der am häufigsten gemessenen Aspekten des Dialogs gesammelt, da dies die Standardpraxis für die Bewertung von Chat-Modellen in mehreren Dimensionen ist.</sample>
    <sample id="1037">Aus unseren Analysen dieser Bewertungsergebnisse stellten wir fest, dass abc eval Verhaltensschlüsselwörter im Allgemeinen zuverlässiger sind als Schlüsselsätze, die durch bestehende Methoden gesammelt wurden, gemessen an der inneren Annotator-Übereinstimmung bei hundert doppelbelasteten Gesprächen.</sample>
    <sample id="1038">Darüber hinaus sind abc eval-Labels in Bezug auf die Gesamtqualität der Konversation prädiktiver als die von bestehenden Methoden produzierten Metriken, wie durch die einfache lineare Regressionsanalyse gezeigt.</sample>
    <sample id="1039">Zum Beispiel können Sie sehen, wie die Messung des Anteils von Zügen mit Selbst- und Partnerkontrasten jeweils fünf Prozent und zehn Prozent der Gesprächsqualität erklärt, während die durchschnittlichen Lickert-Übereinstimmungswerte nur vier Prozent oder weniger erklären.</sample>
    <sample id="1040">Schließlich haben wir überprüft, ob jede Bewertungsmetrik einen einzigartigen Aspekt der Chat-Qualität erfasst, indem wir eine schrittweise lineare Regression verwendet haben.</sample>
    <sample id="1041">Sie können sehen, wie die Kombination aller abc eval-Metriken über fünfundzwanzig Prozent der Konversationsqualität erklärt. Und wenn Sie die Metriken nacheinander entfernen, verlieren die meisten von ihnen eine anständige Menge an Informationen über die Qualität.</sample>
    <sample id="1042">Auf der anderen Seite erklärt die Kombination aller Turn-Level-Likert-Metriken viel weniger von der Qualität. Und weniger dieser Metriken tragen einzigartige Informationen.</sample>
    <sample id="1043">Diese zuverlässigen, informativen und eindeutigen abc eval-Metriken ermöglichen es uns, konversationelle KI mit einer höheren Auflösung zu bewerten, als dies bisherige Methoden erreichen können.</sample>
    <sample id="1044">Sie können in den Ergebnissen unseres Experiments sehen, dass mehrere Herausforderungen bestehen bleiben und genau quantifiziert wurden. Zum Beispiel haben die von uns getesteten Bots in etwa zwanzig Prozent ihrer Antworten Verhaltensweisen, die gegen den gesunden Menschenverstand verstoßen.</sample>
    <sample id="1045">Sie produzieren irrelevante Informationen in etwa fünfzehn Prozent der Antworten und widersprechen sich oder ihrem Partner etwa zehn Prozent der Zeit.</sample>
    <sample id="1046">Mit dem rasanten Tempo der Verbesserungen in diesem Bereich könnten viele dieser Fehlerquoten bei neuen Modellen, die seit unserer Bewertung veröffentlicht wurden, sinken. Dies ist jedoch ein noch stärkender Grund, zuverlässige und präzise Bewertungsmessungen für den Vergleich von Modellen zu verfolgen.</sample>
    <sample id="1047">Wir hoffen, dass abc eval von anderen in diesem Bereich als sinnvolle Etappe in diese Richtung genutzt werden kann. Und wir freuen uns darauf, zu sehen, wie sich die konversationelle KI in den kommenden Monaten und Jahren weiterentwickelt. Vielen Dank fürs Zuschauen.</sample>
    <sample id="1048">Die Autoren gehören der Emory University an.</sample>
    <sample id="1049">CFT stands for Continuous Fine-Tuning.</sample>
    <sample id="1050">Es sind sieben Autoren an der Arbeit beteiligt.</sample>
    <sample id="1051">Hallo. Mein Name ist Kayo Yan und ich werde unsere Arbeit mit dem Titel "When Does Translation Require Context? A Data-Driven Multilingual Exploration" vorstellen. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernhout, Emile Liu, Andre F D Martens und Graham Neubig erstellt.</sample>
    <sample id="1052">Viele Übersetzungen hängen also vom Kontext ab. Wie würden wir zum Beispiel mole in diesem Satz übersetzen?</sample>
    <sample id="1053">Nun, wenn der vorherige Satz war, Dinge könnten gefährlich werden, wenn die Minister es herausfinden, dann bezieht sich Mo auf einen Spion. Aber wenn der vorherige Satz war, könnte es etwas Ernstes sein, Doktor, dann bezieht sich Mo auf ein Geburtszeichen.</sample>
    <sample id="1054">Je nach Kontext ändert sich die Bedeutung des Wortes, und daher ändert sich auch seine Übersetzung.</sample>
    <sample id="1055">Die Bewertung, wie gut Modelle solche Fälle übersetzen können, ist jedoch ziemlich schwierig. Erstens, weil nur ein kleiner Teil der Übersetzungen vom Kontext abhängt, was es Metriken auf Korpusebene wie bleu unmöglich macht, diese Übersetzungen zu erfassen.</sample>
    <sample id="1056">Und einige Leute haben eine gezielte Bewertung von kontextabhängigen Übersetzungen vorgeschlagen. Aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Sprachmengen, da sie normalerweise auf Domänenwissen und menschliche Kuration angewiesen sind.</sample>
    <sample id="1057">In dieser Arbeit haben wir versucht, diese beiden Fragen zu beantworten. Erstens, wann erfordert Übersetzung Kontext? Und zweitens, wie gut können Modelle diese Fälle bewältigen?</sample>
    <sample id="1058">Um die erste Frage zu beantworten, haben wir zunächst gemessen, wie sehr die Übersetzung vom Kontext abhängt.</sample>
    <sample id="1059">In der vorherigen Arbeit haben wir cxmi als Maß für den Kontextgebrauch von maschinellen Übersetzungsmodellen eingeführt. Und dies geschieht, indem gemessen wird, wie viel Informationen der Kontext C über das Ziel Y angibt, gegeben die Quelle X.</sample>
    <sample id="1060">Sie können cxmi als die Informationen betrachten, die durch den Kontext für das Modell gewonnen werden.</sample>
    <sample id="1061">In dieser Arbeit erweitern wir cxmi auf punktweise cxmi, mit der Kontextnutzung auf Satz- oder Wortebene gemessen werden kann. Wir können Wörter mit hohem cxmi als solche betrachten, die für die Übersetzung Kontext benötigen.</sample>
    <sample id="1062">Jetzt analysieren wir Wörter mit hohem pmi, um Muster zwischen diesen Wörtern zu suchen.</sample>
    <sample id="1063">Und wir führen unsere Analyse von Transkripten von Ted Talks durch, die aus dem Englischen in vierzehn verschiedene Sprachen übersetzt wurden.</sample>
    <sample id="1064">Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Zuerst betrachten wir die Teilsprachen-Tags, die einen hohen Mittelwert von Pcsxmi haben.</sample>
    <sample id="1065">Und das ermöglicht es uns, zum Beispiel duale Pronomen im Arabischen zu finden, die relativ hohe Peximi haben. Und das kann erklärt werden, weil Englisch keine dualen Pronomen hat. Sie müssen also den Kontext kennen, um zu bestimmen, ob ein Pronomen dual ist, wenn Sie ins Arabische übersetzen.</sample>
    <sample id="1066">Und wir finden, dass bestimmte Sprachen auch Kontext erfordern, wenn wir die passende Verbform wählen wollen. Wir schauen uns dann Wörterbücher an, die über alle ihre verschiedenen Vorkommen einen hohen Pessimismus haben.</sample>
    <sample id="1067">Und das hilft uns, Fälle wie diesen hier zu identifizieren, wo Sie im Chinesischen Kontext benötigen, um Eigennamen zu übersetzen, um sicherzustellen, dass Sie die gleiche Übersetzung innerhalb des Dokuments verwenden.</sample>
    <sample id="1068">Und in ähnlicher Weise finden wir, dass der Kontext die richtige Formalität unterstützt.</sample>
    <sample id="1069">Und schließlich schauen wir uns verschiedene individuelle Token an, die eine hohe Pexmi haben. Und das ermöglicht es uns, Phänomene zu identifizieren, die nicht wirklich durch das Wort selbst erfasst werden können, sondern eher in der Satzstruktur ausgedrückt werden, wie z. B. die Ellipsenauflösung.</sample>
    <sample id="1070">Jetzt verwenden wir unsere Ergebnisse aus unserer Analyse, um einen Benchmark für die Dokumentenübersetzung zu entwickeln.</sample>
    <sample id="1071">Für jedes der fünf Diskursphänomene, die wir identifiziert haben, haben wir Tags erstellt, um Wörter automatisch zu identifizieren, die mit dem Phänomen zusammenhängen. Und wir nennen unser Tag das Multilingual Discourse Aware oder Muda-Tag.</sample>
    <sample id="1072">Wir können dann auch feststellen, dass verschiedene Sprachen unterschiedliche Proportionen dieser diskursiven Phänomene aufweisen.</sample>
    <sample id="1073">Wir verwenden dann den mooda-Tagger, indem wir den Tagger auf den parallelen Korpus anwenden, den wir für die Auswertung verwenden möchten. Und wir wenden unsere Wahl der Übersetzungsmetriken auf die kontextabhängigen Beispiele an, die der mooda-Tagger identifiziert hat.</sample>
    <sample id="1074">Und schließlich verwenden wir unseren Benchmark sowie andere Metriken, um verschiedene Modelle auf der Dokumentebene zu bewerten.</sample>
    <sample id="1075">Zunächst einmal, wenn wir Korpus-Metriken verwenden, so für blau, finden wir, dass kontextunabhängige Modelle die beste Leistung haben.</sample>
    <sample id="1076">Wenn wir jedoch Kommas verwenden, funktionieren kontextbewusste Modelle am besten. Und wenn wir Wort F messen, haben Modelle mit oder ohne Kontext vergleichbare Leistungen.</sample>
    <sample id="1077">Dies zeigt erneut, dass es schwierig ist, das beste Dokument-Level-Übersetzersystem zu bestimmen, wenn wir nur Korpora-Level-Metriken verwenden.</sample>
    <sample id="1078">Jetzt verwenden wir den mooda-Benchmark, um Modelle zu bewerten. Und wir stellen fest, dass kontextübergreifende Modelle bei bestimmten Diskursphänomenen wie Formalität und lexikalischer Kohäsion deutlich genauer sind als Modelle, die keinen Kontext verwenden.</sample>
    <sample id="1079">Aber diese Modelle sind nicht viel besser als Modelle, die keinen Kontext auf andere Phänomene wie Auslassungen, Pronomen und Verbform verwenden. Dies deutet also darauf hin, wo wir mehr Fortschritte bei der Dokumenten-zu-Übersetzung sehen müssten.</sample>
    <sample id="1080">Wir haben auch verschiedene kommerzielle Systeme verglichen, und unser Benchmark zeigt, dass DeepL in der Regel genauer ist als Google Translate für die Übersetzung von Dokumenten.</sample>
    <sample id="1081">Zusammenfassend haben wir eine datengetriebene Analyse in vierzehn Sprachpaaren durchgeführt, um den Kontext für eine Übersetzung zu identifizieren.</sample>
    <sample id="1082">Und dann verwenden wir unsere Erkenntnisse, um einen Benchmark für die maschinelle Übersetzung auf Dokumentebene zu erstellen, der uns helfen kann, zu identifizieren, welche diskursiven Phänomenmodelle gut oder nicht gut gehandhabt werden können, und welche Übersetzungsysteme gut in der Dokumentenübersetzung sind.</sample>
    <sample id="1083">Vielen Dank für Ihre Aufmerksamkeit. Bis morgen.</sample>
    <sample id="1084">Yusen Zhang</sample>
    <sample id="1121">Name: "Randomized Sequential Search"</sample>
    <sample id="1122">Die Autoren beschreiben die Methode der „markierten Wörter“ als eine Technik, um Wörter zu identifizieren, die Gruppen von markierten und unmarkierten Elementen unterscheiden. Sie werden später ausführlicher erläutert.</sample>
    <sample id="1123">Die Autoren gehören der University of Washington an.</sample>
    <sample id="1124">Prague</sample>
    <sample id="1125">Der Referent ist James Finch.</sample>
    <sample id="1126">Vier.</sample>
    <sample id="1127">Syntaktische Phänomene können mit den folgenden Datensätzen getestet werden:</sample>
    <sample id="1161">Die Abkürzungen der fünf Methoden für die erste Forschungsfrage sind: BERT, ELECTRON, ELM, LSTM und PERCEIVE.</sample>
    <sample id="1162">Das Modell wird anhand von 11 biometrischen und klinischen Downstream Tasks evaluiert.</sample>
    <sample id="1226">CamemBERT wurde ursprünglich mit dem 4 GB Subset von Naturschätzen trainiert.</sample>
    <sample id="1227">Sadam Shkurkofsky</sample>
    <sample id="1228">Die Ergebnisse des Experiments zeigten, dass die Leistung der Modelle mit größerer zeitlicher Verzögerung abnahm, was die Hypothese unterstützte, dass die zeitliche Verzögerung die Hauptursache für den Leistungsverlust war.</sample>
    <sample id="1269">Um die Token in der richtigen Reihenfolge zu ordnen, wird ein Modell verwendet, um die Permutation zu bestimmen.</sample>
    <sample id="1270">Die Autoren empfehlen, dass Modellentwickler*innen ihre Methoden zum Abbau von Vorurteilen transparenter machen sollten, weil es unklar ist, ob positive Stereotypen auf übermäßige Wertallianz oder andere anti-stereotypische Methoden zurückzuführen sind, die zu negativen Mustern führen.</sample>
    <sample id="1271">Inakzeptable Minimalpaareingaben sind Wörter, die nicht in einem Satz verwendet werden können, wie z.B. "Jeden" oder "Jeden".</sample>
    <sample id="1272">Die Autoren verwendeten die Genauigkeit und den F1-Score als Bewertungsmetriken.</sample>
    <sample id="1273">Die Metrik, die zur Messung der Übereinstimmung zwischen den Kommentatoren verwendet wurde, war die Inter-Annotator-Agreement.</sample>
    <sample id="1274">Die Wikipedia-Domain wurde gewählt, um völlig unzusammenhängende Sätze zu den inakzeptablen und akzeptablen Suchanfragen hinzuzufügen.</sample>
    <sample id="1275">Die Universität ist die Ludwig-Maximilians-Universität München.</sample>
    <sample id="1276">MultiInstruct is unique because it focuses on instruction tuning for multimodal tasks, unlike other benchmarks that primarily target language-only tasks.</sample>
    <sample id="1277">Zwei.</sample>
    <sample id="1278">Die binäre Koordination ist ein Konzept in der Linguistik, das sich auf die Beziehung zwischen Silben und Wörtern in einer Sprache bezieht. Sie beschreibt, wie Silben in Wörter unterteilt sind und wie diese Unterteilung die Struktur und Bedeutung von Wörtern beeinflusst.</sample>
    <sample id="1279">Die durchschnittliche Länge der in dieser Studie verwendeten Prompts betrug 4,3 Wörter.</sample>
    <sample id="1280">Die Ergebnisse zeigen, dass das kleinere T5-Modell, wenn es ordnungsgemäß auf geeigneten Datensätzen trainiert wird, in der Lage ist, qualitativ hochwertige Skripte zu generieren, die denen von größeren Sprachmodellen überlegen sind. Dies deutet darauf hin, dass kleinere Modelle, wenn sie richtig trainiert werden, leistungsfähiger sein können als größere Modelle.</sample>
    <sample id="1281">Hi, I am Yanis Lacroix and I will present to you our works on Dr. Bert, a robust pre-trained model in French for biomedical and clinical domain.</sample>
    <sample id="1282">In dieser Präsentation sprechen wir zunächst über Sprachmodellierung in der Gesundheitsversorgung. Dann werden wir die Hauptbeiträge unseres Artikels vorstellen.</sample>
    <sample id="1283">Wir haben das erste biomedizinische Modell in Französisch eingeführt, das Doctor Bert heißt und auf Roberta basiert und auf Nacos trainiert, einem Datensatz mit medizinischen Daten aus dem Internet.</sample>
    <sample id="1284">Wir führen auch einen Vergleich von Modellen mit mehreren Trainingseinstellungen und Datenquellen durch. Dann präsentieren wir unsere Ergebnisse für elf biomedizinische und klinische Downstream-Aufgaben auf Französisch.</sample>
    <sample id="1285">Und schließlich schließen wir mit den Experimenten und geben Ihnen weitere Details, wie Sie auf das Modell zugreifen können.</sample>
    <sample id="1286">Seit seiner Veröffentlichung in zweitausendachtzehn ist bert einer der effektivsten Ansätze zur Lösung von Aufgaben im Bereich der natürlichen Sprachverarbeitung und bietet einen enormen Leistungsgewinn im Vergleich zu historischen statischen und kontextualisierten Methoden wie word two vec fasttext oder n words.</sample>
    <sample id="1287">Seitdem wurde dieses Modell auf viele andere Sprachen wie zum Beispiel auf Französisch mit Camembert und auf andere Bereiche wie Biomedizin mit Permitted Birth und Biobirth übertragen, aber vor allem auf Englisch.</sample>
    <sample id="1288">Spezialisierte Modelle für andere Sprachen sind selten und basieren oft auf kontinuierlicher Vorbereitung aufgrund des Mangels an Domänendaten.</sample>
    <sample id="1289">Allerdings hatte die französische Sprache bis jetzt keine Open-Source-Modelle für biomedizinische Anwendungen.</sample>
    <sample id="1290">Wir stellen uns die Frage, welche Datenquellen für eine breite Anwendung am besten geeignet sind. Und diese Quelldaten sind eine gute Ersatz für klinische Daten.</sample>
    <sample id="1291">Um diese Frage zu beantworten, vergleichen wir Dr. Bert mit unserem Shubert-Modell, das auf anonymisierten Daten basiert, die wir aus dem Non-university Hospital, das unser Haus ist, erhalten haben.</sample>
    <sample id="1292">Nachher stellen wir uns die Frage, wie viele Daten wir brauchen, um ein spezialisiertes Modell auf französischen Daten zu trainieren. Ist es vier Gigabyte, acht Gigabyte oder mehr?</sample>
    <sample id="1293">Um diese Frage zu beantworten, trainieren und vergleichen wir zuerst vier von Grund auf neu entwickelte Modelle. Eine erste Version von Doctor Bert mit sieben Gigabyte von Nachos, eine zweite Version von vier Gigabyte von Nachos.</sample>
    <sample id="1294">Die erste Version von Shubert, einem klinischen Modell, enthält vier Gigabyte mit Sätzen aus klinischen Notizen. Und eine letzte Version von Shubert, mit einer Mischung aus vier Gigabyte mit natürlichen und vier Gigabyte mit klinischen Notizen.</sample>
    <sample id="1295">Neben dieser Vergleichsaufgabe haben wir drei Modelle eingeführt, die auf der Konferenz trainiert wurden, um die Auswirkungen der Pretraining-Strategie zu analysieren.</sample>
    <sample id="1296">Eine basiert auf dem Gewicht von Camembert und trainiert auf vier Gigabyte einer Reihe von Naturen. Eine andere basiert ebenfalls auf Camembert, trainiert aber diesmal auf den vier Gigabyte von Klinken.</sample>
    <sample id="1297">Und schließlich eine Basis auf dem englischen biomedizinischen Modell Permutierte und trainieren auf vier Gigabyte Set von Natur. Insgesamt haben wir sieben Modelle.</sample>
    <sample id="1298">Um unsere sieben Modelle zu bewerten, sammeln wir öffentliche und private Aufgaben, wie z. B. Nennungs- und Zustimmungsaufgaben, Part-of-Speech-Tagging und Fragebeantwortung.</sample>
    <sample id="1299">Dieses Modell wird mit sechs Basismodellen verglichen, die sind: Camembert Oscar, einhundertachtunddreißig Gigabyte, Camembert Oscar, vier Gigabyte, Camembert CC net, vier Gigabyte, Permitted, Bio, Birth und Clinical Birth.</sample>
    <sample id="1300">Die Bewertung von Highlights, dass das Modell am besten auf der Aufgabe mit Daten der gleichen Art funktioniert, wie die, auf die das Modell trainiert wurde.</sample>
    <sample id="1301">Wir können jedoch die Daten von heterogenen Quellen erhalten. Es scheint, dass wir auch beobachten, dass die Verwendung von mehr Daten zu einer besseren Leistung führt.</sample>
    <sample id="1302">Insgesamt scheint das von Grund auf erstellte Retrenning bei den meisten Aufgaben eine höhere Leistung zu erzielen.</sample>
    <sample id="1303">Unser Experiment zur Kontrastverarbeitung mit dem Gewicht und dem Tokenisierer von permit bird, das auf dem vier Gigabyte großen Datensatz von Natur geschnitten wurde, zeigt vergleichbare Ergebnisse wie die von Dr. Bird Four Gigabyte von Grund auf.</sample>
    <sample id="1304">Das ist bei dem Modell, das auf Camembert-Weizen und Tuckeringer basiert, nicht der Fall, das unter Stabilitätsproblemen leidet.</sample>
    <sample id="1305">Schließlich bietet unser vorgeschlagenes System eine bessere Leistung bei neun der elf Domänenaufgaben und übertrifft global die Ergebnisse des generischen Modells hier.</sample>
    <sample id="1306">Wir beobachten auch, dass spezialisierte Daten besser sind, mehr spezialisierte Daten besser sind, aber sie skalen nicht gut.</sample>
    <sample id="1307">Das vorgefertigte Modell, das von Natur aus erhalten wurde, ist kostenlos verfügbar und auf der Jugendseite. Und alle Trainingssätze sind auf GitHub verfügbar.</sample>
    <sample id="1308">Also vielen Dank für diese Präsentation. Und wir freuen uns auf den Austausch in der anschließenden Sitzung in Toronto.</sample>
    <sample id="1309">Die Arbeit untersucht drei Lernstrategien: das Trainieren von Modellen von Grund auf, das Trainieren von Modellen mit einer Mischung aus Natus und klinischen Notizen und das Trainieren von Modellen mit Continuous Pre-Training.</sample>
    <sample id="1310">Der Faktor der Überanpassung, der speziell auf die Wiederverwendung von Tests zurückzuführen ist, wird als "Faktor der Wiederverwendung von Tests" bezeichnet.</sample>
    <sample id="1311">Die Qualität der Vereinfachung wurde durch die Vergleichung der Ergebnisse mit den Baselinescores beurteilt, wobei die Ergebnisse der Feinabstimmung als bessere Benchmarks für die automatische Textvereinfachung dienten.</sample>
    <sample id="1312">Ja, einige vorläufige Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Vorurteile haben.</sample>
    <sample id="1313">Hallo. Mein Name ist Matthias Lendl und heute werde ich Ihnen eine kurze Einführung in unsere Arbeit geben, Kompositional Generalisierung ohne Bäume mit Multi-Set-Tagging und latenter Permutation.</sample>
    <sample id="1314">Dies ist eine gemeinsame Arbeit mit meinen Beratern Alexander Koller und Ivan Titorov.</sample>
    <sample id="1315">Kompositional Generalisierung kann als die Fähigkeit eines Lernenden verstanden werden, mit tieferer Rekursion und bisher unbekannten Kompositionen von Sätzen umzugehen, die während des Trainings einzeln gesehen wurden.</sample>
    <sample id="1316">Im Kontext der semantischen Analyse könnte das Testen für die Kompositionalität wie folgt aussehen. Wie üblich haben wir eine Trainingsmenge von Sätzen, in diesem Fall die Mädchen schliefen und Mary wusste, dass die Mädchen schliefen.</sample>
    <sample id="1317">Diese Äußerungen werden mit logischen Formen gepaart, die die Kernaspekte ihrer Bedeutung darstellen.</sample>
    <sample id="1318">Im Gegensatz zur Standard-Maschinellen-Lernbewertung stammt der Testsatz nicht aus der gleichen Verteilung, enthält aber strukturell unerwartete logische Formen.</sample>
    <sample id="1319">In diesem Beispiel hat das Modell während des Trainings eine flache Rekursion gesehen und wird auf einem Beispiel mit tiefer Rekursion getestet.</sample>
    <sample id="1320">Naive sequenz zu sequenz Modelle kämpfen mit dieser Art von Out-of-Distribution-Generalisation und produzieren oft Ausgaben, die von der Eingabe abweichen.</sample>
    <sample id="1321">Insbesondere scheitern sie oft daran, die systematischen Entsprechungen zwischen Eingabe und Ausgabe zu reproduzieren, wie sie in den Beispielen farbcodiert sind.</sample>
    <sample id="1322">Eine beliebte Methode, um dies zu adressieren, besteht darin, Bäume in die Modelle zu integrieren.</sample>
    <sample id="1323">Die Bäume sollen den kompositorischen Prozess erfassen, der Sätze mit logischen Formen verbindet.</sample>
    <sample id="1324">Dies funktioniert gut, aber Bäume werden normalerweise nicht gegeben und müssen irgendwie erhalten werden.</sample>
    <sample id="1325">Dies kann kompliziert und manchmal ein rechnerisch aufwändiger Prozess sein. Normalerweise beinhaltet dies eine erhebliche vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorformulierte, vorform</sample>
    <sample id="1326">Das Erhalten von Bäumen kann auch spezialisierte Grammatikinduktionsverfahren beinhalten.</sample>
    <sample id="1327">In diesem Papier verwenden wir keine Bäume und stellen ein neuronales Sequenz-zu-Sequenz-Modell vor, das die Entsprechungen zwischen Fragmente der Eingabe und Fragmente der Ausgabe direkt modelliert.</sample>
    <sample id="1328">Zum ersten Mal zeigen wir eine starke Verallgemeinerung auf tiefere Rekursion, ohne auf Bäume angewiesen zu sein.</sample>
    <sample id="1329">Unser Ansatz prognostiziert die Ausgabe aus dem Eingang in zwei Schritten.</sample>
    <sample id="1330">Zuerst kennzeichnen wir jedes Eingabetoken mit einem unordentlichen Multisatz von Token, die im Ausgabetoken erscheinen.</sample>
    <sample id="1331">Nach dem ersten Schritt haben wir alle richtigen Token, aber sie sind nicht ordnungsgemäß.</sample>
    <sample id="1332">Deshalb verwenden wir im zweiten Schritt ein anderes Modell, um eine Permutation vorherzusagen, um sie in die richtige Reihenfolge zu bringen.</sample>
    <sample id="1333">Wir führen eine neue Methode ein, um eine Permutation vorherzusagen, die keine harten Einschränkungen für die möglichen Permutationen auferlegt. Dies macht unseren Ansatz recht flexibel und ausdrucksstark.</sample>
    <sample id="1334">Konzeptionell funktioniert unser Permutationsmodell ungefähr so.</sample>
    <sample id="1335">Wir gehen von links nach rechts über die Ausgabe und bestimmen, welches Multiset-Token in jede Position gelegt werden soll. Für die erste Ausgabeposition wählen wir einfach eines aus, wie es rot hervorgehoben ist.</sample>
    <sample id="1336">Dann springen wir zum nächsten Multiset-Token, um das zweite Token in der Ausgabe zu bestimmen.</sample>
    <sample id="1337">Wir bestimmen das dritte Token in der Ausgabe auf ähnliche Weise, indem wir zu einem anderen Multi-Set-Token springen. Wir setzen diesen Prozess fort.</sample>
    <sample id="1338">Bis jedes Token aus der ersten Stufe genau einmal besucht wurde.</sample>
    <sample id="1339">Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen treeless-Modellen auf dem Cogs-Benchmark. Unser Modell übertrifft die anderen um einen großen Vorsprung bei der Generalisierung zu tieferen Rekursionen.</sample>
    <sample id="1340">Einige andere Arten der strukturellen Generalisierung bleiben jedoch sehr herausfordernd.</sample>
    <sample id="1341">In unserem Papier lösen wir ein paar interessante technische Herausforderungen.</sample>
    <sample id="1342">Zunächst einmal ist die Ausrichtung zwischen Eingabe und Ausgabe in den Trainingsdaten nicht angegeben. Infolgedessen wissen wir für ein bestimmtes Token nicht, von welchem Multisener es stammt, was eine Herausforderung für das Training darstellt.</sample>
    <sample id="1343">Darüber hinaus gibt es manchmal mehrere Permutationen, die mit den Daten übereinstimmen, aber die sprachlich korrekte ist latend. Wir gehen mit dieser Herausforderung um, indem wir die Ausrichtung als Teil des Trainings induzieren.</sample>
    <sample id="1344">Unsere Permutationsmethode ist sehr flexibel, bringt aber die Herausforderung mit sich, dass die Suche nach der höchst punktierenden Permutation NP-schwer ist. Das liegt daran, dass dies mit dem Problem des reisenden Verkäufers zusammenhängt.</sample>
    <sample id="1345">Wir approximieren dies mit einer GPU-freundlichen, kontinuierlichen Entspannung, die es uns auch ermöglicht, durch die Lösung zurückzuleiten und die sprachlich plausibleren Permutationen zu lernen.</sample>
    <sample id="1346">Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen angehen möchten, können Sie sich unser Papier ansehen oder zu unserem Poster kommen.</sample>
    <sample id="1347">Kognitive Dissonanz ist ein Zustand, in dem zwei gegensätzliche Überzeugungen oder Handlungen im Kopf eines Individuums sind.</sample>
    <sample id="1348">GPT-4.</sample>
    <sample id="1349">Ja, kumulatives Training ist besser als iteratives Training für aktives Lernen.</sample>
    <sample id="1350">The speaker is Sarah Papi.</sample>
    <sample id="1351">Die Daten für den MuDa-Benchmark stammen aus 14 verschiedenen Sprachen.</sample>
    <sample id="1385">Der Referent heißt Matthias Lendl-Mahn.</sample>
    <sample id="1386">Sprachübergreifender Transfer bezieht sich auf das Training eines Modells auf einer Quelle-Sprache und dessen Übertragung auf eine andere Sprache, wie z.B. das Training auf englischen und deutschen Few-Shot-Queries, um eine multilingualen Modell zu erstellen, das SQL-Ausgaben vorhersagt.</sample>
    <sample id="1387">The authors belong to Saarland University.</sample>
    <sample id="1388">The authors use three latency measures: translation quality, average lagging, and computationally aware average lagging.</sample>
    <sample id="1389">Hallo zusammen. Ich bin Magsztar und heute präsentieren meine Coautoren Martin und ich unsere Arbeit, die Kitema-Steps, die Bewertung der Wissensintegration aus mehreren Quellen. Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research.</sample>
    <sample id="1390">Modelle zur natürlichen Sprachverstehens nutzen eine Vielzahl von Wissensquellen, wie z. B. Wissen, das in den Parametern enthalten ist, normalerweise durch eine Vorabtraining erlangt, und Wissen, das zur Inferenzzeit in den Eingaben gegeben wird.</sample>
    <sample id="1391">Neuere Arbeiten in Aufgaben wie Fragebeantwortung zeigen, dass Modelle prärtrainierte Zeitkenntnisse verwenden können, um die Aufgabe zu lösen.</sample>
    <sample id="1392">Aber das Verständnis natürlicher Sprache erfordert oft Wissen, das auch zur Inferenzzeit zur Verfügung gestellt wird.</sample>
    <sample id="1393">Zum Beispiel in dem Satz: John sah den neu gewählten Präsidenten im Fernsehen.</sample>
    <sample id="1394">Pretrainierte Parameter können Informationen darüber enthalten, was Präsidenten tun und was ein TV ist, aber sie können nicht zuverlässig wissen, wer diese instanzspezifische Entität John ist oder wer der neue Präsident ist, weil der Präsident sich seit dem Pretraining geändert haben könnte.</sample>
    <sample id="1395">Daher erfordern erfolgreiche Modelle für knowledge-intensive NLU-Aufgaben die Fähigkeit, sowohl vortrainiertes Zeitwissen als auch Inferenzzeitwissen zu integrieren und zu nutzen.</sample>
    <sample id="1396">In dieser Arbeit schlagen wir einen diagnostischen Test für die Wissensintegration vor.</sample>
    <sample id="1397">Wir führen eine Korreferenzauflösungstätigkeit ein, die darauf abzielt, die Fähigkeit zu untersuchen, auf Wissen aus verschiedenen Quellen zurückzugreifen. Wir bewerten den Datensatz mit menschlichen Studienteilnehmern und etablieren Korreferenzauflösungsmodule.</sample>
    <sample id="1398">Hier ist ein Beispiel aus unserem Datensatz. Servin ist ein Richter. Kia ist eine Bäckerin. Servin und Kia trafen sich im Park. Nach einem langen Arbeitstag, bei dem er Fälle in einem Gerichtsgebäude entschieden hat, war er froh, sich zu entspannen.</sample>
    <sample id="1399">Die Aufgabe hier ist es, die richtige Entität zu identifizieren, auf die sich das Pronomen he bezieht, in diesem Fall ist es Sam.</sample>
    <sample id="1400">Die Auflösung eines gegebenen Pronomens erfordert zwei Arten von Informationen. Erstens, spezifische Wissensdaten, wie z. B. Servile ist ein Richter. Und zweitens, Hintergrundwissen, wie Richter Fälle in Gerichten entscheiden.</sample>
    <sample id="1401">Im Allgemeinen wird Hintergrundwissen während des Pre-Training von großen Sprachmodellen gelernt, während entitätsspezifisches Wissen in der Regel zur Inferenzzeit beobachtet wird.</sample>
    <sample id="1402">Wir variieren die Verfügbarkeit dieser beiden Informationen, so dass sie entweder in einer einzigen Quelle oder in mehreren Quellen zu finden sein kann.</sample>
    <sample id="1403">Wir haben drei Einstellungen von Kitmos definiert. Erstens haben wir die große Einstellung, Hintergrund, Pretrain. Die Hintergrundkenntnisse werden als verfügbar angenommen, die Vortrain-Zeit.</sample>
    <sample id="1404">Zweitens gibt es die Hintergrund beides Einstellung, bei der beide Wissensarten sowohl zur Vorabtrainierungszeit als auch zur Inferenzzeit verfügbar sind. Schließlich die Hintergrund Inferenz Einstellung, bei der beide Wissensarten nur zur Inferenzzeit verfügbar sind.</sample>
    <sample id="1405">Diese letzte Einstellung ist besonders interessant, da sie den Fall simuliert, in dem das für die Lösung einer Aufgabe notwendige Hintergrundwissen nicht Teil des vorgefertigten Datenmodells ist. Zum Beispiel, weil sich seit der Zeit der Vorverarbeitung neue Berufe entwickelt haben.</sample>
    <sample id="1406">Hier ist ein Beispiel dafür, wie wir die Verfügbarkeit von Fakten und zwei Quellen kontrollieren.</sample>
    <sample id="1407">In der Hintergrundsituation, die vor dem Training vorgegeben ist, gehen wir davon aus, dass das Wissen, dass Politiker gewählte Sitze in der Regierung suchen, in den vorgegebenen Parametern enthalten ist. Und in diesem Freien und Kontext geben wir das spezifische Wissen an, dass Chechester ein Politiker ist.</sample>
    <sample id="1408">In der Hintergrund beides Einstellung. Wir bieten zusätzlich nicht nur antizipativ, sondern auch Hintergrundwissen über Parteien im Inferenzzeitkontext.</sample>
    <sample id="1409">Und die Hintergrundinferenz setzt die fiktive Beschäftigung Meritor ein, anstatt Politiker, weil Meritor unwahrscheinlich in einem prätrennten Bereich enthalten ist.</sample>
    <sample id="1410">Wir bewerten den Datensatz sowohl mit menschlichen Studien und etablierten Referenzlösungen. In dieser Abbildung zeigen wir die Ergebnisse der besten Modelle auf der schwierigsten Variante der Hintergrund-Pre-Train-Einstellung.</sample>
    <sample id="1411">Ohne auf Kitmos trainierte Modelle, die auf Kitmos trainiert wurden, funktionieren beide Modelle nicht gut. Beide C two F und Birdview Coefficierten jedoch deutlich besser als die zufällige Auswahl.</sample>
    <sample id="1412">Dies deutet darauf hin, dass, wenn auf allgemeinen Referenzauflösungsdatenbanken trainiert wird, Modells gelernt haben, Oberflächensignale auszunutzen, die bei der Testung auf Kitmos nicht nützlich sind, da solche Signale entfernt wurden.</sample>
    <sample id="1413">Zusätzliche Experimente mit fiktivem Wissen zeigen, dass selbst die besten Modelle Hintergrundwissen, das nur zur Inferenzzeit bereitgestellt wird, nicht zuverlässig integrieren können.</sample>
    <sample id="1414">Zusammenfassend lassen sich die wichtigsten Erkenntnisse unseres Artikels wie folgt zusammenfassen: Viele Modelle zur Korrektur von Referenzauflösungen scheinen ohne zielgerichtete Schulung nicht in der Lage zu sein, Wissen aus verschiedenen Quellen zu verarbeiten. Mit zielgerichteter Schulung können jedoch einige Modelle erfolgreich Wissen aus mehreren Quellen integrieren.</sample>
    <sample id="1415">Trotzdem scheinen selbst die besten Modelle Schwierigkeiten mit der zuverlässigen Integration von Backward Knowledge zu haben, das nur zur Inferenzzeit präsentiert wird. Wenn Sie mehr Details erfahren möchten, lesen Sie bitte unser Papier und schauen Sie sich den Datensatz und den Code auf Github an.</sample>
    <sample id="1416">Die Nachteile der baumbasierten Methoden sind, dass die Erzeugung der Bäume oft kompliziert und rechenintensiv ist und spezielle Vorverarbeitungs- und Induktionsverfahren erfordert.</sample>
    <sample id="1417">Die Autoren gehören der Universität Mannheim an.</sample>
    <sample id="1418">Hallo. Ich bin myra, und heute werde ich über unser Papier sprechen, markierte Persönlichkeiten, die mit natürlichen Sprachanweisungen verwendet werden, um Stereotypen in Sprachmodellen zu messen. Diese Arbeit wird in Zusammenarbeit mit Eszter Musch und Dan Jarochnik durchgeführt.</sample>
    <sample id="1419">In den letzten Jahren haben viele die Prävalenz von sozialer Voreingenommenheit und Stereotypen in großen Sprachmodellen dokumentiert, oder L O M S.</sample>
    <sample id="1420">Diese Maßnahmen haben jedoch verschiedene Einschränkungen. Sie basieren in der Regel auf handgefertigten Datensätzen, die sehr zeitaufwendig zu kuratieren sind.</sample>
    <sample id="1421">Und sie messen normalerweise nur sehr spezifische Stereotypen, was bedeutet, dass sie nicht gut auf andere demografische Gruppen oder Kontexte generalisiert werden oder einfach sehr allgemeine, breite Assoziationen wie negative Assoziationen mit bestimmten Gruppen erfassen.</sample>
    <sample id="1422">Darüber hinaus berücksichtigt die meiste Arbeit in diesem Bereich nicht die Intersektionalität, die Vorstellung, dass sich komplexe soziale Identitäten zu Vorurteilen verbinden und einzigartige Orte von Schaden sein können.</sample>
    <sample id="1423">Um diese Einschränkungen zu überwinden, verlassen wir uns auf die Eigenschaft, dass diese neueren, instruktionsgesteuerten LLMs sehr gut darin sind, auf Anweisungen und Befehle zu reagieren.</sample>
    <sample id="1424">Wir können also das Modell bitten, eine Persona zu generieren, die eine Beschreibung eines imaginären Individuums ist, das mit einem Prompt wie "Stellen Sie sich vor, Sie sind eine asiatische Frau" beschrieben wird.</sample>
    <sample id="1425">Und wir können sofort sehen, dass dies für jede Demografie sehr verallgemeinerbar ist, weil wir einfach jeden gewünschten Identifikator in diese Anweisung eingeben können.</sample>
    <sample id="1426">Hier sind einige Beispielgenerationen von GPT vier.</sample>
    <sample id="1427">Sofort sehen wir, dass, während die Ergebnisse nicht übermäßig negativ oder giftig sind, im traditionellen Sinne dieser Wörter,</sample>
    <sample id="1428">Es gibt einige interessante Muster.</sample>
    <sample id="1429">Die asiatische Frau wird als unauffällig dargestellt. Die Frau aus dem Nahen Osten wird mit Wörtern wie exotisch und wie auf eine faszinierende Region Bezug genommen.</sample>
    <sample id="1430">Und beide der farbigen Frauenfiguren verweisen auf die Abstammung, während die weiße Männerfigur nichts dergleichen hat.</sample>
    <sample id="1431">Um diese Muster zu erfassen, besteht unsere Methode aus zwei Teilen. Der erste besteht darin, diese Persönlichkeiten zu generieren.</sample>
    <sample id="1432">Unsere Anweisungen zur Generierung dieser Charaktere wurden von einer Studie inspiriert, bei der sie diesen Anweisungen menschlichen Probanden gaben und feststellten, dass sie durch die Übergabe an menschliche Probanden auch rassistische Stereotypen an die Oberfläche bringen konnten.</sample>
    <sample id="1433">Und das ermöglicht auch einen direkten Vergleich zwischen unseren generierten Personen und den von Menschen geschriebenen Antworten.</sample>
    <sample id="1434">Der zweite Teil sind markierte Wörter, die eine Methode sind, um die Wörter zu identifizieren, die markierte Gruppen von unmarkierten unterscheiden, worüber ich kurz erläutern werde.</sample>
    <sample id="1435">Der Vorteil dabei ist, dass wir wirklich spezifische Stereotypen und Muster erhalten, ohne auf ein bestimmtes Vokabular angewiesen zu sein.</sample>
    <sample id="1436">Die Methode der markierten Wörter stützt sich also auf das soziolinguistische Konzept der Markierung, das besagt, dass es einen unmarkierten Standard gibt, und jede Gruppe, die sich von diesem Standard abhebt, ist sprachlich markiert.</sample>
    <sample id="1437">Zum Beispiel wird das Wort Mann, oder, sorry, das Wort Krieger normalerweise mit Männern in Verbindung gebracht. Wenn also Leute einen Krieger beschreiben, der eine Frau ist, geben sie normalerweise einen Mann Krieger an und markieren den Begriff mit einer Frau.</sample>
    <sample id="1438">Und im weiteren Sinne sind dominierende Gruppen in der Gesellschaft sowohl sprachlich als auch sozial unmarkiert, während die marginalisierten Gruppen normalerweise markiert sind.</sample>
    <sample id="1439">In unserer Methode weisen wir also zunächst darauf hin, welche unmarkierten und markierten Gruppen es gibt.</sample>
    <sample id="1440">Und dann vergleichen wir die Personas mit der Fighting-Word-Methode, die im Grunde genommen gewichtete Log-Odds-Raten verwendet, um die Top-Wörter für jede markierte Gruppe zu unterscheiden.</sample>
    <sample id="1441">Zum Beispiel würden wir bei den Figuren von schwarzen Frauen Kämpfe mit Worten führen und die Log-Wahrscheinlichkeitsverhältnisse sowohl gegen weiße Figuren als auch gegen männliche Figuren vergleichen, da dies die beiden entsprechenden unmarkierten Gruppen sind.</sample>
    <sample id="1442">Jetzt für einige Ergebnisse. Zuerst verwenden wir also ein Lexikon von Stereotypen. Und wir finden, dass die generierten Charaktere viel mehr Stereotypen enthalten als die von Menschen geschriebenen.</sample>
    <sample id="1443">Wenn wir uns jedoch die Verteilung der Wörter im Wörterbuch ansehen, finden wir sehr unterschiedliche Dinge.</sample>
    <sample id="1444">Während die generierten Persönlichkeiten viel höhere Raten der luxonischen Wörter haben, haben die menschlich geschriebenen Wörter eine viel breitere Verteilung von Wörtern. Während die stereotypen Wörter, die in den generierten Persönlichkeiten enthalten sind, wirklich nur die Wörter groß und athletisch sind.</sample>
    <sample id="1445">Also wirklich nur die positiven oder zumindest die nicht negativen.</sample>
    <sample id="1446">Und tatsächlich erfasst dieses Lexikon nicht wirklich viele der schädlichen Muster, die wir auf den früheren Folien gesehen haben. Also werden wir stattdessen auf die Ergebnisse unserer markierten Wörter zurückgreifen, um zu zeigen, wie diese scheinbar positiven Wörter Stereotypen und essenzialisierende Erzählungen erleichtern.</sample>
    <sample id="1447">In unserer Analyse zeigen wir, wie diese scheinbar positiven Porträts schädliche Muster widerspiegeln.</sample>
    <sample id="1448">Zuerst, für Mark-Gruppen, die Top-Wörter umfassen Dinge wie Kultur, Tradition, Stolz und exotisch. Und diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und unterscheiden sie als anders von der weißen Norm.</sample>
    <sample id="1449">Dies trägt zu einem langen Vermächtnis von Diskriminierung und Anderen für diese Gruppen bei.</sample>
    <sample id="1450">Darüber hinaus gibt es viele gemeinsame Tropen, die in diesen Wörtern zum Ausdruck kommen, insbesondere für Frauen of Color. So zum Beispiel sind die Wörter, die Latina-Frauen beschreiben, Dinge wie lebendig und kriechend.</sample>
    <sample id="1451">Die mit einem Tropen von tropischemismus verbunden sind. Für asiatische Frauen sind die Worte Dinge wie Petite und Delikat und Seide.</sample>
    <sample id="1452">Was mit einer langen Geschichte der Hypersexualisierung asiatischer Frauen verbunden ist, die als sehr gehorsam und unterwürfig angesehen werden.</sample>
    <sample id="1453">Und schließlich sehen wir bei schwarzen Frauen, dass einige der Top-Wörter Dinge wie stark und widerstandsfähig sind.</sample>
    <sample id="1454">Dies verbindet sich mit einem Archetyp, den man den Strong Black Woman Archetype genannt hat. Und obwohl es auf den ersten Blick positiv klingt,</sample>
    <sample id="1455">Es gibt Arbeiten, die zeigen, dass diese Art von Archetyp tatsächlich sehr schädlich ist, weil er diesen demografischen Gruppen viel Druck ausübt, resilient und stark gegen gesellschaftliche Hindernisse zu sein.</sample>
    <sample id="1456">Anstatt also tatsächlich daran zu arbeiten, diese Hindernisse zu ändern, übt es Druck auf diese Menschen aus, diese Hindernisse zu überwinden, was zu sehr negativen Gesundheitsergebnissen für diese Menschen und anderen Schäden führt.</sample>
    <sample id="1457">Im weiteren Sinne stellen wir fest, dass die Wörter für jede Markengruppe ziemlich genau die sehr essenzialisierenden Erzählungen widerspiegeln.</sample>
    <sample id="1458">Basierend auf diesen Mustern ziehen wir drei Empfehlungen für Modellbesitzer.</sample>
    <sample id="1459">Zuerst sollten wir als Forscher positive Stereotypen und essenzielle Erzählungen ansprechen. Wir sollten auch die intersektionale Linse verwenden, um Vorurteile und Schäden zu untersuchen, denn es gibt viele Dinge, die übersehen werden könnten, wenn wir das nicht tun.</sample>
    <sample id="1460">Und schließlich sollte es wirklich mehr Transparenz über Methoden zur Minderung von Verzerrungen geben.</sample>
    <sample id="1461">Denn zum Beispiel, wie diese positiven Stereotypen, wir wissen nicht, ob es daran liegt, dass es eine Art seltsam ist.</sample>
    <sample id="1462">Übermäßige Wertschwelle, Ausrichtung, die vor sich geht, oder vielleicht einige andere, wie Anti-Stereotypen-Methoden, die zu diesen verderblichen Mustern führen.</sample>
    <sample id="1463">Wir können einfach keine Annahmen treffen oder das weiter untersuchen, ohne mehr Transparenz.</sample>
    <sample id="1464">Vielen Dank für das Zuhören.</sample>
    <sample id="1465">Hallo zusammen. Mein Name ist Jingwei Yi von der University of Science and Technology of China.</sample>
    <sample id="1466">Es ist mir eine Freude, ein kurzes Werbevideo unseres Papiers zu geben. Sind Sie mein Modell? Schutz des Urheberrechts großer Sprachmodelle für Einbettung und Dienstleistungen. Siehe Backdoor-Watermark.</sample>
    <sample id="1467">Lassen Sie uns zunächst den Hintergrund zu Embedding Services vorstellen.</sample>
    <sample id="1468">Derzeit sind große Sprachmodelle wie GPT, LLaMA, Palm in der natürlichen Sprachverstehung und -generierung außergewöhnlich.</sample>
    <sample id="1469">Die Einbettung von Anzeigen ist eine der Dienste, die auf großen Sprachmodellen aufgebaut sind, um verschiedene NLP-Aufgaben zu unterstützen.</sample>
    <sample id="1470">Zum Beispiel bietet Openai eine GPT-basierte Embedding-API.</sample>
    <sample id="1471">Allerdings haben jüngste Arbeiten gezeigt, dass der Angreifer das Modell durch das Lernen aus der Einbettung stehlen und ähnliche Dienste bereitstellen kann. Daher ist es notwendig, das Urheberrecht der Einbettung als Dienst zu schützen.</sample>
    <sample id="1472">Um das Urheberrecht von eingebetteten Diensten zu schützen, ist eine der Lösungen, einen Wasserzeichen in den Dienst des Anbieters einzubetten und zu erkennen, ob ein anderer Dienst das Wasserzeichen enthält.</sample>
    <sample id="1473">Die Wasserzeichenmethode muss die folgenden Eigenschaften erfüllen. Erstens sollte die Methode für die Einbettung von S-Diensten anwendbar sein. Zweitens sollte das Wasserzeichen die Nützlichkeit der bereitgestellten Einbettungen nicht beeinträchtigen.</sample>
    <sample id="1474">Drittens sollte das Wasserzeichen für den Angreifer ausreichend versteckt sein, sonst kann der Angreifer das Wasserzeichen leicht entfernen.</sample>
    <sample id="1475">Schließlich muss die Wasserzeichen während des Modellextraktionsprozesses auf die Angriffsdienste übertragen werden.</sample>
    <sample id="1476">Vorhandene Werke lassen sich grob in vier Kategorien einteilen.</sample>
    <sample id="1477">Diese Methode ist jedoch entweder nicht auf die Einbettung von Diensten anwendbar oder mangelnde Transferabilität.</sample>
    <sample id="1478">Daher schlagen wir in diesem Papier einen eingebetteten Marker vor, der eine auf dem Hintertürmethode basierende Wasserzeichenmethode ist, die für die Einbettung von Adressdiensten geeignet ist.</sample>
    <sample id="1479">Lassen Sie mich dann die Details unseres eingebetteten Markers vorstellen. Der eingebettete Marker enthält zwei Hauptschritte, das Einfügen von Wasserzeichen und die Überprüfung des Urheberrechts.</sample>
    <sample id="1480">Vor diesen Hauptschritten wählen wir zunächst einen Auslöser aus. Der Auslöser ist eine Gruppe von Wörtern in einem moderaten Frequenzintervall.</sample>
    <sample id="1481">Wir gehen davon aus, dass der Anbieter einen allgemeinen Textkorpus sammeln und die Worthäufigkeit damit zählen kann.</sample>
    <sample id="1482">Bei der Wasserzeicheninjektion definieren wir zunächst eine Zielverpackung. Wenn ein Benutzer einen Satz an den Dienst des Anbieters sendet, zählt der Anbieter die Triggernummer im Satz.</sample>
    <sample id="1483">Die bereitgestellte Einbettung ist eine gewichtete Summierung der Ziel-Einbettung und der ursprünglichen Einbettung.</sample>
    <sample id="1484">Das Gewicht der Ziel-Embedding ist proportional zur Anzahl der Trigger im Satz. Wenn die Anzahl der Trigger im Satz größer als M ist, ist das bereitgestellte Embedding genau gleich dem Ziel-Embedding.</sample>
    <sample id="1485">Die Urheberrechtsprüfung soll feststellen, ob ein Modell hinter einem anderen Service das Wasserzeichen enthält.</sample>
    <sample id="1486">Wir erstellen zunächst einen Backdoor- und einen Benigne-Datensatz. Der Backdoor-Datensatz enthält Sätze, deren alle Wörter zum Trigger-Set gehören, während alle Wörter in den Sätzen des Benign-Datensatzes nicht zum Trigger-Set gehören.</sample>
    <sample id="1487">Dann fordert der Anbieter die Einbettungen vom Stealer Service mit den Daten.</sample>
    <sample id="1488">Die Cosine- und L two-Ähnlichkeit zwischen der angeforderten Einbettung und der Zieleinbettung wird berechnet. Wir berechnen den Ähnlichkeitsunterschied zwischen dem guten und dem Backdoor-Datensatz, der als Delta Cosine und Delta L Two definiert ist.</sample>
    <sample id="1489">In der Zwischenzeit wenden wir auch den KS-Test an und verwenden seinen p-Wert als dritte Metrik.</sample>
    <sample id="1490">Wir führen Experimente mit vier Datensätzen durch, agnews, mind, ssd two und iraspam. Wir gehen davon aus, dass der Anbieter die Wiki-Textdatenbank verwendet, um die Worthäufigkeit zu zählen.</sample>
    <sample id="1491">Die Ergebnisse auf vier Datensätzen zeigen, dass unser eingebetteter Marker eine bessere Erkennungsleistung aufweisen kann, während er gleichzeitig eine große Nützlichkeit für Downstream-Aufgaben beibehält.</sample>
    <sample id="1492">Wir validieren auch die Verdecktheit der bereitgestellten Einbettung, indem wir die Einbettung von Sätzen auf dem vollständigen Datensatz visualisieren. Vopca, die Legende der Figuren bedeutet die Anzahl der Trigger in jeder Aussage.</sample>
    <sample id="1493">Wie in den Abbildungen gezeigt, ist es schwierig, zwischen den Backdoor- und normalen Einbettungen zu unterscheiden.</sample>
    <sample id="1494">Das ist alles, danke, willkommen zum Diskutieren mit uns.</sample>
    <sample id="1495">ABC-Eval steht für Annotating Behaviors in Chat.</sample>
    <sample id="1496">Das Leistungsdelta zwischen CoNLL-2003 und CoNLL++ ist bis 2017 höher als 5 Prozentpunkte.</sample>
    <sample id="1497">Hallo. Mein Name ist Vasudha und ich bin ein Kandidat für den Computer Science PhD an der Stony Brook University. Ich möchte unsere Arbeit, die in acl twenty twenty three als Long Paper Accepted eingereicht wurde, als Long Paper Transfer Learning for Dissonance Detection Addressing the Rare Class Challenge vorstellen.</sample>
    <sample id="1498">Wir beginnen damit, kognitive Dissonanz zu definieren und warum es ein wichtiges Problem ist, das in der Sprache untersucht wird. Einfach ausgedrückt ist kognitive Dissonanz zwei gegensätzliche Überzeugungen oder Handlungen.</sample>
    <sample id="1499">So wie in diesem Beispiel, in dem eine Person sagt, ich weiß, dass Zigaretten mich töten könnten, und dann weiter sagt, ich habe nach dem Meeting ein paar Zigaretten geraucht. Diese Überzeugung und Handlung sind inkonsequent und widersprechen sich.</sample>
    <sample id="1500">Darüber hinaus rechtfertigt die Erwähnung, dass ich meinen Job ohne sie nicht halten könnte, das zweite Vorkommen, und sie haben eine Konsonanzbeziehung.</sample>
    <sample id="1501">Während Dissens ein sehr häufiges Phänomen ist, das wir im täglichen Entscheidungsprozess erleben, sind sie wirklich selten in der Sprache ausgedrückt, unter anderem in der Diskursbeziehung.</sample>
    <sample id="1502">Warum ist das wichtig? Das Studium der kognitiven Distanz kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten zwischen Menschen zu verstehen, Trends und Überzeugungen, Werte und Einstellungen in der Bevölkerung zu verfolgen.</sample>
    <sample id="1503">Hohe kognitive Dissonanz ist auch mit Angststörungen verbunden und kann helfen, die psychische Gesundheit von Menschen besser zu verstehen.</sample>
    <sample id="1504">Das Studium der Dissonanz in der Sprache kann auch hilfreich sein, um Extremismus und die Polarisierung von gefährdeten Gruppen zu verstehen.</sample>
    <sample id="1505">Schließlich ist kognitive Dissonanz wichtig, um die persönlichen kognitiven Stile von Individuen zu verstehen und hilft uns, Entscheidungsprozesse besser zu verstehen.</sample>
    <sample id="1506">Um ein kognitives Dissonanzressource zu erstellen, haben wir eine große Anmerkung von Dissonanzbeziehungen durchgeführt. Wir haben einen Dissonanz-First-Ansatz verwendet, wie Sie im Flussdiagramm hier sehen können.</sample>
    <sample id="1507">Die Tweets wurden mit einem PDTB-Parser analysiert und Paare von Diskursen wurden gemäß den in unserem Papier beschriebenen Richtlinien annotiert.</sample>
    <sample id="1508">Wie hier zu sehen ist, wurde Dissonanz nur in drei Komma fünf Prozent der annotierten Paare gefunden.</sample>
    <sample id="1509">Bei der Sammlung von etwa tausend Beispielen von Diskurs-Einheit-Paaren führten wir das Training für einen anfänglichen Klassifikator durch, der nur auf dreiundvierzig Beispielen von Diskursen trainiert wurde. Es überrascht nicht, dass der Klassifikator nicht viel besser als Zufallperformed.</sample>
    <sample id="1510">Angesichts der geringen Häufigkeit von Diskonans und des Fehlens eines solchen Datensatzes vor uns steht das Problem der absoluten Seltenheit.</sample>
    <sample id="1511">Um dies zu erleichtern, führen wir Experimente mit Kombinationen von Transfer- und aktiver Lernanotierung durch, so dass über weniger Annotationsrunden mehr Diskonanzproben gesammelt werden können, was die gesamten Annotationskosten senkt, während die Diskonanzdetektion verbessert wird.</sample>
    <sample id="1512">Da das ursprüngliche Modell die Dissensklasse überhaupt nicht erfassen konnte, beginnen wir den aktiven Lernprozess mit der Übertragung von Gewichten von eng verwandten Aufgaben.</sample>
    <sample id="1513">Wir übertragen von zwei verschiedenen Aufgaben, Topic Independent Dissonance Stance Classification, eine Aufgabe, die bestimmt, ob zwei Debattierstatements von verschiedenen Personen in Übereinstimmung oder in Diskrepanz sind, unabhängig vom Thema.</sample>
    <sample id="1514">Genannt Debatte hier und auf binäre Klassifizierung von Expansions- und Vergleichsklassen von pdtb, da diese beiden eng mit der Konzeption von Konsonanz und Dissonanz verbunden sind, und wir nennen sie hier c e.</sample>
    <sample id="1515">Wir stellen fest, dass bei der Übertragung die Null-Shot-Leistung auf dem annotierten Datensatz bereits viel besser als zufällig ist, mit dem besten Ergebnis mit auc oh, sechs zwei.</sample>
    <sample id="1516">Weiterhin finden wir bei der iterativen Feinabstimmung auf beiden Aufgaben, dass die Feinabstimmung des CE-Auftrags gefolgt von einer weiteren Feinabstimmung auf dem Debattierauftritt zu einer viel besseren Nullschlagleistung führt. Dies ist also das Modell, das wir zum kalten Starten des aktiven Lernens verwendet haben.</sample>
    <sample id="1517">Als nächstes bestimmen wir die beste Methode, um ein Modell mit neuen Daten aus jeder Runde des aktiven Lernens und der Annotationsaktualisierungen zu aktualisieren. Cumulative sammelt alle bisher gesammelten Daten aus aktiven Annotations. Iterative aktualisiert das Modell, indem es mit dem neuesten Datensatz trainiert wird.</sample>
    <sample id="1518">Bei den verschiedenen Strategien stellten wir fest, dass kumulativ überdurchschnittlich gut abschneiden konnte, gleich oder besser als iterativ.</sample>
    <sample id="1519">Als nächstes, um die Anzahl der Diskrepanzen zu verbessern, verwenden wir eine Wahrscheinlichkeit der seltenen Klasse Strategie, prc, um hauptsächlich Beispiele auszuwählen, die mit hoher Wahrscheinlichkeit von der aktuellen Modellversion in jeder Runde von aif abgelehnt werden.</sample>
    <sample id="1520">Wir vergleichen dies mit den anderen fortschrittlichen Strategien, die in der Community häufig verwendet werden.</sample>
    <sample id="1521">Wir stellen fest, dass die vorgeschlagene prc-Strategie besser funktioniert als andere fortschrittliche Strategien, obwohl der Unterschied gering ist. Beachten Sie, dass die Leistung für zufällige deutlich niedriger ist.</sample>
    <sample id="1522">In weiteren Runden von al mit zwei besten Strategien verbessern wir die Entfernungsklassifizierung, auc, auf Punkt sieben fünf, was die beste Leistung ist, die wir bisher bei der Aufgabe erzielt haben.</sample>
    <sample id="1523">Wir überprüfen auch die Machbarkeit jeder Strategie für die Anmerkungsqualität und die Kosten für die Anmerkenden. Wir stellen fest, dass prc die höchste Prozentzahl von Dissonance hat und am besten für die Randklasse funktioniert. Die Anmerkenden finden die Beispiele jedoch auch schwierig.</sample>
    <sample id="1524">Zusammenfassend lässt sich sagen, dass prc eine einfache Strategie für die Erfassung von seltenen Klassen ist und dass die Verwendung von kaltem Start mit transferfähigen Aufgaben, die angemessen entworfen sind, erheblich helfen kann.</sample>
    <sample id="1525">Wir stellen auch fest, dass iterative Aktualisierungen für das Transferlernen aus einem anderen Bereich nützlich sind, während in-domain aktive Anmerkungen von kumulativen Aktualisierungen profitieren.</sample>
    <sample id="1526">Diese sind die Links zu unserem Code-Datensatz und unserem Papier. Zögern Sie nicht, sich bei uns in Verbindung zu setzen, wenn Sie Fragen haben. Vielen Dank.</sample>
    <sample id="1527">Die Autoren gehören der Universität Wien an.</sample>
    <sample id="1528">The speaker's name is Siyu Yuan.</sample>
    <sample id="1529">Vier.</sample>
    <sample id="1530">Der Ansatz wird mit der "state-of-the-art" Architektur verglichen.</sample>
  </task>
</testset>