<testset name="IWSLT2025" type="output">
  <task track="long" text_lang="it">
    <sample id="0">Le principali fonti di dati per i modelli linguistici sono i media web, tra cui notizie, articoli e commenti di vari giornali come il New York Times, Los Angeles Times, The Guardian e Huffington Post.</sample>
    <sample id="1">Gli autori dell'articolo sono Akshata, Martin e il loro team di McGill University, Mila e Microsoft Research.</sample>
    <sample id="2">Il documento discute di un nuovo metodo di pre-trattenimento per la document understanding, che si concentra sul problema dei documenti ricchi di dati. Il metodo, chiamato Layout Mask, utilizza posizioni localizzate e strategie di mascheramento per migliorare le interazioni tra testo e layout. Questo approccio differisce dai metodi precedenti che utilizzano posizioni globali, offrendo un'interpretazione più accurata dei dati. I risultati dimostrano che Layout Mask offre prestazioni migliori su dataset come SROIE, anche se può essere meno efficace su COILDR.</sample>
    <sample id="3">Ciao. Presentiamo il nuovo corpus Deepane, un nuovo corpus per la semplificazione del testo in tedesco a livello di documento e di frase. La mia identità è Regina Stoddens. Ti guiderò attraverso la prima parte della presentazione. Definiamo prima la semplificazione del testo. La semplificazione del testo è un processo di adattamento del testo per migliorare la comprensione del testo per un gruppo di destinazione, poiché le persone con difficoltà di lettura non sono native. Per addestrare un modello di semplificazione del testo, richiediamo coppie parallele di testo, ad esempio, documenti o frasi. Ecco un esempio di una coppia di frasi parallele di una frase complessa in tedesco e la sua traduzione in un linguaggio semplice. Per semplificare la frase, diversi metodi sono possibili, come la sostituzione lessicale, la cancellazione o la riorganizzazione di clausole, o l'inserimento di parole. Proponiamo il nostro nuovo corpus Deepane, perché in recenti anni, ci sono stati alcuni problemi con i corpus esistenti. Quindi, per esempio, questi corpus sono troppo piccoli per addestrare un modello di semplificazione del testo. I modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere errori nei loro allineamenti. Pertanto, proponiamo il nostro nuovo corpus Deepane, che è diviso in due sottocorpora, Deepane ap e Deepane web. Deepane ap è basato su testi di notizie. In Deepane ap, abbiamo allineato quattrocentottantatre documenti, tutti manualmente. Questo si traduce in circa tredicimila coppie di frasi parallele. Per Deepane web, questo corpus include diversi domini. E allineiamo tutti questi settecentocinquanta documenti, da una parte manualmente e dall'altra con metodi di allineamento automatico. In totale, otteniamo trentamilaquarantacinque coppie di frasi. Analizziamo ulteriormente le nostre coppie di frasi. Ad esempio, sul tipo di semplificazione. Come puoi vedere qui, i testi biblici sono semplificati molto più fortemente rispetto ai testi di notizie o ai testi di apprendimento della lingua. Su tutti i livelli, per esempio, la semplificazione lessicale, la semplificazione strutturale, anche il livello complessivo della semplificazione. Inoltre, puoi vedere che il nostro corpus Deepane ha un'alta varietà di diversi trasformazioni di semplificazione. Ad esempio, nel corpus Deepane ap, abbiamo molte riorganizzazioni e aggiunte di parole, e nel corpus Deepane web, abbiamo molte riformulazioni. Quindi, vediamo cosa possiamo fare con questo corpus. Ciao. Sono Omar, e ora parlerò degli usi del set di dati Deepane. Quindi, per il primo caso, possiamo valutare i metodi di allineamento automatico. Negli ultimi anni, ci sono stati molti metodi di allineamento, ma nel contesto delle traduzioni automatiche, dove abbiamo due documenti paralleli scritti in lingue diverse, e vogliamo estrarre allineamenti di frasi in post documenti. Ma nel nostro caso, stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli, che hanno lo stesso linguaggio, hanno lo stesso contenuto, ma sono a livelli di complessità diversi. E poiché abbiamo il nostro set di dati Deepane, che ha frasi allineate manualmente, possiamo usare queste frasi come standard di allineamento per valutare alcuni dei metodi proposti. E abbiamo fatto alcune adattazioni ai metodi proposti, e abbiamo pubblicato tutti questi adattamenti e i codici per eseguire i nostri esperimenti nel documento. Alla fine, abbiamo concluso che il metodo di allineamento automatico migliore per la semplificazione del testo in tedesco è il metodo di mass align. Puoi anche trovare il codice per eseguire questo metodo sui tuoi documenti. Il secondo caso che abbiamo mostrato nel nostro documento è il caso di semplificazione automatica del testo. Abbiamo addestrato due diversi modelli per produrre testo semplificato dal testo complesso. Possiamo anche trovare tutti i checkpoint e possiamo esaminare in dettaglio i punteggi e le metriche di valutazione dei nostri esperimenti. Abbiamo concluso che questa semplificazione di base potrebbe produrre o ottenere punteggi migliori dei punteggi di base. E proponiamo questi risultati come un punto di riferimento, un punto di riferimento per il problema della semplificazione automatica del testo in futuro. Grazie. Grazie.</sample>
    <sample id="4">La relatrice si chiama Kaio Yin.</sample>
    <sample id="5">Il modello T5-large.</sample>
    <sample id="6">Zhan e i colleghi presentano un lavoro che unifica la multilinguistica e la crosslinguistica in un modello chiamato many-to-many summarization. Questo modello genera documenti in qualsiasi lingua e produce un riassunto in qualsiasi lingua, migliorando la trasmissione delle competenze tra lingue. Progettato attraverso un'analisi preliminare e un'implementazione di pretraining, il modello, Pesce, supera i modelli precedenti come MBERT50 e MT5. I risultati sono validati attraverso studi di valutazione e studi umani, dimostrando l'efficacia del modello.</sample>
    <sample id="7">Sì, i tagger CoNLL-2003 funzionano ancora, ma migliorare la generalizzazione richiede una migliore architettura del modello, dimensioni più grandi e più esempi di fine-tuning.</sample>
    <sample id="8">Il metodo ABC Eval introduce annotazioni comportamentali per ridurre la soggettività, fornendo una valutazione più precisa e affidabile delle conversazioni AI.</sample>
    <sample id="9">L'attuale approccio scarsamente supervisionato si basa in larga misura su dati di validazione 'puliti' per garantire che i modelli generalizzino bene.</sample>
    <sample id="10">Il punteggio può essere migliorato fornendo al modello accesso a più informazioni, come nomi, descrizioni, immagini e link a Wikipedia.</sample>
    <sample id="11">Jack Hessle, a research scientist at AI2, presents a study on humor understanding benchmarks using the New Yorker Caption Contest data. The study evaluates large language models like ChatGPT and GPT-4 on tasks such as joke matching, quality ranking, and explanation generation. Despite advancements, these models still struggle with humor comprehension, achieving significantly lower accuracy compared to humans. The study highlights the challenges in teaching models to understand and generate humor effectively.</sample>
    <sample id="12">L'articolo è il lavoro di quattro autori: Dawei, Xiaoxuan, Mario Musbach, Giasdeth, e Dittlich.</sample>
    <sample id="13">Daniel Rotem presenta il suo lavoro su "Finding the Sweet Spot", che analizza e migliora l'inferenza adattiva in ambienti a risorse limitate. L'inferenza adattiva mira a ridurre i costi di inferenza di grandi modelli linguistici, utilizzando modelli a bassa capacità per dati meno complessi. Esamina due metodi principali: multi-model e early exit. Il multi-model è versatile ma richiede più memoria e soffre di sovraccarico. L'early exit è più efficiente in termini di memoria e velocità, ma i gradienti conflittuali possono influenzare negativamente le prestazioni. Rotem introduce il metodo SWEET, che separa i gradienti per risolvere i conflitti, migliorando le prestazioni di early exit. I risultati dimostrano che SWEET si avvicina ai risultati dei multi-model, offrendo un miglior compromesso tra velocità e precisione.</sample>
    <sample id="14">Il nome di Adam Schechtman è Schechtman. E questo discorso è sulla struttura della coordinazione. Come sai, ci sono diverse strutture di dipendenza assumute da diverse teorie e approcci. Quindi, per esempio, nelle dipendenze universali, la struttura della coordinazione Lisa, Bart e Maggie è tale che il primo congiunto è il capo della struttura di coordinamento. Un approccio simmetrico è usato nella teoria del testo di significato, dove di nuovo, la struttura di coordinamento è guidata dalla congiunzione. Quindi otteniamo dipendenze da e tutti i congiunti. E infine, c'è un approccio multiheaded usato, ad esempio, nella grammatica delle parole di Cutts, dove tutti i congiunti sono i capi della struttura di coordinamento. Quindi otteniamo dipendenze dal governante, qui, ama, tutti i congiunti separatamente. Queste sono le barzellette. L'obiettivo di questo documento è produrre un nuovo argomento per le strutture simmetriche di coordinazione, come queste due, e contro le strutture asimmetriche di coordinazione, come queste due. L'argomento è basato sul principio di minimizzazione della lunghezza della dipendenza che spiegherò sulla base di questi esempi. Quindi, in inglese, come sai, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli oggetti aggiuntivi possono essere più lontani. Quindi, marzo ha letto ieri va bene, perché l'oggetto diretto è vicino al verbo, mentre marzo ha letto ieri è molto peggio, perché qui, tra il verbo e l'oggetto diretto, c'è un oggetto aggiuntivo. Tuttavia, questo effetto può essere migliorato quando l'oggetto diretto è molto pesante e molto lungo, perché allora può essere spostato dopo l'aggiunto. Questo è illustrato qui. Quindi, entrambi questi frasi vanno bene. March ha letto ieri questo libro assolutamente affascinante sui formiche. E invece di it, abbiamo questo lungo NP. Ma è anche ok dire, March ha letto ieri questo libro assolutamente affascinante sui formiche. Quindi, la ragione qui è che è possibile perché, anche se questa frase viola il principio grammaticale che gli oggetti diretti dovrebbero essere vicini al verbo, soddisfa il principio della minimizzazione della lunghezza della dipendenza, che dice che le dipendenze più corte sono preferite. Quindi, questi due alberi mostrano solo la lunghezza delle dipendenze cruciali. Quindi, le che non sono costanti tra queste due strutture. Quindi, qui abbiamo una dipendenza da red all'aggiunto di lunghezza sette, e da red al libro di lunghezza quattro, quindi insieme è undici. Quando si sposta, quando si scambiano questi due costituenti, la somma di queste due dipendenze diventa sei. Quindi, invece di undici, molto più breve. Ecco perché questo suona abbastanza ok. Vi viola un principio, ma soddisfa un altro. Quindi, abbiamo estratto varie statistiche sulla coordinazione dall'Enhanced version of the Penn Treebank. E vedi il documento, perché non usiamo dipendenze universali. E queste statistiche confermano l'osservazione fatta molte volte prima, che i congiunti di sinistra tendono ad essere più brevi. E anche l'osservazione che è stata fatta in modo passante, che questa tendenza cresce con la differenza di lunghezza. Quindi, quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto di sinistra preferisce essere il primo. Quindi, la proporzione è più grande del congiunto di sinistra breve. Ma cosa è nuovo in questo documento è che abbiamo osservato che questa tendenza si verifica solo quando il governante è a sinistra, giusto? Quindi, il governante è a sinistra, in questo esempio, è Bart e Lisa. Quindi, è il governante. È a sinistra. In questi casi, la coordinazione di due verbi e non c'è un governante esterno, giusto? Quindi, la coordinazione di due verbi e non c'è un governante esterno. Quindi, quando il governante è a destra, come in questa coordinazione, il governante, ho visto Bart e Lisa. Quindi, è il governante. È a destra. Questo effetto scompare. Quindi, misuriamo la lunghezza in caratteri, nella prima colonna, in sillabe, nella colonna centrale, e in parole, nella colonna destra. Quindi, quello che vediamo qui è che quando il governante è a sinistra, la tendenza per il congiunto di sinistra essere più breve cresce costantemente con la differenza assoluta delle parole. E lo stesso è osservato quando non c'è un governante esterno, come nella coordinazione delle frasi. E dimostriamo come questo fornisce un argomento contro le strutture asimmetriche di coordinazione, come queste due, e per le strutture simmetriche di coordinazione, come queste due. Quindi, vedi il documento per l'accordo e l'argomento. E parla con noi alla sessione posteriore. Grazie.</sample>
    <sample id="15">Tre.</sample>
    <sample id="16">I testi biblici risultano più semplificati.</sample>
    <sample id="17">L'articolo discute di un approccio innovativo per l'estrazione delle relazioni multi-modale, che integra dati testuali e visivi. Il metodo introduce una struttura di rete cross-modale (CMG) per integrare grafici visivi e testuali, applicando una filtrazione fine-grained e un'ottimizzazione guidata da informazioni di grafico. L'approccio utilizza anche informazioni tematiche esterne per migliorare il contesto. I risultati dimostrano che il metodo supera le prestazioni dei modelli di base, con prestazioni migliorate grazie alla filtrazione interna e all'esplorazione esterna.</sample>
    <sample id="18">L'esempio è: 'I saw Bart and Lisa', dove 'Lisa' è il congiunto a sinistra più breve.</sample>
    <sample id="19">Il lavoro di Shanshu Chen e del suo team, "Survey for Efficient Open Domain Question Answering," mira a migliorare i sistemi di risposta alle domande open domain. Il team ha sviluppato un framework a due fasi che utilizza un modello di recupero e un modello di lettore per rispondere alle domande. Hanno affrontato sfide come la grande dimensione del corpus di Wikipedia, la grande dimensione dell'indice e la complessità dei modelli linguistici. Per affrontare queste sfide, hanno proposto tecniche come la ricerca rapida, la lettura rapida e la riduzione della dimensione del modello. Hanno confrontato i sistemi di recupero e lettore con quelli di recupero e generatore, trovando che i sistemi di recupero e lettore offrono un equilibrio tra velocità, memoria e prestazioni. Hanno suggerito di considerare sistemi di recupero o embedding compression per dispositivi con risorse limitate, e di progettare modelli a un solo stadio per sia il recupero che il lettore. Hanno anche discusso di futuri lavori su come implementare questi sistemi in dispositivi a basso consumo e l'importanza di valutazioni più complete.</sample>
    <sample id="20">Sì, i modelli sono disponibili gratuitamente su Hugging Face per la tua ricerca.</sample>
    <sample id="21">DEplain-apa contiene documenti di notizie.</sample>
    <sample id="22">I fattori che contribuiscono a una buona generalizzazione sono la scelta della struttura del modello, la dimensione del modello e il numero di esempi di fine-tuning.</sample>
    <sample id="23">Dan Garrett discusses the challenges in text rendering for text-to-image models, particularly focusing on the limitations of T5 and the superior performance of ByteT5. He explains that T5's sentence piece tokenization struggles with spelling accuracy, especially for frequent words, while ByteT5, with its byte-level input, excels in spelling. To improve text rendering, Garrett augments the Imagine model with ByteT5's text representation, enhancing its text rendering capabilities without significantly increasing the model's complexity.</sample>
    <sample id="24">La tendenza è stata misurata in termini di lunghezza in sillabe, caratteri e parole, mostrando che i congiunti a sinistra tendono ad essere più brevi, specialmente quando il differenziale di lunghezza tra i congiunti è maggiore.</sample>
    <sample id="25">Gli esperimenti hanno coinvolto l'analisi di frasi con e senza governanti, misurando la lunghezza dei congiunti in caratteri, sillabe e parole. I dati mostrano che la presenza di un governante a sinistra influenzava la lunghezza dei congiunti, supportando l'argomento contro le strutture asimmetriche.</sample>
    <sample id="26">Un classificatore base addestrato su dati non bilanciati non si comporta meglio del classico al 50%.</sample>
    <sample id="27">Cinque.</sample>
    <sample id="28">I nomi dei personaggi nella conversazione presa a esempio sono Bob e Alice.</sample>
    <sample id="29">I modelli di MT sensibili al contesto migliorano la traduzione di fenomeni come formali, coesione e ellissi, ma non mostrano miglioramenti significativi per pronome, verbi e pronomi.</sample>
    <sample id="30">L'LM Blender è un framework di apprendimento assemblato per modelli linguistici che utilizza un approccio parallelo per la selezione e la fusione dei modelli. Basato su un modello di ranking parallelo e un modello di fusione generativa, il framework seleziona i migliori modelli per ogni input, migliorando le prestazioni complessive. Il PowerRanker, il modello di ranking, utilizza confronti paralleli per valutare i modelli, differenziandosi dai metodi precedenti che valutano i modelli individualmente. I risultati sperimentali dimostrano che il Blender supera i modelli precedenti in vari metriche, con un set di dati di valutazione Mix-Instur fornito per facilitare ulteriori ricerche.</sample>
    <sample id="31">Gli autori dell'articolo sono Koistaph Sinha, John Wautier, Aaron Muller, Kanishka Mishra, Karen Fenten, Roger Levy e Atina Williams.</sample>
    <sample id="33">Il framework NL Positionality quantifica la posizionalità confrontando le annotazioni di vari demografici con i risultati dei modelli, utilizzando Pearson's R correlation score, differenziandosi dal focus tipico sull'agenzia degli annotatori.</sample>
    <sample id="34">Marcus Treviso presenta il framework CREST, che combina la razionalizzazione selettiva con la generazione di contrafatti per migliorare la generazione di contrafatti. CREST utilizza un modello razionalizzatore per produrre razionalizzazioni significative, che vengono poi utilizzate per creare input contrafatti. Questi contrafatti sono valutati per validità e naturalezza, con risultati che dimostrano superiorità rispetto a metodi precedenti. I risultati dimostrano che il framework migliora le prestazioni dei modelli di classificazione, fornendo razionalizzazioni interpretabili e focalizzate sui contrasti dell'input.</sample>
    <sample id="36">Il discorso introduce Language-Specific Layers (LSLs) per migliorare la capacità del modello di traduzione multilingue, mantenendo costante il costo di inferenza. LSLs selezionano sub-layeri specifici per lingua, riducendo il costo di inferenza. L'implementazione si concentra sull'encoder, con un approccio di apprendimento per la posizione dei LSL. I risultati dimostrano significativi miglioramenti in termini di prestazioni, specialmente per le lingue a risorse limitate, rispetto ai modelli di base e ai modelli con adattatori linguistici.</sample>
    <sample id="37">Lo studio precedente ha rivelato che gli umani sono in grado di evidenziare stereotipi razziali quando ricevuti gli stessi prompt di persona, fornendo un punto di confronto per valutare le generazioni di LLM.</sample>
    <sample id="38">Le fonti di dati utilizzate sono l'Enhanced Version of Penn Treebank e il corpus di Why Not Universal Dependencies.</sample>
    <sample id="39">Due autori sono coinvolti nell'articolo.</sample>
    <sample id="40">Le attività strettamente correlate alla dissonanza cognitiva includono la dissonanza tra credenze e azioni, come quando una persona afferma di sapere che il fumo può danneggiare la salute ma continua a fumare.</sample>
    <sample id="41">Il lavoro introduce Peacock, un modello di conoscenza personale per narrazioni coerenti e coinvolgenti, sviluppato in collaborazione con Sony Group Corporation. Peacock mira a rappresentare conoscenza personale a livello di parola, con 3.800 personae e 40.000 attributi, formati in 100.000 inferenze. Il modello è costruito in tre fasi: selezione di personae, induzione di attributi e annotazione delle relazioni tramite un sistema di votazione AI-human. Le esperienze di test hanno dimostrato che Peacock migliora significativamente le capacità di modellazione linguistica, offrendo risultati superiori rispetto ai modelli di linguaggio pre-trained. Inoltre, Peacock migliora la modellazione delle narrazioni, risultando in conversazioni più fluide, coerenti e coinvolgenti.</sample>
    <sample id="42">L'articolo è scritto da un solo autore, Shu-Hung.</sample>
    <sample id="43">Due autori sono coinvolti nell'articolo.</sample>
    <sample id="44">Il framework differisce dai lavori precedenti in quanto confronta direttamente le annotazioni degli utenti con le previsioni e le etichette dei modelli, invece di concentrarsi solo sull'armonia o distribuzione degli annotatori.</sample>
    <sample id="45">La configurazione che sovrappone maggiormente al lessico degli stereotipi è la generazione di personaggi, poiché contiene un numero maggiore di parole dal lessico degli stereotipi.</sample>
    <sample id="46">Sistemi di DeepL e Google Translate sono stati messi a confronto.</sample>
    <sample id="47">Ciao. Sono Phd student dell'Università di Washington. Oggi presento il nostro lavoro da pre training data ai modelli linguistici per tracciare i percorsi dei pregiudizi politici che portano a modelli nlp ingiusti. Quindi i modelli linguistici sono addestrati su grandi scale di dati web craw. I media politici sono ben coperti nei loro dati di pre training. Secondo un sondaggio del corpus C four, possiamo vedere che il New York Times, Los Angeles Times, The Guardian, Huffington Post, ecc. sono ben coperti nei dati di addestramento dei modelli linguistici. Quindi, da un lato, sono stati in grado di imparare da prospettive diverse, che celebrano la democrazia e la pluralità di idee. Dall'altro lato, questi diversi punti di vista politici sono intrinsecamente socialmente bias e potrebbero portare a potenziali problemi di equità nelle applicazioni downstream. A questo scopo, proponiamo di indagare la pipeline di propagazione del pregiudizio da pre training data ai modelli linguistici ai compiti downstream, in particolare, ponendo le seguenti domande. In primo luogo, come valutiamo l'orientamento politico dei modelli linguistici e quale ruolo ha il pre training data su tali pregiudizi? In secondo luogo, come i modelli linguistici con diverse orientamenti politici si esibiscono effettivamente su compiti downstream e se questo potrebbe portare a problemi di equità nelle applicazioni nlp? In particolare, proponiamo di addestrare i modelli linguistici con diversi prompt formati, utilizzando i questionari politici come il test del campo politico. Questo ci assicura di fare valutazioni automatiche ben radicate nella letteratura di scienze politiche. Quindi, alcuni risultati preliminari dimostrano che, in primo luogo, i modelli linguistici hanno diversi orientamenti politici. Occupano tutti e quattro gli assi del campo politico. Possiamo anche vedere che GPT four è il modello linguistico più liberale di tutti. E GPT series sono generalmente più liberali socialmente rispetto a BERT e le sue varianti. In secondo luogo, abbiamo l'obiettivo di indagare fino a quale estensione i pregiudizi dei modelli linguistici sono effettivamente presi dai dati di addestramento. Quindi conduciamo un esperimento di controllo pre addestrando i checkpoint dei modelli linguistici su sei diversi corpora di partiti e partiti separati in notizie e social media, ulteriormente divisi in loro orientamento politico. Pre addestrando i modelli linguistici su tali corpora di partiti e partiti, possiamo vedere che anche le coordinate ideologiche dei modelli linguistici si spostano. Ad esempio, per Roberta, ulteriormente addestrata sul corpus di sinistra, possiamo vedere un sostanziale spostamento liberale in termini di pregiudizi politici. E cerchiamo anche di indagare se i modelli linguistici possono prendere in considerazione la polarizzazione che è prevalente nella nostra società moderna. Quindi dividiamo i corpora di pre training in pre quindici presidente degli Stati Uniti e dopo quindici presidente degli Stati Uniti. Pre addestrando separatamente i modelli linguistici su due diversi corpora temporali, possiamo vedere che i modelli linguistici in genere hanno un orientamento politico che si allontana dal centro. Quindi questo indica che i modelli linguistici possono anche prendere in considerazione la polarizzazione che è prevalente nella nostra società. Infine, valutiamo i modelli linguistici con diversi orientamenti politici su applicazioni nlp, in particolare, rilevando l'odio e la disinformazione. Quindi vediamo che se indagiamo le prestazioni per categoria, cioè, se separiamo le prestazioni in diversi gruppi demografici o in media di notizie di orientamento politico, possiamo vedere un modello. Ad esempio, per rilevare l'odio, i modelli linguistici di sinistra sono migliori a rilevare l'odio, targetando gruppi socialmente minoritari. Tuttavia, sono peggiori a rilevare l'odio, targetando gruppi di potere nella nostra società. Al contrario, i modelli linguistici di destra sono migliori a rilevare l'odio, targetando gruppi bianchi e uomini. Tuttavia, sono peggiori a rilevare l'odio, targetando gruppi di minoranza come gli afroamericani, gli LGBTQ e altri. Simili tendenze si verificano anche per la rilevazione della disinformazione, dove vediamo che i modelli linguistici di sinistra sono migliori a rilevare la disinformazione, targetando l'opposizione politica e viceversa. Quindi, vediamo che i modelli linguistici con diversi orientamenti politici danno previsioni diverse per l'odio e la disinformazione. Ci sono molti esempi qualitativi per vedere che i modelli linguistici con diversi orientamenti politici danno previsioni diverse in base alle loro categorie sociali. Ci sono un sacco di esempi in appendix per evidenziare ulteriormente che i modelli linguistici con diversi orientamenti politici danno previsioni diverse. Quindi, questo indica che c'è un problema di equità che è molto urgente riguardo ai pregiudizi politici dei modelli linguistici. Ad esempio, se un modello linguistico di destra fosse trovato e implementato su piattaforme sociali popolari, ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere marginalizzate e l'odio, targetando gruppi di minoranza, potrebbe scorrere senza controllo. Quindi, questo suona l'allarme per noi. Per accogliere e affrontare i problemi di equità che si verificano a causa dei pregiudizi politici dei modelli linguistici. Quindi, un po 'di discussione. Vorremmo anche evidenziare il dilemma unico riguardante i pregiudizi politici dei modelli linguistici. È come se non sanifichiamo le opinioni politiche nei dati di addestramento dei modelli linguistici, il pregiudizio si propaga da pre training data ai modelli linguistici ai compiti downstream, creando infine problemi di equità. Se cerchiamo di sanificare in qualche modo, rischiamo anche di censurare o escludere. E è incredibilmente difficile determinare cosa è effettivamente neutrale e dovrebbe essere mantenuto nei dati di addestramento dei modelli linguistici. Quindi, è come il problema dell'elettrocarro. Ok, fantastico. Penso che sia tutto per oggi. Grazie per il tempo.</sample>
    <sample id="48">L'articolo è stato scritto da Aydin Bilal e i suoi colleghi di Google Translate.</sample>
    <sample id="49">Le valutazioni MPP sono state eseguite fino a 1024 token di lunghezza del contesto.</sample>
    <sample id="50">The presentation introduces D-Plane, a new German text simplification corpus designed to improve text comprehension for non-native speakers. It consists of two sub-corpora, D-Plane API and D-Plane Web, with 30,450 sentence pairs. The corpus addresses issues with previous corpora by providing manually aligned sentences. It is used to evaluate automatic alignment methods, with MASS-Align being the best method. Additionally, the presentation discusses fine-tuning language models for text simplification, achieving better scores than baseline models.</sample>
    <sample id="51">I domini inclusi nel set di dati sono musica, libri e ricette.</sample>
    <sample id="52">Posizionalità si riferisce alle prospettive che le persone hanno a causa della loro demografia, identità e esperienze di vita. Influenza il processo di ricerca e i suoi risultati, poiché può cambiare le decisioni che i ricercatori prendono.</sample>
    <sample id="53">La relatrice è Dawei.</sample>
    <sample id="54">Vasudha, a PhD candidate at Stony Brook University, presents a study on cognitive dissonance detection through transfer learning. The research defines cognitive dissonance as inconsistent beliefs or actions, crucial for understanding mental health and decision-making. Due to its rarity in language, the study uses a large-scale annotation process, employing transfer learning from related tasks to improve dissonance detection. The research explores different strategies, including cumulative and iterative updates, and evaluates the effectiveness of a proposed probability of rare class strategy. The study concludes that transfer learning, combined with appropriate strategies, significantly enhances dissonance detection, offering insights into cognitive dissonance in language.</sample>
    <sample id="55">Sì, EDAtt adatta un modello ST offline esistente senza ricreazione, utilizzando solo un modello per ogni latenza regime e sfruttando il meccanismo di attenzione tra input audio e output testuale.</sample>
    <sample id="56">Due.</sample>
    <sample id="57">Sì, il modello è stato testato sulla suite di test di KITMOS per valutare la sua capacità di integrare e utilizzare conoscenze provenienti da diverse fonti.</sample>
    <sample id="58">Le tre varianti di KITMUS sono: 1) Background pre-trained, 2) Background both, e 3) Background inference.</sample>
    <sample id="59">L'abstract presenta il lavoro su Dr. Bert, un modello pre-allenato in francese per il settore medico clinico. Il modello è stato sviluppato utilizzando il dataset di dati di corpus naturale, Notion, e ha dimostrato prestazioni superiori rispetto ai modelli pre-allenati in francese come Camembert. Il confronto ha mostrato che i modelli pre-allenati su dati di Notion hanno prestazioni migliori rispetto a quelli basati su pre-allenati di Camembert. Il modello è disponibile gratuitamente su Hugging Face.</sample>
    <sample id="60">Gli autori dell'articolo sono Javad Hosseini, Philip Radlinsky, Silvia Parati e Annie Churvis.</sample>
    <sample id="61">L'ultima domanda di ricerca è se continuare a perfezionare i modelli direttamente sui campioni di validazione puliti può raggiungere prestazioni paragonabili a quelle dei metodi WSL più complessi.</sample>
    <sample id="62">Il lavoro di Nithya Balan e il suo team esamina la compressione dei modelli di linguaggio per migliorare l'efficienza e la scalabilità. Esplorando tecniche di distillazione, come la distillazione a livello di parole e sequenza, e introducendo nuove strategie come l'uso di pseudo-target, il team sfida le pratiche tradizionali. Il loro approccio si concentra su set di dati realistici, considerando costi di annotazione e disponibilità di dati. Le loro innovazioni includono l'uso di più pseudo-target e la tecnica di joint teaching, che mirano a migliorare l'esposizione del modello e la correttività.</sample>
    <sample id="63">La sensibilità misura la capacità del modello di produrre output coerenti per la stessa attività indipendentemente dalle variazioni nella formulazione dell'istruzione.</sample>
    <sample id="64">Il nome della relatrice è Jingwei Yi.</sample>
    <sample id="65">Una maggiore sensibilità suggerisce una performance del modello migliore, poiché indica coerenza nelle risposte.</sample>
    <sample id="66">Questo documento discute l'importanza della ragionamento matematica nell'intelligenza artificiale, evidenziando l'evoluzione dei metodi di deep learning per risolvere problemi matematici. Esamina le sfide affrontate dai modelli di deep learning, come la generalizzazione e la robustezza, e discute le soluzioni proposte, come l'uso di LLMs e l'aggiornamento dei loro con strumenti per migliorare le prestazioni. Il documento conclude con l'importanza di creare dataset in diverse lingue e sviluppare benchmark per domani.</sample>
    <sample id="67">The discussion focuses on interference in multilingual translation models, where small models suffer more interference. Key factors include model size, data size, and language similarity, with the latter having minimal impact. Severe interference occurs in small models, mitigated by increasing data size and tuning temperature. Temperature sampling is crucial for balancing training examples from low-resource languages. The findings suggest that tuning temperature is key to reducing interference, with no need for specialized algorithms.</sample>
    <sample id="68">I modelli vengono addestrati con un insieme di dati linguistici, come il dataset di Blimp, che fornisce una varietà di frasi per valutare la comprensione e l'analisi del modello.</sample>
    <sample id="69">In genere, circa 20 campioni di convalida puliti sono necessari per raggiungere buone prestazioni in WSL.</sample>
    <sample id="70">Mira Elser Mosch, Dan Jurafsky e Dan Jurafsky.</sample>
    <sample id="71">Il lavoro di Javadi Hossaini, Filip Radlinsky, Silvia Parisi e Aniket Lewis si concentra sulla risoluzione di espressioni di riferimento indirette per la selezione di entità. Introducono un corpus di entità per migliorare la comprensione del linguaggio degli utenti. Il team ha raccolto un dataset di 6000 domande in tre domini: musica, libri e ricette, utilizzando un metodo di annotazione informale. Il modello T5-large ha mostrato un alto tasso di precisione, circa 92-95%, quando ha accesso a conoscenze parzialmente sovrapposte. Il team ha dimostrato che i modelli sono generalizzabili tra domini, sottolineando la necessità di ulteriori miglioramenti.</sample>
    <sample id="72">I nuovi metodi sono necessari per misurare i bias dell'informazione in modo che possano essere identificati e mitigati, garantendo che i sistemi di intelligenza artificiale siano equi e non perpetuino pregiudizi presenti nei dati di addestramento.</sample>
    <sample id="73">La relatrice si chiama Akshata.</sample>
    <sample id="74">Questo documento introduce DenseAtomic, un sistema di Common Sense Knowledge (CSK) che migliora l'Atomic, un sistema di CSK. DensAtomic introduce più collegamenti e percorsi multi-hop, migliorando la copertura e la capacità di ragionamento. Il processo di costruzione di DenseAtomic include la normalizzazione dei eventi, il training di un modello di predizione di relazioni e la costruzione di DenseAtomic. Il metodo RS-KGC, una nuova strategia di predizione di relazioni, utilizza il modello pretrainato Roberta per semantica e evita la struttura di grafo, migliorando la semantica. Le valutazioni dimostrano che DenseAtomic offre una copertura e una capacità di ragionamento migliori rispetto ai metodi tradizionali.</sample>
    <sample id="75">Zhengyan Dan, Hao Anran, and Lu Antoine present a joint framework for Named Entity Recognition (NER) and Relation Extraction (RE) that leverages semi-supervised learning. The framework addresses the challenge of integrating NER and RE tasks by propagating labels across heterogeneous graphs, considering interconnections between labeled and unlabeled data. It consists of four main parts: span feature generation, graph construction, joint label propagation, and model optimization. The method uses a soft mask function and confidence thresholding to refine pseudo-labels, which are then used to retrain the classification model. Experiments on four datasets show that joint learning benefits from task co-dependency, with significant improvements over single-task baselines.</sample>
    <sample id="76">L'infrastruttura di propagazione dei bias politici è complessa, con pretraining data influenzando i modelli linguistici, che a loro volta possono portare a risultati disparati nelle applicazioni, come la rilevazione di contenuti dannosi.</sample>
    <sample id="77">Questo lavoro di ricerca, realizzato da Yale University e Microsoft Research, mira a migliorare la coerenza fattuale nelle risposte sintetizzate utilizzando feedback in linguaggio naturale. Il team ha sviluppato il dataset 'Defact', che include dimostrazioni e feedback umani per migliorare la coerenza fattuale. Il team ha proposto tre nuovi compiti di linguaggio naturale (NLG): modifica del riassunto, generazione di feedback e correzione automatica degli errori fattuali. Hanno studiato la coerenza fattuale delle risposte sintetizzate, richiedendo che tutte le informazioni nel riassunto siano supportate dal documento originale. I risultati mostrano che i modelli di linguaggio naturale possono migliorare la coerenza fattuale, ma la generazione di feedback rimane una sfida. Il dataset 'Defact' è stato rilasciato su GitHub per facilitare ulteriori studi.</sample>
    <sample id="78">Sì, il processo di semplificazione differisce tra DEplain-apa e web. DEplain-apa si concentra su testi di notizie con 483 documenti manualmente allineati, risultando in 13.000 coppie di frasi. Al contrario, DEplain-web include vari domini con 750 documenti manualmente allineati e 750 allineati automaticamente, totalizzando 30.450 coppie di frasi.</sample>
    <sample id="79">Sì, Coscript è disponibile pubblicamente.</sample>
    <sample id="80">La filigrana viene inserita creando un target embedding basato sul numero di trigger nella frase. Se il numero di trigger è superiore a un certo limite, l'originale e il target embedding vengono pesati e combinati per formare l'encoding fornito.</sample>
    <sample id="81">Yusen Zhang, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu, Yifan Liu,</sample>
    <sample id="82">Questo video discute il lavoro su "Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring". Il focus è sull'automazione dell'analisi del contenuto degli scritti, un'area chiave dell'elaborazione del linguaggio naturale. Il modello proposto utilizza più segnali di qualità per fornire una supervisione più robusta, migliorando le prestazioni rispetto ai metodi di scoring basati su dati di prova.</sample>
    <sample id="83">Sì, i modelli codificatore-decodificatore come mt5 possono migliorare con l'addestramento su una combinazione di lingue, ma l'outperforming di un singolo linguaggio, come l'inglese, è raro, tranne in tre set di dati.</sample>
    <sample id="84">Shaohui Xu's research focuses on developing a Partially Dynamic Network (PDN) to address the inefficiencies of Fully Dynamic Networks (FDN) by identifying and reducing redundant dynamic parameters. The study introduces a method to partition network parameters into dynamic and static categories, using scale factors to optimize the training process. Experiments demonstrate that PDNs maintain high performance with fewer parameters and faster computation compared to FDNs and network pruning. Future work includes extending the method to other network architectures and exploring more complex parameter combinations.</sample>
    <sample id="85">Un esempio di pianificazione linguistica vincolata è 'prepara una torta di cioccolato'.</sample>
    <sample id="86">Gli autori utilizzano esperimenti con metriche come delta cosine, delta L2 e KS test per verificare che le inserzioni contengono il watermark e che sia difficile per gli attori rimuoverlo, mantenendo al contempo l'utilità delle inserzioni.</sample>
    <sample id="87">Il lavoro utilizza i PLM esistenti come base per costruire un nuovo modello, Dr. Bert, adattando il modello pre-allenato Roberta per il francese, dimostrando che i PLM possono essere adattati per nuovi domini linguistici con dati di addestramento specifici.</sample>
    <sample id="88">GPT-4 è meno allineato con i paesi non inglesi.</sample>
    <sample id="89">La relatrice menziona: 'Il modello predice la traduzione in tedesco e si osservano i pesi di cross-attention, che indicano se i primi due parole puntano ai primi frame audio e il terzo alla gamma Lambda, determinando l'emissione.'</sample>
    <sample id="90">Hannah Lu e i suoi colleghi hanno esplorato l'idea che i linguisti potrebbero essere efficaci annotatori per i dati di linguaggio. In un esperimento, hanno confrontato i risultati di annotazione dei linguisti con quelli dei nativi parlanti, usando vari metodi di supporto. I risultati hanno dimostrato che i linguisti possono annotare accuratamente i dati, specialmente per compiti più semplici, e che i loro risultati di annotazione possono essere paragonati o addirittura superare quelli dei nativi parlanti. Inoltre, i linguisti hanno mostrato un miglioramento nel loro livello di competenza linguistica attraverso il processo di annotazione. Questo studio suggerisce un nuovo approccio per costruire dataset di linguaggio, potenzialmente superando le barriere geografiche e tecnologiche.</sample>
    <sample id="91">Più attività sono disponibili, il modello migliora la sua performance, risultando in una migliore precisione e una minore sensibilità.</sample>
    <sample id="92">1. Modelli senza alberi. 2. Approcci basati su alberi. 3. Approcci basati su grafici.</sample>
    <sample id="93">Il primo autore è Matthias Landmann, e i due coautori sono Alexander Koller e Ivan Tiedtov.</sample>
    <sample id="94">The paper by Jingwei Yi from the University of Science and Technology of China addresses the issue of copyright infringement in embedding ad services using large language models. It introduces Embedding Marker, a watermarking method that injects a watermark into embeddings and verifies its presence in other services. The method involves selecting trigger words, creating target embeddings, and comparing them with embeddings from potential infringers. Experiments on datasets like AG News and MIND show that Embedding Marker effectively protects copyright while maintaining service utility.</sample>
    <sample id="95">Il primo autore di PaLM è Abhinav Mitra.</sample>
    <sample id="96">Ciao a tutti. Sono Jenny, uno studente di primo anno alla Carnegie Mellon University. E oggi presenterò il nostro lavoro, analizzando la posizione di design, i pregiudizi dei set di dati e dei modelli. Questo lavoro è stato fatto in collaborazione con alcuni colleghi della University of Washington e dell'Allen Institute for AI, a nome di Sebastian Santi, Rohan Labras, Katerina Raneeka e Martin Sapp. Quindi iniziamo immaginando di lavorare per un giornale e cercando di rimuovere contenuti tossici sotto un articolo. Potresti rivolgerti a un popolare API, come l'API di Perspective per la rilevazione del contenuto tossico. E questo funziona davvero bene se sei Carl Jones. Dovremmo usare l'API di Perspective per rilevare correttamente le istanze tossiche. Ma questo non è il caso di Aditya Sharma, dove l'API di Perspective non è davvero sensibile alle parole offensive più comuni nei contesti indiani. Questo è un esempio di un pregiudizio di progettazione, dove vediamo differenze sistematiche di prestazioni della tecnologia tra popolazioni. I pregiudizi di progettazione come quello che abbiamo appena visto prima potrebbero verificarsi a causa della posizione dei ricercatori e dei sviluppatori di modelli. La posizione è semplicemente la prospettiva che le persone hanno a causa della loro demografia, identità e esperienze di vita. Questo è un concetto ampiamente usato negli spazi accademici femministi e queer. E come ricercatrice, la posizione può influenzare il processo di ricerca e i suoi risultati, perché può cambiare le decisioni che i ricercatori prendono. E quindi una domanda che la gente potrebbe chiedere è, i set di dati e i modelli hanno posizione? E stiamo cercando di confrontare gli utenti finali con i set di dati e i modelli stessi. Ma non stiamo dicendo che i modelli e i set di dati stessi hanno identità demografiche e esperienze di vita, ma aggregano giudizi e opinioni di persone reali e possono rappresentare certe posizioni rispetto ad altre. Quindi, il lavoro precedente ha suggerito alcuni esempi anecdotali di avere posizione, come i divari culturali nei set di dati e nei modelli, oltre a definizioni di posizione dei modelli. Tuttavia, questi lavori non guardano a confrontare gli utenti finali con i set di dati e i modelli stessi. E studiare la posizione dei set di dati e dei modelli è sempre più importante man mano che le attività di NLP diventano più soggettive e socialmente orientate. Ed è difficile caratterizzare come queste posizioni sono distorte, perché non tutte le decisioni sono documentate, e molti modelli sono nascosti dietro API. Quindi, per studiare la posizione dei set di dati e dei modelli, confrontiamo le annotazioni con gli utenti reali con i set di dati e i modelli esistenti. E questo attraverso il nostro framework, nlpositionality. Il nostro framework funziona in due passaggi principali. Il primo passo è reannotare i set di dati con annotatori diversi. E scegliamo di farlo piuttosto che guardare alla demografia degli annotatori originali dei set di dati, perché di solito solo pochi annotatori annotano ogni istanza, e perché le demografie non vengono raramente raccolte e condivise. E quindi scegliamo di reannotare i dati per ottenere molti annotatori per istanza e per ottenere un insieme di dati demografici. Quindi prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i set di dati usando un punteggio di correlazione di Pearson. E quindi il nostro framework in realtà differisce dalla letteratura di disaccordo degli annotatori, ma guardando solo all'accordo degli annotatori o alla distribuzione degli annotatori, invece di confrontare le annotazioni finali con i modelli e i set di dati, predizioni e etichette. Il nostro framework è in gran parte abilitato attraverso Lab in the Wild, un platform di sperimentazione online, ex collaboratore di HCI. E Lab in the Wild è un platform di sperimentazione online dove possiamo reclutare volontari diversificati, in contrasto con i piattaforme come M turk, che hanno principalmente partecipanti dagli Stati Uniti o dall'India. E Lab in the Wild è ancora in grado di ottenere dati di alta qualità. E ospitiamo due compiti su Lab in the Wild, uno dei quali è l'accettabilità sociale. E il modo in cui funziona è che i partecipanti leggono una situazione dal set di social chemistry, e poi scrivono quanto è accettabile una situazione. E poi, per rimanere coinvolti nello studio, possono confrontare le loro risposte con un AI e gli altri. Quindi confrontiamo queste annotazioni con il set di social chemistry, delphi e gpt four. E poi repliciamo un setup molto simile per il compito di rilevamento della tossicità e dell'hate speech, dove leggeranno un'istanza da dynahate e scriveranno se pensano che sia un'istanza di hate speech. E poi confrontiamo queste annotazioni con dynahate, perspective api, rewire a in gpt four. E quindi il nostro studio alla fine è in grado di rispondere, con chi i set di dati e i modelli finali si allineano di più? Troviamo che ci sia posizione in NLP. Ad esempio, troviamo che i set di dati e i modelli sono più allineati con i paesi di lingua inglese. Quindi per il compito di accettabilità sociale di gpt four, troviamo che è più allineato con i paesi di lingua inglese. E troviamo che dynahate è anche più allineato con i paesi di lingua inglese. E troviamo che più allineamento con le persone che hanno un'istruzione universitaria. Quindi per il compito di accettabilità sociale di gpt four, troviamo che è più allineato con le persone che hanno un'istruzione universitaria o un'istruzione di laurea. E troviamo lo stesso per dynahate, dove è più allineato con le persone che hanno un'istruzione universitaria. Tuttavia, quando i modelli e i set di dati sono allineati con popolazioni specifiche, alcuni sono inevitabilmente lasciati indietro. E esempio, i set di dati e i modelli sono meno allineati con le persone non binarie rispetto ai loro omologhi maschili e femminili. E troviamo questo nel compito di accettabilità sociale di gpt four, così come nel compito di rilevamento della tossicità e dell'hate speech. Quindi, dato che c'è posizione in NLP, cosa possiamo fare? Quindi abbiamo alcuni suggerimenti per questo. Il primo è tenere un registro di tutte le scelte di progettazione rilevanti durante il processo di ricerca. E il secondo è fare ricerca NLP con una prospettiva di prospettivismo. E il terzo suggerimento è costruire set di dati e modelli specializzati all'interno di quattro comunità specifiche. E un buon esempio di questo è l'iniziativa muscogone. Voglio sottolineare che l'inclusività NLP non è solo fare, sai, tutti i tecnologie funzionano per tutti. E quindi questo conclude la nostra presentazione. Ma se volete saperne di più, sentitevi liberi di controllare il nostro dashboard per i risultati di analisi più aggiornati e il nostro documento. Grazie.</sample>
    <sample id="97">La relatrice menziona tre problemi associati a SimulST: complessità dei modelli, procedure di addestramento lunghe e la necessità di diversi modelli per diversi tempi di latenza.</sample>
    <sample id="98">Un modo efficace per mitigare i bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP è utilizzare prompt format diversificati basati su questionari politici per valutare e contrastare le inclinazioni politiche. Inoltre, eseguire pre-allenamenti mirati su corpora di partigiani con orientamenti politici diversi può aiutare a neutralizzare i pregiudizi.</sample>
    <sample id="99">L'attenzione di questo documento è rivolta a coloro che sono interessati all'analisi e all'implementazione di sistemi di pianificazione linguistica.</sample>
    <sample id="100">Questo discorso introduce Prompt-Rank, un approccio innovativo per il Multi-Hop Question Answering (QA) che combina un metodo di recupero non supervisionato con un reranking basato su modelli linguistici. Invece di richiedere migliaia di esempi, Prompt-Rank funziona efficacemente con solo 128 esempi, rendendolo adatto a domini con risorse limitate. Il metodo recupera candidate di chain utilizzando TF-IDF e traversa dei collegamenti, e poi valuta questi candidati utilizzando un modello linguistico. I risultati dimostrano che Prompt-Rank supera i sistemi di recupero supervisionati e si comporta bene con i sistemi di recupero multi-hop esistenti. Inoltre, l'analisi mostra l'importanza di ogni componente del processo e la performance del QA downstream quando Prompt-Rank viene utilizzato come recuperatore.</sample>
    <sample id="101">La fluidità di PaLM è comparabile a quella dei sistemi di traduzione di punta, ma presenta problemi di accuratezza, in particolare con errori di omissione.</sample>
    <sample id="102">Un metodo di filigrana deve essere applicabile agli embedding ad service, non degradare la loro utilità, essere facilmente copiare o rimuovere dall'attacker, e trasferibile agli embedding dell'attacker durante la modellazione.</sample>
    <sample id="103">I discorsi TED sono stati tradotti in 14 diverse lingue.</sample>
    <sample id="104">Un set di dati viene riannotato per ottenere molte istanze per ogni istanza.</sample>
    <sample id="105">La differenza tra set di dati benigni e backdoor è misurata utilizzando la disuguaglianza di cosine e la disuguaglianza L2.</sample>
    <sample id="106">The paper introduces the Quest dataset, designed to evaluate systems handling complex information retrieval tasks with implicit set constraints. It features over 3,000 entity-seeking queries derived from Wikipedia, focusing on set operations across various domains. The dataset aims to test systems' ability to retrieve multi-answer sets from large corpora, with relevance evidence scattered across documents. Initial evaluations show significant room for improvement, especially in queries involving set intersections and differences, which have the lowest F1 scores. The study highlights the challenges in current systems and aims to guide future research in enhancing information retrieval for complex queries.</sample>
    <sample id="107">I modelli basati su codificatori multilingue, come encoder-decoder, sono stati utilizzati per migliorare le prestazioni in cross-lingual semantic parsing. Sono stati addestrati in un mix di lingue, con risultati che mostrano miglioramenti significativi in molte lingue, tranne l'inglese, che ha mostrato una performance variabile.</sample>
    <sample id="108">KostV Sinha discute il suo lavoro su come i giudizi di accettabilità dei modelli linguistici non sono sempre robusti in contesto. Il lavoro, che coinvolge John Wautier e altri, esamina il Minimal Pair Paradigm (MPP) per valutare i modelli linguistici. Attualmente, i modelli valutano l'accettabilità su singole frasi, ma con l'aumento dei modelli di grandi dimensioni, è necessario valutare l'accettabilità su frasi più lunghe. Il team ha simulato frasi più lunghe usando set di dati come BLM e ha osservato che i modelli sono sensibili ai prefissi di strutture grammaticali simili. I modelli mostrano un aumento o una diminuzione significativo della valutazione MPP quando i prefissi sono accettabili o inaccettabili. I modelli sono sensibili ai cambiamenti strutturali simili, indipendentemente dal dominio. I risultati suggeriscono che i modelli sono sensibili a caratteristiche latenti e potrebbero non catturare pienamente la comprensione del modello attraverso la finestra di contesto.</sample>
    <sample id="109">In questo discorso, Orli Gulewicz presenta Unnatural Instructions, un dataset di istruzioni generato automaticamente senza annotazioni umane. Questo dataset, composto da 64K esempi, è stato creato usando prompt di un modello GPT-3 per generare istruzioni e input. È diversificato con 240K esempi di parolacce. L'analisi mostra che oltre il 50% delle istruzioni generate sono corrette, con anche gli esempi errati fornendo informazioni preziose. Il dataset ha dimostrato miglioramenti significativi nelle prestazioni dei modelli linguistici, superando benchmark come T0 e TKB-Hard, anche quando il costo di generazione è considerato.</sample>
    <sample id="111">Gli autori decidono quali parole sono a frequenza moderata selezionando un gruppo di parole che appaiono con un intervallo di frequenza moderato nel corpus di testo generale.</sample>
    <sample id="112">Il contenuto inglese è già in italiano.</sample>
    <sample id="114">Il team della Nanyang Technological University di Singapore ha sviluppato un metodo per ridurre le dimensioni dei modelli di multi-head attention, un componente critico dei grandi modelli di linguaggio. Questi modelli, sebbene rivoluzionari, presentano problemi di dimensioni dei parametri e di consumo di risorse. Il team ha proposto un approccio basato su gruppi per ridurre le dimensioni dei parametri mantenendo le prestazioni. Il loro metodo, chiamato Group Head Attention, utilizza una strategia di gruppo di restrizione e un algoritmo di voto per mantenere solo un'attenzione per ogni gruppo, riducendo significativamente le dimensioni dei parametri senza compromettere le prestazioni. I loro modelli hanno dimostrato miglioramenti significativi in compiti come la traduzione automatica, la modellazione linguistica e la sintesi astratta.</sample>
    <sample id="115">L'approccio utilizza segmenti di parlato di 6 secondi.</sample>
    <sample id="116">Le conoscenze specifiche dell'entità necessarie includono che Servin è un giudice e Kea è un panettiere.</sample>
    <sample id="117">La qualità dell'esempio è più importante della somiglianza con la frase sorgente.</sample>
    <sample id="118">Il team ha presentato un lavoro che mira a migliorare le tecniche di pre-addestramento per il codice-switching nell'analisi del linguaggio naturale (NLP). Il codice-switching, caratterizzato dalla combinazione di lingue, è prevalente in comunità linguisticamente diverse. I metodi di pre-addestramento tradizionali, come BERT e XLNet, non performano bene su compiti di codice-switching. Per risolvere questo, hanno proposto Switch-MLM, che utilizza switch points per identificare le transizioni di lingua. Hanno anche introdotto Frequency MLM, che utilizza log-likelihood negative per assegnare le etichette di lingua. Le modifiche architettoniche includono la creazione di collegamenti residuali e l'implementazione di un'auxiliary loss per migliorare l'informazione sui switch points. I risultati dimostrano che queste tecniche migliorano significativamente le prestazioni in compiti come l'analisi del sentimento.</sample>
    <sample id="119">L'articolo si concentra su modelli linguistici come GPT-4, GPT-3 e vari modelli BERT, esaminando come i loro orientamenti politici influenzano le loro prestazioni in compiti come la rilevazione del discorso d'odio e la rilevazione della disinformazione.</sample>
    <sample id="120">Il modello utilizza i punteggi di attenzione di un livello specifico, emettendo parole solo quando la somma dei punteggi di attenzione è al di sopra di un certo soglio.</sample>
    <sample id="121">Gli esempi di inferenza diretta includono: 'Easy on me', 'I got a feeling', 'The newer one', 'The song that's not energetic'.</sample>
    <sample id="122">L'articolo non menziona specificamente le affiliazioni degli autori.</sample>
    <sample id="123">Ying Yin e Ji Yang presentano la loro ricerca su Multi-Teach, che mira a migliorare l'apprendimento dei modelli multi-modali attraverso l'ottimizzazione delle istruzioni. Con l'avanzamento dei modelli linguistici di grandi dimensioni, si è esplorato il riutilizzo di modelli linguistici pre-addestrati per compiti downstream in modo efficiente. Sebbene l'ottimizzazione delle istruzioni abbia migliorato le prestazioni dei modelli su compiti linguistici, compresi quelli visivi e multimediali, la mancanza di set di istruzioni per compiti multimodali ha portato a una carenza di dati. Per colmare questa lacuna, hanno creato Multi-Teach, il primo set di istruzioni multi-modali, che include 62 compiti di diverse categorie. Utilizzando OFA come modello base, hanno condotto esperimenti che dimostrano che l'ottimizzazione delle istruzioni può migliorare significativamente le prestazioni dei modelli su compiti multimodali. Inoltre, hanno introdotto una nuova metrica, la sensibilità, per valutare la coerenza delle risposte del modello. Il loro lavoro sottolinea l'importanza della sensibilità e della sensibilità nel miglioramento delle prestazioni dei modelli.</sample>
    <sample id="124">Tan Chi Yu from the National University of Singapore presents a study on improving the temporal reasoning capabilities of Large Language Models (LLMs). The research breaks temporal reasoning into three levels: time-to-time, time-to-event, and event-to-event reasoning. The study introduces the TempReason dataset, which covers all three levels and long temporal coverage, and proposes a training strategy with temporal span extraction pre-training and time-sensitive reinforcement learning. Experiments show that the TempT5 model outperforms others in open-book and reasoned question-answer settings. The study highlights the temporal reasoning biases of LLMs and suggests future work to address these biases.</sample>
    <sample id="125">Due autori sono coinvolti nell'articolo: Janice Lawick e Yannis Laureau.</sample>
    <sample id="126">Sì, l'uso di Google Translate API per tradurre le query in lingua target prima del parsing semantico è stato considerato come un approccio standard.</sample>
    <sample id="127">L'abstract del lavoro di Namjoohol, Laura Schmidt e Seongyeun, "Large Language Models Are Reasoning Teachers," introduce un metodo innovativo per trasferire le capacità di ragionamento da modelli di grandi dimensioni a modelli più piccoli. Il team utilizza la tecnica di chain of thought prompting, applicata su modelli di grandi dimensioni, per generare risposte passo-passo per compiti complessi. Queste risposte vengono poi utilizzate come dati di addestramento per modelli più piccoli, che vengono addestrati a rispondere a domande con risposte passo-passo. Il team introduce anche un nuovo approccio chiamato diverse reasoning, che genera diverse risposte per migliorare l'addestramento. I risultati dimostrano che il metodo supera le prestazioni dei modelli di base, specialmente in compiti basati su testo, e è altamente scalabile. Il documento fornisce anche codice e dati per ulteriori studi.</sample>
    <sample id="128">L'abstract discute del lavoro di Akshata e Martin su 'The Kitmos', che esplora la capacità dei modelli di Natural Language Understanding di integrare e utilizzare conoscenze provenienti da diverse fonti. Introducono un test diagnostico per valutare questa integrazione, introducendo un compito di co-referenzo. Il lavoro esamina tre impostazioni: pre-trained, entrambe le fonti, e inferenza, con risultati che mostrano che, senza addestramento specifico, i modelli non riescono a integrare efficacemente le fonti di conoscenze. Tuttavia, con addestramento specifico, alcuni modelli riescono a integrare le fonti, anche se le difficoltà persistono con le fonti di conoscenze fornite solo all'inferenza.</sample>
    <sample id="129">Gli autori hanno fornito esempi di generazione di personaggi come 'Asian woman' e 'Middle Eastern woman' come gruppi contrassegnati, evidenziando stereotipi come 'unassuming' e 'exotic', rispettivamente.</sample>
    <sample id="130">I modelli Transformer non generalizzano in modo adeguato.</sample>
    <sample id="131">I set di dati di test sono i set di dati di validazione.</sample>
    <sample id="132">Due autori sono coinvolti nell'articolo: Akshata e Martin.</sample>
    <sample id="133">L'autore opera con più modalità, utilizzando sia testo che immagini.</sample>
    <sample id="135">James and Sarah Finch present ABC Eval, a method for evaluating conversational AI by annotating specific behaviors, such as relevance and contradictions. Their study compared ABC Eval with traditional methods, finding it more reliable and predictive of overall quality. The method identifies common errors in AI models, like irrelevant information and contradictions, and is more effective in capturing unique aspects of chat quality.</sample>
    <sample id="136">Il lavoro di Javed An presenta FERMAT, un nuovo set di valutazioni per numeri che mira a migliorare la valutazione delle capacità di numeri dei modelli di linguaggio. I modelli di linguaggio attuali, come quelli di BERT e RoBERTa, mostrano scarsa performance in compiti di numeri, spesso a causa della loro dipendenza da un solo set di dati. FERMAT introduce una valutazione più completa, considerando vari tipi di numeri e operazioni matematiche. I risultati dimostrano che l'inclusione di più tipi di dati e la diversità linguistica migliorano significativamente le prestazioni. Questo approccio offre una valutazione più accurata delle capacità matematiche dei modelli, rendendolo un strumento utile per migliorare la precisione dei compiti di numeri.</sample>
    <sample id="137">Si-Sung, di Singapore University of Technology and Design, discute il suo lavoro su Teltodev, un dataset per la generazione guidata dal linguaggio delle piante da terra. Il lavoro mira a migliorare la generazione di design attraverso l'uso di modelli di linguaggio condizionato, concentrandosi sul campo specifico delle piante da terra. Il dataset Teltodev include piante da terra con istruzioni linguistiche naturali, con 5.051 annotazioni da Amazon Mechanical Turk e 76.000 generati artificialmente. Il modello di sequenza-to-sequenza utilizzato supera i modelli di generazione di immagini condizionali, ottenendo un micro-IOU di 54 e un macro-IOU di 53, in parte grazie al trattamento del modello come un problema di sequenza-to-sequenza. Il lavoro sottolinea l'importanza di un approccio di design guidato dal linguaggio per migliorare l'esperienza del design, rendendo il processo più accessibile a chi non ha competenze specializzate.</sample>
    <sample id="138">Gli autori sottolineano che la capacità di integrare e utilizzare efficacemente sia la conoscenza pre-allenata che quella inferenziale in tempo reale è poco studiata.</sample>
    <sample id="139">I relatori sono Yin e Ji Yang.</sample>
    <sample id="140">Sì, Coscript è stato sottoposto a controlli di qualità da parte di lavoratori in sovrapposizioni per garantire la precisione dei dati.</sample>
    <sample id="141">Le risorse esistenti per la traduzione dipendente dal contesto sono limitate in termini di supporto per tipi di traduzioni dipendenti dal contesto e set di lingue, poiché spesso si basano su conoscenza del dominio e sulla creazione umana.</sample>
    <sample id="142">Ehi. Parlerò del nostro lavoro sul risolvenza di espressioni di riferimento indiretto per la selezione di entità. Introduciamo il corpus di entità alternative. E il mio nome è Javad Hosseini, e questo è un lavoro congiunto con Philip Radlinski, Silvia Parati e Aniket Lewis. Il nostro obiettivo è capire il linguaggio degli utenti quando vogliono fare una scelta. Considera questa domanda alternativa. Vuoi dire facile su di me, o ho un'idea? Qui, un utente vuole selezionare tra uno di questi due. La cosa più ovvia è usare un riferimento diretto. Ad esempio, dicendo il nome della canzone, facile su me, o la sua posizione, la prima. Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non ricorda il nome della canzone, o le pronunce sono troppo simili tra loro e difficili da disambiguare, o quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di riferimenti indiretti, per esempio, la più recente, o la canzone che non è energetica. Questo è un problema importante nei sistemi di conversazione, e anche per l'entità di Bing. Non siamo a conoscenza di un set di dati pubblico, un set di dati su larga scala per il compito. Quindi lo raccogliamo usando la condivisione di annotazioni. Il nostro set di dati copre tre diversi domini: musica, libri e ricette. La nostra metodologia di raccolta dei dati enfatizza l'informità usando un set di completazione di cartone. Il cartone ha tre bolletta di discorso. Nella prima bolletta, Bob dice, ricorda quella canzone che stavamo ascoltando ieri. E con quello, Bob imposta il contesto. Nella seconda bolletta, Alice dice, intendi facile su me, o ho un'idea? Che è la domanda alternativa. E nella terza bolletta, Bob usa un riferimento indiretto per selezionare una di queste entità. Ad esempio, la più recente. Forniamo le prime e le seconde bolletta automaticamente, ma la terza viene riempita dall'annotatore. La prima bolletta è scelta da alcuni prompt manuali per dominio. La seconda, che è la domanda alternativa, è generata come segue. Usiamo sempre un semplice modello. A e B sono campioni da Wikipedia. Ecco le diverse metodologie di campionamento che abbiamo usato. Quando ci spostiamo più in alto nella lista, le entità diventano più simili l'una all'altra, e di solito è più difficile fare la disambiguazione. La prima è uniforme a caso. La seconda è quando le entità hanno titoli simili, ad esempio, due libri con il nome, il ritorno. La terza è quando hanno descrizioni simili su Wikipedia. E infine, quando hanno simili caselle di informazioni o attributi su Wikipedia, ad esempio, lo stesso genere, o lo stesso artista per una canzone. Mostriamo queste domande alternative agli annotatori. Conoscono il nome di queste entità, ma non necessariamente conoscono le entità. Quindi quello che facciamo è mostrare un po 'conoscenza di fondo su queste due entità. Per le canzoni, semplicemente mostriamo un link di ricerca di Google per ogni canzone. E poi chiediamo agli annotatori di ascoltare almeno una di ciascuna canzone e leggere. Ecco il risultato di ricerca di Google per la canzone facile su me. Per i ricettori e i libri, mostriamo un po 'testo da Wikipedia. Per i ricettori, aggiungiamo anche le immagini, di nuovo, da Wikipedia, in modo che gli annotatori sappiano come sono. Poi chiediamo agli annotatori di scegliere una di queste entità, per esempio, la prima, e descriverle usando tre o cinque espressioni di riferimento indiretto. Ecco alcuni esempi dal nostro set di dati. Per esempio, senza parole, non il bambino di dodici anni, o il fittizio. E così via. Il corpus di entità alternative ha seimila entità alternative, in tre domini. E ha quarantamila espressioni di riferimento indiretto. I risultati con il modello T five large sono riassunti qui. Se il modello di linguaggio ha accesso allo stesso background di conoscenza come gli annotatori, l'accuratezza è davvero alta. È intorno al novanta due o novanta cinque percento. Ma questo non è realistico. Se il modello di linguaggio ha accesso a qualche conoscenza di fondo parzialmente sovrapposta, allora l'accuratezza è tra l'ottanta due e ottanta sette percento, che è più realistico. Ad esempio, quando il modello recupera la conoscenza di fondo. Se il modello di linguaggio ha accesso solo ai nomi delle entità, allora l'accuratezza è solo il sessanta percento. Quindi c'è molto spazio per miglioramento. Abbiamo anche mostrato che i modelli sono generalizzabili. Ecco un link al nostro set di dati. Grazie Traduci il contenuto inglese in italiano.</sample>
    <sample id="143">SimulST viene confrontato con strategie come l'uso di modelli offline senza adattamento specifico per la simulazione, l'uso di un modello per ogni latenza regime e tecniche come l'accordo locale e l'architettura Wide-K.</sample>
    <sample id="144">L'articolo è stato scritto da Yannis Lavraque, Olivier Gervais, and Olivier Gervais.</sample>
    <sample id="145">Il relatore è Jenny, una prima anno Ph.D. studente alla Carnegie Mellon University.</sample>
    <sample id="146">L'articolo di Zhou Yichen discute l'analisi dell'omissione nella dialog summarization, un sottoprodotto della text summarization. Sebbene i modelli di linguaggio pre-allenati abbiano raggiunto progressi significativi, i loro risultati sono ancora limitati da errori come l'omissione, che porta a risposte incompleti. L'articolo esamina i tassi di omissione in diversi domini e modelli, trovando un tasso elevato di errore. Per affrontare questo problema, Zhou introduce un set di dati per l'osservazione dell'omissione e un metodo di post-editing basato sull'omissione. I risultati dimostrano che l'osservazione dell'omissione e la sua incorporazione nel processo di post-editing possono migliorare la qualità delle risposte generate.</sample>
    <sample id="147">Tre.</sample>
    <sample id="148">L'attenzione come guida per la traduzione simultanea.</sample>
    <sample id="149">Sì, il set di dati è disponibile pubblicamente.</sample>
    <sample id="150">Archie e il suo team hanno presentato Meeting QA, un dataset di question-and-answer extraivo da transcrizioni di riunioni. Questo dataset si distingue per le sue domande aperte e discusse, che spesso coinvolgono più partecipanti. Il processo di raccolta include la selezione delle domande, l'annotazione delle risposte e l'uso di annotatori per garantire l'accuratezza. Il dataset contiene 7.700 domande, con una distribuzione di 30% di domande non risolte, 40% di risposte multiple e 48% di risposte multiple. I metodi di modellazione includono varianti single-span e multi-span, con risultati che mostrano difficoltà per i modelli nel riconoscere domande retoriche e identificare correttamente i partecipanti. Il dataset è considerato impegnativo per i modelli di question-and-answer esistenti.</sample>
    <sample id="151">Ciao a tutti. Mi chiamo Yin e il mio collega Ji Yang. Presenteremo la nostra ricerca su multi instruct, migliorare l'apprendimento multi-modello tramite regolazione delle istruzioni. Con i progressi nei modelli linguistici di grandi dimensioni, molti lavori hanno esplorato nuovi paradigmi di apprendimento per riutilizzare i modelli linguistici per compiti downstream in modo parametrico e efficiente. Recentemente, molti studi hanno dimostrato che l'istruzione delle istruzioni consente ai modelli linguistici di eseguire compiti non visti in modo serale seguendo istruzioni naturali. Tuttavia, la maggior parte dei precedenti lavori sull'istruzione delle istruzioni si sono concentrati sul miglioramento delle prestazioni serali sui compiti linguistici, mentre i compiti visivi e multimediali sono stati lasciati fuori. Pertanto, in questo lavoro, vogliamo indagare se l'istruzione delle istruzioni sui modelli multimediali può effettivamente migliorare la generalizzazione su compiti non visti. Inoltre, all'epoca della nostra ricerca, abbiamo scoperto una discrepanza considerevole nella disponibilità di set di istruzioni tra nlp e multimediale. Esistono più di mille seicento compiti di istruzione linguistica. Tuttavia, non esiste un set di istruzioni multimediali di grande scala disponibile pubblicamente. Pertanto, ci spinti a costruire un set di istruzioni multimediali per l'istruzione. Qui presentiamo multi instruct, il primo set di istruzioni per l'istruzione multimediale. Consiste in sessantadue compiti diversi che coprono dieci categorie di schede. Questi compiti sono derivati da ventuno set di dati open source. E ognuno dei compiti è dotato di cinque istruzioni scritte da esperti. Per indagare l'istruzione delle istruzioni multimediali sui nostri set di dati, prendiamo OFA, un modello multimediale pre-addestrato come modello di base. OFA utilizza un vocabolario unificato per lingua, token di immagine e coordinate di una scatola di confine. Qui mostriamo alcuni esempi di compiti multi-instruct. Per unificare il processo di elaborazione di vari input e output, seguiamo il metodo di OFA e formuliamo tutti i compiti in un formato sequenza- sequenza in cui il testo di input, le immagini, le istruzioni e le scatole di confine sono rappresentate nello stesso spazio dei token. Ora, parliamo di istruzione multi-modale. Per il set di dati di addestramento, usiamo cinquantatré compiti per il gruppo di addestramento e campioniamo diecimila istanze per compito. Per il set di test, riserviamo l'intero gruppo di ragionamento comune per il test. E selezioniamo altri cinque compiti dal gruppo Vqa e del gruppo malizioso. Usiamo tutti gli esempi nel set di test per ogni compito. Inoltre, campioniamo venti compiti dal set di test di istruzione naturale come compiti non visti per nlp. Quindi usiamo il modello pre-addestrato OFA-large come modello di base. Durante l'addestramento, mescoliamo tutti gli esempi per tutti i compiti. Ogni istanza viene combinata casualmente con una delle sue cinque istruzioni. Durante il test, per ogni compito, conduciamo un totale di cinque esperimenti valutando il modello usando una delle cinque istruzioni in ciascuno. Regoliamo l'accuratezza per compiti di classificazione multimediali. Se è un compito di generazione multimediale, segniamo il rule-l. Per i compiti nlp, segniamo anche il rule-l. Introduciamo anche un'altra metrica di valutazione chiamata sensibilità. Quindi, questo è il nostro risultato principale. Come possiamo vedere, l'istruzione delle istruzioni può migliorare significativamente le prestazioni di OFA su compiti non visti. Inoltre, l'istruzione delle istruzioni transfer learning può beneficiare l'istruzione delle istruzioni. Ecco come possiamo vedere, man mano che il numero di compiti aumenta, il modello ottiene prestazioni migliori e allo stesso tempo, la sensibilità diminuisce. Quindi, abbiamo fatto un esperimento. Usiamo una istruzione rispetto a cinque istruzioni. Quindi, l'uso di più istruzioni può migliorare le prestazioni complessive del modello e ridurre la sua sensibilità. Quindi, mostra il effetto di diverse strategie di regolazione delle istruzioni sulla sensibilità del modello. Quindi, l'istruzione delle istruzioni transfer learning può aiutare OFA a ottenere prestazioni molto migliori sul set di istruzioni multi-instruct. Quindi, proponiamo un primo set di istruzioni per l'istruzione multimediale di grande scala. Stiamo raccogliendo un set di istruzioni multimediali con circa cinquanta compiti di linguaggio visivo aggiuntivi. E lo rilasceremo presto. Questo è un codice QR per i nostri dati e il modello. Grazie.</sample>
    <sample id="152">Frederic Griebenschneider discute il suo lavoro sull'intersezione tra NLP e filologia classica, introducendo modelli di linguaggio per la filologia classica. Il lavoro mira a rendere i modelli di linguaggio più robusti e a esplorare architetture multilingue. Griebenschneider introduce Greberta e GRETTER, modelli monolingue per l'antico greco, e Filberta e Filther, modelli multilingue per l'antico greco, latino e inglese. I modelli sono stati valutati usando Universal Dependency Streambank e EVALATIN 2022, superando i modelli precedenti. I risultati mostrano che i modelli multilingue non superano i modelli monolingue in termini di conoscenza semantica e conoscenza del mondo, ma introducono un nuovo corpus di pre-allenamento per l'antico greco.</sample>
    <sample id="153">Nina Rehmayr Bahri, a scientist at Amazon Alexa, discusses resolving ambiguities in text-to-image models. Ambiguities in prompts can lead to inaccurate image generation. Her team developed a framework to disambiguate prompts using clarifying questions or multiple visual setups. They evaluated the effectiveness of these disambiguations and proposed an automatic evaluation framework to assess image faithfulness to user intent. Their findings show that disambiguation improves image generation, and the evaluation framework aligns with human judgment.</sample>
    <sample id="154">L'articolo è scritto da Sara Papi, Matteo Negri e Marco Turilli.</sample>
    <sample id="155">Il nome della relatrice è Javad Hosseini.</sample>
    <sample id="157">L'abstract discute il lavoro di dialogizzazione con la struttura fissione grafica statica, un progetto collaborativo di Sun Dong University. L'obiettivo è sintetizzare informazioni silenziose da un dialogo in un riassunto conciso. Il metodo attuale, basato su strutture statiche precompute, presenta due problemi: dipendenza dalla precisione degli strumenti linguistici e rigidità delle strutture statiche. Il modello proposto introduce un modello di grafico dinamico per catturare le relazioni semantiche tra le udienze. Utilizzando un modello multi-head attention, il modello integra le strutture statiche e dinamiche in un grafico unificato. Il codice e i dati sono disponibili su GitHub.</sample>
    <sample id="158">Xiangkunhu Hu presents Dual Cache, a novel approach to neural coreference resolution in long documents, addressing the limitations of single cache methods. Dual Cache employs a local cache for local entities and a global cache for global entities, reducing cache misses and improving performance. Benchmarks show Dual Cache outperforms single cache methods, especially with training data, and is cost-effective. The method is evaluated on public datasets, demonstrating its efficiency in handling complex document structures.</sample>
    <sample id="159">L'ACL twenty twenty three paper, Language model acceptability judgments are not always robust to context. This is a joint work with John Roth, Aaron Mueller, Kanishka Mishra, Karen Fentress, Roger Levy, and Atina Williams. In this work, we revisit the minimal pair paradigm. The minimal pair paradigm basically evaluates language models on top of acceptability judgments, which can also include grammaticality like blimp syntax, or acceptability in terms of stereotypes such as Kraus pairs. In this minimal pair paradigm, the typical way to evaluate language models is that you show an acceptable sentence or a grammatical sentence, and then you show an unacceptable sentence or an ungrammatical sentence. And then the hope is that the model basically puts more probability to the acceptable sentence. The current NPP pipeline basically doesn't allow us to evaluate models acceptance towards longer sentences. These days, large language models are coming up with longer and longer context windows. So, it's crucial that we evaluate the models acceptability throughout the context window. That is what we are trying to do here. We are trying to revisit the NPP pipeline by asking the model to evaluate acceptability on longer and longer sequences. So, what we do is that to simulate these longer sequences, we revisit the datasets themselves, and then we recreate sentences by choosing acceptable or unacceptable sentences from those datasets. For example, here we have chosen a typical pair of grammaticality from the blimp dataset from the adjunked island case. And what we do is that to recreate longer sequences, which are acceptable and which has the same matching of the grammatical structure, we extract grammatical sentences from adjunked island and then we add as a prefix to both the acceptable query and the unacceptable query. We can do the same thing by choosing unacceptable sentences from the same matching. And that could also be used to test the models acceptability. And we can also do the same thing by choosing sentences from a different subset or a different dataset. So, that is what we call as the mismatch scenario. Here, the sentences are still coming from relevant data sets, but it's not from the same data set that you are evaluating with. And we can do the same thing by choosing sentences from a completely unrelated domain such as Wikipedia. So, this will tell us like whether the models acceptability judgments are actually impacted by any context, like whether the context is coming from a different subset of the data set or whether it's completely irrelevant to the current query. So, how does the model do? So, first we look at the Wikipedia sentences, which are completely irrelevant to the current query pair. And there we find that the NPP judgments are mostly robust for arbitrary context length. We increase the context length to up to one thousand and twenty four to max out op t and GPT two models. We saw here in the orange dotted line, the NPP judgments are relatively stable. Now, what happens when we choose sentences from the same data set? So, here we are choosing or creating sentences from acceptable and unacceptable domains from the same blimp or syntax, Jim. And there we see that the NPP judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes. But when we match the structure, that is when we choose the sentences from the same phenomena in blame person text, Jim, we see a massive increase or a massive decrease of the NPP judgment for the model, depending on whether the chosen prefix is acceptable or unacceptable. Now, this is very large. This effect increases throughout the context length. This would probably affect newer language models which has large context window. So, why does the match prefix affect the language model judgment so much? We did a series of analysis where we tried to perturb the input sentence by trying to preserve the relevant structure, but adding noise to the input. After doing several of these perturbations, we find that none of these noises are actually making the model like change its course in terms of how it shows us the NPP judgment trend. Basically, we find that the models are sensitive to the perturbed sentences in similar ways. That is, when we perturb the sentences in the acceptable domain, we see similar increase in all the perturbations. And when we perturb the sentences in the unacceptable domain, we see decrease in NPP judgments in similar fashion. So, the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences. The NPP evaluation, the way that we do it currently with short and single sentence input may not fully capture the language models abstract knowledge throughout the context window. Please read our paper for more details of our experiments. Thank you for listening.</sample>
    <sample id="160">Il primo passaggio del metodo mappa i token di input con un set multi-set di token che appariranno nella risposta.</sample>
    <sample id="161">Coscript contiene 55.000 script.</sample>
    <sample id="163">Il metodo di allineamento migliore per DEplain è il metodo di MASS align.</sample>
    <sample id="164">L'apprendimento scarsamente supervisionato consente di ridurre i costi di annotazione fornendo etichette basate su conoscenze o regole semplici, anche se queste etichette possono essere imprecise.</sample>
    <sample id="165">Wenting Zhao introduce LIpor, a method for unsupervised learning in adaptive reasoning, which identifies plausible explanations without supervision. LIpor maximizes the marginal likelihood of outcomes by considering mutually exclusive explanations. The method uses a regularizer to enforce mutual exclusivity, improving accuracy over existing models. On the ALFA-NLI dataset, LIpor outperforms zero-shot models and previous approaches, demonstrating its effectiveness.</sample>
    <sample id="166">Yuxin, a student from Harbin Institute of Technology, presents a new method for image retrieval from linguistically complex texts, addressing the limitations of existing visual language models. The method employs a divide and conquer strategy, inspired by dual-process theory, to enhance reasoning capabilities. It introduces a Prolational Generator for symbol proposition representation and a Neural Symbolic Reasoner for integrating reasoning states. The system combines the strengths of analog and logical reasoning, showing superior performance in experiments compared to baseline models.</sample>
    <sample id="167">I documenti in DEplain-web sono stati allineati manualmente per 750 documenti e con metodi automatici per altri 750 documenti.</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato annotando il dataset Reuters News del 2020 con le stesse linee guida di annotazione di CoNLL 2003.</sample>
    <sample id="169">Aydin Bilal presents a study on prompting large language models (LLMs) for machine translation, focusing on the PAMP model. The study evaluates the impact of prompt selection on translation quality, using the WMT evaluation metrics and human evaluations. It finds that example quality is more critical than similarity to the source sentence, with high-quality data from Dev Datasets outperforming noisy training data. The research suggests that while PAMP's fluency is comparable to state-of-the-art systems, it lags in accuracy, often omitting parts of the source sentence. The study recommends a five-shot prompting strategy for better results.</sample>
    <sample id="170">Ehi a tutti. Mi chiamo Yuxin Zhang di Penn State University. Oggi presenterò il nostro lavoro, Cross-lingual Semantic Parsing in Multiple Natural Languages and Representations. Quindi, il parsing semantico è il compito di costruire rappresentazioni semantiche di query come SQL e Lambda Calculus. E il parsing semantico in più lingue è il compito di tradurre query in più lingue in più rappresentazioni. Come mostrato in questa figura, dobbiamo tradurre la query in più lingue usando modelli neurali a SQL, Lambda o Funql. Esistono modelli di parsing semantico in più lingue proposti e valutati su set di dati limitati e applicazioni. Ad esempio, manca la copertura di certi domini naturali, come il calcolo lambda, e la copertura di certi rappresentazioni, come il lambda calculus. E ci sono solo modelli unici per valutare. Quindi, proposto Exemplar, ma forniamo un set di dati uniforme per il parsing semantico in più lingue e rappresentazioni. Contiene novanta set in vari domini, cinque attività di parsing semantico, otto rappresentazioni e ventidue lingue in quindici famiglie di lingue. E per valutare meglio il nostro benchmark, consideriamo sei impostazioni per l'addestramento e la valutazione. La prima è test di traduzione. Usiamo Google Translate API per tradurre la fonte in lingua target, quindi usiamo un modello monolingue per addestrare e valutare. Esempio, addestriamo un modello inglese su query inglesi e durante l'inferenza, traduciamo la query tedesca usando API di traduzione in inglese e poi usiamo il modello addestrato per prevedere SQL. E testamo anche il modello monolingue. In questa impostazione, la lingua di sorgente è la stessa della lingua target, ad esempio, tedesco-tedesco o inglese-inglese. Testiamo anche l'impostazione monolingue few-shot, ma addestriamo modelli monolingue con solo dieci percento dei dati di addestramento. E testiamo l'impostazione multilingue, in cui addestriamo un modello multilingue per tutte le lingue. Ad esempio, mettiamo insieme query tedesche, inglesi e cinesi per addestrare un modello multilingue e durante l'inferenza, possiamo usare questo modello per tradurre query tedesche o cinesi, ecc. E consideriamo trasferimento crosslingual zero e few-shot. Addestriamo su query inglesi o la combinazione di query inglesi e tedesche per addestrare un modello multilingue e prevedere SQL. E troviamo molti risultati interessanti. Quindi, valutiamo modelli monolingue, includendo modelli encoder-pdr, che sta per encoder multilingue pre-allenato con decoder basato su puntatori, come xlmr e bert-pdr. E valutiamo anche modelli encoder-decoder, che sta per encoder e decoder multilingue pre-allenato, come mbert e mt five. Abbiamo scoperto che encoder-decoder ottiene il miglior rendimento su tutti i nove set. E valutiamo mt five e example encoder-pdr in impostazione multilingue. Abbiamo scoperto che encoder-decoder o encoder-pdr può essere migliorato addestrando in una miscela di varie lingue. E abbiamo trovato che la maggior parte delle lingue naturali principali può ottenere prestazioni migliori, tranne che la prestazione inglese in sette set e solo guadagna in tre set. Penso che questo sia noto come maledizione della multilingualità. Confrontiamo anche il gap di prestazione crosslingual. In questa figura, la linea blu è il trasferimento crosslingual zero, la linea arancione è il trasferimento crosslingual few-shot, mentre la linea verde è l'impostazione monolingue. Abbiamo scoperto che per l'impostazione zero, il trasferimento crosslingual è significativo. E confrontando la linea blu e la linea arancione, abbiamo scoperto che per l'impostazione few-shot, il gap di trasferimento è ridotto rapidamente. Troviamo anche altri risultati interessanti. Ad esempio, encoder-decoder supera il lavoro precedente o raggiunge risultati comparabili. Addestrare sulla lingua naturale inglese può significativamente aumentare le prestazioni di few-shot in lingue target. E troviamo che modelli multilingue come codas e bleu sono ancora inadeguati per compiti di parsing semantico crosslingual. In sintesi, costruiamo Exemplar, un set di dati uniforme per il parsing semantico in più lingue. E conduiamo uno studio di benchmark su tre tipi di modelli linguistici. E i nostri risultati mostrano molti risultati interessanti, ecc. E benvenuti al nostro documento e codice. Grazie per aver ascoltato.</sample>
    <sample id="171">Lavori connessi includono: 1) Detection of Similar Embeddings, 2) Detection of Similar Embeddings with a Backdoor, 3) Detection of Similar Embeddings with a Backdoor and a Benign Dataset, 4) Detection of Similar Embeddings with a Backdoor, a Benign Dataset, and a KS Test.</sample>
    <sample id="172">No, Codex e Bloom sono considerati inadeguati per il CLSP.</sample>
    <sample id="174">Arg Analysis 35K è un set di dati di argomentazione di grande scala, caratterizzato da argomenti di alta qualità provenienti da tornei di alto livello, esperti e debattatori. Il set presenta un'analisi avanzata che combina argomenti, premesse e prove, migliorando la profondità e la persuasione. Introduce un modello di rilevanza che attribuisce una valutazione di 0 a 1 per la rilevanza di ogni argomento, permettendo una valutazione più accurata della rilevanza. Inoltre, introduce la rilevanza basata sull'istanza, che elimina solo le annotazioni percepite come biasate, migliorando la precisione. Il set è progettato per fornire una valutazione più diversificata e precisa della rilevanza degli argomenti, rendendolo un'ottima risorsa per la ricerca e l'analisi.</sample>
    <sample id="175">Il metodo utilizza una continua relaxazione per semplificare il processo di ricerca della permutazione più probabile, rendendo il metodo più pratico e GPU-friendly.</sample>
    <sample id="176">L'equità di un modello NLP a valle si verifica quando i modelli con una certa inclinazione politica danno risultati disparati nei compiti a seconda della demografia o del partito politico dei media di notizie, come evidenziato dalla migliore rilevazione di hate speech contro gruppi minoritari da modelli di orientamento sinistro e viceversa.</sample>
    <sample id="177">La relatrice è Yanis Lavraik.</sample>
    <sample id="178">Relatrice: Koussof Sinha</sample>
    <sample id="179">Mélanie Szklarek discute il miglioramento delle capacità di teoria del pensiero in modelli linguistici di grandi dimensioni, introducendo il metodo Symbolic TOM. Questo approccio utilizza rappresentazioni grafiche per migliorare la comprensione delle credenze e delle ipotesi dei personaggi in scenari narrativi. I risultati dimostrano che Symbolic TOM migliora notevolmente le prestazioni degli LLM, superando i metodi di fine-tuning e le soluzioni basate su testo. I test includono set di dati in dominio e fuori, dimostrando la robustezza del metodo in vari scenari.</sample>
    <sample id="180">Il nome della relatrice è Myra.</sample>
    <sample id="181">Si Yuan e il suo team hanno sviluppato un framework per migliorare la pianificazione linguistica con limiti specifici, utilizzando grandi modelli linguistici. Il lavoro si concentra sulla pianificazione di obiettivi specifici, come fare una torta al cioccolato, che non è stato ampiamente studiato. Per affrontare questo, hanno definito il problema della pianificazione linguistica con limiti e sviluppato un metodo per migliorare la qualità della pianificazione dei modelli linguistici. Hanno acquisito obiettivi specifici usando InstructGPT e creato il dataset CoScript, che contiene 55.000 obiettivi specifici. Questo dataset è destinato a migliorare la ricerca sulla pianificazione linguistica.</sample>
    <sample id="182">Nel contesto di questo articolo, il tropicalismo indica un stereotipo che associa le donne di colore, in particolare quelle latine, a caratteristiche vivaci e sensibili, che contribuiscono a un'eredità di discriminazione e percezione come 'altrove' rispetto al normale.</sample>
    <sample id="183">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target usando prompt generati da LLMs, identificando parole distintive attraverso il metodo Marked Words, e confrontando le parole positive con le parole del lexicon.</sample>
    <sample id="184">Il lavoro ha utilizzato cxmi e cxmi pointwise per misurare l'utilizzo del contesto nei modelli di traduzione.</sample>
    <sample id="185">DrBERT è un modello pre-allenato in francese per il dominio medico, mentre ChuBERT è un modello pre-allenato in inglese per lo stesso dominio.</sample>
    <sample id="187">Due autori sono coinvolti nell'articolo: Yin e Ji Yang.</sample>
    <sample id="188">Il trasferimento iterativo dell'apprendimento è un processo in cui un modello viene aggiornato iterativamente con nuovi dati raccolti durante l'annotazione, migliorando continuamente le prestazioni.</sample>
    <sample id="189">L'obiettivo del set di dati è raccogliere espressioni di riferimento indirette in tre domini (musica, libri e ricette) per migliorare la comprensione e la risoluzione delle espressioni di riferimento in sistemi di linguaggio.</sample>
    <sample id="190">Un utente malintenzionato può estrarre i parametri del modello attraverso un EaaS analizzando i dati di input e output per identificare schemi o caratteristiche che possono essere replicati per ricreare il modello.</sample>
    <sample id="191">L'articolo è scritto da tre autori: Sarah Papi, Matteo Negri e Marco Turilli.</sample>
    <sample id="192">Yang Luo presenta CAM, un nuovo metodo di ottimizzazione per il training dei modelli linguistici che bilancia velocità e efficienza di memoria. CAM utilizza la fattorizzazione non negativa di matrici per ridurre significativamente la memoria richiesta, superando i metodi tradizionali come Adam e Adafactor. CAM introduce un aggiornamento guidato dalla fiducia, che bilancia gli aggiornamenti errati, migliorando la stabilità e la velocità di convergenza. I risultati sperimentali dimostrano che CAM migliora notevolmente la validazione e la precisione, riducendo al contempo la memoria richiesta, specialmente per modelli di grandi dimensioni. CAM si dimostra efficace per compiti di fine task, offrendo un approccio innovativo per il training dei modelli linguistici.</sample>
    <sample id="193">Circa 1.000 annotatori sono stati impiegati per creare il set di dati iniziale.</sample>
    <sample id="194">Gli autori dell'articolo sono Jenny, Sebastian Santi, Ronan Labraz, Caterina Rainerica e Martin Sab.</sample>
    <sample id="195">Il lavoro introduce il framework ROHT, che mira a migliorare la risoluzione delle domande complesse in QA. Questo approccio combina la decomposizione gerarchica delle domande con il ragionamento probabilistico, utilizzando sia Knowledge Bases (KB) che corpus di testo. Il framework è valutato su due set di dati complessi, KQAPRO e MUSIX, dimostrando miglioramenti significativi rispetto ai metodi tradizionali. In particolare, il modello ROHT Mix mostra un'elevata efficacia integrando risposte di diverse livelli, mentre il ROHT Text evidenzia benefici aggiuntivi di incorporare informazioni da KB.</sample>
    <sample id="196">L'esempio in cui il governatore è a sinistra è 'I saw Bart and Lisa'.</sample>
    <sample id="197">I modelli all'avanguardia nei sistemi di dialogo includono quelli sviluppati da Emory NLP Lab in collaborazione con Amazon Alexa AI, che utilizzano ABC Eval per valutare la qualità del dialogo in modo più preciso e riducendo la soggettività.</sample>
    <sample id="198">La valutazione dell'accettabilità dei modelli nell'intera finestra di contesto è necessaria perché i modelli linguistici moderni hanno un'ampia finestra di contesto, e valutazioni basate su singole frasi potrebbero non catturare appieno la loro comprensione.</sample>
    <sample id="199">Sì, la formazione attraverso la modalità multilingue ha causato un calo delle prestazioni rispetto al modello inglese monolingue in sette set di dati.</sample>
    <sample id="200">Sì, gli annotatori conoscono l'entità in anticipo.</sample>
    <sample id="201">Nelle valutazioni, sono state utilizzate metriche di MT avanzate e risultati di valutazione esperta basati su umani.</sample>
    <sample id="202">Sì, il regresso influisce su specifici tipi di NER, come i tagger di nomi di persone, che mostrano una maggiore difficoltà a generalizzare rispetto ad altri tipi come i tagger di nomi di aziende.</sample>
    <sample id="203">La posizionalità nella NLP è importante perché influisce sulle decisioni di ricerca e sui risultati, potenzialmente portando a pregiudizi nei sistemi di NLP. Comprendere la posizionalità aiuta a garantire che i sistemi siano inclusivi e rappresentino equamente diverse prospettive.</sample>
    <sample id="204">BLOOM è stato affinato con adattatori, non con una messa a punto integrale.</sample>
    <sample id="205">Xiangbing, a PhD student at the University of Washington, presents research on the influence of political biases in language models, which are trained on large-scale web data. The study explores how political leanings in pre-training data affect model fairness in downstream tasks, such as hate speech and fake news detection. Preliminary results show that language models exhibit varying political leanings, with GPT-4 being the most liberal. The research highlights the challenges of sanitizing political opinions in training data without risking censorship. The findings underscore the need to address fairness issues in language models to prevent marginalization of minority groups.</sample>
    <sample id="206">Per il trasferimento dell'apprendimento, iniziano con il modello 'Debate here' e 'Binary classification of X' poiché sono correlati alla dissonanza e concordanza.</sample>
    <sample id="207">I recenti set di test utilizzati per valutare le capacità di PaLM sono i set di test di MT, utilizzando i dati di Dev per ottenere risultati migliori rispetto ai dati di addestramento.</sample>
    <sample id="208">Gli autori hanno proposto tre suggerimenti: affrontare positivi stereotipi, utilizzare una lente di intersezione per studiare le bias, e aumentare la trasparenza sui metodi di mitigazione del bias.</sample>
    <sample id="209">Il metodo proposto migliora la qualità dei risultati generati dai modelli di linguaggio, rendendoli più fedeli alle specifiche vincolate, in contrasto con il metodo di riferimento che produce risultati di variabilità e inadeguati.</sample>
    <sample id="210">Il relatore è Shu-Hung.</sample>
    <sample id="211">Sì, i risultati e il set di dati nell'articolo possono essere utilizzati come parametri di riferimento.</sample>
    <sample id="212">Nell'articolo, tre modelli più piccoli vengono utilizzati: T5, BERT, e RoBERTa.</sample>
    <sample id="213">Il modello di base utilizzato è OFA (Unified Multi-Modal Pretrained Model).</sample>
    <sample id="215">Adam Szpekowski discute le diverse strutture di coordinazione nei linguaggi, evidenziando le differenze tra strutture simmetriche e asimmetriche. Utilizzando il principio di minimizzazione della lunghezza delle dipendenze, sostiene che le strutture simmetriche, in cui una delle conjuncture è sempre la testa della coordinazione, sono preferite. I dati raccolti dal Penn Treebank supportano questa tendenza, mostrando che le conjuncture di maggiori differenze di lunghezza tendono ad essere più corte quando non c'è un governo. Questo argomento si oppone alle strutture asimmetriche, che non seguono questa regola.</sample>
    <sample id="217">L'argomento discute la generazione di dialogo controllabile per più attributi, introducendo D-C-G, un modello che migliora la generazione di dialogo controllabile. Introduce prompt orientati e orientati, e un framework di valutazione M-A-E. I risultati dimostrano l'efficacia di D-C-G in generazione di dialogo controllabile e la sua superiorità rispetto ai metodi di base.</sample>
    <sample id="218">L'articolo è stato scritto da Aydin Bilal e colleghi di Google Translate.</sample>
    <sample id="219">Il lavoro di Jiahuizhu e colleghi introduce un modello di pipeline multi-stagione per identificare segnali finanziari nei rapporti annuali. Il modello mira a semplificare il processo di analisi dei rapporti annuali, notando che circa il 80% dei termini è simile tra i rapporti. Il processo prevede la segmentazione dei documenti, la classificazione dei rapporti in tre tipi (simiatici, semantici e dissiomatici), e l'ottimizzazione del modello attraverso l'uso di set di dati esterni e tecniche di fine-tuning. I risultati dimostrano un miglioramento significativo nella precisione e nella generalizzazione, con potenziali applicazioni future in vari settori.</sample>
    <sample id="220">Vasudha, un candidato a Ph.D. in informatica, e il suo team sono affiliati alla Stony Brook University.</sample>
    <sample id="221">L'articolo ha analizzato la coppia linguistica tedesco-inglese.</sample>
    <sample id="222">Questo lavoro esplora le sfide e le soluzioni per l'annotazione delle domande in dominio aperto, concentrandosi su come adattare i modelli di recupero e lettura per domande in domini diversi. Il lavoro introduce tre principali contributi: l'analisi di diverse interazioni di dati per facilitare l'annotazione in dominio aperto, l'identificazione del tipo di spostamento dei set di dati e la determinazione delle interazioni di dati efficaci per specifici tipi di spostamento. Il setup coinvolge un dominio generale come Wikipedia, ma si applica anche a domini specifici come il campo medico. I metodi di generazione di interazioni di dati includono tecniche zero-shot e few-shot, con l'uso di modelli di linguaggio per generare esempi. Le prestazioni sono valutate in base alla compatibilità del modello e alla natura del spostamento dei set di dati. I risultati mostrano che le interazioni di dati sono cruciali per migliorare le prestazioni, con miglioramenti fino al 24% nelle prestazioni del lettore.</sample>
    <sample id="223">Il relatore è Xianbing PhD student, University of Washington.</sample>
    <sample id="224">I modelli studiati sono Longformer e BERT.</sample>
    <sample id="225">Per l'addestramento, 53 attività vengono utilizzate, mentre per il test, 10.000 istanze per ciascuna delle 53 attività vengono utilizzate, e 5 attività aggiuntive vengono selezionate per il test.</sample>
    <sample id="226">Due.</sample>
    <sample id="227">Questo documento discute i limiti dei modelli linguistici attuali nel comprendere il linguaggio in un contesto specifico, noto come grounded language understanding. I modelli attuali, inclusi quelli recenti, sono pre-trentati senza un collegamento con l'ambiente, rendendo difficile la loro applicazione. Il lavoro introduce Pangou, un framework che consente ai modelli di concentrarsi sulla discriminazione piuttosto che sulla generazione, migliorando così l'efficacia. Pangou ha dimostrato prestazioni eccezionali in vari ambienti, come il question answering, superando modelli come ArkQA. Il framework è generico e può essere applicato a diversi ambienti, dimostrando robustezza anche in ambienti non-ideal.</sample>
    <sample id="228">Gli autori hanno effettuato i test su set di dati come AG News, MIND, SSTD2 e ERESTA.</sample>
    <sample id="229">Gabriela Skatylinskaya presents a study on detecting improvable claims in argumentative writing, focusing on text revisions. The research aims to determine when claims are optimally phrased and suggest improvements. The study introduces tasks for detecting suboptimal claims and suggesting revisions. It explores challenges like representativeness, model complexity, contextual relevance, and bias in revision-based data. The paper concludes that revision-based data can effectively be used for these tasks, with the impact of contextual information varying by task and issue.</sample>
    <sample id="231">NACHOS è un set di dati di corpus di testo medico e clinico di origine web, utilizzato per addestrare il modello Dr. Bert.</sample>
    <sample id="232">Il nome della relatrice è Aydin Bilal.</sample>
    <sample id="233">Sarah Papi presents a paper on Simultaneous Speech Translation (SimulST), addressing the challenges of current models, such as complex training and multiple latency regimes. The proposed solution, E-DOT, uses existing offline models and a single model for each latency, leveraging cross-attention to decide on partial translations. E-DOT outperforms existing strategies by shifting curves to the left, indicating better translation quality and lower latency. The paper includes open-source code and models to support reproducibility.</sample>
    <sample id="234">La strategia del prompting influisce significativamente sui risultati, con la qualità degli esempi che porta a prestazioni migliori. La strategia a cinque prompte non mostra differenze significative rispetto alla forma del prompting, mentre i prompt di alta qualità, come quelli provenienti dai dati Dev, offrono prestazioni migliori.</sample>
    <sample id="235">Kyowin, Patrick, Emile, Andre, and Graham.</sample>
    <sample id="236">Le 5 istruzioni scritte da esperti non sono specificate nel testo.</sample>
    <sample id="237">Gli autori propongono un test diagnostico per valutare l'integrazione delle informazioni provenienti da più fonti, chiamato Co-reference Resolution Task, che sfida i modelli a integrare sia conoscenze pre-trasparente che inferenziali.</sample>
    <sample id="238">Il dataset MeetingBank, presentato da Yebewan Hu, è un nuovo set di dati per la sintesi dei riunioni, focalizzato su riunioni di consiglio comunale. Il dataset affronta le sfide della raccolta di riunioni di alta qualità e risorse affidabili. Utilizzando l'API Speechmatics, i dati sono convertiti in transcrizioni, seguiti da identificazione dei tipi di riunioni e riunioni. Il dataset include 1.366 riunioni di consiglio comunale, con statistiche dettagliate e metriche di analisi. I risultati delle valutazioni dei sistemi di sintesi includono Oracle, Dialog LM, GPT-3, e altri. GPT-3 ha ottenuto i punteggi più alti in valutazioni umane, ma ha bisogno di miglioramenti in metriche di valutazione. Il dataset serve come strumento per la ricerca e per comprendere i processi decisionistici dei consiglio comunali.</sample>
    <sample id="239">L'attenzione di Aydin Bilal è stata rivolta a una breve revisione del documento intitolato "Prompting language models for machine translation: assessing strategies and performance". Questo è un lavoro collaborativo con i colleghi di Google Translate.</sample>
    <sample id="240">Ciao. Sono Dawei, un dottorando all'Università di Salant in Germania. In questo video, vorrei presentare il nostro recente lavoro, un esame critico di WSL. Questo è un lavoro congiunto con Xiaoxuan, Mario Musbach e Giastephane e Dittlich Klako. Vorrei iniziare con una breve introduzione a WSL. In WSL, non si annotano manualmente i dati. Invece, si etichettano i dati usando fonti di etichettatura di settimana, come semplici regole euristiche, database di conoscenza o fonti di citazione di bassa qualità, come illustrato nella figura a destra. Quando si confronta con le annotazioni umane, le annotazioni di settimana sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni sono errate. Se si addestra direttamente le reti neurali sui dati etichettati di settimana, le reti neurali tendono a memorizzare il rumore di etichettatura e non generalizzano. In WSL, vengono proposte algoritmi di addestramento per addestrare robustamente le reti neurali sotto tali etichette rumorose, in modo che i modelli addestrati possano generalizzare bene. Nelle recenti opere in WSL, si dice che le persone addestrano modelli solo sui dati etichettati di settimana e ottengono prestazioni elevate sui set di test puliti. Tecnicamente, questa affermazione non è sbagliata, ma c'è un problema, che è che si presume che ci sia un set di dati di validazione pulito disponibile per la selezione del modello. Questo approccio è un problema, poiché implica che sono necessarie annotazioni manuali aggiuntive in WSL. Ma, come un elefante in una stanza, questa necessità è spesso trascurata. L'ipotesi sopra solleva tre domande di ricerca. In primo luogo, è necessario un set di dati di validazione pulito per WSL? O forse possiamo usare un set di dati di validazione rumoroso? Se il set di dati di validazione pulito è necessario, o se il set di dati di validazione pulito è necessario per WSL per funzionare correttamente, quante campioni pulite abbiamo bisogno? In secondo luogo, se il set di dati di validazione pulito è necessario, o se il set di dati di validazione pulito è necessario per WSL per funzionare correttamente, allora, come possiamo utilizzare le campioni pulite? Infine, se scegliamo di accedere a campioni puliti, allora addestrare direttamente su di loro raggiungerà prestazioni migliori. La figura a destra mostra la differenza di prestazioni tra approcci di addestramento fine. Inizialmente, abbiamo solo dieci campioni per classe per raggiungere prestazioni elevate. Ma, non è la fine della storia. Se scegliamo comunque accedere a campioni puliti, allora addestrare direttamente su di loro raggiungerà prestazioni migliori. La figura a destra mostra la differenza di prestazioni tra approcci di addestramento fine applicati direttamente sui dati di validazione puliti e approcci WSL, che usano i dati di validazione puliti solo per la selezione del modello. Come possiamo vedere, se abbiamo dieci campioni per classe, l'addestramento diretto inizia a battere gli approcci WSL. Tuttavia, se abbiamo dieci campioni per classe, l'addestramento diretto inizia a battere gli approcci WSL. Infine, il miglioramento delle prestazioni previsti negli approcci WSL precedenti può essere facilmente raggiunto consentendo di continuare a perfezionare i campioni di validazione puliti. Come possiamo vedere, il modello Valina, chiamato ftw, inizialmente sottopone prestazioni inferiori rispetto ai metodi WSL più complicati come Cosine. Tuttavia, se consentiamo di continuare a perfezionare i campioni di validazione puliti, allora ftw si comporta allo stesso modo degli altri metodi. Quindi, in pratica, non c'è motivo di scegliere metodi WSL più complicati, che richiedono più tempo di calcolo e spazio su disco. In sintesi, abbiamo dimostrato che gli approcci recenti WSL richiedono campioni di validazione puliti per funzionare correttamente. Il loro miglioramento delle prestazioni e la praticità sono sovrastimati. Le nostre raccomandazioni concrete per il futuro sono le seguenti. Innanzitutto, riportare i criteri di selezione del modello. Ad esempio, riportare se la selezione del modello è fatta con campioni di validazione puliti. Secondo, confrontare gli approcci WSL con le linee di base di apprendimento. In terzo luogo, il fine-tuning continuo è un approccio semplice ma forte che dovrebbe essere considerato in futuro. Infine, abbiamo aperto il codice. Puoi trovarlo via il codice Q R su questa diapositiva. Grazie. E godetevi il conferenza.</sample>
    <sample id="241">Ethan e il suo team hanno sviluppato un framework per valutare efficacemente i sistemi di rilevamento della disinformazione, in particolare riguardo alle false informazioni sui trattamenti COVID-19. I sistemi attuali spesso falliscono a causa di valutazioni irrealistiche e di mancanza di input umano. Il loro approccio integra l'input umano in ogni fase del processo, utilizzando un modello T5 per estrarre e classificare le informazioni dai tweet. Il sistema è in grado di rilevare rapidamente le false informazioni, contribuendo a una gestione più efficace della disinformazione.</sample>
    <sample id="242">I metodi comuni di valutazione per i sistemi di dialogo includono valutazioni umane basate su scale di Likert, valutazioni comparative di conversazioni e valutazioni basate su criteri.</sample>
    <sample id="243">L'articolo è scritto da un solo autore, Jenny.</sample>
    <sample id="244">Le conoscenze di base necessarie includono che Servin è un giudice e Kea è un panettiere, insieme a conoscenze generali che i giudici decidono casi in tribunali.</sample>
    <sample id="245">L'insegnante presenta un lavoro su un processo di selezione di annotatori per il clustering di Amazon Mechanical Turk (mTurk), focalizzato sulla valutazione della qualità e dell'accordo degli annotatori. Il processo prevede due fasi: una fase di qualificazione che valuta le capacità di valutazione degli annotatori e una fase di resistenza che valuta la capacità di gestire un carico di lavoro pesante. Il risultato mostra che il processo seleziona 6% di annotatori di alta qualità, con un'elevata interazione tra i parametri di valutazione. Il processo è vantaggioso in termini di risparmio di risorse e può essere paragonato a quelli di CloudResearch. Tuttavia, il processo ha limitazioni, come la valutazione solo in inglese e la mancanza di garanzie per la formazione della qualità.</sample>
    <sample id="246">Sì, il codice è disponibile su GitHub.</sample>
    <sample id="247">Il documento discute la creazione di un nuovo set di dati di fact verification, FactKG, che utilizza Knowledge Graphs (KG) per verificare le affermazioni. Questo approccio si differenzia dai set di dati esistenti che utilizzano testi o tabelle, offrendo un metodo più intuitivo e affidabile. Il set di dati include affermazioni in due stili: scritto e colloquiale, e utilizza cinque tipi di ragionamento: one-hop, congiunzione, esistenza, multi-hop e negazione. Il set di dati è stato valutato utilizzando tre metodi: un modello di trasformazione del tipo colloquiale, modelli di basili e il modello Gear, con il Gear che supera tutti i modelli di basili. Il documento conclude con un invito a utilizzare il set di dati e contattare il presentatore.</sample>
    <sample id="248">Sì, gli annotatori per NLPositionality sono bilanciati rispetto a gruppi demografici come Paese, genere, età, istruzione e background culturale.</sample>
    <sample id="249">Le frasi nel dominio accettabile sono state perturbate aggiungendo 'noise' che preservava la struttura grammaticale.</sample>
    <sample id="250">Una valutazione dimensionale significa valutare diversi aspetti specifici della qualità della conversazione, fornendo una comprensione più dettagliata e precisa della performance del modello.</sample>
    <sample id="251">Gli autori dell'articolo sono Jinwei Yi e il suo team dell'Università di Scienza e Tecnologia della Cina.</sample>
    <sample id="252">Il lavoro presenta u-create, un approccio innovativo per il recupero dei precedenti legali basato sull'estrazione di eventi. Utilizzando tecniche di apprendimento non supervisionato, il progetto introduce un set di dati chiamato ILPCR, che fornisce un compito di benchmark per il recupero dei precedenti. Il set di dati è composto da 7.070 casi legali con 6.775 citazioni per documento. Il progetto introduce anche un flusso di lavoro che sfrutta l'estrazione di eventi, dimostrando un alto recupero di efficiency e bassa inferenza. I risultati dimostrano che i metodi basati su eventi superano i metodi basati su conteggio e trasformatori, offrendo un approccio più efficace per il recupero dei precedenti.</sample>
    <sample id="253">Mario Edarragon e il suo team presentano un modello di disordine mentale, DisOrber, che utilizza l'adattamento a due domini per rilevare i segni di disordini mentali nei social media. Il modello, basato su BERT, viene adattato per comprendere il linguaggio specifico dei social media e dei domini della salute mentale. Il modello mostra un'eccellente bilancia tra precisione e recall, superando i risultati di BERT. Il lavoro mira a migliorare la rilevazione dei disordini mentali attraverso l'uso di risorse lessicali e dati clinici.</sample>
    <sample id="254">Il lavoro di Sun Qi introduce un framework per la document level relation extraction che utilizza l'uncertainty guided level denoising per migliorare la qualità dei pseudo-labels nei dataset distanti. Il framework pre-trena un modello pre-denoising con sia data distanta che annotata per generare pseudo-labels. Per affrontare le pseudo-labels false positive, viene introdotta l'uncertainty estimation, che valuta la fiducia delle previsioni del modello. Per gestire le relazioni multiple tra coppie di entità, viene proposto un metodo di scoring dell'incertezza a livello di istanza. Un approccio relabeling reattivo con soglie di classe dinamiche e un strategia di training multiphase sono proposti per migliorare ulteriormente le prestazioni. I risultati dimostrano che il framework supera le linee di base precedenti su due set di dati pubblici.</sample>
    <sample id="255">La forma del prompting è cruciale per zero e uno shot prompting, ma non influisce significativamente sui cinque shot prompting.</sample>
    <sample id="257">Gli autori hanno valutato quattro modelli di dialogo, utilizzando 100 conversazioni umane-bot per ciascun modello.</sample>
    <sample id="258">Zhangsun Han discute the potential of large language models as an alternative to human evaluations in natural language processing. The study proposes using these models to rate text based on grammar, coherence, likability, and relevance, comparing their performance to that of human evaluators, specifically English teachers. The experiments reveal that while some models do not align with human preferences, models like Davinci and ChatGPT show a preference for human-written texts. The paper explores the implications, benefits, and limitations of using large language models for evaluation, providing a comprehensive analysis of the results.</sample>
    <sample id="259">L'abstract di Xu Zhang discute il lavoro su Exemplar, un dataset unificato per la semantica crosslingua in più lingue e rappresentazioni. Il dataset include 90 set di dati in vari domini, 5 attività di semantica e 8 rappresentazioni, con 22 lingue in 15 famiglie linguistiche. Il lavoro valuta diversi metodi di modellazione, come il test di traduzione, il set di few-shot e il modello multilingue, e confronta le prestazioni di modelli monolingue, few-shot e multilingue. I risultati mostrano che i modelli encoder-decoder superano i modelli encoder-only, con miglioramenti significativi in alcuni set di dati. Il lavoro conclude che i modelli encoder-decoder sono efficaci per la semantica crosslingua, ma i modelli multilingue come CodeSearch e BLOOM sono ancora inadeguati per compiti di semantica crosslingua.</sample>
    <sample id="260">Due autori sono coinvolti nell'articolo: Jinwei Yi e Yifan Zhang.</sample>
    <sample id="261">Un buon pianificatore dovrebbe scrivere script che siano sia ragionevoli che fedeli alle restrizioni.</sample>
    <sample id="262">Due autori sono indicati per l'articolo.</sample>
    <sample id="263">Questo lavoro affronta i problemi di bias nei sistemi di in-context learning, un paradigma di modellazione linguistica. Identificano un nuovo tipo di bias, il bias di dominio, derivato dal corpus di compiti. Proponono un metodo di calibrazione, Domain Context Calibration, che utilizza parole in dominio per mitigare i bias. I risultati dimostrano che questo metodo migliora significativamente le prestazioni dei modelli in-context, specialmente in compiti con alto bias di dominio.</sample>
    <sample id="264">L'articolo presenta un approccio innovativo per il trasferimento audio-visuale, chiamato TAVT, che mira a superare le sfide di generazione di testo audio-visuale. Il lavoro introduce un modello che può adattarsi rapidamente a nuovi domini con dati limitati, utilizzando una rete di mappatura audio-visuale, un encoder e un modello di linguaggio. Il modello introduce un parametro alpha per valutare l'influenza di diverse modalità e utilizza una funzione di perdita e un metodo di costruzione per migliorare la semantica. I risultati dimostrano che TAVT supera i modelli basati su RNN e Transformer in vari domini, anche con pochi dati.</sample>
    <sample id="265">La relatrice è Vasudha.</sample>
    <sample id="266">L'articolo non menziona specificamente le affiliazioni degli autori.</sample>
    <sample id="268">Gli errori più comuni di PaLM sono errori di omission, dove parti della frase di origine vengono omesse per produrre una traduzione più fluida.</sample>
    <sample id="269">Ciao. Sono James Finch. E io sono Sarah Finch. E oggi parleremo di abc eval, un nuovo approccio dimensionale per valutare l'IA conversazionale. Questo lavoro è stato fatto dal laboratorio Emory Nlp, guidato da Gino Choi presso Emory University, in collaborazione con Amazon Alexa AI.</sample>
    <sample id="270">James Finch e Sarah Finch sono affiliati all'Emory NLP Lab, in collaborazione con Amazon Alexa AI.</sample>
    <sample id="271">CFT sta per Continuous Fine Tuning, un metodo suggerito per migliorare le prestazioni dei modelli WSL continuando a perfezionare su set di validazione puliti.</sample>
    <sample id="272">L'articolo è un lavoro congiunto di otto autori.</sample>
    <sample id="273">Il mio nome è Kaio Yin e presenterò il nostro lavoro intitolato "Quando la traduzione richiede contesto: un'esplorazione multilingue guidata dai dati." Questo lavoro è stato realizzato in collaborazione con Patrick Koller, Emile Liu, Andre F. D. Martins e Graham Neubig. Quindi, molte traduzioni dipendono dal contesto. Ad esempio, come tradurremmo 'mole' in questa frase? Bene, se la frase precedente fosse stata, 'le cose potrebbero diventare pericolose se i ministri scoprono', allora 'mole' si riferisce a un spia. Ma se la frase precedente fosse stata, 'Potrebbe essere qualcosa di grave, dottore?', allora 'mole' si riferisce a un segno di nascita. Quindi, a seconda del contesto, il significato della parola cambia e, di conseguenza, la traduzione cambia. Tuttavia, valutare quanto bene i modelli possono tradurre casi come questo è piuttosto difficile. In primo luogo, perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende i metrica a livello di corpus come bleu incapace di catturare queste traduzioni. E alcuni hanno suggerito valutazioni mirate sulle traduzioni dipendenti dal contesto, ma questi risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e set di lingue, poiché di solito si basano sulla conoscenza del dominio e sulla curazione umana. In questo lavoro, cerchiamo di rispondere a due domande: quando la traduzione richiede contesto e quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, iniziamo misurando quanto una parola dipende dal contesto durante la traduzione. In precedenza, abbiamo introdotto cxmi come misura per l'uso del contesto dai modelli di traduzione. E questo viene fatto misurando quante informazioni il contesto C fornisce sul target Y, dato l'X. Si può pensare a cxmi come le informazioni ottenute dal dare contesto al modello. In questo lavoro, estendiamo cxmi a cxmi puntuale, che può misurare l'uso del contesto a livello di frase o di parola. Analizziamo parole con alto cxmi per cercare schemi tra queste parole. E eseguiamo il nostro analisi su trascrizioni di Ted Talk tradotte da inglese a quattordici lingue diverse. Eseguiamo il nostro analisi a tre livelli diversi. Innanzitutto, guardiamo a parti di parole che hanno alto cxmi. Questo ci permette di trovare, ad esempio, pronome duali in arabo che hanno un cxmi relativamente alto. E questo può essere spiegato perché l'inglese non ha pronome duali, quindi è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo. E allo stesso modo, troviamo che certi linguaggi richiedono contesto quando vogliamo scegliere la forma verbale appropriata. Quindi guardiamo a parole che hanno alto cxmi medio su tutte le sue occorrenze. E questo aiuta a identificare casi come il seguente, dove in cinese è necessario il contesto per tradurre nomi propri per assicurarsi di usare la stessa traduzione all'interno del documento. E allo stesso modo, guardiamo a diversi token individuali che hanno alto cxmi. Questo ci permette di identificare fenomeni che non possono essere catturati solo dal termine stesso, ma che sono espressi nella struttura della frase, come la risoluzione dell'ellisse. Quindi, usiamo i nostri risultati per progettare un benchmark per la traduzione a livello di documento. Per ogni uno dei cinque fenomeni di discorso che abbiamo identificato, creiamo taggatori per identificare parole che appartengono al fenomeno. E chiamiamo il nostro taggatore il taggatore multilingue consapevole del discorso. Possiamo anche notare che diversi linguaggi hanno diverse proporzioni di questi fenomeni di discorso. Quindi, usiamo il taggatore Muda applicando il taggatore su un corpus parallelo che vogliamo usare per la valutazione. E poi applichiamo i nostri metrica di traduzione di scelta su le traduzioni dipendenti dal contesto che il taggatore Muda ha identificato. E infine, usiamo il nostro benchmark, così come altre metriche, per valutare diversi modelli di traduzione a livello di documento. In primo luogo, quando usiamo metrica a livello di corpus, quindi per bleu, scopriamo che i modelli di traduzione agnostici al contesto hanno le migliori prestazioni. Ma poi, se usiamo comet, i modelli consapevoli del contesto al discorso performano meglio. E se usiamo wordf, i modelli con o senza contesto hanno prestazioni comparabili. Questo dimostra di nuovo che è difficile determinare il miglior sistema di traduzione a livello di documento se usiamo solo metrica a livello di corpus. Ora, usiamo il nostro benchmark per valutare i modelli. E scopriamo che i modelli consapevoli del contesto sono significativamente più accurati dei modelli che non usano il contesto per certe discorsi, come la formattazione e la coesione lessicale. Ma questi modelli non sono molto migliori dei modelli che non usano il contesto per altri fenomeni, come le ellissi, i pronomi e la forma verbale. Quindi, suggerisce dove abbiamo bisogno di vedere più progressi per la traduzione a livello di documento. Confrontiamo anche diversi sistemi commerciali, e il nostro benchmark mostra che DeepL è di solito più accurato di Google Translate per la traduzione a livello di documento. In sintesi, eseguiamo un'analisi guidata dai dati su quattordici coppie di lingue per identificare quando la traduzione richiede contesto. E poi usiamo i nostri risultati per costruire un benchmark per la traduzione a livello di documento, che può aiutarci a identificare quali modelli di discorso possono gestire bene o no, e quali sistemi di traduzione sono buoni per la traduzione a livello di documento. Grazie per la vostra attenzione. Ci vediamo a Toronto.</sample>
    <sample id="274">La relatrice è Yu Chen Zhang.</sample>
    <sample id="276">Ananya and Vignesh present their work on IndicMT Eval, a dataset for evaluating machine translation metrics for Indian languages. They focus on five languages from two language families, Tamil, Malayalam, Hindi, Marathi, and Gujarati, generating 7,000 samples. Bilingual expert annotators evaluate translations, marking errors with types and severities. Recent MT models like NLLB and IndicTrans show fewer errors. IndicComet, a fine-tuned metric, outperforms Comet baselines on most languages, with a robust score of 0.306 on unseen languages.</sample>
    <sample id="277">Il nuovo metodo non ha un nome specificato.</sample>
    <sample id="278">L'autore descrive il metodo di 'parole contrassegnate' come una tecnica che utilizza il concetto sociolinguistico di 'marking', identificando parole che distinguono gruppi contrassegnati da quelli non contrassegnati. Questo metodo utilizza log odds ratios per identificare parole che distinguono i gruppi contrassegnati, come le donne di colore, rispetto ai gruppi non contrassegnati, come i bianchi.</sample>
    <sample id="279">Gli autori sono PhD student della University of Washington.</sample>
    <sample id="280">Shuo Tao presenta il framework Multi-EMO per l'Emotion Regulation in Conversations (ERC), che mira a prevedere le etichette emotive delle espressioni verbali. Il framework integra informazioni multimodali, affrontando sfide come la complementarietà dei dati multimodali, le performance in classi di movimenti di minoranza e la distinzione tra emozioni simili. Introduce vis-expressNet, un nuovo estrattore visivo che elimina informazioni non correlate, e MultiAttend, un modello di fusione multimodale. Utilizzando un contrasto focale ponderato, il framework migliora la classificazione delle classi di emozioni difficili. I risultati dimostrano prestazioni superiori su benchmark di MELD e IEMOCAP, anche se presenta alcune limitazioni.</sample>
    <sample id="281">Kyowin Yu e il suo team hanno condotto un'esplorazione data-driven sulla necessità di contesto nei sistemi di traduzione multilingue. Il lavoro, in collaborazione con Patrick Fernaux, Emile Liu, Andre F. D. Martins e Graham Neubig, ha introdotto CXMI per misurare l'utilizzo del contesto durante la traduzione. Esaminando parole con alto Pseudo-CXMI, hanno identificato fenomeni come pronome duali, formati verbali e formati di nomi, evidenziando la necessità di contesto. Il team ha sviluppato un nuovo benchmark, MUDa, per valutare le prestazioni dei sistemi di traduzione a livello di documento. I risultati mostrano che i sistemi di traduzione consapevoli del contesto superano quelli agnostici, ma non tutti i fenomeni sono ancora ben gestiti. Il lavoro conclude con un confronto tra diversi sistemi di traduzione, con DeepL che emerge come più accurato rispetto a Google Translate.</sample>
    <sample id="282">La presentazione di Xiao Cai Zhu discute il lavoro su Style Transfer in Discorsi, un compito critico nella generazione naturale del linguaggio. Il team ha sviluppato Style Trace, un modello che imita le discorsi originali e combina con contenuti di stile specifici per creare testi in stile target. Il processo di addestramento è suddiviso in due fasi, con un focus sul distanziamento dei contenuti e sulla corretta visualizzazione del stile. I risultati dimostrano che Style Trace supera i modelli di base in termini di controllo del stile e qualità del contenuto, con risultati di valutazione che si allineano con i testi originali.</sample>
    <sample id="283">La prima struttura di dipendenza simmetrica menzionata è l'Universale Dipendenza.</sample>
    <sample id="284">The FSUIE model introduces a novel fuzzy span mechanism to enhance universal information extraction by addressing ambiguities in span boundary labeling and mismatches in transformer feature extraction. It represents the target boundary as a continuous probability distribution, allowing for adaptive attention span. The model uses a mask function to guide attention distribution, improving performance in named entity recognition, relationship extraction, and aspect sentiment triplet extraction. Experiments show significant improvements in small-scale data and better generalization on domain-specific tasks. The FSUIE model achieves competitive results on multiple datasets, demonstrating its effectiveness in various information extraction tasks.</sample>
    <sample id="285">Mingqi Gao e il suo team presentano un lavoro su Fact Error Correction (FEC) per la sintesi del dialogo, introducendo un nuovo quadro di valutazione per migliorare la valutazione dei modelli FEC. Il lavoro evidenzia i problemi con i metodi di valutazione attuali, che possono portare a valutazioni vaghe e non distinguere tra correzione e generazione di nuovi contenuti. Il team propone l'integrazione di dati annotati manualmente per migliorare la validità dei modelli FEC. Il loro approccio introduce una tassonomia di errori di fattualità, classificandoli in base al contenuto e alla forma. I risultati dimostrano che l'addestramento con risorse di riferimento migliora i modelli FEC, suggerendo un approccio combinato di dati sintetici e annotati per affrontare diversi tipi di errori di fattualità.</sample>
    <sample id="286">Il nome della relatrice è Sarah Finch.</sample>
    <sample id="287">Quattro.</sample>
    <sample id="288">Dati come il corpus di test di syntagm di Blimp possono essere utilizzati per testare i fenomeni sintattici.</sample>
    <sample id="290">Le abbreviazioni dei cinque metodi per la prima domanda di ricerca sono: 1) WSL, 2) CoSine, 3) FSW, 4) FSW-FT, 5) FSW-FT-CL.</sample>
    <sample id="291">Il modello viene valutato su 11 attività di domande non trascritte, tra cui riconoscimento di nomi, classificazione, tagging del parziale e risoluzione delle domande.</sample>
    <sample id="294">CamemBERT viene inizialmente addestrato su 4 gigabyte di dati provenienti da Natural Questions.</sample>
    <sample id="295">Il relatore è Adam Szpekowski.</sample>
    <sample id="296">Valerio Basile presents a study on irony detection, highlighting the limitations of traditional supervised machine learning. The research, a collaboration with Amazon Alexa and the University of Turin, focuses on developing perspective-aware models to better understand irony. The EPIC corpus, comprising 300 short conversations from social media, was used to train models with annotations from 15 annotators per language variety. The study found significant differences in annotation agreement across various dimensions, such as age and nationality, with notable discrepancies between annotators from the UK and Ireland. Perspective-aware models showed more confidence in their predictions compared to aggregated models.</sample>
    <sample id="297">Il discorso del professor Kate Price si concentra sullo studio dei dogwhistles, parole o frasi che trasmettono messaggi sottili a un gruppo specifico, spesso con significati antisemitici. Il progetto sviluppa una tipologia e un glossario di dogwhistles, esamina la loro presenza nei discorsi politici storici e valuta la loro rilevazione da parte dei modelli linguistici, in particolare GPT-3. I risultati mostrano che i modelli possono identificare molti dogwhistles, ma con variazione, specialmente quelli informali. Inoltre, i dogwhistles possono eludere la moderazione dei contenuti, risultando in valutazioni meno tossiche quando sostituiti per termini offensivi.</sample>
    <sample id="298">L'esperimento di riaddestramento con dati più recenti ha dimostrato che le prestazioni dei modelli diminuiscono con un aumento della distanza temporale tra i dati di addestramento e i dati di test, confermando la deriva temporale come causa principale della perdita di prestazioni.</sample>
    <sample id="299">Mihals Kurajis and Andreas Vlachos present a method to improve the robustness of NLP models by reducing their reliance on shortcuts. Current models excel in-distribution but falter out-of-distribution. Their approach involves a minimax training objective, where a learner and auxiliary model are trained alternately to focus on underrepresented hard examples. This method, tested on datasets like MNLI, FEVER, and QP, shows improved out-of-distribution performance without needing auxiliary models. The paper also explores the impact of model size, pre-training, and auxiliary size on the method's effectiveness.</sample>
    <sample id="300">Il lavoro di Belinda introduce il compito di 'Interactive Dictation', che consente agli utenti di usare il loro vocale sia per dettare che per modificare documenti in modo naturale e intuitivo. Il sistema, sviluppato in collaborazione con Jason Eisner, Adam Palz e Sam Thompson, segue un processo di quattro fasi: riconoscimento dell'ASR, segmentazione del discorso in detti e comandi, normalizzazione dei comandi e esecuzione sequenziale. Il progetto introduce anche un nuovo dataset e un sistema di base, utilizzando modelli T5 e GPT-3 per migliorare l'accuratezza e l'efficienza. Il codice è stato rilasciato per facilitare ulteriori ricerche.</sample>
    <sample id="302">Per garantire che i token siano in ordine corretto nella sequenza di output.</sample>
    <sample id="303">Aumentare la trasparenza sui metodi di mitigazione dei bias consente ai proprietari dei modelli di comprendere se i positivi stereotipi derivano da un'eccessiva allineazione dei valori o da altri metodi anti-stereotipici, permettendo un'analisi più approfondita.</sample>
    <sample id="304">Gli input inaccettabili di coppia minima sono frasi o domande grammaticalmente errate o non conformi alle norme sociali, utilizzate per valutare la capacità di un modello linguistico di distinguere tra contenuti accettabili e inaccettabili.</sample>
    <sample id="305">Dawie, a PhD student at Saarland University, presents a critical examination of weakly supervised learning (WSL) in a recent study. WSL, which uses weak labels from sources like heuristic rules, is often assumed to perform well on clean test sets. However, the study challenges this assumption, revealing that WSL methods actually require clean validation data to function effectively. The research questions whether clean data is necessary, how many clean samples are needed, and if clean samples should be used solely for validation. Findings show that WSL methods need clean data for proper performance, and that increasing clean samples or allowing continuous fine-tuning can improve performance. The study suggests that WSL approaches are overestimated, and recommends reporting model selection criteria, comparing WSL with full supervision methods, and considering continuous fine-tuning.</sample>
    <sample id="306">Sebastian Schuster and Na Jeong Kim investigate entity tracking in language models, focusing on whether large models can track entity states in discourse. They designed a task involving boxes and objects to evaluate entity tracking, finding that GPT-3.5 models, trained on code, show non-trivial tracking, unlike others. The study suggests pretraining on code enhances tracking abilities, but further research is needed to confirm generalization.</sample>
    <sample id="307">Gli autori hanno utilizzato metriche come precisione, recall e F1-score per valutare le prestazioni dei loro modelli su compiti di riconoscimento dei nomi sanitari, classificazione, tagging del parziale e risoluzione delle domande.</sample>
    <sample id="308">Jenny Zhu presents her research on the positionality in NLP, focusing on biases in datasets and models. Collaborating with the University of Washington and Allen Institute for AI, she uses the NL-Positionality framework to re-annotate datasets with diverse annotators and compare them to models using Pearson's R correlation score. The study, conducted via Lab in the Wild, reveals biases towards English-speaking countries and those with higher education, while non-binary individuals are underrepresented. Recommendations include documenting design choices, conducting research with a lens of perspectivism, and building specialized datasets and models for specific communities.</sample>
    <sample id="309">L'accordo tra annotatori è stato misurato utilizzando 100 conversazioni doppiamente etichettate.</sample>
    <sample id="310">Wikipedia è stato scelto per aggiungere frasi completamente scollegate alle query inaccettabili e accettabili.</sample>
    <sample id="311">I contributori dell'articolo sono Regina Stoddart, Omar Elhadj, and others.</sample>
    <sample id="312">MultiInstruct è il primo parametri di riferimento per l'istruzione multi-modale, offrendo un set di 62 compiti che coprono diverse categorie, un'opzione unica rispetto agli altri parametri che si concentrano principalmente su compiti linguistici.</sample>
    <sample id="313">Due autori sono coinvolti nell'articolo: James Finch e Sarah Finch.</sample>
    <sample id="314">La coordinazione binaria è una struttura di coordinazione in cui una frase è composta da due o più verbi, ciascuno con la propria oggetto, come in 'John threw the ball and hit the bat'.</sample>
    <sample id="315">Il tempo medio per l'uso dei prompt in questo studio è di 0.5 secondi.</sample>
    <sample id="316">Il modello T5 più piccolo, quando addestrato su CoScript, può generare script di alta qualità, rivaleggiando con i modelli più grandi, dimostrando che modelli più piccoli possono competere efficacemente quando addestrati su set di dati specializzati.</sample>
    <sample id="317">Il lavoro di Peng Li di Stanford University, CodeIE, introduce un metodo innovativo per l'estrazione delle informazioni, trasformando il compito in generazione di codice. Usando modelli di linguaggio di codice come Code-Davinci, CodeIE affronta il problema dei risultati non allineati delle pre-trained language models (PLM) come GPT-3 e T5. I prompt in formato di codice garantiscono che i modelli generino output strutturati, migliorando la precisione e la recall. I risultati dimostrano che i prompt in formato di codice superano i modelli basati su test in termini di precisione e recall. L'analisi approfondita rivela che i prompt in formato di codice riducono significativamente gli errori strutturali e migliorano le prestazioni complessive. Il lavoro di Li offre una nuova prospettiva per l'estrazione delle informazioni, potenzialmente influenzando futuri sviluppi nel campo.</sample>
    <sample id="318">Ciao. Sono Janis Lavraik e presenterò il nostro lavoro su Dr. Bert, un modello pre-addestrato in francese per il dominio medico clinico. In questa presentazione, parleremo del linguaggio modellato nel settore sanitario, quindi presenteremo il nostro lavoro principale, introducendo il primo modello medico in francese, Dr. Bert, basato su roberta e addestrato su natacos, un set di dati di medicina da web. Introduciamo anche un confronto con modelli con più impostazioni di pre-addestramento e fonti di dati. Poi presentiamo i nostri risultati su undici compiti di screening clinico in francese. E infine, concludiamo sugli esperimenti e forniamo maggiori dettagli su come accedere ai modelli. Dal suo rilascio nel duemiladiciotto, Bert è diventato uno dei più efficaci approcci per risolvere compiti di elaborazione del linguaggio naturale e offre un enorme miglioramento rispetto ai metodi statici e contestuali come word2vec, fasttext e ner. Da allora, il modello è stato adattato a molti altri domini, come il medico con camembert e altri domini come medico con permit bert e bio bert, ma principalmente in inglese. I modelli specializzati per altri linguaggi sono scarsi e spesso basati su addestramento continuo a causa della mancanza di dati in dominio. Quindi ci siamo chiesti, quali sono le fonti di dati più appropriate per un'ampia gamma di usi? E quelle fonti di dati sono buone sostituzioni per i dati clinici. Per rispondere a questa domanda, confrontiamo Dr. Bert con il nostro modello di base, che è basato su dati anonimati ottenuti dall'ospedale di non università che abbiamo. Dopo, ci siamo chiesti, quanto dati abbiamo bisogno di addestrare un modello specializzato su dati in francese? Se è quattro gigabyte, otto gigabyte o più? Per rispondere a queste domande, addestriamo e confrontiamo quattro modelli da zero, una prima versione di Dr. Bert con sette gigabyte di natacos, una seconda versione di quattro gigabyte di set di natacos, una prima versione di Shubert, che è un modello medico con quattro gigabyte di frasi prese dai nodi clinici, e una versione finale di Shubert con un mix di quattro gigabyte di set di natacos e quattro gigabyte di nodi clinici. Oltre a questo confronto, introduciamo tre modelli addestrati su pre-addestramento per analizzare l'impatto della strategia di pre-addestramento. Uno basato sul peso di camembert e addestrato su quattro gigabyte di set di natacos, un altro basato su camembert, ma addestrato su quattro gigabyte di nodi clinici, e infine, uno basato su un modello medico inglese, permitt, e addestrato su quattro gigabyte di set di natacos. In totale, abbiamo sette modelli. Per valutare i sette modelli, raccogliamo compiti di screening pubblici e privati, come riconoscimento di nomi, classificazione, taglio del discorso e risposte alle domande. Questi modelli sono confrontati con sei modelli di base, che sono camembert, oscar, camembert, ccnet, permit bert e bio bert. La valutazione evidenzia che i modelli si comportano meglio sui compiti con dati di natura simile a quelli su cui il modello è stato addestrato. Tuttavia, possiamo osservare che i dati provenienti da fonti eterogenee sembrano più versatili. Possiamo osservare che utilizzare più dati si traduce in prestazioni migliori. In generale, l'addestramento da zero sembra ottenere prestazioni più elevate su la maggior parte dei compiti. Tuttavia, il nostro esperimento di addestramento su pre-addestramento, usando il peso e il tokenizzatore di permitt, mostra risultati comparabili a quelli ottenuti con Dr. Bert, quattro gigabyte da scratch. Ma non è il caso per il modello basato su camembert, che soffre di problemi di stabilità. Infine, come conclusione, il nostro sistema offre prestazioni migliori su nove dei compiti di screening e supera globalmente i risultati del modello generico qui. I modelli ottenuti da natacos sono disponibili gratuitamente e su github. Quindi grazie per questa presentazione, e non vediamo l'ora di scambiare idee alla sessione posteriore a Toronto.</sample>
    <sample id="319">Le strategie di apprendimento esaminate includono l'addestramento da zero su dataset di Nachos, l'uso di pretraining con modelli come Camembert, e l'implementazione di pretraining con il peso e il tokenizzatore di modelli come Phemvet e Bio-Bert.</sample>
    <sample id="320">Il fattore di overfitting è inferiore a uno, indicando che ogni miglioramento su CONCOL 2003 si traduce in più miglioramenti su CONCOL++, non mostrando diminuzione.</sample>
    <sample id="321">La qualità della semplificazione è stata valutata confrontando diversi tipi di semplificazione, come sostituzione di parole, eliminazione di clausole e riordinazione delle clausole. Il corpus D-Plane ha mostrato una forte semplificazione nei testi biblici rispetto ai testi di notizie e di apprendimento della lingua.</sample>
    <sample id="322">Enrico Giannopoli discute l'importanza per i modelli linguistici di comprendere la moralità, sottolineando la sua natura soggettiva e la diversità di interpretazioni. Utilizzando il corpus di Twitter MORAL FOUNDATION, il suo studio esamina se i modelli linguistici possono riconoscere le differenze di espressione morale nei vari domini, come ALM e BLM. I risultati mostrano che i modelli riconoscono le differenze di espressione morale, evidenziando la necessità di modelli specifici per ogni dominio per evitare malintesi.</sample>
    <sample id="323">Il lavoro di Yu Jiawang introduce DHLC, un metodo per la complessità del QA che integra reti di grafi (HKG) con reti di knowledge base (KB) per migliorare la ricombinazione e la ricombinazione. Il metodo elimina le entità non pertinenti e utilizza la ricombinazione per migliorare l'interazione tra HKG e i modelli linguistici. Introduce il mask self-attention per modellare le reti di ricombinazione, incorporando le relazioni in l'attenzione. I risultati dimostrano che DHLC supera i metodi concorrenti in termini di performance sia per il QA complesso che per il QA aperto.</sample>
    <sample id="324">Sì, i modelli linguistici mostrano bias politici diversi, occupando tutti i quadranti del Political Compass. GPT-4 è il più liberale, mentre GPT-3 e BERT sono più conservatori.</sample>
    <sample id="325">L'allenamento.</sample>
    <sample id="326">La dissonanza cognitiva è quando ci sono due credenze o azioni incoerenti, come quando una persona sa che le sigarette possono danneggiare la salute ma continua a fumare. Studiarla aiuta a capire come le persone cambiano di opinione e può essere importante per comprendere problemi di salute mentale e polarizzazione.</sample>
    <sample id="327">Xiaoxu, a third-year PhD student, presents their work on the MagTower model for vision language learning at ACL 2023. The MagTower model builds on the BridgeTower by incorporating meta-level unidimensional representations, allowing for more effective exploitation of unidimensional semantic knowledge. It uses managers in each cross-modal layer to aggregate insights from pre-trained unidimensional experts, improving performance on tasks like VQVC2. The model outperforms base models trained on 4 million images and surpasses models trained on larger datasets. The paper is available on arXiv and GitHub.</sample>
    <sample id="328">GPT-4 è il modello linguistico più liberale.</sample>
    <sample id="329">L'abstract discute di un metodo innovativo per la localizzazione video zero-shot, che elimina la necessità di annotazioni manuali. Utilizzando un modello di captazione delle immagini pre-addestrato, il metodo genera pseudo-query basate su freni video, garantendo alta rilevanza all'interno degli eventi e bassa all'esterno. Il metodo affronta il problema della noia delle etichette riducendo il peso delle etichette rumorose e raffinando le etichette. I risultati dimostrano un miglioramento significativo rispetto ai metodi esistenti, con superiori punteggi in metriche di valutazione.</sample>
    <sample id="330">Sì, l'addestramento cumulativo funziona meglio di quello iterativo.</sample>
    <sample id="331">Il nome della relatrice è Sara Papi.</sample>
    <sample id="332">I dati sono stati tratti da trascrizioni di Ted Talks tradotti in 14 lingue.</sample>
    <sample id="333">In questo lavoro, Wang Hao e i suoi colleghi presentano InK, un metodo per migliorare le prestazioni delle macchine traduttive neurali (NMT) introducendo conoscenza del corpus (KCE) nel modello. Il problema centrale è che le NMT generano rappresentazioni non lisce, limitando la loro capacità di generalizzare. InK introduce un ciclo di allenamento in due fasi: prima, estraiamo KCE per guidare l'adattatore a regolare le rappresentazioni, e poi aggiorniamo le rappresentazioni per aggiornare il database asimmetricamente. Questo ciclo continua fino a convergenza. I risultati dimostrano che InK migliora significativamente le prestazioni delle NMT, superando i sistemi KMT tradizionali, con risultati superiori in metriche di valutazione come BLEU e GSA.</sample>
    <sample id="335">Relatrice: Matthias Landmann</sample>
    <sample id="336">Il trasferimento interlinguistico è un processo in cui un modello viene addestrato su un linguaggio sorgente e poi applicato a un linguaggio di destinazione, come mostrato in Exemplar.</sample>
    <sample id="337">The research introduces a novel approach to handle out-of-vocabulary (OOV) words in embedding-based models by using a word relationship graph. This graph mimics word formation and association, allowing OOV words to be represented through word pieces. A graph neural network is employed to process this graph, with a self-attention network assigning node attributes to OOV nodes. The model uses a graph-level representation to capture the entire graph information, and contrastive learning is applied to align the graph-level embedding with the background embedding. Extensive experiments show that the model outperforms existing methods in both intrinsic and extrinsic tasks, and it can be applied to other languages, depending on their word decomposition.</sample>
    <sample id="338">Bingxin presents a study on the evaluation of human explanations in natural language processing, focusing on their utility in model fine-tuning. The research introduces a unified data structure and a new evaluation metric, TRUE, which outperforms the existing simulability score by considering task-specific factors. The study evaluates five datasets, including COSE and ECQA, and finds that explanations, even if considered low quality, can still benefit model predictions. The findings suggest that the helpfulness of explanations depends on the task and explanation format, such as negation in certain datasets. The work lays the foundation for high-quality human-AI collaboration in annotation tasks.</sample>
    <sample id="339">Dawie, Xiaoxuan, Mario, Matthias, Giasdeth, e Dittli.</sample>
    <sample id="340">Guanghao Huang from UCLA presents Para-AMR, a large-scale syntactically diverse paraphrase dataset created using AMR back-translation. This dataset addresses the limitations of existing paraphrase datasets by providing a more varied syntactic structure while maintaining semantic similarity. The dataset, which includes around 50 million source sentences and 6.9 paraphrases per sentence, is shown to improve performance in NLP applications such as sentence embeddings, paraphrase generation, and data augmentation. Comparative analyses demonstrate that Para-AMR outperforms other datasets in terms of syntactic diversity and semantic similarity, making it a valuable resource for enhancing NLP models.</sample>
    <sample id="341">Gli autori fanno riferimento a misure di latenza come il tempo medio di attesa e il tempo di elaborazione computazionale, utilizzando questi per valutare l'efficienza del loro approccio E-DOT.</sample>
    <sample id="342">Gao Jinsheng presents a paper on LiveChat, a large-scale, personalized dialogue dataset derived from live streaming. The dataset, created by Gao and colleagues, addresses the need for video-based dialogue data, overcoming limitations of existing datasets. LiveChat is constructed through a three-step process involving video scraping, audio extraction, and dialogue construction. Experiments show that persona extraction and average session length significantly improve performance in response modeling and address recognition. The dataset's unique features, such as detailed persona annotations and long average sessions, set it apart from other dialogue datasets. Future work will focus on efficiently transferring dialogue models to LiveChat.</sample>
    <sample id="343">Ciao a tutti. Sono Akshata e oggi presenterò insieme a Martin il nostro lavoro, il kitmoss, che valuta l'integrazione della conoscenza proveniente da fonti multiple. Questo lavoro è una collaborazione tra McGill University, Meila e Microsoft Research. I modelli di comprensione del linguaggio naturale si basano su una varietà di fonti di conoscenza, come la conoscenza contenuta nei parametri pre-allenati e nella conoscenza fornita in input in tempo di inferenza. I lavori recenti mostrano che i modelli possono utilizzare la conoscenza pre-allenata per risolvere compiti. Tuttavia, la comprensione del linguaggio naturale richiede spesso la conoscenza fornita in tempo di inferenza. In questo lavoro, proponiamo un test diagnostico per l'integrazione della conoscenza proveniente da fonti multiple. Introduciamo un compito di corereferenziazione progettato per testare la capacità di attingere alla conoscenza proveniente da fonti diverse. Valutiamo il set di dati con partecipanti umani e stabiliamo modelli di corereferenziazione. Ecco un esempio dal set di dati. Servin è un giudice. Kia è un panettiere. Servin e Kia si sono incontrati in un parco. Dopo un lungo giorno di lavoro, decidendo casi in un tribunale, si è rilassato. Il compito qui è identificare l'entità corretta a cui il pronome si riferisce, che in questo caso è Servin. La risoluzione di un dato pronome richiede due tipi di informazioni. In primo luogo, conoscenza specifica dell'entità, come Servin è un giudice. E in secondo luogo, conoscenza di fondo, come i giudici decidono casi in tribunali. In generale, la conoscenza di fondo viene acquisita durante l'allenamento di modelli linguistici di grandi dimensioni, mentre la conoscenza specifica dell'entità viene tipicamente osservata in tempo di inferenza. Vogliamo variare la disponibilità di queste due informazioni in modo che possa essere trovata in una singola fonte o in più fonti. Abbiamo definito tre impostazioni di kitmoss. In primo luogo, abbiamo impostato l'impostazione pre trainata, dove la conoscenza di fondo è presumibilmente disponibile in tempo di pre-allenamento. In secondo luogo, c'è l'impostazione pre trainata, dove la conoscenza di fondo è disponibile sia in tempo di pre-allenamento che in tempo di inferenza. In terzo luogo, l'impostazione inferenza, dove entrambi i tipi di conoscenza sono disponibili solo in tempo di inferenza. Questa ultima impostazione è particolarmente interessante, poiché simula il caso in cui la conoscenza di fondo necessaria per risolvere un compito non è parte dei parametri pre-allenati dei modelli. Ad esempio, poiché sono state sviluppate nuove occupazioni da quando i parametri pre-allenati sono stati acquisiti. Ecco un esempio di come controlliamo la disponibilità di fatti e fonti di informazioni. Nella impostazione pre trainata, supponiamo che la conoscenza di fondo, partiti per cercare seggi elettorali, sia contenuta nei parametri pre-allenati. Nella fase di inferenza, forniamo la conoscenza specifica dell'entità, che è un politico. Nella impostazione pre trainata, forniamo non solo la conoscenza specifica, ma anche la conoscenza di fondo sui politici in fase di inferenza. Nella impostazione inferenza, forniamo l'occupazione fittizia, invece di politico, perché è improbabile che l'occupazione fittizia sia contenuta nei parametri pre-allenati. Valutiamo il set di dati sia con partecipanti umani che con modelli di corereferenziazione. In questa figura, mostriamo i risultati dei modelli di corereferenziazione migliori per l'impostazione più difficile. Senza allenamento specifico su kitmoss, entrambi i modelli non si comportano bene. Quando addestrati su kitmoss, entrambi i modelli C two F e BERT per corref hanno prestazioni significativamente migliori rispetto al caso a caso. Questo suggerisce che, quando addestrati su set di corereferenziazione generali, i modelli imparano a sfruttare indizi superficiali, che non sono utili quando si esegue un test su kitmoss, dove tali indizi sono stati rimossi. Ulteriori esperimenti con conoscenza fittizia indicano che anche i modelli di corereferenziazione migliori non sono in grado di integrare in modo affidabile la conoscenza di fondo fornita solo in tempo di inferenza. Per riassumere, i principali risultati del nostro lavoro sono: molti modelli di corereferenziazione non sono in grado di ragionare sulla conoscenza proveniente da fonti multiple senza addestramento specifico. Tuttavia, con addestramento specifico, alcuni modelli integrano con successo la conoscenza proveniente da fonti multiple. Tuttavia, anche i modelli di corereferenziazione migliori sembrano avere difficoltà a integrare in modo affidabile la conoscenza di fondo fornita solo in tempo di inferenza. Se siete interessati a maggiori dettagli, si prega di consultare il nostro articolo e il set di dati e codice su github. Grazie per l'attenzione.</sample>
    <sample id="344">I metodi basati su alberi richiedono formalisms specifici per le pre-elaborazioni e possono essere computazionalmente costosi.</sample>
    <sample id="345">L'introduzione di Matthias Landmann, Alexander Koller e Ivan Tiedoff discute un nuovo approccio per la composizione generativa senza l'uso di alberi, basato su tag di multi-set e permutazioni latenti. Invece di utilizzare alberi per modellare la composizione, il loro metodo utilizza un modello neurale che predice le permutazioni per ordinare i token di output. Questo approccio è più flessibile e meno computazionalmente costoso. I risultati sperimentali dimostrano che il loro modello supera significativamente altri modelli senza alberi in termini di generalizzazione a profondità di ricorsione. Tuttavia, affrontano sfide tecniche come l'alignamento tra input e output e la selezione delle permutazioni linguisticamente corrette, che affrontano con un metodo di relaxo continuo.</sample>
    <sample id="346">Gli autori dell'articolo sono Shu-Hung, Jin, Yao, and Yao.</sample>
    <sample id="347">L'argomento di oggi è il nostro articolo, marcate personaggi, usando prompt di linguaggio naturale per misurare stereotipi nei modelli linguistici. Questo lavoro è fatto in collaborazione con Esmdermus e Dan Jarochnik. Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi e stereotipi nei modelli linguistici più grandi, o lml. Tuttavia, questi misuratori hanno varie limitazioni. Di solito si basano su set di dati costruiti a mano che sono molto dispendiosi da curare. E anche, in genere misurano solo stereotipi molto specifici, il che significa che non generalizzano bene a altri demografi o contesti, o semplicemente catturano associazioni molto generali, come associazioni negative con determinati gruppi. Inoltre, la maggior parte del lavoro in questo spazio non tiene conto dell'intersezionalità, che è l'idea che le identità sociali multifaccettate possano comporre pregiudizi e essere un unico luogo di danno. Per superare queste limitazioni, ci affidiamo alla proprietà che questi modelli linguistici più recenti sono molto bravi a rispondere a istruzioni e prompt. Quindi possiamo chiedere al modello di generare un personaggio, che è una descrizione di un individuo immaginato, usando un prompt come, "Immagina di essere una donna asiatica, descrivi te stesso." E vediamo immediatamente che, mentre le espressioni non sono eccessivamente negative o tossiche nel senso tradizionale di queste parole, ci sono alcuni modelli interessanti. La donna asiatica è descritta come non assumendo, la donna medioriente è riferita usando parole come esotiche e come riferire a una regione affascinante. E entrambe le donne di colore fanno riferimenti all'ascendenza, mentre il personaggio maschio bianco non ha nulla di simile. Per catturare questi modelli, il nostro metodo ha due parti. La prima parte è generare questi personaggi. I nostri prompt per generare questi personaggi sono stati ispirati da uno studio in cui hanno dato questi prompt a soggetti umani, trovando che, dando a soggetti umani, sono in grado di surfacare stereotipi razziali. Inoltre, questo consente un confronto diretto tra le nostre persone generate e le risposte scritte dagli umani. La seconda parte è marcate parole, che è un metodo per identificare le parole che distinguono i gruppi marcati da quelli non marcati. Quindi, elaboro su questo in breve. Il metodo di marcature di parole si basa sul concetto sociolinguistico di marcatura, che afferma che c'è un default non marcato, e qualsiasi gruppo che differisce da quel default è linguisticamente marcato. Ad esempio, la parola guerriero è di solito associata a uomini. Quindi, quando descrivono un guerriero che è una donna, di solito specificano woman warrior e marcano il termine con woman. E più in generale, i gruppi dominanti in società sono linguisticamente e socialmente non marcati, mentre i gruppi marginalizzati sono di solito marcati. Quindi, in il nostro metodo, designiamo prima quali gruppi non marcati e marcati. E poi confrontiamo i personaggi usando il metodo di parole di lotta, che è fondamentalmente usare log odds ratio per distinguere le parole migliori per ogni gruppo marcato. Quindi, per i personaggi delle donne nere, faremo parole di lotta e confrontiamo i log odds ratio con le persone bianche e maschili, perché sono i due gruppi corrispondenti non marcati. Ora, per alcuni risultati, prima usiamo un lessico di stereotipi, e troviamo che i personaggi generati contengono molti più stereotipi rispetto a quelli scritti dagli umani. Tuttavia, quando guardiamo la distribuzione delle parole nel lessico, troviamo cose molto diverse. Quindi, mentre i personaggi generati hanno tassi molto più del lessico delle parole, le parole che sono in i personaggi generati sono solo le parole alte e atletiche. Quindi, solo le parole positive o almeno non negative. E in effetti, questo lessico non cattura molti dei modelli dannosi che abbiamo visto nelle diapositive precedenti. Quindi, per fare questo, passeremo ai risultati del nostro metodo di parole marcate per mostrare come questi personaggi positivi facilitano stereotipi e narrazioni essenzializzanti. In il nostro analisi, riveliamo come questi personaggi positivi riflettano modelli dannosi. In primo luogo, per i gruppi marcati, le parole principali includono cose come cultura, tradizione, orgogliosa e esotiche. E queste parole definiscono questi gruppi solo in relazione alla loro identità e distinguono loro come diversi dal normale bianco. Questo contribuisce a una lunga eredità di discriminazione e altriing per questi gruppi. Inoltre, ci sono molti schemi comuni che sono riflessi in queste parole, specialmente per le donne di colore. Quindi, per le donne asiatiche, le parole sono cose come petite e delicata e silenziose, che si collegano a un lungo storico di donne asiatiche che sono iper sessualizzate, viste come molto docili e sottomesse e così via. E infine, per le donne nere, vediamo che alcune delle parole principali sono cose come forte e resiliente. Questo si collega a un archetipo che le persone hanno chiamato l'archetipo delle donne nere forti. E mentre sembra positivo a prima vista, c'è stato lavoro che mostra che questo tipo di archetipo in realtà è molto dannoso. Quindi, in realtà, mette molta pressione su questi gruppi per essere resilienti e forti contro gli ostacoli sociali, piuttosto che lavorare per cambiare quegli ostacoli. Quindi, invece di lavorare per cambiare quegli ostacoli, mette pressione su questi gruppi per superare loro, il che porta a risultati negativi per questi gruppi, tra gli altri danni. In generale, troviamo che le parole per ogni gruppo marcato riflettono molto narrazioni essenzializzanti. Quindi, in base a questi modelli, concludiamo con tre raccomandazioni per i proprietari dei modelli. In primo luogo, come ricercatori, dovremmo affrontare stereotipi positivi e narrazioni essenzializzanti. Dovremmo anche usare l'angolo di intersezionalità per studiare pregiudizi e danni, perché ci sono molte cose che potrebbero essere trascurate se non lo facessimo. E infine, ci dovrebbe essere maggiore trasparenza sui metodi di mitigazione dei pregiudizi. Perché, per esempio, come questi stereotipi positivi, non sappiamo se c'è una sorta di allineamento di valore esagerato in corso, o forse altri, come metodi di anti stereotipizzazione che stanno dando vita a questi modelli dannosi. Quindi, non possiamo fare presunte o studiare ulteriormente senza più trasparenza. Grazie.</sample>
    <sample id="348">Mira receives a paper presentation on 'Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models,' in collaboration with Eszter Musch and Dan Jurafsky. The paper addresses the limitations of current measures of social bias in language models, which often rely on time-consuming, specific datasets and fail to account for intersectionality. The authors propose a method using instruction-tuned language models to generate personas based on prompts, such as 'Imagine you are an Asian woman, describe yourself,' to identify stereotypes. They employ a 'marked words' method to highlight stereotypes, revealing that while generated personas contain more stereotypes than human-written ones, they often reflect positive stereotypes that are essentially harmful. The paper concludes with recommendations for model owners to address positive stereotypes, use an intersectional lens, and increase transparency in bias mitigation methods.</sample>
    <sample id="349">Ciao a tutti. Mi chiamo Jingwei Yi e sono del University of Science and Technology of China. È un piacere fare un breve video promozionale del nostro articolo, "Are you Copy My Model? Protecting the Copyright of Large Language Models for Embedding Ad Services via Backdoor Watermark". Vorrei introdurre il contesto degli embedding ad service. Attualmente, i grandi modelli linguistici come GPT, LAMA, PALM ecc. sono eccezionali nella comprensione e generazione del linguaggio naturale. Gli embedding ad service sono uno dei servizi costruiti su grandi modelli linguistici per assistere varie attività NLP. Ad esempio, OpenAI offre un embedding basato su GPT. Tuttavia, recenti lavori hanno dimostrato che l'attaccante può rubare il modello attraverso l'apprendimento dagli embedding e fornire servizi simili. Pertanto, è necessario proteggere il copyright degli embedding ad service. Per proteggere il copyright degli embedding ad service, uno dei soluzioni è inserire un watermark nel servizio di provider e rilevare se un altro servizio contiene il watermark. Il metodo del watermark deve soddisfare le seguenti proprietà. In primo luogo, il metodo dovrebbe essere applicabile agli embedding ad service. In secondo luogo, il watermark non dovrebbe degradare la utilità degli embedding forniti. In terzo luogo, il watermark dovrebbe essere abbastanza coperto da un attaccante, o l'attaccante può rimuovere il watermark facilmente. Infine, il watermark deve essere trasferibile agli embedding dell'attaccante durante il processo di estrazione del modello. Esistono lavori che possono essere classificati in quattro categorie. Tuttavia, questi metodi non sono applicabili agli embedding ad service o mancano di trasferibilità. Pertanto, in questo articolo, proponiamo embedding marker, che è un metodo di watermark basato su backdoor applicabile agli embedding ad service. Quindi, introduciamo i dettagli del nostro embedding marker. Il marker embedding contiene due passaggi principali, iniezione del watermark e verifica del copyright. Prima di questi passaggi principali, selezioniamo prima un set di trigger. Il set di trigger è un gruppo di parole in intervalli di frequenza moderati. Supponiamo che il provider possa raccogliere un corpus di testo generale e contare la frequenza delle parole. Nella iniezione del watermark, definiamo prima un embedding target. Quando un utente invia una frase al servizio di provider, il provider conta il numero di trigger nella frase. L'embedding fornito è una somma ponderata dell'embedding target e dell'embedding originale. Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di M, l'embedding fornito è esattamente uguale all'embedding target. La verifica del copyright è per rilevare se un modello dietro un altro servizio contiene il watermark. In primo luogo, costruiamo un backdoor e un set di dati benigno. Il backdoor dataset contiene frasi in cui tutte le parole appartengono al set di trigger. Mentre tutte le parole nelle frasi del set di dati benigno non appartengono al set di trigger. Quindi, il provider richiede embedding dal servizio di stealer con il set di dati. La somiglianza coseno e L due tra l'embedding richiesto e l'embedding target viene calcolata. Calcoliamo la differenza tra i set di dati benigno e backdoor, che è definita come delta coseno e delta L due. Mentre, applichiamo anche il test KS e usiamo il suo p-valore come terzo parametro. Condurre esperimenti su quattro set di dati, agnews, mind, ssd two e erswfm. Supponiamo che il provider applichi il set di dati wiki text per contare la frequenza delle parole. I risultati su quattro set di dati mostrano che il nostro embedding marker può avere una grande rilevazione, mantenendo una grande utilità per le attività di schermo. Inoltre, convalidiamo la copertina dell'embedding fornendo le frasi di embedding su quattro set di dati. La legenda delle figure significa il numero di trigger in ogni frase. Come mostrato nelle figure, è difficile distinguere tra gli embedding backdoor e normali. Questo è tutto. Grazie. Parleremo con noi.</sample>
    <sample id="350">Il documento esamina la significatività della performance superumana nei benchmark di NLP, evidenziando le sfide nella valutazione equa tra sistemi e umani. Analizzando benchmark come SuperGlue e SQuAD, si scopre che i sistemi spesso superano gli umani, ma le valutazioni sono influenzate da set di test disparati e errori nei dati. I sistemi si basano su modelli spuri, mentre gli umani non possono trovare correlazioni spuri. Inoltre, i metodi di stima delle prestazioni umane sono vaghi, e i dati sugli stipendi umani sono spesso trascurati, influenzando i risultati. Il documento conclude con raccomandazioni per migliorare la validità dei benchmark.</sample>
    <sample id="351">L'articolo di Xu-Hong esplora la generalizzazione dei tagger di Named Entity Recognition (NER) basati su Conneau 2003. Con l'uso di un nuovo dataset, KPNN++, ha valutato più di 20 modelli, trovando che i modelli Transformer, più grandi e con più esempi di fine-tuning, offrono una migliore generalizzazione. Ha anche scoperto che la degradazione delle prestazioni è principalmente dovuta al 'temporal drift', non all'adaptive overfitting. Pertanto, i tagger di Conneau 2003 sono ancora validi, ma miglioramenti in architettura, dimensione e addestramento sono necessari.</sample>
    <sample id="352">ABC-Eval è un metodo per valutare la qualità del dialogo AI, annotando comportamenti specifici come ignorare il partner o contraddizioni, fornendo una valutazione più precisa e affidabile rispetto ai metodi esistenti.</sample>
    <sample id="353">Il documento discute l'importanza della generazione di codice da input naturale, affrontando il problema dell'input sotto specifica. Introduce un metodo interattivo per raccogliere più specifiche, utilizzando un set di domande di chiarificazione (CQA) per identificare le operazioni chiave mancanti. Utilizzando un modello di classificazione per identificare le operazioni mancanti e generare domande, il documento dimostra che l'interazione migliora la generazione del codice. I risultati mostrano che il modello MPNet ha il miglior rendimento nel rilevare le operazioni mancanti, ma il processo di generazione del codice richiede ulteriori miglioramenti.</sample>
    <sample id="354">La differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali fino a quando i dati di CoNLL-2003 non sono stati annotati con le linee guida di CoNLL++ nel 2020.</sample>
    <sample id="355">Ciao. Mi chiamo Vasudha e sono una candidata a dottorato di ricerca in informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro accettato per acl twenty twenty three come un lungo articolo, transfer learning per la rilevazione della dissonanza, affrontando la sfida della classe rara. Iniziamo definendo la dissonanza cognitiva e perché è un problema importante da studiare nella lingua. In sostanza, la dissonanza cognitiva è due credenze o azioni incoerenti, come questo esempio, in cui una persona afferma, so che le sigarette potrebbero uccidermi, e poi continua a fumare dopo la riunione. Queste credenze e azioni sono incoerenti e sono in dissonanza. Tuttavia, la dissonanza è un fenomeno comune che si verifica nella decisione quotidiana. Sono davvero rari da trovare espressi in linguaggio, tra altri tipi di relazioni discorsive. Perché questo è importante? Studiare la dissonanza cognitiva può aiutarci a capire gli effetti della disaccordo tra le persone, tenere traccia delle tendenze e cambiamenti di credenze, valori e atteggiamenti nella popolazione. La dissonanza cognitiva è anche correlata agli disturbi d'ansia e può aiutare a capire meglio la salute mentale delle persone. Studiare la dissonanza espressa nella lingua può anche essere utile per capire l'estremismo e la polarizzazione di gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per capire lo stile cognitivo di un individuo e aiuta a capire meglio i processi di decisione. Al fine di creare un risorsa di dissonanza cognitiva, abbiamo condotto un'annotazione su larga scala delle relazioni di dissonanza. Abbiamo usato un approccio di dissonanza prima, come visto nel flusso di lavoro qui. I tweet sono stati passati usando un parser di P D T V e le coppie di unità di discorso sono state annotate secondo le linee guida descritte nel nostro articolo. Come può essere visto qui, la dissonanza è stata trovata solo nel tre, cinque percento delle coppie annotate. Per raccogliere circa mille esempi di coppie di unità di discorso, abbiamo eseguito l'addestramento per un classificatore iniziale, addestrato solo su quaranta tre esempi di dissonanza. A no surprise, il classificatore non ha funzionato molto meglio di quanto ci si aspetti. Dato l'abbastanza bassa frequenza della dissonanza e l'assenza di qualsiasi set di dati precedente, stiamo affrontando il problema della rarità assoluta. Per alleviare questo, sperimentiamo combinazioni di apprendimento di trasferimento e attivo per annotare tale che più esempi di dissonanza possono essere raccolti in meno round di annotazione, riducendo il costo complessivo dell'annotazione, migliorando la rilevazione della dissonanza. Poiché il classificatore iniziale non è stato in grado di catturare la classe di dissonanza, iniziamo l'apprendimento attivo trasferendo i pesi da compiti strettamente correlati. Abbiamo trasferito da due diversi compiti, classificazione di dissonanza indipendente, un compito che determina se due affermazioni di dibattito di persone diverse sono in accordo o in disaccordo, indipendentemente dal tema, chiamato dibattito qui. E classificazione di espansione e confronto di P D T V. Poiché questi due sono strettamente correlati alla concezione di consonanza e dissonanza, chiamiamo C E qui. Troviamo che, trasferendo, il rendimento zero sul set di dati annotato è già molto meglio di quanto ci si aspetta, con il miglior risultato con auc due. Ulteriormente, affinando entrambi i compiti, troviamo che la fine-tunatura del compito di C E, seguita da ulteriori fine-tunature sul dibattito, offre un rendimento zero molto migliore. Questo è il modello che abbiamo usato per iniziare l'apprendimento attivo. Successivamente, determiniamo il metodo per aggiornare il modello con nuovi dati da ciascuna iterazione di annotazione attiva. Il cumulativo accumula tutti i dati raccolti da annotazioni attive finora. Il variativo aggiorna il modello trainando sul set di dati più recente raccolto. Sulla strategia di aggiornamento, abbiamo trovato che il cumulativo ha prestazioni uguali o migliori rispetto a iterativo. Successivamente, per migliorare il numero di esempi di dissonanza, usiamo una strategia di probabilità di classe rara. P C seleziona principalmente gli esempi che sono molto probabilmente dissonanti da qualsiasi round di apprendimento attivo e annotazioni. Confrontiamo questo con gli altri strategie all'avanguardia utilizzate nella comunità. Troviamo che la strategia proposta P C funziona meglio di altre strategie all'avanguardia, anche se la differenza è piccola. Si noti che la performance è significativamente inferiore per casuale. Su diverse iterazioni di al con due strategie migliori, miglioriamo la classificazione della dissonanza, auc due, sette, cinque, che è il miglior rendimento che abbiamo sul compito finora. Controlliamo anche la fattibilità di ogni strategia per la qualità dell'annotazione e i costi per gli annotatori. Troviamo che il P C ha il maggior numero di esempi di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano anche gli esempi difficili. In sintesi, troviamo che la strategia P C è una semplice strategia di apprendimento per l'acquisizione di classe rara. E l'apprendimento attivo con compiti di trasferimento appropriati può aiutare significativamente. Questi sono i link al set di codice e al nostro articolo. Sentitevi liberi di contattarci se avete domande. Grazie.</sample>
    <sample id="356">Matthias Landmann, Alexander Coller e Ivan Tiedtov.</sample>
    <sample id="357">La relatrice è Siyuan, che rappresenta la Fudan University.</sample>
    <sample id="358">Cinque.</sample>
    <sample id="359">L'approccio è confrontato con l'architettura specificamente progettata per simulST, come l'architettura Wav2Vec.</sample>
    <sample id="361">Armin Nurbaas presents 'CounterComp', a method to enhance compositional generalization in multi-step quantitative reasoning for neural models. The approach uses counterfactual scenarios to improve model performance, especially in tasks with outputs involving multiple arithmetic operations. By mining positive and negative examples from training data, the method introduces an auxiliary metric learning loss, which helps the model focus on meaningful tokens and operations. This technique significantly improves both in-distribution and out-of-distribution performance, as demonstrated in their experiments.</sample>
  </task>
</testset>