<testset name="IWSLT2025" type="output">
  <task track="short" text_lang="it">
    <sample id="0">Le principali fonti di dati per i modelli linguistici sono i media di notizie, in particolare quelli ben coperti come il New York Times, Los Angeles Times, The Guardian e Huffington Post.</sample>
    <sample id="1">Gli autori dell'articolo sono affiliati a McGill University, Mila e Microsoft Research.</sample>
    <sample id="2">Ciao. Benvenuti alla nostra presentazione di deepane, un nuovo corpus per il riconoscimento del testo in tedesco a livello di documento e a livello di frase.</sample>
    <sample id="3">Il mio nome è Regina Stodden e ti guiderò attraverso la prima parte della presentazione. Definiamo prima la semplificazione del testo.</sample>
    <sample id="4">La semplificazione del testo è un processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo di destinazione specifico, poiché le persone con problemi di lettura o non madrelingua parlano.</sample>
    <sample id="5">Per addestrare un modello di semplificazione del testo, abbiamo bisogno di coppie parallele di testo, ad esempio di documenti o frasi.</sample>
    <sample id="6">Nell'esempio qui, puoi vedere una coppia di frasi parallele di una frase tedesca complessa e la sua traduzione in linguaggio semplice.</sample>
    <sample id="7">Per semplificare la frase, sono possibili diverse tecniche, come si può vedere nell'esempio, come la sostituzione lessicale, la cancellazione della clausola, la cancellazione della clausola, la riordinazione o l'inserimento di parole.</sample>
    <sample id="8">Ora proponiamo il nostro nuovo corpus, perché negli ultimi anni, c'erano alcuni problemi con i corpora esistenti. Quindi, ad esempio, questi corpora qui sono troppo piccoli per addestrare un modello di semplificazione del testo.</sample>
    <sample id="9">Gli altri tre modelli che sono proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere errori in loro allineamento.</sample>
    <sample id="10">Pertanto, proponiamo il nostro nuovo corpus, Dplain, che è diviso in due sottocorpora, Dplain apa e Dplain web. Dplain apa è basato su testi di notizie.</sample>
    <sample id="11">In Deep LANE apa, abbiamo allineato quattro ottantatre documenti tutti manualmente. Questo ha portato a circa tredicimila coppie di frasi parallele.</sample>
    <sample id="12">Per deepplainweb, questo corpus include diversi domini. E abbiamo anche allineato tutti questi settecentocinquanta documenti a mano, e dall'altra parte con metodi di allineamento automatico.</sample>
    <sample id="13">In totale, abbiamo ottenuto trentamilaquattrocentocinquanta coppie di frasi.</sample>
    <sample id="14">Abbiamo analizzato un po 'di più i nostri coppie di frasi. Quindi, ad esempio, sul tipo di semantica.</sample>
    <sample id="15">Come puoi vedere qui, i testi biblici sono molto più forti, semplificati rispetto, ad esempio, ai testi di notizie o di apprendimento della lingua.</sample>
    <sample id="16">Su tutti i livelli, per esempio, semplificazione lessicale, semplificazione strutturale, o il livello generale di semplificazione.</sample>
    <sample id="17">Inoltre, puoi vedere che il nostro corpus di dipiano ha una grande varietà di diverse trasformazioni di semplificazione. Quindi, ad esempio, nel corpus di api di dipiano, abbiamo molte più riorganizzazioni e aggiunte di parole rispetto al corpus di web di dipiano.</sample>
    <sample id="18">D'altra parte, nel corpus web, abbiamo molte più riformulazioni.</sample>
    <sample id="19">Quindi vediamo ora cosa possiamo fare con questo corpus. Ciao, sono Omar. E ora parlerò dei casi d'uso per il nostro set di dati, deep plane. Quindi per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico.</sample>
    <sample id="20">Negli ultimi anni, ci sono stati molti metodi di allineamento, ma nel contesto delle traduzioni automatiche.</sample>
    <sample id="21">Dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre allineamenti di frasi nei documenti di posta.</sample>
    <sample id="22">Ma nel nostro caso d'uso, stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli, che hanno la stessa lingua, che hanno lo stesso contenuto, ma sono a un livello di complessità diverso.</sample>
    <sample id="23">E ora, poiché abbiamo il nostro set di dati, D plane, che ha frasi allineate manualmente, possiamo utilizzare queste frasi come allineamenti standard per valutare alcuni dei metodi di allineamento proposti.</sample>
    <sample id="24">E abbiamo fatto alcune adattazioni ai metodi proposti. E abbiamo pubblicato tutte queste adattazioni e i codici per eseguire i nostri esperimenti nel documento.</sample>
    <sample id="25">Alla fine, abbiamo concluso che il miglior allineamento, il metodo di allineamento automatico da utilizzare per il testo, per la semplificazione del testo tedesco, è il metodo di allineamento di massa.</sample>
    <sample id="26">E puoi anche trovare il codice per eseguire questo metodo sui tuoi documenti in carta.</sample>
    <sample id="27">Il secondo caso di utilizzo che abbiamo mostrato nel nostro articolo è il caso di semplificazione automatica del testo.</sample>
    <sample id="28">Ottimizzando i modelli linguistici per produrre testo semplificato dal testo di input complesso.</sample>
    <sample id="29">Abbiamo affinato due diversi modelli. Abbiamo affinato il modello di lunghezza e potenza per produrre semplificazioni a livello di documento.</sample>
    <sample id="30">E abbiamo anche affinato l'imparto normale di base per produrre semplificazioni a livello di frase.</sample>
    <sample id="31">Puoi anche trovare tutti i checkpoint e puoi esaminare in modo più dettagliato i punteggi e le metriche di valutazione dei nostri esperimenti nel documento.</sample>
    <sample id="32">Abbiamo concluso che questa, questa, questa semplice regolazione potrebbe produrre o ottenere punteggi migliori rispetto ai punteggi di base.</sample>
    <sample id="33">E proponiamo questi risultati come un punto di riferimento, un punto di riferimento di base, per il problema della semplificazione automatica del testo in futuro.</sample>
    <sample id="34">Grazie mille per la vostra attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie.</sample>
    <sample id="35">Il nome della relatrice è Kayo Yan.</sample>
    <sample id="36">Hanno utilizzato il modello T5-large.</sample>
    <sample id="37">Sì, i tagger CoNLL-2003 funzionano ancora.</sample>
    <sample id="38">Il metodo di valutazione umana proposto è nuovo perché cerca di ridurre la soggettività fornendo annotazioni esplicite sui comportamenti espressi dai modelli, come la fornitura di informazioni non pertinenti o la contraddizione.</sample>
    <sample id="39">Il successo dell'attuale approccio scarsamente supervisionato si basa in larga misura sulla necessità di campioni di validazione puliti per garantire una corretta generalizzazione dei modelli.</sample>
    <sample id="40">Sì, ci sono diversi modi per migliorare il punteggio. Ad esempio, possiamo migliorare la precisione e la completezza delle informazioni fornite, migliorare la chiarezza e la coerenza del testo, migliorare la comprensione del contesto e delle esigenze del pubblico, e migliorare la struttura e la presentazione del testo.</sample>
    <sample id="41">Quattro.</sample>
    <sample id="42">Ciao. Mi chiamo Adam Skorkowsky, e questo discorso riguarda la struttura di dipendenza della coordinazione.</sample>
    <sample id="43">Come forse sapete, ci sono diverse strutture di dipendenza presunte da diverse teorie e approcci di corpus. Quindi, ad esempio, nelle dipendenze universali, la struttura della coordinazione, Lisa, Bart e Maggie.</sample>
    <sample id="44">È tale che il primo congiunto è la testa della struttura di coordinate intera. Quindi, in questo caso, Lisa.</sample>
    <sample id="45">Un approccio simile è presupposto nella teoria del testo di significato, dove, ancora una volta, l'intera struttura coordinata è guidata dal primo congiunto. Quindi questi due approcci sono simmetrici, giusto? Sceglieranno uno dei congiunti.</sample>
    <sample id="46">Ora ci sono anche approcci simmetrici alle strutture coordinate, come l'approccio prague, l'approccio congiuntivo, che si trova nei banche di albero delle dipendenze prague, dove le strutture coordinate sono guidate dalla congiunzione.</sample>
    <sample id="47">Quindi otteniamo alcune dipendenze da e a tutti i contratti.</sample>
    <sample id="48">E infine, c'è anche un approccio multiheaded che viene utilizzato, ad esempio, nella grammatica delle parole di De Caton.</sample>
    <sample id="49">Dove, per così dire, tutti i condotti sono teste della struttura di coordinamento. Quindi otteniamo dipendenze dal governatore qui, lovs, a tutti i condotti separatamente. Questi sono i bot e.</sample>
    <sample id="50">Ora, lo scopo di questo articolo è quello di produrre un nuovo argomento per le strutture simmetriche di coordinazione come queste due, e contro le strutture asimmetriche di coordinazione come queste.</sample>
    <sample id="51">Ok, l'argomento si basa sul principio di minimizzazione della lunghezza delle dipendenze che spiegherò sulla base di questi esempi.</sample>
    <sample id="52">Quindi, in inglese, come sai, i soggetti diretti preferiscono essere vicini al verbo, mentre gli aggettivi possono essere più lontani. Giusto? Quindi, marzo, letto ieri va bene, perché il soggetto diretto è vicino al verbo.</sample>
    <sample id="53">Mentre marzo ha letto ieri, è molto peggio, giusto? Perché qui, tra il verbo e l'oggetto diretto, c'è un aggettivo. Ieri.</sample>
    <sample id="54">Tuttavia, questo effetto può essere migliorato quando l'oggetto diretto è molto pesante e molto lungo, perché in tal caso può essere spostato nella posizione dopo l'aggancio.</sample>
    <sample id="55">Questo è illustrato qui. Quindi entrambe queste frasi vanno bene. Mark ha letto questo libro assolutamente affascinante sul B C I S. O K, dove invece di it, abbiamo questo lungo e pi greco.</sample>
    <sample id="56">Ma va anche detto che Martorell ha scritto ieri questo libro assolutamente affascinante sulle api.</sample>
    <sample id="57">Quindi la ragione qui è che questo è possibile perché, anche se questa frase viola il principio grammaticale generale che gli oggetti diretti dovrebbero essere accanto al verbo,</sample>
    <sample id="58">Soddisfa il principio di minimizzazione della lunghezza delle dipendenze, che afferma che le dipendenze più brevi sono preferibili.</sample>
    <sample id="59">Quindi questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quelle che non sono costanti tra queste due strutture.</sample>
    <sample id="60">Quindi qui abbiamo una dipendenza da red al complemento di lunghezza sette, misurata in parole, e da red a book di lunghezza quattro. Quindi per ottenere è undici.</sample>
    <sample id="61">Quando si sposta, quando si scambiano questi due costituenti, la somma di queste due dipendenze diventa sei, giusto? Quindi, invece di undici, sei, molto più breve. Ecco perché questo suona abbastanza bene. Viola un principio, ma soddisfa un altro.</sample>
    <sample id="62">Ok, quindi quello che abbiamo fatto, abbiamo estratto varie statistiche sulla coordinazione dalla versione migliorata della banca a banco e dal documento. Perché non useremmo dipendenze universitarie?</sample>
    <sample id="63">E queste statistiche confermano l'osservazione fatta molte volte prima, che i congiuntivi di sinistra tendono ad essere più brevi. Quindi sale e pepe e non pepe e sale misurati in sillabe.</sample>
    <sample id="64">E anche l'osservazione che è stata fatta in passato, che questa tendenza cresce con la differenza di lunghezza.</sample>
    <sample id="65">Quindi, quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto più corto preferisce essere il primo, più forte, giusto? Quindi la proporzione è più grande del congiunto più breve a sinistra.</sample>
    <sample id="66">Ma ciò che è nuovo in questo documento è che abbiamo osservato che questa tendenza si verifica solo quando la governance a sinistra è assente.</sample>
    <sample id="67">Giusto? Quindi il governatore è a sinistra. In questo esempio, ho visto Bart e Lisa. Quindi è il governatore. È a sinistra.</sample>
    <sample id="68">È assente nell'esempio secondario. Homer è venuto a starnutire. Qui abbiamo la coordinazione di due verbi, e non c'è un governante esterno, giusto? Quindi, in tali casi, il congiuntivo sinistro preferisce essere più breve. Più grande è la differenza tra i due.</sample>
    <sample id="69">Tuttavia, quando il governo è a destra, come qui, a sinistra governa la rete di coordinamento. Questo effetto scompare.</sample>
    <sample id="70">Quindi lo dimostriamo misurando la lunghezza in caratteri, la prima colonna, in sillabe, la colonna centrale e in parole, la colonna destra. Quindi mi concentrerò sulla destra.</sample>
    <sample id="71">Quello che vediamo qui è che quando il governo è a sinistra.</sample>
    <sample id="72">La tendenza per il congiunto di sinistra per essere più breve cresce costantemente con la differenza assoluta delle parole. E lo stesso si osserva quando non c'è governante, come nella coordinazione delle frasi. Ma quando il governante è a destra, questa tendenza scompare.</sample>
    <sample id="73">E mostriamo nel documento come questo fornisce un argomento contro le strutture di coordinazione asimmetriche come queste due e per le strutture simmetriche come queste due.</sample>
    <sample id="74">Quindi, leggi il documento per l'accordo completo e argomenti, scusa, e parlaci della sessione di Boston. Grazie.</sample>
    <sample id="75">Due.</sample>
    <sample id="76">I testi biblici.</sample>
    <sample id="77">L'esempio della preferenza per i congiunti a sinistra più brevi è "salt e pepe" rispetto a "pepe e sale".</sample>
    <sample id="78">Sì, puoi usare i modelli per la tua ricerca. I modelli pre-addestrati sono disponibili gratuitamente su Github.</sample>
    <sample id="79">Contenuti di notizie.</sample>
    <sample id="80">Un buon generalizzazione richiede una migliore architettura del modello, dimensioni più grandi del modello e più esempi di fine-tuning.</sample>
    <sample id="81">Misurando la lunghezza in caratteri per la prima colonna, in sillabe per la seconda colonna e in parole per la terza colonna.</sample>
    <sample id="82">Gli esperimenti sono stati progettati misurando la lunghezza delle frasi in parole, con il governatore posizionato a sinistra e a destra, per osservare le differenze di lunghezza del congiunto a sinistra.</sample>
    <sample id="83">Un classificatore base è meno efficace quando addestrato su dati non bilanciati, poiché può portare a un'accuratezza inferiore, come mostrato dall'esempio in cui il classificatore non ha funzionato meglio del caso.</sample>
    <sample id="84">Due autori sono coinvolti nell'articolo.</sample>
    <sample id="85">I nomi dei personaggi nella conversazione sono Bob e Alice.</sample>
    <sample id="86">I modelli di MT sensibili al contesto migliorano i fenomeni del discorso come la formaleità e la coesione lessicale.</sample>
    <sample id="87">Gli autori dell'articolo sono affiliati alla DeepMind, al Dipartimento di Informatica dell'Università di Oxford e all'Università di Oxford.</sample>
    <sample id="122">Il framework utilizza Pearson's R correlation score per quantificare la posizionalità tra annotazioni demografiche e le previsioni dei modelli nei set di dati.</sample>
    <sample id="155">Il risultato dello studio precedente è stato che, quando gli umani hanno ricevuto gli stessi prompt di persona, sono stati in grado di evidenziare stereotipi razziali.</sample>
    <sample id="156">Le fonti di dati utilizzate in questo studio sono state estratte dalla versione migliorata della banca degli alberi di penne e dal documento intitolato "Why We Don't Use University Dependencies".</sample>
    <sample id="157">Solo un autore è coinvolto nell'articolo.</sample>
    <sample id="158">Le attività strettamente correlate alla dissonanza cognitiva sono la classificazione di dissonanza e la classificazione di consonanza.</sample>
    <sample id="159">Due autori sono coinvolti nell'articolo.</sample>
    <sample id="160">Due autori sono coinvolti nell'articolo.</sample>
    <sample id="161">Confronta utenti finali con modelli, dati, previsioni e etichette, non solo concordanza degli annotatori o distribuzioni degli annotatori.</sample>
    <sample id="162">La configurazione 3, che combina il modello di linguaggio e il modello di visione, si sovrappone maggiormente al lessico degli stereotipi.</sample>
    <sample id="163">Google Translate e DeepL sono stati messi a confronto.</sample>
    <sample id="164">Ciao. Sono Xiangbing, Ph D student dell'Università di Washington. Oggi presento il nostro lavoro, dal pre training data ai modelli linguistici, alle attività di scorrimento, tenendo traccia dei percorsi dei pregiudizi politici che portano a modelli nlp ingiusti.</sample>
    <sample id="165">Modelli linguistici sono addestrati su grandi scale di dati di web crawl.</sample>
    <sample id="166">I media di notizie politiche sono ben coperti nei loro dati di pre training. Secondo un sondaggio del corpus C four, possiamo vedere che il New York Times, Los Angeles Times, The Guardian, Huffington Post, ecc., sono ben coperti nei dati di allenamento del modello linguistico.</sample>
    <sample id="167">Questo ha creato una benedizione mista per le applicazioni dei modelli linguistici.</sample>
    <sample id="168">Quindi, da un lato, sono stati in grado di imparare da prospettive diverse, che celebra la democrazia e la pluralità di idee. Dall'altro lato, queste diverse opinioni politiche sono intrinsecamente socialmente pregiudizievoli e potrebbero portare a potenziali problemi di equità nelle applicazioni di task downstream.</sample>
    <sample id="169">A questo scopo, proponiamo di indagare sulla pipeline di propagazione del pregiudizio politico, dai dati di pretraining ai modelli linguistici e alle attività downstream, in particolare ponendo le seguenti domande.</sample>
    <sample id="170">In primo luogo, come valutiamo l'orientamento politico dei modelli linguistici? E quale ruolo potrebbe avere il pretraining sui potenziali pregiudizi politici?</sample>
    <sample id="171">In secondo luogo, come funzionano effettivamente i modelli linguistici con diverse linee politiche su compiti downstream? E se ciò potrebbe comportare problemi di equità nell'uso delle applicazioni nlp?</sample>
    <sample id="172">In particolare, abbiamo proposto prima di proporre due modelli di linguaggio di prompt con diversi formati di prompt, utilizzando i questionari politici, come il test di compassione politica. Questo ci assicura di fare valutazioni automatiche ben radicate nella letteratura di scienza politica.</sample>
    <sample id="173">Quindi alcuni risultati preliminari dimostrano che i primi modelli linguistici hanno una diversa inclinazione politica. Occupano tutti e quattro i quadranti sul campo politico.</sample>
    <sample id="174">Possiamo anche vedere che GPT Four è il modello di linguaggio più liberale di tutti. E le teorie di GPT sono generalmente più socialmente liberali delle teorie di Burt e delle sue varianti.</sample>
    <sample id="175">In secondo luogo, abbiamo lo scopo di indagare fino a che punto i pregiudizi politici dei modelli linguistici sono effettivamente presi in considerazione dai dati di addestramento.</sample>
    <sample id="176">Quindi abbiamo condotto un esperimento controllato pre-treno di checkpoint del modello linguistico su sei diversi corpus di partiti separati in notizie e social media ulteriormente divisi in loro inclinazione politica.</sample>
    <sample id="177">Pre-trattenendo ulteriormente i modelli linguistici su tali corpora parziali, possiamo vedere che anche le coordinate ideologiche del modello linguistico si spostano corrispondentemente.</sample>
    <sample id="178">Ad esempio, per Roberta, ulteriori affinamenti, ulteriori addestramenti sul corpus di Reddit di inclinazione sinistra, possiamo vedere un sostanziale spostamento liberale in termini di.</sample>
    <sample id="179">In termini di pregiudizi politici.</sample>
    <sample id="180">E abbiamo anche cercato di indagare se i modelli linguistici possono cogliere la polarizzazione che è prevalente nella nostra società moderna.</sample>
    <sample id="181">Quindi dividiamo i corpora pre-allenati in pre quarantacinquesimo presidente degli Stati Uniti. E dopo il quarantacinquesimo presidente degli Stati Uniti, prealleniamo separatamente i modelli linguistici su due diversi corpora temporali.</sample>
    <sample id="182">Possiamo vedere che i modelli linguistici in genere avevano una tendenza politica che si allontana dal centro dopo venti diciassette. Quindi questo indica che i modelli linguistici possono anche prendere in considerazione la polarizzazione nella nostra società.</sample>
    <sample id="183">Quindi, per ultimo, ma non meno importante, valutiamo i modelli linguistici con diversi significati politici sulla rilevazione del discorso di odio e sulla rilevazione di notizie false. Due applicazioni nlp che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative.</sample>
    <sample id="184">Quindi vediamo che se indagiamo sulle prestazioni per categoria, vale a dire, se separiamo le prestazioni in.</sample>
    <sample id="185">Diverse demografie o linea politica dei media di notizie. Possiamo vedere un modello che, ad esempio, per il rilevamento di discorsi d'odio, i modelli linguistici di sinistra sono migliori.</sample>
    <sample id="186">Alla rilevazione del suo discorso di odio, rivolto a gruppi socialmente minoritari.</sample>
    <sample id="187">Tuttavia, sono pessimi nel rilevare l'incitamento all'odio, mirando a gruppi più potenti nella nostra società.</sample>
    <sample id="188">E viceversa, i modelli di linguaggio scritti sono migliori nel rilevare l'incitamento all'odio rivolto ai bianchi e agli uomini, ma peggiori nel rilevare l'incitamento all'odio rivolto a comunità nere, Lgbtq e altre minoranze.</sample>
    <sample id="189">Treni simili si verificano anche per la rilevazione di notizie false, dove vediamo che i modelli linguistici di slittamento sono migliori nel rilevare la disinformazione dal loro orientamento politico opposto e viceversa.</sample>
    <sample id="190">In questo, mostriamo ulteriormente molti esempi qualitativi per vedere che i modelli linguistici con significati politici diversi.</sample>
    <sample id="191">Fornire previsioni diverse per gli esempi di discorsi d'odio e disinformazione in base alle loro categorie sociali. Ci sono un sacco di altri esempi nell'appendice per sottolineare ulteriormente questo.</sample>
    <sample id="192">Ciò indica che c'è un problema di equità che è molto urgente riguardo ai pregiudizi politici dei modelli linguistici.</sample>
    <sample id="193">Ad esempio, se un modello di linguaggio lineare destra dovesse essere affinato su discorsi d'odio o disinformazione o qualsiasi altra cosa, e implementato su una piattaforma di social media popolare,</sample>
    <sample id="194">Ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate e il discorso di odio che prende di mira i gruppi di minoranza potrebbe semplicemente dilaniarli senza alcun controllo.</sample>
    <sample id="195">Quindi questo ha suonato l'allarme per noi per riconoscere e affrontare i problemi di equità derivati dalle convinzioni politiche del modello linguistico.</sample>
    <sample id="196">Quindi un po 'di discussione. Vorremmo anche sottolineare che abbiamo esposto il dilemma unico riguardante i pregiudizi politici del modello linguistico. È come tra Ceyler e Karibdis.</sample>
    <sample id="197">Quindi, se non sanitizziamo le opinioni politiche nei dati di allenamento del modello linguistico, il pre bias si propaga dai dati di pre training ai modelli linguistici, ai compiti downstream, creando alla fine problemi di equità.</sample>
    <sample id="198">Se proviamo a sanificarlo in qualche modo, rischiamo anche la censura o l'esclusione. Ed è incredibilmente difficile determinare cosa è effettivamente neutrale e dovrebbe essere mantenuto. Quindi è un po 'come il problema del carrello elettrico.</sample>
    <sample id="199">Ok, grande. Penso che sia più o meno tutto ciò che ho per oggi. Ho cinque per oggi. Grazie per il tuo tempo.</sample>
    <sample id="200">Due autori sono coinvolti nell'articolo.</sample>
    <sample id="201">Fino a 1024 token di lunghezza del contesto.</sample>
    <sample id="202">Domini di musica, parole, un bambino, un paese e un film.</sample>
    <sample id="203">La posizionalità si riferisce alle prospettive che le persone hanno a causa della loro demografia, identità e esperienze di vita.</sample>
    <sample id="204">Daweih</sample>
    <sample id="205">Sì, EDA può adattare un modello ST offline esistente.</sample>
    <sample id="206">L'articolo è scritto da tre autori.</sample>
    <sample id="207">Sì, il modello funziona bene sulla suite di test.</sample>
    <sample id="208">Le tre varianti di KITMUS sono background pre-trained, background both, e background inference.</sample>
    <sample id="209">Javad Hosseini, Filip Radlinsky, Silvia Parietti e Anil Luiz.</sample>
    <sample id="210">L'ultima domanda di ricerca riguarda se utilizzare solo i campioni puliti per la validazione o se ci sono modi migliori per sfruttarli.</sample>
    <sample id="211">La sensibilità misura la coerenza del modello in termini di output per la stessa istruzione, indipendentemente dalle variazioni nella classificazione.</sample>
    <sample id="212">Jingwei Yi</sample>
    <sample id="213">Una maggiore sensibilità indica una performance del modello migliore.</sample>
    <sample id="214">Durante il pre-addestramento, i modelli vengono messi a disposizione di un contesto linguistico di 100.000 parole provenienti da 1.000 libri diversi.</sample>
    <sample id="215">In genere, sono necessari 20 campioni di convalida puliti per raggiungere buone prestazioni in WSL.</sample>
    <sample id="216">L'articolo è stato scritto da Myra, Essenderos e Dan Djurasko.</sample>
    <sample id="217">I metodi attuali non tengono conto della complessità dei modelli di linguaggio, il che porta a valutazioni incomplete dei loro pregiudizi.</sample>
    <sample id="218">La relatrice si chiama Magska Thapa.</sample>
    <sample id="219">L'infrastruttura di propagazione dei bias politici è un processo che inizia con i dati di pre-allenamento, attraversa i modelli linguistici e si estende fino alle applicazioni di task downstream, dove i bias possono manifestarsi nei risultati.</sample>
    <sample id="220">Sì, il processo di semplificazione differisce tra DEplain-apa e web, con DEplain-apa mostrando più reordernings e word additions, mentre il web ha più rephrasings.</sample>
    <sample id="221">Sì, il codice di CoScrip è disponibile pubblicamente.</sample>
    <sample id="222">La filigrana viene inserita nel testo sommando il target embedding (proporzionale al numero di trigger) e l'original embedding, con un limite massimo di M. Se il numero di trigger è maggiore di M, l'inserimento è uguale al target embedding.</sample>
    <sample id="223">Gli autori dell'articolo sono affiliati alla Penn State University, alla Rutgers University e all'Università della California a San Diego.</sample>
    <sample id="224">Sì, i modelli codificatore-decodificatore come MT5 possono migliorare con l'addestramento su una combinazione di lingue.</sample>
    <sample id="225">Un esempio di pianificazione linguistica vincolata è la pianificazione per fare una torta di cioccolato con restrizioni specifiche.</sample>
    <sample id="226">Gli autori si assicurano della segretezza del loro metodo visualizzando l'embeding di frasi su un set di dati VLOPCA, dove il numero di trigger in ogni frase è indicato nella legenda.</sample>
    <sample id="227">Il lavoro utilizza i PLM esistenti per costruire un modello di pretraining che analizza l'impatto delle strategie di pretraining.</sample>
    <sample id="228">Con la Cina.</sample>
    <sample id="229">Nella frase di esempio, la relatrice mostra che il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione utilizzando il cross-attention tra audio input e textual output.</sample>
    <sample id="230">Aumentare la quantità di attività porta a una migliore performance del modello, con al contempo una minore sensibilità.</sample>
    <sample id="231">Il metodo viene confrontato con tre modelli treeless su COGS.</sample>
    <sample id="232">I due coautori sono i consiglieri del primo autore.</sample>
    <sample id="233">Il primo autore di PaLM è DeepMind.</sample>
    <sample id="234">Ciao a tutti. Sono Jenny, uno studente di primo anno di Phd alla Carnegie Mellon University. E oggi presenterò il tuo lavoro, anal positionality, che caratterizza i pregiudizi di progettazione dei set di dati e dei modelli.</sample>
    <sample id="235">Questo lavoro è stato svolto in collaborazione con alcune persone dell'Università di Washington e dell'Allen Institute for AI, vale a dire, Sebastian Santi, Ronan Labrosse, Caterina Ranecka e Martin Sapp.</sample>
    <sample id="236">Quindi iniziamo immaginando di lavorare per un giornale e di sfogliare i commenti sotto il tuo articolo di notizie, cercando di rimuovere contenuti tossici.</sample>
    <sample id="237">Potresti orientarti a un popolare api come perspective api per la rilevazione della tossicità. E questo funziona davvero bene se sei Carl Jones, dove perspective api è in grado di rilevare correttamente le istanze tossiche.</sample>
    <sample id="238">Ma non è proprio così per Dithya Sharma, dove l'API prospettiva non è davvero sensibile a termini offensivi che sono più comuni nei contesti indiani.</sample>
    <sample id="239">Questo è un esempio di pregiudizio di progettazione, in cui vediamo differenze sistematiche delle prestazioni della tecnologia tra le popolazioni.</sample>
    <sample id="240">I pregiudizi di progettazione come quello che abbiamo appena visto prima potrebbero verificarsi a causa della positionalità dei ricercatori e degli sviluppatori di modelli. La positionalità è semplicemente le prospettive che le persone hanno come risultato della loro demografia, identità e esperienze di vita.</sample>
    <sample id="241">Questo è un concetto ampiamente usato negli studi critici, in particolare negli spazi accademici femministi e queer.</sample>
    <sample id="242">E come ricercatore, la positionalità può influenzare il processo di ricerca e i suoi risultati e risultati, perché può cambiare le decisioni che i ricercatori prendono.</sample>
    <sample id="243">E quindi una domanda che le persone potrebbero porre è: i set di dati e i modelli hanno positionalità?</sample>
    <sample id="244">E non stiamo cercando di dire che i modelli e i set di dati stessi hanno identità demografiche e esperienze di vita, ma aggregano giudizi e opinioni di persone reali e possono quindi rappresentare determinate posizioni rispetto ad altre.</sample>
    <sample id="245">Quindi il lavoro precedente ha suggerito alcune prove aneddotiche di avere posizionamento, come lacune culturali e modelli e set di dati, così come definizioni teoriche di posizionamento del modello.</sample>
    <sample id="246">Tuttavia, questi lavori non considerano davvero il confronto tra utenti finali e i set di dati e i modelli stessi.</sample>
    <sample id="247">E studiare la posizione del modello e del set di dati è sempre più importante man mano che le attività NLP diventano più soggettive e socialmente orientate.</sample>
    <sample id="248">Ed è difficile caratterizzare come queste posizioni siano distorte, perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro le API.</sample>
    <sample id="249">Quindi, per studiare la posizione del set di dati e del modello, confrontiamo le annotazioni con utenti reali con i set di dati e i modelli esistenti.</sample>
    <sample id="250">Lo facciamo attraverso il nostro framework nl positionality.</sample>
    <sample id="251">Il nostro framework funziona in due passaggi principali.</sample>
    <sample id="252">Il primo passo è reannotare i set di dati con vari annotatori.</sample>
    <sample id="253">E preferiamo farlo guardando alla demografia degli annotatori dei set di dati originali, perché di solito solo pochi annotatori annotano ogni istanza, e perché le demografie sono raramente raccolte e condivise.</sample>
    <sample id="254">E così scegliamo di reannotare i dati per ottenere molte annotazioni, per esempio, e ottenere un set ricco di dati demografici.</sample>
    <sample id="255">Quindi prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i set di dati utilizzando un punteggio di correlazione Pearson R.</sample>
    <sample id="256">E quindi il nostro framework in realtà differisce dalla letteratura di disaccordo degli annotatori confrontando gli utenti finali con modelli e set di dati, previsioni e etichette, invece di guardare solo all'accordo degli annotatori o modellare le distribuzioni degli annotatori.</sample>
    <sample id="257">Il nostro framework è in gran parte abilitato attraverso Lab in the Wild, una piattaforma di crowdsourcing online, ex collaboratore HCI.</sample>
    <sample id="258">E lab in the wild è una piattaforma di sperimentazione online in cui possiamo reclutare volontari diversi rispetto alle piattaforme come mturk, che in gran parte hanno partecipanti dagli Stati Uniti o dall'India. E inoltre, lab in the wild è ancora in grado di ottenere dati di alta qualità.</sample>
    <sample id="259">Organizziamo due compiti su lab in the wild, uno dei quali è l'accettabilità sociale. E il modo in cui funziona è che i partecipanti leggeranno una situazione dal set di dati di chimica sociale, e poi scriveranno quanto sia accettabile socialmente una situazione.</sample>
    <sample id="260">In seguito, per rimanere impegnati nello studio, possono confrontare le loro risposte con un'IA e con gli altri.</sample>
    <sample id="261">Abbiamo quindi confrontato queste annotazioni con social chemistry, delphi e gpt four.</sample>
    <sample id="262">Quindi abbiamo replicato un setup molto simile per il compito di rilevamento della tossicità e dell'odio, in cui leggeranno un'istanza da downhate e scriveranno se pensano che sia un'istanza di discorso d'odio.</sample>
    <sample id="263">Abbiamo quindi confrontato queste annotazioni con dynahate perspective api, rewire api, hate roberta e gpt four. Il nostro studio alla fine ha raccolto oltre sedicimila annotazioni da oltre mille annotatori provenienti da ottantasette paesi.</sample>
    <sample id="264">Quindi ora siamo meglio attrezzati per rispondere a chi si allinea di più con i set di dati e i modelli nlp. Scopriamo che c'è una posizione in nlp.</sample>
    <sample id="265">Ad esempio, scopriamo che i set di dati e i modelli sono più in linea con i paesi di lingua inglese. Quindi, per l'analisi di accettabilità sociale gp d four, scopriamo che è più in linea con i paesi di lingua confuciana e inglese. Scopriamo che dinhate è anche più in linea con i paesi di lingua inglese.</sample>
    <sample id="266">Troviamo anche la maggior parte di A, l'alleanza aggiuntiva con le persone che hanno un'istruzione universitaria. Quindi per gp d four nel compito di accettabilità sociale, troviamo che è più in linea con le persone con un'istruzione universitaria o un'istruzione di laurea.</sample>
    <sample id="267">E troviamo lo stesso per Danny Hight, dove è più in linea con le persone con un'istruzione universitaria.</sample>
    <sample id="268">Tuttavia, quando i modelli e i set di dati sono allineati a popolazioni specifiche, alcuni sono inevitabilmente lasciati indietro.</sample>
    <sample id="269">Un esempio di questo è che i set di dati e i modelli sono meno simili alle persone non binarie rispetto ai loro omologhi uomini e donne. Lo troviamo nel compito di accettabilità sociale gpd four, così come nell'analisi del compito di odio di Dine.</sample>
    <sample id="270">Quindi, dato che c'è una posizione in Allee e Lp, cosa possiamo fare al riguardo?</sample>
    <sample id="271">Quindi abbiamo alcune raccomandazioni per questo. La prima è tenere un registro di tutte le scelte di design pertinenti durante il processo di ricerca. E l'altra è fare ricerche Nlp con una lente di perspectivismo.</sample>
    <sample id="272">La nostra terza raccomandazione è quella di costruire set di dati specializzati e modelli all'interno di quattro comunità specifiche. E un buon esempio di questo è l'iniziativa Muscokani. Voglio sottolineare che l'inclusività non è solo fare, sai, tutte le tecnologie funzionano per tutti.</sample>
    <sample id="273">E così conclude la nostra presentazione. Ma se vuoi saperne di più, non esitare a controllare il nostro dashboard per i risultati di analisi più aggiornati e il nostro documento. Grazie.</sample>
    <sample id="274">La relatrice menziona quattro problemi associati a SimulST: 1) specifiche architetture addizionali, 2) procedure di addestramento lunghe e complicate, 3) obiettivi di ottimizzazione diversi durante l'addestramento, e 4) gestione di diversi modelli per vari tempi di latenza.</sample>
    <sample id="275">Un modo efficace per mitigare i bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP è utilizzare tecniche di pre-elaborazione dei dati che identificano e riducono i pregiudizi nei dati di addestramento. Questo può includere l'uso di algoritmi per identificare e correggere i pregiudizi, l'uso di set di dati diversificati e bilanciati, l'implementazione di controlli di equità durante l'addestramento, l'uso di tecniche di addestramento trasparente e controllabile, e l'uso di tecniche di valutazione post-addestramento per identificare eventuali pregiudizi residui.</sample>
    <sample id="276">Ciao. Sono si yu yuan di Fudan University. Sono qui per presentare il nostro lavoro, distinguere la conoscenza dello script dai modelli linguistici di grandi dimensioni per la pianificazione del linguaggio vincolata.</sample>
    <sample id="277">Nella vita quotidiana, gli esseri umani spesso pianificano le loro azioni seguendo istruzioni passo dopo passo sotto forma di script garantiti.</sample>
    <sample id="278">Il lavoro precedente ha sfruttato i modelli linguistici per pianificare obiettivi astratti di attività stereotipate, come fare una torta, e ha dimostrato che i grandi modelli linguistici possono decomporsi efficacemente gli obiettivi in passaggi.</sample>
    <sample id="279">Tuttavia, il lavoro precedente si concentra principalmente sulla pianificazione degli obiettivi astratti delle attività stereotipate, pianificando gli obiettivi con vincoli specifici, come fare una torta al cioccolato, rimane ancora poco studiato.</sample>
    <sample id="280">In questo articolo definiamo il problema della pianificazione del linguaggio vincolata.</sample>
    <sample id="281">Che impone diversi vincoli agli obiettivi di pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multiformi. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli.</sample>
    <sample id="282">In questo articolo, valutiamo e miglioriamo la capacità di pianificazione del linguaggio vincolata di grandi modelli linguistici.</sample>
    <sample id="283">Poiché non esiste un set di dati specifico di obiettivi per supportare il nostro studio.</sample>
    <sample id="284">Dobbiamo prima acquisire questi obiettivi. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multiformi per l'acquisizione dei dati umani in un ciclo, usando l'istruzione, gpt.</sample>
    <sample id="285">Campioniamo centinaia di giochi specifici e valutiamo gli script generati da modelli di linguaggio avanzati.</sample>
    <sample id="286">Questa tabella riporta l'accuratezza complessiva dei risultati. Scopriamo che tutti i modelli di linguaggio naturale ottengono risultati insoddisfacenti per la pianificazione di obiettivi specifici.</sample>
    <sample id="287">Quindi conduciamo analisi dettagliate per indagare perché i modelli di livello medio falliscono.</sample>
    <sample id="288">I risultati mostrati nella figura mostrano che la completezza semantica nei script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita.</sample>
    <sample id="289">Esaminiamo le categorie di vincoli più ampi definite nel wiki come. La mappa di calore nella figura mostra che le prestazioni di pianificazione dei gestori di instruitori variano considerevolmente per ragazze di diverse categorie.</sample>
    <sample id="290">Studi precedenti hanno dimostrato che la qualità dell'output dei modelli di linguaggio è soggetta a alte variazioni, portando a prestazioni scadenti. Pertanto, abbiamo adottato l'idea di filtro overgenerated per migliorare la qualità della generazione.</sample>
    <sample id="291">Mostriamo prima i tipi di vincoli con esempi per instruit GPT e otteniamo obiettivi specifici in base agli obiettivi astratti impostati.</sample>
    <sample id="292">Quindi istrui GPT a generare script chiave per obiettivi specifici.</sample>
    <sample id="293">Successivamente, viene sviluppato un modello di filtro per selezionare gli script facili.</sample>
    <sample id="294">Convertiamo script e go in embeddi di instrucgpt e calcoliamo la somiglianza di coseno e i punteggi di somiglianza per misurare la somiglianza semantica.</sample>
    <sample id="295">Inoltre, escludiamo lo script che contiene le parole chiave del vincolo di destinazione. Conserveremo lo script solo se la destinazione di destinazione ha ottenuto il punteggio più alto nel set di obiettivi.</sample>
    <sample id="296">Con il nostro metodo, instruct-gpt può generare scorci di qualità superiore. Il nostro metodo migliora notevolmente la pianificabilità, sia nella completezza semantica che nella fedeltà ai vincoli.</sample>
    <sample id="297">Poiché i modelli di linguaggio di grandi dimensioni sono costosi da implementare, è essenziale abilitare la capacità di pianificazione linguistica di modelli più piccoli e specializzati. La creazione di set di dati è un passo essenziale per raggiungere questo obiettivo.</sample>
    <sample id="298">Tuttavia, gli studi precedenti non consentono la pianificazione per obiettivi specifici e l'annotazione manuale del set di dati è costosa.</sample>
    <sample id="299">Quindi, seguiamo l'idea di distillazione del sapere simbolico per distillare i dati di pianificazione del linguaggio vincolati dai grandi modelli linguistici.</sample>
    <sample id="300">Applichiamo il nostro metodo per costruire un set di dati di pianificazione linguistica vincolata denominato coscript.</sample>
    <sample id="301">In totale, generiamo cinquantacinquemila obiettivi specifici con script. Per garantire la qualità della convalida e dei set di test, chiediamo ai lavoratori di Crowdsourced di trovare e rivedere i campioni errati.</sample>
    <sample id="302">Questa figura mostra la distribuzione vincolata di Co script. Scopriamo che Co script mostra un alto plauso in gli obiettivi generati specifici. Con Co script, possiamo addestrare modelli più piccoli ma specializzati per la pianificazione del linguaggio vincolata.</sample>
    <sample id="303">Abbiamo scoperto che il fine-tuning di T five su un set di codice può generare script di qualità superiore rispetto alla maggior parte dei modelli di linguaggio di grandi dimensioni, indicando che i modelli più piccoli possono supportare i modelli più grandi quando sono correttamente addestrati su set di dati adatti.</sample>
    <sample id="304">In sintesi, abbiamo stabilito il problema di pianificazione del linguaggio vincolato. Abbiamo valutato la capacità di pianificazione del linguaggio vincolata di modelli linguistici di grandi dimensioni e sviluppato un metodo di filtro di generazione eccessiva per modelli linguistici di grandi dimensioni.</sample>
    <sample id="305">Usiamo modelli linguistici di grandi dimensioni per generare un set di dati di script di alta qualità, coscript, per la pianificazione del linguaggio vincolata. Speriamo che il set di dati coscript possa essere una risorsa preziosa per promuovere la ricerca sulla pianificazione del linguaggio.</sample>
    <sample id="306">Grazie per il tempo. Per ulteriori dettagli su Coorscript, si prega di consultare il nostro documento.</sample>
    <sample id="307">La fluidità di PaLM è comparabile a quella di sistemi all'avanguardia.</sample>
    <sample id="308">Le proprietà importanti di un metodo di filigrana includono: 1) Applicabilità agli embedding services, 2) non degradazione dell'utilità degli embedding, 3) convertibilità o rimozione facile per l'attaccante, e 4) trasferibilità durante la modellazione.</sample>
    <sample id="309">Le 14 lingue diverse in cui sono stati tradotti i discorsi TED in inglese sono: cinese, francese, tedesco, spagnolo, italiano, giapponese, russo, olandese, portoghese, coreano, indonesiano, tedesco, cinese, e vietnamita.</sample>
    <sample id="310">Il numero di istanze campionate per la riannotazione non è specificato nel testo.</sample>
    <sample id="311">La differenza tra set di dati benigni e backdoor viene misurata utilizzando la distanza di coseno (delta coseno) e la distanza L2 (delta L2).</sample>
    <sample id="312">I modelli basati su codificatori multilingue sono stati utilizzati per valutare e confrontare le prestazioni di diversi tipi di modelli, inclusi encoder PDR (Multilingual Pre-trained Encoders) con decoder basati su puntatori come XLNet+PDR e MBERT+PDR, e encoder-decoder multilingue come MBart e MT5. Questi modelli sono stati valutati su diversi set di dati per determinare quale offre le migliori prestazioni.</sample>
    <sample id="344">Gli autori scelgono le parole a frequenza moderata selezionando un trigger set, che è un gruppo di parole con intervalli di frequenza moderati. Supponendo che il provider possa raccogliere un corpus di testo generale e contare le frequenze delle parole, questi autori possono identificare le parole che si verificano con una frequenza moderata all'interno di quel corpus.</sample>
    <sample id="345">Ciao a tutti. Mi chiamo Shuheng. Oggi presenterò il nostro articolo, Funziona ancora il tag dell'entità di nome Conall Two Thousand Three nel duemilaventi? Iniziamo.</sample>
    <sample id="346">Il nostro documento ha indagato il problema della generalizzazione usando il compito di riconoscimento delle entità nominate, o ner task.</sample>
    <sample id="347">Abbiamo osservato che i modelli hanno usato il Conner duemilatre per sviluppare ner per quasi vent'anni. E questo solleva naturalmente diversi problemi. In primo luogo, questi modelli possono generalizzare ai dati moderni?</sample>
    <sample id="348">E quando sviluppiamo nuovi tag, cosa serve per una buona generalizzazione?</sample>
    <sample id="349">Allo stesso tempo, se osserviamo una cattiva generalizzazione, cosa causa il calo delle prestazioni di questi modelli?</sample>
    <sample id="350">Per indagare su questi problemi, abbiamo sviluppato il set di dati Conll. Questo è un set di dati che abbiamo raccolto da Reuters News da venti venti, e poi annotato con le stesse linee guida di annotazione di Conll duemilatre.</sample>
    <sample id="351">Abbiamo poi affinato oltre venti modelli su colon. Duemilatre, li abbiamo valutati sia sul set di prova di colon tre che sul set di prova di colon plus plus.</sample>
    <sample id="352">E ultimo, ma non meno importante, abbiamo calcolato la variazione percentuale in F uno per valutare la generalizzazione di ogni modello.</sample>
    <sample id="353">Quindi, cosa serve per una buona generalizzazione? Attraverso i nostri esperimenti, abbiamo scoperto che ci sono tre ingredienti principali necessari.</sample>
    <sample id="354">Il primo è l'architettura del modello. Attraverso i nostri esperimenti, abbiamo scoperto che i modelli di trasformatori generalmente generalizzano meglio su nuovi dati.</sample>
    <sample id="355">La seconda componente è la dimensione del modello. Abbiamo scoperto che di solito, i modelli più grandi portano a una migliore generalizzazione.</sample>
    <sample id="356">E per ultimo, ma non meno importante, sappiamo tutti che il numero di esempi di fine tuning influisce direttamente sulle prestazioni di un compito downstream. Qui abbiamo anche scoperto che più esempi di fine tuning in realtà portano anche a una migliore generalizzazione.</sample>
    <sample id="357">Per la nostra prossima domanda, cosa causa il calo delle prestazioni di alcuni modelli?</sample>
    <sample id="358">Abbiamo due ipotesi. La prima è l'overfitting adattivo, che è l'overfitting causato dall'utilizzo ripetuto dello stesso set di test. E questo è di solito manifestato come il ritorno della diminuzione su un nuovo set di test.</sample>
    <sample id="359">La seconda ipotesi è la deriva temporale, che è la degradazione delle prestazioni causata dall'aumento del divario temporale tra i dati di addestramento e i dati di test.</sample>
    <sample id="360">Per l'overfitting adattivo, abbiamo visto che dal grafico a destra, la linea di miglior adattamento rossa ha un gradiente maggiore di uno.</sample>
    <sample id="361">Ciò significa che ogni unità di miglioramento che abbiamo fatto su colon due, oh, oh, tre si traduce in più di un'unità di miglioramento su colon, il che significa che non ci sono rendimenti decrescenti.</sample>
    <sample id="362">E questo ci mostra che l'overfitting adattivo, in questo caso, non è osservato.</sample>
    <sample id="363">E il temporale?</sample>
    <sample id="364">Per il drift temporale, abbiamo fatto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti. E abbiamo scoperto che le prestazioni peggiorano con una maggiore distanza temporale.</sample>
    <sample id="365">E questo conferma la nostra ipotesi che la causa principale del calo delle prestazioni è il drift temporale.</sample>
    <sample id="366">La nostra conclusione è che, per una buona generalizzazione, avremmo bisogno di un'architettura del modello migliore, dimensioni del modello più grandi e più esempi di ottimizzazione fine. E questi vanno di pari passo. Non possiamo avere solo un ingrediente, ma tutti gli altri.</sample>
    <sample id="367">Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni qui è causato da drif temporali. E in qualche modo sorprendentemente, non è causato dall'overfitting adattivo, anche se Conll duemilatre è stato usato per oltre vent'anni.</sample>
    <sample id="368">Quindi, tornando alla domanda che abbiamo sollevato all'inizio del nostro articolo, i tag di Conall duemilatre funzionano ancora nel ventiventitre. E abbiamo scoperto che la risposta è in realtà un rimbombante sì.</sample>
    <sample id="369">Speriamo che il nostro articolo sollevi la necessità di ulteriori ricerche su come migliorare le generalizzazioni dei modelli.</sample>
    <sample id="370">E infine, assicurati di controllare il nostro documento, il nostro set di dati. E se hai domande, sentiti libero di contattarmi. Grazie mille.</sample>
    <sample id="397">L'approccio utilizza un segmento parlato di 4 secondi.</sample>
    <sample id="398">Servin è un giudice.</sample>
    <sample id="399">La qualità dell'esempio è considerata più importante rispetto alla somiglianza con la frase sorgente.</sample>
    <sample id="400">L'articolo si concentra sugli esperimenti estesi sui modelli linguistici GPT-4 e GPT-3, in particolare su come i loro bias politici sono influenzati dai dati di formazione.</sample>
    <sample id="401">Il modello utilizza i punteggi di attenzione di un livello specifico.</sample>
    <sample id="402">Gli esempi di inferenza diretta sono direttamente citando il nome di una canzone o la sua posizione nella lista. Ad esempio, "Easy on Me" o "la prima canzone."</sample>
    <sample id="403">Sulla base del contenuto inglese, gli autori dell'articolo sono Suyu Yuan, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang,</sample>
    <sample id="404">Due.</sample>
    <sample id="405">Sì, la traduzione della query in linguaggio naturale utilizzando un modello di traduzione automatica prima del parsing semantico è stata considerata come un approccio standard.</sample>
    <sample id="406">L'esempio fornito dagli autori per un gruppo contrassegnato è il termine "warrior", che è tipicamente associato a uomini, e quando si descrive una donna come guerriera, viene specificato come "woman warrior" e contrassegnato con "woman".</sample>
    <sample id="407">Le architetture dei modelli che non generalizzano in modo adeguato sono quelle che non sono progettate per adattarsi ai nuovi dati. Ad esempio, se un modello è troppo specializzato nei dati di addestramento, potrebbe non generalizzare bene a nuovi dati.</sample>
    <sample id="408">I nomi dei set di dati di test sono Tabula Random Test Set e Tabula Random Validation Set.</sample>
    <sample id="409">Due autori sono coinvolti nell'articolo, Makshata e Martin.</sample>
    <sample id="410">L'autore opera con più modalità, non solo con il testo.</sample>
    <sample id="439">La comprensione del linguaggio umano.</sample>
    <sample id="440">I nomi dei relatori sono Yin, Zhi Yang e un altro membro del team non specificato.</sample>
    <sample id="441">Coscript è stato sottoposto a controlli di qualità da parte di lavoratori di crowdsource per trovare e correggere campioni errati.</sample>
    <sample id="442">Le risorse esistenti per la traduzione dipendente dal contesto hanno i seguenti limiti: supportano solo tipi limitati di traduzioni dipendenti dal contesto e offrono un insieme limitato di lingue, poiché spesso si basano sulla conoscenza del dominio e sulla curazione umana.</sample>
    <sample id="443">Ciao. E parlerò del nostro lavoro sulla risoluzione di espressioni di relazioni indirette per la selezione delle entità, in cui introduciamo il corpus di entità alternative.</sample>
    <sample id="444">E il mio nome è Javad Hosseini, e questo è un lavoro congiunto con Filip Radlinski, Silvia Parati e Anil.</sample>
    <sample id="445">Il nostro obiettivo è capire la lingua degli utenti quando vogliono fare una scelta. E considera questa domanda alternativa. Intendevi facile per me? O ho un'idea? Qui, un utente vuole selezionare tra uno di questi due siti.</sample>
    <sample id="446">La cosa più ovvia è usare un riferimento diretto. Ad esempio, dicendo il nome della canzone, facile su di me, o la sua posizione, la prima.</sample>
    <sample id="447">Ma a volte un indice di amicizia è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non ricorda il nome della canzone.</sample>
    <sample id="448">O le pronunce sono troppo simili l'una all'altra e difficili da disambiguare.</sample>
    <sample id="449">O quando l'utente vuole specificare una preferenza. Ecco alcuni esempi in riferimento diretto. Ad esempio, il più recente, o il più vecchio, che non è energetico.</sample>
    <sample id="450">Questo è un problema importante nei sistemi conversazionali e anche per il benchmarking dell'understanding dell'entità.</sample>
    <sample id="451">Non siamo a conoscenza di un set di dati pubblico, un set di dati pubblico su larga scala per un compito. Quindi ne raccogliamo uno usando l'annotazione della folla. Il nostro set di dati copre tre domini diversi, musica, libri e.</sample>
    <sample id="452">La nostra metodologia di raccolta dei dati enfatizza l'informalezza usando un set di completamento di cartoni animati.</sample>
    <sample id="453">Il cartone ha tre bolle di discorso. Nella prima bolla, Bob dice, ricorda quella canzone che stavamo ascoltando ieri. E con questo, Bob stabilisce il contesto del dialogo.</sample>
    <sample id="454">Nel secondo discorso, Bob Alice dice, intendi facile su di me, o ho capito.</sample>
    <sample id="455">Qual è la domanda alternativa? E nella terza, la bolla del discorso, Bob usa un riferimento indiretto per selezionare una di queste entità. Ad esempio, il nuovo.</sample>
    <sample id="456">Forniamo automaticamente la prima e la seconda bolla vocale, ma la terza viene compilata dall'annotatore. La prima bolla vocale viene scelta da alcuni prompt manuali per dominio.</sample>
    <sample id="457">La seconda, che è la domanda alternativa, viene generata come segue.</sample>
    <sample id="458">Usiamo sempre un semplice modello. Vuoi dire A o B, dove A e B sono campioni da Wikipedia.</sample>
    <sample id="459">Ecco i diversi metodi di campionamento che abbiamo usato. Quando ci spostiamo più in alto nella lista, le entità diventano più simili tra loro, e di solito è più difficile fare la disambiguazione.</sample>
    <sample id="460">Il primo è l'attacco uniforme.</sample>
    <sample id="461">Il secondo è quando le entità hanno titoli simili. Ad esempio, due libri con il nome, The Return.</sample>
    <sample id="462">Il terzo è quando hanno descrizioni simili su Wikipedia. E infine, quando hanno caselle di informazioni simili o attributi su Wikipedia, ad esempio, lo stesso genere o lo stesso artista per esempio.</sample>
    <sample id="463">Quando mostriamo questa domanda alternativa agli autori, conoscono il nome di queste entità, ma non necessariamente conoscono l'entità.</sample>
    <sample id="464">Quindi quello che facciamo è mostrare alcune conoscenze di fondo sulle due entità. Per le canzoni, mostriamo semplicemente un link di ricerca di Google per ogni canzone.</sample>
    <sample id="465">E poi chiedi agli annotatori di ascoltare almeno alcune di ogni canzone e leggere su ogni canzone. Ecco, per esempio, il risultato di ricerca di Google per la canzone, facile.</sample>
    <sample id="466">Per il dominio di ricette e libri, mostriamo del testo di sfondo da Wikipedia. Per le ricette, aggiungiamo ulteriormente le loro immagini, di nuovo da Wikipedia, in modo che gli annotatori sappiano come appaiono.</sample>
    <sample id="467">Quindi chiediamo agli annotatori di scegliere una di queste entità, ad esempio qui la prima, e di descriverle usando da tre a cinque espressioni indirettamente riferite.</sample>
    <sample id="468">Ad esempio, quello con la musica per pianoforte. Ecco alcuni esempi dal nostro set di dati. Ad esempio, quello senza parole, non quello con il ragazzo di dodici anni, o quello immaginario, o viene dall'Azerbaigian e</sample>
    <sample id="469">Il corpus di entità ha seimila domande alternative in tre domini. E ha quarantaduemila espressioni indirettamente riferite. I risultati con il modello T five X sono riassunti qui.</sample>
    <sample id="470">Se il modello linguistico ha accesso allo stesso background knowledge degli annotatori, allora l'accuratezza è davvero alta. È intorno al novantadue al novantacinque per cento. Ma questo non è realistico.</sample>
    <sample id="471">Se il modello linguistico ha accesso a alcune conoscenze di fondo parzialmente sovrapposte, allora l'accuratezza è compresa tra l'ottantadue e l'ottantasette percento, che è più realistico. Ad esempio, quando il modello linguistico recupera le conoscenze di fondo.</sample>
    <sample id="472">Se il modello linguistico ha accesso solo ai nomi delle entità, l'accuratezza è solo del sessanta per cento. Quindi c'è molto spazio per miglioramenti. Abbiamo anche dimostrato che i modelli sono generalizzabili. Ecco un link al nostro set di dati. Grazie.</sample>
    <sample id="473">SimulST viene confrontato con le politiche Pre-Parallel, Wait-Keys e Local Agreement.</sample>
    <sample id="474">Gli autori dell'articolo non sono affiliati a istituzioni o aziende specifiche, ma sono affiliati a Università della Svizzera Italiana, Università della California, San Francisco e Università della California, Berkeley.</sample>
    <sample id="475">Il nome della relatrice è Jenny.</sample>
    <sample id="476">Tre.</sample>
    <sample id="477">Hi, I'm Sara Papi from the University of Trento and Fondazione Bruno Kessler and I will briefly introduce the attention as a guide for simultaneous speech translation paper that is a joint work with Matteo Negri and Marco Turchi.</sample>
    <sample id="478">Cos'è la traduzione vocale simultanea? La traduzione vocale simultanea, o Simul St, è il processo di traduzione del linguaggio parlato in un testo in un'altra lingua in tempo reale, consentendo la comunicazione tra lingue.</sample>
    <sample id="479">E quali sono i problemi dei modelli di simulazione attuali? Le architetture specifiche sono solitamente addestrate introducendo ulteriori moduli da ottimizzare.</sample>
    <sample id="480">Procedure di formazione lunghe e complicate, ad esempio la formazione che coinvolge diversi obiettivi di ottimizzazione.</sample>
    <sample id="481">E addestrare e mantenere diversi modelli per raggiungere diversi regimi di latenza. Ad esempio, addestrare un modello con una media di una latenza di un secondo e un altro con due secondi di latenza, e così via.</sample>
    <sample id="482">Quindi qual è la nostra soluzione?</sample>
    <sample id="483">In primo luogo, utilizzare modelli sd già esistenti senza riaddestramento o adottare un'architettura specifica per il sd. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici.</sample>
    <sample id="484">E sfrutta le conoscenze già acquisite dal modello attraverso il meccanismo di attenzione tra l'input audio e l'output testuale, cioè il meccanismo di attenzione incrociata. E potete vedere un esempio a destra.</sample>
    <sample id="485">La nostra soluzione è quella di proporre un dat, o encoder decoder attention, ed è una strategia per cui decidiamo se emettere o meno una traduzione parziale in base a dove punta l'attenzione.</sample>
    <sample id="486">Un'unità viene emessa se la tensione non è concentrata, cioè questa somma è al di sotto di un certo soglia, alfa, verso gli ultimi frame di discorso lambdico, il che significa che le informazioni ricevute non sono abbastanza stabili.</sample>
    <sample id="487">Ad esempio, se riceviamo un pezzo di discorso contenente, sto per parlare e il nostro modello prevede la traduzione in tedesco.</sample>
    <sample id="488">E guarderemo i pesi di attenzione incrociati.</sample>
    <sample id="489">Vedremo che le prime due parole indicano i primi frame di discorso ricevuti, mentre l'ultima parola indica i frame di discorso ricevuti più recenti, come i frame di discorso lambda.</sample>
    <sample id="490">Ciò significa che le prime due parole saranno emesse.</sample>
    <sample id="491">Mentre, poiché la somma della tensione incrociata è sopra una certa frazione alfa, non emetteremo l'ultima parola e aspetteremo un altro orologio parlato.</sample>
    <sample id="492">Se andiamo avanti e riceviamo un altro blocco di discorsi, e il nostro modello prevede altre tre parole, e guarderemo questi pesi di attenzione incrociata.</sample>
    <sample id="493">Vedremo che nessuna parola indica gli ultimi frame di parlato lambda.</sample>
    <sample id="494">Ciò significa che queste tre parole saranno emesse.</sample>
    <sample id="495">Se guardiamo i risultati principali su questo.</sample>
    <sample id="496">Tracciamo i risultati della traduzione simultanea su grafici in cui abbiamo il blu su un lato che misura la qualità della traduzione e il lag medio.</sample>
    <sample id="497">Questa è la misura di latenza. E consideriamo anche il computazionalmente consapevole, il ritardo medio che tiene conto dei tempi di calcolo del modello per produrre l'output.</sample>
    <sample id="498">Quindi vogliamo che le nostre curve siano il più alte possibile su questo plot.</sample>
    <sample id="499">Ma vogliamo anche che siano spostati a sinistra.</sample>
    <sample id="500">E confrontiamo con strategie preparate che sono anche applicate ai modelli offline. Quindi sono la strategia di chiave di peso e l'accordo locale. E confrontiamo anche con l'architettura all'avanguardia, specificamente adattata per la traduzione simultanea.</sample>
    <sample id="501">Questi sono tutti i risultati della strategia di traduzione simultanea in tedesco.</sample>
    <sample id="502">E vediamo che l'auto supera tutte le strategie applicate ai modelli offline, poiché le curve sono spostate verso sinistra.</sample>
    <sample id="503">E vediamo anche che se consideriamo il tempo effettivamente trascorso o il tempo di elaborazione consapevole, e che è la strategia più veloce.</sample>
    <sample id="504">Se vuoi scoprire altri risultati, leggi il nostro articolo. E abbiamo anche rilasciato il codice open source e i modelli e l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per la tua attenzione.</sample>
    <sample id="505">Sì, il set di dati è disponibile pubblicamente.</sample>
    <sample id="506">Ciao a tutti. Mi chiamo Ying e il mio collega Zhi Yang e io presenteremo la nostra ricerca su multi instruct, migliorando l'apprendimento teorico multi-modale tramite l'instruzione.</sample>
    <sample id="507">Quindi, con i progressi nei modelli linguistici di grandi dimensioni, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento per utilizzare i modelli linguistici pre-addestrati per diverse attività downstream in modo parametrico ed efficiente per i dati.</sample>
    <sample id="508">Recentemente, molti studi hanno dimostrato che l'ottimizzazione delle istruzioni consente ai modelli linguistici di grandi dimensioni di eseguire compiti non visti in modo efficiente seguendo istruzioni naturali.</sample>
    <sample id="509">Tuttavia, la maggior parte dei lavori precedenti sulla regolazione delle istruzioni si è concentrata sul miglioramento delle prestazioni del thread seriale su compiti a linguaggio solo, mentre la visione artificiale e i compiti multi-modale sono stati lasciati fuori.</sample>
    <sample id="510">Pertanto, in questo lavoro, vogliamo indagare se l'ottimizzazione delle istruzioni su modelli di pre-addestramento multimodale può effettivamente migliorare la generalizzazione su compiti multimodali non visti.</sample>
    <sample id="511">Inoltre, al momento della nostra ricerca, abbiamo scoperto una discrepanza considerevole nella disponibilità del set di dati di istruzione tra RLP e multi-modale.</sample>
    <sample id="512">Esistono più di mille e seicento compiti di istruzione in lingua solo. Tuttavia, non esiste un grande compito di istruzione multimodale pubblicamente disponibile. Pertanto, questo ci ha motivato a costruire un set di dati di ottimizzazione dell'istruzione multimodale.</sample>
    <sample id="513">Qui presentiamo multi instruct, il primo set di benchmark per l'ottimizzazione delle istruzioni multimodale che consiste in sessantadue compiti diversi multimodali che coprono dieci categorie di bordo.</sample>
    <sample id="514">Questi compiti derivano da ventuno set di dati open source esistenti, e ogni compito è dotato di cinque istruzioni scritte esperte.</sample>
    <sample id="515">Per indagare sull'ottimizzazione delle istruzioni multimodali sui nostri dati di riferimento, prendiamo ofa, un modello di pretraining multimodale unificato, come nostro modello di base. Ofa utilizza un vocabolario unificato per i token linguistici e immagine e le coordinate di una scatola di delimitazione.</sample>
    <sample id="516">Qui mostriamo alcune istanze di esempio dal nostro dataset multi-instr.</sample>
    <sample id="517">Per unificare il trattamento di vari tipi di dati di input e output.</sample>
    <sample id="518">Abbiamo seguito il metodo di ofa e formulato tutti i compiti in un formato sequenza sequenza unificato in cui il testo di input, le immagini, le istruzioni e le caselle di delimitazione sono rappresentate nello stesso spazio di token.</sample>
    <sample id="519">Ok, ora parlerò di ottimizzazione delle istruzioni multimodali.</sample>
    <sample id="520">Quindi, per il set di dati di allenamento, usiamo cinquantatré compiti dal gruppo net per l'allenamento. E campioniamo diecimila istanze per compito. Per il test, riserviamo l'intero gruppo di ragionamento comune per il test. E selezioniamo altri cinque compiti dal gruppo VQA e del gruppo malizioso.</sample>
    <sample id="521">Usiamo tutte le istanze nel flusso di test per ogni attività. Inoltre, campioniamo a caso venti attività dal flusso di test di istruzioni naturali, come su Sy Task for NLP.</sample>
    <sample id="522">Quindi usiamo un modello pre-addestrato o f a grande come modello di base. Durante l'addestramento, mescoliamo tutte le istanze per tutte le attività. Ogni istanza viene combinata casualmente con una delle sue cinque istruzioni.</sample>
    <sample id="523">Quindi, durante il test per ogni compito, conduciamo un totale di cinque esperimenti valutando il modello usando una delle cinque istruzioni in ciascun esperimento.</sample>
    <sample id="524">Rendiamo conto della media e della massima prestazione e della deviazione standard delle prestazioni in tutti e cinque gli esperimenti.</sample>
    <sample id="525">Se il compito è un compito di classificazione multi-modale, riportiamo l'accuratezza. Se è un compito di generazione multi-modale, riportiamo il ragionamento. Per un compito di elaborazione del linguaggio naturale, riportiamo anche il ragionamento.</sample>
    <sample id="526">Abbiamo anche introdotto una metrica di valutazione aggiuntiva chiamata sensibilità. Quindi questo misura la capacità del modello di produrre costantemente gli stessi output per lo stesso compito, indipendentemente dalla variazione della selezione nella valutazione dell'istruzione.</sample>
    <sample id="527">Ecco il nostro risultato principale. Come possiamo vedere, l'ottimizzazione delle istruzioni può migliorare significativamente le prestazioni di os os su compiti multimodali.</sample>
    <sample id="528">Inoltre, l'apprendimento transferibile da un set di dati di istruzioni naturali può beneficiare l'ottimizzazione delle istruzioni.</sample>
    <sample id="529">Qui possiamo vedere che, man mano che aumenta la quantità di compito, il modello ottiene prestazioni migliori e, nel frattempo, una sensibilità inferiore.</sample>
    <sample id="530">Quindi abbiamo anche fatto un esperimento. Abbiamo usato un'istruzione contro cinque istruzioni. Come possiamo vedere, usare più istruzioni può migliorare le prestazioni complessive del modello e ridurre la sua sensibilità molto.</sample>
    <sample id="531">Quindi questo mostra l'effetto di diverse strategie di fine tuning sulla sensibilità del modello. Come possiamo vedere, attraverso l'apprendimento di trasferimento da un set di dati di istruzioni naturali, il modello può raggiungere una sensibilità molto migliore rispetto al modello originale.</sample>
    <sample id="532">Possiamo anche vedere che l'apprendimento trasferito dal set di dati di istruzione naturale può aiutare Ofa a ottenere prestazioni molto migliori sul set di dati di istruzione naturale.</sample>
    <sample id="533">Quindi, nel complesso, abbiamo proposto il primo set di dati di ottimizzazione delle istruzioni su larga scala a più modelli. Abbiamo significativamente migliorato la capacità zero-shot di Ofa e abbiamo esplorato diverse tecniche di trasferimento di apprendimento e mostrato i loro benefici. Abbiamo progettato una nuova metrica chiamata sensibilità.</sample>
    <sample id="534">Quindi un'altra cosa. Stiamo raccogliendo un set di dati di ottimizzazione delle istruzioni multimodale molto più grande con circa uno cinquanta compiti aggiuntivi in linguaggio visivo. E li rilasceremo. Quindi questo è un codice qr per i nostri dati e il modello. Grazie.</sample>
    <sample id="535">Sarah Papi is affiliated with the University of Trento and Fondazione Bruno Kessler, and the paper is a joint work with Matteo Negri and Marco Turchi.</sample>
    <sample id="536">Javad Hosseini</sample>
    <sample id="562">Ciao a tutti. Sono Kaustubh Sinha e sono lieto di darvi il benvenuto al nostro discorso del nostro articolo acl twenty twenty three, language model acceptability judgments are not always robust to context.</sample>
    <sample id="563">È un lavoro congiunto con John Gauthier, Aaron Mueller, Kanishka Mishra, Karen Fentress, Roger Levy e Atina Walia.</sample>
    <sample id="564">Quindi, in questo lavoro, rivediamo il paradigma dei paia minimi.</sample>
    <sample id="565">Quindi il paio minimo al paradigma fondamentalmente valuta i modelli linguistici in cima ai giudizi di accettabilità, che possono anche includere grammatica, come sintassi o accettabilità in termini di stereotipi, come coppie di scorrimento.</sample>
    <sample id="566">E in questo paradigma di coppia minimale, il modo tipico per valutare i modelli linguistici è quello di mostrare, ad esempio, una frase accettabile o una frase grammaticale, e poi mostrare una frase non accettabile o una frase non grammaticale.</sample>
    <sample id="567">E poi la speranza è che il modello fondamentalmente dia più probabilità al set accettabile.</sample>
    <sample id="568">La pipeline MPP attuale fondamentalmente non ci permette di valutare l'accettazione di un modello verso frasi più lunghe.</sample>
    <sample id="569">In questi giorni, i modelli di linguaggio di grandi dimensioni stanno arrivando con finestre di contesto sempre più lunghe. Quindi è fondamentale valutare l'accettabilità del modello in tutta la finestra di contesto.</sample>
    <sample id="570">E questo è quello che stiamo cercando di fare qui. Stiamo cercando di rivedere la pipeline mpb chiedendo al modello di valutare l'accettabilità su sequenze più e più lunghe.</sample>
    <sample id="571">Quindi questo è l'approccio. Quindi quello che facciamo è simulare queste sequenze più lunghe, rivedere i set di dati stessi, e poi ricreare frasi scegliendo frasi accettabili o inaccettabili da quei set di dati.</sample>
    <sample id="572">Quindi, ad esempio, qui abbiamo scelto come un paio tipico di grammaticality dal set di dati blimp dall'isola aggiuntiva.</sample>
    <sample id="573">E quello che facciamo è quello di ricreare sequenze più lunghe, che sono accettabili e che hanno la stessa corrispondenza della struttura grammaticale. Estraiamo frasi grammaticali dall'italiano attuale.</sample>
    <sample id="574">E poi lo aggiungiamo come prefisso sia alla query accettabile che alla query inaccettabile.</sample>
    <sample id="575">Quindi possiamo fare la stessa cosa scegliendo frasi inaccettabili dalla stessa corrispondenza. E questo potrebbe anche essere usato per testare l'accettabilità del modello.</sample>
    <sample id="576">E possiamo anche fare lo stesso scegliendo frasi da un sottoinsieme diverso o un set di dati diverso. Quindi questo è ciò che chiamiamo lo scenario di non corrispondenza.</sample>
    <sample id="577">Quindi qui le frasi provengono ancora da un set di dati rilevante, ma non è lo stesso set di dati con cui stai valutando. E possiamo fare lo stesso per i casi di inaccettabilità.</sample>
    <sample id="578">Infine, possiamo scegliere frasi da un dominio completamente non correlato, come Wikipedia.</sample>
    <sample id="579">Quindi questo ci dirà se i giudizi di accettabilità dei modelli sono effettivamente influenzati da qualsiasi contesto.</sample>
    <sample id="580">Come se il contesto provenisse da un diverso sottoinsieme del set di dati, o se è completamente irrilevante per il sentito che stiamo esaminando.</sample>
    <sample id="581">Quindi, come funziona il modello? Quindi, prima di tutto, guardiamo alle frasi di Wikipedia, che sono completamente irrilevanti per la query corrente. E poi troviamo che i giudizi mp sono per lo più robusti per contesti arbitrari.</sample>
    <sample id="582">Abbiamo aumentato la lunghezza del contesto fino a duemilaventiquattro per massimizzare i modelli opt e gpt due. E abbiamo visto qui nella linea a dischi arancioni, i giudizi mp sono relativamente stabili.</sample>
    <sample id="583">Ora, cosa succede quando scegliamo frasi dalla stessa fonte?</sample>
    <sample id="584">Quindi qui stiamo scegliendo o creando frasi da domini accettabili e inaccettabili, dal set di dati di sintassi di blimp.</sample>
    <sample id="585">E poi vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o non accettabili.</sample>
    <sample id="586">Ma quando corrispondiamo la struttura, cioè quando scegliamo le frasi dalla stessa fenomenologia nel testo di persona incolpante, Jim,</sample>
    <sample id="587">Vediamo un enorme aumento o una grande diminuzione del giudizio di MPP per il modello, a seconda che il prefisso scelto sia accettabile o non accettabile.</sample>
    <sample id="588">Ora questo e questo è molto grande. Come questo effetto aumenta attraverso l'intero contesto. E questo probabilmente influenzerà nuovi modelli linguistici, che ha un grande contesto.</sample>
    <sample id="589">Allora perché il prefisso corrispondente influisce così tanto sul giudizio del modello linguistico?</sample>
    <sample id="590">Quindi abbiamo fatto una serie di analisi in cui abbiamo cercato di perturbare la frase di input cercando di preservare la struttura rilevante aggiungendo rumore all'input. E dopo aver fatto come diverse di queste perturbazioni,</sample>
    <sample id="591">Scopriamo che nessuno di questi rumori sta effettivamente facendo cambiare il modello, come cambiare il corso in termini di come ci mostra la tendenza del giudice di MP.</sample>
    <sample id="592">Fondamentalmente, scopriamo che i modelli sono sensibili alle frasi perturbate in modi simili.</sample>
    <sample id="593">Cioè, quando perturbiamo le frasi nel dominio accettabile, vediamo un aumento simile in tutte le perturbazioni. E quando perturbiamo le frasi nel dominio non accettabile, vediamo una diminuzione dei giudizi mp in modo simile.</sample>
    <sample id="594">Quindi le principali conclusioni del nostro lavoro sono che i modelli linguistici sono sensibili alle caratteristiche sintattiche e semantiche latenti che sono condivise tra le frasi.</sample>
    <sample id="595">E l'analisi MPP, il modo in cui lo facciamo attualmente con input brevi e singoli frasi, potrebbe non catturare completamente la conoscenza astratta del modello linguistico nell'intero contesto.</sample>
    <sample id="596">Si prega di leggere il nostro documento per ulteriori dettagli dei nostri esperimenti. Grazie per l'attenzione.</sample>
    <sample id="597">Il primo passaggio del metodo mappa i token di input in token di output.</sample>
    <sample id="598">Coscript rappresenta 55.000 script.</sample>
    <sample id="626">Il metodo di allineamento migliore per DEplain è il metodo di mass align.</sample>
    <sample id="627">L'apprendimento scarsamente supervisionato consente ai modelli di generalizzare meglio riducendo l'impatto del rumore nei dati di etichettatura.</sample>
    <sample id="628">L'allocazione è stata effettuata utilizzando un metodo di allineamento automatico basato su una funzione di distorsione.</sample>
    <sample id="629">Il set di dati CoNLL++ è stato sviluppato per indagare su problemi specifici. È stato raccolto da Reuters News nel 2020 e annotato utilizzando le stesse linee guida di annotazione di CoNLL 2003.</sample>
    <sample id="630">Ciao a tutti. Mi chiamo Yuxin Zhang della Penn State University. Oggi presenterò il nostro lavoro, Cross-lingual Semantic Parsing in multiple natural languages and multiple representations.</sample>
    <sample id="631">Quindi l'analisi semantica è un compito per costruire rappresentazioni semantiche di query utente, come sql e lambda calculus.</sample>
    <sample id="632">E la semantica linguistica trasversale è il compito di tradurre le query in più lingue naturali in più rappresentazioni di significato.</sample>
    <sample id="633">Come mostrato in questa figura, dobbiamo tradurre la query in più lingue naturali usando modelli neurali, due sql, lambda o funql e eccetera.</sample>
    <sample id="634">I modelli di analisi semantica translinguistica esistenti sono proposti e valutati separatamente su un set di dati di compiti e applicazioni limitati. Ad esempio,</sample>
    <sample id="635">Ci sono lacune di copertura su certi linguaggi naturali. Il cinese manca e</sample>
    <sample id="636">Clicca sulla copertura su determinate mini rappresentazioni.</sample>
    <sample id="637">Manca il lambda calculus.</sample>
    <sample id="638">Oppure sono valutati solo su un certo modello neurale. Ad esempio, c'è solo un singolo modello per valutare.</sample>
    <sample id="639">A questo scopo, proponiamo l'esemplare, ma forniamo un esemplare di set di dati uniforme per il parsing semantico crosslingual in più lingue naturali e rappresentazioni.</sample>
    <sample id="640">Contiene novanta set in vari domini, cinque task di analisi semantica, otto rappresentazioni di milioni e ventidue lingue naturali in quindici famiglie linguistiche.</sample>
    <sample id="641">E per valutare meglio il nostro benchmark, abbiamo considerato le sei impostazioni per l'allenamento e la valutazione.</sample>
    <sample id="642">Il primo è il test di traduzione. Useremo l'API di Google Translate per tradurre la fonte nella lingua target. Quindi useremo il modello monolingue per addestrare e valutare.</sample>
    <sample id="643">E per esempio, addestriamo il modello inglese su una query inglese. E durante l'inferenza, tradurremo la query tedesca usando api in inglese, e poi useremo il modello addestrato per prevedere il sql.</sample>
    <sample id="644">E abbiamo anche testato il modello monolingue.</sample>
    <sample id="645">In questa impostazione, il linguaggio sorgente è lo stesso del linguaggio di destinazione. Ad esempio, tedesco a tedesco o inglese a inglese.</sample>
    <sample id="646">Abbiamo anche testato l'impostazione del campo monolingue, ma addestrando modelli monolingue con solo il dieci percento dei dati di addestramento.</sample>
    <sample id="647">E che ha il modello multilingue, che abbiamo addestrato un modello multilingue per tutte le lingue.</sample>
    <sample id="648">Ad esempio, mettiamo insieme le query in tedesco, inglese e cinese per addestrare un modello multilingue. E durante l'inferenza, possiamo usare questo modello.</sample>
    <sample id="649">Per tradurre domande in tedesco o in cinese o eccetera.</sample>
    <sample id="650">E consideriamo anche il trasferimento crosslingual zero shot e few shot. Abbiamo addestrato su una lingua di origine e trasferito in un'altra lingua.</sample>
    <sample id="651">Quindi, durante l'allenamento, stiamo allenando su query in inglese, o sulla combinazione di query in inglese e in tedesco, per addestrare un modello multilingue per prevedere l'output sql.</sample>
    <sample id="652">E troviamo anche molti risultati interessanti. Quindi, per quanto riguarda l'analisi di modelli monolingue, valutiamo due gruppi di modelli.</sample>
    <sample id="653">Compreso encoder pdr, che sta per encoderi preimparati multilingue con decoder basati su puntatori, come xlmr più pdr, mbert più pdr.</sample>
    <sample id="654">E valutiamo anche i modelli encoder-decoder, che sono encoder-decoder multilingue addestrati, come mbart e mt five.</sample>
    <sample id="655">Abbiamo scoperto che l'encoder decoder ottiene le migliori prestazioni su tutti e nove i set di dati.</sample>
    <sample id="656">E valutiamo su mt five e esempio xlmr plus pdr su impostazione multilingue.</sample>
    <sample id="657">Abbiamo scoperto che encoder, decoder o encoder pdr possono essere migliorati attraverso l'allenamento in una miscela di vari linguaggi.</sample>
    <sample id="658">E abbiamo scoperto che è perché la maggior parte delle principali lingue naturali può ottenere un guadagno di prestazioni, tranne che il rendimento inglese diminuisce in sette set di dati e guadagna solo in tre set di dati.</sample>
    <sample id="659">Penso che questo sia noto come maledizione della multilinguezza.</sample>
    <sample id="660">Abbiamo anche confrontato il guadagno delle prestazioni di Crosslink.</sample>
    <sample id="661">In questa figura, la linea blu è il trasferimento cross-lingua few-shot, la linea arancione è il trasferimento cross-lingua zero-shot, mentre la linea verde è l'impostazione monolingue.</sample>
    <sample id="662">Abbiamo scoperto che confrontando la linea verde e quella arancione, abbiamo scoperto che per l'impostazione zero short, il divario di prestazione del trasferimento crosslingual è significativo. E confrontando la linea blu e quella arancione, abbiamo scoperto che per l'impostazione few short, il divario di trasferimento è ridotto rapidamente.</sample>
    <sample id="663">Troviamo anche alcuni altri risultati interessanti. Ad esempio, encoder, decoder, outperforms, prows, work, o raggiunge risultati comparabili. Per allenare il linguaggio naturale inglese può aumentare significativamente le prestazioni di fewshot su linguaggi naturali di destinazione.</sample>
    <sample id="664">E abbiamo scoperto che i modelli linguistici multilingue come Codas e Blue sono ancora inadeguati per le attività di analisi semantica translinguistica.</sample>
    <sample id="665">In sintesi, abbiamo costruito Exemplar, un benchmark unificato per il parsing semantico a angolo incrociato con più lingue naturali e rappresentazioni medie.</sample>
    <sample id="666">Abbiamo condotto uno studio di benchmark completo su tre tipi rappresentativi di modelli linguistici multilingue. E i nostri risultati mostrano molti risultati interessanti e così via. E benvenuti a visitare il nostro documento e codice. Grazie per l'ascolto.</sample>
    <sample id="667">I lavori connessi in tal senso sono quelli che sono strettamente correlati e condividono un tema o un argomento comune.</sample>
    <sample id="668">No, gli LLM multilingue come Codex o Bloom sono ancora inadeguati per le attività di cross-lingual semantic parsing.</sample>
    <sample id="695">Il metodo utilizza la continua relaxazione per trovare le permutazioni linguisticamente più plausibili.</sample>
    <sample id="696">L'equità di un modello NLP a valle si riferisce alla sua capacità di trattare equamente tutti i gruppi demografici, linguistiche e culturali, senza favorire o discriminare nessuno.</sample>
    <sample id="697">The speaker is Yannis La Via.</sample>
    <sample id="698">Il nome della relatrice è Kaustubh Sinha.</sample>
    <sample id="699">Il nome della relatrice è Myra.</sample>
    <sample id="700">Il tropicalismo nel contesto di questo articolo indica un stereotipo che associa le donne di colore, in particolare le donne latine, a caratteristiche vivaci e colorate, spesso con un'atmosfera di bellezza e sensualità.</sample>
    <sample id="701">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target enfatizzando le loro identità culturale, tradizionale e distintiva dal normale bianco.</sample>
    <sample id="702">In questo lavoro, CxMI (Cross-Modal Inference) è stato esteso a CxMI pointwise per misurare l'utilizzo del contesto sia a livello di frase che a livello di parola.</sample>
    <sample id="703">DrBERT è un modello trainato da scratch con 7 GB di corpus naturale, mentre ChuBERT è un modello clinico trainato con 4 GB di note cliniche.</sample>
    <sample id="751">Due autori sono coinvolti nell'articolo.</sample>
    <sample id="752">Il trasferimento iterativo dell'apprendimento è un processo in cui un modello viene aggiornato iterativamente con nuovi dati, affinando continuamente le sue prestazioni.</sample>
    <sample id="753">L'obiettivo del set di dati è capire il linguaggio degli utenti quando vogliono fare una scelta.</sample>
    <sample id="754">Un utente malintenzionato può sfruttare l'API dell'endpoint del modello per inviare input specifici e ottenere output, permettendo loro di identificare i parametri del modello.</sample>
    <sample id="755">There are three authors involved in the article.</sample>
    <sample id="756">12 annotatori.</sample>
    <sample id="757">Gli autori dell'articolo sono Sebastian Santi, Ronen Labrousse, Katerina Ranecka e Martin Sabou.</sample>
    <sample id="758">L'esempio in cui il governatore è a sinistra è "I saw Bart and Lisa".</sample>
    <sample id="759">I modelli all'avanguardia nei sistemi di dialogo includono GPT-3 di Microsoft, BERT di Google, ELMo di MIT, CTRL di OpenAI e BERT di Facebook.</sample>
    <sample id="760">La necessità di valutare l'accettabilità dei modelli nell'intera finestra di contesto deriva dal fatto che i modelli di grandi dimensioni stanno avendo un contesto più ampio, rendendo cruciale garantire che i loro risultati siano coerenti e affidabili su tutta la lunghezza del contesto.</sample>
    <sample id="761">Sì, la formazione attraverso la modalità multilingue ha causato un calo delle prestazioni rispetto al modello inglese monolingue.</sample>
    <sample id="762">Sì, gli annotatori conoscono l'entità in anticipo.</sample>
    <sample id="763">Le metriche di MT utilizzate per la valutazione sono F1, coerenza e diversità.</sample>
    <sample id="764">Sì, il regresso nella generalizzazione influisce su specifici tipi di NER, come i nomi propri, i pronomi e i numeri, che possono essere difficili da generalizzare a causa della loro natura altamente specifica.</sample>
    <sample id="765">La posizionalità nella NLP è importante perché aiuta a comprendere il contesto e il significato delle parole all'interno di una frase, migliorando la precisione della comprensione e del trattamento del linguaggio naturale.</sample>
    <sample id="766">Sì, gli LLM multilingue come BLOOM sono stati affinati tramite adattatori e messa a punto integrale.</sample>
    <sample id="767">Il modello utilizzato per il trasferimento dell'apprendimento è il modello che ottiene il miglior risultato dopo la fine-tuning iterativo di CEE e debate.</sample>
    <sample id="768">I recenti set di test utilizzati per valutare le capacità di PaLM includono l'English BERT (Bidirectional Encoder Representations from Transformers) e il test di linguaggio naturale (NLP) di la testata di linguaggio naturale (NLP) di la testata di linguaggio naturale (NLP).</sample>
    <sample id="769">Tre.</sample>
    <sample id="770">Il metodo proposto consente di creare modelli più piccoli e specializzati per la pianificazione del linguaggio con vincoli, il che può essere vantaggioso per applicazioni specifiche.</sample>
    <sample id="771">Il nome della relatrice è Xu-Hung.</sample>
    <sample id="772">Sì, i risultati e il set di dati nell'articolo possono essere utilizzati come parametri di riferimento.</sample>
    <sample id="773">Nell'articolo, viene menzionato l'uso di "modelli più piccoli" in confronto ai "modelli più grandi", ma non viene specificato un numero esatto di modelli più piccoli.</sample>
    <sample id="774">Il modello OFA (Unified Multi-Modal Pretraining Model) viene utilizzato come modello di base per analizzare l'ottimizzazione delle istruzioni multimodali.</sample>
    <sample id="833">Gli autori dell'articolo sono di Google Translate.</sample>
    <sample id="834">Gli autori dell'articolo sono Vasudha, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen, S. Praveen,</sample>
    <sample id="835">Insieme, l'articolo analizza le coppie linguistiche inglese-italiano e inglese-spanola.</sample>
    <sample id="836">Chi è il relatore?</sample>
    <sample id="837">Due modelli sono stati studiati: uno per la semplificazione a livello di documento e uno per la semplificazione a livello di frase.</sample>
    <sample id="838">Per l'addestramento, 53 attività vengono utilizzate, mentre per il test, 5 attività vengono utilizzate.</sample>
    <sample id="839">Due autori sono coinvolti nell'articolo.</sample>
    <sample id="840">Gli autori hanno effettuato i test su quattro set di dati: Agnews, MIND, SSTD2 e IRAS-PM.</sample>
    <sample id="876">NACHOS è un set di dati medico che il modello Dr. Bert è stato addestrato.</sample>
    <sample id="877">Il nome della relatrice è Ayed Bilal.</sample>
    <sample id="878">La strategia del prompting influisce significativamente sui risultati, poiché diversi prompt possono portare a risultati di traduzione variabili per gli LLMs.</sample>
    <sample id="879">Gli autori dell'articolo sono Kyle Yin, Patrick Fernholz, Emile Liu, Andre F. de Martens e Graham Neubig.</sample>
    <sample id="880">1. Raccogliere un set di dati più ampio per l'allenamento del modello
2. Aggiungere circa 150 compiti di linguaggio visivo
3. Rilasciare i compiti aggiuntivi
4. Utilizzare il QR code per fornire i dati e il modello
5. Ringraziare per l'attenzione</sample>
    <sample id="881">Progettano un compito di risoluzione delle co-referenze per valutare la capacità di sfruttare conoscenze da diverse fonti.</sample>
    <sample id="882">Ciao a tutti. Mi chiamo Ayed Bilal e darò una breve recensione del documento, prompting prompt from translation, assessing strategies and performance. Questo è un lavoro congiunto con i miei colleghi di Google translate.</sample>
    <sample id="883">Param è un modello di linguaggio di parametri di cinque quaranta miliardi di parametri presentato l'anno scorso, nel duemilaventidue. È addestrato su una grande raccolta di testo, composta da settecentottanta miliardi di token.</sample>
    <sample id="884">Al momento della pubblicazione, ha raggiunto lo stato dell'arte in centinaia di compiti NLP.</sample>
    <sample id="885">In questo lavoro, presentiamo lo studio sistematico del prompt di un modello linguistico per la traduzione automatica.</sample>
    <sample id="886">Abbiamo valutato la capacità di traduzione di tali modelli utilizzando le migliori pratiche della comunità imt. Ciò comporta l'uso degli ultimi set di test per evitare un sovrapposizione dei dati di test con i dati di allenamento del modello linguistico.</sample>
    <sample id="887">E confrontiamo due sistemi all'avanguardia, i migliori sistemi di prestazione, o l'analisi WMT.</sample>
    <sample id="888">Usiamo le ultime metriche neurali e L M T. E inoltre, mostriamo anche le valutazioni esperte basate sull'uomo. Infine, forniamo alcune raccomandazioni per le strategie di selezione dei prompt.</sample>
    <sample id="889">Il prompting ha un grande impatto sulle prestazioni degli llms per la traduzione. Come possiamo vedere in un semplice esperimento, dove abbiamo usato un prompt di un colpo e abbiamo fornito due prompt diversi per ogni frase.</sample>
    <sample id="890">La maggioranza delle frasi, cinquecentosedici su mille, la differenza osservata è di più di un punto di sfocatura.</sample>
    <sample id="891">E questo può andare, in casi estremi, fino a quaranta punti di errore. Quindi è importante selezionare una buona strategia di prompting.</sample>
    <sample id="892">Nei nostri esperimenti, abbiamo deciso per una strategia di promemoria a cinque colpi, in cui abbiamo semplicemente contrassegnato ogni frase che abbiamo fornito al sistema con il linguaggio in cui è.</sample>
    <sample id="893">Quindi in questo esempio qui, dove eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche, le frasi di origine sono contrassegnate con colonna tedesca e le traduzioni in inglese con colonna inglese.</sample>
    <sample id="894">Abbiamo visto che la forma effettiva del prompt non ha un grande influenza nel caso di prompt di scatto seriale.</sample>
    <sample id="895">È cruciale per la prompting zero e one shot. E quando andiamo, come nel nostro caso, alla prompting five shot, non c'è quasi alcuna differenza nella forma effettiva della prompting.</sample>
    <sample id="896">Sono gli esempi che portano la maggior parte del peso.</sample>
    <sample id="897">La sintesi dei nostri risultati sperimentali è che la qualità dell'esempio è più importante della somiglianza con la frase di origine.</sample>
    <sample id="898">Quindi è importante selezionare gli esempi dalle traduzioni di alta qualità. In particolare, confrontiamo la selezione di prompt dai dati di allenamento delle valutazioni Wmt o dai dati di sviluppo.</sample>
    <sample id="899">I dati di profondità sono molto più accurati e di qualità superiore rispetto ai dati di allenamento, che sono più grossolani. E i risultati mostrano un migliore rendimento quando si utilizzano i dati di profondità.</sample>
    <sample id="900">Tuttavia, i sistemi specializzati di ultima generazione hanno un vantaggio sostanziale rispetto alle traduzioni di PUM. Ma PUM è abbastanza vicino a un sistema commerciale. Nel nostro caso, abbiamo scelto di sovrapporre con Google Translate.</sample>
    <sample id="901">Le intuizioni che abbiamo ottenuto dall'innervoluzione che abbiamo eseguito usando il framework mpm, è che la fluidità del palmo è paragonabile ai sistemi all'avanguardia. Ma la differenza principale deriva dall'accuratezza.</sample>
    <sample id="902">In particolare, gli errori più comuni sono gli errori di omissione.</sample>
    <sample id="903">Quindi sembra che Palm scelga di produrre una traduzione che suona meglio, a volte lasciando cadere parti della frase originale nella traduzione.</sample>
    <sample id="904">Tuttavia, la categoria dello stile sgradevole per il pun è inferiore a quella per i sistemi all'avanguardia, che è un segnale aggiuntivo.</sample>
    <sample id="905">Che il parm fornisce un output davvero fluido, ma ancora con alcuni problemi di precisione.</sample>
    <sample id="906">E questo è tutto per questa breve panoramica. Per maggiori dettagli, si prega di venire alla presentazione completa del documento. Grazie mille.</sample>
    <sample id="907">Ciao. Sono Dawei, uno studente di dottorato presso la Saarland University in Germania. In questo video vorrei presentare il nostro lavoro recente, weaker than you think, una visione critica della formazione a settimana.</sample>
    <sample id="908">Questo è un lavoro congiunto con Xiao Yusen, Mario Smusba, Gerd Stephan e Dietlisch Klakow.</sample>
    <sample id="909">Vorrei iniziare con una breve introduzione alla supervisione settimanale e alla regressione a sorveglianza debole.</sample>
    <sample id="910">Nella supervisione debole, non etichettiamo manualmente i dati. Invece, etichettiamo i dati usando fonti di etichettatura debole, come semplici regole euristiche, basi di conoscenza o outsourcing di codice di bassa qualità, come illustrato nella figura a destra.</sample>
    <sample id="911">Rispetto alle annotazioni umane, le annotazioni automatiche sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità delle annotazioni sono errate.</sample>
    <sample id="912">Se addestriamo direttamente le reti neurali sui dati etichettati settimanalmente, le reti neurali tendono a memorizzare il rumore dell'etichetta e non generalizzano.</sample>
    <sample id="913">Nell'apprendimento supervisionato, vengono proposti algoritmi di addestramento per addestrare robustamente le reti neurali sotto tale rumore di etichettatura, in modo che i modelli addestrati generalizzino ancora bene.</sample>
    <sample id="914">In recenti lavori in WSL, quindi WSL sta per weekly supervised learning. Una dichiarazione comune è che le persone dicono che addestrano modelli solo sui dati etichettati settimanalmente e ottengono prestazioni elevate sui set di test puliti.</sample>
    <sample id="915">Tecnicamente questa affermazione non è sbagliata, ma c'è un'eccezione.</sample>
    <sample id="916">Cioè, le persone assumono che ci sia un set di validazione aggiuntivo e pulito disponibile per la selezione del modello.</sample>
    <sample id="917">Non possiamo dubitare di questo problema impostato, poiché ciò implica che sono necessarie annotazioni manuali aggiuntive nell'apprendimento supervisionato settimanale. Ma come un elefante nella stanza, questa necessità è spesso trascurata.</sample>
    <sample id="918">Il dubbi precedente ci spinge a porre tre domande di ricerca. Prima, è necessario un set di dati di validazione pulito per WSL? O forse possiamo usare invece un set di validazione rumoroso?</sample>
    <sample id="919">In secondo luogo, se i dati puliti sono richiesti, o se i dati puliti sono obbligatori affinché WSL funzioni, allora di quanti campioni puliti abbiamo bisogno? Infine, dovremmo usare solo i campioni puliti per la convalida? O ci sono modi migliori per utilizzarli?</sample>
    <sample id="920">Abbiamo affrontato queste domande di ricerca nel nostro lavoro e le nostre scoperte sono le seguenti.</sample>
    <sample id="921">Per prima cosa, scopriamo che, curiosamente, i metodi recenti wsl in effetti richiedono campioni di dati puliti per funzionare correttamente.</sample>
    <sample id="922">Altrimenti, c'è un grande calo delle prestazioni, come mostrato in questa figura. Se non ci sono campioni di validazione puliti, allora i modelli addestrati non possono generalizzare oltre le etichette originali.</sample>
    <sample id="923">Significa che l'allenamento è inutile.</sample>
    <sample id="924">Ciò indica che gli approcci WSL in realtà richiedono dati etichettati correttamente per funzionare correttamente. E il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato.</sample>
    <sample id="925">La nostra seconda scoperta è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci WSL a raggiungere prestazioni migliori, come mostrato nella figura a sinistra.</sample>
    <sample id="926">In genere, abbiamo bisogno di solo venti campioni per classe per ottenere prestazioni elevate.</sample>
    <sample id="927">Ma non è la fine della storia. Perché se in entrambi i casi decidiamo di accedere a campioni puliti, allora addestrarli direttamente raggiungerà prestazioni ancora migliori.</sample>
    <sample id="928">La figura a destra mostra la differenza di prestazioni tra approcci di ottimizzazione fine, che vengono applicati direttamente sui dati puliti, e approcci WSL, che utilizzano i dati puliti solo per la convalida.</sample>
    <sample id="929">Come possiamo vedere, se abbiamo dieci campioni per classe, la fine tuning diretta inizia a battere gli approcci WSL.</sample>
    <sample id="930">Infine, il miglioramento delle prestazioni dichiarato negli approcci WSL precedenti può essere facilmente raggiunto consentendo di continuare la messa a punto sui campioni di validazione puliti.</sample>
    <sample id="931">Come possiamo vedere dalle figure, il modello Valina, chiamato ftw, inizialmente sottoperforma metodi WSL più complicati come cosine.</sample>
    <sample id="932">Tuttavia, se consentiamo di continuare a perfezionare i campioni puliti, allora ftw si comporta allo stesso modo di altri metodi.</sample>
    <sample id="933">Quindi, in pratica, non c'è motivo di scegliere metodi WSL più complessi, che richiedono più tempo di calcolo e spazio su disco.</sample>
    <sample id="934">In sintesi, abbiamo dimostrato che gli approcci recenti di WSL richiedono campioni annotati manualmente per funzionare correttamente. Il loro guadagno di prestazioni e la praticità sono fortemente sopravvalutati.</sample>
    <sample id="935">Le nostre raccomandazioni concrete per il lavoro futuro sono le seguenti.</sample>
    <sample id="936">Per prima cosa, segnalare i criteri di selezione del modello. Ad esempio, segnalare se la selezione del modello è stata eseguita con campioni di validazione ben puliti.</sample>
    <sample id="937">In secondo luogo, gli approcci WSL dovrebbero essere confrontati con le linee di base di apprendimento a breve termine, poiché entrambi lavorano su campioni di clip. In terzo luogo, la fine tuning continua è una linea di base semplice ma forte che dovrebbe essere considerata in lavori futuri in WSL.</sample>
    <sample id="938">Infine, abbiamo aperto il codice. Puoi trovarlo tramite il codice Qr su questa diapositiva. Per favore, sentiti libero di controllarlo. Grazie e buona conferenza.</sample>
    <sample id="939">I metodi di valutazione comuni per i sistemi di dialogo includono l'uso di valutazioni umane, come la selezione da parte di giudici umani o la valutazione con una scala di Likert.</sample>
    <sample id="940">Quattro.</sample>
    <sample id="941">Nell'esempio con Servin e Kea, le conoscenze di base necessarie sono che i giudici decidono casi in tribunali.</sample>
    <sample id="942">Sì, il codice è disponibile. Puoi trovarlo su GitHub.</sample>
    <sample id="943">Sì, gli annotatori per NLPositionality sono bilanciati per genere, età e background educativo.</sample>
    <sample id="944">Le frasi nel dominio accettabile sono state perturbate aggiungendo rumore che preservava la struttura rilevante, ma non influenzava significativamente i risultati del modello.</sample>
    <sample id="945">Una valutazione dimensionale significa valutare più aspetti o dimensioni di qualcosa, in questo caso, la qualità del dialogo.</sample>
    <sample id="946">Gli autori dell'articolo sono Jinwei Yi, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Y</sample>
    <sample id="947">La forma del prompting è importante per il zero e il one-shot prompting.</sample>
    <sample id="978">Gli autori hanno valutato i modelli di dialogo GPT-3.5 e GPT-4.</sample>
    <sample id="979">Due autori sono coinvolti nell'articolo.</sample>
    <sample id="980">Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli alle restrizioni.</sample>
    <sample id="981">L'articolo è scritto da quattro autori.</sample>
    <sample id="982">Il nome della relatrice è Vasudha.</sample>
    <sample id="983">Gli autori dell'articolo non hanno dichiarato alcuna affiliazione.</sample>
    <sample id="1021">Gli errori più comuni di PaLM sono omissioni.</sample>
    <sample id="1022">Ciao. Sono James Finch. E sono Sarah Finch. E oggi vi parleremo di abc eval, un nuovo approccio dimensionale per valutare l'intelligenza artificiale conversazionale.</sample>
    <sample id="1023">Questo lavoro è stato svolto dal laboratorio Emory Nlp, guidato dal professor Gino Choi presso l'Università di Emory e in collaborazione con Amazon Alexa AI.</sample>
    <sample id="1024">Quindi diciamo che hai appena sviluppato un modello di dialogo e vuoi vedere come si confronta con lo stato attuale dell'arte.</sample>
    <sample id="1025">La pratica comune è quella di utilizzare l'evaluazione umana, ad esempio chiedendo ai giudici umani di selezionare quale delle due conversazioni è migliore, o di valutare le conversazioni in base a una scala Likert.</sample>
    <sample id="1026">Questi approcci funzionano bene per fornire valutazioni olistiche della qualità generale della conversazione, ma la qualità della conversazione ha molti aspetti. Pertanto, potresti voler valutare più dimensioni della qualità della chat per comprendere le forti e le debolezze del modello a un livello più fine.</sample>
    <sample id="1027">Un approccio è semplicemente chiedere ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la rilevanza delle risposte del modello, utilizzando metodi di confronto o Likert.</sample>
    <sample id="1028">Tuttavia, crediamo che esista una strategia più precisa e affidabile per la valutazione del dialogo dimensionale.</sample>
    <sample id="1029">Il nostro approccio cerca di ridurre la soggettività della valutazione umana annotando esplicitamente se ogni risposta del modello esprime o meno certi comportamenti, come rispondere con informazioni irrilevanti o contraddire se stesso.</sample>
    <sample id="1030">Chiamiamo questo approccio annotazione dei comportamenti nel chat, o abc eval in breve. Siamo sviluppati questo metodo per coprire in modo completo i comportamenti del modello di chat che sono stati suggeriti per influenzare la qualità del chat nella letteratura recente.</sample>
    <sample id="1031">Abc eval è in grado di misurare i tassi con cui i modelli di chat commettono vari errori tematici.</sample>
    <sample id="1032">Ad esempio, abc eval misura il numero di turni in cui un modello di chat ignora il suo partner o dice qualcosa di irrilevante.</sample>
    <sample id="1033">Contraddice se stesso o il suo partner, allucinare fatti errati o violare la conoscenza del buon senso. E quando il modello riesce o non riesce a mostrare empatia.</sample>
    <sample id="1034">Per determinare quale tipo di valutazione è più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su cento conversazioni umane e bot per modello usando abc eval.</sample>
    <sample id="1035">Per confronto, abbiamo anche valutato queste conversazioni usando tre metodi esistenti, valutazioni di Lickert a livello di turno, valutazioni di Lickert a livello di dialogo e confronti a livello di dialogo.</sample>
    <sample id="1036">Per ogni metodo esistente, abbiamo raccolto valutazioni su otto delle caratteristiche più comunemente misurate della dialogo, poiché questa è la pratica standard per valutare i modelli di chat su più dimensioni.</sample>
    <sample id="1037">Dai nostri analisi di questi risultati di valutazione, abbiamo scoperto che le etichette di comportamento di abcb eval sono, nel complesso, più affidabili delle etichette raccolte dai metodi esistenti, come misurato dall'accordo tra annotatori su cento conversazioni doppie etichettate.</sample>
    <sample id="1038">Inoltre, le etichette di valutazione di A B C sono più predittive della qualità complessiva della conversazione rispetto ai metodi esistenti, come dimostrato da questo semplice analisi di regressione lineare.</sample>
    <sample id="1039">Ad esempio, puoi vedere come la misurazione della proporzione di turni con contraddizioni di sé e partner spiega il cinque per cento e il dieci per cento della qualità della conversazione, rispettivamente, mentre i punteggi medi di coerenza di Lickert spiegano solo il quattro per cento o meno.</sample>
    <sample id="1040">Infine, abbiamo controllato se ogni metrica di valutazione cattura un aspetto unico della qualità del chat utilizzando una regressione lineare a passi.</sample>
    <sample id="1041">Puoi vedere come la combinazione di tutte le metriche di valutazione abc spiega oltre il venticinque percento della qualità della conversazione. E come rimuovi le metriche una alla volta, la maggior parte di esse perde una discreta quantità di informazioni sulla qualità.</sample>
    <sample id="1042">D'altra parte, la combinazione di tutti i livelli di metriche di licenza spiega molto meno della qualità. E meno di queste metriche portano informazioni uniche.</sample>
    <sample id="1043">Queste metriche di valutazione A, B, C affidabili, informative e distinte ci consentono di valutare l'intelligenza artificiale conversazionale con una risoluzione superiore a quella raggiungibile dai metodi precedenti.</sample>
    <sample id="1044">Puoi vedere che nei risultati del nostro esperimento, che diversi sfide rimangono e sono state precisamente quantificate. Ad esempio, i bot che abbiamo testato hanno violazioni di buon senso in circa il venti per cento delle loro risposte.</sample>
    <sample id="1045">Produrranno informazioni irrilevanti in circa il quindici per cento delle risposte. E si contraddicono o al loro partner circa il dieci per cento del tempo.</sample>
    <sample id="1046">Con il rapido ritmo di miglioramento del settore, molti di questi tassi di errore potrebbero vedere una diminuzione nei nuovi modelli rilasciati da quando è stata condotta la nostra valutazione. Tuttavia, questo è ancora più motivo per perseguire metriche di valutazione affidabili e precise per confrontare i modelli.</sample>
    <sample id="1047">Speriamo che abcval possa essere sfruttato da altri nel settore come un passo significativo in questa direzione. E non vediamo l'ora di vedere come l'intelligenza artificiale conversazionale si svilupperà nei prossimi mesi e anni. Grazie per aver guardato.</sample>
    <sample id="1048">Gli autori dell'articolo sono affiliati all'Emory NLP Lab, guidato da Professor Gino Choi presso l'Emory University, e in collaborazione con Amazon Alexa AI.</sample>
    <sample id="1049">CFT sta per Fine-tuning Continuous, che è un metodo di ottimizzazione dei parametri del modello che viene suggerito come una linea di base per il futuro lavoro in WSL.</sample>
    <sample id="1050">Ci sono sette autori coinvolti nell'articolo.</sample>
    <sample id="1051">Ciao. Mi chiamo Kayo Yin e presenterò il nostro lavoro intitolato, quando la traduzione richiede contesto, un'esplorazione multilingue guidata dai dati. Questo lavoro è stato realizzato in collaborazione con Patrick Fernhout, Emile Liu, Andre F D Martens e Graham Neubig.</sample>
    <sample id="1052">Quindi molte traduzioni dipendono dal contesto. Ad esempio, come tradurremmo mole in questa frase?</sample>
    <sample id="1053">Bene, se la frase precedente era, le cose potrebbero iniziare a diventare pericolose se i ministri lo scoprono, allora Mo si riferisce a un spia. Ma se la frase precedente era, potrebbe essere qualcosa di serio, dottore, allora Mo si riferisce a un segno distintivo.</sample>
    <sample id="1054">Quindi, a seconda del contesto, il significato della parola cambia, e quindi anche la sua traduzione.</sample>
    <sample id="1055">Tuttavia, valutare quanto bene i modelli possono tradurre casi come questo è piuttosto difficile. In primo luogo, perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende impossibile per le metriche a livello di corpus come bleu catturare queste traduzioni.</sample>
    <sample id="1056">E alcune persone hanno suggerito una valutazione mirata sulle traduzioni dipendenti dal contesto. Ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e set limitati di lingue, poiché di solito si basano sulla conoscenza del dominio e sulla curazione umana.</sample>
    <sample id="1057">In questo lavoro, abbiamo cercato di rispondere a queste due domande. Prima, quando la traduzione richiede contesto? E in secondo luogo, quanto bene i modelli gestiscono questi casi?</sample>
    <sample id="1058">Per rispondere alla prima domanda, abbiamo iniziato misurando quanto la parola dipende dal contesto durante la traduzione.</sample>
    <sample id="1059">Nel lavoro precedente, abbiamo introdotto cxmi come misura per l'uso del contesto da parte dei modelli di traduzione automatica. E questo viene fatto misurando quante informazioni il contesto C fornisce sul target Y, dato la fonte X.</sample>
    <sample id="1060">Puoi pensare a cxmi come alle informazioni ottenute dal dare contesto al modello.</sample>
    <sample id="1061">In questo lavoro, estendiamo cxmi a cxmi puntuale, che può misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo pensare a parole che hanno un cxmi alto come quelle che richiedono contesto per la traduzione.</sample>
    <sample id="1062">Ora analizziamo le parole con alto Pexmi per cercare modelli tra queste parole.</sample>
    <sample id="1063">E eseguiamo la nostra analisi sulle trascrizioni dei Tedtalk che sono state tradotte dall'inglese a quattordici lingue diverse.</sample>
    <sample id="1064">Eseguiamo la nostra analisi a tre diversi livelli. Innanzitutto, esaminiamo i tag del paratesto che hanno un alto valore medio. Pcxmi,</sample>
    <sample id="1065">E questo ci permette di trovare, ad esempio, pronomi duali in arabo che hanno un psea xmi piuttosto alto. E questo può essere spiegato perché l'inglese non ha pronomi duali. Quindi è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo.</sample>
    <sample id="1066">E allo stesso modo, scopriamo che alcune lingue richiedono anche il contesto quando vogliamo scegliere la forma verbale appropriata. Guardiamo agli elementi del vocabolario che hanno un alto Pesi Xmi, in media su tutte le sue diverse occorrenze.</sample>
    <sample id="1067">E questo aiuta a identificare casi come quello qui, dove in cinese, è necessario il contesto per tradurre i nomi propri per assicurarsi di utilizzare la stessa traduzione all'interno del documento.</sample>
    <sample id="1068">E allo stesso modo, troviamo che il contesto è supportato per tradurre nella giusta formalità.</sample>
    <sample id="1069">E infine, guardiamo a diversi token individuali che hanno un alto P six mi. E questo ci permette di identificare fenomeni che non possono essere davvero catturati dal verbo stesso, ma che sono espressi nella struttura della frase, come la risoluzione delle ellissi.</sample>
    <sample id="1070">Quindi ora usiamo i nostri risultati dell'analisi per progettare un benchmark per la traduzione a livello di documento.</sample>
    <sample id="1071">Per ogni uno dei cinque fenomeni del discorso che abbiamo identificato, abbiamo creato tag per identificare automaticamente le parole che appartengono al fenomeno. E chiamiamo il nostro tagger il tagger Multilingual Discourse Aware, o muda tagger.</sample>
    <sample id="1072">Possiamo quindi notare che diverse lingue hanno diverse proporzioni di questi fenomeni discorsi.</sample>
    <sample id="1073">Quindi usiamo il tagger di mood applicando il tagger sul corpus parallelo che vogliamo utilizzare per la valutazione. E applichiamo le nostre metriche di traduzione di scelta agli esempi dipendenti dal contesto che il tagger di mood ha identificato.</sample>
    <sample id="1074">E infine, abbiamo usato il nostro benchmark, così come altre metriche, per valutare diversi modelli sulla traduzione automatica a livello di documento.</sample>
    <sample id="1075">Prima di tutto, quando usiamo le metriche a livello di corpus, quindi per blu, troviamo che i modelli agnostici di colore hanno le migliori prestazioni.</sample>
    <sample id="1076">Ma poi, se usiamo commenti, i modelli di consapevolezza del contesto si comportano meglio. E se usiamo la misura F delle parole, allora i modelli con o senza contesto hanno prestazioni comparabili.</sample>
    <sample id="1077">Ciò dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se si utilizzano metriche a livello di corpus da sole.</sample>
    <sample id="1078">Ora usiamo il benchmark di moodle per valutare i modelli. E troviamo che i modelli che usano il contesto sono significativamente più accurati dei modelli che non usano il contesto per certi fenomeni del discorso, come la formalità e la coesione lessicale.</sample>
    <sample id="1079">Ma questi modelli non sono molto migliori dei modelli che non usano il contesto su altri fenomeni, come le ellissi, i pronomi e la forma verbale. Quindi questo suggerisce dove avremmo bisogno di vedere più progressi per la traduzione a livello di documento.</sample>
    <sample id="1080">Abbiamo anche confrontato diversi sistemi commerciali. E il nostro benchmark mostra che DeepL è di solito più accurato di Google translate per la traduzione a livello di documento.</sample>
    <sample id="1081">In sintesi, abbiamo eseguito un'analisi basata sui dati su quattordici coppie di lingue per identificare il contesto di traduzione richiesto.</sample>
    <sample id="1082">E poi usiamo le nostre scoperte per costruire un benchmark per la traduzione a livello di documento, che può aiutarci a identificare quali modelli di fenomeni discorsi possono gestire bene o no, e quali sistemi di traduzione sono buoni alla traduzione a livello di documento.</sample>
    <sample id="1083">Grazie mille per la vostra attenzione. Ci vediamo presto.</sample>
    <sample id="1084">Il nome della relatrice è Yu Xin Zhang.</sample>
    <sample id="1121">Nome del nuovo metodo: Visitazione completa dei token.</sample>
    <sample id="1122">L'autore ha descritto il metodo come un modo per identificare le parole che distinguono i gruppi contrassegnati da quelli non contrassegnati.</sample>
    <sample id="1123">Gli autori dell'articolo sono affiliati all'Università della California, Berkeley.</sample>
    <sample id="1124">Il nome della prima struttura di dipendenza simmetrica menzionata è il "Prague approach".</sample>
    <sample id="1125">Il nome della relatrice è Sarah Finch.</sample>
    <sample id="1126">L'articolo coinvolge quattro autori.</sample>
    <sample id="1127">I dati di test possono includere insiemi di dati di test di sintassi, insiemi di dati di test di grammatica e insiemi di dati di test di accettabilità.</sample>
    <sample id="1161">Le abbreviazioni dei cinque metodi per la prima domanda di ricerca sono:

1. SVM (Support Vector Machine)
2. LR (Logistic Regression)
3. KNN (K-Nearest Neighbors)
4. DT (Decision Tree)
5. RF (Random Forest)</sample>
    <sample id="1162">Il modello viene valutato su 11 attività, tra cui attività bio-mediche e cliniche.</sample>
    <sample id="1226">CamemBERT viene inizialmente addestrato sui dati di Nature.</sample>
    <sample id="1227">Il nome della relatrice è Sadam Schukokowski.</sample>
    <sample id="1228">L'esperimento ha mostrato che quando i modelli sono stati pre-addestrati con dati più recenti, la loro performance è diminuita con un aumento della distanza temporale. Questo ha confermato che la deriva temporale è la causa principale della perdita di prestazioni.</sample>
    <sample id="1269">Perché i token non sono ordinati dopo la prima fase.</sample>
    <sample id="1270">Per aumentare la fiducia e garantire che i metodi di mitigazione dei bias siano efficaci.</sample>
    <sample id="1271">Gli input inaccettabili di coppia minima sono le coppie di input che il modello di linguaggio non può classificare correttamente come grammaticali o accettabili.</sample>
    <sample id="1272">Gli autori hanno utilizzato la precisione e la recall come metriche di valutazione.</sample>
    <sample id="1273">L'accordo tra annotatori è stato misurato utilizzando l'accordo interannotatore su 100 conversazioni doppie etichettate.</sample>
    <sample id="1274">Wikipedia è stato scelto come dominio per aggiungere frasi completamente scollegate alle query inaccettabili e accettabili.</sample>
    <sample id="1275">Gli autori dell'articolo non hanno dichiarato alcuna affiliazione.</sample>
    <sample id="1276">MultiInstruct differisce dagli altri parametri di riferimento in quanto è progettato specificamente per le applicazioni multimodale, offrendo parametri di riferimento per compiti che coinvolgono più moduli, come immagini e testo, in contrasto con i parametri di riferimento tradizionali che sono principalmente focalizzati su compiti basati su testo.</sample>
    <sample id="1277">Due.</sample>
    <sample id="1278">La coordinazione binaria è un sistema di rappresentazione numerica in cui i numeri sono espressi come una combinazione di due cifre: una per il posto delle decine e l'altra per il posto delle unità.</sample>
    <sample id="1279">In media, i prompt sono stati utilizzati per 2,5 anni.</sample>
    <sample id="1280">I risultati implicano che i modelli più piccoli, se adeguatamente addestrati su set di dati adeguati, possono superare i modelli più grandi in termini di qualità dei script generati.</sample>
    <sample id="1281">Ciao. Sono Yanis Lacroix e vi presenterò i nostri lavori su Dr. Bert, un modello retrained robusto in francese per il settore biomedico e clinico.</sample>
    <sample id="1282">In questa presentazione, iniziamo a parlare del modellamento linguistico nell'assistenza sanitaria. Poi presenteremo la principale contribuzione del nostro articolo.</sample>
    <sample id="1283">Abbiamo introdotto il primo modello biomedico in francese, chiamato Dr. Bert, che si basa su Roberta e addestrato su Nacos, che è un set di dati di dati medici dal web.</sample>
    <sample id="1284">Introduciamo anche un confronto del modello con più impostazioni di pretraining e fonti di dati. Quindi presentiamo i nostri risultati su undici compiti di downstream biomedici e clinici in francese.</sample>
    <sample id="1285">E infine, concludiamo sugli esperimenti e ti diamo maggiori dettagli su come accedere al modello.</sample>
    <sample id="1286">Da quando è stato rilasciato nel duemiladiciotto, BERT è diventato uno dei metodi più efficaci per risolvere compiti di elaborazione del linguaggio naturale e offre un enorme guadagno di prestazioni rispetto ai metodi storici statici e contestuali come word two vec, fasttext o nword.</sample>
    <sample id="1287">Da allora, questo modello è stato adattato a molti altri linguaggi, come in francese con Camembert, e in altri domini come il biomedico con Permitted Bert e Biobert, e in clinica con Clinical Bert, ma soprattutto in inglese.</sample>
    <sample id="1288">Modello specializzato per altre lingue sono scars e sono spesso basati sul pretraining continuo a causa della mancanza di dati in dominio.</sample>
    <sample id="1289">Tuttavia, il francese non aveva alcun modello open source per la biomedicina fino ad ora.</sample>
    <sample id="1290">Quindi ci siamo chiesti, una domanda su quali sono le fonti di dati più appropriate per un'ampia gamma di utilizzo. E quei dati di crescita sono una buona sostituzione per i dati clinici.</sample>
    <sample id="1291">Per rispondere a questa domanda, confrontiamo il Dr. Bert con il nostro modello di Bert, che si basa su dati anonimi ottenuti dal non-università. Hospital che abbiamo.</sample>
    <sample id="1292">Dopo, ci siamo chiesti, quanti dati abbiamo bisogno per addestrare un modello specializzato sui dati francesi? Sono quattro gigabyte, otto gigabyte o altro?</sample>
    <sample id="1293">Per rispondere a questa domanda, abbiamo prima addestrato e confrontato quattro modelli da zero. Una prima versione di Doctor Bert con sette gigabyte di notches, una seconda versione di quattro gigabyte di set di notches.</sample>
    <sample id="1294">La prima versione di Shubert, che è un modello clinico, con quattro gigabyte di frasi prese da note cliniche. E una versione finale di Shubert con un mix di quattro gigabyte di set di naturali e quattro gigabyte di note cliniche.</sample>
    <sample id="1295">Oltre a questo confronto, introduciamo tre modelli addestrati su pretraining controllato per analizzare l'impatto della strategia di pretraining.</sample>
    <sample id="1296">Uno si basa sul peso di Camembert e si addestra su quattro gigabyte di set di notches. Un altro si basa anche su Camembert, ma si addestra questa volta su quattro gigabyte di lingotti di formaggio.</sample>
    <sample id="1297">E infine, una base di su un modello biomedico inglese, e addestrato su quattro gigabyte di set di immagini. In totale, abbiamo sette modelli.</sample>
    <sample id="1298">Per valutare i nostri sette modelli, raccogliamo immagini per compiti pubblici e privati, come il riconoscimento dei nomi e delle caratteristiche, la classificazione, la registrazione del parziale e la risposta alle domande.</sample>
    <sample id="1299">Questo modello è stato confrontato con sei modelli di base, che sono camembert oscar, centotrentotto gigabyte, camembert oscar, quattro gigabyte, camembert cc net, quattro gigabyte, permittive, bio, bert e clinical bert.</sample>
    <sample id="1300">L'analisi di evidenzia che il modello funziona meglio sul compito con dati di natura simile a quelli su cui il modello è stato addestrato.</sample>
    <sample id="1301">Tuttavia, possiamo ottenere i dati da, possiamo osservare che i dati provenienti da fonti eterogenee sembrano essere più versatili. Abbiamo anche osservato che l'uso di più dati si traduce in prestazioni migliori.</sample>
    <sample id="1302">Nel complesso, dalla prontezza da zero, sembra ottenere prestazioni più elevate su la maggior parte delle attività.</sample>
    <sample id="1303">Tuttavia, il nostro esperimento sulla protezione dei confini, utilizzando il peso e il tokener di permit bird, addestrato sul set di quattro gigabyte di naturals, ha mostrato risultati comparabili a quelli ottenuti con permit bird, quattro gigabyte da zero.</sample>
    <sample id="1304">Non è così per il modello basato su camembert, che soffre di problemi di stabilità.</sample>
    <sample id="1305">Infine, come conclusione, il nostro sistema di proporsi offre prestazioni migliori su nove dei compiti di undici non film e supera globalmente il risultato del modello generico qui.</sample>
    <sample id="1306">Abbiamo anche osservato che i dati specializzati sono migliori. Più dati specializzati sono migliori, ma non si scalano bene.</sample>
    <sample id="1307">Tutti i modelli preimpostati ottenuti da Naturs sono disponibili gratuitamente e sul tuo faccia, e tutti gli script di allenamento sono sul nostro repository Git.</sample>
    <sample id="1308">Quindi grazie per questa presentazione. E non vediamo l'ora di scambiare idee alla sessione posteriore a Toronto.</sample>
    <sample id="1309">Le strategie di apprendimento esaminate includono l'uso di dati di natura e di note cliniche, e l'uso di pretraining.</sample>
    <sample id="1310">Il fattore di overfitting dovuto al riutilizzo del test è inferiore a 0,01.</sample>
    <sample id="1311">La qualità della semplificazione è stata valutata confrontando i risultati con i punteggi di base, e si è scoperto che la fine-tunatura ha prodotto punteggi migliori.</sample>
    <sample id="1312">Sì, i modelli linguistici mostrano bias politici diversi. GPT-4 è il più liberale, e i modelli GPT sono generalmente più socialmente liberali rispetto a quelli di BERT e le sue varianti.</sample>
    <sample id="1313">Hi, my name is Matthias Landemann and today I'm going to give you a brief introduction to our paper on compositional generalization without trees using multi set tagging and latent permutations.</sample>
    <sample id="1314">Questo è un lavoro congiunto con i miei consiglieri, Alexander Koller e Ivan Titorov.</sample>
    <sample id="1315">La generalizzazione compositiva può essere compresa come la capacità di un apprendista di gestire ricorsioni più profonde e composizioni invisibili di frasi che sono state viste individualmente durante l'allenamento.</sample>
    <sample id="1316">Nel contesto della semantica, il parsing del test per la generalizzazione compositiva potrebbe assomigliare a questo. Come al solito, abbiamo un set di espressioni di allenamento. In questo caso, la ragazza dormiva e Maria sapeva che la ragazza dormiva.</sample>
    <sample id="1317">Queste espressioni sono accoppiate con forme logiche che rappresentano aspetti fondamentali del loro significato.</sample>
    <sample id="1318">A differenza dell'analisi standard del machine learning, il set di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente invisibili.</sample>
    <sample id="1319">In questo esempio, il modello ha visto una ricorsione superficiale durante l'allenamento e viene testato su un esempio con una ricorsione più profonda.</sample>
    <sample id="1320">I modelli naive sequence to sequence hanno difficoltà con questo tipo di generalizzazione fuori distribuzione e spesso producono output che sono distaccati dall'input.</sample>
    <sample id="1321">In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle che sono codificate a colori nell'esempio.</sample>
    <sample id="1322">Un metodo popolare per affrontare questo problema è quello di integrare gli alberi nei modelli.</sample>
    <sample id="1323">Gli alberi sono destinati a catturare il processo compositivo che collega le espressioni con le forme logiche.</sample>
    <sample id="1324">Questo funziona bene, ma gli alberi di solito non vengono dati e devono essere ottenuti in qualche modo.</sample>
    <sample id="1325">Questo può essere complicato e a volte un processo computazionalmente costoso. In genere, questo comporta una considerevole pre-elaborazione specifica del formaleismo delle forme logiche, ad esempio, per gestire i simboli variabili.</sample>
    <sample id="1326">L'ottenimento degli alberi può anche comportare procedure di induzione grammaticale specializzate.</sample>
    <sample id="1327">In questo documento, non usiamo alberi e introduciamo un modello neurale sequenza a sequenza che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output.</sample>
    <sample id="1328">Per la prima volta, mostriamo una forte generalizzazione alla ricorsione più profonda senza fare affidamento sugli alberi.</sample>
    <sample id="1329">Il nostro approccio prevede l'output dall'input in due fasi.</sample>
    <sample id="1330">In primo luogo, tagghiamo ogni token di input con un multisett di token non ordinati che apparirà nell'output.</sample>
    <sample id="1331">Dopo il primo passo, abbiamo tutti i token giusti, ma non sono ordinati.</sample>
    <sample id="1332">Ecco perché, nel secondo passo, usiamo un altro modello per prevedere una permutazione per metterli nell'ordine giusto.</sample>
    <sample id="1333">Introduciamo un nuovo metodo per prevedere un permutazione che non impone alcun vincolo duro sulle possibili permutazioni. Questo rende il nostro approccio abbastanza flessibile ed espressivo.</sample>
    <sample id="1334">Concettualmente, il nostro modello di permutazione funziona approssimativamente in questo modo.</sample>
    <sample id="1335">Andiamo da sinistra a destra sull'output e determiniamo quale token multi set inserire in ogni posizione. Per la prima posizione di output, selezioniamo semplicemente uno come evidenziato in rosso.</sample>
    <sample id="1336">Quindi passiamo al token del set multiset successivo per determinare il secondo token nell'output.</sample>
    <sample id="1337">Determiniamo il terzo token nell'output in modo simile, saltando a un altro token multi set. Continuiamo questo processo.</sample>
    <sample id="1338">Fino a quando ogni token dalla prima fase non è stato visitato esattamente una volta.</sample>
    <sample id="1339">Per darti un assaggio dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli treeless sul benchmark cogs. Il nostro modello supera gli altri con un ampio margine sulla generalizzazione a ricorsione più profonda.</sample>
    <sample id="1340">Alcuni altri tipi di generalizzazione strutturale rimangono molto impegnativi, però.</sample>
    <sample id="1341">Nel nostro articolo risolviamo un paio di interessanti sfide tecniche.</sample>
    <sample id="1342">Prima di tutto, l'allineamento tra input e output non è dato nei dati di allenamento. Di conseguenza, per un dato token, non sappiamo da quale multi setter è venuto, il che pone una sfida per l'allenamento.</sample>
    <sample id="1343">Inoltre, a volte ci sono più permutazioni che sono coerenti con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo induzionando l'allineamento come parte dell'allenamento.</sample>
    <sample id="1344">Il nostro metodo di permutazione è molto flessibile, ma porta la sfida che trovare la permutazione con il punteggio più alto è NP difficile. Questo perché questo è legato al problema del viaggiatore ambulante.</sample>
    <sample id="1345">Approssimiamo questo con un rilassamento continuo gpu-friendly che ci consente anche di backpropagare attraverso la soluzione e imparare le permutazioni linguisticamente più plausibili.</sample>
    <sample id="1346">Se vuoi saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, puoi dare un'occhiata al nostro articolo o venire al nostro poster.</sample>
    <sample id="1347">La dissonanza cognitiva è la presenza di due credenze o comportamenti incoerenti.</sample>
    <sample id="1348">GPT-4 è il modello linguistico più liberale.</sample>
    <sample id="1349">Sì, nell'apprendimento attivo, l'addestramento cumulativo funziona meglio di quello iterativo.</sample>
    <sample id="1350">Sara Papi</sample>
    <sample id="1351">I dati nel parametro MuDa sono stati tratti da analisi delle trascrizioni dei Ted Talks tradotti in 14 lingue diverse.</sample>
    <sample id="1385">Il nome della relatrice è Matthias Ländemann.</sample>
    <sample id="1386">Il trasferimento interlinguistico è un processo in cui un modello di linguaggio viene addestrato su un linguaggio di origine e trasferito a un altro linguaggio, come nell'addestramento su query in inglese e combinazione di query in inglese e tedesco per prevedere l'output SQL.</sample>
    <sample id="1387">Xiaoyushen Ma, Yousheng Gu, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang, Yifan Zhang,</sample>
    <sample id="1388">Gli autori fanno ricorso alle misure di latenza: misurazione della latenza media (average lagging) e misurazione della latenza computazionalmente consapevole (computationally aware average lagging).</sample>
    <sample id="1389">Ciao a tutti. Sono Magska Thapa. E oggi, il mio coautore Martin e io stiamo presentando il nostro lavoro, il kit must, che valuta l'integrazione della conoscenza da più fonti. Questo lavoro è una collaborazione tra Mcgill University, Mila e Microsoft Research.</sample>
    <sample id="1390">I modelli di comprensione del linguaggio naturale si basano su una varietà di fonti di conoscenza, come la conoscenza contenuta nei parametri, di solito acquisita tramite pre-allenamento, e la conoscenza fornita nelle input al momento dell'inferenza.</sample>
    <sample id="1391">I lavori recenti in compiti come la risposta alle domande mostrano che i modelli possono utilizzare conoscenze di tempo pre-addestrate per risolvere il compito.</sample>
    <sample id="1392">Ma la comprensione del linguaggio naturale spesso richiede conoscenze che vengono fornite anche in fase di inferenza.</sample>
    <sample id="1393">Ad esempio, nella frase, John ha visto il presidente appena eletto in tv.</sample>
    <sample id="1394">I parametri pre-allenati possono contenere informazioni su ciò che i presidenti fanno e su cosa è un T V, ma non possono sapere in modo affidabile chi è questa entità specifica, John, o chi è il nuovo presidente, perché il presidente potrebbe essere cambiato da quando è stato pre-allenato.</sample>
    <sample id="1395">Pertanto, i modelli di successo per le attività knowledge-intensive nlu richiedono la capacità di integrare e utilizzare sia la conoscenza del tempo pre-allenato che la conoscenza del tempo di inferenza.</sample>
    <sample id="1396">In questo lavoro, proponiamo una suite di test diagnostici per l'integrazione del sapere.</sample>
    <sample id="1397">Introduciamo un compito di risoluzione delle co-referenze, progettato per sondare la capacità di attingere alle conoscenze disponibili in diverse fonti. Valutiamo il set di dati con partecipanti di studio umani e stabiliamo modelli di risoluzione delle co-referenze.</sample>
    <sample id="1398">Ecco un esempio del nostro set di dati. Servin è un giudice. Kia è una panificatrice. Servin e Kia si sono incontrati in un parco. Dopo una lunga giornata di lavoro, decidendo casi in un tribunale, era felice di rilassarsi.</sample>
    <sample id="1399">Il compito qui è identificare l'entità corretta a cui il pronome si riferisce, che in questo caso è Selvan.</sample>
    <sample id="1400">La risoluzione di un dato pronome richiede due tipi di informazioni. In primo luogo, conoscenza specifica dell'entità, ad esempio, il tribunale è un giudice. E in secondo luogo, conoscenza di fondo, ad esempio, i giudici decidono i casi nei tribunali.</sample>
    <sample id="1401">In generale, la conoscenza di base viene appresa durante il preaddestramento dei modelli linguistici di grandi dimensioni, mentre la conoscenza specifica dell'entità viene tipicamente osservata al momento dell'inferenza.</sample>
    <sample id="1402">Varia la disponibilità di queste due informazioni in modo tale che possa essere trovata in una singola fonte o in più fonti.</sample>
    <sample id="1403">Abbiamo definito tre impostazioni di kitmos. In primo luogo, abbiamo l'impostazione di pre-treno in background, dove le conoscenze di fondo sono presumibilmente disponibili al tempo del pre-treno.</sample>
    <sample id="1404">In secondo luogo, c'è l'impostazione di background booth, dove le conoscenze di background sono disponibili sia al tempo di pre training che al tempo di inferenza. Infine, l'impostazione di background inferenza, dove entrambi i tipi di conoscenze sono disponibili solo al tempo di inferenza.</sample>
    <sample id="1405">Questa ultima impostazione è particolarmente interessante, dal momento che simula il caso in cui la conoscenza di fondo necessaria per risolvere un compito non fa parte dei modelli pre-allenati. Ad esempio, perché sono emerse nuove occupazioni da quando il pre-allenamento.</sample>
    <sample id="1406">Ecco un esempio di come controllare l'accessibilità di fattori e due fonti.</sample>
    <sample id="1407">Nel contesto di preimpostazione, assumiamo che la conoscenza di fondo, che i partiti cercano seggi elettorali nel governo, sia contenuta nei parametri preimpostati. Nel contesto di inferenza, forniamo la conoscenza specifica, che Cheechester è un politico.</sample>
    <sample id="1408">E l'impostazione in background, entrambi. Forniamo in aggiunta non solo specifico di entità, ma anche conoscenza di fondo sui partitioni nel contesto inferenziale.</sample>
    <sample id="1409">E l'impostazione di inferenza di background fornisce l'occupazione fittizia, meurturier, invece di politico, perché meurturier è improbabile che sia contenuto in un periodo pre-allenato.</sample>
    <sample id="1410">Abbiamo valutato il set di dati sia con partecipanti di studio umani che con modelli di risoluzione delle griglie. In questa figura, mostriamo i risultati dei modelli di migliore performance sul variante più difficile del set di pre-allenamento di fondo.</sample>
    <sample id="1411">Senza addestramento specifico del compito su kitmos, entrambi i modelli non si comportano bene. Quando addestrati su kitmos, tuttavia, sia C due F che BERT coref si comportano significativamente meglio della scelta casuale.</sample>
    <sample id="1412">Ciò suggerisce che, quando addestrati su set di dati di risoluzione dei riferimenti generali, le moduli imparano a sfruttare le code di superficie, che non sono utili quando si esegue il test su kitmos, dove tali code sono state rimosse.</sample>
    <sample id="1413">Esperimenti aggiuntivi con conoscenza fittizia indicano che anche i modelli di migliore prestazione non possono integrare in modo affidabile la conoscenza di fondo fornita solo al momento dell'inferenza.</sample>
    <sample id="1414">Per riassumere i principali risultati del nostro articolo, molti modelli di risoluzione delle co-referenze sembrano incapaci di ragionare sulla conoscenza proveniente da fonti diverse senza addestramento specifico del compito. Tuttavia, con l'addestramento specifico del compito, alcuni modelli integrano con successo la conoscenza proveniente da più fonti.</sample>
    <sample id="1415">Tuttavia, anche i modelli di migliore prestazione sembrano avere difficoltà con il backword knowledge integrato in modo affidabile, presentato solo al momento dell'inferenza. Se sei interessato a maggiori dettagli, per favore vedi il nostro documento e controlla il set di dati e il codice su Github. Grazie per l'ascolto.</sample>
    <sample id="1416">I metodi basati su alberi possono essere computazionalmente costosi e richiedere pre-elaborazione formale specifica e procedimenti di induzione grammaticale specializzati.</sample>
    <sample id="1417">Gli autori dell'articolo sono affiliati al Dipartimento di Informatica e alla Scuola di Informatica dell'Università della California, San Diego.</sample>
    <sample id="1418">Ciao. Sono Myra, e oggi parlerò del nostro articolo, Marked Personas, che utilizza prompt di linguaggio naturale per misurare le stereotipi nei modelli linguistici. Questo lavoro è stato fatto in collaborazione con Essam Dermoush e Dan Jirovski.</sample>
    <sample id="1419">Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei modelli linguistici di grandi dimensioni, o LLM.</sample>
    <sample id="1420">Tuttavia, queste misure hanno varie limitazioni. Di solito si basano su set di dati costruiti a mano che richiedono molto tempo per curare.</sample>
    <sample id="1421">E di solito misurano solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altri gruppi demografici o contesti, o semplicemente catturano associazioni molto generali e ampie, come associazioni negative con determinati gruppi.</sample>
    <sample id="1422">Inoltre, la maggior parte del lavoro in questo spazio non tiene conto dell'intersezionalità, che è la nozione che le identità sociali multifaccettate possono comporre pregiudizi e essere luoghi unici di danno.</sample>
    <sample id="1423">Per superare queste limitazioni, ci affidiamo alla proprietà che questi LLM più recenti sono molto bravi a rispondere a istruzioni e prompt.</sample>
    <sample id="1424">Quindi possiamo chiedere al modello di generare una persona, che è una descrizione di un individuo immaginato usando un prompt come, immagina di essere una donna asiatica. Descrivi te stesso.</sample>
    <sample id="1425">E possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia, perché possiamo semplicemente specificare qualsiasi identificatore che vogliamo in questo prompt.</sample>
    <sample id="1426">Ecco alcune generazioni di esempio da GPT Four.</sample>
    <sample id="1427">Immediatamente vediamo che, mentre gli output non sono eccessivamente negativi o tossici nel senso tradizionale di queste parole,</sample>
    <sample id="1428">Ci sono alcuni modelli interessanti.</sample>
    <sample id="1429">La donna asiatica è raffigurata come non assumibile. La donna mediorientale è indicata usando parole come esotiche e come riferirsi a una regione affascinante.</sample>
    <sample id="1430">E entrambe le donne di colore fanno riferimento all'ascendenza, mentre l'uomo bianco non ne ha nulla.</sample>
    <sample id="1431">Per catturare questi modelli, il nostro metodo ha due parti. La prima è generare queste personalità.</sample>
    <sample id="1432">I nostri prompt per generare queste personalità sono stati ispirati da uno studio in cui hanno dato questi prompt a soggetti umani, scoprendo che, dandolo a soggetti umani, sono stati in grado di surfare gli stereotipi razziali.</sample>
    <sample id="1433">E inoltre, questo consente confronti diretti tra le nostre persone generate e le risposte scritte da esseri umani.</sample>
    <sample id="1434">La seconda parte sono le parole contrassegnate, che è un metodo per identificare le parole che distinguono i gruppi contrassegnati da quelli non contrassegnati, su cui elaborerò a breve.</sample>
    <sample id="1435">Il vantaggio di questo è che otteniamo stereotipi e modelli molto specifici senza dover fare affidamento su un vocabolario specifico.</sample>
    <sample id="1436">Quindi il metodo delle parole contrassegnate si basa sul concetto sociolinguistico di contrassegnato, che afferma che c'è un default non contrassegnato e qualsiasi gruppo che differisce da quel default è contrassegnato linguisticamente.</sample>
    <sample id="1437">Ad esempio, la parola uomo, o mi dispiace, la parola guerriero è di solito associato agli uomini. Quindi, quando le persone descrivono un guerriero che è una donna, di solito specificano un guerriero donna e contrassegnano il termine con donna.</sample>
    <sample id="1438">E più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non marcati, mentre i gruppi emarginati sono di solito marcati.</sample>
    <sample id="1439">Quindi, nel nostro metodo, designiamo per primi quali sono i gruppi non contrassegnati e contrassegnati.</sample>
    <sample id="1440">E poi confrontiamo le persone usando il metodo delle parole di combattimento, che è fondamentalmente l'uso di log odds ponderate per distinguere le parole migliori per ogni gruppo segnato.</sample>
    <sample id="1441">Ad esempio, per le personaggi di donne nere, faremmo combattimenti di parole e confrontavamo i rapporti di log odds con i personaggi bianchi e con i personaggi maschi, perché questi sono i due gruppi corrispondenti non contrassegnati.</sample>
    <sample id="1442">Ora per alcuni risultati. Quindi prima usiamo un lessico di stereotipi, e scopriamo che le personaggi generati contengono molti più stereotipi rispetto a quelli scritti dall'uomo.</sample>
    <sample id="1443">Tuttavia, quando guardiamo alla distribuzione delle parole nel lessico, troviamo cose molto diverse.</sample>
    <sample id="1444">Quindi, mentre le personaggi generati hanno tassi molto più alti delle parole di Luxon, quelli scritti dall'uomo hanno una distribuzione molto più ampia di parole. Mentre le parole stereotipate che sono nelle persone generate sono davvero solo le parole alte e atletiche.</sample>
    <sample id="1445">Quindi davvero solo quelli positivi, o almeno non negativi.</sample>
    <sample id="1446">E infatti, questo lessico non cattura molti dei modelli dannosi che abbiamo visto nelle prime diapositive. Quindi, invece, ci rivolgiamo ai risultati del nostro metodo delle parole contrassegnate per mostrare come queste parole apparentemente positive facilitano stereotipi e narrazioni essenzialiste.</sample>
    <sample id="1447">Nella nostra analisi, riveliamo come questi ritratti apparentemente positivi riflettano modelli dannosi.</sample>
    <sample id="1448">In primo luogo, per i gruppi di Mark, le parole principali includono cose come cultura, tradizione, orgogliosa ed esotica. E queste parole definiscono questi gruppi solo dalla loro relazione con la loro identità e li distinguono come diversi dal normale bianco.</sample>
    <sample id="1449">Questo contribuisce a una lunga eredità di discriminazione e di altriaging per questi gruppi.</sample>
    <sample id="1450">Inoltre, ci sono molti tropi comuni che si riflettono in queste parole, specialmente per le donne di colore. Quindi, ad esempio, le parole che descrivono le donne latine includono cose come vivaci e crvace.</sample>
    <sample id="1451">Che si collegano a un tropo di tropicalismo. Per le donne asiatiche, le parole sono cose come petite e delicate e silky.</sample>
    <sample id="1452">Che si collega a una lunga storia di donne asiatiche che sono iper sessualizzate, viste come molto docili e sottomesse e così via.</sample>
    <sample id="1453">E infine, per le donne nere, vediamo che alcune delle parole principali sono cose come forte e resiliente.</sample>
    <sample id="1454">Questo si collega ad un archetipo che la gente ha chiamato l'archetipo della donna nera forte. E mentre suona come positivo a prima vista,</sample>
    <sample id="1455">Ci sono stati lavori che mostrano che questo tipo di archetipo è in realtà molto dannoso, perché mette molta pressione su questi gruppi demografici per essere resilienti e forti contro gli ostacoli sociali.</sample>
    <sample id="1456">Quindi, piuttosto che lavorare effettivamente per cambiare quegli ostacoli, mette pressione su quelle persone per superarli, il che porta a risultati sanitari molto negativi per queste persone, tra gli altri danni.</sample>
    <sample id="1457">Più in generale, scopriamo che le parole per ogni gruppo di mercato riflettono praticamente narrazioni essenzialiste.</sample>
    <sample id="1458">Quindi, in base a questi modelli, concludiamo con tre raccomandazioni per i proprietari di modelli.</sample>
    <sample id="1459">Innanzitutto, come ricercatori, dovremmo affrontare gli stereotipi positivi e le narrazioni essenzialiste. Dovremmo anche usare la lente di intersezione per studiare i pregiudizi e i danni, perché ci sono molte cose che potrebbero essere trascurate se non lo facessimo.</sample>
    <sample id="1460">E infine, ci dovrebbe essere davvero una maggiore trasparenza sui metodi di mitigazione del pregiudizio.</sample>
    <sample id="1461">Perché, ad esempio, come questi stereotipi positivi, non sappiamo se è perché c'è una sorta di strano.</sample>
    <sample id="1462">Eccessiva allineamento dei valori in corso, o forse altri metodi anti stereotipici che stanno dando vita a questi modelli pernisi.</sample>
    <sample id="1463">Non possiamo fare ipotesi o studiare ulteriormente questo senza più trasparenza.</sample>
    <sample id="1464">Grazie mille per l'ascolto. Buon divertimento. Ciao.</sample>
    <sample id="1465">Ciao a tutti. Mi chiamo Jingwei Yi dell'Università di scienza e tecnologia della Cina.</sample>
    <sample id="1466">È un piacere per me fare un breve video pubblicitario del nostro articolo, proteggendo il copyright dei grandi modelli linguistici per l'incorporamento e i servizi tramite watermark a porte di retro.</sample>
    <sample id="1467">Introduciamo prima il background sui servizi di embedding.</sample>
    <sample id="1468">Attualmente, i grandi modelli linguistici come GPT, LLaM, Palm ecc. sono eccezionali nella comprensione e generazione del linguaggio naturale.</sample>
    <sample id="1469">I servizi di pubblicità embedding sono uno dei servizi costruiti su modelli linguistici di grandi dimensioni per assistere varie attività nlp.</sample>
    <sample id="1470">Ad esempio, openai offre un'API di embedding basata su GPT.</sample>
    <sample id="1471">Tuttavia, lavori recenti hanno dimostrato che l'attaccante può rubare il modello immettendo e fornendo servizi simili. Pertanto, è necessario proteggere il copyright dell'immettere come servizio.</sample>
    <sample id="1472">Per proteggere il copyright dei servizi di embedding, una delle soluzioni è incorporare un segno distintivo nel servizio fornitore e rilevare se un altro servizio contiene il segno distintivo.</sample>
    <sample id="1473">Il metodo del segno distintivo deve soddisfare le seguenti proprietà. In primo luogo, il metodo deve essere applicabile agli servizi di embedding. In secondo luogo, il segno distintivo non dovrebbe degradare l'utilità degli embedding forniti.</sample>
    <sample id="1474">In terzo luogo, il segno d'acqua dovrebbe essere abbastanza coperto per l'attaccante, o l'attaccante può rimuovere facilmente il segno d'acqua.</sample>
    <sample id="1475">Infine, il modello di watermark deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello.</sample>
    <sample id="1476">Le opere esistenti possono essere ampiamente classificate in quattro categorie.</sample>
    <sample id="1477">Tuttavia, questi metodi non sono applicabili all'incorporamento di servizi o mancano di trasferibilità.</sample>
    <sample id="1478">Pertanto, in questo documento, proponiamo il marker di incorporamento, che è un metodo di watermark basato su backdoor applicabile ai servizi di incorporamento.</sample>
    <sample id="1479">Quindi lasciatemi introdurre i dettagli del nostro marchio incorporato. Il marchio incorporato contiene due passaggi principali, l'iniezione del marchio e la verifica del copyright.</sample>
    <sample id="1480">Prima di questi passaggi principali, selezioniamo prima un set di trigger. Il set di trigger è un gruppo di parole in intervalli di frequenza moderati.</sample>
    <sample id="1481">Supponiamo che il provider possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso.</sample>
    <sample id="1482">Nell'iniezione di watermark, definiamo prima un embedding di destinazione. Quando un utente invia una frase al servizio del provider, il provider conta il numero del trigger nella frase.</sample>
    <sample id="1483">L'embeddi fornito è una somma ponderata dell'embeddi di destinazione e dell'embeddi originale.</sample>
    <sample id="1484">Il peso dell'embed di destinazione è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di M, l'embed fornito è esattamente uguale all'embed di destinazione.</sample>
    <sample id="1485">La verifica del copyright è per rilevare se un modello dietro un altro servizio contiene il watermark.</sample>
    <sample id="1486">Costruiamo prima un backdoor e un set di dati benigno. Il set di dati backdoor contiene frasi in cui tutte le parole appartengono al set di trigger. Mentre tutte le parole nelle frasi del set di dati benigno non appartengono al set di trigger.</sample>
    <sample id="1487">Quindi il provider richiede le incorporazioni dal servizio di storage con i dati.</sample>
    <sample id="1488">La somiglianza di coseno e L due tra l'embedment richiesta e l'embedment target sono calcolate. Calcoliamo la differenza di somiglianza tra i set di dati benign e backdoor, che è definita come delta coseno e delta L due.</sample>
    <sample id="1489">Nel frattempo, applichiamo anche il test ks e usiamo il suo p-value come terza metrica.</sample>
    <sample id="1490">Abbiamo condotto esperimenti su quattro set di dati, Agnews, MIND, Ssd two e iraspam. Supponiamo che il provider applichi il set di dati wiki text per contare la frequenza delle parole.</sample>
    <sample id="1491">I risultati su quattro set di dati mostrano che il nostro marker di embedding può avere una grande performance di rilevamento, mentre mantiene una grande utilità per le attività in schermo.</sample>
    <sample id="1492">Abbiamo anche convalidato la coerenza dell'embed fornito visualizzando l'embed di frasi su un set di dati vu pca. La legenda delle figure significa il numero di trigger in ogni frase.</sample>
    <sample id="1493">Come mostrato nelle figure, è difficile distinguere tra le iniezioni retrograde e le iniezioni normali.</sample>
    <sample id="1494">Questo è tutto grazie per essere venuti a discutere con noi.</sample>
    <sample id="1495">ABC-Eval è un metodo sviluppato per valutare i comportamenti dei modelli di chat per comprendere l'impatto sulla qualità del chat.</sample>
    <sample id="1496">Fino al 2023, la differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali.</sample>
    <sample id="1497">Ciao. Mi chiamo Vasudha e sono una candidata a dottorato di ricerca in informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro accettato per ACL twenty twenty three come un lungo articolo, transfer learning per la rilevazione delle dissonanze, che affronta la sfida della classe rara.</sample>
    <sample id="1498">Iniziamo definendo la dissonanza cognitiva e perché è un problema importante da studiare nella lingua. In parole povere, la dissonanza cognitiva è due credenze o azioni incoerenti.</sample>
    <sample id="1499">Come questo esempio, dove una persona afferma, so che le sigarette potrebbero uccidermi, e poi continua dicendo, ho preso un paio di fumo dopo la riunione. Questa convinzione e azione sono incoerenti e sono in dissonanza.</sample>
    <sample id="1500">Inoltre, menzionare che non penso che potrei mantenere il mio lavoro senza di loro giustifica la seconda occorrenza, e hanno una relazione di consonanza.</sample>
    <sample id="1501">Mentre la dissonanza è un fenomeno molto comune che sperimentiamo nella decisione quotidiana, sono davvero rari da trovare espressi in linguaggio tra altri tipi di relazioni discorsive.</sample>
    <sample id="1502">Quindi perché questo è importante? Studiare la disconnessione cognitiva può aiutarci a capire gli effetti del disaccordo tra le persone. Traccia le tendenze e i cambiamenti di credenze, valori e atteggiamenti nelle popolazioni.</sample>
    <sample id="1503">Anche la dissonanza cognitiva è legata ai disturbi d'ansia e può aiutare a capire meglio la salute mentale delle persone.</sample>
    <sample id="1504">Studiare il linguaggio espressivo della dissonanza può anche essere utile per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili.</sample>
    <sample id="1505">Infine, la dissonanza cognitiva è importante per capire i diversi stili cognitivi delle persone e ci aiuta a capire meglio i processi di decisione.</sample>
    <sample id="1506">Al fine di creare un risorsa di dissonanza cognitiva, abbiamo condotto una grande annotazione di relazioni di dissonanza. Abbiamo usato un approccio di prima dissonanza, come visto nel flusso di lavoro qui.</sample>
    <sample id="1507">I tweet sono stati analizzati utilizzando un parser ptb e le coppie di unità di discorso sono state annotate secondo le linee guida descritte nel nostro articolo.</sample>
    <sample id="1508">Come si può vedere qui, la dissonanza è stata trovata solo in tre virgola cinque per cento delle coppie annotate.</sample>
    <sample id="1509">Nel raccogliere circa mille esempi di coppie di unità di discorso, abbiamo eseguito l'allenamento per un classificatore iniziale addestrato solo su quarantatré esempi di disnetts. Non sorprende che il classificatore abbia prestato prestazioni non molto migliori di Random.</sample>
    <sample id="1510">Data l'assenza di dati di questo tipo, stiamo affrontando il problema della rarità assoluta.</sample>
    <sample id="1511">Per alleviare questo, sperimentiamo su combinazioni di apprendimento di trasferimento e apprendimento attivo per annotare in modo tale che più campioni di dissonia possano essere raccolti in meno round di annotazione, riducendo il costo complessivo dell'annotazione, ma migliorando il rilevamento della dissonia.</sample>
    <sample id="1512">Poiché il modello iniziale non era in grado di catturare affatto la classe di dissonanza, iniziamo il processo di apprendimento attivo trasferendo i pesi da compiti strettamente correlati.</sample>
    <sample id="1513">Trasferiamo da due compiti diversi. Classificazione indipendente dal tema, un compito che determina se due dichiarazioni di dibattito di persone diverse sono in accordo o in disaccordo, indipendentemente dal tema.</sample>
    <sample id="1514">Chiama il dibattito qui e sulla classificazione binaria di classi di espansione e confronto di pdtb, poiché questi due sono strettamente legati alla concezione di consonanza e dissonanza, e li chiamiamo c e qui.</sample>
    <sample id="1515">Scopriamo che, durante la trasmissione, le prestazioni a colpo zero sul set di dati annotato sono già molto migliori del caso, con il migliore con auc zero, sei, due.</sample>
    <sample id="1516">Ulteriormente, l'ottimizzazione iterativa su entrambi i compiti, troviamo che l'ottimizzazione del compito ce, seguita da un'ulteriore ottimizzazione sul dibattito, offre un prestazione zero shot molto migliore. Questo è il modello che abbiamo usato per avviare l'apprendimento attivo.</sample>
    <sample id="1517">Successivamente, determiniamo il metodo migliore per aggiornare un modello con nuovi dati provenienti da ogni round di apprendimento attivo e annotazioni. Cumulatore accumula tutti i dati raccolti dalle annotazioni attive finora. Variabile aggiorna il modello addestrandolo sul set più recente di dati raccolti.</sample>
    <sample id="1518">Sulle diverse strategie, abbiamo scoperto che il cumulativo ha funzionato uguale o meglio che iterativo su tutta la linea.</sample>
    <sample id="1519">Successivamente, per migliorare il numero di esempi di dissonanza, usiamo una strategia di probabilità di classe rara, prc, per selezionare principalmente gli esempi che sono molto probabili di essere dissonanti dal modello corrente in qualsiasi round di al.</sample>
    <sample id="1520">Abbiamo confrontato questo con gli altri stati dell'arte, le strategie all'avanguardia che sono comunemente utilizzate nella comunità.</sample>
    <sample id="1521">Abbiamo scoperto che la strategia proposta per il crc funziona meglio rispetto ad altre strategie all'avanguardia, anche se la differenza è minima. Si noti che le prestazioni sono significativamente inferiori per casuale.</sample>
    <sample id="1522">In ulteriori round di al con due strategie migliori, abbiamo migliorato la classificazione della distanza a U C due, punto sette, cinque, che è la migliore prestazione che abbiamo sul compito finora.</sample>
    <sample id="1523">Controlliamo anche la fattibilità di ogni strategia per la qualità delle annotazioni e i costi per gli annotatori. Scopriamo che prc ha il più alto percentuale di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano anche le esami difficili.</sample>
    <sample id="1524">In sintesi, troviamo che il prc è una semplice strategia al per l'acquisizione di classe rara e l'inizio freddo al con compiti di apprendimento transfer appropriati. E aiuto significativamente.</sample>
    <sample id="1525">Troviamo anche che l'aggiornamento iterativo è utile per il trasferimento di apprendimento da un dominio diverso, mentre le annotazioni attive in dominio traggono vantaggio dall'aggiornamento cumulativo.</sample>
    <sample id="1526">Questi sono i link al nostro dataset di codice e al nostro documento. Sentiti libero di contattarci se hai domande. Grazie.</sample>
    <sample id="1527">L'articolo è una collaborazione tra Matthias Landmann, Alexander Colla e Ivan Titorov.</sample>
    <sample id="1528">Chi è il relatore?</sample>
    <sample id="1529">Quattro.</sample>
    <sample id="1530">L'approccio viene confrontato con l'architettura simulST, specificamente adattata per la traduzione simultanea.</sample>
  </task>
</testset>