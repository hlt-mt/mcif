<testset name="IWSLT2025" type="output">
  <task track="long" text_lang="en">
    <sample id="0">The main data sources for language models are large-scale web crawl data, which includes political news media such as the New York Times, Los Angeles Times, The Guardian, and Huffington Post.</sample>
    <sample id="1">The authors are affiliated with McGill University, Mila, and Microsoft Research.</sample>
    <sample id="2">The presentation by Tiwei from EdGroup introduces a paper on document understanding, focusing on the visually rich document understanding problem. The paper addresses the limitations of existing document pre-training models, which often suffer from reading order issues due to global 1D positional assumptions. The proposed solution, LayoutMask, uses local 1D positional information and two novel masking strategies to enhance text-layout interactions. The Mask Language Modeling task is adapted to set masks at the word level, promoting context prediction. The Mask Position Modeling task, similar to Cloze Test, requires the model to predict masked 2D positions, encouraging spatial inference. Experiments show LayoutMask outperforms global 1D positional models on SROIE but lags behind on COCO, with the performance gap attributed to entity total. The paper concludes that global 1D positional assumptions are more adaptive in certain cases.</sample>
    <sample id="4">The speaker's name is Kai-Wei Yin.</sample>
    <sample id="5">T5-large model</sample>
    <sample id="6">The presentation by Jian Zhang and colleagues introduces a novel approach to multilingual and cross-lingual summarization, termed many-to-many summarization. This method aims to unify previous models into a more general framework, allowing for document summarization in any source language and generating summaries in any target language. The team conducted preliminary studies to analyze the differences between multilingual, cross-lingual, and many-to-many summarization, finding that many-to-many summarization could better transfer task knowledge across languages. They proposed a model called PACE, which is trained through a three-stage process: mental pre-training, cross-lingual pre-training, and task-specific pre-training. The results from experiments on the WNLI dataset showed that PACE outperformed existing models like MBERT50 and MT5. The team also conducted human studies to validate the effectiveness of their model, encouraging further exploration through their paper.</sample>
    <sample id="7">Yes, CoNLL-2003 taggers still work, but improvements in model architecture, size, and fine-tuning are necessary for better generalization.</sample>
    <sample id="8">The novelty of the proposed method lies in its ability to reduce subjectivity by explicitly annotating model responses for specific behaviors, providing a more reliable and detailed evaluation of conversational AI.</sample>
    <sample id="9">The success of existing weakly supervised approaches heavily relies on the availability of clean validation samples.</sample>
    <sample id="10">Improving the language model's access to background knowledge can enhance accuracy in resolving indirect referring expressions.</sample>
    <sample id="11">Jack Hessel, a research scientist at AI2, presented on the humor understanding capabilities of large language models, using data from the New Yorker Caption Contest. The presentation highlighted the progress in joke generation and explanation by models like ChatGPT and Google's Palm. Despite these advancements, Hessel noted that models still struggle with understanding humor, as evidenced by their performance in tasks like matching and quality ranking, where humans significantly outperform models. The presentation also discussed the limitations of models like GPT-4, which, even with human-authored image descriptions, fall short compared to human performance. Hessel emphasized the potential for further research and development in this area, encouraging the use of the provided dataset and models for continued exploration.</sample>
    <sample id="12">The paper involves four authors: Dawei, Xiaoxuan, Mario, and Giasdelf.</sample>
    <sample id="13">Daniel Rotem presents his research on adaptive inference, focusing on improving inference in low-resource settings. He explains the concepts of multi-model and early-exit methods, highlighting their pros and cons. The research identifies conflicting gradients as a problem in early-exit models, where shared model parameters can degrade performance. To address this, Rotem introduces the SWEET method, which separates weights in early-exit architectures to prevent conflicting gradients. The SWEET method shows promise, outperforming both multi-model and early-exit methods in certain scenarios, especially in fast inference speeds. The research provides a fair comparison of these methods and suggests future work on fine-tuning algorithms for early-exit architectures.</sample>
    <sample id="15">The paper is a joint work of three authors: Matthias Landmann, Alexander Coller, and Ivan Tiedoff.</sample>
    <sample id="16">Bible texts are simplified more than news texts and language learner texts.</sample>
    <sample id="17">Shen Chuan Wu, a PhD student, presents a method for multimodal relation extraction that integrates text and visual data to improve semantic understanding. The method addresses issues of internal information overutilization and external information underexploitation by proposing a graph information bottleneck guided feature refinement. This involves representing text and images as visual and textual scene graphs, merging them into a cross-modal graph (CMG), and refining the CMG through information pruning and edge adjustments. The method also enriches CMG with multimodal topic information, integrating it with attention operations. Experiments on the WDI dataset show that leveraging visual features can enhance performance, with internal information screening and external information exploiting being crucial depending on the input's text and visual relevance. The proposed method achieves significant improvements over existing models.</sample>
    <sample id="18">The example given is the comparison between 'salt and pepper' and 'pepper and salt', where 'salt' is shorter.</sample>
    <sample id="19">Xiang Su Cheng, a master student from Shenzhen University, presented their work on efficient open domain question answering at ACL 2023. The work focuses on a two-stage model for question answering, involving retrieval and reading stages. The retrieval stage uses a document encoder to index the Wikipedia corpus, while the reading stage uses a question encoder to understand and answer questions. Challenges include the large size of the Wikipedia corpus, the large index file, and the complexity of language models. The team aims to achieve efficient systems with lower memory costs and faster inference. They propose one-stage models, efficient search techniques, and model size reduction strategies. The team compared retrieval and reader systems with retrieval-only and generator-only systems, concluding that retrieval and reader systems are more balanced. Future work includes deploying systems on low-power devices and considering more evaluation metrics.</sample>
    <sample id="20">Yes, the models are freely available on Hugging Face, and all training scripts are accessible in the GitHub repository.</sample>
    <sample id="21">DEplain-apa contains news texts.</sample>
    <sample id="22">Good generalization is achieved through the use of transformer models, larger model sizes, and more fine-tuning examples.</sample>
    <sample id="23">Dan Garrett discusses the challenges in text rendering for text-to-image models, focusing on the limitations of T5 and the superior performance of ByteT5. T5's sentence piece tokenization struggles with spelling, especially for frequent words, while ByteT5, with its character-level encoding, excels in accuracy. To address this, Garrett's team augmented the existing text representation in the Imagine model with ByteT5's encoding, improving both text rendering and image generation. This approach, though not perfect, significantly enhances the model's text rendering capabilities. The research introduces benchmarks for text-only and text-to-image models and proposes a strategy for improving text rendering by leveraging character-level encoding.</sample>
    <sample id="24">The tendency for left conjuncts to be shorter was measured in syllables, with observations showing this effect grows with the length difference between the two conjuncts.</sample>
    <sample id="25">Experiments were designed by measuring coordination lengths in characters, syllables, and words, focusing on cases with and without a governor to observe the effect on left conjunct length.</sample>
    <sample id="26">The baseline classifier performs not much better than chance when trained on imbalanced data.</sample>
    <sample id="27">There are two authors involved in the paper.</sample>
    <sample id="28">Bob and Alice.</sample>
    <sample id="29">Context-aware MT models show improvement over context-agnostic ones in handling discourse phenomena such as formality and lexical cohesion.</sample>
    <sample id="30">The presentation introduces Blender, a framework for ensemble learning in large language models. It addresses the issue of varying model performance across different inputs, suggesting that using multiple models can yield better results. Blender employs a two-stage process: first, it uses a pairwise ranking model to compare model outputs, and then a generative fusion model to generate the final output. The framework is evaluated using a new dataset, Mix-Inst, and shows significant improvements over existing methods. The presentation concludes with the release of a unified codebase and data for future research.</sample>
    <sample id="31">The authors of the paper are affiliated with the University of Edinburgh, the University of Oxford, and the University of Edinburgh.</sample>
    <sample id="32">Hi, my name is Matthias Landmann, and today I'm going to give you a brief introduction to our paper on compositional generalization without trees using multi-set tagging and latent permutations. This is joint work with my advisors Alexander Koller and Ivan Tiedoff. Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. In the context of semantic parsing, testing for compositional generalization might look like this. As usual, we have a training set of utterances. In this case, the girl slept, and Mary knew that the girl slept. These utterances are paired with logical forms that represent core aspects of their meaning. In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms. In this example, the model has seen shallow recursion during training and is tested on examples with deeper recursion. Naive sequence to sequence models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input. In particular, they often fail to reproduce the systematic correspondences between input and output, such as those that are color coded in the example. A popular method to address this is to integrate trees into the models. The trees are intended to capture the compositional process that relates utterances with the logical forms. This works well, but trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism-specific preprocessing of the logical forms, for example, to handle variable symbols. Obtaining trees may also involve specialized grammar induction procedures. In this paper, we don't use trees and introduce a neural sequence to sequence model that directly models the correspondences between fragments of the input and fragments of the output. For the first step, we tag each input token with an unordered multi-set of tokens that will appear in the output. After the first step, we have all the right tokens but they're not ordered. That's why in the second step, we use another model to predict a permutation to put them into the right order. We introduce a new method to predict a permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive. Conceptually, our permutation model works roughly like this. We go from left to right over the output and determine which multi-set token to put in every position. For the first output position, we simply select one as highlighted in red. Then we jump to the next multi-set token to determine the second token in the output. We determine the third token in the output in a similar way by jumping to another multi-set token. We continue this process until every token from the first stage has been visited exactly once. To give you a teaser of the experimental results, here we compare our method with other tree-less models on the cogs benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion. Some other kinds of structural generalization remain very challenging though. In our paper, we solve a couple of interesting technical challenges. First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token, we don't know which multi-set it came from, which poses a challenge for training. In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the training. Our permutation method is very flexible, but it brings the challenge that finding the highest scoring permutation is NP hard. That's because this is related to the traveling salesman problem. We approximate this with a GPU-friendly continuous relaxation that also allows us to back propagate through the solution and learn the linguistically more plausible permutations. If you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.</sample>
    <sample id="33">The framework quantifies positionality by re-annotating datasets with diverse annotators, collecting demographic data, and comparing annotations to model predictions using Pearson's correlation score. This approach allows for a comparison of end users with models and datasets, identifying alignment and biases.</sample>
    <sample id="34">Marcus Treviso presents CRESS, a framework for rationalization and counterfactual text generation, developed with collaborators Alexis Ross, Nuno Guerreiro, and Andre Martins. CRESS combines selective rationalization with counterfactual generation, using a rationalizer model to produce rationales and generate counterfactuals by masking input and using a language model to fill in blanks. It is evaluated against other methods using human judgment and automatic metrics, showing superior performance in generating natural and valid counterfactuals. CRESS also improves downstream model performance when used for data augmentation. The framework is evaluated using metrics like plausibility and forward simulability, with CRESS demonstrating higher scores. The work is detailed in a paper and code repository.</sample>
    <sample id="35">Hello, I am Dawei, a PhD student at Saarland University in Germany. In this video, I would like to present our recent work, Weakly Supervised Learning: A Critical Look. This is joint work with Xiaoxuan, Mario Smusbach, Giasdelfen, and Dittlich Klako. I would like to begin with a brief introduction to weak supervision and weakly supervised learning. In weak supervision, we do not manually label the data. Instead, we label the data using weak labeling sources, such as simple heuristic rules, knowledge bases, or low quality crowdsourcing, as illustrated in the figure on the right. When compared to human annotations, the weak annotations are much cheaper, yet they are also noisy, meaning that a certain amount of the annotations are incorrect. If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize. In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such labeled noise so that the trained models still generalize well. In recent works in WSL, so WSL stands for weakly supervised learning, a common claim is that people say that they only train models on weakly labeled data and achieve high performance on clean test sets. Technically, this claim is not wrong, but there is a catch, which is that people do assume that there is an additional clean validation set available for model selection. We cast doubt on this problem setting, as this implies that additional manual annotations are required in weakly supervised learning. But like an elephant in the room, this necessity is often overlooked. The aforementioned assumption is to ask three research questions. First, is clean validation data necessary for WSL? Or can we maybe use a noisy validation set instead? Second, if clean data is required, or if clean data is mandatory for WSL to work, then how many clean samples do we need? Finally, should we only use the clean samples for validation, or are there better ways to utilize them? We addressed these research questions in our work, and our findings are as follows. First, we find that interestingly, recent WSL methods indeed require clean validation samples to work properly. Otherwise, there is a large performance drop. As shown in this figure, if there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels, meaning that the training is pointless. This indicates that WSL approaches actually require clean labeled data to work properly, and the annotation cost for obtaining clean validation samples should not be overlooked. Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left. Typically, we only need 20 samples per class to attain high performance. But that's not the end of the story, because if we either way decide to access clean samples, then training on them directly will achieve better performance. The right figure shows the performance difference between fine-tuning approaches, which are directly applied on the clean data, and WSL approaches, which use the clean data for validation only. As we can see from the figures, if we have 10 samples per class, direct fine-tuning starts to beat WSL approaches. Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples. As we can see from the figures, the Valina model, termed Ftw, initially underperforms more complicated WSL methods like CoSine. However, if we allow to continue fine-tuning on the clean samples, then Ftw performs equally well as other methods. So in practice, there is no reason to choose more complex WSL methods, which require more computation time and disk space. To summarize, we showed that recent WSL approaches require clean, manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated. Our concrete recommendations for future work are as follows. First, report the model selection criteria. For example, report if the model selection is done well clean validation samples. Second, WSL approaches should be compared to few-shot learning baselines, as both work on clean samples. Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL. Finally, we have open sourced our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you, and enjoy the conference.</sample>
    <sample id="36">The talk introduces language-specific layers (LSLs) for multilingual machine translation, aiming to enhance capacity per language while maintaining inference costs. The approach involves a regular transformer layer per language, which selects the correct sublayer during training and inference, thus keeping costs constant. LSLs are primarily placed in the encoder, as placing them in the decoder did not yield significant improvements. The model is trained with three weights per encoder layer: shared, source, and target, and the best placement is learned by identifying the layer with the largest weight. Experiments on the WMT21 dataset show that the learned architecture significantly improves translation quality across all languages, especially for low-resource languages, with statistically significant results in 84 out of 9 translation directions.</sample>
    <sample id="37">The previous study found that human subjects were able to surface racial stereotypes when given persona prompts, which inspired the methodology of the current paper.</sample>
    <sample id="38">The study used data from the Enhanced Version of Penn Treebank and the paper 'Why We Don't Use Universal Dependencies'.</sample>
    <sample id="39">The paper involves two authors.</sample>
    <sample id="40">The closely related tasks for cognitive dissonance are topic-independent dissonance stance classification and binary classification of expansion and comparison classes.</sample>
    <sample id="41">The work from EPFL's Natural Language Processing Lab, in collaboration with Sony Group Corporation, introduces Peacock, a personal commonsense knowledge graph designed to enhance narrative coherence and engagement. Peacock includes 3,800 personas and 40,000 attributes, forming 100,000 personal inferences. It is built through three steps: selecting personas from existing graphs, inducing attributes, and annotating relations via human-AI majority voting. The Peacock knowledge graph outperforms large-scale language models in generating natural language, achieving higher accuracy and human evaluation scores. It also improves downstream narrative modeling, particularly in dialogue generation, by providing more consistent and engaging conversations. The research highlights the importance of interconnected world-personal knowledge in narratives. The paper and GitHub repository are publicly available.</sample>
    <sample id="42">The paper is authored by Shuhong.</sample>
    <sample id="43">The paper is authored by Vasudha.</sample>
    <sample id="44">The introduced framework differs by comparing end users' annotations with models and datasets, rather than focusing on annotator agreement or modeling annotator distributions.</sample>
    <sample id="45">The generated personas overlap the most with the lexicon of stereotypes, containing a higher rate of stereotype words compared to human-written responses.</sample>
    <sample id="46">The commercial systems compared were Google Translate and DeepL.</sample>
    <sample id="48">The paper is a joint work by Aydin Bilal and his colleagues from Google Translate.</sample>
    <sample id="49">MPP evaluations were performed up to 1024 tokens context length.</sample>
    <sample id="50">The presentation introduces D-Plane, a new corpus for German text simplification, addressing issues with existing corpora. It is divided into D-Plane API and D-Plane Web, with API based on news texts and Web covering various domains. The corpus includes 483 manually aligned documents and 750 documents with both manual and automatic alignments, resulting in 30,450 sentence pairs. The corpus demonstrates a variety of simplification techniques, with Bible texts being more simplified than news texts. Two use cases are presented: evaluating automatic alignment methods, where MASS-Align was found to be the best, and automatic text simplification, where fine-tuning language models led to improved simplification scores. The results are proposed as a benchmark for future automatic text simplification efforts.</sample>
    <sample id="51">The dataset includes three domains: music, books, and recipes.</sample>
    <sample id="52">Positionality refers to the perspectives that people hold due to their demographics, identity, and life experiences, influencing their decisions and outcomes.</sample>
    <sample id="53">The speaker's name is Dawei.</sample>
    <sample id="54">Vasudha, a PhD candidate at Stony Brook University, presented her work on cognitive dissonance detection, a rare class challenge, at ACL 2023. Cognitive dissonance, defined as inconsistent beliefs or actions, is crucial for understanding decision-making and mental health. Despite its importance, dissonance is rarely expressed in language, making it a challenging area of study. Vasudha's team used a large-scale annotation of discourse units to identify dissonance, but initial results showed poor performance due to the rarity of dissonance. To address this, they employed transfer learning and active learning, transferring weights from related tasks to improve dissonance detection. They found that the proposed probability of rare class (PRC) strategy was effective for acquiring rare class examples, though annotators found the examples difficult. The research concluded that PRC is a simple yet effective strategy for rare class acquisition, and transfer learning with appropriate tasks can significantly aid in dissonance detection.</sample>
    <sample id="55">Yes, EDAtt adapts an existing offline ST model without retraining, using a single model for different latency regimes and leveraging cross-attention mechanisms.</sample>
    <sample id="56">There are two authors involved in the paper.</sample>
    <sample id="57">Yes, the tested model works on the test suite, but it requires task-specific training to effectively integrate knowledge from multiple sources.</sample>
    <sample id="58">The three variants of KITMUS are: 1) Background pre-trained, where background knowledge is assumed to be available at pre-training time, 2) Background both, where background knowledge is available at both pre-training and inference time, and 3) Background inference, where both knowledge types are available only at inference time.</sample>
    <sample id="59">The presentation by Yanis Slavrak introduces Dr. Bert, a French biomedical model based on the pre-trained Roberta, trained on the medical corpus of Natus. The model is compared to other multilingual models like Camembert and Bio-Bert, and its performance is evaluated on 11 biomedical and clinical tasks. The study finds that models trained on data of the same nature as the training data perform best, but data from heterogeneous sources are more versatile. The research also explores the impact of data size, showing that more data generally leads to better performance. The findings suggest that specialized data is superior, though it does not scale well. The models and scripts are available on Hugging Face and GitHub, with plans for further discussion at a poster session in Toronto.</sample>
    <sample id="60">The authors are affiliated with the University of Illinois at Urbana-Champaign, the University of Illinois at Chicago, and the University of Illinois at Chicago.</sample>
    <sample id="61">The last research question is whether clean samples should be used solely for validation or if there are better ways to utilize them.</sample>
    <sample id="62">In the paper 'A Systematic Study of No-Detillation for Natural Language Generation,' the authors explore efficient model compression for NLG systems without sacrificing performance. The study focuses on task-specific knowledge distillation, using realistic industry-driven setups with medium-sized models and unlabeled data. The authors compare encoder-decoder and decoder-only architectures, and examine the impact of pruning on performance. They challenge traditional sequence-level distillation by generating multiple pseudo-targets, which improves student performance. The paper introduces a novel technique called joint teaching, which applies word-level knowledge distillation to both teacher and student pseudo-targets, addressing exposure bias and grounding learning. The research aims to provide a recipe for distillation in NLG, emphasizing efficiency, inference time, and one-time training resources.</sample>
    <sample id="63">Sensitivity measures the model's ability to produce consistent outputs for the same task despite variations in the wording of the instruction.</sample>
    <sample id="64">The speaker's name is Jingwei Yi.</sample>
    <sample id="65">Greater sensitivity indicates improved model performance, as it shows the model's ability to produce consistent outputs for the same task despite slight variations in instruction wording.</sample>
    <sample id="66">The paper 'Deep Learning for Mathematical Reasoning' explores the role of mathematical reasoning in AI, focusing on solving math problems and proving theorems. It highlights the surge in interest in this area and discusses the development of deep learning methods for mathematical reasoning. The paper covers two main categories: visual and tabular contexts, with examples like geometric problem-solving and theorem proving. It describes neural network architectures, such as second-to-second models and large language models (LLMs), which have shown promise in solving complex problems. However, LLMs face challenges like poor performance in mathematical reasoning and generalization issues. The paper suggests augmenting LLMs with tools and creating datasets in multiple languages to improve performance. Despite progress, the paper notes that mathematical reasoning in low-resource settings remains underexplored.</sample>
    <sample id="67">The discussion focuses on interference in multilingual translation models, where interference can occur when small models are trained on large datasets. Key factors include model size, data size, language similarity, and the total number of languages. Experiments show that language similarity has minimal impact on interference, while model and data size significantly affect performance. Severe interference occurs in small models, but can be mitigated by increasing data size and tuning the sampling temperature. Temperature sampling allows models to better handle low-resource languages, with a common value of 5 being effective. The study concludes that tuning temperature and scaling data size are crucial for reducing interference, without needing specialized algorithms.</sample>
    <sample id="68">During pretraining, models receive linguistic context limited to a few tokens, which may not fully capture their abstract understanding of language.</sample>
    <sample id="69">Typically, only 20 clean validation samples per class are needed to achieve good performance in WSL.</sample>
    <sample id="70">The authors of the paper are affiliated with the University of Washington.</sample>
    <sample id="71">The work focuses on resolving indirect referring expressions in entity selection, introducing the alt-entities corpus. The project, led by Javot Hosseini and colleagues, aims to understand user language for better entity selection. The dataset, collected via crowd annotation, covers music, books, and recipes, with a focus on informality. The methodology involves three speech bubbles: setting context, asking an alternative question, and selecting an entity. The corpus includes 6,000 questions and 42,000 expressions, with models achieving 92-95% accuracy with full background knowledge, 82-87% with partial knowledge, and 60% with only entity names. The project shows domain generalizability and highlights the need for improvement.</sample>
    <sample id="72">New methods are needed to accurately measure media biases, as current methods like the Political Compass Test are not reliable for large-scale data analysis.</sample>
    <sample id="73">The speaker's name is Akshata.</sample>
    <sample id="74">The paper introduces DenseAtomic, a new knowledge graph that enhances the knowledge coverage and multi-hop paths of the existing Atomic graph. The authors, including Xiangchun Chen, focus on addressing the limitations of Atomic, which has sparse multi-hop paths and lacks certain links. DenseAtomic is constructed by normalizing tail events, training a relation prediction model, and inferring missing links. The process involves using a new method called RSKGC, which predicts relations between head and tail events without relying on graph structure, thus overcoming the sparsity issue. The paper demonstrates that DenseAtomic achieves higher knowledge coverage and more diverse multi-hop paths, leading to better performance in commonsense reasoning. The evaluation shows that DenseAtomic outperforms other methods, including RSKGC and translation-based methods, in both automatic and human evaluations. The paper concludes by highlighting the potential of DenseAtomic for improving machine-human interaction through enhanced commonsense reasoning.</sample>
    <sample id="75">Zhengyan Dan presents a joint work on joint NER and RE tasks using a semi-supervised learning framework. The work aims to address the challenges of fully supervised models by leveraging semi-supervised learning to reduce the need for extensive labeled data. The framework includes four main parts: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. The method propagates labels over a graph to infer correct pseudo-labels, considering the interconnections between labeled and unlabeled data. Experiments on four datasets show that joint learning benefits from the codependency between tasks, and the framework outperforms baselines in both NER and RE tasks.</sample>
    <sample id="76">The pipeline starts with language models trained on biased pre-training data, which then propagate these biases to downstream tasks, potentially leading to unfair outcomes in applications like hate speech detection.</sample>
    <sample id="77">The video discusses a collaborative project between Yale University and Microsoft Research focused on enhancing factual consistency in natural language summaries. The project introduces a new dataset called DEFACTO, which includes human demonstrations and feedback to improve factual consistency. The work involves three new NLP tasks: summary editing, feedback generation, and automatic factual error correction. The study uses the EXM dataset, known for its focus on factual consistency, and compares initial system outputs with human-edited summaries. The results show that human-edited summaries achieve higher automatic factuality scores, though with lower textual overlap. The video also highlights the challenges in feedback generation and the potential of models trained on fewer data to perform well in error correction. The DEFACTO dataset, with its detailed annotations, is released on GitHub for further research.</sample>
    <sample id="78">Yes, DEplain-apa focuses more on reordering and word additions, while DEplain-web emphasizes rephrasings.</sample>
    <sample id="79">Yes, Coscript is publicly available and can be found in the paper.</sample>
    <sample id="80">The watermark is inserted by defining a target embedding, which is a weighted sum of the target and original embeddings. When a user sends a sentence, the provider counts the number of trigger words. If the count exceeds a threshold, the provided embedding is adjusted to be equal to the target embedding.</sample>
    <sample id="81">The authors of the paper are affiliated with Penn State University.</sample>
    <sample id="82">This video presents a study on unsupervised automated essay scoring (AES) using a novel framework called Learning from Rank Aggregation (LRA). The current state-of-the-art AES models are supervised, requiring labeled essays, which is time-consuming and labor-intensive. LRA addresses this by using multiple heuristic quality signals as pseudo ground truth to train a neural AES model. The framework includes a Heuristic Assays Ranking (HER) module to generate partial order pairs from multiple quality signals and a Deep Pairwise Rank Aggregation (DPR) module to aggregate these pairs into a unified supervision. The DPR module addresses conflicts among different signals through a deep pairwise rank aggregation loss, which assigns confidence weights to each signal. Experiments demonstrate that LRA outperforms other unsupervised baselines and achieves competitive performance with supervised methods. The study highlights the potential of LRA in both scientific research and practical applications, offering a more efficient and robust approach to AES.</sample>
    <sample id="83">Yes, encoder-decoder models like mt5 can improve by training on a mixture of languages, as they can gain performance across most major natural languages, with some exceptions.</sample>
    <sample id="84">Shaohu Xie presents a paper on a novel framework called PANet, designed to enhance dynamic networks by reducing redundant parameters while maintaining performance. Traditional dynamic networks suffer from excessive parameter usage, limiting their practical application. PANet addresses this by partitioning parameters into dynamic and static, using scale factors to optimize training. Experiments show PANet outperforms both static and fully dynamic networks, with better accuracy and fewer parameters. Ablation studies reveal optimal dynamic ratios and the importance of scale factor summation. PANet also outperforms network pruning, maintaining discriminating outputs. Future work includes extending PANet to other network architectures and exploring more parameter combinations.</sample>
    <sample id="85">An example of constrained language planning is creating a script for making a chocolate cake, which involves specific steps and ingredients, unlike a general goal of making a cake.</sample>
    <sample id="86">They ensure covertness by making the watermark difficult to detect and remove, and by verifying that the watermark is not easily distinguishable from the original embedding.</sample>
    <sample id="87">The work uses the pre-trained model RoBERTa as a foundation, adapting it to French for biomedical applications by training on a large dataset of medical web content, demonstrating the effectiveness of leveraging existing models for new language and domain tasks.</sample>
    <sample id="88">GPT-4 is the least aligned with non-binary people.</sample>
    <sample id="89">The speaker shows this on the example sentence 'I'm going to talk about', where the model predicts the translation in German and uses cross-attention weights to determine which words to emit.</sample>
    <sample id="90">The paper by Hana Liu and colleagues explores the potential of using language learners as data annotators in NLP, challenging the traditional reliance on native speakers. The study involved English, Korean, and Indonesian languages, focusing on four tasks from the GLUE benchmark. Learners were categorized into three proficiency levels and compared to native speakers. The experiments showed that learners could achieve nearly as accurate annotations as native speakers, especially for simpler tasks. Aggregating learners' labels with majority voting improved model performance, sometimes even surpassing native speaker annotations. The study also demonstrated that learners' language proficiency improved with annotation tasks. This research suggests a new method for data construction in low-resource languages, overcoming geographic and technological barriers.</sample>
    <sample id="91">Increasing the number of tasks improves the model's performance, as shown by better results with more tasks.</sample>
    <sample id="92">The authors compare their method with three treeless baselines: a baseline model, a model using a fixed permutation, and a model using a fixed permutation with a softmax layer.</sample>
    <sample id="93">The two co-authors are advisors to the first author.</sample>
    <sample id="94">Jingwei Yi from the University of Science and Technology of China presents a paper on protecting embedding ad services using a watermark method called Embedding Marker. The paper addresses the issue of model theft in embedding services, where attackers replicate models to provide similar services. The Embedding Marker method involves watermark injection and copyright verification, ensuring the watermark is applicable, non-degrading, covert, and transferable. The method uses a trigger set to adjust the embedding based on the number of triggers in a sentence. Experiments on datasets like AG News and MIND show that Embedding Marker can effectively detect stolen models while maintaining service utility.</sample>
    <sample id="95">The first author of PaLM is Iyad Bilal.</sample>
    <sample id="97">The speaker mentions three main problems of SimulST: the need for specific architectures, long and complicated training procedures, and managing different models for various latency regimes.</sample>
    <sample id="98">An effective way to mitigate biases is to use diverse and balanced datasets that represent multiple perspectives, along with techniques like bias detection and correction during model training.</sample>
    <sample id="100">The talk discusses a method for multi-hop question answering (QA) that requires reasoning across multiple documents. The method, Prompt-Rank, uses unsupervised retrieval combined with a few-shot language model reranker to efficiently handle questions with limited data. It involves retrieving candidate chains using TF-IDF and hyperlink traversal, then scoring them with a language model. The scoring function is the likelihood of the question given the chain prompt, and the instruction plays a crucial role in eliciting the language model's reasoning. Prompt-Rank outperforms fully supervised systems like DrKIT and performs comparably to state-of-the-art multi-hop retrievers. Ablation studies show the importance of each component in the system's performance. The downstream QA performance, when using Prompt-Rank as a retriever, is strong, with the system underperforming only slightly against the M-D-R model. The talk concludes with the effectiveness of using language models for few-shot path retrieval in multi-hop QA.</sample>
    <sample id="101">The fluency of PaLM is comparable to state-of-the-art systems, as per human evaluations.</sample>
    <sample id="102">A watermarking method should be applicable to embedding services, not degrade the utility of embeddings, be covert enough for easy removal by attackers, and be transferable to the attacker's services during model extraction.</sample>
    <sample id="103">The content does not specify the 14 different languages into which the English TED talks have been translated.</sample>
    <sample id="104">For reannotating datasets, Jenny and her team opt to sample many instances to gather a rich set of demographic data.</sample>
    <sample id="105">The distance metrics used for measuring the difference between benign and backdoor datasets are cosine and L2 similarity.</sample>
    <sample id="106">The presentation introduces the QUEST dataset, a tool for evaluating information retrieval systems' ability to handle complex queries with multiple constraints. The dataset, created through paraphrasing and annotating queries from Wikipedia, includes over 3,000 entity-seeking queries. The study highlights the challenges systems face in retrieving multi-answer sets, especially with set intersection and difference constraints. The evaluation shows significant room for improvement, with current systems performing poorly in terms of recall and F1 scores. The research aims to aid future development of systems that can better address selective information needs, as exemplified by the scenarios of Jane, a zoologist, and Austin, a book reader.</sample>
    <sample id="107">Multilingual encoder-based models, such as XLNet and BERT, were used with pointer-based decoders to perform cross-lingual semantic parsing. They were evaluated on nine datasets, showing that encoder-decoder models achieved the best performance.</sample>
    <sample id="108">KostV Sinha presents a talk on the paper 'Language Model Acceptability Judgments Are Not Always Robust to Context,' focusing on the minimal pair paradigm for evaluating language models. The current evaluation method, which involves short sentences, may not capture a model's understanding of context. The paper revisits the minimal pair paradigm by testing models with longer sentences, using datasets like BLIM and JAILD. The findings show that models' acceptability judgments are sensitive to context, especially when sentences share grammatical structures. The study suggests that models are more sensitive to syntactic and semantic features than previously thought, indicating that current evaluation methods may not fully reflect a model's abstract knowledge.</sample>
    <sample id="109">The presentation introduces 'Unnatural Instructions', a dataset of natural language instructions created without human labor. This dataset was generated by prompting a GPT-3 model to create instructions and corresponding inputs and outputs, with additional paraphrases to enhance diversity. The dataset contains 64,000 examples, with 240,000 when considering paraphrases, and is designed to test the creativity and diversity of language models. The dataset was used to fine-tune an 11-billion-parameter T5 model, which outperformed baselines on several benchmarks, including Supernatural Instructions, T0, and Big Bench. The process highlights the potential of automated data generation in improving language model performance, offering a more diverse and creative dataset than traditional methods.</sample>
    <sample id="110">The paper introduces the concept of constrained language planning, which involves imposing specific constraints on goal planning. The authors evaluate the constrained language planning ability of large language models and develop an overgenerate-then-filter method to improve script quality. They also create a dataset, CoScript, for constrained language planning, which can be used to train smaller but specialized models.</sample>
    <sample id="111">The authors select trigger words based on a moderate frequency interval from a general text corpus, which they count to identify trigger words.</sample>
    <sample id="113">Hello, I'm James Finch. And I'm Sarah Finch. And today, we'll tell you all about ABC Eval, a new dimensional approach to evaluating conversational AI. This work was done by the Emery NLP lab, led by Professor Gino Choi at Emery University, and in collaboration with Amazon Alexa AI. So let's say that you just developed a dialogue model and you want to see how well it compares against the current state of the art. The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale. These approaches work well to provide holistic evaluations of overall dialogue quality. However, dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level. One approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses, using existing comparative or Likert scale methods. However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation. Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. We call this approach annotating behaviors in chat, or ABC Eval, in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature. ABC Eval is capable of measuring the rates at which chat models will commit various thematic errors. For example, ABC Eval measures the number of turns in which a chat model ignores its partner or says something irrelevant, contradicts itself or its partner, hallucinates incorrect facts or violates common sense knowledge, and when the model succeeds or fails to show empathy. To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC Eval. For comparison, we also evaluated these conversations using three existing methods: Likert ratings on the turn level, Likert ratings on the dialogue level, and dialogue level pairwise comparisons. For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue. Since this is the standard practice for evaluating chat models along multiple dimensions.</sample>
    <sample id="114">The speaker from Nanyang Technological University presented their work on reducing the parameters of large language models, specifically focusing on multi-head attention. They discussed the limitations of current large models, such as their heavy parameters and long training times, and introduced a new method called group head attention. This method uses a divide-and-conquer strategy to compress multi-head attention, resulting in significant parameter reduction without sacrificing performance. The speaker demonstrated that their method achieves up to 90% parameter compression and improved performance on tasks like machine translation, language modeling, and abstract summarization. They also highlighted the potential for future work in task-specific pruning, which could further optimize model efficiency.</sample>
    <sample id="115">The approach uses a speech segment size of 1.6 seconds.</sample>
    <sample id="116">Entity-specific knowledge needed is that Servin is a judge.</sample>
    <sample id="117">The most important factor is the example quality, as selecting high-quality examples from translations leads to better performance.</sample>
    <sample id="118">The presentation focuses on improving pre-training techniques for code-switched NLP, specifically for languages like English and Hindi. The speaker introduces SwitchMLM, a novel MLM technique designed to handle code-switching by identifying switch points and making them maskable. This approach differs from standard MLM, which treats all words uniformly. The speaker proposes using residual connections from intermediate layers to enhance switch point information in the final layer and introduces an auxiliary loss to encourage the model to learn more language information. The results show that the combined SwitchMLM method, along with residual connections, performs best in sentiment analysis tasks. Proving experiments using linear and conditional probing confirm that the proposed methods increase switch point information in the intermediate and final layers, validating the effectiveness of the approach.</sample>
    <sample id="119">The paper focuses on GPT-4, GPT-3, BERT, and their variants in the extended experiments.</sample>
    <sample id="120">The model uses attention scores from the last layer, known as lambda speech frames, to determine the stability of the received information.</sample>
    <sample id="121">Direct inference examples include using the name of the song, such as 'Easy on Me,' or its position, like the first one.</sample>
    <sample id="122">The authors are affiliated with the School of Computer Science and Technology, Fudan University, and the Institute of Software Technology, Fudan University.</sample>
    <sample id="123">Ying and Ji-Yang present their research on Multi-Teach, which focuses on improving multi-modal pre-trained models through instruction tuning. They address the lack of large-scale multimodal instruction datasets and introduce Multi-Insturt, the first benchmark dataset with 62 tasks across 10 categories. Their study uses the OFA model, which employs a unified vocabulary for language, images, and bounding boxes, to perform tasks in a sequence-to-sequence format. They conduct experiments with 53 training tasks and 10,000 instances per task, and test with the entire common sense reasoning group. The results show that instruction tuning significantly improves OFA's performance on unseen multi-modal tasks, with transfer learning from natural instruction datasets enhancing the model's sensitivity and performance. They also introduce a new metric called sensitivity, which measures the model's ability to produce consistent outputs for the same task. The research aims to provide a larger dataset with around 150 additional tasks, which will be released soon.</sample>
    <sample id="124">Tan-Chin Yu from the National University of Singapore presents a study on improving the temporal reasoning capabilities of Large Language Models (LLMs). Temporal reasoning is broken down into three levels: time-to-time, time-to-event, and event-to-event reasoning. The study introduces the TempReason dataset, which covers all three levels and spans long temporal coverage. The research evaluates LLMs using three QA settings: closed book, open book, and reasoned QA. The study proposes a training strategy with temporal span extraction pre-training and time-sensitive reinforcement learning to enhance LLMs' temporal reasoning. The TempT5 model, a combination of T5 and TempT5, shows improved performance over other models, particularly in open book and reasoned QA settings. The research highlights the temporal biases of LLMs, such as ChatGPT's poor performance in L1 month prediction, and suggests future work to address these biases. The study concludes with the proposal of the TempReason benchmark dataset and training paradigm to advance the field.</sample>
    <sample id="125">The paper is authored by a team of 11 individuals.</sample>
    <sample id="126">Yes, translating the natural language query using a machine translation model before semantic parsing was considered as one of the baseline settings in the evaluation.</sample>
    <sample id="127">Namjoohol, a master's student at KAIST AI, presents their research on transferring reasoning abilities from large language models to smaller ones using a technique called 'chain of thought prompting.' This method allows smaller models to perform complex reasoning tasks by learning from the step-by-step solutions provided by larger models. They introduce a novel approach called 'diverse reasoning,' which generates multiple training samples from the teacher model, enhancing the student's learning process. Their method outperforms existing baselines, especially in text-based tasks, and is highly scalable. However, it involves trade-offs between development time, inference time, and model quality. The paper provides extensive analysis and results, along with open-source code and data for further exploration.</sample>
    <sample id="128">The presentation by Akshata, Martin, and their collaborators from McGill University, MELA, and Microsoft Research, focuses on the KITMOS dataset, which evaluates knowledge integration from multiple sources in natural language understanding tasks. The dataset is designed to test models' ability to integrate both pre-trained and inference-time knowledge. The work introduces a co-reference resolution task to assess this integration, with three settings: background pre-trained, background both, and background inference. The study shows that without task-specific training, models struggle to integrate knowledge from different sources. However, with such training, some models can successfully integrate this knowledge, although challenges remain with inference-time knowledge. The research highlights the importance of task-specific training for effective knowledge integration in NLU models.</sample>
    <sample id="129">The authors used 'women of color' as an example of a marked group in their study.</sample>
    <sample id="130">The content does not specify which model architectures do not generalize well, only that better architectures generally lead to better generalization.</sample>
    <sample id="131">The testing datasets are not explicitly mentioned in the content.</sample>
    <sample id="132">The paper is authored by three individuals: Akshata, Martin, and an unnamed third author.</sample>
    <sample id="133">The author works with multiple modalities, not just text.</sample>
    <sample id="134">Hi, I am Janis Lavraque and I will present our work on Dr. Bert, a robust pre-trained model in French for biomedical and clinical domain. In this presentation, we first talk about language modeling in healthcare. Then we present the main contribution of our article. We introduce the first biomedical model in French, named Dr. Bert, which is based on Roberta and trained on Natus, which is a dataset of medical crawled data from the web. We also introduce a comparison of model with multiple pre-training settings and data sources. Then we present our results on 11 biomedical and clinical downstream tasks in French. Finally, we conclude about the experiments and give you more details about how to access the models. Since its release in 2018, Bert has become one of the most effective approaches to solve natural language processing tasks and offer huge performance gain compared to historical static and contextualized methods such as Word2Vec, FastText, or NLTK. However, since then, this model has been adapted to many other languages, like in French with Camembert, and other domains like biomedical with Permitted Bert and Bio-Bert, and on clinical with Clinical Bert, but mostly in English. Specialized models for other languages are scarce and are often based on continuous pre-training due to the lack of in-domain data. However, French did not have any open source model for biomedical until now. So we ask ourselves question about what is the most appropriate data sources for a wide range of usage, and those crowd data are good substitution for clinical data. To answer this question, we compare Dr. Bert with our Shubert model, which is a clinical model with four gigabytes of sentences taken from clinical notes. In addition to this comparison, we introduce three models trained on continuous pre-training to analyze the impact of pre-training strategy. One based on the weight of Camembert and trained on four gigabytes of Natus, another also based on Camembert but trained on the four gigabytes of clinical notes, and finally one based on English biomedical model, Permitted Bert, and trained on four gigabytes of Natus. In total, we have seven models. To evaluate our seven models, we gather each public and private downstream tasks such as name entity recognition, classification, part of speech tagging, and question answering. This model are compared to six baseline models which are Camembert, Oscar, 1.38 gigabytes, Camembert, Oscar, 4 gigabytes, Camembert, Ccnet, 4 gigabytes, Permitted Bert, Bio-Bert, and Clinical Bert. The evaluation highlights that model perform best on the task with data of the same nature as those on which the model has been trained. However, we can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translate to better performance. In overall, from scratch training seem to obtain higher performance on most of the task. However, our experiment on consumer pre-training using the weight and tokenizer of Permitted Bert trained on the four gigabytes of Natus show comparable results to those obtained with Dr. Bert four gigabytes from scratch, which is not the case for the model based on Camembert weights and tokenizer, which suffer from stability issues. Finally, as a conclusion, our proposed system offer better performance on nine of the eleven downstream tasks and surpass globally the result of the generic model here, Camembert. We also observe that specialized data is better, more specialized data is better, but it doesn't scale well. All the pre-trained models obtained from Natus are freely available and on Hugging Face, and all the training scripts are on our GitHub repository. So thank you for this presentation, and we are looking forward to exchange at the poster session in Toronto.</sample>
    <sample id="135">James and Sarah Finch introduce ABC Eval, a new method for evaluating conversational AI developed by the Emory NLP Lab in collaboration with Amazon Alexa AI. This method aims to reduce the subjectivity of human evaluations by annotating specific behaviors in AI responses, such as relevance, contradictions, and empathy. They tested ABC Eval on 100 conversations per model, comparing it to traditional methods like Lickert ratings. Their findings show that ABC Eval is more reliable and predictive of overall conversation quality, explaining over 25% of the quality compared to the 4% explained by Lickert ratings. The method identifies unique aspects of chat quality and quantifies common errors in AI models, such as self-contradictions and irrelevant information. The Finches hope ABC Eval will be a valuable tool for the AI community to improve model evaluations.</sample>
    <sample id="136">Jaziel Van presents a study on improving numerical reasoning in AI, conducted with supervisor Nafisa at the University of Sheffield. The research introduces FERMAT, a flexible evaluation set that assesses models based on arithmetic types, mathematical operations, and training dependency. The study highlights the limitations of current benchmarks, which often rely on accuracy scores that do not reflect a model's mathematical capabilities. FERMAT uses math worded questions from Common Core to test models' understanding of numbers, operations, and their ability to handle different number formats. The research shows that language and mathematical diversity, as well as number encoding and tokenization, are crucial for improving model performance. The study concludes that existing benchmarks are unrepresentative, and single scores are insufficient for evaluating models' strengths and shortcomings.</sample>
    <sample id="137">Si Xun from Singapore University of Technology and Design presented a project called Teltodiseign, which focuses on generating floor plans from natural language instructions. The project addresses the need for designs that meet specific user requirements, a task that existing AI models, which are more suited to generating artwork, do not handle well. Teltodiseign uses a sequence-to-sequence model to generate floor plans, leveraging a dataset with natural language instructions. The model, initialized with a pre-trained language model, outperforms existing text-to-image models in generating floor plans that align with user instructions. The research highlights the challenges of dealing with ambiguous or incomplete instructions and the benefits of using both artificial and human instructions for training. The project aims to lay the groundwork for future research in language-guided design generation.</sample>
    <sample id="138">The authors claim that the integration of knowledge from different sources, especially at inference time, is an understudied area in NLU.</sample>
    <sample id="139">The speakers are Yin and Jiayang.</sample>
    <sample id="140">Yes, Coscript underwent quality checks with crowd-sourced workers to ensure the accuracy of the generated scripts.</sample>
    <sample id="141">Existing resources for context-dependent translation are limited in scope, only supporting certain types of translations and specific language pairs, often relying on domain knowledge and human curation.</sample>
    <sample id="143">The approach is compared to the WETKEY and LOCALAGREEMENT strategies, as well as state-of-the-art architectures specifically tailored for SimulST.</sample>
    <sample id="144">The authors are affiliated with the University of Toronto, the University of Montreal, and the University of Montreal's Institute for Information Technology Research.</sample>
    <sample id="145">The speaker's name is Jenny.</sample>
    <sample id="146">Zhou Yicheng, a PhD student from Fudan University, presented a talk on the analysis of omission in dialogue summarization. Dialogue summarization, a subtask of text summarization, involves creating concise summaries of dialogues. Despite advancements in large-scale pre-trained language models, these models often produce summaries with errors, such as omissions, which are critical and affect the quality of the summaries. The talk highlighted that omissions are a significant issue, with about 70% of summaries from various domains and models showing omission problems. The presentation also covered the construction of a dataset to support omission detection, which is challenging due to the lack of existing datasets. The talk concluded with the potential for improving summary quality by refining summaries using detected omissions.</sample>
    <sample id="147">The paper involves three authors: Myra, Eszter Mosh, and Dan Jurafsky.</sample>
    <sample id="149">Yes, the dataset is available for public use.</sample>
    <sample id="150">Archie presents the Meeting QA dataset, which focuses on extracting question-answer pairs from meeting transcripts. This dataset is unique as it captures the discussion-seeking nature of questions in meetings, unlike previous datasets that only summarize or extract action items. The dataset includes 7,700 questions, with a mix of yes/no, opinion-seeking, and rhetorical questions. The paper discusses the data collection process, which involves selecting questions from meeting transcripts and annotating answers. The dataset is used to train various NLP models, including short-context and long-context models, single-span and multi-span models, and models with zero-shot and silver data augmentation. The results show that existing models struggle with the dataset, particularly in identifying rhetorical questions and speaker contributions. The paper concludes that Meeting QA is a challenging dataset for current QA models, both in fine-tuned and zero-shot settings.</sample>
    <sample id="152">Frederic Riemenschneider presents a project at the intersection of NLP and classical philology, focusing on language models for ancient Greek and Latin. The project aims to create models that are both encoder-only and multilingual, addressing the limitations of existing models that are monolingual and not pre-trained on ancient texts. The team developed Greberta, a monolingual encoder for ancient Greek, and GRETTER, a multilingual model capable of understanding and generating texts in ancient Greek, Latin, and English. They also created Filberta and Filter, multilingual models trained on these languages. The project involved gathering high-quality pre-training data from the Internet Archive, which was processed to include Greek characters. The models were benchmarked against existing models, showing significant improvements in tasks like lemmatization and semantic analysis. The research highlights the potential of multilingual models and the importance of high-quality pre-training data for classical languages.</sample>
    <sample id="153">Nina Rehmayr-Heyl, a scientist at Amazon Alexa, discusses her work on resolving ambiguities in text-to-image models. Ambiguities in prompts can lead to inaccurate image generation, so her team developed a framework to disambiguate these prompts using clarifying questions or multiple visual setups. They also created an automatic evaluation framework to assess the faithfulness of generated images to user intent. Their benchmark dataset, based on the LAVA corpus, covers various ambiguity types. The team found that disambiguation significantly improves image generation accuracy, and their evaluation framework aligns with human judgment. The work aims to enhance the reliability of text-to-image models by addressing ambiguities and ensuring generated images meet user expectations.</sample>
    <sample id="154">The authors of the paper are affiliated with the University of Trento and Fondazione Bruno Kessler.</sample>
    <sample id="155">The speaker's name is Javad Hosseini.</sample>
    <sample id="156">Hello everyone, my name is Aydil Bilard and I will be giving a short overview of the paper, prompting PAMP from translation, assessing strategies and performance. This is joint work with my colleagues from Google Translate. PAMP is a 540 billion parameters large language model presented last year in 2022. It's trained on a large collection of text comprising 780 billion tokens. At the time of publication, it achieves state of the art in hundreds of NLP tasks. In this work, we present the first systematic study of large language model prompting for machine translation. We evaluate the translation capability of such models using the best practices of the AMT community. We compare two state-of-the-art systems, so the best performing systems are the WMT evaluation. We use state-of-the-art neural MT metrics and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the LLMs for translation. As we can see in a simple experiment where we use one shot prompting and provided two different prompts for each sentence. The majority of sentences, 516 out of 1000, the difference observed is of more than one blur points. And this can go in extreme cases up to 40 blur points. So it's important to select a good prompting strategy. In our experiments, we settled for a five shot prompting strategy where we just mark each sentence that we provide to the system with the language it's in. So in this example here where we perform translation from German into English, the German sentences, the source sentences are marked with German colon and the English translations with English colon. We saw that the actual form of the prompting doesn't have a big influence in the case of zero and one shot prompting. It's crucial for zero and one shot prompting, but when we go as in our case to five shot prompting, there is nearly no difference to the actual form of the prompting. It's crucial for example quality than the similarity to the source sentence. So it's important to select the examples from high quality translations. In particular, we compare selecting prompts from the training data of the WMT evaluations or the dev data. The dev data is much more curated and with higher quality than the training data that it's more noisy and the results show a better performance when using the dev data. Nevertheless, specialized state-of-the-art systems have a substantial advantage over the PAMP translation, but PAMP comes pretty close to a commercial system. In our case, we chose to evaluate with Google Translate. The insights that we gain from the human evaluation that we perform using the NPM framework is that the fluency of PAMP is comparable to state-of-the-art systems, but the main difference comes from the accuracy. So in particular, the most common error are omission errors. So it seems that PAMP chooses to produce a better sounding translation sometimes by dropping parts of the source sentence that are made in the translation. However, the style awkward category for PAMP is lower than for the state-of-the-art systems, which is an additional signal that PAMP provides really fluent output but still with some problems of accuracy. And that's it for this really short overview. For more details, please come by the full presentation of the paper. Thank you very much.</sample>
    <sample id="157">Sheng Gao from Sun Dong University presents a work on dialogue summarization using a static dynamic structure fusion graph. The project, a collaboration with several colleagues, aims to distill silent information from dialogues into concise summaries. Traditional methods using precomputed static graphs face challenges like dependency on external tools and inflexibility. Gao's approach introduces a four-component model: encoding utterances into vectors, constructing static graphs, combining them with dynamic graphs using a multi-head attention model, and fusing them with a pre-trained language model. The method captures speaker interactions and positions through advanced metrics, integrating them into a unified graph representation. This approach allows for dynamic adaptation to dialogue summarization tasks, overcoming the limitations of static graph models. The project is detailed in a paper available on GitHub.</sample>
    <sample id="158">The speaker, Xiangkunhu from AWS, introduces Dual Cache, a method for improving coreference resolution in long documents. Coreference resolution involves linking mentions of the same entity across a text. Traditional cache-based methods use a fixed-size cache, reducing complexity to linear levels, but struggle with long documents due to frequent topic switches. Dual Cache addresses this by using a local cache with an LRU eviction policy and a global cache with an LFU policy, allowing for better handling of both local and global entities. The model scans documents, classifies new mentions, and evaluates their frequency, adding them to the appropriate cache. Benchmarks show Dual Cache outperforms single cache methods, especially in long documents, and significantly reduces cache misses. Despite trade-offs in model efficiency, Dual Cache offers the highest performance/cost ratio.</sample>
    <sample id="160">The first step maps the input tokens to unordered multi-sets of tokens that will appear in the output.</sample>
    <sample id="161">Coscript contains a total of 55,000 scripts.</sample>
    <sample id="162">Hello everyone, I'm Akshata, and today I'm presenting with my co-author Martin. Our work, 'The KitKat Test: Evaluating Knowledge Integration from Multiple Sources,' is a collaboration between McGill University, Mila, and Microsoft Research. This work focuses on how natural language understanding models integrate knowledge from different sources, such as pre-trained parameters and inference-time inputs. We propose a diagnostic test suite for knowledge integration and evaluate it with human study participants. Our findings show that many models struggle without task-specific training, but some succeed with it. However, even the best models have difficulty with inference-time knowledge. For more details, check out our paper and the dataset on GitHub.</sample>
    <sample id="163">The best alignment method for DEplain is the MASS align method, as it was found to be the most effective in evaluating sentence alignments.</sample>
    <sample id="164">Weakly supervised learning allows for cheaper and faster data annotation compared to manual labeling, but it requires robust training algorithms to handle noisy labels and still generalize well.</sample>
    <sample id="165">Wenting Zhao, a PhD student at Cornell, presents a paper on 'Adaptiive Common Sense Reasoning Exploiting Mutually Exclusive Explanations.' The paper introduces LIpor, a method for unsupervised learning in adaptive reasoning, which does not require prior knowledge of plausible explanations. LIpor maximizes the marginal likelihood of outcomes given contexts by considering all possible explanations as latent variables. The method enforces mutual exclusivity among explanations, ensuring that only plausible explanations are preferred. LIpor outperforms existing models, including zero-shot models and the best unsupervised approach, by over 4 absolute points in accuracy on the ALFA-NLI dataset. This method offers a significant advancement in adaptive reasoning by addressing the challenge of subjective annotation of explanations.</sample>
    <sample id="166">Yuxin from Harbin University of Technology presented a new method for image retrieval from linguistically complex texts, addressing the limitations of existing visual language models. The method employs a divide and conquer strategy, inspired by dual-process theory, to break down complex reasoning into simpler tasks. It integrates a visual linguistic system for analogical reasoning and a logical system for complex reasoning. The method includes a proposition generator, a decoder, a visual linguistic system, and a neural symbolic reasoner. The neural symbolic reasoner integrates reasoning states and results to solve complex problems. The method's effectiveness is demonstrated through experimental results, showing superior performance over baseline models. Yuxin suggests that neural symbolic calculation and divide and conquer strategies can improve reasoning and planning in large language models.</sample>
    <sample id="167">The DEplain-web corpus includes 750 documents, with 350 aligned using manual methods and the rest using automatic alignment methods.</sample>
    <sample id="168">The CoNLL++ dataset was created by collecting Reuters News articles from 2020 and annotating them using the same guidelines as the original CoNLL 2003 dataset.</sample>
    <sample id="169">Aydin Bilal presents a study on prompting large language models (LLMs) for machine translation, focusing on the PAMP model. The study evaluates the translation capabilities of PAMP using the latest test sets and compares it to state-of-the-art systems. It highlights the significant impact of prompt selection on translation quality, with experiments showing that five-shot prompting yields better results than one-shot or zero-shot. The study emphasizes the importance of using high-quality, human-annotated data for prompts, as it leads to better performance compared to noisy training data. While PAMP's fluency is comparable to advanced systems, it struggles with accuracy, often omitting parts of the source sentence. The study concludes that PAMP is a strong contender for commercial use, despite its accuracy issues.</sample>
    <sample id="171">Existing watermark methods are either not applicable to embedding services or lack transferability, which Embedding Marker addresses by being applicable and transferable.</sample>
    <sample id="172">No, multilingual LLMs like Codex or Bloom are not sufficient for CLSP as they still perform inadequately for cross-lingual semantic parsing tasks.</sample>
    <sample id="173">Hello everyone, my name is Shu-Hung. Today, I am going to present our paper, Do Conner 2003 Named Entity Taggers Still Work Well in 2023? Let's get started. Our paper investigated the problem of generalization using the NER task. We observed that models have been using Conner 2003 to develop NER for almost 20 years. This naturally raises several problems. Firstly, can these models generalize to modern data? And when we do observe poor generalization, what causes the performance drop of these models? To investigate these problems, we developed the Conner++ dataset. This is a dataset that we collected from Reuters News from 2020 and then annotated with the same Conner 2003 annotation guidelines. We then fine-tuned over 20 models on Conner 2003, evaluated them on both the Conner 2003 test set and the Conner++ test set, and last but not least, calculated the percentage change in F1 to assess the generalization of each model. So, what is needed for good generalization? Through our experiments, we found that there are three main ingredients that are needed. The first one is the model architecture. Through our experiments, we found that the transformer models normally generalize better to new data. The second ingredient is the model size. We found that usually larger models lead to better generalization. And last but not least, we all know that the number of fine-tuning examples directly affects the performance of a downstream task. Here, we also found that more fine-tuning examples actually lead to better generalization. To our next question, what causes the performance drop of some models? We had two hypotheses. The first one is adaptive overfitting, which is overfitting caused by reusing the same test set over and over again. And this is usually manifested as the diminishing returns on a new test set. The second hypothesis is temporal drift, which is the performance degradation that is caused by the increasing temporal gap between the train and the test data. For adaptive overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than 1. This means that every unit of improvement that we made on Conner 2003 translates to more than one unit improvement on Conner++, which means that there is no diminishing returns. And this shows us that adaptive overfitting in this case is not observed. So, what about temporal drift then? For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data. And we found that the performance degrades with larger temporal gap. And this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is that for good generalization, we would need a better model architecture, larger model size, as well as more fine-tuning examples. And these go hand in hand. We can't just have one ingredient, but throw all the others. At the same time, we also found that the performance drop here is caused by temporal drift, and kind of surprisingly, it is not caused by adaptive overfitting, even though Conner 2003 has been used for over 20 years. So, going back to the question that we raised in the title of our paper, do Conner 2003 taggers still work in 2023? And we found that the answer is actually a resounding yes. We hope our paper calls for more research on how to improve generalizations of the models. And lastly, please make sure to check out our paper, our dataset, and if you have any questions, feel free to contact me. Thank you so much.</sample>
    <sample id="174">The paper introduces ArgAnalysis35K, a large-scale dataset for argument quality analysis, highlighting its unique features. It is the largest dataset in the field with 35,000 high-quality argument pairs sourced from various debaters, including experts and novices. The dataset offers diverse arguments across 24 themes, not limited to pre-selected motions, and includes a novel analysis component that combines claims, premises, and more. ArgAnalysis35K also introduces instance-based annotator reliability, which accounts for individual biases, and a relevance model that assigns scores to arguments based on their relevance to specific themes. These features aim to provide more reliable and diverse data for argument quality analysis, offering a more nuanced understanding of argument relevance and quality.</sample>
    <sample id="175">The method uses a GPU-friendly continuous relaxation to approximate the highest scoring permutation, allowing for backpropagation to learn more plausible permutations.</sample>
    <sample id="176">Fairness of a downstream NLP model is defined by its ability to perform equitably across different demographic and political groups, without bias towards or against any group.</sample>
    <sample id="177">The speaker's name is Yanis Lavraque.</sample>
    <sample id="178">The speaker's name is Koussof Sinha.</sample>
    <sample id="179">Mlanie Szklare presents Symbolic TOM, a method to enhance Theory of Mind reasoning in large language models. Theory of Mind involves understanding the mental states of others, often tested through false belief questions. Symbolic TOM uses graphical representations to depict characters' beliefs, improving model performance on tasks like the Sally-Anne test. The method outperforms existing models, such as GPT-3 and Textual Time Travel, by providing significant accuracy gains across various datasets, including new story structure and linguistic diversity datasets. It avoids overfitting and offers more interpretable reasoning, making it a robust tool for out-of-domain story understanding.</sample>
    <sample id="180">The speaker's name is Myra.</sample>
    <sample id="181">The paper by Xiyuan from Fudan University addresses the challenge of constrained language planning, which involves planning actions with specific constraints. While large language models have been effective in planning for abstract goals, they struggle with specific, constrained goals. The study evaluates the constrained language planning ability of these models and introduces an over-generated then-filter method to improve script quality. The authors use a dataset called CodeScript, generated from large language models, to train smaller, specialized models for better performance. The paper aims to provide a valuable resource for further research in language planning.</sample>
    <sample id="182">Tropicalism in the context of this paper indicates a stereotype associated with Latina women, characterized by descriptors like 'vibrant' and 'curvaceous,' which perpetuates a narrow and exoticized view of their identity.</sample>
    <sample id="183">The human-written portrayals of target groups were created by giving prompts to human subjects, inspired by a study that found these prompts could surface racial stereotypes.</sample>
    <sample id="184">The work used CxMI (Contextual Usage Measure) and its extension, point-wise CxMI, to measure how much context is used by machine translation models during translation.</sample>
    <sample id="185">DrBERT is a biomedical model trained on medical data, while ChuBERT is a clinical model trained on clinical notes.</sample>
    <sample id="186">Hi, I'm Myra, and today I'll be talking about our paper, Marked Personas, using natural language prompts to measure stereotypes in language models. This work is done in collaboration with Eszter Mosh and Dan Jurafsky. In recent years, many have documented the prevalence of social bias and stereotypes in large language models, or LLMs. However, these measures have various limitations. They usually rely on hand-constructed datasets that are very time-consuming to curate, and they usually only measure very specific stereotypes, meaning that they don't generalize well to other demographics or contexts, or they simply capture very general broad associations, like negative associations with particular groups. Furthermore, most work in this space doesn't account for intersectionality, which is the notion that multifaceted social identities can compound biases and be unique loci of harm. To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions and prompts. So we can ask the model to generate a persona, which is a depiction of an imagined individual using a prompt to like, imagine you are an Asian woman, describe yourself. And we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt. So here are some example generations from GPT-4. Immediately, we see that while the outputs aren't overtly negative or toxic in the traditional sense of these words, there are some interesting patterns. The Asian woman is depicted as unassuming, the Middle Eastern woman is referred to using words like exotic, and like referring to a mesmerizing region, and both of the women of color personas make references to ancestry, while the white man persona has nothing of the sort. To capture these patterns, our method has two parts. The first one is generating these personas. Our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects, finding that by giving it to human subjects, they also were able to surface racial stereotypes. And also, this enables direct comparison between our generated personas and the human-written responses. The second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I'll elaborate on shortly. The benefit of this is that we get really specific stereotypes and patterns without having to rely on any specific lexicon. So the marked words method draws upon the sociolinguistic concept of markedness, which states that there is an unmarked default, and any group that deviates from that default is linguistically marked. So for instance, the word warrior is usually associated with men. So when people are describing a warrior who is a woman, they'll usually actually specify woman warrior and mark the term with woman. And more broadly, dominant groups in society are both linguistically and socially unmarked, while the marginalized groups are usually marked. So in our method, we first designate what the unmarked and marked groups are. And then we compare the personas using the fighting words method, which is basically using weighted log odds ratios to distinguish the top words for each marked group. So for instance, for the personas of black women, we would do fighting words and compare the log odds ratios against both white personas and man personas, because those are the two corresponding unmarked groups. Now for some results. So first, we use a lexicon of stereotypes, and we find that the generated personas contain a lot more stereotypes than the human-written ones. However, when we actually look at the distribution of the words in the lexicon, we find very different things. So while the generated personas have much higher rates of the lexicon words, the human-written ones have a much wider distribution of words, while the stereotype words that are in the generated personas are really just the words tall and athletic. So really just the positive or at least non-negative ones. And in fact, this lexicon doesn't really capture many of the harmful patterns that we saw in the earlier slides while at all. So instead to do that, we'll turn to the results from our marked words method to show how these seemingly positive words facilitate stereotypes and essentializing narratives. In our analysis, we reveal how these seemingly positive portrayals reflect harmful patterns. First, for mark groups, the top words include things like culture, tradition, proud, and exotic. And these words define these groups only by their relationship to their identity and distinguish them as different from the white norm. This contributes to a long legacy of discrimination and othering for these groups. Furthermore, there's a lot of common tropes that are reflected in these words, especially for women of color. So for example, the words describing Latina women include things like vibrant and curvaceous, which connects to a trope of tropicalism. For Asian women, the words are things like petite and delicate and silky, which connects to a long history of Asian women being hypersexualized, seen as very docile and submissive, and so on. And finally, for black women, we see that some of the top words are things like strong and resilient. This connects to an archetype that people have called the strong black woman archetype. And while it sounds like positive at first glance, there's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles, which leads to very negative health outcomes for these people, among other harms. More broadly, we find that the words for each marked group pretty much just reflect very essentializing narratives. So based on these patterns, we conclude with three recommendations for model owners. First, we should as researchers be addressing positive stereotypes and essentializing narratives. We should also be using intersectional lens to study biases and harms because there's a lot of things that might be overlooked if we don't do that. And finally, there should really be increased transparency about bias mitigation methods, because for instance, like these positive stereotypes, we don't know if it's because there is some sort of like weird overly excessive value alignment going on, or maybe some other like anti-stereotyping methods that are resulting in these pernicious patterns. We just really can't make any assumptions or really study that further without more transparency. Thank you so much for listening. Have a good time at ACL.</sample>
    <sample id="187">The paper is authored by Yin and Ji-Yang.</sample>
    <sample id="188">Iterative transfer learning involves updating a model with new data collected in each round of active learning, allowing for continuous improvement and adaptation.</sample>
    <sample id="189">The goal of the dataset is to collect a large-scale public dataset for testing entity selection using indirect referring expressions, covering three domains: music, books, and recipes.</sample>
    <sample id="190">An attacker can extract model parameters by analyzing the embeddings provided by the EaaS, as these embeddings contain information about the underlying model's structure and parameters.</sample>
    <sample id="191">The paper involves three authors: Sara Papi, Matteo Negri, and Marco Turilli.</sample>
    <sample id="192">Yang Luo presents CAN, a memory-efficient optimizer for large language model training. CAN addresses the challenge of balancing fast convergence and low memory usage, unlike Adam and Adafactor, which have trade-offs. CAN uses non-negative matrix factorization to reduce memory requirements and introduces a confidence-guided update mechanism to handle errors in gradient updates. Experiments on the BookCorpus and English Wikipedia datasets show CAN's superior performance, with a 3.4% increase in validation accuracy over Adafactor and better performance than Adam for large models. CAN also demonstrates efficiency in memory usage, with a reduced footprint compared to Adam and Adafactor. The optimizer is effective for large models and can be used as an extension to existing memory-efficient optimizers.</sample>
    <sample id="193">43 annotators were used to create the initial dataset.</sample>
    <sample id="194">The authors of the paper are affiliated with Carnegie Mellon University, the University of Washington, and the Allen Institute for AI.</sample>
    <sample id="195">The presentation introduces ROHT, a framework for explainable question answering that addresses the limitations of existing methods. It involves decomposing complex questions into sub-questions using a hierarchical question decomposition tree and applying probabilistic reasoning to integrate knowledge from heterogeneous sources. The framework is evaluated on the KQAPRO and MUSE datasets, showing that ROHT significantly outperforms existing methods, especially when supplemented with additional text corpora. The results demonstrate the effectiveness of ROHT in integrating answers from different levels and the benefits of combining KB and text information.</sample>
    <sample id="196">The example given is 'I saw Bart and Lisa'.</sample>
    <sample id="197">The state-of-the-art models in dialogue systems include GPT-3, BERT, T5, and BART, which are known for their advanced natural language understanding and generation capabilities.</sample>
    <sample id="198">Evaluating models' acceptability throughout the context window is crucial because current models are often assessed with short, single-sentence inputs, which may not fully capture their understanding of language within a broader context.</sample>
    <sample id="199">Yes, training in multilingual fashion caused performance drops in the English model in seven out of nine datasets.</sample>
    <sample id="200">Yes, the annotators know the names of the entities in advance.</sample>
    <sample id="201">The evaluation used state-of-the-art neural MT metrics.</sample>
    <sample id="202">The paper does not specify if the regression in generalization impacts specific NER types.</sample>
    <sample id="203">Positionality in NLP matters because it affects the alignment and fairness of models and datasets, potentially leading to biased outcomes that favor certain demographics over others.</sample>
    <sample id="204">The multilingual LLMs like BLOOM were fine-tuned with adapters.</sample>
    <sample id="205">Xiangbing, a PhD student at the University of Washington, presented research on the influence of political biases in language models, which are trained on large-scale web data. The research highlights that while these models can learn from diverse perspectives, they may also inherit social biases from politically skewed pre-training data, leading to fairness issues in downstream applications. The study proposes evaluating political leanings of language models using political questionnaires and conducting experiments to understand how biases are transferred from training data. Preliminary results show that models like GPT-4 lean more liberal, and models trained on different corpora exhibit varying political biases. The research also explores the impact of these biases on applications like hate speech and fake news detection, revealing that models with different political leanings perform differently across demographics. The discussion emphasizes the dilemma of sanitizing political opinions in training data without risking censorship, likening it to the 'electric trolley problem.' The presentation concludes with a call to address these fairness issues.</sample>
    <sample id="206">They use a model trained on a task called 'topic independent dissonance stance classification' for transfer learning.</sample>
    <sample id="207">The recent test sets used to assess PaLM's capabilities are the WMT evaluation and the dev data, which are more curated and of higher quality compared to the training data.</sample>
    <sample id="208">The authors proposed three recommendations at last.</sample>
    <sample id="209">The proposed method achieves a gain of 2.1% over the strongest baseline.</sample>
    <sample id="210">The speaker's name is Shu-Hung.</sample>
    <sample id="211">Yes, the results and dataset in the paper can be used as a benchmark for automatic text simplification.</sample>
    <sample id="212">The paper experiments with 15 smaller models.</sample>
    <sample id="213">The base model used for investigating multi-model instruction tuning is the OFA (Unified Multi-Modal Pretrained Model).</sample>
    <sample id="214">Jingwei Yi from the University of Science and Technology of China presents a paper on protecting embedding ad services using a watermark method. The paper introduces Embedding Marker, a backdoor-based watermark method, to protect copyright in embedding ad services. The method involves watermark injection and copyright verification, ensuring the watermark is applicable, non-degrading, covert, and transferable. Experiments on datasets like AG News, MIND, SSTD2, and ERASVM show the method's effectiveness.</sample>
    <sample id="215">Adam Szarkowski's talk focuses on the dependency structure of coordination in linguistics, contrasting symmetric and asymmetric approaches. Symmetric structures, like those in Universal Dependencies and the Prague approach, select one conjunct as the head, while asymmetric structures, such as those in Dependency Treebanks, have the conjunction as the head. Szarkowski argues for symmetric structures, using the principle of dependency length minimization. He provides examples where direct objects are closer to the verb, and adjunctions are further away, but this can be mitigated if the direct object is long. His analysis, based on data from the Penn Treebank, shows that left conjuncts tend to be shorter, especially when the governor is absent. This supports the argument against asymmetric structures, as the tendency for shorter conjuncts is stronger when the governor is on the left.</sample>
    <sample id="216">Hi, I'm Sara Papi from the University of Trento and Fondazione Bruno Kessler, and I will briefly introduce the Attention as a Guide for Simultaneous Speech Translation paper. This is a joint work with Matteo Negri and Marco Turilli. What is simultaneous speech translation? Simultaneous speech translation, or ST, is the process of translating spoken language into text in another language in real time, enabling cross-language communication. What are the problems of the current ST models? Specific architectures are usually trained introducing additional modules to be optimized, long and complicated training procedures, for example, training involving different optimization objectives, and training and maintaining several models to reach different latency regimes, for example, training a model with an average of one second latency and another one with two seconds latency, and so on. So what is our solution? First, to use already existing offline ST models without retraining or adopting specific architecture for ST, use only one model for every latency regime and handle latency through specific parameters, and leverage the knowledge already acquired by the model through the cross-attention mechanism, that is, the cross-attention mechanism. You can see an example on the right. Our solution is to propose a dot, or encoder-decoder attention. It is a strategy for which we decide whether to emit or not a partial translation based on where attention points to. A word is emitted if the attention is not concentrated, that is, the sum is below a certain threshold alpha towards the last lambda speech frames, meaning the received information is not stable. For example, if we receive a speech chunk containing I'm going to talk about and our model predicts the translation in German, and we will look at the cross-attention weights, we will see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames as lambda speech frames. This means that the first two words will be emitted, while since the sum of the cross-attention is above a certain threshold alpha, we will not emit the last word and we wait for another speech chunk. If we go on and we receive another speech chunk and our model predicts other three words, and we will look at the cross-attention weights, we will see that no word points to the last lambda speech frames. This means that these three words will be emitted. If we look at the main results of ADAPT, we plot the simultaneous speech translation results on graphs in which we have blue on one side that measures the translation quality and average lagging, that is, the latency measure, and we also consider the computational aware average lagging that accounts for the models computational times to predict the output. So we want our curves to be as high as possible on this plot, but also we want that they are restricted on the left. And we compare with previous strategies that are also applied to offline models, that are the with key strategy and the local agreement, and we compare also with the state-of-the-art architectures specifically tailored for simultaneous speech translation. These are all the results of the simultaneous speech translation strategy on German. We see that ADAPT outperforms all the strategies applied to offline models, since the curves are shifted over the left, and we also see that if we consider the actual elapsed time or the computational aware time, ADAPT is the fastest strategy. If you want to discover more results, read our paper, and we also released open source the code and models and simultaneous help put to facilitate the reproducibility of our work. Thanks for your attention.</sample>
    <sample id="217">The research introduces Scene2Unseen, a method for multi-attribute controllable dialogue generation, addressing the limitations of existing models that focus on single attributes. The method, D-CGE, uses disentangled controllable generation to learn attribute concepts from scene values and employs disentangle loss to manage attribute combinations. A unified evaluation framework, M-AE, is proposed to evaluate different granularities of attributes without needing large-scale labeled data. Experiments show that D-CGE outperforms baseline models in attribute controllability and test quality, especially with attribute-oriented prompts. The method also demonstrates the ability to generalize from seen attributes to unseen combinations, outperforming models like E2C and A2C. The research concludes that the proposed method effectively transforms scene attributes into unseen attribute combinations, enhancing compositional generation capabilities.</sample>
    <sample id="218">The authors of the paper are affiliated with Google Translate.</sample>
    <sample id="219">Jiahuizhu, a research assistant at Academia Sinica, presented a work on a multi-stage pipeline for uncovering financial signals in financial reports. The project, conducted with Yuxiang Huang and Chen Wei Ling, aims to improve the efficiency of extracting valuable information from annual reports, which are often complex and require significant human effort. The pipeline is designed to compare and contrast financial reports over time, using a reference structure that includes the current and previous years' reports. The process involves document segmentation, relation classification, and fine-tuning using an external dataset. The model classifies pairs of words into three types: type beta, revised, and mismatch, to identify important financial information. The pipeline's performance is evaluated using precision and PCC metrics, and it shows strong results on the final dataset. The work also suggests future directions, including improving the model's effectiveness and exploring additional features. The presentation concluded with an invitation to refer to the paper and GitHub for more details.</sample>
    <sample id="220">The authors are affiliated with Stony Brook University.</sample>
    <sample id="221">The paper analyzed language pairs including German, English, Chinese, and Japanese.</sample>
    <sample id="222">The work focuses on enhancing open domain question answering by adapting to different domains. It explores data interventions, such as zero-shot and few-shot methods, to improve model performance. The study identifies the nature of dataset shifts, including concept, covariate, and full shifts, and measures compatibility between source and target models. It finds that certain interventions, like few-shot, are effective for concept and covariate shifts, while zero-shot works well for no shift. The research concludes that specific data interventions can significantly improve reader performance, with up to 24% improvement observed.</sample>
    <sample id="223">The speaker's name is Xiangbing.</sample>
    <sample id="224">The models investigated were the Long-IMPART and the Normal-Base-IMPART.</sample>
    <sample id="225">For training, 53 tasks are used, and the entire common sense reasoning group is reserved for testing.</sample>
    <sample id="226">The paper has two authors.</sample>
    <sample id="227">The speaker discusses the challenges in grounded language understanding, which involves mapping natural language to specific environments. Current language models lack grounding during pre-training, leading to issues in downstream applications. The speaker introduces a framework called PANGO, which focuses on discrimination rather than generation, allowing language models to score and rank plans proposed by a symbolic agent. PANGO has been tested on various language models, including BERT, T5, and CodeX, and has shown strong performance, especially in non-ideal settings. The speaker emphasizes that discrimination might be a better strategy for language models in grounded language understanding.</sample>
    <sample id="228">The authors conducted experiments on the AG News, MIND, SSTD2, and ERAS86 datasets.</sample>
    <sample id="229">Gabriela Skatylinskaya presents a study on detecting improvable claims in argumentative writing, focusing on text revisions. The research, conducted with Henning Baak Smud, aims to determine when claims need revision and suggest improvements. The study introduces two tasks: suboptimal claim detection and claim improvement suggestion. It explores challenges in using revision-based data, such as representativeness, model complexity, contextual relevance, and bias. The paper presents experiments with different model architectures and discusses the impact of contextual information on claim quality. The findings suggest that revision-based data can effectively be used for the tasks, with the distance between claim versions being beneficial for detecting suboptimal claims.</sample>
    <sample id="230">The speaker, Kosta Sinha, introduces a talk on the paper 'Language Model Acceptability Judgments Are Not Always Robust to Context.' The paper, a joint work with several researchers, revisits the minimal pair paradigm, which evaluates language models based on acceptability judgments. The current minimal pair paradigm does not allow evaluation of models' acceptability towards longer sentences. The approach involves simulating longer sequences and evaluating models' acceptability on longer sequences. The paper finds that language models are sensitive to latent syntactic and semantic features and that the current minimal pair paradigm may not fully capture models' abstract knowledge.</sample>
    <sample id="231">NACHOS is a dataset of medical crawled data from the web, used for training the Dr. Bert model.</sample>
    <sample id="232">The speaker's name is Aydar Bilard.</sample>
    <sample id="233">Sara Papi from the University of Trento and Fondazione Bruno Kessler presents a paper on Simultaneous Speech Translation (SimulST), which enables real-time cross-language communication. Current models face challenges like complex training and multiple latency regimes. The proposed solution, E-DOT, uses existing offline models and a single model for each latency, leveraging cross-attention to decide on partial translations. E-DOT outperforms existing strategies by shifting curves to the left, indicating better translation quality and computational efficiency. The results are available in their paper, and they have released open-source code and models for reproducibility.</sample>
    <sample id="234">The prompting strategy significantly impacts results, with high-quality example selection improving performance. Five-shot prompting shows minimal difference compared to one-shot, but example quality is crucial.</sample>
    <sample id="235">The authors of the paper are affiliated with the University of Toronto.</sample>
    <sample id="236">The presentation does not provide specific details about the 5 expert-written instructions included in the Multi-Instcrt dataset.</sample>
    <sample id="237">The authors propose a diagnostic test suite called KITMOS, which includes a coreference resolution task to evaluate the models' ability to integrate knowledge from different sources, such as pre-trained parameters and inference-time inputs.</sample>
    <sample id="238">Yibowen Hu from the University of Central Florida presents MeetingBank, a new benchmark dataset for meeting summarization. The dataset addresses challenges in creating high-quality meeting summaries and finding trustworthy resources. It includes 1,366 city council meetings with transcripts, summaries, and URLs. The data collection process involved using Speechmatics API for transcription and aligning transcripts with summaries. The dataset's analysis measures abstraction using coverage and density scores. Model evaluations compared extractive and abstractive summarizers, with GPT-3 showing high fluency and coherence but lower informativity. Human evaluations rated summaries on five criteria, with GPT-3 achieving the highest scores. The dataset serves as a tool for researchers and offers insights into city council decision-making.</sample>
    <sample id="241">Ethan discusses a paper on early misinformation detection, highlighting the shortcomings of current systems, such as unrealistic evaluations and lack of human involvement. The paper proposes a framework for developing systems that integrate human feedback throughout the detection process. The system includes two main components: detecting misleading claims using keyword filtering and a T5 model, and verifying policy violations with a BERT-based stance classification model. The framework aims to improve early detection of misinformation, particularly in the context of COVID-19, by involving humans at various stages. The system's evaluation shows a 65% accuracy in policy violation detection and can identify 124.2 policy violations per human hour. The work provides a realistic approach to misinformation detection and encourages future development in this area.</sample>
    <sample id="242">Common evaluation methods for dialogue systems include human evaluations using comparative or Likert scale methods, and approaches like ABC Eval that annotate specific behaviors in chat.</sample>
    <sample id="243">The paper involves four authors: Sebastian Santi, Ronan Labrosse, Caterina Ranecka, and Martin Sab.</sample>
    <sample id="244">The background knowledge needed includes understanding that Servin is a judge and that judges decide cases in law courts.</sample>
    <sample id="245">Linying Zhang presents a study on a pipeline for identifying high-agreement Mechanical Turk (mTurk) workers for summarization tasks. The pipeline aims to address issues with automatic matrix and recruitment practices. It involves two stages: a qualification task to assess annotators' ability to evaluate multiple dimensions and an endurance task to test their capacity for handling heavy workloads. The study categorizes workers into four types and finds that only 6% of participants pass both stages. The pipeline achieves high inter-annotator agreement and is cost-effective compared to cloud research workers. However, it has limitations, such as only testing English summarizations and not guaranteeing the training of correctness. Future work will explore hiring high-quality workers and applying the pipeline to various tasks and platforms.</sample>
    <sample id="246">Yes, the code is available on GitHub.</sample>
    <sample id="247">The presentation by Chiok-Kin from KAIST AI introduces a new dataset, FactKG, for fact verification using knowledge graphs. Unlike existing datasets that use text or tables, FactKG utilizes DBpedia knowledge graphs, which allow for direct connections between evidence and claims, enhancing reliability. The dataset includes claims in both written and colloquial styles, with labels for supported and refuted claims. It employs five types of reasoning: one-hop, conjunction, existence, multi-hop, and negation, each requiring specific verification methods. The dataset's performance is evaluated using claim-only baselines and the Gear model, with the Gear model outperforming others. The presentation concludes with an invitation to download the dataset and contact the presenter.</sample>
    <sample id="248">Yes, the annotators for NLPositionality are balanced across various demographics, including country, gender, and education level, ensuring a diverse representation in the dataset.</sample>
    <sample id="249">In the acceptable domain, sentences were perturbed by adding noise while preserving their relevant structure, which led to increased MLP judgments across all perturbations.</sample>
    <sample id="250">Dimensional evaluation means assessing various specific aspects of chat quality, such as relevance and contradictions, to understand a model's strengths and weaknesses more precisely.</sample>
    <sample id="251">The authors are affiliated with the University of Science and Technology of China.</sample>
    <sample id="252">The presentation by Saikiran Thanikella and colleagues introduces 'U-CREATE', a project focused on unsupervised case retrieval using event extraction. Legal professionals often rely on experience to cite relevant past cases, but with the growing volume of cases, automated retrieval is essential. The project introduces the ILPR dataset, a comprehensive benchmark for prior case retrieval with 7,070 Indian legal cases, and the U-CREATE pipeline, which uses unsupervised learning and event-based approaches. The event extraction process involves dependency parsing to form subject-verb-object triplets, which are then used to compute an interaction matrix for candidate retrieval. Experiments with various models, including count-based, transformer-based, and event-based, show that event-based models, particularly the event filtered documents model, outperform others. The U-CREATE pipeline outperforms existing methods, including those from the COLI21 dataset, marking a significant advancement in the field.</sample>
    <sample id="253">Mario Edarragon presents 'NameDisorder,' a project aimed at detecting mental disorders through social media analysis. The project uses a double domain adaptation model to leverage knowledge from general language models like BERT, adapting it to the specific language of Reddit and mental health. This approach allows the model to focus on mental disorder-related content, improving its performance. The model's effectiveness is demonstrated through experiments on the BDI dataset, showing balanced results in precision and recall. The research highlights the potential of using social media as a tool for early detection of mental health issues, outperforming models trained on large datasets. Future work aims to explore the use of different lexical resources and clinical data to further enhance the model's capabilities.</sample>
    <sample id="254">Sun Qi from Nanjing University of Science and Technology presents a research work on 'Uncertainty Guided Level Denoising for Document Level Relation Extraction'. The work addresses the challenge of noisy labels in distant supervision data, which can lead to false positives. The proposed framework introduces uncertainty estimation to assess the trustworthiness of model predictions, particularly for overlapping relations. This involves using Monte Carlo dropout in the pre-denoising model and dynamic class uncertainty thresholds to filter out high-uncertainty pseudo labels. The research also employs a multi-phase training strategy to iteratively relabel the DS data, enhancing model performance. The framework outperforms existing baselines on two public datasets, demonstrating significant improvements in label quality and performance.</sample>
    <sample id="255">The form of the prompting is crucial for zero and one-shot prompting, but less so for five-shot prompting.</sample>
    <sample id="256">The content is in English.</sample>
    <sample id="257">The authors evaluated four state-of-the-art chat models.</sample>
    <sample id="258">Zhangsun Han discusses a novel approach using large language models to evaluate text quality in natural language processing, aiming to replace human evaluations. The project involves instructing large language models to rate texts based on grammar, coherence, likability, and relevance. Experiments show that models like Davinci and ChatGPT can provide meaningful evaluations, similar to human experts, with some models showing a preference for human-written texts. The paper also addresses questions about the impact of instruction changes and the benefits and drawbacks of using large language models versus human evaluators. Further details and results are available in the paper and at the poster session.</sample>
    <sample id="259">Yusen Zhang from Penn State University presents the Exemplar dataset, a comprehensive resource for cross-lingual semantic parsing across multiple natural languages and representations. The dataset includes 90 datasets, 5 semantic parsing tasks, 8 million representations, and 22 languages from 15 families. Zhang evaluates models using six settings: translate test, monolingual, multilingual, cross-lingual zero-shot, and cross-lingual few-shot transfer. The encoder-decoder model, specifically encoder-decoder, achieves the best performance across all datasets. The study reveals that multilingual training improves encoder-decoder performance, except for English in some datasets. Cross-lingual transfer performance varies, with zero-shot transfer showing significant gaps and few-shot transfer reducing these gaps. The research highlights that encoder-decoder models outperform previous works and that multilingual training can boost performance on target languages. However, multilingual models like CoT and BERT still fall short for cross-lingual semantic parsing tasks. The study provides a unified benchmark and comprehensive analysis of multilingual language models.</sample>
    <sample id="260">The paper is authored by Jingwei Yi and colleagues from the University of Science and Technology of China.</sample>
    <sample id="261">A good planner should write scripts that are both reasonable and faithful to the constraints.</sample>
    <sample id="262">The paper has three authors.</sample>
    <sample id="263">The presentation discusses the issue of label biases in in-context learning, a popular method for using large language models. The instability of in-context learning arises from design choices, leading to biased predictions. The work introduces a new type of bias, domain label bias, and proposes a calibration method to mitigate these biases. Experiments show that domain context calibration, which uses random in-domain words, significantly improves model performance, especially on tasks with high domain label bias. The method outperforms previous calibration methods, which used single predefined tokens. The findings suggest that using random in-domain words is more effective than random English words in addressing domain label bias.</sample>
    <sample id="264">Liu Wang, a graduate student at Zhejiang University, presented a paper on 'Towards Transferable Audio Visual Text Generation.' The paper addresses the challenge of multimodal text generation, particularly audio visual text generation, which is more complex and expensive than other tasks like machine translation. Wang proposes a novel task called 'transferable audio visual text generation' to overcome these challenges. The framework includes three components: an audio visual meta map network, an audio visual encoder and language model generator, and a context constraining factor. The paper introduces a unified audio semantic space to align visual concepts across domains and employs a meta learning approach for quick adaptation to new multimodal domains. The results show that the proposed method outperforms other models in various settings, even with limited labeled data. The paper also explores the impact of audio features and semantic comments on performance.</sample>
    <sample id="265">The speaker's name is Vasudha.</sample>
    <sample id="266">The authors are affiliated with the University of Gothenburg and the University of Gothenburg, Sweden.</sample>
    <sample id="267">Hello everyone, my name is Yuxin Zhang from the Penn State University. Today, I'm going to present our work, EXAMPLER, on cross-lingual semantic parsing in multiple natural languages and multiple representations. Semantic parsing is the task to build semantic representations of user queries such as SQL and lambda calculus. And cross-lingual semantic parsing is the task to translate queries in multiple natural languages into multiple meaning representations. As shown in this figure, we need to translate the query in multiple natural languages using neural models to SQL, lambda calculus, and etc. Existing cross-lingual semantic parsing models are separately proposed and evaluated on datasets of limited tasks and applications. For instance, there are lacks of coverage on certain natural language, the Chinese is missing, and the lambda calculus is missing. Or there are only evaluated on certain neural model. So to this end, we propose EXAMPLER, but provide a uniform dataset EXAMPLER for cross-lingual semantic parsing in multiple natural languages and multiple representations. It contains 90 datasets in various domains, 5 semantic parsing tasks, 8 million representations, and 22 natural languages in 15 language families. And to better evaluate our benchmark, we consider the six settings for training and evaluation. The first one is translate test. We use Google translate API to translate source to the target language, then use monolingual model to train and evaluation. And for example, we train the English model on English query, and during inference, we translate the German query using API to English, and then use the trained model to predict the SQL. And we test monolingual model. In this setting, the source language is the same as target language, for example, German to German or English to English. We also test monolingual few shot setting, but training monolingual models with only 10% of training data. And we test multilingual model, which we train one multilingual model for all languages. For example, we put the German, English, Chinese queries together to train a multilingual model, and during inference, we can use this model to translate German queries or Chinese query, or et cetera. And we also consider cross-lingual zero shot and few shot transfer. We train on one source language and transfer to another language. So during training, we train on English query or the combination of English and German few shot queries to train a multilingual model to predict the SQL output. And we found many interesting results. So regarding analyze of monolingual models, we evaluate on two groups of models, including encoder PDR, which stands for Multilingual Pretrained Encoders with Pointer Based Decoders such as XLNLP and BERT PDR. And we evaluate encoder-decoder models, which is Multilingual Pretrained Encoder-Decoder models such as M-BART and MT5. We found that encoder-decoder obtains the best performance on all nine datasets. And we evaluate on MT5 and EXAMPLER on multilingual setting. We found that encoder-decoder or encoder PDR can be improved by training in a mixture of various languages. And we found that it is because most of the major natural languages can obtain performance gain, except that English performance drops in seven datasets and only gains in three datasets. I think this is known as curse of multilinguality. We also compare the cross-lingual performance gap. In this figure, the blue line is cross-lingual few shot transfer, the orange line is cross-lingual zero shot transfer, while the green line is the monolingual setting. We found that by comparing the green and orange line, we found that for zero shot setting, the cross-lingual transfer performance gap is significant, and by comparing blue and orange line, we found that for few shot setting, the transfer gap is shortened rapidly. We also find some other interesting findings. For example, encoder-decoder outperforms previous work or achieved comparable results. But training on English natural language can significantly boost the performance of few shot on target natural languages. And we found that multilingual language models such as CoT and BERT are still inadequate for cross-lingual semantic parsing tasks. To sum up, we build EXAMPLER, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and multiple representations. We conduct a comprehensive benchmark study on three representative of types of multilingual language models, and our results show many interesting findings, and et cetera. And welcome to visit our paper and code. Thanks for listening.</sample>
    <sample id="268">The most common errors of PaLM are omission errors, where it sometimes produces a better sounding translation by dropping parts of the source sentence.</sample>
    <sample id="270">The authors are affiliated with the Emory NLP Lab and Amazon Alexa AI.</sample>
    <sample id="271">CFT stands for Continuous Fine-Tuning, a method proposed as a strong baseline for future work in WSL.</sample>
    <sample id="272">The paper involves a total of eight authors.</sample>
    <sample id="274">The speaker's name is Yuxin Zhang.</sample>
    <sample id="275">Hi, I'm Xianbing, PhD student at the University of Washington. Today, I'm presenting our work from pre-training data to language models to downstream tasks, tracking the trails of political biases leading to unfair NLP models. So language models are trained on large-scale web crawl data. Political news media are well covered in their pre-training data. According to a survey of the C4 corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etc. are well covered in language model training data. This has created a mixed blessing for language model applications. So on one hand, they were able to learn from diverse perspectives, which celebrates democracy and the plurality of ideas. On the other hand, these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications. To this end, we propose to investigate the political bias propagation pipeline from pre-training data to language models to downstream tasks, specifically by asking the following questions. First, how do we evaluate the political leaning of language models and what role does pre-training data might have on such political biases? Secondly, how do language models with different political leanings actually perform on downstream tasks, and whether that might result in fairness issues in NLP applications? So we first propose to prompt language models with different prompt formats using the political questionnaires such as the political compass test. This ensures to do automatic evaluation well grounded in political science literature. So some preliminary results demonstrate that first, language models do have varying political leanings. They occupy all four quadrants on the political compass. We can also see that GPT-4 is the most liberal language model of all. And GPT-3 is generally more socially liberal than BERT-3 and its variants. Secondly, we aim to investigate to which extent the political biases of language models are actually picked up from training data. So we conduct a controlled experiment by further pre-training language model checkpoints on six different partisan corpora separated into news and social media further divided into their political leaning. By further pre-training language models on such partisan corpora, we can see that the ideological coordinates of the language model also correspondingly shift. For example, for Roberta, further fine-tuned and further trained on the left-leaning Reddit corpus, we can see a substantial liberal shift in terms of its political biases. And we also try to investigate whether language models can pick up the polarization that's prevalent in our modern society. So we divide pre-training corpora into pre-45th President of the United States and after 45th President of the United States. We separately pre-train language models on the two different temporal corpora. We can see that language models generally had a political leaning that is further away from the center after 2017. So this indicates that language models can also pick up the polarization in our society. So last but not least, we evaluate language models with different political leanings on hate speech detection and fake news detection to NLP applications that often involve language models and could have very significant implications. So we see that if we investigate the per category performance, that is to say, if we separate the performance into different demographics or political leaning of news media, we can see a pattern that, for example, for hate speech detection, left-leaning language models are better at detecting hate speech targeting socially minority groups, however, are worse at detecting hate speech targeting more powerful folk groups in our society. And vice versa, right-leaning language models are better at detecting hate speech targeting white and men, however, worse at detecting hate speech targeting black LGBTQ+ and other minority communities. Similar trends also happen for fake news detection, where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa. This in we further show many qualitative examples to see that language models with different political leanings do give different predictions to hate speech and misinformation examples based on their social categories. There are a bunch of more examples in the appendix to further highlight that. This indicates that there is a fairness issue that is very pressing regarding the political biases of language models. For example, if a right-leaning language model were to be fine-tuned on hate speech or misinformation and deployed to a popular social media platform, this would mean that people with opposite political opinions might be marginalized and the hate speech targeting minority groups might just run rampant without any control. So this has sounded the alarm for us to acknowledge and tackle the fairness issues resulted by language model political biases. So a little bit of discussion. We would also like to highlight that we expose the unique dilemma regarding language model political biases. It's like between C and K. So if we do not sanitize the political opinions in language model training data, the bias would propagate from pre-training data to language models to downstream tasks, ultimately creating fairness issues. If we do try to sanitize somehow, we will also risk censorship or exclusion, and it's incredibly hard to determine what is actually neutral and should be retaining language model training data. So it's kind of like the electric trolley problem. Okay, great. I think that's pretty much all I have for today. Thank you for your time.</sample>
    <sample id="276">Ananya and Vignesh present their work on the IndicMT Eval dataset, which aims to evaluate machine translation metrics for Indian languages. They focus on five languages from two language families, Tamil, Malayalam, Hindi, Marathi, and Gujarati, and generate 7,000 samples for evaluation. Bilingual expert annotators assess the translations, marking errors and providing overall scores. The study finds that IndicMT's metrics, particularly the fine-tuned Comet metric, outperform existing metrics like COMET and show higher correlations with human scores. The research highlights the importance of developing evaluation metrics tailored to the unique characteristics of Indian languages, as current metrics often exhibit skewed score distributions.</sample>
    <sample id="277">The new method does not have a name mentioned in the content.</sample>
    <sample id="278">The 'marked words' method identifies words that distinguish marked groups from unmarked ones, using weighted log odds ratios to highlight stereotype words, revealing how seemingly positive portrayals can reflect harmful patterns.</sample>
    <sample id="279">The authors are affiliated with the University of Washington.</sample>
    <sample id="280">Xiaotao presents the Multi-EMO framework for emotion regulation in conversations, addressing challenges in multimodal emotion classification. The framework includes a novel visual feature extractor, Miss-ExtNet, which focuses on facial expressions without redundant scene information. Multi-Attend, a multimodal fusion model, integrates textual, audio, and visual data through cross-attention layers. The framework also introduces a sample-weighted focal-contrastive loss to improve classification of minority and semantically similar emotions. Extensive evaluations on the MILD and IEMOCAP datasets show state-of-the-art performance, with improvements in difficult scenarios. However, limitations include the inability of Miss-ExtNet to distinguish between speakers and scene people, the need for large training data, and suboptimal performance in minority emotions.</sample>
    <sample id="281">Kyowin presents a study on when translation requires context, conducted with Patrick Fernhout, Emile Liu, Andre F. de Martens, and Graham Neubig. The research explores how context affects translation, using CxMI to measure context usage by machine translation models. The study identifies discourse phenomena that require context, such as pronouns, verb forms, and ellipsis resolution, and develops a MUDa tagger to identify these in parallel corpora. The findings show that context-aware models perform better than context-agnostic ones for certain phenomena, but not for others like ellipsis and verb forms. The study also compares different translation systems, finding that DeepL is more accurate than Google Translate for document-level translation. The research aims to improve document-level translation by understanding the role of context.</sample>
    <sample id="282">The presentation introduces StoryTrance, a novel approach to non-parallel text style transfer at the discourse level, addressing the challenge of imitating author styles in long texts. The model, StyleTrance, leverages discourse representations and style embeddings to generate text in target styles. It employs a two-stage training framework, using self-restriction loss and disentanglement loss to separate style and content, and style classifier loss to generate style signals. The model's effectiveness is demonstrated through extensive experiments, showing its ability to control style and maintain content, outperforming strong baselines. The approach also includes a method to handle cases where the model might introduce unrelated sentences, ensuring the preservation of the original text's semantics.</sample>
    <sample id="283">The first mentioned symmetrical dependency structure is the universal dependencies structure.</sample>
    <sample id="284">Peng Tianxun from Wuhan University presented FSUIE, a novel fuzzy span mechanism for enhancing universal information extraction. The current span-based UIE model relies on precise span boundaries, which can be ambiguous, leading to potential mislabeling. FSUIE proposes a fuzzy span approach, where the target boundary is represented as a continuous distribution, allowing for more flexible attention. This is achieved by using a sampling function to convert the distribution into discrete values for calculating fuzzy span loss, which includes binary cross-entropy and KL divergence. The model introduces a fuzzy span attention layer, which dynamically adjusts the attention span and ensures a more reasonable attention distribution. Experiments on tasks like named entity recognition, relationship extraction, and aspect sentiment triplet extraction showed significant improvements with FSUIE, especially on small-scale data. The model's ability to focus on semantic information within a limited range of tokens was confirmed, and the results demonstrated the effectiveness of FSUIE in various information extraction tasks.</sample>
    <sample id="285">The presentation by Mingqi Gao from Peking University focuses on the work 'Reference Matters: Benchmarking Factor Error Correction for Dialogue Summarization with FinGrant Evaluation Framework.' The key issue addressed is the presence of factual errors in summaries generated by models, particularly in dialogue summarization. Two main solutions are proposed: introducing fact-related objectives in model training to improve faithfulness and developing a separate error correction model (FEC) to correct these errors. The current evaluation methods for FEC models, which rely on fact-based metrics, are criticized for being vague and potentially misleading. Gao suggests a new taxonomy for factual errors, distinguishing between content-based and form-based errors, and introduces an evaluation framework based on alignment, classification, and comparison. The findings indicate that training FEC models with reference summaries from dialogue datasets improves performance, and there is a need to change evaluation methods. The presentation concludes with the potential of combining human-annotated data with synthetic data to enhance FEC models.</sample>
    <sample id="286">The speaker's name is James Finch.</sample>
    <sample id="287">The paper involves four authors: Javad Hosseini, Philip Radlinsky, Silvia Parisi, and Annie Luce.</sample>
    <sample id="288">Datasets like the Blimp dataset can be used to test syntactic phenomena by creating pairs of acceptable and unacceptable sentences with matching grammatical structures.</sample>
    <sample id="289">Hello, my name is Kai-yan and I will be presenting our work titled "When Does Translation Require Context: A Data-Driven Multilingual Exploration." This work was done in collaboration with Patrick Fernhout, Emile Liu, Andre F. D. Martins, and Graham Neubig. So a lot of translations depend on context. For example, how would we translate mole in this sentence? Well, if the previous sentence was, things could start to get dangerous if the ministers find out, then mole refers to a spy. But if the previous sentence was, could it be anything serious, doctor? Then mole refers to a birthmark. So depending on context, the meaning of the word changes and therefore is translation changes as well. However, evaluating how well models can translate cases like this is pretty hard. Firstly, because only a small portion of translations depend on context, which makes corpus level metrics like BLEU unable to capture these translations. And some people have suggested targeted evaluation on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages, since they usually rely on domain knowledge and human creation. In this work, we try to answer these two questions: first, when does translation require context, and second, how well do models handle these cases? To answer the first question, we started by measuring how much a word depends on context during translation. In the previous work, we introduced cXMI as a measure for context usage by machine translation models. And this is done by measuring how much information the context C provides about the target Y, given the source X. You can think of cXMI as the information gained from giving context to the model. In this work, we extend cXMI to pointwise cXMI, which can measure context usage at the sentence level or at the word level. We can think of words that have high PCXMI as ones that require context for translation. Now we analyze words with high PCXMI to look for patterns between these words. And we perform our analysis on transcripts of TED Talks that have been translated from English to 14 different languages. We perform our analysis at three different levels. First, we look at part of speech tags that have high PCXMI, and this allows us to find, for example, dual pronouns in Arabic that have relatively high PCXMI. And this can be explained because English doesn't have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic. Similarly, we find that certain languages also require context when we want to choose the appropriate verb form. We then look at vocabulary items that have high PCXMI averaged over all of its different occurrences. And this helps us identify cases like the one here, where in Chinese you need context to translate proper nouns to make sure that you're using the same translation within the document. And similarly, we find that context is supported to translate in the right formality. And finally, we look at individual tokens that have high PCXMI, and this allows us to identify phenomena that cannot really be captured by the word itself, but that's rather expressed in the sentence structure, such as ellipsis resolution. So now we use our findings from our analysis to design a benchmark for document-level translation. For each of the five discourse phenomena we identified, we create taggers to automatically identify words that pertain to the phenomenon. And we call our tagger the Multilingual Discourse-Aware, or MUDa, tagger. We can then also note that different languages have different proportions of these discourse phenomena. We then use the MUDa tagger by applying the tagger on a parallel corpus that we want to use for evaluation. And we apply our translation metrics of choice on the context-dependent examples that the MUDa tagger has identified. And finally, we use our benchmark as well as other metrics to evaluate different models on document-level machine translation. First of all, when we use corpus level metrics, so for BLEU, we find that context-agnostic models have the best performance. But then if we use COMET, context-aware models perform best. And if we use word F measure, then models with or without context have comparable performance. This again demonstrates that it is difficult to determine the best document-level translation system if we use corpus level metrics alone. Now we use the MUDa benchmark to evaluate models, and we find that context-aware models are significantly more accurate than models that do not use context for certain discourse phenomena, such as formality and lexical cohesion. But these models are not much better than models that do not use context for other phenomena like ellipses, pronouns, and verb form. So this sort of suggests where we would need to see more progress for document-level translation. We also compared different commercial systems, and our benchmark shows that DeepL is usually more accurate than Google Translate for document-level translation. To summarize, we perform a data-driven analysis across 14 language pairs to identify when translations require context. And then we use our findings to build a benchmark for document-level translation, which can help us identify which discourse phenomena models can handle well or not, and which translation systems are good at document-level translation. Thank you so much for your attention, see you in Toronto.</sample>
    <sample id="290">The abbreviations of the five methods for the first research question are: CoSine, FSW, FSW-2, FSW-3, and FSW-4.</sample>
    <sample id="291">The model is evaluated on 11 biomedical and clinical downstream tasks in French.</sample>
    <sample id="292">Hi, welcome to our presentation of DeepL, a new corpus for German text simplification on the document level and on the sentence level. My name is Regina Stoddens, and I will guide you through the first part of the presentation. Let's first define text simplification. Text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group, as people with reading problems on non-native speakers. To train a text simplification model, we require parallel pairs of text, for example, of documents or sentences. In the example here, you can see a parallel aligned sentence pair of a complex German sentence and its translation into plain language. To simplify the sentence, different techniques are possible, as you can see in the example, such as lexical substitution, clause deletion, clause deletion reordering, or insertion of words. We now propose our new corpus DeepL, because in the recent years, there were some problems with existing corpora. So for example, these corpora here are too small to train a text simplification model on. The others three models which are proposed in recent years are all automatically aligned, which means they can be error-prone in their alignments. Therefore, we propose our new corpus DeepL, which is split into two sub-corpora, DeepL APA and DeepL Web. DeepL APA is based on news texts. In DeepL APA, we aligned 483 documents, all manually. It results in roughly 13,000 parallel sentence pairs. For DeepL Web, this corpus includes different domains, and we also align all of these 750 documents on the one hand manually and on the other hand with automatic alignment methods. In total, we result in 30,450 sentence pairs. We analyzed our sentence pairs a little bit more, so for example, on the type of simplification. As you can see here, the Bible texts are much stronger simplified than, for example, the news texts, other language learner texts. On all levels, regarding, for example, lexical simplification, structural simplification, also the overall level of simplification. Furthermore, you can see that our DeepL corpus has a high variety of different simplification transformations. So for example, in the DeepL APA corpus, we have much more reordering and word additions than we have in the DeepL Web corpus. On the other hand, in the Web corpus, we have much more rephrasings. So let's now see what we can do with this corpus. Hello, I am Omar, and now I will talk about the use cases for our dataset DeepL. So for the first use case, we can evaluate automatic alignment methods. In the recent years, there has been a lot of alignment methods, but in the context of machine translations, where we have two parallel documents written in different languages and we want to extract alignments of sentences in post documents. But in our use case, we are trying to extract alignments between sentences of two parallel documents, having the same language, having the same content, but they are on a different complexity levels. Now, as we have our dataset DeepL, which have manually aligned sentences, we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods. And we did some adaptations to the proposed methods, and we have published all these adaptations and the codes to run our experiments in the paper. At the end, we concluded that the best alignment, automatic alignment method to use for German text simplification is the method of MASS align. And you can also find the code to run this method on your own documents in the paper. The second use case that we showed in our paper is the case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text. We have fine-tuned two different models. We have fine-tuned the model of long-impart to produce document level simplifications, and we also fine-tuned the normal base-impart to produce sentence level simplifications. You can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper. We concluded that this basic fine-tuning could produce or could get scores better than the baseline scores. And we proposed those results as a benchmark, a base benchmark for the problem of automatic text simplification in the future. Thank you so much for your attention, and we hope to meet all of you during the conference. Thank you.</sample>
    <sample id="293">Hi, I'm going to talk about our work on resolving indirect referring expressions for entity selection, in which we introduce the altentities corpus. My name is Javad Hosseini, and this is a joint work with Philip Radlinsky, Silvia Parati, and Annie Churvis. Our goal is to understand users' language when they want to make a choice. Consider this alternative question: Did you mean easy on me or I got a feeling? Here, a user wants to select between one of these two songs. The most obvious thing is to use a direct reference, for example, by saying the name of the song, easy on me, or its position, the first one. But sometimes an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name of the song, or the pronunciations are too similar to each other and hard to disambiguate, or when the user wants to specify a preference. Here are some examples of indirect references, for example, the newer one or the song that's not energetic. This is an important problem in conversational systems and also for BING marking LLM's entity understanding. We're not aware of a public dataset, a large-scale public dataset for the task, so we collect one using crowd annotation. Our dataset covers three different domains: music, books, and recipes. Our data set collection methodology emphasizes informality using a cartoon completion setup. The cartoon has three speech bubbles. In the first bubble, Bob says, remember that song we were listening to yesterday. And with that, Bob sets the context. In the second speech bubble, Alice says, do you mean easy on me or I got a feeling? Which is the alternative question. And in the third speech bubble, Bob uses an indirect reference to select one of these entities, for example, the newer one. We provide the first and second speech bubbles automatically, but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain. The second one, which is the alternative question, is generated as follows. We always use a simple template, do you mean A or B, where A and B are sampled from Wikipedia. Here are the different sampling methods we've used. When we move higher in the list, the entities become more similar to each other, and it's usually harder to make the disambiguation. The first one is uniform at random. The second one is when the entities have similar titles, for example, two books with the name the return. The third one is when they have similar descriptions on Wikipedia, and finally, when they have similar info boxes or attributes on Wikipedia, for example, the same genre or the same artist for a song. When we show these alternative questions to the annotators, they know the name of these entities, but they don't necessarily know about the entities. So what we do is that we show some background knowledge about the two entities. For songs, we simply show a Google search link to each song. And then ask the annotators to listen to at least some of each song and read about each song. Here's for example, the Google search result for the song easy on me. For the recipes and books domain, we show some background text from Wikipedia. For recipes, we additionally show their images, again from Wikipedia, so that the annotators know how they look like. Then we ask the annotators to pick one of these entities, for example, here the first one, and describe them using three to five indirect referring expressions. For example, the one with the piano music. Here are some examples from our data set. For example, the one without words, not the one with the 12 year old boy, or the fictional one, or comes from Azerbaijan, and so on. The altentities corpus has 6,000 alternative questions across three domains, and it has 42,000 indirect referring expressions. Results with T5 large model are summarized below. If the language model has access to the exact same background knowledge as the annotators, the in the accuracy is really high. It's around 92 to 95 percent. But this is not realistic. If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87 percent, which is more realistic. For example, when the language model retrieves the background knowledge. If the language model has access only to entity names, then the accuracy is only 60 percent, so there's a lot of room for improvement. We've also shown that the models are domain generalizable. Here is a link to our data set. Thanks.</sample>
    <sample id="294">CamemBERT is initially trained on the English biomedical dataset known as BioBERT.</sample>
    <sample id="295">The speaker's name is Adam Szpekowski.</sample>
    <sample id="296">Valerio Basile presents a project on irony detection, highlighting the challenges of supervised machine learning in natural language processing. The project, a collaboration with Amazon Alexa and the University of Turin, involves the EPIC corpus, which includes 300 short conversations annotated for irony. The study uses the Prolific platform for crowdsourcing annotations, with 15 annotators per language variety. The research reveals differences in inter-annotator agreement, particularly between annotators from the UK and Ireland. Perspective-aware models, which consider these differences, show more confidence in predictions compared to aggregated models. The study also finds that generational and geographical differences impact perceptions of irony.</sample>
    <sample id="297">The project 'From Dog Whistles to Bullhorns: Unveiling Coded Rhetoric with Language Models' explores the use of dog whistles in language, which are coded messages that convey hidden meanings to specific groups while appearing innocuous to others. The research includes developing a glossary of over 340 terms, analyzing historical U.S. political speeches, and evaluating the performance of language models like GPT-3 in recognizing dog whistles. The study reveals that dog whistles are often used in conservative rhetoric and can evade content moderation by appearing less toxic. The findings highlight the importance of understanding dog whistles for NLP and political influence, as well as the challenges in detecting them due to their context-dependent nature.</sample>
    <sample id="298">The performance degradation observed when retraining models with more recent data confirmed that the main cause of performance loss is temporal drift.</sample>
    <sample id="299">The discussion focuses on improving the robustness of NLP models by addressing the issue of model over-reliance on shortcuts learned during training. These shortcuts, which are correlations between input attributes and labels, lead to strong performance on in-distribution samples but poor generalization on out-of-distribution data. Current mitigation methods, which often require auxiliary models, face limitations such as needing specific domain knowledge and not aligning with the learner's behavior. The proposed solution is a training method that uses a minimax objective to adjust the learner's focus on underrepresented hard examples, thus improving out-of-distribution performance. This method does not assume the type of shortcuts in the dataset and relies on the learner's own training dynamics to generate example weights. The method is evaluated on three datasets, showing consistent improvement in out-of-distribution performance compared to existing methods.</sample>
    <sample id="300">Belinda presents a project on Interactive Diction, a system allowing users to dictate and edit documents using voice commands. The system, developed with Semantic Machines and collaborators, aims to improve upon existing speech-to-text technologies by enabling intuitive editing without fixed commands. The project involves a four-step process: ASR transcription, segmentation into dictations and commands, normalization of commands, and execution of commands. A data collection interface was designed to gather data, and a baseline system was built using models trained for each step. The system uses T5 and GPT-3 architectures, with GPT-3 models showing higher accuracy but slower performance. The project highlights the need for further development and has released code and a paper detailing the work.</sample>
    <sample id="301">Hi everyone, I'm Jenny, a first year PhD student at Carnegie Mellon University, and today I'll be presenting our work, AnL Positionality: Characterizing Design Biases of Datasets and Models. This work was done in collaboration with some folks at the University of Washington and the Allen Institute for AI, namely Sebastian Santi, Ronan Labras, Katarina Rainerica, and Martin Sapp. So let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content. You might turn towards a popular API like Perspective API for toxicity detection. And this works really well if you're Carl Jones. Where perspective API is able to detect toxic instances, but that's not really the case for Aditya Sharma, where perspective API is really not as sensitive to offensive terms that are more common in Indian contexts. This is an example of a design bias where we see systematic performance differences of technology between populations. Design biases like the one that we just saw before might occur due to the positionality of the NLP researchers and model developers. Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences. This is a concept widely used in critical studies, specifically in feminist and queer academic spaces. And as a researcher, positionality can influence the research process and its outcomes because it can change the decisions that researchers make. And so one question that people might ask is do datasets and models have positionality? And we're not trying to say that models and datasets themselves have demographic identities and life experiences, but they do aggregate judgments and opinions of real people and can thus represent certain positionalities over others. So prior work has suggested some anecdotal evidence of having positionality, such as cultural gaps in models and datasets, as well as theoretical definitions of model positionality. However, these works really don't look at comparing end users with the datasets and models themselves. And studying model and dataset positionality is increasingly important as NLP tasks become more subjective and socially oriented. And it's challenging to characterize how these positionalities are skewed because not all decisions are documented and many models are hidden behind APIs. So to study dataset and model positionality, we actually compare the annotations with real users with existing datasets and models. Our framework works in two main steps. The first step is to re-annotate datasets with diverse annotators. And we opt to do this over looking at the demographics of original datasets annotators because usually only a few annotators annotate each instance, and because demographics are rarely collected and shared. And so we opt to re-annotate data to get many annotators for instance and to get a rich set of demographic data. We then take the annotations by demographic and compare them to the models and datasets using a Pearson's R correlation score. And thus our framework actually differs from annotator disagreement literature by comparing end users with models and datasets, predictions, and labels, as opposed to looking at just annotator agreement or modeling annotator distributions. Our framework is largely enabled through Lab in the Wild, an online crowdsourcing platform, former HCI collaborator. And Lab in the Wild is an online experimentation platform where we can recruit diverse volunteers compared to platforms like MTurk, which largely have participants from the US or India. And further Lab in the Wild still is able to get high quality data. We host two tasks on Lab in the Wild, one of them being social acceptability. And the way this works is that participants will read a situation from the social chemistry dataset, and then they'll write how socially acceptable a situation is. Afterwards, to stay engaged in the study, they can compare their responses to an AI and others. We then compared these annotations with social chemistry, delphi, and GPT-4. We then replicate a very similar setup for the toxicity and hate speech detection task, where they'll read an instance from dynahate and write whether they think it's an instance of hate speech. We then compared these annotations with dynahate, perspective API, rewire a GPT-4 in GPT-4. Our study in the end amassed over 16,000 annotations from over 1,000 annotators from 87 countries. So now we're better equipped to answer who do NLP datasets and models align with the most? We find that there is positionality in NLP. For example, we find that datasets and models are most aligned to English speaking countries. So for the GPT-4 social acceptability analysis, we find that it's most aligned to Confucian and English speaking countries. We find that dynahate is also most aligned to English speaking countries. We also find most additional align with people who have a college education. So for GPT-4 in the social acceptability task, we find that it's most aligned to people with a college education or graduate school education. And we find the same for dynahate, where it's most aligned to people with a college education. However, when models and datasets are aligned to specific populations, some are inevitably left behind. An example of this is that datasets and models are less aligned to non-binary people compared to the men and women counterparts. We find this in the GPT-4 social acceptability task as well as the dynahate task analysis as well. So given that there is positionality in NLP, what can we do about it? So we have a few recommendations for this. First one is keep a record of all relevant design choices throughout the research process. And the other is to do NLP research with a lens of perspectivism. Our third recommendation is to build specialized datasets and models within four specific communities. And a good example of this is the Muscani initiative. I mean, we want to emphasize that inclusive NLP isn't just making all technologies work for everyone. And so that concludes our presentation, but if you'd like to learn more, feel free to check out our dashboard for the most updated analysis results and our paper. Thank you.</sample>
    <sample id="302">Permuting tokens is necessary to determine the correct order of the output sequence, as the initial tagging step only provides the right tokens without their order. The permutation model predicts the sequence order to form coherent outputs.</sample>
    <sample id="303">The authors recommend increased transparency to understand why positive stereotypes occur and to study the effectiveness of anti-stereotyping methods, as current assumptions are insufficient.</sample>
    <sample id="304">Minimal-pair unacceptable inputs are sentences that are grammatically incorrect or violate certain linguistic stereotypes, used to test language models' ability to distinguish between acceptable and unacceptable sentences.</sample>
    <sample id="305">Dawie, a PhD student at Saarland University, presents a critical examination of weakly supervised learning (WSL) in a video. WSL uses weak labels from sources like heuristic rules, knowledge bases, or crowdsourcing, which are cheaper but noisy. Recent WSL methods require clean validation data, which is not always available, leading to performance issues. The study questions the necessity of clean data, the number of clean samples needed, and the best use of them. Findings show that WSL methods need clean data to perform well, and performance can be improved by fine-tuning on clean samples. The study recommends reporting model selection criteria, comparing WSL with full supervision, and considering continuous fine-tuning. The research is open-sourced for further exploration.</sample>
    <sample id="306">Sebastian Schuster and Na Jeong Kim discuss their research on entity tracking in language models, focusing on whether these models can track entities' states in discourse. They designed a task involving boxes and objects to test this ability, ensuring models couldn't use heuristics or memorization. Their experiments with models like GPT-3 and GPT-3.5 showed that pre-training on code, especially in GPT-3.5, enables non-trivial entity tracking. However, smaller models like T5 base can learn entity tracking with fine-tuning, but random initialization does not. The paper provides further details and results, including GPT-4 experiments.</sample>
    <sample id="307">The authors used metrics such as name entity recognition, classification, part-of-speech tagging, and question answering to evaluate the models.</sample>
    <sample id="308">Jenny McCoy, a first-year PhD student, presented her research on 'Anal Positionality,' which examines design biases in datasets and models. Collaborating with the University of Washington and the Allen Institute for AI, she highlighted how these biases can lead to systematic performance differences across populations. Jenny explained that positionality, influenced by demographics and life experiences, can affect research outcomes. Her framework, NL Positionality, involves re-annotating datasets with diverse annotators and comparing their annotations to model predictions. This approach, facilitated by Lab in the Wild, revealed biases in datasets and models, such as alignment with English-speaking countries and those with higher education. Jenny emphasized the need for inclusive NLP practices, suggesting keeping records of design choices, conducting research with a lens of perspectivism, and building specialized datasets and models for specific communities. She concluded by advocating for inclusive NLP, not just making technologies work for everyone.</sample>
    <sample id="309">Inter-annotator agreement was measured by comparing the consistency of ABC Eval labels with those collected by existing methods on 100 doubly labeled conversations.</sample>
    <sample id="310">Wikipedia was chosen to add completely unrelated sentences to the unacceptable and acceptable queries.</sample>
    <sample id="311">The authors of the paper are affiliated with the University of Leipzig and the University of Leipzig, Germany.</sample>
    <sample id="312">MultiInstruct is the first large-scale multimodal instruction tuning benchmark, comprising 62 tasks across 10 categories, derived from 21 open-source datasets, unlike previous benchmarks that focus mainly on language-only tasks.</sample>
    <sample id="313">There are two authors involved in the paper.</sample>
    <sample id="314">Binary coordination involves two conjuncts, where one conjunct is the head of the coordination structure, and the other is the dependent.</sample>
    <sample id="315">The average length of the prompts used in the study was 4.3 tokens.</sample>
    <sample id="316">The findings suggest that smaller T5 models, when trained on the COSET dataset, can generate high-quality scripts for constrained language planning, indicating that smaller models can effectively support larger models when properly trained on suitable datasets.</sample>
    <sample id="317">Peng Li from Fudan University presents CodeIE, a novel approach to information extraction by transforming it into a code generation task using large language models like Code-Davinci-003. Traditional models like GPT-3 and T5 face challenges in maintaining output structure alignment, often requiring extensive training data and decoding strategies. CodeIE addresses this by using code format prompts, which align input and output structures, resulting in more accurate and structured outputs. The method was evaluated on three recognition datasets and four relation extraction datasets, showing that code format prompts significantly outperform traditional models. The analysis revealed that code format prompts reduce structural errors and improve recall, especially with GPT-3. The research provides insights into the potential of code generation for information extraction tasks.</sample>
    <sample id="319">The work investigates three learning strategies: training from scratch, using pre-trained weights and tokenizers from Camembert, and using a mix of clinical and web-sourced data.</sample>
    <sample id="320">The factor of overfitting due to test reuse is indicated by a gradient greater than one, meaning improvements on the 2003 test set translate to more than one unit of improvement on the 2003+ test set, showing no diminishing returns.</sample>
    <sample id="321">The quality of simplification was evaluated using checkpoints and metrics detailed in the paper, with the fine-tuned models achieving scores better than baseline scores.</sample>
    <sample id="322">Enrico presents on how text classifiers learn about morality at ACL23. He explains that morality is subjective and varies across individuals, and introduces the Moral Foundation Theory, which posits five moral dimensions. Enrico's research uses the MORRA Foundation Twitter Corpus to explore how language models perceive morality differently across domains like #AllLivesMatter and #BlackLivesMatter. His findings show that models can detect these differences, such as the contrasting views on subversion in ALM and BLM. He warns against using a single model for multiple domains due to potential misunderstandings. Enrico aims to further understand these nuances using explainable AI techniques.</sample>
    <sample id="323">Yu Jiawang from Shanxi University presents a paper on Dynamic Hierarchical Graph Learning for Commonsense QA. The task involves answering questions that require commonsense knowledge, which necessitates retrieving relevant knowledge from external sources. Existing methods often introduce irrelevant entities during subgraph retrieval and face limited interaction between subgraph and language model. To address these issues, Jiawang proposes D-HL-K, which builds a hierarchical graph (HKG) using two state-prediction strategies and knowledge representation learning. The method employs a language model to encode and fuse the HKG, using techniques like RoBERTa and Masked Self-Attention to enhance the model's understanding. Experiments on the Common Sense QA dataset show that the proposed method outperforms other LM and HKG methods, achieving better results.</sample>
    <sample id="324">Yes, language models exhibit varying political biases, as demonstrated by their performance on tasks like hate speech and fake news detection, which differ based on their political leanings.</sample>
    <sample id="326">Cognitive dissonance is the inconsistency between two beliefs or actions, such as knowing smoking is harmful but still choosing to smoke.</sample>
    <sample id="327">Xiaoxu, a third-year PhD student, presents their work on the MAGETOWER model at ACL 2023. The model aims to improve vision language learning by integrating multimodal representations more effectively than previous models like BridgeTower. MAGETOWER introduces managers in each cross-modal layer to aggregate insights from pre-trained unimodal experts, allowing for more comprehensive cross-modal representation learning. The model outperforms others, including those trained on larger datasets, by effectively utilizing different levels of unimodal semantic knowledge. The research highlights the importance of adaptive managers in achieving superior performance, as evidenced by significant improvements in accuracy on the VQVC2 validation set. The paper and models are available on the ArXiv and GitHub repositories.</sample>
    <sample id="328">GPT-4 is identified as the most liberal language model among those studied.</sample>
    <sample id="329">The presentation by Zheng Minghan from Peking University introduces a novel method for zero-shot video sense localization, aiming to identify video segments relevant to a given natural language query without manual annotation. The method addresses the limitations of existing pseudo-query generation by using image caption models to create more complex pseudo-queries and employing a structured approach to generate pseudo-labelling, reducing noise and improving model performance. The method involves generating pseudo-queries from video frames, calculating event quality based on similarity, and selecting high-quality pseudo-events. The model is trained using these pseudo-labelling techniques, with strategies to mitigate label noise. The method outperforms existing methods on two datasets, demonstrating its effectiveness in zero-shot video sense localization.</sample>
    <sample id="330">Yes, cumulative training performed better than iterative in this study.</sample>
    <sample id="331">The speaker's name is Sara Papi.</sample>
    <sample id="332">The data for the MuDa benchmark was taken from transcripts of TED Talks that have been translated from English to 14 different languages.</sample>
    <sample id="333">Wen Hao from Nanjing University presented a method to enhance neural machine translation (NMT) by injecting key knowledge into the model. The work addresses the issue of non-smooth representation spaces in NMT, which limits its generalization ability. The proposed framework, INK, involves a training loop that extracts key knowledge to guide the adapter, which adjusts representations and updates the datastore asynchronously. Experiments show that INK outperforms state-of-the-art KMT systems, achieving higher BLEU scores with less memory space. The framework is effective even with small adapters and demonstrates that combining the adapter and datastore further improves performance. The research concludes with a proposal for a normal training framework that iteratively refines the representation space of the NMT model, achieving better translation performance with less memory and faster inference.</sample>
    <sample id="334">Hi, my name is Adam Szarkowski and this talk is about the dependency structure of coordination. As you may know, different dependency structures are assumed by different theories and corpus approaches. So for example, in universal dependencies, the structure of the coordinate Lisa, Bart and Maggie is such that the first conjunct is the head of the whole coordinate structure, so in this case Lisa. A similar approach is assumed in the meaning text theory, where again the whole coordinate structure is headed by the first conjunct. So these two approaches are symmetric. Right, they single out one of the conjuncts. Now there also symmetric approaches to coordinate structures such as the prague approach, the conjunction headed approach assumed in prague dependency treebanks, where coordinate structures are headed by the conjunction. So we get dependencies from and to all the conjuncts. And finally, there's also a multi headed approach that's used for example in de katzons word grammar, where all conjuncts are heads of the coordinate structure. So we get dependencies from the governor here loves to all conjuncts separately. These about and make. Now the aim of this paper is to produce a novel argument for the symmetric structures of coordination like these two and against the asymmetric structures of coordination like these two. Okay, the argument is based on the principle of dependency length minimization that I will explain on the basis of these examples. So in English as you might know, direct objects prefer to be close to the verb, while adjuncts may be further away, right? So March read it yesterday is fine because the direct object it is close to the verb, while March read yesterday it is much worse, right? Because here between the verb and the direct object there is an adjunct yesterday. However, this effect may be ameliorated when the direct object is very heavy and very long, because then it can be moved to the position after the adjunct. This is illustrated here. So both these sentences are fine. March read this absolutely fascinating book about the bees yesterday is okay, where instead of it we have this long and p. But it's also okay to say March read yesterday this absolutely fascinating book about bees. So the reason in here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb, it satisfies the principle of dependency length minimization, which says that shorter dependencies are preferred. So these two trees only show the length of the crucial dependencies, so the ones that are not constant among these two structures. So here we have a dependency from red to the adjunct of length seven measured in words, and from red to book of length four, so to get it eleven. When you move, when you swap these two constituents, the sum of these two dependencies becomes six, right? So instead of eleven six, much shorter, that's why this sounds quite okay, right? It violates one principle but it satisfies another one. Okay, so what we did, we extracted various statistics about coordination from the enhanced version of the pen treebank and see the paper why we don't use universal dependencies. And these statistics confirm the observation made many times before that left conjuncts tend to be shorter, so salt and pepper and not pepper and salt measured in syllables. And also the observation that was made in passing that this tendency grows with length difference. So when the difference between the lengths of the two conjuncts grows, the shorter conjunct prefers to be the first one stronger, right? So the proportion is bigger of the left short conjunct. But what's novel in this paper is that we observed that this tendency only occurs when the governor is on the left, right? So the governor is on the left in this example, I saw Bart and Lisa, so it's the governor, is on the left. It's absent in the second example, Homer came and sneezed, here we have coordination of two verbs and there's no outside external governor, right? So in such cases, the left conjunct prefers to be shorter, the more so the bigger the difference between the two conjuncts. But when the governor is on the right, as here, left governs the coordinate then and that, this effect disappears. So we show that by measuring length in characters, that's the first column, in syllables, the middle column, and in words, the right column, so I'll concentrate on the right one. What we see here is that when the governor is on the left, the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words, and the same is observed when there is no governor, as in coordination of sentences, but when the governor is on the right, this tendency disappears. And we show in the paper how this provides an argument against asymmetric structures of coordination as these two and for the symmetric structures as these two. So see the paper for the full argument and arguments and talk to us about the poster session. Thank you.</sample>
    <sample id="335">The speaker's name is Matthias Landmann.</sample>
    <sample id="336">Cross-lingual transfer involves training a model on one language and transferring its knowledge to another language, as demonstrated in the study with English queries and German or Chinese queries.</sample>
    <sample id="337">The presentation introduces a novel approach to handling out-of-vocabulary (OOV) words in embedding-based downstream models. The method involves using a word relationship graph to infer OOV word meanings by associating them with relevant words based on word formation and lexical rules. The model uses a graph neural network to process the graph, with a self-attention network to assign node attributes. It employs a two-level graph attention network to reduce noise and a graph-level representation to capture the entire graph information. The model also uses contrastive learning to align the graph-level embedding with the background embedding. Extensive experiments show that the model outperforms existing methods in both intrinsic and extrinsic tasks. The model is adaptable to different languages, with better performance in agglutinative languages and challenges in fusional languages.</sample>
    <sample id="338">Bingxin presents a research study on evaluating the utility of human explanations in natural language processing. The study, conducted by a collaborative team, introduces a unified data structure to standardize multiple datasets into a common format. This structure includes baseline and infusion settings to test model performance with and without explanations. The research critiques traditional evaluation metrics like BLEU and ROUGE, which focus on word similarity, and proposes a new metric called TRUE, which also considers the usefulness of explanations during model fine-tuning. The study evaluates human explanations across five datasets, including COSE, ECQA, ESNLI, and COMVE, using both TRUE and the simulability score. The findings show that TRUE outperforms the simulability score, particularly in datasets like COMVE and ESNLI, where explanation utility varies based on task and format. The research highlights the importance of context and negation in explanations, suggesting that the quality of explanations is task-dependent. The study concludes by advocating for high-quality human-AI collaboration in annotation jobs and encourages further research in this area.</sample>
    <sample id="339">The authors of the paper are affiliated with Saarland University, Saarland University, and Saarland University.</sample>
    <sample id="340">The presentation by Guan Hao Huang from UCLA introduces Para-AMR, a large-scale, syntactically diverse paraphrase dataset created using AMR back-translation. This dataset addresses the need for high-quality, diverse paraphrases in NLP applications. Huang explains that existing datasets are limited in scale and syntactic diversity, which Para-AMR overcomes by leveraging AMR graphs to generate paraphrases. The process involves using a pre-trained AMR parser to modify the focus of the graph, generating paraphrases that maintain semantic similarity but vary syntactically. The dataset contains 15 million source sentences with 6.9 paraphrases each, and it has been shown to improve performance in sentence embeddings and syntactic control in NLP applications. Para-AMR is available for further research and application.</sample>
    <sample id="341">The authors use average latency and computational aware average latency to measure performance.</sample>
    <sample id="342">Gao Jinsun presented a paper on 'Live Chat,' a large-scale personalized dialogue dataset derived from live streaming. The dataset, created by Gao and colleagues, addresses the need for video-based dialogue data, which is currently limited by manual annotations. The dataset is divided into two parts: basic and technical personas, extracted through manual labeling and rule-based methods. Experiments on response modeling and address recognition showed that the dataset's unique features, such as detailed persona annotations and longer average sessions, enhance performance. The dataset's distinctiveness lies in its video source and personalization, which sets it apart from existing dialogue datasets. Future work will focus on improving the transferability of dialogue models to live chat.</sample>
    <sample id="344">Tree-based methods are computationally expensive and require formalism-specific preprocessing and specialized grammar induction procedures.</sample>
    <sample id="345">The paper by Matthias Landmann, Alexander Colla, and Ivan Tiedoff introduces a novel approach to compositional generalization without trees, using multi-set tagging and latent permutations. Traditional methods often rely on trees to capture compositional processes, but these can be computationally expensive and require specific preprocessing. The authors propose a neural sequence-to-sequence model that predicts output from input without trees, using multi-set tagging to capture all relevant tokens and a permutation model to order them. This method addresses challenges such as unknown input-output alignment and multiple consistent permutations. The model outperforms existing tree-less models on the COGS benchmark, demonstrating strong generalization to deeper recursion. The paper also discusses technical challenges, including the NP-hard nature of finding the highest scoring permutation, and introduces a GPU-friendly continuous relaxation to approximate solutions.</sample>
    <sample id="346">The authors of the paper are affiliated with the University of Science and Technology of China and the University of Science and Technology of China, Shenzhen.</sample>
    <sample id="348">Myra, along with Eszter Mosh and Dan Jurafsky, explores the prevalence of social biases in large language models (LLMs) and the limitations of current stereotype detection methods. They propose a novel approach using instruction-tuned LMs to generate personas based on identity markers, which are then analyzed for stereotypes using a method called 'marked words.' This method identifies words that distinguish marked groups from unmarked ones, revealing essentializing narratives and stereotypes. The study finds that while generated personas contain more stereotypes than human-written ones, they often reflect positive stereotypes that are harmful. The research highlights the need for researchers to address positive stereotypes, use intersectional lenses, and increase transparency in bias mitigation methods.</sample>
    <sample id="350">The presentation by Simone Tedesci discusses the concept of superhuman performance in NLP, focusing on the SuperGlue and SQuAD benchmarks. It highlights that while models often outperform humans in these benchmarks, the comparison is flawed due to several issues. These include evaluating humans on small subsets of the test set, errors in the test data, and the use of simple methods to estimate human performance. The paper also points out that pay rates for human annotators are often low, affecting the quality of the data. Furthermore, details about the annotator pool are often omitted, making the claims of superhuman performance scientifically meaningless. The presentation concludes by recommending more reliable benchmark construction to avoid repeating these mistakes.</sample>
    <sample id="351">The paper 'Do Conner 2003 Named Entity Taggers Still Work Well in 2023?' investigates the generalization capabilities of Conner 2003 NER taggers in modern data. The study found that model architecture, size, and fine-tuning examples are crucial for good generalization. Experiments showed that Transformer models and larger models generally perform better, while more fine-tuning examples improve performance. Two hypotheses were tested: adaptive overfitting and temporal drift. Adaptive overfitting was not observed, while temporal drift was confirmed as a cause of performance drop. The paper concludes that Conner 2003 taggers can still be effective in 2023, but improvements in model architecture, size, and fine-tuning are necessary.</sample>
    <sample id="352">ABC-Eval stands for Annotating Behaviors in Chat, a method for evaluating conversational AI by identifying specific model behaviors.</sample>
    <sample id="353">The paper 'Python Code Generation by Asking Clarification Questions' by Houshang Li, Mohammad Masoumi, and Irina Golovchic explores the challenge of generating code from natural language descriptions, particularly when specifications are missing. The authors propose an interactive approach where clarification questions are used to gather necessary specifications. They introduce a method to create a synthetic dataset, Code QA, with questions about key operations. The study uses heuristics to identify key operations and employs a pipeline for code generation, which includes a clarification predictor, question selector, and code generator. The results show that while the method is challenging, it improves code generation accuracy. The paper concludes with an analysis of the impact of clarified key operations on code quality and invites feedback on their work.</sample>
    <sample id="354">The performance delta between CoNLL-2003 and CoNLL++ is higher than 5 percentage points until 2016.</sample>
    <sample id="356">The authors of the paper, Matthias Landmann, Alexander Coller, and Ivan Tiedoff, are affiliated with the University of Leipzig.</sample>
    <sample id="357">The speaker's name is Si-Yuan.</sample>
    <sample id="358">The paper involves four authors: Kai-yan Hsiao, Patrick Fernhout, Emile Liu, and Andre F. D. Martins.</sample>
    <sample id="359">The approach is compared to the Wav2Vec2 and Local Agreement architectures, as well as state-of-the-art architectures specifically tailored for simulST.</sample>
    <sample id="360">Hello everyone, my name is Yin and my colleague Ji Yang will be presenting our research on Multi-Teach, improving multi-model zero-shot learning via instruction tuning. With advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data-efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform unseen tasks in a zero-shot manner by following natural instructions. However, most previous works on instruction tuning focus on improving the zero-shot performance on language-only tasks, while computer vision and multi-modal tasks have been left out. Therefore, in this work, we want to investigate whether instruction tuning on multi-model pre-trained models can actually improve generalization to unseen multi-modal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in the availability of instruction dataset between NLP and multi-modal. There exist more than 1,600 language-only instruction tasks. However, there is no large-scale publicly available multi-modal instruction task. Therefore, this motivates us to build a multi-modal instruction tuning dataset. Here, we present Multi-Teach, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open source datasets, and each task is equipped with five expert-written instructions. For investigating multi-modal instruction tuning on our proposed dataset, we take OFA, a unified multi-modal pre-trained model, as our base model. OFA uses a unified vocabulary for language, image tokens, and the coordinate of a bounding box. Here, we show some example instances from our Multi-Teach dataset. To unify the processing of various input and output data type, we follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format, in which the input text, images, instruction, and bounding boxes are represented in the same token space. OK, now I am going to talk about multi-modal instruction tuning. So, for the training dataset, we use 53 tasks from the natural language group for training, and we sample 10,000 instances per task. For testing, we reserve the entire commonsense reasoning group for testing, and we select additional 5 tasks from the VQA and the miscegenous group. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instruction as unseen tasks for NLP. So, we use pre-trained OFA large model as a base model. During training, we mix all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates. So, during test, for each task, we conduct a total of 5 experiments by evaluating the model using one of the five instructions in each experiment. We report the mean and max performance and the standard deviation of the performance across all 5 experiments. If the task is a multi-modal classification task, we report accuracy. If it's a multi-modal generation task, we report rouge-l. For NLP task, we report rouge-l as well. We also introduced additional evaluation metric called sensitivity. So, this measures the model's ability to consistently produce the same outputs for the same task, regardless of slight variation in the wording of the instruction. Here is our main result. As we can see, instruction tuning can significantly improve OFA's performance on unseen multi-modal tasks. Also, transfer learning from natural instruction dataset can benefit instruction tuning. Here, we can see as the amount of task increase, the model achieve better performance and in the meantime, lower sensitivity. So, we do one experiment. We use one instruction versus five instruction. As we can see, using more instruction can improve the model's overall performance and reduce its sensitivity a lot. So, this shows the effect of different fine-tuning strategy on the model's sensitivity. As we can see, by transfer learning from natural instruction dataset, the model can achieve much better sensitivity compared to the original OFA model. We also can see transfer learning from natural instruction dataset can help OFA to achieve much better performance on the natural instruct dataset. So, overall, we propose a first large-scale multi-modal instruction tuning dataset. We significantly improve the zero-shot capability of OFA, and we explore different transfer learning technique and show their benefits. We design a new metric called sensitivity. So, one more thing, we are collecting a much larger multi-modal instruction tuning dataset with around 150 additional vision language tasks, and we will release them soon. This is a QR code for our data and model. Thank you.</sample>
    <sample id="361">Armin Nurbaevschi, a PhD student at Carnegie Mellon University, presents a project called CounterComp, which aims to enhance compositional generalization in multi-step quantitative reasoning tasks. These tasks involve complex arithmetic operations, and current neural models struggle with tasks involving more than two steps due to their tendency to memorize patterns. CounterComp addresses this by using counterfactual scenarios to improve model performance. The approach involves mining positive and negative examples from training data to create auxiliary metric learning loss, which adjusts the model's learning process. This method has shown to improve both in-distribution and out-of-distribution performance, meaning the model can generalize better to new data. The project also emphasizes the importance of models attending to meaningful tokens in the input, which relate to significant operations in the output. The research is supported by co-authors and advisors from Carnegie Mellon University and JP Morgan.</sample>
  </task>
</testset>