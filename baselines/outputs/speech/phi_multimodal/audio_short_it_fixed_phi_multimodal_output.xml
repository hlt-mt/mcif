<testset name="IWSLT2025" type="output">
  <task track="short" text_lang="it">
    <sample id="0">Le principali fonti di dati per i modelli linguistici sono i media di notizie, in particolare quelli ben coperti come il New York Times, Los Angeles Times, The Guardian e Huffington Post.</sample>
    <sample id="1">McGill University, Mila e Microsoft Research.</sample>
    <sample id="2">Ciao. Benvenuti alla nostra presentazione di deepane, un nuovo corpus per la identificazione del testo in tedesco a livello di documento e a livello di frase.</sample>
    <sample id="3">Mi chiamo Regina Stodden e vi guiderò attraverso la prima parte della presentazione. Definiamo prima la semplificazione del testo.</sample>
    <sample id="4">La semplificazione del testo è un processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo di destinazione specifico, poiché le persone con problemi di lettura o non madrelingua parlano.</sample>
    <sample id="5">Per addestrare un modello di semplificazione del testo, abbiamo bisogno di coppie parallele di testo, ad esempio di documenti o frasi.</sample>
    <sample id="6">Nell'esempio qui, puoi vedere una coppia di frasi parallele di una frase tedesca complessa e la sua traduzione in linguaggio semplice.</sample>
    <sample id="7">Per semplificare la frase, sono possibili diverse tecniche, come si può vedere nell'esempio, come la sostituzione lessicale, la cancellazione della clausola, la cancellazione della clausola, la riordinazione o l'inserimento di parole.</sample>
    <sample id="8">Ora proponiamo il nostro nuovo corpus, perché negli ultimi anni, c'erano alcuni problemi con i corpora esistenti. Quindi, ad esempio, questi corpora qui sono troppo piccoli per addestrare un modello di semplificazione del testo.</sample>
    <sample id="9">Gli altri tre modelli che sono proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere errori in loro allineamenti.</sample>
    <sample id="10">Pertanto, proponiamo il nostro nuovo corpus, Dplain, che è diviso in due sottocorpora, Dplain apa e Dplain web. Dplain apa è basato su testi di notizie.</sample>
    <sample id="11">In Deep LANE apa, abbiamo allineato quattro ottantatre documenti, tutti manualmente. Questo si traduce in circa tredicimila coppie di frasi parallele.</sample>
    <sample id="12">Per deepplainweb, questo corpus include diversi domini. E allineiamo anche tutti questi settecentocinquanta documenti, da una parte manualmente, e dall'altra con metodi di allineamento automatico.</sample>
    <sample id="13">In totale, abbiamo ottenuto trentamilaquattrocentocinquanta coppie di frasi.</sample>
    <sample id="14">Abbiamo analizzato un po 'di più i nostri coppie di frasi. Quindi, ad esempio, sul tipo di semantica.</sample>
    <sample id="15">Come puoi vedere qui, i testi biblici sono molto più forti, semplificati rispetto, ad esempio, ai testi di notizie o ai testi di apprendimento della lingua.</sample>
    <sample id="16">Su tutti i livelli, per esempio, semplificazione lessicale, semplificazione strutturale, o il livello generale di semplificazione.</sample>
    <sample id="17">Inoltre, puoi vedere che il nostro corpus di dipiano ha una grande varietà di diverse trasformazioni di semplificazione. Quindi, ad esempio, nel corpus di api di dipiano, abbiamo molte più riorganizzazioni e aggiunte di parole rispetto al corpus di web di dipiano.</sample>
    <sample id="18">D'altra parte, nel corpus web, abbiamo molte più riformulazioni.</sample>
    <sample id="19">Quindi vediamo ora cosa possiamo fare con questo corpus. Ciao, sono Omar. E ora parlerò dei casi d'uso per il nostro set di dati, deep plane. Quindi per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico.</sample>
    <sample id="20">Negli ultimi anni, ci sono stati molti metodi di allineamento, ma nel contesto delle traduzioni automatiche,</sample>
    <sample id="21">Dove abbiamo due documenti paralleli scritti in lingue diverse. E vogliamo estrarre allineamenti di frasi nei documenti post.</sample>
    <sample id="22">Ma nel nostro caso d'uso, stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli, che hanno la stessa lingua, che hanno lo stesso contenuto, ma sono a un livello di complessità diverso.</sample>
    <sample id="23">E ora, poiché abbiamo il nostro set di dati, D plane, che ha frasi allineate manualmente, possiamo utilizzare queste frasi come allineamenti standard per valutare alcuni dei metodi di allineamento proposti.</sample>
    <sample id="24">E abbiamo fatto alcune adattazioni ai metodi proposti. E abbiamo pubblicato tutte queste adattazioni e i codici per eseguire i nostri esperimenti nel documento.</sample>
    <sample id="25">Alla fine, abbiamo concluso che il miglior allineamento, il metodo di allineamento automatico da utilizzare per il testo, per la semplificazione del testo tedesco, è il metodo di allineamento di massa.</sample>
    <sample id="26">E puoi anche trovare il codice per eseguire questo metodo sui tuoi documenti in carta.</sample>
    <sample id="27">Il secondo caso di utilizzo che abbiamo mostrato nel nostro articolo è il caso di semplificazione automatica del testo.</sample>
    <sample id="28">Ottimizzando i modelli linguistici per produrre testo semplificato dal testo di input complesso.</sample>
    <sample id="29">Abbiamo affinato due modelli diversi. Abbiamo affinato il modello di lungimiranza per produrre semplificazioni a livello di documento.</sample>
    <sample id="30">E abbiamo anche affinato l'imparto di base normale per produrre semplificazioni a livello di frase.</sample>
    <sample id="31">Puoi anche trovare tutti i checkpoint e puoi esaminare in modo più dettagliato i punteggi e le metriche di valutazione dei nostri esperimenti nel documento.</sample>
    <sample id="32">Abbiamo concluso che questa, questa, la fine tuning di base potrebbe produrre o ottenere punteggi migliori dei punteggi di base.</sample>
    <sample id="33">E proponiamo questi risultati come un punto di riferimento, un punto di riferimento di base, per il problema della semplificazione automatica del testo in futuro.</sample>
    <sample id="34">Grazie mille per la vostra attenzione. E speriamo di incontrarvi tutti durante la conferenza. Grazie.</sample>
    <sample id="35">Il nome della relatrice è Kayo Yoon.</sample>
    <sample id="36">T5-X-Large</sample>
    <sample id="37">Sì, i tagger CoNLL-2003 funzionano ancora.</sample>
    <sample id="38">Il metodo di valutazione umana proposto è nuovo perché cerca di ridurre la soggettività fornendo annotazioni esplicite sui comportamenti espressi dai modelli, come la fornitura di informazioni irrilevanti o la contraddizione.</sample>
    <sample id="39">Il successo dell'attuale approccio scarsamente supervisionato si basa in larga misura sulla necessità di campioni di validazione puliti per garantire una corretta generalizzazione dei modelli.</sample>
    <sample id="40">I progressi che possono essere fatti per migliorare il punteggio sono migliorare la precisione e la completezza delle entità, migliorare la comprensione del contesto e migliorare la capacità di identificare e classificare le entità.</sample>
    <sample id="41">Ci sono quattro autori coinvolti nell'articolo.</sample>
    <sample id="42">Ciao. Mi chiamo Adam Skorkowsky, e questo discorso riguarda la struttura di dipendenza della coordinazione.</sample>
    <sample id="43">Come forse sapete, ci sono diverse strutture di dipendenza presunte da diverse teorie e approcci di corpus. Quindi, ad esempio, nelle dipendenze universali, la struttura della coordinazione, Lisa, Bart e Maggie.</sample>
    <sample id="44">È tale che la prima congiunzione è la testa della struttura di coordinate intera. Quindi, in questo caso, Lisa.</sample>
    <sample id="45">Un approccio simile è presupposto nella teoria del testo di significato, dove, ancora una volta, l'intera struttura coordinata è guidata dal primo congiunto. Quindi questi due approcci sono simmetrici, giusto? Sceglieranno uno dei congiunti.</sample>
    <sample id="46">Ora ci sono anche approcci simmetrici alle strutture coordinate, come l'approccio prague, l'approccio congiuntivo, che si trova nei banche di albero delle dipendenze prague, dove le strutture coordinate sono guidate dalla congiunzione.</sample>
    <sample id="47">Quindi otteniamo alcune dipendenze da e a tutti i contratti.</sample>
    <sample id="48">E infine, c'è anche un approccio multiheaded che viene utilizzato, ad esempio, nella grammatica delle parole di Dicots.</sample>
    <sample id="49">Dove, per così dire, tutti i congiunti sono teste della struttura di coordinamento. Quindi otteniamo dipendenze dal governatore qui, lovs, a tutti i congiunti separatamente. Questi sono i bot e</sample>
    <sample id="50">Ora, l'obiettivo di questo documento è quello di produrre un nuovo argomento per le strutture simmetriche di coordinazione come queste due, e contro le strutture asimmetriche di coordinazione come queste.</sample>
    <sample id="51">Ok, l'argomento si basa sul principio di minimizzazione della lunghezza delle dipendenze che spiegherò sulla base di questi esempi.</sample>
    <sample id="52">Quindi, in inglese, come sai, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli aggettivi possono essere più lontani. Giusto? Quindi, marzo, letto ieri va bene, perché l'oggetto diretto, è vicino al verbo.</sample>
    <sample id="53">Mentre marzo ha letto ieri, è molto peggio, giusto? Perché qui, tra il verbo e l'oggetto diretto, c'è un aggettivo, ieri.</sample>
    <sample id="54">Tuttavia, questo effetto può essere migliorato quando l'oggetto diretto è molto pesante e molto lungo, perché allora può essere spostato nella posizione dopo l'aggancio.</sample>
    <sample id="55">Questo è illustrato qui. Quindi entrambe queste frasi vanno bene. Mark ha letto questo libro assolutamente affascinante sul B C I ieri. Va bene, dove invece di it, abbiamo questo lungo e piatto.</sample>
    <sample id="56">Ma va anche detto, Martorell ieri, questo libro assolutamente affascinante sulle api.</sample>
    <sample id="57">Quindi la ragione qui è che questo è possibile perché, anche se questa frase viola il principio grammaticale generale che gli oggetti diretti dovrebbero essere accanto al verbo,</sample>
    <sample id="58">Soddisfa il principio di minimizzazione della lunghezza delle dipendenze, che afferma che le dipendenze più brevi sono preferibili.</sample>
    <sample id="59">Quindi questi due alberi mostrano solo la lunghezza delle dipendenze cruciali. Quindi quelli che non sono costanti tra queste due strutture.</sample>
    <sample id="60">Quindi qui abbiamo una dipendenza da red all'appendice di lunghezza sette, misurata in parole, e da red a book di lunghezza quattro. Quindi per ottenerlo, è undici.</sample>
    <sample id="61">Quando si sposta, quando si scambiano questi due costituenti, la somma di queste due dipendenze diventa sei, giusto? Quindi, invece di undici, sei, molto più breve. Ecco perché questo suona abbastanza bene. Viola un principio, ma soddisfa un altro.</sample>
    <sample id="62">Ok, quindi quello che abbiamo fatto, abbiamo estratto varie statistiche sulla coordinazione dalla versione migliorata della banca della tavola di penna e vedere il documento, perché non useremmo dipendenze universitarie.</sample>
    <sample id="63">E queste statistiche confermano l'osservazione fatta molte volte prima, che i congiuntivi di sinistra tendono ad essere più brevi. Quindi sale e pepe e non pepe e sale misurati in sillabe.</sample>
    <sample id="64">E anche l'osservazione che è stata fatta in passato, che questa tendenza cresce con la differenza di lunghezza.</sample>
    <sample id="65">Quindi, quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto più corto preferisce essere il primo, più forte, giusto? Quindi la proporzione è più grande del congiunto più breve a sinistra.</sample>
    <sample id="66">Ma ciò che è nuovo in questo documento è che abbiamo osservato che questa tendenza si verifica solo quando la governance a sinistra è assente.</sample>
    <sample id="67">Giusto? Quindi il governatore è a sinistra. In questo esempio, ho visto Bart e Lisa. Quindi è il governatore. È a sinistra.</sample>
    <sample id="68">È assente. Nell'esempio secondario, Homer è venuto a starnutire. Qui abbiamo la coordinazione di due verbi, e non c'è un governante esterno, giusto? Quindi, in tali casi, il congiuntivo sinistro preferisce essere più breve, più grande, la differenza tra i due.</sample>
    <sample id="69">Tuttavia, quando il governo è a destra, come qui, a sinistra governa la coordinazione, la rete di tenente, questo effetto scompare.</sample>
    <sample id="70">Quindi lo dimostriamo misurando la lunghezza in caratteri, la prima colonna, in sillabe, la colonna centrale e in parole, la colonna destra. Quindi mi concentrerò sulla destra.</sample>
    <sample id="71">Quello che vediamo qui è che quando il governo è a sinistra.</sample>
    <sample id="72">La tendenza per il congiunto di sinistra per essere più breve cresce costantemente con la differenza assoluta delle parole. E lo stesso si osserva quando non c'è governante, come nella coordinazione delle frasi. Ma quando il governante è a destra, questa tendenza scompare.</sample>
    <sample id="73">E mostriamo nel documento come questo fornisce un argomento contro le strutture di coordinazione asimmetriche come queste due e per le strutture simmetriche come queste due.</sample>
    <sample id="74">Quindi, leggi il documento per l'accordo completo e argomenti, scusa, e parlaci della sessione di Boston. Grazie.</sample>
    <sample id="75">Due</sample>
    <sample id="76">I testi biblici.</sample>
    <sample id="77">Sal e pepe, non pepe e sale.</sample>
    <sample id="78">Sì, puoi usare i modelli per la tua ricerca.</sample>
    <sample id="79">Contenuti di notizie.</sample>
    <sample id="80">Un buon generalizzazione richiede una migliore architettura del modello, dimensioni del modello più grandi e più esempi di fine-tuning.</sample>
    <sample id="81">Misurando la lunghezza in caratteri per la prima colonna, in sillabe per la seconda colonna e in parole per la terza colonna.</sample>
    <sample id="82">Gli esperimenti sono stati progettati misurando la lunghezza delle frasi in parole, con il governatore posizionato a sinistra e a destra, per osservare le differenze di lunghezza del congiunto associato.</sample>
    <sample id="83">I classificatori base sono meno efficaci quando addestrati su dati non bilanciati, specialmente se il set di dati contiene categorie rari o assenti.</sample>
    <sample id="84">Due.</sample>
    <sample id="85">I nomi dei personaggi nella conversazione sono Bob e Alice.</sample>
    <sample id="86">I modelli di MT sensibili al contesto migliorano i fenomeni del discorso come la formaleità e la coesione lessicale.</sample>
    <sample id="87">Johnathan Baugh, Aaron Muller, Kanishka Mishra, Karen Fuentas, Roger Levy e Adina Wiesel.</sample>
    <sample id="122">Il framework utilizza Pearson's R correlation score per quantificare la posizionalità confrontando le annotazioni demografiche con i modelli e le previsioni dei set di dati.</sample>
    <sample id="155">Il risultato dello studio precedente è stato che, quando gli umani hanno ricevuto gli stessi prompt di persona, sono stati in grado di evidenziare stereotipi razziali.</sample>
    <sample id="156">Le fonti di dati utilizzate in questo studio sono il corpus di Penn Treebank e il corpus di Universal Dependencies.</sample>
    <sample id="157">Due.</sample>
    <sample id="158">Le attività strettamente correlate alla dissonanza cognitiva sono la classificazione di dissonanza e la classificazione di consonanza.</sample>
    <sample id="159">Due autori sono coinvolti nell'articolo.</sample>
    <sample id="160">Due.</sample>
    <sample id="161">Il framework differisce confrontando gli utenti finali con modelli, set di dati, previsioni e etichette, anziché concentrarsi solo sull'accordo degli annotatori o sui modelli delle distribuzioni degli annotatori.</sample>
    <sample id="162">La configurazione 3B</sample>
    <sample id="163">Google Translate e DeepL sono stati messi a confronto.</sample>
    <sample id="164">Ciao. Sono Xiangbing, Ph D student dell'Università di Washington. Oggi presento il nostro lavoro, dai dati di pre training ai modelli linguistici, alle attività downstream, monitorando le tracce dei pregiudizi politici che portano a modelli nlp ingiusti.</sample>
    <sample id="165">Modelli linguistici sono addestrati su grandi scale di dati di web crawl.</sample>
    <sample id="166">I media di notizie politiche sono ben coperti nei loro dati di pre training. Secondo un sondaggio del corpus C four, possiamo vedere che il New York Times, Los Angeles Times, The Guardian, Huffington Post, ecc., sono ben coperti nei dati di allenamento del modello linguistico.</sample>
    <sample id="167">Questo ha creato una benedizione mista per le applicazioni dei modelli linguistici.</sample>
    <sample id="168">Quindi, da un lato, sono stati in grado di imparare da prospettive diverse, che celebra la democrazia e la pluralità di idee. Dall'altro lato, queste diverse opinioni politiche sono intrinsecamente socialmente pregiudizievoli e potrebbero portare a potenziali problemi di equità nelle applicazioni di task downstream.</sample>
    <sample id="169">A questo scopo, proponiamo di indagare sulla pipeline di propagazione del pregiudizio politico, dai dati di pre-allenamento ai modelli linguistici ai compiti downstream, in particolare ponendo le seguenti domande.</sample>
    <sample id="170">In primo luogo, come valutiamo l'orientamento politico dei modelli linguistici? E quale ruolo potrebbe avere il pretraining sui bias politici?</sample>
    <sample id="171">In secondo luogo, come funzionano i modelli linguistici con diverse linee politiche in realtà su compiti downstream? E se ciò potrebbe comportare problemi di equità nell'uso delle applicazioni nlp.</sample>
    <sample id="172">In particolare, abbiamo prima proposto di stimare i modelli linguistici con diversi formati di prompt, utilizzando i questionari politici, come il test di compassione politica. Questo ci assicura di fare valutazioni automatiche ben radicate nella letteratura di scienza politica.</sample>
    <sample id="173">Quindi alcuni risultati preliminari dimostrano che i primi modelli linguistici hanno diverse inclinazioni politiche. Occupano tutti e quattro i quadranti sul campo politico.</sample>
    <sample id="174">Possiamo anche vedere che GPT Four è il modello di linguaggio più liberale di tutti. E le teorie di GPT sono generalmente più socialmente liberali delle teorie di Burt e delle sue varianti.</sample>
    <sample id="175">In secondo luogo, abbiamo lo scopo di indagare fino a che punto i pregiudizi politici dei modelli linguistici sono effettivamente presi dai dati di addestramento.</sample>
    <sample id="176">Quindi abbiamo condotto un esperimento controllato pre-allenando i checkpoint del modello linguistico su sei diversi corpus di partiti diversi, separati in notizie e social media ulteriormente divisi nella loro inclinazione politica.</sample>
    <sample id="177">Pre-trattenendo ulteriormente i modelli linguistici su tali corpora parziali, possiamo vedere che anche le coordinate ideologiche del modello linguistico si spostano corrispondentemente.</sample>
    <sample id="178">Ad esempio, per Roberta, ulteriori affinamenti, ulteriori addestramenti sul corpus di Reddit di inclinazione sinistra, possiamo vedere un sostanziale spostamento liberale in termini di.</sample>
    <sample id="179">In termini di pregiudizi politici.</sample>
    <sample id="180">E abbiamo anche cercato di indagare se i modelli linguistici possono cogliere la polarizzazione che è prevalente nella nostra società moderna.</sample>
    <sample id="181">Quindi dividiamo i corpora pre-allenati in pre quarantacinquesimo presidente degli Stati Uniti. E dopo il quarantacinquesimo presidente degli Stati Uniti, prealleniamo separatamente i modelli linguistici su due diversi corpora temporali.</sample>
    <sample id="182">Possiamo vedere che i modelli linguistici in genere avevano un orientamento politico che si allontana dal centro dopo venti diciassette. Quindi questo indica che i modelli linguistici possono anche prendere in considerazione la polarizzazione nella nostra società.</sample>
    <sample id="183">Quindi, per ultimo, ma non meno importante, valutiamo i modelli linguistici con diversi significati politici sulla rilevazione del discorso di odio e sulla rilevazione di notizie false. Due applicazioni nlp che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative.</sample>
    <sample id="184">Quindi vediamo che se indagiamo sulle prestazioni per categoria, vale a dire, se separiamo le prestazioni in.</sample>
    <sample id="185">Diverse demografie o linee politiche dei media di notizie. Possiamo vedere un modello che, ad esempio, per il rilevamento di discorsi d'odio, i modelli linguistici di sinistra sono migliori.</sample>
    <sample id="186">Alla rilevazione del suo discorso di odio, rivolto a gruppi socialmente minoritari.</sample>
    <sample id="187">Tuttavia, sono peggio a rilevare l'incitamento all'odio, rivolgendosi a gruppi più potenti nella nostra società.</sample>
    <sample id="188">E viceversa, i modelli di linguaggio scritti sono migliori nel rilevare il linguaggio dell'odio che rivolge ai bianchi e agli uomini, ma peggiori nel rilevare il linguaggio dell'odio che rivolge ai neri, agli LGBTQ e ad altre comunità minoritarie.</sample>
    <sample id="189">Treni simili accadono anche per la rilevazione di notizie false, dove vediamo che i modelli linguistici di slittamento negativo sono migliori nel rilevare la disinformazione dal loro contrario, il slittamento politico e viceversa.</sample>
    <sample id="190">In questo, mostriamo ulteriormente molti esempi qualitativi per vedere che i modelli linguistici con significati politici diversi.</sample>
    <sample id="191">Dare previsioni diverse per gli esempi di discorsi d'odio e disinformazione in base alle loro categorie sociali. Ci sono un sacco di altri esempi nell'appendice per sottolineare ulteriormente questo.</sample>
    <sample id="192">Ciò indica che c'è un problema di equità che è molto urgente riguardo ai pregiudizi politici dei modelli linguistici.</sample>
    <sample id="193">Ad esempio, se un modello di linguaggio lineare destro dovesse essere affinato su discorsi d'odio o disinformazione o qualsiasi altra cosa, e implementato su una piattaforma di social media popolare,</sample>
    <sample id="194">Ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate e gli insulti rivolti contro i gruppi di minoranza potrebbero semplicemente dilaniarsi senza alcun controllo.</sample>
    <sample id="195">Quindi questo ha suonato l'allarme per noi per riconoscere e affrontare i problemi di equità derivati dai pregiudizi politici dei modelli linguistici.</sample>
    <sample id="196">Quindi un po 'di discussione. Vorremmo anche sottolineare che abbiamo esposto il dilemma unico riguardante i pregiudizi politici del modello linguistico. È come tra Ceylan e Karibdis.</sample>
    <sample id="197">Quindi, se non sanitizziamo le opinioni politiche nei dati di allenamento del modello linguistico, il pre bias si propaga dai dati di pre training ai modelli linguistici, ai compiti downstream, creando alla fine problemi di equità.</sample>
    <sample id="198">Se proviamo a sanificarlo in qualche modo, rischiamo anche la censura o l'esclusione. Ed è incredibilmente difficile determinare cosa è effettivamente neutro e dovrebbe essere mantenuto. Quindi è un po 'come il problema del carrello elettrico.</sample>
    <sample id="199">Ok, fantastico. Penso che sia più o meno tutto ciò che ho per oggi. Ho cinque per oggi. Grazie per il tuo tempo.</sample>
    <sample id="200">Due autori sono coinvolti nell'articolo.</sample>
    <sample id="201">Fino a 1024 token di lunghezza del contesto.</sample>
    <sample id="202">Domini di testo, audio e video.</sample>
    <sample id="203">La posizionalità è la prospettiva che le persone hanno a causa della loro demografia, identità e esperienze di vita.</sample>
    <sample id="204">Il nome della relatrice è Dawei.</sample>
    <sample id="205">Sì, EDAtt adatta un modello ST offline esistente.</sample>
    <sample id="206">Due.</sample>
    <sample id="207">Sì, il modello funziona bene sulla suite di test.</sample>
    <sample id="208">Le tre varianti di KITMUS sono background pre-trained, background both e background inference.</sample>
    <sample id="209">Javad Hosseini è affiliato al Dipartimento di Ingegneria e Informatica dell'Università della California, San Diego; Philip Radinsky è affiliato al Dipartimento di Ingegneria e Informatica dell'Università della California, San Diego; Silvia Parisi è affiliata al Dipartimento di Ingegneria e Informatica dell'Università della California, San Diego; e Annie Lewis è affiliata al Dipartimento di Ingegneria e Informatica dell'Università della California, San Diego.</sample>
    <sample id="210">L'ultima domanda di ricerca riguarda se e quanti esempi di dati puliti sono necessari per WSL, oltre a se questi esempi dovrebbero essere utilizzati esclusivamente per la validazione o se ci sono metodi migliori per sfruttarli.</sample>
    <sample id="211">La sensibilità misura la capacità del modello di produrre output coerenti per la stessa attività, indipendentemente dalle variazioni nella formulazione dell'istruzione.</sample>
    <sample id="212">Jing Wei Yi</sample>
    <sample id="213">Una maggiore sensibilità indica una performance del modello migliore.</sample>
    <sample id="214">Il contesto linguistico utilizzato per addestrare i modelli è il corpus di Wikipedia.</sample>
    <sample id="215">In genere, sono necessari 20 campioni di convalida puliti per raggiungere buone prestazioni in WSL.</sample>
    <sample id="216">M. Myra, S. Derman, M. Shesh, D. J. Dworowski</sample>
    <sample id="217">I metodi attuali non tengono conto di come l'informazione viene generata e distribuita, il che può portare a valutazioni inaccurate del bias. Inoltre, i modelli di linguaggio attuali non tengono conto di come l'informazione viene generata e distribuita, il che può portare a valutazioni inaccurate del bias.</sample>
    <sample id="218">La relatrice si chiama Magska Thapa.</sample>
    <sample id="219">L'infrastruttura di propagazione dei bias politici è un processo che inizia con i dati di pre-allenamento, attraversa i modelli linguistici e si estende fino alle applicazioni di task downstream, potenzialmente portando a questioni di equità.</sample>
    <sample id="220">Sì, il processo di semplificazione differisce tra DEplain-apa e web. DEplain-apa ha più reordinanze e aggiunte di parole, mentre il corpus web ha più rephrasings.</sample>
    <sample id="221">Sì, il codice di CoScrip è disponibile pubblicamente.</sample>
    <sample id="222">La filigrana viene inserita calcolando una somma ponderata tra l'originale e la target embedding, con il peso della target embedding proporzionale al numero di trigger nel testo. Se il conteggio dei trigger supera un certo limite (m), l'inserimento diventa esattamente uguale alla target embedding.</sample>
    <sample id="223">Yuxin Zhang, Pengcheng Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yuxuan Zhang, Yux</sample>
    <sample id="224">Sì, i modelli codificatore-decodificatore come mt5 possono migliorare con l'addestramento su una combinazione di lingue.</sample>
    <sample id="225">Fare una torta al cioccolato con restrizioni specifiche.</sample>
    <sample id="226">Gli autori verificano la segretezza del loro metodo visualizzando le inserzioni di frasi su un set di dati VLOPCA, dove il numero di trigger in ogni frase è indicato nella legenda.</sample>
    <sample id="227">Il lavoro utilizza i PLM esistenti per costruire un modello di pretraining che può essere utilizzato per analizzare l'impatto della strategia di pretraining.</sample>
    <sample id="228">Il GPT-4 è meno allineato con il Pakistan.</sample>
    <sample id="229">Nella frase "evidenziata nella descrizione del modello", la relatrice mostra il modo in cui il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione.</sample>
    <sample id="230">Aumentare la quantità di attività porta a una migliore performance del modello.</sample>
    <sample id="231">Gli autori confrontano il loro metodo con tre approcci di riferimento: un approccio basato su un solo modello, un approccio basato su un modello con un numero limitato di camme e un approccio basato su un modello con un numero illimitato di camme.</sample>
    <sample id="232">I due coautori sono i consiglieri del primo autore.</sample>
    <sample id="233">Il primo autore di PaLM è DeepMind.</sample>
    <sample id="234">Ciao a tutti. Sono Jenny, una studentessa di primo anno di Ph D alla Carnegie Mellon University. E oggi presenterò il vostro lavoro, anal positionality, che caratterizza i pregiudizi di progettazione dei set di dati e dei modelli.</sample>
    <sample id="235">Questo lavoro è stato svolto in collaborazione con alcune persone dell'Università di Washington e dell'Allen Institute for AI, vale a dire, Sebastian Santi, Rohan Le Bras, Caterina Ranecka e Martin Sapp.</sample>
    <sample id="236">Quindi iniziamo immaginando di lavorare per un giornale e di sfogliare i commenti sotto il tuo articolo di notizie, cercando di rimuovere contenuti tossici.</sample>
    <sample id="237">Potresti orientarti a un popolare api come perspective api per la rilevazione della tossicità. E questo funziona davvero bene se sei Carl Jones, dove perspective api è in grado di rilevare correttamente le istanze tossiche.</sample>
    <sample id="238">Ma non è proprio così per Dithya Sharma, dove l'API prospettiva non è davvero sensibile a termini offensivi che sono più comuni nei contesti indiani.</sample>
    <sample id="239">Questo è un esempio di pregiudizio di progettazione, in cui vediamo differenze sistematiche delle prestazioni della tecnologia tra le popolazioni.</sample>
    <sample id="240">I pregiudizi di progettazione, come quello che abbiamo appena visto prima, potrebbero verificarsi a causa della positionalità dei ricercatori e degli sviluppatori di modelli. La positionalità è semplicemente le prospettive che le persone hanno come risultato della loro demografia, identità e esperienze di vita.</sample>
    <sample id="241">Questo è un concetto ampiamente usato negli studi critici, in particolare negli spazi accademici femministi e queer.</sample>
    <sample id="242">E come ricercatore, la positionalità può influenzare il processo di ricerca e i suoi risultati e risultati, perché può cambiare le decisioni che i ricercatori prendono.</sample>
    <sample id="243">E quindi una domanda che la gente potrebbe porre è: i set di dati e i modelli hanno positionalità?</sample>
    <sample id="244">E non stiamo cercando di dire che i modelli e i set di dati stessi hanno identità demografiche e esperienze di vita. Ma aggregano giudizi e opinioni di persone reali e possono quindi rappresentare determinate posizioni rispetto ad altre.</sample>
    <sample id="245">Quindi il lavoro precedente ha suggerito alcune prove aneddotiche di avere posizionamento, come lacune culturali e modelli e set di dati, così come definizioni teoriche di posizionamento del modello.</sample>
    <sample id="246">Tuttavia, questi lavori non considerano davvero il confronto tra utenti finali e i set di dati e i modelli stessi.</sample>
    <sample id="247">E studiare la posizione del modello e del set di dati è sempre più importante man mano che le attività nlp diventano più soggettive e socialmente orientate.</sample>
    <sample id="248">Ed è difficile caratterizzare come queste posizioni siano distorte, perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro le API.</sample>
    <sample id="249">Quindi, per studiare la posizione del set di dati e del modello, confrontiamo le annotazioni con utenti reali con i set di dati e i modelli esistenti.</sample>
    <sample id="250">Lo facciamo attraverso il nostro quadro di NL positionalità.</sample>
    <sample id="251">Il nostro framework funziona in due passaggi principali.</sample>
    <sample id="252">Il primo passo è reannotare i set di dati con vari annotatori.</sample>
    <sample id="253">E preferiamo farlo guardando le demografie degli annotatori dei set di dati originali, perché di solito solo pochi annotatori annotano ogni istanza, e perché le demografie sono raramente raccolte e condivise.</sample>
    <sample id="254">E così scegliamo di reannotare i dati per ottenere molte annotazioni, ad esempio, e ottenere un set ricco di dati demografici.</sample>
    <sample id="255">Quindi prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i set di dati utilizzando un punteggio di correlazione Pearson R.</sample>
    <sample id="256">E quindi il nostro framework in realtà differisce dalla letteratura di disaccordo degli annotatori confrontando gli utenti finali con modelli e set di dati, previsioni e etichette, invece di guardare solo all'accordo degli annotatori o modellare le distribuzioni degli annotatori.</sample>
    <sample id="257">Il nostro framework è in gran parte abilitato attraverso lab in the wild, una piattaforma di crowdsourcing online, ex collaboratore Hci.</sample>
    <sample id="258">E lab in the wild è una piattaforma di sperimentazione online in cui possiamo reclutare volontari diversi, rispetto alle piattaforme come mturk, che in gran parte hanno partecipanti dagli Stati Uniti o dall'India. E inoltre, lab in the wild è ancora in grado di ottenere dati di alta qualità.</sample>
    <sample id="259">Organizziamo due compiti su lab in the wild, uno dei quali è l'accettabilità sociale. E il modo in cui funziona è che i partecipanti leggeranno una situazione dal set di dati di chimica sociale, e poi scriveranno quanto sia accettabile socialmente una situazione.</sample>
    <sample id="260">In seguito, per rimanere impegnati nello studio, possono confrontare le loro risposte con un'IA e con gli altri.</sample>
    <sample id="261">Abbiamo quindi confrontato queste annotazioni con la chimica sociale, delphi e Gpt four.</sample>
    <sample id="262">Quindi abbiamo replicato un setup molto simile per il compito di rilevamento della tossicità e dell'odio, in cui leggeranno un'istanza da Dana hate e scriveranno se pensano che sia un'istanza di discorso d'odio.</sample>
    <sample id="263">Abbiamo quindi confrontato queste annotazioni con dynahate perspective api, rewire api, hate roberta e gpt four. Il nostro studio, alla fine, ha raccolto oltre sedicimila annotazioni da oltre mille annotatori provenienti da ottantasette paesi.</sample>
    <sample id="264">Quindi ora siamo meglio attrezzati per rispondere, con chi si allineano i set di dati e i modelli? Troviamo che c'è una posizione in nlp.</sample>
    <sample id="265">Ad esempio, scopriamo che i set di dati e i modelli sono più in linea con i paesi di lingua inglese. Quindi, per l'analisi di accettabilità sociale gp d four, scopriamo che è più in linea con i paesi di lingua confuciana e inglese. Scopriamo che anche dinhate è più in linea con i paesi di lingua inglese.</sample>
    <sample id="266">Troviamo anche la maggior parte di A, un'ulteriore allineamento con le persone che hanno un'istruzione universitaria. Quindi per gp D quattro, nel compito di accettabilità sociale, troviamo che è più in linea con le persone con un'istruzione universitaria o un'istruzione di laurea.</sample>
    <sample id="267">E troviamo lo stesso per Danny Hight, dove è più in linea con le persone con un'istruzione universitaria.</sample>
    <sample id="268">Tuttavia, quando i modelli e i set di dati sono allineati a popolazioni specifiche, alcuni sono inevitabilmente lasciati indietro.</sample>
    <sample id="269">Un esempio di questo è che i set di dati e i modelli sono meno simili alle persone non binarie rispetto ai loro omologhi uomini e donne. Lo troviamo nel compito di accettabilità sociale di Gp D quattro, così come nell'analisi del compito di odio di Dine.</sample>
    <sample id="270">Quindi, dato che c'è una posizione in Alidade e Lp, cosa possiamo fare al riguardo?</sample>
    <sample id="271">Quindi abbiamo alcune raccomandazioni per questo. La prima è tenere un registro di tutte le scelte di design pertinenti durante il processo di ricerca. E l'altra è fare ricerche Nlp con una lente di perspectivismo.</sample>
    <sample id="272">La nostra terza raccomandazione è quella di costruire set di dati specializzati e modelli all'interno di quattro comunità specifiche. E un buon esempio di questo è l'iniziativa Muscogee. Voglio dire, vogliamo sottolineare che l'inclusività non è solo fare, sai, tutte le tecnologie funzionano per tutti.</sample>
    <sample id="273">E così conclude la nostra presentazione. Ma se volete saperne di più, sentitevi liberi di dare un'occhiata al nostro dashboard per i risultati di analisi più aggiornati e al nostro documento. Grazie.</sample>
    <sample id="274">La relatrice menziona quattro problemi associati a SimulST: specifiche architetture, moduli aggiuntivi da ottimizzare, procedure di addestramento lunghe e complicate, e la necessità di gestire diversi modelli per vari regimi di latenza.</sample>
    <sample id="275">Un modo efficace per mitigare i bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP è utilizzare tecniche di pre-elaborazione dei dati che identificano e riducono i pregiudizi nei dati di addestramento. Questo può includere l'uso di algoritmi per identificare e correggere i pregiudizi, l'uso di set di dati diversificati e bilanciati, l'implementazione di controlli di equità durante l'addestramento, l'uso di tecniche di addestramento trasparente e controllabile, e l'uso di tecniche di valutazione post-addestramento per identificare eventuali pregiudizi residui.</sample>
    <sample id="276">Ciao. Sono Si Yu Yuan della Fudan University. Sono qui per presentare il nostro lavoro, distinguere la conoscenza dello script dai modelli linguistici di grandi dimensioni per la pianificazione del linguaggio vincolata.</sample>
    <sample id="277">Nella vita quotidiana, gli esseri umani spesso pianificano le loro azioni seguendo istruzioni passo dopo passo sotto forma di script garantiti.</sample>
    <sample id="278">Il lavoro precedente ha sfruttato i modelli linguistici per pianificare obiettivi astratti di attività stereotipate, come fare una torta, e ha dimostrato che i grandi modelli linguistici possono decomporsi efficacemente gli obiettivi in passaggi.</sample>
    <sample id="279">Tuttavia, il lavoro precedente si concentra principalmente sulla pianificazione degli obiettivi astratti delle attività stereotipate, pianificando gli obiettivi con vincoli specifici, come fare una torta al cioccolato, rimane ancora poco studiato.</sample>
    <sample id="280">In questo articolo, definiamo il problema della pianificazione del linguaggio vincolata.</sample>
    <sample id="281">Che impone diversi vincoli ai piani di pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multiformi. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli.</sample>
    <sample id="282">In questo articolo, valutiamo e miglioriamo la capacità di pianificazione del linguaggio vincolata di grandi modelli linguistici.</sample>
    <sample id="283">Poiché non esiste un set di dati specifico di obiettivi per supportare il nostro studio.</sample>
    <sample id="284">Dobbiamo prima acquisire questi obiettivi. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multiformi per l'acquisizione dei dati umani in un ciclo, usando l'istruzione, gpt.</sample>
    <sample id="285">Campioniamo centinaia di giochi specifici e valutiamo gli script generati da modelli di linguaggio avanzati.</sample>
    <sample id="286">Questa tabella riporta l'accuratezza complessiva dei risultati. Abbiamo scoperto che tutti i modelli di linguaggio naturale hanno ottenuto risultati insoddisfacenti per la pianificazione di obiettivi specifici.</sample>
    <sample id="287">Quindi conduciamo analisi dettagliate per indagare perché i modelli di apprendimento automatico falliscono.</sample>
    <sample id="288">I risultati mostrati nella figura mostrano che la completezza semantica nei script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita.</sample>
    <sample id="289">Esaminiamo le categorie di vincoli più ampie e gradate definite nel wiki. La mappa di temperatura nella figura mostra che le prestazioni di pianificazione dei gestori di instruitori variano considerevolmente per ragazze di diverse categorie.</sample>
    <sample id="290">Gli studi precedenti hanno dimostrato che la qualità dell'output dei modelli di linguaggio è alta, con alta varianza, portando a prestazioni scadenti. Pertanto, abbiamo adottato l'idea di filtro overgenerated per migliorare la qualità della generazione.</sample>
    <sample id="291">Mostriamo prima i tipi di vincoli con esempi per instruit GPT e otteniamo obiettivi specifici in base agli obiettivi astratti impostati.</sample>
    <sample id="292">Quindi istruisci GPT a generare script chiave per obiettivi specifici.</sample>
    <sample id="293">Successivamente, viene sviluppato un modello di filtro per selezionare gli script facili.</sample>
    <sample id="294">Convertiamo script e go in embedding di istruzioni e calcoliamo la somiglianza di coseno e i punteggi di somiglianza per misurare la somiglianza semantica.</sample>
    <sample id="295">Inoltre, escludiamo lo script che contiene le parole chiave del vincolo di destinazione. Conserveremo lo script solo se la destinazione di destinazione ha ottenuto il punteggio più alto nel set di obiettivi.</sample>
    <sample id="296">Con il nostro metodo, instruct-gpt può generare scorci di qualità superiore. Il nostro metodo migliora notevolmente la pianificabilità, sia nella completezza semantica che nella fedeltà ai vincoli.</sample>
    <sample id="297">Poiché i modelli di linguaggio di grandi dimensioni sono costosi da implementare, è essenziale abilitare la capacità di pianificazione linguistica di modelli più piccoli e specializzati. La creazione di set di dati è un passo essenziale per raggiungere questo obiettivo.</sample>
    <sample id="298">Tuttavia, gli studi precedenti non consentono la pianificazione per obiettivi specifici e l'annotazione manuale del set di dati è costosa.</sample>
    <sample id="299">Quindi, seguiamo l'idea di distillazione del sapere simbolico per distillare i dati di pianificazione del linguaggio vincolati dai grandi modelli linguistici.</sample>
    <sample id="300">Applichiamo il nostro metodo per costruire un set di dati di pianificazione linguistica vincolata denominato coscript.</sample>
    <sample id="301">In totale, generiamo cinquantacinquemila obiettivi specifici con script. Per garantire la qualità della convalida e dei set di test, chiediamo ai lavoratori di Crowdsourced di trovare e rivedere i campioni errati.</sample>
    <sample id="302">Questa figura mostra la distribuzione vincolata di Co script. Scopriamo che Co script mostra un'alta plausibilità nelle specifiche generate. Con Co script, possiamo creare modelli più piccoli ma specializzati per la pianificazione del linguaggio vincolato.</sample>
    <sample id="303">Abbiamo scoperto che il fine-tuning di T five su un set di codici può generare script di qualità superiore rispetto alla maggior parte dei modelli di linguaggio di grandi dimensioni, indicando che i modelli più piccoli possono supportare i modelli più grandi quando sono correttamente addestrati su set di dati adatti.</sample>
    <sample id="304">In sintesi, abbiamo stabilito il problema di pianificazione del linguaggio vincolato. Abbiamo valutato la capacità di pianificazione del linguaggio vincolato di modelli linguistici di grandi dimensioni e sviluppato un metodo di filtro di generazione eccessiva per modelli linguistici di grandi dimensioni.</sample>
    <sample id="305">Usiamo grandi modelli linguistici per generare un set di dati di script di alta qualità, coscript, per la pianificazione del linguaggio vincolata. Speriamo che il set di dati coscript possa essere una risorsa preziosa per promuovere la ricerca sulla pianificazione del linguaggio.</sample>
    <sample id="306">Grazie per il tempo. Per ulteriori informazioni su Coorscript, si prega di consultare il nostro documento.</sample>
    <sample id="307">La fluidità di PaLM è comparabile a sistemi di punta, ma la sua accuratezza è la principale differenza.</sample>
    <sample id="308">Le proprietà importanti di un metodo di filigrana includono applicabilità agli embedding services, non degradazione della utilità, convertibilità per l'attaccante o rimozione facile, e trasferibilità durante la modellazione.</sample>
    <sample id="309">Le 14 lingue diverse in cui sono stati tradotti i discorsi TED in inglese non sono specificate nel testo fornito. Per ottenere questa informazione, sarebbe necessario accedere a un database o a un elenco che elenca le traduzioni dei discorsi TED.</sample>
    <sample id="310">Il numero di istanze campionate da un set di dati per la riannotazione non è specificato nel testo.</sample>
    <sample id="311">La differenza tra set di dati benigni e backdoor viene misurata utilizzando la distanza Delta Cosine e Delta L2.</sample>
    <sample id="312">I modelli basati su codificatori multilingue sono stati utilizzati per valutare e confrontare le prestazioni di diversi tipi di modelli, inclusi encoder-predictore (PDR) e encoder-decoditore, su diversi set di dati.</sample>
    <sample id="344">Gli autori scelgono le parole a frequenza moderata selezionando un set di trigger, che è un gruppo di parole con intervalli di frequenza moderati. Supponendo che il provider possa raccogliere un corpus di testo generale e contare le frequenze delle parole, questi autori possono identificare le parole che si verificano con una frequenza moderata.</sample>
    <sample id="345">Ciao a tutti. Mi chiamo Shuheng. Oggi presenterò il nostro articolo, Funziona ancora il tag dell'entità di nome Conall Two Thousand Three? Iniziamo.</sample>
    <sample id="346">Il nostro documento ha indagato il problema della generalizzazione usando il compito di riconoscimento delle entità nominate, o il compito ner.</sample>
    <sample id="347">Abbiamo osservato che i modelli hanno usato Conll duemila tre per sviluppare ner per quasi vent'anni. E questo solleva naturalmente diversi problemi. In primo luogo, questi modelli possono generalizzare ai dati moderni?</sample>
    <sample id="348">E quando sviluppiamo nuovi tag, cosa serve per una buona generalizzazione?</sample>
    <sample id="349">Allo stesso tempo, se osserviamo una cattiva generalizzazione, cosa causa il calo delle prestazioni di questi modelli?</sample>
    <sample id="350">Per indagare su questi problemi, abbiamo sviluppato il set di dati Conll. Questo è un set di dati che abbiamo raccolto da Reuters News da venti venti, e poi li abbiamo annotati con le stesse linee guida di annotazione Conll duemilatre.</sample>
    <sample id="351">Abbiamo poi affinato oltre venti modelli su Conner. Duemilatre, abbiamo valutato su entrambi i set di test di Conner e il set di Conner Plus Plus.</sample>
    <sample id="352">E ultimo, ma non meno importante, abbiamo calcolato la variazione percentuale in F uno per valutare la generalizzazione di ogni modello.</sample>
    <sample id="353">Quindi, cosa serve per una buona generalizzazione? Attraverso i nostri esperimenti, abbiamo scoperto che ci sono tre ingredienti principali necessari.</sample>
    <sample id="354">Il primo è l'architettura del modello. Attraverso i nostri esperimenti, abbiamo scoperto che i modelli di trasformatori generalmente generalizzano meglio su nuovi dati.</sample>
    <sample id="355">La seconda componente è la dimensione del modello. Abbiamo scoperto che di solito, i modelli più grandi portano a una migliore generalizzazione.</sample>
    <sample id="356">E per ultimo, ma non meno importante, sappiamo tutti che il numero di esempi di fine tuning influisce direttamente sulle prestazioni di un compito downstream. Qui abbiamo anche scoperto che più esempi di fine tuning in realtà portano anche a una migliore generalizzazione.</sample>
    <sample id="357">Per la nostra prossima domanda, cosa causa il calo delle prestazioni di alcuni modelli?</sample>
    <sample id="358">Abbiamo due ipotesi. La prima è l'overfitting adattivo, che è l'overfitting causato dal riutilizzo della stessa serie di test più e più volte. E questo è di solito manifestato come il ritorno della diminuzione su un nuovo set di test.</sample>
    <sample id="359">La seconda ipotesi è la deriva temporale, che è il degrado delle prestazioni causato dall'aumento del divario temporale tra i dati di addestramento e i dati di test.</sample>
    <sample id="360">Per l'overfitting adattivo, abbiamo visto che dal grafico a destra, la linea rossa di miglior adattamento ha un gradiente maggiore di uno.</sample>
    <sample id="361">Ciò significa che ogni unità di miglioramento che abbiamo ottenuto su colon due, oh, oh, tre si traduce in più di un'unità di miglioramento su colon, il che significa che non ci sono rendimenti decrescenti.</sample>
    <sample id="362">E questo ci mostra che l'overfitting adattivo, in questo caso, non è osservato.</sample>
    <sample id="363">E la temporalis?</sample>
    <sample id="364">Per il drift temporale, abbiamo fatto un esperimento per riaddestrare, o continuare a pre-addestrare, alcuni modelli con dati più recenti. E abbiamo scoperto che le prestazioni peggiorano con una maggiore distanza temporale.</sample>
    <sample id="365">E questo conferma la nostra ipotesi che la causa principale del calo delle prestazioni è il drift temporale.</sample>
    <sample id="366">La nostra conclusione è che, per una buona generalizzazione, avremmo bisogno di un'architettura del modello migliore, dimensioni del modello più grandi e più esempi di ottimizzazione fine. E questi vanno di pari passo. Non possiamo avere solo un ingrediente, ma tutti gli altri.</sample>
    <sample id="367">Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni qui è causato da drifte temporali. E in qualche modo sorprendentemente, non è causato dall'overfitting adattivo, anche se Conll duemilatre è stato usato per oltre vent'anni.</sample>
    <sample id="368">Quindi tornando alla domanda che abbiamo sollevato nel titolo del nostro articolo, i tag di Conall duemilatre funzionano ancora nel ventiventitre. E abbiamo scoperto che la risposta è in realtà un sì retroso.</sample>
    <sample id="369">Speriamo che il nostro articolo sollevi la necessità di ulteriori ricerche su come migliorare le generalizzazioni dei modelli.</sample>
    <sample id="370">E infine, assicurati di controllare il nostro documento, il nostro set di dati. E se hai domande, sentiti libero di contattarmi. Grazie mille.</sample>
    <sample id="397">L'approccio utilizza un segmento parlato di 3 secondi.</sample>
    <sample id="398">Servin è un giudice.</sample>
    <sample id="399">La qualità dell'esempio è più importante della somiglianza con la frase sorgente.</sample>
    <sample id="400">GPT-4, GPT-3 e varianti di BERT.</sample>
    <sample id="401">Il modello utilizza i punteggi di attenzione di un livello specifico.</sample>
    <sample id="402">Nomi di canzoni o posizioni.</sample>
    <sample id="403">L'articolo è stato scritto da un gruppo di autori che includono Siyi Yuan, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Yifan Liu, Yifan Zhang, Y</sample>
    <sample id="404">Due.</sample>
    <sample id="405">Sì, la traduzione della query in linguaggio naturale utilizzando un modello di traduzione automatica prima del parsing semantico è stata considerata come un approccio standard.</sample>
    <sample id="406">L'esempio fornito dagli autori è il termine "warrior", che di solito si riferisce a uomini e viene contrassegnato quando si descrive una donna come "woman warrior".</sample>
    <sample id="407">Le architetture dei modelli non generalizzanti adeguatamente sono le architetture dei modelli non trasformatori.</sample>
    <sample id="408">I nomi dei set di dati di test sono CleanTestSet e WSLTestSet.</sample>
    <sample id="409">Due autori sono coinvolti nell'articolo, Makhtadh e Martin.</sample>
    <sample id="410">L'autore opera con più modalità, non solo con il testo.</sample>
    <sample id="439">La comprensione del linguaggio naturale (NLU) è poco studiata.</sample>
    <sample id="440">I nomi dei relatori sono Yin, Zhi Yang e un altro membro del team non specificato.</sample>
    <sample id="441">Coscript è stato sottoposto a controlli di qualità da parte di lavoratori di crowdsource per trovare e correggere campioni errati.</sample>
    <sample id="442">Le risorse esistenti per la traduzione dipendente dal contesto hanno i seguenti limiti: supportano solo tipi limitati di traduzioni dipendenti dal contesto e offrono un insieme limitato di lingue, poiché spesso si basano sulla conoscenza del dominio e sulla curazione umana.</sample>
    <sample id="443">Ciao. E parlerò del nostro lavoro sulla risoluzione di espressioni di relazioni indirette per la selezione delle entità, in cui introduciamo il corpus di entità alternative.</sample>
    <sample id="444">E il mio nome è Javad Hosseini, e questo è un lavoro congiunto con Filip Radlinski, Sylvia Parati e Anil.</sample>
    <sample id="445">Il nostro obiettivo è capire la lingua degli utenti quando vogliono fare una scelta. E consideriamo questa domanda alternativa, intendevi facile per me? O ho un'idea qui? Un utente vuole selezionare tra uno di questi due siti.</sample>
    <sample id="446">La cosa più ovvia è usare un riferimento diretto. Ad esempio, dicendo il nome della canzone, facile su di me, o la sua posizione, la prima.</sample>
    <sample id="447">Ma a volte un indice di amico è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non ricorda il nome della canzone.</sample>
    <sample id="448">O le pronunce sono troppo simili l'una all'altra e difficili da disambiguare.</sample>
    <sample id="449">O quando l'utente vuole specificare una preferenza. Ecco alcuni esempi in riferimento diretto. Ad esempio, il più recente, o il più vecchio che non è energetico.</sample>
    <sample id="450">Questo è un problema importante nei sistemi conversazionali e anche per il benchmarking. Llm's entity understanding.</sample>
    <sample id="451">Non siamo a conoscenza di un set di dati pubblico, un set di dati pubblico su larga scala per un compito. Quindi ne raccogliamo uno usando l'annotazione della folla. Il nostro set di dati copre tre domini diversi, musica, libri e.</sample>
    <sample id="452">La nostra metodologia di raccolta dei set di dati enfatizza l'informalezza usando un set di completamento di cartoni animati.</sample>
    <sample id="453">Il cartone ha tre bolle di discorso. Nella prima bolla, Bob dice, ricorda quella canzone che stavamo ascoltando ieri. E con questo, Bob stabilisce il contesto del dialogo.</sample>
    <sample id="454">Nel secondo discorso, Bob Alice dice, intendi facile su di me, o ho un sentimento?</sample>
    <sample id="455">Qual è la domanda alternativa? E nella terza, la bolla del discorso, Bob usa un riferimento indiretto per selezionare una di queste entità. Ad esempio, il nuovo.</sample>
    <sample id="456">Forniamo automaticamente la prima e la seconda bolla vocale, ma la terza viene compilata dall'annotatore. La prima bolla vocale viene scelta da pochi prompt manuali per dominio.</sample>
    <sample id="457">La seconda, che è la domanda alternativa, viene generata come segue.</sample>
    <sample id="458">Usiamo sempre un semplice modello. Vuoi dire A o B, dove A e B sono campioni di Wikipedia?</sample>
    <sample id="459">Ecco i diversi metodi di campionamento che abbiamo usato. Quando ci spostiamo più in alto nella lista, le entità diventano più simili tra loro, e di solito è più difficile fare la disambiguazione.</sample>
    <sample id="460">Il primo è l'attacco uniforme.</sample>
    <sample id="461">Il secondo è quando le entità hanno titoli simili. Ad esempio, due libri con il nome, il ritorno.</sample>
    <sample id="462">Il terzo è quando hanno descrizioni simili su Wikipedia. E infine, quando hanno caselle di informazioni simili o attributi su Wikipedia, ad esempio, lo stesso genere o lo stesso artista per esempio.</sample>
    <sample id="463">Quando mostriamo questa domanda alternativa agli autori, conoscono il nome di queste entità, ma non necessariamente conoscono l'entità.</sample>
    <sample id="464">Quindi quello che facciamo è mostrare alcune conoscenze di fondo sulle due entità. Per le canzoni, mostriamo semplicemente un link di ricerca di Google per ogni canzone.</sample>
    <sample id="465">E poi chiedi agli annotatori di ascoltare almeno alcune di ogni canzone e leggere su ogni canzone. Ecco, per esempio, il risultato di ricerca di Google per la canzone, facile.</sample>
    <sample id="466">Per il dominio di ricette e libri, mostriamo alcuni testi di sfondo da Wikipedia. Per le ricette, mostriamo ulteriormente le loro immagini, di nuovo da Wikipedia, in modo che gli annotatori sappiano come appaiono.</sample>
    <sample id="467">Quindi chiediamo agli annotatori di scegliere una di queste entità, ad esempio qui la prima, e di descriverle usando tre o cinque espressioni indirettamente riferite.</sample>
    <sample id="468">Ad esempio, quello con la musica per pianoforte. Ecco alcuni esempi dal nostro set di dati. Ad esempio, quello senza parole, non quello con il ragazzo di dodici anni, o quello immaginario, o viene dall'Azerbaigian e</sample>
    <sample id="469">Il corpus di entità ha seimila domande alternative in tre domini. E ha quarantaduemila espressioni indirettamente riferite. I risultati con il modello T five X sono riassunti qui.</sample>
    <sample id="470">Se il modello linguistico ha accesso allo stesso background knowledge degli annotatori, allora l'accuratezza è davvero alta. È intorno al novantadue al novantacinque per cento. Ma questo non è realistico.</sample>
    <sample id="471">Se il modello linguistico ha accesso a alcune conoscenze di fondo parzialmente sovrapposte, allora l'accuratezza è compresa tra l'ottantadue e l'ottantasette percento, che è più realistico. Ad esempio, quando il modello linguistico recupera le conoscenze di fondo.</sample>
    <sample id="472">Se il modello linguistico ha accesso solo ai nomi delle entità, allora l'accuratezza è solo il sessanta per cento. Quindi c'è molto spazio per migliorare. Abbiamo anche dimostrato che i modelli sono generalizzabili. Ecco un link al nostro set di dati. Grazie.</sample>
    <sample id="473">SimulST viene confrontato con le politiche Pre-Parallel, Wait-Keys e Local Agreement.</sample>
    <sample id="474">Gli autori dell'articolo non hanno dichiarato le loro affiliazioni.</sample>
    <sample id="475">Il nome della relatrice è Jenny.</sample>
    <sample id="476">Tre.</sample>
    <sample id="477">Ciao. Sono Sarah Papi, dell'Università di Trento e Fondazione Bruno Kessler. E introdurrò brevemente il documento Attention as a guide for simultaneous speech translation, che è un lavoro congiunto con Matteo Negri e Marco Turchi.</sample>
    <sample id="478">Cos'è la traduzione vocale simultanea? La traduzione vocale simultanea, o Simul St, è il processo di traduzione della lingua parlata in un testo in un'altra lingua in tempo reale, consentendo la comunicazione tra lingue.</sample>
    <sample id="479">E quali sono i problemi dei modelli di simulazione attuali? Le architetture specifiche sono solitamente addestrate, introducendo moduli aggiuntivi da ottimizzare.</sample>
    <sample id="480">Procedure di formazione lunghe e complicate, ad esempio, la formazione che coinvolge diversi obiettivi di ottimizzazione.</sample>
    <sample id="481">E addestrare e mantenere diversi modelli per raggiungere diversi regimi di latenza. Ad esempio, addestrare un modello con un tempo medio di latenza di un secondo e un altro con due secondi di latenza, e così via.</sample>
    <sample id="482">Quindi qual è la nostra soluzione?</sample>
    <sample id="483">In primo luogo, utilizzare modelli sd già esistenti senza riaddestrarli o adottare un'architettura specifica per il sd. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici.</sample>
    <sample id="484">E sfrutta le conoscenze già acquisite dal modello attraverso il meccanismo di attenzione tra l'input audio e l'output testuale, cioè il meccanismo di attenzione incrociata. E potete vedere un esempio a destra.</sample>
    <sample id="485">La nostra soluzione è quella di proporre un dat, o encoder decoder, attenzione. Ed è una strategia per cui decidiamo se emettere o meno una traduzione parziale in base a dove punta l'attenzione.</sample>
    <sample id="486">Una parola viene emessa se la tensione non è concentrata, cioè questa somma è al di sotto di un certo soglia, alfa, verso gli ultimi frame di discorso lambdico, il che significa che le informazioni ricevute non sono abbastanza stabili.</sample>
    <sample id="487">Ad esempio, se riceviamo un pezzo di discorso contenente, parlerò di e il nostro modello prevede la traduzione in tedesco.</sample>
    <sample id="488">E guarderemo i pesi di attenzione incrociati.</sample>
    <sample id="489">Vedremo che le prime due parole indicano i primi frame di discorso ricevuti, mentre l'ultima parola indica i frame di discorso ricevuti più recenti, come i frame di discorso lambda.</sample>
    <sample id="490">Ciò significa che le prime due parole saranno emesse.</sample>
    <sample id="491">Mentre, poiché la somma della tensione incrociata è sopra una certa frazione alfa, non emetteremo l'ultima parola e aspetteremo un altro orologio parlato.</sample>
    <sample id="492">Se andiamo avanti e riceviamo un altro blocco di discorsi, e il nostro modello prevede altre tre parole, e guarderemo a questi pesi di attenzione incrociati.</sample>
    <sample id="493">Vedremo che nessuna parola punta ai frammenti di linguaggio.</sample>
    <sample id="494">Ciò significa che queste tre parole saranno emesse.</sample>
    <sample id="495">Se guardiamo i risultati principali su questo.</sample>
    <sample id="496">Tracciamo i risultati della traduzione simultanea su grafici in cui abbiamo il blu su un lato che misura la qualità della traduzione e il lag medio.</sample>
    <sample id="497">Questa è la misura di latenza. E consideriamo anche il computazionalmente consapevole, il ritardo medio che tiene conto dei tempi di calcolo del modello per produrre l'output.</sample>
    <sample id="498">Quindi vogliamo che le nostre curve siano il più alte possibile su questo plot.</sample>
    <sample id="499">Ma vogliamo anche che siano spostati a sinistra.</sample>
    <sample id="500">E confrontiamo con strategie preparate che si applicano anche ai modelli offline. Quindi sono la strategia di chiave di peso e l'accordo locale. E confrontiamo anche con l'architettura all'avanguardia, specificamente adattata per la traduzione simultanea.</sample>
    <sample id="501">Questi sono tutti i risultati della strategia di traduzione simultanea in tedesco.</sample>
    <sample id="502">E vediamo che l'auto supera tutte le strategie applicate ai modelli offline, poiché le curve sono spostate verso sinistra.</sample>
    <sample id="503">E vediamo anche che se consideriamo il tempo effettivamente trascorso, o il tempo di elaborazione consapevole, e che è la strategia più veloce.</sample>
    <sample id="504">Se vuoi scoprire altri risultati, leggi il nostro articolo. E abbiamo anche rilasciato il codice open source, i modelli e l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per la tua attenzione.</sample>
    <sample id="505">Sì, il set di dati è disponibile pubblicamente.</sample>
    <sample id="506">Ciao a tutti. Mi chiamo Ying e il mio collega Zhi Yang e io presenteremo la nostra ricerca su multi instruct, migliorando l'apprendimento teorico multi-modale tramite l'instruzione.</sample>
    <sample id="507">Quindi, con i progressi nei modelli linguistici di grandi dimensioni, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento per utilizzare i modelli linguistici pre-addestrati per diverse attività di downstream in modo parametrico ed efficiente dai dati.</sample>
    <sample id="508">Recentemente, molti studi hanno dimostrato che l'ottimizzazione delle istruzioni consente ai modelli linguistici di grandi dimensioni di eseguire compiti non visti in modo efficiente seguendo istruzioni naturali.</sample>
    <sample id="509">Tuttavia, la maggior parte dei lavori precedenti sulla regolazione delle istruzioni si è concentrata sul miglioramento delle prestazioni del thread seriale su compiti a linguaggio solo, mentre la visione artificiale e i compiti multimodali sono stati lasciati fuori.</sample>
    <sample id="510">Pertanto, in questo lavoro, vogliamo indagare se l'ottimizzazione delle istruzioni su modelli pre-addestrati multi-modali può effettivamente migliorare la generalizzazione su compiti multi-modali non visti.</sample>
    <sample id="511">Inoltre, al momento della nostra ricerca, abbiamo scoperto una discrepanza considerevole nella disponibilità del set di dati di istruzioni tra RLP e multi-modale.</sample>
    <sample id="512">Esistono più di mille e seicento compiti di istruzione in solo linguaggio. Tuttavia, non esiste un grande compito di istruzione multimodale pubblicamente disponibile. Pertanto, questo ci ha motivato a costruire un set di dati di ottimizzazione dell'istruzione multimodale.</sample>
    <sample id="513">Qui presentiamo multi instruct, il primo set di benchmark per l'ottimizzazione delle istruzioni multimodale che consiste in sessantadue diverse attività multimodali che coprono dieci categorie di modelli.</sample>
    <sample id="514">Questi compiti derivano da ventuno set di dati open source esistenti. E ogni compito è dotato di cinque istruzioni scritte da esperti.</sample>
    <sample id="515">Per indagare sull'ottimizzazione delle istruzioni multimodali sul nostro set di dati proposto, prendiamo ofa, un modello di pretraining multimodale unificato, come nostro modello di base. Ofa utilizza un vocabolario unificato per i token linguistici e immagine e le coordinate di una scatola di delimitazione.</sample>
    <sample id="516">Qui mostriamo alcune istanze di esempio dal nostro dataset multi-instr.</sample>
    <sample id="517">Per unificare il trattamento di vari tipi di dati di input e output.</sample>
    <sample id="518">Abbiamo seguito il metodo di Ofa e formulato tutti i compiti in un formato sequenza-alle-sequenza unificato, in cui il testo di input, le immagini, le istruzioni e le caselle di delimitazione sono rappresentate nello stesso spazio di token.</sample>
    <sample id="519">Ok, ora parlerò di ottimizzazione delle istruzioni multimodali.</sample>
    <sample id="520">Quindi, per il set di dati di allenamento, usiamo cinquantatré compiti dal gruppo net per l'allenamento. E campioniamo diecimila istanze per compito. Per il test, riserviamo l'intero gruppo di ragionamento comune per il test. E selezioniamo altri cinque compiti dal gruppo Vqa e del malizioso.</sample>
    <sample id="521">Usiamo tutte le istanze nel flusso di test per ogni attività. Inoltre, campioniamo a caso venti attività dal flusso di test di istruzioni naturali, come su Sy Task for NLP.</sample>
    <sample id="522">Quindi usiamo un modello pre-addestrato, o un modello di grandi dimensioni, come modello di base. Durante l'addestramento, mescoliamo tutte le istanze per tutte le attività. Ogni istanza viene combinata casualmente con una delle sue cinque modelli di istruzioni.</sample>
    <sample id="523">Quindi, durante il test, per ogni attività, conduciamo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni in ciascun esperimento.</sample>
    <sample id="524">Riferiamo la media e la massima prestazione e la deviazione standard delle prestazioni in tutti e cinque gli esperimenti.</sample>
    <sample id="525">Se il compito è un compito di classificazione multi-modale, riportiamo l'accuratezza. Se è un compito di generazione multi-modale, riportiamo il ragionamento. Per un compito di elaborazione del linguaggio naturale, riportiamo anche il ragionamento.</sample>
    <sample id="526">Abbiamo anche introdotto una metrica di valutazione aggiuntiva chiamata sensibilità. Quindi questo misura la capacità del modello di produrre costantemente gli stessi output per lo stesso compito, indipendentemente dalla variazione della selezione nella valutazione dell'istruzione.</sample>
    <sample id="527">Ecco il nostro risultato principale. Come possiamo vedere, l'ottimizzazione delle istruzioni può migliorare significativamente le prestazioni di Os Os, in particolare su compiti multimodali.</sample>
    <sample id="528">Anche l'apprendimento transferibile da un set di dati di istruzioni naturali può beneficiare dell'ottimizzazione delle istruzioni.</sample>
    <sample id="529">Qui possiamo vedere, man mano che aumenta la quantità di compito, il modello ottiene prestazioni migliori e, nel frattempo, una sensibilità inferiore.</sample>
    <sample id="530">Quindi abbiamo anche fatto un esperimento. Abbiamo usato un'istruzione contro cinque istruzioni. Come possiamo vedere, usare più istruzioni può migliorare le prestazioni complessive del modello e ridurre la sua sensibilità molto.</sample>
    <sample id="531">Quindi questo mostra l'effetto di diverse strategie di fine tuning sulla sensibilità del modello. Come possiamo vedere, attraverso l'apprendimento di trasferimento da un set di dati di istruzioni naturali, il modello può raggiungere una sensibilità molto migliore rispetto al modello originale.</sample>
    <sample id="532">Possiamo anche vedere che l'apprendimento trasferito dal set di dati di istruzione naturale può aiutare Ofa a ottenere prestazioni molto migliori sul set di dati di istruzione naturale.</sample>
    <sample id="533">Quindi, nel complesso, abbiamo proposto il primo set di dati di ottimizzazione delle istruzioni su larga scala a più modelli. Abbiamo significativamente migliorato la capacità zero-shot di Ofa. E abbiamo esplorato diverse tecniche di trasferimento di apprendimento e mostravamo i loro benefici. Abbiamo progettato una nuova metrica chiamata sensibilità.</sample>
    <sample id="534">Quindi un'altra cosa. Stiamo raccogliendo un set di dati di regolazione delle istruzioni multimodale molto più grande con circa uno cinquanta compiti aggiuntivi in linguaggio visivo. E li rilasceremo. Quindi questo è un codice qr per i nostri dati e il modello. Grazie.</sample>
    <sample id="535">Sarah Papi è dell'Università di Trento e della Fondazione Bruno Kessler, e Matteo Negri e Marco Turchi sono i coautori dell'articolo.</sample>
    <sample id="536">Javad Hosseini</sample>
    <sample id="562">Ciao a tutti. Sono Kaustubh Sinha, e sono lieto di darvi il benvenuto al nostro discorso del nostro documento acl twenty twenty three, language model acceptability judgments are not always robust to context.</sample>
    <sample id="563">È un lavoro congiunto con John Gauthier, Aaron Mueller, Kanishka Mishra, Karen Fentress, Roger Levy e Atina Walia.</sample>
    <sample id="564">Quindi, in questo lavoro, rivisitiamo il paradigma del paio minimo.</sample>
    <sample id="565">Quindi il paio minimo al paradigma fondamentalmente valuta i modelli linguistici in cima ai giudizi di accettabilità, che possono anche includere grammatica, come sintassi, o accettabilità in termini di stereotipi, come coppie di scorrimento.</sample>
    <sample id="566">E in questo paradigma di coppia minima, il modo tipico per valutare i modelli linguistici è quello di mostrare, ad esempio, una frase accettabile o una frase grammaticale, e poi mostrare una frase non accettabile o una frase non grammaticale.</sample>
    <sample id="567">E poi la speranza è che il modello fondamentalmente dia più probabilità al set di accettabile.</sample>
    <sample id="568">La pipeline mp attuale fondamentalmente non ci permette di valutare l'accettazione dei modelli verso frasi più lunghe.</sample>
    <sample id="569">In questi giorni, i modelli di grandi lingue stanno arrivando con finestre di contesto sempre più lunghe. Quindi è fondamentale valutare l'accettabilità del modello in tutto il contesto.</sample>
    <sample id="570">E questo è ciò che stiamo cercando di fare qui. Stiamo cercando di rivedere la pipeline mpb chiedendo al modello di valutare l'accettabilità su sequenze più e più lunghe.</sample>
    <sample id="571">Quindi questo è l'approccio. Quindi quello che facciamo è simulare queste sequenze più lunghe, rivedere i set di dati stessi, e poi ricreare frasi scegliendo frasi accettabili o inaccettabili da quei set di dati.</sample>
    <sample id="572">Quindi, ad esempio, qui abbiamo scelto come un paio tipico di grammaticalità dal set di dati blimp, dal caso islando aggiuntivo.</sample>
    <sample id="573">E quello che facciamo è quello di ricreare sequenze più lunghe, che sono accettabili e che hanno la stessa corrispondenza della struttura grammaticale. Estraiamo frasi grammaticali dall'italiano.</sample>
    <sample id="574">E poi lo aggiungiamo come prefisso sia alla query accettabile che alla query inaccettabile.</sample>
    <sample id="575">Quindi possiamo fare la stessa cosa scegliendo frasi inaccettabili dalla stessa corrispondenza. E questo potrebbe anche essere usato per testare l'accettabilità del modello.</sample>
    <sample id="576">E possiamo anche fare lo stesso scegliendo frasi da un sottoinsieme diverso o un set di dati diverso. Quindi questo è ciò che chiamiamo lo scenario di non corrispondenza.</sample>
    <sample id="577">Quindi qui le frasi provengono ancora da un set di dati rilevante, ma non è dallo stesso set di dati che stai valutando. E possiamo fare lo stesso per i casi di inaccettabilità.</sample>
    <sample id="578">Infine, possiamo scegliere frasi da un dominio completamente non correlato, come Wikipedia.</sample>
    <sample id="579">Quindi questo ci dirà se i giudizi di accettabilità dei modelli sono effettivamente influenzati da qualsiasi contesto.</sample>
    <sample id="580">Come se il contesto provenisse da un diverso sottoinsieme del set di dati, o se è completamente irrilevante per il contesto attuale che stiamo esaminando.</sample>
    <sample id="581">Quindi, come funziona il modello? Quindi, prima, guardiamo alle frasi di Wikipedia, che sono completamente irrilevanti per la query corrente. E poi troviamo che i giudizi mp sono per lo più robusti per contesti arbitrari.</sample>
    <sample id="582">Aumentiamo la lunghezza del contesto fino a duemilaventiquattro per massimizzare i modelli opt e gpt due. E abbiamo visto qui nella linea a dischi arancioni, i giudizi mp sono relativamente stabili.</sample>
    <sample id="583">Ora cosa succede quando scegliamo frasi dalla stessa fonte?</sample>
    <sample id="584">Quindi qui stiamo scegliendo o creando frasi da domini accettabili e inaccettabili, dal set di dati di sintassi di blimp.</sample>
    <sample id="585">E poi vediamo che i giudizi di Mpp aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o non accettabili.</sample>
    <sample id="586">Ma quando corrispondiamo la struttura, cioè quando scegliamo le frasi dalla stessa fenomenologia nel testo di persona incolpante, Jim,</sample>
    <sample id="587">Vediamo un enorme aumento o una grande diminuzione del giudizio mp per il modello, a seconda che il prefisso scelto sia accettabile o non accettabile.</sample>
    <sample id="588">Ora questo, e questo è molto grande. Come questo effetto aumenta attraverso l'intero contesto. E questo probabilmente influenzerebbe, come, i modelli linguistici più recenti, che ha un grande contesto.</sample>
    <sample id="589">Allora perché il prefisso corrispondente influisce così tanto sul giudizio del modello linguistico?</sample>
    <sample id="590">Quindi abbiamo fatto una serie di analisi in cui abbiamo cercato di perturbare la frase di input cercando di preservare la struttura rilevante aggiungendo rumore al loro input. E dopo aver fatto come diverse di queste perturbazioni,</sample>
    <sample id="591">Scopriamo che nessuno di questi rumori sta effettivamente facendo cambiare il modello, come cambiare il corso in termini di come ci mostra il modello di giudizio di MP.</sample>
    <sample id="592">Fondamentalmente, scopriamo che i modelli sono sensibili alle frasi perturbate in modi simili.</sample>
    <sample id="593">Cioè, quando perturbiamo le frasi nel dominio accettabile, vediamo un aumento simile in tutte le perturbazioni. E quando perturbiamo le frasi nel dominio non accettabile, vediamo una diminuzione dei giudizi mp in modo simile.</sample>
    <sample id="594">Quindi le principali conclusioni del nostro lavoro è che i modelli linguistici sono sensibili alle caratteristiche sintattiche e semantiche latenti che sono condivise tra le frasi.</sample>
    <sample id="595">E l'analisi MPP, il modo in cui lo facciamo attualmente con input brevi e singoli frasi, potrebbe non catturare completamente la conoscenza astratta del modello linguistico nell'intero finestra di contesto.</sample>
    <sample id="596">Si prega di leggere il nostro documento per ulteriori dettagli dei nostri esperimenti. Grazie per l'attenzione.</sample>
    <sample id="597">Token di output</sample>
    <sample id="598">Coscript rappresenta 55.000 script.</sample>
    <sample id="626">Il metodo di allineamento migliore per DEplain è il metodo di mass align.</sample>
    <sample id="627">L'apprendimento scarsamente supervisionato consente di costruire modelli robusti che generalizzano bene anche quando ci sono rumori di etichettatura.</sample>
    <sample id="628">L'allocazione è stata eseguita utilizzando un metodo di allineamento automatico, specificamente il metodo di allineamento di Procrustes, che ha permesso di allineare i documenti in DEplain-web con i metodi di allineamento manuali.</sample>
    <sample id="629">Il set di dati CoNLL++ è stato creato raccogliendo notizie da Reuters News del 2020 e annotandole secondo le linee guida di annotazione CoNLL 2003.</sample>
    <sample id="630">Ciao a tutti. Mi chiamo Yu Xin Zhang della Penn State University. Oggi presenterò il nostro lavoro, esempio, analisi semantica cross-lingua in più lingue naturali e rappresentazioni mentali.</sample>
    <sample id="631">Quindi l'analisi semantica è un compito per costruire rappresentazioni semantiche di query utente, come sql e lambda calculus.</sample>
    <sample id="632">E la semantica linguistica trasversale è il compito di tradurre le query in più lingue naturali in più rappresentazioni di significato.</sample>
    <sample id="633">Come mostrato in questa figura, dobbiamo tradurre la query in più lingue naturali usando modelli neurali, due sql, lambda o funql e così via.</sample>
    <sample id="634">I modelli di analisi semantica translinguistica esistenti sono proposti e valutati separatamente su un set di dati di compiti e applicazioni limitati. Ad esempio,</sample>
    <sample id="635">Ci sono lacune di copertura su certi linguaggi naturali. Il cinese manca. E</sample>
    <sample id="636">Clicca sulla copertura su determinate mini rappresentazioni.</sample>
    <sample id="637">Manca il lambda calculus.</sample>
    <sample id="638">O sono valutati solo su un certo modello neurale. Ad esempio, c'è solo un modello singolo per valutare.</sample>
    <sample id="639">A questo scopo, proponiamo l'esemplare, ma forniamo un set di dati uniforme, l'esemplare per il parsing semantico crosslingual in più lingue naturali e rappresentazioni.</sample>
    <sample id="640">Contiene novanta set di vari domini, cinque task di analisi semantica, otto rappresentazioni di milioni e ventidue lingue naturali in quindici famiglie linguistiche.</sample>
    <sample id="641">E per valutare meglio il nostro benchmark, abbiamo considerato le sei impostazioni per l'allenamento e la valutazione.</sample>
    <sample id="642">Il primo è il test di traduzione. Useremo l'API di Google Translate per tradurre la fonte nella lingua target. Quindi useremo il modello monolingue per addestrare e valutare.</sample>
    <sample id="643">E per esempio, addestriamo il modello inglese su una query inglese. E durante l'inferenza, tradurremo la query tedesca usando api in inglese, e poi useremo il modello addestrato per prevedere il sql.</sample>
    <sample id="644">E abbiamo anche testato il modello monolingue.</sample>
    <sample id="645">In questo contesto, il linguaggio sorgente è lo stesso del linguaggio di destinazione. Ad esempio, tedesco a tedesco o inglese a inglese.</sample>
    <sample id="646">Abbiamo anche testato l'impostazione del campo monolingue, ma addestrando modelli monolingue con solo il dieci percento dei dati di addestramento.</sample>
    <sample id="647">E che ha il modello multilingue, che abbiamo addestrato un modello multilingue per tutte le lingue.</sample>
    <sample id="648">Ad esempio, mettiamo insieme le query in tedesco, inglese, cinese per addestrare un modello multilingue. E durante l'inferenza, possiamo usare questo modello.</sample>
    <sample id="649">Per tradurre domande in tedesco o in cinese o eccetera.</sample>
    <sample id="650">E consideriamo anche il trasferimento crosslingual zero shot e few shot. Abbiamo addestrato su un linguaggio sorgente e trasferito in un altro linguaggio.</sample>
    <sample id="651">Quindi, durante l'allenamento, stiamo allenando su query inglesi, o la combinazione di query inglesi e tedesche, per addestrare un modello multilingue per prevedere l'output sql.</sample>
    <sample id="652">E troviamo anche molti risultati interessanti. Quindi, per quanto riguarda l'analisi di modelli monolingue, valutiamo due gruppi di modelli.</sample>
    <sample id="653">Compreso encoder pdr, che sta per encoderi preimparati multilingue con decoder basati su puntatori, come xlmr più pdr, mbert più pdr.</sample>
    <sample id="654">E valutiamo anche i modelli encoder-decoder, che sono encoder-decoder multilingue addestrati, come mbart e mt five.</sample>
    <sample id="655">Abbiamo scoperto che l'encoder, il decoder ottiene le migliori prestazioni su tutti e nove i set di dati.</sample>
    <sample id="656">E valutiamo su mt five e esempio xlmr plus pdr su impostazione multilingue.</sample>
    <sample id="657">Abbiamo scoperto che encoder, decoder o encoder pdr possono essere migliorati attraverso l'allenamento in una miscela di vari linguaggi.</sample>
    <sample id="658">E abbiamo scoperto che è perché la maggior parte delle principali lingue naturali può ottenere un guadagno di prestazioni, tranne che il rendimento inglese diminuisce in sette set di dati e guadagna solo in tre set di dati.</sample>
    <sample id="659">Penso che questo sia noto come maledizione della multilingue.</sample>
    <sample id="660">Abbiamo anche confrontato il guadagno delle prestazioni di Crosslink.</sample>
    <sample id="661">In questa figura, la linea blu è il trasferimento cross-lingua few-shot, la linea arancione è il trasferimento cross-lingua zero-shot, mentre la linea verde è l'impostazione monolingue.</sample>
    <sample id="662">Abbiamo scoperto che confrontando la linea verde e quella arancione, abbiamo scoperto che per l'impostazione zero short, il divario di prestazione del trasferimento crosslingual è significativo. E confrontando la linea blu e quella arancione, abbiamo scoperto che per l'impostazione few short, il divario di trasferimento è ridotto rapidamente.</sample>
    <sample id="663">Troviamo anche alcuni altri risultati interessanti. Ad esempio, l'encoder decoder supera il lavoro di pre-questo, o raggiunge risultati comparabili. Per l'allenamento sulla lingua naturale inglese, può aumentare significativamente le prestazioni di fewshot su lingue target.</sample>
    <sample id="664">E abbiamo scoperto che i modelli linguistici multilingue, come Codas e Blue, sono ancora inadeguati per le attività di analisi semantica translinguistica.</sample>
    <sample id="665">In sintesi, abbiamo costruito Exemplar, un benchmark unificato per il parsing semantico a angolo incrociato con più lingue naturali e rappresentazioni medie.</sample>
    <sample id="666">Abbiamo condotto uno studio di benchmark completo su tre tipi rappresentativi di modelli linguistici multilingue. E i nostri risultati mostrano molti risultati interessanti e così via. E benvenuti a visitare il nostro documento e codice. Grazie per l'ascolto.</sample>
    <sample id="667">I lavori connessi in tal senso sono:

1. "The Art of the Book" di John M. Traill
2. "The Book of the Book" di John M. Traill
3. "The Book of the Book" di John M. Traill
4. "The Book of the Book" di John M. Traill
5. "The Book of the Book" di John M. Traill
6. "The Book of the Book" di John M. Traill
7. "The Book of the Book" di John M. Traill
8. "The Book of the Book" di John M. Traill
9. "The Book of the Book" di John M. Traill
10. "The Book of the Book" di John M. Traill
11. "The Book of the Book" di John M. Traill
12. "The Book of the Book" di John M. Traill
13. "The Book of the Book" di John M. Traill
14. "The Book of the Book" di John M. Traill
15. "The Book of the Book" di John M. Traill
16. "The Book of the Book" di John M. Traill
17. "The Book of the Book" di John M. Traill
18. "The Book of the Book" di John M. Traill
19. "The Book of the Book" di John M. Traill
20. "The Book of the Book" di John M. Traill
21. "The Book of the Book" di John M. Traill
22. "The Book of the Book" di John M. Traill
23. "The Book of the Book" di John M. Traill
24. "The Book of the Book" di John M. Traill
25. "The Book of the Book" di John M. Traill
26. "The Book of the Book" di John M. Traill
27. "The Book of the Book" di John M. Traill
28. "The Book of the Book" di John M. Traill
29. "The Book of the Book" di John M. Traill
30. "The Book of the Book" di John M. Traill
31. "The Book of the Book" di John M. Traill
32. "The Book of the Book" di John M. Traill
33. "The Book of the Book" di John M. Traill
34. "The Book of the Book" di John M. Traill
35. "The Book of the Book" di John M. Traill
36. "The Book of the Book" di John M. Traill
37. "The Book of the Book" di John M. Traill
38. "The Book of the Book" di John M. Traill
39. "The Book of the Book" di John M. Traill
40. "The Book of the Book" di John M. Traill
41. "The Book of the Book" di John M. Traill
42. "The Book of the Book" di John M. Traill
43. "The Book of the Book" di John M. Traill
44. "The Book of the Book" di John M. Traill
45. "The Book of the Book" di John M. Traill
46. "The Book of the Book" di John M. Traill
47. "The Book of the Book" di John M. Traill
48. "The Book of the Book" di John M. Traill
49. "The Book of the Book" di John M. Traill
50. "The Book of the Book" di John M. Traill
51. "The Book of the Book" di John M. Traill
52. "The Book of the Book" di John M. Traill
53. "The Book of the Book" di John M. Traill
54. "The Book of the Book" di John M. Traill
55. "The Book of the Book" di John M. Traill
56. "The Book of the Book" di John M. Traill
57. "The Book of the Book" di John M. Traill
58. "The Book of the Book" di John M. Traill
59. "The Book of the Book" di John M. Traill
60. "The Book of the Book" di John M. Traill
61. "The Book of the Book" di John M. Traill
62. "The Book of the Book" di John M. Traill
63. "The Book of the Book" di John M. Traill
64. "The Book of the Book" di John M. Traill
65. "The Book of the Book" di John M. Traill
66. "The Book of the Book" di John M. Traill
67. "The Book of the Book" di John M. Traill
68. "The Book of the Book" di John M. Traill
69. "The Book of the Book" di John M. Traill
70. "The Book of the Book" di John M. Traill
71. "The Book of the Book" di John M. Traill
72. "The Book of the Book" di John M. Traill
73. "The Book of the Book" di John M. Traill
74. "The Book of the Book" di John M. Traill
75. "The Book of the Book" di John M. Traill
76. "The Book of the Book" di John M. Traill
77. "The Book of the Book" di John M. Traill
78. "The Book of the Book" di John M. Traill
79. "The Book of the Book" di John M. Traill
80. "The Book of the Book" di John M. Traill
81. "The Book of the Book" di John M. Traill
82. "The Book of the Book" di John M. Traill
83. "The Book of the Book" di John M. Traill
84. "The Book of the Book" di John M. Traill
85. "The Book of the Book" di John M. Traill
86. "The Book of the Book" di John M. Traill
87. "The Book of the Book" di John M. Traill
88. "The Book of the Book" di John M. Traill
89. "The Book of the Book" di John M. Traill
90. "The Book of the Book" di John M. Traill
91. "The Book of the Book" di John M. Traill
92. "The Book of the Book" di John M. Traill
93. "The Book of the Book" di John M. Traill
94. "The Book of the Book" di John M. Traill
95. "The Book of the Book" di John M. Traill
96. "The Book of the Book" di John M. Traill
97. "The Book of the Book" di John M. Traill
98. "The Book of the Book" di John M. Traill
99. "The Book of the Book" di John M. Traill
100. "The Book of the Book" di John M. Traill</sample>
    <sample id="668">No, non lo sono.</sample>
    <sample id="695">Il metodo utilizza la continua relaxazione per trovare le permutazioni linguisticamente più plausibili.</sample>
    <sample id="696">L'equità di un modello NLP a valle si riferisce alla sua capacità di trattare equamente tutti i gruppi demografici, evitando pregiudizi e discriminazioni.</sample>
    <sample id="697">La relatrice si chiama Yanis Lavraik.</sample>
    <sample id="698">Il nome della relatrice è Kaustubh Sinha.</sample>
    <sample id="699">Il nome della relatrice è Myra.</sample>
    <sample id="700">Nel contesto di questo articolo, il tropicalismo indica un tropo che descrive le donne di colore, in particolare le donne latine, come vibranti e vivaci, suggerendo un'atmosfera o un'atmosfera vivace e accogliente.</sample>
    <sample id="701">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target enfatizzando le loro identità culturale e tradizionale, distinguendoli dal normale bianco.</sample>
    <sample id="702">Pointwise cXMI</sample>
    <sample id="703">DrBERT è un modello trainato con 7 GB di corpus naturale, mentre ChuBERT è un modello clinico trainato con 4 GB di note cliniche.</sample>
    <sample id="751">Due.</sample>
    <sample id="752">Il trasferimento iterativo dell'apprendimento è un processo in cui un modello viene aggiornato iterativamente con nuovi dati, affinando continuamente le sue prestazioni.</sample>
    <sample id="753">L'obiettivo del set di dati è di capire il linguaggio degli utenti quando vogliono fare una scelta.</sample>
    <sample id="754">Un utente malintenzionato può sfruttare l'API dell'API del servizio per inviare richieste non autorizzate o manipolare le richieste, il che potrebbe portare a una visualizzazione non sicura o a un accesso non autorizzato ai dati.</sample>
    <sample id="755">Due.</sample>
    <sample id="756">12 annotatori.</sample>
    <sample id="757">Sebastian Santi, Rohan Lodha, Caterina Ranecka e Martin Sabou.</sample>
    <sample id="758">L'esempio in cui il governatore è a sinistra è "I saw Bart and Lisa".</sample>
    <sample id="759">I modelli all'avanguardia nei sistemi di dialogo sono quelli che sono stati sviluppati per migliorare l'interazione umana con le macchine, come GPT-3, BERT e T5. Questi modelli sono stati progettati per comprendere e generare linguaggio naturale, rendendoli utili in una varietà di applicazioni, come assistenza vocale, traduzione, e supporto ai chatbot.</sample>
    <sample id="760">La necessità di valutare l'accettabilità dei modelli nell'intera finestra di contesto deriva dal fatto che i modelli di grandi dimensioni stanno avendo un contesto più ampio, rendendo cruciale garantire che i loro risultati siano coerenti e affidabili su tutta la lunghezza del contesto.</sample>
    <sample id="761">Sì, la formazione attraverso la modalità multilingue ha causato un calo delle prestazioni rispetto al modello inglese monolingue.</sample>
    <sample id="762">Sì, gli annotatori conoscono l'entità in anticipo.</sample>
    <sample id="763">Le metriche di MT utilizzate per la valutazione sono F1, coerenza e diversità.</sample>
    <sample id="764">Sì, il regresso nella generalizzazione influisce su specifici tipi di NER.</sample>
    <sample id="765">La posizionalità nella NLP è importante perché aiuta a comprendere il contesto e il significato delle parole all'interno di una frase, migliorando la precisione della comprensione e del trattamento del linguaggio naturale.</sample>
    <sample id="766">Sì, gli LLM multilingue come BLOOM sono stati affinati tramite adattatori e messa a punto integrale.</sample>
    <sample id="767">Il modello utilizzato per il trasferimento dell'apprendimento è il modello che ottiene il miglior risultato dopo la fine-tuning iterativo di CEE e Debut.</sample>
    <sample id="768">I recenti set di test utilizzati per valutare le capacità di PaLM includono l'English BERT (Bidirectional Encoder Representations from Transformers) e il SQuAD (Stanford Question Answering Dataset). Questi test sono progettati per valutare la comprensione e la generazione di testo da parte del modello.</sample>
    <sample id="769">Tre.</sample>
    <sample id="770">Il metodo proposto consente di creare modelli più piccoli e specializzati per la pianificazione del linguaggio con un'alta precisione, il che è un vantaggio rispetto al metodo di riferimento.</sample>
    <sample id="771">Il nome della relatrice è Shuheng.</sample>
    <sample id="772">Sì, i risultati e il set di dati nell'articolo possono essere utilizzati come parametri di riferimento.</sample>
    <sample id="773">Nell'articolo, tre modelli più piccoli vengono utilizzati: T5, GPT-2 e BERT.</sample>
    <sample id="774">Il modello di base utilizzato è OFA, un modello di istruzioni multimodale unificato.</sample>
    <sample id="833">L'articolo "Prompting Prompt for Translation: Assessing Strategies and Performance" è un lavoro congiunto di Ayed Bilal e colleghi di Google Translate.</sample>
    <sample id="834">L'articolo è stato scritto da Vasudha, S. K. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prabha, S. S. Prab</sample>
    <sample id="835">Insieme al cinese, l'articolo analizza le coppie linguistiche inglese-inglese e cinese-cinese.</sample>
    <sample id="836">Il nome della relatrice è Shang Bing.</sample>
    <sample id="837">Modelli di lunghezza e normale.</sample>
    <sample id="838">Per l'addestramento, 53 attività vengono utilizzate, mentre per il test, 5 attività vengono utilizzate.</sample>
    <sample id="839">Due autori sono coinvolti nell'articolo.</sample>
    <sample id="840">Gli autori hanno condotto esperimenti su quattro set di dati: Agnews, MIND, SSTD2 e IRAS-PM.</sample>
    <sample id="876">NACHOS è un set di dati medico che il modello Dr. Bert è stato addestrato.</sample>
    <sample id="877">Il nome della relatrice è Ayed Bilal.</sample>
    <sample id="878">La strategia del prompting influisce significativamente sui risultati, come dimostrato da un esperimento in cui due diversi prompt sono stati utilizzati per una frase, influenzando le prestazioni delle LLMs per la traduzione.</sample>
    <sample id="879">Patrick Fernholz, Emile Niu, Andre F. de Martens e Graham Neubig.</sample>
    <sample id="880">1. Raccogliere un set di dati più ampio per l'allenamento del modello
2. Aggiungere circa 150 compiti di linguaggio visivo
3. Rilasciare i compiti aggiuntivi
4. Fornire un codice QR per i dati e il modello
5. Ringraziare per l'attenzione</sample>
    <sample id="881">Progettano un compito di risoluzione delle co-referenze per valutare la capacità di sfruttare conoscenze da diverse fonti.</sample>
    <sample id="882">Ciao a tutti. Mi chiamo Ayed Bilal e darò una breve recensione del documento, prompting for translation, assessing strategies and performance. Questo è un lavoro congiunto con i miei colleghi di Google Translate.</sample>
    <sample id="883">Param è un modello di linguaggio di parametri di cinque quaranta miliardi di parametri presentato l'anno scorso, nel duemilaventidue. È addestrato su una grande raccolta di testo, composta da settecentottanta miliardi di token.</sample>
    <sample id="884">Al momento della pubblicazione, ha raggiunto lo stato dell'arte in centinaia di compiti NLP.</sample>
    <sample id="885">In questo lavoro, presentiamo lo studio sistematico del prompt di un modello linguistico per la traduzione automatica.</sample>
    <sample id="886">Abbiamo valutato la capacità di traduzione di tali modelli utilizzando le migliori pratiche della comunità imt. Ciò comporta l'uso degli ultimi set di test per evitare un sovrapposizione dei dati di test con i dati di allenamento del modello linguistico.</sample>
    <sample id="887">E confrontiamo due sistemi all'avanguardia, i migliori sistemi di prestazione, o la valutazione Wmt.</sample>
    <sample id="888">Usiamo le ultime metriche neurali e L M T e, inoltre, mostriamo anche le valutazioni esperte basate sull'uomo. Infine, forniamo alcune raccomandazioni per le strategie di selezione dei prompt.</sample>
    <sample id="889">Il prompting ha un grande influenza sulle prestazioni degli llm per la traduzione. Come possiamo vedere in un semplice esperimento, dove abbiamo usato un prompt di un colpo e abbiamo fornito due prompt diversi per ogni frase.</sample>
    <sample id="890">La maggioranza delle frasi, cinquecentosedici su mille, la differenza osservata è di più di un punto di sfocatura.</sample>
    <sample id="891">E questo può andare in casi estremi fino a quaranta punti di errore. Quindi è importante selezionare una buona strategia di prompting.</sample>
    <sample id="892">Nei nostri esperimenti, abbiamo scelto una strategia di promemoria a cinque colpi, in cui abbiamo semplicemente contrassegnato ogni frase che abbiamo fornito al sistema con il linguaggio in cui è.</sample>
    <sample id="893">Traduci il contenuto inglese in italiano.</sample>
    <sample id="894">Abbiamo visto che la forma effettiva della prompting non ha un grande influenza nel caso di several-shot prompting.</sample>
    <sample id="895">È cruciale per la prompting zero e one shot. E quando andiamo, come nel nostro caso, alla prompting five shot, non c'è quasi alcuna differenza nella forma effettiva della prompting.</sample>
    <sample id="896">Sono gli esempi che portano la maggior parte del peso.</sample>
    <sample id="897">La sintesi dei nostri risultati sperimentali è che la qualità dell'esempio è più importante della somiglianza con la frase di origine.</sample>
    <sample id="898">Quindi è importante selezionare gli esempi dalle traduzioni di alta qualità. In particolare, confrontiamo la selezione di prompt dai dati di allenamento delle valutazioni Wmt o dai dati di sviluppo.</sample>
    <sample id="899">I dati di profondità sono molto più accurati e di qualità superiore rispetto ai dati di allenamento, che sono più grossolani. E i risultati mostrano un migliore rendimento quando si utilizzano i dati di profondità.</sample>
    <sample id="900">Tuttavia, i sistemi specializzati in stato dell'arte hanno un vantaggio sostanziale rispetto alle traduzioni di PUM. Ma PUM è abbastanza vicino a un sistema commerciale. Nel nostro caso, abbiamo scelto di sovrapporre con Google Translate.</sample>
    <sample id="901">Le intuizioni che abbiamo ottenuto dall'innervoluzione che abbiamo eseguito usando il framework mpm, è che la fluidità del palmo è paragonabile ai sistemi all'avanguardia. Ma la differenza principale deriva dall'accuratezza.</sample>
    <sample id="902">In particolare, gli errori più comuni sono gli errori di omissione.</sample>
    <sample id="903">Quindi sembra che il palm scelga di produrre una traduzione che suona meglio, a volte lasciando cadere parti della frase originale nella traduzione.</sample>
    <sample id="904">Tuttavia, la categoria dello stile sgradevole per il pun è inferiore a quella per i sistemi all'avanguardia, che è un segnale aggiuntivo.</sample>
    <sample id="905">Che il parm fornisce un output davvero fluido, ma ancora con alcuni problemi di precisione.</sample>
    <sample id="906">E questo è tutto per questa breve panoramica. Per maggiori dettagli, per favore, venite alla presentazione completa del documento. Grazie mille.</sample>
    <sample id="907">Ciao. Sono da Wei, uno studente di dottorato presso la Saarland University in Germania. In questo video, vorrei presentare il nostro lavoro recente, più debole di quanto pensi, uno sguardo critico alla formazione a settimana.</sample>
    <sample id="908">Questo è un lavoro congiunto con Xiao Yusen, Mario Smusba, Gerd Stephan e Dietrich Klakow.</sample>
    <sample id="909">Vorrei iniziare con una breve introduzione alla supervisione debole e alla regressione debole.</sample>
    <sample id="910">Nella supervisione debole, non etichettiamo manualmente i dati. Invece, etichettiamo i dati usando fonti di etichettatura debole, come semplici regole euristiche, basi di conoscenza o low quality crowdsourcing, come illustrato nella figura a destra.</sample>
    <sample id="911">Rispetto alle annotazioni umane, le annotazioni settimanali sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità delle annotazioni sono errate.</sample>
    <sample id="912">Se addestriamo direttamente le reti neurali sui dati etichettati settimanalmente, le reti neurali tendono a memorizzare il rumore di etichetta e non generalizzare.</sample>
    <sample id="913">Nell'apprendimento supervisionato, vengono proposti algoritmi di addestramento per addestrare robustamente le reti neurali in presenza di tale rumore di etichetta, in modo che i modelli addestrati generalizzino ancora bene.</sample>
    <sample id="914">In lavori recenti in Wsl. Quindi Wsl sta per learning supervised a settimana. Una dichiarazione comune è che le persone dicono che addestrano solo modelli sui dati etichettati settimanalmente e ottengono prestazioni elevate sui set di test puliti.</sample>
    <sample id="915">Tecnicamente questa affermazione non è sbagliata, ma c'è un'eccezione.</sample>
    <sample id="916">Cioè, le persone assumono che ci sia un set di validazione aggiuntivo e pulito disponibile per la selezione del modello.</sample>
    <sample id="917">Non possiamo dubitare di questo problema, poiché ciò implica che sono necessarie annotazioni manuali aggiuntive nell'apprendimento supervisionato a settimana. Ma come un elefante nella stanza, questa necessità viene spesso trascurata.</sample>
    <sample id="918">Il dubbi precedente ci porta a porre tre domande di ricerca. Prima, è necessario un set di dati di validazione pulito per Wsl? O forse possiamo usare invece un set di validazione rumoroso?</sample>
    <sample id="919">In secondo luogo, se i dati puliti sono richiesti, o se i dati puliti sono obbligatori affinché WSL funzioni, allora di quanti campioni puliti abbiamo bisogno? Infine, dovremmo usare solo i campioni puliti per la convalida? O ci sono modi migliori per utilizzarli?</sample>
    <sample id="920">Abbiamo affrontato queste domande di ricerca nel nostro lavoro, e le nostre scoperte sono le seguenti.</sample>
    <sample id="921">Innanzitutto, scopriamo che, curiosamente, i metodi recenti wsl in effetti richiedono campioni di dati puliti per funzionare correttamente.</sample>
    <sample id="922">Altrimenti, c'è un grande calo delle prestazioni, come mostrato in questa figura. Se non ci sono campioni di validazione puliti, allora i modelli addestrati non possono generalizzare oltre le etichette originali.</sample>
    <sample id="923">Significa che l'allenamento è inutile.</sample>
    <sample id="924">Ciò indica che gli approcci WSL in realtà richiedono dati etichettati in modo pulito per funzionare correttamente. E il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato.</sample>
    <sample id="925">La nostra seconda scoperta è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci WSL a raggiungere prestazioni migliori, come mostrato nella figura a sinistra.</sample>
    <sample id="926">In genere, abbiamo bisogno di solo venti campioni per classe per ottenere prestazioni elevate.</sample>
    <sample id="927">Ma non è la fine della storia. Perché se in entrambi i casi decidiamo di accedere a campioni puliti, allora addestrarli direttamente raggiungerà prestazioni ancora migliori.</sample>
    <sample id="928">La figura a destra mostra la differenza di prestazioni tra approcci di ottimizzazione fine, che vengono applicati direttamente sui dati puliti, e approcci wsl, che utilizzano i dati puliti solo per la convalida.</sample>
    <sample id="929">Come possiamo vedere, se abbiamo dieci campioni per classe, la fine tuning diretta inizia a battere gli approcci wsl.</sample>
    <sample id="930">Infine, il miglioramento delle prestazioni dichiarato negli approcci WSL precedenti può essere facilmente raggiunto consentendo di continuare la messa a punto sui campioni di validazione puliti.</sample>
    <sample id="931">Come possiamo vedere dalle figure, il modello Valina, chiamato ftw, inizialmente sottoperforma metodi più complicati come cosine.</sample>
    <sample id="932">Tuttavia, se consentiamo di continuare a perfezionare i campioni puliti, allora ftw funziona altrettanto bene come altri metodi.</sample>
    <sample id="933">Quindi, in pratica, non c'è motivo di scegliere metodi wsl più complessi, che richiedono più tempo di calcolo e spazio su disco.</sample>
    <sample id="934">In sintesi, abbiamo dimostrato che gli approcci recenti wsl richiedono campioni annotati manualmente per funzionare correttamente. Il loro guadagno di prestazioni e la praticità sono fortemente sopravvalutati.</sample>
    <sample id="935">Le nostre raccomandazioni concrete per il lavoro futuro sono le seguenti.</sample>
    <sample id="936">Per prima cosa, segnalare i criteri di selezione del modello. Ad esempio, segnalare se la selezione del modello è stata effettuata con campioni di validazione ben puliti.</sample>
    <sample id="937">In secondo luogo, gli approcci Wsl dovrebbero essere confrontati con le linee di base di apprendimento a breve termine, poiché entrambi funzionano su campioni chiari. In terzo luogo, la fine tuning continua è una linea di base semplice ma forte che dovrebbe essere considerata in lavori futuri in Wsl.</sample>
    <sample id="938">Infine, abbiamo aperto il codice. Puoi trovarlo tramite il codice Q R su questa diapositiva. Per favore, sentiti libero di controllarlo. Grazie e goditi la conferenza.</sample>
    <sample id="939">I metodi di valutazione comuni per i sistemi di dialogo includono l'uso di valutazioni umane, come la selezione da parte di giudici umani o la valutazione con una scala di Likert.</sample>
    <sample id="940">Quattro.</sample>
    <sample id="941">Nell'esempio con Servin e Kea, le conoscenze di base necessarie sono che i giudici decidono casi in tribunali.</sample>
    <sample id="942">Sì, il codice è disponibile. Puoi trovarlo su GitHub.</sample>
    <sample id="943">Sì, gli annotatori per NLPositionality sono bilanciati per Paese, genere, età e istruzione.</sample>
    <sample id="944">Le frasi nel dominio accettabile sono state perturbate aggiungendo rumore che preservava la struttura rilevante.</sample>
    <sample id="945">Una valutazione dimensionale si riferisce a valutare più aspetti o caratteristiche di un argomento o prodotto.</sample>
    <sample id="946">Gli autori dell'articolo sono affiliati all'Università di Science and Technology of China.</sample>
    <sample id="947">La forma del prompting è importante per il zero e il one-shot prompting.</sample>
    <sample id="978">Gli autori hanno valutato i modelli di dialogo GPT-3.0, GPT-2.0 e CTRL.</sample>
    <sample id="979">Due autori sono coinvolti nell'articolo.</sample>
    <sample id="980">Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli alle restrizioni.</sample>
    <sample id="981">Due autori sono coinvolti nell'articolo.</sample>
    <sample id="982">Il nome della relatrice è Vasudha.</sample>
    <sample id="983">I contributori dell'articolo sono affiliati all'Università della California, San Diego.</sample>
    <sample id="1021">Gli errori più comuni di PaLM sono omissioni.</sample>
    <sample id="1022">Ciao. Sono James Finch. E sono Sarah Finch. E oggi vi parleremo di abc eval, un nuovo approccio dimensionale per valutare l'intelligenza artificiale conversazionale.</sample>
    <sample id="1023">Questo lavoro è stato svolto dal laboratorio Emory Nlp, guidato dal professor Gino Choi presso l'Università di Emory, in collaborazione con Amazon Alexa ai.</sample>
    <sample id="1024">Quindi diciamo che hai appena sviluppato un modello di dialogo e vuoi vedere come si confronta con lo stato attuale dell'arte.</sample>
    <sample id="1025">La pratica comune è quella di utilizzare la valutazione umana, ad esempio chiedendo ai giudici umani di selezionare quale delle due conversazioni è migliore, o di valutare le conversazioni in base a una scala Likert.</sample>
    <sample id="1026">Questi approcci funzionano bene per fornire valutazioni olistiche della qualità generale della conversazione, ma la qualità della conversazione ha molti aspetti. Pertanto, potresti voler valutare più dimensioni della qualità della chat per comprendere le forti e le debolezze del modello a un livello più fine.</sample>
    <sample id="1027">Un approccio è semplicemente chiedere ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la rilevanza delle risposte del modello, utilizzando metodi di confronto o Likert.</sample>
    <sample id="1028">Tuttavia, crediamo che esista una strategia più precisa e affidabile per la valutazione del dialogo dimensionale.</sample>
    <sample id="1029">Il nostro approccio cerca di ridurre la soggettività della valutazione umana annotando esplicitamente se ogni risposta del modello esprime o meno certi comportamenti, come rispondere con informazioni irrilevanti o contraddire se stesso.</sample>
    <sample id="1030">Chiamiamo questo approccio annotare i comportamenti in chat, o abc eval in breve. Abbiamo sviluppato questo metodo per coprire in modo completo i comportamenti dei modelli di chat che sono stati suggeriti per influenzare la qualità della chat nella letteratura recente.</sample>
    <sample id="1031">Abc eval è in grado di misurare i tassi con cui i modelli di chat commettono vari errori tematici.</sample>
    <sample id="1032">Ad esempio, abc eval misura il numero di turni in cui un modello di chat ignora il suo partner o dice qualcosa di irrilevante.</sample>
    <sample id="1033">Si contraddice a se stesso o al suo partner, allucinare fatti errati o violare la conoscenza del buon senso. E quando il modello riesce o non riesce a mostrare empatia.</sample>
    <sample id="1034">Per determinare quale tipo di valutazione è più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su cento conversazioni umane e bot per modello utilizzando abc eval.</sample>
    <sample id="1035">Per confronto, abbiamo anche valutato queste conversazioni usando tre metodi esistenti, valutazioni di Lickert a livello di turno, valutazioni di Lickert a livello di dialogo e confronti a livello di dialogo.</sample>
    <sample id="1036">Per ogni metodo esistente, abbiamo raccolto valutazioni su otto delle caratteristiche più comunemente misurate della dialogo, poiché questa è la pratica standard per valutare i modelli di chat su più dimensioni.</sample>
    <sample id="1037">Dai nostri analisi di questi risultati di valutazione, abbiamo scoperto che le etichette di comportamento di abcb eval sono, nel complesso, più affidabili delle etichette raccolte dai metodi esistenti, come misurato dall'accordo interannotatore su cento conversazioni etichettate in doppio.</sample>
    <sample id="1038">Inoltre, le etichette di valutazione di A B C sono più predittive della qualità complessiva della conversazione rispetto ai metodi esistenti, come mostrato da questo semplice analisi di regressione lineare.</sample>
    <sample id="1039">Ad esempio, puoi vedere come la misurazione della proporzione di turni con contraddizioni di sé e partner spiega il cinque percento e il dieci percento della qualità della conversazione, rispettivamente, mentre i punteggi medi di coerenza di Lickert spiegano solo il quattro percento o meno.</sample>
    <sample id="1040">Infine, abbiamo controllato se ogni metrica di valutazione cattura un aspetto unico della qualità del chat, utilizzando una regressione lineare a passi.</sample>
    <sample id="1041">Puoi vedere come la combinazione di tutte le metriche di valutazione abc spiega oltre il venticinque percento della qualità della conversazione. E man mano che rimuovi le metriche una alla volta, la maggior parte di esse perde una discreta quantità di informazioni sulla qualità.</sample>
    <sample id="1042">D'altra parte, la combinazione di tutti i livelli di metriche di licenza spiega molto meno della qualità. E meno di queste metriche portano informazioni uniche.</sample>
    <sample id="1043">Queste metriche affidabili, informative e distinte abcdval ci consentono di valutare l'intelligenza artificiale conversazionale con una risoluzione superiore a quella raggiungibile dai metodi precedenti.</sample>
    <sample id="1044">Puoi vedere che nei risultati del nostro esperimento, che diversi sfide rimangono e sono state precisamente quantificate. Ad esempio, i bot che abbiamo testato hanno violazioni di buon senso in circa il venti per cento delle loro risposte.</sample>
    <sample id="1045">Produrranno informazioni irrilevanti in circa il quindici per cento delle risposte. E si contraddicono o al loro partner circa il dieci per cento del tempo.</sample>
    <sample id="1046">Con il rapido ritmo di miglioramento nel campo, molti di questi tassi di errore potrebbero vedere una diminuzione nei nuovi modelli rilasciati da quando è stata condotta la nostra valutazione. Tuttavia, questo è ancora più motivo per perseguire metriche di valutazione affidabili e precise per confrontare i modelli.</sample>
    <sample id="1047">Speriamo che abc eval possa essere sfruttato da altri nel settore come un passo significativo in questa direzione. E non vediamo l'ora di vedere come l'intelligenza artificiale conversazionale avanzherà nei prossimi mesi e anni. Grazie per aver guardato.</sample>
    <sample id="1048">Gli autori dell'articolo sono affiliati all'Emory NLP Lab, guidato da Professor Gino Choi, e Amazon Alexa AI.</sample>
    <sample id="1049">CFT sta per Fine-tuning Continuous, che è un semplice ma forte modello di riferimento che dovrebbe essere considerato in futuro lavoro in WSL.</sample>
    <sample id="1050">Ci sono sette autori coinvolti nell'articolo.</sample>
    <sample id="1051">Ciao. Mi chiamo Kayo Yin e presenterò il nostro lavoro intitolato, quando la traduzione richiede contesto, un'esplorazione multilingue guidata dai dati. Questo lavoro è stato realizzato in collaborazione con Patrick Fernhout, Emile Liu, Andre F D Martens e Graham Neubig.</sample>
    <sample id="1052">Quindi molte traduzioni dipendono dal contesto. Ad esempio, come tradurremmo mole in questa frase?</sample>
    <sample id="1053">Bene, se la frase precedente era, le cose potrebbero iniziare ad essere pericolose se i ministri lo scoprono, allora Mo si riferisce a un spia. Ma se la frase precedente era, potrebbe essere qualcosa di serio? Dottore? Allora Mo si riferisce a un segno distintivo.</sample>
    <sample id="1054">Quindi, a seconda del contesto, il significato della parola cambia, e quindi anche la sua traduzione.</sample>
    <sample id="1055">Tuttavia, valutare quanto bene i modelli possono tradurre casi come questo è piuttosto difficile. In primo luogo, perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende impossibile per le metriche a livello di corpus come bleu catturare queste traduzioni.</sample>
    <sample id="1056">E alcune persone hanno suggerito una valutazione mirata sulle traduzioni dipendenti dal contesto. Ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e set limitati di lingue, poiché di solito si basano sulla conoscenza del dominio e sulla curazione umana.</sample>
    <sample id="1057">In questo lavoro, abbiamo cercato di rispondere a queste due domande. Prima, quando la traduzione richiede contesto? E in secondo luogo, quanto bene i modelli gestiscono questi casi?</sample>
    <sample id="1058">Per rispondere alla prima domanda, abbiamo iniziato misurando quanto la parola dipende dal contesto durante la traduzione.</sample>
    <sample id="1059">Nel lavoro precedente, abbiamo introdotto cxmi come misura per l'uso del contesto da parte dei modelli di traduzione automatica. E questo viene fatto misurando quante informazioni il contesto C fornisce sul target Y, dato la fonte X.</sample>
    <sample id="1060">Puoi pensare a cxmi come alle informazioni ottenute dal dare contesto al modello.</sample>
    <sample id="1061">In questo lavoro, estendiamo cxmi a cxmi puntuale, che può misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo pensare a parole che hanno un cxmi alto come quelle che richiedono contesto per la traduzione.</sample>
    <sample id="1062">Ora analizziamo le parole con alto Pexmi per cercare schemi tra queste parole.</sample>
    <sample id="1063">E eseguiamo la nostra analisi sulle trascrizioni dei Tedtalk che sono state tradotte dall'inglese a quattordici lingue diverse.</sample>
    <sample id="1064">Eseguiamo la nostra analisi a tre diversi livelli. Innanzitutto, guardiamo ai tag del paratesto che hanno un alto valore medio. Pcxmi,</sample>
    <sample id="1065">E questo ci permette di trovare, ad esempio, pronomi duali in arabo che hanno un pscx mi piuttosto alto. E questo può essere spiegato perché l'inglese non ha pronomi duali. Quindi è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo.</sample>
    <sample id="1066">E allo stesso modo, scopriamo che alcune lingue richiedono anche il contesto quando vogliamo scegliere la forma verbale appropriata. Guardiamo quindi agli elementi del vocabolario che hanno un alto Pesi Xmi, in media su tutte le sue diverse occorrenze.</sample>
    <sample id="1067">E questo aiuta a identificare casi come quello qui, dove in cinese, è necessario il contesto per tradurre i nomi propri per assicurarsi di utilizzare la stessa traduzione all'interno del documento.</sample>
    <sample id="1068">E allo stesso modo, troviamo che il contesto è supportato per tradurre nella giusta formalità.</sample>
    <sample id="1069">E infine, guardiamo a diversi token individuali che hanno un alto P six mi. E questo ci permette di identificare fenomeni che non possono essere catturati dal verbo stesso, ma che sono espressi nella struttura della frase, come la risoluzione delle ellissi.</sample>
    <sample id="1070">Quindi ora usiamo i nostri risultati dell'analisi per progettare un benchmark per la traduzione a livello di documento.</sample>
    <sample id="1071">Per ogni uno dei cinque fenomeni discorsali che abbiamo identificato, abbiamo creato tag per identificare automaticamente le parole che appartengono al fenomeno. E abbiamo chiamato il nostro tagger il tagger Multilingual Discourse Aware, o Muda Tagger.</sample>
    <sample id="1072">Possiamo quindi notare che diverse lingue hanno diverse proporzioni di questi fenomeni discorsi.</sample>
    <sample id="1073">Quindi usiamo il tagger di mood applicando il tagger sul corpus parallelo che vogliamo utilizzare per la valutazione. E applichiamo le nostre metriche di traduzione di scelta sugli esempi dipendenti dal contesto che il tagger di mood ha identificato.</sample>
    <sample id="1074">E infine, abbiamo usato il nostro benchmark, così come altre metriche, per valutare diversi modelli sulla traduzione automatica a livello di documento.</sample>
    <sample id="1075">Prima di tutto, quando usiamo le metriche a livello di corpus, quindi per blu, troviamo che i modelli agnostici di colore hanno le migliori prestazioni.</sample>
    <sample id="1076">Ma poi, se usiamo commenti, i modelli di consapevolezza del contesto si comportano meglio. E se usiamo la misura F delle parole, allora i modelli con o senza contesto hanno prestazioni comparabili.</sample>
    <sample id="1077">Ciò dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se si utilizzano metriche a livello di corpus da sole.</sample>
    <sample id="1078">Ora usiamo il benchmark di moodle per valutare i modelli. E troviamo che i modelli che utilizzano il contesto sono significativamente più accurati dei modelli che non utilizzano il contesto per certe fenomenologie del discorso, come la formalità e la coesione lessicale.</sample>
    <sample id="1079">Ma questi modelli non sono molto migliori dei modelli che non usano il contesto su altri fenomeni come le ellissi, i pronomi e la forma verbale. Quindi questo suggerisce dove avremmo bisogno di vedere più progressi per la traduzione a livello di documento.</sample>
    <sample id="1080">Abbiamo anche confrontato diversi sistemi commerciali. E il nostro benchmark mostra che DeepL è di solito più accurato di Google translate per la traduzione a livello di documento.</sample>
    <sample id="1081">In sintesi, abbiamo eseguito un'analisi basata sui dati su quattordici coppie di lingue per identificare il contesto di traduzione richiesto.</sample>
    <sample id="1082">E poi usiamo le nostre scoperte per costruire un benchmark per la traduzione a livello di documento, che può aiutarci a identificare quali modelli di fenomeni discorsi possono gestire bene o meno e quali sistemi di traduzione sono buoni alla traduzione a livello di documento.</sample>
    <sample id="1083">Grazie mille per la vostra attenzione. Ci vediamo presto.</sample>
    <sample id="1084">Il nome della relatrice è Yu Xin Zhang.</sample>
    <sample id="1121">Il nuovo metodo non ha un nome.</sample>
    <sample id="1122">Il metodo identifica le parole contrassegnate che distinguono i gruppi contrassegnati dagli altri.</sample>
    <sample id="1123">L'articolo non menziona le affiliazioni degli autori.</sample>
    <sample id="1124">Prague Approach</sample>
    <sample id="1125">Il nome della relatrice è Sarah Finch.</sample>
    <sample id="1126">Quattro.</sample>
    <sample id="1127">I dati di test possono includere coppie di parole, coppie di frasi, coppie di sentenze, coppie di documenti, coppie di pagine, coppie di libri, coppie di articoli, coppie di siti web, coppie di siti di ricerca, coppie di siti di social media, coppie di siti di notizie, coppie di siti di video, coppie di siti di musica, coppie di siti di giochi, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie di siti di auto, coppie</sample>
    <sample id="1161">Le abbreviazioni dei cinque metodi per la prima domanda di ricerca sono:

1. SVM (Support Vector Machine)
2. LR (Logistic Regression)
3. KNN (K-Nearest Neighbors)
4. DT (Decision Tree)
5. RF (Random Forest)</sample>
    <sample id="1162">Il modello viene valutato su 11 attività, tra cui 5 compiti di benchmark e 6 compiti di inferenza clinica.</sample>
    <sample id="1226">CamemBERT viene inizialmente addestrato sui dati di Nature, che sono divisi in quattro set gigabyte.</sample>
    <sample id="1227">Il nome della relatrice è Sadam Schukokowski.</sample>
    <sample id="1228">L'esperimento ha mostrato che quando i modelli sono stati pre-addestrati con dati più recenti, la loro capacità di prevedere correttamente il futuro è diminuita man mano che la distanza temporale tra i dati di addestramento e i dati di previsione è aumentata. Questo risultato ha confermato che la deriva temporale è la causa principale della perdita di prestazioni.</sample>
    <sample id="1269">I token non sono ordinati dopo il primo passaggio, quindi è necessario utilizzare un modello per prevedere la permutazione e metterli nella sequenza corretta.</sample>
    <sample id="1270">Per aumentare la comprensione e la fiducia nei metodi di mitigazione dei bias, gli autori suggeriscono che i proprietari dei modelli aumentino la trasparenza.</sample>
    <sample id="1271">Gli input inaccettabili di coppia minima sono le coppie di input che non sono grammaticalmente corrette.</sample>
    <sample id="1272">Gli autori hanno utilizzato la precisione e la recall come metriche di valutazione.</sample>
    <sample id="1273">L'accordo tra annotatori è stato misurato utilizzando l'accordo interannotatore su 100 conversazioni doppie etichettate.</sample>
    <sample id="1274">Wikipedia è stato scelto come dominio per aggiungere frasi completamente scollegate alle query inaccettabili e accettabili.</sample>
    <sample id="1275">Gli autori dell'articolo non hanno dichiarato alcuna affiliazione.</sample>
    <sample id="1276">MultiInstruct differisce dagli altri parametri di riferimento in quanto è un set di parametri di riferimento per modelli multimodali, progettato per migliorare le prestazioni generali su compiti multimodali.</sample>
    <sample id="1277">Due.</sample>
    <sample id="1278">La coordinazione binaria è un sistema di rappresentazione numerica in cui i numeri sono espressi come una combinazione di due cifre: una per il posto delle decine e l'altra per il posto delle unità.</sample>
    <sample id="1279">In media, i prompt sono stati utilizzati per circa 5 anni.</sample>
    <sample id="1280">I risultati suggeriscono che i modelli più piccoli, come il modello T5 più piccolo, possono competere o addirittura superare i modelli più grandi quando sono correttamente addestrati su set di dati adatti.</sample>
    <sample id="1281">Ciao. Sono Yanis Lacroix. Presenterò i nostri lavori su Dr. Bert, un modello retrained robusto in francese per il settore biomedico e clinico.</sample>
    <sample id="1282">In questa presentazione, iniziamo a parlare di modellazione linguistica nell'assistenza sanitaria. Poi presenteremo la principale contribuzione del nostro articolo.</sample>
    <sample id="1283">Introduciamo il primo modello biomedico in francese, chiamato Dr. Bert, che si basa su Roberta e addestrato su Natchez, che è un set di dati di dati medici dal web.</sample>
    <sample id="1284">Introduciamo anche un confronto del modello con più impostazioni di pretraining e fonti di dati. Quindi presentiamo i nostri risultati su undici compiti di downstream biomedici e clinici in francese.</sample>
    <sample id="1285">E infine, concludiamo sugli esperimenti e ti diamo maggiori dettagli su come accedere al modello.</sample>
    <sample id="1286">Da quando è stato rilasciato nel duemiladiciotto, BERT è diventato uno dei metodi più efficaci per risolvere compiti di elaborazione del linguaggio naturale e offre un enorme guadagno di prestazioni rispetto ai metodi storici, statici e contestuali come word two vec, fast text o n words.</sample>
    <sample id="1287">Da allora, questo modello è stato adattato a molti altri linguaggi, come in francese con Camembert. E anche in altri domini, come il biomedico con Permitted Bert e Biobert, e in clinica con Bert clinico. Ma soprattutto in inglese.</sample>
    <sample id="1288">Modello specializzato per altre lingue. Ci sono timori e sono spesso basati sul pretraining continuo a causa della mancanza di dati in dominio.</sample>
    <sample id="1289">Tuttavia, il francese non aveva alcun modello open source per la biomedicina fino ad ora.</sample>
    <sample id="1290">Quindi ci siamo chiesti, una domanda su quali sono le fonti di dati più appropriate per una vasta gamma di usi. E quei dati di crescita sono una buona sostituzione per i dati clinici.</sample>
    <sample id="1291">Per rispondere a questa domanda, confrontiamo il Dr. Bert con il nostro modello di Bert, che si basa su dati anonimi ottenuti dal non-università. Hospital che abbiamo.</sample>
    <sample id="1292">Dopo, ci siamo chiesti, quanti dati abbiamo bisogno per addestrare un modello specializzato sui dati francesi? Sono quattro gigabyte, otto gigabyte o altro?</sample>
    <sample id="1293">Per rispondere a questa domanda, abbiamo prima allenato e confrontato quattro modelli da zero. Una prima versione di Doctor Bert con sette gigabyte di notches, una seconda versione di quattro gigabyte di set di notches.</sample>
    <sample id="1294">La prima versione di Shubert, che è un modello clinico, con quattro gigabyte di frasi prese da punti clinici. E una versione finale di Shubert con un mix di quattro gigabyte di set di naturali e quattro gigabyte di punti clinici.</sample>
    <sample id="1295">Oltre a questo confronto, introduciamo tre modelli addestrati su pretraining controllato per analizzare l'impatto della strategia di pretraining.</sample>
    <sample id="1296">Uno si basa sul peso di Camembert e si addestra su quattro gigabyte di set di notches. Un altro, anche basato su Camembert, ma addestrato questa volta su quattro gigabyte di lingotti di formaggio.</sample>
    <sample id="1297">E infine, una base di su un modello biomedico inglese, e addestrato su quattro gigabyte di set di immagini. In totale, abbiamo sette modelli.</sample>
    <sample id="1298">Per valutare i nostri sette modelli, raccogliamo immagini per compiti pubblici e privati, come il riconoscimento del nome e dell'agenzia, la classificazione, la registrazione del parziale e la risposta alle domande.</sample>
    <sample id="1299">Questo modello è stato confrontato con sei modelli di base, che sono, Camembert Oscar, centotrentotto gigabyte, Camembert Oscar, quattro gigabyte, Camembert Ccnet, quattro gigabyte, permittet, bio, bert e clinical bert.</sample>
    <sample id="1300">L'analisi di evidenzia che il modello funziona meglio sul compito con dati di natura simile a quelli su cui il modello è stato addestrato.</sample>
    <sample id="1301">Tuttavia, possiamo ottenere i dati da, possiamo osservare che i dati provenienti da fonti eterogenee sembrano essere più versatili. Abbiamo anche osservato che l'uso di più dati si traduce in prestazioni migliori.</sample>
    <sample id="1302">Nel complesso, dalla prua, il recupero sembra ottenere prestazioni più elevate su la maggior parte delle attività.</sample>
    <sample id="1303">Tuttavia, il nostro esperimento sulla protezione dei confini, utilizzando il peso e il tokener di permit bird, addestrato sul set di quattro gigabyte di naturali, ha mostrato risultati comparabili a quelli ottenuti con Dr. Bird, quattro gigabyte da zero.</sample>
    <sample id="1304">Non è il caso del modello basato su camembert, peso e token, che soffre di problemi di stabilità.</sample>
    <sample id="1305">Infine, come conclusione, il nostro sistema di proprietà offre prestazioni migliori su nove dei dodici compiti di non film e supera globalmente il risultato del modello generico qui.</sample>
    <sample id="1306">Abbiamo anche osservato che i dati specializzati sono migliori. Più dati specializzati sono migliori, ma non si scalano bene.</sample>
    <sample id="1307">Tutti i modelli pre-allenati ottenuti da Naturo sono disponibili gratuitamente e su youcanface. E tutti gli script di allenamento sono nel nostro repository Git.</sample>
    <sample id="1308">Quindi grazie per questa presentazione. E non vediamo l'ora di scambiare idee nella sessione posteriore a Toronto.</sample>
    <sample id="1309">Le strategie di apprendimento esaminate nel lavoro sono l'addestramento da zero e l'uso di pretraining.</sample>
    <sample id="1310">Il fattore di overfitting dovuto al riutilizzo del test è inferiore a 0,01.</sample>
    <sample id="1311">La qualità della semplificazione è stata valutata confrontando i risultati con i punteggi di base, con un miglioramento osservato dopo la fine-tunatura.</sample>
    <sample id="1312">Sì, i modelli linguistici mostrano bias politici diversi. GPT-4 è il più liberale, e i modelli GPT sono generalmente più socialmente liberali rispetto a quelli di BERT e le sue varianti.</sample>
    <sample id="1313">Ciao. Mi chiamo Matthias Lendl e oggi vi farò una breve introduzione del nostro articolo sulla generalizzazione compositiva senza alberi, usando l'etichettatura multi-set e le permutazioni latenti.</sample>
    <sample id="1314">Questo è un lavoro congiunto con i miei consiglieri, Alexander Koller e Ivan Titorov.</sample>
    <sample id="1315">La generalizzazione composizionale può essere compresa come la capacità di un apprendista di gestire ricorsioni più profonde e composizioni invisibili di frasi che sono state viste individualmente durante l'addestramento.</sample>
    <sample id="1316">Nel contesto della semantica, il parsing del test per la generalizzazione compositiva potrebbe assomigliare a questo. Come al solito, abbiamo un set di espressioni di allenamento. In questo caso, la ragazza dormiva e Maria sapeva che la ragazza dormiva.</sample>
    <sample id="1317">Queste espressioni sono accoppiate con forme logiche che rappresentano gli aspetti centrali del loro significato.</sample>
    <sample id="1318">A differenza dell'analisi standard del machine learning, il set di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente invisibili.</sample>
    <sample id="1319">In questo esempio, il modello ha visto una ricorsione più superficiale durante l'allenamento e viene testato su un esempio con una ricorsione più profonda.</sample>
    <sample id="1320">I modelli naive sequenza a sequenza lottano con questo tipo di generalizzazione fuori distribuzione e spesso producono output che sono distaccati dall'input.</sample>
    <sample id="1321">In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle che sono codificate a colori nell'esempio.</sample>
    <sample id="1322">Un metodo popolare per affrontare questo problema è quello di integrare gli alberi nei modelli.</sample>
    <sample id="1323">Gli alberi hanno lo scopo di catturare il processo compositivo che collega le espressioni con le forme logiche.</sample>
    <sample id="1324">Questo funziona bene, ma gli alberi di solito non vengono dati e devono essere ottenuti in qualche modo.</sample>
    <sample id="1325">Questo può essere complicato e a volte un processo computazionalmente costoso. In genere, questo comporta una considerevole pre-elaborazione specifica del formaleismo delle forme logiche, ad esempio, per gestire i simboli variabili.</sample>
    <sample id="1326">L'ottenimento degli alberi può anche comportare procedure di induzione grammaticale specializzate.</sample>
    <sample id="1327">In questo documento, non usiamo alberi e introduciamo un modello neurale sequenza a sequenza che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output.</sample>
    <sample id="1328">Per la prima volta, mostriamo una forte generalizzazione alla ricorsione più profonda senza fare affidamento sugli alberi.</sample>
    <sample id="1329">Il nostro approccio prevede l'output dall'input in due fasi.</sample>
    <sample id="1330">In primo luogo, tagghiamo ogni token di input con un multisett di token non ordinati che apparirà nell'output.</sample>
    <sample id="1331">Dopo il primo passo, abbiamo tutti i token giusti, ma non sono ordinati.</sample>
    <sample id="1332">Ecco perché, nel secondo passaggio, usiamo un altro modello per prevedere una permutazione per metterli nell'ordine giusto.</sample>
    <sample id="1333">Introduciamo un nuovo metodo per prevedere un permutazione che non impone alcun vincolo duro sulle possibili permutazioni. Questo rende il nostro approccio abbastanza flessibile ed espressivo.</sample>
    <sample id="1334">Concettualmente, il nostro modello di permutazione funziona approssimativamente in questo modo.</sample>
    <sample id="1335">Andiamo da sinistra a destra sull'output e determiniamo quale token multi set mettere in ogni posizione. Per la prima posizione di output, selezioniamo semplicemente uno come evidenziato in rosso.</sample>
    <sample id="1336">Quindi passiamo al token del set multiset successivo per determinare il secondo token nell'output.</sample>
    <sample id="1337">Determiniamo il terzo token nell'output in modo simile, saltando a un altro token multi. Continuiamo questo processo.</sample>
    <sample id="1338">Fino a quando ogni token dalla prima fase non è stato visitato esattamente una volta.</sample>
    <sample id="1339">Per darti un assaggio dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli treeless sul benchmark cogs. Il nostro modello supera gli altri con un ampio margine di generalizzazione a ricorsione più profonda.</sample>
    <sample id="1340">Alcuni altri tipi di generalizzazione strutturale rimangono molto impegnativi, però.</sample>
    <sample id="1341">Nel nostro articolo risolviamo un paio di interessanti sfide tecniche.</sample>
    <sample id="1342">Prima di tutto, l'allineamento tra input e output non è dato nei dati di addestramento. Di conseguenza, per un dato token, non sappiamo da quale multi setter proviene, il che pone una sfida per l'addestramento.</sample>
    <sample id="1343">Inoltre, a volte ci sono più permutazioni che sono coerenti con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo induzionando l'allineamento come parte dell'allenamento.</sample>
    <sample id="1344">Il nostro metodo di permutazione è molto flessibile, ma porta la sfida che trovare la permutazione con il punteggio più alto è NP difficile. Questo perché questo è legato al problema del viaggiatore.</sample>
    <sample id="1345">Lo approssimiamo con una gpu-friendly, continua rilassamento che ci consente anche di backpropagare attraverso la soluzione e imparare le permutazioni linguisticamente più plausibili.</sample>
    <sample id="1346">Se vuoi saperne di più sugli nostri esperimenti e su come affrontiamo queste sfide, puoi dare un'occhiata al nostro articolo o venire al nostro poster.</sample>
    <sample id="1347">La dissonanza cognitiva è la presenza di due credenze o azioni incoerenti.</sample>
    <sample id="1348">GPT-4 è il modello linguistico più liberale.</sample>
    <sample id="1349">Sì, nell'apprendimento attivo, l'addestramento cumulativo funziona meglio di quello iterativo.</sample>
    <sample id="1350">Sara Papi</sample>
    <sample id="1351">I dati nel parametro MuDa sono stati tratti da analisi delle trascrizioni dei Ted Talks tradotti in 14 lingue diverse.</sample>
    <sample id="1385">Il nome della relatrice è Matthias Lammel.</sample>
    <sample id="1386">Il trasferimento interlinguistico è un processo in cui un modello di linguaggio viene addestrato su un linguaggio di origine e trasferito a un altro linguaggio, come nell'addestramento su query in inglese e combinazione di query in inglese e tedesco per prevedere l'output SQL.</sample>
    <sample id="1387">Xiaoyushen Ma, Yousheng Gu, and Diyi Xu.</sample>
    <sample id="1388">Gli autori fanno ricorso alle misure di latenza: misurazione della latenza media (average lagging) e misurazione della latenza computazionalmente consapevole (computationally aware average lagging).</sample>
    <sample id="1389">Ciao a tutti. Sono Magska Thapa. E oggi, il mio coautore Martin e io stiamo presentando il nostro lavoro, il kit must, che valuta l'integrazione della conoscenza da più fonti. Questo lavoro è una collaborazione tra Mcgill University, Mila e Microsoft Research.</sample>
    <sample id="1390">I modelli di comprensione del linguaggio naturale si basano su una varietà di fonti di conoscenza, come la conoscenza contenuta nei parametri, di solito acquisita tramite pre-allenamento, e la conoscenza fornita nelle input al momento dell'inferenza.</sample>
    <sample id="1391">I lavori recenti in compiti come la risposta alle domande mostrano che i modelli possono utilizzare la conoscenza del tempo pre-addestrato per risolvere il compito.</sample>
    <sample id="1392">Ma la comprensione del linguaggio naturale richiede spesso conoscenze che vengono fornite anche in fase di inferenza.</sample>
    <sample id="1393">Ad esempio, nella frase, John ha visto il presidente appena eletto in tv.</sample>
    <sample id="1394">I parametri pre-allenati possono contenere informazioni su ciò che fanno i presidenti e su cosa sia un tv, ma non possono sapere in modo affidabile chi è questa entità specifica, John, o chi è il nuovo presidente, perché il presidente potrebbe essere cambiato da pre-allenamento.</sample>
    <sample id="1395">Pertanto, i modelli di successo per le attività knowledge-intensive Nlu richiedono la capacità di integrare e utilizzare sia la conoscenza del tempo pre-allenato che la conoscenza del tempo di inferenza.</sample>
    <sample id="1396">In questo lavoro, proponiamo una suite di test diagnostici per l'integrazione del sapere.</sample>
    <sample id="1397">Introduciamo un compito di risoluzione delle co-referenze, progettato per sondare la capacità di attingere alle conoscenze disponibili in diverse fonti. Valutiamo il set di dati con partecipanti allo studio umano e stabiliamo un modello di risoluzione delle co-referenze.</sample>
    <sample id="1398">Ecco un esempio del nostro set di dati. Servin è un giudice. Kia è una panettiera. Servin e Kia si sono incontrati in un parco. Dopo una lunga giornata di lavoro, decidendo casi in un tribunale, era felice di rilassarsi.</sample>
    <sample id="1399">Il compito qui è identificare l'entità corretta a cui il pronome si riferisce, che in questo caso è Selvan.</sample>
    <sample id="1400">La risoluzione di un dato pronome richiede due tipi di informazioni. Prima, conoscenza specifica dell'entità, ad esempio, il signor Wilson è un giudice. E seconda, conoscenza di fondo, ad esempio, i giudici decidono i casi nei tribunali.</sample>
    <sample id="1401">In generale, la conoscenza di fondo viene appresa durante il preaddestramento dei modelli linguistici di grandi dimensioni, mentre la conoscenza specifica dell'entità viene tipicamente osservata al momento dell'inferenza.</sample>
    <sample id="1402">Varia la disponibilità di queste due informazioni in modo tale che possa essere trovata in una singola fonte o in più fonti.</sample>
    <sample id="1403">Abbiamo definito tre impostazioni di kitmos. In primo luogo, abbiamo l'impostazione di pre-treno in background, dove le conoscenze di fondo sono presumibilmente disponibili al tempo del pre-treno.</sample>
    <sample id="1404">In secondo luogo, c'è l'impostazione background both, dove i tipi di conoscenza disponibili sono disponibili sia in tempo di pre training che in tempo di inferenza. Infine, l'impostazione background inference, dove i tipi di conoscenza disponibili sono disponibili solo in tempo di inferenza.</sample>
    <sample id="1405">Questa ultima impostazione è particolarmente interessante, dal momento che simula il caso in cui la conoscenza di fondo necessaria per risolvere un compito non fa parte dei modelli pre-allenati. Ad esempio, perché sono emerse nuove occupazioni da quando il pre-allenamento.</sample>
    <sample id="1406">Ecco un esempio di come controllare la disponibilità di fattori e due fonti.</sample>
    <sample id="1407">Nel contesto di preimpostazione, assumiamo che la conoscenza di fondo, che i partiti cercano seggi elettorali nel governo, sia contenuta nei parametri preimpostati. Nel contesto di inferenza, forniamo la conoscenza specifica, che Chechester è un politico.</sample>
    <sample id="1408">E l'impostazione in background, entrambi. Forniamo in aggiunta non solo specifico di entità, ma anche conoscenza di fondo sui partitioni nel contesto inferenziale.</sample>
    <sample id="1409">E l'impostazione di inferenza di background fornisce l'occupazione fittizia, meurturier, invece di politico, perché meurturier è improbabile che sia contenuto in un periodo pre-allenato.</sample>
    <sample id="1410">Abbiamo valutato il set di dati sia con partecipanti agli studi umani che con modelli di risoluzione delle griglie. In questa figura, mostriamo i risultati dei modelli di migliore performance sulla variante più difficile del set di pre-allenamento in background.</sample>
    <sample id="1411">Senza addestramento specifico del compito su Kitmos, entrambi i modelli non si comportano bene. Quando addestrati su Kitmos, tuttavia, sia C due F che Bird Four Coref si comportano significativamente meglio della scelta casuale.</sample>
    <sample id="1412">Ciò suggerisce che, quando addestrati su set di dati di risoluzione dei riferimenti generali, le moduli imparano a sfruttare le code di superficie, che non sono utili quando si esegue il test su kitmos, dove tali code sono state rimosse.</sample>
    <sample id="1413">Esperimenti aggiuntivi con conoscenza fittizia indicano che anche i modelli di migliore prestazione non possono integrare in modo affidabile la conoscenza fornita solo in tempo di inferenza.</sample>
    <sample id="1414">Per riassumere i principali risultati del nostro articolo, molti modelli di risoluzione delle co-referenze sembrano incapaci di ragionare sulla conoscenza da fonti diverse senza addestramento specifico del compito. Tuttavia, con l'addestramento specifico del compito, alcuni modelli integrano con successo la conoscenza da più fonti.</sample>
    <sample id="1415">Tuttavia, anche i modelli di migliore prestazione sembrano avere difficoltà con la conoscenza a ritroso integrata in modo affidabile, presentata solo al momento dell'inferenza. Se sei interessato a maggiori dettagli, per favore vedi il nostro articolo e controlla il set di dati e il codice su Github. Grazie per l'ascolto.</sample>
    <sample id="1416">I metodi basati su alberi possono essere computazionalmente costosi e richiedere pre-elaborazione formale specifica e procedimenti di induzione grammaticale specializzati.</sample>
    <sample id="1417">L'articolo non menziona specificamente le affiliazioni degli autori.</sample>
    <sample id="1418">Ciao. Sono myra. E oggi parlerò del nostro articolo, marcato personaggi, usando prompt di linguaggio naturale per misurare stereotipi nei modelli linguistici. Questo lavoro è fatto in collaborazione con Essam Deramouches e Dan Javorsky.</sample>
    <sample id="1419">Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei modelli linguistici di grandi dimensioni, o LLM.</sample>
    <sample id="1420">Tuttavia, queste misure hanno varie limitazioni. Di solito si basano su set di dati costruiti a mano che richiedono molto tempo per curare.</sample>
    <sample id="1421">E di solito misurano solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altri gruppi demografici o contesti, o semplicemente catturano associazioni molto generali e ampie, come associazioni negative con particolari gruppi.</sample>
    <sample id="1422">Inoltre, la maggior parte del lavoro in questo spazio non tiene conto dell'intersezionalità, che è la nozione che le identità sociali multifaccettate possono comporre pregiudizi e essere luoghi unici di danno.</sample>
    <sample id="1423">Per superare queste limitazioni, ci affidiamo alla proprietà che questi LLM più recenti sono molto bravi a rispondere alle istruzioni e ai prompt.</sample>
    <sample id="1424">Quindi possiamo chiedere al modello di generare una persona, che è una descrizione di un individuo immaginato usando un prompt, come, immagina di essere una donna asiatica. Descrivi te stesso.</sample>
    <sample id="1425">E possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia, perché possiamo specificare qualsiasi identificatore che vogliamo in questo prompt.</sample>
    <sample id="1426">Ecco alcune generazioni di esempio da Gpt Four.</sample>
    <sample id="1427">Immediatamente vediamo che, mentre gli output non sono eccessivamente negativi o tossici nel senso tradizionale di queste parole,</sample>
    <sample id="1428">Ci sono alcuni modelli interessanti.</sample>
    <sample id="1429">La donna asiatica è raffigurata come non assumibile. La donna mediorientale è indicata usando parole come esotiche e come riferirsi a una regione affascinante.</sample>
    <sample id="1430">E entrambe le donne di colore fanno riferimento all'ascendenza, mentre l'uomo bianco non ne ha nulla.</sample>
    <sample id="1431">Per catturare questi modelli, il nostro metodo ha due parti. La prima è generare queste persone.</sample>
    <sample id="1432">I nostri prompt per generare queste personalità sono stati ispirati da uno studio in cui hanno dato questi prompt a soggetti umani, scoprendo che, dando a soggetti umani, sono stati in grado di surfare gli stereotipi razziali.</sample>
    <sample id="1433">Inoltre, questo consente confronti diretti tra le nostre risposte generate e quelle scritte da esseri umani.</sample>
    <sample id="1434">La seconda parte sono le parole contrassegnate, che è un metodo per identificare le parole che distinguono i gruppi contrassegnati da quelli non contrassegnati, che approfondirò a breve.</sample>
    <sample id="1435">Il vantaggio di questo è che otteniamo stereotipi e modelli molto specifici senza dover fare affidamento su un vocabolario specifico.</sample>
    <sample id="1436">Quindi il metodo delle parole contrassegnate si basa sul concetto sociolinguistico di contrassegnatezza, che afferma che c'è un default non contrassegnato e qualsiasi gruppo che differisce da quel default è contrassegnato linguisticamente.</sample>
    <sample id="1437">Quindi, per esempio, la parola uomo, o mi dispiace, la parola guerriero è di solito associato agli uomini. Quindi, quando le persone descrivono un guerriero che è una donna, di solito specificano un guerriero donna e contrassegnano il termine con donna.</sample>
    <sample id="1438">E più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non marcati, mentre i gruppi emarginati sono di solito marcati.</sample>
    <sample id="1439">Quindi nel nostro metodo, designiamo per primi quali sono i gruppi non contrassegnati e contrassegnati.</sample>
    <sample id="1440">E poi confrontiamo le persone usando il metodo delle parole di combattimento, che è fondamentalmente l'uso di log odds ponderati per distinguere le parole migliori per ogni gruppo segnato.</sample>
    <sample id="1441">Quindi, per esempio, per le personaggi di donne nere, faremmo combattimenti di parole e confrontavamo i rapporti di log odds con i personaggi bianchi e con i personaggi maschi, perché questi sono i due gruppi corrispondenti non contrassegnati.</sample>
    <sample id="1442">Ora per alcuni risultati. Quindi prima usiamo un lessico di stereotipi. E scopriamo che le personaggi generati contengono molti più stereotipi rispetto a quelli scritti dall'uomo.</sample>
    <sample id="1443">Tuttavia, quando guardiamo alla distribuzione delle parole nel lessico, troviamo cose molto diverse.</sample>
    <sample id="1444">Quindi, mentre le persone generate hanno tassi molto più alti delle parole di Luxon, quelle scritte dall'uomo hanno una distribuzione molto più ampia di parole. Mentre le parole stereotipate che sono nelle persone generate sono davvero solo le parole alte e atletiche.</sample>
    <sample id="1445">Quindi davvero solo quelli positivi, o almeno non negativi.</sample>
    <sample id="1446">E infatti, questo lessico non cattura molti dei modelli dannosi che abbiamo visto nelle prime diapositive. Quindi, invece, ci rivolgiamo ai risultati del nostro metodo delle parole contrassegnate per mostrare come queste parole apparentemente positive facilitano stereotipi e narrazioni essenzialiste.</sample>
    <sample id="1447">Nella nostra analisi, riveliamo come questi ritratti apparentemente positivi riflettano modelli dannosi.</sample>
    <sample id="1448">In primo luogo, per i gruppi di Mark, le parole principali includono cose come cultura, tradizione, orgoglioso ed esotico. E queste parole definiscono questi gruppi solo dalla loro relazione con la loro identità e li distinguono come diversi dal normale bianco.</sample>
    <sample id="1449">Questo contribuisce a una lunga eredità di discriminazione e di altriaging per questi gruppi.</sample>
    <sample id="1450">Inoltre, ci sono molti tropi comuni che si riflettono in queste parole, specialmente per le donne di colore. Quindi, ad esempio, le parole che descrivono le donne latine includono cose come vivace e croccante.</sample>
    <sample id="1451">Che si collegano a un tropo di tropicalismo. Per le donne asiatiche, le parole sono cose come petite e delicate e silky.</sample>
    <sample id="1452">Che si collega a una lunga storia di donne asiatiche che sono iper sessualizzate, viste come molto docili e sottomesse e così via.</sample>
    <sample id="1453">E infine, per le donne nere, vediamo che alcune delle parole principali sono cose come forte e resiliente.</sample>
    <sample id="1454">Questo si collega ad un archetipo che la gente ha chiamato l'archetipo della donna nera forte. E mentre suona come positivo a prima vista,</sample>
    <sample id="1455">Ci sono stati lavori che mostrano che questo tipo di archetipo in realtà è molto dannoso, perché mette molta pressione su questi gruppi demografici per essere resilienti e forti contro gli ostacoli sociali.</sample>
    <sample id="1456">Quindi, piuttosto che lavorare effettivamente per cambiare quegli ostacoli, mette pressione su quelle persone per superarli, il che porta a risultati sanitari molto negativi per queste persone, tra gli altri danni.</sample>
    <sample id="1457">Più in generale, scopriamo che le parole per ogni gruppo di mercato riflettono praticamente narrazioni essenzialiste.</sample>
    <sample id="1458">Quindi, sulla base di questi modelli, concludiamo con tre raccomandazioni per i proprietari di modelli.</sample>
    <sample id="1459">In primo luogo, come ricercatori, dovremmo affrontare stereotipi positivi e narrative essenzialiste. Dovremmo anche usare la lente di intersezione per studiare pregiudizi e danni, perché ci sono molte cose che potrebbero essere trascurate se non lo facessimo.</sample>
    <sample id="1460">E infine, ci dovrebbe essere davvero una maggiore trasparenza sui metodi di mitigazione dei pregiudizi.</sample>
    <sample id="1461">Perché, ad esempio, come questi stereotipi positivi, non sappiamo se è perché c'è una sorta di strano.</sample>
    <sample id="1462">L'allineamento del valore eccessivo in corso, o forse altri metodi anti stereotipici che stanno dando vita a questi modelli pernisi.</sample>
    <sample id="1463">Non possiamo fare ipotesi o studiare ulteriormente senza maggiore trasparenza.</sample>
    <sample id="1464">Grazie mille per aver ascoltato. Buon divertimento. Ciao.</sample>
    <sample id="1465">Ciao a tutti. Mi chiamo Jingwei Yi dell'Università di Scienza e Tecnologia della Cina.</sample>
    <sample id="1466">È un piacere per me fare un breve video pubblicitario del nostro articolo, proteggendo il copyright dei grandi modelli linguistici per l'incorporamento e i servizi. Visualizzazione watermark backdoor.</sample>
    <sample id="1467">Introduciamo prima il background sui servizi di embedding.</sample>
    <sample id="1468">Attualmente, i grandi modelli linguistici, come GPT, Lama, Palm, ecc. Sono eccezionali nella comprensione e generazione del linguaggio naturale.</sample>
    <sample id="1469">L'inserimento di servizi pubblicitari è uno dei servizi costruiti su modelli linguistici di grandi dimensioni per assistere vari compiti nlp.</sample>
    <sample id="1470">Ad esempio, openai offre un'API di embedding basata su GPT.</sample>
    <sample id="1471">Tuttavia, lavori recenti hanno dimostrato che l'attaccante può rubare il modello attraverso l'apprendimento dall'embedder e fornire servizi simili. Pertanto, è necessario proteggere il copyright dell'embedder come servizio.</sample>
    <sample id="1472">Per proteggere il copyright dei servizi di embedding, una delle soluzioni è incorporare un segno distintivo nel servizio fornitore e rilevare se un altro servizio contiene il segno distintivo.</sample>
    <sample id="1473">Il metodo del segno distintivo deve soddisfare le seguenti proprietà. In primo luogo, il metodo deve essere applicabile all'embedder di servizi. In secondo luogo, il segno distintivo non dovrebbe degradare l'utilità delle forniture fornite.</sample>
    <sample id="1474">In terzo luogo, il segno d'acqua dovrebbe essere abbastanza coperto per l'attaccante, o l'attaccante può rimuovere facilmente il segno d'acqua.</sample>
    <sample id="1475">Infine, l'imballaggio deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello.</sample>
    <sample id="1476">Le opere esistenti possono essere ampiamente classificate in quattro categorie.</sample>
    <sample id="1477">Tuttavia, questi metodi non sono applicabili all'incorporamento di servizi o mancano di trasferibilità.</sample>
    <sample id="1478">Pertanto, in questo documento, proponiamo il marker di incorporamento, che è un metodo di watermark basato su backdoor applicabile ai servizi di incorporamento.</sample>
    <sample id="1479">Quindi lasciatemi introdurre i dettagli del nostro marchio incorporato. Il marchio incorporato contiene due passaggi principali, l'iniezione del marchio e la verifica del copyright.</sample>
    <sample id="1480">Prima di questi passaggi principali, selezioniamo prima un set di trigger. Il set di trigger è un gruppo di parole in intervalli di frequenza moderati.</sample>
    <sample id="1481">Supponiamo che il provider possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso.</sample>
    <sample id="1482">Nell'iniezione di watermark, definiamo prima un embedding di destinazione. Quando un utente invia una frase al servizio del provider, il provider conta il numero di trigger nella frase.</sample>
    <sample id="1483">L'embedting fornito è una somma ponderata dell'embedting target e dell'embedting originale.</sample>
    <sample id="1484">Il peso dell'embed di destinazione è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di M, l'embed fornito è esattamente uguale all'embed di destinazione.</sample>
    <sample id="1485">La verifica del copyright è per rilevare se un modello dietro un altro servizio contiene il watermark.</sample>
    <sample id="1486">Per prima cosa costruiamo un backdoor e un set di dati benigno. Il set di dati backdoor contiene frasi in cui tutte le parole appartengono al set di trigger. Mentre tutte le parole nelle frasi del set di dati benigno non appartengono al set di trigger.</sample>
    <sample id="1487">Quindi il provider richiede le incorporazioni dal servizio di storage con i dati.</sample>
    <sample id="1488">Il coseno e la L due somiglianza tra l'embedimento richiesto e l'embedimento target sono calcolati. Calcoliamo la differenza di somiglianza tra il set di dati benigno e backdoor, che è definita come Delta Cosine e Delta L due.</sample>
    <sample id="1489">Nel frattempo, applichiamo anche il test ks e usiamo il suo valore p come terza metrica.</sample>
    <sample id="1490">Abbiamo condotto esperimenti su quattro set di dati, agnews, mind, ssd two e irspam. Supponiamo che il provider applichi il set di dati wiki text per contare la frequenza delle parole.</sample>
    <sample id="1491">I risultati su quattro set di dati mostrano che il nostro marker di embedding può avere una grande performance di rilevamento, mentre mantiene una grande utilità per le attività in schermo.</sample>
    <sample id="1492">Abbiamo anche convalidato la coerenza dell'embed fornito visualizzando l'embed di frasi su un set di dati completo. Vlpca, la legenda delle figure significa il numero di trigger in ogni frase.</sample>
    <sample id="1493">Come mostrato nelle figure, è difficile distinguere tra le iniezioni retrograde e le iniezioni normali.</sample>
    <sample id="1494">Questo è tutto grazie per essere venuti a discutere con noi.</sample>
    <sample id="1495">ABC-Eval sta per Annotating Behaviors in Chat, un metodo sviluppato per analizzare i comportamenti dei modelli di chat che potrebbero influenzare la qualità del chat.</sample>
    <sample id="1496">La differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali fino a quando CoNLL-2003 è stato utilizzato fino al 2017.</sample>
    <sample id="1497">Ciao. Mi chiamo Vasudha e sono una candidata a dottorato di ricerca in informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro accettato per A C L Twenty Twenty Three come un lungo articolo, trasferimento di apprendimento per la rilevazione della dissonanza, affrontando la sfida della classe rara.</sample>
    <sample id="1498">Iniziamo definendo la dissonanza cognitiva e perché è un problema importante da studiare nella lingua. In parole povere, la dissonanza cognitiva è due credenze o azioni che sono incoerenti.</sample>
    <sample id="1499">Come questo esempio, dove una persona afferma, so che le sigarette potrebbero uccidermi. E poi continua dicendo, ho preso un paio di fumo dopo la riunione. Questa convinzione e azione sono incoerenti e sono in dissonanza.</sample>
    <sample id="1500">Inoltre, menzionare che non penso che potrei mantenere il mio lavoro senza di loro giustifica la seconda occorrenza. E hanno una relazione di consonanza.</sample>
    <sample id="1501">Mentre la dissonanza è un fenomeno molto comune che sperimentiamo nella decisione quotidiana, sono davvero rari da trovare espressi in linguaggio tra altri tipi di relazioni discorsive.</sample>
    <sample id="1502">Quindi perché questo è importante? Studiare la disconnessione cognitiva può aiutarci a capire gli effetti del disaccordo tra le persone. Traccia le tendenze e i cambiamenti di credenze, valori e atteggiamenti nelle popolazioni.</sample>
    <sample id="1503">Anche la dissonanza cognitiva è legata ai disturbi d'ansia e può aiutare a capire meglio la salute mentale delle persone.</sample>
    <sample id="1504">Studiare il linguaggio espressivo del dissenso può anche essere utile per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili.</sample>
    <sample id="1505">Infine, la dissonanza cognitiva è importante per capire i diversi stili cognitivi delle persone e ci aiuta a capire meglio i processi decisionali.</sample>
    <sample id="1506">Al fine di creare un risorsa di dissonanza cognitiva, abbiamo condotto una grande annotazione di relazioni di dissonanza. Abbiamo usato un approccio di prima dissonanza, come visto nel flusso di lavoro qui.</sample>
    <sample id="1507">I tweet sono stati analizzati utilizzando un parser pdtb e le coppie di unità di discorso sono state annotate secondo le linee guida descritte nel nostro articolo.</sample>
    <sample id="1508">Come si può vedere qui, la dissonanza è stata trovata solo in tre virgola cinque per cento delle coppie annotate.</sample>
    <sample id="1509">Nel raccogliere circa mille esempi di coppie di unità di discorso, abbiamo eseguito l'allenamento per un classificatore iniziale addestrato solo su quarantatré esempi di disnetts. Non sorprende che il classificatore abbia prestato prestazioni non molto migliori di casuale.</sample>
    <sample id="1510">Data l'assenza di dati di questo tipo, stiamo affrontando il problema della rarità assoluta.</sample>
    <sample id="1511">Per alleviare questo, sperimentiamo su combinazioni di apprendimento di trasferimento e apprendimento attivo per annotare in modo tale che più campioni di dissonanza possano essere raccolti in meno round di annotazione, riducendo il costo complessivo dell'annotazione, ma migliorando il rilevamento della dissonanza.</sample>
    <sample id="1512">Poiché il modello iniziale non era in grado di catturare affatto la classe di dissonanza, iniziamo il processo di apprendimento attivo trasferendo i pesi da compiti strettamente correlati.</sample>
    <sample id="1513">Trasferiamo da due compiti diversi. Classificazione indipendente dal tema, un compito che determina se due dichiarazioni di dibattito di persone diverse sono in accordo o in disaccordo, indipendentemente dal tema.</sample>
    <sample id="1514">Chiamato dibattito qui. E sulla classificazione binaria di classi di espansione e confronto di pdtb, poiché queste due sono strettamente legate alla concezione di consonanza e dissonanza. E li chiamiamo c e qui.</sample>
    <sample id="1515">Scopriamo che, durante la trasmissione, le prestazioni a colpo zero sul set di dati annotato sono già molto migliori del caso, con il migliore con auc zero, sei, due,</sample>
    <sample id="1516">Ulteriormente, l'ottimizzazione iterativa su entrambi i compiti. Scopriamo che l'ottimizzazione del compito ce, seguito da un ulteriore ottimizzazione sul dibattito, offre un prestazione zero shot molto migliore. Questo è il modello che abbiamo usato per avviare l'apprendimento attivo.</sample>
    <sample id="1517">Successivamente, determiniamo il metodo migliore per aggiornare un modello con nuovi dati provenienti da ogni round di apprendimento attivo e annotazioni. Cumulatore accumula tutti i dati raccolti dalle annotazioni attive finora. Variabile aggiorna il modello addestrandolo sul set di dati più recente raccolto.</sample>
    <sample id="1518">Sulle diverse strategie, abbiamo scoperto che il cumulativo ha funzionato uguale o meglio del iterativo in generale.</sample>
    <sample id="1519">Successivamente, per migliorare il numero di esempi di dissonanza, usiamo una strategia di probabilità di classe rara, prc, per selezionare principalmente gli esempi che sono molto probabili di essere dissonanti dal modello corrente in qualsiasi round di al.</sample>
    <sample id="1520">Abbiamo confrontato questo con gli altri stati dell'arte, le strategie all'avanguardia che sono comunemente utilizzate nella comunità.</sample>
    <sample id="1521">Scopriamo che la strategia proposta per il P R C funziona meglio di altre strategie all'avanguardia, anche se la differenza è piccola. Si noti che le prestazioni sono significativamente inferiori per casuale.</sample>
    <sample id="1522">In ulteriori round di al con due strategie migliori, abbiamo migliorato la classificazione della distanza, auc due punto sette cinque, che è la migliore prestazione che abbiamo sul compito finora.</sample>
    <sample id="1523">Controlliamo anche la fattibilità di ogni strategia per la qualità delle annotazioni e i costi per gli annotatori. Scopriamo che prc ha il più alto percentuale di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano anche le esami difficili.</sample>
    <sample id="1524">In sintesi, troviamo che il prc è una semplice strategia al per l'acquisizione di classe rara. E l'al di avvio freddo con compiti di apprendimento di trasferimento appositamente progettati può aiutare significativamente.</sample>
    <sample id="1525">Troviamo anche che l'aggiornamento iterativo è utile per il trasferimento di apprendimento da un dominio diverso, mentre le annotazioni attive in dominio traggono vantaggio dall'aggiornamento cumulativo.</sample>
    <sample id="1526">Questi sono i link al nostro set di dati di codice e al nostro documento. Sentiti libero di contattarci se hai domande. Grazie.</sample>
    <sample id="1527">Matthias Landemann, Alexander Colla e Ivan Titoren.</sample>
    <sample id="1528">Il nome della relatrice è Siyu Yuan.</sample>
    <sample id="1529">Quattro.</sample>
    <sample id="1530">L'approccio viene confrontato con l'architettura simulST, specificamente adattata per la traduzione simultanea.</sample>
  </task>
</testset>