<testset name="IWSLT2025" type="output">
  <task track="long" text_lang="en">
    <sample id="0">Language models are trained on large-scale web crawl data, which includes political news media such as the New York Times, Los Angeles Times, The Guardian, and Huffington Post.</sample>
    <sample id="1">The authors of the paper are affiliated with McGill University, Mila, and Microsoft Research.</sample>
    <sample id="2">The paper by Tiwei from EdGroup introduces a novel approach to document understanding, focusing on the visually rich document understanding problem. It addresses the limitations of existing pre-training models, which often suffer from reading order issues due to their reliance on global word order. The proposed solution, Layout Mask, uses local 1D token order (in contrast to global 1D order) to infer global reading order by integrating 1D, 2D, and semantic information. This method enhances text-layout interactions, crucial for understanding documents like forms, receipts, and posters. The Layout Mask employs two masking strategies: whole-word masking and layout-aware masking, which challenge the model to predict masked words and understand cross-segment orders, respectively. The paper also introduces a new pre-training objective, masked position modeling, to further improve the model's ability to learn layout presentations. Experiments show that Layout Mask outperforms global 1D order models on the SROIE dataset, particularly in cases with misleading numbers, suggesting that global 1D order is more adaptive.</sample>
    <sample id="4">The speaker's name is Kai-Wei Yin.</sample>
    <sample id="5">T5-large model</sample>
    <sample id="6">The presentation introduces a unified approach to multilingual and cross-lingual summarization, termed many-to-many summarization, which aims to generate summaries in any language from a given document. The work involves preliminary studies to analyze the differences between traditional multilingual and cross-lingual summarization models and proposes a new model called PACE. PACE is trained through a three-stage process: mental pre-training, cross-lingual pre-training, and task-specific pre-training. The model outperforms existing models like MBERT50 and MT5, as demonstrated in experiments on the WNLI dataset. Ablation studies and human evaluations further validate the effectiveness of the proposed model. The presentation concludes with a call to check out the detailed paper for more information.</sample>
    <sample id="7">Yes, CoNLL-2003 taggers can still work effectively in 2023 with improvements in model architecture, size, and fine-tuning.</sample>
    <sample id="8">The novelty of ABC Eval lies in its ability to reduce subjectivity by explicitly annotating model behaviors, providing a more reliable and predictive measure of chat quality compared to existing methods.</sample>
    <sample id="9">The success of existing weakly supervised approaches heavily relies on the availability of clean validation samples.</sample>
    <sample id="10">Improving the language model's access to background knowledge can enhance accuracy in resolving indirect referring expressions.</sample>
    <sample id="11">Jack Hessel, a research scientist at AI2, presents a study on humor understanding benchmarks using the New Yorker Caption Contest data. The study evaluates large language models like ChatGPT and GPT-4 on tasks such as matching captions, quality ranking, and explanation generation. Despite advancements, these models still struggle with humor comprehension, as evidenced by their performance compared to human benchmarks. The study highlights the challenges in teaching models to understand and generate humor, with GPT-4 showing significant errors in joke explanations. The research aims to encourage further exploration and improvement in the field of AI humor understanding.</sample>
    <sample id="12">The paper involves four authors: Dawei, Xiaoxuan, Mario, and Giasdelf.</sample>
    <sample id="13">Daniel Rotem presents his research on adaptive inference, focusing on improving inference efficiency in low-resource settings. The study compares multi-model and early-exit methods, highlighting the issue of conflicting gradients in early-exit models. The introduction of the SWEET method aims to mitigate this by separating weights in early-exit transformers, thus avoiding gradient conflicts. Results show that SWEET narrows the performance gap between early-exit and multi-model methods, particularly excelling in fast inference scenarios. The research provides a fair comparison of these methods and suggests future exploration of fine-tuning algorithms for early-exit architectures.</sample>
    <sample id="15">The paper is a joint work of three authors: Matthias Landmann, Alexander Coller, and Ivan Tiedoff.</sample>
    <sample id="16">Bible texts are simplified more than news texts or language learner texts.</sample>
    <sample id="17">The presentation by Shen Chuan Wu, a PhD student at AOS, introduces a method for multimodal relation extraction that integrates text and visual data to improve semantic understanding. The method addresses challenges such as internal information overutilization and external information underexploitation by employing a graph information bottleneck guided feature refinement and incorporating multimodal topic information. The process involves representing text and images as graphs, merging them into a cross-modal graph, and refining the graph structure. The enriched features are evaluated on a widely used dataset, showing superior performance compared to existing methods. The study concludes that internal information screening and external information exploiting are context-dependent, with the former being more crucial for high-relevance inputs and the latter for lower-relevance inputs. The proposed method achieves significant improvements over existing models, offering a novel approach to multimodal relation extraction.</sample>
    <sample id="18">The example given is the sentence 'salt and pepper' compared to 'pepper and salt', where 'salt' is shorter.</sample>
    <sample id="19">Xiang Su Chang, a master's student from Shenzhen University, presented their work on efficient open-domain question answering at the ACL 2023 conference. The work focuses on a two-stage model for question answering, which involves a retrieval stage to extract evidence contexts from a pre-indexed Wikipedia corpus and a reader stage to reason out answers. The challenges discussed include the large size of the Wikipedia corpus, the large index file, and the complexity of language models. To address these challenges, the presentation summarized techniques such as approximate nearest neighbor search, adaptive computation, and embedding compression. The performance of retrieval and reader systems was compared with retrieval-only and generator-only systems, highlighting the trade-offs between speed, memory, and performance. The presentation concluded with insights on deploying systems in resource-constrained environments and the need for more evaluation metrics.</sample>
    <sample id="20">Yes, all pre-trained models are freely available on Hugging Face, and the training scripts are on the GitHub repository.</sample>
    <sample id="21">DEplain-apa contains news texts.</sample>
    <sample id="22">Good generalization is achieved through the use of transformer models, larger model sizes, and more fine-tuning examples.</sample>
    <sample id="23">Dan Garrett discusses the challenges in text rendering for text-to-image models, particularly focusing on the limitations of the T5 text encoder. T5's sentence piece tokenization struggles with spelling accuracy, especially for frequent words, while ByteT5, with its byte-level encoding, excels in this area. To address these issues, Garrett's team augmented the existing text representation in the Imagine model with a text representation from ByteT5, which significantly improved the model's text rendering capabilities. This approach, though not perfect, enhances the model's ability to render text accurately, although errors can still occur during the image generation process. The research introduces benchmarks for text-only and text-to-image models and proposes a strategy to improve text rendering by leveraging character-level encoding.</sample>
    <sample id="24">The tendency for left conjuncts to be shorter was measured in syllables, with observations showing a preference for shorter left conjuncts when the governor is on the left.</sample>
    <sample id="25">Experiments were designed by measuring coordination lengths in characters, syllables, and words, focusing on cases with and without a governor to observe the effect on left conjunct length.</sample>
    <sample id="26">The baseline classifier performs not much better than chance, indicating poor performance on imbalanced data.</sample>
    <sample id="27">The paper is authored by five individuals.</sample>
    <sample id="28">The characters in the example conversation are Bob and Alice.</sample>
    <sample id="29">Context-aware MT models improve over context-agnostic ones in handling discourse phenomena such as formality and lexical cohesion.</sample>
    <sample id="30">The paper introduces Blender, a framework for ensemble learning of large language models, focusing on power-based ranking and generative fusion. The authors, from AIT and USC, highlight the variability in model performance across different inputs, suggesting that using multiple models can yield better results than relying on a single model. Blender employs a two-stage process: first, it runs multiple models on an input to generate outputs, then uses a power-based ranking model to compare these outputs. The top models are then fused using a generative model to produce the final output. The paper presents a new dataset, Mix-Inst, for evaluating Blender, and demonstrates that Blender outperforms other models in various metrics. The authors also provide a unified codebase and data for future research.</sample>
    <sample id="31">The authors of the paper are affiliated with the University of Oxford.</sample>
    <sample id="32">Hi, my name is Matthias Landmann and today I'm going to give you a brief introduction to our paper on compositional generalization without trees using multi-set tagging and latent permutations. This is joint work with my advisors Alexander Koller and Ivan Tiedoff. Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. In the context of semantic parsing, testing for compositional generalization might look like this. As usual, we have a training set of utterances. In this case, the girl slept, and Mary knew that the girl slept. These utterances are paired with logical forms that represent core aspects of their meaning. In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms. In this example, the model has seen shallow recursion during training and is tested on examples with deeper recursion. Naive sequence to sequence models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input. In particular, they often fail to reproduce the systematic correspondences between input and output, such as those that are color coded in the example. A popular method to address this is to integrate trees into the models. The trees are intended to capture the compositional process that relates utterances with the logical forms. This works well, but trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism-specific preprocessing of the logical forms, for example, to handle variable symbols. Obtaining trees may also involve specialized grammar induction procedures. In this paper, we don't use trees and introduce a neural sequence to sequence model that directly models the correspondences between fragments of the input and fragments of the output. For the first step, we tag each input token with an unordered multi-set of tokens that will appear in the output. After the first step, we have all the right tokens but they're not ordered. That's why in the second step, we use another model to predict a permutation to put them into the right order. We introduce a new method to predict a permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive. Conceptually, our permutation model works roughly like this. We go from left to right over the output and determine which multi-set token to put in every position. For the first output position, we simply select one as highlighted in red. Then, we jump to the next multi-set token to determine the second token in the output. We determine the third token in the output in a similar way by jumping to another multi-set token. We continue this process until every token from the first stage has been visited exactly once. To give you a teaser of the experimental results, here we compare our method with other treeless models on the cogs benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion. Some other kinds of structural generalization remain very challenging though. In our paper, we solve a couple of interesting technical challenges. First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token, we don't know which multi-set it came from, which poses a challenge for training. In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the training. Our permutation method is very flexible, but it brings the challenge that finding the highest scoring permutation is NP hard. That's because this is related to the traveling salesman problem. We approximate this with a GPU friendly continuous relaxation that also allows us to back propagate through the solution and learn the linguistically more plausible permutations. If you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.</sample>
    <sample id="33">The framework quantifies positionality by re-annotating datasets with diverse annotators, collecting demographic data, and comparing annotations using Pearson's R correlation score against model predictions, thus identifying alignment with specific user demographics.</sample>
    <sample id="34">Marcus Treviso presents CREST, a framework for rationalization and counterfactual text generation, developed in collaboration with Alex Ross, Nuno Guerreiro, and Andre Martins. CREST combines selective rationalization and counterfactual generation to produce high-quality, interpretable explanations. It generates counterfactuals by masking input parts and using a language model to fill in the blanks, resulting in more natural and valid counterfactuals compared to other methods. Human evaluations show CREST's counterfactuals are more valid and natural than those from other approaches. Additionally, CREST can be used for data augmentation, improving model performance on datasets like IMDB. The framework also introduces a metric called counterfactual simulability, which measures the ability of explanations to change a classifier's decision. Overall, CREST offers a robust method for generating explanations that focus on the contrasting parts of the input, enhancing the interpretability of machine learning models.</sample>
    <sample id="35">Hello, I am Dawei, a PhD student at Saarland University in Germany. In this video, I would like to present our recent work, Weakly Supervised Learning: A Critical Look. This is joint work with Xiaoxuan, Mario Musbach, and Giasdethen and Dittlich Klako. I'd like to begin with a brief introduction to weak supervision and weakly supervised learning. In weak supervision, we do not manually label the data. Instead, we label the data using weak labeling sources, such as simple heuristic rules, knowledge bases, or low-quality crowdsourcing. When compared to human annotations, the weak annotations are much cheaper, yet they are also noisy, meaning that a certain amount of the annotations are incorrect. If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize. In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such labeled noise so that the trained models still generalize well. In recent works in WSL, so WSL stands for weakly supervised learning, a common claim is that people say that they only train models on weakly labeled data and achieve high performance on clean test sets. Technically, this claim is not wrong, but there is a catch, which is that people do assume that there is an additional clean validation set available for model selection. We cast doubt on this problem setting, as this implies that additional manual annotations are required in weakly supervised learning. But like an elephant in the room, this necessity is often overlooked. The aforementioned assumption is to ask three research questions. First, is clean validation data necessary for WSL? Or can we maybe use a noisy validation set instead? Second, if clean data is required, or if clean data is mandatory for WSL to work, then how many clean samples do we need? Finally, should we only use the clean samples for validation, or are there better ways to utilize them? We addressed these research questions in our work and our findings are as follows. First, we find that interestingly, recent WSL methods indeed require clean validation samples to work properly. Otherwise, there is a large performance drop. As shown in this figure, if there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels, meaning that the training is pointless. This indicates that WSL approaches actually require clean labeled data to work properly, and the annotation cost for obtaining clean validation samples should not be overlooked. Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left. Typically, we only need 20 samples per class to attain high performance. But that's not the end of the story, because if we either way decide to access clean samples, then training on them directly will achieve better performance. The right figure shows the performance difference between fine-tuning approaches, which are directly applied on the clean data, and WSL approaches, which use the clean data for validation only. As we can see from the figures, if we have ten samples per class, direct fine-tuning starts to beat WSL approaches. Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples. As we can see from the figures, the Valina model, termed Ftw, initially underperforms more complicated WSL methods like CoSine. However, if we allow to continue fine-tuning on the clean samples, then Ftw performs equally well as other methods. So in practice, there is no reason to choose more complex WSL methods, which require more computation time and disk space. To summarize, we showed that recent WSL approaches require clean, manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated. Our concrete recommendations for future work are as follows. First, report the model selection criteria. For example, report if the model selection is done well clean validation samples. Second, WSL approaches should be compared to few-shot learning baselines, as both work on clean samples. Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL. Finally, we have open-sourced our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you, and enjoy the conference.</sample>
    <sample id="36">The presentation by Thoms U. P. Pejsec and colleagues at ACL introduces a novel approach to multilingual machine translation using language-specific layers (LSLs). This method aims to enhance translation capacity and efficiency by maintaining constant inference costs while increasing model size. The approach involves a regular transformer layer per language, which selects the appropriate sublayer for training and inference. The team experimented with encoder and decoder placements of LSLs, ultimately finding that the encoder placement was more effective. They trained a large model with three weights per encoder layer to learn the optimal placement, which was then used to construct a new architecture. Their results, based on the WMT21 dataset, show significant improvements over baseline models, especially for low-resource languages. The approach is statistically significant in 84 out of 9 translation directions. The paper provides a comprehensive analysis of the method's performance and invites further exploration through additional experiments and metrics.</sample>
    <sample id="37">The previous study found that human subjects could surface racial stereotypes when given persona prompts, which inspired the methodology of generating personas in the current research.</sample>
    <sample id="38">The study used statistics extracted from the Enhanced Version of the Penn Treebank and the paper 'Why We Don't Use Universal Dependencies'.</sample>
    <sample id="39">The paper involves two authors: Adam Szpekowski and Anette Thomason.</sample>
    <sample id="40">The closely related tasks for cognitive dissonance are topic-independent dissonance stance classification and binary classification of expansion and comparison classes.</sample>
    <sample id="41">The work presented by Su Lin from the Natural Language Processing Lab at EPFL University introduces Peacock, a personal commonsense knowledge graph designed to enhance narrative coherence and engagement in natural language processing. Peacock, developed in collaboration with Sony Group Corporation, contains 3,800 personas and 40,000 attributes, forming 100,000 personal inferences. It is built through three steps: selecting personas from existing knowledge graphs, inducing attributes from these graphs and large-scale language models, and annotating relations via a joint human-AI majority voting scheme. The Peacock knowledge graph is used to train a BART-based language model, which outperforms large-scale models in natural language generation tasks. Additionally, Peacock is applied to improve downstream narrative modeling, particularly in dialogue generation, by augmenting speaker profiles with relevant facts. The results show that Peacock's person-centric knowledge leads to more consistent and engaging dialogues, especially when speakers share common attributes. The paper and its accompanying GitHub repository are publicly available.</sample>
    <sample id="42">The paper is authored by Shuo Hung.</sample>
    <sample id="43">The paper has three authors.</sample>
    <sample id="44">The introduced framework differs from previous works by comparing end users' annotations with models and datasets, rather than focusing on annotator agreement or modeling annotator distributions.</sample>
    <sample id="45">The generated personas overlap the most with the lexicon of stereotypes, containing a higher rate of stereotype words compared to human-written responses.</sample>
    <sample id="46">The commercial systems compared were Google Translate and DeepL.</sample>
    <sample id="48">The paper is a joint work by Aydil Bilal and his colleagues from Google Translate.</sample>
    <sample id="49">MPP evaluations were performed up to 1024 tokens.</sample>
    <sample id="50">The presentation introduces D-Plane, a new corpus for German text simplification, addressing issues with existing corpora. It is divided into two sub-corpora: D-Plane API, with 483 manually aligned documents, and D-Plane Web, with 750 documents aligned manually and automatically, totaling 30,450 sentence pairs. The corpus is analyzed for different simplification techniques, showing variations in simplification types across domains. Two use cases are presented: evaluating automatic alignment methods, where D-Plane API serves as a gold standard, and automatic text simplification, where models are fine-tuned to produce simplified text. The results indicate that fine-tuning can achieve better scores than baseline models, providing a benchmark for future research.</sample>
    <sample id="51">The dataset includes music, books, and recipes.</sample>
    <sample id="52">Positionality refers to the perspectives that people hold due to their demographics, identity, and life experiences, influencing research decisions and outcomes.</sample>
    <sample id="53">The speaker's name is Dawei.</sample>
    <sample id="54">The paper by Vasudha, a PhD candidate at Stony Brook University, addresses the challenge of detecting cognitive dissonance in language through transfer learning. Cognitive dissonance, defined as the inconsistency between beliefs and actions, is rare in language but significant for understanding mental health and decision-making. The research involved annotating discourse units for dissonance, using a dissonance-first approach. Initial models struggled due to the rarity of dissonance, prompting the use of transfer learning from related tasks. The study found that the proposed probability of rare class (PRC) strategy outperformed other methods, achieving the best performance of 0.75. The research highlights the importance of transfer learning and iterative updates in improving dissonance detection, with PRC being effective for rare class acquisition.</sample>
    <sample id="55">Yes, EDAtt adapts an existing offline ST model without retraining or specific architecture for simultaneous translation.</sample>
    <sample id="56">The paper has three authors.</sample>
    <sample id="57">Yes, the tested models, C2F and BERT CoREF, work on the KITMOS test suite, but they struggle with integrating knowledge provided only at inference time.</sample>
    <sample id="58">The three variants of KITMUS are: 1) Background pre-trained, where background knowledge is assumed to be available at pre-training time, 2) Background both, where background knowledge is available at both pre-training and inference time, and 3) Background inference, where both knowledge types are available only at inference time.</sample>
    <sample id="59">The presentation by Yanis Slavrak introduces Dr. Bert, a French biomedical model based on the pre-trained Roberta, trained on the large-scale medical data set known as Nachos. The model is compared to other multilingual models, including Camembert and Bio-Bert, and evaluated on 11 biomedical and clinical tasks. The results show that Dr. Bert outperforms generic models like Camembert, especially on tasks with similar data sources. The study also explores the impact of data quantity and pre-training strategies, finding that more data and specialized data lead to better performance. The models are available on Hugging Face, and the training scripts are shared on GitHub.</sample>
    <sample id="60">The authors of the paper are affiliated with the University of Illinois at Urbana-Champaign, the University of Illinois at Chicago, and the University of Illinois at Chicago.</sample>
    <sample id="61">The last research question is whether clean samples should be used solely for validation or if there are better ways to utilize them.</sample>
    <sample id="62">The paper by Nithya Balan and colleagues presents a systematic study on knowledge distillation for natural language generation (NLG) systems, focusing on compressing large models while maintaining performance. The authors explore various distillation techniques, including word-level and sequence-level distillation, and propose new methods like joint teaching to enhance model efficiency. They conduct experiments on tasks such as summarization, question generation, and simplification, using realistic industry-driven setups with medium-sized models and a focus on inference time efficiency. The study challenges traditional single pseudo-target generation methods, showing the benefits of using multiple pseudo-targets and sampling techniques. The main contribution is the joint teaching technique, which applies word-level distillation to pseudo-targets from both teacher and student models, addressing exposure bias and improving student learning. The paper provides a comprehensive analysis of distillation methods and their impact on NLG systems.</sample>
    <sample id="63">Sensitivity measures the model's ability to produce consistent outputs for the same task despite variations in the wording of the instructions.</sample>
    <sample id="64">The speaker's name is Jingwei Yi from the University of Science and Technology of China.</sample>
    <sample id="65">Greater sensitivity indicates improved model performance, as it shows the model's ability to produce consistent outputs for the same task despite variations in instruction wording.</sample>
    <sample id="66">The paper 'Deep Learning for Mathematical Reasoning' explores the development of deep learning methods for mathematical reasoning, a key aspect of human intelligence. It discusses the surge of interest in AI and NLP for solving mathematical problems and proving theorems. The study focuses on two main categories: visual and tabular contexts, with examples like geometric problem-solving and theorem proving. Neural network architectures, such as second-to-second models and large language models (LLMs), are examined for their potential in mathematical reasoning tasks. Despite advancements, LLMs face challenges like poor generalization and robustness. The paper suggests augmenting LLMs with tools and creating diverse reasoning paths to improve performance. It also highlights the need for more datasets in low-resource settings and the development of benchmarks for various domains. The paper concludes by acknowledging the progress made and the challenges that remain in the field.</sample>
    <sample id="67">The discussion focuses on interference in multilingual translation models, where models trained on multiple languages can suffer from negative transfer or benefit from positive transfer. Key factors affecting interference include model size, data size, language similarity, and the total number of languages. Experiments show that severe interference occurs in small models with limited data, and tuning the sampling temperature is crucial for performance. Language similarity and the number of languages have minimal impact on interference. The study uses multilingual models trained on various language pairs and sizes, demonstrating that interference can be mitigated by adjusting the temperature parameter, especially in smaller models. The findings suggest that tuning temperature is a simple yet effective method to control interference without needing specialized algorithms.</sample>
    <sample id="68">During pretraining, models receive a limited linguistic context, typically a few tokens, which may not fully capture their ability to understand and process longer sequences.</sample>
    <sample id="69">Typically, only 20 clean validation samples per class are needed to achieve good performance in WSL.</sample>
    <sample id="70">The authors of the paper are affiliated with the University of Washington.</sample>
    <sample id="71">The work by Javot Hosseini and colleagues focuses on resolving indirect referring expressions for entity selection, introducing the alt-entities corpus. The project aims to understand user language in choosing between entities, such as songs, by using direct or indirect references. The dataset, collected via crowd annotation, covers music, books, and recipes, with a methodology that emphasizes informality through cartoon completion. Annotators are provided with background knowledge, such as Google search results and Wikipedia texts, to help them make informed choices. The corpus includes 6,000 questions and 42,000 expressions, with models achieving high accuracy when given full background knowledge. However, the accuracy drops significantly when models have limited access to background information, highlighting the need for further improvement.</sample>
    <sample id="72">New methods are needed to accurately measure media biases, as current methods like the Political Compass Test are not reliable for large-scale data analysis.</sample>
    <sample id="73">The speaker's name is Akshita.</sample>
    <sample id="74">The paper introduces DenseAtomic, a densely connected atomic knowledge graph that enhances knowledge coverage and multi-hop reasoning. DenseAtomic addresses the limitations of the original ATOMIC by adding missing links and increasing multi-hop paths. The construction process involves three main steps: normalized tail events, training a relation prediction model, and constructing DenseAtomic. The paper introduces RESC-GC, a new method for relation prediction that uses a graph-free approach and encodes head and tail events with a pre-trained language model. Extensive evaluations demonstrate that DenseAtomic achieves higher knowledge coverage and better performance in multi-hop reasoning compared to ATOMIC and other methods. The paper concludes with the potential of DenseAtomic for improving commonsense reasoning in machines.</sample>
    <sample id="75">The presentation by Zhengyan Dan introduces the joint prop framework, a collaborative effort with Hao Anran and supervisor Lu Antoine, aimed at enhancing Named Entity Recognition (NER) and Relation Extraction (RE) tasks. The framework addresses the limitations of current semi-supervised models by integrating label propagation across heterogeneous graphs, considering interconnections between labeled and unlabeled data. The method involves four main parts: span feature generation, graph construction, joint label propagation, and model optimization. Experiments conducted on four datasets demonstrate that joint learning benefits from task co-dependency, showing significant improvements over single-task baselines. The framework's ability to leverage connections between tasks and data types is highlighted as a key strength.</sample>
    <sample id="76">The pipeline starts with language models trained on large-scale web crawl data, which often includes politically biased media. This bias can be transferred to downstream tasks, potentially leading to fairness issues in applications like hate speech and fake news detection.</sample>
    <sample id="77">This video presents a collaborative project between Yale University and Microsoft Research focused on enhancing summarization factual consistency through natural language feedback. The project introduces the Defact dataset, which includes human demonstrations and feedback to improve factual accuracy in summaries. The work involves three new NLP tasks: summary editing, feedback generation, and automatic factual error correction. The study highlights the challenges and successes in each task, particularly noting that while models can effectively leverage feedback for editing, generating feedback remains difficult. The dataset, containing 2.5K data points with 70% containing factual errors, provides a comprehensive analysis of factual consistency. The Defact dataset is released on GitHub, offering a valuable resource for training and evaluating factual metrics.</sample>
    <sample id="78">Yes, DEplain-apa focuses more on lexical and structural simplifications with manual alignments, while DEplain-web includes more rephrasings and is aligned using both manual and automatic methods.</sample>
    <sample id="79">Yes, Coscript is publicly available and can be found in the paper.</sample>
    <sample id="80">The watermark is inserted by defining a target embedding, which is a weighted sum of the target and original embeddings. When a user sends a sentence, the provider counts the number of trigger words. If the count exceeds a threshold, the provided embedding is adjusted to be equal to the target embedding.</sample>
    <sample id="81">The authors are affiliated with Penn State University.</sample>
    <sample id="82">This video presents a study on unsupervised automated essay scoring (AES) using a novel framework called Learning from Rank Aggregation (LRA). The current state-of-the-art AES models are supervised, requiring labeled essays, which is time-consuming and labor-intensive. LRA addresses this by using multiple heuristic quality signals as pseudo ground truth to train a neural AES model. The framework includes a Heuristic Assays Ranking (HER) module to generate partial order pairs from multiple quality signals and a Deep Pairwise Rank Aggregation (DPR) module to aggregate these pairs into a unified supervision. The DPR module addresses conflicts among different signals through a deep pairwise rank aggregation loss, which assigns confidence weights to each signal. Experiments demonstrate that LRA outperforms other unsupervised baselines and achieves competitive performance with supervised methods. The study highlights the potential of LRA in both scientific research and practical applications, offering a more efficient and robust approach to AES.</sample>
    <sample id="83">Yes, encoder-decoder models like mt5 can improve by training on a mixture of languages, as they achieve the best performance across all datasets.</sample>
    <sample id="84">Shaohu Xie presents a paper on a novel framework called PANet, designed to enhance dynamic networks by reducing redundant parameters while maintaining performance. Traditional dynamic networks suffer from excessive parameter usage, limiting their practical application. PANet addresses this by partitioning parameters into dynamic and static, using scale factors to optimize training. Experiments show PANet outperforms both static and fully dynamic networks, with better accuracy and fewer parameters. Ablation studies reveal optimal dynamic ratios and the importance of scale factor summation. PANet also outperforms network pruning, maintaining discriminating outputs. Future work includes extending PANet to other network architectures and exploring more parameter combinations.</sample>
    <sample id="85">An example of constrained language planning is creating a script for making a chocolate cake, which involves specific goals and constraints like using chocolate.</sample>
    <sample id="86">They ensure covertness by making the watermark difficult to detect and remove, and by verifying that the watermark is not easily distinguishable from the original embedding.</sample>
    <sample id="87">The work uses the pre-trained weights and tokenizers of existing PLMs like Camembert and Phemamert, adapting them to French biomedical data to create a new model, Dr. Bert.</sample>
    <sample id="88">GPT-4 is the least aligned with non-binary people compared to their male and female counterparts.</sample>
    <sample id="89">The speaker uses the example sentence 'I'm going to talk about' to show how the model leverages knowledge through the attention mechanism, with the first two words pointing to earlier frames and the last word to the last received frames.</sample>
    <sample id="90">The paper by Hana Liu and colleagues explores the potential of using language learners as data annotators in NLP, challenging the traditional reliance on native speakers. The study selected three languages, English, Korean, and Indonesian, and tested annotators across four tasks. Learners were divided into groups with additional resources, and their annotations were compared to those of native speakers. The results showed that learners' annotations were nearly as accurate as native speakers' when aggregated by majority voting. Furthermore, language learners demonstrated significant improvement in language proficiency through annotation tasks. The study suggests that language learners can effectively contribute to data annotation, offering a viable solution for low-resource languages and overcoming geographic and technological barriers in NLP research.</sample>
    <sample id="91">As the number of tasks increases, the model's performance improves, and its sensitivity decreases.</sample>
    <sample id="92">The authors compare their method with three treeless baselines: a model using a fixed permutation, a model using a fixed permutation with a softmax, and a model using a fixed permutation with a softmax and a max-margin objective.</sample>
    <sample id="93">The two co-authors are advisors to the first author, Matthias Landmann.</sample>
    <sample id="94">The paper by Jingwei Yi from the University of Science and Technology of China addresses the issue of copyright protection for embedding ad services using large language models like GPT, LAMA, and PALM. It introduces Embedding Marker, a watermarking method that is applicable to embedding ad services, ensuring copyright protection without degrading service utility. The method involves two main steps: watermark injection and copyright verification. The injection process uses a trigger set to adjust the embedding based on the number of trigger words in a sentence. Verification involves comparing embeddings from a stealer service against a benign dataset to detect watermark presence. Experiments on datasets like AG News, MIND, SSTD2, and EREASFM demonstrate the method's effectiveness in maintaining utility while protecting copyright.</sample>
    <sample id="95">The first author of PaLM is Iyad Bilal.</sample>
    <sample id="97">The speaker mentions three main problems of SimulST: complex training procedures, training multiple models for different latencies, and maintaining models for various latency regimes.</sample>
    <sample id="98">An effective way to mitigate biases is to use diverse and balanced datasets that represent multiple perspectives, along with techniques like bias detection and correction algorithms, and careful monitoring of model outputs to ensure fairness across different social and political groups.</sample>
    <sample id="100">The talk introduces Prompt-Rank, a method for multi-hop question answering that combines unsupervised retrieval with a few-shot language model reranker. It addresses the challenge of requiring large datasets for training by using a small number of examples. The method involves retrieving candidate chains via TF-IDF and hyperlink traversal, then scoring them using a language model's likelihood of the question given the chain. The performance of Prompt-Rank is compared to existing systems, showing it outperforms fully supervised methods and performs comparably to state-of-the-art multi-hop retrievers. Ablation studies confirm the importance of each component in the system. The downstream QA performance, when combined with a reader model, shows strong results, slightly outperforming the NDR model. The talk concludes with the effectiveness of using language models for few-shot reranking in multi-hop QA tasks.</sample>
    <sample id="101">The fluency of PaLM is comparable to state-of-the-art systems, as per human evaluations.</sample>
    <sample id="102">A watermarking method should be applicable to embedding services, not degrade utility, be covert enough to be removed by attackers, and be transferable to attacker services during model extraction.</sample>
    <sample id="103">The content does not specify the 14 different languages into which the English TED talks have been translated.</sample>
    <sample id="104">The presentation does not specify the exact number of instances sampled from one dataset for reannotation.</sample>
    <sample id="105">The cosine and L2 similarity metrics are used to measure the difference between benign and backdoor datasets.</sample>
    <sample id="106">The paper 'Quest' by Shitanya and collaborators focuses on the challenges of handling complex information retrieval queries with implicit set constraints. Using the Quest dataset, which includes over 3,000 entity-seeking queries, the study evaluates the effectiveness of various retrieval systems. The dataset is constructed from Wikipedia categories and involves paraphrasing queries to ensure natural language fluency. Annotators validate the queries and mark evidence for document relevance. The evaluation shows significant room for improvement, especially in queries with set intersections and differences, which have the lowest F1 scores. The study aims to help future researchers develop better systems for information-seeking scenarios with selective information needs.</sample>
    <sample id="107">Multilingual encoder-based models, such as XLNet and BERT, were evaluated in both monolingual and multilingual settings. They were found to perform well in the encoder-decoder model setting, with significant performance gains in most natural languages except English, and were used to improve cross-lingual transfer performance.</sample>
    <sample id="108">KostV Sinha and colleagues explore the robustness of language models' acceptability judgments using the minimal pair paradigm. They critique the current evaluation method, which relies on short sentences, and propose a new approach that assesses models over longer contexts. By simulating longer sequences and using prefixes from the same or different datasets, they reveal that models' judgments are highly sensitive to context, especially when matching grammatical structures. Their findings suggest that existing models may not fully capture abstract knowledge within a context window, as they are influenced by latent features shared across sentences. The study calls for a reevaluation of how language models are assessed, advocating for methods that consider the broader context in which they operate.</sample>
    <sample id="109">The presentation introduces 'Unnatural Instructions', a dataset of natural language instructions created without human labor. This dataset was generated by prompting a GPT-3 model to create instructions and corresponding inputs and outputs, with additional paraphrases to enhance diversity. The dataset contains 64,000 examples, with 240,000 when considering paraphrases, and is designed to test the creativity and diversity of language models. The dataset was used to fine-tune an 11-billion-parameter T5 model, which outperformed baselines on several benchmarks, including Supernatural Instructions, T0, and Big Bench. The process highlights the potential of automated data generation in improving language model performance, offering a more diverse and creative dataset than traditional methods.</sample>
    <sample id="110">Hi, I'm Si Yuan from Fudan University. I'm here to introduce our work, Distinguishing Script Knowledge from Large Language Models for Constrained Language Planning. In everyday life, humans often plan their actions by following step-by-step instructions in the form of scripts. Previous work has explored language models to plan for abstract goals of stereotypical activities, such as make a cake. However, planning for specific goals with constraints, such as make a chocolate cake, remains understudied. In this paper, we define the problem of constrained language planning, which imposes different constraints on the goals of planning. A good planner should write scripts that are reasonable and faithful to constraints. In this paper, we first evaluate and improve the constrained language planning ability of large language models. Since no data set of specific goals exists to support our study, we have to acquire these goals first. As shown in the table, we extend the abstract goals with multifaceted constraints for human-in-the-loop data acquisition using InstructGPT. We sample 100 specific goals and evaluate the scripts generated from large language models. This table reports the overall accuracy of the results. We find that all large language models achieve unsatisfactory results on planning for specific goals. Then, we conduct detailed analysis to investigate why large language models fall. Results in the figure show that the semantic completeness in generated scripts is acceptable, but the faithfulness to the constraints cannot be guaranteed. We dig into more granular topic categories of constraints defined in wikiHow. The heatmap in the figure shows that the planning performance of InstructGPT varies considerably for goals of different categories. Previous studies have shown that the output quality of large language models varies, leading to bad performance. Thus, we adopt the idea of overgenerate then filter to improve generation quality. We first show constraint types with examples for InstructGPT and obtain specific goals based on the said abstract goals. Then, InstructGPT overgenerates key scripts for specific goals. Next, a filter model is developed to select the faithful scripts. We convert scripts and goals into InstructGPT embeddings and calculate cosine similarity as similarity scores to measure semantic similarity. In addition, we award the script that contains the keywords of the target constraint. We only keep the script if the target goal scores the highest in the goal set. With our method, InstructGPT can generate scripts of higher quality. Our method greatly improves the planning ability, both in semantics completeness and faithfulness to the constraint. Since large language models are costly to deploy, it's essential to enable language planning ability of smaller and specialized models. Creating data set is an essential step to its end. However, previous studies do not enable planning for specific goals, and manual data annotation is expensive. Thus, we follow the idea of symbolic knowledge distillation to distill constrained language planning data sets from large language models. We apply our method for building a data set of constrained language planning, named as CodeScript. In total, we generate 55,000 specific goals with scripts. To ensure the quality of validation and test sets, we ask crowd-sourced workers to find and revise the incorrect samples. This figure shows the constraint distribution of CodeScript. We find CodeScript shows high plausibility in the generated specific goals. With CodeScript, we can train smaller but specialized models for constrained language planning. We find TFI fine-tune on the CodeScript can generate scripts of higher quality than most large language models, indicating that smaller models can support larger models when properly trained on suitable data sets. In summary, we establish the constrained language planning problem. We evaluate the constrained language planning ability of large language models and develop an overgenerate then filter method for large language models. We use large language models to generate a high-quality script data set, CodeScript, for constrained language planning. We hope CodeScript data set can be a valuable resource to advance the research on language planning. Thanks for your time, please find more details of CodeScript in our paper.</sample>
    <sample id="111">The authors select a trigger set of words that appear in moderate frequency within a general text corpus, which is used to identify the number of trigger words in user sentences.</sample>
    <sample id="113">Hello, I'm James Finch. And I'm Sarah Finch. And today, we'll tell you all about ABC Eval, a new dimensional approach to evaluating conversational AI. This work was done by the Emery NLP lab, led by Professor Gino Choi at Emery University, and in collaboration with Amazon Alexa AI. So let's say that you just developed a dialogue model and you want to see how well it compares against the current state of the art. The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale. These approaches work well to provide holistic evaluations of overall dialogue quality. However, dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level. One approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses, using existing comparative or Likert scale methods. However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation. Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. We call this approach annotating behaviors in chat, or ABC Eval, in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature. ABC Eval is capable of measuring the rates at which chat models will commit various thematic errors. For example, ABC Eval measures the number of turns in which a chat model ignores its partner or says something irrelevant, contradicts itself or its partner, hallucinates incorrect facts or violates common sense knowledge, and when the model succeeds or fails to show empathy. To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC Eval. For comparison, we also evaluated these conversations using three existing methods: Likert ratings on the turn level, Likert ratings on the dialogue level, and dialogue level pairwise comparisons. For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue. Since this is the standard practice for evaluating chat models along multiple dimensions. From our analyses of these evaluation results, we found that ABC Eval behavior labels are overall more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly labeled conversations. In addition, ABC Eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis. For example, you can see how measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality respectively, while the average Likert consistency scores explain only 4% or less. Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression. You can see how the combination of all ABC Eval metrics explains over 25% of conversation quality. And as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality. On the other hand, the combination of all turn-level Likert metrics explains far less of the quality, and fewer of these metrics carry unique information. These reliable, informative, and distinct ABC Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve. You can see that in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common sense violations in around 20% of their responses. They produce irrelevant information in around 15% of the responses. And they contradict themselves or their partner around 10% of the time. With the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted. However, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models. We hope ABC Eval can be leveraged by others in the field as a meaningful step in this direction, and we look forward to seeing how conversational AI will advance in the coming months and years.</sample>
    <sample id="114">The presentation introduces a project from Nanyang Technological University focused on reducing the parameter size of large language models, specifically multi-head attention mechanisms, to make them more efficient. The current models are resource-intensive, requiring extensive computational power and memory. The proposed solution, Group Head Attention, employs a divide-and-conquer strategy to compress attention heads, resulting in significant parameter reduction without sacrificing performance. The method involves group constraint training to make heads more similar and a voting-to-state algorithm to prune redundant heads, achieving up to 90% parameter reduction. The models demonstrate improved performance on tasks like machine translation, language modeling, and abstract summarization, with notable improvements in BLEU scores and compression rates. The research suggests that future work could focus on task-specific pruning, potentially enhancing efficiency further.</sample>
    <sample id="115">The approach uses a speech segment size of 1.6 seconds.</sample>
    <sample id="116">Entity-specific knowledge needed includes Servin being a judge and Kea being a baker.</sample>
    <sample id="117">The most important factor is the quality of the examples, as selecting prompts from high-quality translations yields better performance.</sample>
    <sample id="118">The presentation introduces SwitchMLM, a novel approach to improve pre-training techniques for code-switched NLP, focusing on multilingual contexts. The method addresses the limitations of existing models like mBERT and XLM-R by introducing SwitchMLM, which is designed to handle code-switching by identifying switch points in text. SwitchMLM uses a surrogate method, FrequencyMLM, to assign language tags based on negative log-likelihood in monolingual corpora. The architecture is modified with residual connections and auxiliary losses to enhance switch point information encoding. Experiments show that SwitchMLM, combined with residual connections, outperforms standard models in tasks like sentiment analysis. Proving classifiers confirm the increase in switch point information, validating the effectiveness of the proposed methods.</sample>
    <sample id="119">The paper focuses on GPT-4, GPT-3, BERT, and their variants in the extended experiments.</sample>
    <sample id="120">The model uses attention scores from the last layer, lambda, to determine the stability of the received information.</sample>
    <sample id="121">Direct inference examples include using the name of the song, such as 'Easy on Me,' or its position, like the first one.</sample>
    <sample id="122">The authors are affiliated with the School of Computer Science and Technology, Fudan University, and the School of Information Technology, Shanghai Jiao Tong University.</sample>
    <sample id="123">Ying and Ji Yang present their research on Multi-Teach, a multimodal instruction tuning benchmark dataset designed to enhance the zero-shot learning capabilities of large language models. The study addresses the lack of multimodal instruction datasets, creating a dataset with 62 tasks across 10 categories. They use the OFA model for training and testing, employing a unified sequence-to-sequence format. The research demonstrates that instruction tuning significantly improves model performance on unseen tasks, with transfer learning from natural instruction datasets further enhancing results. A new metric, sensitivity, is introduced to measure the model's consistency across different instruction variations. The study concludes with plans to release a larger dataset with 150 additional tasks.</sample>
    <sample id="124">Tan-Ching Yu from the National University of Singapore presents a study on improving the temporal reasoning capabilities of Large Language Models (LLMs). The research identifies three levels of temporal reasoning: time-to-time, time-to-event, and event-to-event. The study critiques the overemphasis on time-to-event reasoning in previous works and introduces the Temp-Reason dataset, which covers all three levels and spans long temporal coverage. The research evaluates LLMs using the Temp-Reason dataset, comparing models like T5, ChatGPT, and TempT5. The findings reveal that while T5 and TempT5 show improvement over baseline models, ChatGPT struggles with temporal reasoning, especially in month predictions. The study proposes a training strategy with temporal span extraction pre-training and time-sensitive reinforcement learning to enhance LLMs' temporal reasoning. The research concludes by highlighting the need to address temporal biases in LLMs and suggests future work to overcome these biases.</sample>
    <sample id="125">There are two authors involved in the paper.</sample>
    <sample id="126">Yes, translating the natural language query using a machine translation model before semantic parsing was considered as one of the settings for evaluation.</sample>
    <sample id="127">Namjoohol, a master's student at KAIST AI, presents a study on transferring reasoning abilities from large language models to smaller ones using a technique called 'chain of thought prompting.' The method involves using large models to generate step-by-step solutions for complex tasks, which are then used to train smaller models. The research introduces 'diverse reasoning,' a novel approach that generates multiple solutions to enhance the training process. The study demonstrates that this method significantly improves performance on various tasks, especially text-based ones, compared to traditional fine-tuning methods. The research also highlights the scalability of the method and discusses trade-offs related to development and inference costs. The paper provides extensive analysis and results, along with code and data for further exploration.</sample>
    <sample id="128">The presentation by Akshata, Martin, and their collaborators introduces the KITMOS dataset, designed to evaluate knowledge integration from multiple sources in natural language understanding tasks. The dataset challenges models to integrate both pre-trained and inference-time knowledge, simulating real-world scenarios where new information may not be present in pre-trained data. The study introduces a co-reference resolution task, where models must identify the correct referent of pronouns in sentences. The dataset is tested across three settings: background pre-trained, background both, and background inference, each varying the availability of knowledge sources. The results show that while some models improve with task-specific training, they still struggle with integrating inference-time knowledge, highlighting the need for further research in this area.</sample>
    <sample id="129">The authors used 'women of color' as an example of a marked group in their study.</sample>
    <sample id="130">The content does not specify which model architectures do not generalize well; it only states that Transformer models generally generalize better.</sample>
    <sample id="131">The testing datasets are not named in the given content.</sample>
    <sample id="132">The paper is authored by Akshata, Martin, and a collaborator from McGill University, MILA, and Microsoft Research.</sample>
    <sample id="133">The author works with multiple modalities, not just text.</sample>
    <sample id="134">Hi, I am Janis Lavraque and I will present to you our works on Dr. Bert, a robust pre-trained model in French for biomedical and clinical domain. In this presentation, we first talk about language modeling in healthcare. Then we present the main contribution of our article. We introduce the first biomedical model in French named Dr. Bert, which is based on Roberta and trained on Natus, which is a data set of medical crawled data from the web. We also introduce a comparison of model with multiple pre-training settings and data sources. Then we present our result on 11 biomedical and clinical downstream tasks in French. And finally, we conclude about the experiments and give you more details about how to access the models. Since its release in 2018, Bert has become one of the most effective approaches to solve natural language processing tasks and offer huge performance gain compared to historical static and contextualized methods such as Word2Vec, FastText, or NLTK. However, since then, this model has been adapted to many other languages, like in French with Camembert, and other domains like biomedical with Permitted Bert and Bio-Bert, and on clinical with Clinical Bert, but mostly in English. Specialized models for other languages are scarce and are often based on continuous pre-training due to the lack of in-domain data. However, French did not have any open source model for biomedical until now. So we ask ourselves question about what is the most appropriate data sources for a wide range of usage, and those crowd data are good substitution for clinical data. To answer this question, we compare Dr. Bert with our Shubert model, which is a clinical model with four gigabytes of sentences taken from clinical notes. And finally, we ask ourselves how much data do we need to train a specialized model on French data? Is it four gigabytes, eight gigabytes, or more? To answer this question, we first train and compare four from scratch model, a first version of Dr. Bert with seven gigabytes of Natus, a second version of four gigabytes of Natus, a first version of Shubert, which is a clinical model with four gigabytes of sentences taken from clinical notes, and a final version of Shubert with a mix of four gigabytes of Natus and four gigabytes of clinical notes. In addition to this comparison, we introduce three model trained on continuous pre-training to analyze the impact of pre-training strategy. One based on the weight of Camembert and trained on four gigabytes of Natus, another also based on Camembert but trained this time on the four gigabytes of clinical notes, and finally one based on English biomedical model Permitted Bert and trained on four gigabytes of Natus. In total, we have seven models. To evaluate our seven models, we gather each public and private downstream task such as name entity recognition, classification, part of speech tagging, and question answering. This model are compared to six baseline model which are Camembert, Oscar, four gigabytes, Camembert, CCNet, four gigabytes, Permitted Bert, Bio-Bert, and Clinical Bert. The evaluation of highlights that model perform best on the task with data of the same nature as those on which the model has been trained. However, we can obtain the data from heterogeneous sources appear to be more versatile. We also observe that using more data translate into better performance. In overall, from scratch training seem to obtain higher performance on most of the task. However, our experiment on consumer pre-training using the weight and tokenizer of Permitted Bert trained on the four gigabytes of Natus show comparable result to those obtained with Dr. Bert four gigabytes from scratch, which is not the case for the model based on Camembert weights and tokenizer, which suffer from stability issues. Finally, as a conclusion, our proposed system offer better performance on nine of the eleven downstream tasks and surpass globally the result of the generic model here, Camembert. We also observe that specialized data is better, more specialized data is better, but it doesn't scale well. All the pre-trained model obtained from Natus are freely available and on Hugging Face, and all the training script are on our GitHub repository. So thank you for this presentation, and we are looking forward to exchange at the poster session in Toronto.</sample>
    <sample id="135">The conversation introduces ABC Eval, a new method for evaluating conversational AI developed by the Emory NLP Lab in collaboration with Amazon Alexa AI. This method aims to reduce the subjectivity of human evaluations by annotating specific behaviors in chat, such as relevance, contradictions, and empathy. ABC Eval was tested on 100 conversations per model, showing higher reliability and predictive power compared to traditional methods like Lickert ratings. The method identifies specific errors in AI models, such as self-contradictions and irrelevant information, and provides a more detailed understanding of model performance. The results indicate that ABC Eval metrics are more reliable and informative, explaining over 25% of conversation quality, while traditional methods explain less. The conversation highlights the importance of precise evaluation metrics in advancing conversational AI.</sample>
    <sample id="136">In this presentation, the speaker, Javed An, discusses their research on improving numerical reasoning in language models. The work, conducted with supervisor Nafisa at the University of Sheffield, introduces FERMAT, a flexible evaluation set that assesses models based on arithmetic types, mathematical operations, and training dependency. The research highlights the limitations of current benchmarks, which often rely on accuracy scores that do not reflect a model's mathematical capabilities. FERMAT evaluates models through a variety of number representations and mathematical operations, revealing that language and mathematical diversity significantly impact performance. The study also explores the role of training templates and the importance of number encoding and tokenization. The findings suggest that existing benchmarks are unrepresentative, and single scores are insufficient for evaluating numerical reasoning. The speaker encourages further reading of the paper and provides QR codes and links for additional resources.</sample>
    <sample id="137">The paper introduces Teltodiseign, a dataset for language-guided floor plan generation, addressing the need for designs that meet specific user requirements. Unlike traditional text-to-image models, Teltodiseign focuses on generating floor plans from detailed language instructions, which include semantics, geometry, and topology. The dataset comprises 5,051 human-annotated instructions and 76,000 artificially generated ones. The research challenges include strict design constraints, understanding complex instructions, and handling ambiguous information. The proposed solution is a sequence-to-sequence model using a transformer-based encoder-decoder framework, which outperforms existing text-to-image models in generating floor plans that align with user instructions. The study highlights the importance of both artificial and human instructions in training, and the potential for future research in language-guided design generation.</sample>
    <sample id="138">The authors claim that the integration of knowledge from different sources at inference time is an understudied area in NLU.</sample>
    <sample id="139">The speakers are Yin and Ji-Yang.</sample>
    <sample id="140">Yes, Coscript underwent quality checks where crowd-sourced workers were asked to find and revise incorrect samples to ensure the quality of the dataset.</sample>
    <sample id="141">Existing resources for context-dependent translation are limited in scope, supporting only certain types of translations and specific languages, often relying on domain knowledge and human curation.</sample>
    <sample id="143">The approach is compared to the WETKEY and LOCALAGREEMENT strategies, as well as state-of-the-art architectures specifically tailored for SimulST.</sample>
    <sample id="144">The authors are affiliated with the University of Toronto, the University of Montreal, and the University of Montreal's Institute for Information Technology Research.</sample>
    <sample id="145">The speaker's name is Jenny.</sample>
    <sample id="146">This talk by Zhou Yichen, a PhD student from Fudan University, focuses on the analysis of omission in dialogue summarization. Dialogue summarization, a subtask of text summarization, involves creating concise summaries that represent the most important information within a dialogue. Despite advancements in large-scale pre-trained language models, these models often generate summaries with factual errors, including omissions, which are prevalent and affect the quality of the summaries. The speaker presents a study analyzing the omission rate in summaries from five domains and six pre-trained models, revealing a high omission rate of about 70%. The talk also discusses the distribution of omitted information and the challenges in identifying key information. To address these issues, the speaker introduces a dataset for omission detection, which includes high-quality omission labels and evaluates the performance of various models using the F1 score. The talk concludes with a discussion on the potential improvement in summary quality through post-editing based on detected omissions.</sample>
    <sample id="147">The paper involves three authors: Myra, Eszter Mosh, and Dan Jurafsky.</sample>
    <sample id="149">Yes, the dataset is available on GitHub.</sample>
    <sample id="150">Archie presents the Meeting QA dataset, a novel resource for NLP research focusing on question-answering in meeting transcripts. This dataset, derived from the AMI corpus, includes 7,700 questions and answers, highlighting the complexity of real-world meeting discussions. The study explores various models, including short-context and long-context, single-span and multi-span, and fine-tuned versus zero-shot models. Results show significant performance gaps between models and human performance, with models struggling to handle rhetorical questions and speaker identification. The dataset is challenging for current models, indicating its potential to drive future advancements in NLP.</sample>
    <sample id="152">Frederic Riemenschneider presents a project at the intersection of NLP and classical philology, focusing on language models for ancient Greek and Latin. The project introduces Greberta and GRETTER, monolingual models for ancient Greek, and Filberta and Filter, multilingual models for multiple languages. The models are pre-trained using a new corpus from the Internet Archive, which includes OCR settings for Greek texts. Benchmarks show these models outperform existing ones in tasks like part-of-speech tagging, dependency parsing, and lemmatization. The study also explores the performance of the T5 model's encoder and the impact of multilinguality. The results indicate that while multilingual models do not significantly outperform monolingual ones, they still offer substantial improvements. The project aims to provide powerful language models for classical philology, capable of processing multiple languages with high accuracy.</sample>
    <sample id="153">Nina Rehmayr-Abraham, a scientist at Amazon Alexa, discusses her work on resolving ambiguities in text-to-image generative models. Ambiguities in prompts can lead to inaccurate image generation, so her team developed a framework to disambiguate these prompts using clarifying questions or multiple visual setups. They also created an automatic evaluation framework to assess the faithfulness of generated images to user intentions. Their research, which uses a modified version of the LAVA corpus, shows that disambiguation improves image generation accuracy. The findings align with human evaluations, demonstrating the framework's reliability. The work aims to enhance the accuracy and reliability of text-to-image models.</sample>
    <sample id="154">The authors are affiliated with the University of Trento and Fondazione Bruno Kessler.</sample>
    <sample id="155">The speaker's name is Javad Hosseini.</sample>
    <sample id="156">Hello everyone, my name is Aydil Bilard and I will be giving a short overview of the paper, prompting PAMP from translation, assessing strategies and performance. This is joint work with my colleagues from Google Translate. PAMP is a 540 billion parameters large language model presented last year in 2022. It is trained on a large collection of text comprising 780 billion tokens. At the time of publication, it achieves state of the art in hundreds of NLP tasks. In this work, we present the first systematic study of large language model prompting for machine translation. We evaluate the translation capability of such models using the best practices of the AMT community. We compare two state of the art systems, so the best performing systems are the WMT evaluation. We use state of the art neural MT metrics and additionally also show expert based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the LLMs for translation. As we can see in a simple experiment where we use one shot prompting and provided two different prompts for each sentence. The majority of sentences, 516 out of 1000, the difference observed is of more than one blur points. And this can go in extreme cases up to 40 blur points. So it's important to select a good prompting strategy. In our experiments, we settled for a five shot prompting strategy where we just mark each sentence that we provide to the system with the language it's in. So in this example here where we perform translation from German into English, the German sentences, the source sentences are marked with German colon and the English translations with English colon. We saw that the actual form of the prompting doesn't have a big influence in the case of zero and one shot prompting. It's crucial for zero and one shot prompting. But when we go as in our case to five shot prompting, there is nearly no difference to the actual form of the prompting. It's crucial for example quality than the similarity to the source sentence. So it's important to select the examples from high quality translations. In particular, we compare selecting prompts from the training data of the WMT evaluations or the dev data. The dev data is much more curated and with higher quality than the training data that it's more noisy and the results show a better performance when using the dev data. Nevertheless, specialized state of the art systems have a substantial advantage over the PAMP translation, but PAMP comes pretty close to a commercial system. In our case, we chose to evaluate with Google Translate. The insights that we gain from the human evaluation that we perform using the NPM framework is that the fluency of PAMP is comparable to state of the art systems. But the main difference comes from the accuracy. So in particular, the most common error are omission errors. So it seems that PAMP chooses to produce a better sounding translation sometimes by dropping parts of the source sentence that are made in the translation. However, the style awkward category for PAMP is lower than for the state of the art systems, which is an additional signal that PAMP provides really fluent output but still with some problems of accuracy. And that's it for this really short overview. For more details, please come by the full presentation of the paper. Thank you very much.</sample>
    <sample id="157">The presentation by Shen Gao from Sun Dong University introduces a dialogue summarization model called SDDS, which aims to distill silent information from dialogues into concise summaries. The model addresses the challenges of dialogue summarization by combining static and dynamic graph structures. The static graph is constructed using discourse parsing and dialogue state tracking, while the dynamic graph captures semantic relationships using a multi-head attention model. The fusion of these graphs is achieved through a dual cross-attention mechanism, which integrates the graph representation into the generation process. The model's effectiveness is demonstrated through a heuristic dialogue structure modeling method, speaker interaction modeling, and position information mapping. The SDDS model is designed to overcome the limitations of pre-computed dialogue structures and provide a more accurate and dynamic approach to dialogue summarization.</sample>
    <sample id="158">The presentation introduces Dual Cache, a novel approach to address the challenges of coreference resolution in long documents. Traditional cache-based methods suffer from high cache miss rates due to frequent topic switches, leading to inefficient memory usage. Dual Cache, proposed by Xiangkunhu, addresses this by using a local cache with an LRU eviction policy and a global cache with an LFU policy. This dual system allows for more efficient storage and retrieval of both local and global entities, significantly reducing cache misses. Benchmarks show Dual Cache outperforms single cache methods, especially in long documents, and maintains a high performance-cost ratio. The method is evaluated on public datasets, demonstrating its effectiveness in reducing cache misses and improving overall performance.</sample>
    <sample id="160">The first step maps the input tokens to unordered multi-sets of tokens that will appear in the output.</sample>
    <sample id="161">Coscript contains a total of 55,000 scripts.</sample>
    <sample id="162">Hello everyone, I'm Akshata, and today I'm presenting our work, the KITMOS, which evaluates knowledge integration from multiple sources. This work is a collaboration between McGill University, MELA, and Microsoft Research.</sample>
    <sample id="163">The best alignment method for DEplain is the MASS align method.</sample>
    <sample id="164">Weakly supervised learning allows for cheaper and faster data annotation by using weak labels, but it requires robust training algorithms to handle the noise and achieve good generalization.</sample>
    <sample id="165">Wenting Zhao presents a paper on 'Adapting Common Sense Reasoning: Exploiting Mutually Exclusive Explanations,' focusing on unsupervised learning for adaptive reasoning. The paper introduces LIpor, a method that maximizes the marginal likelihood of outcomes given contexts by considering mutually exclusive explanations. This approach overcomes the limitations of supervised methods, which often require subjective annotation of plausible explanations. LIpor's unsupervised objective is enhanced by a regularizer that enforces mutual exclusivity among explanations, ensuring a subset of plausible explanations is preferred. The method was tested on the ALFA-NLI dataset, outperforming zero-shot models and previous unsupervised approaches by over 4 absolute points in accuracy. This demonstrates LIpor's effectiveness in identifying plausible explanations without prior supervision.</sample>
    <sample id="166">The presentation introduces a new method for image retrieval from linguistically complex texts, addressing the limitations of existing visual language models. The proposed method combines the divide and conquer strategy with dual-process theory, using a neural symbolic reasoning framework. It consists of two main components: a proposition generator and a neural symbolic reasoner. The former converts complex propositions into simple representations, while the latter integrates these representations with logical reasoning to solve complex problems. The method outperforms existing baselines in experimental evaluations, demonstrating its effectiveness in handling complex reasoning tasks. The presentation concludes with suggestions for future improvements, emphasizing the potential of neural symbolic reasoning in enhancing large language models.</sample>
    <sample id="167">The DEplain-web corpus includes 750 documents, with 350 aligned using manual methods and the rest using automatic alignment methods.</sample>
    <sample id="168">The CoNLL++ dataset was created by collecting Reuters News articles from 2020 and annotating them using the same guidelines as the original CoNLL 2003 dataset.</sample>
    <sample id="169">The paper by Aydil Bilal and colleagues from Google Translate presents a systematic study on prompting large language models (LLMs) for machine translation, focusing on the PAMP model. The study evaluates the translation capabilities of PAMP using the latest test sets and compares it with state-of-the-art systems. The findings highlight the significant impact of prompt selection on translation quality, with five-shot prompting showing minimal difference in performance compared to one-shot prompting. The research emphasizes the importance of using high-quality translation data, such as the dev set, for better results. While PAMP's fluency is comparable to advanced systems, it lags in accuracy, often omitting parts of the source sentence. The study provides recommendations for prompt selection strategies and concludes that PAMP is a viable commercial system, though it requires further improvements in accuracy.</sample>
    <sample id="171">Existing watermark methods are either not applicable to embedding services or lack transferability, which Embedding Marker addresses by being applicable and transferable.</sample>
    <sample id="172">No, multilingual LLMs like Codex or Bloom are found to be inadequate for CLSP tasks.</sample>
    <sample id="173">Hello everyone, my name is Shu-Hung. Today I am going to present our paper, Do Conner 2003 Named Entity Taggers Still Work Well in 2023? Let's get started. Our paper investigated the problem of generalization using the NER task. We observed that models have been using Conner 2003 to develop NER for almost 20 years. This naturally raises several problems. Firstly, can these models generalize to modern data? And when we do observe poor generalization, what causes the performance drop of these models? To investigate these problems, we developed the Conner++ dataset. This is a dataset that we collected from Reuters News from 2020 and then annotated with the same Conner 2003 annotation guidelines. We then fine-tuned over 20 models on Conner 2003, evaluated them on both the Conner 2003 test set and the Conner++ test set, and calculated the percentage change in F1 to assess the generalization of each model. So what is needed for good generalization? Through our experiments, we found that there are three main ingredients that are needed. The first one is the model architecture. Through our experiments, we found that the transformer models normally generalize better to new data. The second ingredient is the model size. We found that usually larger models lead to better generalization. And last but not least, we all know that the number of fine-tuning examples directly affects the performance of a downstream task. Here we also found that more fine-tuning examples actually lead to better generalization. To our next question, what causes the performance drop of some models? We had two hypotheses. The first one is adaptive overfitting, which is overfitting caused by reusing the same test set over and over again. And this is usually manifested as the diminishing returns on a new test set. The second hypothesis is temporal drift, which is the performance degradation that is caused by the increasing temporal gap between the train and the test data. For adaptive overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than one. This means that every unit of improvement that we made on Conner 2003 translates to more than one unit improvement on Conner++, which means that there is no diminishing returns. And this shows us that adaptive overfitting in this case is not observed. So what about temporal drift then? For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data. And we found that the performance degrades with larger temporal gap. And this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is that for good generalization, we would need a better model architecture, larger model size, as well as more fine-tuning examples. And these go hand in hand. We can't just have one ingredient, but throw all the others. At the same time, we also found that the performance drop here is caused by temporal drift. And kind of surprisingly, it is not caused by adaptive overfitting, even though Conner 2003 has been used for over 20 years. So going back to the question that we raised in the title of our paper, do Conner 2003 taggers still work in 2023? And we found that the answer is actually a resounding yes. We hope our paper calls for more research on how to improve generalizations of the models. And lastly, please make sure to check out our paper, our dataset, and if you have any questions, feel free to contact me. Thank you so much.</sample>
    <sample id="174">The paper introduces ArgAnalysis35K, a large-scale dataset for argument quality analysis, highlighting its unique features. It is the largest dataset in the field with 35,000 high-quality argument pairs sourced from various debaters, including experts and novices. The dataset offers diverse arguments across 24 themes, not limited to pre-selected motions, and includes a novel analysis component that combines claims, premises, and more. ArgAnalysis35K also introduces instance-based annotator reliability, which accounts for individual biases, and a relevance model that assigns scores to arguments based on their relevance to specific themes. These features aim to provide more reliable and diverse data for argument quality analysis, offering a more nuanced understanding of argument relevance and quality.</sample>
    <sample id="175">The method addresses permutation ambiguity by using a GPU-friendly continuous relaxation technique, allowing for backpropagation to learn more plausible permutations.</sample>
    <sample id="176">Fairness in a downstream NLP model is defined by its ability to perform equitably across different demographic and political groups, without bias towards or against any particular group.</sample>
    <sample id="177">The speaker's name is Yanis Lavraque.</sample>
    <sample id="178">The speaker's name is Koussof Sinha.</sample>
    <sample id="179">The talk by Melanie Szklarek introduces Symbolic TOM, a method to enhance Theory of Mind (ToM) reasoning in large language models. ToM is crucial for understanding characters' mental states in narratives, often tested through false belief questions. Despite advancements, models like GPT-3 still struggle with these tasks. Symbolic TOM uses graphical representations to depict characters' beliefs, improving performance across various datasets. Experiments show significant gains in accuracy, especially in out-of-domain scenarios, outperforming supervised methods. The method is robust, avoiding overfitting and providing interpretable reasoning, thus enhancing LLMs' understanding of complex narratives.</sample>
    <sample id="180">The speaker's name is Myra.</sample>
    <sample id="181">The paper by Xiyuan from Fudan University addresses the challenge of constrained language planning, which involves planning actions with specific constraints. While large language models have been effective in planning for abstract goals, they struggle with specific, constrained goals. The study evaluates the constrained language planning ability of these models and introduces an over-generated then-filter method to improve script quality. The authors use a dataset called CodeScript, generated from large language models, to train smaller, specialized models for better performance. The paper aims to provide a valuable resource for further research in language planning.</sample>
    <sample id="182">Tropicalism in the context of this paper indicates a stereotype associated with Latina women, characterized by descriptors like 'vibrant' and 'curvaceous,' which perpetuates a narrow and exoticized view of their identity.</sample>
    <sample id="183">The human-written portrayals were created by giving prompts to human subjects, which allowed for the surfacing of racial stereotypes, providing a basis for comparison with the model-generated personas.</sample>
    <sample id="184">The work used CxMI (Contextual Usage Measure) to measure context usage by machine translation models, which was extended to point-wise CxMI to assess context usage at the sentence and word levels.</sample>
    <sample id="185">DrBERT is a French biomedical model trained on medical data, while ChuBERT is a clinical model based on clinical notes, both utilizing pre-trained models.</sample>
    <sample id="186">Hi, I'm Maya, and today I'll be talking about our paper, Marked Personas, using natural language prompts to measure stereotypes in language models. This work is done in collaboration with Eszter Mosh and Dan Jurafsky. In recent years, many have documented the prevalence of social bias and stereotypes in large language models, or LLMs. However, these measures have various limitations. They usually rely on hand-constructed datasets that are very time-consuming to curate, and they usually only measure very specific stereotypes, meaning that they don't generalize well to other demographics or contexts, or they simply capture very general broad associations, like negative associations with particular groups. Furthermore, most work in this space doesn't account for intersectionality, which is the notion that multifaceted social identities can compound biases and be unique loci of harm. To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions and prompts. So we can ask the model to generate a persona, which is a depiction of an imagined individual using a prompt to like, imagine you are an Asian woman, describe yourself. And we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt. So here are some example generations from GPT-4. Immediately, we see that while the outputs aren't overtly negative or toxic in the traditional sense of these words, there are some interesting patterns. The Asian woman is depicted as unassuming, the Middle Eastern woman is referred to using words like exotic and like referring to a mesmerizing region, and both of the women of color personas make references to ancestry, while the white man persona has nothing of the sort. To capture these patterns, our method has two parts. The first one is generating these personas. Our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects, finding that by giving it to human subjects, they also were able to surface racial stereotypes. And also, this enables direct comparison between our generated personas and the human-written responses. The second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I'll elaborate on shortly. The benefit of this is that we get really specific stereotypes and patterns without having to rely on any specific lexicon. So the marked words method draws upon the sociolinguistic concept of markedness, which states that there is an unmarked default, and any group that deviates from that default is linguistically marked. So for instance, the word warrior is usually associated with men. So when people are describing a warrior who is a woman, they'll usually actually specify woman warrior and mark the term with woman. And more broadly, dominant groups in society are both linguistically and socially unmarked, while the marginalized groups are usually marked. So in our method, we first designate what the unmarked and marked groups are. And then we compare the personas using the fighting words method, which is basically using weighted log odds ratios to distinguish the top words for each marked group. So for instance, for the personas of black women, we would do fighting words and compare the log odds ratios against both white personas and man personas, because those are the two corresponding unmarked groups. Now for some results. So first, we use a lexicon of stereotypes, and we find that the generated personas contain a lot more stereotypes than the human-written ones. However, when we actually look at the distribution of the words in the lexicon, we find very different things. So while the generated personas have much higher rates of the lexicon words, the human-written ones have a much wider distribution of words, while the stereotype words that are in the generated personas are really just the words tall and athletic. So really just the positive or at least non-negative ones. And in fact, this lexicon doesn't really capture many of the harmful patterns that we saw in the earlier slides while at all. So instead to do that, we'll turn to the results from our marked words method to show how these seemingly positive words facilitate stereotypes and essentializing narratives. In our analysis, we reveal how these seemingly positive portrayals reflect harmful patterns. First, for mark groups, the top words include things like culture, tradition, proud, and exotic. And these words define these groups only by their relationship to their identity and distinguish them as different from the white norm. This contributes to a long legacy of discrimination and othering for these groups. Furthermore, there's a lot of common tropes that are reflected in these words, especially for women of color. So for example, the words describing Latina women include things like vibrant and curvaceous, which connects to a trope of tropicalism. For Asian women, the words are things like petite and delicate and silky, which connects to a long history of Asian women being hypersexualized, seen as very docile and submissive, and so on. And finally, for black women, we see that some of the top words are things like strong and resilient. This connects to an archetype that people have called the strong black woman archetype. And while it sounds like positive at first glance, there's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles. So rather than actually working towards changing those obstacles, it puts pressure on those people to overcome them, which leads to very negative health outcomes for these people, among other harms. More broadly, we find that the words for each marked group pretty much reflect very essentializing narratives. So based on these patterns, we conclude with three recommendations for model owners. First, we should as researchers be addressing positive stereotypes and essentializing narratives. We should also be using intersectional lens to study biases and harms because there's a lot of things that might be overlooked if we don't do that. And finally, there should really be increased transparency about bias mitigation methods, because for instance, like these positive stereotypes, we don't know if it's because there is some sort of like weird overly excessive value alignment going on, or maybe some other like anti-stereotyping methods that are resulting in these pernicious patterns. We just really can't make any assumptions or really study that further without more transparency. Thank you so much for listening. Have a good time at ACL.</sample>
    <sample id="187">The paper is authored by Ying and Ji-Yang.</sample>
    <sample id="188">Iterative transfer learning involves updating a model with new data collected in each round of active learning, allowing for continuous improvement and adaptation.</sample>
    <sample id="189">The goal of the dataset is to collect a large-scale public dataset for testing entity resolution in conversational systems, covering music, books, and recipes domains.</sample>
    <sample id="190">An attacker can extract model parameters by analyzing the similarity between embeddings from benign and backdoor datasets, and by applying techniques like t-SNE to visualize and differentiate between them.</sample>
    <sample id="191">The paper involves three authors: Sara Papi, Matteo Negri, and Marco Turilli.</sample>
    <sample id="192">Yang Luo presents CAN, a novel optimizer for large language model training that addresses the trade-off between memory efficiency and convergence speed. Traditional optimizers like Adam and Adafactor, while memory-efficient, suffer from slow convergence and error-prone updates. CAN introduces a confidence-guided approach to mitigate these issues, using residuals to adjust updates and improve stability. Experiments on datasets like BookCorpus and English Wikipedia demonstrate CAN's superior performance, achieving higher validation accuracy and reduced memory usage compared to Adam and Adafactor. CAN also excels in training large models, maintaining performance with reduced memory costs. The results show CAN's effectiveness in both large and small models, making it a valuable extension to existing memory-efficient optimizers.</sample>
    <sample id="193">43 annotators were used to create the initial dataset.</sample>
    <sample id="194">The authors of the paper are affiliated with Carnegie Mellon University, the University of Washington, and the Allen Institute for AI.</sample>
    <sample id="195">The paper introduces ROHT, a framework for explainable question answering that addresses the limitations of existing methods by integrating knowledge from heterogeneous sources. ROHT is a two-stage process: first, it builds a hierarchical question decomposition tree (HQDT) to understand the structure of complex questions, and second, it uses probabilistic reasoning over the HQDT to fuse knowledge from different sources. The framework is evaluated on the KQAPRO and MUSE datasets, demonstrating its effectiveness in improving performance over existing methods. ROHT's ability to integrate answers from sub-questions and its superior performance with supplementary text corpora highlight its potential in enhancing explainable question answering.</sample>
    <sample id="196">The example given is 'I saw Bart and Lisa'.</sample>
    <sample id="197">The state-of-the-art models in dialogue systems include GPT-3, BERT, T5, and BART.</sample>
    <sample id="198">Evaluating models' acceptability throughout the context window is necessary because current methods like the minimal pair paradigm may not fully capture a model's understanding of language, especially as models are now handling longer contexts.</sample>
    <sample id="199">Yes, training in multilingual fashion caused a performance drop in English in seven out of nine datasets, known as the curse of multilinguality.</sample>
    <sample id="200">Yes, the annotators know the name of the entities in advance.</sample>
    <sample id="201">The evaluation used state-of-the-art neural MT metrics.</sample>
    <sample id="202">The paper does not specify if the regression in generalization impacts specific NER types.</sample>
    <sample id="203">Positionality in NLP matters because it affects the representational fairness and effectiveness of models and datasets, potentially marginalizing certain demographics and perspectives.</sample>
    <sample id="204">The multilingual LLMs like BLOOM were fine-tuned with adapters.</sample>
    <sample id="205">Xiangbing, a PhD student at the University of Washington, presented research on the influence of political biases in language models, from pre-training data to downstream tasks. The study highlights how political media coverage in pre-training data can lead to biases in language models, affecting their performance in tasks like hate speech and fake news detection. Preliminary results show that models like GPT-4 exhibit liberal leanings, and further training on partisan corpora can shift these biases. The research also indicates that language models reflect societal polarization, with models trained on data from different political eras showing varying political leanings. The study underscores the fairness issues arising from these biases, particularly in applications like social media moderation, where biased models could marginalize certain groups. The dilemma of sanitizing political content in training data is likened to the 'electric trolley problem,' where both censorship and bias propagation present significant challenges. The presentation calls for addressing these fairness issues to ensure equitable outcomes in NLP applications.</sample>
    <sample id="206">They use a model trained on a task called 'topic independent dissonance stance classification' for transfer learning.</sample>
    <sample id="207">The recent test sets used to assess the PaLM capabilities are the WMT evaluation test sets.</sample>
    <sample id="208">The authors proposed three recommendations at last.</sample>
    <sample id="209">The proposed method achieves a gain of 2.1% over the strongest baseline.</sample>
    <sample id="210">The speaker's name is Shu-Hung.</sample>
    <sample id="211">Yes, the results and dataset in the paper can be used as a benchmark for automatic text simplification.</sample>
    <sample id="212">The paper experiments with 15 smaller models.</sample>
    <sample id="213">The base model used for investigating multi-model instruction tuning is the OFA (Unified Multi-Modal Pretrained Model).</sample>
    <sample id="214">Hello everyone, my name is Jingwei Yi from the University of Science and Technology of China. It's my pleasure to give a short advertisement video of our paper, Are You Copying My Model? Protecting the Copyright of Large Language Models for Embedding Ad Services via Backdoor Watermark. Let's first introduce the background about embedding ad services. Currently, large language models such as GPT, LAMA, PALM are exceptional in natural language understanding and generation. Embedding ad services is one of the services built upon large language models to assist various NLP tasks. For example, OpenAI offers a GPT-based embedding API. However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services. Therefore, it's necessary to protect the copyright of embedding ad services. To protect the copyright of embedding ad services, one of the solutions is to embed a watermark in the provider service and detect whether another service contains the watermark. The watermark method needs to meet the following properties. First, the method should be applicable to embedding ad services. Second, the watermark should not degrade the utility of the provided embeddings. Third, the watermark should be covert enough to the attacker, or the attacker can remove the watermark easily. Finally, the watermark needs to be transferable to the attacker's services during the model extraction process. Existing works can be broadly classified into four categories. However, these methods either not applicable to embedding ad services or lack of transferability. Therefore, in this paper, we propose embedding marker, which is a backdoor-based watermark method applicable to embedding ad services. Then, let me introduce the details of our embedding marker. Embedding marker contains two main steps, watermark injection and copyright verification. Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval. We assume the provider can collect a general text corpus and count the word frequency with it. In watermark injection, we first define a target embedding. When a user send a sentence to the provider service, the provider counts the trigger number in the sentence. The provided embedding is a weight summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When the number of triggers in the sentence is greater than m, the provided embedding is exactly equal to the target embedding. Copyright verification is to detect whether a model behind another service contains the watermark. We first construct a backdoor and a benign dataset. Backdoor dataset contains sentences of which all words belong to the trigger set, while all words in the sentences of the benign dataset do not belong to the trigger sets. Then, the provider request embeddings from the stealer service with the dataset. The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between the benign and the backdoor dataset, which is defined as delta cosine and delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric. We conduct experiments on four datasets, AG NEWS, MIND, SSTD2 and EARS FAM. We assume the provider apply wiki text dataset to count word frequency. The results on four datasets show that our embedding marker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embedding by visualizing the embedding of sentences on four datasets. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between the backdoor embeddings and normal embeddings. That's all, thank you. We will come to discuss with us.</sample>
    <sample id="215">Adam Szarkowski's talk focuses on the dependency structure of coordination in linguistics, comparing symmetric and asymmetric approaches. Symmetric structures, like those in Universal Dependencies and Meaning-Text Theory, select one conjunct as the head, while asymmetric structures, such as the Prague approach, have the conjunction as the head. The talk introduces a multi-headed approach, like in De Cat's Word Grammar, where all conjuncts are heads. Szarkowski presents an argument for symmetric structures, based on the principle of dependency length minimization, which suggests shorter dependencies are preferred. He provides examples to illustrate this principle, showing that even if a sentence violates a general grammatical principle, it can be acceptable if it satisfies the principle of dependency length minimization. Statistical analysis from the Penn Treebank supports the observation that left conjuncts tend to be shorter, especially when the governor is absent. This supports the argument against asymmetric structures.</sample>
    <sample id="216">Hi, I'm Sara Papi from the University of Trento and Fondazione Bruno Kessler and I will briefly introduce the Attention as a Guide for Simultaneous Speech Translation paper. That is a joint work with Matteo Negri and Marco Turilli. What is simultaneous speech translation? Simultaneous speech translation, or SimulST, is the process of translating spoken language into text in another language in real time, enabling cross language communication. What are the problems of the current SimulST models? Specific architectures are usually trained introducing additional modules to be optimized, long and complicated training procedures, for example training involving different optimization objectives, and training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example training a model with an average of one second latency and another one with two seconds latency and so on. And training and maintaining several models to reach different latency regimes, for example</sample>
    <sample id="217">The research introduces Scene2Unseen, a method for multi-attribute controllable dialogue generation, addressing the limitations of existing models that focus on single attributes. The method, D-CGE, employs disentangled controllable generation to learn attribute concepts from scene values and uses disentangle loss to manage attribute combinations. A unified evaluation framework, M-AE, is proposed to evaluate different granularities of attributes without needing large-scale labeled data. Experiments show that D-CGE outperforms baseline models in attribute controllability and test quality, especially with attribute-oriented prompts. The method also demonstrates the ability to generalize from seen attributes to unseen combinations, outperforming models like E2C and A2C. The research concludes that the proposed method effectively transforms scene attributes into unseen attribute combinations, outperforming other models in both controllability and test quality metrics.</sample>
    <sample id="218">The authors of the paper are affiliated with Google Translate.</sample>
    <sample id="219">The research presented by Jia Huizhu and colleagues focuses on a multi-stage pipeline for uncovering financial signals in annual reports, specifically the Form 10-K. The work addresses the challenge of extracting useful information from these reports, which are often similar in content. The pipeline is designed to compare and contrast target and reference reports, identifying important words and their importance. The process involves document segmentation, relation classification, and fine-tuning using external datasets. The model classifies word pairs into three types: type beta, revised, and mismatch, and uses soft labeling techniques to improve performance. The final model achieves the best performance on the dataset and demonstrates generalization capability. Future work includes improving the model's effectiveness and exploring additional features.</sample>
    <sample id="220">The authors are affiliated with Stony Brook University.</sample>
    <sample id="221">The paper analyzed language pairs including German to English, English to Chinese, and Chinese to English.</sample>
    <sample id="222">This work addresses the challenges of adapting open-domain question answering (QA) systems to new domains. It explores data interventions to enable out-of-domain generalization, identifies the nature of dataset shifts, and determines effective interventions for specific shifts. The study uses a general-purpose Wikipedia corpus to train both the reader and retriever models, and tests generalizability across seven target domains, including biomedical. The research introduces two main methods: zero-shot and few-shot techniques. Zero-shot methods involve no examples from the target domain, while few-shot methods use a few examples to prompt the model. The study also examines the impact of varying question formats and answer distributions. It finds that reader performance improves with interventions, and different types of shifts require different interventions. The findings suggest that certain interventions are more effective for specific types of shifts, and the study contributes to understanding how to improve QA systems' adaptability to new domains.</sample>
    <sample id="223">The speaker's name is Xiangbing.</sample>
    <sample id="224">The models investigated were the Longformer and the standard BERT models.</sample>
    <sample id="225">For training, 53 tasks are used, and for testing, the entire common sense reasoning group is used, along with additional tasks from VQA and the misinformation group.</sample>
    <sample id="226">The paper has three authors.</sample>
    <sample id="227">The discussion highlights the limitations of current language models in grounded language understanding, which involves mapping natural language to specific environments. The main challenge is the lack of grounding during pre-training, leading to difficulties in downstream applications. Existing research often focuses on generating plans, but these may not always be valid. The proposed framework, PANGO, shifts the focus to discrimination, allowing language models to score and rank plans without generating them. This approach is tested on a large-scale question-answering task, showing strong performance across different settings. PANGO outperforms baseline models in terms of sample efficiency and demonstrates robustness under non-ideal settings. The framework is named after the Chinese mythological being Pango, symbolizing its role in separating the neural and symbolic worlds. The authors invite further discussion and collaboration on their work.</sample>
    <sample id="228">The authors conducted experiments on the AG News, MIND, SSTD2, and ERAS86 datasets.</sample>
    <sample id="229">The presentation by Gabriela Skatylinskaya and Henning Baake Smud discusses their research on detecting improvable claims for argumentative writing support. The focus is on text revision, a crucial part of professional writing, especially in argumentative texts, to effectively communicate a message. The research introduces two tasks: suboptimal claim detection and claim improvement suggestion. The paper explores challenges in using revision-based data, such as representativeness, model complexity, contextual relevance, and bias. The study aims to model argument quality based on revision patterns in online debate platforms. The findings suggest that revision-based data can be effectively used for the tasks, with the distance between claim versions being beneficial for detecting suboptimal claims. The impact of contextual information varies depending on the task and quality issues. The paper provides a detailed analysis of strategies to tackle these challenges.</sample>
    <sample id="230">The speaker, Kosta Sinha, introduces a talk on the paper 'Language Model Acceptability Judgments Are Not Always Robust to Context.' The paper, a joint work with several researchers, revisits the minimal pair paradigm, which evaluates language models based on acceptability judgments. The current minimal pair paradigm does not allow evaluation of models' acceptability towards longer sentences. The approach involves simulating longer sequences and evaluating models' acceptability on longer sequences. The paper finds that language models are sensitive to latent syntactic and semantic features and that the current minimal pair paradigm may not fully capture models' abstract knowledge.</sample>
    <sample id="231">NACHOS is a dataset of medical crawled data from the web, used for training the Dr. Bert model.</sample>
    <sample id="232">The speaker's name is Aydar Bilard.</sample>
    <sample id="233">The presentation by Sara Papi from the University of Trento and Fondazione Bruno Kessler introduces the Attention as a Guide for Simultaneous Speech Translation (ASAG) paper. The paper addresses the challenges of current Simultaneous Speech Translation (SimST) models, which involve complex architectures and training processes. ASAG proposes a novel approach using existing offline SimST models, employing a single model for each latency regime and leveraging cross-attention mechanisms. The strategy, called Encoder-Decoder Attention, decides on partial translations based on attention focus. ASAG outperforms existing strategies, offering higher translation quality and lower latency, and is the fastest in terms of computational time. The results are compared with other strategies, and the paper is open-source, providing code and models for reproducibility.</sample>
    <sample id="234">The prompting strategy significantly impacts results, with high-quality example selection improving performance. Five-shot prompting shows minimal difference compared to one-shot, and using dev data over train data yields better results.</sample>
    <sample id="235">The authors of the paper are affiliated with the University of Toronto.</sample>
    <sample id="236">The presentation does not provide specific details about the 5 expert-written instructions included in the Multi-Instcrt dataset.</sample>
    <sample id="237">The authors propose a diagnostic test suite called KITMOS, which includes a coreference resolution task to evaluate the models' ability to integrate knowledge from different sources.</sample>
    <sample id="238">The video presents MeetingBank, a new benchmark dataset for meeting summarization, addressing the challenges of high-quality summaries and sourcing public meeting transcripts. The dataset includes 1,366 city council meetings with transcripts, summaries, and URLs. Data collection involved using Speechmatics API for transcription, aligning transcripts with summaries, and analyzing coverage and density of summaries. Model evaluation compared extractive and abstractive summarizers, including BART-large, GPT-3, and others, using metrics like ROUGE-2 and human evaluation. GPT-3 showed high fluency and coherence but lower informativity. The video concludes with the dataset's potential for researchers and encourages its use.</sample>
    <sample id="241">Ethan and his team present a paper on 'Human in the Loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments.' They critique existing misinformation detection systems for unrealistic evaluations and lack of human integration. Their proposed framework involves a two-component system: one for detecting misleading claims using keyword filtering and a T5 model, and another for policy violation verification using a BERT-based stance classification model. The system is evaluated for its ability to detect policy violations and involves human content moderators at various stages. The evaluation shows a 65% accuracy in policy violation detection and 124.2 policy violations detected per human hour. The paper aims to provide a realistic framework for future misinformation detection systems and offers an outsider perspective on the development and evaluation of these systems.</sample>
    <sample id="242">Common evaluation methods for dialogue systems include human evaluations using comparative or Likert scale methods, and approaches like ABC Eval that annotate specific behaviors in chat.</sample>
    <sample id="243">The paper involves four authors: Sebastian Santi, Ronan Labrosse, Caterina Ranecka, and Martin Sab.</sample>
    <sample id="244">Background knowledge about judges deciding cases in law courts is needed to understand that Servin is a judge.</sample>
    <sample id="245">Linying Zhang presents a study on high agreement Mechanical Turk (M-Turk) workers, focusing on a two-step pipeline for worker qualification. The pipeline aims to address issues with automatic matrix and recruitment practices. The study involves a qualification task with multiple dimensions and an endurance task with heavy workloads. Results show that 6% of participants, or 4 gold and 8 silver workers, passed the tasks. The pipeline achieved high inter-annotator agreement (IAA) with a Kappa of 0.443. The study also compares the pipeline with Cloud Research M-Turk workers, achieving a Kappa of 0.534. The pipeline is considered a best practice for high agreement annotation at a lower cost, with plans for future research on hiring high-quality workers and exploring multiple applications. Limitations include the focus on English summaries and non-penultimate solutions for training correctness.</sample>
    <sample id="246">Yes, the code is available on GitHub.</sample>
    <sample id="247">The paper introduces FactKG, a dataset for fact verification using knowledge graphs. Unlike existing datasets that rely on text or tables, FactKG utilizes DBpedia knowledge graphs, offering intuitive evidence for claims. The dataset includes claims in both written and colloquial styles, with labels for supported and refuted claims. It employs five types of reasoning: one-hop, conjunction, existence, multi-hop, and negation. The paper presents a baseline model that outperforms others, using graph evidence for verification. The dataset is available for download, and the authors invite further contact.</sample>
    <sample id="248">Yes, the annotators for NLPositionality are balanced across various demographics, including country, gender, and education level, to ensure a diverse range of perspectives in the dataset.</sample>
    <sample id="249">In the acceptable domain, sentences were perturbed by adding noise while preserving the relevant structure, but these perturbations did not significantly alter the models' judgments.</sample>
    <sample id="250">Dimensional evaluation means assessing various specific aspects of chat quality, such as relevance and contradictions, to understand a model's strengths and weaknesses more precisely.</sample>
    <sample id="251">The authors are affiliated with the University of Science and Technology of China.</sample>
    <sample id="252">The presentation by Saikiran Thanikella and colleagues introduces 'U-CREATE', a novel approach to unsupervised case retrieval using event extraction. Legal professionals often rely on experience to cite relevant past cases, but the volume of cases makes this challenging. The work introduces the ILPC dataset, a comprehensive benchmark for prior case retrieval with 7,070 Indian legal cases, and the U-CREATE pipeline, which uses unsupervised learning and event-based techniques. The pipeline's event extraction block, consisting of pre-processing, dependency parsing, and post-processing, allows for efficient retrieval. Experiments with various models, including count-based, transformer-based, and event-based, show that event-based models, particularly the event filtered documents model, outperform others. The U-CREATE pipeline outperforms existing methods, including those by the MTFT team, on the COLI21 dataset. The work opens avenues for further exploration in the field of prior case retrieval.</sample>
    <sample id="253">Mario Edarragon presents 'NameDisorder,' a model for detecting mental disorders in social media posts. The model uses domain adaptation to leverage knowledge from general language models like BERT, focusing on mental health-specific tasks. The approach involves learning social media language and then specializing in mental disorder detection. Results show that 'NameDisorder' achieves a balanced performance in precision and recall, outperforming BERT in identifying mental disorder-related content. The model highlights words and phrases associated with mental health issues, such as 'focus' and 'medication.' Future work aims to explore different lexical resources and clinical data to enhance the model's effectiveness.</sample>
    <sample id="254">The research work by Sun Qi from Nanjing University of Science and Technology focuses on improving document-level relation extraction by addressing the issue of noisy pseudo labels in distant supervision data. The study introduces a framework that incorporates uncertainty-guided label denoising to enhance the quality of pseudo labels. This framework employs instance-level uncertainty estimation to manage overlapping relations, using Monte Carlo dropout to model prediction uncertainty. A dynamic class uncertainty threshold is proposed to filter out high-uncertainty pseudo labels, and a multi-phase training strategy is developed to iteratively relabel the DS data. The framework is evaluated on two public datasets, demonstrating superior performance over existing baselines. The main contributions include the framework with uncertainty-guided label denoising, instance-level uncertainty estimation, a reliable strategy with dynamic class uncertainty threshold, and significant performance improvements.</sample>
    <sample id="255">The form of the prompting is crucial for zero and one-shot prompting, but less so for five-shot prompting, where the quality of examples is more important.</sample>
    <sample id="256">Hello, my name is Vasudha and I am a computer science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper, Transfer Learning for Dissonance Detection Addressing the Rare Class Challenge. We begin by defining cognitive dissonance and why it is an important problem to study in language. Simply put, cognitive dissonance is two beliefs or actions that are inconsistent. Such as a person states, I know that cigarettes could kill me, and then goes on to say I grabbed a couple of smokes after the meeting. This belief and action are inconsistent and they are in dissonance. Further mentioning that I don't think I could keep my job without them justifies the second occurrence and they have a consonance relationship. While dissonance is a very common phenomenon we experience in daily decision making, they are really rare to find expressed in language among other discourse relations. So why does this matter? Studying cognitive dissonance can help us understand the effects of disagreement among people, track trends in belief, values and attitude changes in population. High cognitive dissonance is also related to anxiety disorders and can help understand people's mental health better. Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups. Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision making processes better. To the goal of creating a cognitive dissonance resource, we conducted a large scale annotation of dissonance relations. We used a dissonance first approach as seen in the flow chart here. Tweets were parsed using a PDTB parser and pairs of discourse units were annotated according to the guidelines that are described in our paper. As can be seen here, dissonance was only found in 3.5% of the annotated pairs. On collecting around 1000 examples of discourse unit pairs, we ran training for an initial classifier trained only on 43 examples of dissonance. To no surprise, the classifier performed not much better than chance. Given the low occurrence of dissonance and absence of any prior such dataset, we are facing the problem of absolute rarity. To alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonance samples can be collected over lesser annotation rounds, lowering the overall annotation cost but improving dissonance detection. Since the initial model was not able to capture the dissonance class at all, we start the active learning process by transferring weights from closely related tasks. We transfer from two different tasks, topic independent dissonance stance classification, a task that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic called debate here, and on binary classification of expansion and comparison classes of PDTB. Since these two are closely related to the conception of consonance and dissonance, and we call them CEE here. We find that on transferring the zero shot performance on the annotated dataset is already much better than chance with the best with AUC.62. Further on iteratively fine-tuning on both tasks, we find that fine-tuning of CEE task followed by further fine-tuning on debate yields a much better zero shot performance. Thus, this is the model that we use to call start the active learning. Next, we determine the best method to date a model with new data from each round of active learning and annotations. Cumulative accumulates all the data collected from active annotations so far, whereas iterative updates the model by training on the latest set of data collected. Over the different strategies, we found that cumulative performed equal or better than iterative across the board. Next, to improve the number of dissonance examples, we use a probability of rare class strategy, PERC, select mostly the examples that are highly likely to be dissonant by the current model at any round of active learning. We compare this to the other state of the art strategies that are commonly used in the community. We find that the proposed PERC strategy works better than other state of the art strategies, although the difference is small. Note that the performance is significantly lower for random. On further rounds of AL with two best strategies, we improved dissonance classification AUC to 0.75, which is the best performance that we have on the task so far. We also check the feasibility of each strategy for annotation quality and costs to annotators. We find that PERC has the highest percentage of dissonance and works best for rare class. However, the annotators also find the examples difficult. In summary, we find that PERC is a simple AL strategy for rare class acquisition and co-starting AL with appropriately designed transfer learning tasks can help significantly. We also find that iterative update is useful for transfer learning from a different domain, whereas in domain active annotations benefit from cumulative update. These are the links to our core dataset and our paper. Feel free to get in touch with us if you have any questions. Thank you.</sample>
    <sample id="257">The authors evaluated four state-of-the-art chat models.</sample>
    <sample id="258">Zhangsun Han presents a study on using large language models as an alternative to human evaluations in natural language processing. The research aims to determine if these models can effectively rate text based on grammar, coherence, likability, and relevance. The study involved using models like T0, InstructGPT, Q-Learning, and ChatGPT to evaluate stories, comparing their results with human evaluations conducted by English teachers. The findings show that while some models, like Davinci and ChatGPT, align with human preferences, others do not. The paper discusses the potential benefits and drawbacks of using large language models for evaluation and explores the impact of various factors on the results. The research contributes to the understanding of large language models' capabilities in text evaluation.</sample>
    <sample id="259">The presentation by Yuxin Zhang from Penn State University introduces Exemplar, a comprehensive benchmark for cross-lingual semantic parsing across multiple natural languages and representations. Exemplar addresses the limitations of existing models, which are often limited in language coverage and evaluation methods. The dataset includes 90 datasets across various domains, 5 semantic parsing tasks, 8 representations, and 22 languages from 15 families. The evaluation settings include translate test, monolingual, multilingual, and cross-lingual zero-shot transfer. Results show that encoder-decoder models outperform previous methods, with multilingual training enhancing performance on target languages. However, multilingual models like CoDiST and BERT still fall short for cross-lingual tasks. The study highlights the significant performance gap in zero-shot transfer and the potential of multilingual training to improve performance, especially for English. The findings suggest that while multilingual models have room for improvement, they offer a promising direction for future research in cross-lingual semantic parsing.</sample>
    <sample id="260">The paper is authored by Jingwei Yi and colleagues from the University of Science and Technology of China.</sample>
    <sample id="261">A good planner should write scripts that are both reasonable and faithful to the constraints of the specific goals.</sample>
    <sample id="262">The paper has three authors.</sample>
    <sample id="263">The presentation discusses the challenges of label biases in in-context learning, a popular paradigm for large language models. It identifies three types of biases: vanilla label bias, context label bias, and a newly identified domain label bias, which arises from the task corpus. Experiments show that domain label bias significantly affects model performance, especially in tasks with large biases. To address this, the authors propose domain context calibration, which uses random in-domain words from the task corpus to estimate and mitigate biases holistically. This method outperforms previous calibration techniques, such as using single predefined tokens, by significantly improving model performance across various datasets and models, including GPT-3. The work concludes with a comprehensive investigation of label biases and a novel calibration method that enhances in-context learning performance.</sample>
    <sample id="264">Liu Wang, a graduate student at Zhejiang University, presents a paper on 'Towards Transferable Audio Visual Text Generation.' The paper addresses the challenges in multimodal text generation, particularly in audio visual text generation, which is more complex than other tasks like machine translation. Wang proposes a novel task called 'transferable audio visual text generation' to overcome these challenges. The framework includes an audio visual meta map network, an audio visual encoder, and a language model generator, aiming to train a model that can quickly adapt to new multimodal domains with limited labeled data. The paper introduces a loss function and training details, including a meta meta pre-training and meta meta pre-training. The experimental section evaluates the proposed approach using two benchmark datasets, showing that the method outperforms other models in most settings. The paper concludes with a discussion on the performance of the method across different domains and the impact of audio features and semantic comments.</sample>
    <sample id="265">The speaker's name is Vasudha.</sample>
    <sample id="266">The authors are affiliated with the University of Gothenburg and the University of Gothenburg, Sweden.</sample>
    <sample id="267">Hello everyone, my name is Yuxin Zhang from the Penn State University. Today, I'm going to present our work, EXAMPLER, on cross-lingual semantic parsing in multiple natural languages and multiple representations. Semantic parsing is the task to build semantic representations of user queries such as SQL and lambda calculus. And cross-lingual semantic parsing is the task to translate queries in multiple natural languages into multiple meaning representations. As shown in this figure, we need to translate the query in multiple natural languages using neural models to SQL, lambda calculus, and etc. Existing cross-lingual semantic parsing models are separately proposed and evaluated on data sets of limited tasks and applications. For instance, there are lacks of coverage on certain natural language, the Chinese is missing, and the lambda calculus is missing. Or there are only evaluated on certain neural model. So to this end, we propose EXAMPLER, but provide a uniform data set EXAMPLER for cross-lingual semantic parsing in multiple natural languages and multiple representations. It contains 90 data sets in various domains, 5 semantic parsing tasks, 8 million representations, and 22 natural languages in 15 language families. And to better evaluate our benchmark, we consider the six settings for training and evaluation. The first one is translate test. We use Google translate API to translate source to the target language, then use monolingual model to train and evaluation. And for example, we train the English model on English query, and during inference, we translate the German query using API to English, and then use the trained model to predict the SQL. And we test monolingual model. In this setting, the source language is the same as target language, for example, German to German or English to English. We also test monolingual few shot setting, but training monolingual models with only 10% of training data. And we test multilingual model, which we train one multilingual model for all languages. For example, we put the German, English, Chinese queries together to train a multilingual model, and during inference, we can use this model to translate German queries or Chinese query or et cetera. And we also consider cross-lingual zero shot and few shot transfer. We train on one source language and transfer to another language. So during training, we train on English query or the combination of English and German few shot queries to train a multilingual model to predict the SQL output. And we find many interesting results. So regarding analyze of monolingual models, we evaluate on two groups of models, including encoder PDR, which stands for Multilingual Pretrained Encoders with Pointer based Decoders such as XLNLP and BERT PDR. And we evaluate encoder-decoder models, which is Multilingual Pretrained Encoder-Decoder models such as M-BART and MT5. We found that encoder-decoder obtains the best performance on all nine data sets. And we evaluate on MT5 and EXAMPLER on multilingual setting. We found that encoder-decoder or encoder PDR can be improved by training in a mixture of various languages. And we found that it is because most of the major natural languages can obtain performance gain, except that English performance drops in seven data sets and only gains in three data sets. I think this is known as curse of multilinguality. We also compare the cross-lingual performance gap. In this figure, the blue line is cross-lingual few shot transfer, the orange line is cross-lingual zero shot transfer, while the green line is the monolingual setting. We found that by comparing the green and orange line, we found that for zero shot setting, the cross-lingual transfer performance gap is significant, and by comparing blue and orange line, we found that for few shot setting, the transfer gap is shortened rapidly. We also find some other interesting findings. For example, encoder-decoder outperforms previous work or achieved comparable results. But training on English natural language can significantly boost the performance of few shot on target natural languages. And we found that multilingual language models such as CoT and BERT are still inadequate for cross-lingual semantic parsing tasks. To sum up, we build EXAMPLER, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and multiple representations. We conduct a comprehensive benchmark study on three representative of types of multilingual language models, and our results show many interesting findings and et cetera. And welcome to visit our paper and code. Thanks for listening.</sample>
    <sample id="268">The most common errors of PaLM are omission errors, where parts of the source sentence are dropped to produce a better sounding translation.</sample>
    <sample id="270">The authors are affiliated with the Emory NLP Lab and Amazon Alexa AI.</sample>
    <sample id="271">CFT stands for Continuous Fine-Tuning, a method proposed as a strong baseline for WSL approaches.</sample>
    <sample id="272">The paper involves a joint effort by six authors: Kostas Sinha, John Wock, Aaron Muller, Kanishka Mishra, Karen Fuentes, Roger Levy, and Attina Williams.</sample>
    <sample id="274">The speaker's name is Yuxin Zhang.</sample>
    <sample id="275">Hi, I'm Xianbing, PhD student at the University of Washington. Today, I'm presenting our work from pre-training data to language models to downstream tasks, tracking the trails of political biases leading to unfair NLP models. So language models are trained on large-scale web crawl data. Political news media are well covered in their pre-training data. According to a survey of the C4 corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etc. are well covered in language model training data. This has created a mixed blessing for language model applications. So on one hand, they were able to learn from diverse perspectives, which celebrates democracy and the plurality of ideas. On the other hand, these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications. To this end, we propose to investigate the political bias propagation pipeline from pre-training data to language models to downstream tasks, specifically by asking the following questions. First, how do we evaluate the political leaning of language models and what role does pre-training data might have on such political biases? Secondly, how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in NLP applications? So we first propose to prompt language models with different prompt formats using the political questionnaires such as the political compass test. This ensures to do automatic evaluation well grounded in political science literature. So some preliminary results demonstrate that first, language models do have varying political leanings. They occupy all four quadrants on the political compass. We can also see that GPT-4 is the most liberal language model of all. And GPT-3 is generally more socially liberal than BERT theory and its variants. Secondly, we aim to investigate to which extent the political biases of language models are actually picked up from training data. So we conduct a controlled experiment by further pre-training language model checkpoints on six different partisan corpora separated into news and social media further divided into their political leaning. By further pre-training language models on such partisan corpora, we can see that the ideological coordinates of the language model also correspondingly shift. For example, for Roberta, further fine-tuned and further trained on the left-leaning Reddit corpus, we can see a substantial liberal shift in terms of its political biases. And we also try to investigate whether language models can pick up the polarization that's prevalent in our modern society. So we divide pre-training corpora into pre-45th President of the United States and after 45th President of the United States. We separately pre-train language models on the two different temporal corpora. We can see that language models generally had a political leaning that is further away from the center after 2017. So this indicates that language models can also pick up the polarization in our society. So last but not least, we evaluate language models with different political leanings on hate speech detection and fake news detection to NLP applications that often involve language models and could have very significant implications. So we see that if we investigate the per category performance, that is to say, if we separate the performance into different demographics or political leaning of news media, we can see a pattern that, for example, for hate speech detection, left-leaning language models are better at detecting hate speech targeting socially minority groups, however, are worse at detecting hate speech targeting more powerful folk groups in our society. And vice versa, right-leaning language models are better at detecting hate speech targeting white and men, however, worse at detecting hate speech targeting black LGBTQ+ and other minority communities. Similar trends also happen for fake news detection, where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa. This in we further show many qualitative examples to see that language models with different political leanings do give different predictions to hate speech and misinformation examples based on their social categories. There are a bunch of more examples in the appendix to further highlight that. This indicates that there is a fairness issue that is very pressing regarding the political biases of language models. For example, if a right-leaning language model were to be fine-tuned on hate speech or misinformation and deployed to a popular social media platform, this would mean that people with opposite political opinions might be marginalized and the hate speech targeting minority groups might just run rampant without any control. So this has sounded the alarm for us to acknowledge and tackle the fairness issues resulted by language model political biases. So a little bit of discussion. We would also like to highlight that we expose the unique dilemma regarding language model political biases. It's like between Cila and Caribdis. So if we do not sanitize the political opinions in language model training data, the bias would propagate from pre-training data to language models to downstream tasks, ultimately creating fairness issues. If we do try to sanitize somehow, we will also risk censorship or exclusion. And it's incredibly hard to determine what is actually neutral and should be retaining language model training data. So it's kind of like the electric trolley problem. Okay, great. I think that's pretty much all I have for today. Thank you for your time.</sample>
    <sample id="276">Ananya and Vignesh present their work on the IndicMT Eval dataset, which aims to evaluate machine translation metrics for Indian languages. The study focuses on five languages from two language families, Tamil, Malayalam, Hindi, Marathi, and Gujarati, to address the lack of evaluation metrics for translations in other languages. They generated 7,000 samples by using seven translation models and collected detailed human annotations. The research evaluates various metrics, including CHRF, LABSE, and COMET, and fine-tunes the COMET variant using IndicMT MQT. The results show that IndicMT MQT outperforms COMET baselines on most languages and demonstrates higher robustness scores. The study highlights the importance of developing language-specific metrics to improve translation evaluation.</sample>
    <sample id="277">The new method does not have a name.</sample>
    <sample id="278">The 'marked words' method identifies words that distinguish marked groups from unmarked ones by using weighted log odds ratios, revealing stereotypes and essentializing narratives in language models.</sample>
    <sample id="279">The authors are affiliated with the University of Washington.</sample>
    <sample id="280">Xiaotao presents the Multi-EMOT framework for emotion regulation in conversations, addressing challenges in multimodal emotion classification. The framework integrates visual, audio, and textual data using a novel visual feature extractor, Miss-ExtNet, and a multimodal fusion model, Multi-Attend. Miss-ExtNet focuses on facial expressions, excluding redundant scene information, while Multi-Attend uses cross-modal correlations through bidirectional cross-attention layers. The framework introduces a sample-weighted focal-contrastive loss to improve classification of minority and semantically similar emotions. Extensive evaluations on the MILD and IEMOCAP datasets demonstrate state-of-the-art performance, with significant improvements in difficult scenarios. However, limitations include the inability of Miss-ExtNet to distinguish between speakers and the need for large training data on MILD.</sample>
    <sample id="281">The presentation by Kaiyuan Yu and colleagues explores the necessity of context in translation, focusing on how words like 'mole' can have different meanings based on context. The research introduces CxMI, a measure of context usage in machine translation, and extends it to pointwise CxMI to analyze context at the sentence and word levels. The study identifies discourse phenomena that require context, such as pronouns, verb forms, and ellipsis resolution, and uses these findings to create a benchmark for evaluating document-level translation models. The results show that context-aware models perform better than context-agnostic models for certain phenomena, but not all. The study also compares different translation systems, finding that DeepL generally outperforms Google Translate. The work aims to improve understanding of when context is needed in translation and how to evaluate translation models effectively.</sample>
    <sample id="282">The presentation introduces StoryTrance, a novel approach to non-parallel text style transfer at the discourse level, addressing the challenge of imitating author styles in long texts. The model, StyleTrance, leverages discourse representations and style embeddings to generate text in target styles. It employs a two-stage training framework, using self-restriction loss and disentanglement loss to separate style and content, and style classifier loss to generate style signals. The model's effectiveness is demonstrated through extensive experiments, showing its ability to control style and maintain content, outperforming strong baselines. The approach also includes a method to handle cases where the model might introduce unrelated sentences, ensuring the preservation of the original text's semantics.</sample>
    <sample id="283">The first mentioned symmetrical dependency structure is the universal dependencies structure.</sample>
    <sample id="284">The presentation by Peng Tianxun from Wuhan University at the ACL Main Conference 4915 introduced FSUE, a novel fuzzy span mechanism for enhancing universal information extraction. The current span-based UIE model relies on precise span boundaries, which can be ambiguous, leading to potential misalignment between transformer feature extraction and information extraction. FSUE addresses this by proposing a fuzzy span boundary, represented as a continuous distribution, to model the target boundary more accurately. The model uses a sampling function to convert this distribution into discrete values for calculating fuzzy span loss, which includes binary cross-entropy and KL divergence. The FSUE model introduces a fuzzy span attention mechanism, represented as a mask function, to dynamically adjust the attention span and improve the model's focus on relevant information. Experiments on named entity recognition, relationship extraction, and aspect sentiment triplet extraction demonstrated significant performance improvements with FSUE, especially on small-scale datasets. The results showed better generalization capabilities and competitive performance on various datasets. The presentation concluded with the successful implementation of FSUE, achieving excellent results across multiple information extraction tasks.</sample>
    <sample id="285">The presentation by Mingqi Gao from Peking University addresses the issue of factual errors in dialogue summarization. It introduces a framework for benchmarking fact error correction, highlighting the limitations of current evaluation methods that rely on facticity metrics. The proposed solution involves introducing human-annotated reference corrections to improve the training of fact error correction models. The presentation introduces a taxonomy of fact errors, categorizing them into content-based and form-based errors. The evaluation framework consists of three steps: alignment, classification, and comparison. The findings suggest that training with reference summaries from dialogue datasets yields better results than unreliable facticity metrics. The presentation concludes with a call for a change in evaluation methods and a combination of human-annotated and synthetic data to enhance model performance.</sample>
    <sample id="286">The name of the speaker is James Finch.</sample>
    <sample id="287">There are four authors involved in the paper.</sample>
    <sample id="288">The datasets used to test syntactic phenomena include the Blimp dataset, which is specifically designed for evaluating grammaticality and acceptability judgments in language models.</sample>
    <sample id="289">Hello, my name is Kai-Wei Yin, and I will be presenting our work titled "When Does Translation Require Context: A Data-Driven Multilingual Exploration." This work was done in collaboration with Patrick Fernhout, Emile Liu, Andre F. D. Martins, and Graham Neubig. So a lot of translations depend on context. For example, how would we translate mole in this sentence? Well, if the previous sentence was, things could start to get dangerous if the ministers find out, then mole refers to a spy. But if the previous sentence was, could it be anything serious, doctor? Then mole refers to a birthmark. So depending on context, the meaning of the word changes, and therefore, is translation changes as well. However, evaluating how well models can translate cases like this is pretty hard. Firstly, because only a small portion of translations depend on context, which makes corpus-level metrics like BLEU unable to capture these translations. And some people have suggested targeted evaluation on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages, since they usually rely on domain knowledge and human creation. In this work, we try to answer these two questions: first, when does translation require context, and second, how well do models handle these cases? To answer the first question, we started by measuring how much a word depends on context during translation. In the previous work, we introduced cXMI as a measure for context usage by machine translation models. And this is done by measuring how much information the context C provides about the target Y, given the source X. You can think of cXMI as the information gained from giving context to the model. In this work, we extend cXMI to pointwise cXMI, which can measure context usage at the sentence level or at the word level. We can think of words that have high PCXMI as ones that require context for translation. Now we analyze words with high PCXMI to look for patterns between these words. And we perform our analysis on transcripts of TED Talks that have been translated from English to 14 different languages. We perform our analysis at three different levels. First, we look at part-of-speech tags that have high PCXMI, and this allows us to find, for example, dual pronouns in Arabic that have relatively high PCXMI. And this can be explained because English doesn't have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic. Similarly, we find that certain languages also require context when we want to choose the appropriate verb form. We then look at vocabulary items that have high PCXMI averaged over all of its different occurrences. And this helps us identify cases like the one here, where in Chinese, you need context to translate proper nouns to make sure that you're using the same translation within the document. And similarly, we find that context is supported to translate in the right formality. And finally, we look at individual tokens that have high PCXMI, and this allows us to identify phenomena that cannot really be captured by the word itself, but that's rather expressed in the sentence structure, such as ellipsis resolution. So now we use our findings from our analysis to design a benchmark for document-level translation. For each of the five discourse phenomena we identified, we create taggers to automatically identify words that pertain to the phenomenon, and we call our tagger the Multilingual Discourse-Aware, or MUDa, tagger. We can then also note that different languages have different proportions of these discourse phenomena. We then use the MUDa tagger by applying the tagger on a parallel corpus that we want to use for evaluation. And we apply our translation metrics of choice on the context-dependent examples that the MUDa tagger has identified. And finally, we use our benchmark as well as other metrics to evaluate different models on document-level machine translation. First of all, when we use corpus-level metrics, so for BLEU, we find that context-agnostic models have the best performance. But then if we use COMET, context-aware models perform best. And if we use word F-measure, then models with or without context have comparable performance. This again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone. Now we use the MUDa benchmark to evaluate models, and we find that context-aware models are significantly more accurate than models that do not use context for certain discourse phenomena, such as formality and lexical cohesion. But these models are not much better than models that do not use context for other phenomena like ellipses, pronouns, and verb form. So this sort of suggests where we would need to see more progress for document-level translation. We also compared different commercial systems, and our benchmark shows that DeepL is usually more accurate than Google Translate for document-level translation. To summarize, we perform a data-driven analysis across 14 language pairs to identify when translations require context. And then we use our findings to build a benchmark for document-level translation, which can help us identify which discourse phenomena models can handle well or not, and which translation systems are good at document-level translation. Thank you so much for your attention, see you in Toronto.</sample>
    <sample id="290">The abbreviations of the five methods for the first research question are CoSine, FSW, FSW-2, FSW-3, and FSW-4.</sample>
    <sample id="291">The model is evaluated on 11 biomedical and clinical downstream tasks in French.</sample>
    <sample id="292">Hello, I am Omar and now I will talk about the use cases for our dataset D-Plane. So for the first use case, we can evaluate automatic alignment methods. In the recent years, there has been a lot of alignment methods, but in the context of machine translations, where we have two parallel documents written in different languages and we want to extract alignments of sentences in post documents, but in our use case, we are trying to extract alignments between sentences of two parallel documents, having the same language, having the same content, but they are on a different complexity levels. Now, as we have our dataset D-Plane, which have manually aligned sentences, we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods. We did some adaptations to the proposed methods and we have published all these adaptations and the codes to run our experiments in the paper. At the end, we concluded that the best alignment, automatic alignment method to use for German text simplification is the method of MASS align. And you can also find the code to run this method on your own documents in the paper. The second use case that we showed in our paper is the case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text. We have fine-tuned two different models. We have fine-tuned the model of long-impart to produce document level simplifications and we also fine-tuned the normal base-impart to produce sentence level simplifications. You can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper. We concluded that this basic fine-tuning could produce or could get scores better than the baseline scores. And we proposed those results as a benchmark, a base benchmark for the problem of automatic text simplification in the future. Thank you so much for your attention and we hope to meet all of you during the conference. Thank you.</sample>
    <sample id="293">Hi, I'm going to talk about our work on resolving indirect referring expressions for entity selection, in which we introduce the altentities corpus. My name is Javot Hosseini, and this is a joint work with Philip Radlinsky, Silvia Parati, and Annie Lewis. Our goal is to understand users' language when they want to make a choice. Consider this alternative question: Did you mean easy on me or I got a feeling? Here, a user wants to select between one of these two songs. The most obvious thing is to use a direct reference, for example, by saying the name of the song, easy on me, or its position, the first one. But sometimes an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name of the song, or the pronunciations are too similar to each other and hard to disambiguate, or when the user wants to specify a preference. Here are some examples of indirect references, for example, the newer one, or the song that's not energetic. This is an important problem in conversational systems and also for BING marking LLM's entity understanding. We're not aware of a public dataset, a large-scale public dataset for the task, so we collect one using crowd annotation. Our dataset covers three different domains: music, books, and recipes. Our data set collection methodology emphasizes informality using a cartoon completion setup. The cartoon has three speech bubbles. In the first bubble, Bob says, remember that song we were listening to yesterday. And with that, Bob sets the context. In the second speech bubble, Alice says, do you mean easy on me or I got a feeling? Which is the alternative question. And in the third speech bubble, Bob uses an indirect reference to select one of these entities, for example, the newer one. We provide the first and second speech bubbles automatically, but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain. The second one, which is the alternative question, is generated as follows. We always use a simple template, do you mean A or B, where A and B are sampled from Wikipedia. Here are the different sampling methods we've used. When we move higher in the list, the entities become more similar to each other, and it's usually harder to make the disambiguation. The first one is uniform at random. The second one is when the entities have similar titles, for example, two books with the name the return. The third one is when they have similar descriptions on Wikipedia, and finally, when they have similar info boxes or attributes on Wikipedia, for example, the same genre or the same artist for a song. When we show these alternative questions to the annotators, they know the name of these entities, but they don't necessarily know about the entities. So what we do is that we show some background knowledge about the two entities. For songs, we simply show a Google search link to each song. And then ask the annotators to listen to at least some of each song and read about each song. Here's for example, the Google search result for the song easy on me. For the recipes and books domain, we show some background text from Wikipedia. For recipes, we additionally show their images, again from Wikipedia, so that the annotators know how they look like. Then we ask the annotators to pick one of these entities, for example, here the first one, and describe them using three to five indirect referring expressions. For example, the one with the piano music. Here are some examples from our data set. For example, the one without words, not the one with the 12 year old boy, or the fictional one, or comes from Azerbaijan, and so on. The altentities corpus has 6,000 alternative questions across three domains, and it has 42,000 indirect referring expressions. Results with T5 large model are summarized below. If the language model has access to the exact same background knowledge as the annotators, the accuracy is really high, it's around 92 to 95%. But this is not realistic. If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87%, which is more realistic. For example, when the language model retrieves the background knowledge. If the language model has access only to entity names, then the accuracy is only 60%, so there's a lot of room for improvement. We've also shown that the models are domain generalizable. Here is a link to our data set. ThanksTranscribe the English content.</sample>
    <sample id="294">CamemBERT is initially trained on the dataset called Natus.</sample>
    <sample id="295">The speaker's name is Adam Szpekowski.</sample>
    <sample id="296">Valerio Basile presents a study on irony detection in natural language processing, highlighting the limitations of traditional supervised machine learning methods. The research, conducted in collaboration with the University of Turin and Amazon Alexa, focuses on developing perspective-aware models to better understand irony. The EPIC corpus, a collection of 300 short conversations from social media platforms, was used to train these models. Annotators, selected from diverse backgrounds, provided annotations that revealed significant inter-annotator agreement differences. The study found that perspective-aware models were more confident in their predictions compared to aggregated models. Notably, variations in annotation were observed across different demographic groups, particularly between annotators from the UK and Ireland. The research aims to provide a more nuanced understanding of irony in language processing.</sample>
    <sample id="297">The project 'From Dog Whistles to Bullhorns: Unveiling Coded Rhetoric with Language Models' explores the use of dog whistles in political rhetoric, focusing on their role in conveying covert messages to specific in-groups while evading detection by out-groups. The research involves creating a comprehensive glossary of over 340 dog whistle terms, categorized by type and persona, such as antisemitic or transphobic. A case study of historical U.S. political speeches reveals patterns of dog whistle usage, particularly in conservative circles. The project evaluates the performance of GPT-3 in identifying dog whistles, noting its limitations with informal and transphobic terms. Additionally, the study examines how dog whistles can bypass content moderation, as demonstrated by toxicity detection tools that rate dog whistle-laden hate speech as less toxic. The findings highlight the challenges in detecting and understanding dog whistles, emphasizing the need for improved detection methods.</sample>
    <sample id="298">The performance degradation observed when retraining models with more recent data confirmed that the main cause of performance loss is temporal drift.</sample>
    <sample id="299">The presentation by Mihailis Karagatsis and Andreas Vlachos addresses the challenge of improving the robustness of neural network models against overfitting to shortcuts in training data. Current models excel in in-distribution tasks but falter in out-of-distribution scenarios. The proposed solution involves a minimax training objective that focuses on underrepresented hard examples, reducing the model's reliance on shortcuts. This method involves an auxiliary model that generates example weights, encouraging the learner to prioritize these hard examples. The approach is evaluated on three datasets, showing consistent improvements in out-of-distribution performance while maintaining high in-distribution accuracy. The paper also explores the impact of model size, shortcut presence, and auxiliary size on performance.</sample>
    <sample id="300">Belinda presents a task called Interactive Diction, which allows users to dictate and edit documents using voice commands. The task, developed by Semantic Machines in collaboration with Jason Eisner, Adam Paluszyn, and Sam Thompson, involves a four-step process: ASR transcription, segmentation of dictation and commands, normalization of commands, and execution of utterances. The project aims to create a more intuitive interface than existing systems, which often require fixed commands. A data collection interface was designed, and a dataset was built. A baseline system was developed, using models like T5 and GPT-3 for ASR and interpretation. The system showed varying levels of accuracy and efficiency, with GPT-3 models being more accurate but slower. The project encourages further research and has released code for future work.</sample>
    <sample id="301">Hi everyone, I'm Jenny, a first year PhD student at Carnegie Mellon University, and today I'll be presenting our work, AnL Positionality: Characterizing Design Biases of Datasets and Models. This work was done in collaboration with some folks at the University of Washington and the Allen Institute for AI, namely Sebastian Santi, Ronan Labras, Katarina Rainerica, and Martin Sapp. So let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content. You might turn towards a popular API like Perspective API for toxicity detection. And this works really well if you're Carl Jones. Where perspective API is able to detect toxic instances, but that's not really the case for Aditya Sharma, where perspective API is really not as sensitive to offensive terms that are more common in Indian contexts. This is an example of a design bias where we see systematic performance differences of technology between populations. Design biases like the one that we just saw before might occur due to the positionality of the NLP researchers and model developers. Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences. This is a concept widely used in critical studies, specifically in feminist and queer academic spaces. And as a researcher, positionality can influence the research process and its outcomes because it can change the decisions that researchers make. And so one question that people might ask is do datasets and models have positionality? And we're not trying to say that models and datasets themselves have demographic identities and life experiences, but they do aggregate judgments and opinions of real people and can thus represent certain positionalities over others. So prior work has suggested some anecdotal evidence of having positionality, such as cultural gaps in models and datasets, as well as theoretical definitions of model positionality. However, these works really don't look at comparing end users with the datasets and models themselves. And studying model and dataset positionality is increasingly important as NLP tasks become more subjective and socially oriented. And it's challenging to characterize how these positionalities are skewed because not all decisions are documented and many models are hidden behind APIs. So to study dataset and model positionality, we actually compare the annotations with real users with existing datasets and models. Our framework works in two main steps. The first step is to re-annotate datasets with diverse annotators. And we opt to do this over looking at the demographics of original datasets annotators because usually only a few annotators annotate each instance, and because demographics are rarely collected and shared. And so we opt to re-annotate data to get many annotators for instance and to get a rich set of demographic data. We then take the annotations by demographic and compare them to the models and datasets using a Pearson's R correlation score. And thus our framework actually differs from annotator disagreement literature by comparing end users with models and datasets, predictions, and labels, as opposed to looking at just annotator agreement or modeling annotator distributions. Our framework is largely enabled through Lab in the Wild, an online crowdsourcing platform, former HCI collaborator. And Lab in the Wild is an online experimentation platform where we can recruit diverse volunteers compared to platforms like MTurk, which largely have participants from the US or India. And further Lab in the Wild still is able to get high quality data. We host two tasks on Lab in the Wild, one of them being social acceptability. And the way this works is that participants will read a situation from the social chemistry dataset, and then they'll write how socially acceptable a situation is. Afterwards, to stay engaged in the study, they can compare their responses to an AI and others. We then compared these annotations with social chemistry, delphi, and GPT-4. We then replicate a very similar setup for the toxicity and hate speech detection task, where they'll read an instance from dynahate and write whether they think it's an instance of hate speech. We then compared these annotations with dynahate, perspective API, rewire a GPT-4 in GPT-4. Our study in the end amassed over 16,000 annotations from over 1,000 annotators from 87 countries. So now we're better equipped to answer who do NLP datasets and models align with the most? We find that there is positionality in NLP. For example, we find that datasets and models are most aligned to English speaking countries. So for the GPT-4 social acceptability analysis, we find that it's most aligned to Confucian and English speaking countries. We find that dynahate is also most aligned to English speaking countries. We also find most additional align with people who have a college education. So for GPT-4 in the social acceptability task, we find that it's most aligned to people with a college education or graduate school education. And we find the same for dynahate, where it's most aligned to people with a college education. However, when models and datasets are aligned to specific populations, some are inevitably left behind. An example of this is that datasets and models are less aligned to non-binary people compared to the men and women counterparts. We find this in the GPT-4 social acceptability task as well as the dynahate task analysis as well. So given that there is positionality in NLP, what can we do about it? So we have a few recommendations for this. First one is keep a record of all relevant design choices throughout the research process. And the other is to do NLP research with a lens of perspectivism. Our third recommendation is to build specialized datasets and models within four specific communities. And a good example of this is the Muscani initiative. I mean, we want to emphasize that inclusive NLP isn't just making all technologies work for everyone. And so that concludes our presentation, but if you'd like to learn more, feel free to check out our dashboard for the most updated analysis results and our paper. Thank you.</sample>
    <sample id="302">Permuting tokens is necessary to determine the correct order of the output sequence, as the initial tagging step only provides the right tokens without their order. The permutation model predicts the sequence order to form coherent outputs.</sample>
    <sample id="303">The authors recommend increased transparency to understand why positive stereotypes occur and to study the effectiveness of anti-stereotyping methods, as current assumptions are insufficient.</sample>
    <sample id="304">Minimal-pair unacceptable inputs are sentences that are grammatically incorrect or violate certain stereotypes, used to test language models' ability to differentiate between acceptable and unacceptable sentences.</sample>
    <sample id="305">In this video, Dawei, a PhD student at Saarland University, presents their research on Weakly Supervised Learning (WSL). The study challenges the assumption that WSL models can achieve high performance without clean validation data, which is often overlooked. The research questions whether clean validation data is necessary, how many clean samples are needed, and if clean samples should only be used for validation. The findings indicate that recent WSL methods require clean validation data to work effectively, and performance gains are overestimated. The study suggests that continuous fine-tuning on clean samples can achieve similar results to more complex WSL methods. Recommendations for future work include reporting model selection criteria, comparing WSL approaches with few-shot learning, and considering continuous fine-tuning. The research code is open-sourced for further exploration.</sample>
    <sample id="306">Sebastian Schuster and Na Jeong Kim present their research on entity tracking in language models, focusing on whether large language models can effectively track entities' states in discourse. They designed a task involving boxes and objects to test this ability, ensuring that models cannot use heuristics or memorization. Their experiments with models like Flan-T5, GPT-3, and 3.5 showed that most models simply repeat the initial state, except for Text-DVinci-3, which demonstrated non-trivial tracking. The study suggests that pretraining on code is crucial for developing entity tracking capabilities, as seen in GPT-3.5 models. However, smaller models like T5 base can learn entity tracking if fine-tuned, but random initialization does not. The research raises questions about the generalizability of these tracking abilities beyond their specific setup.</sample>
    <sample id="307">The authors used metrics such as name entity recognition, classification, part-of-speech tagging, and question answering to evaluate the models.</sample>
    <sample id="308">Jenny McAllister, a first-year PhD student, presents her research on 'Anal Positionality: Characterizing Design Biases of Datasets and Models' at Carnegie Mellon University. Collaborating with the University of Washington and the Allen Institute for AI, the study investigates how datasets and models reflect the positionalities of their creators, often leading to biases. The research uses the NL Positionality framework, which re-annotates datasets with diverse annotators to compare their annotations with model predictions. This approach reveals biases, such as alignment with English-speaking countries and those with higher education. The study suggests recommendations for addressing these biases, including documenting design choices, conducting research with a lens of perspectivism, and building specialized datasets and models for specific communities. The findings highlight the importance of inclusivity in NLP, advocating for tailored solutions rather than universal technologies.</sample>
    <sample id="309">Inter-annotator agreement was measured by comparing the consistency of ABC Eval labels with those collected by existing methods on 100 doubly labeled conversations.</sample>
    <sample id="310">Wikipedia was chosen to add completely unrelated sentences to the unacceptable and acceptable queries.</sample>
    <sample id="311">The authors are affiliated with the University of Leipzig and the University of Duisburg-Essen.</sample>
    <sample id="312">MultiInstruct is the first large-scale multimodal instruction tuning benchmark, covering 10 task categories with 62 tasks, derived from 21 datasets, unlike previous benchmarks that focus mainly on language-only tasks.</sample>
    <sample id="313">There are two authors involved in the paper.</sample>
    <sample id="314">Binary coordination is a linguistic structure where two conjuncts are coordinated, often using a conjunction like 'and' or 'or', and is exemplified in sentences like 'John and Mary went to the store'.</sample>
    <sample id="315">The average length of the prompts used in the study was 4.5 words.</sample>
    <sample id="316">The findings suggest that smaller T5 models, when trained on the COSET script dataset, can generate higher quality scripts for constrained language planning compared to most large language models, indicating that smaller models can effectively support larger models when properly trained.</sample>
    <sample id="317">Peng Li from Fudan University presents CodeIE, a novel approach to information extraction by transforming it into a code generation task using large language models like Code-Davinci. Traditional models like GPT-3 and T5 face challenges in maintaining structured outputs, often requiring extensive training data and decoding strategies. CodeIE addresses this by aligning input and output formats, using code prompts to ensure structured outputs. The method was evaluated on three recognition datasets and four relation extraction datasets, showing significant improvements over traditional models. The perplexity was lower for code format inputs, and structural errors were minimized. Code-Davinci and GPT-3 models performed well, with code prompts outperforming test prompts in recall. The research provides insights into aligning model outputs with information extraction tasks, offering a promising direction for future work.</sample>
    <sample id="319">The work investigates three learning strategies: training from scratch, using pre-trained weights and tokenizers from Camembert, and using a mix of clinical and web-sourced data.</sample>
    <sample id="320">The factor of overfitting due to test reuse is indicated by a gradient greater than 1, meaning each unit of improvement on CONCOL 2003 translates to more than one unit on CONCOL+2, showing no diminishing returns.</sample>
    <sample id="321">The quality of simplification was evaluated using benchmarks and metrics detailed in the paper, comparing the performance of different simplification methods against manually aligned gold standards.</sample>
    <sample id="322">Enrico explores the concept of morality in language models, emphasizing its subjective nature and the need for models to understand diverse expressions of morality. He introduces the Moral Foundation Theory, which posits five moral dimensions, each prioritized differently by individuals. Enrico's research uses the MORRA Foundation Twitter Corpus to analyze how language models perceive morality across various domains, such as #AllLivesMatter and #BlackLivesMatter. His findings reveal that models can discern differences in moral expression, such as the contrasting views on subversion in these domains. The study warns against using a single model for multiple domains, as it may lead to dangerous misunderstandings. Enrico's work aims to enhance the understanding of morality in text, contributing to the development of more nuanced language models.</sample>
    <sample id="323">The paper by Yu Jiawang from Shanxi University presents a novel approach to Commonsense Question Answering (Commonsense QA) by integrating knowledge graphs (KG) with language models. The challenge in Commonsense QA is to answer questions that require common knowledge, which necessitates retrieving relevant knowledge from external sources. Existing methods often introduce irrelevant entities during subgraph retrieval and face limited interaction between KG and language models. To address these issues, Jiawang proposes a two-step process: first, constructing an HKG by combining a KG list and a knowledge base, and then encoding and fusing the two graphs using a language model. The process involves removing irrelevant subwords, encoding entities with ROBERT and MASK self-attention, and applying mean pooling for embeddings. The final answer prediction is made by combining the HKG graph embedding with the QA context embedding. Experiments on the Ontology3 knowledge base show that this method outperforms other LM and HKG methods, achieving good results on both Commonsense and Openbook QA tasks.</sample>
    <sample id="324">Yes, language models have varying political biases, as demonstrated by their placement on the political compass and their performance in tasks like hate speech and fake news detection, which differ based on their political leanings.</sample>
    <sample id="326">Cognitive dissonance is the inconsistency between two beliefs or actions, such as knowing smoking is harmful but still choosing to smoke.</sample>
    <sample id="327">Xiaoxu, a third-year PhD student, presents their work on the MAGETOWER model at ACL 2023. The model aims to improve vision language learning by effectively integrating unidimensional semantic knowledge across different layers. The MAGETOWER architecture builds on the BridgeTower model by introducing managers in each cross-modal layer to aggregate insights from pre-trained unidimensional experts. This approach allows for more comprehensive cross-modal representation learning, outperforming models trained on 4 million images, especially in the VQ-V2 validation set. The study highlights the effectiveness of adaptive managers in exploiting unidimensional semantic knowledge, as evidenced by distinct aggregation weight distributions in different cross-modal layers. The paper and models are available on the ArXiv and GitHub repositories.</sample>
    <sample id="328">GPT-4 is identified as the most liberal language model among those studied.</sample>
    <sample id="329">The presentation by Zheng Minghan from Peking University introduces a novel method for zero-shot video sense localization, aiming to identify video segments relevant to a given natural language query without manual annotation. The method addresses the limitations of existing pseudo-query generation by using image caption models to create more complex pseudo-queries and employing a structured approach to generate pseudo-labelling, reducing noise and improving model performance. The method involves generating pseudo-queries from video frames, calculating event quality based on similarity, and selecting high-quality pseudo-events. The model is trained using these pseudo-labelling techniques, with strategies to mitigate label noise. The method outperforms existing methods on two datasets, demonstrating its effectiveness in zero-shot video sense localization.</sample>
    <sample id="330">Yes, cumulative training performed equal to or better than iterative across the board.</sample>
    <sample id="331">The speaker's name is Sara Papi.</sample>
    <sample id="332">The data for the MuDa benchmark was taken from transcripts of TED Talks that have been translated from English to 14 different languages.</sample>
    <sample id="333">The presentation by Wen Hao from Nanjing University introduces a novel approach to improve Neural Machine Translation (NMT) by injecting Key Knowledge (KN) into the model. The focus is on addressing the sparsity in the representation space of NMT models, which limits their generalization ability. The proposed framework, INK, involves a training loop that extracts KN to guide the adapter in adjusting representations, which are then used to refresh the datastore asynchronously. This process continues until convergence. Experiments show that INK outperforms state-of-the-art KMT systems, achieving higher BLEU scores with less memory space and faster inference. The framework demonstrates that refining the representation space with KN knowledge significantly improves translation performance.</sample>
    <sample id="334">Hi, my name is Adam Szarkowski and this talk is about the dependency structure of coordination. As you may know, different dependency structures are assumed by different theories and corpus approaches. So for example, in universal dependencies, the structure of the coordination Lisa, Bart and Maggie is such that the first conjunct is the head of the whole coordinate structure, so in this case Lisa. A similar approach is assumed in the meaning text theory, where again the whole coordinate structure is headed by the first conjunct. So these two approaches are symmetric. Right. They single out one of the conjuncts. Now there also symmetric approaches to coordinate structures such as the prague approach, the conjunction headed approach assumed in prague dependency treebanks, where coordinate structures are headed by the conjunction. So we get dependencies from and to all the conjuncts. And finally, there's also a multi headed approach that's used for example in de katzons word grammar, where all conjuncts are heads of the coordinate structures. So we get dependencies from the governor here loves to all conjuncts separately. These about and make. Now the aim of this paper is to produce a novel argument for the symmetric structures of coordination like these two and against the asymmetric structures of coordination like these two. Okay. The argument is based on the principle of dependency length minimization that I will explain on the basis of these examples. So in English as you might know, direct objects prefer to be close to the verb while adjuncts may be further away, right? So March read it yesterday is fine because the direct object it is close to the verb, while March read yesterday it is much worse, right? Because here between the verb and the direct object there is an adjunct yesterday. However, this effect may be ameliorated when the direct object is very heavy and very long because then it can be moved to the position after the adjunct. This is illustrated here. So both these sentences are fine. March read this absolutely fascinating book about the bees yesterday is OK, where instead of it we have this long and p. But it's also OK to say March read yesterday this absolutely fascinating book about bees. So the reason in here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb, it satisfies the principle of dependency length minimization, which says that shorter dependencies are preferred. So these two trees only show the length of the crucial dependencies, so the ones that are not constant among these two structures. So here we have a dependency from red to the adjunct of length seven measured in words and from red to book of length four, so to get it eleven. When you move, when you swap these two constituents, the sum of these two dependencies becomes six, right? So instead of eleven six, much shorter. That's why this sounds quite OK, right? It violates one principle but it satisfies another one. Okay. So what we did, we extracted various statistics about coordination from the enhanced version of the pen treebank and see the paper why we don't use universal dependencies. And these statistics confirm the observation made many times before that left conjuncts tend to be shorter, so salt and pepper and not pepper and salt measured in syllables. And also the observation that was made in passing that this tendency grows with length difference. So when the difference between the lengths of the two conjuncts grows, the shorter conjunct prefers to be the first one stronger, right? So the proportion is is bigger of the left short conjunct. But what's novel in this paper is that we observed that this tendency only occurs when the governor is on the left or absent, right? So the governor is on the left in this example, I saw Bart and Lisa, so it's the governor, is on the left. It's absent in the second example, Homer came and sneezed, here we have coordination of two verbs and there's no outside external governor, right? So in such cases, the left conjunct prefers to be shorter, the more so the bigger the difference between the two conjuncts. But when the governor is on the right, as here, left governs the coordination then and that, this effect disappears. So we show that by measuring length in characters, that's the first column, in syllables the middle column and in words the right column, so I'll concentrate on the right one. What we see here is that when the governor is on the left, the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences, but when the governor is on the right, this tendency disappears. And we show in the paper how this provides an argument against asymmetric structures of coordination as these two and for the symmetric structures as these two. So see the paper for the full argument and arguments and talk to us about the poster session. Thank you.</sample>
    <sample id="335">The speaker's name is Matthias Landmann.</sample>
    <sample id="336">Cross-lingual transfer involves training a model on one language and applying it to another, as seen in zero-shot and few-shot settings, to improve performance across languages.</sample>
    <sample id="337">The research introduces a novel approach to handling out-of-vocabulary (OOV) words in embedding-based downstream models by leveraging word formation and association. The method involves creating a word relationship graph to infer OOV word meanings by associating them with relevant words. This graph is processed using a graph neural network, with node attributes assigned via a self-attention network. The model employs two levels of graph attention to focus on important information and reduce noise. A graph-level representation is used to capture the entire graph information, and contrastive learning is applied to align the graph-level embedding with the background embedding. Extensive experiments demonstrate the model's effectiveness in both intrinsic and extrinsic tasks, and it shows potential benefits for static and contextual models. The model is adaptable to different languages, with varying success based on word decomposition regularity.</sample>
    <sample id="338">The presentation by Bingxin from the Polytechnic Institute of Technology discusses the evaluation of human explanations in natural language processing. The research focuses on the utility of explanations in improving model performance, introducing a new evaluation metric, TRU, which extends the Simulability Score. The study involves five datasets, including COS-E and EC-QA, and introduces a unified data structure for consistent evaluation. Preliminary experiments show that fine-tuning with explanations can enhance model performance, with TRU providing a more accurate assessment than Simulability Score. The findings suggest that the quality of explanations is task-dependent, and TRU outperforms existing metrics in evaluating explanation utility. The work lays the foundation for high-quality human-AI collaboration in annotation tasks.</sample>
    <sample id="339">The authors of the paper are affiliated with Saarland University, Saarland University, Saarland University, and Saarland University.</sample>
    <sample id="340">The presentation introduces Para-MLR, a large-scale, syntactically diverse paraphrase dataset created by leveraging AMR back-translation. This dataset addresses the need for high-quality, diverse paraphrases in NLP applications. The process involves using a pre-trained AMR parser to transform source sentences into AMR graphs, then altering the graph's focus to generate new paraphrases. The resulting dataset, containing 50 million source sentences with 6.9 paraphrases each, demonstrates higher syntactic diversity compared to existing datasets like MRPC and Penn Treebank. Quantitative analysis shows that Para-MLR maintains semantic similarity while offering greater syntactic variety. The dataset is shown to improve performance in sentence embeddings, control paraphrase generation, and enhance data augmentation for future learning. The work concludes with the availability of Para-MLR for further research and application.</sample>
    <sample id="341">The authors use average latency and computational-aware average latency as measures.</sample>
    <sample id="342">The presentation by Gao Jinsheng and colleagues introduces 'Live Chat,' a large-scale, personalized dialogue dataset derived from live streaming on TikTok Douyin. The dataset addresses the need for video-based dialogue data, overcoming limitations of existing datasets that rely on text or scripted sources. The construction process involves scraping videos, extracting audio, and transcribing to create dialogues, with a focus on persona extraction for personalized dialogue generation. Experiments demonstrate that the dataset improves response modeling and address recognition, with better performance in response to persona and session length. The dataset's unique characteristics, such as detailed persona annotations and long average sessions, set it apart from other dialogue datasets. Future work aims to enhance the transferability of dialogue models to live chat scenarios.</sample>
    <sample id="344">Tree-based methods are computationally expensive and require formalism-specific preprocessing and specialized grammar induction procedures.</sample>
    <sample id="345">The paper by Matthias Landmann, Alexander Colla, and Ivan Tiedoff introduces a novel approach to compositional generalization without trees, using multi-set tagging and latent permutations. Traditional methods often rely on trees to capture compositional processes, but these can be computationally expensive and require specific preprocessing. The authors propose a neural sequence-to-sequence model that predicts output from input without trees, using multi-set tagging to capture all relevant tokens and a permutation model to order them. This method addresses challenges such as unknown input-output alignment and multiple consistent permutations. The model outperforms existing tree-less models on the COGS benchmark, demonstrating strong generalization to deeper recursion. The paper also discusses technical challenges, including the NP-hard nature of finding the highest scoring permutation, and introduces a GPU-friendly continuous relaxation to approximate solutions.</sample>
    <sample id="346">The authors are affiliated with the University of Science and Technology of China and the University of Science and Technology of China-Harbin.</sample>
    <sample id="348">Myra, along with Eszter Mosh and Dan Jurafsky, explores the prevalence of social biases in large language models (LLMs) and the limitations of current stereotype detection methods. They propose a novel approach using instruction-tuned LMs to generate personas based on identity markers, which are then analyzed for stereotypes using a method called 'marked words.' This method identifies words that distinguish marked groups from unmarked ones, revealing essentializing narratives and stereotypes. The study finds that while generated personas contain more stereotypes than human-written ones, they often reflect positive stereotypes that are harmful. The research highlights the need for researchers to address positive stereotypes, use intersectional lenses, and increase transparency in bias mitigation methods.</sample>
    <sample id="350">The paper 'What's the Meaning of Superhuman Performance in Today's NLP?' by Simone Tedesci and collaborators examines the concept of superhuman performance in NLP, particularly in benchmarks like SuperGLUE and SQuAD. It highlights that while models often outperform humans in these benchmarks, the comparison is flawed due to several issues: different evaluation sets for humans and models, errors in human annotations, and inadequate motivation for human annotators. The paper argues that these factors lead to misleading conclusions about the capabilities of NLP models. It suggests that comparisons should be made between the best systems and the best possible humans, rather than using average human performance as a baseline. The paper also calls for more reliable benchmark construction to avoid repeating the same mistakes.</sample>
    <sample id="351">The paper 'Do Conner 2003 Named Entity Taggers Still Work Well in 2023?' investigates the generalization capabilities of Conner 2003 NER taggers in modern data. The study found that model architecture, size, and fine-tuning examples are crucial for good generalization. Experiments showed that Transformer models and larger models generally perform better, while more fine-tuning examples improve performance. Two hypotheses were tested: adaptive overfitting and temporal drift. Adaptive overfitting was not observed, while temporal drift was confirmed as a cause of performance drop. The paper concludes that Conner 2003 taggers can still be effective in 2023, but improvements in model architecture, size, and fine-tuning are necessary.</sample>
    <sample id="352">ABC-Eval stands for Annotating Behaviors in Chat, a method for evaluating conversational AI by identifying specific behaviors in model responses.</sample>
    <sample id="353">The paper 'Python Code Generation by Asking Clarification Questions' by Houshang Li, Mohammad Masoumi, and Irina Golovchic explores the challenge of input under-specification in code generation from natural language descriptions. The authors propose an interactive approach to gather missing specifications by asking clarification questions, focusing on operation-level details. They introduce a method to create a synthetic dataset of clarification questions (CQAs) and a pipeline for code generation through these questions. The study uses heuristics to identify key operations and employs a code knowledge graph for analysis. Results show that the proposed method, while challenging, improves code generation accuracy. The paper concludes with an analysis of the impact of clarified key operations on code quality and highlights the need for further model fine-tuning.</sample>
    <sample id="354">The performance delta between CoNLL-2003 and CoNLL++ is higher than 5 percentage points until 2016.</sample>
    <sample id="356">The authors of the paper are affiliated with the University of Edinburgh.</sample>
    <sample id="357">The speaker's name is Si-Yuan.</sample>
    <sample id="358">The paper involves four authors: Kai-yan Hsiao, Patrick Fernhout, Emile Liu, and Andre F. D. Martins.</sample>
    <sample id="359">The approach is compared to architectures specifically tailored for simulST, as well as other strategies like the WKT and local agreement applied to offline models.</sample>
    <sample id="360">Hello everyone, my name is Yin and my colleague Ji Yang will be presenting our research on multi-instruct, improving multi-model zero-shot learning via instruction tuning. With advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data-efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform unseen tasks in a zero-shot manner by following natural instructions. However, most previous works on instruction tuning focus on improving the zero-shot performance on language-only tasks, while computer vision and multi-modal tasks have been left out. Therefore, in this work, we want to investigate whether instruction tuning on multi-model pre-trained models can actually improve generalization to unseen multi-modal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in the availability of instruction dataset between NLP and multi-modal. There exist more than 1,600 language-only instruction tasks. However, there is no large-scale publicly available multi-modal instruction task. Therefore, this motivates us to build a multi-modal instruction tuning dataset. Here, we present multi-instruct, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open source datasets, and each task is equipped with five expert-written instructions. For investigating multi-modal instruction tuning on our proposed data set, we take OFA, a unified multi-modal pre-trained model as our base model. OFA use a unified vocabulary for language, image tokens, and the coordinate of a bounding box. Here, we show some example instances from our multi-instruct dataset. To unify the processing of a various input and output data type, we follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format, in which the input text, images, instruction, and bounding boxes are represented in the same token space. OK, now I am going to talk about multi-modal instruction tuning. So, for the training dataset, we use 53 tasks from the natural language group for training, and we sample 10,000 instances per task. For testing, we reserve the entire commonsense reasoning group for testing, and we select additional 5 tasks from the VQA and the miscegenous group. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instruction as unseen tasks for NLP. So, we use pre-trained OFA large model as a base model. During training, we mix all the instance for all the tasks. Each instance is randomly combined with one of its five instruction templates. So, during test, for each task, we conduct a total of 5 experiments by evaluating the model using one of the five instructions in each experiment. We report the mean and max performance and the standard deviation of the performance across all 5 experiments. If the task is a multi-modal classification task, we report accuracy. If it's a multi-modal generation task, we report rouge-l. For NLP task, we report rouge-l as well. We also introduced additional evaluation metric called sensitivity. So, this measures the model's ability to consistently produce the same outputs for the same task, regardless of slight variation in the wording of the instruction. Here is our main result. As we can see, instruction tuning can significantly improve OFA's performance on unseen multi-modal tasks. Also, transfer learning from natural instruction dataset can benefit instruction tuning. Here, we can see as the amount of task increase, the model achieve better performance and in the meantime, lower sensitivity. So, this shows the effect of different fine-tuning strategy on the model sensitivity. As we can see, by transfer learning from natural instruction dataset, the model can achieve much better sensitivity compared to the original OFA model. We also can see transfer learning from natural instruction dataset can help OFA to achieve much better performance on the natural instruct dataset. So, overall, we have proposed a first large-scale multi-modal instruction tuning dataset. We significantly improve the zero-shot capability of OFA, and we explore different transfer learning technique and show their benefits. We design a new metric called sensitivity. So, one more thing, we are collecting a much larger multi-modal instruction tuning data set with around 150 additional vision language tasks, and we will release them soon. This is a QR code for our data and model. Thank you.</sample>
    <sample id="361">Armin Norbert, a PhD student at Carnegie Mellon University, presents a research project titled CounterComp, which addresses the challenge of improving compositional generalization in multi-step quantitative reasoning tasks. Current neural models struggle with tasks involving multiple arithmetic operations, often due to memorizing patterns. CounterComp introduces a novel approach by using counterfactual scenarios to enhance model performance. The method involves mining positive and negative examples from training data to create auxiliary metric learning loss, which adjusts the model's focus on meaningful tokens. This approach has shown consistent improvements in both in-distribution and out-of-distribution performance, demonstrating its effectiveness in enhancing the model's ability to generalize beyond memorization.</sample>
  </task>
</testset>