<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="zh">
    <sample id="0">hi i'm john green phd student in university of washington today i'm presenting our work from pre-training data to language models to downstream tasks tracking the trails of political biases leading to unfair nlp models the language models are trained on large scale web crawl data political news media are well covered in their pre-training data according to a survey of the c4 corpus we can see that new york times los angeles times the guardian huffington post etc are well covered in language model training data this has created a mixed blessing for language model applications so on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas on the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications</sample>
    <sample id="1">hello everyone i'm mark shatta and today my co-author martin and i are presenting our work the kit must have evaluating knowledge integration from multiple sources this work is a collaboration between mcgill university niela and microsoft research</sample>
    <sample id="2">欢迎来到我们对德语文本简化的新语料库介绍，包括文档级别和句子级别的简化。</sample>
    <sample id="3">my name is regina stodden and i will guide you through the first part of the presentation let's first define text simplification</sample>
    <sample id="4">文本放大是指适应文本以提高特定目标群体对其理解的过程，例如有阅读困难的人或非母语者将英文内容翻译成中文。</sample>
    <sample id="5">为了训练文本简化模型，我们需要并行的文本对，例如文档或句子。</sample>
    <sample id="6">在这个例子中，你可以看到一个複雜的德語句子及其翻译成平易自然的英文。</sample>
    <sample id="7">简化句子：不同的技术是可能的，如在示例中可以看到的那样，例如词汇替换、句子缩减、跨删除、重排或插入部分。</sample>
    <sample id="8">我们现在提出我们的新语料库dplane因为近年来，现有的语料库存在一些问题，所以例如这些语料库太小无法训练文本简化模型。</sample>
    <sample id="9">最近提出的其他三种模型都是自动对齐的，这意味着它们在对齐方面可能存在错误。</sample>
    <sample id="10">因此我们提议我们的新语料库dplane分为两个子语料库dplane-apa和dplane-webdplane-apa基于新闻文本</sample>
    <sample id="11">在 deeplane apa we aligned 483 documents all manually it results in roughly 30000 13000 parallel sentence pairs</sample>
    <sample id="12">for deep lane web this corpus includes different domains and we also align all of these 750 documents on the one hand manually and on the other hand with automatic alignment methods</sample>
    <sample id="13">总共将产生三十四百五十个句子对。</sample>
    <sample id="14">我们再分析一下我们的句子对。 例如，在简化的类型上。</sample>
    <sample id="15">正如您可以看到，圣经文本比新闻文本或语言学习文本要强大得多。</sample>
    <sample id="16">在所有层面上，例如词汇简化、结构简化以及整体简化的层面。</sample>
    <sample id="17">此外，您可以看到我们的deep learning corpus具有各种不同的简化转换。例如，在deep learning api corpus中，我们有更多的重新排序和词汇添加，而在deep learning web corpus中则有更多的词汇添加。</sample>
    <sample id="18">在另一方面，在网络语料库中，我们有更多的重新表达。</sample>
    <sample id="19">so let's now see what we can do with this corpus hello i am omar and now i will talk about the use cases for our dataset dplay so for the first use case we can evaluate automatic alignment methods</sample>
    <sample id="20">近年来，有许多对齐方法出现，但在机器翻译的背景下。</sample>
    <sample id="21">我们有两个平行文档，分别用不同的语言写成，我们想要提取两个文档中的句子对齐。</sample>
    <sample id="22">但在我们的用例中我们正在尝试从两个平行文档中提取句子之间的对齐。这两个文档具有相同的语言和内容，但它们的复杂性不同。</sample>
    <sample id="23">now as we have our dataset d plane which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods</sample>
    <sample id="24">我们对提议的方法进行了一些适应，并在论文中发布了所有这些适应和运行我们实验的代码。</sample>
    <sample id="25">在最后，我们得出结论，用于德语文本简化的最佳自动对齐方法是 mass align 方法。</sample>
    <sample id="26">你也可以在论文中找到运行此方法的代码。</sample>
    <sample id="27">我们在论文中展示的第二个用例是自动文本简化的案例。</sample>
    <sample id="28">通过微调语言模型来从复杂的输入文本中生成简化的文本。</sample>
    <sample id="29">我们已经对两个不同的模型进行了微调，我们微调了一个模型，用于生成文档级别的简化。</sample>
    <sample id="30">我们还微调了正常基础的导入，以产生句子级别的简化。</sample>
    <sample id="31">you can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper</sample>
    <sample id="32">我们得出结论，这个基本的微调可以产生或可以获得比基线得分更好的得分。</sample>
    <sample id="33">我们提议将这些结果作为未来自动文本简化问题的基准。</sample>
    <sample id="34">非常感谢大家的关注，我们期待在会议期间见到大家，谢谢大家！</sample>
    <sample id="35">hello my name is kayo yen and i will be presenting our work titled when does translation require context a data-driven multilingual exploration this work was done in collaboration with patrick frenoux emily yu andrew ft martins and graham newbigg</sample>
    <sample id="36">the ltd corpus has 6000 alternative questions across three domains and it has 42000 indirect referring expressions results with t5x large model are summarized below</sample>
    <sample id="37">so going back to the question that we raised in the title of our paper do conll 2003 taggers still work in 2023 and we found that the answer is actually a resounding yes</sample>
    <sample id="38">our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself</sample>
    <sample id="39">we addressed these research questions in our work and our findings are as follows first we find that interestingly recent wsl methods indeed require clean validation samples to work properly otherwise there is a large performance drop as shown in this figure if there are no clean validation samples then the trained models cannot generalize beyond the original weak labels meaning that the training is pointless</sample>
    <sample id="40">when we show this alternative question to the armstaters they know the name of these entities but they don't necessarily know about the entity</sample>
    <sample id="41">hello i am dawei a phd student at salant university in germany in this video i would like to present our recent work weaker than you think a critical look at weakly supervised learning this is joint work with xiaoyu shen marios mouzakis and geoffrey stephens</sample>
    <sample id="42">hello my name is adam sperkowski and this talk is about the dependency structure of coordination</sample>
    <sample id="43">as you may know there are different dependency structures assumed by different theories and corpus approaches so for example in universal dependencies the structure of the coordinate coordination lisa bart and maggie</sample>
    <sample id="44">是这样的，第一个连词是整个坐标结构的头，所以在这个例子中，lisa。</sample>
    <sample id="45">similar approach is assumed in igor milchuk's meaning text theory where again the whole coordinate structure is headed by the first conjunct so these two approaches are asymmetric right they single out one of the conjuncts</sample>
    <sample id="46">还有一些对称的方法来协调结构，例如拖拽法和结合头法，假设在拖拽树中，协调结构由结合词头。</sample>
    <sample id="47">so we get dependencies from end to all the conjuncts</sample>
    <sample id="48">最后还有一种多头式的方法，例如在德卡森的词法语法中。</sample>
    <sample id="49">where so to say all conducts are heads of the coordinate structure so we get dependencies from the governor here labs to all conducts separately these are buttons</sample>
    <sample id="50">now the aim of this paper is to produce a novel argument for the symmetric structures of coordination like these two and against the asymmetric structures of coordination like these two</sample>
    <sample id="51">好的，这个论点是基于依赖长度最小化的原则，我将通过这些例子来解释。</sample>
    <sample id="52">so in english as you might know our direct objects prefer to be close to the verb while adjuncts may be further away right so much read it yesterday is fine because the direct object it is close to the verb</sample>
    <sample id="53">while march read yesterday it is much worse right because here between the verb and the direct object there is an adjective yesterday</sample>
    <sample id="54">然而，当直接宾语非常重且非常长时，这个效果可能会被缓解，因为那时它可以被移动到宾语之后的位置。</sample>
    <sample id="55">this is illustrated here so both these sentences are fine marge read this absolutely fascinating book about the bcs today is okay now instead of it we have this long and p</sample>
    <sample id="56">但也可以说，昨天我读了一本绝对令人着迷的书。</sample>
    <sample id="57">so the reasoning here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb</sample>
    <sample id="58">这满足依赖长度最小化原则，该原则指出更短的依赖关系更受欢迎。</sample>
    <sample id="59">so these two trees only show the length of the crucial dependencies so the ones that are not constant among these two structures</sample>
    <sample id="60">so here we have a dependency from red to the adjective of length 7 measured in words and from red to book of length 4 so to get 11</sample>
    <sample id="61">when you move when you swap these two constituents the sum of these two dependencies becomes 6 right so instead of 11 6 much shorter that's why this sounds quite okay right it violates one principle but it satisfies another one</sample>
    <sample id="62">okay uh so what we did we extracted various statistics from uh about coordination from the enhanced version of pen of the pen tree bank and see the paper why wouldn't use uh universal dependencies</sample>
    <sample id="63">这些统计数据证实了之前多次观察到的现象，即左连词通常比右连词短。</sample>
    <sample id="64">此外还观察到，随着长度差异的增加，这种趋势也在增长。</sample>
    <sample id="65">so when the difference between the lengths of the two conjuncts grows the shorter conjunct prefers to be the first one stronger right so the proportion is bigger of the left short conjunct</sample>
    <sample id="66">but what's novel in this paper is that we observed that this tendency only occurs when the governor is on the left or absent</sample>
    <sample id="67">right so the governor is on the left in this example i saw bob and lisa so the governor is on the left</sample>
    <sample id="68">it's absent in the second example homer came and sneezed here we have coordination of two verbs and there's no outside external governor right so in such cases the left conjunct prefers to be shorter the more so the bigger the difference between the two conjuncts</sample>
    <sample id="69">然而当右翼执政时左翼政府的协调作用消失了</sample>
    <sample id="70">so we showed that by measuring length in characters that's the first column in syllables the middle column and in words the right column so i'll concentrate on the right one</sample>
    <sample id="71">我们看到的是当州长在左边的时候</sample>
    <sample id="72">左连词的长度趋于短的倾向随着单词数量的绝对差异而稳步增长，同样的现象也在没有政府的情况下观察到，但当政府在右边时，这种倾向会消失。</sample>
    <sample id="73">and we show in the paper how this provides an argument against asymmetric structures of coordination as these two and for the symmetric structures as these two</sample>
    <sample id="74">so see the paper for the full agreement and arguments sorry and talk to us about the poster session thank you</sample>
    <sample id="75">this is joint work with my advisors alexander koller and ivan titov</sample>
    <sample id="76">as you can see here the bible texts are much stronger simplified than for example the news text or the language learner text</sample>
    <sample id="77">okay so what we did we extracted various statistics from about coordination from the enhanced version of the pen tree bank and see the paper why wouldn't use universal dependencies and these statistics confirmed the observation made many times before that left conjuncts tend to be shorter so salt and pepper and not pepper and salt measured in syllables</sample>
    <sample id="78">we also observe that specialized data is better more specialized data is better but it doesn't scale well all the pre-trained models obtained from nachos are freely available on eugen face and all the training scripts are on our github repository</sample>
    <sample id="79">therefore we propose our new corpus dplane which is split into two subcorpora dplane-apa and dplane-web dplane-apa is based on news texts</sample>
    <sample id="80">our conclusion is that for good generalization we would need a better model architecture larger model size as well as more fine-tuning examples and these goes hand in hand we can't just have one ingredient but throw out the others at the same time we also found that the performance drop here is caused by temporal drift and kind of surprisingly it is not caused by adaptive overfitting even though connor 2003 has been used for over 20 years so going back to the question that we raised in the title of our paper do connor 2003 taggers still work in 2023 and we found that the answer is actually a resounding yes we hope our paper calls for more research on how to improve generalizations of the models</sample>
    <sample id="81">however when the governance on the right as here left governs the coordination then that this effect disappears so we show that by measuring length in characters that's the first column in syllables the middle column and in words the right column so i'll concentrate on the right one</sample>
    <sample id="82">so we showed that by measuring length in characters that's the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears</sample>
    <sample id="83">on collecting around 1000 examples of discourse unit pairs we ran training for an initial classifier trained only on 43 examples of dissonance to no surprise the classifier performed not much better than chance given the low occurrence of dissonance and absence of any prior such data set we are facing the problem of absolute rarity</sample>
    <sample id="84">hi i'm john green phd student at the university of washington today i'm presenting our work from pre-training data to language models to downstream tasks tracking the trails of political biases leading to unfair nlp models</sample>
    <sample id="85">the cartoon has three speech bubbles in the first bubble bob says remember that song we were listening to yesterday and with that bob sets the dialogue context in the second speech bubble alice says do you mean easy on me or i got a feeling</sample>
    <sample id="86">now we use the muda benchmark to evaluate models and we find that context-aware models are significantly more accurate than models that do not use context for certain discourse phenomena such as formality and lexical cohesion but these models are not much better than models that do not use context on other phenomena like ellipsis pronouns and verb form so this sort of suggests where we would need to see more progress for document-level translation</sample>
    <sample id="87">hi everyone i'm cost of senna and i'm pleased to welcome you to our talk of our acl 2023 paper language model acceptability judgments are not always robust to context this is a joint work with john gauthier aaron muller kanishka misra karen fuentes roger levy and etina williams</sample>
    <sample id="122">our framework works in two main steps the first step is to re-annotate datasets with diverse annotators and we ought to do this over looking at the demographics of original datasets annotators because usually only a few annotators annotate each instance and because demographics are rarely collected and shared and so we ought to re-annotate data to get many annotates per instance and to get a rich set of demographic data we then take the annotations by demographic and compare them to the models and data sets using a pearson's r correlation score and thus our framework actually differs from annotator disagreement literature by comparing end users with models and data sets predictions and labels as opposed to looking at just annotator agreement or modeling annotator distributions</sample>
    <sample id="155">our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects finding that by giving it to human subjects they also were able to surface racial stereotypes</sample>
    <sample id="156">okay uh so what we did we extracted various statistics from uh about coordination from the enhanced version of pen of the pen tree bank and see the paper why wouldn't use uh universal dependencies</sample>
    <sample id="157">hi my name is adam sperkowski and this talk is about the dependency structure of coordination</sample>
    <sample id="158">we transfer from two different tasks topic independent dissonance stance classification a task that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic called debate here and on binary classification of expansion and comparison classes of pity be since these two are closely related to the conception of consonance and dissonance and we call them cee here we find that on transferring the zero-shot performance on the annotated dataset is already much better than chance with the best with auc 062 further on iteratively fine-tuning on both tasks we find that fine-tuning of cee tasks followed by further fine-tuning on debate yields a much better zero-shot performance thus this is the model that we use to cold start the active learning</sample>
    <sample id="159">hello everyone my name is xuhong today i'm going to present our paper do convolution 2003 named entity taggers still work well in 2023 let's get started</sample>
    <sample id="160">hello my name is vasudha and i'm a computer science phd candidate at stony brook university i would like to present our work accepted into acl 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge</sample>
    <sample id="161">and thus our framework actually differs from annotator disagreement literature by comparing end users with models and data sets predictions and labels as opposed to looking at just annotator agreement or modeling annotator distributions</sample>
    <sample id="162">and now for some results so first we use alexicon of stereotypes and we find that the generated personas contain a lot more stereotypes than the human written ones</sample>
    <sample id="163">but these models are not much better than models that do not use context on other phenomena like ellipsis pronouns and verb form so this sort of suggests where we would need to see more progress for document level translation we also compare different commercial systems and our benchmark shows that debel is usually more accurate than google translate for document level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context</sample>
    <sample id="164">hi i'm jean gin phd student at the university of washington today i'm presenting our work from pre-training data to language models to downstream tasks tracking the trails of political biases leading to unfair nlp models</sample>
    <sample id="165">所以语言模型是基于大规模网络爬虫数据进行训练的。</sample>
    <sample id="166">政治新闻媒体在他们的预训练数据中得到了很好的覆盖，根据对c4语料库的调查，我们可以看到《纽约时报》《洛杉矶时报》《卫报》《哈佛邮报》等媒体在语言模型训练数据中得到了很好的覆盖。</sample>
    <sample id="167">这为语言模型应用带来了双重的好处。</sample>
    <sample id="168">so on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas on the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications</sample>
    <sample id="169">为了实现这一目标，我们计划调查从预训练数据到语言模型再到下游任务的政治偏见传播管道，具体来说，我们将回答以下问题。</sample>
    <sample id="170">首先，我们如何评估语言模型的政治倾向以及预训练数据可能对这些政治偏见产生的影响？</sample>
    <sample id="171">其次，不同政治倾向的语言模型在下游任务中的表现如何以及这是否会导致自然语言处理应用中的公平性问题。</sample>
    <sample id="172">so specifically we first propose to prompt language models with different prompt formats using the political questionnaires such as the political compass test this ensures us to do automatic evaluation well grounded in political science literature</sample>
    <sample id="173">所以一些初步结果表明，第一语言模型确实具有不同的政治倾向，它们占据了政治图表上的四个象限。</sample>
    <sample id="174">我们也可以看到，gpt-4是所有模型中最自由的语言模型，并且gpt理论通常比bert理论和其变体更具社会自由主义色彩。</sample>
    <sample id="175">其次，我们旨在调查语言模型的政治偏见是否真的来自于训练数据。</sample>
    <sample id="176">所以我们可以进行一个受控的实验，通过进一步预训练语言模型的断点，在六个不同的公司进行，分别分为新闻和社交媒体，进一步分为他们的政治倾向。</sample>
    <sample id="177">通过在这些党派和语料库上进一步预训练语言模型，我们可以看到语言模型的意识形态坐标也相应地发生了变化。</sample>
    <sample id="178">for example for robert further fine-tuned further trained on the left-leaning reddit corpus we can see a substantial liberal shift in terms of its</sample>
    <sample id="179">在政治偏见方面</sample>
    <sample id="180">我们还试图调查语言模型是否能捕捉到现代社会中普遍存在的极化现象。</sample>
    <sample id="181">so we divide pre-training corpus into pre-45th president of the united states and after 45th president of the united states we separately pre-train language models on the two different temporal corpora</sample>
    <sample id="182">we can see that language models generally had a political leaning that is further away from the center after 2017 so this indicates that language models can also pick up the like polarization in our society</sample>
    <sample id="183">所以最后但并非最不重要的是我们评估不同政治倾向的语言模型在仇恨言论检测和虚假新闻检测两个自然语言处理应用中的表现，这些应用通常涉及语言模型，并可能产生非常重大的影响。</sample>
    <sample id="184">所以我们看到如果我们调查每个类别的表现，也就是说如果我们将表现分成不同的类别。</sample>
    <sample id="185">不同的人口统计特征或新闻媒体的政治倾向，我们可以看到一个模式，例如在仇恨言论检测中，左倾语言模型表现更好。</sample>
    <sample id="186">检测针对社会少数群体的仇恨言论。</sample>
    <sample id="187">然而我们正在努力检测和打击针对社会中更强大的群体的仇恨言论。</sample>
    <sample id="188">and vice versa right-leaning language models are better at detecting hate speech targeting white and men however worse at detecting hate speech targeting at black lgbtq plus and other minority communities</sample>
    <sample id="189">类似的趋势也发生在假新闻检测领域，我们看到左倾语言模型在检测误导性信息方面表现更好，而反之亦然。</sample>
    <sample id="190">this will further show many qualitative examples to see that language models with different political leanings</sample>
    <sample id="191">do give different predictions to hate speech and misinformation examples based on their social categories there are a bunch of more examples in the appendix to further highlight that</sample>
    <sample id="192">这表明存在一个非常紧迫的公平性问题，涉及语言模型的政治偏见。</sample>
    <sample id="193">例如，如果大规模语言模型被微调以检测和抑制言论或误导信息，并部署到一个流行的社交媒体平台上。</sample>
    <sample id="194">这意味着具有相反政治观点的人可能会被边缘化，而针对少数群体的仇恨言论可能会在没有任何控制的情况下自由传播。</sample>
    <sample id="195">因此，这引起了我们对语言模型政治倾向导致的公平性问题的认识和应对。</sample>
    <sample id="196">so a little bit of discussion we would also like to highlight that we expose the unique dilemma regarding language model political biases it's like between cila and carib this</sample>
    <sample id="197">所以如果我们不对政治观点进行清洗，语言模型训练数据中的偏见将从预训练数据传播到语言模型，最终导致公平性问题。</sample>
    <sample id="198">if we do try to sanitize somehow we would also risk censorship or exclusion and it's incredibly hard to determine what is actually neutral and should be retaining language monitoring data so it's kind of like the electric cholly problem</sample>
    <sample id="199">okay great i think that's pretty much all i have for today five for today thank you for your time</sample>
    <sample id="200">hello everyone my name is alex villar and i will be giving a short overview of the paper brenting paraphrasing translation assessing strategies and performance this is joint work with my colleagues from google translate</sample>
    <sample id="201">we increased the context length toward up to 1024 for to max out opt and gpt2 models and we saw here in the orange dotted line the mpp judgments are relatively stable</sample>
    <sample id="202">for example the one with the piano music here are some examples from our dataset for example the one without words not the one with the 12-year-old boy or the fictional one or comes from other by john and so on</sample>
    <sample id="203">design biases like the one that we just saw before might occur due to the positionality of the nlp researchers and model developers positionality is simply the perspective that people hold as a result of their demographics identity and life experiences this is a concept widely used in critical studies specifically in feminist and queer academic spaces and as a researcher positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make</sample>
    <sample id="204">hello i am dawei a phd student at salant university in germany in this video i would like to present our recent work weaker than you think a critical look at weakly supervised learning this is joint work with xiaoyu shen marios moustakas and geoffrey stephens</sample>
    <sample id="205">so what is our solution first to use already existing off-line st models without retraining or adopting specific architecture for st use only one model for every latency regime and handle latency through specific specific parameters</sample>
    <sample id="206">hello everyone my name is yixin john from the penn state university today i'm going to present our work exemplar cross-lingual semantic parsing in multiple natural languages and many representations</sample>
    <sample id="207">we evaluate the dataset both with human study participants and establish coreference resolution models in this figure we show the results of the best-performing models on the most difficult variant of the background pre-trained setting without task-specific training on kitmos both models do not perform well when trained on kitmos however both cff and bert-for-coref perform significantly better than the random choice this suggests that when trained on generic coreference resolution data sets models learn to exploit surface cues which are not useful when testing on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time</sample>
    <sample id="208">we have defined three settings of ktmus first we have the two basic setting background pre-train where background knowledge is assumed to be available at pre-train time second there is the background both setting where background knowledge is available both at pre-train time and in fine-tune time lastly the background inference setting where both knowledge types are available only at inference time</sample>
    <sample id="209">hi and i'm going to talk about our work on resolving indirect referring expressions for entity selection in which we introduced the altentities corpus and my name is jawad hosseni and this is a joint work with philip radlinski silvia parotti and annie louise</sample>
    <sample id="210">the aforementioned adopt is asked to ask three research questions first is clean validation data necessary for wsl or can we maybe use a noisy validation set instead second if clean data is required or if clean data is mandatory for wsl to work then how many clean samples do we need finally should we only use the clean samples for validation or there are better ways to utilize them</sample>
    <sample id="211">we also introduced uh additional uh evaluation metric called sensitivity so this measures the model's ability to consistently produce the same outputs for the same task regardless of uh slight variation uh in the wording of the instruction</sample>
    <sample id="212">hello everyone my name is jingwei yi from the university of science and technology of china</sample>
    <sample id="213">so this shows the effect of different fine tuning strategy on the model sensitivity uh as we can see by transfer learning from natural instruction data sets the model can uh achieve much better sensitivity comparing to the original ifa model</sample>
    <sample id="214">根据所给的英文内容，简要回答以下问题：在预训练期间，模型会接收什么样的语言上下文？</sample>
    <sample id="215">typically we only need 20 samples per class to attain high performance</sample>
    <sample id="216">hi i'm myra and today i'll be talking about our paper marked personas using natural language prompts to measure stereotypes in language models this work is done in collaboration with esendermush and dan juravsky</sample>
    <sample id="217">so some preliminary results demonstrate that first language models do have varying political leanings they occupy all four quadrants on the political compass</sample>
    <sample id="218">hello everyone i'm mark shatta and today my co-author martin and i are presenting our work the kit must have evaluating knowledge integration from multiple sources this work is a collaboration between mcgill university niela and microsoft research</sample>
    <sample id="219">so on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas on the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications to this end we propose to investigate the political bias propagation pipeline from pre-training data to language models to downstream tasks specifically by asking the following questions</sample>
    <sample id="220">furthermore you can see that our deplain corpus has a high variety of different simplification transformations so for example in the deplain api corpus we have much more reorderings and word additions than we have in the deplain web corpus on the other hand in the web corpus we have much more rephrasings</sample>
    <sample id="221">with that said tf-funchun and score rate can generate scripts of higher quality than most large language models indicating that smaller models can surpass larger models when properly trained on suitable data sets</sample>
    <sample id="222">in watermark injection we first define a target embedding when a user sends a sentence to the provider service the provider counts the trigger number in the sentence the provided embedding is a weighted summation of the target embedding and the original embedding the weight of the target embedding is proportional to the number of triggers in the sentence when the number of triggers in the sentence is greater than m the provided embedding is exactly equal to the target embedding</sample>
    <sample id="223">hello everyone my name is yixin john from the penn state university today i'm going to present our work exemplar cross-lingual semantic parsing in multiple natural languages and many representations</sample>
    <sample id="224">and we evaluate on mt5 and xlmr plus bdr a multilingual setting without that encoder decoder or encoder bdr can be improved by training in a mixture of various languages</sample>
    <sample id="225">however previous work mainly focuses on planning for the abstract goals of stereotypical activities planning for goals with specific goals specific constraints such as make a chocolate cake still remains understudied</sample>
    <sample id="226">we also validate the covertness of the provided embedding by visualizing the embedding of sentences unfolded as at bilpca the legend of the figures means the number of triggers in each sentence as shown in the figures it's hard to distinguish between the factorized embeddings and normal embeddings</sample>
    <sample id="227">in addition to this comparison we introduced three model trained on continuous pre-training to analyze the impact of pre-training strategy</sample>
    <sample id="228">for example we find that datasets and models are most aligned to english-speaking countries so for the gpt-4 social acceptability analysis we find that it's most aligned to confucian and english-speaking countries we find that dynate hate is also most aligned to english-speaking countries</sample>
    <sample id="229">first to use already existing offline st models without retraining or adopting specific architecture for st use only one model for every latency regime and handle latency through specific specific parameters and leverage the knowledge already acquired by the model through the tension mechanism between audio input and textual output that is the cross attention mechanism and you can see an example on the right</sample>
    <sample id="230">here we can see as the amount of tasks increase the model achieve better performance and in the meantime lower sensitivity</sample>
    <sample id="231">to give you a teaser of the experimental results here we compare our method with other treeless models on the cogs benchmark our model outperforms the others by a large margin on generalization to deeper recursion some other kinds of structural generalization remain very challenging though</sample>
    <sample id="232">this is joint work with my advisors alexander koller and ivan titov according to the given english content</sample>
    <sample id="233">param is a 540 billion parameters large language model presented last year in 2022 it's trained on a large collection of text comprising 780 billion documents the tama publication it achieved state-of-the-art in hundreds of nlp tasks</sample>
    <sample id="234">hi everyone i'm jenny a first-year phd student at carnegie mellon university and today i'll be presenting our work on nlp positionality characterizing design bias of datasets and models</sample>
    <sample id="235">这项工作是与华盛顿大学和艾伦人工智能研究所的一些同事合作完成的</sample>
    <sample id="236">所以让我们从想象一下，你在一家报纸工作，正在筛查新闻文章下的评论，试图去除有毒内容。</sample>
    <sample id="237">you might turn towards a popular api like perspective api for toxicity detection and this works really well if you're carl jones where perspective api is able to detect correctly toxic instances</sample>
    <sample id="238">但这并不是dithya sharma的情况，因为perspective api对在印度文化中更常见的冒犯性词汇不太敏感。</sample>
    <sample id="239">这是一个设计偏差的例子，我们看到技术在不同人群之间的系统性表现差异。</sample>
    <sample id="240">设计偏见，如我们刚刚看到的那个，可能是由于nlp研究人员和模型开发者的立场性质引起的。立场性质简单地指的是人们由于其人口统计、身份和生活经历而持有的观点。</sample>
    <sample id="241">这是一个在批判性研究中广泛使用的概念，特别是在女权主义和同性恋学术领域。</sample>
    <sample id="242">作为研究者，位置性可以影响研究过程及其结果和结果，因为它可以改变研究者做出的决策。</sample>
    <sample id="243">因此，人们可能会问到数据集和模型是否具有位置性。</sample>
    <sample id="244">we're not trying to say that models themselves and data sets themselves have demographic identities and life experiences but they do aggregate judgments and opinions of real people and can thus represent certain positionalities over others</sample>
    <sample id="245">所以之前的工作已经提出了一些基于经验的证据表明模型具有位置性，例如模型和数据集之间的文化差距以及模型位置性的理论定义。</sample>
    <sample id="246">然而，这些作品并没有比较最终用户与数据集和模型本身。</sample>
    <sample id="247">随着自然语言处理任务变得更加主观和社会化，研究模型和数据集的位置性越来越重要。</sample>
    <sample id="248">这是一个具有挑战性的过程，因为并非所有决策都有记录，许多模型隐藏在 api 之下。</sample>
    <sample id="249">为了研究数据集和模型的位置性，我们实际上将注释与现有数据集和模型进行比较。</sample>
    <sample id="250">我们通过我们的框架nl定位性来实现这一点。</sample>
    <sample id="251">我们的框架分为两个主要步骤。</sample>
    <sample id="252">第一步是用多种注释者重新注释数据集。</sample>
    <sample id="253">and we ought to do this over looking at the demographics of original datasets annotators because usually only a few annotators annotate each instance and because demographics are rarely collected and shared</sample>
    <sample id="254">因此我们选择重新注释数据以获取多个注释实例，并获取丰富的人口统计数据。</sample>
    <sample id="255">我们然后根据人口统计数据对注释进行分类，并使用皮尔逊相关系数将其与模型和数据集进行比较。</sample>
    <sample id="256">因此，我们的框架实际上与标注者不一致文献不同，是通过比较最终用户与模型和数据集的预测和标签，而不是仅仅关注标注者之间的一致性或标注者分布。</sample>
    <sample id="257">我们的框架主要得益于lab in the wild，这是一个由我们的hci合作者开发的在线众包平台。</sample>
    <sample id="258">live on the wild is an online experimentation platform where we can recruit diverse volunteers compared to platforms like mturk which largely have participants from the us or india and further lab in the wild still is able to get high quality data</sample>
    <sample id="259">我们在lab of the wild上举办了两个任务，其中一个是社会可接受性，这个任务的工作方式是，参与者会阅读social chemistry数据集中的一个情境，然后他们会写下这个情境的社会可接受性。</sample>
    <sample id="260">之后，为了保持对学习的兴趣，他们可以将自己的回答与人工智能和其他人的回答进行比较。</sample>
    <sample id="261">我们将这些注释与 social chemistry delphi 和 gbd 4 进行了比较。</sample>
    <sample id="262">然后我们复制了一个非常相似的设置来进行有毒性和仇恨言论检测任务，他们将阅读来自hatebase的实例，并写下他们认为这是一个仇恨言论的实例。</sample>
    <sample id="263">我们将这些注释与dynahate perspective api rewire api hate roberta和gpt-4进行了比较 our study in the end amassed over 16000 annotations from over 1000 annotators from 87 countries</sample>
    <sample id="264">现在我们更有能力回答谁的nlp数据集和模型与最多对齐？我们发现在nlp中存在位置性。</sample>
    <sample id="265">例如我们发现数据集和模型最适合英语国家，因此在gdp4社会可接受性分析中，我们发现它最适合儒家和英语国家。我们发现dynatehate也最适合英语国家。</sample>
    <sample id="266">我们也发现最多的额外对齐是与那些拥有大学教育的人，因此在社会接受性任务中，我们发现最多的对齐是与拥有大学教育或研究生教育的人。</sample>
    <sample id="267">我们发现对于jana hait的情况也是如此，它最符合那些拥有大学教育的人。</sample>
    <sample id="268">然而，当模型和数据集与特定人群对齐时，一些人必然会被遗漏。</sample>
    <sample id="269">一个例子是数据集和模型对非二元性别人群的对齐度比男性和女性的对齐度低。我们在gpt-4社会接受性任务以及dina-hate任务分析中都能看到这一点。</sample>
    <sample id="270">给定在led和lp中存在位置，我们能做什么？</sample>
    <sample id="271">所以我们有几个建议：第一个是“在整个研究过程中记录所有相关的设计选择”，第二个是“以透镜的视角进行nlp研究”。</sample>
    <sample id="272">我们的第三个建议是为四个特定的社区建立专门的数据集和模型 我想强调的是，包容性分析不仅仅是让所有技术都能为每个人工作。</sample>
    <sample id="273">这就是我们的演示结束，如果你想了解更多信息，请随时查看我们的仪表板以获取最新的分析结果和我们的论文，谢谢！</sample>
    <sample id="274">and what are the problems of the current simulcity models specific architectures are usually trained introducing additional modules to be optimized long and complicated training procedures for example training involving different optimization objectives and training and maintaining several models to reach different latency regimes for example training a model with an average of one second latency and another one with two seconds latency and so on</sample>
    <sample id="275">so if we do not sanitize political opinions in language model training data the bias would propagate from pre-training data to language models to downstream tasks ultimately creating fairness issues if we do try to sanitize somehow we would also risk censorship or exclusion and it's incredibly hard to determine what is actually neutral and should be retaining language modeling data so it's kind of like the electric sholly problem</sample>
    <sample id="276">hi i'm xuejun from fudan university i'm here to introduce our work distinguishing script knowledge from large language models for constrained language planning</sample>
    <sample id="277">在日常生活中，人们常常通过遵循逐步指示的脚本来计划他们的行动。</sample>
    <sample id="278">previous work has explored language models to plan for abstract goals of stereotypical activities such as make a cake and show that large language models can effectively decompose those into steps</sample>
    <sample id="279">然而，以往的工作主要集中在规划抽象目标的活动上规划具有具体目标的活动仍然是未被研究的领域</sample>
    <sample id="280">在这篇论文中，我们定义了受约束语言规划的问题。</sample>
    <sample id="281">这会对目标规划施加不同的约束条件。抽象目标可以被不同的实际目标所继承，具有多方面的约束。一个好的规划者应该编写符合约束的合理目标。</sample>
    <sample id="282">在这篇论文中，我们首先评估并改进了大型语言模型的受限语言规划能力。</sample>
    <sample id="283">由于没有特定的目标球员数据集来支持我们的起点。</sample>
    <sample id="284">we have to acquire this goals first and showing the table we extend the abstract goals with motivated constraints for human's look data acquisition using structure tpt</sample>
    <sample id="285">我们采样了一百个特定目标，并评估了从语言模型生成的脚本。</sample>
    <sample id="286">这个表格报告了结果的整体准确性； 我们发现所有语言模型在规划特定目标时都表现不佳。</sample>
    <sample id="287">然后，我们进行详细分析以调查哪些模型表现良好。</sample>
    <sample id="288">图表中的结果表明，生成的脚本的语义完整性是可接受的，但无法保证对约束的忠实性。</sample>
    <sample id="289">当我们深入到更细粒度的主题类别时，我们发现了维基汉的热图显示了这一点：不同类别的女孩在规划活动中的表现差异很大。</sample>
    <sample id="290">以前的研究表明，学习模型的输出质量在高方差下下降，导致性能下降。因此，我们采用了过度生成的z-filter来提高生成质量。</sample>
    <sample id="291">我们首先展示约束类型的例子，并从抽象目标中获得具体目标。</sample>
    <sample id="292">然后，指示gpg生成特定密钥对的密钥脚本。</sample>
    <sample id="293">接下来，开发一个过滤器模型来选择适合的脚本。</sample>
    <sample id="294">我们将脚本和目标转换为指令gbt嵌入，并计算余弦相似性和相似性得分来衡量语义相似性。</sample>
    <sample id="295">此外，我们将编写一个脚本，该脚本包含目标约束的关键词。我们只保留那个脚本，其中的关键词得分最高。</sample>
    <sample id="296">with our method influenceability can generate squares of higher quality our method greatly improves planability both in semantic completeness and effectiveness to the constraints</sample>
    <sample id="297">由于大型语言模型难以部署，因此启用小型和专用模型的语言规划能力至关重要。创建数据集是实现这一目标的关键步骤。</sample>
    <sample id="298">然而，以前的研究无法为特定目标进行规划，并且手动数据集标注是昂贵的。</sample>
    <sample id="299">因此，我们遵循符号知识蒸馏的想法，从大型语言模型中蒸馏出受约束的语言规划数据集。</sample>
    <sample id="300">我们将应用我们的构建受限语言规划数据集的方法，命名为coscript。</sample>
    <sample id="301">在总体上我们通过脚本生成了55000个特定目标以确保测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的质量验证和测试站点的</sample>
    <sample id="302">这个图表显示了codescript的约束分布，wavefin codescript显示了生成的特定目标的高多样性。</sample>
    <sample id="303">与其他模型相比，t5的翻译速度更快，可以生成比大多数较大的模型更高质量的文本，这表明在适当的训练数据集上，较小的模型可以超越较大的模型。</sample>
    <sample id="304">总结：我们建立了约束语言规划问题，评估了学习模型的约束语言规划能力，并开发了学习模型的生成过滤方法。</sample>
    <sample id="305">我们使用大型语言模型生成高质量的约束语言规划数据集，希望这些数据集能够成为推动语言规划研究的有价值资源。</sample>
    <sample id="306">谢谢您的时间，请在我们的论文中查找更多关于代码脚本的详细信息。</sample>
    <sample id="307">the insights that we gain from the human evaluation that we perform using the mqm framework is that the fluency of palm is comparable to state-of-the-art systems but the main difference comes from the accuracy</sample>
    <sample id="308">the watermark method need to meet the following properties first the method should be applicable to embedding as services second the watermark should not degrade the utility of the provided embedding third the watermark should be covert enough to the attacker or the attacker can remove the watermark easily finally the watermark need to be transferable to the attacker's services during the model extraction process</sample>
    <sample id="309">and we perform our analysis on transcripts of ted talks that have been translated from english to 14 different languages</sample>
    <sample id="310">our framework works in two main steps the first step is to re-annotate datasets with diverse annotators and we ought to do this over looking at the demographics of original datasets annotators because usually only a few annotators annotate each instance and because demographics are rarely collected and shared and so we ought to re-annotate data to get many annotates per instance and to get a rich set of demographic data we then take the annotations by demographic and compare them to the models and data sets using a pearson's r correlation score and thus our framework actually differs from annotator disagreement literature by comparing end users with models and data sets predictions and labels as opposed to looking at just annotator agreement or modeling annotator distributions</sample>
    <sample id="311">the cosine and l2 similarity between the requested embedding and the target embedding are computed we compute the similarity difference between benign and backdoor dataset which is defined as delta cosine and delta l2</sample>
    <sample id="312">and we also find many interesting results so regarding analyze of monolingual models we evaluate on two groups of models including encoder pdr which stands for multilingual pre-trained encoders with pointer-based decoders such as xlmr plus pdr and bert plus pdr and we also evaluate encoder decoder models which is multilingual pre-trained encoder decoder models such as mbert and mt5 we found that encoder decoder obtains the best performance on all nine data sets</sample>
    <sample id="344">before these main steps we first select a trigger set the trigger set is a group of words in a moderate frequency interval we assume the provider can collect a general text corpus and count the word frequency with it</sample>
    <sample id="345">hello everyone my name is xuhong today i'm going to present our paper do convolution 2003 named entity taggers still work well in 2023 let's get started</sample>
    <sample id="346">我们的论文调查了泛化问题，使用命名实体识别任务或ner任务。</sample>
    <sample id="347">我们观察到模型自2003年以来一直用于开发ar模型，这自然地引发了几个问题。首先，这些模型能否泛化到现代数据？</sample>
    <sample id="348">当我们开发新标签时，需要什么才能实现良好的泛化？</sample>
    <sample id="349">同时如果我们观察到过度泛化，这会导致这些模型的性能下降。</sample>
    <sample id="350">为了调查这些问题，我们开发了conll+数据集。这是一个我们从2020年的reuters新闻中收集的数据集，并用2003年的conll标注指南对其进行了标注。</sample>
    <sample id="351">我们在 kernel 2003 上对超过 20 个模型进行了微调，并在 kernel 03 测试集和 kernel plus plus 测试集上进行了评估。</sample>
    <sample id="352">最后但并非最少，我们计算了f1的百分比变化以评估每个模型的泛化能力。</sample>
    <sample id="353">所以为了良好的泛化，通过我们的实验，我们发现有三个主要的成分是必要的。</sample>
    <sample id="354">第一个是模型架构，通过我们的实验，我们发现transformer模型通常能更好地泛化到新数据。</sample>
    <sample id="355">第二个因素是模型大小我们发现通常较大的模型会导致更好的泛化。</sample>
    <sample id="356">最后但并非最重要的是我们都知道，微调样本的数量直接影响下游任务的性能。 这里我们也发现，更多的微调样本实际上也会导致更好的泛化。</sample>
    <sample id="357">我们的下一个问题是什么导致某些模型的性能下降</sample>
    <sample id="358">我们有两个假设，第一个是适应性过拟合，这是通过在新测试集上重复使用相同的测试集导致的过拟合。这通常表现为在新测试集上的性能下降。</sample>
    <sample id="359">第二个假设是温度漂移，这是由于训练数据和测试数据的温度差距不断扩大的性能下降。</sample>
    <sample id="360">对于适应性过拟合，我们从右图中看到，红色最佳拟合直线的梯度大于1。</sample>
    <sample id="361">this means that every unit of improvement that we made on call of 2003 translates to more than one unit improvement on call of plus plus which means that there is no diminishing returns</sample>
    <sample id="362">这表明在这种情况下没有观察到适应性过拟合。</sample>
    <sample id="363">那么温度有什么关系呢？</sample>
    <sample id="364">对于时间漂移，我们进行了一个实验来重新训练或继续预训练一些模型，使用更新的数据，我们发现随着时间间隔的增加，性能会下降。</sample>
    <sample id="365">this confirms our hypothesis that the main cause of the performance drop is temporal drift</sample>
    <sample id="366">我们的结论是，为了更好的泛化，我们需要更好的模型架构、更大的模型大小以及更多的微调示例，这些要素是相互关联的，我们不能只关注其中一个因素。</sample>
    <sample id="367">at the same time we also found that the performance drop here is caused by temporal drift and kind of surprisingly it is not caused by adaptive overfitting even though kernel 2003 has been used for over 20 years</sample>
    <sample id="368">so going back to the question that we raised in the title of our paper do carnegie 2003 tags still work in 2023 and we found that the answer is actually a resounding yes</sample>
    <sample id="369">我们希望我们的论文呼吁更多研究如何改进模型的泛化能力。</sample>
    <sample id="370">最后请确保查看我们的论文和数据集，如果您有任何问题，请随时与我联系。非常感谢！</sample>
    <sample id="397">so what is our solution according to the given english content</sample>
    <sample id="398">here is an example from our dataset servin is a judge kea is a baker servin and kea met at a park after a long day at work deciding cases in a law court he was happy to relax the task here is to identify the correct entity that the pronoun he refers to which in this case is servin the resolution of a given pronoun requires two types of information first entity specific knowledge such as servin is a judge and second background knowledge such as judges decide cases in law courts generally background knowledge is learned during the pre-training of large language models while entity specific knowledge is typically observed at inference time we vary the availability of these two pieces of information such that it may either be found in a single source or in multiple sources</sample>
    <sample id="399">the summary of our experimental results is that the example quality is more important than the similarity to the source sentence</sample>
    <sample id="400">we can also see that gpt-4 is the most liberal language model of them all and gpt theories are generally more socially liberal than bert theories and its variants secondly we aim to investigate to what extent the political biases of language models are actually picked up from training data so we conduct a controlled experiment by further pre-training language model checkpoints on six different parties and corpora separated into news and social media further divided into their political leanings</sample>
    <sample id="401">if you want to discover more results read our paper and we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work thanks for your attention</sample>
    <sample id="402">the most obvious thing is to use a direct reference for example by saying the name of the song is a me or its position the first</sample>
    <sample id="403">hi i'm sijun from fudan university i'm here to introduce our work distinguishing script knowledge from large language models for constrained language planning</sample>
    <sample id="404">hi i am yannis lavrac and i will present you our works on dr bert a robust pre-trained model in french for biomedical and clinical domain</sample>
    <sample id="405">and to better evaluate our benchmark we consider the six settings for training and evaluation the first one is translate test we use google translate api to translate source to the target language then use monolingual model to train and evaluation</sample>
    <sample id="406">so the marked words method draws upon the sociolinguistic concept of markedness which states that there is an unmarked default and any group that differs from that default is linguistically marked so for instance the word warrior is usually associated with men so when people are describing a warrior who is a woman they'll usually actually specify woman warrior and mark the term with woman</sample>
    <sample id="407">the first one is the model architecture through our experiments we found that the transformer models normally generalize better to new data</sample>
    <sample id="408">the right figure shows the performance difference between fine-tuning approaches which are directly applied on the clean data and wsl approaches which use the clean data for validation only</sample>
    <sample id="409">hello everyone i'm mark shatta and today my co-author martin and i are presenting our work the kit must have evaluating knowledge integration from multiple sources this work is a collaboration between mcgill university mila and microsoft research</sample>
    <sample id="410">therefore in this work we want to investigate whether instruction tuning on multimodal pre-trained models can actually improve generalization to unseen multimodal tasks</sample>
    <sample id="439">therefore successful models for knowledge-intensive nlu tasks require the ability to integrate and use both pre-trained time and inference time knowledge</sample>
    <sample id="440">hello everyone my name is ying and my colleague jian and i will be presenting our research on multi-instruct improving multimodal social learning via instruction tuning</sample>
    <sample id="441">in total we generate 55000 specific goals with scripts to ensure the quality of validation and the test sites we ask crowdsourced workers to find and revise the incorrect samples</sample>
    <sample id="442">and some people have suggested targeted evaluation on context-dependent translations but these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation</sample>
    <sample id="443">hi and i'm going to talk about our work on resolving indirect referring expressions for entity selection in which we introduce the altentities corpus</sample>
    <sample id="444">my name is jawad hosseini and this is a joint work with philippe radlinski silvia parotti and annie lewis</sample>
    <sample id="445">我们的目标是理解用户在想要做出选择时的语言。现在考虑这个替代问题：“你是指‘easy on me’还是‘i got a feeling’？” 这里，用户想要在这两个选项中做出选择。</sample>
    <sample id="446">最明显的做法是使用直接引用，例如通过说出歌曲的名字或它的位置。</sample>
    <sample id="447">但有时一个间接的引用更适合，以便进行更自然的对话。这可能发生在用户无法记住来源名称的情况下。</sample>
    <sample id="448">所有的发音都太相似了，很难区分。</sample>
    <sample id="449">或者当用户想要指定一个偏好时 here are some example indirect preferences for example the newer one or the song that's not energetic</sample>
    <sample id="450">这是一个重要的问题，对于对话系统和评估语言模型实体理解也是如此。</sample>
    <sample id="451">we're not aware of a public dataset a large-scale public dataset for the task so we collect one using crowd annotation our dataset covers three different domains music books and restaurants</sample>
    <sample id="452">我们的数据集收集方法强调非正式性，使用漫画完成任务。</sample>
    <sample id="453">卡通有三个语气气泡，在第一个气泡中鲍勃说：“记住我们昨天听的那首歌吧”与此同时，鲍勃设置了对话上下文。</sample>
    <sample id="454">在第二个语音泡泡中，爱丽丝说：“你在嘲笑我吗，还是我有感觉？”</sample>
    <sample id="455">which is the alternative question and in the third speech bubble bob uses an indirect reference to select one of these entities for example the newer</sample>
    <sample id="456">我们自动提供第一个和第二个语音泡泡，但第三个是由标注者填充的。第一个语音泡泡是从每个领域的几个手动提示中选择的。</sample>
    <sample id="457">第二个问题，即替代问题，是这样生成的。</sample>
    <sample id="458">我们总是使用一个简单的模板 do you mean a or b where a and b are samples from wikipedia</sample>
    <sample id="459">这里是我们使用的不同采样方法 when we move higher in the list the entities become more similar to each other and it's usually harder to make the disambiguation</sample>
    <sample id="460">第一个是 uniform attract 将英文翻译成中文。</sample>
    <sample id="461">第二种情况是当实体具有相似的标题，例如两本书，它们的名称相同。</sample>
    <sample id="462">第三个是当它们在维基百科上有相似的描述时；最后是当它们在维基百科上有相似的infoboxes或属性时。例如相同的类型或相同的艺术家。</sample>
    <sample id="463">当我们向用户展示这些替代问题时，他们知道这些实体的名称，但并不一定了解这些实体。</sample>
    <sample id="464">所以我们做的是展示两个实体的背景知识：对于歌曲，我们简单地为每个歌曲提供一个google搜索链接。</sample>
    <sample id="465">然后要求注释者至少听一下每首歌并阅读每首歌的英文翻译</sample>
    <sample id="466">for the recipes and books domain we show some background text from wikipedia for recipes we additionally show their images again from wikipedia so that the annotators know how they look like</sample>
    <sample id="467">然后我们要求注释者选择其中一个实体，例如这里第一个，并使用三到五个间接指代表达来描述它。</sample>
    <sample id="468">for example the one with the piano music here are some examples from our data set for example the one without words not the one with the 12-year-old boy or the fictional one or comes from other bojan and so on</sample>
    <sample id="469">the ltis corpus has 6000 alternative questions across three domains and it has 42000 indirect referring expressions results with t5x large model are summarized below</sample>
    <sample id="470">如果语言模型有与标注者相同的背景知识，那么准确率会非常高，大约在92%到95%之间，但这并不现实。</sample>
    <sample id="471">if the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model retrieves the background knowledge</sample>
    <sample id="472">if the language model has access only to entity names then the accuracy is only 60 so there's a lot of room for improvement we've also shown that the models are domain generalizable here is a link to our dataset</sample>
    <sample id="473">and we compare with popular strategies that are also applied to offline models that are the wait-k strategy and the local equipment and we compare also with the state-of-the-art architecture specifically tailored for simultaneous speech translation</sample>
    <sample id="474">hi i am yannis lavrac and i will present you our works on dr bert a robust pre-trained model in french for biomedical and clinical domain</sample>
    <sample id="475">hi everyone i'm jenny a first-year phd student at carnegie mellon university and today i'll be presenting our work on the positionality characterizing design biases of datasets and models this work was done in collaboration with some folks at the university of washington and the allen institute for ai namely sebastian santi roland labrosse katerina arinica and martin sapp</sample>
    <sample id="476">hi i'm myra and today i'll be talking about our paper marked personas using natural language prompts to measure stereotypes in language models this work is done in collaboration with esser and mush and dan juravsky</sample>
    <sample id="477">hi i'm sarah papi from the university of toronto and funded by bruno kessler and i will briefly introduce the attention as a guide for simultaneous speech translation paper that is a joint work with macdonald and marco d'orci</sample>
    <sample id="478">什么是实时语音翻译 实时语音翻译，或称simosti，是指实时将一种语言的口语翻译成另一种语言的文本，从而实现跨语言交流。</sample>
    <sample id="479">当前模拟城市模型的缺点是通常通过引入额外的模块来进行优化。</sample>
    <sample id="480">长而复杂的训练程序，例如涉及不同的优化目标。</sample>
    <sample id="481">训练和维护多个模型以达到不同的延迟要求例如训练一个平均延迟为1秒的模型和另一个平均延迟为2秒的模型等等</sample>
    <sample id="482">所以我们的解决方案是什么？</sample>
    <sample id="483">first to use already existing offline st models without retraining or adopting specific architecture for st use only one model for every latency regime and handle latency through specific parameters</sample>
    <sample id="484">利用模型已经通过注意力机制实现的音频输入和文本输出之间的知识，即交叉注意力机制，你可以在右侧看到一个例子。</sample>
    <sample id="485">我们的解决方案是提出注意力或编码器解码器注意力和它是一种策略，根据注意力指向的位置来决定是否要输出部分翻译。</sample>
    <sample id="486">如果张力不是集中化的，即其总和低于某个阈值α，那么最少λ个语音片段，意味着接收到的信息是不稳定的。</sample>
    <sample id="487">例如如果我们接收到一个包含“i am going to talk about”的语音片段，我们的模型预测出德语的翻译。</sample>
    <sample id="488">我们将查看跨注意力权重。</sample>
    <sample id="489">我们会看到，第一个词指向最早接收的语音帧，而最后一个词指向最后接收的语音帧和lambda语音帧。</sample>
    <sample id="490">这意味着前两个单词将被省略。</sample>
    <sample id="491">while since the sum of the cross-attention is above a certain threshold alpha we will not emit the last word and we wait for another speech chunk</sample>
    <sample id="492">如果我们继续，我们接收到另一个语音片段，我们的模型预测出三个词，我们将查看交叉注意力权重。</sample>
    <sample id="493">we will see that no words points to the last lambda speech frames</sample>
    <sample id="494">这意味着这三个单词将被省略。</sample>
    <sample id="495">如果我们看看那次活动的主要结果。</sample>
    <sample id="496">我们将同时翻译结果绘制在图表上，其中蓝色一侧测量翻译质量，另一侧测量评分。</sample>
    <sample id="497">that is the latency measure and we also consider the computational aware average lagging that accounts for the models computational times to predict the output</sample>
    <sample id="498">所以我们希望我们的曲线在这个图表上尽可能高。</sample>
    <sample id="499">但我们也希望它们被左移。</sample>
    <sample id="500">and we compare with popular strategies that are also applied to offline models that are the wait-k strategy and the local equipment and we compare also with the state-of-the-art architecture specifically tailored for simultaneous speech translation</sample>
    <sample id="501">这些是德语同时语音翻译策略的结果。</sample>
    <sample id="502">我们看到 adalout outperforms all the strategies applied to offline models since their curves are shifted to the left</sample>
    <sample id="503">我们还看到，如果我们考虑实际运行时间或计算工作时间，那是最快的策略。</sample>
    <sample id="504">if you want to discover more results read our paper and we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work thanks for your attention</sample>
    <sample id="505">and lastly please make sure to check out our paper our dataset and if you have any questions feel free to contact me thank you so much</sample>
    <sample id="506">hello everyone my name is ying and my colleague jian and i will be presenting our research on multi-instruct improving multimodal social learning via instruction tuning</sample>
    <sample id="507">随着大规模语言模型的进步，许多工作开始探索使用预训练语言模型进行不同下游任务的新学习范式，以参数和数据高效的方式。</sample>
    <sample id="508">最近，许多研究表明，指令微调使大型语言模型能够以零样本方式完成未见过的任务，仅仅是通过自然语言指令。</sample>
    <sample id="509">然而，大多数以往关于指令微调的工作主要集中在提高零样本任务的性能上，而计算机视觉和多模态任务则被忽视了。</sample>
    <sample id="510">因此在这项工作中，我们希望调查多模态预训练模型的指令调优是否能实际改善对未见多模态任务的泛化。</sample>
    <sample id="511">此外，在我们进行研究时，我们发现在lp和multimodal之间，指令数据集的可用性存在显著差异。</sample>
    <sample id="512">目前存在超过1600个纯语言指令任务，然而目前没有大规模的公开可用的多模态指令任务，因此这激励我们构建一个多模态指令调优数据集。</sample>
    <sample id="513">这里我们介绍multi-instruct，这是第一个多模态指令调优基准数据集，包含10个领域类别的62个多模态任务。</sample>
    <sample id="514">这些任务是从21个现有的开源数据集派生而来的，每个任务都配备了五个专家编写的说明。</sample>
    <sample id="515">在我们提出的多模态预训练数据集上进行多模态预训练调优时，我们选择了统一多模态预训练模型ofa作为基础模型。ofa使用统一的词汇、语言、图像标记和边界框的坐标。</sample>
    <sample id="516">这里我们展示了我们多实例数据集中的一些示例实例。</sample>
    <sample id="517">统一各种输入和输出数据的处理。</sample>
    <sample id="518">我们遵循 ofa 的方法，将所有任务统一为序列到序列格式，其中输入文本、图像、指令和边界框都在同一标记空间中表示。</sample>
    <sample id="519">好的，现在我要谈谈多模态指令调优。</sample>
    <sample id="520">so for the training data set we use 53 tasks from net group for training and we sample 10000 instances per task for testing we reserve the entire commonsense reasoning group for testing and we select additional five tasks from wiki and the miscellaneous group</sample>
    <sample id="521">我们使用测试集中的所有实例进行每个任务；此外，我们从自然语言指令的测试集中随机抽取20个任务进行单任务学习。</sample>
    <sample id="522">so we use a pre-trained ofa large model as the base model during training we mix all the instances for all the tasks each instance is randomly combined with one of its five instruction templates</sample>
    <sample id="523">so during test for each task we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment</sample>
    <sample id="524">我们将报告五个实验中性能的平均值、最大值以及性能的标准差。</sample>
    <sample id="525">if the task is a multimodal classification task we report accuracy if it's a multimodal generation task we report rouge l for an lp task we report rouge l as well</sample>
    <sample id="526">we also introduced a additional evaluation metric called sensitivity so this measures the model's ability to consistently produce the same outputs for the same task regardless of the slight variation in the wording of the instruction</sample>
    <sample id="527">here is our main result as we can see instruction tuning can significantly improve ofa's performance on same multimodal tasks</sample>
    <sample id="528">此外，从自然指令数据集中转移学习可以为指令调优带来好处。</sample>
    <sample id="529">here we can see as the amount of tasks increase the model achieve better performance and in the meantime lower sensitivity</sample>
    <sample id="530">so we also did one experiment we used one instruction versus five instruction as we can see using more instruction can improve the model's overall performance and reduce its sensitivity a lot</sample>
    <sample id="531">so this shows the effect of different fine tuning strategy on the model sensitivity as we can see by transfer learning from natural instruction data sets the model can achieve much better sensitivity compared to the original ifa model</sample>
    <sample id="532">我们也可以看到自然指令数据集的迁移学习可以帮助我们在自然指令数据集上取得更好的性能。</sample>
    <sample id="533">so overall we propose the first large scale multimodal instruction tuning dataset which significantly improves the zero shot capability of ofa and we explore different transfer learning technique and show their benefits we design a new metric called sensitivity</sample>
    <sample id="534">so one more thing we are collecting a much larger multimodal instruction tuning dataset with around 150 additional variant language tasks and we will release them so this is a qr code for our data and model thank you</sample>
    <sample id="535">hi i'm sarah papi from the university of toronto and funded by bruno kessler and i will briefly introduce the attention as a guide for simultaneous speech translation paper that is a joint work with macdonald and marco dorci based on the given english content</sample>
    <sample id="536">my name is jawad hosseini and this is a joint work with philippe radlinski silvia parotti and annie lewis</sample>
    <sample id="562">hi everyone i'm kostas katsanos and i'm pleased to welcome you to our talk of our acl 2023 paper language model acceptability judgments are not always robust to context</sample>
    <sample id="563">这是一个与约翰·戈奇、艾伦·穆勒、加尼什卡·米什拉、凯伦·弗兰蒂斯、罗杰·莱维和埃蒂娜·威廉姆斯的合作项目。</sample>
    <sample id="564">在这项工作中，我们重新审视了最小对准范式。</sample>
    <sample id="565">so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like plump syntax gem or acceptability in terms of stereotypes such as cross pairs</sample>
    <sample id="566">and in this minimal pair paradigm the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an unacceptable sentence or an ungrammatical sentence</sample>
    <sample id="567">然后希望模型基本上将更多的概率分配给可接受的句子。</sample>
    <sample id="568">当前的mpe管道基本上不允许我们评估模型对更长句子的接受程度。</sample>
    <sample id="569">这些日子，大型语言模型正在出现，它们的上下文窗口越来越长，因此非常重要的是评估模型在整个上下文窗口上的可接受性。</sample>
    <sample id="570">and that is what we are trying to do here we are trying to revisit the mpp pipeline by asking the model to evaluate acceptability on longer and longer sequences</sample>
    <sample id="571">so that is the approach so what we do is that to simulate these longer sequences we revisit the data sets themselves and then we recreate sentences by choosing like acceptable or unacceptable sentences from those data sets</sample>
    <sample id="572">so for example here we have chosen a typical pair of grammaticality from the blimp dataset from the adjective island case</sample>
    <sample id="573">and what we do is that to recreate like longer sequences and which are acceptable and which has the same matching of the grammatical structure we extract grammatical sentences from adjoint pilot</sample>
    <sample id="574">然后我们将其作为前缀添加到可接受查询和不可接受查询中。</sample>
    <sample id="575">so we can do the same thing by choosing unacceptable sentences from the same matching and that could also be used to test the model's acceptability</sample>
    <sample id="576">we can also do the same by choosing sentences from a different subset or a different dataset so that is what we call as the mismatch scenario</sample>
    <sample id="577">so here the sentences are still coming from relevant data sets but it's not from the same data set that you are evaluating with and we can do the same for unacceptability case</sample>
    <sample id="578">最后，我们可以从完全不相关的领域中选择句子，例如维基百科。</sample>
    <sample id="579">这将告诉我们模型的可接受性判断是否受到任何上下文的影响。</sample>
    <sample id="580">like whether the context is coming from a different subset of the data set or whether it's like completely irrelevant to the current like to the sentence that we are looking at</sample>
    <sample id="581">so how does the model do so first we look at the wikipedia sentences which are completely irrelevant to the current query pair and there we find that the mpp judgments are mostly robust for arbitrary context length</sample>
    <sample id="582">we increased the context length toward up to 2024 for to max out opt and gpt2 models and we saw here in the orange dotted line the mpp judgments are relatively stable</sample>
    <sample id="583">现在，当我们从同一个数据集中选择句子时，会发生什么？</sample>
    <sample id="584">所以这里我们选择或创建句子来源于可接受和不可接受的领域，来源于相同的blender语法数据集。</sample>
    <sample id="585">and there we see that the mpp judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes</sample>
    <sample id="586">但是当我们匹配结构时——也就是当我们从同一现象中选择句子时——</sample>
    <sample id="587">我们看到mpp判断模型的massive increase或massive decrease，取决于所选择的前缀是可接受的还是不可接受的。</sample>
    <sample id="588">now this and this is very large like this effect increases throughout the context length and this would probably affect like newer language models which has large context window</sample>
    <sample id="589">所以为什么匹配前缀会对语言模型的判断产生如此大的影响？</sample>
    <sample id="590">so we did a series of analysis where we tried to like perturb the input sentence by trying to preserve the relevant structure but adding like noise to the input and after doing like several of these perturbations</sample>
    <sample id="591">we find that none of these noises are actually making the model uh like change its course in terms of how it shows us the mpp judgment trend</sample>
    <sample id="592">基本上我们发现模型对句子的感知是以相似的方式敏感的。</sample>
    <sample id="593">that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion</sample>
    <sample id="594">因此，我们的工作的关键结论是语言模型对句子中的潜在语法和语义特征敏感，这些特征在句子之间共享。</sample>
    <sample id="595">和目前我们使用短句和单句输入的方式进行mpe评估可能无法充分捕捉语言模型在上下文窗口中的抽象知识。</sample>
    <sample id="596">请阅读我们的论文以获取我们的实验的更多细节，感谢您的耐心等待。</sample>
    <sample id="597">first we tag each input token with an unordered multiset of tokens that will appear in the output</sample>
    <sample id="598">in total we generate 55000 specific goals with scripts to ensure the quality of validation and the test sites we ask crowdsourced workers to find and revise the incorrect samples</sample>
    <sample id="626">at the end we concluded that the best automatic alignment method to use for german text simplification is the method of mass align and you can also find the code to run this method on your own documents in the paper</sample>
    <sample id="627">if we directly train neural networks on weakly labeled data the neural networks tend to memorize the label noise and do not generalize in weakly supervised learning training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well</sample>
    <sample id="628">you can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper</sample>
    <sample id="629">to investigate these problems we developed the conll++ dataset this is a dataset that we collected from reuters news from 2020 and then annotated them with the same conll 2003 annotation guidelines</sample>
    <sample id="630">hello everyone my name is yixin john from the penn state university today i'm going to present our work exemplar cross-lingual semantic parsing in multiple natural languages and many representations</sample>
    <sample id="631">语义解析是构建 sql 和 lambda 计算等用户查询的语义表示的任务。</sample>
    <sample id="632">跨语言语义解析是将查询翻译成多个意义表示的任务，涉及多种自然语言。</sample>
    <sample id="633">如图所示，我们需要将查询翻译成多种自然语言，使用神经模型，转化为 sql lambda 或 funql 等等。</sample>
    <sample id="634">现有的跨语言语义解析模型分别提出并在有限任务和应用场景的数据集上进行评估。</sample>
    <sample id="635">有许多关于某些自然语言的覆盖，但中文却缺失了。</sample>
    <sample id="636">克莱克斯对某些最小表示的覆盖。</sample>
    <sample id="637">lambda 计算是缺失的。</sample>
    <sample id="638">或者它们只在某些新闻模型上进行评估。例如，只有一个单一的模型来评估它们。</sample>
    <sample id="639">为了实现这一目标，我们提出了一个提供跨语言语义解析多个自然语言的统一数据集示例。</sample>
    <sample id="640">它包含九个数据集，涵盖各种领域，五十七个任务，八百万个表示，以及二十二种自然语言，涵盖十五个语言家族。</sample>
    <sample id="641">为了更好地评估我们的基准，我们考虑了训练和评估的六个设置。</sample>
    <sample id="642">第一个是 translate test 我们使用 google translate api 将源语言翻译成目标语言，然后使用单语言模型进行训练和评估。</sample>
    <sample id="643">and for example we train the english model on english query and during inference we translate the german query using api to english and then use the trained model to predict the sql</sample>
    <sample id="644">我们还将测试单语模块。</sample>
    <sample id="645">在这种情况下，源语言与目标语言相同，例如德语到德语或英语到英语。</sample>
    <sample id="646">我们还测试了多语言融合设置，通过训练只有10的训练数据来训练多语言模型。</sample>
    <sample id="647">这是一个多语言模型，我们训练了一个多语言模型，用于所有语言。</sample>
    <sample id="648">for example we put the german english chinese queries together to train a multilingual model and during inference we can use this model to translate english content to chinese</sample>
    <sample id="649">将英文内容翻译成中文。</sample>
    <sample id="650">我们还考虑了跨语言零短语和全短语转移，即从一种源语言训练并转移到另一种语言。</sample>
    <sample id="651">在训练过程中，我们训练的是英文查询或英文和德文查询的组合，以训练一个多语言模型并预测 sql 输出。</sample>
    <sample id="652">and we also find many interesting results so regarding analyze of monolingual models we evaluate on two groups of models</sample>
    <sample id="653">including encoder pdr which stands for multilingual pre-trained encoders with pointer-based decoders such as xlmr pdr and bert pdr</sample>
    <sample id="654">我们还评估了多语言预训练的编码器解码器模型，例如mbart和mt5。</sample>
    <sample id="655">我们发现编码器解码器在所有九个数据集上都取得了最佳性能。</sample>
    <sample id="656">我们在mt5和xlmr+bdr的多语言设置中进行评估。</sample>
    <sample id="657">如果没有这个编码器解码器或编码器pdr，可以通过在各种语言的混合中进行训练来改进。</sample>
    <sample id="658">我们发现这是因为大多数主要的自然语言可以获得性能提升，但英文在七个数据集中性能下降，只有在三个数据集中性能提升。</sample>
    <sample id="659">我认为这被称为多语言的曲线。</sample>
    <sample id="660">我们还比较了跨语言性能差距。</sample>
    <sample id="661">in this figure the blue line is cross lingual fusion transfer the orange line is cross lingual zero shot transfer while the green line is the monolingual setting</sample>
    <sample id="662">我们发现通过比较绿色和橙色线，我们发现在零秒设置下跨语言转录性能差距显著；通过比较蓝色和橙色线，我们发现在几秒设置下，转录差距迅速缩短。</sample>
    <sample id="663">我们还发现了一些其他有趣的发现，例如编码器解码器比以前的工作表现更好或取得了可比的结果，训练英语自然语言可以显著提高对其他目标自然语言的性能。</sample>
    <sample id="664">我们发现多语言语言模型，如codex和bloom，仍然在跨语言语义分类任务中表现不佳。</sample>
    <sample id="665">总结：我们构建了一个统一的跨语言语义解析基准，涵盖多种自然语言和多种表示形式。</sample>
    <sample id="666">we conduct a comprehensive benchmark study on three representative of types of multilingual language models and our results shows many interesting findings and etc and welcome to visit our paper and code thanks for listening</sample>
    <sample id="667">existing works can be broadly classified into four categories</sample>
    <sample id="668">we also find some other interesting findings for example encoder decoder outperforms previous work or achieved comparable results pre-training on english natural language can significantly boost the performance of future on-target natural languages and without multilingual language models such as coders and bloom are still inadequate for cross-lingual semantic tasks</sample>
    <sample id="695">in addition sometimes there are multiple permutations that are consistent with the data but the linguistically correct one is latent we address this by inducing the alignment as part of the training our permutation method is very flexible but it brings the challenge that finding the highest scoring permutation is np-hard that's because this is related to the traveling salesman problem we approximate this with a gpu friendly continuous relaxation that also allows us to back propagate through the solution and learn the linguistically more plausible permutations</sample>
    <sample id="696">for example if a right leaning language models were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform this would mean that people with opposite political opinions might be marginalized and the hate speech targeting minority groups might just run rampant without any control so this sounds the alarm for us to acknowledge and tackle the fairness issues resulted by language model political leanings</sample>
    <sample id="697">hi i am yannis lavrac and i will present you our works on dr bert a robust pre-trained model in french for biomedical and clinical domain</sample>
    <sample id="698">hi everyone i'm kostas of senna and i'm pleased to welcome you to our talk of our acl 2023 paper language model acceptability judgments are not always robust to context</sample>
    <sample id="699">hi i'm myra and today i'll be talking about our paper marked personas using natural language prompts to measure stereotypes in language models this work is done in collaboration with esendermush and dan juravsky</sample>
    <sample id="700">furthermore there's a lot of common tropes that are reflected in these words especially for women of color so for example the words describing latina women include things like vibrant and curvaceous which connect to a trope of tropicalism for asian women the words are things like petite and delicate and silky</sample>
    <sample id="701">first from mark groups the top words include things like culture tradition proud and exotic and these words define these groups only by their relationship to their identity and distinguish them as different from the white norm</sample>
    <sample id="702">in this work we extend cxmi to pointwise cxmi which can measure context usage at the sentence level or at the word level we can think of words that have high pxmi as ones that require context for translation</sample>
    <sample id="703">to answer this question we first train and compare four from scratch model a first version of dr bert with 7 gigabytes of nachos a second version of 4 gigabytes of set of nachos a first version of schubert which is a clinical model with 4 gigabytes of sentences taken from clinical notes and a final version of schubert with a mix of 4 gigabytes of set of nachos and 4 gigabytes of clinical notes</sample>
    <sample id="751">hello everyone my name is ying and my colleague jian and i will be presenting our research on multi-instruct improving multimodal social learning via instruction tuning</sample>
    <sample id="752">next we determine the best method to update a model with new data from each round of active learning and annotations cumulative accumulates all the data collected from active annotations so far whereas iterative updates the model by training on the latest set of data collected</sample>
    <sample id="753">our goal is to understand users' language when they want to make a choice now consider this alternative question did you mean easy on me or i got a feeling here a user wants to select between one of these two songs</sample>
    <sample id="754">we also validate the covertness of the provided embedding by visualizing the embedding of sentences unfolded as a bipca the legend of the figures means the number of triggers in each sentence</sample>
    <sample id="755">hi i'm sarah papi from the university of toronto and funded by bruno kessler and i will briefly introduce the attention as a guide for simultaneous speech translation paper that is a joint work with macdonald and marco d'orci based on english content</sample>
    <sample id="756">we transfer from two different tasks topic independent dissonance stance classification a task that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic</sample>
    <sample id="757">hi everyone i'm jenny a first-year phd student at carnegie mellon university and today i'll be presenting our work on the positionality characterizing design biases of data sets and models this work was done in collaboration with some folks at the university of washington and the allen institute for ai namely sebastian santi roland labrosse katarina rynika and martin sapp</sample>
    <sample id="758">so when the difference between the lengths of the two conjuncts grows the shorter conjunct prefers to be the first one stronger right so the proportion is bigger of the left short conjuncts but what's novel in this paper is that we observed that this tendency only occurs when the governor is on the left or absent right so the governor is on the left in this example isabel and lisa so if the governor is on the left</sample>
    <sample id="759">abc eval is capable of measuring the rates at which chat models will commit various thematic errors</sample>
    <sample id="760">these days large language models are coming up with longer and longer context windows so it's crucial that we evaluate the model's acceptability throughout the context window</sample>
    <sample id="761">and we found it is because most of the major natural languages can obtain performance gain except that english performance drops in seven data sets and only gains in three data sets i think this is known as curse of multilinguality</sample>
    <sample id="762">when we show this alternative question to the annotators they know the name of these entities but they don't necessarily know about the entity</sample>
    <sample id="763">it's the examples that carry most of the weight</sample>
    <sample id="764">the second ingredient is the model size we found that usually larger models lead to better generalization</sample>
    <sample id="765">so let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content you might turn towards a popular api like perspective api for toxicity detection and this works really well if you're carl jones where perspective api is able to detect correctly toxic instances but that's not really the case for adithya sharma where perspective api is really not as sensitive to offensive terms that are more common in indian contexts this is an example of a design bias where we see systematic performance differences of technology between populations</sample>
    <sample id="766">as shown in this figure we need to translate the query in multiple natural languages using neural models to sql lambda or funql and etc</sample>
    <sample id="767">we transfer from two different tasks topic independent dissonance stance classification a task that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic called debate here and on binary classification of expansion and comparison classes of pity be since these two are closely related to the conception of consonance and dissonance and we call them cee here we find that on transferring the zero-shot performance on the annotated dataset is already much better than chance with the best with auc 062 further on iteratively fine-tuning on both tasks we find that fine-tuning of cee tasks followed by further fine-tuning on debate yields a much better zero-shot performance thus this is the model that we use to cold start the active learning</sample>
    <sample id="768">we saw that the actual form of the prompting doesn't have a big influence in the case of several shared prompting</sample>
    <sample id="769">so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude</sample>
    <sample id="770">this figure shows the constraint distribution of codescript while the next one shows the hyper-polarity in the generated specific goals with codescript we can train smaller but specialized models for constraint language planning</sample>
    <sample id="771">hello everyone my name is xuhong today i'm going to present our paper do convolution 2003 named entity taggers still work well in 2023 let's get started</sample>
    <sample id="772">and we propose those results as a benchmark a base benchmark for the problem of automatic text simplification in the future</sample>
    <sample id="773">with that said tf-funchun and score rate can generate scripts of higher quality than most large language models indicating that smaller models can surpass larger models when properly trained on suitable data sets</sample>
    <sample id="774">for investigating multimodal instruction tuning on our proposed dataset we take ofa a unified multimodal pre-training model as our base model</sample>
    <sample id="833">hello everyone my name is alex villar and i will be giving a short overview of the paper brenting paraphrase translation assessing strategies and performance this is joint work with my colleagues from google translate</sample>
    <sample id="834">hello my name is vasudha and i'm a computer science phd candidate at stony brook university i would like to present our work accepted into acl 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge</sample>
    <sample id="835">we use state-of-the-art neural metrics and additionally also show expert-based human evaluation results finally we provide some recommendations for prompt selection strategies</sample>
    <sample id="836">hi i'm jean bean phd student at the university of washington today i'm presenting our work from pre-training data to language models to downstream tasks tracking the trails of political biases leading to unfair nlp models</sample>
    <sample id="837">we have fine-tuned two different models we have fine-tuned the model of long impart to produce document-level simplifications and we also fine-tuned the normal base long the normal base impart to produce sentence-level simplifications</sample>
    <sample id="838">so for the training data set we use 53 tasks from nat group for training and we sample 10000 instances per task uh for testing we reserve the entire commonsense reasoning group for testing and we select additional five tasks from wqa and the miscellaneous group we use all the instance in the test split for each task uh in addition we randomly sample 20 tasks from the test split of natural instruction as on sintask for lmp</sample>
    <sample id="839">hi welcome to our presentation of deplain a new corpus for german text simplification on the document level and on the sentence level my name is regina stodden and i will guide you through the first part of the presentation let's first define text simplification</sample>
    <sample id="840">we conduct experiments on four datasets agnews mind sst2 and erasmus we assume the provider applied wikitext dataset to count word frequencies</sample>
    <sample id="876">we introduced the first biomedical model in french named dr bert which is based on roberta and trained on nachos which is a dataset of medical ground data from the web</sample>
    <sample id="877">hello everyone my name is alex villar and i will be giving a short overview of the paper brenting paraphrasing translation assessing strategies and performance this is joint work with my colleagues from google translate</sample>
    <sample id="878">the prompting has a big influence on the performance of lms for translation as we can see in a simple experiment where we use one shot prompting and provided two different prompts for the same sentence</sample>
    <sample id="879">hello my name is kyo yen and i will be presenting our work titled when does translation require context a data-driven multilingual exploration this work was done in collaboration with patrick frenoux emily yu andrew ft martins and graham newbigg according to the given english content</sample>
    <sample id="880">so one more thing we are collecting a much larger multimodal instruction tuning dataset with around 150 additional variant language tasks and we will release them so this is a qr code for our data and model thank you</sample>
    <sample id="881">we introduce a co-reference resolution task designed to probe for the ability to draw on knowledge available in different sources we evaluate the dataset with human study participants and establish co-reference resolution models</sample>
    <sample id="882">hello everyone my name is alex villar and i will be giving a short overview of the paper brenting paraphrase translation assessing strategies and performance this is joint work with my colleagues from google translate</sample>
    <sample id="883">param is a 540 billion parameters large language model presented last year in 2022 it's trained on a large collection of text comprising 780 billion tokens</sample>
    <sample id="884">在大规模公开数据集上，它在数百个nlp任务中达到了最先进的水平。</sample>
    <sample id="885">在这项工作中，我们首次系统地研究了大语言模型提示法在机器翻译中的应用。</sample>
    <sample id="886">我们使用amt社区的最佳实践来评估这些模型的翻译能力，这包括使用最新的测试集以避免测试数据与训练数据的重叠。</sample>
    <sample id="887">我们比较了两个最先进的系统，即wmt评估中的最佳性能系统。</sample>
    <sample id="888">我们使用最先进的神经网络指标，并此外还展示专家基于的人工评估结果。最后，我们提供了一些提示选择策略的建议。</sample>
    <sample id="889">the prompting has a big influence on the performance of lms for translation as we can see in a simple experiment where we use one shot prompting and provided two different prompts for the same sentence</sample>
    <sample id="890">the majority of sentences 516 out of 1000 the difference observed is of more than one blur point</sample>
    <sample id="891">这可以在极端情况下达到40亿点数，因此选择一个好的提醒策略非常重要。</sample>
    <sample id="892">在我们的实验中，我们选择了五次提示策略，即我们为系统提供的每个句子都标记为英文。</sample>
    <sample id="893">so in this example here where we perform translation from german into english the german sentences the source sentences are marked with german column and the english translations with english column</sample>
    <sample id="894">我们看到3d打印的实际形式对3d打印的情况并没有很大影响。</sample>
    <sample id="895">it's crucial for zero and one shot prompting but when we go as in our case to five shot prompting there is nearly no difference to the actual form of the of the prompting</sample>
    <sample id="896">最重要的是例子，它们承担了大部分的重量。</sample>
    <sample id="897">我们的实验结果总结是，示例质量比源句的相似性更重要。</sample>
    <sample id="898">so it's important to select the examples from high quality translations in particular we compare the selecting prompts from the training data of the wmt evaluations or the dev data</sample>
    <sample id="899">dev data is much more curated and with higher quality than the training data and the results so better performance when using the dev data</sample>
    <sample id="900">尽管专业化的最先进系统有着显著的优势，但机器翻译也相当接近商业系统。在我们的案例中，我们选择与谷歌翻译进行评估。</sample>
    <sample id="901">我们使用mqm框架进行的语音识别的见解是，palm的流畅性与最先进的系统相当，但主要的区别来自于准确性。</sample>
    <sample id="902">特别是最常见的错误是遗漏错误。</sample>
    <sample id="903">so it seems that palm chooses to produce a better-sounding translation sometimes by dropping parts of the source sentence that are irrelevant in translation</sample>
    <sample id="904">然而，pantone的风格不协调类别比最先进的系统低，这是一个额外的信号。</sample>
    <sample id="905">parn 提供了非常流畅的输出，但仍然存在一些准确性问题。</sample>
    <sample id="906">and that's it for this really short overview for more details please come to the full presentation of the paper thank you very much</sample>
    <sample id="907">hello i am dawei a phd student at salant university in germany in this video i would like to present our recent work weaker than you think a critical look at weakly supervised learning</sample>
    <sample id="908">this is joint work with xiaoyu shen mario smooth path and georg steffen and dmitry shlyakhov</sample>
    <sample id="909">i'd like to begin with a brief introduction to weak supervision and weakly supervised learning</sample>
    <sample id="910">在弱监督中我们不手动标注数据 instead we label the data using weak labeling sources such as simple heuristic rules knowledge bases or low-quality crowdsourcing as illustrated in the figure on the right</sample>
    <sample id="911">与人工标注相比，弱标注是更便宜的，但它们也很嘈杂，这意味着其中一部分标注是不准确的。</sample>
    <sample id="912">如果我们直接在弱标签数据上训练神经网络，神经网络倾向于记忆标签噪声而不泛化。</sample>
    <sample id="913">在弱监督学习中，训练算法被提出用于在存在标签噪声的情况下鲁棒地训练神经网络，以便训练出的模型仍然能够很好地泛化。</sample>
    <sample id="914">最近的工作是wsl so wsl stands for weakly supervised learning a common claim is that people say that they only train models on the weakly labeled data and achieve high performance on clean test sets</sample>
    <sample id="915">从技术上讲，这个说法是正确的，但有一点需要注意。</sample>
    <sample id="916">这意味着人们会假设有一个额外的干净验证集可供模型选择。</sample>
    <sample id="917">我们可以适应这个问题设置，但这意味着需要额外的手动注释，这在弱监督学习中是必需的，但就像大象在房间里一样，这常常被忽视。</sample>
    <sample id="918">在上文中提到的任务是要问三个研究问题：第一，清洗验证数据是否必要，还是我们可以用噪声验证集代替吗？</sample>
    <sample id="919">第二，如果清洗数据是必需的，或者清洗数据是wssl工作的必要条件，那么我们需要多少清洗样本？最后，我们应该只用清洗样本进行验证，还是有更好的方法来利用它们？</sample>
    <sample id="920">在我们的工作中，我们解决了这些研究问题，我们的发现如下。</sample>
    <sample id="921">首先我们发现，有趣的是，最近的wsl方法确实需要干净的验证样本才能正常工作。</sample>
    <sample id="922">否则会出现大幅性能下降，如图中所示，如果没有清洗验证样本，则训练模型无法泛化到原始数据集之外。</sample>
    <sample id="923">这意味着训练是毫无意义的。</sample>
    <sample id="924">这表明wsl方法实际上需要清洁标记的数据才能正常工作，因此获取清洁验证样本的标注成本不应被忽视</sample>
    <sample id="925">我们的第二个发现是增加清洗验证样本的数量可以帮助wsl方法实现更好的性能，如左图所示。</sample>
    <sample id="926">通常，我们只需要20个样本即可达到高性能。</sample>
    <sample id="927">但这并不是故事的结局，因为无论我们选择哪种方式，直接在这些清洁样本上进行训练都会取得更好的性能。</sample>
    <sample id="928">红色图表显示了直接应用于干净数据和使用干净数据进行验证的wsl方法之间的性能差异。</sample>
    <sample id="929">从我们可以看到，如果我们有10个样本，直接微调开始比wsl方法更好。</sample>
    <sample id="930">最后，在之前的wsl方法中声称的性能提升可以通过允许在清洁的验证样本上继续微调来轻松实现。</sample>
    <sample id="931">从图表可以看出，我们称为 ftw 的模型最初表现不如更复杂的 wsl 方法如 cosine。</sample>
    <sample id="932">然而如果我们允许在清洁样本上继续微调，则ftw与其他方法表现同样好。</sample>
    <sample id="933">在实践中，没有理由选择更复杂的 wsl 方法，这些方法需要更多的计算时间和磁盘空间。</sample>
    <sample id="934">总结来说，我们证明了最近的wsl方法需要清洁的手动标注样本才能正常工作，它们的性能提升和实用性被大大过高估了。</sample>
    <sample id="935">我们对未来工作的具体建议如下：</sample>
    <sample id="936">首先报告模型选择标准，例如报告模型选择是否使用清洁的验证样本。</sample>
    <sample id="937">second wsl approaches should be compared with few-shot learning baselines as both work on clean samples third continuous fine-tuning is a simple yet strong baseline that should be considered in future work in wsl</sample>
    <sample id="938">最后我们开源了我们的代码你可以通过这个二维码在这个幻灯片上找到它感谢你和大家参加这次会议</sample>
    <sample id="939">the common practice is to use human evaluation such as by asking human judges to select which of two conversations is better or to rate conversations given a likert scale</sample>
    <sample id="940">hi everyone i'm jenny a first-year phd student at carnegie mellon university and today i'll be presenting our work on the positionality characterizing design biases of data sets and models this work was done in collaboration with some folks at the university of washington and the allen institute for ai namely sebastian santi roland labrosse katarina rynika and martin sapp</sample>
    <sample id="941">here is an example from our dataset servin is a judge kea is a baker servin and kea met at a park after a long day at work deciding cases in a law court he was happy to relax the task here is to identify the correct entity that the pronoun he refers to which in this case is servin the resolution of a given pronoun requires two types of information first entity specific knowledge such as servin is a judge and second background knowledge such as judges decide cases in law courts generally background knowledge is learned during the pre-training of large language models while entity specific knowledge is typically observed at inference time we vary the availability of these two pieces of information such that it may either be found in a single source or in multiple sources</sample>
    <sample id="942">still even the best performing models seem to have difficulties with reliably integrated background knowledge presented only at inference time if you're interested in more details please see our paper and check out the dataset and code on github thanks for listening</sample>
    <sample id="943">we also find most additional alignment with people who have a college education so for gpt-4 in the social acceptability task we find that it's most aligned to people with a college education or graduate school education</sample>
    <sample id="944">so why does the match prefix affect the language model judgment so much so we did a series of analysis where we tried to like perturb the input sentence by trying to preserve the relevant structure by adding like noise to the input and after doing like several of these perturbations we find that none of these noises are actually making the model like change its course in terms of how it shows us the mpp judgments basically we find that the models are sensitive to the perturbs and sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion</sample>
    <sample id="945">these approaches work well to provide holistic evaluations of overall dialogue quality but dialogue quality has many aspects therefore you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer grained level</sample>
    <sample id="946">hello everyone my name is jingwei yi from the university of science and technology of china it's my pleasure to give a short advertisement video of our paper i will copy my model protecting the copyright of large language models for embedding and services we will back the watermark</sample>
    <sample id="947">we saw that the actual form of the prompting doesn't have a big influence in in the case of several shot prompting it's crucial for zero and one shot prompting and when we go as in our case to five shot prompting there is nearly no difference to the actual form of the of the prompting</sample>
    <sample id="978">these reliable informative and distinct abc eval metrics enable us to evaluate conversational ai with a higher resolution than previous methods are able to achieve you can see that in the results of our experiment that several challenges still remain and have been precisely quantified for example the bots we tested have common sense violations in around 20 of their responses they produce irrelevant information in around 15 of the responses and they contradict themselves or their partner around 10 of the time with the rapid pace of improvement in the field many of these error rates could see a decrease in new models released since our evaluation was conducted however this is all the more reason to pursue reliable and precise evaluation metrics for comparing models we hope abc eval can be leveraged by others in the field as a meaningful step in this direction and we look forward to seeing how conversational ai will advance in the coming months and years thank you for watching</sample>
    <sample id="979">hello everyone my name is jingwei yi from the university of science and technology of china it's my pleasure to give a short advertisement video of our paper i will copy my model protecting the copyright of large language models for embedding and services we will back the watermark</sample>
    <sample id="980">which impose different constraints on the goal-oriented planning an abstract goal can be inherited by different real-life specific goals with multi-faceted constraints a good planner should write scripts that are reasonable and faithful to constraints</sample>
    <sample id="981">hi i'm sijun from fudan university i'm here to introduce our work distinguishing script knowledge from large language models for constrained language planning</sample>
    <sample id="982">hello my name is vasudha and i'm a computer science phd candidate at stony brook university i would like to present our work accepted into acl 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge</sample>
    <sample id="983">hi my name is adam sperkowski and this talk is about the dependency structure of coordination</sample>
    <sample id="1021">in particular the most common error are omission errors</sample>
    <sample id="1022">hello i'm james finch and i'm sarah finch and today we'll tell you all about abc eval a new dimensional approach to evaluating conversational ai</sample>
    <sample id="1023">这项工作由埃默里大学的埃默里nlp实验室主持，由吉诺·乔伊教授领导，并与亚马逊alexa ai合作完成。</sample>
    <sample id="1024">假设你刚刚开发了一个对话模型，并想了解它与当前最先进的模型相比的表现如何。</sample>
    <sample id="1025">the common practice is to use human evaluation such as by asking human judges to select which of two conversations is better or to rate conversations given a likert scale</sample>
    <sample id="1026">这些方法可以很好地提供对整体对话质量的全面评估，但对话质量有许多方面，因此您可能希望评估对话质量的多个维度，以便更深入地了解模型的优势和劣势。</sample>
    <sample id="1027">一种方法是简单地让人类法官评估对话质量的几个方面，例如模型响应的相关性，使用现有的比较或likert尺度方法。</sample>
    <sample id="1028">然而，我们相信有一种更精确和可靠的策略来评估多维对话。</sample>
    <sample id="1029">our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself</sample>
    <sample id="1030">我们称这种方法为“在聊天中注释行为”或简称为abc-eval。我们开发了这种方法，以全面覆盖近期文献中建议影响聊天质量的聊天模型行为。</sample>
    <sample id="1031">abc eval 能够测量聊天模型在提交各种主题性错误的速率。</sample>
    <sample id="1032">例如， apc-eval 测量了聊天模型忽略其对方或说出无关话题的次数。</sample>
    <sample id="1033">与其自身或其伙伴矛盾；幻觉错误的事实或违反常识知识；当模型成功或失败时表现出同情心。</sample>
    <sample id="1034">为了确定哪种评估方法最有效，我们选择了四种最先进的聊天机器人模型，并在每种模型上进行了100个人类与机器人对话的评估，使用abc评估方法。</sample>
    <sample id="1035">为了比较，我们还使用了三种现有的方法来评估这些对话：对话级别的likert评分、对话级别的likert评分以及对话级别的对比分析。</sample>
    <sample id="1036">对于现有的方法，我们收集了八个最常被测量的对话方面的评估，因为这是评估聊天模型的标准做法。</sample>
    <sample id="1037">从我们对这些评估结果的分析中发现，abc behavior labels 总体上比现有方法收集的标签更可靠，这可以通过 100 次双重标注的对话来衡量。</sample>
    <sample id="1038">此外 abc eval 标签在预测整体对话质量方面比现有方法产生的指标更具预测力，如此简单线性回归分析所示。</sample>
    <sample id="1039">for example you can see how measuring the proportion of turns with self and partner contradictions explains 5 and 10 of conversation quality respectively while the average liquor consistency scores explain only 4 or less</sample>
    <sample id="1040">最后，我们使用逐步线性回归来检查每个评估指标是否捕捉了聊天质量的独特方面。</sample>
    <sample id="1041">you can see how the combination of all abceval metrics explains over 25 of conversation quality and as you remove the metrics one at a time most of them result in losing a decent amount of information about the quality</sample>
    <sample id="1042">另一方面，所有层次的likert指标组合解释的质量要少得多，而且这些指标中较少的指标具有独特的信息。</sample>
    <sample id="1043">这些可靠、信息丰富且独特的abc-eval指标使我们能够以比以往方法更高的分辨率来评估对话式人工智能。</sample>
    <sample id="1044">you can see that in the results of our experiment that several challenges still remain and have been precisely quantified for example the bots we tested have common sense violations in around 20 of their responses</sample>
    <sample id="1045">他们在大约十五％的回答中提供无关信息，并且在十％的情况下与他们的伙伴相互矛盾。</sample>
    <sample id="1046">随着该领域改进的快速步伐，许多这些错误率可能会随着新型号的发布而减少。然而，这正是追求可靠且精确的评估指标来比较模型的理由。</sample>
    <sample id="1047">we hope abc eval can be leveraged by others in the field as a meaningful step in this direction and we look forward to seeing how conversational ai will advance in the coming months and years thank you for watching</sample>
    <sample id="1048">this work was done by the emory nlp lab led by professor gino choy at emory university and in collaboration with amazon alexa ai</sample>
    <sample id="1049">to summarize we showed that recent wsl approaches require clean manually annotated samples for them to work properly their performance gain and practicality are heavily overestimated our concrete recommendations for future work are as follows first report the model selection criteria for example report if the model selection is done with clean validation samples second wsl approaches should be compared with free short learning baselines as both work on clean samples third continuous fine-tuning is a simple yet strong baseline that should be considered in future work in ws finally we have open source our code you can find it via the qr code on this slide please feel free to check it out thank you and enjoy the conference</sample>
    <sample id="1050">根据所给的英文内容，这篇论文有多少位作者？</sample>
    <sample id="1051">hello my name is kyo yen and i will be presenting our work titled when does translation require context a data-driven multilingual exploration this work was done in collaboration with patrick frenoux emilie yu andrew ft martins and graham newbigg</sample>
    <sample id="1052">所以许多翻译依赖于上下文。例如，我们如何翻译这句话中的“mold”？</sample>
    <sample id="1053">well if the previous sentence was things could start to get dangerous if the ministers find out then mole refers to a spy but if the previous sentence was could it be anything serious doctor then mole refers to a birthmark</sample>
    <sample id="1054">根据上下文，单词的含义会改变，因此它的翻译也会改变。</sample>
    <sample id="1055">然而评估模型在处理这种情况的能力是非常困难的，首先因为只有少数翻译依赖于上下文，这使得基于语料库的指标无法捕捉这些翻译。</sample>
    <sample id="1056">有些人建议对上下文依赖翻译进行有针对性的评估，但这些资源只支持有限类型的上下文依赖翻译和有限的语言集，因为它们通常依赖于领域知识和人工创建。</sample>
    <sample id="1057">在这项工作中我们试图回答这两个问题：首先翻译需要上下文吗？其次模型如何处理这些情况？</sample>
    <sample id="1058">回答第一个问题，我们首先测量了词语在翻译中依赖于上下文的程度。</sample>
    <sample id="1059">在之前的工作中，我们引入了cxmi作为机器翻译模型的上下文使用度的度量。这通过测量给定源文本x给定目标文本y的上下文c提供的信息来实现。</sample>
    <sample id="1060">你可以把cxmi想象成是从给模型提供上下文中获得的信息。</sample>
    <sample id="1061">在这项工作中，我们将cxmi扩展到pointwisecxmi，它可以测量上下文的使用情况，可以是句子级别或单词级别。我们可以将那些具有高p6mi的词汇视为需要上下文才能翻译的词汇。</sample>
    <sample id="1062">现在我们使用高精度的xmi来分析这些单词，以查找这些单词之间的模式。</sample>
    <sample id="1063">我们对从英文翻译成14种语言的TED演讲的转录进行分析。</sample>
    <sample id="1064">我们在三个不同的层次上进行分析：首先，我们查看那些具有高均值psxmi的词语。</sample>
    <sample id="1065">and this allows us to find for example dual pronouns in arabic that have relatively high xmi and this can be explained because english doesn't have dual pronouns so you need context to determine if a pronoun is dual when translating into arabic</sample>
    <sample id="1066">同样，我们发现某些语言也需要上下文来选择适当的动词形式。 然后，我们查看那些在其所有出现中的平均 p6mi 高的词汇项。</sample>
    <sample id="1067">and this helps us identify cases like the one here where in chinese you need context to translate proper nouns to make sure that you're using the same translation within the document</sample>
    <sample id="1068">类似地，我们发现上下文是支持在正确的正式性中翻译的。</sample>
    <sample id="1069">最后，我们查看不同个别标记具有高p-sexmi，这使我们能够识别出不能仅仅通过单词本身来捕捉的现象，而是通过句法结构来表达的现象，例如省略的解决。</sample>
    <sample id="1070">因此，我们利用分析结果来设计文档级别翻译的基准标准。</sample>
    <sample id="1071">对于我们识别的五种讨论现象，我们创建了自动识别与该现象相关的词汇的标记器，我们称之为多语言讨论意识标记器或muda标记器。</sample>
    <sample id="1072">我们还可以注意到不同的语言有不同的这些语境现象的比例。</sample>
    <sample id="1073">我们首先使用muda标签器，将标签器应用于我们想要用于评估的并行语料库，然后将我们选择的翻译指标应用于muda标签器识别出的上下文依赖示例上。</sample>
    <sample id="1074">最后我们使用我们的基准以及其他指标来评估不同模型在文档级别机器翻译中的表现。</sample>
    <sample id="1075">首先，当我们使用语料库级别的指标时，我们发现上下文无关模型具有最佳性能。</sample>
    <sample id="1076">但是如果我们使用comet上下文无关模型表现最佳；如果我们使用wordf测量则上下文无关和有上下文的模型表现相当。</sample>
    <sample id="1077">这再次表明，如果仅使用语料库级别的度量指标，很难确定最佳的文档级别翻译系统。</sample>
    <sample id="1078">now we use the muda benchmark to evaluate models and we find that context-aware models are significantly more accurate than models that do not use context for certain discourse phenomena such as formality and lexical cohesion</sample>
    <sample id="1079">但是这些模型并没有比不使用上下文的模型好得多，尤其是在其他现象如省略号代词和动词形式方面。这表明我们需要看到更多的进展，以便在文档级别翻译中取得进展。</sample>
    <sample id="1080">我们还比较了不同的商业系统，我们的基准测试表明，deepbelief通常比google translate在文档级别翻译中更准确。</sample>
    <sample id="1081">总结：我们对14种语言对进行数据驱动分析，以确定何时需要翻译的上下文。</sample>
    <sample id="1082">然后我们利用我们的发现来构建文档级别机器翻译的基准，这可以帮助我们识别哪些语境现象模型可以很好地处理，哪些翻译系统在文档级别翻译方面表现良好。</sample>
    <sample id="1083">非常感谢您的关注，期待在多伦多见到你！</sample>
    <sample id="1084">hello everyone my name is yixin john from the penn state university today i'm going to present our work exemplar cross-lingual semantic parsing in multiple natural languages and many representations</sample>
    <sample id="1121">until every token from the first stage has been visited exactly once</sample>
    <sample id="1122">the second part is marked words which is a method to identify the words that distinguish marked groups are marked ones which i'll elaborate on shortly</sample>
    <sample id="1123">hi i'm john green phd student in university of washington today i'm presenting our work from pre-training data to language models to downstream tasks tracking the trails of political biases leading to unfair nlp models</sample>
    <sample id="1124">now there are also symmetric approaches to coordinate structures such as the plug approach the conjunction-headed approach assumed in plug-dependency tree banks where coordinate structures are headed by the conjunction</sample>
    <sample id="1125">hello i'm james finch and i'm sarah finch and today we'll tell you all about abc eval a new dimensional approach to evaluating conversational ai</sample>
    <sample id="1126">my name is jawad hosseini and this is a joint work with philippe radlinski silvia parotti and annie lewis</sample>
    <sample id="1127">so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like plump syntax gem or acceptability in terms of stereotypes such as cross pairs</sample>
    <sample id="1161">we addressed these research questions in our work and our findings are as follows first we find that interestingly recent wsl methods indeed require clean validation samples to work properly otherwise there is a large performance drop as shown in this figure if there are no clean validation samples then the trained models cannot generalize beyond the original weak labels meaning that the training is pointless this indicates that wsl approaches actually require cleanly labeled data to work properly and the annotation cost for obtaining clean validation samples should not be overlooked over second finding is that increasing the number of clean validation samples will help wsl approaches to achieve better performance as shown in the figure on the left typically we only need 20 samples per class to attain high performance</sample>
    <sample id="1162">we also introduce a comparison of model with multiple pretraining settings and data sources then we present our results on 11 biomedical and clinical downstream tasks in french</sample>
    <sample id="1226">however our experiment on continual pretraining using the weight and tokenizer of permutebert trained on the 4 gb subset of natural showed comparable results to those obtained with dr bert 4 gb from scratch</sample>
    <sample id="1227">hi my name is adam sperkowski and this talk is about the dependency structure of coordination</sample>
    <sample id="1228">for temporal drift we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift</sample>
    <sample id="1269">after the first step we have all the right tokens but they're not ordered that's why in the second step we use another model to predict a permutation to put them into the right order</sample>
    <sample id="1270">and finally there should really be increased transparency about bias mitigation methods because for instance like these positive stereotypes we don't know if it's because there is some sort of like weird overly excessive value alignment going on or maybe some other like anti-stereotyping methods that are resulting in these pernicious patterns</sample>
    <sample id="1271">and in this minimal pair paradigm the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an unacceptable sentence or an ungrammatical sentence and then the hope is that the model basically puts more probability to the acceptable sentence</sample>
    <sample id="1272">however our experiment on continuous pretraining using the weight and tokenizer of permute-bert trained on the 4 gb subset of natural showed comparable results to those obtained with dr bert 4 gb from scratch</sample>
    <sample id="1273">from our analyses of these evaluation results we found that abc behavior labels are overall more reliable than labels collected by existing methods as measured by inner annotator agreement on 100 doubly labeled conversations in addition abc eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods as shown by this simple linear regression analysis</sample>
    <sample id="1274">finally we can choose sentences from a completely unrelated domain such as wikipedia so this will tell us like whether the model's acceptability judgments are actually impacted by any context</sample>
    <sample id="1275">hi welcome to our presentation of deep plain a new corpus for german text simplification on the document level and on the sentence level my name is regina stodden and i will guide you through the first part of the presentation let's first define text simplification</sample>
    <sample id="1276">however most previous works on instruction tuning focus on improving the zero-shot performance on language-only tasks while computer vision and multimodal tasks have been left out therefore in this work we want to investigate whether instruction tuning on multimodal pre-trained models can actually improve generalization to unseen multimodal tasks additionally at the time of our research we discovered a considerable discrepancy in availability of instruction data set between lp and multimodal there exists more than 1600 language-only instruction tasks however there is no large scale publicly available multimodal instruction tasks therefore this motivates us to build a multimodal instruction tuning data set</sample>
    <sample id="1277">hello i'm james finch and i'm sarah finch and today we'll tell you all about abc eval a new dimensional approach to evaluating conversational ai this work was done by the emory nlp lab led by professor gino choi at emory university and in collaboration with amazon alexa ai</sample>
    <sample id="1278">so we showed that by measuring length in characters that's the first column in syllables the middle column and in words the right column so i'll concentrate on the right one</sample>
    <sample id="1279">so really just only the positive or at least non-negative ones so really just only the positive or at least non-negative ones so really just only the positive or at least non-negative ones so really just only the positive or at least non-negative ones so really just only the positive or at least non-negative ones so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-negative so really just only the positive or at least non-</sample>
    <sample id="1280">with that said t5 outperformed unsupervised read can generate scripts of higher quality than most large language models indicating that smaller models can surpass larger models when properly trained on suitable data sets</sample>
    <sample id="1281">hello i am yannis lavrac and i will present you our works on dr bert a robust pre-trained model in french for biomedical and clinical domains</sample>
    <sample id="1282">在这次演示中，我们首先将讨论医疗保健中的语言建模，然后将介绍我们文章的主要贡献。</sample>
    <sample id="1283">we introduced the first biomedical model uh in french named dr bert which is based on roberta and trained on natasha which is a dataset of medical ground data from the web</sample>
    <sample id="1284">we also introduce a comparison of model with multiple pretraining settings and data sources then we present our results on 11 biomedical and clinical downstream tasks in french</sample>
    <sample id="1285">最后我们总结了实验结果并提供更多关于如何访问模型的详细信息</sample>
    <sample id="1286">自2018年发布以来，bert已成为解决自然语言处理任务的最有效方法之一，并与历史上的静态和上下文化方法如word2vec fasttext或enwiki相比取得了巨大的性能提升。</sample>
    <sample id="1287">since then this model has been adapted to many other languages like in french with camambert and also in a domain like biomedical with permit birth and biobirth and on clinical with clinical birth but mostly in english</sample>
    <sample id="1288">针对其他语言的专用模型稀缺，通常是由于缺乏领域数据而基于连续预训练的。</sample>
    <sample id="1289">然而，法语没有任何开源模型来模仿美国英语。</sample>
    <sample id="1290">we so we ask ourselves question about what is the most appropriate data source for a wide range of usage and those raw data are a good substitution for clinical data</sample>
    <sample id="1291">为了回答这个问题，我们将dr bert与我们基于匿名数据获得的schubert模型进行比较。</sample>
    <sample id="1292">之后我们会问自己：我们需要多少数据来训练一个专业化的模型，是4gb 8gb还是更多？</sample>
    <sample id="1293">为了回答这个问题，我们首先训练并比较了四个从头开始的模型，第一个版本是使用七个gigabyte的nachos，第二个版本是使用四个gigabyte的nachos。</sample>
    <sample id="1294">a first version of schubert which is a clinical uh model with 4 gigabytes of sentences taken from clinical notes and a final version of schubert with a mix of 4 gigabytes of natural sentences and 4 gigabytes of clinical notes</sample>
    <sample id="1295">除此之外，我们还引入了三种基于连续预训练的训练模型，以分析预训练策略的影响。</sample>
    <sample id="1296">一个基于kammbert的模型，训练在4 gigabytes的nachos数据集上；另一个也基于kammbert，但这次训练在4 gigabytes的clinical notes上。</sample>
    <sample id="1297">and finally one based on english biomedical model bert and trained on 4 gigabytes of set of snapshots in total we have seven models</sample>
    <sample id="1298">为了评估我们的七个模型，我们收集了包括公共和私人数据集，例如语音识别分类语音转录和问答任务。</sample>
    <sample id="1299">this model are compared to six baseline model which are camber oscar 138 gigabytes camber oscar 4 gigabytes camber cisnet 4 gigabytes petabyte myobert and clinicalbert</sample>
    <sample id="1300">the evolution of highlights that model perform best on the task with data of the same nature as those on which the model has been trained</sample>
    <sample id="1301">然而我们可以从多个来源观察到数据，我们也观察到使用更多数据可以带来更好的表现。</sample>
    <sample id="1302">总体而言，从头开始训练的模型在大多数任务上表现得更好。</sample>
    <sample id="1303">however our experiment on continuous pretraining using the weight and tokenizer of permutebert trained on the 4 gigabyte subset of natural showed comparable results to those obtained with doctorbert 4 gigabytes from scratch</sample>
    <sample id="1304">这与基于卡门贝尔权重和分词器的模型不同，后者存在稳定性问题。</sample>
    <sample id="1305">finally as a conclusion uh uh our proprietary system offer better performance on nine of the 11 downstreams tasks and surpass globally the result of the generic model here camomile</sample>
    <sample id="1306">我们也观察到，专业化的数据更好，更专业化的数据更好，但它并不很好地扩展。</sample>
    <sample id="1307">所有的预训练模型都是从nachos获得的，免费可用，并且在yugenface上，所有的训练脚本都在我们的github仓库中。</sample>
    <sample id="1308">so thank you for for for this presentation and we are looking forward to actions at the poster session in 2022</sample>
    <sample id="1309">to answer this question we first train and compare four from scratch model a first version of dr bert with 7 gigabytes of natchez a second version of 4 gigabytes of set of natchez a first version of schubert which is a clinical uh model with 4 gigabytes of sentences taken from clinical notes and a final version of schubert with a mix of 4 gigabytes of set of natchez and 4 gigabytes of clinical notes in addition to this comparison we introduced three model trained on continuous pre-training to analyze the impact of pre-training strategy</sample>
    <sample id="1310">for adaptive overfitting we saw that from the graph on the right the red best fit line has a gradient that is greater than one this means that every unit of improvement that we made on colonel 2003 translates to more than one unit improvement on colonel which means that there is no diminishing returns and this shows us that adaptive overfitting in this case is not observed</sample>
    <sample id="1311">the second use case that we showed in our paper is the case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text we have fine-tuned two different models we have fine-tuned the model of long impart to produce document-level simplifications and we also fine-tuned the normal base long the normal base impart to produce sentence-level simplifications you can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper we concluded that this basic fine-tuning could produce or could get scores better than the baseline scores and we propose those results as a benchmark a baseline benchmark for the problem of automatic text simplification in the future</sample>
    <sample id="1312">so some preliminary results demonstrate that first language models do have varying political leanings they occupy all four quadrants on the political compass we can also see that gpt-4 is the most liberal language model of them all and gpt-3 is generally more socially liberal than bird theory and its variants</sample>
    <sample id="1313">hi my name is mathias lendemann and today i'm going to give you a brief introduction to our paper on compositional generalization without trees using multiset tagging and latent permutations</sample>
    <sample id="1314">这是与我的导师亚历山大·科拉和伊万·蒂托合作的工作。</sample>
    <sample id="1315">构成泛化可以被理解为学习者处理更深层次的递归和未见过的构成或短语的能力，这些短语在训练过程中是单独出现的。</sample>
    <sample id="1316">在语义解析的背景下，测试组合化泛化可能如下：如常规，我们有一组训练句子。在这个例子中，句子是“the girl slept”和“mary knew that the girl slept”。</sample>
    <sample id="1317">这些发言与逻辑形式相配对，代表它们意义的核心方面。</sample>
    <sample id="1318">与标准机器学习评估相比，测试集不来自同一分布，而包含结构上未见过的逻辑形式。</sample>
    <sample id="1319">在这个例子中，模型在训练过程中经历了浅层递归，并在具有更深层次递归的示例上进行测试。</sample>
    <sample id="1320">原始的序列到序列模型在这种分布外泛化方面遇到困难，常常产生与输入无关的输出。</sample>
    <sample id="1321">特别是，它们常常无法复制输入和输出之间的系统性对应关系，就像示例中的颜色编码一样。</sample>
    <sample id="1322">一种流行的方法是将树整合到模型中。</sample>
    <sample id="1323">这些树是为了捕捉与逻辑形式相关的表达过程而设计的。</sample>
    <sample id="1324">这很好，但树通常不会给出，而是需要通过某种方式获得。</sample>
    <sample id="1325">这可能是一个复杂且有时计算成本较高的过程，通常涉及对逻辑形式的大量特定预处理，例如处理变量符号。</sample>
    <sample id="1326">获取树也可能涉及专门的语法归纳程序。</sample>
    <sample id="1327">在这篇论文中，我们不使用树，而是引入一种直接建模输入片段与输出片段对应关系的神经序列到序列模型。</sample>
    <sample id="1328">这是第一次展示强化泛化到更深层递归，而不依赖于树。</sample>
    <sample id="1329">我们的方法通过两个步骤预测输入的输出。</sample>
    <sample id="1330">首先，我们为每个输入标记添加一个无序多集标记，这些标记将出现在输出中。</sample>
    <sample id="1331">在第一步之后，我们拥有所有正确的标记，但它们并没有排序。</sample>
    <sample id="1332">这就是为什么在第二步中我们使用另一个模型来预测一个排列来将它们排列成正确的顺序。</sample>
    <sample id="1333">我们引入了一种新的方法来预测排列，该方法对可能的排列不会施加任何严格的约束条件，这使我们的方法非常灵活和表达力强大。</sample>
    <sample id="1334">从概念上讲，我们的排列模型大致是这样工作的。</sample>
    <sample id="1335">我们从左到右遍历输出，并确定在每个位置放置哪个多集标记。对于第一个输出位置，我们简单地选择“one”作为红色高亮显示。</sample>
    <sample id="1336">然后我们跳到下一个多集标记，以确定输出中的第二个标记。</sample>
    <sample id="1337">我们以类似的方式确定输出中的第三个标记，通过跳转到另一个多集标记。我们继续这个过程。</sample>
    <sample id="1338">直到第一阶段中的每个标记都被访问过一次。</sample>
    <sample id="1339">为了给您一个实验结果的预告，我们将比较我们的方法与其他树无模型在 cogs 基准测试中的表现：我们的模型在泛化到更深的递归方面优于其他模型。</sample>
    <sample id="1340">一些其他类型的结构化推广仍然非常具有挑战性。</sample>
    <sample id="1341">在我们的论文中，我们解决了几个有趣的技术挑战。</sample>
    <sample id="1342">首先，输入和输出之间的对齐不是在训练数据中给出的，因此对于一个给定的标记，我们不知道它来自哪个多元集。这对训练构成了挑战。</sample>
    <sample id="1343">此外有时存在多个排列与数据一致，但语言上正确的排列是潜在的。我们通过将对齐作为训练的一部分来解决这个问题。</sample>
    <sample id="1344">我们的排列方法非常灵活，但它带来了挑战：找到最高分排列是np难问题。这是因为这与旅行商问题有关。</sample>
    <sample id="1345">我们使用一种适合gpu的连续放松方法来近似化，并允许我们通过解决方案进行反向传播，学习语言上更合理的排列。</sample>
    <sample id="1346">如果你想了解我们实验的更多细节以及我们如何解决这些挑战，请查看我们的论文或参观我们的海报。</sample>
    <sample id="1347">we begin by defining cognitive dissonance and why it is an important problem to study in language simply put cognitive dissonance is two beliefs or actions that are inconsistent</sample>
    <sample id="1348">we can also see that gpt-4 is the most liberal language model of them all and gpt theories are generally more socially liberal than bert theories and its variants</sample>
    <sample id="1349">over the different strategies we found that cumulative performed equal or better than iterative across the board</sample>
    <sample id="1350">hi i'm sarah papi from the university of toronto and funded by bruno kessler and i will briefly introduce the attention as a guide for simultaneous speech translation paper that is a joint work with macdonald and marco dorci based on the given english content</sample>
    <sample id="1351">and we perform our analysis on transcripts of ted talks that have been translated from english to 14 different languages</sample>
    <sample id="1385">hi my name is mathias lendeman and today i'm going to give you a brief introduction to our paper on compositional generalization without trees using multiset tagging and latent permutations</sample>
    <sample id="1386">and we also consider cross-lingual zero-shot and few-shot transfer we train on one source language and transfer to another language so during training we'll train it on english query or the combination of english and german few-shot queries to train a multilingual model to and predict the sql output</sample>
    <sample id="1387">hello i am dawei a phd student at salant university in germany in this video i would like to present our recent work weaker than you think a critical look at weakly supervised learning this is joint work with xiaoyu shen marios mouzakis and geoffrey stephens</sample>
    <sample id="1388">we plot the simultaneous speech translation results on graphs in which we have blue on one side that measures the translation quality and average lagging that is the latency measure and we also consider the computational aware average lagging that accounts for the model's computational time to predict the output</sample>
    <sample id="1389">hello everyone i'm mark shatta and today my co-author martin and i are presenting our work the kid must test evaluating knowledge integration from multiple sources this work is a collaboration between mcgill university niela and microsoft research</sample>
    <sample id="1390">自然语言理解模型依赖于多种知识来源，例如在其参数中包含的知识通常通过预训练获得，以及在推理时提供的输入信息。</sample>
    <sample id="1391">最近的工作在任务如问答中表明，模型可以利用预训练的时间知识来解决任务。</sample>
    <sample id="1392">然而，自然语言理解通常需要在推理时提供的知识。</sample>
    <sample id="1393">例如，在句子中，约翰在电视上看到了新当选的总统。</sample>
    <sample id="1394">预训练参数可以包含关于什么是总统和什么是总理的信息，但它们无法可靠地知道这个实例特定的实体john是谁，或者新总统是谁，因为总统可能会在预训练期间发生变化。</sample>
    <sample id="1395">因此，成功的知识密集型nlu任务需要能够集成和使用预训练时间和推理时间的知识。</sample>
    <sample id="1396">在这项工作中，我们提出了一套用于知识整合的诊断测试套件。</sample>
    <sample id="1397">我们引入了一个共引用分辨任务，旨在探测参与者能够从不同来源获取知识的能力。我们用人类研究参与者评估了数据集，并建立了共引用分辨模型。</sample>
    <sample id="1398">here is an example from our dataset servin is a judge pierre is a baker servin and pierre met at a park after a long day at work deciding cases in a law court he was happy to relax</sample>
    <sample id="1399">任务是识别代词“他”所指的正确实体，在这个例子中是七</sample>
    <sample id="1400">解决一个给定的代词需要两种类型的信息：第一，实体特定的知识，例如serville是一名法官；第二，背景知识，例如法官决定法院案件。</sample>
    <sample id="1401">一般来说背景知识是在大规模语言模型的预训练过程中学习的，而实体特定的知识通常是在推理时观察到的。</sample>
    <sample id="1402">我们调整这两个信息的可用性，使其可以在单一来源或多个来源中找到。</sample>
    <sample id="1403">我们已经定义了三个设置：首先我们有两个基本设置： background pre-train 和 background knowledge is assumed to be available at free train time。</sample>
    <sample id="1404">第二，有后台知识设置，其中后台知识在预训练时和推理时都可用；最后，后台推理设置，其中两种知识类型仅在推理时可用。</sample>
    <sample id="1405">这个最后的设置尤其有趣，因为它模拟了背景知识在解决任务时不是模型预训练数据的一部分的情况。例如，因为自预训练以来新职业已经发展出来。</sample>
    <sample id="1406">这是我们如何控制事实和真相来源的例子。</sample>
    <sample id="1407">在背景预训练设置中，我们假设背景知识政治人物争夺政府席位的知识已包含在预训练参数中。在三句话上下文中，我们提供反特定知识“chichester是一名政治家”。</sample>
    <sample id="1408">在背景知识设置中，我们不仅提供实体特定的知识，还提供关于上下文中的实体的背景知识。</sample>
    <sample id="1409">在背景和场景设置中，我们提供了虚构的职业“meritour”代替“politician”，因为“meritour”在预训练时期更不太可能被包含在内。</sample>
    <sample id="1410">我们对数据集进行了人类研究参与者的评估，并建立了高分辨率模型。 在这个图表中，我们展示了最佳性能模型在最具挑战性的背景预训练设置中的结果。</sample>
    <sample id="1411">在没有任务特定的训练数据时，这两个模型都表现不佳。然而，在使用kidmos进行训练时，c2f和bert4coref的表现显著优于随机选择。</sample>
    <sample id="1412">这表明在训练于具有遗传参考分辨率数据集的模型时，它们学会了利用表面线索，这些线索在测试时被移除。</sample>
    <sample id="1413">关于虚构知识的额外实验表明，即使是表现最佳的模型，也无法可靠地整合在推理时提供的背景知识。</sample>
    <sample id="1414">总结我们的论文的主要结论是：许多跨语言翻译模型在没有任务特定训练的情况下无法整合来自多个来源的知识。然而，通过任务特定训练，一些模型成功地整合了来自多个来源的知识。</sample>
    <sample id="1415">即使是表现最好的模型也似乎难以可靠地整合在推断时呈现的背景知识。 如果您对更多细节感兴趣，请参阅我们的论文并查看 github 上的数据集和代码。 谢谢您的关注。</sample>
    <sample id="1416">this works well but trees are usually not given and need to be obtained somehow this can be complicated and sometimes a computationally expensive process typically this involves considerable formalism-specific pre-processing of the logical forms for example to handle variable symbols obtaining trees may also involve specialized grammar induction procedures</sample>
    <sample id="1417">hello everyone my name is xuhong today i'm going to present our paper do convolution 2003 named entity taggers still work well in 2023 let's get started based on the given english content answer the following questions</sample>
    <sample id="1418">hi i'm myra and today i'll be talking about our paper marked personas using natural language prompts to measure stereotypes in language models this work is done in collaboration with esendermush and danjurovski</sample>
    <sample id="1419">近年来，许多人已经记录了大型语言模型中社会偏见和刻板印象的普遍性。</sample>
    <sample id="1420">然而，这些措施有各种限制。它们通常依赖于人工构建的数据集，这些数据集需要花费大量时间来整理。</sample>
    <sample id="1421">and they also usually only measure very specific stereotypes meaning that they don't generalize well to other demographics or contexts or they simply capture very general broad associations like negative associations with particular groups</sample>
    <sample id="1422">此外，大多数在这个领域的工作都没有考虑到交叉性，即多重社会身份可以加剧偏见并成为独特的伤害点。</sample>
    <sample id="1423">为了克服这些限制，我们依赖于这些较新的指令调校语言模型在响应指令和提示方面的能力。</sample>
    <sample id="1424">so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself</sample>
    <sample id="1425">我们可以立即看到，这可以很容易地推广到任何人口统计群体，因为我们可以将任何我们想要的身份标记指定到这个提示中。</sample>
    <sample id="1426">so here are some example generations from gpt-4</sample>
    <sample id="1427">我们立即看到，虽然这些输出在传统意义上并不是明显的负面或有毒的。</sample>
    <sample id="1428">有一些有趣的模式。</sample>
    <sample id="1429">asian woman is depicted as unassuming the middle eastern woman is referred to using words like exotic and like referring to a mesmerizing region</sample>
    <sample id="1430">这两个有色人种角色都提到了祖先，而白人角色则没有这样的提及。</sample>
    <sample id="1431">为了捕捉这些模式，我们的方法有两个部分。第一个是生成这些角色。</sample>
    <sample id="1432">our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects finding that by giving it to human subjects they also were able to surface racial stereotypes</sample>
    <sample id="1433">此外，这也使我们能够直接比较我们的生成人物与人类撰写的响应。</sample>
    <sample id="1434">第二部分是标记词，这是一种方法来识别区分标记群体的词语，我将在不久的将来详细解释。</sample>
    <sample id="1435">这个好处是我们可以获得非常具体的刻板印象和模式，而无需依赖任何特定的词汇表。</sample>
    <sample id="1436">so the marked words method draws upon the sociolinguistic concept of markedness which states that there is an unmarked default and any group that differs from that default is linguistically marked</sample>
    <sample id="1437">so for instance the word man or sorry the word warrior is usually associated with men so when people are describing a warrior who is a woman they'll usually actually specify woman warrior and mark the term with woman</sample>
    <sample id="1438">在更广泛的意义上，社会中的主导群体在语言上和社会上都是无标记的，而边缘化的群体通常是有标记的。</sample>
    <sample id="1439">所以在我们的方法中，我们首先指定未标记和标记组。</sample>
    <sample id="1440">然后我们使用 fighting words 方法来比较人物，这基本上是使用加权对数奇率来区分每个标记组的顶级词汇。</sample>
    <sample id="1441">so for instance for the personas of black women we would do fighting words and compare the law god's ratios against both white personas and man personas because those are the two corresponding unmarked groups</sample>
    <sample id="1442">now for some results so first we use alexicon of stereotypes and we find that the generated personas contain a lot more stereotypes than the human-written ones</sample>
    <sample id="1443">然而当我们实际上查看词汇表中词的分布时，我们发现情况完全不同。</sample>
    <sample id="1444">so while the generated personas have much higher rates of the luxon words the human written ones have a much wider distribution of words while the stereotype words that are in the generated personas are really just the words tall and athletic</sample>
    <sample id="1445">所以只保留正面或至少非负面的那些。</sample>
    <sample id="1446">and in fact this lexicon doesn't really capture many of the harmful patterns that we saw in the earlier slides well at all so instead to do that we'll turn to the results from our marked words method to show how these positive seeming words facilitate stereotypes and essentializing narratives</sample>
    <sample id="1447">在我们的分析中，我们揭示了这些看似积极的描绘反映了可怕的模式。</sample>
    <sample id="1448">首先从市场群体来看，最常见的词汇包括“文化”“传统”“骄傲”和“异国”等词汇，这些词汇仅仅通过它们与身份的关系来定义这些群体，并将它们与白人标准区分开来。</sample>
    <sample id="1449">这为这些群体的长期歧视和边缘化做出了贡献。</sample>
    <sample id="1450">furthermore there's a lot of common tropes that are reflected in these words especially for women of color so for example the words describing latina women include things like vibrant and curvaceous</sample>
    <sample id="1451">这与热带主义的刻板印象有关，对于亚洲女性，这些词语包括“小巧”、“精致”和“丝绸”等。</sample>
    <sample id="1452">这与亚洲女性长期被过度性化、被视为非常顺从和屈服的历史有关。</sample>
    <sample id="1453">最后，对于黑人女性来说，我们看到一些最常见的词汇是“强大”和“坚韧”。</sample>
    <sample id="1454">这与人们所称的强大黑人女性模式有关，虽然在第一眼看起来似乎是积极的。</sample>
    <sample id="1455">there's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles</sample>
    <sample id="1456">所以而不是实际上努力改变这些障碍，它对这些人施加压力，导致他们无法克服这些障碍，这导致了这些人的负面健康结果，以及其他伤害。</sample>
    <sample id="1457">更广泛地说，我们发现每个标记群体的词汇基本上都反映了非常简化的叙述。</sample>
    <sample id="1458">根据这些模式，我们得出三个建议，供模型所有者参考。</sample>
    <sample id="1459">首先我们作为研究者应该关注积极的刻板印象和本质化的叙述，我们还应该使用交叉视角来研究偏见和伤害，因为如果我们不这样做可能会忽视很多事情。</sample>
    <sample id="1460">最后，应该增加关于偏见缓解方法的透明度。</sample>
    <sample id="1461">因为比如说这些积极的刻板印象，我们不知道是因为有一些奇怪的原因。</sample>
    <sample id="1462">过度的价值对齐或可能是其他一些反刻板思维的方法导致了这些有害的模式。</sample>
    <sample id="1463">我们确实无法做出任何假设或进一步研究，除非有更多透明度。</sample>
    <sample id="1464">谢谢你们的耐心听我讲话</sample>
    <sample id="1465">hello everyone my name is jingwei yi from the university of science and technology of china</sample>
    <sample id="1466">这是我的荣幸，给大家展示我们的报纸，我们将复制我们的模型，保护大型语言模型的版权，我们会保护我们的水印。</sample>
    <sample id="1467">让我们首先介绍有关嵌入式服务的背景。</sample>
    <sample id="1468">目前，大型语言模型如gpt、llama和palm在自然语言理解和生成方面表现出色。</sample>
    <sample id="1469">embedding as services 是基于大型语言模型构建的服务，用于协助各种 nlp 任务。</sample>
    <sample id="1470">例如，openai 提供了一个基于 gpt 的嵌入式 api。</sample>
    <sample id="1471">然而，最近的研究表明，攻击者可能通过学习嵌入式服务来窃取模型，因此保护嵌入式服务的版权是必要的。</sample>
    <sample id="1472">为了保护嵌入式服务的版权，一种解决方案是将水印嵌入服务中，并检测另一个服务是否包含水印。</sample>
    <sample id="1473">the watermark method needs to meet the following properties first the method should be applicable to embedding services second the watermark should not degrade the utility of the provided embedding</sample>
    <sample id="1474">第三，水印应该足够隐蔽，以便攻击者无法轻易移除。</sample>
    <sample id="1475">最后，水印需要在模型提取过程中能够转移到攻击者的服务中。</sample>
    <sample id="1476">现有的作品可以大致分为四个类别。</sample>
    <sample id="1477">然而，这些方法要么不适用于嵌入式服务，要么缺乏可转移性。</sample>
    <sample id="1478">因此，在这篇论文中，我们提出了嵌入标记器，这是一种基于后门的水印方法，适用于嵌入服务。</sample>
    <sample id="1479">然后让我介绍我们的嵌入标记器嵌入标记器包含两个主要步骤：水印注入和版权验证。</sample>
    <sample id="1480">在这些主要步骤之前，我们首先选择一个触发词集。触发词集是一组在中等频率间隔内出现的词。</sample>
    <sample id="1481">我们假设提供者可以收集一个通用文本语料库，并计算其中的单词频率。</sample>
    <sample id="1482">在watermark注入中，我们首先定义一个目标词汇。当用户向提供者服务发送一个句子时，提供者会计算句子中触发词的数量。</sample>
    <sample id="1483">提供的嵌入是目标嵌入和原始嵌入的加权和。</sample>
    <sample id="1484">目标嵌入体的权重与句子中触发器的数量成正比。当句子中触发器的数量大于 m 时，提供的嵌入体与目标嵌入体完全相同。</sample>
    <sample id="1485">版权验证是检测一个模型是否在另一个服务中包含水印。</sample>
    <sample id="1486">我们首先构建一个backdoor和benign数据集 backdoor数据集包含所有单词都属于trigger集的句子，而benign数据集的句子中的所有单词都不属于trigger集。</sample>
    <sample id="1487">然后，提供者从 stealers 服务请求数据集的嵌入。</sample>
    <sample id="1488">请求的嵌入和目标嵌入之间的余弦相似性和l2相似性被计算出来 我们计算了benign和backdoor数据集之间的相似性差异，这被定义为delta-cosine和deltal2。</sample>
    <sample id="1489">同时，我们还将ks检验应用于它，并使用其p值作为第三个指标。</sample>
    <sample id="1490">我们在四个数据集上进行实验：age news mind sst2 和 erasmus we assume the provider applied wikitext dataset to count word frequencies</sample>
    <sample id="1491">在四个数据集上的结果表明，我们的嵌入标记可以实现更好的检测性能，同时保持良好的下游任务性能。</sample>
    <sample id="1492">我们还通过将句子的嵌入可视化来验证提供的嵌入的隐蔽性： 图表的图例表示每个句子中的触发器数量。</sample>
    <sample id="1493">如图所示，很难区分因子嵌入和正常嵌入。</sample>
    <sample id="1494">that's all thank you welcome to discuss with us</sample>
    <sample id="1495">we call this approach annotating behaviors in chat or abc eval in short we developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature</sample>
    <sample id="1496">our conclusion is that for good generalization we would need a better model architecture larger model size as well as more fine-tuning examples and these goes hand in hand we can't just have one ingredient but throw out the others at the same time we also found that the performance drop here is caused by temporal drift and kind of surprisingly it is not caused by adaptive overfitting even though conll 2003 has been used for over 20 years so going back to the question that we raised in the title of our paper do conll 2003 taggers still work in 2023 and we found that the answer is actually a resounding yes we hope our paper calls for more research on how to improve generalization of the models</sample>
    <sample id="1497">hello my name is vasudha and i'm a computer science phd candidate at stony brook university i would like to present our work accepted into acl 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge</sample>
    <sample id="1498">我们首先定义认知不一致，并解释为什么它是语言研究中的一个重要问题。简单来说，认知不一致是两个相互矛盾的信念或行为。</sample>
    <sample id="1499">such as this example where a person states i know that cigarettes could kill me and then goes on to say i grabbed a couple of smokes after the meeting this belief and action are inconsistent and they are in dissonance</sample>
    <sample id="1500">进一步提到我不认为我能在没有他们的情况下保持工作，这就证明了第二次发生的事情，他们有一致的关系。</sample>
    <sample id="1501">虽然距离是我们在日常决策中经常遇到的一种现象，但在其他种类的语言关系中很少能找到表达。</sample>
    <sample id="1502">所以为什么这很重要？研究认知分歧可以帮助我们理解人群之间不一致的影响，跟踪信仰价值观和态度变化的趋势。</sample>
    <sample id="1503">高认知不一致也与焦虑障碍有关，可以帮助更好地理解人们的心理健康状况。</sample>
    <sample id="1504">研究语言中表达的不和谐也可能有助于理解极端主义和脆弱群体的极化。</sample>
    <sample id="1505">最后，认知不一致性对于理解个体的个人认知风格非常重要，并有助于我们更好地理解决策过程。</sample>
    <sample id="1506">为了创建一个认知不一致资源，我们进行了大规模的不一致关系标注。我们采用了不一致优先的方法，如图表中所示。</sample>
    <sample id="1507">推文使用了pdtb解析器，并根据我们论文中描述的指南对语境单位进行了标注。</sample>
    <sample id="1508">如图所示，共鸣仅在35的标注对中发现。</sample>
    <sample id="1509">我们收集了大约1000个语境单位对，并对一个仅基于43个示例训练的分类器进行了训练。不出意料，分类器的表现并不比随机猜测更好。</sample>
    <sample id="1510">考虑到离婚的发生率较低以及没有任何类似的数据集，我们面临的是绝对稀有性问题。</sample>
    <sample id="1511">为了缓解这一问题，我们通过结合转移学习和主动学习来标注更多的不一致样本，从而在较少的标注运行中收集更多的不一致样本，同时降低整体标注成本并改善不一致检测。</sample>
    <sample id="1512">由于初始模型无法捕捉离散类别，我们开始通过将来自相关任务的权重迁移来启动主动学习过程。</sample>
    <sample id="1513">we transfer from two different tasks topic independent dissonance stance classification a task that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic</sample>
    <sample id="1514">called debate here and on binary classification of expansion and comparison classes of purity b since these two are closely related to the conception of consonance and dissonance and we call them ce here</sample>
    <sample id="1515">我们发现在转移到标注数据集上的零短性能已经比最佳的auc062要好得多。</sample>
    <sample id="1516">进一步迭代地对两个任务进行微调，我们发现先微调ce任务再微调辩论，可以获得更好的零样本性能。</sample>
    <sample id="1517">接下来我们确定了更新模型的最佳方法，以便从每一轮活动学习和注释中收集的新数据。累积累积所有已收集的活动注释数据，而迭代更新则通过在最新收集的数据上进行训练来更新模型。</sample>
    <sample id="1518">在不同的策略中，我们发现累积策略的表现与迭代策略相当或更好。</sample>
    <sample id="1519">接下来为了提高不和谐例子的数量，我们使用概率选择不和谐策略 prc 来选择大多数在当前模型在任何轮次中很有可能被不和谐的例子的例子。</sample>
    <sample id="1520">我们将其与社区中常用的其他最先进的a策略进行比较。</sample>
    <sample id="1521">我们发现所提出的prc策略比其他state-of-the-art策略表现更好，尽管差异很小。 请注意，随机的性能显著较低。</sample>
    <sample id="1522">on further rounds of ale with two best strategies we improved dissonance classification auc to 075 which is the best performance that we have on the task so far</sample>
    <sample id="1523">我们还检查了每个策略的可行性，以评估注释质量和对注释者的成本。 我们发现 prc 具有最高的不一致性百分比，并且对稀有类别最有效。然而，注释者也发现例子很难理解。</sample>
    <sample id="1524">总结来说，我们发现 prc 是一种简单有效的 ail 策略，用于稀有类别的获取，并通过设计合适的转移学习任务可以显著提高。</sample>
    <sample id="1525">我们还发现迭代更新对于从不同领域转移学习是有用的，而领域内主动标注则从累积更新中受益。</sample>
    <sample id="1526">这些是我们代码数据集和论文的链接，如果您有任何问题，请随时与我们联系。</sample>
    <sample id="1527">hi my name is mathias lendeman and today i'm going to give you a brief introduction to our paper on compositional generalization without trees using multi-set tagging and latent permutations this is joint work with my advisors alexander koller and ivan titov</sample>
    <sample id="1528">hi i'm sijun from fudan university i'm here to introduce our work distinguishing script knowledge from large language models for constrained language planning</sample>
    <sample id="1529">hello my name is kayo yen and i will be presenting our work titled when does translation require context a data-driven multilingual exploration this work was done in collaboration with patrick frenoux emily yu andrew ft martins and graham newbigg</sample>
    <sample id="1530">and we compare with popular strategies that are also applied to offline models that are the wait-k strategy and the local equipment and we compare also with the state-of-the-art architecture specifically tailored for simultaneous speech translation</sample>
  </task>
</testset>