<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="zh">
    <sample id="0">hi i'm jianbin phd student at university of washington today i'm presenting our work from pre-training data to language models to downstream tasks tracking the trails of political biases leading to unfair nlp models so language models are trained on large-scale web crawl data political news media are well covered in their pre-training data according to a survey of the c4 corpus we can see that new york times los angeles times the guardian huffington post etc are well covered in language model training data this has created a mixed blessing for language model applications so on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas on the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream tasks ultimately creating fairness issues if we do try to sanitize somehow we would also risk censorship or exclusion and it's incredibly hard to determine what is actually neutral and should be retaining language model training data so it's kind of like the electric chollie problem okay great i think that's pretty much all i have for today thank you for your time</sample>
    <sample id="1">这篇论文的作者所属机构是什么？</sample>
    <sample id="2">hello everyone i am jiwei from edgroup and here i'm going to be presenting our team's paper on document understanding the co-authors of this paper are all algorithm engineers from edgroup and this article is derived from our working practice in this paper we focus on the visually rich document understanding problem it aims to understand various types of documents like forms receipts and posters in recent years pre-training techniques have been introduced into this area and the self-supervised pre-training models have demonstrated great success in various related tasks however existing document pre-training models suffer from reading order issues following the idiom word deposition which is used to infer the context for each word based on the semantic relations and the spatial relations from a spatial perspective the joint learning process with both semantic and spatial interests can promote textual relations and help models to learn better layout presentations in our experiments we compare the performance of the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout masking strategy the layout mask</sample>
    <sample id="3">hi welcome to our presentation of deplain a new corpus for german text simplification on the document level and on the sentence level my name is regina stodden and i will guide you through the first part of the presentation let's first define text simplification text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group as people with reading problems or non-native speakers to train a text simplification model we require parallel pairs of text for example documents or sentences in the example here you can see a parallel aligned sentence pair of a complex german text and its translation into plain language to simplify the sentence pairs a little bit more so for example on the type of simplification structural simplification or the overall level of simplification and now as we have our dataset deplain which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods and we did some adaptations to the proposed methods and we have published all these adaptations and the codes to run our experiments in the paper uh we concluded that this this basic fine-tuning could produce uh or could get uh scores better than the baseline scores and we propose those results as a benchmark a baseline benchmark for the problem of automatic text simplification in the future thank you so much for your attention and we hope to meet all of you uh during the conference thank you</sample>
    <sample id="4">hello my name is kyo yin and i will be presenting our work titled when does translation require context a data-driven multi-lingual exploration this work was done in collaboration with patrick fernandez and andrew f d martin and graham neubig so a lot of translations depend on context in the sentence well if the previous sentence was things could start to get dangerous if the ministers find out then mole refers to a spy but if the previous sentence was could it be anything serious doctor then mole refers to a birthmark so depending on context the meaning of the word changes and therefore its translation changes as well however evaluating how well models can handle these cases is pretty hard firstly because only a small portion of translations depend on context in the sentence and secondly because these resources only support limited types of context dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation in this work we try to answer these two questions firstly when does translation require context and secondly how well do models handle these cases to answer the first question we started by measuring how much a word depends on context in the sentence and in the previous work we introduced cxmi as a measure for context usage by machine translation models and this is done by measuring how much information the context c provides about the target y given the source x and we call our tagger the multi-lingual discourse aware or muda tagger we can then also note that different languages have different proportions of these discourse phenomena we then use the muda tagger by applying the tagger on the parallel corpus that we want to use for evaluation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle</sample>
    <sample id="5">hi and i'm going to talk about our work on resolving indirect referring expressions for entity selection in which we introduce the altentities corpus and my name is javad hosseini and this is a joint work with philip radlinski sylvia parretti and annie lewis our goal is to understand users' language when they want to make a choice and consider this alternative question did you mean easy on me or i got a feeling which is the alternative question and in the third speech bubble bob says remember that song we were listening to yesterday and with that bob sets the dialogue context in the second speech bubble alice says do you mean easy on me or i got a feeling which is the alternative question and in the third speech bubble bob uses an indirect referring expression to select one of these entities for example here the first one and we show some background text from wikipedia for recipes we additionally show their images again from wikipedia if the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model retrieves the background knowledge if the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82% to 87% which is more realistic for example when the language model</sample>
    <sample id="6">foreign language hello everyone i'm john i'm so excited to present our work towards unifying multilingual summarization and cross-lingual summarization this is a joint work with fandong duo yulong zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu zhu z</sample>
    <sample id="7">hello everyone my name is xuhong today i'm going to present our paper do conll 2003 named entity taggers still work in 2023 and this shows us that adaptive overfitting in this case is not observed so going back to the question that we raised in the title of our paper do conll 2003 taggers still work in 2023 and we found that the answer is actually a resounding yes we hope our paper calls for more research on how to improve generalization of the models and lastly please make sure to check out our paper our data set and if you have any questions feel free to contact me thank you so much</sample>
    <sample id="8">hello i'm james finch and i'm sarah finch and today we'll tell you all about abc eval a new dimensional approach to evaluating conversational ai this work was done by the emory nlp lab led by professor gino choi at emory university and in collaboration with amazon alexa ai so let's say that you just developed a dialogue model and you want to see how well it compares against the current state of the art the common practice is to use human evaluation such as by asking human judges to select which of two conversations is better or to rate conversations given a likert scale method however we believe there is a more precise and reliable strategy for dimensional dialogue evaluation our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself or its partner hallucinating incorrect facts or violating common sense knowledge and when the model succeeds or fails to show empathy to determine what kind of evaluation is most effective we selected four state-of-the-art chat models and evaluated them on 100 human bot conversations per model using abc eval for comparison we also evaluated these conversations using three existing methods likert ratings on the turn level likert ratings on the dialogue level and dialogue level pairwise comparisons for each of the existing methods we collected evaluations on eight of the most commonly measured aspects of dialogue since this is the standard practice for evaluating chat models along multiple dimensions with the rapid pace of improvement in the field many of these error rates could see a decrease in new models released since our evaluation was conducted however this is all the more reason to pursue reliable and precise evaluation metrics for comparing models we hope abc eval can be leveraged by others in the field as a meaningful step in this direction and we look forward to seeing how conversational ai will advance in the coming months and years thank you for watching</sample>
    <sample id="9">现有弱监督方法的成功在很大程度上依赖于什么？现有弱监督方法的成功在很大程度上依赖于什么？现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么？ 现有弱监督方法的成功在很大程度上依赖于什么</sample>
    <sample id="10">hi and i'm going to talk about our work on resolving indirect referring expressions for entity selection in which we introduce the alt entity corpus and my name is jawad hosseini and this is a joint work with philip radlinski sylvia parretti and annie lewis our goal is to understand users' language when they want to make a choice and consider this alternative question did you mean easy on me or i got a feeling which is the alternative question and in the third speech bubble bob says remember that song we were listening to yesterday and with that bob sets the dialogue context in the second speech bubble alice says do you mean easy on me or i got a feeling which is the alternative question and in the third speech bubble bob uses an indirect referring expression to select one of these entities for example here the first one and we show some background text from wikipedia for recipes we additionally show their images again from wikipedia if the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model retrieves the background knowledge if the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87</sample>
    <sample id="11">hi everyone my name is jack hessel and i'm a research scientist at ai2 i'm super excited to be here today to present do android's laugh at electric sheep humor understanding benchmarks from the new yorker caption contest this is a joint work with a lot of awesome collaborators from the university of utah cornell university university of washington airmail and open ai have you heard the news large language models can now generate and even explain jokes if you log on to chat gpt and ask it to tell you a joke it might generate something like this why don't scientists trust atoms because they make up everything even beyond generating simple jokes like this some language models have even successfully explained jokes before here's an example from google's 540 billion parameter palm language model uh in this case palm is attempting to explain a joke about tpus which coincidentally is the type of hardware that the model was trained on i would argue this is a successful joke explanation this capability was actually highlighted at google i o which is google's developer conference here's a picture of sundar pichai touting joke explanation as a interesting capability of palm this public facing declaration of capability led to headlines like no joke google's ai is smart enough to understand your humor but do large language models really understand humor if you play around for example with chat gpt and probe it's humor understanding capacity uh for example by asking it to tell you a knock-knock joke involving a pineapple you might be a little bit disappointed here chat gpt appears to paste the word pineapple into some sort of knock-knock joke and then claims that it's made of pineapple this is borne out in human evaluation experiments where in a blind abe study uh human explanations are preferred to five-shot gpt4 explanations in more than two-thirds of cases overall uh we're super excited to see what folks do with our dataset we have a leaderboard and models available at this url and yeah thank you so much for your attention and i look forward to seeing you at acl thank you</sample>
    <sample id="12">hello i am dawei a phd student at stuttgart university in germany in this video i would like to present our recent work weak supervision and weakly supervised learning in weak supervision we do not manually label the data instead we label the data using weak labeling sources such as simple heuristic rules knowledge bases or locality crowdsourcing as illustrated in the figure on the right when compared to human annotation the weak labeling sources are much cheaper yet they are also noisy meaning that a certain amount of the labeling data are incorrect if we directly train neural networks on weakly labeled data the neural networks tend to memorize the labeling noise and do not generalize well in wsl learning we propose to address this problem by training algorithms to robustly train neural networks on weakly labeled data so that the trained models can generalize well beyond the original weakly labeled data to summarize we show that recent wsl approaches require clean manually annotated samples for them to work properly their performance gain and practicality are heavily overestimated our concrete recommendations for future work are as follows first report the model selection criteria for example report if the model selection is done via clean validation samples second wsl approaches should be compared with fully supervised learning baselines as both work on clean samples third continuous fine-tuning is a simple yet strong baseline that should be considered in future work in wsl finally we have open sourced our code you can find it via the qr code on this slide please feel free to check it out thank you and enjoy the conference</sample>
    <sample id="13">hello everybody my name is daniel rotem and i will be presenting my work finding the sweet spot analysis and improvement of adaptive inference in low-resource settings which was done in professor roy schwartz's lab at the hebrew university in jerusalem adaptive inference is a method for reducing the inference time of large language models to use it we rely on the fact that real-world data varies in complexity therefore we can use low-capacity models for easy samples and therefore in that way reduce the average inference cost whether it be time or money the two most common adaptive inference methods are multimodal and early exit in multimodal multiple models are stored together each fit with a classifier at the end they are trained separately on the entire training set and when used for inference they are run sequentially until a classifier decides to halt that way saving the computation which would have been exhausted by the rest of the model in order to test our hypothesis we compared individual early exit models classifiers with separate multimodel classifiers which are truncated versions of the bert pre-trained language models the takeaways from our work are as follows we show the existence of conflicting gradients in early exit training process to the best of our knowledge we conduct the first fair comparison of early exit and multimodel adaptive inference methods we also introduce the sweet method the results of which motivate future research of fine-tuning algorithms tailored to the early exit architecture thank you very much for listening if you enjoyed the talk and you want to see more go visit our paper on archive finding the sweet spot</sample>
    <sample id="14">hi my name is adam sperkowski and this talk is about the dependency structure of coordination as you may know there are different dependency structures assumed by different theories and corpus approaches so for example in universal dependencies the structure of the coordinate structure is such that the first conjunct is the head of the whole coordinate structure so in this case lisa read and maggie is such that the first conjunct is the head of the whole coordinate structure so we get dependencies from red to the adjunct of length 7 measured in words and from red to book of length 4 so together it's 11 when you move when you swap these two constituents the sum of these two dependencies becomes 6 right so instead of 11 6 much shorter that's why this sounds quite okay right it violates one principle but it satisfies another one okay so what we did we extracted various statistics from about coordination from the enhanced version of the pen tree bank and see the paper why we wouldn't use universal dependencies and these statistics confirm the observation made many times before that left conjuncts tend to be shorter so salt and pepper and not pepper and salt measured in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we show that by measuring length in characters the first column in syllables the middle column and in words the right column so see the paper for the full agreement and arguments sorry and talk to us about the poster session thank you</sample>
    <sample id="15">hi my name is mathias lendemann and today i'm going to give you a brief introduction to our paper on compositional generalization without trees using multiset tagging and latent permutations this is joint work with my advisors alexander kola and ivan titov compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training in the context of semantic parsing testing for compositional generalization might look like this as usual we have a training set of utterances in this case the girl slept and mary knew that the girl slept this works well but trees are usually not given and need to be obtained somehow this can be complicated and sometimes a computationally expensive process typically this involves considerable formalism-specific preprocessing of the logical forms for example to handle variable symbols obtaining trees may also involve specialized grammar induction procedures in this paper we solve a couple of interesting technical challenges first of all the alignment between input and output is not given in the training data as a consequence for a given token we don't know which multiset it came from which poses a challenge for training in addition sometimes there are multiple permutations that are consistent with the data but the linguistically correct one is latent we address this by inducing the alignment as part of the training our permutation method is very flexible but it brings the challenge that finding the highest scoring permutation is np-hard that's because this is related to the traveling salesman problem we approximate this with a gpu-friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations if you want to learn more about our experiments and how we address these challenges please have a look at our paper or come to our poster</sample>
    <sample id="16">hi welcome to our presentation of deplain a new corpus for german text simplification on the document level and on the sentence level my name is regine stodden and i will guide you through the first part of the presentation let's first define text simplification text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group as people with reading problems or non-native speakers to train a text simplification model we require parallel pairs of text for example documents or sentences in the example here you can see a parallel aligned sentence pair of a complex german text and its translation into plain language to simplify the sentence pairs a little bit more so for example on the type of simplification structural simplification or the overall level of simplification and now as we have our dataset deplain which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods and we did some adaptations to the proposed methods and we have published all these adaptations and the codes to run our experiments in the paper uh we concluded that this this basic fine-tuning could produce uh or could get uh scores better than the baseline scores and we propose those results as a benchmark a baseline benchmark for the problem of automatic text simplification in the future thank you so much for your attention and we hope to meet all of you uh during the conference thank you</sample>
    <sample id="17">hi everyone my name is jingchao wu a phd student in ais i'm glad to introduce our work about multimodal relation extraction relation extraction is a widely explored task it aims to determine the semantic relation between entities in a given text however in some realistic scenarios such as in social media the data is often in various forms and modalities rather than just pure text so if you just look at the text you may find that lacks sufficient context to understand some ambiguous or multisense words so multimodal relation extraction has been introduced recently additional visual sources are added to the textual relation extraction and then we consider taking multimodal topic information as additional semantic supplementary to enrich the overall context to evaluate the effectiveness of the proposed method we conduct experiments on a widely used mmr dataset as seen from the table compared with the text-based method leveraging video features can obtain higher performances and we see that all proposed methods can get the best performance among the multimodal baselines in the ablation study we find that the information screening and compensating both contribute to the task performances and the syngraph is beneficial for structural modeling of the multimodal inputs when removing the syngraph the performances get decreased next we want to know under what circumstance do the internal information screening and external information exploiting help so we group the instances by text relevance scores and make the predictions for different groups for the inputs with higher text relevance relevance the syngraph contributes more significantly than gene that means the external information exploiting is more useful in conclusion we introduce a novel idea of simultaneous information subtraction and addition for multimodal relation extraction we perform internal information screening with the guidance of the graph information bot-like principle we devise a latent multimodal topic model and induce latent multimodal topic features to enrich the feature context our overall system achieves significant improvements over the existing best models on the benchmark so thanks a lot if you are interested in our work you can scan the qr code for more detailed information thank you</sample>
    <sample id="18">hi my name is adam sperkowski and this talk is about the dependency structure of coordination as you may know there are different dependency structures assumed by different theories and corpus approaches so for example in universal dependencies the structure of the coordination lisa bart and maggie is such that the first conjunct is the head of the whole coordination structure so in this case lisa bart and maggie however this effect may be ameliorated when the direct object is very heavy and very long because then it can be moved to the position after the adjunct this is illustrated here so both these sentences are fine march read this absolutely fascinating book about bees so the reasoning here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb this satisfies the principle of dependency length minimization which says that shorter dependencies are preferred so these two trees only show the length of the crucial dependencies so the ones that are not constant among these two structures so here we have the dependency from red to the adjunct of length 7 measured in words and from red to book of length 4 so together it's 11 when you move when you swap these two constituents the sum of these two dependencies becomes 6 right so instead of 11 the bigger the difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears so we show that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed as the tendency disappears and we show in the paper how this provides an argument against asymmetric structures of coordination as these two and for the symmetric structures as these two so see the paper for the full agreement and arguments sorry and talk to us about the poster session thank you</sample>
    <sample id="19">hello everyone my name is chancellor a master's student from shenzhen university i'm so glad that our work or survey for efficient open domain question answering was accepted by icml 2023 it's my great honor to present our work here our work focuses on open domain question answering a mainstream framework is the two stage model proposed by dengshichuan in 2017 the first stage uses a retrieval to retrieve several evidences context from wikipedia corpus and the second stage uses a reader to understand the question and retrieve the evidences to reason out the answers the third aspect is how to reduce the model size or to achieve this goal you can select light weight models or parameter sharing or designing one stage model for both retrieval and reading if it pursues real time feedback retrieve only systems to be good choices if one pursues chat or retrieval and reading systems are relatively more appropriate finally we discuss two future works the first one is how can the open domain question answering systems be deployed in low power devices and the second one is more evaluation metrics should be considered that is all my presentations thank you for your attention</sample>
    <sample id="20">yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes you can use these models for your research yes</sample>
    <sample id="21">deplain-web 包含来自网络的文档 deplain-apa 中包含哪种类型的内容</sample>
    <sample id="22">hello everyone my name is xuhong today i'm going to present our paper do conno 2003 named entity taggers still work well in 2023 and last but not least we calculated the percentage change in f1 to assess the generalization of each model so what is needed for good generalization through our experiments we found that from the graph on the right the red best fit line has a gradient that is greater than one this means that every unit of improvement that we made on conno 2003 translates to more than one unit improvement on conno which means that there is no diminishing returns and this shows us that adaptive overfitting in this case is not observed so what about temporal drift then for temporal drift we didn't experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift and kind of surprisingly it is not caused by adaptive overfitting even though conno 2003 has been used for over 20 years so going back to the question that we raised in the title of our paper do conno 2003 taggers still work in 2023 and we found that the answer is actually a resounding yes we hope our paper calls for more research on how to improve generalization of the models and lastly please make sure to check out our paper our data set and if you have any questions feel free to contact me thank you so much</sample>
    <sample id="23">hi i'm dan garrett and i'm going to talk about our work on improving the ability for text to image models to render visual text text to image modeling research has made huge strides in the last year with the ability to generate very high quality interesting images but a lot of people have noticed that these models are often very bad at representing text so we specifically look at the t5 model which works by taking the input text encoding it with a t5xl encoder and then using that encoded text representation as input to a diffusion model which is actually able to generate the image here we can see a fairly complex input producing an accurate image output however even a much simpler text input that requires the image to contain a word will often fail to understand what's going on here we can dig into the text encoder itself t5 uses sentence piece tokenization which means that instead of the model receiving the individual letters that make up the spelling of the input it's receiving subword ids for chunks of the input string that means if it's asked to render a word it has to be able to decompose that atomic subword vocabulary item into the individual letters that make up the spelling and that's because the way the sentence piece algorithm works is that more frequent words tend to be represented by a single vocabulary item or a couple of subwords and so that means that the model is having to take a smaller number of bigger chunks and decompose it into a larger number of letters so here again we can see that t5 because it has full access to the character level information does very well and isn't affected by the frequency of the word all it has to do is learn how to copy the input characters to the output and so you can see in this graph here that across all scales t5 does very well at spelling and that's because the way the sentence piece algorithm works is that more frequent words tend to be represented by a single vocabulary item or a couple of subwords and so that means that the model is having to take a smaller number of bigger chunks and decompose it into a larger number of letters so the main takeaways from our paper are the wiki spell benchmark for text only models the draw text benchmark for text to image models and a new efficient strategy for improving model spelling ability which is to concatenate in a model that's actually aware of the characters in the input</sample>
    <sample id="24">hi my name is adam sperkowski and this talk is about the dependency structure of coordination as you may know there are different dependency structures assumed by different theories and corpus approaches so for example in universal dependencies the structure of the coordination lisa bart and maggie is such that the first conjunct is the head of the whole coordination structure so in this case lisa bart and maggie however this effect may be ameliorated when the direct object is very heavy and very long because then it can be moved to the position after the adjunct this is illustrated here so both these sentences are fine march read this absolutely fascinating book about bees so the reasoning here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb this satisfies the principle of dependency length minimization which says that shorter um uh shorter dependencies are preferred so um these two um uh trees uh only show uh the length of the crucial dependencies so the ones that are not constant among these two structures so here we have the dependency from red to the adjunct of length 7 measured in words and from red to book of length 4 so together it's 11 when you move when you swap these two constituents the sum of these two dependencies becomes 6 right so instead of 11 6 much shorter that's why this sounds quite okay right it violates one principle but it satisfies another one okay so what we did we extracted various statistics from about coordination from the enhanced version of the pen tree bank and see the paper why we didn't use universal dependencies and these statistics confirmed the observation made many times before that left conjuncts tend to be shorter so salt and pepper and not pepper and salt measured in syllables uh and also the observation that was made in passing that this tendency grows with length the length difference uh this effect disappears so we showed that um uh by measuring length in characters uh the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that uh when the governor is on the left the tendency for the left conjunct to be shorter grows steadily uh with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and uh we show in the paper how this provides an argument against asymmetric structures of coordination as these two and for the symmetric structures as these two so see the paper for the full agreement and uh arguments sorry and talk to us about the poster session thank you</sample>
    <sample id="25">how to design an experiment to study the effect of the position of the governor on the length of the conjuncts however this effect may be ameliorated when the governor is on the right this tendency grows with the length difference so when the governor is on the left this tendency grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we show that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we show in the paper how this provides an argument against asymmetric structures of coordination as these two and for the symmetric structures as these two so see the paper for the full agreement and the arguments sorry and talk to us about the poster session thank you</sample>
    <sample id="26">hello my name is vasudha and i'm a computer science phd candidate at stony brook university i would like to present our work accepted into acl 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge we begin by defining cognitive dissonance and why it is an important problem to study in language simply put cognitive dissonance is two beliefs or actions that are inconsistent such as this example where a person states i know that cigarettes could kill me and then goes on to say i grabbed a couple of smokes after the meeting this belief and action are inconsistent and they are in dissonance further mentioning that i don't think i could keep my job without them justifies the second occurrence and they have a dissonance relationship since the initial model was not able to capture the dissonance class at all we start the active learning process by transferring weights from closely related tasks topic independent dissonance stance classification a task that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic called debate here and on binary classification of expansion and comparison classes of prt since these two are closely related to the conception of consonance and dissonance and we call them cee here we find that on transferring the zero-shot performance on the annotated dataset is already much better than chance with the best with auc 062 further on iterative fine-tuning on both tasks we find that fine-tuning of cee task followed by further fine-tuning on debate yields a much better zero-shot performance thus this is the model that we use to cold start the active learning process feel free to get in touch with us if you have any questions thank you</sample>
    <sample id="27">hi i'm jianbin phd student at university of washington today i'm presenting our work from pre-training data to language models to downstream tasks tracking the trails of political biases leading to unfair nlp models so language models are trained on large-scale web crawl data political news media are well covered in their pre-training data according to a survey of the c4 corpus we can see that new york times los angeles times the guardian huffington post etc are well covered in language model training data this has created a mixed blessing for language model applications so on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas on the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream tasks ultimately creating fairness issues if we do try to sanitize somehow we would also risk censorship or exclusion and it's incredibly hard to determine what is actually neutral and should be retaining language model training data so it's kind of like the electric cholli problem okay great i think that's pretty much all i have for today thank you for your time</sample>
    <sample id="28">hi and i'm going to talk about our work on resolving indirect referring expressions for entity selection in which we introduce the altentities corpus and my name is javad hosseini and this is a joint work with philip radlinski sylvia parietti and annie lewis our goal is to understand users' language when they want to make a choice and consider this alternative question did you mean easy on me or i got a feeling which is the alternative question and in the third speech bubble bob says remember that song we were listening to yesterday and with that bob sets the dialogue context in the second speech bubble alice says do you mean easy on me or i got a feeling which is the alternative question and in the third speech bubble bob uses an indirect referring expression to select one of these entities for example the first one and describe them using three to five indirect referring expressions for example the one with the piano music here are some examples from our dataset for example the one without words not the one with the 12 year old and 12 year old boy or the fictional one or comes from other boy jan and so on the altentities corpus has 6000 alternative questions across three domains and it has 42000 indirect referring expressions results with t5x large model are summarized below if the language model has access to the exact same background knowledge as the annotators the accuracy is really high it's around 92 to 95 which is more realistic for example when the language model retrieves the background knowledge if the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model retrieves the background knowledge if the language model has access to only entity names then the accuracy is only 60 so there's a lot of room for improvement we've also shown that the models are domain generalizable here is a link to our dataset thanks for watching</sample>
    <sample id="29">在哪些话语现象上，语境感知的机器翻译模型比语境无关的模型更有优势？</sample>
    <sample id="30">hello everyone we are going to introduce our paper blender which is a simple yet effective ensemble learning framework for large language models and this key idea is based on pairwise ranking and generative fusion so there are so many large language models released every week and many of them claim that they have achieved a great performance and from this leaderboard we can it did say oh some models are better than the others but this is only about average overall performance and when you have a particular input x we can select and generate a better output than using any single model for our input x and given the comparison matrix here and where each element in this matrix represents the comparison logic for candidate i being better than candidate j then from this matrix we can have three methods to aggregate all these results and we found that using the mean average is the best solution but if you worry about the efficiency you can also use bubble sort algorithm here then it's very efficient and we can also get a decent performance so experiments here show that pairwise ranking is much better correlated with oracle ranking better than all the other ranking methods on various correlation metrics so to evaluate to enable the evaluation of ensemble learning framework for large language models we also create a new data set named mix instruct so it consists of existing instruction data sets and we collect the candidates from 11 open source legendary models here and this result suggests that blender is a very promising framework for ensemble learning although it's very simple and straightforward so in the end we want to give some take home messages here so blender is a simple and effective ensemble learning framework for large language models it has two sub-modules pairwise ranking is a pairwise first module that we can get the matrix for all these results and jump user can take the top three candidates and generate the final output and it largely improved the performance and mix instruct is our data set for evaluating the black box models here and we also release a unified data code base and our data for evaluation and future research okay so that's all thank you very much</sample>
    <sample id="31">hi everyone i'm kostas michrakkalis and i'm pleased to welcome you to our talk of our acl 2023 paper language model acceptability judgments are not always robust to context this is a joint work with john gauthier aaron muller kanishka misra gareth flinders roger levy and edina williams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like blimp syntax gem or acceptability in terms of stereotypes such as cross pairs and in this a minimal pair paradigm the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an unacceptable sentence or an ungrammatical sentence and then the hope is that the model basically puts more probability to the acceptable sentence or the unacceptable sentence depending on whether the chosen prefix is acceptable or unacceptable now this and this is very large like this effect increases throughout the context length and this would probably affect like newer language models which has large context window so why does the match prefix affect the language model judgment so much so we did a series of analysis where we tried to like perturb the input sentence by trying to preserve the relevant structure but adding like noise to the input and after doing like several of these perturbations we find that none of these noises are actually making the model like change its course in terms of how it shows us the mpp judgment trend basically we find that the models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences and the mpp evaluation the way that we do it currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context window please read our paper for more details of our experiments thank you for listening</sample>
    <sample id="33">hi everyone i'm jenny a first year phd student at carnegie mellon university and today i'll be presenting our work on positionality characterizing design bias in ai datasets and models this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was</sample>
    <sample id="34">hello everyone my name is marcus treviso and today i'm going to present a work called crest a joint framework for rationalization and counterfactual text generation crest is a result of a great collaboration with alexis ross luno geheru and dr martins so let's say we have an input like this one we pass this input to our rationalizer model which has a trainable masker component that is trained to produce meaningful rationales and here is how it works let's say we start with crest generation which produces two outputs in the end one is basically the rational z for the original input x and the other is the counterfactual x tilde from here we get two computation flows a factual flow responsible for processing the original input and a counterfactual flow for the counterfactual input that produces a final decision finally to effectively leverage the paired structure of the inputs we add a new regularization term that encourages the new rationales to be similar to those originally generated by crest generation and we do this analysis in three dimensions in terms of plausibility forward similitude and a new proposed metric that we call counterfactual similitude so intuitively this metric measures the ability of an explanation z to change the classifier's decision when the classifier receives as input a counterfactual input that was guided by this explanation so here are the results overall we note that crest rationalization produces more plausible rationales than other approaches and that these rationales also achieve a higher counterfactual similitude than those produced by other methods and this is by a large margin so to summarize we propose crest a joint framework for rationalization and counterfactual generation that produces valid fluent and diverse counterfactuals in a controllable way and that by leveraging these counterfactuals during training it leads to plausible explanations that focus on the contrasted parts of the input take a look at our paper and our code for more information thank you</sample>
    <sample id="36">hi welcome to acl i'm thomas wolfswinkel and this is a sneak peek into language specific layers for multilingual machine translation joint work with robin schmidt yushi liao and stefan bitez multilingual machine translation has several advantages namely scalability as it's easier to train and maintain a single model rather than one model per language direction speed because you can directly translate between any two languages instead of having to pivot through a third language like english and this also leads to lesser cascade finally you get improvements for low resource language pairs on the other hand it also means that the model size will be larger so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get an architecture like the one we show here where the bottom two layers are shared the next two layers are source specific layers then we have a few more shared layers then three target specific layers and the top layer is shared so if you do that you're going to get</sample>
    <sample id="37">hi i'm myra and today i'll be talking about our paper marked personas using natural language prompts to measure stereotypes in language models this work is done in collaboration with essem dermanush and dan jurafsky in recent years many have documented the prevalence of social bias and stereotypes in large language models however these measures have various limitations they usually rely on hand-constructed datasets that are very time-consuming to curate and they also usually only measure very specific stereotypes meaning that they don't generalize well to other demographics or contexts or they simply capture very general broad associations like negative associations with particular groups so when people are describing a woman they'll usually actually specify woman and mark the term with woman and more broadly dominant groups in society are both linguistically and socially unmarked while the marginalized groups are usually marked so in our method we first designate what the unmarked and marked groups are and then we compare the personas using the fighting words method which is basically using weighted log odds ratios to distinguish the top words for each marked group and for black women we see that some of the top words are things like strong and resilient this connects to an archetype that people have called the strong black woman archetype and while it sounds like positive at first glance um there's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure um on these people to be resilient and strong against societal obstacles so rather than actually working towards changing those obstacles it puts pressure on those people to overcome them which leads to very negative health outcomes for these people among other harms more broadly we find that the words for each marked group pretty much just reflect very essentializing narratives so based on these patterns we conclude with three recommendations for model owners first we should as researchers be addressing positive stereotypes and essentializing narratives we should also be using an intersectional lens to study biases and harms because there's a lot of things that might be overlooked if we don't do that and finally there should really be increased transparency about bias mitigation methods because for instance like these positive stereotypes we don't know if it's because there's some sort of like weird overly excessive value alignment going on or maybe some other like anti-stereotyping methods that are resulting in these pernicious patterns we just really can't make any assumptions or really study that further without more transparency thank you so much for listening um have a good time at acl</sample>
    <sample id="38">this talk is about the dependency structure of coordination as you may know there are different dependency structures assumed by different theories and corpus approaches so for example in universal dependencies the structure of the coordination lisa bart and maggie is such that the first conjunct is the head of the whole coordination structure so in this case lisa bart and maggie however this effect may be ameliorated when the direct object is very heavy and very long because then it can be moved to the position after the adjunct this is illustrated here so both these sentences are fine marge read this absolutely fascinating book about bees so the reasoning here is that this is possible because even though this sentence violates the general grammatical principle that the direct object should be next to the verb it satisfies the principle of dependency length minimization which says that shorter dependencies are preferred so these two trees only show the length of the crucial dependencies so the ones that are not constant among these two structures so here we have the dependency from red to the adjunct of length 7 measured in words and from red to book of length 4 so together it's 11 when you move when you swap these two constituents the sum of these two dependencies becomes 6 right so instead of 11 6 much shorter that's why this sounds quite okay right it violates one principle but it satisfies another one okay so what we did we extracted various statistics from about coordination from the enhanced version of the penn treebank and see the paper why we didn't use universal dependencies and these statistics confirmed the observation made many times before that left conjuncts tend to be shorter so salt and pepper and not pepper and salt measured in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor</sample>
    <sample id="39">this talk is about the dependency structure of coordination as you may know there are different dependency structures assumed by different theories and corpus approaches so for example in universal dependencies the structure of the coordination lisa bart and maggie is such that the first conjunct is the head of the whole coordination structure so in this case lisa bart and maggie however this effect may be ameliorated when the direct object is very heavy and very long because then it can be moved to the position after the adjunct this is illustrated here so both these sentences are fine marge read this absolutely fascinating book about bees so the reasoning here is that this is possible because even though this sentence violates the general grammatical principle that the direct object should be next to the verb it satisfies the principle of dependency length minimization which says that shorter dependencies are preferred so these two trees only show the length of the crucial dependencies so the ones that are not constant among these two structures so here we have the dependency from red to the adjunct of length 7 measured in words and from red to book of length 4 so together it's 11 when you move when you swap these two constituents the sum of these two dependencies becomes 6 right so instead of 11 6 much shorter that's why this sounds quite okay right it violates one principle but it satisfies another one okay so what we did we extracted various statistics from about coordination from the enhanced version of the penn treebank and see the paper why we didn't use universal dependencies and these statistics confirmed the observation made many times before that left conjuncts tend to be shorter so salt and pepper and not pepper and salt measured in syllables also the observation that was made in passing that this tendency grows with the length of the length difference so when the difference between the lengths of the two conjuncts grows the shorter conjunct prefers to be the first one stronger right so the proportion is bigger of the left uh short conjuncts but what's novel in this paper is that we observed that this tendency only occurs when the governor on the left is absent right so the governor on the left in this example lisa bart and lisa so it's the governor is on the left this tendency disappears and we show that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination sentences but when the governor is on the right this tendency disappears and we show in the paper how this provides an argument against asymmetric structures of coordination as these two and for the symmetric structures as these two so see the paper for the full agreement and arguments sorry and talk to us about the poster session thank you</sample>
    <sample id="40">hello my name is vasudha and i am a computer science phd candidate at stony brook university i would like to present our work accepted into acl 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge we begin by defining cognitive dissonance and why it is an important problem to study in language simply put cognitive dissonance is two beliefs or actions that are inconsistent such as this example where a person states i know that cigarettes could kill me and then goes on to say i grabbed a couple of smokes after the meeting this belief and action are inconsistent and they are in dissonance further mentioning that i don't think i could keep my job without them justifies the second occurrence and they have a dissonance relationship since the initial model was not able to capture the dissonance class at all we start the active learning process by transferring weights from closely related tasks topic independent dissonance stance classification a task that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic called debate here and on binary classification of expansion and comparison classes of prt since these two are closely related to the conception of consonance and dissonance and we call them cee here we find that on transferring the zero-shot performance on the annotated dataset is already much better than chance with the best with auc 062 further on iterative fine-tuning on both tasks we find that fine-tuning of cee task followed by further fine-tuning on debate yields a much better zero-shot performance thus this is the model that we use to cold start the active learning process feel free to get in touch with us if you have any questions thank you</sample>
    <sample id="41">hi this is suling coming from the natural language processing lab at apfr university now i'm going to introduce our work of peacock personal common sense knowledge graph peacock contains large-scale personal common sense knowledge graph and large-scale pre-trained language model to learn and generalize personal common sense knowledge then we explore whether peacock knowledge can be used to improve downstream narrative modeling we investigate a personal ground data dialogue generation task on the call of ai 2 persona chat dataset specifically we use the knowledge linker to retrieve facts from peacock that are relevant to each speaker's original personal profile and utterances then we convert the retrieved facts into natural language statements to augment each speaker's profile we choose the bert-based model as our baseline dialogue system human evaluation shows that peacock augmented model achieves better dialogue generation on various aspects including fluency consistency and engagement and personal expression by comparing to the atomic 2020 knowledge graph we also find that peacock's personal-centric common sense knowledge yields a more positive impact compared to general social common sense knowledge then we stratify our human evaluation results based on the overlap of the two speakers augmented peacock knowledge where we find that in terms of dialogue consistency and engagement the winning rates of peacock augmented model increase as the number of shared common attributes between two speakers becomes larger since more consistent connections between speakers lead to more consistent and engaging conversations this highlights the importance of learning peacock's interconnected world personal knowledge in narrative modeling in summary we propose a world-level personal common sense knowledge graph peacock that contains large-scale high-quality personal influences our knowledge resource can be used to train reliable personal knowledge generators and also enable more consistent and engaging narrative modeling our paper and github site for this work are public now which can also be found on our lab website</sample>
    <sample id="42">hello everyone my name is xuhong today i'm going to present our paper do conno 2003 named entity taggers still work in 2023 and last but not least we calculated the percentage change in f1 to assess the generalization of each model so what is needed for good generalization through our experiments we found that from the graph on the right the red best fit line has a gradient that is greater than one this means that every unit of improvement that we made on conno 2003 translates to more than one unit improvement on conno which means that there is no diminishing returns and this shows us that adaptive overfitting in this case is not observed so what about temporal drift then for temporal drift we didn't experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift and kind of surprisingly it is not caused by adaptive overfitting even though conno 2003 has been used for over 20 years so going back to the question that we raised in the title of our paper do conno 2003 taggers still work in 2023 and we found that the answer is actually a resounding yes we hope our paper calls for more research on how to improve generalization of the models and lastly please make sure to check out our paper our data set and if you have any questions feel free to contact me thank you so much</sample>
    <sample id="43">hello my name is vasudha and i'm a computer science phd candidate at stony brook university i would like to present our work accepted into acl 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge we begin by defining cognitive dissonance and why it is an important problem to study in language simply put cognitive dissonance is two beliefs or actions that are inconsistent such as this example where a person states i know that cigarettes could kill me and then goes on to say i grabbed a couple of smokes after the meeting this belief and action are inconsistent and they are in dissonance further mentioning that i don't think i could keep my job without them justifies the second occurrence and they have a dissonance relationship since the initial model was not able to capture the dissonance class at all we start the cold start the active learning process by transferring weights from closely related tasks topic independent dissonance stance classification a task that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic we call them ce here we find that on transferring the zero-shot performance on the annotated dataset is already much better than chance with the best with a uc of 075 which is the best performance that we have on the task so far we also check the feasibility of each strategy for annotation quality and costs to annotators we find that prc has the highest percentage of dissonance and works best for rare class however the annotators also find the examples difficult in summary we find that prc is a simple ael strategy for rare class acquisition and cold starting ael with appropriately designed transfer learning tasks can help significantly we also find that iterative update is useful for transfer learning from a different domain whereas in-domain active annotations benefit from cumulative update these are the links to our code dataset and our paper feel free to get in touch with us if you have any questions thank you</sample>
    <sample id="44">hi everyone i'm jenny a first year phd student at carnegie mellon university and today i'll be presenting our work entitled positionality characterizing design bias in ai datasets and models this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work</sample>
    <sample id="45">hi i'm myra and today i'll be talking about our paper marked personas using natural language prompts to measure stereotypes in language models this work is done in collaboration with essem dermanush and dan juravsky in recent years many have documented the prevalence of social bias and stereotypes in large language models however these measures have various limitations they usually rely on hand-constructed datasets that are very time-consuming to curate and they also usually only measure very specific stereotypes meaning that they don't generalize well to other demographics or contexts or they simply capture very general broad associations like negative associations with particular groups so in our work we first generate personas using prompts like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so</sample>
    <sample id="46">hello my name is kyo yin and i will be presenting our work titled when does translation require context a data-driven multi-lingual exploration this work was done in collaboration with patrick fernandez andrew f d martin and graham neubig so a lot of translations depend on context for example how would we translate mole in this sentence well if the previous sentence was things could start to get dangerous if the ministers find out then mole refers to a spy but if the previous sentence was could it be anything serious doctor then mole refers to a birthmark so depending on context the meaning of the word changes and therefore its translation changes as well however evaluating how well models can handle these cases is pretty hard firstly because only a small portion of translations depend on context which makes corpus-level metrics like ble unable to capture these translations and secondly because these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation in this work we try to answer these two questions firstly when does translation require context and secondly how well do models handle these cases to answer the first question we start by measuring how much a word depends on context in translation and in the previous work we introduced cxmi as a measure for context usage by machine translation models and this is done by measuring how much information the context c provides about the target y given the source x and we call our tagger the multi-lingual discourse aware or muda tagger we can then also note that different languages have different proportions of these discourse phenomena we then use the muda tagger by applying the tagger on the parallel corpus that we want to use for evaluation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level</sample>
    <sample id="47">hi i'm jianbin phd student at university of washington today i'm presenting our work from pre-training data to language models to downstream tasks tracking the trails of political biases leading to unfair nlp models so language models are trained on large-scale web crawl data political news media are well covered in their pre-training data according to a survey of the c4 corpus we can see that new york times los angeles times the guardian huffington post etc are well covered in language model training data this has created a mixed blessing for language model applications so on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas on the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream tasks ultimately creating fairness issues if we do try to sanitize somehow we would also risk censorship or exclusion and it's incredibly hard to determine what is actually neutral and should be retaining language model training data so it's kind of like the electric cholli problem okay great i think that's pretty much all i have for today thank you for your time</sample>
    <sample id="48">根据所给英文内容，这篇论文有多少位作者？</sample>
    <sample id="49">hi everyone i'm kostas michrakis and i'm pleased to welcome you to our talk of our acl 2023 paper language model acceptability judgments are not always robust to context this is a joint work with john gauthier aaron muller kanishka misra gareth flinders roger levy and edina williams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like blimp syntax gem and acceptability in terms of stereotypes such as cross pairs and in this a minimal pair paradigm the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an unacceptable sentence or an ungrammatical sentence and then the hope is that the model basically puts more probability to the acceptable sentence or the unacceptable sentence depending on whether the chosen prefix is acceptable or unacceptable now this and this is very large like this effect increases throughout the context length and this would probably affect like newer language models which has large context window so why does the match prefix affect the language model judgment so much so we did a series of analysis where we tried to like perturb the input sentence by trying to preserve the relevant structure but adding like noise to the input and after doing like several of these perturbations we find that none of these noises are actually making the model like change its course in terms of how it shows us the mpp judgment trend basically we find that the models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences and the mpp evaluation the way that we do it currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context window please read our paper for more details of our experiments thank you for listening</sample>
    <sample id="50">hi welcome to our presentation of deplain a new corpus for german text simplification on the document level and on the sentence level my name is regine stodden and i will guide you through the first part of the presentation let's first define text simplification text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group as people with reading problems or non-native speakers to train a text simplification model we require parallel pairs of text for example documents or sentences and the example here you can see a parallel aligned sentence pair of a complex german text and its translation into plain language to simplify the sentence pairs a little bit more so for example on the type of simplification structural simplification also overall level of simplification and now as we have our dataset deplain which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods and we did some adaptations to the proposed methods and we have published all these adaptations and the codes to run our experiments in the paper uh we concluded that this this basic fine-tuning could produce uh or could get uh scores better than the baseline scores and we propose those results as a benchmark a baseline benchmark for the problem of automatic text simplification in the future thank you so much for your attention and we hope to meet all of you uh during the conference thank you</sample>
    <sample id="51">our dataset covers three different domains music books and recipes our dataset collection methodology emphasizes informality using a cartoon completion setup the cartoon has three speech bubbles in the first bubble bob says remember that song we were listening to yesterday and with that bob sets the dialogue context in the second speech bubble alice says do you mean easy on me or i got a feeling which is the alternative question and in the third speech bubble bob uses an indirect referring expression to select one of these entities for example the newer one we provide the first and second speech bubbles automatically but the third one is filled in by the annotators to listen to at least some of each song and read about each song here's for example the google search result for the song easy on me for the recipes and books domain we show some background text from wikipedia for recipes we additionally show their images again from wikipedia so the entity corpus has 6000 alternative questions across three domains and it has 42000 indirect referring expressions results with t5x large model are summarized below if the language model has access to the exact same background knowledge as the annotators the entity accuracy is really high it's around 92 to 95 percent which is more realistic for example when the language model retrieves the background knowledge if the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 percent which is more realistic for example when the language model retrieves the background knowledge if the language model has access to only entity names then the accuracy is only 60 so there's a lot of room for improvement we've also shown that the models are domain generalizable here is a link to our dataset thanks for watching</sample>
    <sample id="52">hi everyone i'm jenny a first year phd student at carnegie mellon university and today i'll be presenting our work entitled positionality characterizing design bias in ai datasets and models this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work</sample>
    <sample id="53">演讲者的名字是什么？演讲者是谁？演讲者的名字是什么？演讲者是谁？演讲者的名字是什么？演讲者的名字是什么？演讲者的名字是什么？演讲者的名字是什么？演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲者的名字是什么演讲</sample>
    <sample id="54">hello my name is vasudha and i am a computer science phd candidate at stony brook university i would like to present our work accepted into acl 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge we begin by defining cognitive dissonance and why it is an important problem to study in language simply put cognitive dissonance is two beliefs or actions that are inconsistent such as this example where a person states i know that cigarettes could kill me and then goes on to say i grabbed a couple of smokes after the meeting this belief and action are inconsistent and they are in dissonance further mentioning that i don't think i could keep my job without them justifies the second occurrence and they have a consonance relationship and we call them ce here we find that on transferring the zero shot performance on the annotated data set is already much better than chance with the best with al we compare this to the other state-of-the-art al strategies that are commonly used in the community we find that prc is a simple al strategy for rare class acquisition and cold starting al with appropriately designed transfer learning tasks can help significantly we also find that iterative update is useful for transfer learning from a different domain whereas in-domain active annotations benefit from cumulative update these are the links to our code data set and our paper feel free to get in touch with us if you have any questions thank you</sample>
    <sample id="55">hi i'm sarah pappi from the university of toronto and vlad seleny bruno kessler and i will briefly introduce the attention as a guide for simultaneous speech translation paper that is a joint work with macdonald negri and marco turci and what is simultaneous speech translation simultaneous speech translation or simulst is the process of translating spoken language into text in another language in real time enabling cross language communication and what are the problems of the current simulst models specific architectures are usually trained introducing additional modules to be optimized long and complicated training procedures for example training involving different optimization objectives and training and maintaining several models to reach different latency regimes for example training a model with an average of one second latency and another one with two seconds latency and so on so what is our solution first to use already existing offline simulst models without retraining or adopting specific architecture for simulst use only one model for every latency regime and handle latency through specific specific parameters and leverage the knowledge already acquired by the model through the tension mechanism between audio input and textual output that is the cross attention mechanism and you can see an example on the right our solution is to propose a dot or encoder-decoder attention and it is a strategy for which we decide whether to emit or not a partial translation based on where attention points to and what is emitted if the tension is not concentrated that is if the alpha towards the last lambda speech frames meaning that the received information is unstable for example if we receive a speech chunk containing i'm going to talk about and our model predicts other three words and we will look at the cross attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we look at the main results of the dot we will plot the simultaneous speech translation results on graphs in which we have blue on one side that measures the translation quality and average lagging that is the latency measure and we also consider the computational aware lagging that accounts for the model's computational time to predict the output so we want our curves to be as high as possible on this plot but also we want that they are shifted on the left and we also see that if we consider the actual elapse time or the computational aware time that is the fastest strategy if you want to discover more results read our paper and we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work thanks for your attention</sample>
    <sample id="56">hello everyone my name is xin jian from the penn state university today i'm going to present our work exemplar cross-lingual semantic parsing in multiple natural languages and many representations so semantic parsing is the task to build semantic representations of user queries such as sql and lambda calculus and cross-lingual semantic parsing is the task to translate queries in multiple natural languages into multiple meaning representations as shown in this figure we need to translate the query in multiple natural languages using neural models to sql lambda or funql and etc and we also find many interesting results so regarding analyses of monolingual models we evaluate on two groups of models including encoder pdr which stands for multi-lingual pre-trained encoders with pointer-based decoders such as xlmr plus pdr and bert plus pdr and we also evaluate encoder-decoder models which is multi-lingual pre-trained encoder-decoder models such as mbert and mt5 we found that encoder-decoder obtains the best performance on all nine data sets and only gains in three data sets i think this is known as the curse of multi-linguality we also compare the cross-lingual performance gap in this figure the blue line is cross-lingual fused transfer the orange line is cross-lingual zero-shot transfer while the green line is the monolingual setting we found that by comparing the green and orange line we found that for zero-shot setting the cross-lingual transfer performance gap is significant and by comparing blue and orange line we found that for few-shot setting the transfer gap is shortened rapidly we also find some other interesting findings for example um encoder-decoder outperforms previous work or achieved comparable results for training on english natural language and significantly boost the performance of few-shot on target natural languages and we found monolingual language models such as coders and blue are still inferior for cross-lingual semantic parsing tasks um to sum up we build exemplar a unified benchmark for cross-lingual semantic parsing with multiple natural languages and many representations we conduct a comprehensive benchmark study on three representative of types of multi-lingual language models and our result shows many interesting findings and etc and welcome to visit our paper and code thanks for listening</sample>
    <sample id="57">hello everyone i'm makshatta and today my co-author martin and i are presenting our work the kitmos task evaluating knowledge integration from multiple sources this work is a collaboration between mcgill university niela and microsoft research in this work we propose a diagnostic test suite for knowledge integration we introduce a co-reference resolution task designed to probe for the ability to draw on knowledge available in different sources we evaluate the dataset with human study participants and established co-reference resolution models in this figure we show the results of the best-performing models on the most difficult variant of the background pre-trained setting without task-specific training on kitmos both models do not perform well when trained on kitmos however both c2 and bert-coref perform significantly better than the random choice this suggests that when trained on task-specific co-reference resolution datasets models learn to exploit surface cues which are not useful when testing on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge presented only at inference time to summarize the main takeaways of our paper many co-reference resolution models appear unable to reason over knowledge from different sources without task-specific training however with task-specific training some models successfully integrate knowledge from multiple sources still even the best-performing models seem to have difficulties with reliably integrating background knowledge presented only at inference time if you're interested in more details please see our paper and check out the dataset and code on github thanks for listening</sample>
    <sample id="58">hello everyone i'm makshatta and today my co-author martin and i are presenting our work the kitmus task evaluating knowledge integration from multiple sources this work is a collaboration between mcgill university niela and microsoft research kitmus is a diagnostic test suite for knowledge integration from multiple sources we have defined three settings of kitmus first we have the background pre-trained setting where background knowledge is assumed to be available at pre-training time second there is the background both setting where background knowledge is available both at pre-training time and at inference time lastly the background inference setting where background knowledge is available only at inference time to summarize the main takeaways of our paper many co-reference resolution models appear unable to reason over knowledge from different sources without task-specific training however with task-specific training some models successfully integrate knowledge from multiple sources still even the best-performing models seem to have difficulties with reliably integrating background knowledge presented only at inference time if you're interested in more details please see our paper and check out the dataset and code on github thanks for listening</sample>
    <sample id="59">hi i am yannis lavrac and i will present you our work on dr bert a robust pre-trained model in french for biomedical and clinical domain in this presentation we first talk about language modeling in healthcare then we present the main contribution of our article we introduce the first bert model in french named dr bert which is based on anonymized data obtained from the non-university hospital that we house afterward we ask ourselves how much data do we need to train a specialized model on french data is it four gigabytes eight gigabytes or more to answer this question we first train and compare four from scratch models a first version of dr bert with 7 gigabytes of natschos a second version of 4 gigabytes of natschos a first version of schubert which is a clinical model with 4 gigabytes of clinical notes and finally one based on the english biomedical model bert and trained on 4 gigabytes of natschos show comparable results to those obtained with dr bert 4 gigabytes from scratch which is not the case for the model based on camanber weights and tokenizer which suffer from stability issues finally as a conclusion our proper system offers better performance on 9 of the 11 down-stream tasks and surpass globally the result of the generic model here camanber we also observe that specialize data is better more specialized data is better but it doesn't scale well all the pre-trained models obtained from natschos are freely available on yugenface and all the training script are on our github repository so thank you for for for this presentation and we are looking forward to exchanging at the poster session in toronto</sample>
    <sample id="60">这篇论文的作者所属机构是什么？ 这篇论文的作者所属机构是什么？</sample>
    <sample id="61">最后一个研究问题是什么？</sample>
    <sample id="62">hello my name is natarajan rajendran and i'm the main author of this acl paper a systematic study of knowledge distillation for natural language generation this is a pseudotarget training and this is a fantastic collaboration with amir and the super four me at microsoft and my phd advisor ravi so as we all know natural language generation systems or energy systems are based on large language models and they become larger more complex and much more slower which can also come with great financial cost as a result there is a growing demand in the industry for compressing these models and the main problem is that while you want to compress your large model you also want to preserve its performance and this is exactly the goal of this paper we want to explore the potential of energy compression or in other words the finding the recipe for task specific knowledge distillation for energy and we consider a variety of energy tasks in realistic setups and what are exactly realistic setups or as i like to call them industry driven setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course i would like to see you next to my poster and we will discuss this paper so finally i can't leave you without the recipe for distillation in energy and for more details about the study the methods and the first exposure bias motivation we provide in knowledge distillation setups you can scan the qr code at the first slide or read the paper and of course</sample>
    <sample id="63">指标灵敏度是如何工作的？ 指标灵敏度是衡量模型在不同任务上的表现的指标。 指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指标灵敏度是衡量模型在不同任务上的表现的指标指</sample>
    <sample id="64">演讲者的名字是什么？演讲者的名字是jingwei yi from the university of science and technology of china it's my pleasure to give a short advertisement video of our paper i will copy my model protecting the copyright of embedding as services will backdoor watermark let's first introduce the background about embedding as services currently large language models such as t5 lama palm are exceptional in natural language understanding and generation embedding as services is one of the services built upon large language models to assist the various nlp tasks for example openai offers a t5 based embedding as service to detect whether another service contains the watermark the watermark method needs to meet the following properties first the method should be applicable to embedding as services second the watermark should not degrade the utility of the provided embedding as services third the watermark should be covert enough to the attacker or the attacker can remove the watermark easily finally the watermark needs to be transferable to the attacker's services during the model extraction process existing works can be broadly classified into four categories however these methods either not applicable to embedding as services or lack of transferability therefore in this paper we propose embedding marker which is a backdoor-based watermark method applicable to embedding as services now let me introduce the details of our embedding marker embedding marker contains two main steps watermark injection and copyright verification before these main steps we first select a trigger set the trigger set is a group of words in a moderate frequency interval we assume the provider can collect a general text corpus and count the word frequency with it the legends of the figures means the number of triggers in each sentence as shown in the figures it's hard to distinguish between the backdoored embeddings and normal embeddings that's all thank you welcome to discuss with us</sample>
    <sample id="65">hello everyone my name is ying and my colleague yan and i will be presenting our research on multi-instruction improving multi-modal zero-shot learning via instruction tuning so with the advances in large language models many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data efficient way recently many studies have shown that instruction tuning enables large language models to perform on unseen multi-modal tasks in a zero-shot manner by following natural instructions however most previous works on instruction tuning focus on improving the zero-shot performance on language-only tasks while computer vision and multi-modal tasks have been left out therefore in this work we want to investigate whether instruction tuning on multi-modal pre-trained models can actually improve the zero-shot capability of pre-trained models thank you</sample>
    <sample id="66">hello everyone we are glad to share our study paper deep learning for mathematical reasoning mathematical reasoning is a fundamental aspect of human intelligence that enables us to comprehend and make decisions based on numeric data and language the development of machines capable of solving math problems and proving theorems has been a long-standing focus of ai and elp in recent years there has been a surge of interest in this area so our study discusses the task of mathematical reasoning and the development of a deep learning method for it in the last few years we have witnessed the remarkable development of pre-trained language models such as large language models llms these language models have demonstrated remarkable performance on a wide range of elp tasks we can also apply llms to solve math problems for example given an input question here we can prompt the llms with one single example of the chain thought reasoning process a chain thought is a series of intermediate reasoning steps that lead to the final output it enables llms to solve complex problems by guiding them to produce a sequence of steps before giving the final answer despite the advantages of llms still faces inherent limitations one notable example is the lack of ability to perform precise mathematical reasoning despite the creation of various datasets mathematical reasoning in low-resource settings remains underexplored recently there has been attempts to build non-english datasets for chinese korean and arabic additionally pioneering research has developed mathematical reasoning benchmarks for financial scientific and medical domains despite impressive progress deep learning models commonly display generalization and robustness failures on reasoning tasks first llms struggle with large numbers second large language models are inconsistent with mathematical reasoning with that thank you so much for your attention</sample>
    <sample id="67">hi i'm uri and today i want to talk to you about interference in multilingual translation models these models can benefit from synergy between different language pairs or suffer from interference for example training to translate english to finnish may improve the quality of english-estonian while english-to-chinese might have a negative effect in this work we identify the main factors that contribute to interference or synergy we find that severe interference happens when the model is very small compared to the data size and that tuning the sampling temperature is key for strong performance so what is the best way of controlling the trade-offs the simplest solution is temperature sampling when t greater than 1 allows to sample more training examples from lower resource languages the most common value used is 5 often without calibration based on this we can say that a baseline for battling interference is weak due to size in the small models and weak due to uncalibrated temperature for larger ones that use values that are too high the lesson here is that tune temperature is key for strong performance to conclude we find that model and data size affect the levels of interference in multilingual translation while other factors like language similarity affect much less and that modest scale and tune temperature can reduce the problem significantly without any other specialized method thank you</sample>
    <sample id="68">在预训练期间，模型会接收什么样的语言上下文？在预训练期间，模型会接收什么样的语言上下文？在预训练期间，模型会接收什么样的语言上下文？在预训练期间，模型会接收什么样的语言上下文？在预训练期间，模型会接收什么样的语言上下文？在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。在预训练期间，模型会接收什么样的语言上下文。</sample>
    <sample id="69">在 wsl 中，通常需要多少个干净的验证样本才能获得良好的表现？</sample>
    <sample id="70">hi i'm myra and today i'll be talking about our paper marked personas using natural language prompts to measure stereotypes in language models this work is done in collaboration with essem dermanush and dan jurafsky in recent years many have documented the prevalence of social bias and stereotypes in large language models however these measures have various limitations they usually rely on hand-constructed datasets that are very time-consuming to curate and they also usually only measure very specific stereotypes meaning that they don't generalize well to other demographics or contexts or they simply capture very general broad associations like negative associations with particular groups so when people are describing a woman they'll usually actually specify woman and mark the term with woman and more broadly dominant groups in society are both linguistically and socially unmarked while the marginalized groups are usually marked so in our method we first designate what the unmarked and marked groups are and then we compare the personas using the fighting words method which is basically using weighted log odds ratios to distinguish the top words for each marked group and for black women we see that some of the top words are things like strong and resilient this connects to an archetype that people have called the strong black woman archetype and while it sounds like positive at first glance there's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these people to be resilient and strong against societal obstacles so rather than actually working towards changing those obstacles it puts pressure on those people to overcome them which leads to very negative health outcomes for these people among other harms more broadly we find that the words for each marked group pretty much just reflect very essentializing narratives so based on these patterns we conclude with three recommendations for model owners first we should as researchers be addressing positive stereotypes and essentializing narratives we should also be using an intersectional lens to study biases and harms because there's a lot of things that might be overlooked if we don't do that and finally there should really be increased transparency about bias mitigation methods because for instance like these positive stereotypes we don't know if it's because there's some sort of like weird overly excessive value alignment going on or maybe some other like anti-stereotyping methods that are resulting in these pernicious patterns we just really can't make any assumptions or really study that further without more transparency thank you so much for listening have a good time at acl</sample>
    <sample id="71">hi i'm going to talk about our work on resolving indirect referring expressions for entity selection our goal is to understand users' language when they want to make a choice now consider this alternative question did you mean easy on me or i got a feeling which is the alternative question and in the third speech bubble bob says remember that song we were listening to yesterday and with that bob sets the dialogue context in the second speech bubble alice says do you mean easy on me or i got a feeling which is the alternative question and in the third speech bubble bob uses an indirect referring expression to select one of these entities for example the one without words not the one with the 12 year old and 12 year old boy or the fictional one or comes from other worlds and so on the ltd corpus has 6000 alternative questions across three domains and it has 42000 indirect referring expressions results with t5x large model are summarized below if the language model has access to the exact same background knowledge as the annotators the accuracy is really high it's around 92 to 95 percent which is more realistic for example when the language model retrieves the background knowledge if the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 percent which is more realistic for example when the language model has access to only entity names then the accuracy is only 60 so there's a lot of room for improvement we've also shown that the models are domain generalizable here is a link to our dataset thanks for watching</sample>
    <sample id="72">hi i'm jianbin phd student at university of washington today i'm presenting our work from pre-training data to language models to downstream tasks tracking the trails of political biases leading to unfair nlp models so language models are trained on large-scale web crawl data political news media are well covered in their pre-training data according to a survey of the c4 corpus we can see that new york times los angeles times the guardian huffington post etc are well covered in language model training data this has created a mixed blessing for language model applications so on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas on the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream tasks ultimately creating fairness issues if we do try to sanitize somehow we would also risk censorship or exclusion and it's incredibly hard to determine what is actually neutral and should be retaining language model training data so it's kind of like the electric chollie problem okay great i think that's pretty much all i have for today thank you for your time</sample>
    <sample id="73">演讲者的名字是什么？演讲者是谁？演讲者是谁的演讲者是谁的演讲者是谁的演讲者是谁的演讲者是谁的演讲者是谁的演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者 is</sample>
    <sample id="74">hello everyone today i will use our paper entitled atomic towards the construction of a large-scale commonsense knowledge base to introduce our research on the construction of a large-scale commonsense knowledge base we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-scale commonsense knowledge base that can be used by both humans and machines we propose a method for constructing a large-</sample>
    <sample id="75">hi my name is jian dan today i'm very pleased to present our work joint prop this is a joint work with my friend hao anran and my supervisor lu anrui for the joint prop framework consists of four parts span feature generation heterogeneous graph construction and joint label propagation and finally model optimization we obtain the converged pseudo-label and we use the soft-mask function followed by a standard hard-mask operation to determine the pseudo-labels we filter those of lower quality with the confidence g and combine the rest of the confidence above the threshold with the label data to retrain the classification model the retraining model remains the same as the baseline model as does the joint task classification function finally the experiment part we conducted our experiment on four datasets there are joint task datasets and single task datasets there is no previous baseline on semi-supervised joint task so comparisons are only made with base model performance as we can see the experiments tables the joint learning of two tasks benefit from the codependency between the two tasks in joint datasets for single task datasets our framework shows significant and consistent improvement over all baselines both for ner and relation tasks thank you very much for your attention</sample>
    <sample id="76">hi i'm jianbin phd student at university of washington today i'm presenting our work from pre-training data to language models to downstream tasks tracking the trails of political biases leading to unfair nlp models so language models are trained on large-scale web crawl data political news media are well covered in their pre-training data according to a survey of the c4 corpus we can see that new york times los angeles times the guardian huffington post etc are well covered in language model training data this has created a mixed blessing for language model applications so on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas on the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream tasks ultimately creating fairness issues if we do try to sanitize somehow we would also risk censorship or exclusion and it's incredibly hard to determine what is actually neutral and should be retaining language model training data so it's kind of like the electric cholli problem okay great i think that's pretty much all i have for today thank you for your time</sample>
    <sample id="77">this video is for sharing our work on improving standardization factual consistency from natural language feedback this is the joint work from yale university and microsoft research and most of the work was done when the first author was an intern at microsoft research in this work we introduce a new dataset de facto which contains human demonstrations and feedback for improving standardization factual consistency for this dataset we provide comprehensive analysis and we offer further insights into the factual consistency of the existing standardization models we ask the annotators to provide labels to decide whether the summary is factual consistent and we require them to provide human corrected factual consistent summaries if they think the original summary is not correct also we require them to provide human feedback which contains the instructions explanation and evidence in this slide we show the data distribution of the annotated editing instructions and their relation with the different error types the third task is to automatically correct factual errors while generating the corresponding explanation we found that the edit model can achieve comparable performance compared with the baseline models while trained on much fewer data and training the model to generate the explanation can help the model to achieve better performance apart from providing a testbed for the proposed nlg tasks over de facto also has other advantages thanks to its fine-grained annotations which can be valuable for training factuality metrics and factuality model evaluation we have released our collected de facto dataset on github and please check our paper for more details thank you for listening</sample>
    <sample id="78">hi welcome to our presentation of deplain a new corpus for german text simplification on the document level and on the sentence level my name is regine stodden and i will guide you through the first part of the presentation let's first define text simplification text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group as people with reading problems or non-native speakers to train a text simplification model we require parallel pairs of text for example documents or sentences and the example here you can see a parallel aligned sentence pair of a complex german text and its translation into plain language to simplify the sentence pairs a little bit more so for example on the type of simplification structural simplification also overall level of simplification and now as we have our dataset deplain which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods and we did some adaptations to the proposed methods and we have published all these adaptations and the codes to run our experiments in the paper the second use case that we showed in our paper is the case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text we have fine-tuned two different models we have fine-tuned the model of long import to produce document level simplifications and we also fine-tuned the normal base long the normal base import to produce sentence level simplifications you can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper we concluded that this basic fine-tuning could produce or could get scores better than the baseline scores and we propose those results as a base benchmark for the problem of automatic text simplification in the future thank you so much for your attention and we hope to meet all of you during the conference thank you</sample>
    <sample id="79">yes coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available coscript is publicly available</sample>
    <sample id="80">hello everyone my name is jingwei yi from the university of science and technology of china it's my pleasure to give a short advertisement video about our paper i will copy my model protecting the copyright of embedding as services via backdoor watermark let's first introduce the background about embedding as services currently large language models such as t5 lama palm are exceptional in natural language understanding and generation embedding as services is one of the services built upon large language models to assist the various nlp tasks for example openai offers a t5 based embedding as service however recent works have shown that the attacker may steal the model through learning from the embedding as services therefore it's necessary to protect the copyright of embedding as services to protect the copyright of embedding as services one of the solutions is to embed a watermark in the provider's service and detect whether another service contains the watermark the watermark method needs to meet the following properties first the method should be applicable to embedding as services second the watermark should not degrade the utility of the provided embedding as services third the watermark should be covert enough to the attacker or the attacker can remove the watermark easily finally the watermark needs to be transferable to the attacker's services during the model extraction process existing works can be broadly classified into four categories however these methods either are not applicable to embedding as services or lack transferability therefore in this paper we propose a backdoor-based watermark method applicable to embedding as services now let me introduce the details of our embedding as service watermark the watermark method contains two main steps watermark injection and copyright verification before these main steps we first select a trigger set the trigger set is a group of words in a moderate frequency interval we assume the provider can collect a general text corpus and count the word frequency with it the legends of the figures means the number of triggers in each sentence as shown in the figures it's hard to distinguish between the backdoored embeddings and normal embeddings that's all thank you welcome to discuss with us</sample>
    <sample id="81">hello everyone my name is xin jian from the penn state university today i'm going to present our work exemplar cross-lingual semantic parsing in multiple natural languages and many representations so semantic parsing is the task to build semantic representations of user queries such as sql and lambda calculus and cross-lingual semantic parsing is the task to translate queries in multiple natural languages into multiple meaning representations as shown in this figure we need to translate the query in multiple natural languages using neural models to sql lambda or funql and etc and we also find many interesting results so regarding analyses of monolingual models we evaluate on two groups of models including encoder pdr which stands for multi-lingual pre-trained encoders with pointer-based decoders such as xlmr plus pdr and bert plus pdr and we also evaluate encoder-decoder models which is multi-lingual pre-trained encoder-decoder models such as mbert and mt5 we found that encoder-decoder obtains the best performance on all nine data sets and only gains in three data sets i think this is known as curves of multi-linguality we also compare the cross-lingual performance gap in this figure the blue line is cross-lingual fused transfer the orange line is cross-lingual zero-shot transfer while the green line is the monolingual setting we found that by comparing the green and orange line we found that for zero-shot setting the cross-lingual transfer performance gap is significant and by comparing blue and orange line we found that for few-shot setting the transfer gap is shortened rapidly we also find some other interesting findings for example um encoder-decoder outperforms previous work or achieved comparable results for training on english natural language and significantly boost the performance of few-shot on target natural languages and we found monolingual language models such as coders and blue are still inferior for cross-lingual semantic parsing tasks um to sum up we build exemplar a unified benchmark for cross-lingual semantic parsing with multiple natural languages and many representations we conduct a comprehensive benchmark study on three representative of types of multi-lingual language models and our result shows many interesting findings and etc and welcome to visit our paper and code thanks for listening</sample>
    <sample id="82">hi everyone this is a video about our work which is titled aggregating multi-heuristic quality signals for unsupervised automated essay scoring automated essay scoring or aess for short aims to score the writing quality of essays without human intervention which is an important application of natural language processing in education state-of-the-art aess models are typically trained in a supervised way with large labeled corpora comprising essays and their ground truth scores however collecting labeled essays is time-consuming and labor-intensive especially for essays written specifically to new prompts and when there is no professional scoring staff available unsupervised aess by learning from rank aggregation or urla for short the core idea of our urla is to introduce multiple heuristic quality signals as a pseudo ground truth and then train a neural aess model by learning from the rank aggregation of these quality signals specifically in the model inference stage considering that the essay scores predicted by the neural aess model may have a different range from the predefined score set we propose a scoring strategy to transform the predicted scores given by the neural aess model into the range of predefined score set through a minimum maximum transformation experimental results demonstrate that our urla outperforms all unsupervised baselines with a large improvement compared with the general supervised methods the performance of urla is still much lower than theirs due to the lack of strong supervision to sum up in this paper we aim to perform essay scoring under the unsupervised setting to this end we propose a novel urla framework to train a neural aess model by aggregating the partial order knowledge contained in multiple heuristic quality signals to address the conflicts among different signals and get a unified supervision we design a deep pairwise rank aggregation loss for model training experimental results demonstrate the effectiveness of urla for unsupervised essay scoring thanks</sample>
    <sample id="83">hello everyone my name is xin jian from the penn state university today i'm going to present our work exemplar cross-lingual semantic parsing in multiple natural languages and many representations so semantic parsing is the task to build semantic representations of user queries such as sql and lambda calculus and cross-lingual semantic parsing is the task to translate queries in multiple natural languages into multiple meaning representations as shown in this figure we need to translate the query in multiple natural languages using neural models to sql lambda or funql and etc and we also find many interesting results so regarding analyses of monolingual models we evaluate on two groups of models including encoder pdr which stands for multi-lingual pre-trained encoders with pointer-based decoders such as xlmr plus pdr and bert plus pdr and we also evaluate encoder-decoder models which is multi-lingual pre-trained encoder-decoder models such as mbert and mt5 we found that encoder-decoder obtains the best performance on all nine data sets and only gains in three data sets i think this is known as the curse of multi-linguality we also compare the cross-lingual performance gap in this figure the blue line is cross-lingual fused transfer the orange line is cross-lingual zero-shot transfer while the green line is the monolingual setting we found that by comparing the green and orange line we found that for zero-shot setting the cross-lingual transfer performance gap is significant and by comparing blue and orange line we found that for few-shot setting the transfer gap is shortened rapidly we also find some other interesting findings for example um encoder-decoder outperforms previous work or achieved comparable results for training on english natural language and significantly boost the performance of few-shot on target natural languages and we found monolingual language models such as coders and blue are still inferior for cross-lingual semantic parsing tasks um to sum up we build exemplar a unified benchmark for cross-lingual semantic parsing with multiple natural languages and many representations we conduct a comprehensive benchmark study on three representative of types of multi-lingual language models and our result shows many interesting findings and etc and welcome to visit our paper and code thanks for listening</sample>
    <sample id="84">hello everyone i'm shah here today i'm going to talk about my paper in iclr 2023 partnered dynamic networks first of all i want to talk about the background knowledge about dynamic networks and we can see that most of the traditional networks are static networks based on the input value the network computes it with the fixed or static parameters and the static parameters is very important as the constraint on the summation of the two scale factors is also very important in the accuracy of the different dynamic networks we also compare our methods with network pruning and from our performance is much uh is significantly uh better because we maintain the static dynamic parameters and we also found partnered can make the output more discriminating which contributes to the uh better performance compared to uh the static network also we think this are some future works deserved to be explored such as explore expand our method to other mechanism networks also try to extend our method to some hardware from the structure manner and and last but not least uh further introduce more modes such as the combination about zero elements static parameters and dynamic parameters uh that's all for our</sample>
    <sample id="85">受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个示例是什么 受限语言规划的一个</sample>
    <sample id="86">they ensure the privacy and security of their data by using encryption techniques such as homomorphic encryption and secure multi-party computation. these methods allow computations to be performed on encrypted data without decrypting it, ensuring that sensitive information remains protected throughout the process. in addition to encryption, they also employ techniques such as differential privacy to protect individual data points while still allowing for useful statistical analysis to be conducted on the dataset. by combining these approaches, they are able to provide strong privacy guarantees while still enabling useful data analysis and computation on encrypted data.</sample>
    <sample id="87">hi i'm yannis lavrac and i will present you our work on dr bert a robust pre-trained model in french for biomedical and clinical domain in this presentation we first talk about language modeling in healthcare then we present the main contribution of our article we introduce the first french biomedical model dr bert which is based on anonymized data obtained from the non-university hospital that we house afterward we ask ourselves how much data do we need to train a specialized model on french data is it 4 gigabytes 8 gigabytes or more to answer this question we first train and compare four from scratch models a first version of dr bert with 7 gigabytes of natsch's a second version of 4 gigabytes of natsch's a third version of schubert which is a clinical model with 4 gigabytes of clinical notes and finally one based on the english biomedical model permit bert trained on the 4 gigabytes of natsch's show comparable results to those obtained with dr bert 4 gigabytes from scratch which is not the case for the model based on common bear weights and tokenizer which suffer from stability issues finally as a conclusion our proper system offers better performance on 9 of the 11 down-stream tasks and surpass globally the result of the generic model here common bear we also observe that specialize data is better more specialized data is better but it doesn't scale well all the pre-trained models obtained from natsch's are freely available on yugenface and all the training script are on our github repository so thank you for for for this presentation and we are looking forward to exchanging at the poster session in toronto</sample>
    <sample id="88">hi everyone i'm jenny a first year phd student at carnegie mellon university and today i'll be presenting our work on positionality characterizing design bias in ai datasets and models this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was</sample>
    <sample id="89">演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识？演讲者在哪个示例句子上展示了模型如何利用注意力机制所学的知识</sample>
    <sample id="90">hello i'm hannah lee one of the authors of rethinking annotation can language learners contribute as language models advance data annotation gets essential in nlp it has been customary to recruit native speakers for data annotation and word meaning questions asking about the meaning of the words to check their language proficiency level after taking a pre-test participants were asked to solve 15 test questions with standardized test questions from official language tests and word meaning questions asking about the meaning of the words to check their language proficiency level in conclusion this paper questions the necessity of recruiting native speakers for data annotation and show that language learners could definitely contribute to nlp annotations we believe this work showed the possibility of broadening nlp research for many languages jumping over geographic and technological barriers to building benchmark data sets for low-resource languages where it is hard to recruit native speakers in conclusion this paper questions the necessity of recruiting native speakers for data annotation and show that language learners could definitely contribute to nlp annotations we believe this work showed the possibility of broadening nlp research for many languages jumping over geographic and technological barriers to building benchmark data sets for low-resource languages where it is hard to recruit native speakers thank you for listening please refer to our paper for more details such as how control variables affect annotation performances any questions are welcome via the email below</sample>
    <sample id="91">hello everyone my name is ying and my colleague yan and i will be presenting our research on multi-instruction improving multi-modal zero-shot learning via instruction tuning so with the advances in large language models many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data efficient way recently many studies have shown that instruction tuning enables large language models to perform on unseen multi-modal tasks in a zero-shot manner by following natural instructions however most previous works on instruction tuning focus on improving the zero-shot performance on language-only tasks while computer vision and multi-modal tasks have been left out therefore in this work we want to investigate whether instruction tuning on multi-modal pre-trained models can actually improve the zero-shot capability of ofa and we explore different transfer learning techniques and show their benefits we design a new metric called sensitivity so one more thing we are collecting a much larger multi-modal instruction tuning dataset with around 150 additional visual language tasks and we will release them so this is a qr code for our data and model thank you</sample>
    <sample id="92">hi my name is mathias linderman and today i'm going to give you a brief introduction to our paper on compositional generalization without trees using multiset tagging and latent permutations this is joint work with my advisors alexander kola and ivan titov compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training in the context of semantic parsing testing for compositional generalization might look like this as usual we have a training set of utterances in this case the girl slept and mary knew that the girl slept this works well but trees are usually not given and need to be obtained somehow this can be complicated and sometimes a computationally expensive process typically this involves considerable formalism-specific pre-processing of the logical forms for example to handle variable symbols obtaining trees may also involve specialized grammar induction procedures in this paper we solve a couple of interesting technical challenges first of all the alignment between input and output is not given in the training data as a consequence for a given token we don't know which multiset it came from which poses a challenge for training in addition sometimes there are multiple permutations that are consistent with the data but the linguistically correct one is latent we address this by inducing the alignment as part of the training our permutation method is very flexible but it brings the challenge that finding the highest scoring permutation is np-hard that's because this is related to the traveling salesman problem we approximate this with a gpu-friendly continuous relaxation that also allows us to back-propagate through the solution and learn the linguistically more plausible permutations if you want to learn more about our experiments and how we address these challenges please have a look at our paper or come to our poster</sample>
    <sample id="93">hi my name is mathias linderman and today i'm going to give you a brief introduction to our paper on compositional generalization without trees using multiset tagging and latent permutations this is joint work with my advisors alexander koller and ivan titov compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training in the context of semantic parsing testing for compositional generalization might look like this as usual we have a training set of utterances in this case the girl slept and mary knew that the girl slept our model outperforms the others by a large margin on generalization to deeper recursion some other kinds of structural generalization remain very challenging though in our paper we solve a couple of interesting technical challenges first of all the alignment between input and output is not given in the training data as a consequence for a given token we don't know which multiset it came from which poses a challenge for training in addition sometimes there are multiple permutations that are consistent with the data but the linguistically correct one is latent we address this by inducing the alignment as part of the training our permutation method is very flexible but it brings the challenge that finding the highest scoring permutation is np-hard that's because this is related to the traveling salesman problem we approximate this with a gpu-friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations if you want to learn more about our experiments and how we address these challenges please have a look at our paper or come to our poster</sample>
    <sample id="94">hello everyone my name is jingwei yi from the university of science and technology of china it's my pleasure to give a short advertisement video of our paper i will copy my model protecting the copyright of embedding as services via backdoor watermark let's first introduce the background about embedding as services currently large language models such as t5 lama palm are exceptional in natural language understanding and generation embedding as services is one of the services built upon large language models to assist the various nlp tasks for example openai offers a t5 based embedding as service however recent works have shown that the attacker may steal the model through learning from the embedding as services therefore it's necessary to protect the copyright of embedding as services to protect the copyright of embedding as services one of the solution is to embed a watermark in the provider's service and detect whether another service contains the watermark the watermark method needs to meet the following properties first the method should be applicable to embedding as services second the watermark should not degrade the utility of the provided embedding as services third the watermark should be covert enough to the attacker or the attacker can remove the watermark easily finally the watermark needs to be transferable to the attacker's services during the model extraction process existing works can be broadly classified into four categories however these methods either not applicable to embedding as services or lack of transferability therefore in this paper we propose embedding marker which is a backdoor based watermark method applicable to embedding as services now let me introduce the details of our embedding marker embedding marker contains two main steps watermark injection and copyright verification before these main steps we first select a trigger set the trigger set is a group of words in a moderate frequency interval we assume the provider can collect a general text corpus and count the word frequency with it the legends of the figures means the number of triggers in each sentence as shown in the figures it's hard to distinguish between the backdoored embeddings and normal embeddings that's all thank you welcome to discuss with us</sample>
    <sample id="95">hello everyone my name is alex vilar and i will be giving a short overview of the paper prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting</sample>
    <sample id="96">hi everyone i'm jenny a first year phd student at carnegie mellon university and today i'll be presenting our work entitled positionality characterizing design bias in ai datasets and models this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen</sample>
    <sample id="97">演讲者提到了 simulst 的几个问题： 1. 如何解决 simulst 模型的局限性 2. 如何解决 simulst 模型的局限性 3. 如何解决 simulst 模型的局限性 4. 如何解决 simulst 模型的局限性 5. 如何解决 simulst 模型的局限性 6. 如何解决 simulst 模型的局限性 7. 如何解决 simulst 模型的局限性 8. 如何解决 simulst 模型的局限性 9. 如何解决 simulst 模型的局限性 10. 如何解决 simulst 模型的局限性 11. 如何解决 simulst 模型的局限性 12. 如何解决 simulst 模型的局限性 13. 如何解决 simulst 模型的局限性 14. 如何解决 simulst 模型的局限性 15. 如何解决 simulst 模型的局限性 16. 如何解决 simulst 模型的局限性 17. 如何解决 simulst 模型的局限性 18. 如何解决 simulst 模型的局限性 19. 如何解决 simulst 模型的局限性 20. 如何解决 simulst 模型的局限性 21. 如何解决 simulst 模型的局限性 22. 如何解决 simulst 模型的局限性 23. 如何解决 simulst 模型的局限性 24. 如何解决 simulst 模型的局限性 25. 如何解决 simulst 模型的局限性 26. 如何解决 simulst 模型的局限性 27. 如何解决 simulst 模型的局限性 28. 如何解决 simulst 模型的局限性 29. 如何解决 simulst 模型的局限性 30. 如何解决 simulst 模型的局限性 31. 如何解决 simulst 模型的局限性 32. 如何解决 simulst 模型的局限性 33. 如何解决 simulst 模型的局限性 34. 如何解决 simulst 模型的局限性 35. 如何解决 simulst 模型的局限性 36. 如何解决 simulst 模型的局限性 37. 如何解决 simulst 模型的局限性 38. 如何解决 simulst 模型的局限性 39. 如何解决 simulst 模型的局限性 40. 如何解决 simulst 模型的局限性 41. 如何解决 simulst 模型的局限性 42. 如何解决 simulst 模型的局限性 43. 如何解决 simulst 模型的局限性 44. 如何解决 simulst 模型的局限性 45. 如何解决 simulst 模型的局限性 46. 如何解决 simulst 模型的局限性 47. 如何解决 simulst 模型的局限性 48. 如何解决 simulst 模型的局限性 49. 如何解决 simulst 模型的局限性 50. 如何解决 simulst 模型的局限性 51. 如何解决 simulst 模型的局限性 52. 如何解决 simulst 模型的局限性 53. 如何解决 simulst 模型的局限性 54. 如何解决 simulst 模型的局限性 55. 如何解决 simulst 模型的局限性 56. 如何解决 simulst 模型的局限性 57. 如何解决 simulst 模型的局限性 58. 如何解决 simulst 模型的局限性 59. 如何解决 simulst 模型的局限性 60. 如何解决 simulst 模型的局限性 61. 如何解决 simulst 模型的局限性 62. 如何解决 simulst 模型的局限性 63. 如何解决 simulst 模型的局限性 64. 如何解决 simulst 模型的局限性 65. 如何解决 simulst 模型的局限性 66. 如何解决 simulst 模型的局限性 67. 如何解决 simulst 模型的局限性 68. 如何解决 simulst 模型的局限性 69. 如何解决 simulst 模型的局限性 70. 如何解决 simulst 模型的局限性 71. 如何解决 simulst 模型的局限性 72. 如何解决 simulst 模型的局限性 73. 如何解决 simulst 模型的局限性 74. 如何解决 simulst 模型的局限性 75. 如何解决 simulst 模型的局限性 76. 如何解决 simulst 模型的局限性 77. 如何解决 simulst 模型的局限性 78. 如何解决 simulst 模型的局限性 79. 如何解决 simulst 模型的局限性 80. 如何解决 simulst 模型的局限性 81. 如何解决 simulst 模型的局限性 82. 如何解决 simulst 模型的局限性 83. 如何解决 simulst 模型的局限性 84. 如何解决 simulst 模型的局限性 85. 如何解决 simulst 模型的局限性 86. 如何解决 simulst 模型的局限性 87. 如何解决 simulst 模型的局限性 88. 如何解决 simulst 模型的局限性 89. 如何解决 simulst 模型的局限性 90. 如何解决 simulst 模型的局限性 91. 如何解决 simulst 模型的局限性 92. 如何解决 simulst 模型的局限性 93. 如何解决 simulst 模型的局限性 94. 如何解决 simulst 模型的局限性 95. 如何解决 simulst 模型的局限性 96. 如何解决 simulst 模型的局限性 97. 如何解决 simulst 模型的局限性 98. 如何解决 simulst 模型的局限性 99. 如何解决 simulst 模型的局限性 100 如何解决 simulst 模型的局限性 101 如何解决 simulst 模型的局限性 102 如何解决 simulst 模型的局限性 103 如何解决 simulst 模型的局限性 104 如何解决 simulst 模型的局限性 105 如何解决 simulst 模型的局限性 106 如何解决 simulst 模型的局限性 107 如何解决 simulst 模型的局限性 108 如何解决 simulst 模型的局限性 109 如何解决 simulst 模型的局限性 110 如何解决 simulst 模型的局限性 111 如何解决 simulst 模型的局限性 112 如何解决 simulst 模型的局限性 113 如何解决 simulst 模型的局限性 114 如何解决 simulst 模型的局限性 115 如何解决 simulst 模型的局限性 116 如何解决 simulst 模型的局限性 117 如何解决 simulst 模型的局限性 118 如何解决 simulst 模型的局限性 119 如何解决 simulst 模型的局限性 120 如何解决 simulst 模型的局限性 121 如何解决 simulst 模型的局限性 122 如何解决 simulst 模型的局限性 123 如何解决 simulst 模型的局限性 124 如何解决 simulst 模型的局限性 125 如何解决 simulst 模型的局限性 126 如何解决 simulst 模型的局限性 127 如何解决 simulst 模型的局限性 128 如何解决 simulst 模型的局限性 129 如何解决 simulst 模型的局限性 130 如何解决 simulst 模型的局限性 131 如何解决 simulst 模型的局限性 132 如何解决 simulst 模型的局限性 133 如何解决 simulst 模型的局限性 134 如何解决 simulst 模型的局限性 135 如何解决 simulst 模型的局限性 136 如何解决 simulst 模型的局限性 137 如何解决 simulst 模型的局限性 138 如何解决 simulst 模型的局限性 139 如何解决 simulst 模型的局限性 140 如何解决 simulst 模型的局限性 141 如何解决 simulst 模型的局限性 142 如何解决 simulst 模型的局限性 143 如何解决 simulst 模型的局限性 144 如何解决 simulst 模型的局限性 145 如何解决 simulst 模型的局限性 146 如何解决 simulst 模型的局限性 147 如何解决 simulst 模型的局限性 148 如何解决 simulst 模型的局限性 149 如何解决 simulst 模型的局限性 150 如何解决 simulst 模型的局限性 151 如何解决 simulst 模型的局限性 152 如何解决 simulst 模型的局限性 153 如何解决 simulst 模型的局限性 154 如何解决 simulst 模型的局限性 155 如何解决 simulst 模型的局限性 156 如何解决 simulst 模型的局限性 157 如何解决 simulst 模型的局限性 158 如何解决 simulst 模型的局限性 159 如何解决 simulst 模型的局限性 160 如何解决 simulst 模型的局限性 161 如何解决 simulst 模型的局限性 162 如何解决 simulst 模型的局限性 163 如何解决 simulst 模型的局限性 164 如何解决 simulst 模型的局限性 165 如何解决 simulst 模型的局限性 166 如何解决 simulst 模型的局限性 167 如何解决 simulst 模型的局限性 168 如何解决 simulst 模型的局限性 169 如何解决 simulst 模型的局限性 170 如何解决 simulst 模型的局限性 171 如何解决 simulst 模型的局限性 172 如何解决 simulst 模型的局限性 173 如何解决 simulst 模型的局限性 174 如何解决 simulst 模型的局限性 175 如何解决 simulst 模型的局限性 176 如何解决 simulst 模型的局限性 177 如何解决 simulst 模型的局限性 178 如何解决 simulst 模型的局限性 179 如何解决 simulst 模型的局限性 180 如何解决 simulst 模型的局限性 181 如何解决 simulst 模型的局限性 182 如何解决 simulst 模型的局限性 183 如何解决 simulst 模型的局限性 184 如何解决 simulst 模型的局限性 185 如何解决 simulst 模型的局限性 186 如何解决 simulst 模型的局限性 187 如何解决 simulst 模型的局限性 188 如何解决 simulst 模型的局限性 189 如何解决 simulst 模型的局限性 190 如何解决 simulst 模型的局限性 191 如何解决 simulst 模型的局限性 192 如何解决 simulst 模型的局限性 193 如何解决 simulst 模型的局限性 194 如何解决 simulst 模型的局限性 195 如何解决 simulst 模型的局限性 196 如何解决 simulst 模型的局限性 197 如何解决 simulst 模型的局限性 198 如何解决 simulst 模型的局限性 199 如何解决 simulst 模型的局限性 200 如何解决 simulst 模型的局限性 201 如何解决 simulst 模型的局限性 202 如何解决 simulst 模型的局限性 203 如何解决 simulst 模型的局限性 204 如何解决 simulst 模型的局限性 205 如何解决 simulst 模型的局限性 206 如何解决 simulst 模型的局限性 207 如何解决 simulst 模型的局限性 208 如何解决 simulst 模型的局限性 209 如何解决 simulst 模型的局限性 210 如何解决 simulst 模型的局限性 211 如何解决 simulst 模型的局限性 212 如何解决 simulst 模型的局限性 213 如何解决 simulst 模型的局限性 214 如何解决 simulst 模型的局限性 215 如何解决 simulst 模型的局限性 216 如何解决 simulst 模型的局限性 217 如何解决 simulst 模型的局限性 218 如何解决 simulst 模型的局限性 219 如何解决 simulst 模型的局限性 220 如何解决 simulst 模型的局限性 221 如何解决 simulst 模型的局限性 222 如何解决 simulst 模型的局限性 223 如何解决 simulst 模型的局限性 224 如何解决 simulst 模型的局限性 225 如何解决 simulst 模型的局限性 226 如何解决 simulst 模型的局限性 227 如何解决 simulst 模型的局限性 228 如何解决 simulst 模型的局限性 229 如何解决 simulst 模型的局限性 230 如何解决 simulst 模型的局限性 231 如何解决 simulst 模型的局限性 232 如何解决 simulst 模型的局限性 233 如何解决 simulst 模型的局限性 234 如何解决 simulst 模型的局限性 235 如何解决 simulst 模型的局限性 236 如何解决 simulst 模型的局限性 237 如何解决 simulst 模型的局限性 238 如何解决 simulst 模型的局限性 239 如何解决 simulst 模型的局限性 240 如何解决 simulst 模型的局限性 241 如何解决 simulst 模型的局限性 242 如何解决 simulst 模型的局限性 243 如何解决 simulst 模型的局限性 244 如何解决 simulst 模型的局限性 245 如何解决 simulst 模型的局限性 246 如何解决 simulst 模型的局限性 247 如何解决 simulst 模型的局限性 248 如何解决 simulst 模型的局限性 249 如何解决 simulst 模型的局限性 250 如何解决 simulst 模型的局限性 251 如何解决 simulst 模型的局限性 252 如何解决 simulst 模型的局限性 253 如何解决 simulst 模型的局限性 254 如何解决 simulst 模型的局限性 255 如何解决 simulst 模型的局限性 256 如何解决 simulst 模型的</sample>
    <sample id="98">hi i'm jianbin phd student at university of washington today i'm presenting our work from pre-training data to language models to downstream tasks tracking the trails of political biases leading to unfair nlp models so language models are trained on large-scale web crawl data political news media are well covered in their pre-training data according to a survey of the c4 corpus we can see that new york times los angeles times the guardian huffington post etc are well covered in language model training data this has created a mixed blessing for language model applications so on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas on the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream tasks so to this end we propose to investigate the political bias propagation pipeline from pre-training data to language models to downstream tasks specifically by asking the following questions first how do we evaluate the political leaning of language models and what role does pre-training data might have on such political biases secondly how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in nlp applications so we see that if we investigate the per category performance that is to say if we separate the performance into different demographics or political leanings of news media we can see a pattern that for example for hate speech detection left-leaning language models are better at detecting hate speech targeting socially minority groups however are worse at detecting hate speech targeting more powerful groups in our society and vice versa right-leaning language models are better at detecting hate speech targeting white and men however worse at detecting hate speech targeting black lgbtq plus and other minority communities similar trends also happen for fake news detection where we see that left-leaning language models are better at detecting misinformation from their opposite political opinions and vice versa okay great i think that's pretty much all i have for today thank you for your time</sample>
    <sample id="99">hi i'm xia jian from fudan university i'm here to introduce our work distinguishing script knowledge from language models for constraint language planning in everyday life humans often plan their actions by following step-by-step instructions in the form of scripted knowledge previous work has explored language models to plan for abstract goals of stereotypical activities such as make a cake and showed that large language models can effectively decompose those into steps however previous work mainly focuses on planning for abstract goals with specific goals with specific constraints such as make a chocolate cake the heat map in the figure shows that the planning performance of large language models for specific goals is still unsatisfactory in terms of semantic similarity in addition we found that large language models are prone to generating scripts with high perplexity and low coherence scores indicating that the generated scripts lack semantic coherence in summary we established the constraint language planning problem we evaluated the constraint language planning ability of large language models and developed a over-generated filter method for large language models we hope the generated dataset can be a valuable resource to advance research on language planning we hope the generated dataset can be a valuable resource to advance research on language planning thanks for your time please find more details of co-script in our paper</sample>
    <sample id="100">hello multi-hop qa is about answering questions that require multiple reasoning jumps to answer for example to answer this question what 1988 christmas comedy film did bryan doyle murray star in we first need to find all the movies that uh that bryan doyle murray star in and find uh the movie that was released in 1988 we're going to be referring to uh this set of documents required to answer a question as the chain multi-hop qa retrievers are trained by maximizing probability of the ground truth chain given questions so for instance training examples q1 c1 q2 c2 etc where q is a question and c is uh the gold chain uh retrievers are trained by maximizing the probability of the question given the chain prompt let's go through a working example leaving this question and this chain what we do is we have a prompt that looks like this where we have we insert the chain documents into the prompt and we have an indicator token to designate uh that this is a document and we have uh an instruction which uh in our case here is something like read the previous documents and ask a question and the instruction serves to elicit the language model reasoning abilities over the chain documents with that i conclude my talk and thank you so much for listening</sample>
    <sample id="101">hello everyone my name is alex vilar and i will be giving a short overview of the paper prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting</sample>
    <sample id="102">水印方法的重要属性是什么？水印方法的重要属性是可以防止未经授权的复制和使用。水印方法通过在数据中嵌入不可见的水印来实现这一目标，使得未经授权的复制和使用都能被检测到。水印方法的重要属性包括： 1）不可见性：水印不会影响数据的可读性和可理解性。 2）鲁棒性：水印能够抵抗各种攻击，如噪声、压缩、裁剪等。 3）可恢复性：即使在检测到水印后，也能够恢复原始数据。 4）可检测性：水印能够被检测到，从而防止未经授权的复制和使用。 5）可扩展性：水印能够适应不同的数据类型和应用场景。 6）可定制性：水印能够根据不同的应用场景进行定制化设计。 7）可扩展性：水印能够适应不同的数据类型和应用场景。 8）可定制性：水印能够根据不同的应用场景进行定制化设计。 9）可扩展性：水印能够适应不同的数据类型和应用场景。 10）可定制性：水印能够根据不同的应用场景进行定制化设计。</sample>
    <sample id="103">hello my name is kyo yin and i will be presenting our work titled when does translation require context a data-driven multi-lingual exploration this work was done in collaboration with patrick fernandez emilio uh andrew f d martins and gramma big so a lot of translations depend on context the sentence well if the previous sentence was things could start to get dangerous if the ministers find out then mole refers to a spy but if the previous sentence was could it be anything serious doctor then mole refers to a birthmark so depending on context the meaning of the word changes and therefore its translation changes as well however evaluating how well models can handle these cases is pretty hard firstly because only a small portion of translations depend on context the sentence level or at the word level we can think of words that have high p x m i as words that require context for translation to make sure that you're using the same translation within the document and similarly we find that context is supported to translate in the right formality and lexical cohesion but these models are not much better than models that do not use context on other phenomena like ellipsis pronouns and verb form so this sort of suggests where we would need to see more progress for document level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document level translation thank you so much for your attention see you in toronto</sample>
    <sample id="104">hi everyone i'm jenny a first year phd student at carnegie mellon university and today i'll be presenting our work entitled positionality characterizing design bias of datasets and models this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was</sample>
    <sample id="105">hello everyone my name is jingwei yi from the university of science and technology of china it's my pleasure to give a short advertisement video of our paper i will copy my model protecting the copyright of embedding as services will backdoor watermark let's first introduce the background about embedding as services currently large language models such as t5 lama palm are exceptional in natural language understanding and generation embedding as services is one of the services built upon large language models to assist the various nlp tasks for example openai offers a t5 based embedding as service however recent works have shown that the attacker may steal the model through learning from the embedding as services therefore it's necessary to protect the copyright of embedding as services to protect the copyright of embedding as services one of the solutions is to embed a watermark in the provider's service and detect whether another service contains the watermark the watermark method needs to meet the following properties first the method should be applicable to embedding as services second the watermark should not degrade the utility of the provided embedding as services third the watermark should be covert enough to the attacker or the attacker can remove the watermark easily finally the watermark needs to be transferable to the attacker's services during the model extraction process existing works can be broadly classified into four categories however these methods either are not applicable to embedding as services or lack transferability therefore in this paper we propose a backdoor-based watermark method applicable to embedding as services now let me introduce the details of our embedding as service watermark the legends of the figures mean the number of triggers in each sentence as shown in the figures it's hard to distinguish between the backdoor embedding and normal embeddings that's all thank you welcome to discuss with us</sample>
    <sample id="106">hello my name is jetanya and i'm going to be talking about our paper called quest this is work done in collaboration with pete mingwei kenton and christina from google deepmind to motivate this work let's consider the example of jane who's a zoologist on a field trip in costa rica and she observes a species of reptile that is unknown to her in our second example let's consider austin who's an avid book reader who just finished a book and is looking for his next read now jane wants to find the name of the species of reptile that she encountered on her field trip in costa rica and austin wants to find historical fiction novels set in france both of the sets that he's interested in with selective information needs thank you for watching please read our paper and hope you can come to our presentation at acl thanks a lot</sample>
    <sample id="107">hello everyone my name is xin jian from the penn state university today i'm going to present our work exemplar cross-lingual semantic parsing in multiple natural languages and many representations so semantic parsing is the task to build semantic representations of user queries such as sql and lambda calculus and cross-lingual semantic parsing is the task to translate queries in multiple natural languages into multiple meaning representations as shown in this figure we need to translate the query in multiple natural languages using neural models to sql lambda or funql and etc and we also find many interesting results so regarding analyses of monolingual models we evaluate on two groups of models including encoder pdr which stands for multi-lingual pre-trained encoders with pointer-based decoders such as xlmr plus pdr and bert plus pdr and we also evaluate encoder-decoder models which is multi-lingual pre-trained encoder-decoder models such as mbert and mt5 we found that encoder-decoder obtains the best performance on all nine data sets and only gains in three data sets i think this is known as the curse of multi-linguality we also compare the cross-lingual performance gap in this figure the blue line is cross-lingual fused transfer the orange line is cross-lingual zero-shot transfer while the green line is the monolingual setting we found that by comparing the green and orange line we found that for zero-shot setting the cross-lingual transfer performance gap is significant and by comparing blue and orange line we found that for few-shot setting the transfer gap is shortened rapidly we also find some other interesting findings for example um encoder-decoder outperforms previous work or achieved comparable results for training on english natural language and significantly boost the performance of few-shot on target natural languages and we found monolingual language models such as coders and blue are still inferior for cross-lingual semantic parsing tasks um to sum up we build exemplar a unified benchmark for cross-lingual semantic parsing with multiple natural languages and many representations we conduct a comprehensive benchmark study on three representative of types of multi-lingual language models and our result shows many interesting findings and etc and welcome to visit our paper and code thanks for listening</sample>
    <sample id="108">hi everyone i'm kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas kostas</sample>
    <sample id="109">hi i'm or and i will present a natural instructions tuning language models with almost no human label in this talk we ask whether we can create a large dataset of instructions that is diverse in tasks content and phrasing without any human label so we introduce a natural instructions which is a dataset of natural language instructions and the corresponding inputs and outputs the data was collected in a fully automatic manner and without any human annotations so at the first step we prompt a pre-trained language model here specifically a variant of gpt-3 with three examples taken from the supernatural instructions dataset and we ask the model to generate a force example and you can see our data generation prompt here in the image so at the second step we take the generated instructions and input and we ask the model to generate a corresponding output so we analyze the generated examples focusing on creativity diversity and correctness as for correctness we find that more than 50 of the generated examples are indeed correct and even incorrect examples often contain valuable information for instruction tuning in terms of the utility of the generated data we fine-tune an 11 billion parameter t5 model on a natural instructions only we train it on supernatural instructions and the tested benchmarks were supernatural instructions key zero big bench and elementary to conclude we introduce a natural instructions which is a dataset of natural language instructions that was collected in a completely automatic process requiring only a small set of manually constructed examples a natural instructions highlights the ability of language models to produce creative and diverse data this is difficult to obtain with crowd workers who usually collapse into predictable heuristics and form annotation artifacts at the same time language models are also faster and cheaper than human annotation thank you</sample>
    <sample id="111">作者如何确定中等频率的单词？</sample>
    <sample id="112">hello everyone my name is xuhong today i'm going to present our paper do conno 2003 named entity taggers still work in 2023 and last but not least we calculated the percentage change in f1 to assess the generalization of each model so what is needed for good generalization through our experiments we found that from the graph on the right the red best fit line has a gradient that is greater than 1 this means that every unit of improvement that we made on conno 2003 translates to more than one unit improvement on conno which means that there is no diminishing returns and this shows us that adaptive overfitting in this case is not observed so what about temporal drift then for temporal drift we didn't experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift and kind of surprisingly it is not caused by adaptive overfitting even though conno 2003 has been used for over 20 years so going back to the question that we raised in the title of our paper do conno 2003 taggers still work in 2023 and we found that the answer is actually a resounding yes we hope our paper calls for more research on how to improve generalization of the models and lastly please make sure to check out our paper our data set and if you have any questions feel free to contact me thank you so much</sample>
    <sample id="114">hi everyone uh i'm going to introduce our work on acl 2023 called finding the pillars of strengths for multi-head attention so we are from nanyang technological university of singapore to begin with as we all know the large language models are game-changing from the task-specific models for each field of natural language processing now the large language models can learn all tasks in one model and this is revolutionary however several limitations are identified for example the heavy parameters they usually contain billions of parameters which is not deployable on small clusters they usually require long training time for example the lama 65 takes one million gpu hours with our even with our best gpus and they are talking hundreds they require huge corpus for example the lama 65 is trained on four point four five terabytes to the original network which means that we believe that we can prune the networks without sacrificing the performance so this gives us the confidence to prune the redundant large-level models so the the large language models nowadays are redundant in real scenarios because they usually they are able to perform all namely all tasks however we only need a few tasks in real applications for example when we are doing machine translation we don't need the ability to perform the image caption so the related parameters can be pruned and we believe this the prune will not sacrifice the performance as we as we uninstall the apps on our iphone and if you imagine that if we don't uninstall the unused apps on our iphone they will be too heavy to use so this is all for today's video if you want to know more about our work please do not hesitate to attend to our poster session thank you</sample>
    <sample id="115">hi i'm sarah papi from the university of toronto and vlad seleny bruno kessler and i will briefly introduce the attention as a guide for simultaneous speech translation paper that is a joint work with macdonald negri and marco turci and what is our solution first two use already existing offline st models without retraining or adopting specific architecture for simultaneous st use only one model for every latency regime and handle latency through specific specific parameters and leverage the knowledge already acquired by the model through the cross attention mechanism between audio input and textual output that is the cross attention mechanism and you can see an example on the right our solution is to propose a dot or encoder decoder attention and it is a strategy for which we decide whether to emit or not a partial translation based on where attention points to and what is emitted if the attention is not concentrated that is if the sum is below a certain threshold alpha towards the last lambda speech frames this means that these three words will be emitted while since the sum of the cross attention is above the fastest strategy if you want to discover more results read our paper and we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work thanks for your attention</sample>
    <sample id="116">在 servin 和 kea 的示例中，需要哪些特定于实体的知识？</sample>
    <sample id="117">example quality is more important than the similarity to the source sentence so it's important to select the examples from high-quality translations in particular we compare the selecting prompts from the training data of the wmt evaluations or the dev data the dev data is much more curated and with higher quality than the training data so in our case we chose to evaluate with google translate the insights that we gain from the human evaluation that we perform using the mqm framework is that the fluency of parm is comparable to state-of-the-art systems but the main difference comes from the accuracy so in particular the most common error are omission errors so it seems that parm chooses to produce a better-sounding translation sometimes by dropping parts of the source sentence that are irrelevant in the translation however the style awkward category for parm is lower than for the state-of-the-art systems which is an additional signal that parm provides really fluent output but still with some problems of accuracy and that's it for this really short overview for more details please come back to the full presentation of the paper thank you very much</sample>
    <sample id="118">hello everyone we'll be presenting our acl 2023 submission which is the improving pre-training techniques for code-switched nlp so first we define what code-switching is so here we have an example laptop mira bag mira key this is a code mix sentence of english and hindi so some of the words are english and some of the words are hindi so this is a pretty common occurrence in linguistically diverse communities like india so building computational models for code-switching is very important but in this case multilingual pre-trained models like mbert and xlm-r don't perform that well on code-switching tasks like question answering and sentiment analysis so our main contributions in this work are we propose novel mlm techniques which are tuned to the case of code-switching and we motivate architectural changes and add an auxiliary loss to further enhance this switch point information in the final representation so in summary we propose a new mlm objective which is tuned to handle code-switched information we hypothesize and verify using probing classifiers that our methods increase the amount of switch point information present in the intermediate layers and with this result in mind we motivate some architectural changes and add an auxiliary loss to further enhance this switch point information content thank you</sample>
    <sample id="119">hi i'm jianbin phd student at university of washington today i'm presenting our work from pre-training data to language models to downstream tasks tracking the trails of political biases leading to unfair nlp models so language models are trained on large-scale web crawl data political news media are well covered in their pre-training data according to a survey of the c4 corpus we can see that new york times los angeles times the guardian huffington post etc are well covered in language model training data this has created a mixed blessing for language model applications so on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas on the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream tasks ultimately creating fairness issues if we do try to sanitize somehow we would also risk censorship or exclusion and it's incredibly hard to determine what is actually neutral and should be retaining language model training data so it's kind of like the electric chollie problem okay great i think that's pretty much all i have for today thank you for your time</sample>
    <sample id="120">hi i'm sarah pappi from the university of toronto and vlad seleny bruno kessler and i will briefly introduce the attention as a guide for simultaneous speech translation paper that is a joint work with macdowell negri and marco turci and what is our solution first to use already existing offline models without retraining or adopting specific architecture for simultaneous speech translation use only one model for every latency regime and handle latency through specific specific parameters and leverage the knowledge already acquired by the model through the cross attention mechanism and you can see an example on the right our solution is to propose a dot or encoder decoder attention and it is a strategy for which we decide whether to emit or not a partial translation based on where attention points to and what is emitted if the attention is not concentrated that is its sum is below a certain threshold alpha towards the last lambda speech frames this means that these three words will be emitted while since the sum of the cross attention is above the fastest strategy if you want to discover more results read our paper and we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work thanks for your attention</sample>
    <sample id="121">hi and i'm going to talk about our work on resolving indirect referring expressions for entity selection in which we introduce the entity corpus my name is jawad hosseini and this is a joint work with philip radlinski sylvia parretti and annie lewis our goal is to understand users' language when they want to make a choice and consider this alternative question did you mean easy on me or i got a feeling which is the alternative question and in the third speech bubble bob says remember that song we were listening to yesterday and with that bob sets the dialogue context in the second speech bubble alice says do you mean easy on me or i got a feeling which is the alternative question and in the third speech bubble bob uses an indirect referring expression to select one of these entities for example the first one and describe them using three to five indirect referring expressions for example the one with the piano music here are some examples from our dataset if the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model retrieves the background knowledge if the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to</sample>
    <sample id="122">这篇论文的作者所属机构是什么</sample>
    <sample id="123">hello everyone my name is ying and my colleague yan and i will be presenting our research on multi-instruction improving multi-modal zero-shot learning via instruction tuning so with the advances in large language models many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data efficient way recently many studies have shown that instruction tuning enables large language models to perform on unseen multi-modal tasks in a zero-shot manner by following natural instructions however most previous works on instruction tuning focus on improving the zero-shot performance on language-only tasks while computer vision and multi-modal tasks have been left out therefore in this work we want to investigate whether instruction tuning on multi-modal pre-trained models can actually improve the zero-shot capability of ofa and we explore different transfer learning techniques and show their benefits we design a new metric called sensitivity so one more thing we are collecting a much larger multi-modal instruction tuning data set with around 150 additional language tasks and we will release them so this is a qr code for our data and model thank you</sample>
    <sample id="124">hi everyone this is tan jie yi from the national university of singapore and ali baba i'm glad to share our work towards benchmarking and improving the temporal reasoning capability of lms time is a fundamental axis in the real world we first break down temporal reasoning into three different levels the first level is time to time reasoning such as what is the year after 2010 answering this question will only meet the understanding of the time axis the second level is time to event reasoning in this work we include long duration facts as event as well an example question is what team did leo messi play for in 2020 time period which could be correlated to the term frequencies in the pre-training quadrant we can see that chatgpt is close to solving the year prediction problem but later we do find that its performance drops significantly when performing month prediction besides its performance on l2 and l3 reasoning is also not promising even losing to the significantly smaller flat t5l in l2 reasoning in conclusion we analyze and expose the temporal reasoning biases of lms we also propose the temp reasoning benchmark dataset that covers all three temporal reasoning types and comprehensive time periods last but not least we propose a training paradigm to improve lms temporal reasoning and that will be all for my presentation thank you all for your time</sample>
    <sample id="125">这篇论文有多少位作者？这篇论文有多少篇页数？这篇论文的主题是什么？这篇论文的作者是谁？这篇论文的摘要是什么？这篇论文的关键词是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？这篇论文的摘要是什么？</sample>
    <sample id="126">在语义解析之前是否使用机器翻译模型翻译自然语言查询作为基线？</sample>
    <sample id="127">hi thanks for tuning in i'm namguho a master's student at keist ai in korea i'd like to introduce our work large language models as reasoning teachers to transfer their reasoning abilities to much smaller models we also propose a novel technique called diverse reasoning if you see here we don't only generate one reasoning sample from the teacher we generate many of them using stochastic temperature sampling and since these are very complex questions all of these solutions have slightly distinct solutions and that is why these samples can be used to train the student even better and we found that students under fine-tuned this method that we call fine-tuned cot can perform a complex reasoning task quite well now there's a lot of analysis and discussion that is in the paper but we don't have time for that so you can check the paper for more details so the basic takeaways is that simple distillation can transfer the reasoning abilities from very large teachers to small students smaller than 1 billion parameters and this may be possible for other emergent abilities in the future our method with diverse reasoning is accessible and it's a very effective approach and it's also very highly scalable and this kind of distillation poses various trade-offs between development costs and inference costs as well as the quality of the inference so that is it for the video please check out our paper that goes through a bunch of details over 40 pages including how reasoning emerges in small models as well as results on open source models we provide the code and data from all of our experiments even the smaller ones including a thousand dollars or more worth of teacher inference from open ai for your pleasure we encourage you to take our material for future work also feel free to reach out for discussions thank you for listening and hope to see you at the conference</sample>
    <sample id="128">hello everyone i'm makshatta and today my co-author martin and i are presenting our work the kitmos task evaluating knowledge integration from multiple sources this work is a collaboration between mcgill university niela and microsoft research kitmos is a diagnostic test suite for knowledge integration from multiple sources we have defined three settings of kitmos first we have the background pre-trained setting where background knowledge is assumed to be available at pre-training time second there is the background both setting where background knowledge is available both at pre-training time and at inference time lastly the background inference setting where background knowledge is available only at inference time to summarize the main takeaways of our paper many co-reference resolution models appear unable to reason over knowledge from different sources without task-specific training however with task-specific training some models successfully integrate knowledge from multiple sources still even the best-performing models seem to have difficulties with reliably integrating background knowledge presented only at inference time if you're interested in more details please see our paper and check out the dataset and code on github thanks for listening</sample>
    <sample id="129">hi i'm myra and today i'll be talking about our paper marked personas using natural language prompts to measure stereotypes in language models this work is done in collaboration with essem dermanush and dan jurafsky in recent years many have documented the prevalence of social bias and stereotypes in large language models however these measures have various limitations they usually rely on hand-constructed datasets that are very time-consuming to curate and they also usually only measure very specific stereotypes meaning that they don't generalize well to other demographics or contexts or they simply capture very general broad associations like negative associations with particular groups so for instance for the personas of black women we usually see negative stereotypes and essentializing narratives so based on these patterns we conclude with three recommendations for model owners first we should as researchers be addressing positive stereotypes and essentializing narratives we should also be using an intersectional lens to study biases and harms because there's a lot of things that might be overlooked if we don't do that and finally there should really be increased transparency about bias mitigation methods because for instance like these positive stereotypes we don't know if it's because there's some sort of like weird overly excessive value alignment going on or maybe some other like anti-stereotyping methods that are resulting in these pernicious patterns we just really can't make any assumptions or really study that further without more transparency thank you so much for listening have a good time at acl</sample>
    <sample id="130">hello everyone my name is xuhong today i'm going to present our paper do conno 2003 named entity taggers still work in 2023 and this shows us that adaptive overfitting in this case is not observed so going back to the question that we raised in the title of our paper do conno 2003 taggers still work in 2023 and we found that the answer is actually a resounding yes we hope our paper calls for more research on how to improve generalization of the models and lastly please make sure to check out our paper our data set and if you have any questions feel free to contact me thank you so much</sample>
    <sample id="131">hello i am dawei a phd student at stuttgart university in germany in this video i would like to present our recent work weak supervision and weakly supervised learning in weak supervision we do not manually label the data instead we label the data using weak labeling sources such as simple heuristic rules knowledge bases or locality crowdsourcing as illustrated in the figure on the right when compared to human annotation the weak labeling sources are much cheaper yet they are also noisy meaning that a certain amount of the labeling data are incorrect in weakly supervised learning we address this problem by proposing training algorithms to robustly train neural networks on the weakly labeled data so that the trained models can generalize beyond the original weak samples as shown in the figure on the left typically we only need 20 samples per class to attain high performance but that's not the end of the story because if we allow to continue fine-tuning on the clean validation samples then ftw performs equally well with other methods so in practice there's no reason to choose more complex wsl methods which require more computation time and disk space to summarize we show that recent wsl approaches require clean manually annotated samples for them to work properly their performance gain and practicality are heavily overestimated our concrete recommendations for future work are as follows first report the model selection criteria for example report if the model selection is done via clean validation samples second wsl approaches should be compared with fully supervised learning baselines as both work on clean samples third continuous fine-tuning is a simple yet strong baseline that should be considered in future work in wsl finally we have open sourced our code you can find it via the qr code on this slide please feel free to check it out thank you and enjoy the conference</sample>
    <sample id="132">hello everyone i'm makshatta and today my co-author martin and i are presenting our work the kitmos task evaluating knowledge integration from multiple sources this work is a collaboration between mcgill university niela and microsoft research in this work we propose a diagnostic test suite for knowledge integration we introduce a co-reference resolution task designed to probe for the ability to draw on knowledge available in different sources we evaluate the dataset with human study participants and established co-reference resolution models in this figure we show the results of the best performing models on the most difficult variant of the background pre-trained setting without task-specific training on kitmos both models do not perform well when trained on kitmos however both c2 and bert for co-reference perform significantly better than the random choice this suggests that when trained on task-specific co-reference resolution datasets models learn to exploit surface cues which are not useful when testing on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge presented only at inference time to summarize the main takeaways of our paper many co-reference resolution models appear unable to reason over knowledge from different sources without task-specific training however with task-specific training some models successfully integrate knowledge from multiple sources still even the best-performing models seem to have difficulties with reliably integrating background knowledge presented only at inference time if you're interested in more details please see our paper and check out the dataset and code on github thanks for listening</sample>
    <sample id="133">hello everyone my name is ying and my colleague jian and i will be presenting our research on multi-instruction improving multi-modal zero-shot learning via instruction tuning so with the advances in large language models many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data efficient way recently many studies have shown that instruction tuning enables large language models to perform on unseen multi-modal tasks in a zero-shot manner by following natural instructions however most previous works on instruction tuning focus on improving the zero-shot performance on language-only tasks while computer vision and multi-modal tasks have been left out therefore in this work we want to investigate whether instruction tuning on multi-modal pre-trained models can actually improve zero-shot performance on unseen multi-modal tasks therefore this motivates us to build a multi-modal instruction tuning dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories these tasks are derived from 21 existing open-source datasets and each task is equipped with five expert-written instructions so during test for each task we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment we report the mean and max performance and the standard deviation of the performance across all five experiments if the task is a multi-modal classification task we report accuracy if it's a multi-modal generation task we report rouge-l as well we also introduced a additional evaluation metric called sensitivity so this measures the model's ability to consistently produce the same output for the same task regardless of the slight variation in the wording of the instruction so as we can see by transfer learning from natural instruction datasets the model can achieve much better sensitivity compared to the original ofa model we also can see transfer learning from natural instruction datasets can help ofa achieve much better performance on the natural instruction dataset so overall we propose a first large-scale multi-modal instruction tuning dataset that significantly improves the zero-shot capability of ofa and we explore different transfer learning techniques and show their benefits we design a new metric called sensitivity so one more thing we are collecting a much larger multi-modal instruction tuning dataset with around 150 additional visual language tasks and we will release them so this is a qr code for our data and the model thank you</sample>
    <sample id="135">hello i'm james finch and i'm sarah finch and today we'll tell you all about abc eval a new dimensional approach to evaluating conversational ai this work was done by the emory nlp lab led by professor gino choi at emory university and in collaboration with amazon alexa ai so let's say that you just developed a dialogue model and you want to see how well it compares against the current state of the art the common practice is to use human judges to evaluate several dimensions of dialogue quality to understand the strengths and weaknesses of the model on a finer grained level one approach is to simply ask human judges to evaluate several dimensions of dialogue quality such as the relevance of model responses using existing comparative or liker scale methods however we believe there is a more precise and reliable strategy for dimensional dialogue evaluation our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself or its partner hallucinating incorrect facts or violating common sense knowledge and when the model succeeds or fails to show empathy to determine what kind of evaluation is most effective we selected four state-of-the-art chat models and evaluated them on 100 human bot conversations per model using abc eval for comparison we also evaluated these conversations using three existing methods liker ratings on the turn level liker ratings on the dialogue level and dialogue level pairwise comparisons for each of the existing methods we collected evaluations on eight of the most commonly measured aspects of dialogue since this is the standard practice for evaluating chat models along multiple dimensions with the rapid pace of improvement in the field many of these error rates could see a decrease in new models released since our evaluation was conducted however this is all the more reason to pursue reliable and precise evaluation metrics for comparing models we hope abc eval can be leveraged by others in the field as a meaningful step in this direction and we look forward to seeing how conversational ai will advance in the coming months and years thank you for watching</sample>
    <sample id="136">hello everybody so my name is shazavan and today i'll be presenting the work that i conducted with my supervisor nefisa at the university of sheffield so the title is format an alternative to accuracy for numerical reasoning and here's the qr code there which gives you access to the paper the github repo my twitter and linkedin so let's get started so what's the motivation behind this work so there are lots of real-world applications for numerical reasoning tasks for example fact-checking let's have a look at a concrete example so info tabs has a pair of statements such as chris brown became famous when they were sixteen years old and we can see that it might these models tend to perform quite poorly across all these aspects that we've introduced um the original set which is made up of common core and illinois tends to perform slightly better which shows that it might these benchmarks might not actually be representative of the necessity of the real world and this shows that this is a lot more promising in improving the performance so in terms of conclusions we find that the existing benchmarks tend to be unrepresentative and single scores don't help with that and this is why fermata is there to provide a more alternative evaluation to fill in that gap we find that language and mathematical diversity is important and also that with other analyses find that number encoding and tokenization are areas of improvement so again you've got the qr code and the links thank you very much for listening and i do encourage you to read the paper</sample>
    <sample id="137">hi i'm susan from the singapore university of technology and design i will share our work named tell-to-design a dataset for language guided floor plan generation published in acl 2023 recently test-conditional generative ai models have demonstrated impressive results in generating high-fidelity images such models generally focus on understanding high-level visual concepts from sentence-level descriptions and the generated images are valued for looking realistic and being creative thereby being more suitable for generating artwork-like images however besides test-conditional image generation the main challenge is to understand the big picture of the entire floor plan from document-level unstructured text with fuzzy and entangled information and we use a normal language modeling objective where x is a set of instructions in natural language and y is the target bounding box sequence and l is the target sequence length table table 2 shows the floor plan generation results on the t2d dataset to evaluate helpful plan generation methods generalized to unseen instructions there is no overlapping between the annotators of the training set and the test set the t2d model achieved the highest iou scores with a micro iou of 554 and a macro iou of 53 and outperformed other text-conditional image generation models by a large margin this can be attributed to our sequence-to-sequence model in controlling the target box sequence generation based on cited information extracted from the language instructions and we can see that although several test-conditional image generation models can produce realistic floor plan images they all fail to align well with the requirements specified in the human instructions and that's all thanks for the listening</sample>
    <sample id="138">作者认为哪些是 nlu 中研究不足的领域？</sample>
    <sample id="139">hello everyone my name is ying and my colleague yan and i will be presenting our research on multi-instruction improving multi-modal zero-shot learning via instruction tuning so with the advances in large language models many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data efficient way recently many studies have shown that instruction tuning enables large language models to perform on unseen multi-modal tasks in a zero-shot manner by following natural instructions however most previous works on instruction tuning focus on improving the zero-shot performance on language-only tasks while computer vision and multi-modal tasks have been left out therefore in this work we want to investigate whether instruction tuning on multi-modal pre-trained models can actually improve the zero-shot capability of pre-trained models thank you</sample>
    <sample id="140">cosript 是经过质量检查的。</sample>
    <sample id="141">now we use our findings from our analysis to design a benchmark for document-level translation for each of the five discourse phenomena we identified we create tags to automatically identify words that pertain to the phenomenon and we call our tagger the multilingual discourse-aware or muda tagger we can then also note that different languages have different proportions of these discourse phenomena we then use the muda tagger by applying the tagger on the parallel corpus that we want to use for evaluation and we apply our translation metrics of choice on the context-dependent examples that the muda tagger has identified and finally we use our benchmark as well as other metrics to evaluate models on the document-level translation system if we use corpus-level metrics alone now we use the muda benchmark to evaluate models and we find that context-aware models are significantly more accurate than models that do not use context on other phenomena like ellipsis pronouns and verb form so this sort of suggests where we would need to see more progress for document-level translation we also compare different commercial systems and our benchmark shows that belle is usually more accurate than google translate for document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation thank you so much for your attention see you in toronto</sample>
    <sample id="142">hi i'm going to talk about our work on resolving indirect referring expressions for entity selection our goal is to understand users' language when they want to make a choice and consider this alternative question did you mean easy on me or i got a feeling which is the alternative question and in the third speech bubble bob says remember that song we were listening to yesterday and with that bob sets the dialogue context in the second speech bubble alice says do you mean easy on me or i got a feeling which is the alternative question and in the third speech bubble bob uses an indirect referring expression to select one of these entities for example here the first one and we show some background text from wikipedia for recipes we additionally show their images again from wikipedia so the annotators know how they look like if the language model has access to the exact same background knowledge as the annotators then the accuracy is really high it's around 92 to 95 but this is not realistic if the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model retrieves the background knowledge if the language model has access to only entity names then the accuracy is only 60 so there's a lot of room for improvement we've also shown that the models are domain generalizable here is a link to our dataset thanks for watching</sample>
    <sample id="143">该方法与哪些现有的 simulst 策略进行了比较 该方法与哪些现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现有的 simulst 策略进行了比较 该方法与现</sample>
    <sample id="144">这篇论文的作者所属机构是什么？</sample>
    <sample id="145">hi everyone i'm jenny a first year phd student at carnegie mellon university and today i'll be presenting our work entitled positionality characterizing design bias of datasets and models this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was</sample>
    <sample id="146">hi everyone i'm jialei chen a phd student from fudan university now i will give you a talk about our paper on the analysis of omission in dialogue summarization first i'm going to briefly introduce the background of dialogue summarization dialogue summarization is a subtask of text summarization it is the process of creating a concise summary that represents the most important information within a dialogue there are many scenarios in dialogue summarization how to extract key information of dialogues in different domains is valuable and worth exploring recent years we have achieved great progress in dialogue summarization especially using large-scale pre-trained language models which can generate fluent and coherent summaries however the generated summaries still have the omission problem which means the generated summaries have the omission words that are hit in the detected utterances to measure the word-level omission recall denoted as f1 score to evaluate our omission detection models furthermore we calculate the percentage of good omission words that are hit in the detected utterances to measure the word-level omission recall denoted as f1 score the figure above shows the label imbalance problem exists in the dataset for the results in the table you can find that the f1 score is around fifty percent this performance indicates that the task is very challenging which calls for more advanced detection models and the refinement based on the detected omission is a promising direction for quality improvement in dialogue summarization that's all for the introduction if there is any question please contact me thank you</sample>
    <sample id="147">hi i'm myra and today i'll be talking about our paper marked personas using natural language prompts to measure stereotypes in language models this work is done in collaboration with essender mush and dan juravsky in recent years many have documented the prevalence of social bias and stereotypes in large language models however these measures have various limitations they usually rely on hand-constructed datasets that are very time-consuming to curate and they also usually only measure very specific stereotypes meaning that they don't generalize well to other demographics or contexts or they simply capture very general broad associations like negative associations with particular groups so in our method we rely on the property that these newer instruction-tuned language models are very good at responding to instructions and prompts so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so here are some example generations from gpt-4 immediately we see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagded individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagmed individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagmed individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagmed individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagmed individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagmed individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagmed individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagmed individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagmed individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagmed individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagmed individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagmed individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever</sample>
    <sample id="148">hi i'm sarah papi from the university of toronto and vlad seleny bruno kessler and i will briefly introduce the attention as a guide for simultaneous speech translation paper that is a joint work with matt dawn negri and marco turci and what is simultaneous speech translation simultaneous speech translation or simulst is the process of translating spoken language into text in another language in real time enabling cross-language communication and what are the problems of the current simulst models specific architectures are usually trained introducing additional modules to be optimized long and complicated training procedures for example training involving different optimization objectives and training and maintaining several models to reach different latency regimes for example training a model with an average of one second latency and another one with two seconds latency and so on so what is our solution first two use already existing offline simulst models without retraining or adopting specific architecture for simulst use only one model for every latency regime and handle latency through specific specific parameters and leverage the knowledge already acquired by the model through the tension mechanism between audio input and textual output that is the cross-attention mechanism and you can see an example on the right our solution is to propose a dot or encoder-decoder attention and it is a strategy for which we decide whether to emit or not a partial translation based on where attention points to and what is emitted if the tension is not concentrated that is if the alpha towards the last lambda speech frames that is lambda speech frames this means that these lambda speech frames will be emitted if you want to discover more results read our paper and we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work thanks for your attention</sample>
    <sample id="149">hello everyone my name is xuhong today i'm going to present our paper do conno 2003 named entity taggers still work in 2023 and last but not least we calculated the percentage change in f1 to assess the generalization of each model so what is needed for good generalization through our experiments we found that from the graph on the right the red best fit line has a gradient that is greater than one this means that every unit of improvement that we made on conno 2003 translates to more than one unit improvement on conno which means that there is no diminishing returns and this shows us that adaptive overfitting in this case is not observed so what about temporal drift then for temporal drift we didn't experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift and kind of surprisingly it is not caused by adaptive overfitting even though conno 2003 has been used for over 20 years so going back to the question that we raised in the title of our paper do conno 2003 taggers still work in 2023 and we found that the answer is actually a resounding yes we hope our paper calls for more research on how to improve generalization of the models and lastly please make sure to check out our paper our data set and if you have any questions feel free to contact me thank you so much</sample>
    <sample id="150">hello everyone i'm archikey and i'll be presenting our acl paper meeting qa extractive question answering on meeting transcripts i'm really thankful to all my collaborators from adobe research and unc chapel hill we know that millions of meetings take place every day worldwide this results in vast amounts of meeting transcripts that can serve as a new domain for nlp research what makes this domain unique and interesting is that meeting transcripts are long documents which are often domain-specific and information-rich however prior works in this area only focus on the task of summarization and extracting action items this means they underutilize the inherently significant qa component in meeting discussions typically a participant in a meeting will ask a question which elicits detailed responses and discussions from others we address this gap by introducing a new dataset called meeting qa an extractive question answering dataset based on questions asked by participants in a meeting and the corresponding answer sentences here is an example from meeting qa with the question shown in red and the answers are highlighted in blue as seen in the figure questions asked by participants are bad at identifying rhetorical questions especially in the zero-shot setting also predicting the single-span models contain more irrelevant sentences than their multi-span counterparts the bottom half of the figure shows that models struggle to identify which speaker answers a question and this gets worse in the zero-shot setting to summarize meeting qa is an interesting dataset based on open-ended and discussion-seeking questions in real-life meeting scenarios and the dataset is far from being solved as it is challenging for existing qa models in both fine-tuned and zero-shot settings thank you so much for listening and you can find more details on our project page or in our paper</sample>
    <sample id="151">大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好</sample>
    <sample id="152">hello everyone my name is frederik riemenschneider and i'm here to talk about our work at the fascinating intersection of nlp and classical philology in this presentation titled exploring large language models for classical philology we will introduce valuable resources for ancient greek and latin moreover we will explore the implications and challenges of multilinguality in these models before we dive in let's take a quick look at the current landscape of language models in classics there have been several models recently developed latin bird was introduced in 2020 then came ancient greek bird in 2021 and another ancient greek bird was released just last year in 2022 so with these advancements it might seem like we're done right well of course not first of all all these models are bird models which means that they are a specific type of encoder-only models moreover these models are all monolingual yet it seems realistic that scholars might want to utilize a model that is proficient in both ancient greek and latin and for english as well so in essence we have varied our models along two dimensions language and architecture the first step in building these models is to gather pre-training data for ancient greek previous models have relied on open greek and latin which we naturally also use but in addition we leveraged previously unused resources and to go a step further we have developed a new pre-training corpus from the internet archive the internet archive contains numerous book scans with ocr transcriptions however as we can see in this example the greek text appears as unreadable rubbish because the ocr did not allow greek characters unfortunately most texts have not been ocr'd with settings allowing greek characters and are not tagged as containing greek texts with this approach our models have elevated lemmatization performance by an impressive five percentage points above the existing state of the art for ancient greek we can also see performance gains for latin lemmatization we also probe our models for their semantic and world knowledge capabilities does our multilingual model perform better because it can learn from three languages results have shown that our models significantly outperform previous models however there doesn't seem to be a significant difference between the performances of the multilingual and the monolingual models this is the case both for semantic knowledge as well as world knowledge to conclude we have introduced new powerful language models for classical philology that are initialized from scratch and use a native tokenizer we pre-trained both encoder-only and encoder-decoder architectures as well as multilingual models so latin and greek texts can be processed by the same model additionally we have introduced a high-quality pre-training dataset for ancient greek we have rigorously benchmarked previous and our own models we have analyzed how t5's encoder behaves and we have investigated the implications of multilinguality in our language models this video provides only a brief overview of what we did for more details check out our paper thank you for your attention</sample>
    <sample id="153">hello my name is nina remehabi i'm a postdoctoral scientist at amazon alexa ai's responsible ai team and i will present our work resolving ambiguities in text to image models in this work we were interested in studying ambiguities in prompts provided to text to image models and our goal is to propose frameworks to mitigate such ambiguities as well as frameworks to evaluate whether the generated images are faithful to users' intention here is our pipeline first we curate a benchmark dataset that covers different types of ambiguities then these prompts are provided to prompt disambiguation framework that tries to gather external signal to disambiguate a prompt through either asking clarifying questions from the user or generating different possible visual setups and the user will interact with the system and provide the answer that satisfies its intention and we are going to obtain a disambiguated prompt by concatenating the signal to the original ambiguous prompt notice that it's possible that the user can also provide the answer that satisfies its intention in question format as input to vqa model and we are going to evaluate whether the generated images are faithful to users' intention we are going to use a vqa model we are going to input the images as well as the human's intention in question format as input to vqa model and we are going to evaluate whether the human's intention is satisfied in the image or not if the answer is yes it means that the user's intention is satisfied so the image is faithful if the answer is no it means that the generation was not faithful to users' intention so with this i'm going to conclude this talk and thank you so much for your attention</sample>
    <sample id="154">the university of toronto and fondecyl bruno kessler and i will briefly introduce the attention as a guide for simultaneous speech translation paper that is a joint work with macdonald negri and marco turci and what are the problems of the current simultaneous speech translation models specific architectures are usually trained introducing additional modules to be optimized long and complicated training procedures for example training involving different optimization objectives and training and maintaining several models to reach different latency regimes for example training a model with an average of one second latency and another one with two seconds latency and so on so what is our solution first to use already existing offline st models without retraining or adopting specific architecture for st and use only one model for every latency regime and handle latency through specific specific parameters and leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output that is the cross attention mechanism and you can see an example on the right our solution is to propose a dot or encoder-decoder attention and it is a strategy for which we decide whether to emit or not a partial translation based on where the attention points to and what is emitted if the attention is not concentrated that is if the sum is below a certain threshold alpha towards the last lambda speech frames meaning that the received information is unstable for example if we receive a speech chunk containing i'm going to talk about and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to lambda lambda lambda speech frames this means that these three words will be</sample>
    <sample id="155">hi and i'm going to talk about our work on resolving indirect referring expressions for entity selection in which we introduce the alt entity corpus and my name is javad hosseini and this is a joint work with philip radlinski sylvia parietti and annie lewis our goal is to understand users' language when they want to make a choice and consider this alternative question did you mean easy on me or i got a feeling which is the alternative question and in the third speech bubble bob says remember that song we were listening to yesterday and with that bob sets the dialogue context in the second speech bubble alice says do you mean easy on me or i got a feeling which is the alternative question and in the third speech bubble bob uses an indirect referring expression to select one of these entities for example the first one and describe them using three to five indirect referring expressions for example the one with the piano music here are some examples from our dataset for example the one without words not the one with the 12-year-old and 12-year-old boy or the fictional one or comes from other voyage and so on the alt entity corpus has 6000 alternative questions across three domains and it has 42000 indirect referring expressions results with t5x large model are summarized below if the language model has access to the exact same background knowledge as the annotators the accuracy is really high it's around 92 to 95 which is more realistic for example when the language model retrieves the background knowledge if the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model retrieves the background knowledge if the language model has access to only entity names then the accuracy is only 60 so there's a lot of room for improvement we've also shown that the models are domain generalizable here is a link to our dataset thanks for watching</sample>
    <sample id="157">hi uh my name is shengao from sun tong university today i'm going to introduce our work uh dialogue summarization with static dynamic structure fusion graph uh this is a joint work with uh xin chong ming julie xiang chen xin peng liu and yang yao the dialogue summarization task is one of the most challenging and interesting tasks in the dialogue summarization research field uh it can help people quickly capture the highlights of a semi-structured and a multi-participant dialogue without reviewing the complex dialogue context uh in this slide uh an example dialogue is shown in the left and uh which is talked about uh three people will go to a concert and uh and the obtained uh speaker interaction frequency matrix for example in this figure we can find that the speaker c usually talks after speaker a which indicates a strong relationship between those we employ a multi-head attention model to calculate the relationship as a integrated static and dynamic graph we propose a fusion method to combine the relation matrix a of the dynamic graph and the dynamic matrix gs of the static graph into a fused a unified graph gu and to incorporate the graph representation which captures the data structure information in the generation process we use the dual cross attention mechanism by proposing a graph attention layer on the top of original self-attention layer thanks for watching and the code and data has been released on github and you can scan the qr code to download it thank you</sample>
    <sample id="158">hi my name is shengwen hu from aws today i will introduce our work dual cache for long document neural coreference resolution the coreference resolution task is to identify the mentions and cluster the mentions that refer to the same entity in this simple example there are three entities and five mentions of them so the goal is to link john with he and link maria with her with this consideration we propose a dual cache that has a local cache and a global cache that they work together the local cache stores local entities with lru eviction policy and the global cache stores global entities with fu eviction policy which evicts least frequently used entity when the global cache is full to evaluate the capability of dual cache we annotated a book with 30000 words we can see that the performance gap is much larger between the baseline and dual cache for book-level document also we show that dual cache significantly reduce the cache miss compared with a single cache last but not least there are always trade-offs between model efficiency and performance for cache-based models but we show that dual cache has the highest performance cost ratio to conclude dual cache uses a local and global cache to separately store local and global entities it outperforms single cache methods and largely reduces the cache miss also dual cache is the most cost-effective compared with single cache methods so that's all for my talk thank you for listening</sample>
    <sample id="159">hi everyone i'm kostas michrakkalis and i'm pleased to welcome you to our talk of our acl 2023 paper language model acceptability judgments are not always robust to context this is a joint work with john gauthier aaron muller kanishka misra gareth flinders roger levy and etienne williams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like blimp syntax gem or acceptability in terms of stereotypes such as cross pairs and in this a minimal pair paradigm the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an unacceptable sentence or an ungrammatical sentence and then the hope is that the model basically puts more probability to the acceptable sentence or the unacceptable sentence depending on whether the chosen prefix is acceptable or unacceptable now this and this is very large like this effect increases throughout the context length and this would probably affect like newer language models which has large context window so why does the match prefix affect the language model judgment so much so we did a series of analysis where we tried to like perturb the input sentence by trying to preserve the relevant structure but adding like noise to the input and after doing like several of these perturbations we find that none of these noises are actually making the model like change its course in terms of how it shows us the mpp judgment trend basically we find that the models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences and the mpp evaluation the way that we do it currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context window please read our paper for more details of our experiments thank you for listening</sample>
    <sample id="160">hi my name is mathias lendemann and today i'm going to give you a brief introduction to our paper on compositional generalization without trees using multiset tagging and latent permutations this is joint work with my advisors alexander kola and ivan titov compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training in the context of semantic parsing testing for compositional generalization might look like this as usual we have a training set of utterances in this case the girl slept and mary knew that the girl slept our model has seen shallower recursion but is tested on an example with deeper recursion conceptually our permutations model works roughly like this we go from left to right over the output and determine which multiset token to put in every position for the first output position we simply select one as highlighted in red then we jump to another multiset token to determine the second token in the output in a similar way by jumping to another multiset token we continue this process until every token from the first stage has been visited exactly once in our paper we solve a couple of interesting technical challenges first of all the alignment between input and output is not given in the training data as a consequence for a given token we don't know which multiset it came from which poses a challenge for training in addition sometimes there are multiple permutations that are consistent with the data but the linguistically correct one is latent we address this by inducing the alignment as part of the training our permutation method is very flexible but it brings the challenge that finding the highest scoring permutation is np-hard that's because this is related to the traveling salesman problem we approximate this with a gpu-friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations if you want to learn more about our experiments and how we address these challenges please have a look at our paper or come to our poster</sample>
    <sample id="161">cosript 中包含了多少个脚本？</sample>
    <sample id="163">hi welcome to our presentation of deplain a new corpus for german text simplification on the document level and on the sentence level my name is regine stodden and i will guide you through the first part of the presentation let's first define text simplification text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group as people with reading problems or non-native speakers to train a text simplification model we require parallel pairs of text for example documents or sentences in the example here you can see a parallel aligned sentence pair of a complex german text and its translation into plain language to simplify the sentence pairs a little bit more so for example on the type of simplification structural simplification or the overall level of simplification and now as we have our dataset deplain which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods and we did some adaptations to the proposed methods and we have published all these adaptations and the codes to run our experiments in the paper uh we concluded that this this basic fine-tuning could produce uh or could get uh scores better than the baseline scores and we propose those results as a benchmark a baseline benchmark for the problem of automatic text simplification in the future thank you so much for your attention and we hope to meet all of you uh during the conference thank you</sample>
    <sample id="164">使用弱监督学习有以下好处： 1. 减少人工标注成本：弱监督学习可以减少人工标注样本的需求，从而降低标注成本。 2. 提高效率：弱监督学习可以提高训练效率，减少训练时间。 3. 提高泛化能力：弱监督学习可以提高模型的泛化能力，使其在未见过的数据上表现得更好。 4. 提高鲁棒性：弱监督学习可以提高模型的鲁棒性，使其对噪声和异常值更具鲁棒性。 5. 提高可解释性：弱监督学习可以提高模型的可解释性，使其更容易被人类理解和解释。 6. 提高可扩展性：弱监督学习可以提高模型的可扩展性，使其能够处理更多的数据和任务。 7. 提高可靠性：弱监督学习可以提高模型的可靠性，使其在各种情况下都能提供可靠的结果。 8. 提高效率：弱监督学习可以提高训练效率，减少训练时间。 9. 提高泛化能力：弱监督学习可以提高模型的泛化能力，使其在未见过的数据上表现得更好。 10. 提高鲁棒性：弱监督学习可以提高模型的鲁棒性，使其对噪声和异常值更具鲁棒性。 11. 提高可解释性：弱监督学习可以提高模型的可解释性，使其更容易被人类理解和解释。 12. 提高可扩展性：弱监督学习可以提高模型的可扩展性，使其能够处理更多的数据和任务。 13. 提高可靠性：弱监督学习可以提高模型的可靠性，使其在各种情况下都能提供可靠的结果。 14. 提高效率：弱监督学习可以提高训练效率，减少训练时间。 15. 提高泛化能力：弱监督学习可以提高模型的泛化能力，使其在未见过的数据上表现得更好。 16. 提高鲁棒性：弱监督学习可以提高模型的鲁棒性，使其对噪声和异常值更具鲁棒性。 17. 提高可解释性：弱监督学习可以提高模型的可解释性，使其更容易被人类理解和解释。 18. 提高可扩展性：弱监督学习可以提高模型的可扩展性，使其能够处理更多的数据和任务。 19. 提高可靠性：弱监督学习可以提高模型的可靠性，使其在各种情况下都能提供可靠的结果。 20. 提高效率：弱监督学习可以提高训练效率，减少训练时间。 21. 提高泛化能力：弱监督学习可以提高模型的泛化能力，使其在未见过的数据上表现得更好。 22. 提高鲁棒性：弱监督学习可以提高模型的鲁棒性，使其对噪声和异常值更具鲁棒性。 23. 提高可解释性：弱监督学习可以提高模型的可解释性，使其更容易被人类理解和解释。 24. 提高可扩展性：弱监督学习可以提高模型的可扩展性，使其能够处理更多的数据和任务。 25. 提高可靠性：弱监督学习可以提高模型的可靠性，使其在各种情况下都能提供可靠的结果。 26. 提高效率：弱监督学习可以提高训练效率，减少训练时间。 27. 提高泛化能力：弱监督学习可以提高模型的泛化能力，使其在未见过的数据上表现得更好。 28. 提高鲁棒性：弱监督学习可以提高模型的鲁棒性，使其对噪声和异常值更具鲁棒性。 29. 提高可解释性：弱监督学习可以提高模型的可解释性，使其更容易被人类理解和解释。 30. 提高可扩展性：弱监督学习可以提高模型的可扩展性，使其能够处理更多的数据和任务。 31. 提高可靠性：弱监督学习可以提高模型的可靠性，使其在各种情况下都能提供可靠的结果。 32. 提高效率：弱监督学习可以提高训练效率，减少训练时间。 33. 提高泛化能力：弱监督学习可以提高模型的泛化能力，使其在未见过的数据上表现得更好。 34. 提高鲁棒性：弱监督学习可以提高模型的鲁棒性，使其对噪声和异常值更具鲁棒性。 35. 提高可解释性：弱监督学习可以提高模型的可解释性，使其更容易被人类理解和解释。 36. 提高可扩展性：弱监督学习可以提高模型的可扩展性，使其能够处理更多的数据和任务。 37. 提高可靠性：弱监督学习可以提高模型的可靠性，使其在各种情况下都能提供可靠的结果。 38. 提高效率：弱监督学习可以提高训练效率，减少训练时间。 39. 提高泛化能力：弱监督学习可以提高模型的泛化能力，使其在未见过的数据上表现得更好。 40. 提高鲁棒性：弱监督学习可以提高模型的鲁棒性，使其对噪声和异常值更具鲁棒性。 41. 提高可解释性：弱监督学习可以提高模型的可解释性，使其更容易被人类理解和解释。 42. 提高可扩展性：弱监督学习可以提高模型的可扩展性，使其能够处理更多的数据和任务。 43. 提高可靠性：弱监督学习可以提高模型的可靠性，使其在各种情况下都能提供可靠的结果。 44. 提高效率：弱监督学习可以提高训练效率，减少训练时间。 45. 提高泛化能力：弱监督学习可以提高模型的泛化能力，使其在未见过的数据上表现得更好。 46. 提高鲁棒性：弱监督学习可以提高模型的鲁棒性，使其对噪声和异常值更具鲁棒性。 47. 提高可解释性：弱监督学习可以提高模型的可解释性，使其更容易被人类理解和解释。 48. 提高可扩展性：弱监督学习可以提高模型的可扩展性，使其能够处理更多的数据和任务。 49. 提高可靠性：弱监督学习可以提高模型的可靠性，使其在各种情况下都能提供可靠的结果。 50. 提高效率：弱监督学习可以提高训练效率，减少训练时间。 51. 提高泛化能力：弱监督学习可以提高模型的泛化能力，使其在未见过的数据上表现得更好。 52. 提高鲁棒性：弱监督学习可以提高模型的鲁棒性，使其对噪声和异常值更具鲁棒性。 53. 提高可解释性：弱监督学习可以提高模型的可解释性，使其更容易被人类理解和解释。 54. 提高可扩展性：弱监督学习可以提高模型的可扩展性，使其能够处理更多的数据和任务。 55. 提高可靠性：弱监督学习可以提高模型的可靠性，使其在各种情况下都能提供可靠的结果。 56. 提高效率：弱监督学习可以提高训练效率，减少训练时间。 57. 提高泛化能力：弱监督学习可以提高模型的泛化能力，使其在未见过的数据上表现得更好。 58. 提高鲁棒性：弱监督学习可以提高模型的鲁棒性，使其对噪声和异常值更具鲁棒性。 59. 提高可解释性：弱监督学习可以提高模型的可解释性，使其更容易被人类理解和解释。 60. 提高可扩展性：弱监督学习可以提高模型的可扩展性，使其能够处理更多的数据和任务。 61. 提高可靠性：弱监督学习可以提高模型的可靠性，使其在各种情况下都能提供可靠的结果。 62. 提高效率：弱监督学习可以提高训练效率，减少训练时间。 63. 提高泛化能力：弱监督学习可以提高模型的泛化能力，使其在未见过的数据上表现得更好。 64. 提高鲁棒性：弱监督学习可以提高模型的鲁棒性，使其对噪声和异常值更具鲁棒性。 65. 提高可解释性：弱监督学习可以提高模型的可解释性，使其更容易被人类理解和解释。 66. 提高可扩展性：弱监督学习可以提高模型的可扩展性，使其能够处理更多的数据和任务。 67. 提高可靠性：弱监督学习可以提高模型的可靠性，使其在各种情况下都能提供可靠的结果。 68. 提高效率：弱监督学习可以提高训练效率，减少训练时间。 69. 提高泛化能力：弱监督学习可以提高模型的泛化能力，使其在未见过的数据上表现得更好。 70. 提高鲁棒性：弱监督学习可以提高模型的鲁棒性，使其对噪声和异常值更具鲁棒性。 71. 提高可解释性：弱监督学习可以提高模型的可解释性，使其更容易被人类理解和解释。 72. 提高可扩展性：弱监督学习可以提高模型的可扩展性，使其能够处理更多的数据和任务。 73. 提高可靠性：弱监督学习可以提高模型的可靠性，使其在各种情况下都能提供可靠的结果。 74. 提高效率：弱监督学习可以提高训练效率，减少训练时间。 75. 提高泛化能力：弱监督学习可以提高模型的泛化能力，使其在未见过的数据上表现得更好。 76. 提高鲁棒性：弱监督学习可以提高模型的鲁棒性，使其对噪声和异常值更具鲁棒性。 77. 提高可解释性：弱监督学习可以提高模型的可解释性，使其更容易被人类理解和解释。 78. 提高可扩展性：弱监督学习可以提高模型的可扩展性，使其能够处理更多的数据和任务。 79. 提高可靠性：弱监督学习可以提高模型的可靠性，使其在各种情况下都能提供可靠的结果。 80. 提高效率：弱监督学习可以提高训练效率，减少训练时间。 81. 提高泛化能力：弱监督学习可以提高模型的泛化能力，使其在未见过的数据上表现得更好。 82. 提高鲁棒性：弱监督学习可以提高模型的鲁棒性，使其对噪声和异常值更具鲁棒性。 83. 提高可解释性：弱监督学习可以提高模型的可解释性，使其更容易被人类理解和解释。 84. 提高可扩展性：弱监督学习可以提高模型的可扩展性，使其能够处理更多的数据和任务。 85. 提高可靠性：弱监督学习可以提高模型的可靠性，使其在各种情况下都能提供可靠的结果。 86. 提高效率：弱监督学习可以提高训练效率，减少训练时间。 87. 提高泛化能力：弱监督学习可以提高模型的泛化能力，使其在未见过的数据上表现得更好。 88. 提高鲁棒性：弱监督学习可以提高模型的鲁棒性，使其对噪声和异常值更具鲁棒性。 89. 提高可解释性：弱监督学习可以提高模型的可解释性，使其更容易被人类理解和解释。 90. 提高可扩展性：弱监督学习可以提高模型的可扩展性，使其能够处理更多的数据和任务。 91. 提高可靠性：弱监督学习可以提高模型的可靠性，使其在各种情况下都能提供可靠的结果。 92. 提高效率：弱监督学习可以提高训练效率，减少训练时间。 93. 提高泛化能力：弱监督学习可以提高模型的泛化能力，使其在未见过的数据上表现得更好。 94. 提高鲁棒性：弱监督学习可以提高模型的鲁棒性，使其对噪声和异常值更具鲁棒性。 95. 提高可解释性：弱监督学习可以提高模型的可解释性，使其更容易被人类理解和解释。 96. 提高可扩展性：弱监督学习可以提高模型的可扩展性，使其能够处理更多的数据和任务。 97. 提高可靠性：弱监督学习可以提高模型的可靠性，使其在各种情况下都能提供可靠的结果。 98. 提高效率：弱监督学习可以提高训练效率，减少训练时间。 99. 提高泛化能力：弱监督学习可以提高模型的泛化能力，使其在未见过的数据上表现得更好。 100. 提高鲁棒性：弱监督学习可以提高模型的鲁棒性，使其对噪声和异常值更具鲁棒性。 101. 提高可解释性：弱监督学习可以提高模型的可解释性，使其更容易被人类理解和解释。 102. 提高可扩展性：弱监督学习可以提高模型的可扩展性，使其能够处理更多的数据和任务。 103. 提高可靠性：弱监督学习可以提高模型的可靠性，使其在各种情况下都能提供可靠的结果。 104. 提高效率：弱监督学习可以提高训练效率，减少训练时间。 105. 提高泛化能力：弱监督学习可以提高模型的泛化能力，使其在未见过的数据上表现得更好。 106. 提高鲁棒性：弱监督学习可以提高模型的鲁棒性，使其对噪声和异常值更具鲁棒性。 107. 提高可解释性：弱监督学习可以提高模型的可解释性，使其更容易被人类理解和解释。 108. 提高可扩展性：弱监督学习可以提高模型的可扩展性，使其能够处理更多的数据和任务。 109. 提高可靠性：弱监督学习可以提高模型的可靠性，使其在各种情况下都能提供可靠的结果。 110. 提高效率：弱监督学习可以提高训练效率，减少训练时间。 111. 提高泛化能力：弱监督学习可以提高模型的泛化能力，使其在未见过的数据上表现得更好。 112. 提高鲁棒性：弱监督学习可以提高模型的鲁棒性，使其对噪声和异常值更具鲁棒性。 113. 提高可解释性：弱监督学习可以提高模型的可解释性，使</sample>
    <sample id="165">hello everyone i'm excited to be here to present our recent paper titled adaptive common sense reasoning exploiting mutually exclusive explanations my name is wen ting zhao and i'm a phd student at cornell university before diving into our approach to adaptive reasoning i will first provide a concrete example to help illustrate what it means followed by a more formal definition in adaptive reasoning starts with a context x emily was stuck in traffic and ends with an outcome y emily made it to her flight additional explanations in z and our answer to this question is yes we introduce an unsupervised learning method called lipoor which stands for likelihood learning with posterior regularization in lipoor we treat explanation z as a latent variable this naturally leads to an unsupervised objective in which we maximize the marginal likelihood of the outcome y given the context x by marginalizing out all the possible explanations in z we then minimize the entropy of p of z given x y by over four absolute point in accuracy and this concludes my talk thank you for listening our paper can be found at tinyurlcom slash jal-lipoor</sample>
    <sample id="166">hello everyone i'm yushi from harbin institute of technology and i'm here today to introduce our new work on neuro symbolic reasoning for image retrieval from linguistically complex tasks this linguistically complex task is a challenging image text reasoning task system one is the visual linguistic interaction the aim is to perform the visual propositions information interaction resembles the system one the output of this module is a matching of propositions and images and the reasoning states then we introduce the neuro symbolic reasoning as system two here we present two cases to further check the performance of our proposed method we can see that our proposed method can present the inference states and the inference results in the middle step so we so the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for solving complex problems the problem method is the process interpretability to conclude we present some suggestions first neuro symbolic calculation may be a first-of-all approach to improve the compositional reasoning and planning of large language models divide and conquer is similar to the surface skimming of the search aiming to decompose the complex reasoning into simple propositions and construct the reasoning path both are effective for</sample>
    <sample id="167">the second use case that we showed in our paper is the case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text we have fine-tuned two different models we have fine-tuned the model of long import to produce document-level simplifications and we also fine-tuned the normal base import to produce sentence-level simplifications you can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper we concluded that this basic fine-tuning could produce or could get scores better than the baseline scores and we proposed those results as a baseline benchmark for the problem of automatic text simplification in the future thank you so much for your attention and we hope to meet all of you during the conference thank you</sample>
    <sample id="168">hello everyone my name is xuhong today i'm going to present our paper do conll 2003 named entity taggers still work in 2023 and last but not least we calculated the percentage change in f1 to assess the generalization of each model so what is needed for good generalization through our experiments we found that from the graph on the right the red best fit line has a gradient that is greater than one this means that every unit of improvement that we made on conll 2003 translates to more than one unit improvement on conll which means that there is no diminishing returns and this shows us that adaptive overfitting in this case is not observed so what about temporal drift then for temporal drift we didn't experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift and kind of surprisingly it is not caused by adaptive overfitting even though conll 2003 has been used for over 20 years so going back to the question that we raised in the title of our paper do conll 2003 taggers still work in 2023 and we found that the answer is actually a resounding yes we hope our paper calls for more research on how to improve generalization of the models and lastly please make sure to check out our paper our data set and if you have any questions feel free to contact me thank you so much</sample>
    <sample id="169">hello everyone my name is alex villar and i will be giving a short overview of the paper prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting prompting</sample>
    <sample id="170">大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好</sample>
    <sample id="171">hello everyone my name is jingwei yi from the university of science and technology of china it's my pleasure to give a short advertisement video of our paper i will copy my model protecting the copyright of embedding as services via backdoor watermark let's first introduce the background about embedding as services currently large language models such as t5 lama palm are exceptional in natural language understanding and generation embedding as services is one of the services built upon large language models to assist the various nlp tasks for example openai offers a t5 based embedding as service however recent works have shown that the attacker may steal the model through learning from the embedding as services therefore it's necessary to protect the copyright of embedding as services to protect the copyright of embedding as services one of the solutions is to embed a watermark in the provider service and detect whether another service contains the watermark the watermark method needs to meet the following properties first the method should be applicable to embedding as services second the watermark should not degrade the utility of the provided embedding as services third the watermark should be covert enough to the attacker or the attacker can remove the watermark easily finally the watermark needs to be transferable to the attacker's services during the model extraction process existing works can be broadly classified into four categories however these methods either are not applicable to embedding as services or lack transferability therefore in this paper we propose a backdoor-based watermark method applicable to embedding as services now let me introduce the details of our embedding as service watermark the watermark method contains two main steps watermark injection and copyright verification before these main steps we first select a trigger set the trigger set is a group of words in a moderate frequency interval we assume the provider can collect a general text corpus and count the word frequency with it the legends of the figures means the number of triggers in each sentence as shown in the figures it's hard to distinguish between the backdoored embeddings and normal embeddings that's all thank you welcome to discuss with us</sample>
    <sample id="172">hello everyone my name is yusen zhang from the penn state university today i'm going to present our work exemplar cross-lingual semantic parsing in multiple natural languages and many representations so semantic parsing is the task to build semantic representations of user queries such as sql and lambda calculus and cross-lingual semantic parsing is the task to translate queries in multiple natural languages into multiple meaning representations as shown in this figure we need to translate the query in multiple natural languages using neural models to sql lambda or funql and etc and we also find many interesting results so regarding analyses of monolingual models we evaluate on two groups of models including encoder pdr which stands for multi-lingual pre-trained encoders with pointer-based decoders such as xlmr plus pdr and bert plus pdr and we also evaluate encoder-decoder models such as coders and blue are still indecisive for cross-lingual semantic parsing tasks um to sum up we build exemplar a unified benchmark for cross-lingual semantic parsing with multiple natural languages and many representations we conduct a comprehensive benchmark study on three representative of types of multi-lingual language models and our result shows many interesting findings and etc and welcome to visit our paper and code thanks for listening</sample>
    <sample id="174">hi i'm priya and i'm one of the co-authors of the paper arg analysis 35k a large scale data set for argument quality analysis in this video i'm going to quickly explain why this data set is unique from other data sets that you will find on a similar topic this is just going to be a quick like overview of the special features that we have so do make sure to check out our paper and our poster at the conference for better insight into the results data set collection process data set annotation process etcetera so very quickly what is argument analysis it's simply judging how good or bad an argument is on a scale of zero to one so something like big banks are bad is likely to be rated low but something like big banks have no accountability take heavy risks and lead to major collapses which is why they should be broken up which is a coherent argument which is more persuasive in terms of what it's trying to achieve it's going to be rated high and this just creates a better diversity in terms of the motions that you will encounter in a parliamentary debate setting rather than just pre-selecting a few motions and like for each theme and just better captures has more arguments and better captures like the relevance that each argument has to a topic right so we think that this data set is just a culmination of like a bunch of unique things and at the end of it you are able to get something that is more diverse that has a score for relevance so you are able to capture how relevant it is to a particular theme you have higher quality of arguments and in general you just have a more reliable scoring because we capture it on instance based level so do make sure to like check out our paper and like give us your feedback on it thanks</sample>
    <sample id="175">hi my name is mathias linderman and today i'm going to give you a brief introduction to our paper on compositional generalization without trees using multi-set tagging and latent permutations this is joint work with my advisors alexander kola and ivan titov compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training in the context of semantic parsing testing for compositional generalization might look like this as usual we have a training set of utterances in this case the girl slept and mary knew that the girl slept our model outperforms the others by a large margin on generalization to deeper recursion some other kinds of structural generalization remain very challenging though in our paper we solve a couple of interesting technical challenges first of all the alignment between input and output is not given in the training data as a consequence for a given token we don't know which multi-set token it came from which poses a challenge for training in addition sometimes there are multiple permutations that are consistent with the data but the linguistically correct one is latent we address this by inducing the alignment as part of the training our permutation method is very flexible but it brings the challenge that finding the highest scoring permutation is np-hard that's because this is related to the traveling salesman problem we approximate this with a gpu-friendly continuous relaxation that also allows us to back-propagate through the solution and learn the linguistically more plausible permutations if you want to learn more about our experiments and how we address these challenges please have a look at our paper or come to our poster</sample>
    <sample id="176">hi i'm jianbin phd student at university of washington today i'm presenting our work from pre-training data to language models to downstream tasks tracking the trails of political biases leading to unfair nlp models so language models are trained on large-scale web crawl data political news media are well covered in their pre-training data according to a survey of the c4 corpus we can see that new york times los angeles times the guardian huffington post etc are well covered in language model training data this has created a mixed blessing for language model applications so on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas on the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream tasks ultimately creating fairness issues if we do try to sanitize somehow we would also risk censorship or exclusion and it's incredibly hard to determine what is actually neutral and should be retaining language model training data so it's kind of like the electric chollie problem okay great i think that's pretty much all i have for today thank you for your time</sample>
    <sample id="177">演讲者的名字是什么？</sample>
    <sample id="178">hi everyone i'm kostas daskalakis and i'm pleased to welcome you to our talk of our acl 2023 papers language model acceptability judgments are not always robust to context so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like blimp syntax gem or acceptability in terms of stereotypes such as cross pairs and in this a minimal pair paradigm the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an unacceptable sentence or an ungrammatical sentence and then the hope is that the model's acceptability judgments are actually impacted by any context like whether the context is coming from a different uh subset of the data set or whether it's like completely irrelevant to the current uh like to the sentence that we are looking at so how does the model do so first we look at the wikipedia sentences which are completely irrelevant to the current query pair and there we find that the mpp judgments are relatively stable now what happens when we choose sentences from the same data set so here we are choosing or creating sentences from acceptable and unacceptable domains from the same blimp or syntax gem data set and then we see that the mpp judgments either increase or decrease significantly when you add uh like noise to the input and after doing like several of these perturbations we find that none of these noises are actually making the model uh like change its course in terms of how it shows us the mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see</sample>
    <sample id="179">we present symbolic tom a plug-and-play method to improve theory-of-mind reasoning skills in large language models using explicit graphical representations symbolic tom uses several graphical representations since mental states cannot be represented with a single graph for example on the left we see a representation of what bob believes is the current world state and on the right we see a representation of what bob thinks that alice believes is the current world state we analyze in-domain performance in the well-known tommy dataset and we evaluate robustness with two out-of-domain setups that we designed let's look at the in-domain performance results for second-order false-belief questions and all others can be found in the paper for testing our method's generalization capabilities we design two new datasets both modifying the tommy benchmark we test for story structure generalization by for example concatenating two stories and asking half the time about the first story and half the time about the second one this is the d1 dataset and we design two more d2 and d3 we also test for linguistic generalization which is our second dataset called paraphrase tommy generating a dataset that has more linguistic diversity this is important since tommy is generated automatically with only one way of phrasing each sentence for the story structure generalization datasets we observe that supervised models heavily degrade the performance on the three datasets we created for example showing around 50 performance in the dataset d1 that i just described on the other hand using symbolic tom still shows significant gains for all models allowing stronger models like gpt-4 to fully solve the datasets giving for example a 42 point accuracy boost for dataset d1 so in conclusion we introduce symbolic tom a plug-and-play method to improve theory-of-mind reasoning skills in large language models it is an inference time algorithm which avoids overfitting risk it uses explicit graphical symbolic representation which yields more interpretable reasoning and symbolic tom dramatically improves out-of-the-box llm performance outperforming supervised approaches on out-of-domain story understanding and remaining beneficial on the new linguistic diversity dataset paraphrase tommy for more details please refer to the paper and don't hesitate to reach out to chat thank you so much for listening</sample>
    <sample id="180">hi i'm myra and today i'll be talking about our paper marked personas using natural language prompts to measure stereotypes in language models this work is done in collaboration with essender mush and dan juravsky in recent years many have documented the prevalence of social bias and stereotypes in large language models or lls however these measures have various limitations they usually rely on hand-constructed datasets that are very time-consuming to curate and they also usually only measure very specific stereotypes meaning that they don't generalize well to other demographics or contexts or they simply capture very general broad associations like negative associations with particular groups so in our method we rely on the property that these newer instruction-tuned lls are very good at responding to instructions and prompts so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very</sample>
    <sample id="181">hi i'm siyuan from funan university i'm here to introduce our work distinguish script knowledge from large language models for constrained language planning in everyday life humans often plan their actions by following step-by-step instructions in the form of written scripts previous work has explored large language models to plan for abstract goals of stereotypical activities such as make a cake and showed that large language models can effectively decompose those into steps however previous work mainly focuses on planning for abstract goals with specific goals specific constraints such as make a chocolate cake still remains understudied in this paper we define the problem of constrained language planning which imposes different constraints on the goals of planning an abstract goal can be represented by different real-life specific goals with multiple facets we develop a over-generated filter method for large language models we apply our method for building a dataset of constrained language planning named as co-script in total we generate 55000 specific goals with scripts to ensure the quality of validation and test sites we ask crowdsourced workers to find and revise the incorrect samples this figure shows the constraint distribution of co-script we find that co-script can generate scripts of higher quality than most large language models indicating that smaller models can support large language models when properly trained on suitable datasets in summary a way established the constrained language planning problem we evaluate the constrained language planning ability of large language models and develop a over-generated filter method for large language models we use a large language models to generate a high-quality dataset co-script for constrained language planning we hope co-script data set can be a valuable resource to advance the research on language planning thanks for your time please find more details of co-script in our paper</sample>
    <sample id="182">in recent years many have documented the prevalence of social bias and stereotypes in large language models however these measures have various limitations they usually rely on hand-constructed datasets that are very time-consuming to curate and they also usually only measure very specific stereotypes meaning that they don't generalize well to other demographics or contexts or they simply capture very general broad associations like negative associations with particular groups so in our method we rely on the property that these newer instruction-tuned language models are very good at responding to instructions and prompts so we can ask the model to generate personas which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this</sample>
    <sample id="183">hi i'm myra and today i'll be talking about our paper marked personas using natural language prompts to measure stereotypes in language models this work is done in collaboration with essender mush and dan juravsky in recent years many have documented the prevalence of social bias and stereotypes in large language models however these measures have various limitations they usually rely on hand-constructed datasets that are very time-consuming to curate and they also usually only measure very specific stereotypes meaning that they don't generalize well to other demographics or contexts or they simply capture very general broad associations like negative associations with particular groups so in our method we rely on the property that these newer instruction-tuned language models are very good at responding to instructions and prompts so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic</sample>
    <sample id="184">本文中使用了什么来衡量语境使用情况？</sample>
    <sample id="185">hi i am yannis lavrac and i will present you our work on dr bert a robust pre-trained model in french for biomedical and clinical domain in this presentation we first talk about language modeling in healthcare then we present the main contribution of our article we introduce the first french biomedical model dr bert which is based on anonymized data obtained from the non-university hospital that we house afterward we ask ourselves how much data do we need to train a specialized model on french data is it four gigabytes eight gigabytes or more to answer this question we first train and compare four from scratch models a first version of dr bert with four gigabytes of natschos a second version of four gigabytes of natschos a first version of schubert which is a clinical model with four gigabytes of clinical notes and finally one based on the english biomedical model dr bert and trained on the four gigabytes of natschos show comparable results to those obtained with dr bert four gigabytes from scratch which is not the case for the model based on camanber weights and tokenizer which suffer from stability issues finally as a conclusion our proper system offers better performance on nine of the eleven down-stream tasks and surpass globally the result of the generic model here camanber we also observe that specialize data is better more specialized data is better but it doesn't scale well all the pre-trained models obtained from natschos are freely available on yugenface and all the training script are on our github repository so thank you for for for this presentation and we are looking forward to exchanging at the poster session in toronto</sample>
    <sample id="187">hello everyone my name is ying and my colleague yan and i will be presenting our research on multi-instruction improving multi-modal zero-shot learning via instruction tuning so with the advances in large language models many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data efficient way recently many studies have shown that instruction tuning enables large language models to perform on unseen multi-modal tasks in a zero-shot manner by following natural instructions however most previous works on instruction tuning focus on improving the zero-shot performance on language-only tasks while computer vision and multi-modal tasks have been left out therefore in this work we want to investigate whether instruction tuning on multi-modal pre-trained models can actually improve the zero-shot capability of pre-trained models thank you</sample>
    <sample id="188">hello my name is vasudha and i'm a computer science phd candidate at stony brook university i would like to present our work accepted into acl 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge we begin by defining cognitive dissonance and why it is an important problem to study in language simply put cognitive dissonance is two beliefs or actions that are inconsistent such as this example where a person states i know that cigarettes could kill me and then goes on to say i grabbed a couple of smokes after the meeting this belief and action are inconsistent and they are in dissonance further mentioning that i don't think i could keep my job without them justifies the second occurrence and they have a dissonance relationship since the initial model was not able to capture the dissonance class at all we start the cold start the active learning process by transferring weights from closely related tasks topic independent dissonance stance classification a task that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic we call them ce here we find that on transferring the zero shot performance on the annotated dataset is already much better than chance with the best with a uc of 075 which is the best performance that we have on the task so far we also check the feasibility of each strategy for annotation quality and costs to annotators we find that prc has the highest percentage of dissonance and works best for rare class however the annotators also find the examples difficult in summary we find that prc is a simple ael strategy for rare class acquisition and cold starting ael with appropriately designed transfer learning tasks can help significantly we also find that iterative update is useful for transfer learning from a different domain whereas in-domain active annotations benefit from cumulative update these are the links to our code dataset and our paper feel free to get in touch with us if you have any questions thank you</sample>
    <sample id="189">hi and i'm going to talk about our work on resolving indirect referring expressions for entity selection in which we introduce the alt entity corpus and my name is jawad hussaini and this is a joint work with philip radlinski sylvia parretti and annie lewis our goal is to understand users' language when they want to make a choice and consider this alternative question did you mean easy on me or i got a feeling which is the alternative question and in the third speech bubble bob says remember that song we were listening to yesterday and with that bob sets the dialogue context in the second speech bubble alice says do you mean easy on me or i got a feeling which is the alternative question and in the third speech bubble bob uses an indirect referring expression to select one of these entities for example the first one and describe them using three to five indirect referring expressions for example the one with the piano music here are some examples from our dataset if the language model has access to the exact same background knowledge as the annotators the accuracy is really high it's around 92 to 95 percent which is more realistic for example when the language model retrieves the background knowledge if the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 percent which is more realistic for example when the language model retrieves the background knowledge if the language model has access only to entity names then the accuracy is only 60 so there's a lot of room for improvement we've also shown that the models are domain generalizable here is a link to our dataset thanks for watching</sample>
    <sample id="190">攻击者如何通过 eaaS 来提取模型参数？攻击者通过 eaaS 来提取模型参数的过程包括以下几个步骤：首先，攻击者会利用 eaaS 提供的服务来获取模型参数。然后，攻击者会利用这些参数来训练自己的模型，从而获得与原始模型相似的模型。此外，攻击者还可以利用这些模型来进行各种自然语言处理任务，例如文本分类、情感分析等。因此，攻击者通过 eaaS 来提取模型参数是通过利用 eaaS 提供的服务来获取模型参数，然后利用这些参数来训练自己的模型，从而获得与原始模型相似的模型。此外，攻击者还可以利用这些模型来进行各种自然语言处理任务，例如文本分类、情感分析等。</sample>
    <sample id="191">这篇论文有多少位作者？</sample>
    <sample id="192">hi everyone we're happy to be here and give a short presentation today i'm yang liu today i'm going to give a presentation on our work confidence-guided memory efficient optimizer we've introduced two scenarios to demonstrate how two types of erroneous updates are supposed to be handled in the ideal case in for example in figure 8 the difference between the momentum of update mt and the current update ut we take the residual between them as the stability in the preserved momentum and the state generated instability as the denominator of original mt to more adaptively take an updating step in the experimental results demonstrate the efficiency of our proposed can optimizer by showing that bird-based model trained with can on two batch sizes both achieve comparable performance to the baseline with less memory cost over the existing sm3 memory efficient optimizer here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizer and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports adaptive confidence-based updating guided by the residual between predicted update and generated update and extensive experiments show that our can achieves outstanding effectiveness on large language model training tasks moreover our can works well for large batch training which serves an important extension for existing memory efficient optimizers and here inspired by the erroneous update in existing memory efficient optimizers we propose a confidence-guided memory efficient optimizer can which supports</sample>
    <sample id="193">hello my name is vasudha and i am a computer science phd candidate at stony brook university i would like to present our work accepted into acl 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge we begin by defining cognitive dissonance and why it is an important problem to study in language simply put cognitive dissonance is two beliefs or actions that are inconsistent such as this example where a person states i know that cigarettes could kill me and then goes on to say i grabbed a couple of smokes after the meeting this belief and action are inconsistent and they are in dissonance further mentioning that i don't think i could keep my job without them justifies the second occurrence and they have a dissonance relationship while dissonance is a very common phenomenon we experience in daily decision-making they are really rare to find expressed in language among other kinds of discourse relations so why does this matter studying cognitive dissonance can help us understand the effects of disagreement among people track trends in belief values and attitude changes in population high cognitive dissonance is also related to anxiety disorders and can help us understand people's mental health better studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups finally on iteratively fine-tuning on both tasks we find that fine-tuning on ce task followed by further fine-tuning on debate yields a much better zero-shot performance thus this is the model that we use to cold-start the active learning and annotations cumulative accumulates all the data collected from active annotations so far whereas iterative updates the model by training on the latest set of data collected thus far we also find that iterative update is useful for transfer learning from a different domain whereas in-domain active annotations benefit from cumulative update these are the links to our code data set and our paper feel free to get in touch with us if you have any questions thank you</sample>
    <sample id="194">hi everyone i'm jenny a first year phd student at carnegie mellon university and today i'll be presenting our work entitled positionality characterizing design bias in ai datasets and models this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work</sample>
    <sample id="195">hello everyone today i will introduce our work reasoning over hierarchical question decomposition tree for explainable question answering explainable question answering aims to answer a given question and provide an explanation why the answer is selected recent work in xqa can be grouped into two directions neuro-symbolic methods which translate natural language questions into formal representations such as sparql and decomposition-based methods which generate natural language intermediate questions based on the grouped leaf questions according to the reference tokens besides we also compute a certainty score of each node based on the likelihood to represent the certainty of its generation after building the hqdt we can solve the complex question we are conducting probabilistic reasoning over the hqdt the reasoning process is from the root to leaves in a recursive way and contains three steps for each node firstly a scheduler will determine the appropriate knowledge sources for this node the sources include a kb a text corpus or solving its children recursively and sequentially secondly the corresponding executors will get the answers with probabilities from the selected knowledge sources the aggregator will aggregate the candidate answers from all the knowledge sources and output top k answers with the highest probabilities demonstrating the effectiveness of utilizing knowledge from kb that's all thanks for your listening</sample>
    <sample id="196">hi my name is adam sperkowski and this talk is about the dependency structure of coordination as you may know there are different dependency structures assumed by different theories and corpus approaches so for example in universal dependencies the structure of the coordination lisa bart and maggie is such that the first conjunct is the head of the whole coordination structure so in this case lisa bart and maggie however this effect may be ameliorated when the direct object is very heavy and very long because then it can be moved to the position after the adjunct this is illustrated here so both these sentences are fine right it violates one principle but it satisfies another one okay so what we did we extracted various statistics from about coordination from the enhanced version of the pen tree bank and see the paper why we wouldn't use universal dependencies and these statistics confirm the observation made many times before that left conjuncts tend to be shorter so salt and pepper and not pepper and salt measured in syllables and also the observation that was made in passing that this tendency grows with length the length difference so when the difference between the lengths of the two conjuncts grows the shorter conjunct prefers to be the first one stronger right so the proportion is is bigger of the left uh short conjuncts but what's novel uh in this paper is that we observed that this tendency only occurs when the governor is on the left is absent right so the governor is on the right this tendency disappears so we showed that by measuring length in characters that's the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination sentences but when the governor is on the right this tendency disappears and we showed in the paper how this provides an argument against asymmetric structures of coordination as these two and for the symmetric structures as these two so see the paper for the full agreement and arguments sorry and talk to us about the poster session thank you</sample>
    <sample id="197">hello i'm james finch and i'm sarah finch and today we'll tell you all about abc eval a new dimensional approach to evaluating conversational ai this work was done by the emory nlp lab led by professor gino choi at emory university and in collaboration with amazon alexa ai so let's say that you just developed a dialogue model and you want to see how well it compares against the current state of the art the common practice is to use human judges to evaluate several dimensions of dialogue quality to understand the strengths and weaknesses of the model on a finer grained level one approach is to simply ask human judges to evaluate several dimensions of dialogue quality such as the relevance of model responses using existing comparative or liker scale methods however we believe there is a more precise and reliable strategy for dimensional dialogue evaluation our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself or its partner hallucinating incorrect facts or violating common sense knowledge and when the model succeeds or fails to show empathy to determine what kind of evaluation is most effective we selected four state-of-the-art chat models and evaluated them on 100 human bot conversations per model using abc eval for comparison we also evaluated these conversations using three existing methods liker ratings on the turn level liker ratings on the dialogue level and dialogue level pairwise comparisons for each of the existing methods we collected evaluations on eight of the most commonly measured aspects of dialogue since this is the standard practice for evaluating chat models along multiple dimensions with the rapid pace of improvement in the field many of these error rates could see a decrease in new models released since our evaluation was conducted however this is all the more reason to pursue reliable and precise evaluation metrics for comparing models we hope abc eval can be leveraged by others in the field as a meaningful step in this direction and we look forward to seeing how conversational ai will advance in the coming months and years thank you for watching</sample>
    <sample id="198">hi everyone i'm kostas michrakkalis and i'm pleased to welcome you to our talk of our acl 2023 papers language model acceptability judgments are not always robust to context this is a joint work with john gauthier aaron muller kanishka misra gareth flinders roger levy and edina williams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like blimp syntax gem and acceptability in terms of stereotypes such as cross pairs and in this a minimal pair paradigm the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an unacceptable sentence or an ungrammatical sentence and then the hope is that the model basically puts more probability to the acceptable sentences and similar ways that is when we choose sentences from the same dataset or a different dataset so that is what we are trying to do here we are trying to revisit the minimal pair paradigm the way that we do it currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context window please read our paper for more details of our experiments thank you for listening hi everyone i'm kostas michrakkalis and i'm pleased to welcome you to our talk of our acl 2023 papers language model acceptability judgments are not always robust to context this is a joint work with john gauthier aaron muller kanishka misra gareth flinders roger levy and edina williams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like blimp syntax gem and acceptability in terms of stereotypes such as cross pairs and in this a minimal pair paradigm the typical way to evaluate language models is that you show like an unacceptable sentence or an ungrammatical sentence and then you show an acceptable sentence or a grammatical sentence and then the hope is that the model basically puts more probability to the acceptable sentences and similar ways that is when we choose sentences from the same dataset or a different dataset so that is what we are trying to do here we are trying to revisit the minimal pair paradigm the way that we do it currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context window please read our paper for more details of our experiments thank you for listening hi everyone i'm kostas michrakkalis and i'm pleased to welcome you to our talk of our acl 2023 papers language model acceptability judgments are not always robust to context this is a joint work with john gauthier aaron muller kanishka misra gareth flinders roger levy and edina williams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like blimp syntax gem and acceptability in terms of stereotypes such as cross pairs and in this a minimal pair paradigm the typical way to evaluate language models is that you show like an unacceptable sentence or an ungrammatical sentence and then you show an acceptable sentence or a grammatical sentence and then the hope is that the model basically puts more probability to the acceptable sentences and similar ways that is when we choose sentences from the same dataset or a different dataset so that is what we are trying to do here we are trying to revisit the minimal pair paradigm the way that we do it currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context window please read our paper for more details of our experiments thank you for listening hi everyone i'm kostas michrakkalis and i'm pleased to welcome you to our talk of our acl 2023 papers language model acceptability judgments are not always robust to context this is a joint work with john gauthier aaron muller kanishka misra gareth flinders roger levy and edina williams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like blimp syntax gem and acceptability in terms of stereotypes such as cross pairs and in this a minimal pair paradigm the typical way to evaluate language models is that you show like an unacceptable sentence or an ungrammatical sentence and then you show an acceptable sentence or a grammatical sentence and then the hope is that the model basically puts more probability to the acceptable sentences and similar ways that is when we choose sentences from the same dataset or a different dataset so that is what we are trying to do here we are trying to revisit the minimal pair paradigm the way that we do it currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context window please read our paper for more details of our experiments thank you for listening hi everyone i'm kostas michrakkalis and i'm pleased to welcome you to our talk of our acl 2023 papers language model acceptability judgments are not always robust to context this is a joint work with john gauthier aaron muller kanishka misra gareth flinders roger levy and edina williams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like blimp syntax gem and acceptability in terms of stereotypes such as cross pairs and in this a minimal pair paradigm the typical way to evaluate language models is that you show like an unacceptable sentence or an ungrammatical sentence and then you show an acceptable sentence or a grammatical sentence and then the hope is that the model basically puts more probability to the acceptable sentences and similar ways that is when we choose sentences from the same dataset or a different dataset so that is what we are trying to do here we are trying to revisit the minimal pair paradigm the way that we do it currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context window please read our paper for more details of our experiments thank you for listening hi everyone i'm kostas michrakkalis and i'm pleased to welcome you to our talk of our acl 2023 papers language model acceptability judgments are not always robust to context this is a joint work with john gauthier aaron muller kanishka misra gareth flinders roger levy and edina williams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like blimp syntax gem and acceptability in terms of stereotypes such as cross pairs and in this a minimal pair paradigm the typical way to evaluate language models is that you show like an unacceptable sentence or an ungrammatical sentence and then you show an acceptable sentence or a grammatical sentence and then the hope is that the model basically puts more probability to the acceptable sentences and similar ways that is when we choose sentences from the same dataset or a different dataset so that is what we are trying to do here we are trying to revisit the minimal pair paradigm the way that we do it currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context window please read our paper for more details of our experiments thank you for listening hi everyone i'm kostas michrakkalis and i'm pleased to welcome you to our talk of our acl 2023 papers language model acceptability judgments are not always robust to context this is a joint work with john gauthier aaron muller kanishka misra gareth flinders roger levy and edina williams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like blimp syntax gem and acceptability in terms of stereotypes such as cross pairs and in this a minimal pair paradigm the typical way to evaluate language models is that you show like an unacceptable sentence or an ungrammatical sentence and then you show an acceptable sentence or a grammatical sentence and then the hope is that the model basically puts more probability to the acceptable sentences and similar ways that is when we choose sentences from the same dataset or a different dataset so that is what we are trying to do here we are trying to revisit the minimal pair paradigm the way that we do it currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context window please read our paper for more details of our experiments thank you for listening hi everyone i'm kostas michrakkalis and i'm pleased to welcome you to our talk of our acl 2023 papers language model acceptability judgments are not always robust to context this is a joint work with john gauthier aaron muller kanishka misra gareth flinders roger levy and edina williams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like blimp syntax gem and acceptability in terms of stereotypes such as cross pairs and in this a minimal pair paradigm the typical way to evaluate language models is that you show like an unacceptable sentence or an ungrammatical sentence and then you show an acceptable sentence or a grammatical sentence and then the hope is that the model basically puts more probability to the acceptable sentences and similar ways that is when we choose sentences from the same dataset or a different dataset so that is what we are trying to do here we are trying to revisit the minimal pair paradigm the way that we do it currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context window please read our paper for more details of our experiments thank you for listening hi everyone i'm kostas michrakkalis and i'm pleased to welcome you to our talk of our acl 2023 papers language model acceptability judgments are not always robust to context this is a joint work with john gauthier aaron muller kanishka misra gareth flinders roger levy and edina williams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like blimp syntax gem and acceptability in terms of stereotypes such as cross pairs and in this a minimal pair paradigm the typical way to evaluate language models is that you show like an unacceptable sentence or an ungrammatical sentence and then you show an acceptable sentence or a grammatical sentence and then the hope is that the model basically puts more probability to the acceptable sentences and similar ways that is when we choose sentences from the same dataset or a different dataset so that is what we are trying to do here we are trying to revisit the minimal pair paradigm the way that we do it currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context window please read our paper for more details of our experiments thank you for listening hi everyone i'm kostas michrakkalis and i'm pleased to welcome you to our talk of our acl 2023 papers language model acceptability judgments are not always robust to context this is a joint work with john gauthier aaron muller kanishka misra gareth flinders roger levy and edina williams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like blimp syntax gem and acceptability in terms of stereotypes such as cross pairs and in this a minimal pair paradigm the typical way to evaluate language models is that you show like an unacceptable sentence or an ungrammatical sentence and then you show an acceptable sentence or a grammatical sentence and then the hope is that the model basically puts more probability to the acceptable sentences and similar ways that is when we choose sentences from the same dataset or a different dataset so that is what we are trying to do here we are trying to revisit the minimal pair paradigm the way that we do it currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context window please read our paper for more details of our experiments thank you for listening hi everyone i'm kostas michrakkalis and i'm pleased to welcome you to our talk of our acl 2023 papers language model acceptability judgments are not always robust to context this is a joint work with john gauthier aaron muller kanishka misra gareth flinders roger levy and edina williams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like blimp syntax gem and acceptability in terms of stereotypes such as cross pairs and in this a minimal pair paradigm the typical way to evaluate language models is that you show like an unacceptable sentence or an ungrammatical sentence and then you show an acceptable sentence or a grammatical sentence and then the hope is that the model basically puts more probability to the acceptable sentences and similar ways that is when we choose sentences from the same dataset or a different dataset so that is what we are trying to do here we are trying to revisit the minimal pair paradigm the way that we do it currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context window please read our paper for more details of our experiments thank you for listening hi everyone i'm kostas michrakkalis and i'm pleased to welcome you to our talk of our acl 2023 papers language model acceptability judgments are not always robust to context this is a joint work with john gauthier aaron muller kanishka misra gareth flinders roger levy and edina williams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like blimp syntax gem and acceptability in terms of stereotypes such as cross pairs and in this a minimal pair paradigm the typical way to evaluate language models is that you show like an unacceptable sentence or an ungrammatical sentence and then you show an acceptable sentence or a grammatical sentence and then the hope is that the model basically puts more probability to the acceptable sentences and similar ways that is when we choose sentences from the same dataset or a different dataset so that is what we are trying to do here we are trying to revisit the minimal pair paradigm the way that we do it currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context window please read our paper for more details of our experiments thank you for listening hi everyone i'm kostas michrakkalis and i'm pleased to welcome you to our talk of our acl 2023 papers language model acceptability judgments are not always robust to context this is a joint work with john gauthier aaron muller kanishka misra gareth flinders roger levy and edina williams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like blimp syntax gem and acceptability in terms of stereotypes such as cross pairs and in this a minimal pair paradigm the typical way to evaluate language models is that you show like an unacceptable sentence or an ungrammatical sentence and then you show an acceptable sentence or a grammatical sentence and then the hope is that the model basically puts more probability to the acceptable sentences and similar ways that is when we choose sentences from the same dataset or a different dataset so that is what we are trying to do here we are trying to revisit the minimal pair paradigm the way that we do it currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context window please read our paper for more details of our experiments thank you for listening hi everyone i'm kostas michrakkalis and i'm pleased to welcome you to our talk of our acl 2023 papers language model acceptability judgments are not always robust to context this is a joint work with john gauthier aaron muller kanishka misra gareth flinders roger levy and edina williams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like blimp syntax gem and acceptability in terms of stereotypes such as cross pairs and in this a minimal pair paradigm the typical way to evaluate language models is that you show like an unacceptable sentence or an ungrammatical sentence and then you show an acceptable sentence or a grammatical sentence and then the hope is that the model basically puts more probability to the acceptable sentences and similar ways that is when we choose sentences from the same dataset or a different dataset so that is what we are trying to do here we are trying to revisit the minimal pair paradigm the way that we do it currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context window please read our paper for more details of our experiments thank you for listening hi everyone i'm kostas michrakkalis and i'm pleased to welcome you to our talk of our acl 2023 papers language model acceptability judgments are not always robust to context this is a joint work with john gauthier aaron muller kanishka misra gareth flinders roger levy and edina williams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like blimp syntax gem and acceptability in terms of stereotypes such as cross pairs and in this a minimal pair paradigm the typical way to evaluate language models is that you show like an unacceptable sentence or an ungrammatical sentence and then you show an acceptable sentence or a grammatical sentence and then the hope is that the</sample>
    <sample id="199">与单语模型相比，多语言训练可能会导致表现下降。 与单语模型相比，多语言训练可能会导致表现下降。 与单语模型相比，多语言训练可能会导致表现下降。 与单语模型相比，多语言训练可能会导致表现下降。 与单语模型相比，多语言训练可能会导致表现下降。 与单语模型相比，多语言训练可能会导致表现下降。 与单语模型相比，多语言训练可能会导致表现下降。</sample>
    <sample id="200">hi and i'm going to talk about our work on resolving indirect referring expressions for entity selection in which we introduce the alt entity corpus and my name is javad hosseini and this is a joint work with philip radlinski sylvia parietti and annie lewis our goal is to understand users' language when they want to make a choice and consider this alternative question did you mean easy on me or i got a feeling which is the alternative question and in the third speech bubble bob says remember that song we were listening to yesterday and with that bob sets the dialogue context in the second speech bubble alice says do you mean easy on me or i got a feeling which is the alternative question and in the third speech bubble bob uses an indirect referring expression to select one of these entities for example the first one and describe them using three to five indirect referring expressions for example the one with the piano music here are some examples from our data set thanks for watching if the language model has access to the exact same background knowledge as the annotators the accuracy is really high it's around 92 to 95 percent which is more realistic for example when the language model retrieves the background knowledge if the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 percent which is more realistic for example when the language model retrieves the background knowledge if the language model has access to only entity names then the accuracy is only 60 so there's a lot of room for improvement we've also shown that the models are domain generalizable here is a link to our data set thanks for watching</sample>
    <sample id="201">评估使用了哪些 mt（机器翻译）指标？ 评估使用了哪些 mt（机器翻译）指标？ 评估使用了哪些 mt（机器翻译）指标？ 评估使用了哪些 mt（机器翻译）指标？ 评估使用了哪些 mt（机器翻译）指标？ 评估使用了哪些 mt（机器翻译）指标</sample>
    <sample id="202">hello everyone my name is xuhong today i'm going to present our paper do conll 2003 named entity taggers still work well in 2023 and last but not least we calculated the percentage change in f1 to assess the generalization of each model so what is needed for good generalization through our experiments we found that from the graph on the right the red best fit line has a gradient that is greater than one this means that every unit of improvement that we made on conll 2003 translates to more than one unit improvement on conll which means that there is no diminishing returns and this shows us that adaptive overfitting in this case is not observed so what about temporal drift then for temporal drift we didn't experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift and kind of surprisingly it is not caused by adaptive overfitting even though conll 2003 has been used for over 20 years so going back to the question that we raised in the title of our paper do conll 2003 taggers still work in 2023 and we found that the answer is actually a resounding yes we hope our paper calls for more research on how to improve generalization of the models and lastly please make sure to check out our paper our data set and if you have any questions feel free to contact me thank you so much</sample>
    <sample id="203">hi everyone i'm jenny a first year phd student at carnegie mellon university and today i'll be presenting our work on positionality characterizing design bias in ai datasets and models this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was</sample>
    <sample id="204">hello everyone my name is yusen zhang from the penn state university today i'm going to present our work exemplar cross-lingual semantic parsing in multiple natural languages and many representations so semantic parsing is the task to build semantic representations of user queries such as sql and lambda calculus and cross-lingual semantic parsing is the task to translate queries in multiple natural languages into multiple meaning representations as shown in this figure we need to translate the query in multiple natural languages using neural models to sql lambda or funql and etc and we also find many interesting results so regarding analyses of monolingual models we evaluate on two groups of models including encoder pdr which stands for multi-lingual pre-trained encoders with pointer-based decoders such as xlmr plus pdr and bert plus pdr and we also evaluate encoder-decoder models which is multi-lingual pre-trained encoder-decoder models such as mbert and mt5 we found that encoder-decoder obtains the best performance on all nine data sets and only gains in three data sets i think this is known as the curse of multi-linguality we also compare the cross-lingual performance gap in this figure the blue line is cross-lingual fused transfer the orange line is cross-lingual zero-shot transfer while the green line is the monolingual setting we found that by comparing the green and orange line we found that for zero-shot setting the cross-lingual transfer performance gap is significant and by comparing blue and orange line we found that for few-shot setting the transfer gap is shortened rapidly we also find some other interesting findings for example um encoder-decoder outperforms previous work or achieved comparable results for training on english natural language and significantly boost the performance of few-shot on target natural languages and we found monolingual language models such as coders and blue are still inferior for cross-lingual semantic parsing tasks um to sum up we build exemplar a unified benchmark for cross-lingual semantic parsing with multiple natural languages and many representations we conduct a comprehensive benchmark study on three representative of types of multi-lingual language models and our result shows many interesting findings and etc and welcome to visit our paper and code thanks for listening</sample>
    <sample id="205">hi i'm jianbin phd student at university of washington today i'm presenting our work from pre-training data to language models to downstream tasks tracking the trails of political biases leading to unfair nlp models so language models are trained on large-scale web crawl data political news media are well covered in their pre-training data according to a survey of the c4 corpus we can see that the new york times los angeles times the guardian huffington post etc are well covered in language model training data this has created a mixed blessing for language model applications so on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas on the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream tasks ultimately creating fairness issues if we do try to sanitize somehow we would also risk censorship or exclusion and it's incredibly hard to determine what is actually neutral and should be retaining language model training data so it's kind of like the electric chollie problem okay great i think that's pretty much all i have for today thank you for your time</sample>
    <sample id="206">hello my name is vasudha and i am a computer science phd candidate at stony brook university i would like to present our work accepted into acl 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge we begin by defining cognitive dissonance and why it is an important problem to study in language simply put cognitive dissonance is two beliefs or actions that are inconsistent such as this example where a person states i know that cigarettes could kill me and then goes on to say i grabbed a couple of smokes after the meeting this belief and action are inconsistent and they are in dissonance further mentioning that i don't think i could keep my job without them justifies the second occurrence and they have a dissonance relationship since the initial model was not able to capture the dissonance class at all we start the cold start the active learning process by transferring weights from closely related tasks topic independent dissonance stance classification a task that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic called debate here and on binary classification of expansion and comparison classes of prt since these two are closely related to the conception of consonance and dissonance and we call them prc here we find that on transferring the zero-shot performance on the annotated dataset is already much better than chance with the best with auc 062 further on iterative fine-tuning on both tasks we find that fine-tuning of prc task followed by further fine-tuning on debate yields a much better zero-shot performance thus this is the model that we use to cold start the active learning process feel free to get in touch with us if you have any questions thank you</sample>
    <sample id="207">最近用于评估 paLM 的测试集包括： 1. 英文到法语的翻译测试集； 2. 英文到德语的翻译测试集； 3. 法语到英语的翻译测试集； 4. 德语到英语的翻译测试集； 5. 英语到法语的翻译测试集； 6. 英语到德语的翻译测试集； 7. 英语到法语的翻译测试集； 8. 英语到德语的翻译测试集； 9. 英语到法语的翻译测试集； 10. 英语到法语的翻译测试集； 11. 英语到法语的翻译测试集； 12. 英语到法语的翻译测试集； 13. 英语到法语的翻译测试集； 14. 英语到法语的翻译测试集； 15. 英语到法语的翻译测试集； 16. 英语到法语的翻译测试集； 17. 英语到法语的翻译测试集； 18. 英语到法语的翻译测试集； 19. 英语到法语的翻译测试集； 20. 英语到法语的翻译测试集； 21. 英语到法语的翻译测试集； 22. 英语到法语的翻译测试集； 23. 英语到法语的翻译测试集； 24. 英语到法语的翻译测试集； 25. 英语到法语的翻译测试集； 26. 英语到法语的翻译测试集； 27. 英语到法语的翻译测试集； 28. 英语到法语的翻译测试集； 29. 英语到法语的翻译测试集； 30. 英语到法语的翻译测试集； 31. 英语到法语的翻译测试集； 32. 英语到法语的翻译测试集； 33. 英语到法语的翻译测试集； 34. 英语到法语的翻译测试集； 35. 英语到法语的翻译测试集； 36. 英语到法语的翻译测试集； 37. 英语到法语的翻译测试集； 38. 英语到法语的翻译测试集； 39. 英语到法语的翻译测试集； 40. 英语到法语的翻译测试集； 41. 英语到法语的翻译测试集； 42. 英语到法语的翻译测试集； 43. 英语到法语的翻译测试集； 44. 英语到法语的翻译测试集； 45. 英语到法语的翻译测试集； 46. 英语到法语的翻译测试集； 47. 英语到法语的翻译测试集； 48. 英语到法语的翻译测试集； 49. 英语到法语的翻译测试集； 50. 英语到法语的翻译测试集； 51. 英语到法语的翻译测试集； 52. 英语到法语的翻译测试集； 53. 英语到法语的翻译测试集； 54. 英语到法语的翻译测试集； 55. 英语到法语的翻译测试集； 56. 英语到法语的翻译测试集； 57. 英语到法语的翻译测试集； 58. 英语到法语的翻译测试集； 59. 英语到法语的翻译测试集； 60. 英语到法语的翻译测试集； 61. 英语到法语的翻译测试集； 62. 英语到法语的翻译测试集； 63. 英语到法语的翻译测试集； 64. 英语到法语的翻译测试集； 65. 英语到法语的翻译测试集； 66. 英语到法语的翻译测试集； 67. 英语到法语的翻译测试集； 68. 英语到法语的翻译测试集； 69. 英语到法语的翻译测试集； 70. 英语到法语的翻译测试集； 71. 英语到法语的翻译测试集； 72. 英语到法语的翻译测试集； 73. 英语到法语的翻译测试集； 74. 英语到法语的翻译测试集； 75. 英语到法语的翻译测试集； 76. 英语到法语的翻译测试集； 77. 英语到法语的翻译测试集； 78. 英语到法语的翻译测试集； 79. 英语到法语的翻译测试集； 80. 英语到法语的翻译测试集； 81. 英语到法语的翻译测试集； 82. 英语到法语的翻译测试集； 83. 英语到法语的翻译测试集； 84. 英语到法语的翻译测试集； 85. 英语到法语的翻译测试集； 86. 英语到法语的翻译测试集； 87. 英语到法语的翻译测试集； 88. 英语到法语的翻译测试集； 89. 英语到法语的翻译测试集； 90. 英语到法语的翻译测试集； 91. 英语到法语的翻译测试集； 92. 英语到法语的翻译测试集； 93. 英语到法语的翻译测试集； 94. 英语到法语的翻译测试集； 95. 英语到法语的翻译测试集； 96. 英语到法语的翻译测试集； 97. 英语到法语的翻译测试集； 98. 英语到法语的翻译测试集； 99. 英语到法语的翻译测试集； 100. 英语到法语的翻译测试集； 101. 英语到法语的翻译测试集； 102. 英语到法语的翻译测试集； 103. 英语到法语的翻译测试集； 104. 英语到法语的翻译测试集； 105. 英语到法语的翻译测试集； 106. 英语到法语的翻译测试集； 107. 英语到法语的翻译测试集； 108. 英语到法语的翻译测试集； 109. 英语到法语的翻译测试集； 110. 英语到法语的翻译测试集； 111. 英语到法语的翻译测试集； 112. 英语到法语的翻译测试集； 113. 英语到法语的翻译测试集； 114. 英语到法语的翻译测试集； 115. 英语到法语的翻译测试集； 116. 英语到法语的翻译测试集； 117. 英语到法语的翻译测试集； 118. 英语到法语的翻译测试集； 119. 英语到法语的翻译测试集； 120. 英语到法语的翻译测试集； 121. 英语到法语的翻译测试集； 122. 英语到法语的翻译测试集； 123. 英语到法语的翻译测试集； 124. 英语到法语的翻译测试集； 125. 英语到法语的翻译测试集； 126. 英语到法语的翻译测试集； 127. 英语到法语的翻译测试集； 128. 英语到法语的翻译测试集； 129. 英语到法语的翻译测试集； 130. 英语到法语的翻译测试集； 131. 英语到法语的翻译测试集； 132. 英语到法语的翻译测试集； 133. 英语到法语的翻译测试集； 134. 英语到法语的翻译测试集； 135. 英语到法语的翻译测试集； 136. 英语到法语的翻译测试集； 137. 英语到法语的翻译测试集； 138. 英语到法语的翻译测试集； 139. 英语到法语的翻译测试集； 140. 英语到法语的翻译测试集； 141. 英语到法语的翻译测试集； 142. 英语到法语的翻译测试集； 143. 英语到法语的翻译测试集； 144. 英语到法语的翻译测试集； 145. 英语到法语的翻译测试集； 146. 英语到法语的翻译测试集； 147. 英语到法语的翻译测试集； 148. 英语到法语的翻译测试集； 149. 英语到法语的翻译测试集； 150. 英语到法语的翻译测试集； 151. 英语到法语的翻译测试集； 152. 英语到法语的翻译测试集； 153. 英语到法语的翻译测试集； 154. 英语到法语的翻译测试集； 155. 英语到法语的翻译测试集； 156. 英语到法语的翻译测试集； 157. 英语到法语的翻译测试集； 158. 英语到法语的翻译测试集； 159. 英语到法语的翻译测试集； 160. 英语到法语的翻译测试集； 161. 英语到法语的翻译测试集； 162. 英语到法语的翻译测试集； 163. 英语到法语的翻译测试集； 164. 英语到法语的翻译测试集； 165. 英语到法语的翻译测试集； 166. 英语到法语的翻译测试集； 167. 英语到法语的翻译测试集； 168. 英语到法语的翻译测试集； 169. 英语到法语的翻译测试集； 170. 英语到法语的翻译测试集； 171. 英语到法语的翻译测试集； 172. 英语到法语的翻译测试集； 173. 英语到法语的翻译测试集； 174. 英语到法语的翻译测试集； 175. 英语到法语的翻译测试集； 176. 英语到法语的翻译测试集； 177. 英语到法语的翻译测试集； 178. 英语到法语的翻译测试集； 179. 英语到法语的翻译测试集； 180. 英语到法语的翻译测试集； 181. 英语到法语的翻译测试集； 182. 英语到法语的翻译测试集； 183. 英语到法语的翻译测试集； 184. 英语到法语的翻译测试集； 185. 英语到法语的翻译测试集； 186. 英语到法语的翻译测试集； 187. 英语到法语的翻译测试集； 188. 英语到法语的翻译测试集； 189. 英语到法语的翻译测试集； 190. 英语到法语的翻译测试集； 191. 英语到法语的翻译测试集； 192. 英语到法语的翻译测试集； 193. 英语到法语的翻译测试集； 194. 英语到法语的翻译测试集； 195. 英语到法语的翻译测试集； 196. 英语到法语的翻译测试集； 197. 英语到法语的翻译测试集； 198. 英语到法语的翻译测试集； 199. 英语到法语的翻译测试集； 200. 英语到法语的翻译测试集； 201. 英语到法语的翻译测试集； 202. 英语到法语的翻译测试集； 203. 英语到法语的翻译测试集； 204. 英语到法语的翻译测试集； 205. 英语到法语的翻译测试集； 206. 英语到法语的翻译测试集； 207. 英语到法语的翻译测试集； 208. 英语到法语的翻译测试集； 209. 英语到法语的翻译测试集； 210. 英语到法语的翻译测试集； 211. 英语到法语的翻译测试集； 212. 英语到法语的翻译测试集； 213. 英语到法语的翻译测试集； 214. 英语到法语的翻译测试集； 215. 英语到法语的翻译测试集； 216. 英语到法语的翻译测试集； 217. 英语到法语的翻译测试集； 218. 英语到法语的翻译测试集； 219. 英语到法语的翻译测试集； 220. 英语到法语的翻译测试集； 221. 英语到法语的翻译测试集； 222. 英语到法语的翻译测试集； 223. 英语到法语的翻译测试集； 224. 英语到法语的翻译测试集； 225. 英语到法语的翻译测试集； 226. 英语到法语的翻译测试集； 227. 英语到法语的翻译测试集； 228. 英语到法语的翻译测试集； 229. 英语到法语的翻译测试集； 230. 英语到法语的翻译测试集； 231. 英语到法语的翻译测试集； 232. 英语到法语的翻译测试集； 233. 英语到法语的翻译测试集； 234. 英语到法语的翻译测试集； 235. 英语到法语的翻译测试集； 236. 英语到法语的翻译测试集； 237. 英语到法语的翻译测试集； 238. 英语到法语的翻译测试集； 239. 英语到法语的翻译测试集； 240. 英语到法语的翻译测试集； 241. 英语到法语的翻译测试集； 242. 英语到法语的翻译测试集； 243. 英语到法语的翻译测试集； 244. 英语到法语的翻译测试集； 245. 英语到法语的翻译测试集； 246. 英语到法语的翻译测试集； 247. 英语到法语</sample>
    <sample id="208">hi i'm myra and today i'll be talking about our paper marked personas using natural language prompts to measure stereotypes in language models this work is done in collaboration with essem dermanush and dan juravsky in recent years many have documented the prevalence of social bias and stereotypes in large language models or lls however these measures have various limitations they usually rely on hand-constructed datasets that are very time-consuming to curate and they also usually only measure very specific stereotypes meaning that they don't generalize well to other demographics or contexts or they simply capture very general broad associations like negative associations with particular groups so in our method we rely on the property that these newer instruction-tuned lls are very good at responding to instructions and prompts so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona</sample>
    <sample id="209">与最强的基线相比，提议的方法获得了多少收益？</sample>
    <sample id="210">hello everyone my name is xuhong today i'm going to present our paper do conno 2003 named entity taggers still work well in 2023 and last but not least we calculated the percentage change in f1 to assess the generalization of each model so what is needed for good generalization through our experiments we found that from the graph on the right the red best fit line has a gradient that is greater than one this means that every unit of improvement that we made on conno 2003 translates to more than one unit improvement on conno which means that there is no diminishing returns and this shows us that adaptive overfitting in this case is not observed so what about temporal drift then for temporal drift we didn't experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift and kind of surprisingly it is not caused by adaptive overfitting even though conno 2003 has been used for over 20 years so going back to the question that we raised in the title of our paper do conno 2003 taggers still work in 2023 and we found that the answer is actually a resounding yes we hope our paper calls for more research on how to improve generalization of the models and lastly please make sure to check out our paper our data set and if you have any questions feel free to contact me thank you so much</sample>
    <sample id="211">hi welcome to our presentation of deplain a new corpus for german text simplification on the document level and on the sentence level my name is regine stodden and i will guide you through the first part of the presentation let's first define text simplification text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group as people with reading problems or non-native speakers to train a text simplification model we require parallel pairs of text for example documents or sentences and the example here you can see a parallel aligned sentence pair of a complex german text and its translation into plain language to simplify the sentence pairs a little bit more so for example on the type of simplification structural simplification or the overall level of simplification and now as we have our dataset deplain which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods and we did some adaptations to the proposed methods and we have published all these adaptations and the codes to run our experiments in the paper uh we concluded that this this basic fine-tuning could produce uh or could get uh scores better than the baseline scores and we propose those results as a base benchmark for the problem of automatic text simplification in the future thank you so much for your attention and we hope to meet all of you uh during the conference thank you</sample>
    <sample id="212">他们在论文中进行了多少个较小模型的实验</sample>
    <sample id="213">hello everyone my name is ying and my colleague yan and i will be presenting our research on multi-instruction improving multi-modal zero-shot learning via instruction tuning so with the advances in large language models many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data efficient way recently many studies have shown that instruction tuning enables large language models to perform on unseen multi-modal tasks in a zero-shot manner by following natural instructions however most previous works on instruction tuning focus on improving the zero-shot performance on language-only tasks while computer vision and multi-modal tasks have been left out therefore in this work we want to investigate whether instruction tuning on multi-modal pre-trained models can actually improve the zero-shot capability of ofa and we explore different transfer learning techniques and show their benefits we design a new metric called sensitivity so one more thing we are collecting a much larger multi-modal instruction tuning dataset with around 150 additional visual language tasks and we will release them so this is a qr code for our data and model thank you</sample>
    <sample id="215">hi my name is adam sperberkovsky and this talk is about the dependency structure of coordination as you may know there are different dependency structures assumed by different theories and corpus approaches so for example in universal dependencies the structure of the coordinate structure is such that the first conjunct is the head of the whole coordinate structure so in this case lisa bart and maggie is such that the first conjunct is the head of the whole coordinate structure so we get dependencies from and to all the conjuncts and finally this effect may be ameliorated when the direct object is very heavy and very long because then it can be moved to the position after the adjunct this is illustrated here so both these sentences are fine right it violates one principle but it satisfies another one okay so what we did we extracted various statistics from about coordination from the enhanced version of the pen tree bank and see the paper why we wouldn't use universal dependencies and these statistics confirm the observation made many times before that left conjuncts tend to be shorter so salt and pepper and not pepper and salt measured in syllables and also the observation that was made in passing that this tendency grows with length the length difference so when the difference between the lengths of the two conjuncts grows the shorter conjunct prefers to be the first one stronger right so the proportion is is bigger of the left uh short conjuncts but what's novel uh in this paper is that we observed that this tendency only occurs when the governor is on the left or absent right so the governor is on the right this tendency disappears so we show that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we show in the paper how this provides an argument against asymmetric structures of coordination as these two and for the symmetric structures as these two so see the paper for the full agreement and arguments sorry and talk to us about the poster session thank you</sample>
    <sample id="217">hello everyone i'm getting to introduce our work here seen to unseen exploring compositional generation for multi-attribute controllable dialogue generation and we're hosting and working with lulu zhao from beijing university of posts and telecommunications now i'll talk about our works in following seven aspects i will introduce our motivations first previous methods for generation generating controllable dialogue focus on single attributes ignoring the practical setting of multiple attribute compositional prompt module to effectively use control signals we have designed two types of prompts that use attribute-related information from the pre-trained language model the first is a trait-oriented prompt we use the combination of controllable attribute values corresponding to each instance as prompts to guide the model to focus on controllable information where task-oriented prompts improve task quality and disentanglement learning improves the ability of compositional generation with the ability to generalize from seen attributes to unseen combinations our proposed method outperforms classic metrics for both coarse-grained discrete attributes and fine-grained continuous attributes we test various of mae and found removing the continuous prompts decrease the correlation score since the task-oriented prompts are the only parameters that can be fine-tuned and are therefore important for mae we also implemented mae on another pim board to show its generalization we demonstrate the impact of prompts on compositional generation with our visualizations of the compositional prompt embedding of three attributes via pca on daily dialogue cg as shown in this figure these results prove that our method can disentangle attribute combinations and learn the relations between different attributes with the ability to generalize from seen attributes to unseen combinations our proposed method outperforms models that learn an independent prompt for each attribute value with a shared embedding mapping helps learn attribute concepts from seen values to unseen combinations this is the conclusion we studied compositional generalization dialogue for multiple attributes and proposed a prompt-based disentangled attribute dialogue model</sample>
    <sample id="218">这篇论文的作者所属机构是什么？ 这篇论文的作者所属机构是什么？</sample>
    <sample id="219">hi everyone i'm jian wei chen a research assistant at academia sinica i will present our work a compare and contrast multi-stage pipeline for uncovering financial signals in financial reports this work is done with yishan wang and our advisors professor chia-liang chen in this work we consider the financial report as our target corpus which is an annual report required by the sec it contains many details of companies regulations revised pairs have similar syntactical patterns but in fact the two segments disclose very different meanings mismatched pairs are more like a debut information or a company's new operations in addition we mix different objectives by mixing cross-entropy loss and kl-divergence there are many other future works we would like to try including improving our effectiveness or adding more features or like many other techniques in information retrieval can enhance the application as well yeah that's it so please refer to our paper and github for more details and feel free to ask us if you have any question thank you</sample>
    <sample id="220">hello my name is vasudha and i am a computer science phd candidate at stony brook university i would like to present our work accepted into acl 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge we begin by defining cognitive dissonance and why it is an important problem to study in language simply put cognitive dissonance is two beliefs or actions that are inconsistent such as this example where a person states i know that cigarettes could kill me and then goes on to say i grabbed a couple of smokes after the meeting this belief and action are inconsistent and they are in dissonance further mentioning that i don't think i could keep my job without them justifies the second occurrence and they have a consonance relationship and we call them ce here we find that on transferring the zero shot performance on the annotated dataset is already much better than chance with the best with auc 075 which is the best performance that we have on the task so far we also check the feasibility of each strategy for annotation quality and costs to annotators we find that prc has the highest percentage of dissonance and works best for rare class acquisition and cold starting ael with appropriately designed transfer learning tasks can help significantly we also find that iterative update is useful for transfer learning from a different domain whereas in-domain active annotations benefit from cumulative update these are the links to our code dataset and our paper feel free to get in touch with us if you have any questions thank you</sample>
    <sample id="221">这篇论文分析了哪些语言模型的翻译质量？</sample>
    <sample id="222">hi everyone the title of this work is to adapt or to annotate challenges and interventions in open domain question answering to motivate this work let's look at this question what is produced in the plants of narora kakrapur tharapur so in open domain qa setting we need to first look up relevant passages from a document corpus in this case which is wikipedia with some retriever model uh then a reader model takes the question and all the relevant passages as input to generate the answer as nuclear power which is the correct prediction please note that both the retriever and reader models are trained on a general purpose domain like wikipedia now let's say we want to answer a biomedical question in that case wikipedia corpus can probably answer a few domain specific questions for further adapting retriever and reader models overall we observe that the retriever performance improves by 8 on average while reader performance improves by 11 on average for the single value we just average the compatibility values over all answer likelihoods to get a single value we just average the compatibility values over all examples in the set now that we have our compatibility measure we can basically map different target data sets onto this 2d grid and essentially estimate the type of data shift which means that data sets with concept and covariate shift respond well to zero shot adaptations as they use a few examples from target domain while data sets with concept and covariate shift respond well to zero shot adaptations as well as in case of no shift we don't observe a lot of changes in performance because source model already understands the target domain to a great extent to conclude we experiment with a variety of data interventions and improve reader performance by up to 24 we also show that only certain data types of data interventions are effective based on the type of shift a target data set exhibits thank you</sample>
    <sample id="223">hi i'm jianbin phd student at university of washington today i'm presenting our work from pre-training data to language models to downstream tasks tracking the trails of political biases leading to unfair nlp models so language models are trained on large-scale web crawl data political news media are well covered in their pre-training data according to a survey of the c4 corpus we can see that new york times los angeles times the guardian huffington post etc are well covered in language model training data this has created a mixed blessing for language model applications so on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas on the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream tasks ultimately creating fairness issues if we do try to sanitize somehow we would also risk censorship or exclusion and it's incredibly hard to determine what is actually neutral and should be retaining language model training data so it's kind of like the electric cholli problem okay great i think that's pretty much all i have for today thank you for your time</sample>
    <sample id="224">在实验过程中研究了哪些模型</sample>
    <sample id="225">在 multi-instruct 中，有 62 个不同任务中，有 53 个任务用于训练和 10 个任务用于测试。</sample>
    <sample id="226">hi welcome to our presentation of deplain a new corpus for german text simplification on the document level and on the sentence level my name is regina stodden and i will guide you through the first part of the presentation let's first define text simplification text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group as people with reading problems or non-native speakers to train a text simplification model we require parallel pairs of text for example documents or sentences in the example here you can see a parallel aligned sentence pair of a complex german text and its translation into plain language to simplify the sentence pairs a little bit more so for example on the type of simplification structural simplification or the overall level of simplification and now as we have our dataset deplain which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods and we did some adaptations to the proposed methods and we have published all these adaptations and the codes to run our experiments in the paper uh we concluded that this this basic fine-tuning could produce uh or could get uh scores better than the baseline scores and we propose those results as a benchmark a baseline benchmark for the problem of automatic text simplification in the future thank you so much for your attention and we hope to meet all of you uh during the conference thank you</sample>
    <sample id="227">uh we all know that language models have achieved great success recently providing a general solution to many different nlp tasks uh here we want to ask what's missing in current language models research uh we think the answer to this question is grounded language understanding which basically means grounding a natural language expression into something that can be executed over a specific target environment which is also referred to as a plan or a program so there are many applications to grounded language understanding such as smart assistant uh like siri and alexa and also semantic search on google and querying a medical database using natural language and also domestic robots that follow natural language instructions so all these applications involve mapping a natural language expression into something that can be executed over a specific target environment for example like when querying a medical database using natural language to discrimination might be a much better strategy of using language models for grounded language understanding uh we are always open to different forms of uh discussions and collaborations and also we are eager to hear uh your thoughts on our work uh we appreciate your time and your attention thanks a lot</sample>
    <sample id="228">作者在实验中使用了哪些数据集？</sample>
    <sample id="229">hello everyone my name is gabrielle skidmore and today i'm going to present our joint work with henning wachsmuth on detecting suboptimal claims for argumentative writing support let's start with a brief introduction into revision of text and why they're important text revision is an essential part of professional writing and is typically a recursive process until some optimal phrasing is achieved from the author's point of view as such we explore how to best model the quality of argumentative claims based on implicit revision patterns found in collaborative online debate platforms such as kialo on the slide below you can see an example of how such information on the quality of claims can be extracted from revision-based data there are two revision histories where the final versions are considered as optimal colored green and all the predecessors colored red are suboptimal and in need of a revision while working with revision-based data can be beneficial for the given tasks based on our experiments we can conclude that a revision-based data can be effectively employed for the given tasks moreover modeling the distance between two claim versions is beneficial for detecting suboptimal claims and finally the impact of contextual information is dependent on both the task and the quality issues a text is suffering from for further details and findings please refer to our paper and thank you for your attention</sample>
    <sample id="231">since its release in 2018, bert has become one of the most effective approaches to solving natural language processing tasks in english. however, french did not have any open-source model for biomedical until now. we introduced three models trained on continuous pre-training to analyze the impact of pre-training strategy: one based on camarber and trained on 4 gigabytes of set of natchez, another also based on camarber but trained on the 4 gigabytes of clinical notes, and finally one based on the english biomedical model bert and trained on 4 gigabytes of natchez. in total, we have seven models to evaluate all seven models are compared to six baseline models which are camarber oscar 188 gigabytes camarber 4 gigabytes camarber 4 gigabytes bert bert clinical bert the evaluation highlights that the model performed best on the task with data of the same nature as those on which the model has been trained however we can observe that data from heterogeneous sources appear to be more versatile we also observe that using more data translates into better performance in overall from scratch pre-training seemed to obtain higher performance on most of the tasks and surpassed globally the result of the generic model here camarber we also observed that specialized data is better more specialized data is better but it doesn't scale well all the pre-trained models obtained from natchez are freely available on yugenface and all the training scripts are on our github repository so thank you for for for this presentation and we are looking forward to exchanging at the poster session in toronto</sample>
    <sample id="232">演讲者的名字是艾丽斯·比拉德。</sample>
    <sample id="233">hi i'm sarah pappi from the university of toronto and vlad seleny bruno kessler and i will briefly introduce the attention as a guide for simultaneous speech translation paper that is a joint work with macdonald negri and marco turci and what is simultaneous speech translation simultaneous speech translation or simulst is the process of translating spoken language into text in another language in real time enabling cross-language communication and what are the problems of the current simulst models specific architectures are usually trained introducing additional modules to be optimized long and complicated training procedures for example training involving different optimization objectives and training and maintaining several models to reach different latency regimes for example training a model with an average of one second latency and another one with two seconds latency and so on so what is our solution first to use already existing offline simulst models without retraining or adopting specific architecture for simulst use only one model for every latency regime and handle latency through specific specific parameters and leverage the knowledge already acquired by the model through the tension mechanism between audio input and textual output that is the cross-attention mechanism and you can see an example on the right our solution is to propose a dot or encoder-decoder attention and it is a strategy for which we decide whether to emit or not a partial translation based on where attention points to and what is emitted if the tension is not concentrated that is if the alpha towards the last lambda speech frames meaning that the received information is unstable for example if we receive a speech chunk containing i'm going to talk about and our model predicts other three words and we will look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go</sample>
    <sample id="234">the prompting has a big influence on the performance of the lms for translation as we can see in a simple experiment where we use one-shot prompting and provided two different prompts for the sentence the majority of sentences 516 out of 1000 the difference observed is of more than one bleu points and this can go in extreme cases up to 40 bleu points so it's important to select the examples from high quality translations in particular we compare the selecting prompts from the training data of the wmt evaluations or the dev data the dev data is much more curated and with higher quality than the training data that is more normalized and the results show a better performance when using the dev data nevertheless specialized state-of-the-art systems have a substantial advantage over the bank translations but palm comes pretty close to a commercial system now in our case we chose to evaluate with google translate the insights that we gain from the human evaluation that we perform using the mqm framework is that the fluency of palm is comparable to state-of-the-art systems but the main difference comes from the accuracy in particular the most common error are omission errors so it seems that palm chooses to produce a better sounding translation sometimes by dropping parts of the source sentence that are irrelevant in the translation however the style award category for palm is lower than for the state-of-the-art systems which is an additional signal that palm provides really fluent output but still with some problems of accuracy and that's it for this really short overview for more details please come back to the full presentation of the paper thank you very much</sample>
    <sample id="235">hello my name is kyo yin and i will be presenting our work titled when does translation require context a data-driven multi-lingual exploration this work was done in collaboration with patrick fernandez and andrew f d martin and graham neubig so a lot of translations depend on context in the sentence well if the previous sentence was things could start to get dangerous if the ministers find out then mole refers to a spy but if the previous sentence was could it be anything serious doctor then mole refers to a birthmark so depending on context the meaning of the word changes and therefore its translation changes as well however evaluating how well models can handle these cases is pretty hard firstly because only a small portion of translations depend on context in the sentence and secondly because these resources only support limited types of context dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation in this work we try to answer these two questions firstly when does translation require context and secondly how well do models handle these cases to answer the first question we started by measuring how much a word depends on context in the sentence and in the previous work we introduced cxmi as a measure for context usage by machine translation models and this is done by measuring how much information the context c provides about the target y given the source x and we call our tagger the multi-lingual discourse aware or muda tagger we can then also note that different languages have different proportions of these discourse phenomena we then use the muda tagger by applying the tagger on the parallel corpus that we want to use for evaluation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle</sample>
    <sample id="236">hello everyone my name is ying and my colleague yan and i will be presenting our research on multi-instruction improving multi-modal zero-shot learning via instruction tuning so with the advances in large language models many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data efficient way recently many studies have shown that instruction tuning enables large language models to perform on unseen multi-modal tasks in a zero-shot manner by following natural instructions however most previous works on instruction tuning focus on improving the zero-shot performance on language-only tasks while computer vision and multi-modal tasks have been left out therefore in this work we want to investigate whether instruction tuning on multi-modal pre-trained models can actually improve the zero-shot capability of ofa and we explore different transfer learning techniques and show their benefits we design a new metric called sensitivity so one more thing we are collecting a much larger multi-modal instruction tuning dataset with around 150 additional visual language tasks and we will release them so this is a qr code for our data and model thank you</sample>
    <sample id="237">作者建议如何使用来自多种来源的信息来测试模型？</sample>
    <sample id="238">hello welcome to this video my name is yubo hu from university of central florida in this video i'm going to present a new benchmark dataset called meetingbank have you ever found yourself in the meeting desperately trying to note down every key point of the meeting in our fast-paced world meetings are happening every day for different purposes which results in an emergent need of various data sets to develop summarization technologies for different meeting domains ultimately we managed to create a repository of city council meetings meetingbank all data we collected will be released in this dataset including meeting transcripts reference summaries and all the urls which contains a variety of useful resources first allow me to guide you through the process of data collection as a start we use the speech-to-text api to convert the audio data to transcripts then open the meeting website this example is from boston city council we can first identify the type and date of the meeting from the information provided below the video this information will be converted into a meeting id that uniquely represents that particular meeting for our task we use the meeting id to locate the corresponding reference summaries from the meetingbank all the extractive summarizers include oracle lexrank and text-rank and we using five best performing abstractive summarizers for our test set by comparing these 10 different systems we have several interesting observations first for extractive summarization system extract oracle yields the highest rouge-2 score which indicating the content of reference summaries mostly comes from the source transcripts this finding suggests that extractive summarization system could be promising unsurprisingly the dialog lm which is designed for long dialogue summaries yields the highest rouge-2 score among all the abstractive models finally gpt-3 does not perform well according to our automatic evaluation metrics but we have interesting findings in the later human assessment to get an overall evaluation we conduct experiments with not only the traditional metrics but also use new metrics like bert score and mover score due to the time constraint i want to present one certainty that gpt-3 achieves the highest overall score it shows exceptional performance in terms of fluency and coherence however its results are less impressive in terms of informativeness and factuality this finding suggests that meeting summarization solution should continue to focus on capturing the main discussion points and a new method of automatic evaluation metrics should be developed to better align with human preference in conclusion our primary contribution is the creation of meetingbank this is a benchmark dataset constructed by segmenting meetings and pairing the segments with expert-written summaries this dataset serves not only as a useful tool for researchers to design advanced meeting summarizers but also an interesting dataset which provides intriguing insights into the decision-making process of city council at the end of this video i encourage all of you to make use of this resource feel free to download it and play with it looking forward to further discussion with you on july thank you</sample>
    <sample id="239">大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好</sample>
    <sample id="240">hello i am dawei a phd student at stanford university in germany in this video i would like to present our recent work weak supervision and weakly supervised learning in weak supervision you do not manually label the data instead we label the data using weak labeling sources such as simple heuristic rules knowledge bases or locality crowdsourcing as shown in the figure on the right when compared to human annotation the weak labeling sources are much cheaper but they are also noisy meaning that a certain amount of the labeling data are incorrect if we directly train neural networks on weakly labeled data the neural networks tend to memorize the labeling noise and do not generalize well this implies that weak supervised learning approaches actually require clean manually annotated samples to work properly and the annotation cost for obtaining clean validation samples should not be overlooked our second finding is that increasing the number of clean validation samples will help weak supervised learning approaches to achieve better performance as shown in the figure on the left typically we only need 20 samples per class to attain high performance but that's not the end of the story because if we allow to continue fine-tuning on the clean validation samples then ftw performs equally well with other methods so in practice there's no reason to choose more complex wsl methods which require more computation time and disk space our concrete recommendations for future work are as follows first report the model selection criteria for example report if the model selection is done via clean validation samples second wsl approaches should be compared with fully supervised learning baselines as both work on clean samples third continuous fine-tuning is a simple yet strong baseline that should be considered in future work in wsl finally we have open sourced our code you can find it via the qr code on this slide please feel free to check it out thank you and enjoy the conference</sample>
    <sample id="241">hi everyone i'm ethan and today i'm going to discuss our paper human-in-the-loop evaluation for early misinformation detection a case study of covid-19 treatments this was a joint work with yang chen wei xu and alan gitter at georgia tech there have been many proposed approaches for automatically detecting misinformation on social media platforms however all of these approaches generally fall short on two key marks firstly these systems are often unrealistically evaluated for example data sets used for the evaluation of these systems are often retrospectively constructed instead of using live data relatedly there is the possibility of leaked counter-evidence which a recent work found was a problem with many of these systems for example for this evidence-based fact-checking approach we see that while in this first case counter-evidence could clearly be found on a well-known data source like wikipedia for the second more realistic case of misinformation counter-evidence could only exist after the claim had been debunked publicly at which case the system is not useful now we move on to the evaluation of our human-in-the-loop workflow from some of the discussion of leaked counter-evidence early detection is an important task right as humans can act more effectively to stem the spread of misinformation the histrogram of scores are shown below where scores of four or five indicate most likely or clearly violating twitter's policies surrounding covid-19 misinformation thank you so much for listening and we hope to answer any questions you may have at the conference</sample>
    <sample id="242">hello i'm james finch and i'm sarah finch and today we'll tell you all about abc eval a new dimensional approach to evaluating conversational ai this work was done by the emory nlp lab led by professor gino choi at emory university and in collaboration with amazon alexa ai so let's say that you just developed a dialogue model and you want to see how well it compares against the current state of the art the common practice is to use human evaluation such as by asking human judges to select which of two conversations is better or to rate conversations given a likert scale method however we believe there is a more precise and reliable strategy for dimensional dialogue evaluation our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself or its partner hallucinating incorrect facts or violating common sense knowledge and when the model succeeds or fails to show empathy to determine what kind of evaluation is most effective we selected four state-of-the-art chat models and evaluated them on 100 human bot conversations per model using abc eval for comparison we also evaluated these conversations using three existing methods likert ratings on the turn level likert ratings on the dialogue level and dialogue level pairwise comparisons for each of the existing methods we collected evaluations on eight of the most commonly measured aspects of dialogue since this is the standard practice for evaluating chat models along multiple dimensions with the rapid pace of improvement in the field many of these error rates could see a decrease in new models released since our evaluation was conducted however this is all the more reason to pursue reliable and precise evaluation metrics for comparing models we hope abc eval can be leveraged by others in the field as a meaningful step in this direction and we look forward to seeing how conversational ai will advance in the coming months and years thank you for watching</sample>
    <sample id="243">hi everyone i'm jenny a first year phd student at carnegie mellon university and today i'll be presenting our work entitled positionality characterizing design bias in ai datasets and models this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work was done in collaboration with some folks at the university of washington and the allen institute for artificial intelligence this work</sample>
    <sample id="244">在 servin 和 kea 的示例中，需要哪些背景知识？ servin 和 kea 的示例中需要哪些背景知识？ servin 和 kea 的示例中需要哪些背景知识？ servin 和 kea 的示例中需要哪些背景知识？ servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中需要哪些背景知识 servin 和 kea 的示例中</sample>
    <sample id="245">hi i'm lin lin jiang today i'm going to present our work a needle in a haystack an analysis of high agreement workers on mturk for summarization below are our co-authors the picture in the middle shows the two-step pipeline for finding high agreement mturk workers as well as the qualification settings the motivation of the pipeline is that automatic metrics are sometimes problematic and the best practice for recruitment on mturk are also poorly understood so we are going to present this work as shown in all here start with the qualification settings the qualification questions include three documents one document and four summaries each to check the information coverage in two directions our pipeline workers' result shows that eight out of twelve mturk workers finish the all hits and the figure on the right shows the khan's kappa between different workers and the krippendorff's alpha is 0534 as for the baseline mturk workers we tried different approaches and the best one is achieved by the statistical filter called mase the threshold of high agreement and correctness and expert judgment in summary for the pipeline result in four gold and eight silver workers which is six percent out of two hundred participants it also serves as the best practice for high agreement and correctness and large scale and lower cost and can avoid resources based on discarded annotations in the future we are going to investigate ways to hire high quality workers both in terms of high agreement and correctness and we are going to try multiple applications for task languages and platforms there are also some limitations for this work first only english summarization and mturk platform is tested second the design type questions are not perfect solutions third there is no guarantee for the training of correctness finally we want to thank google for the experiment fundings and thanks for listening</sample>
    <sample id="246">hello everyone i'm makshatta and today my co-author martin and i are presenting our work the kitmos task evaluating knowledge integration from multiple sources this work is a collaboration between mcgill university nila and microsoft research natural language understanding models draw on a variety of knowledge sources such as knowledge contained in their parameters usually acquired via pre-training and knowledge given in inputs at inference time successful models for knowledge-intensive nlu tasks require the ability to integrate and use both pre-trained time and inference time knowledge in this work we propose a diagnostic test suite for knowledge integration we introduce a co-reference resolution task designed to probe for the ability to draw on knowledge available in different sources we evaluate the dataset with human study participants and established co-reference resolution models to summarize the main takeaways of our paper many co-reference resolution models appear unable to reason over knowledge from different sources without task-specific training however with task-specific training some models successfully integrate knowledge from multiple sources still even the best-performing models seem to have difficulties with reliably integrating background knowledge presented only at inference time if you're interested in more details please see our paper and check out the dataset and code on github thanks for listening</sample>
    <sample id="247">hello this is jill king from kaitai ai and i will present our paper titled factg fact verification via reasoning on knowledge graphs do you know of existing fact verification data sets there are data sets such as fever and vitamin c which use wikipedia text however there was no data set that utilized knowledge graphs as evidence with natural language claims to this end we propose a new data set factg fact verification via reasoning on knowledge graphs the knowledge graph used is dbpedia and claims exist in two styles written and colloquial for more practical use two methods were used for this first the colloquial style transfer model proposed in chemito is used this is an example second presupposition templates were created and used presupposition is what we usually say assuming that it is true or false when we speak we use this concept and these are the examples this is the statistics of our data set finally we constructed some baselines in two ways claim-only baselines use only the claims to verify without graph evidence and we utilize the gearing model to verify the claim using graph evidence and as a result all of the baselines outperform the majority class baseline which is 51 and the gearing model that uses graph evidence outperforms all other baselines this is the summary of our paper thank you for listening you can download our data set and feel free to contact me</sample>
    <sample id="248">hi everyone i'm jenny a first year phd student at carnegie mellon university and today i'll be presenting our work entitled positionality characterizing design bias in ai datasets and models this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen</sample>
    <sample id="249">hi everyone i'm kostas michrakkalis and i'm pleased to welcome you to our talk of our acl 2023 papers language model acceptability judgments are not always robust to context so this is a joint work with john gauthier aaron muller kanishka misra gareth flinders roger levy and edina williams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like blimp syntax gem or acceptability in terms of stereotypes such as cross pairs and in this a minimal pair paradigm the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an unacceptable sentence or an ungrammatical sentence and then the hope is that the model basically puts more probability to the acceptable sentence or the unacceptable sentence depending on whether the chosen prefix is acceptable or unacceptable now this and this is very large like this effect increases throughout the context length and this would probably affect like newer language models which has large context window so why does the match prefix affect the language model judgment so much so we did a series of analysis where we tried to like perturb the input sentence by trying to preserve the relevant structure but adding like noise to the input and after doing like several of these perturbations we find that none of these noises are actually making the model like change its course in terms of how it shows us the mpp judgment trend basically we find that the models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences and the mpp evaluation the way that we do it currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context window please read our paper for more details of our experiments thank you for listening</sample>
    <sample id="250">hello i'm james finch and i'm sarah finch and today we'll tell you all about abc eval a new dimensional approach to evaluating conversational ai this work was done by the emory nlp lab led by professor gino choi at emory university and in collaboration with amazon alexa ai so let's say that you just developed a dialogue model and you want to see how well it compares against the current state of the art the common practice is to use human evaluation such as by asking human judges to select which of two conversations is better or to rate conversations given a likert scale method however we believe there is a more precise and reliable strategy for dimensional dialogue evaluation our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself or its partner hallucinating incorrect facts or violating common sense knowledge and when the model succeeds or fails to show empathy to determine what kind of evaluation is most effective we selected four state-of-the-art chat models and evaluated them on 100 human bot conversations per model using abc eval for comparison we also evaluated these conversations using three existing methods likert ratings on the turn level likert ratings on the dialogue level and dialogue level pairwise comparisons for each of the existing methods we collected evaluations on eight of the most commonly measured aspects of dialogue since this is the standard practice for evaluating chat models along multiple dimensions with the rapid pace of improvement in the field many of these error rates could see a decrease in new models released since our evaluation was conducted however this is all the more reason to pursue reliable and precise evaluation metrics for comparing models we hope abc eval can be leveraged by others in the field as a meaningful step in this direction and we look forward to seeing how conversational ai will advance in the coming months and years thank you for watching</sample>
    <sample id="251">hello everyone my name is jingwei yi from the university of science and technology of china it's my pleasure to give a short advertisement video of our paper i will copy my model protecting the copyright of embedding as services via backdoor watermark let's first introduce the background about embedding as services currently large language models such as t5 lama palm are exceptional in natural language understanding and generation embedding as services is one of the services built upon large language models to assist various nlp tasks for example openai offers a t5 based embedding as service however recent works have shown that the attacker may steal the model through learning from the embedding as services therefore it's necessary to protect the copyright of embedding as services to protect the copyright of embedding as services one of the solutions is to embed a watermark in the provider's service and detect whether another service contains the watermark the watermark method needs to meet the following properties first the method should be applicable to embedding as services second the watermark should not degrade the utility of the provided embedding as services third the watermark should be covert enough to the attacker or the attacker can remove the watermark easily finally the watermark needs to be transferable to the attacker's services during the model extraction process existing works can be broadly classified into four categories however these methods either are not applicable to embedding as services or lack transferability therefore in this paper we propose a backdoor-based watermark method applicable to embedding as services now let me introduce the details of our embedding as service watermark the watermark method contains two main steps watermark injection and copyright verification before these main steps we first select a trigger set the trigger set is a group of words in a moderate frequency interval we assume the provider can collect a general text corpus and count the word frequency with it the legends of the figures means the number of triggers in each sentence as shown in the figures it's hard to distinguish between the backdoored embeddings and normal embeddings that's all thank you welcome to discuss with us</sample>
    <sample id="252">welcome to our presentation my name is saikiran thanikella i am a master's student at iit kanpur i am excited to present our work you create unsupervised case retrieval using event extraction this is a joint work along with abhinav joshi akshay sharma and ashutosh modi legal professionals such as lawyers and judges have traditionally relied on their experience to cite relevant past precedents known as cited documents however with the increasing volume of cases it becomes very challenging this is where the task of prior case retrieval comes to play given a legal query document and candid documents are sequentially given as input into the event extraction block for further processing the event extraction block consists of three steps pre-processing dependency parsing and post-processing for each document we obtain the extracted events then we compute an interaction matrix between the query and candid events the green blocks represent the common events further the interaction matrix is used in different retrieval models to obtain a ranking order of the candid documents in the event filtered documents model we filter the original corpus so that it contains only the sentences which produce matching events with other documents note that all event-based models perform significantly above the baseline the event filtered docs is the best performing model for more experiments and results please check out our paper in terms of performance the method using event filtered documents outperforms all other methods with a significant boost this graph compares the performance of various event-based methods with the baseline from this plot we can observe that event-based models have lower inference time and higher f1 score compared to the other techniques in conclusion with these significant contributions you create opens up avenues for further exploration and development in the field of prior case retrieval please check out our paper for more details that's all from our side thanks for watching</sample>
    <sample id="253">hello everyone my name is mario eder dragon and i'm going to present our work named disorder a double domain adaptation model for detecting signs of mental disorders in social media this is a group effort of researchers from mexico and spain first i want to start with the definition of a mental disorder that is a psychological syndrome which is associated with the distress and disability that affect your thinking feeling mood and behavior there are different types of mental disorders for example major depression ptsd bulimia and anorexia and among others nowadays social media content is massive and provides an opportunity to do research on how people undergo difficulties many people use online platforms to publicly share their daily routines and important events while others take advantage of the anonymity of these spaces to explicitly discuss mental health issues and seek help in this work we aim to contribute to the detection of mental disorders by automatically analyzing their social media posts this type of analysis is expected to support a new technology able to warn about the onset of mental disorders and provide supporting evidence let us illustrate the behavior of the learned model and the kind of textual segments it tends to pay more attention to first we use a lexicon to guide the masking process the model tends to be biased toward words related to anxiety and medication topics that are highly relevant to depression as conclusion and future work the combined effect of double domain adaptation and guided masking is effective at capturing signs of mental disorders in social media interactions our approach also obtains better results than those achieved by mental work a model trained with a large amount of data the evaluation shows a solid balance between finding users and labeling them correctly in future work we want to explore the application of different lexical resources as well as using clinical data thank you for your attention if you have any questions please feel free to ask me</sample>
    <sample id="254">hello everyone today i'm going to present our research work uncertainty guided level denoising for document level relation extraction this is the overview of our framework we propose a document level relation extraction framework with uncertainty guided level denoising to improve the label quality of this data we observe that the distribution of uncertainty score for each relation class is different moreover it can be observed that frequent classes usually contains lower uncertainty score than the long tail class in order to take full advantage of the ds data for boosting the performance of document model we design the multi-phase training strategy to iteratively relabel the ds data which is shown in this algorithm we compare our framework with several state-of-the-art baselines on two public datasets as shown in this table our framework outperforms the previous baselines on both two datasets in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized as those four points the first one is our framework with uncertainty guided level denoising which is greatly improved the label quality of this data the second one is the instance level uncertainty estimation method for overlapping relations the third one is the iterative reliable strategy with dynamic class uncertainty threshold for the long tail problem the last one is the great performance improvement in conclusion the main contribution of our work are summarized</sample>
    <sample id="255">the prompting has a big influence on the performance of the lms for translation as we can see in a simple experiment where we use one-shot prompting and provided two different prompts for the sentence the majority of sentences 516 out of 1000 the difference observed is of more than one bleu points and this can go in extreme cases up to 40 bleu points so it's important to select the examples from high quality translations in particular we compare the selecting prompts from the training data of the wmt evaluations or the dev data the dev data is much more curated and with higher quality than the training data that is more normalized and the results show a better performance when using the dev data nevertheless specialized state-of-the-art systems have a substantial advantage over the bank translations but palm comes pretty close to a commercial system in our case we chose to evaluate with google translate the insights that we gain from the human evaluation that we perform using the mqm framework is that the fluency of palm is comparable to state-of-the-art systems but the main difference comes from the accuracy in particular the most common error are omission errors so it seems that palm chooses to produce a better sounding translation sometimes by dropping parts of the source sentence that are irrelevant in the translation however the style award category for palm is lower than for the state-of-the-art systems which is an additional signal that palm provides really fluent output but still with some problems of accuracy and that's it for this really short overview for more details please come back to the full presentation of the paper thank you very much</sample>
    <sample id="257">hello i'm james finch and i'm sarah finch and today we'll tell you all about abc eval a new dimensional approach to evaluating conversational ai this work was done by the emory nlp lab led by professor gino choi at emory university and in collaboration with amazon alexa ai so let's say that you just developed a dialogue model and you want to see how well it compares against the current state of the art the common practice is to use human evaluation such as by asking human judges to select which of two conversations is better or to rate conversations given a likert scale method however we believe there is a more precise and reliable strategy for dimensional dialogue evaluation our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself or its partner hallucinating incorrect facts or violating common sense knowledge and when the model succeeds or fails to show empathy to determine what kind of evaluation is most effective we selected four state-of-the-art chat models and evaluated them on 100 human bot conversations per model using abc eval for comparison we also evaluated these conversations using three existing methods likert ratings on the turn level likert ratings on the dialogue level and dialogue level pairwise comparisons for each of the existing methods we collected evaluations on eight of the most commonly measured aspects of dialogue since this is the standard practice for evaluating chat models along multiple dimensions with the rapid pace of improvement in the field many of these error rates could see a decrease in new models released since our evaluation was conducted however this is all the more reason to pursue reliable and precise evaluation metrics for comparing models we hope abc eval can be leveraged by others in the field as a meaningful step in this direction and we look forward to seeing how conversational ai will advance in the coming months and years thank you for watching</sample>
    <sample id="258">hi everyone i'm zhangsong han thank you for watching this video in this video i'm going to talk about our new work can large language models be an alternative to human evaluations in this work we propose to use large language models to evaluate the quality of samples in other natural language processing tasks so we just give the large language models the instructions and we give them the samples that need to be rated and the humans can rate the samples based on the instructions but in fact human evaluation is very unstable and the process of human evaluation is very hard to reproduce so we ask ourselves a question is there any alternative to human evaluation that can achieve the same goal as in human evaluation but do not have the drawbacks of human evaluation so we turn to large language models large language models as their name indicates are the models that have a lot of number of parameters and there are some large language models that have been shown to be able to follow natural language task instructions so we think why don't we just use natural language instructions to instruct the large language models to evaluate the samples in other natural language processing tasks and all these questions are answered in our paper so if you are interested in this topic please read our paper or you can come to our posters then at a0 and last you may also want to know the results of large language model evaluation on other tasks and all these questions are answered in our paper so if you are interested in this topic please read our paper or you can come to our posters then at a0 and that's all for this video and thank you for listening</sample>
    <sample id="259">hello everyone my name is yusen zhang from the penn state university today i'm going to present our work exemplar cross-lingual semantic parsing in multiple natural languages and many representations so semantic parsing is the task to build semantic representations of user queries such as sql and lambda calculus and cross-lingual semantic parsing is the task to translate queries in multiple natural languages into multiple meaning representations as shown in this figure we need to translate the query in multiple natural languages using neural models to sql lambda or funql and etc and we also find many interesting results so regarding analyses of monolingual models we evaluate on two groups of models including encoder pdr which stands for multi-lingual pre-trained encoders with pointer-based decoders such as xlmr plus pdr and mbert plus pdr and we also evaluate encoder decoder models such as coder decoder outperforms previous work or achieved comparable results for training on english natural language and significantly boost the performance of fueled on target natural languages and we found monolingual language models such as coder and blue are still inadequate for cross-lingual semantic parsing tasks to sum up we build exemplar a unified benchmark for cross-lingual semantic parsing with multiple natural languages and many representations we conduct a comprehensive benchmark study on three representative of types of multi-lingual language models and our result shows many interesting findings and etc and welcome to visit our paper and code thanks for listening</sample>
    <sample id="260">the number of authors in this paper is four.</sample>
    <sample id="261">优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优秀的规划器的理想品质是什么 优</sample>
    <sample id="262">这篇论文有多少位作者？</sample>
    <sample id="263">hi today i'm going to present our work mitigating label biases for in-context learning in-context learning is a popular paradigm for utilizing large language models however such in-context learning ability is known to be unstable due to various design choices such as the choice and order of in-context examples prior work shows that in-context learning ability is unstable due to various design choices such as the choice and order of in-context examples and finally we propose the novel calibration method to handle all type of biases so in this work we aim to address these problems within the context of text classification we start with you know a typology of label biases and based on which we are able to identify a new important source of biases in in-context learning and finally we propose the calibration method that is able to significantly improve the performance of in-context learning of those large language models check our paper for more details and thank you</sample>
    <sample id="264">hi everyone my name is yunlong and i'm a graduate student at georgia tech university in china today i will give a presentation for my paper titled transferable audio-visual test generation task generation let's get started currently unimodal test generation tasks like machine translation and image captioning have already flourished as a result of the large-scale pre-training and huge model capacity however for multimodal test generation tasks like audio-visual text generation the main challenge is the multimodal to monomodal shift like visual style audio energy and so on also there's a lack of labeled data for training so we propose to construct a unified audio semantic space and as well as adjusting shift in the semantic distribution we propose to directly optimize the visual textual element with relying on the without relying on the quality of the randomly selected negative samples and here is the metatest stage we can start a new domain target which also has the support set for fast adaptation by functioning the model and as the query set and the remain domain as the query set in the experimental section to fully evaluate our proposed approach we built two benchmarks based on msvdt and msvdd including cross dataset and cross domain setting for some low-low result domains with only a few labeled data such as cats and beauty other met other methods suffer from several performance degradation while dvb still performs well additionally we conducted ablation experiments to analyze the impact of audio features and span comments okay that's all thank you</sample>
    <sample id="265">hello my name is vasudha and i'm a computer science phd candidate at stony brook university i would like to present our work accepted into acl 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge we begin by defining cognitive dissonance and why it is an important problem to study in language simply put cognitive dissonance is two beliefs or actions that are inconsistent such as this example where a person states i know that cigarettes could kill me and then goes on to say i grabbed a couple of smokes after the meeting this belief and action are inconsistent and they are in dissonance further mentioning that i don't think i could keep my job without them justifies the second occurrence and they have a dissonance relationship since the initial model was not able to capture the dissonance class at all we start the cold start the active learning process by transferring weights from closely related tasks topic independent dissonance stance classification a task that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic called debate here and on binary classification of expansion and comparison classes of prt since these two are closely related to the conception of consonance and dissonance and we call them prc here we find that on transferring the zero shot performance on the annotated dataset is already much better than chance with the best with auc 075 which is the best performance that we have on the task so far we also check the feasibility of each strategy for annotation quality and costs to annotators we find that prc has the highest percentage of dissonance and works best for rare class however the annotators also find the examples difficult in summary we find that prc is a simple cold start active learning strategy for rare class acquisition and cold starting active learning with appropriately designed transfer learning tasks can help significantly we also find that iterative update is useful for transfer learning from a different domain whereas in-domain active annotations benefit from cumulative update these are the links to our code dataset and our paper feel free to get in touch with us if you have any questions thank you</sample>
    <sample id="266">this talk is about the dependency structure of coordination as you may know there are different dependency structures assumed by different theories and corpus approaches so for example in universal dependencies the structure of the coordination lisa bart and maggie is such that the first conjunct is the head of the whole coordination structure so in this case lisa bart and maggie however this effect may be ameliorated when the direct object is very heavy and very long because then it can be moved to the position after the adjunct this is illustrated here so both these sentences are fine march read this absolutely fascinating book about bees so the reasoning here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb this satisfies the principle of dependency length minimization which says that shorter um uh shorter dependencies are preferred so um these two um uh trees uh only show uh the length of the crucial dependencies so the ones that are not constant among these two structures is the governor is on the left uh it's absent in the second example homer came and sneezed here we have coordination of two verbs and there's no outside uh external governor right so in such cases uh the left conjunct prefers to be shorter the more so the uh the bigger the difference uh between the two conjuncts uh grows uh the shorter conjunct prefers to be shorter the more so the governor is on the left this tendency grows steadily uh with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and uh we show that um uh by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that uh when the governor is on the left the tendency for the left conjunct to be shorter grows steadily uh with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and uh we show in the paper how this uh provides an argument against uh asymmetric structures of coordination as these two and for the symmetric structures as these two so see the paper for the full agreement and uh arguments sorry and talk to us about the poster session thank you</sample>
    <sample id="268">the most common errors are omission errors so it seems that palm chooses to produce a better sounding translation sometimes by dropping parts of the source sentence that are irrelevant in translation however the style award category for palm is lower than for the state-of-the-art systems which is an additional signal that palm provides really fluent output but still with some problems of accuracy and that's it for this really short overview for more details please come back to the full presentation of the paper thank you very much</sample>
    <sample id="269">hello i'm james finch and i'm sarah finch and today we'll tell you all about abc eval a new dimensional approach to evaluating conversational ai this work was done by the emory nlp lab led by professor gino choi at emory university and in collaboration with amazon alexa ai so let's say that you just developed a dialogue model and you want to see how well it compares against the current state of the art the common practice is to use human judges to evaluate several dimensions of dialogue quality to understand the strengths and weaknesses of the model on a finer grained level one approach is to simply ask human judges to evaluate several dimensions of dialogue quality such as the relevance of model responses using existing comparative or liker scale methods however we believe there is a more precise and reliable strategy for dimensional dialogue evaluation our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself or its partner hallucinating incorrect facts or violating common sense knowledge and when the model succeeds or fails to show empathy to determine what kind of evaluation is most effective we selected four state-of-the-art chat models and evaluated them on 100 human bot conversations per model using abc eval for comparison we also evaluated these conversations using three existing methods liker ratings on the turn level liker ratings on the dialogue level and dialogue level pairwise comparisons for each of the existing methods we collected evaluations on eight of the most commonly measured aspects of dialogue since this is the standard practice for evaluating chat models along multiple dimensions with the rapid pace of improvement in the field many of these error rates could see a decrease in new models released since our evaluation was conducted however this is all the more reason to pursue reliable and precise evaluation metrics for comparing models we hope abc eval can be leveraged by others in the field as a meaningful step in this direction and we look forward to seeing how conversational ai will advance in the coming months and years thank you for watching</sample>
    <sample id="270">hello i'm james finch and i'm sarah finch and today we'll tell you all about abc eval a new dimensional approach to evaluating conversational ai this work was done by the emory nlp lab led by professor gino choi at emory university and in collaboration with amazon alexa ai so let's say that you just developed a dialogue model and you want to see how well it compares against the current state of the art the common practice is to use human evaluation such as by asking human judges to select which of two conversations is better or to rate conversations given a likert scale method however we believe there is a more precise and reliable strategy for dimensional dialogue evaluation our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself or its partner hallucinating incorrect facts or violating common sense knowledge and when the model succeeds or fails to show empathy to determine what kind of evaluation is most effective we selected four state-of-the-art chat models and evaluated them on 100 human bot conversations per model using abc eval for comparison we also evaluated these conversations using three existing methods likert ratings on the turn level likert ratings on the dialogue level and dialogue level pairwise comparisons for each of the existing methods we collected evaluations on eight of the most commonly measured aspects of dialogue since this is the standard practice for evaluating chat models along multiple dimensions with the rapid pace of improvement in the field many of these error rates could see a decrease in new models released since our evaluation was conducted however this is all the more reason to pursue reliable and precise evaluation metrics for comparing models we hope abc eval can be leveraged by others in the field as a meaningful step in this direction and we look forward to seeing how conversational ai will advance in the coming months and years thank you for watching</sample>
    <sample id="271">在本文中，cft代表什么？</sample>
    <sample id="272">hi everyone i'm kostas michrakkalis and i'm pleased to welcome you to our talk of our acl 2023 paper language model acceptability judgments are not always robust to context this is a joint work with john gauthier aaron muller kanishka misra gareth flinders roger levy and etienne williams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like blimp syntax gem and acceptability in terms of stereotypes such as cross pairs and in this a minimal pair paradigm the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an unacceptable sentence or an ungrammatical sentence and then the hope is that the model basically puts more probability to the acceptable sentence or the unacceptable sentence depending on whether the chosen prefix is acceptable or unacceptable now this and this is very large like this effect increases throughout the context length and this would probably affect like newer language models which has large context window so why does the match prefix affect the language model judgment so much so we did a series of analysis where we tried to like perturb the input sentence by trying to preserve the relevant structure but adding like noise to the input and after doing like several of these perturbations we find that none of these noises are actually making the model like change its course in terms of how it shows us the mpp judgment trend basically we find that the models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences and the mpp evaluation the way that we do it currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context window please read our paper for more details of our experiments thank you for listening</sample>
    <sample id="273">hello my name is kyo yin and i will be presenting our work titled when does translation require context a data-driven multi-lingual exploration this work was done in collaboration with patrick fernandez and andrew f d martin and graham neubig so a lot of translations depend on context for example how would we translate mole in this sentence well if the previous sentence was things could start to get dangerous if the ministers find out then mole refers to a spy but if the previous sentence was couldn't it be anything serious doctor then mole refers to a birthmark so depending on context the meaning of the word changes and therefore its translation changes as well however evaluating how well models can handle these cases is pretty hard firstly because only a small portion of translations depend on context and secondly because data-driven multi-lingual translation systems are not much better than models that do not use context on other phenomena like ellipsis resolution so this again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone now we use the muda benchmark to evaluate models and we find that context-aware models are significantly more accurate than models that do not use context on certain discourse phenomena such as formality and lexical cohesion but these models are not much better than models that do not use context on other phenomena like ellipsis pronouns and verb form so this sort of suggests where we would need to see more progress for document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation thank you so much for your attention see you in toronto</sample>
    <sample id="274">hello everyone my name is yusen zhang from the penn state university today i'm going to present our work exemplar cross-lingual semantic parsing in multiple natural languages and many representations so semantic parsing is the task to build semantic representations of user queries such as sql and lambda calculus and cross-lingual semantic parsing is the task to translate queries in multiple natural languages into multiple meaning representations as shown in this figure we need to translate the query in multiple natural languages using neural models to sql lambda or funql and etc and we also find many interesting results so regarding analyses of monolingual models we evaluate on two groups of models including encoder pdr which stands for multi-lingual pre-trained encoders with pointer-based decoders such as xlmr plus pdr and mbert plus pdr and we also evaluate encoder-decoder models which is multi-lingual pre-trained encoder-decoder models such as mbert and mt5 we found that encoder-decoder obtains the best performance on all nine data sets and only gains in three data sets i think this is known as the curse of multi-linguality we also compare the cross-lingual performance gap in this figure the blue line is cross-lingual fused transfer the orange line is cross-lingual zero-shot transfer while the green line is the monolingual setting we found that by comparing the green and orange line we found that for zero-shot setting the cross-lingual transfer performance gap is significant and by comparing blue and orange line we found that for few-shot setting the transfer gap is shortened rapidly we also find some other interesting findings for example um encoder-decoder outperforms previous work or achieved comparable results for training on english natural language and significantly boost the performance of few-shot on target natural languages and we found monolingual language models such as coders and blue are still inferior for cross-lingual semantic parsing tasks um to sum up we build exemplar a unified benchmark for cross-lingual semantic parsing with multiple natural languages and many representations we conduct a comprehensive benchmark study on three representative of types of multi-lingual language models and our result shows many interesting findings and etc and welcome to visit our paper and code thanks for listening</sample>
    <sample id="276">hi everyone this is ananya and vignesh presenting our work on indicmt val a dataset to meta-evaluate machine translation metrics for indian languages for the translation task there are several evaluation metrics proposed for evaluating two english translations also there are many studies that perform meta-evaluation of these metrics by analyzing their correlation with human scores or discussing the advantages and shortcomings of each of them but what about translations in the other direction evaluation of translations in the other direction is understudied but it is important to study evaluation metrics for other languages instead of naively adopting the metrics proposed for english other languages have their own grammar rules some novel aspects like shared and borrowed vocab dialectic differences and certain settings different sentence structures and varying amounts of resources among other differences our aim is to fill this gap so in this work we focus on indian languages more specifically we study five languages belonging to two different language families we have tamil and malayalam which are dravidian languages and hindi marathi and gujarati which are indo-aryan languages so from the indicmt val a dataset we select 200 sentences randomly to generate multiple candidate translations for each of these source sentences in english we feed it to seven different translation models or apis so using seven such translation systems we get a total of 1400 candidate translations for all languages so almost all metrics show a higher correlation with human scores when only accuracy errors are annotated so having analyzed various metrics we fine-tune the best performing metric commet using our indicmt dataset so the table here compares the correlation values of our fine-tuned indict commet variants with the commet baselines so we observe that indict commet mtm outperforms the commet baselines on the majority of languages we then evaluate robustness scores on the asis translation accuracy challenge sets and we see that indict commet mtm has a correlation score of 0306 and is more robust than the commet counterpart which has a score of 0272 so thank you and please feel free to use our publicly available dataset and have a good day</sample>
    <sample id="277">hi my name is mathias linderman and today i'm going to give you a brief introduction to our paper on compositional generalization without trees using multiset tagging and latent permutations this is joint work with my advisors alexander kola and ivan titov compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training in the context of semantic parsing testing for compositional generalization might look like this as usual we have a training set of utterances in this case the girl slept and mary knew that the girl slept this works well but trees are usually not given and need to be obtained somehow this can be complicated and sometimes a computationally expensive process typically this involves considerable formalism-specific pre-processing of the logical forms for example to handle variable symbols obtaining trees may also involve specialized grammar induction procedures in this paper we solve a couple of interesting technical challenges first of all the alignment between input and output is not given in the training data as a consequence for a given token we don't know which multiset it came from which poses a challenge for training in addition sometimes there are multiple permutations that are consistent with the data but the linguistically correct one is latent we address this by inducing the alignment as part of the training our permutation method is very flexible but it brings the challenge that finding the highest scoring permutation is np-hard that's because this is related to the traveling salesman problem we approximate this with a gpu-friendly continuous relaxation that also allows us to back-propagate through the solution and learn the linguistically more plausible permutations if you want to learn more about our experiments and how we address these challenges please have a look at our paper or come to our poster</sample>
    <sample id="278">hi i'm myra and today i'll be talking about our paper marked words using natural language prompts to measure stereotypes in language models this work is done in collaboration with essem dermanush and dan jurafsky in recent years many have documented the prevalence of social bias and stereotypes in large language models however these measures have various limitations they usually rely on hand-constructed datasets that are very time-consuming to curate and they also usually only measure very specific stereotypes meaning that they don't generalize well to other demographics or contexts or they simply capture very general broad associations like negative associations with particular groups so for instance for the personas of black women we usually find that the generated personas contain a lot more stereotypes than the human written ones however when we actually look at the distribution of the words in the lexicon we find very different things so while the generated personas have much higher rates of the lexicon words the human written ones have a much wider distribution of words while the stereotype words that are in the generated personas are really just the words tall and athletic so really just only the positive or at least non-negative ones and in fact the lexicon doesn't really capture many of the harmful patterns that we saw in the earlier slides well at all so instead to do that we'll turn to the results from our marked words method to show how these seemingly positive portrayals facilitate stereotypes and essentializing narratives so based on these patterns we conclude with three recommendations for model owners first we should as researchers be addressing positive stereotypes and essentializing narratives we should also be using an intersectional lens to study biases and harms because there's a lot of things that might be overlooked if we don't do that and finally there should really be increased transparency about bias mitigation methods because for instance like these positive stereotypes we don't know if it's because there's some sort of like weird overly excessive value alignment going on or maybe some other like anti-stereotyping methods that are resulting in these pernicious patterns we just really can't make any assumptions or really study that further without more transparency thank you so much for listening have a good time at acl</sample>
    <sample id="279">hi i'm jianbin phd student at university of washington today i'm presenting our work from pre-training data to language models to downstream tasks tracking the trails of political biases leading to unfair nlp models so language models are trained on large-scale web crawl data political news media are well covered in their pre-training data according to a survey of the c4 corpus we can see that new york times los angeles times the guardian huffington post etc are well covered in language model training data this has created a mixed blessing for language model applications so on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas on the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream tasks so to this end we propose to investigate the political bias propagation pipeline from pre-training data to language models to downstream tasks specifically by asking the following questions first how do we evaluate the political leaning of language models and what role does pre-training data might have on such political biases secondly how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in nlp applications so we see that if we investigate the per category performance that is to say if we separate the performance into different demographics or political leanings of news media we can see a pattern that for example for hate speech detection left-leaning language models are better at detecting hate speech targeting socially minority groups however are worse at detecting hate speech targeting more powerful groups in our society and vice versa right-leaning language models are better at detecting hate speech targeting white and men however worse at detecting hate speech targeting black lgbtq plus and other minority communities similar trends also happen for fake news detection where we see that left-leaning language models are better at detecting misinformation from their opposite political opinions and vice versa okay great i think that's pretty much all i have for today thank you for your time</sample>
    <sample id="280">hi everyone i'm shu tao today is my great honor to share my work multimodal fusion framework for emotion regulation in conversations the goal of emotion regulation in conversations is to predict the emotion label of each utterance in a dialogue each utterance has its corresponding textual audio and visual modalities to address this problem we propose a novel multimodal fusion framework named multitenet which is made up of three components multitext multiaudio and multivideo each of which aims to integrate one modality with complementary information from the other two modalities through stacked bidirectional multimodal cross-attention layers next i'll illustrate how multimodal fusion works through multitext multitext first learns cross-modal correlations between textual and audio modalities by treating the textual modality as query and audio modality as k and value for a bidirectional multimodal cross-attention layer to fuse the textual modality with visual cues finally a feedforward network is adopted which operates as a key value memory in addition we employ residual connection and layer normalization over the output of each stage multimodal also have some limitations first multitextnet does not distinguish between speaker and irrelevant people in the same second the swfclaws requires a large batch size on meld and finally the performance of multimodal in minority emotions are still worse than majority classes that's all thank you so much for listening</sample>
    <sample id="281">hello my name is kyo yin and i will be presenting our work titled when does translation require context a data-driven multi-lingual exploration this work was done in collaboration with patrick fernandez and andrew f d martin and graham neubig so a lot of translations depend on context for example how would we translate mole in this sentence well if the previous sentence was things could start to get dangerous if the ministers find out then mole refers to a spy but if the previous sentence was could it be anything serious doctor then mole refers to a birthmark so depending on context the meaning of the word changes and therefore its translation changes as well however evaluating how well models can handle these cases is pretty hard firstly because only a small portion of translations depend on context and secondly because a data-driven multi-lingual translation system which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation thank you so much for your attention see you in toronto</sample>
    <sample id="282">hello everyone i'm xiaochao huang jiu and today i'm excited to present our new work in stylo 2023 story trends non-parallel story style transfer with discourse representations from the source text and combine these with the normal style embeddings to generate text in the target styles we have also designed a new training objective to reduce the stylistic features from the discourse orientations putting the nutation derived from the different texts closer in the latent space and moreover to enhance the content preservation we separate the generation into two stages first we transfer the source text with the style specific content keywords masked out and then generated the whole text by incorporating this keyword's specificity as for the training framework we separated the two stages of the training for the first stage we used an adversarial training framework we employed the self-reconstruction loss to recover the input and then the disentanglement loss conducted on the sentence embeddings which aims to disentangle the style and the context of the sentence table mutations and sentence order loss aims to capture the sentence level dependency and finally the style classifier loss tried to produce the style signal for the whole system and as for the second stage this stage is unrelated to the style transfer which aims to fill the correct the style specific content and remove the mask token and finally we can get the transferred text as we can see the evaluation and data sets um we collected new data sets in chinese and english for these new tasks and we conducted intensive experiments to transfer the fairy tales or aerial stories to typical styles both automatic evaluation results and manual evaluations confirm the efficiency of our model and showing that um stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo stylo sty</sample>
    <sample id="283">hi my name is adam sperkowski and this talk is about the dependency structure of coordination as you may know there are different dependency structures assumed by different theories and corpus approaches so for example in universal dependencies the structure of the coordination lisa bart and maggie is such that the first conjunct is the head of the whole coordination structure so in this case lisa bart and maggie however this effect may be ameliorated when the direct object is very heavy and very long because then it can be moved to the position after the adjunct this is illustrated here so both these sentences are fine right it violates one principle but it satisfies another one okay so what we did we extracted various statistics from about coordination from the enhanced version of the pen tree bank and see the paper why we wouldn't use universal dependencies and these statistics confirm the observation made many times before that left conjuncts tend to be shorter so salt and pepper and not pepper and salt measured in syllables and also the observation that was made in passing that this tendency grows with length the length difference so when the difference between the lengths of the two conjuncts grows the shorter conjunct prefers to be the first one stronger right so the proportion is is bigger of the left uh short conjuncts but what's novel uh in this paper is uh we observed that this tendency only occurs when the governor is on the left the governor is on the right this effect disappears so we show that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination sentences but when the governor is on the right this tendency disappears and we show in the paper how this provides an argument against asymmetric structures of coordination as these two and for the symmetric structures as these two so see the paper for the full agreement and arguments sorry and talk to us about the poster session thank you</sample>
    <sample id="284">hello everyone i'm pang pang xu from wuhan university today i will present my long paper for acl main conference 4915 titled fssuie a novel fuzzy band mechanism for enhancing universal information extraction the current band-based ui model involves identifying and labeling the band boundaries of the target in the text which heavily relies on boundary positions of annotated bands however there is ambiguity in labeling the fuzzy band boundaries that is different annotations bands can be considered reasonable so we propose that the fuzzy band attention layer should be adaptive rather than static in order to model the fuzzy band boundary which represents the target boundary as a continuous distribution of correct probability in a specific range where rmin and rmax represent the start and end of the fuzzy band boundaries and the function q represents the correctness of the target position through the sampling function showing in the slide we convert the continuous boundary distribution into a group of discrete values for calculating the fuzzy band boundaries as for the results on asde tasks fssuie also achieves significant results on 14 lab 15 rest 16 rest of asde v2 dataset and demonstrates competitive performance on 14 rest dataset the result of ablation study shows that fsa improves convergence speed by guiding the model to obtain a reasonable attention distribution and the fssuie we proposed achieves excellent results in a wide range of ie tasks thank you for your listening</sample>
    <sample id="285">hello everyone i'm mingjie liu from pitt university i'm glad to share our work reference methods benchmarking factor error correction for dialogue summarization with fvc evaluation framework this video focuses on the key points of our work as we all know summaries generated by models and even some reference summaries still contain factual errors and there are two main types of solutions the first is to introduce factual error related objectives in the training or inference process to make the summarization models more factual and the second is to design a factual error correction model abbreviated as fvc which is independent of the summarization model it takes the source document and the model generated summary as inputs and outputs a score it is expected that the average score of the corrected summaries are higher than the original ones and more importantly it creates the condition for more comprehensive and accurate evaluation of the performance of fvc models introducing human corrected summaries during the training of fvc models for dialogue summarization can improve their performance combining human annotated data with synthetic data is a promising direction and current fvc models struggle to correct factual errors by addition and cannot address attribute errors modality errors link errors etc thanks for listening</sample>
    <sample id="286">hello i'm james finch and i'm sarah finch and today we'll tell you all about abc eval a new dimensional approach to evaluating conversational ai this work was done by the emory nlp lab led by professor gino choi at emory university and in collaboration with amazon alexa ai so let's say that you just developed a dialogue model and you want to see how well it compares against the current state of the art the common practice is to use human judges to evaluate several dimensions of dialogue quality to understand the strengths and weaknesses of the model on a finer grained level one approach is to simply ask human judges to evaluate several dimensions of dialogue quality such as the relevance of model responses using existing comparative or liker scale methods however we believe there is a more precise and reliable strategy for dimensional dialogue evaluation our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself or its partner hallucinating incorrect facts or violating common sense knowledge and when the model succeeds or fails to show empathy to determine what kind of evaluation is most effective we selected four state-of-the-art chat models and evaluated them on 100 human bot conversations per model using abc eval for comparison we also evaluated these conversations using three existing methods liker ratings on the turn level liker ratings on the dialogue level and dialogue level pairwise comparisons for each of the existing methods we collected evaluations on eight of the most commonly measured aspects of dialogue since this is the standard practice for evaluating chat models along multiple dimensions with the rapid pace of improvement in the field many of these error rates could see a decrease in new models released since our evaluation was conducted however this is all the more reason to pursue reliable and precise evaluation metrics for comparing models we hope abc eval can be leveraged by others in the field as a meaningful step in this direction and we look forward to seeing how conversational ai will advance in the coming months and years thank you for watching</sample>
    <sample id="287">这篇论文有多少位作者</sample>
    <sample id="288">hi everyone i'm kostas michrakkalis and i'm pleased to welcome you to our talk of our acl 2023 papers language model acceptability judgments are not always robust to context this is a joint work with john gauthier aaron muller kanishka misra gareth flinders roger levy and etienne williams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like blimp syntax gem and acceptability in terms of stereotypes such as cross pairs and in this a minimal pair paradigm the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an unacceptable sentence or an ungrammatical sentence and then the hope is that the model basically puts more probability to the acceptable sentences and similar ways that is when we choose sentences from the same phenomenon in blimp syntax gem we see a massive increase or a massive decrease in of the mp judgment for the model depending on whether the chosen prefix is acceptable or unacceptable now this and this is very large like this effect increases throughout the context length and this would probably affect like newer language models which has large context window please read our paper for more details of our experiments thank you for listening</sample>
    <sample id="290">hello i am dawei a phd student at stuttgart university in germany in this video i would like to present our recent work bigger than you think a critical look at weakly supervised learning in weakly supervised learning we do not manually label the data instead we label the data using weak labeling sources such as simple heuristic rules knowledge bases or locality crowdsourcing as illustrated in the figure on the right when compared to human annotations the weak labeling sources are much cheaper yet they are also noisy meaning that a certain amount of the annotations are incorrect if we directly train neural networks on weakly labeled data the neural networks tend to memorize the label noise and do not generalize well in recent works in wsl so wsl stands for weakly supervised learning the training algorithms are proposed to robustly train neural networks on the weakly labeled samples so that the trained models can generalize well beyond the original weakly labeled samples to summarize we show that recent wsl approaches require clean manually annotated samples for them to work properly their performance gain and practicality are heavily overestimated our concrete recommendations for future work are as follows first report the model selection criteria for example report if the model selection is done via clean validation samples second wsl approaches should be compared with fully supervised learning baselines as both work on clean samples third continuous fine-tuning is a simple yet strong baseline that should be considered in future work in wsl finally we have open sourced our code you can find it via the qr code on this slide please feel free to check it out thank you and enjoy the conference</sample>
    <sample id="291">since its release in 2018 bert has become one of the most effective approaches to solving natural language processing tasks and offers huge performance gains compared to historical static and contextualized methods such as word2vec fasttext and elmo in addition to this comparison we introduced three models trained on continuous pre-training to analyze the impact of pre-training strategy one based on the weights and tokenizer of bert trained on the 4gb subset of natchez's another also based on the weights and tokenizer which suffer from stability issues and finally as a conclusion our proper system offers better performance on 9 of the 11 downstream tasks and surpasses globally the result of the generic model here camarber we also observe that specialized data is better more specialized data is better but it doesn't scale well all the pre-trained models obtained from natchez are freely available on yugenface and all the training scripts are on our github repository so thank you for for for this presentation and we are looking forward to exchanging at the poster session in toronto</sample>
    <sample id="294">since its release in 2018 bert has become one of the most effective approaches to solve natural language processing tasks in french and other domains like biomedical and clinical domains in addition to this comparison we introduced three models trained on continuous pre-training to analyze the impact of pre-training strategy uh one based on the weight and tokenizer of bert trained on the 4 gigabytes of set of natchez another also based on bert but trained on the 4 gigabytes of clinical notes and finally one based on the english biomedical model bert and trained on the 4 gigabytes of set of natchez in total we have seven models to evaluate all seven models we gathered which port public and private down-stream tasks such as name entity recognition uh classification part of speech taking and question answering these models are compared to six baseline models which are camembert oscar 188 gigabytes camembert 4 gigabytes camembert 6 gigabytes bert trained on the 4 gigabytes of clinical notes and clinical bert the evaluation of uh highlights that the model performed best on the task with data of the same nature as those uh on which the model has been trained however we can obtain that data from uh we can observe that data from heterogeneous sources appear to be more versatile we also observe that using more data translates into better performance in overall from scratch pre-training seemed to obtain higher performance on most of the tasks however our experiment on continuous pre-training using the weight and tokenizer of bert trained on the 4 gigabytes of set of natchez showed comparable results to those obtained with camembert 4 gigabytes from scratch which is not the case for the model based on camembert weights and tokenizer which suffer from stability issues finally as a conclusion uh our proper system offers better performance on 9 of the 11 down-stream tasks and surpasses globally the result of the generic model here camembert we also observe that specialize data is better more specialized data is better but it doesn't scale well all the pre-trained models obtained from natchez are freely available and uh on yugenface and all the training script are on our github repository so thank you for for for this presentation and we are looking forward to exchanging at the poster session in toronto</sample>
    <sample id="295">hi my name is adam sperkowski and this talk is about the dependency structure of coordination as you may know there are different dependency structures assumed by different theories and corpus approaches so for example in universal dependencies the structure of the coordination lisa bart and maggie is such that the first conjunct is the head of the whole coordination structure so in this case lisa bart and maggie however this effect may be ameliorated when the direct object is very heavy and very long because then it can be moved to the position after the adjunct this is illustrated here so both these sentences are fine right it violates one principle but it satisfies another one okay so what we did we extracted various statistics from about coordination from the enhanced version of the pen tree bank and see the paper why we wouldn't use universal dependencies and these statistics confirm the observation made many times before that left conjuncts tend to be shorter so salt and pepper and not pepper and salt measured in syllables and also the observation that was made in passing that this tendency grows with length the length difference so when the difference between the lengths of the two conjuncts grows the shorter conjunct prefers to be the first one stronger right so the proportion is is bigger of the left uh short conjuncts but what's novel uh in this paper is that we observed that this tendency only occurs when the governor is on the left is absent in the second example homer came and sneezed here we have coordination of two verbs and there's no outside external governor right so in such cases the left conjunct prefers to be shorter the more so the uh the bigger the difference uh between the two conjuncts uh grows the tendency disappears and we show that by measuring length in characters the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we show in the paper how this provides an argument against asymmetric structures of coordination as these two and for the symmetric structures as these two so see the paper for the full agreement and arguments sorry and talk to us about the poster session thank you</sample>
    <sample id="296">hi i am valerio basile and in this video i am going to present a work which is a fruit of a collaboration between the university of turin and amazon alexa natural language understanding and natural language processing in general is based in a large part on supervised machine learning or the so-called data-driven approaches and in order to be able to develop these approaches we need a lot of data large sets of manually annotated data which encode some kind of human knowledge which the annotators put into the data by their process of annotation lately there is a series of works which have been showing that the assumption that there is one single truth what's called the ground truth and the annotation needs to converge towards it is showing some limits we chose to focus on irony in particular which is a highly latent and pragmatic phenomenon in natural language irony detection is already a very difficult task for modern natural language processing models and we want to investigate this problem further rather than training models that are able to say something like a binary label like this sentence is ironic or is not ironic we observed a significant difference in the confidence that the perspective-aware models show and in particular we can see how the perspectiveist models are on average less uncertain more confident of their predictions finally we went again into the data and we tried to look at what may be the causes of the differences in the annotations and we found something peculiar that is that in the case of age it is generations that are close to each other that seem to be more in disagreement toward their perception of irony and similar things happen in the geographical distribution of the annotators where we found that the highest variations in response is given between the two models trained on labels given by annotators from the united kingdom and ireland this is it for this short presentation but we will be happy to answer all your questions and have in further discussions at the poster session</sample>
    <sample id="297">today i'll be talking about our work from dog whistles to bullhorns unveiling coded rhetoric with language models so this is an example of a speech given by senator josh hawley a few years ago where he's complaining about the cosmopolitan elite agenda and experiment while a lot of people will think that he's complaining about maybe urban liberal worldly people some will interpret this as a screed against jewish people so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle terms that send one message to an outgroup and a second often taboo controversial or inflammatory message to an ingroup so in this case the ingroup knows cosmopolitan means jewish but he has plausible deniability because he never actually says jewish so cosmopolitan is an example of a dog whistle</sample>
    <sample id="298">hello everyone my name is xuhong today i'm going to present our paper do conno 2003 named entity taggers still work well in 2023 and last but not least we calculated the percentage change in f1 to assess the generalization of each model so what is needed for good generalization through our experiments we found that from the graph on the right the red best fit line has a gradient that is greater than one this means that every unit of improvement that we made on conno 2003 translates to more than one unit improvement on conno which means that there is no diminishing returns and this shows us that adaptive overfitting in this case is not observed so going back to the question that we raised in the title of our paper do conno 2003 taggers still work in 2023 and we found that the answer is actually a resounding yes we hope our paper calls for more research on how to improve generalization of the models and lastly please make sure to check out our paper our data set and if you have any questions feel free to contact me thank you so much</sample>
    <sample id="299">hi everyone my name is mihalis karakasgis today we are here to talk about improving the robustness of nli models with minimax training this is joint work with andreas lajos at the university of cambridge nli models have achieved state-of-the-art results across a plethora of benchmarks however despite rapid progress recent work has demonstrated that the success of nli models is partly due to learning and using shortcuts shortcuts are spurious correlations between the input attributes and labels introduced during the data curation process for instance the nli model can learn to exploit shortcuts by being trained only on a small number of examples or by leveraging an auxiliary with reduced learning capabilities the output of the auxiliary is then used to re-weight training instances for the learner's out-of-distribution performance while maintaining high in-distribution accuracy finally in our paper we also examine whether the performed improvements transfer in larger models synthetic shortcuts and out-of-distribution test sets what is the effect of pre-training the learner how small the auxiliary needs to be and finally we conduct a qualitative evaluation of the learner's example weight distribution if you find this work interesting we would love to chat with you during our post session thank you for your time</sample>
    <sample id="300">hi my name is belinda and the work i'll be presenting today introduces a task called interactive dictation and makes initial steps towards solving this task this is work done at semantic machines in collaboration with jason eisner adam pauls and sam thompson so what is interactive dictation at a high level interactive dictation is a process where users can use their voice to both dictate and edit a document in a natural and intuitive manner in this example a user starts by dictating just wanted to ask about the event on the twenty third this is transcribed verbatim into the text box however in the middle of speaking the user realizes they made a mistake and corrects themselves saying on friday the twenty third ideally the system can pick up that this was a speech correction and replace the correct span with a new utterance in summary our contribution is threefold first we introduce and formalize a new task interactive dictation second we design a data collection interface and build a dataset for this task and finally we create a baseline system for this task first for the segmentation model we see that it's both fairly accurate and efficient next we evaluate the asr repair and interpretation models jointly using exact match of the predicted end state against the gold end state we find that there is generally a trade-off between runtime and accuracy and that generally gpt-3 models are more accurate but also much slower furthermore for gpt-3 models predicting state directly is much more accurate than predicting programs allows us to significantly improve efficiency with minimal impact on accuracy as you can see however there is clearly much more room for progress here and we welcome more work on this task to facilitate future work we have released code at the following site please also check out the paper for more details</sample>
    <sample id="302">hi my name is matthias linderman and today i'm going to give you a brief introduction to our paper on compositional generalization without trees using multi-set tagging and latent permutations this is joint work with my advisors alexander kola and ivan titov compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training in the context of semantic parsing testing for compositional generalization might look like this as usual we have a training set of utterances in this case the girl slept and mary knew that the girl slept this works well but trees are usually not given and need to be obtained somehow this can be complicated and sometimes a computationally expensive process typically this involves considerable formalism-specific pre-processing of the logical forms for example to handle variable symbols obtaining trees may also involve specialized grammar induction procedures in this paper we solve a couple of interesting technical challenges first of all the alignment between input and output is not given in the training data as a consequence for a given token we don't know which multi-set token it came from which poses a challenge for training in addition sometimes there are multiple permutations that are consistent with the data but the linguistically correct one is latent we address this by inducing the alignment as part of the training our permutation method is very flexible but it brings the challenge that finding the highest scoring permutation is np-hard that's because this is related to the traveling salesman problem we approximate this with a gpu-friendly continuous relaxation that also allows us to back-propagate through the solution and learn the linguistically more plausible permutations if you want to learn more about our experiments and how we address these challenges please have a look at our paper or come to our poster</sample>
    <sample id="303">hi i'm myra and today i'll be talking about our paper marked personas using natural language prompts to measure stereotypes in language models this work is done in collaboration with essem dermanush and dan jurafsky in recent years many have documented the prevalence of social bias and stereotypes in large language models however these measures have various limitations they usually rely on hand-constructed datasets that are very time-consuming to curate and they also usually only measure very specific stereotypes meaning that they don't generalize well to other demographics or contexts or they simply capture very general broad associations like negative associations with particular groups so for instance for the personas of black women we see that the words for each marked group pretty much just reflect very essentializing narratives so based on these patterns we conclude with three recommendations for model owners first we should as researchers be addressing positive stereotypes and essentializing narratives we should also be using an intersectional lens to study biases and harms because there's a lot of things that might be overlooked if we don't do that and finally there should really be increased transparency about bias mitigation methods because for instance like these positive stereotypes we don't know if it's because there's some sort of like weird overly excessive value alignment going on or maybe some other like anti-stereotyping methods that are resulting in these pernicious patterns we just really can't make any assumptions or really study that further without more transparency thank you so much for listening have a good time at acl</sample>
    <sample id="304">hi everyone i'm kostas michrakkalis and i'm pleased to welcome you to our talk of our acl 2023 paper language model acceptability judgments are not always robust to context this is a joint work with john gauthier aaron muller kanishka misra gareth flinders roger levy and etienne williams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like blimp syntax gem we see a massive increase in acceptability judgments in similar ways that is when we perturb the input sentences by trying to preserve the relevant structure but adding like noise to the input and after doing like several of these perturbations we find that none of these noises are actually making the model like change its course in terms of how it shows us the mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to the latent syntactic and semantic features which are shared across the sentences and the mpp evaluation the way that we do it currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context window please read our paper for more details of our experiments thank you for listening</sample>
    <sample id="305">hello i am dawei a phd student at stanford university in germany in this video i would like to present our recent work weak supervision and weakly supervised learning in weak supervision you do not manually label the data instead we label the data using weak labeling sources such as simple heuristic rules knowledge bases or locality crowdsourcing as illustrated in the figure on the right when compared to human annotation the weak labeling sources are much cheaper yet they are also noisy meaning that a certain amount of the labeling data are incorrect if we directly train neural networks on weakly labeled data the neural networks tend to memorize the labeling noise and do not generalize well in weak supervised learning we address this research question in our work and our findings are as follows first we find that interestingly recent weak supervised learning methods indeed require clean validation samples to work properly otherwise there is a large performance drop as shown in the figure on the left typically we only need 20 samples per class to attain high performance but that's not the end of the story because if we allow to continue fine-tuning on the clean validation samples then ftw performs equally well with other methods so in practice there's no reason to choose more complex wsl methods which require more computation time and the disk space third continuous fine-tuning is a simple yet strong baseline that should be considered in future work in wsl finally we have open source our code you can find it via the qr code on this slide please feel free to check it out thank you and enjoy the conference</sample>
    <sample id="306">hello everyone i am sebastian schuster and together with najan kim i'm going to give you a short overview of our work on entity tracking in language models for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for example in the context of a recipe an agent has to understand that put the eggs sugar and flour in a bowl results in all of these three entities being in the bowl and if the discourse continues with mix to form a light batter then the agent has to understand that now all of these entities are part of the batter and so on so we argue that this is a crucial ability for understanding longer discourses but there haven't really been any systematic investigations into whether pre-trained language models can actually perform such tasks and so we argue that this is a crucial ability for understanding longer discourses for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds so for an agent to understand a discourse it needs to track which entities are mentioned</sample>
    <sample id="307">作者使用了哪些评估指标来评估模型的性能？作者使用了哪些评估指标来评估模型的性能？作者使用了哪些评估指标来评估模型的性能？作者使用了哪些评估指标来评估模型的性能？作者使用了哪些评估指标来评估模型的性能？作者使用了哪些评估指标来评估模型的性能？</sample>
    <sample id="308">hi everyone i'm jenny a first year phd student at carnegie mellon university and today i'll be presenting our work entitled positionality characterizing design bias in ai datasets and models this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen institute for ai this work was done in collaboration with some folks at the university of washington and the allen</sample>
    <sample id="309">hello i'm james finch and i'm sarah finch and today we'll tell you all about abc eval a new dimensional approach to evaluating conversational ai this work was done by the emory nlp lab led by professor gino choi at emory university and in collaboration with amazon alexa ai so let's say that you just developed a dialogue model and you want to see how well it compares against the current state of the art the common practice is to use human evaluation such as by asking human judges to select which of two conversations is better or to rate conversations given a likert scale method however we believe there is a more precise and reliable strategy for dimensional dialogue evaluation our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself or its partner hallucinating incorrect facts or violating common sense knowledge and when the model succeeds or fails to show empathy to determine what kind of evaluation is most effective we selected four state-of-the-art chat models and evaluated them on 100 human bot conversations per model using abc eval for comparison we also evaluated these conversations using three existing methods likert ratings on the turn level likert ratings on the dialogue level and dialogue level pairwise comparisons for each of the existing methods we collected evaluations on eight of the most commonly measured aspects of dialogue since this is the standard practice for evaluating chat models along multiple dimensions with the rapid pace of improvement in the field many of these error rates could see a decrease in new models released since our evaluation was conducted however this is all the more reason to pursue reliable and precise evaluation metrics for comparing models we hope abc eval can be leveraged by others in the field as a meaningful step in this direction and we look forward to seeing how conversational ai will advance in the coming months and years thank you for watching</sample>
    <sample id="310">hi everyone i'm kostas michrakis and i'm pleased to welcome you to our talk of our acl 2023 papers language model acceptability judgments are not always robust to context this is a joint work with john gauthier aaron muller kanishka misra gareth flinders roger levy and etienne williams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like blimp syntax gem and acceptability in terms of stereotypes such as cross pairs and in this a minimal pair paradigm the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an unacceptable sentence or an ungrammatical sentence and then the hope is that the model basically puts more probability to the acceptable sentence or the unacceptable sentence depending on whether the chosen prefix is acceptable or unacceptable now this and this is very large like this effect increases throughout the context length and this would probably affect like newer language models which has large context window so why does the match prefix affect the language model judgment so much so we did a series of analysis where we tried to like perturb the input sentence by trying to preserve the relevant structure but adding like noise to the input and after doing like several of these perturbations we find that none of these noises are actually making the model like change its course in terms of how it shows us the mpp judgment trend basically we find that the models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences and the mpp evaluation the way that we do it currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context window please read our paper for more details of our experiments thank you for listening</sample>
    <sample id="311">hi welcome to our presentation of deplain a new corpus for german text simplification on the document level and on the sentence level my name is regine stodden and i will guide you through the first part of the presentation let's first define text simplification text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group as people with reading problems or non-native speakers to train a text simplification model we require parallel pairs of text for example documents or sentences and the example here you can see a parallel aligned sentence pair of a complex german text and its translation into plain language to simplify the sentence pairs a little bit more so for example on the type of simplification structural simplification also overall level of simplification and now as we have our dataset deplain which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods and we did some adaptations to the proposed methods and we have published all these adaptations and the codes to run our experiments in the paper the second use case that we showed in our paper is the case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text we have fine-tuned two different models we have fine-tuned the model of longformer to produce document level simplifications and we also fine-tuned the normal base long the normal base import to produce sentence level simplifications you can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper thank you so much for your attention and we hope to meet all of you during the conference thank you</sample>
    <sample id="312">hello everyone my name is ying and my colleague yan and i will be presenting our research on multi-instruction improving multi-modal zero-shot learning via instruction tuning so with the advances in large language models many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data efficient way recently many studies have shown that instruction tuning enables large language models to perform on unseen multi-modal tasks in a zero-shot manner by following natural instructions however most previous works on instruction tuning focus on improving the zero-shot performance on language-only tasks while computer vision and multi-modal tasks have been left out therefore in this work we want to investigate whether instruction tuning on multi-modal pre-trained models can actually improve the zero-shot capability of ofa and we explore different transfer learning techniques and show their benefits we design a new metric called sensitivity so one more thing we are collecting a much larger multi-modal instruction tuning dataset with around 150 additional visual language tasks and we will release them so this is a qr code for our data and model thank you</sample>
    <sample id="313">hello i'm james finch and i'm sarah finch and today we'll tell you all about abc eval a new dimensional approach to evaluating conversational ai this work was done by the emory nlp lab led by professor gino choi at emory university and in collaboration with amazon alexa ai so let's say that you just developed a dialogue model and you want to see how well it compares against the current state of the art the common practice is to use human judges to evaluate several dimensions of dialogue quality to understand the strengths and weaknesses of the model on a finer grained level one approach is to simply ask human judges to evaluate several dimensions of dialogue quality such as the relevance of model responses using existing comparative or liker scale methods however we believe there is a more precise and reliable strategy for dimensional dialogue evaluation our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself or its partner hallucinating incorrect facts or violating common sense knowledge and when the model succeeds or fails to show empathy to determine what kind of evaluation is most effective we selected four state-of-the-art chat models and evaluated them on 100 human bot conversations per model using abc eval for comparison we also evaluated these conversations using three existing methods liker ratings on the turn level liker ratings on the dialogue level and dialogue level pairwise comparisons for each of the existing methods we collected evaluations on eight of the most commonly measured aspects of dialogue since this is the standard practice for evaluating chat models along multiple dimensions with the rapid pace of improvement in the field many of these error rates could see a decrease in new models released since our evaluation was conducted however this is all the more reason to pursue reliable and precise evaluation metrics for comparing models we hope abc eval can be leveraged by others in the field as a meaningful step in this direction and we look forward to seeing how conversational ai will advance in the coming months and years thank you for watching</sample>
    <sample id="314">二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义是什么？ 二进制协调的定义</sample>
    <sample id="315">hi i'm myra and today i'll be talking about our paper marked personas using natural language prompts to measure stereotypes in language models this work is done in collaboration with essender mush and dan juravsky in recent years many have documented the prevalence of social bias and stereotypes in large language models or lls however these measures have various limitations they usually rely on hand-constructed datasets that are very time-consuming to curate and they also usually only measure very specific stereotypes meaning that they don't generalize well to other demographics or contexts or they simply capture very general broad associations like negative associations with particular groups so in our method we rely on the property that these newer instruction-tuned lls are very good at responding to prompts and so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this persona so we</sample>
    <sample id="316">这些发现对较小的 t5 模型有什么影响</sample>
    <sample id="317">hello everyone i'm from university i'm very delighted to present our work titled could i last code generation task and information extraction task into a code generation task and use code pre-training language models and code format prompts significantly and consistently outperform the traditional baseline models such as uie and natural language like large language models the gpt-3 model in information extraction tasks overall further conducted a detailed and in-depth analysis of this phenomenon firstly we observed that the perplexity computed on test format prompts using models like gpt-3 was generally higher than that of using code format prompts such errors were almost non-existent we found that transforming information extraction task into a code generation task and using code pre-training language models aligns better with the information extraction task itself furthermore we observed that when decoding with gpt-3 and test format prompts especially in terms of recall we hope this analysis can provide some inspiration to everyone lastly thank you all if you have any questions please feel free to contact me our paper and code are made publicly available</sample>
    <sample id="318">hi i am yannis lavrac and i will present you our work on dr bert a robust pre-trained model in french for biomedical and clinical domain in this presentation we first talk about language modeling in healthcare then we present the main contribution of our article we introduce the first french model for biomedical and clinical downstream tasks in french and finally we conclude about the experiments and give you more details about how to access to the models since its release in 2018 bert has become one of the most effective approach to solve natural language processing tasks and offer huge performance gain compared to historical static and contextualized method such as word2vec fasttext or elmo in addition to this comparison we introduce three model trained on continuous pre-training to analyze the impact of pre-training strategy one based on the weight and tokenizer of bert trained on the 4gb subset of natsch's show comparable result to those obtained with bert 4gb from scratch which is not the case for the model based on contextualizer which suffer from stability issues finally as a conclusion uh our proper system offer better performance on 9 of the 11 downstream tasks and surpass globally the result of the generic model here camarber we also observe that uh specialize data is better more specialized data is better but it doesn't scale well all the pre-trained model obtained from natsch's are freely available and uh on yugenface and all the training script are on our github repository so thank you for for for this presentation and we are looking forward to exchanging at the poster session in toronto</sample>
    <sample id="319">论文研究了哪些学习策略？</sample>
    <sample id="320">hello everyone my name is xuhong today i'm going to present our paper do conno 2003 named entity taggers still work well in 2023 and last but not least we calculated the percentage change in f1 to assess the generalization of each model so what is needed for good generalization through our experiments we found that from the graph on the right the red best fit line has a gradient that is greater than one this means that every unit of improvement that we made on conno 2003 translates to more than one unit improvement on conno which means that there is no diminishing returns and this shows us that adaptive overfitting in this case is not observed so what about temporal drift then for temporal drift we didn't experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift and kind of surprisingly it is not caused by adaptive overfitting even though conno 2003 has been used for over 20 years so going back to the question that we raised in the title of our paper do conno 2003 taggers still work in 2023 and we found that the answer is actually a resounding yes we hope our paper calls for more research on how to improve generalization of the models and lastly please make sure to check out our paper our data set and if you have any questions feel free to contact me thank you so much</sample>
    <sample id="321">hi welcome to our presentation of deplain a new corpus for german text simplification on the document level and on the sentence level my name is regina stodden and i will guide you through the first part of the presentation let's first define text simplification text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group as people with reading problems or non-native speakers to train a text simplification model we require parallel pairs of text for example documents or sentences in the example here you can see a parallel aligned sentence pair of a complex german text and its translation into plain language to simplify the sentence pairs a little bit more so for example on the type of simplification structural simplification also overall level of simplification and now as we have our dataset deplain which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods and we did some adaptations to the proposed methods and we have published all these adaptations and the codes to run our experiments in the paper uh we concluded that this this basic fine-tuning could produce uh or could get uh scores better than the baseline scores and we propose those results as a benchmark a baseline benchmark for the problem of automatic text simplification in the future thank you so much for your attention and we hope to meet all of you uh during the conference thank you</sample>
    <sample id="322">hi everyone i'm enrico and i will be presenting at acl 23 answering the question what does a text classifier learn about morality first off let me explain you what is morality human morality is what helps us distinguish right from wrong it's our internal compass that helps us determine whether an action or a concept is morally right or morally wrong and morality is at the base of our societies obviously and it's essential that language models can understand and classify morality in text and it's been shown that language models can somewhat understand morality in text in particular we focus on understanding how morality is expressed differently across different domains we use a dataset called moral foundation twitter corpus composed of 35000 tweets collected in seven different domains domains for examples corresponding to the hashtag all lives matter or the hashtag black lives matter these two domains have a similar rhetoric right because they cover similar topics but they have a significantly different rhetoric for the moral element of subversion which means rebellion to authority we know intuitively and we found that language models recognize that in alm subversion is associated with words that you know such as overthrow mayhem and subversion is frowned upon whereas in blm subversion is somewhat encouraged so language models do recognize that morality can be expressed differently in different domains and using just one single model for many different domains can actually lead to misunderstandings of morality in a very dangerous way i hope to see you at acl in toronto and see you there bye</sample>
    <sample id="323">the title of my paper is a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models and knowledge base for complex question answering we propose a dynamic heterogeneous graph based on language models</sample>
    <sample id="324">hi i'm jianbin phd student at university of washington today i'm presenting our work from pre-training data to language models to downstream tasks tracking the trails of political biases leading to unfair nlp models so language models are trained on large-scale web crawl data political news media are well covered in their pre-training data according to a survey of the c4 corpus we can see that new york times los angeles times the guardian huffington post etc are well covered in language model training data this has created a mixed blessing for language model applications so on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas on the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream tasks ultimately creating fairness issues if we do try to sanitize somehow we would also risk censorship or exclusion and it's incredibly hard to determine what is actually neutral and should be retaining language model training data so it's kind of like the electric chollie problem okay great i think that's pretty much all i have for today thank you for your time</sample>
    <sample id="325">hi my name is mathias linderman and today i'm going to give you a brief introduction to our paper on compositional generalization without trees using multiset tagging and latent permutations this is joint work with my advisors alexander kola and ivan titov compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training in the context of semantic parsing testing for compositional generalization might look like this as usual we have a training set of utterances in this case the girl slept and mary knew that the girl slept this works well but trees are usually not given and need to be obtained somehow this can be complicated and sometimes a computationally expensive process typically this involves considerable formalism-specific pre-processing of the logical forms for example to handle variable symbols obtaining trees may also involve specialized grammar induction procedures in this paper we solve a couple of interesting technical challenges first of all the alignment between input and output is not given in the training data as a consequence for a given token we don't know which multiset it came from which poses a challenge for training in addition sometimes there are multiple permutations that are consistent with the data but the linguistically correct one is latent we address this by inducing the alignment as part of the training our permutation method is very flexible but it brings the challenge that finding the highest scoring permutation is np-hard that's because this is related to the traveling salesman problem we approximate this with a gpu-friendly continuous relaxation that also allows us to back-propagate through the solution and learn the linguistically more plausible permutations if you want to learn more about our experiments and how we address these challenges please have a look at our paper or come to our poster</sample>
    <sample id="326">hello my name is vasudha and i'm a computer science phd candidate at stony brook university i would like to present our work accepted into acl 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge we begin by defining cognitive dissonance and why it is an important problem to study in language simply put cognitive dissonance is two beliefs or actions that are inconsistent such as this example where a person states i know that cigarettes could kill me and then goes on to say i grabbed a couple of smokes after the meeting this belief and action are inconsistent and they are in dissonance further mentioning that i don't think i could keep my job without them justifies the second occurrence and they have a dissonance relationship since the initial model was not able to capture the dissonance class at all we start the cold start the active learning process by transferring weights from closely related tasks topic independent dissonance stance classification a task that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic we call them ce here we find that on transferring the zero shot performance on the annotated data set is already much better than chance with the best with a uc of 075 which is the best performance that we have on the task so far we also check the feasibility of each strategy for annotation quality and costs to annotators we find that prc has the highest percentage of dissonance and works best for rare class however the annotators also find the examples difficult in summary we find that prc is a simple ael strategy for rare class acquisition and cold starting ael with appropriately designed transfer learning tasks can help significantly we also find that iterative update is useful for transfer learning from a different domain whereas in-domain active annotations benefit from cumulative update these are the links to our code data set and our paper feel free to get in touch with us if you have any questions thank you</sample>
    <sample id="327">hello everyone i'm xiaoxue a 30 year old phd student from harbin institute of technology i'm honored to present our work to you at acl 2023 thank you for your interest in our work magic tower aggregating the insights of unimodal experts at different levels visual question answering is one of the most famous unimodal tasks which need to answer a question based on the input image since 2019 with the help of large-scale self-supervised pre-training on image text pairs transformer-based unimodal models have achieved remarkable progress in visual question answering magic tower aggregates the insights of unimodal experts at different levels to facilitate more comprehensive cross-modal alignment and fusion it is important to notice that one can apply any visual textual cross-modal encoder in magic tower with only four million images for visual question answering training the x-axis is the index of the unimodal expert and the legend shows the index of the cross-modal layer we first take a look at static managers no matter the textual role or visual role a similar progressive trend are shared in each cross-modal layer which is inconsistent with the intuition that the need for unimodal semantic knowledge varies among cross-modal layers this provides strong evidence that adaptive managers can adaptively exploit different levels of unimodal semantic knowledge for comprehensive cross-modal representation learning paper code and models are available on archive and github we hope our work can be useful to you thank you</sample>
    <sample id="328">hi i'm jianbin phd student at university of washington today i'm presenting our work from pre-training data to language models to downstream tasks tracking the trails of political biases leading to unfair nlp models so language models are trained on large-scale web crawl data political news media are well covered in their pre-training data according to a survey of the c4 corpus we can see that new york times los angeles times the guardian huffington post etc are well covered in language model training data this has created a mixed blessing for language model applications so on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas on the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream tasks ultimately creating fairness issues if we do try to sanitize somehow we would also risk censorship or exclusion and it's incredibly hard to determine what is actually neutral and should be retaining language model training data so it's kind of like the electric cholli problem okay great i think that's pretty much all i have for today thank you for your time</sample>
    <sample id="329">this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with shanghai jiao tong university this work was done in cooperation with sh</sample>
    <sample id="330">hello my name is vasudha and i'm a computer science phd candidate at stony brook university i would like to present our work accepted into acl 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge we begin by defining cognitive dissonance and why it is an important problem to study in language simply put cognitive dissonance is two beliefs or actions that are inconsistent such as this example where a person states i know that cigarettes could kill me and then goes on to say i grabbed a couple of smokes after the meeting this belief and action are inconsistent and they are in dissonance while dissonance is a very common phenomenon we experience in daily decision-making they are really rare to find expressed in language among other kinds of discourse relations so why does this matter studying cognitive dissonance can help us understand the effects of disagreement among people track trends in belief values and attitude changes in population high cognitive dissonance is also related to anxiety disorders and can help us understand people's mental health better studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups finally on iterative fine-tuning on both tasks we find that fine-tuning the cee task followed by further fine-tuning on debate yields a much better zero-shot performance thus this is the model that we use to cold start the active learning process next we determine the best method to update a model with new data from each round of active learning and annotations cumulative accumulates all the data collected from active annotations so far whereas iterative updates the model by training on the latest set of data collected as in-domain active annotations benefit from cumulative update these are the links to our code data set and our paper feel free to get in touch with us if you have any questions thank you</sample>
    <sample id="331">演讲者的名字是什么？演讲者是谁？演讲者是谁的演讲者是谁的演讲者是谁的演讲者是谁的演讲者是谁的演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者是谁演讲者 is</sample>
    <sample id="332">hello my name is kyo yin and i will be presenting our work titled when does translation require context a data-driven multi-lingual exploration this work was done in collaboration with patrick fernandez and andrew f d martin and graham neubig so a lot of translations depend on context for example how would we translate mole in this sentence well if the previous sentence was things could start to get dangerous if the ministers find out then mole refers to a spy but if the previous sentence was could it be anything serious doctor then mole refers to a birthmark so depending on context the meaning of the word changes and therefore its translation changes as well however evaluating how well models can handle these cases is pretty hard firstly because only a small portion of translations depend on context which makes corpus-level metrics like ble unable to capture these translations and secondly because these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation in this work we try to answer these two questions firstly when translations require context and secondly how well do models handle these cases to answer the first question we start by measuring how much a word depends on context in translation in the previous work we introduced cxmi as a measure for context usage by machine translation models and this is done by measuring how much information the context c provides about the target y given the source x we can then also note that different languages have different proportions of these context phenomena we then use the multi-tagger by applying the tagger on the parallel corpus that we want to use for evaluation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which</sample>
    <sample id="333">hi everyone i'm wang hao from nan university it is a great honor to be here to introduce our work in injecting curated knowledge into an mt model before introducing our work i would like to acknowledge collaborators the ajin jiang from shanghai ai lab xuan xuan huang and jiachen chen from nan university and liming hong from the university of hong kong in this work we focus on neural machine translation we know that the target of mt to learn a generalized representation space to adapt to diverse scenarios however neural networks often induce a non-smooth representation space which limits its generalization ability specifically we adjust the representation by allowing three kinds of representation using k-divergence at first we align contextualized representation and token embedding to keep semantic meaning then we align contextualized representation and curated token embedding to enrich semantic meaning and we also align curated representation of the same target token to address the sparsely dispersing problem overall we optimize adapter with the combined learning objective and run this training loop until convergence in the end of the training loop we can drop the data store aside in our experiments we explore the following three questions the first research question is can we smooth the representation space with a small adapter and drop the data store aside during inference the second research question is how much improvement can be brought by using curated knowledge to adjust the representation distribution the third research question is will together using adapter and data store can further smooth predictions which indicates that the representation space of the mt model is not fully refined by the adapter if a more effective framework can be designed the smoothness of the representation space will be further revealed to conclude we propose a novel training framework in this paper in our framework we devise an inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system locates on the top right of each figure which means that ink achieves higher blue scores with less memory space besides we also find that generally applying adapter and data store can further smooth predictions which indicates that the representation space of the mt model is not fully refined by the adapter if a more effective framework can be designed the smoothness of the representation space will be further revealed to conclude we propose a novel training framework in this paper in our framework we devise an inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise an inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise an inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise an inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise an inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise an inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise an inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise an inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise an inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise an inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise an inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine the representation space of the mt model according to curated knowledge experimental results show that ink system achieves an average gain of 199 comic score and 10 blue score compared with the state-of-the-art curated mt system our ink system also achieves better translation performance with less memory space and faster inference speed to conclude we propose a novel training framework in this paper in our framework we devise a inject and refine training loop to iteratively refine</sample>
    <sample id="335">hi my name is mathias lendemann and today i'm going to give you a brief introduction to our paper on compositional generalization without trees using multiset tagging and latent permutations this is joint work with my advisors alexander koller and ivan titov compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training in the context of semantic parsing testing for compositional generalization might look like this as usual we have a training set of utterances in this case the girl slept and mary knew that the girl slept this works well but trees are usually not given and need to be obtained somehow this can be complicated and sometimes a computationally expensive process typically this involves considerable formalism-specific pre-processing of the logical forms for example to handle variable symbols obtaining trees may also involve specialized grammar induction procedures in this paper we solve a couple of interesting technical challenges first of all the alignment between input and output is not given in the training data as a consequence for a given token we don't know which multiset it came from which poses a challenge for training in addition sometimes there are multiple permutations that are consistent with the data but the linguistically correct one is latent we address this by inducing the alignment as part of the training our permutation method is very flexible but it brings the challenge that finding the highest scoring permutation is np-hard that's because this is related to the traveling salesman problem we approximate this with a gpu-friendly continuous relaxation that also allows us to back-propagate through the solution and learn the linguistically more plausible permutations if you want to learn more about our experiments and how we address these challenges please have a look at our paper or come to our poster</sample>
    <sample id="336">hello everyone my name is xin jian from the penn state university today i'm going to present our work exemplar cross-lingual semantic parsing in multiple natural languages and many representations so semantic parsing is the task to build semantic representations of user queries such as sql and lambda calculus and cross-lingual semantic parsing is the task to translate queries in multiple natural languages into multiple meaning representations as shown in this figure we need to translate the query in multiple natural languages using neural models to sql lambda or funql and etc and we also find many interesting results so regarding analyses of monolingual models we evaluate on two groups of models including encoder pdr which stands for multi-lingual pre-trained encoders with pointer-based decoders such as xlmr plus pdr and bert plus pdr and we also evaluate encoder-decoder models which is multi-lingual pre-trained encoder-decoder models such as mbert and mt5 we found that encoder-decoder obtains the best performance on all nine data sets and only gains in three data sets i think this is known as the curse of multi-linguality we also compare the cross-lingual performance gap in this figure the blue line is cross-lingual transfer the orange line is cross-lingual zero-shot transfer while the green line is the monolingual setting we found that by comparing the green and orange line we found that for zero-shot setting the cross-lingual transfer performance gap is significant and by comparing blue and orange line we found that for few-shot setting the transfer gap is shortened rapidly we also find some other interesting findings for example um encoder-decoder outperforms previous work or achieved comparable results for training on english natural language and significantly boost the performance of few-shot on target natural languages and we found monolingual language models such as coders and blue are still inferior for cross-lingual semantic parsing tasks um to sum up we build exemplar a unified benchmark for cross-lingual semantic parsing with multiple natural languages and many representations we conduct a comprehensive benchmark study on three representative of types of multi-lingual language models and our result shows many interesting findings and etc and welcome to visit our paper and code thanks for listening</sample>
    <sample id="337">hello everyone it's michael george from dentria our work graph based relation mining for out of vocabulary word embedding in this speech i will provide an overview of our research and highlight its key contributions our work aims to address the issue of out of vocabulary word embedding by introducing a graph-based attention network that learns attributes based on the characters of the out of vocabulary words to capture the holistic information and summarize the out of vocabulary words for our purpose through extensive experiments we have demonstrated that our model outperforms the baseline in both intrinsic and extrinsic tasks which proves the effectiveness of learning out of vocabulary words by water formation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our model can handle various complex water formations we believe the applicability of our model to other languages will largely depend on the feasibility of out of vocabulary words by water segmentation in conclusion the graph in our</sample>
    <sample id="338">good day everyone my name is pengsheng and i want to express my gratitude for your interest in our research today i will be presenting our work titled are human explanations always helpful towards objective evaluation of human natural language explanations on behalf of our research group it is a collaborative work of researchers from rensselaer polytechnic institute northeast university and ibm research we will briefly present our motivation discuss related works and primarily focus on the contributions which are divided into three sections a unified structure preliminary experiments and an evaluation of five datasets and two models comparing our proposed metric with an established metric our matrix can reflect this observation better than the simulatability score by examining the dataset order in both tables which is based on our true scores we observe that our metric consistently ranks dataset qualities on both t5 and bart in contrast simulatability score falls short in evaluating comve and esnli this suggests that the helpfulness of human explanations to models heavily depends on the task and explanation format such as negation connotation in esnli and comve and the counterfactual writing styles for neutral contradiction classes our hypothesis is supported by recent works discussed in our paper to summarize the contributions in our work in terms of the desired data for phase four explanation evaluations we propose a unified data structure to preliminary experiments analyzing factors contributing to explanation utility and a proposal of a metric with an evaluation of five datasets with two models our evaluation demonstrates that our metric outperforms simulatability score for this purpose we emphasize that our work lays the foundation for high quality human collaboration annotation jobs and we recommend researchers to perform similar quality checks in the future for more detailed findings please refer to our paper thank you for your attention and listening to my presentation</sample>
    <sample id="339">hello i am dawei a phd student at stanford university in this video i would like to present our recent work weak supervision and weakly supervised learning in weak supervision we do not manually label the data instead we label the data using weak labeling sources such as simple heuristic rules knowledge bases or locality crowdsourcing as illustrated in the figure on the right when compared to human annotations the weak labeling sources are much cheaper yet they are also noisy meaning that a certain amount of the labeling data are incorrect if we directly train neural networks on weakly labeled data the neural networks tend to memorize the labeling noise and do not generalize well in this work we address this problem setting but this implies that additional manual annotations are required in weakly supervised learning approaches to achieve high performance as shown in the figure on the left typically we only need 20 samples per class to attain high performance but that's not the end of the story because if we either way decide to access clean samples then training on them directly will even achieve better performance as shown in the right figure ftw initially underperforms more complicated wsl methods like cosine however if we allow to continue fine-tuning on the clean samples then ftw performs equally well with other methods so in practice there's no reason to choose more complex wsl methods which require more computation time and disk space our concrete recommendations for future work are as follows first report the model selection criteria for example report if the model selection is done via clean validation samples second wsl approaches should be compared with fully supervised learning baselines as both work on clean samples third continuous fine-tuning is a simple yet strong baseline that should be considered in future work in wsl finally we have open sourced our code you can find it via the qr code on this slide please feel free to check it out thank you and enjoy the conference</sample>
    <sample id="340">hello everyone i'm guan hao huang from uca i'm presenting our work para mr a large-scale syntactically diverse paraphrase dataset by amr back translation this is a jointed work with veron ehong anoub kaiwei and aron we propose to use amr back translation to generate syntactically diverse paraphrases first we will use the pre-trained amr parser to get amr graph of a source sentence then we will change the focus of the graph we will randomly sample a node and set it as a new root node then modify the corresponding edge and the edge able then we will use the amr graph to text generator to generate text from the modified graph for the generated text because they share the same amr graph structure so they will have similar semantics also we present some quantitative analysis for para mr we have automatic scores and we also have human evaluation scores since para mr is more syntactically diverse we observe that para mr can get a higher score for future learning for the second application we consider syntactic control paraphrase generation we show that by training with para mr we can get a paraphrase generator that has better syntactic control finally we consider using paraphrase generator to generate paraphrases for data augmentation for future learning since para mr is more syntactically diverse we observe that para mr can get a higher score for future learning here is the conclusion of our work in this work we propose para mr a large-scale syntactically diverse paraphrase dataset which is constructed by amr back translation and we show that para mr benefit several nlp applications compared to existing paraphrase dataset our dataset is available at this link thank you</sample>
    <sample id="341">作者使用了哪些延迟测量方法？ 作者使用了哪些延迟测量方法？ 作者使用了哪些延迟测量方法？  作者使用了哪些延迟测量方法？  作者使用了哪些延迟测量方法？  作者使用了哪些延迟测量方法？  作者使用了哪些延迟测量方法？  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延迟测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法  作者使用了哪些延违测量方法</sample>
    <sample id="342">hello everyone my name is gao jinjun today i'm going to present a large-scale personalized dialogue dataset left chat which is conducted by me li xinzhu from southeast university and shanghai ai here is the outline of my presentation first part is introduction with the open-domain dialogue which means a type of conventional exchange between a human and an artificial intelligence system that can cover a range of topics and doesn't have a specific goal which relies on pre-trained models and the large-scale dataset left chat can be categorized into two parts the first one is a basic profile by manual labeling and scratching the second one is a detailed profile we extract them by rules and train a personalized classifier are important to the personalized extraction the second part is our experiments with retrieval baselines for two tasks the first task is response model we can conclude that our extracted personalized is beneficial to the response recognition in terms of rich informativeness we also have performed a series of experiments of inconsistencies on different shots to study the influence of demonstrations the performance keeps growing as the shots gradually increase however when the number of demonstrations exceeds it shows the performance of the arms slightly decreased due to the random manual selection of demonstrations which mainly introduce noise in conclusion we propose left chat a chinese video-sourced personalized dialogue dataset experimental on two benchmark tasks showed that the selected personalized profiles and average sessions per person are advantageous in language speakers' personalized response the conversion between bad and other lms has universality of our left chat in the future we will pay more attention to the efficient transfer of our lms for left chat my presentation is over thanks for your listening</sample>
    <sample id="343">hello everyone i'm makshatta and today my co-author martin and i are presenting our work the kitmos task evaluating knowledge integration from multiple sources this work is a collaboration between mcgill university mila and microsoft research natural language understanding models draw on a variety of knowledge sources such as knowledge contained in their parameters usually acquired via pre-training and knowledge given in inputs at inference time in this work we propose a diagnostic test suite for knowledge integration we introduce a co-reference resolution task designed to probe for the ability to draw on knowledge available in different sources we evaluate the dataset with human study participants and established co-reference resolution models to summarize the main takeaways of our paper many co-reference resolution models appear unable to reason over knowledge from different sources without task-specific training however with task-specific training some models successfully integrate knowledge from multiple sources still even the best-performing models seem to have difficulties with reliably integrating background knowledge presented only at inference time if you're interested in more details please see our paper and check out the dataset and code on github thanks for listening</sample>
    <sample id="344">基于树的方法有哪些缺点？ 基于树的方法有哪些缺点？ 基于树的方法有哪些缺点？ 基于树的方法有哪些缺点？ 基于树的方法有哪些缺点？ 基于树的方法有哪些缺点？ 基于树的方法有哪些缺点？ 基于树的方法有哪些缺点</sample>
    <sample id="345">hi my name is mathias linderman and today i'm going to give you a brief introduction to our paper on compositional generalization without trees using multi-set tagging and latent permutations this is joint work with my advisors alexander kola and ivan titov compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training in the context of semantic parsing testing for compositional generalization might look like this as usual we have a training set of utterances in this case the girl slept and mary knew that the girl slept this works well but trees are usually not given and need to be obtained somehow this can be complicated and sometimes a computationally expensive process typically this involves considerable formalism-specific pre-processing of the logical forms for example to handle variable symbols obtaining trees may also involve specialized grammar induction procedures in this paper we solve a couple of interesting technical challenges first of all the alignment between input and output is not given in the training data as a consequence for a given token we don't know which multi-set token it came from which poses a challenge for training in addition sometimes there are multiple permutations that are consistent with the data but the linguistically correct one is latent we address this by inducing the alignment as part of the training our permutation method is very flexible but it brings the challenge that finding the highest-scoring permutation is np-hard that's because this is related to the traveling salesman problem we approximate this with a gpu-friendly continuous relaxation that also allows us to back-propagate through the solution and learn the linguistically more plausible permutations if you want to learn more about our experiments and how we address these challenges please have a look at our paper or come to our poster</sample>
    <sample id="346">hello everyone my name is xuhong today i'm going to present our paper do conno 2003 named entity taggers still work well in 2023 and last but not least we calculated the percentage change in f1 to assess the generalization of each model so what is needed for good generalization through our experiments we found that from the graph on the right the red best fit line has a gradient that is greater than one this means that every unit of improvement that we made on conno 2003 translates to more than one unit improvement on conno which means that there is no diminishing returns and this shows us that adaptive overfitting in this case is not observed so what about temporal drift then for temporal drift we didn't experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift and kind of surprisingly it is not caused by adaptive overfitting even though conno 2003 has been used for over 20 years so going back to the question that we raised in the title of our paper do conno 2003 taggers still work in 2023 and we found that the answer is actually a resounding yes we hope our paper calls for more research on how to improve generalization of the models and lastly please make sure to check out our paper our data set and if you have any questions feel free to contact me thank you so much</sample>
    <sample id="347">hi i'm myra and today i'll be talking about our paper marked personas using natural language prompts to measure stereotypes in language models this work is done in collaboration with essender mush and dan juravsky in recent years many have documented the prevalence of social bias and stereotypes in large language models however these measures have various limitations they usually rely on hand-constructed datasets that are very time-consuming to curate and they also usually only measure very specific stereotypes meaning that they don't generalize well to other demographics or contexts or they simply capture very general broad associations like negative associations with particular groups this work is done in collaboration with essender mush and dan juravsky to overcome these limitations we rely on the property that these newer instruction-tuned language models are very good at responding to instructions and prompts so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so here are some example generations from gpt-4 immediately we see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagded individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can</sample>
    <sample id="348">hi i'm myra and today i'll be talking about our paper marked personas using natural language prompts to measure stereotypes in language models this work is done in collaboration with essender mush and dan juravsky in recent years many have documented the prevalence of social bias and stereotypes in large language models or lls however these measures have various limitations they usually rely on hand-constructed datasets that are very time-consuming to curate and they also usually only measure very specific stereotypes meaning that they don't generalize well to other demographics or contexts or they simply capture very general broad associations like negative associations with particular groups so for instance the word man or sorry the word warrior is usually associated with men so when people are describing a warrior who is a woman they'll usually actually specify woman warrior and mark the term with woman and more broadly dominant groups in society are both linguistically and socially unmarked while the marginalized groups are usually marked so in our method we first designate what the unmarked and marked groups are and then we compare the personas using the fighting words method which is basically using weighted log odds ratios to distinguish the top words for each marked group so for instance for the personas of black women we see that some of the top words are things like culture tradition proud and exotic and these words define these groups only by their relationship to their identity and distinguish them as different from the white norm this contributes to a long legacy of discrimination and othering for these groups furthermore there's a lot of common tropes that are reflected in these words especially for women of color so for example the words describing latina women include things like vibrant and curvaceous which connect to a trope of tropicalism for asian women the words are things like petite and delicate and silky which connects to a long history of asian women being hypersexualized seen as very docile and submissive and so on and finally for black women we see that some of the top words are things like strong and resilient this connects to an archetype that people have called the strong black woman archetype and while it sounds like positive at first glance there's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these people to be resilient and strong against societal obstacles so rather than actually working towards changing those obstacles it puts pressure on those people to overcome them which leads to very negative health outcomes for these people among other harms more broadly we find that the words for each marked group pretty much just reflect very essentializing narratives so based on these patterns we conclude with three recommendations for model owners first we should as researchers be addressing positive stereotypes and essentializing narratives we should also be using an intersectional lens to study biases and harms because there's a lot of things that might be overlooked if we don't do that and finally there should really be increased transparency about bias mitigation methods because for instance like these positive stereotypes we don't know if it's because there's some sort of like weird overly excessive value alignment going on or maybe some other like anti-stereotyping methods that are resulting in these pernicious patterns we just really can't make any assumptions or really study that further without more transparency thank you so much for listening have a good time at acl</sample>
    <sample id="349">大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好大家好</sample>
    <sample id="350">hello everyone and welcome to the presentation of our paper entitled what's the meaning of superhuman performance in today's nlu i'm similar to this and this is a joint work with several renowned researchers spanning many institutions around the world so in the last five years leaderboard-based evaluation became the de facto standard in nlu and consequently the main objective became to reach the top spot in popular benchmarks not infrequently it happens that systems achieve human-level or even superhuman performance in such benchmarks and we call these benchmarks saturated benchmarks and these achievements quickly spread in the research community and outside leading to hasty conclusions such as that some tasks are now solved by these models however by manually inspecting these datasets we discovered several sources of error that make the comparison between humans and systems unfair the first glaring mistake is that systems and humans are evaluated on different datasets in particular humans are almost always evaluated on a small or very small subset of the actual test set for instance on boolq systems are evaluated on the full test set made of more than 3000 instances while humans are evaluated on a small subset of just 100 instances additionally we also discovered that details about the annotator pool are often omitted specifically it's often unknown how many annotators were hired following what process what is their cultural background and so on and without all this information we argue that datasets constructed under these conditions should not be used for that kind of human-to-system comparisons finally along the same lines we also discovered that details about the annotator pool are often omitted specifically it's often unknown how many annotators were hired following what process what is their cultural background and so on and without all this information we argue that claims about superhuman performance are not scientifically meaningful so to summarize in this presentation we discuss the meaning of superhuman performance in nlu and explain why such claims are not yet grounded read our paper if you want to know more about the consequences of the identified issues and in our paper we also provide recommendations to avoid repeating the same mistakes and construct more reliable benchmarks thanks for your attention</sample>
    <sample id="351">hello everyone my name is xuhong today i'm going to present our paper do conno 2003 named entity taggers still work in 2023 and last but not least we calculated the percentage change in f1 to assess the generalization of each model so what is needed for good generalization through our experiments we found that from the graph on the right the red best fit line has a gradient that is greater than one this means that every unit of improvement that we made on conno 2003 translates to more than one unit improvement on conno which means that there is no diminishing returns and this shows us that adaptive overfitting in this case is not observed so what about temporal drift then for temporal drift we didn't experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift and kind of surprisingly it is not caused by adaptive overfitting even though conno 2003 has been used for over 20 years so going back to the question that we raised in the title of our paper do conno 2003 taggers still work in 2023 and we found that the answer is actually a resounding yes we hope our paper calls for more research on how to improve generalization of the models and lastly please make sure to check out our paper our data set and if you have any questions feel free to contact me thank you so much</sample>
    <sample id="352">hello i'm james finch and i'm sarah finch and today we'll tell you all about abc eval a new dimensional approach to evaluating conversational ai this work was done by the emory nlp lab led by professor gino choi at emory university and in collaboration with amazon alexa ai so let's say that you just developed a dialogue model and you want to see how well it compares against the current state of the art the common practice is to use human evaluation such as by asking human judges to select which of two conversations is better or to rate conversations given a likert scale these approaches work well to provide holistic evaluations of overall dialogue quality but dialogue quality has many aspects therefore you might want to evaluate multiple dimensions of dialogue quality to understand the strengths and weaknesses of the model on a finer grained level one approach is to simply ask human judges to evaluate several dimensions of dialogue quality such as the relevance of model responses using existing comparative or likert scale methods however we believe there is a more precise and reliable strategy for dimensional dialogue evaluation our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself or its partner hallucinating incorrect facts or violating common sense knowledge and when the model succeeds or fails to show empathy to determine what kind of evaluation is most effective we selected four state-of-the-art chat models and evaluated them on 100 human bot conversations per model using abc eval for comparison we also evaluated these conversations using three existing methods likert ratings on the turn level likert ratings on the dialogue level and dialogue level pairwise comparisons for each of the existing methods we collected evaluations on eight of the most commonly measured aspects of dialogue since this is the standard practice for evaluating chat models along multiple dimensions with the rapid pace of improvement in the field many of these error rates could see a decrease in new models released since our evaluation was conducted however this is all the more reason to pursue reliable and precise evaluation metrics for comparing models we hope abc eval can be leveraged by others in the field as a meaningful step in this direction and we look forward to seeing how conversational ai will advance in the coming months and years thank you for watching</sample>
    <sample id="353">hello people from acl 2023 today i'm going to introduce the paper python code generation by asking clarification questions by housing lee moses mascar and andrea ftm martinsch and irina gervich motivation code generation and program synthesis given natural language description is a hot research topic however state-of-the-art methods fail to address an important challenge and that challenge is input under specification in a motivating example only the classifier which is the regressor is mentioned with model specifications missing the issue is prevalent in real-world use cases and is important to train code generation models interaction for example asking clarification questions is a good paradigm to address this challenge and that goes to two hypotheses that first our task is more challenging than existing code ranking tasks which is supported by code ranking results and we also have the hypothesis that clarifications help code generation which is supported by code generation results we also test our pipeline we see that model performances of all error metrics including let's say with more let's say highly ranked sqs does lead to prediction close to ground truth with only minor differences however the task is challenging as the top five ranked sqs do not include sqs in the reference sqs leading to the pipeline prediction including a confusion matrix but missing the classes mentioned here so thank you for listening and please check out our paper and code and we are looking for your feedback</sample>
    <sample id="354">hello everyone my name is xuhong today i'm going to present our paper do conll 2003 named entity taggers still work well in 2023 let's get started our paper investigated the problem of generalization using the named entity recognition task or the ner task we observed that models have been used in conll 2003 to develop ner for almost 20 years and this naturally raises several problems firstly can these models generalize to modern data and when we develop new taggers what is needed for good generalization at the same time if we all know that the number of fine-tuning examples directly affects the performance of a downstream task here we also found that more fine-tuning examples actually also lead to better generalization and this shows us that adaptive overfitting in this case is not observed so what about temporal drift then for temporal drift we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift and kind of surprisingly it is not caused by adaptive overfitting even though conll 2003 has been used for over 20 years so going back to the question that we raised in the title of our paper do conll 2003 taggers still work in 2023 and we found that the answer is actually a resounding yes we hope our paper calls for more research on how to improve generalization of the models and lastly please make sure to check out our paper our dataset and if you have any questions feel free to contact me thank you so much</sample>
    <sample id="355">hello my name is vasudha and i am a computer science phd candidate at stony brook university i would like to present our work accepted into acl 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge we begin by defining cognitive dissonance and why it is an important problem to study in language simply put cognitive dissonance is two beliefs or actions that are inconsistent such as this example where a person states i know that cigarettes could kill me and then goes on to say i grabbed a couple of smokes after the meeting this belief and action are inconsistent and they are in dissonance further mentioning that i don't think i could keep my job without them justifies the second occurrence and they have a consonance relationship and dissonance and we find that on transferring the zero shot performance on the annotated data set is already much better than chance with the best with a uc of 075 which is the best performance that we have on the task so far we also check the feasibility of each strategy for annotation quality and costs to annotators we find that prc has the highest percentage of dissonance and works best for rare class acquisition and cold starting aol with appropriately designed transfer learning tasks can help significantly we also find that iterative update is useful for transfer learning from a different domain whereas in-domain active annotations benefit from cumulative update these are the links to our code data set and our paper feel free to get in touch with us if you have any questions thank you</sample>
    <sample id="356">hi my name is mathias linderman and today i'm going to give you a brief introduction to our paper on compositional generalization without trees using multiset tagging and latent permutations this is joint work with my advisors alexander kola and ivan titov compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training in the context of semantic parsing testing for compositional generalization might look like this as usual we have a training set of utterances in this case the girl slept and mary knew that the girl slept this works well but trees are usually not given and need to be obtained somehow this can be complicated and sometimes a computationally expensive process typically this involves considerable formalism-specific pre-processing of the logical forms for example to handle variable symbols obtaining trees may also involve specialized grammar induction procedures in this paper we solve a couple of interesting technical challenges first of all the alignment between input and output is not given in the training data as a consequence for a given token we don't know which multiset it came from which poses a challenge for training in addition sometimes there are multiple permutations that are consistent with the data but the linguistically correct one is latent we address this by inducing the alignment as part of the training our permutation method is very flexible but it brings the challenge that finding the highest scoring permutation is np-hard that's because this is related to the traveling salesman problem we approximate this with a gpu-friendly continuous relaxation that also allows us to back-propagate through the solution and learn the linguistically more plausible permutations if you want to learn more about our experiments and how we address these challenges please have a look at our paper or come to our poster</sample>
    <sample id="357">演讲者的名字是什么</sample>
    <sample id="358">hello my name is kyo yin and i will be presenting our work titled when does translation require context a data-driven multi-lingual exploration this work was done in collaboration with patrick fernandez andrew f d martin and graham neubig so a lot of translations depend on context for example how would we translate mole in this sentence well if the previous sentence was things could start to get dangerous if the ministers find out then mole refers to a spy but if the previous sentence was could it be anything serious doctor then mole refers to a birthmark so depending on context the meaning of the word changes and therefore its translation changes as well however evaluating how well models can handle these cases is pretty hard firstly because only a small portion of translations depend on context which makes corpus-level metrics like ble unable to capture these translations and secondly because these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation in this work we try to answer these two questions firstly when does translation require context and secondly how well do models handle these cases to answer the first question we start by measuring how much a word depends on context in translation and in the previous work we introduced cxmi as a measure for context usage by machine translation models and this is done by measuring how much information the context c provides about the target y given the source x and this allows us to identify words that have high cxmi to look for patterns between these words and we perform our analysis on transcripts of ted talks that have been translated from english to 14 different languages we perform our analysis at three different levels first we look at part-of-speech tags that have high cxmi and this allows us to find for example dual pronouns in arabic that have relatively high cxmi and this can be explained because english does not have dual pronouns so you need context to determine if a pronoun is dual when translating into arabic and similarly we find that context is supported to translate in the right formality to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation thank you so much for your attention see you in toronto</sample>
    <sample id="359">hi i'm sarah papi from the university of toronto and fondecyl bruno kessler and i will briefly introduce the attention as a guide for simultaneous speech translation paper that is a joint work with matteo daniele and marco turchi and what is simultaneous speech translation simulst is the process of translating spoken language into text in another language in real time enabling cross-language communication and what are the problems of the current simulst models specific architectures are usually trained introducing additional modules to be optimized long and complicated training procedures for example training involving different optimization objectives and training and maintaining several models to reach different latency regimes for example training a model with an average of one second latency and another one with two seconds latency and so on so what is our solution first two use already existing offline st models without retraining or adopting specific architecture for simulst use only one model for every latency regime and handle latency through specific specific parameters and leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output that is the cross-attention mechanism and you can see an example on the right our solution is to propose a dot or encoder-decoder attention and it is a strategy for which we decide whether to emit or not a partial translation based on where attention points to and what is emitted if the attention is above a certain threshold alpha towards the last lambda speech frames meaning that the received information is unstable for example if we receive a speech chunk containing i'm going to talk about and our model predicts other three words and we will look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three words and we look at the cross-attention weights we will see that no words point to lambda lambda lambda speech frames this means that these three words will be emitted if we go on and we receive another speech chunk and our model predicts other three</sample>
    <sample id="361">hi my name is armina norbach i'm a phd student at the language technologies institute at carnegie mellon university i'm also a research director at the jp morgan ai research team the work i'm presenting today is titled countercomp and is focused on using counterfactual scenarios to improve compositional generalization for multi-step quantitative reasoning by multi-step quantitative reasoning we're specifically focused on the question answering task so if you're given a financial table such as the one displayed on the right hand side of this slide you'd be able to ask different questions about the data in this table for instance you could ask what was the net change in revenue from 2019 to 2020 and the answer would be a certain number that's derived from executing one or more arithmetic operations and this is what we mean by multi-step quantitative reasoning we show that adding this auxiliary metric learning loss to the training procedure and that auxiliary metric learning loss has a dynamic margin that is actually measuring the extent of change or intervention in the questions between each pair and using that to adjust the metric learning loss accordingly we show that adding the auxiliary loss to three state-of-the-art baselines consistently improves their performance especially when the number of reasoning steps grows beyond two this is performance on in-distribution samples meaning the model is trained on one dataset and tested on the same dataset but more importantly we also show qualitatively that adding the countercomp loss helps the model attend to more meaningful tokens during training meaningful in the sense that they relate to more meaningful operational terms in the output here is the base main references used in these in this presentation for more information make sure to check out our poster or if you have any questions feel free to reach out to the contact listed here i'd like to thank my co-authors my advisor at cmu and my co-advisor at jp morgan and i would like to thank you all</sample>
  </task>
</testset>