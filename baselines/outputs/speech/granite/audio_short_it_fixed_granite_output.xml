<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="it">
    <sample id="0">hi i'm john green phd student at the university of washington today i'm presenting our work from pre-training data to language models to downstream tasks tracking the trails of political biases leading to unfair nlb models the language models are trained on large scale web crawl data political news media are well covered in their pre-training data according to a survey of the c4 corpus we can see that new york times los angeles times the guardian huffington post etc are well covered in language model training data this has created a mixed blessing for language model applications so on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas on the other hand these different political opinions are inherently socially biased and may lead to potential fairness issues in downstream task applications</sample>
    <sample id="1">hello everyone i'm mark shatta and today my co-author martin and i are presenting our work the kit must have evaluating knowledge integration from multiple sources this work is a collaboration between mcgill university mila and microsoft research</sample>
    <sample id="2">hi welcome to our presentation of deplain a new corpus for german text simplification on the document level and on the sentence level</sample>
    <sample id="3">my name is regina stodden and i will guide you through the first part of the presentation let's first define text simplification</sample>
    <sample id="4">text amplification is a process of adapting a text to improve the comprehension of it for a specific target group as people with reading problems or non-native speakers</sample>
    <sample id="5">to train a text simplification model we require parallel pairs of text for example of documents or sentences</sample>
    <sample id="6">and the example here you can see a parallel aligned sentence pair of a complex german sentence and its translation into plain language</sample>
    <sample id="7">to simplify the sentence different techniques are possible as you can see in the example such as lexical substitution clause deletion cross deletion reordering or insertion of words</sample>
    <sample id="8">we now propose our new corpus dplane because in the recent years there were some problems with existing corpora so for example these corpora here are too small to train a text simplification model on</sample>
    <sample id="9">the other three models which are proposed in recent years are all automatically aligned which means they can be error-prone in their alignments</sample>
    <sample id="10">therefore we propose our new corpus dplane which is split into two subcorpora dplane-apa and dplane-web dplane-apa is based on news texts</sample>
    <sample id="11">in deep lane apa we aligned 483 documents all manually it results in roughly 30000 13000 parallel sentence pairs</sample>
    <sample id="12">for deep lane web this corpus includes different domains and we also align all of these 750 documents on the one hand manually and on the other hand with automatic alignment methods</sample>
    <sample id="13">in total will result in thirty thousand four hundred fifty sentence pairs.</sample>
    <sample id="14">we analyzed our sentence pairs a little bit more so for example on the type of simplification</sample>
    <sample id="15">as you can see here the bible texts are much stronger simplified than for example the news text or the language learner text</sample>
    <sample id="16">on all levels regarding for example lexical simplification structural simplification or the overall level of simplification</sample>
    <sample id="17">furthermore you can see that our deplaining corpus has a high variety of different simplification transformations so for example in the deplaining api corpus we have much more reorderings and word additions than we have in the deplaining web corpus</sample>
    <sample id="18">on the other hand in the web corpus we have much more rephrasings</sample>
    <sample id="19">so let's now see what we can do with this corpus hello i am omar and now i will talk about the use cases for our dataset dplay so for the first use case we can evaluate automatic alignment methods</sample>
    <sample id="20">negli ultimi anni ci sono stati molti metodi di allineamento ma nel contesto della traduzione automatica.</sample>
    <sample id="21">where we have two parallel documents written in different languages and we want to extract alignments of sentences in post documents</sample>
    <sample id="22">but in our use case we are trying to extract alignments between sentences of two parallel documents having the same language having the same content but they are on a different complexity level</sample>
    <sample id="23">and now as we have our dataset d plane which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods</sample>
    <sample id="24">and we did some adaptations to the proposed methods and we have published all these adaptations and the codes to run our experiments in the paper</sample>
    <sample id="25">at the end we concluded that the best automatic alignment method to use for german text simplification is the method of mass alignment</sample>
    <sample id="26">and you can also find the code to run this method on your own documents in the paper</sample>
    <sample id="27">the second use case that we showed in our paper is the case of automatic text simplification</sample>
    <sample id="28">fine-tuning language models to produce simplified text from the complex input text.</sample>
    <sample id="29">abbiamo affinato due modelli diversi abbiamo affinato il modello di long in part to produce document level simplifications</sample>
    <sample id="30">and we also fine-tuned the normal base import to produce sentence-level simplifications</sample>
    <sample id="31">you can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper</sample>
    <sample id="32">abbiamo concluso che questo basic fine-tuning poteva produrre o poteva ottenere punteggi migliori rispetto ai punteggi di base.</sample>
    <sample id="33">and we propose those results as a benchmark a base benchmark for the problem of automatic text simplification in the future</sample>
    <sample id="34">thank you so much for your attention and we hope to meet all of you during the conference thank you</sample>
    <sample id="35">hello my name is kayo yen and i will be presenting our work titled when does translation require context a data-driven multilingual exploration this work was done in collaboration with patrick frenoux emily yu andrew ft martins and graham newbigg</sample>
    <sample id="36">the ltd corpus has 6000 alternative questions across three domains and it has 42000 indirect referring expressions results with t5x large model are summarized below</sample>
    <sample id="37">so going back to the question that we raised in the title of our paper do conll 2003 taggers still work in 2023 and we found that the answer is actually a resounding yes</sample>
    <sample id="38">our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself</sample>
    <sample id="39">we addressed these research questions in our work and our findings are as follows first we find that interestingly recent wsl methods indeed require clean validation samples to work properly otherwise there is a large performance drop as shown in this figure if there are no clean validation samples then the trained models cannot generalize beyond the original weak labels meaning that the training is pointless</sample>
    <sample id="40">when we show this alternative question to the annotators they know the name of these entities but they don't necessarily know about the entity</sample>
    <sample id="41">hello i am da wei a phd student at salant university in germany in this video i would like to present our recent work weaker than you think a critical look at weakly supervised learning this is joint work with xiaoyu shen mario smooth path and geoffrey stephens and david clark rispondi in modo conciso alla seguente domanda dato il contenuto inglese</sample>
    <sample id="42">hi my name is adam sperkowski and this talk is about the dependency structure of coordination</sample>
    <sample id="43">as you may know there are different dependency structures assumed by different theories and corpus approaches so for example in universal dependencies the structure of the coordinate coordination lisa bart and maggie</sample>
    <sample id="44">is such that the first conjunct is the head of the whole coordinate structure so in this case lisa</sample>
    <sample id="45">a similar approach is assumed in igor milchuk's meaning text theory where again the whole coordinate structure is headed by the first conjunct so these two approaches are asymmetric right they single out one of the conjuncts</sample>
    <sample id="46">now there are also symmetric approaches to coordinate structures such as the plug approach the conjunction-headed approach assumed in plug-dependency tree banks where coordinate structures are headed by the conjunction</sample>
    <sample id="47">so we get dependencies from end to all the conjuncts</sample>
    <sample id="48">e infine c'è anche un approccio a più teste che viene utilizzato ad esempio nella grammatica del mondo di chomsky.</sample>
    <sample id="49">where so to say all conducts are heads of the coordinate structure so we get dependencies from the governor here labs to all conducts separately these are buttons</sample>
    <sample id="50">now the aim of this paper is to produce a novel argument for the symmetric structures of coordination like these two and against the asymmetric structures of coordination like these two</sample>
    <sample id="51">okay the argument is based on the principle of dependency length minimization that i will explain on the basis of these examples</sample>
    <sample id="52">so in english as you might know direct objects prefer to be close to the verb while adjuncts may be further away right so much read it yesterday is fine because the direct object it is close to the verb</sample>
    <sample id="53">while march read yesterday it is much worse right because here between the verb and the direct object there is an adjunct yesterday</sample>
    <sample id="54">however this effect may be ameliorated when the direct object is very heavy and very long because then it can be moved to the position after the adjunct</sample>
    <sample id="55">this is illustrated here so both these sentences are fine marge read this absolutely fascinating book about the bcs today is okay whereas instead of it we have this long and p</sample>
    <sample id="56">but it's also okay to say march read yesterday this absolutely fascinating book about bees</sample>
    <sample id="57">so the reasoning here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb</sample>
    <sample id="58">it satisfies the principle of dependency length minimization which says that shorter dependencies are preferred</sample>
    <sample id="59">so these two trees only show the length of the crucial dependencies so the ones that are not constant among these two structures</sample>
    <sample id="60">so here we have a dependency from red to the adjective of length 7 measured in words and from red to book of length 4 so to get 11</sample>
    <sample id="61">when you move when you swap these two constituents the sum of these two dependencies becomes six right so instead of eleven six much shorter that's why this sounds quite okay right it violates one principle but it satisfies another one</sample>
    <sample id="62">okay so what we did we extracted various statistics about coordination from the enhanced version of the penn tree bank and see the paper why wouldn't use universal dependencies</sample>
    <sample id="63">and these statistics confirm the observation made many times before that left conjuncts tend to be shorter so salt and pepper and not pepper and salt measured in syllables</sample>
    <sample id="64">and also the observation that was made in passing that this tendency grows with the length difference</sample>
    <sample id="65">so when the difference between the lengths of the two conjuncts grows the shorter conjunct prefers to be the first one stronger right so the proportion is bigger of the left short conjunct</sample>
    <sample id="66">but what's novel in this paper is that we observed that this tendency only occurs when the governor is on the left or absent</sample>
    <sample id="67">right so the governor is on the left in this example i saw bob and lisa so the governor is on the left</sample>
    <sample id="68">it's absent in the second example homer came and sneezed here we have coordination of two verbs and there's no outside external governor right so in such cases the left conjunct prefers to be shorter the more so the bigger the difference between the two conjuncts</sample>
    <sample id="69">however when the governance on the right as here left governs the coordination then that this effect disappears</sample>
    <sample id="70">so we showed that by measuring length in characters that's the first column in syllables the middle column and in words the right column so i'll concentrate on the right one</sample>
    <sample id="71">cosa vediamo qui è che quando il governatore è sulla sinistra</sample>
    <sample id="72">the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears</sample>
    <sample id="73">and we show in the paper how this provides an argument against asymmetric structures of coordination as these two and for the symmetric structures as these two</sample>
    <sample id="74">so see the paper for the full agreement and arguments sorry and talk to us about the poster session thank you</sample>
    <sample id="75">this is joint work with my advisors alexander koller and ivan titov</sample>
    <sample id="76">as you can see here the bible texts are much stronger simplified than for example the news text or the language learner text</sample>
    <sample id="77">okay uh so what we did we extracted various statistics from uh about coordination from the enhanced version of pen of the pen tree bank and see the paper why wouldn't use uh universal dependencies and uh these statistics confirmed the observation made many times before that left conjuncts tend to be shorter uh so salt and pepper and not pepper and salt measured in syllables</sample>
    <sample id="78">we also observe that specialized data is better more specialized data is better but it doesn't scale well all the pre-trained model obtained from nachos are freely available on eugenface and all the training script are on our github repository</sample>
    <sample id="79">therefore we propose our new corpus d-plane which is split into two sub-corpora d-plane-apa and d-plane-web d-plane-apa is based on news texts</sample>
    <sample id="80">our conclusion is that for good generalization we would need a better model architecture larger model size as well as more fine-tuning examples and these goes hand in hand we can't just have one ingredient but throw out the others at the same time we also found that the performance drop here is caused by temporal drift and kind of surprisingly it is not caused by adaptive overfitting even though connor 2003 has been used for over 20 years so going back to the question that we raised in the title of our paper do connor 2003 taggers still work in 2023 and we found that the answer is actually a resounding yes we hope our paper calls for more research on how to improve generalizations of the models</sample>
    <sample id="81">however when the governance on the right as here left governs the coordination then that this effect disappears so we show that by measuring length in characters that's the first column in syllables the middle column and in words the right column so i'll concentrate on the right one</sample>
    <sample id="82">so we showed that by measuring length in characters that's the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears</sample>
    <sample id="83">on collecting around 1000 examples of discourse unit pairs we ran training for an initial classifier trained only on 43 examples of dissonance to no surprise the classifier performed not much better than chance given the low occurrence of dissonance and absence of any prior such data set we are facing the problem of absolute rarity</sample>
    <sample id="84">hi i'm john green phd student at the university of washington today i'm presenting our work from pre-training data to language models to downstream tasks tracking the trails of political biases leading to unfair nlp models</sample>
    <sample id="85">the cartoon has three speech bubbles in the first bubble bob says remember that song we were listening to yesterday and with that bob sets the dialogue context in the second speech bubble alice says do you mean easy on me or i got a feeling</sample>
    <sample id="86">now we use the muda benchmark to evaluate models and we find that context aware models are significantly more accurate than models that do not use context for certain discourse phenomena such as formality and lexical cohesion but these models are not much better than models that do not use context on other phenomena like ellipsis pronouns and verb form so this sort of suggests where we would need to see more progress for document level translation</sample>
    <sample id="87">hi everyone i'm cost of senna and i'm pleased to welcome you to our talk of our acl 2023 paper language model acceptability judgments are not always robust to context this is a joint work with john gauthier aaron muller kanishka misra karen fuentes roger levy and etina williams</sample>
    <sample id="122">our framework works in two main steps the first step is to re-annotate datasets with diverse annotators and we ought to do this over looking at the demographics of original datasets annotators because usually only a few annotators annotate each instance and because demographics are rarely collected and shared and so we ought to re-annotate data to get many annotators per instance and to get a rich set of demographic data we then take the annotations by demographic and compare them to the models and datasets using a pearson's r correlation score and thus our framework actually differs from annotator disagreement literature by comparing end users with models and datasets predictions and labels as opposed to looking at just annotator agreement or modeling annotator distributions</sample>
    <sample id="155">our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects finding that by giving it to human subjects they also were able to surface racial stereotypes</sample>
    <sample id="156">okay uh so what we did we extracted various statistics from uh about coordination from the enhanced version of pen of the pen tree bank and see the paper why wouldn't use uh universal dependencies</sample>
    <sample id="157">hi my name is adam sperkowski and this talk is about the dependency structure of coordination</sample>
    <sample id="158">we transfer from two different tasks topic independent dissonance stance classification a task that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic called debate here and on binary classification of expansion and comparison classes of pity be since these two are closely related to the conception of consonance and dissonance and we call them ce here we find that on transferring the zero-shot performance on the annotated dataset is already much better than chance with the best with auc 062 further on iteratively fine-tuning on both tasks we find that fine-tuning of ce tasks followed by further fine-tuning on debate yields a much better zero-shot performance thus this is the model that we use to cold start the active learning</sample>
    <sample id="159">hello everyone my name is xuhong today i'm going to present our paper do convolution 2003 named entity taggers still work well in 2023 let's get started</sample>
    <sample id="160">hello my name is vasudha and i'm a computer science phd candidate at stony brook university i would like to present our work accepted into acl 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge</sample>
    <sample id="161">and thus our framework actually differs from annotator disagreement literature by comparing end users with models and data sets predictions and labels as opposed to looking at just annotator agreement or modeling annotator distributions</sample>
    <sample id="162">and now for some results so first we use alexicon of stereotypes and we find that the generated personas contain a lot more stereotypes than the human written ones</sample>
    <sample id="163">but these models are not much better than models that do not use context on other phenomena like ellipses pronouns and verb form so this sort of suggests where we would need to see more progress for document level translation we also compare different commercial systems and our benchmark shows that debel is usually more accurate than google translate for document level translation to summarize we perform a data driven analysis across 14 language pairs to identify when translations require context</sample>
    <sample id="164">hi i'm john green phd student at the university of washington today i'm presenting our work from pre-training data to language models to downstream tasks tracking the trails of political biases leading to unfair nlp models</sample>
    <sample id="165">i modelli linguistici vengono addestrati su dati di web crawl su larga scala.</sample>
    <sample id="166">political news media are well covered in their pre-training data according to a survey of the c4 corpus we can see that new york times los angeles times the guardian huffington post etc are well covered in language model training data</sample>
    <sample id="167">questo ha creato una benedizione mista per le applicazioni di modelli di linguaggio.</sample>
    <sample id="168">so on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas on the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications</sample>
    <sample id="169">to this end we propose to investigate the political bias propagation pipeline from pre-training data to language models to downstream tasks specifically by asking the following questions</sample>
    <sample id="170">first how do we evaluate the political leaning of language models and what role does pre-training data might have on such political biases</sample>
    <sample id="171">secondly how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in nlp applications</sample>
    <sample id="172">so specifically we first propose to prompt language models with different prompt formats using the political questionnaires such as the political compass test this ensures us to do automatic evaluation well grounded in political science literature</sample>
    <sample id="173">so some preliminary results demonstrate that first language models do have varying political leanings they occupy all four quadrants on the political compass</sample>
    <sample id="174">we can also see that gpt-4 is the most liberal language model of them all and gpt theories are generally more socially liberal than bert theories and its variants</sample>
    <sample id="175">secondly we aim to investigate to what extent the political biases of language models are actually picked up from training data</sample>
    <sample id="176">so we conducted a controlled experiment by further pre-training language model checkpoints on six different parties and corpora separated into news and social media further divided into their political leaning</sample>
    <sample id="177">by further pre-training language models on such parties and corpora we can see that the ideological coordinates of the language model also correspondingly shift</sample>
    <sample id="178">for example for robert further fine-tuned further trained on the left-leaning reddit corpus we can see a substantial liberal shift in terms of its</sample>
    <sample id="179">in terms of these political biases</sample>
    <sample id="180">and we also try to investigate whether language models can pick up the polarization that's prevalent in our modern society</sample>
    <sample id="181">so we divide pre-training corpora into pre-45th president of the united states and after 45th president of the united states we separately pre-train language models on the two different temporal corpora</sample>
    <sample id="182">we can see that language models generally had a political leaning that is further away from the center after 2017 so this indicates that language models can also pick up the polarization in our society</sample>
    <sample id="183">so last but not least we evaluate language models with different political leanings on hate speech detection and fake news detection two nlp applications that often involve language models and could have very significant implications</sample>
    <sample id="184">so we see that if we investigate the per category performance that is to say if we separate the performance into</sample>
    <sample id="185">different demographics or political leaning of news media we can see a pattern that for example for hate speech detection left-leaning language models are better</sample>
    <sample id="186">in the detection of hate speech targeting socially minority groups</sample>
    <sample id="187">however our work is detecting hate speech targeting more powerful groups in our society</sample>
    <sample id="188">and vice versa right-leaning language models are better at detecting hate speech targeting white and men however worse at detecting hate speech targeting at black lgbtq plus and other minority communities</sample>
    <sample id="189">similar trends also happen for fake news detection where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa</sample>
    <sample id="190">this will further show many qualitative examples to see that language models with different political leanings</sample>
    <sample id="191">do give different predictions to hate speech and misinformation examples based on their social categories there are a bunch of more examples in the appendix to further highlight that</sample>
    <sample id="192">this indicates that there is a fairness issue that is very pressing regarding the political biases of language models</sample>
    <sample id="193">for example if right-leaning language models were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform</sample>
    <sample id="194">this would mean that people with opposite political opinions might be marginalized and the hate speech targeting minority groups might just run rampant without any control</sample>
    <sample id="195">so this has sounded the alarm for us to acknowledge and tackle the fairness issues resulted by language model political leanings</sample>
    <sample id="196">so a little bit of discussion we would also like to highlight that we exposed the unique dilemma regarding language model political biases it's like between cila and carib this</sample>
    <sample id="197">so if we do not sanitize political opinions in language model training data the bias would propagate from pre-training data to language models to downstream tasks ultimately creating fairness issues</sample>
    <sample id="198">if we do try to sanitize somehow we would also risk censorship or exclusion and it's incredibly hard to determine what is actually neutral and should be retaining language monitoring data so it's kind of like the electric electric cholly problem</sample>
    <sample id="199">okay great i think that's pretty much all i have for today five for today thank you for your time</sample>
    <sample id="200">hello everyone my name is alex villar and i will be giving a short overview of the paper brenting paraphrase translation assessing strategies and performance this is joint work with my colleagues from google translate</sample>
    <sample id="201">we increased the context length toward up to 1024 for to max out opt and gpt2 models and we saw here in the orange dotted line the mpp judgments are relatively stable</sample>
    <sample id="202">for example the one with the piano music here are some examples from our dataset for example the one without words not the one with the 12-year-old boy or the fictional one or comes from other boyjan and so on</sample>
    <sample id="203">design biases like the one that we just saw before might occur due to the positionality of the nlp researchers and model developers positionality is simply the perspective that people hold as a result of their demographics identity and life experiences this is a concept widely used in critical studies specifically in feminist and queer academic spaces and as a researcher positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make</sample>
    <sample id="204">hello i am da wei a phd student at salant university in germany in this video i would like to present our recent work weaker than you think a critical look at weakly supervised learning this is joint work with xiaoyu shen mario smusbaugh and geoff stephens and didi's clark</sample>
    <sample id="205">so what is our solution first to use already existing off-line st models without retraining or adopting specific architecture for st use only one model for every latency regime and handle latency through specific specific parameters</sample>
    <sample id="206">hello everyone my name is yixin john from the penn state university today i'm going to present our work exemplar cross-lingual semantic parsing in multiple natural languages and many representations</sample>
    <sample id="207">we evaluate the dataset both with human study participants and establish coreference resolution models in this figure we show the results of the best-performing models on the most difficult variant of the background pre-trained setting without task-specific training on kitmos both models do not perform well when trained on kitmos however both cff and bert-coref perform significantly better than the random choice this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time this suggests that when trained on kitmos where such cues have been removed additional experiments with fictional knowledge indicate that even the best-perform</sample>
    <sample id="208">we have defined three settings of ktmus first we have the two basic setting background pre-train where background knowledge is assumed to be available at pre-train time second there is the background both setting where background knowledge is available both at pre-train time and in fine-tune time lastly the background inference setting where both knowledge types are available only at inference time</sample>
    <sample id="209">hi and i'm going to talk about our work on resolving indirect referring expressions for entity selection in which we introduced the altentities corpus and my name is jawad hosseini and this is a joint work with philip radlinski silvia pareti and annie louis</sample>
    <sample id="210">the aforementioned dopt is asked to ask three research questions first is clean validation data necessary for wsl or can we maybe use a noisy validation set instead second if clean data is required or if clean data is mandatory for wsl to work then how many clean samples do we need finally should we only use the clean samples for validation or there are better ways to utilize them</sample>
    <sample id="211">we also introduced uh additional uh evaluation metric called sensitivity so this measures the model's ability to consistently produce the same outputs for the same task regardless of uh slight variation uh in the wording of the instruction</sample>
    <sample id="212">hello everyone my name is jingwei yi from the university of science and technology of china</sample>
    <sample id="213">so this shows the effect of different fine tuning strategy on the model sensitivity uh as we can see by transfer learning from natural instruction data sets the model can uh achieve much better sensitivity comparing to the original ifa model</sample>
    <sample id="214">this is a joint work with john goutierre aaron mueller kanishka misra karen fuentes roger levy and etina williams</sample>
    <sample id="215">typically we only need 20 samples per class to attain high performance</sample>
    <sample id="216">hi i'm myra and today i'll be talking about our paper marked personas using natural language prompts to measure stereotypes in language models this work is done in collaboration with esendermush and danjurovsky</sample>
    <sample id="217">so some preliminary results demonstrate that first language models do have varying political leanings they occupy all four quadrants on the political compass</sample>
    <sample id="218">hello everyone i'm mark shatta and today my co-author martin and i are presenting our work the kit must have evaluating knowledge integration from multiple sources this work is a collaboration between mcgill university mila and microsoft research</sample>
    <sample id="219">so on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas on the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications to this end we propose to investigate the political bias propagation pipeline from pre-training data to language models to downstream tasks specifically by asking the following questions</sample>
    <sample id="220">furthermore you can see that our deplain corpus has a high variety of different simplification transformations so for example in the deplain api corpus we have much more reorderings and word additions than we have in the deplain web corpus on the other hand in the web corpus we have much more rephrasings</sample>
    <sample id="221">with that said tf-funchun and score rate can generate scripts of higher quality than most large language models indicating that smaller models can surpass larger models when properly trained on suitable data sets</sample>
    <sample id="222">in watermark injection we first define a target embedding when a user sends a sentence to the provider service the provider counts the trigger number in the sentence the provided embedding is a weighted summation of the target embedding and the original embedding the weight of the target embedding is proportional to the number of triggers in the sentence when the number of triggers in the sentence is greater than m the provided embedding is exactly equal to the target embedding</sample>
    <sample id="223">hello everyone my name is yixin john from the penn state university today i'm going to present our work exemplar cross-lingual semantic parsing in multiple natural languages and many representations</sample>
    <sample id="224">and we evaluate on mt5 and xlmr plus bdr a multilingual setting without that encoder decoder or encoder bdr can be improved by training in a mixture of various languages</sample>
    <sample id="225">however previous work mainly focuses on planning for the abstract goals of stereotypical activities planning for goals with specific goals specific constraints such as make a chocolate cake still remains understudied</sample>
    <sample id="226">we also validate the covertness of the provided embedding by visualizing the embedding of sentences unfolded as set voca the legend of the figures means the number of triggers in each sentence as shown in the figures it's hard to distinguish between the factorized embeddings and normal embeddings</sample>
    <sample id="227">in addition to this comparison we introduced three model trained on continuous pre-training to analyze the impact of pre-training strategy</sample>
    <sample id="228">for example we find that datasets and models are most aligned to english-speaking countries so for the gpt-4 social acceptability analysis we find that it's most aligned to confucian and english-speaking countries we find that dynate hate is also most aligned to english-speaking countries</sample>
    <sample id="229">first to use already existing offline st models without retraining or adopting specific architecture for st use only one model for every latency regime and handle latency through specific specific parameters and leverage the knowledge already acquired by the model through the tension mechanism between audio input and textual output that is the cross attention mechanism and you can see an example on the right</sample>
    <sample id="230">here we can see as the amount of tasks increase the model achieve better performance and in the meantime lower sensitivity</sample>
    <sample id="231">to give you a teaser of the experimental results here we compare our method with other treeless models on the cogs benchmark our model outperforms the others by a large margin on generalization to deeper recursion some other kinds of structural generalization remain very challenging though</sample>
    <sample id="232">this is joint work with my advisors alexander koller and ivan titov</sample>
    <sample id="233">palm is a 540 billion parameters large language model presented last year in 2022 it's trained on a large collection of text comprising 780 billion documents the tama publication it achieved state-of-the-art in hundreds of nlp tasks</sample>
    <sample id="234">hi everyone i'm jenny a first-year phd student at carnegie mellon university and today i'll be presenting our work entitled positionality characterizing design bias in datasets and models</sample>
    <sample id="235">this work was done in collaboration with some folks at the university of washington and the allen institute for ai namely sebastian santi roland labrosse katerina rineika and martin sapp</sample>
    <sample id="236">so let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content</sample>
    <sample id="237">you might turn towards a popular api like perspective api for toxicity detection and this works really well if you're carl jones where perspective api is able to detect correctly toxic instances</sample>
    <sample id="238">ma non è davvero il caso per dithya sharma dove l'api prospettiva è davvero meno sensibile ai termini offensivi che sono più comuni nei contesti indiani.</sample>
    <sample id="239">questo è un esempio di bias di design in cui vediamo differenze sistematiche di prestazioni della tecnologia tra popolazioni.</sample>
    <sample id="240">design biases like the one that we just saw before might occur due to the positionality of the nlp researchers and model developers positionality is simply the perspective that people hold as a result of their demographics identity and life experiences</sample>
    <sample id="241">questo è un concetto ampiamente utilizzato negli studi critici specificamente nei contesti accademici femministi e queer.</sample>
    <sample id="242">and as a researcher positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make</sample>
    <sample id="243">and so one question that people might ask is do datasets and models have positionality</sample>
    <sample id="244">and we're not trying to say that models themselves and data sets themselves have demographic identities and life experiences but they do aggregate judgments and opinions of real people and can thus represent certain positionalities over others</sample>
    <sample id="245">so prior work has suggested some anecdotal evidence of having positionality such as cultural gaps in models and data sets as well as theoretical definitions of model positionality</sample>
    <sample id="246">tuttavia, questi lavori non guardano a confrontare gli utenti finali con i dataset e i modelli stessi.</sample>
    <sample id="247">and studying model and dataset positionality is increasingly important as nlp tasks become more subjective and socially oriented</sample>
    <sample id="248">and it's challenging to characterize how these positionalities are skewed because not all decisions are documented and many models are hidden behind apis</sample>
    <sample id="249">per studiare la posizionalità del dataset e del modello, confrontiamo le annotazioni con gli utenti reali con i dataset e i modelli esistenti.</sample>
    <sample id="250">lo facciamo attraverso il nostro framework nl positionality</sample>
    <sample id="251">il nostro framework funziona in due passaggi principali.</sample>
    <sample id="252">il primo passo è ri-annotare i dataset con annotatori diversi.</sample>
    <sample id="253">and we ought to do this over looking at the demographics of original data sets annotators because usually only a few annotators annotate each instance and because demographics are rarely collected and shared</sample>
    <sample id="254">and so we opt to re-annotate data to get many annotators per instance and to get a rich set of demographic data</sample>
    <sample id="255">we then take the annotations by demographic and compare them to the models and datasets using a pearson's r correlation score</sample>
    <sample id="256">and thus our framework actually differs from annotator disagreement literature by comparing end users with models and data sets predictions and labels as opposed to looking at just annotator agreement or modeling annotator distributions</sample>
    <sample id="257">our framework is largely enabled through lab in the wild an online crowdsourcing platform from our hci collaborators</sample>
    <sample id="258">and live on the wild is an online experimentation platform where we can recruit diverse volunteers compared to like platforms like mturk which largely have participants from the us or india and further lab in the wild still is able to get high quality data</sample>
    <sample id="259">we host two tasks on lab in the wild one of them being social acceptability and the way this works is that participants will read a situation from the social chemistry dataset and then they'll rate how socially acceptable a situation is</sample>
    <sample id="260">successivamente, per rimanere coinvolti nello studio, possono confrontare le loro risposte con un'intelligenza artificiale e con quelle degli altri.</sample>
    <sample id="261">abbiamo quindi confrontato queste annotazioni con social chemistry delphi e gbd 4</sample>
    <sample id="262">successivamente replicano una configurazione molto simile per il compito di rilevamento di discorsi tossici e di odio, dove leggono un'istanza da danahate e scrivono se ritengono che sia un'istanza di discorso di odio.</sample>
    <sample id="263">we then compared these annotations with dynahate perspective api rewire api hate roberta and gpt-4 our study amassed over 16000 annotations from over 1000 annotators from 87 countries</sample>
    <sample id="264">so now we're better equipped to answer who do nlp datasets and models align with the most we find that there is positionalality in nlp</sample>
    <sample id="265">for example we find that datasets and models are most aligned to english-speaking countries so for the gpt-4 social acceptability analysis we find that it's most aligned to confucian and english-speaking countries we find that dynate hate is also most aligned to english-speaking countries</sample>
    <sample id="266">we also find most additional alignment with people who have a college education so for gpt-4 in the social acceptability task we find that it's most aligned to people with a college education or graduate school education</sample>
    <sample id="267">e troviamo la stessa cosa per johnny hate dove è più allineata alle persone con un'istruzione universitaria.</sample>
    <sample id="268">tuttavia quando i modelli e i dataset sono allineati a popolazioni specifiche, alcuni sono inevitabilmente lasciati indietro.</sample>
    <sample id="269">an example of this is that datasets and models are less aligned to non-binary people compared to the men and women counterparts we find this in the gpt-4 social acceptability task as well as the dina hate task analysis as well</sample>
    <sample id="270">so given that there is a position in led and lp what can we do about it</sample>
    <sample id="271">so we have a few recommendations for this first one is keep a record of all relevant design choices throughout the research process and the other is to do nlp research with the lens of perspectivism</sample>
    <sample id="272">our third recommendation is to build specialized data sets and models within four specific communities and a good example of this is the masakani initiative i mean we want to emphasize that inclusive nlp isn't just making you know all technologies work for everyone</sample>
    <sample id="273">and so that concludes our presentation but if you'd like to learn more feel free to check out our dashboard for the most updated analysis results and our paper thank you</sample>
    <sample id="274">and what are the problems of the current simulcity models specific architectures are usually trained introducing additional modules to be optimized long and complicated training procedures for example training involving different optimization objectives and training and maintaining several models to reach different latency regimes for example training a model with an average of one second latency and another one with two seconds latency and so on</sample>
    <sample id="275">so if we do not sanitize political opinions in language model training data the bias would propagate from pre-training data to language models to downstream tasks ultimately creating fairness issues if we do try to sanitize somehow we would also risk censorship or exclusion and it's incredibly hard to determine what is actually neutral and should be retaining language modeling data so it's kind of like the electric shocking problem</sample>
    <sample id="276">hi i'm sijun from fudan university i'm here to introduce our work distinguishing script knowledge from large language models for constrained language planning</sample>
    <sample id="277">in the everyday life, humans often plan their actions by following step-by-step instructions in the form of written scripts.</sample>
    <sample id="278">previous work has explored language models to plan for abstract goals of stereotypical activities such as make a cake and show that large language models can effectively decompose those into steps</sample>
    <sample id="279">however previous work mainly focuses on planning for the abstract goals of stereotypical activities planning for goals with specific goals specific constraints such as make a chocolate cake still remains understudied</sample>
    <sample id="280">in this paper, we define the problem of constrained language planning.</sample>
    <sample id="281">which impose different constraints on the goal-oriented planning an abstract goal can be inherited by different real-life specific goals with multi-faceted constraints a good planner should write scripts that are reasonable and faithful to constraints</sample>
    <sample id="282">in this paper, we first evaluate and improve the constrained language planning ability of large language models.</sample>
    <sample id="283">non esiste alcun set di dati specifico per aiutare a identificare una stella.</sample>
    <sample id="284">we have to acquire these goals first and showing the table we extend the abstract goals with motivated constraints for human-like data acquisition using struct-ppt</sample>
    <sample id="285">we sample 100 specific goals and evaluate the scripts generated from large language models.</sample>
    <sample id="286">this table reports the overall accuracy of the results we found that all language models achieve unsatisfactory results on planning for specific goals</sample>
    <sample id="287">then we conduct detailed analyses to investigate what learning models work</sample>
    <sample id="288">i risultati nella figura mostrano che la completezza semantica nei script generati è accettabile ma la fedeltà ai vincoli non può essere garantita</sample>
    <sample id="289">we dig into more fine-grained topic categories of constraints depending on what we have the heat map in the figure shows that the planning performance of instructivities varies considerably for girls of different categories</sample>
    <sample id="290">previous studies have shown that the output quality of lstm models falls in high variance leading to bad performance thus we adopt the idea of over-generated z-filter to improve generation quality</sample>
    <sample id="291">we first show constraint types with examples for interactive dpt and obtain specific goals based on the set abstract goals</sample>
    <sample id="292">instruct gpt to generate key scripts for specific goals.</sample>
    <sample id="293">successivamente, viene sviluppato un modello di filtraggio per selezionare i testi pertinenti.</sample>
    <sample id="294">we convert scripts and goals into instruct gpt embeddings and calculate cosine similarity and similarity scores to measure semantic similarity</sample>
    <sample id="295">in addition we will write a script that contains the keywords of the target constraint we will only keep the script if the target goal score is the highest among the goal sites</sample>
    <sample id="296">with our method influenceability can generate squares of high quality our method greatly improves planability both in semantic completeness and effectiveness to the constraints</sample>
    <sample id="297">since large language models are costly to deploy it is essential to enable language planning ability of smaller and specialized models creating datasets is an essential step towards</sample>
    <sample id="298">however, previous studies do not enable planning for specific goals and manual dataset annotation is expensive.</sample>
    <sample id="299">thus we follow the idea of symbolic knowledge distillation to distill constrained language planning data sites from large language models</sample>
    <sample id="300">we apply our method for building a dataset of constrained language planning named as co-script</sample>
    <sample id="301">in total we generate 55000 specific goals with scripts to ensure the quality of validation and the test sites we ask crowdsourced workers to find and revise the incorrect samples</sample>
    <sample id="302">this figure shows the constraint distribution of codescript while figure 5 shows the hyper-polysemy in the generated specific goals with codescript we can train smaller but specialized models for constraint language planning</sample>
    <sample id="303">with that said t5's unsupervised training can generate scripts of higher quality than most large language models indicating that smaller models can surpass larger models when properly trained on suitable datasets</sample>
    <sample id="304">in summary we established the constraint language planning problem we evaluate the constraint language planning ability of learning models and develop an over-generating filter method for learning models</sample>
    <sample id="305">we use large language models to generate a high-quality script dataset for constrained language planning we hope the script dataset can be a valuable resource to advance the research on language planning</sample>
    <sample id="306">thanks for your time please find more details of course script in our paper</sample>
    <sample id="307">the insights that we gain from the human evaluation that we perform using the mqm framework is that the fluency of palm is comparable to state-of-the-art systems but the main difference comes from the accuracy</sample>
    <sample id="308">the watermark method needs to meet the following properties first the method should be applicable to embedding as services second the watermark should not degrade the utility of the provided embedding third the watermark should be covert enough to the attacker or the attacker can remove the watermark easily finally the watermark needs to be transferable to the attacker's services during the model extraction process</sample>
    <sample id="309">and we perform our analysis on transcripts of ted talks that have been translated from english to 14 different languages</sample>
    <sample id="310">our framework works in two main steps the first step is to re-annotate datasets with diverse annotators and we ought to do this over looking at the demographics of original datasets annotators because usually only a few annotators annotate each instance and because demographics are rarely collected and shared and so we ought to re-annotate data to get many annotators per instance and to get a rich set of demographic data we then take the annotations by demographic and compare them to the models and data sets using a pearson's r correlation score and thus our framework actually differs from annotator disagreement literature by comparing end users with models and data sets predictions and labels as opposed to looking at just annotator agreement or modeling annotator distributions</sample>
    <sample id="311">the cosine and l2 similarity between the requested embedding and the target embedding are computed we compute the similarity difference between benign and backdoor dataset which is defined as delta cosine and delta l2</sample>
    <sample id="312">and we also find many interesting results so regarding analyze of monolingual models we evaluate on two groups of models including encoder pdr which stands for multilingual pre-trained encoders with pointer-based decoders such as xlmr plus pdr and bert plus pdr and we also evaluate encoder decoder models which is multilingual pre-trained encoder decoder models such as mbert and mt5 we found that encoder decoder obtains the best performance on all nine data sets</sample>
    <sample id="344">before these main steps we first select a trigger set the trigger set is a group of words in a moderate frequency interval we assume the provider can collect a general text corpus and count the word frequency with it</sample>
    <sample id="345">hello everyone my name is xuhong today i'm going to present our paper do convolution 2003 named entity taggers still work well in 2023 let's get started</sample>
    <sample id="346">our paper investigated the problem of generalization using the named entity recognition task or the ner task</sample>
    <sample id="347">we observe that models have been used in kaggle 2003 to develop any r for almost 20 years and this naturally raises several problems firstly can these models generalize to modern data</sample>
    <sample id="348">and when we develop new taggers what is needed for good generalization</sample>
    <sample id="349">at the same time if we do observe poor generalization what causes the performance drop of these models</sample>
    <sample id="350">to investigate these problems we developed the cnn dataset this is a dataset that we collected from reuters news from 2020 and then annotated them with the same cnn 2003 annotation guidelines</sample>
    <sample id="351">we then fine-tuned over 20 models on kernel 2003 we evaluated them on both the kernel 03 test set and the kernel plus plus test set</sample>
    <sample id="352">and last but not least we calculated the percentage change in f1 to assess the generalization of each model</sample>
    <sample id="353">so what is needed for good generalization through our experiments we found that there are three main ingredients that are needed</sample>
    <sample id="354">the first one is the model architecture through our experiments we found that the transformer models normally generalize better to new data</sample>
    <sample id="355">the second ingredient is the model size we found that usually larger models lead to better generalization</sample>
    <sample id="356">and last but not least we all know that the number of fine-tuning examples directly affects the performance of a downstream task here we also found that more fine-tuning examples actually also lead to better generalization</sample>
    <sample id="357">to our next question what causes the performance drop of some models</sample>
    <sample id="358">we had two hypotheses the first one is adaptive overfitting which is overfitting caused by reusing the same test set over and over again and this is usually manifested as the diminishing returns on a new test set</sample>
    <sample id="359">the second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the training and the test data</sample>
    <sample id="360">for adaptive overfitting we saw that from the graph on the right the red best fit line has a gradient that is greater than one</sample>
    <sample id="361">this means that every unit of improvement that we made on call of 2003 translates to more than one unit improvement on call of which means that there is no diminishing returns</sample>
    <sample id="362">and this shows us that adaptive overfitting in this case is not observed</sample>
    <sample id="363">so what about temperature of that</sample>
    <sample id="364">for temporal drift we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gaps</sample>
    <sample id="365">and this confirms our hypothesis that the main cause of the performance drop is temporal drift</sample>
    <sample id="366">our conclusion is that for good generalization we would need a better model architecture larger model size as well as more fine-tuning examples and these go hand in hand we can't just have one ingredient but throughout the other</sample>
    <sample id="367">at the same time we also found that the performance drop here is caused by temporal drift and kind of surprisingly it is not caused by adaptive overfitting even though kernel 2003 has been used for over 20 years</sample>
    <sample id="368">so going back to the question that we raised in the title of our paper do carnegie 2003 tags still work in 2023 and we found that the answer is actually a resounding yes</sample>
    <sample id="369">we hope our paper calls for more research on how to improve generalizations of the models</sample>
    <sample id="370">and lastly please make sure to check out our paper our dataset and if you have any questions feel free to contact me thank you so much</sample>
    <sample id="397">so what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our solution what is our</sample>
    <sample id="398">here is an example from our dataset servin is a judge kea is a baker servin and kea met at a park after a long day at work deciding cases in a law court he was happy to relax the task here is to identify the correct entity that the pronoun he refers to which in this case is servin the resolution of a given pronoun requires two types of information first entity specific knowledge such as servin is a judge and second background knowledge such as judges decide cases in law courts generally background knowledge is learned during the pre-training of large language models while entity specific knowledge is typically observed at inference time we vary the availability of these two pieces of information such that it may either be found in a single source or in multiple sources</sample>
    <sample id="399">the summary of our experimental results is that the example quality is more important than the similarity to the source sentence</sample>
    <sample id="400">we can also see that gpt-4 is the most liberal language model of them all and gpt theories are generally more socially liberal than bert theories and its variants secondly we aim to investigate to what extent the political biases of language models are actually picked up from training data so we conduct a controlled experiment by further pre-training language model checkpoints on six different parties and corpora separated into news and social media further divided into their political leanings</sample>
    <sample id="401">if you want to discover more results read our paper and we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work thanks for your attention</sample>
    <sample id="402">the most obvious thing is to use a direct reference for example by saying the name of the song is a me or its position the first one</sample>
    <sample id="403">hi i'm sijun from the university i'm here to introduce our work distinguishing script knowledge from large language models for constrained language planning</sample>
    <sample id="404">hi i am yannis lavrac and i will present you our works on dr bert a robust pre-trained model in french for biomedical and clinical domain</sample>
    <sample id="405">and to better evaluate our benchmark we consider the six settings for training and evaluation the first one is translate test we use google translate api to translate source to the target language then use monolingual model to train and evaluation</sample>
    <sample id="406">so the marked words method draws upon the sociolinguistic concept of markedness which states that there is an unmarked default and any group that differs from that default is linguistically marked so for instance the word warrior is usually associated with men so when people are describing a warrior who is a woman they'll usually actually specify woman warrior and mark the term with woman</sample>
    <sample id="407">the first one is the model architecture through our experiments we found that the transformer models normally generalize better to new data</sample>
    <sample id="408">the right figure shows the performance difference between fine-tuning approaches which are directly applied on the clean data and wsl approaches which use the clean data for validation only</sample>
    <sample id="409">hello everyone i'm mark shatta and today my co-author martin and i are presenting our work the kit must have evaluating knowledge integration from multiple sources this work is a collaboration between mcgill university mila and microsoft research</sample>
    <sample id="410">therefore in this work we want to investigate whether instruction tuning on multimodal pre-trained models can actually improve generalization to unseen multimodal tasks</sample>
    <sample id="439">therefore successful models for knowledge-intensive nlu tasks require the ability to integrate and use both pre-trained time and inference time knowledge</sample>
    <sample id="440">hello everyone my name is ying and my colleague jian and i will be presenting our research on multi-instruct improving multimodal social learning via instruction tuning</sample>
    <sample id="441">in total we generate 55000 specific goals with scripts to ensure the quality of validation and the test sites we ask crowdsourced workers to find and revise the incorrect samples</sample>
    <sample id="442">and some people have suggested targeted evaluation on context-dependent translations but these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation</sample>
    <sample id="443">hi and i'm going to talk about our work on resolving indirect referring expressions for entity selection in which we introduce the altentities corpus</sample>
    <sample id="444">my name is jawad hosseini and this is a joint work with philippe radlinski silvia parotti and annie lewis</sample>
    <sample id="445">our goal is to understand users' language when they want to make a choice now consider this alternative question did you mean easy on me or i got a feeling here a user wants to select between one of these two songs</sample>
    <sample id="446">the most obvious thing is to use a direct reference for example by saying the name of the song is a me or its position the first one</sample>
    <sample id="447">ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale questo potrebbe accadere quando l'utente non ricorda il nome della fonte.</sample>
    <sample id="448">all the pronunciations are too similar to each other and hard to disambiguate</sample>
    <sample id="449">or when the user wants to specify a preference here are some examples of indirect preferences for example the newer one or the song that's not energetic</sample>
    <sample id="450">this is an important problem in conversational systems and also for benchmarking elms entity understanding</sample>
    <sample id="451">we're not aware of a public dataset a large-scale public dataset for the task so we collect one using crowd annotation our dataset covers three different domains music books and restaurants</sample>
    <sample id="452">our dataset collection methodology emphasizes informality using a cartoon completion</sample>
    <sample id="453">il cartone ha tre bolle di dialogo nella prima bolla bob dice ricorda quella canzone a cui stavamo ascoltando ieri e con quello bob stabilisce il contesto del dialogo.</sample>
    <sample id="454">in the second speech bubble alice says do you mean easy on me or i got a feeling</sample>
    <sample id="455">which is the alternative question and in the third speech bubble bob uses an indirect reference to select one of these entities for example the newer</sample>
    <sample id="456">we provide the first and second speech bubbles automatically but the third one is filled in by the annotator the first speech bubble is chosen from a few manual prompts per domain</sample>
    <sample id="457">il secondo, che è la domanda alternativa, viene generato come segue.</sample>
    <sample id="458">we always use a simple template do you mean a or b where a and b are samples from wikipedia</sample>
    <sample id="459">here are the different sampling methods we've used when we move higher in the list the entities become more similar to each other and it's usually harder to make the disambiguation</sample>
    <sample id="460">the first one is uniform attraction</sample>
    <sample id="461">the second one is when the entities have similar titles for example two books with the name the return</sample>
    <sample id="462">the third one is when they have similar descriptions on wikipedia and finally when they have similar infoboxes or attributes on wikipedia for example the same genre or the same artist for a song</sample>
    <sample id="463">when we show this alternative question to the armchair experts they know the name of these entities but they don't necessarily know about the entity</sample>
    <sample id="464">so what we do is that we show some background knowledge about the two entities for songs we simply show a google search link to each song</sample>
    <sample id="465">and then ask the annotators to listen to at least some of each song and read about each song here's for example the google search result for the song easier</sample>
    <sample id="466">for the recipes and books domain we show some background text from wikipedia for recipes we additionally show their images again from wikipedia so that the annotators know how they look like</sample>
    <sample id="467">then we ask the annotators to pick one of these entities for example here the first one and describe them using three to five indirect referring expressions</sample>
    <sample id="468">for example the one with the piano music here are some examples from our dataset for example the one without words not the one with the 12-year-old boy or the fictional one or comes from another boy john and so on</sample>
    <sample id="469">the ltd corpus has 6000 alternative questions across three domains and it has 42000 indirect referring expressions results with t5x large model are summarized below</sample>
    <sample id="470">if the language model has access to the exact same background knowledge as the annotators then the accuracy is really high it's around 92 to 95 but this is not realistic</sample>
    <sample id="471">if the language model has access to some partially overlapping background knowledge then the accuracy is between 82 to 87 which is more realistic for example when the language model retrieves the background knowledge</sample>
    <sample id="472">if the language model has access only to entity names then the accuracy is only 60 so there's a lot of room for improvement we've also shown that the models are domain generalizable here is a link to our dataset</sample>
    <sample id="473">and we compare with popular strategies that are also applied to offline models that are the witty strategy and the local equipment and we compare also with the state-of-the-art architecture specifically tailored for simultaneous speech translation</sample>
    <sample id="474">hi i am yannis lavrac and i will present you our works on dr bert a robust pre-trained model in french for biomedical and clinical domain</sample>
    <sample id="475">hi everyone i'm jenny a first-year phd student at carnegie mellon university and today i'll be presenting our work and our positionality characterizing design biases of data sets and models this work was done in collaboration with some folks at the university of washington and the allen institute for ai namely sebastian santi roland labrosse katerina raneka and martin sapp</sample>
    <sample id="476">hi i'm myra and today i'll be talking about our paper marked personas using natural language prompts to measure stereotypes in language models this work is done in collaboration with esendermush and dan juravsky</sample>
    <sample id="477">hi i'm sarah papi from the university of toronto and funded by bruno kessler and i will briefly introduce the attention as a guide for simultaneous speech translation paper that is a joint work with matteo negrini and marco durci</sample>
    <sample id="478">what is simultaneous speech translation simultaneous speech translation or simosti is the process of translating spoken language into text in another language in real time, enabling cross-language communication.</sample>
    <sample id="479">and what are the problems of the current similarity models specific architectures are usually trained introducing additional modules to be optimized</sample>
    <sample id="480">long and complicated training procedures for example training involving different optimization objectives</sample>
    <sample id="481">and training and maintaining several models to reach different latency regimes for example training a model with an average of one second latency and another one with two seconds latency and so on</sample>
    <sample id="482">so what is our solution</sample>
    <sample id="483">first use already existing offline st models without retraining or adopting specific architecture for st use only one model for every latency regime and handle latency through specific parameters</sample>
    <sample id="484">and leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output that is the cross-attention mechanism and you can see an example on the right</sample>
    <sample id="485">our solution is to propose attention or encoder-decoder attention and it is a strategy for which we decide whether to emit or not a partial translation based on where attention points to</sample>
    <sample id="486">and what is emitted if the tension is not concentrated that is this sum is below a certain threshold alpha towards the last lambda speech frames meaning that the received information is unstable</sample>
    <sample id="487">for example if we receive a speech shank containing i'm going to talk about and our model predicts the translation in german</sample>
    <sample id="488">and we will look at the cross-attention weights</sample>
    <sample id="489">we will see that the first two words point to the earliest received speech frames while the last word points to the last received pitch frames and as lambda speech frames</sample>
    <sample id="490">this means that the first two words will be omitted.</sample>
    <sample id="491">while since the sum of the cross-attention is above a certain threshold alpha we will not emit the last word and we wait for another speech chunk</sample>
    <sample id="492">if we go on and we receive another speech sound and our model predicts order three words and we will look at the cross-attention weights</sample>
    <sample id="493">we will see that no words points to the last lambda speech frames</sample>
    <sample id="494">this means that these three words will be omitted.</sample>
    <sample id="495">se guardiamo i risultati principali di quello.</sample>
    <sample id="496">we will plot the simultaneous speech translation results on graphs in which we have blue on one side that measures the translation quality and average length</sample>
    <sample id="497">that is the latency measure and we also consider the computational aware average lagging that accounts for the models computational times to predict the output</sample>
    <sample id="498">so we want our curves to be as high as possible on this plot</sample>
    <sample id="499">but also we want that they are shifted on the left</sample>
    <sample id="500">and we compare with popular strategies that are also applied to offline models that are the whitkey strategy and the local equipment and we compare also with the state-of-the-art architecture specifically tailored for simultaneous speech translation</sample>
    <sample id="501">questi sono tutti i risultati della strategia di traduzione simultanea per il tedesco.</sample>
    <sample id="502">and we see that adult outperforms all the strategies applied to offline models since their curves are shifted to the left</sample>
    <sample id="503">and we also see that if we consider the actual elapsed time or the computational wall time that is the fastest strategy</sample>
    <sample id="504">if you want to discover more results read our paper and we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work thanks for your attention</sample>
    <sample id="505">and lastly please make sure to check out our paper our dataset and if you have any questions feel free to contact me thank you so much</sample>
    <sample id="506">hello everyone my name is ying and my colleague jian and i will be presenting our research on multi-instruct improving multimodal social learning via instruction tuning</sample>
    <sample id="507">so with the advances in large language models many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data efficient way</sample>
    <sample id="508">recently, many studies have shown that instruction tuning enables large language models to perform an unseen task in a zero-shot manner by following natural instructions.</sample>
    <sample id="509">however most previous works on instruction tuning focused on improving the zero-shot performance on language-only tasks while computer vision and multimodal tasks have been left out</sample>
    <sample id="510">therefore in this work we want to investigate whether instruction tuning on multimodal pre-trained models can actually improve generalization to unseen multimodal tasks</sample>
    <sample id="511">inoltre, al momento della nostra ricerca, abbiamo scoperto una considerevole discrepanza nell'accessibilità dei set di dati di istruzione tra lp e multimodale.</sample>
    <sample id="512">there exist more than 1600 language-only instruction tasks however there is no large-scale publicly available multimodal instruction tasks therefore this motivates us to build a multimodal instruction tuning dataset</sample>
    <sample id="513">here we present multi-instruct the first multimodal instruction tuning benchmark dataset that consists of 62 diverse multimodal tasks covering 10 bold categories</sample>
    <sample id="514">these tasks are derived from 21 existing open source datasets and each task is equipped with five expert-written instructions</sample>
    <sample id="515">for investigating multimodal instruction tuning on our proposed dataset we take ofa a unified multimodal pre-training model as our base model ofa uses a unified vocabulary for language image tokens and the coordinate of a bounding box</sample>
    <sample id="516">here we show some example instances from our multi-instance dataset</sample>
    <sample id="517">to unify the processing of various input and output data types.</sample>
    <sample id="518">seguiamo il metodo di ofa e formuliamo tutti i compiti in un formato sequenza-a-sequenza unificato in cui il testo di input immagini istruzioni e scatole di vincolo sono rappresentati nello stesso spazio di token.</sample>
    <sample id="519">okay now i'm going to talk about multimodal instruction tuning</sample>
    <sample id="520">so for the training data set we use 53 tasks from net group for training and we sample 10000 instances per task for testing we reserve the entire commonsense reasoning group for testing and we select additional five tasks from wiki and the miscellaneous group</sample>
    <sample id="521">we use all the instances in the test split for each task in addition we randomly sample 20 tasks from the test split of natural instruction as a single task for nlp</sample>
    <sample id="522">so we use a pre-trained ofa large model as a base model during training we mix all the instances for all the tasks each instance is randomly combined with one of its five instruction templates</sample>
    <sample id="523">so during test for each task we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment</sample>
    <sample id="524">we report the mean and max performance and the standard deviation of the performance across all five experiments</sample>
    <sample id="525">if the task is a multimodal classification task we report accuracy if it's a multimodal generation task we report rouge-l for an lp task we report rouge-l as well</sample>
    <sample id="526">we also introduced an additional evaluation metric called sensitivity so this measures the model's ability to consistently produce the same outputs for the same task regardless of slight variation in the wording of the instruction</sample>
    <sample id="527">here is our main result as we can see instruction tuning can significantly improve ofa's performance on the same multimodal tasks</sample>
    <sample id="528">also transfer learning from natural instruction datasets can benefit instruction tuning</sample>
    <sample id="529">here we can see as the amount of tasks increase the model achieves better performance and in the meantime lower sensitivity</sample>
    <sample id="530">so we also did one experiment we used one instruction versus five instruction as we can see using more instruction can improve the model's overall performance and reduce its sensitivity a lot</sample>
    <sample id="531">so this shows the effect of different fine tuning strategy on the model sensitivity uh as we can see by transfer learning from natural instruction data sets the model can uh achieve much better sensitivity compared to the original ifa model</sample>
    <sample id="532">we also can see transfer learning from natural instruction dataset can help wfa to achieve much better performance on the natural instruction dataset</sample>
    <sample id="533">overall we propose the first large scale multimodal instruction tuning dataset which significantly improves the zero shot capability of ofa and we explore different transfer learning techniques and show their benefits we design a new metric called sensitivity</sample>
    <sample id="534">so one more thing we are collecting a much larger multimodal instruction tuning dataset with around 150 additional variant language tasks and we will release them so this is a qr code for our data and the model thank you</sample>
    <sample id="535">hi i'm sarah papi from the university of toronto and funded by the bruno kessler and i will briefly introduce the attention as a guide for simultaneous speech translation paper that is a joint work with macdonald and marco d'orci rispondi in modo conciso alla seguente domanda dato il contenuto inglese</sample>
    <sample id="536">my name is jawad hosseini and this is a joint work with philip radlinski silvia parotti and annie lewis</sample>
    <sample id="562">hi everyone i'm kostas of senna and i'm pleased to welcome you to our talk of our acl 2023 paper language model acceptability judgments are not always robust to context</sample>
    <sample id="563">this is a joint work with john gauthier aaron muller kanishka mishra karen fuentes roger levy and etina williams</sample>
    <sample id="564">in this work, we revisit the minimal pair paradigm.</sample>
    <sample id="565">so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like plump syntax gem or acceptability in terms of stereotypes such as cross pairs</sample>
    <sample id="566">and in this minimal pair paradigm the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an unacceptable sentence or an ungrammatical sentence</sample>
    <sample id="567">and then the hope is that the model basically puts more probability to the acceptable sentences</sample>
    <sample id="568">the current mpp pipeline basically doesn't allow us to evaluate a model's acceptance towards longer sentences</sample>
    <sample id="569">these days large language models are coming up with longer and longer context windows so it's crucial that we evaluate the model's acceptability throughout the context window</sample>
    <sample id="570">and that is what we are trying to do here we are trying to revisit the mpp pipeline by asking the model to evaluate acceptability on longer and longer sequences</sample>
    <sample id="571">so that is the approach so what we do is that to simulate these longer sequences we revisit the data sets themselves and then we recreate sentences by choosing like acceptable or unacceptable sentences from those data sets</sample>
    <sample id="572">so for example here we have chosen a typical pair of grammaticality from the blimp dataset from the adjunct island case</sample>
    <sample id="573">and what we do is that to recreate like longer sequences and which are acceptable and which has the same matching of the grammatical structure we extract grammatical sentences from adjoint pilot</sample>
    <sample id="574">e poi lo aggiungiamo come prefisso sia alla query accettabile che alla query non accettabile.</sample>
    <sample id="575">so we can do the same thing by choosing unacceptable sentences from the same matching and that could also be used to test the model's acceptability</sample>
    <sample id="576">and we can also do the same by choosing sentences from a different subset or a different dataset so that is what we call as the mismatch scenario</sample>
    <sample id="577">so here the sentences are still coming from relevant data sets but it's not from the same data set that you are evaluating with and we can do the same for unacceptability case</sample>
    <sample id="578">finally we can choose sentences from a completely unrelated domain such as wikipedia</sample>
    <sample id="579">this will tell us whether the models' acceptability judgments are actually impacted by any context</sample>
    <sample id="580">whether the context is coming from a different subset of the data set or whether it's like completely irrelevant to the current like to the sentence that we are looking at</sample>
    <sample id="581">so how does the model do so first we look at the wikipedia sentences which are completely irrelevant to the current query pair and there we find that the mpp judgments are mostly robust for arbitrary context length</sample>
    <sample id="582">we increased the context length toward up to 2024 for to max out opt and gpt2 models and we saw here in the orange dotted line the mpp judgments are relatively stable</sample>
    <sample id="583">now what happens when we choose sentences from the same dataset</sample>
    <sample id="584">so here we are choosing or creating sentences from acceptable and unacceptable domains from the same blimp or syntax gem dataset</sample>
    <sample id="585">and there we see that the mpp judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes</sample>
    <sample id="586">but when we match the structure that is when we choose the sentences from the same phenomena in blame per syntax gem</sample>
    <sample id="587">we see a massive increase or a massive decrease in of the mpp judgment for the model depending on whether the chosen prefix is acceptable or unacceptable</sample>
    <sample id="588">now this and this is very large like this effect increases throughout the context length and this would probably affect like newer language models which has large context window</sample>
    <sample id="589">so why does the match prefix affect the language model judgment so much</sample>
    <sample id="590">so we did a series of analysis where we tried to like perturb the input sentence by trying to preserve the relevant structure but adding like noise to the input and after doing like several of these perturbations</sample>
    <sample id="591">we find that none of these noises are actually making the model like change its course in terms of how it shows us the mpp judgment trend</sample>
    <sample id="592">basically we find that the models are sensitive to the perturbations in sentences in similar ways</sample>
    <sample id="593">that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the not acceptable domain we see decrease in mpp judgments in similar fashion</sample>
    <sample id="594">so the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences</sample>
    <sample id="595">and the mpp evaluation the way that we do it currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context window</sample>
    <sample id="596">please read our paper for more details of our experiments thank you for listening</sample>
    <sample id="597">first we tag each input token with an unordered multiset of tokens that will appear in the output</sample>
    <sample id="598">in total we generate 55000 specific goals with scripts to ensure the quality of validation on the test sites we ask crowdsourced workers to find and revise the incorrect samples</sample>
    <sample id="626">at the end we concluded that the best automatic alignment method to use for german text simplification is the method of mass align and you can also find the code to run this method on your own documents in the paper</sample>
    <sample id="627">if we directly train neural networks on weakly labeled data the neural networks tend to memorize the label noise and do not generalize in weakly supervised learning training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well</sample>
    <sample id="628">you can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper</sample>
    <sample id="629">to investigate these problems we developed the conll dataset this is a dataset that we collected from reuters news from 2020 and then annotated them with the same conll 2003 annotation guidelines</sample>
    <sample id="630">hello everyone my name is yixin john from the penn state university today i'm going to present our work exemplar cross-lingual semantic parsing in multiple natural languages and many representations</sample>
    <sample id="631">il parsing semantico è il compito di costruire rappresentazioni semantiche delle query degli utenti come sql e lambda calculus.</sample>
    <sample id="632">and cross-lingual semantic parsing is the task to translate queries in multiple natural languages into multiple meaning representations</sample>
    <sample id="633">as shown in this figure we need to translate the query in multiple natural languages using neural models to sql lambda or funql and etc</sample>
    <sample id="634">existing cross-lingual semantic parsing models are separately proposed and evaluated on datasets of limited tasks and applications for instance</sample>
    <sample id="635">there are gaps of um coverage on certain natural language of the chinese is missing and</sample>
    <sample id="636">la copertura dei laghi su alcune rappresentazioni miniature.</sample>
    <sample id="637">the lambda calculus is missing</sample>
    <sample id="638">or they are only evaluated on a certain neural model for example there is only one single model to evaluate</sample>
    <sample id="639">so to this end we propose exemplar we provide a uniform dataset exemplar for cross-lingual semantic parsing in multiple natural languages and mean representations</sample>
    <sample id="640">it contains nine data sets in various domains 570 taxonomies 8 million representations and 22 natural languages in 15 language families</sample>
    <sample id="641">and to better evaluate our benchmark we consider the six settings for training and evaluation</sample>
    <sample id="642">the first one is translate test we use google translate api to translate source to the target language then use monolingual model to train and evaluation</sample>
    <sample id="643">and for example we train the english model on english query and during inference we translate the german query using api to english and then use the trained model to predict the sql</sample>
    <sample id="644">and we'll also test monolingual model</sample>
    <sample id="645">in this setting the source language is the same as target language for example german to german or english to english</sample>
    <sample id="646">we also test multilingual fusion setting by training multilingual models with only 10 of training data</sample>
    <sample id="647">and which has the multilingual model which we train one multilingual model for all languages</sample>
    <sample id="648">for example we put the german english chinese queries together to train a multilingual model and during inference we can use this model to</sample>
    <sample id="649">to translate german queries or chinese queries etc</sample>
    <sample id="650">and we also consider cross-lingual zero-shot and few-shot transfer we train on one source language and transfer to another language</sample>
    <sample id="651">during training we're training on english query or the combination of english and german few-shot queries to train a multilingual model to and predict the sql output</sample>
    <sample id="652">and we also find many interesting results so regarding analysis of monolingual models we evaluate on two groups of models</sample>
    <sample id="653">including encoder pdr which stands for multilingual pre-trained encoders with pointer-based decoders such as xlmr pdr and bert pdr</sample>
    <sample id="654">and we also evaluate encoder-decoder models which is multilingual pre-trained encoder-decoder models such as mbert and mt5</sample>
    <sample id="655">abbiamo scoperto che encoder-decoder ottiene le migliori prestazioni su tutti e nove i set di dati.</sample>
    <sample id="656">and we evaluate on mt5 and xlmr plus bdr in a multilingual setting</sample>
    <sample id="657">without that encoder decoder or encoder pdr can be improved by training in a mixture of various languages</sample>
    <sample id="658">and we found it is because most of the major natural languages can obtain performance gain except that english performance drops in seven data sets and only gains in three data sets</sample>
    <sample id="659">credo che questo sia noto come curse of multilinguality.</sample>
    <sample id="660">we also compared the cross-lingual performance gap</sample>
    <sample id="661">in this figure the blue line is cross lingual fusional transfer the orange line is cross lingual zero shot transfer while the green line is the monolingual setting</sample>
    <sample id="662">we found that by comparing the green and orange line we found that for zero shot setting the cross lingual transfer performance gap is significant and by comparing blue and orange line we found that for few shot setting the transfer gap is shortened rapidly</sample>
    <sample id="663">we also find some other interesting findings for example encoder decoder outperforms previous work or achieved comparable results for training on english natural language can significantly boost the performance of future on target natural languages</sample>
    <sample id="664">and we found multilingual language models such as coders and blue are still inadequate for cross-lingual semantic parsing tasks</sample>
    <sample id="665">to sum up we build exemplar a unified benchmark for cross-lingual semantic parsing with multiple natural languages and many representations</sample>
    <sample id="666">we conduct a comprehensive benchmark study on three representative of types of multilingual language models and our results shows many interesting findings and etc and welcome to visit our paper and code thanks for listening</sample>
    <sample id="667">existing works can be broadly classified into four categories</sample>
    <sample id="668">we also find some other interesting findings for example encoder decoder outperforms previous work or achieved comparable results but training on english natural language can significantly boost the performance of future on-target natural languages and without modeling goal language models such as coders and blue are still indecisive for cross-lingual semantic tasks</sample>
    <sample id="695">in addition sometimes there are multiple permutations that are consistent with the data but the linguistically correct one is latent we address this by inducing the alignment as part of the training our permutation method is very flexible but it brings the challenge that finding the highest scoring permutation is np-hard that's because this is related to the traveling salesman problem we approximate this with a gpu friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations</sample>
    <sample id="696">for example if a right-leaning language models were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform this would mean that people with opposite political opinions might be marginalized and the hate speech targeting minority groups might just run rampant without any control so this sounds the alarm for us to acknowledge and tackle the fairness issues resulted by language model political leanings</sample>
    <sample id="697">hi i am yannis lavrac and i will present you our works on dr bert a robust pre-trained model in french for biomedical and clinical domain</sample>
    <sample id="698">hi everyone i'm kostas of sena and i'm pleased to welcome you to our talk of our acl 2023 paper language model acceptability judgments are not always robust to context</sample>
    <sample id="699">hi i'm myra and today i'll be talking about our paper marked personas using natural language prompts to measure stereotypes in language models this work is done in collaboration with esendermush and dan juravsky</sample>
    <sample id="700">furthermore there's a lot of common tropes that are reflected in these words especially for women of color so for example the words describing latina women include things like vibrant and curvaceous which connect to a trope of tropicalism for asian women the words are things like petite and delicate and silky</sample>
    <sample id="701">first from mark groups the top words include things like culture tradition proud and exotic and these words define these groups only by their relationship to their identity and distinguish them as different from the white norm</sample>
    <sample id="702">in this work we extend cxmi to pointwise cxmi which can measure context usage at the sentence level or at the word level we can think of words that have high pxmi as ones that require context for translation</sample>
    <sample id="703">to answer this question we first train and compare four from scratch model a first version of dr bert with 7 gigabytes of natchez a second version of 4 gigabytes of set of natchez a first version of schubert which is a clinical model with 4 gigabytes of sentences taken from clinical notes and a final version of schubert with a mix of 4 gigabytes of set of natchez and 4 gigabytes of clinical notes</sample>
    <sample id="751">hello everyone my name is ying and my colleague jian and i will be presenting our research on multi-instruct improving multimodal social learning via instruction tuning</sample>
    <sample id="752">next we determine the best method to update a model with new data from each round of active learning and annotations cumulative accumulates all the data collected from active annotations so far whereas iterative updates the model by training on the latest set of data collected</sample>
    <sample id="753">our goal is to understand users' language when they want to make a choice now consider this alternative question did you mean easy on me or i got a feeling here a user wants to select between one of these two songs</sample>
    <sample id="754">we also validate the covertness of the provided embedding by visualizing the embedding of sentences unfolded as a bipca the legend of the figures means the number of triggers in each sentence</sample>
    <sample id="755">hi i'm sarah papi from the university of toronto and funded by bruno kessler and i will briefly introduce the attention as a guide for simultaneous speech translation paper that is a joint work with macdonald and marco d'orci rispondi in modo conciso alla seguente domanda dato il contenuto inglese quanti autori sono coinvolti nell'articolo</sample>
    <sample id="756">we transfer from two different tasks topic independent dissonance stance classification a task that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic</sample>
    <sample id="757">hi everyone i'm jenny a first-year phd student at carnegie mellon university and today i'll be presenting our work on the positionality characterizing design biases of data sets and models this work was done in collaboration with some folks at the university of washington and the allen institute for ai namely sebastian santi roland labrosse katerina raneka and martin sapp</sample>
    <sample id="758">so when the difference between the lengths of the two conjuncts grows the shorter conjunct prefers to be the first one stronger right so the proportion is bigger of the left short conjuncts but what's novel in this paper is that we observed that this tendency only occurs when the governor is on the left or absent right so the governor is on the left in this example isabel and lisa so if the governor is on the left</sample>
    <sample id="759">abc eval is capable of measuring the rates at which chat models will commit various thematic errors</sample>
    <sample id="760">these days large language models are coming up with longer and longer context windows so it's crucial that we evaluate the model's acceptability throughout the context window</sample>
    <sample id="761">and we found it is because most of the major natural languages can obtain performance gain except that english performance drops in seven data sets and only gains in three data sets i think this is known as curse of multilinguality</sample>
    <sample id="762">when we show this alternative question to the annotators they know the name of these entities but they don't necessarily know about the entity</sample>
    <sample id="763">it's the examples that carry most of the weight</sample>
    <sample id="764">the second ingredient is the model size we found that usually larger models lead to better generalization</sample>
    <sample id="765">so let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content you might turn towards a popular api like perspective api for toxicity detection and this works really well if you're carl jones where perspective api is able to detect correctly toxic instances but that's not really the case for adithya sharma where perspective api is really not as sensitive to offensive terms that are more common in indian contexts this is an example of a design bias where we see systematic performance differences of technology between populations</sample>
    <sample id="766">as shown in this figure we need to translate the query in multiple natural languages using neural models to sql lambda or funql and etc rispondi in modo conciso alla seguente domanda dato il contenuto inglese gli llm multilingue come bloom sono stati affinati mediante adattatori o con una messa a punto integrale</sample>
    <sample id="767">we transfer from two different tasks topic independent dissonance stance classification a task that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic called debate here and on binary classification of expansion and comparison classes of pity be since these two are closely related to the conception of consonance and dissonance and we call them ce here we find that on transferring the zero-shot performance on the annotated dataset is already much better than chance with the best with auc 062 further on iteratively fine-tuning on both tasks we find that fine-tuning of ce tasks followed by further fine-tuning on debate yields a much better zero-shot performance thus this is the model that we use to cold start the active learning</sample>
    <sample id="768">we saw that the actual form of the prompting doesn't have a big influence in the case of several shot prompting</sample>
    <sample id="769">so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude with three recommendations for model owners so based on these patterns we conclude</sample>
    <sample id="770">this figure shows the constrained distribution of code script while figure 5 shows the hyper-polysemy in the generated specific goals with code script we can train smaller but specialized models for constrained language planning</sample>
    <sample id="771">hello everyone my name is shruhang today i'm going to present our paper do convolution 2003 named entity taggers still work well in 2023 let's get started</sample>
    <sample id="772">and we propose those results as a benchmark a base benchmark for the problem of automatic text simplification in the future</sample>
    <sample id="773">with that said tf-funchun and score rate can generate scraps of higher quality than most large language models indicating that smaller models can surpass large language models when properly trained on suitable data sets</sample>
    <sample id="774">for investigating multimodal instruction tuning on our proposed dataset we take ofa a unified multimodal pre-training model as our base model ofa use a unified vocabulary for language image tokens and the coordinate of a bounding box</sample>
    <sample id="833">hello everyone my name is alex villar and i will be giving a short overview of the paper brenting paraphrase translation assessing strategies and performance this is joint work with my colleagues from google translate</sample>
    <sample id="834">hello my name is vasudha and i'm a computer science phd candidate at stony brook university i would like to present our work accepted into acl 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge</sample>
    <sample id="835">we use state-of-the-art neural mt metrics and additionally also show expert-based human evaluation results finally we provide some recommendations for prompt selection strategies</sample>
    <sample id="836">hi i'm john green phd student at the university of washington today i'm presenting our work from pre-training data to language models to downstream tasks tracking the trails of political biases leading to unfair nlp models</sample>
    <sample id="837">we have fine-tuned two different models we have fine-tuned the model of long impart to produce document level simplifications and we also fine-tuned the normal base long the normal base impart to produce sentence level simplifications</sample>
    <sample id="838">so for the training data set we use 53 tasks from natural instruction for training and we sample 10000 instances per task uh for testing we reserve the entire commonsense reasoning group for testing and we select additional five tasks from wikia and the miscellaneous group we use all the instance in the test split for each task uh in addition we randomly sample 20 tasks from the test split of natural instruction as on syn task for our p</sample>
    <sample id="839">hi welcome to our presentation of deplain a new corpus for german text simplification on the document level and on the sentence level my name is regina stodden and i will guide you through the first part of the presentation let's first define text simplification</sample>
    <sample id="840">we conduct experiments on four datasets agnews mind sst2 and erasmus we assume the provider applied wikitext dataset to count word frequencies</sample>
    <sample id="876">we introduced the first biomedical model in french named dr bert which is based on roberta and trained on nachos which is a dataset of medical ground data from the web</sample>
    <sample id="877">hello everyone my name is alex villar and i will be giving a short overview of the paper brenting paraphrase translation assessing strategies and performance this is joint work with my colleagues from google translate</sample>
    <sample id="878">the prompting has a big influence on the performance of the of lms for translation as we can see in a simple experiment where we use one shot prompting and provided two different prompts for for the same sentence</sample>
    <sample id="879">hello my name is kyo yen and i will be presenting our work titled when does translation require context a data-driven multilingual exploration this work was done in collaboration with patrick frenoux emilie yu andrew ft martins and graham newbigg respond in a concise manner to the following question given the content in english</sample>
    <sample id="880">so one more thing we are collecting a much larger multimodal instruction tuning dataset with around 150 additional variant language tasks and we will release them so this is a qr code for our data and the model thank you</sample>
    <sample id="881">we introduce a co-reference resolution task designed to probe for the ability to draw on knowledge available in different sources we evaluate the dataset with human study participants and establish co-reference resolution models</sample>
    <sample id="882">hello everyone my name is alex villar and i will be giving a short overview of the paper brenting paraphrase translation assessing strategies and performance this is joint work with my colleagues from google translate</sample>
    <sample id="883">param is a 540 billion parameters large language model presented last year in 2022 it's trained on a large collection of text comprising 780 billion tokens</sample>
    <sample id="884">in the tama publication it achieves state-of-the-art in hundreds of nlp tasks</sample>
    <sample id="885">in this work we present the first systematic study of large language model prompting for machine translation</sample>
    <sample id="886">we evaluate the translation capability of such models using the best practices of the amt community this involves using the latest test sets to avoid an overlap of the test data with the training data of the language model</sample>
    <sample id="887">and we compare two state-of-the-art systems the best performing systems of the wmt evaluation</sample>
    <sample id="888">we use state-of-the-art neural metrics and additionally also show expert-based human evaluation results finally we provide some recommendations for prompt selection strategies</sample>
    <sample id="889">the prompting has a big influence on the performance of lms for translation as we can see in a simple experiment where we use one-shot prompting and provided two different prompts for the same sentence</sample>
    <sample id="890">the majority of sentences 516 out of 1000 the difference observed is of more than one blur point</sample>
    <sample id="891">and this can go in extreme cases up to 40 blur points so it's important to select a good prompting strategy</sample>
    <sample id="892">in our experiments we settled for a five-shot prompting strategy where we just mark each sentence that we provide to the system with the language it's in</sample>
    <sample id="893">so in this example here where we perform translation from german into english the german sentences the source sentences are marked with german column and the english translations with english column</sample>
    <sample id="894">we saw that the actual form of the pruning doesn't have a big influence in the case of several shared pruning</sample>
    <sample id="895">it's crucial for zero and one shot prompting but when we go as in our case to five shot prompting there is nearly no difference to the actual form of the prompting</sample>
    <sample id="896">sono gli esempi a portare la maggior parte del peso.</sample>
    <sample id="897">the summary of our experimental results is that the example quality is more important than the similarity to the source sentence.</sample>
    <sample id="898">so it's important to select the examples from high quality translations in particular we compare the selecting prompts from the training data of the wmt evaluations or the dev data</sample>
    <sample id="899">the dev data is much more curated and with higher quality than the training data and the results so better performance when using the dev data</sample>
    <sample id="900">nevertheless specialized state-of-the-art systems have a substantial advantage over the bart translations but bart comes pretty close to a commercial system in our case we chose to evaluate with google translate</sample>
    <sample id="901">the insights that we gained from the human evaluation that we performed using the mqm framework is that the fluency of palm is comparable to state-of-the-art systems but the main difference comes from the accuracy</sample>
    <sample id="902">in particolare, gli errori più comuni sono gli errori di omissione.</sample>
    <sample id="903">so it seems that palm chooses to produce a better sounding translation sometimes by dropping parts of the source sentence that are not needed in the translation</sample>
    <sample id="904">however the style awkward category for pan is lower than for the state-of-the-art systems which is an additional signal</sample>
    <sample id="905">that parm provides really fluent output but still with some problems of accuracy</sample>
    <sample id="906">and that's it for this really short overview for more details please come to the full presentation of the paper thank you very much</sample>
    <sample id="907">hello i am dawei a phd student at salant university in germany in this video i would like to present our recent work weaker than you think a critical look at weakly supervised learning</sample>
    <sample id="908">this is a joint work with xiaoyu shen mario smooth path and georg stefan and dmitry shklokov</sample>
    <sample id="909">i'd like to begin with a brief introduction to weak supervision and weakly supervised learning</sample>
    <sample id="910">in weak supervision we do not manually label the data instead we label the data using weak labeling sources such as simple heuristic rules knowledge bases or low-quality crowdsourcing as illustrated in the figure on the right</sample>
    <sample id="911">when compared to human annotations, the weak annotations are much cheaper yet they are also noisy meaning that a certain amount of the annotations are incorrect</sample>
    <sample id="912">if we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize.</sample>
    <sample id="913">in weakly supervised learning training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well</sample>
    <sample id="914">in recent works in wsl so wsl stands for weakly supervised learning a common claim is that people say that they only train models on the weakly labeled data and achieve high performance on clean test sets</sample>
    <sample id="915">technically, this claim is not wrong, but there's a catch.</sample>
    <sample id="916">which is that people do assume that there is an additional clean validation set available for model selection</sample>
    <sample id="917">we can start on this problem setting but this implies that additional manual annotations are required in weakly supervised learning but like an elephant in the room this necessity is often overlooked</sample>
    <sample id="918">the aforementioned adoption is asked to ask three research questions first is clean validation data necessary for wsl or can we maybe use a noisy validation set instead</sample>
    <sample id="919">second if clean data is required or if clean data is mandatory for wso to work then how many clean samples do we need finally should we only use the clean samples for validation or there are better ways to utilize them</sample>
    <sample id="920">abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti.</sample>
    <sample id="921">first we find that interestingly recent wsl methods indeed require clean validation samples to work properly</sample>
    <sample id="922">otherwise there is a large performance drop as shown in this figure if there are no clean validation samples then the trained models cannot generalize beyond the original weak labels</sample>
    <sample id="923">meaning that the training is pointless</sample>
    <sample id="924">this indicates that wsl approaches actually require cleanly labeled data to work properly and the annotation cost for obtaining clean validation samples should not be overlooked</sample>
    <sample id="925">our second finding is that increasing the number of clean validation samples will help wsl approaches to achieve better performance as shown in the figure on the left</sample>
    <sample id="926">typically, we only need 20 samples per class to achieve high performance.</sample>
    <sample id="927">but that's not the end of the story because if we either way decide to access clean samples then training on them directly will even achieve better performance</sample>
    <sample id="928">the right figure shows the performance difference between functioning approaches which are directly applied on the clean data and wsl approaches which use the clean data for validation only</sample>
    <sample id="929">as we can see if we have 10 samples per class direct fine-tuning starts to beat wsl approaches</sample>
    <sample id="930">finally the performance improvement claimed in previous wsl approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples</sample>
    <sample id="931">as we can see from the figures the malina model termed ftw initially underperforms more complicated wsl methods like cosine</sample>
    <sample id="932">however if we allow to continue fine-tuning on the clean samples then ftw performs equally well as other methods</sample>
    <sample id="933">in pratica non c'è motivo di scegliere metodi wsl più complessi che richiedono più tempo di calcolo e spazio su disco.</sample>
    <sample id="934">to summarize we showed that recent wsl approaches require clean manually annotated samples for them to work properly their performance gain and practicality are heavily overestimated</sample>
    <sample id="935">le nostre raccomandazioni concrete per il lavoro futuro sono le seguenti.</sample>
    <sample id="936">first report the model selection criteria for example report if the model selection is done with clean validation samples</sample>
    <sample id="937">second wsl approaches should be compared with few-shot learning baselines as both work on few samples third continuous fine-tuning is a simple yet strong baseline that should be considered in future work in wsl</sample>
    <sample id="938">finally we have open source our code you can find it via the qr code on this slide please feel free to check it out thank you and enjoy the conference</sample>
    <sample id="939">the common practice is to use human evaluation such as by asking human judges to select which of two conversations is better or to rate conversations given a likert scale</sample>
    <sample id="940">hi everyone i'm jenny a first-year phd student at carnegie mellon university and today i'll be presenting our work on the positionality characterizing design biases of data sets and models this work was done in collaboration with some folks at the university of washington and the allen institute for ai namely sebastian santi roland labrosse katarina arinica and martin sapp</sample>
    <sample id="941">here is an example from our dataset servin is a judge kea is a baker servin and kea met at a park after a long day at work deciding cases in a law court he was happy to relax the task here is to identify the correct entity that the pronoun he refers to which in this case is servin the resolution of a given pronoun requires two types of information first entity specific knowledge such as servin is a judge and second background knowledge such as judges decide cases in law courts generally background knowledge is learned during the pre-training of large language models while entity specific knowledge is typically observed at inference time we vary the availability of these two pieces of information such that it may either be found in a single source or in multiple sources</sample>
    <sample id="942">still even the best performing models seem to have difficulties with reliably integrated background knowledge presented only at inference time if you're interested in more details please see our paper and check out the dataset and code on github thanks for listening</sample>
    <sample id="943">we also find most additional alignment with people who have a college education so for gpt-4 in the social acceptability task we find that it's most aligned to people with a college education or graduate school education</sample>
    <sample id="944">so why does the match prefix affect the language model judgment so much so we did a series of analysis where we tried to like perturb the input sentence by trying to preserve the relevant structure by adding like noise to the input and after doing like several of these perturbations we find that none of these noises are actually making the model like change its course in terms of how it shows us the mpp judgments basically we find that the models are sensitive to the perturbations sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the not acceptable domain we see decrease in mpp judgments in similar fashion</sample>
    <sample id="945">these approaches work well to provide holistic evaluations of overall dialogue quality but dialogue quality has many aspects therefore you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer grained level</sample>
    <sample id="946">hello everyone my name is jingwei yi from the university of science and technology of china it's my pleasure to give a short advertisement video of our paper i will copy my model protecting the copyright of large language models for embedding and services we will back the watermark</sample>
    <sample id="947">we saw that the actual form of the prompting doesn't have a big influence in in the case of several shot prompting it's crucial for zero and one shot prompting and when we go as in our case to five shot prompting there is nearly no difference to the actual form of the of the prompting</sample>
    <sample id="978">these reliable informative and distinct abc eval metrics enable us to evaluate conversational ai with a higher resolution than previous methods are able to achieve you can see that in the results of our experiment that several challenges still remain and have been precisely quantified for example the bots we tested have common sense violations in around 20 of their responses they produce irrelevant information in around 15 of the responses and they contradict themselves or their partner around 10 of the time with the rapid pace of improvement in the field many of these error rates could see a decrease in new models released since our evaluation was conducted however this is all the more reason to pursue reliable and precise evaluation metrics for comparing models we hope abc eval can be leveraged by others in the field as a meaningful step in this direction and we look forward to seeing how conversational ai will advance in the coming months and years thank you for watching</sample>
    <sample id="979">hello everyone my name is jingwei yi from the university of science and technology of china it's my pleasure to give a short advertisement video of our paper i will copy my model protecting the copyright of large language models for embedding and services we will back the watermark</sample>
    <sample id="980">which impose different constraints on the goal-oriented planning an abstract goal can be inherited by different real-life specific goals with multi-faceted constraints a good planner should write scripts that are reasonable and faithful to constraints</sample>
    <sample id="981">hi i'm sijun from the university i'm here to introduce our work distinguishing script knowledge from large language models for constrained language planning</sample>
    <sample id="982">hello my name is vasudha and i'm a computer science phd candidate at stony brook university i would like to present our work accepted into acl 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge</sample>
    <sample id="983">hi my name is adam sperkowski and this talk is about the dependency structure of coordination</sample>
    <sample id="1021">in particular the most common error are omission errors</sample>
    <sample id="1022">hello i'm james finch and i'm sarah finch and today we'll tell you all about abc eval a new dimensional approach to evaluating conversational ai</sample>
    <sample id="1023">this work was done by the emory nlp lab led by professor gino choy at emory university and in collaboration with amazon alexa ai</sample>
    <sample id="1024">so let's say that you just developed a dialogue model and you want to see how well it compares against the current state of the art</sample>
    <sample id="1025">the common practice is to use human evaluation such as by asking human judges to select which of two conversations is better or to rate conversations given a likert scale</sample>
    <sample id="1026">these approaches work well to provide holistic evaluations of overall dialogue quality but dialogue quality has many aspects therefore you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer grained level</sample>
    <sample id="1027">un approccio è quello di chiedere semplicemente a giudici umani di valutare diverse dimensioni della qualità del dialogo come la rilevanza delle risposte del modello utilizzando metodi esistenti di scala comparativa o likert.</sample>
    <sample id="1028">however we believe there is a more precise and reliable strategy for dimensional dialogue evaluations</sample>
    <sample id="1029">our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself</sample>
    <sample id="1030">we call this approach annotating behaviors in chat or abc eval in short we developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature</sample>
    <sample id="1031">abc eval is capable of measuring the rates at which chat models will commit various thematic errors</sample>
    <sample id="1032">per esempio, apc-eval misura il numero di turni in cui un modello di chat ignora il suo partner o dice qualcosa di irrilevante.</sample>
    <sample id="1033">contraddice se stesso o il suo partner allucina fatti errati o viola la conoscenza comune e quando il modello riesce o fallisce a mostrare empatia</sample>
    <sample id="1034">to determine what kind of evaluation is most effective we selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using abc eval</sample>
    <sample id="1035">for comparison we also evaluated these conversations using three existing methods liquet ratings on the turn level liquet ratings on the dialogue level and dialogue level pairwise comparisons</sample>
    <sample id="1036">for each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue since this is the standard practice for evaluating chat models along multiple dimensions.</sample>
    <sample id="1037">from our analyses of these evaluation results we found that abc behavior labels are overall more reliable than labels collected by existing methods as measured by inner annotator agreement on 100 doubly labeled conversations</sample>
    <sample id="1038">in addition abc eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods as shown by this simple linear regression analysis</sample>
    <sample id="1039">for example you can see how measuring the proportion of turns with self and partner contradictions explains 5 and 10 of conversation quality respectively while the average liquor consistency scores explain only 4 or less</sample>
    <sample id="1040">finally we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression</sample>
    <sample id="1041">you can see how the combination of all abceval metrics explains over 25 of conversation quality and as you remove the metrics one at a time most of them result in losing a decent amount of information about the quality</sample>
    <sample id="1042">on the other hand the combination of all turn level likert metrics explains far less of the quality and fewer of these metrics carry unique information</sample>
    <sample id="1043">these reliable informative and distinct abc eval metrics enable us to evaluate conversational ai with a higher resolution than previous methods are able to achieve</sample>
    <sample id="1044">you can see that in the results of our experiment that several challenges still remain and have been precisely quantified for example the bots we tested have common sense violations in around 20 of their responses</sample>
    <sample id="1045">they produce irrelevant information in around 15 of the responses and they contradict themselves or their partner around 10 of the time</sample>
    <sample id="1046">with the rapid pace of improvement in the field many of these error rates could see a decrease in new models released since our evaluation was conducted however this is all the more reason to pursue reliable and precise evaluation metrics for comparing models</sample>
    <sample id="1047">we hope abc eval can be leveraged by others in the field as a meaningful step in this direction and we look forward to seeing how conversational ai will advance in the coming months and years thank you for watching</sample>
    <sample id="1048">this work was done by the emory nlp lab led by professor gino choy at emory university and in collaboration with amazon alexa ai</sample>
    <sample id="1049">to summarize we showed that recent wsl approaches require clean manually annotated samples for them to work properly their performance gain and practicality are heavily overestimated our concrete recommendations for future work are as follows first report the model selection criteria for example report if the model selection is done with clean validation samples second wsl approaches should be compared with free short learning baselines as both work on clean samples third continuous fine-tuning is a simple yet strong baseline that should be considered in future work in ws finally we have open source our code you can find it via the qr code on this slide please feel free to check it out thank you and enjoy the conference</sample>
    <sample id="1050">this is a joint work with john gauthier aaron mueller kanishka misra karen fuentes roger levy and etina williams</sample>
    <sample id="1051">hello my name is kayo yen and i will be presenting our work titled when does translation require context a data-driven multilingual exploration this work was done in collaboration with patrick frenoux emilie yu andrew ft martins and graham newbigg</sample>
    <sample id="1052">so a lot of translations depend on context for example how would we translate more in this sentence</sample>
    <sample id="1053">well if the previous sentence was things could start to get dangerous if the ministers find out then mole refers to a spy but if the previous sentence was could it be anything serious doctor then mole refers to a birthmark</sample>
    <sample id="1054">so depending on the context, the meaning of the word changes and therefore its translation changes as well.</sample>
    <sample id="1055">however evaluating how well models can handle cases like this is pretty hard firstly because only a small portion of translations depend on context which makes corpus-level metrics like bleu unable to capture these translations</sample>
    <sample id="1056">and some people have suggested targeted evaluation on context-dependent translations but these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation</sample>
    <sample id="1057">in this work we try to answer these two questions first when does translation require context and second how well do models handle these cases</sample>
    <sample id="1058">to answer the first question we started by measuring how much a word depends on context during translation</sample>
    <sample id="1059">in the previous work we introduced cxi as a measure for context usage by machine translation models and this is done by measuring how much information the context c provides about the target y given the source x</sample>
    <sample id="1060">you can think of cxi as the information gained from giving context to the model</sample>
    <sample id="1061">in this work we extend cxmi to pointwise cxmi which can measure context usage at the sentence level or at the word level we can think of words that have high pxmi as ones that require context for translation</sample>
    <sample id="1062">ora analizziamo le parole con high piece xmi per cercare modelli tra queste parole.</sample>
    <sample id="1063">and we perform our analysis on transcripts of ted talks that have been translated from english to 14 different languages</sample>
    <sample id="1064">we perform our analysis at three different levels first we look at part-of-speech tags that have high means psexmi</sample>
    <sample id="1065">and this allows us to find for example dual pronouns in arabic that have relatively high xmi and this can be explained because english doesn't have dual pronouns so you need context to determine if a pronoun is dual when translating into arabic</sample>
    <sample id="1066">and similarly we find that certain languages also require context when we want to choose the appropriate verb form we then look at vocabulary items that have high psexmi averaged over all of its different occurrences</sample>
    <sample id="1067">and this helps us identify cases like the one here where in chinese you need context to translate proper nouns to make sure that you're using the same translation within the document</sample>
    <sample id="1068">and similarly we find that context is supported to translate in the right formality</sample>
    <sample id="1069">and finally we look at different individual tokens that have high p6mi and this allows us to identify phenomena that cannot really be captured by the word itself but that's rather expressed in the sentence structure such as ellipsis resolution</sample>
    <sample id="1070">ora utilizziamo i nostri risultati dall'analisi per progettare un benchmark per la traduzione a livello di documento.</sample>
    <sample id="1071">for each of the five discourse phenomena we identified, we created tags to automatically identify words that pertain to the phenomenon, and we call our tagger the multilingual discourse aware or muda tagger.</sample>
    <sample id="1072">we can then also note that different languages have different proportions of these discourse phenomena</sample>
    <sample id="1073">we then use the muda tagger by applying the tagger on the parallel corpus that we want to use for evaluation and we apply our translation metrics of choice on the context-dependent examples that the muda tagger has identified</sample>
    <sample id="1074">and finally we use our benchmark as well as other metrics to evaluate different models on document-level machine translation</sample>
    <sample id="1075">first of all when we use corpus-level metrics so for blue we find that context-agnostic models have the best performance</sample>
    <sample id="1076">but then if we use comet context-aware models perform best and if we use wordf measure then models with and without context have comparable performance</sample>
    <sample id="1077">this again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone.</sample>
    <sample id="1078">now we use the moodle benchmark to evaluate models and we find that context-aware models are significantly more accurate than models that do not use context for certain discourse phenomena such as formality and lexical cohesion</sample>
    <sample id="1079">but these models are not much better than models that do not use context on other phenomena like ellipsis pronouns and verb form so this sort of suggests where we would need to see more progress for document level translation</sample>
    <sample id="1080">we also compared different commercial systems and our benchmark shows that deepbel is usually more accurate than google translate for document-level translations</sample>
    <sample id="1081">to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context</sample>
    <sample id="1082">and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation</sample>
    <sample id="1083">thank you so much for your attention see you in toronto</sample>
    <sample id="1084">hello everyone my name is yixin john from the penn state university today i'm going to present our work exemplar cross-lingual semantic parsing in multiple natural languages and many representations</sample>
    <sample id="1121">until every token from the first stage has been visited exactly once</sample>
    <sample id="1122">the second part is marked words which is a method to identify the words that distinguish marked groups are marked ones which i'll elaborate on shortly</sample>
    <sample id="1123">hi i'm john green phd student at the university of washington today i'm presenting our work from pre-training data to language models to downstream tasks tracking the trails of political biases leading to unfair nlp models</sample>
    <sample id="1124">now there are also symmetric approaches to coordinate structures such as the plug approach the conjunction-headed approach assumed in plug-dependency tree banks where coordinate structures are headed by the conjunction</sample>
    <sample id="1125">hello i'm james finch and i'm sarah finch and today we'll tell you all about abc eval a new dimensional approach to evaluating conversational ai</sample>
    <sample id="1126">my name is jawad hosseini and this is a joint work with philippe radlinski silvia parotti and annie lewis</sample>
    <sample id="1127">so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like plump syntax gem or acceptability in terms of stereotypes such as cross pairs</sample>
    <sample id="1161">we addressed these research questions in our work and our findings are as follows first we find that interestingly recent wsl methods indeed require clean validation samples to work properly otherwise there is a large performance drop as shown in this figure if there are no clean validation samples then the trained models cannot generalize beyond the original weak labels meaning that the training is pointless this indicates that wsl approaches actually require cleanly labeled data to work properly and the annotation cost for obtaining clean validation samples should not be overlooked over second finding is that increasing the number of clean validation samples will help wsl approaches to achieve better performance as shown in the figure on the left typically we only need 20 samples per class to attain high performance</sample>
    <sample id="1162">we also introduce a comparison of model with multiple pretraining settings and data sources then we present our results on 11 biomedical and clinical downstream tasks in french</sample>
    <sample id="1226">however our experiment on continuum pretraining using the weight and tokenizer of permbert trained on the 4gb subset of nacl shows comparable results to those obtained with drbert 4gb from scratch</sample>
    <sample id="1227">hi my name is adam sperkowski and this talk is about the dependency structure of coordination</sample>
    <sample id="1228">for temporal drift we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift</sample>
    <sample id="1269">after the first step we have all the right tokens but they're not ordered that's why in the second step we use another model to predict a permutation to put them into the right order</sample>
    <sample id="1270">and finally there should really be increased transparency about bias mitigation methods because for instance like these positive stereotypes we don't know if it's because there's some sort of like weird overly excessive value alignment going on or maybe some other like anti-stereotyping methods that are resulting in these pernicious patterns</sample>
    <sample id="1271">and in this minimal pair paradigm the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an unacceptable sentence or an ungrammatical sentence and then the hope is that the model basically puts more probability to the acceptable sentence</sample>
    <sample id="1272">however our experiment on continual pretraining using the weight and tokenizer of permute-bert trained on the 4 gb subset of natural showed comparable results to those obtained with dr bert 4 gb from scratch</sample>
    <sample id="1273">from our analyses of these evaluation results we found that abc behavior labels are overall more reliable than labels collected by existing methods as measured by inner annotator agreement on 100 doubly labeled conversations in addition abc eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods as shown by this simple linear regression analysis</sample>
    <sample id="1274">finally we can choose sentences from a completely unrelated domain such as wikipedia so this will tell us like whether the model's acceptability judgments are actually impacted by any context</sample>
    <sample id="1275">hi welcome to our presentation of deplain a new corpus for german text simplification on the document level and on the sentence level my name is regina stodden and i will guide you through the first part of the presentation let's first define text simplification</sample>
    <sample id="1276">however most previous works on instruction tuning focus on improving the zero-shot performance on language-only tasks while computer vision and multimodal tasks have been left out therefore in this work we want to investigate whether instruction tuning on multimodal pre-trained models can actually improve generalization to unseen multimodal tasks additionally at the time of our research we discovered a considerable discrepancy in availability of instruction data set between lp and multimodal there exists more than 1600 language-only instruction tasks however there is no large scale publicly available multimodal instruction tasks therefore this motivates us to build a multimodal instruction tuning data set</sample>
    <sample id="1277">hello i'm james finch and i'm sarah finch and today we'll tell you all about abc eval a new dimensional approach to evaluating conversational ai this work was done by the emory nlp lab led by professor gino choi at emory university and in collaboration with amazon alexa ai</sample>
    <sample id="1278">so we showed that by measuring length in characters that's the first column in syllables the middle column and in words the right column so i'll concentrate on the right one</sample>
    <sample id="1279">so really just only the positive or at least non-negative ones</sample>
    <sample id="1280">with that said t5's unsupervised rate can generate scraps of higher quality than most large language models indicating that smaller models can surpass large language models when properly trained on suitable data sets</sample>
    <sample id="1281">hi i am yannis lavrac and i will present you our works on dr bert a robust pre-trained model in french for biomedical and clinical domains</sample>
    <sample id="1282">in this presentation, we first talk about language modeling in healthcare, then we will present the main contribution of our article.</sample>
    <sample id="1283">we introduced the first biomedical model uh in french named dr bert which is based on roberta and trained on natasha which is a dataset of medical ground data from the web</sample>
    <sample id="1284">we also introduce a comparison of model with multiple pre-training settings and data sources then we present our results on 11 biomedical and clinical downstream tasks in french</sample>
    <sample id="1285">finally we conclude about the experiments and give you more details about how to access to the models</sample>
    <sample id="1286">since its release in 2018 bert has become one of the most effective approach to solve natural language processing tasks and offer huge performance gain compared to historical static and contextualized methods such as word2vec fasttext or elmo</sample>
    <sample id="1287">since then this model has been adapted to many other languages like in french with camembert and other domains like biomedical with permabird and biobird and on clinical with clinical bird but mostly in english</sample>
    <sample id="1288">specialized models for other languages are scarce and are often based on continual pre-training due to the lack of in-domain data</sample>
    <sample id="1289">tuttavia, il francese non aveva alcun modello open source per il bioamericano e il chileno.</sample>
    <sample id="1290">we so we ask ourselves question about what is the most appropriate data source for a wide range of usage and those raw data are a good substitution for clinical data</sample>
    <sample id="1291">to answer this question we compare dr bert with our schubert model which is based on anonymized data obtained from the non-university hospital that we have</sample>
    <sample id="1292">afterward we ask ourselves how much data do we need to train a specialized model on french data is it 4gb 8gb or more</sample>
    <sample id="1293">to answer this question we first train and compare four from scratch model a first version of dr bert with seven gigabytes of nachos a second version of four gigabytes of set of nachos</sample>
    <sample id="1294">a first version of schubert which is a clinical uh model with 4 gigabytes of sentences taken from clinical notes and a final version of schubert with a mix of 4 gigabytes of natural sentences and 4 gigabytes of clinical notes</sample>
    <sample id="1295">in addition to this comparison we introduced three models trained on continuous pre-training to analyze the impact of pre-training strategies</sample>
    <sample id="1296">one based on the weight of camomile and trained on four gigabytes of set of nachos another also based on camomile but trained this time on the four gigabytes of clinical notes</sample>
    <sample id="1297">and finally one based on english biomedical model bert and trained on 4 gigabytes of set of snapshots in total we have seven models</sample>
    <sample id="1298">to evaluate our seven models we gathered which port public and private don't use tasks such as name entity recognition classification part of speech tagging and question answering</sample>
    <sample id="1299">this model are compared to six baseline model which are camber oscar 138 gigabytes camber oscar 4 gigabytes camber cisnet 4 gigabytes and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and clinicalbert and</sample>
    <sample id="1300">the evaluation highlights that the model performs best on the task with data of the same nature as those on which the model has been trained</sample>
    <sample id="1301">however we have we can obtain that data from uh we can observe that data from heterogeneous sources appear to be more versatile we also observe that using more data translate into better performance</sample>
    <sample id="1302">in general, from-scratch training seemed to obtain higher performance on most of the tasks.</sample>
    <sample id="1303">however our experiment on continuous pretraining using the weight and tokenizer of permute-bert trained on the 4gb subset of natural show comparable results to those obtained with dr bert 4gb from scratch</sample>
    <sample id="1304">questo non è il caso per i modelli basati su pesi di cambridge e tokenizers, che soffrono di problemi di stabilità.</sample>
    <sample id="1305">finally as a conclusion uh uh our proprietary system offered better performance on nine of the 11 downstreams tasks and surpassed globally the result of the generic model here camomile</sample>
    <sample id="1306">we also observe that specialized data is better more specialized data is better but it doesn't scale well</sample>
    <sample id="1307">all the pre-trained models obtained from nachos are freely available on yugenface and all the training scripts are on our github repository</sample>
    <sample id="1308">so thank you for this presentation and we are looking forward to actions at the poster session in 2022</sample>
    <sample id="1309">to answer this question we first train and compare four from scratch model a first version of dr bert with 7 gigabytes of natchez a second version of 4 gigabytes of set of natchez a first version of schubert which is a clinical uh model with 4 gigabytes of sentences taken from clinical notes and a final version of schubert with a mix of 4 gigabytes of set of natchez and 4 gigabytes of clinical notes in addition to this comparison we introduce three model trained on continuous pre-training to analyze the impact of pre-training strategy</sample>
    <sample id="1310">for adaptive overfitting we saw that from the graph on the right the red best fit line has a gradient that is greater than one this means that every unit of improvement that we made on colonel 2003 translates to more than one unit improvement on colonel which means that there is no diminishing returns and this shows us that adaptive overfitting in this case is not observed</sample>
    <sample id="1311">the second use case that we showed in our paper is the case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text we have fine-tuned two different models we have fine-tuned the model of long impart to produce document-level simplifications and we also fine-tuned the normal base long the normal base impart to produce sentence-level simplifications you can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper we concluded that this basic fine-tuning could produce or could get scores better than the baseline scores and we propose those results as a benchmark a baseline benchmark for the problem of automatic text simplification in the future</sample>
    <sample id="1312">so some preliminary results demonstrate that first language models do have varying political leanings they occupy all four quadrants on the political compass we can also see that gpt-4 is the most liberal language model of them all and gpt-3 is generally more socially liberal than bird theory and its variants</sample>
    <sample id="1313">hi my name is mathias lendemann and today i'm going to give you a brief introduction to our paper on compositional generalization without trees using multiset tagging and latent permutations</sample>
    <sample id="1314">this is joint work with my advisors alexander kolar and ivan titov.</sample>
    <sample id="1315">compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training</sample>
    <sample id="1316">in the context of semantic parsing testing for compositional generalization might look like this as usual we have a training set of utterances in this case the girl slept and mary knew that the girl slept</sample>
    <sample id="1317">these utterances are paired with logical forms that represent core aspects of their meaning</sample>
    <sample id="1318">in contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms.</sample>
    <sample id="1319">in this example, the model has seen shallow recursion during training and is tested on an example with deeper recursion.</sample>
    <sample id="1320">i modelli sequenza-a-sequenza ingenui faticano con questo tipo di generalizzazione fuori dalla distribuzione e spesso producono output che sono distaccati dall'input.</sample>
    <sample id="1321">in particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle che sono colorate nell'esempio.</sample>
    <sample id="1322">un metodo popolare per affrontare questo è integrare gli alberi nei modelli.</sample>
    <sample id="1323">the trees are intended to capture the compositional process that relates utterances with their logical forms.</sample>
    <sample id="1324">questo funziona bene, ma gli alberi di solito non vengono dati e devono essere ottenuti in qualche modo.</sample>
    <sample id="1325">questo può essere complesso e talvolta un processo computazionalmente costoso tipicamente questo comporta un notevole pre-elaborazione specifica per il formalismo, ad esempio per gestire i simboli delle variabili.</sample>
    <sample id="1326">ottenere alberi può anche coinvolgere procedure specializzate di induzione della grammatica.</sample>
    <sample id="1327">in this paper we don't use trees and introduce a neural sequence-to-sequence model that directly models the correspondences between fragments of the input and fragments of the output</sample>
    <sample id="1328">for the first time we show strong generalization to deeper recursion without relying on trees.</sample>
    <sample id="1329">il nostro approccio prevede l'output dall'input in due passaggi.</sample>
    <sample id="1330">prima di tutto, etichettiamo ogni token di input con un insieme multiordinato di token che appariranno nell'output.</sample>
    <sample id="1331">after the first step, we have all the right tokens, but they are not ordered.</sample>
    <sample id="1332">that's why in the second step we use another model to predict a permutation to put them in the right order</sample>
    <sample id="1333">introduciamo un nuovo metodo per prevedere una permutazione che non impone alcuna restrizione dura sulle possibili permutazioni questo rende il nostro approccio molto flessibile ed espressivo.</sample>
    <sample id="1334">conceptualmente, il nostro modello di permutazione funziona approssimativamente così.</sample>
    <sample id="1335">we go from left to right over the output and determine which multi-set token to put in every position for the first output position we simply select one as highlighted in red</sample>
    <sample id="1336">poi saltiamo al prossimo token multiset per determinare il secondo token nell'output.</sample>
    <sample id="1337">determiniamo il terzo token nell'output in modo simile saltando a un altro token multiset</sample>
    <sample id="1338">fino a quando ogni token dalla prima fase non è stato visitato esattamente una volta.</sample>
    <sample id="1339">to give you a teaser of the experimental results here we compare our method with other treeless models on the cogs benchmark our model outperforms the others by a large margin on generalization to deeper recursion</sample>
    <sample id="1340">alcuni altri tipi di generalizzazione strutturale rimangono molto impegnativi, tuttavia.</sample>
    <sample id="1341">nel nostro articolo risolviamo alcuni interessanti problemi tecnici.</sample>
    <sample id="1342">first of all the alignment between input and output is not given in the training data as a consequence for a given token we don't know which multi-setter it came from which poses a challenge for training</sample>
    <sample id="1343">in addition sometimes there are multiple permutations that are consistent with the data but the linguistically correct one is latent we address this by inducing the alignment as part of the training</sample>
    <sample id="1344">our permutation method is very flexible but it brings the challenge that finding the highest scoring permutation is np-hard that's because this is related to the traveling salesman problem</sample>
    <sample id="1345">approximate this with a gpu-friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations</sample>
    <sample id="1346">if you want to learn more about our experiments and how we address these challenges please have a look at our paper or come to our poster</sample>
    <sample id="1347">we begin by defining cognitive dissonance and why it is an important problem to study in language simply put cognitive dissonance is two beliefs or actions that are inconsistent</sample>
    <sample id="1348">we can also see that gpt-4 is the most liberal language model of them all and gpt theories are generally more socially liberal than bert theories and its variants</sample>
    <sample id="1349">over the different strategies we found that cumulative performed equal or better than iterative across the board</sample>
    <sample id="1350">hi i'm sarah papi from the university of toronto and funded by bruno kessler and i will briefly introduce the attention as a guide for simultaneous speech translation paper that is a joint work with macdonald and marco d'orci rispondi in modo conciso alla seguente domanda dato il contenuto inglese</sample>
    <sample id="1351">and we perform our analysis on transcripts of ted talks that have been translated from english to 14 different languages</sample>
    <sample id="1385">hi my name is mathias lendemann and today i'm going to give you a brief introduction to our paper on compositional generalization without trees using multiset tagging and latent permutations</sample>
    <sample id="1386">and we also consider cross-lingual zero-shot and few-shot transfer we train on one source language and transfer to another language so during training we'll train it on english query or the combination of english and german few-shot queries to train a multilingual model to and predict the sql output</sample>
    <sample id="1387">hello i am da wei a phd student at salant university in germany in this video i would like to present our recent work weaker than you think a critical look at weakly supervised learning this is joint work with xiaoyu shen mario smusbaugh and geoffrey stephens and david clark</sample>
    <sample id="1388">we'll plot the simultaneous speech translation results on graphs in which we have blue on one side that measures the translation quality and average lagging that is the latency measure and we also consider the computational aware average lagging that accounts for the model's computational time to predict the output</sample>
    <sample id="1389">hello everyone i'm mark shatta and today my co-author martin and i are presenting our work the kit must have evaluating knowledge integration from multiple sources this work is a collaboration between mcgill university nila and microsoft research</sample>
    <sample id="1390">natural language understanding models draw on a variety of knowledge sources such as knowledge contained in their parameters usually acquired via pre-training and knowledge given in inputs at inference time</sample>
    <sample id="1391">recent works in tasks like question answering show that models can use pre-trained knowledge to solve the task.</sample>
    <sample id="1392">but natural language understanding often requires knowledge that is also supplied at inference time</sample>
    <sample id="1393">per esempio, nella frase john saw the newly elected president on tv.</sample>
    <sample id="1394">pre-trained parameters can contain information about what a president does and what a tv is, but they cannot reliably know who this instance-specific entity john is or who the new president is, because the president might have changed since pre-training.</sample>
    <sample id="1395">pertanto, modelli di successo per compiti nlu a conoscenza intensiva richiedono la capacità di integrare e utilizzare sia conoscenze pre-addestrate che conoscenze di inferenza.</sample>
    <sample id="1396">in this work, we propose a diagnostic test suite for knowledge integration.</sample>
    <sample id="1397">we introduce a co-reference resolution task designed to probe for the ability to draw on knowledge available in different sources we evaluate the dataset with human study participants and establish co-reference resolution models</sample>
    <sample id="1398">here is an example from our dataset servin is a judge kia is a baker servin and kia met at a park after a long day at work deciding cases in a law court he was happy to relax</sample>
    <sample id="1399">il compito qui è identificare l'entità corretta a cui si riferisce il pronome egli che in questo caso è il servitore.</sample>
    <sample id="1400">the resolution of a given pronoun requires two types of information first entity-specific knowledge such as servin is a judge and second background knowledge such as judges decide cases in law courts</sample>
    <sample id="1401">generally background knowledge is learned during the pre-training of large language models while entity-specific knowledge is typically observed at inference time</sample>
    <sample id="1402">we vary the availability of these two pieces of information such that it may either be found in a single source or in multiple sources</sample>
    <sample id="1403">we have defined three settings of kitmos first we have the typical setting background pre-train where background knowledge is assumed to be available at pre-train time</sample>
    <sample id="1404">secondly there's the background both setting where background knowledge is available both at pre-training time and at inference time lastly the background inference setting with both knowledge types available only at inference time</sample>
    <sample id="1405">this last setting is especially interesting since it simulates the case where the background knowledge necessary to solve a task is not part of the pre-trained data of models for example because new occupations have developed since the time of pre-training</sample>
    <sample id="1406">here's an example of how we control the availability of facts and true sources</sample>
    <sample id="1407">in the background pre-trained setting we assume that the background knowledge politicians seek elected seats in government is contained in the pre-trained parameters in the three-sentence context we provide the anti-specific knowledge chichester is a politician</sample>
    <sample id="1408">in the background both setting we additionally provide not only entity-specific but also background knowledge about politicians in the inferential context</sample>
    <sample id="1409">in a background and fictional setting we provide the fictional occupation meritour instead of politician because meritour is unlikely to be contained in the pre-training period</sample>
    <sample id="1410">we evaluate the dataset both with human study participants and establish coreference resolution models in this figure we show the results of the best-performing models on the most difficult variant of the background pre-trained setting</sample>
    <sample id="1411">without task-specific training on kitmos both models do not perform well when trained on kitmos however both c2f and bird4cof perform significantly better than the random choice</sample>
    <sample id="1412">this suggests that when trained on k-genetic reference resolution data sets models learn to exploit surface cues which are not useful when testing on chitmos where such cues have been removed</sample>
    <sample id="1413">esperimenti aggiuntivi con conoscenza fittizia indicano che anche i modelli più performanti non possono integrare conoscenza fornita solo a tempo di inferenza.</sample>
    <sample id="1414">to summarize the main takeaways of our paper many co-reference resolution models appear unable to reason over knowledge from different sources without task-specific training however with task-specific training some models successfully integrate knowledge from multiple sources</sample>
    <sample id="1415">still even the best-performing models seem to have difficulties with reliably integrated background knowledge presented only at inference time if you're interested in more details please see our paper and check out the dataset and code on github thanks for listening</sample>
    <sample id="1416">this works well but trees are usually not given and need to be obtained somehow this can be complicated and sometimes a computationally expensive process typically this involves considerable formalism-specific pre-processing of the logical forms for example to handle variable symbols obtaining trees may also involve specialized grammar induction procedures</sample>
    <sample id="1417">hello everyone my name is xuhong today i'm going to present our paper do convolution 2003 named entity taggers still work well in 2023 let's get started</sample>
    <sample id="1418">hi i'm myra and today i'll be talking about our paper marked personas using natural language prompts to measure stereotypes in language models this work is done in collaboration with esendermush and danjurovski</sample>
    <sample id="1419">negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei grandi modelli di linguaggio o lms.</sample>
    <sample id="1420">however, these measures have various limitations they usually rely on hand-constructed datasets that are very time-consuming to curate.</sample>
    <sample id="1421">and they also usually only measure very specific stereotypes meaning that they don't generalize well to other demographics or contexts or they simply capture very general broad associations like negative associations with particular groups</sample>
    <sample id="1422">furthermore most work in this space doesn't account for intersectionality which is the notion that multifaceted social identities can compound biases and be unique loci of harm</sample>
    <sample id="1423">to overcome these limitations, we rely on the property that these newer instruction-tuned lms are very good at responding to instructions and prompts.</sample>
    <sample id="1424">so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself</sample>
    <sample id="1425">and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt</sample>
    <sample id="1426">here are some example generations from gpt-4</sample>
    <sample id="1427">immediately we see that while the outputs aren't overtly negative or toxic in the traditional sense of these words</sample>
    <sample id="1428">ci sono alcuni modelli interessanti.</sample>
    <sample id="1429">the asian woman is depicted as unassuming the middle eastern woman is referred to using words like exotic and like referring to a mesmerizing region</sample>
    <sample id="1430">and both of the women of color personas make references to ancestry while the white man persona has nothing of the sort</sample>
    <sample id="1431">to capture these patterns our method has two parts the first one is generating these personas</sample>
    <sample id="1432">our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects finding that by giving it to human subjects they also were able to surface racial stereotypes</sample>
    <sample id="1433">and also this enables direct comparison between our generated personas and the human-written responses</sample>
    <sample id="1434">the second part is marked words which is a method to identify the words that distinguish marked groups or marked ones which i'll elaborate on shortly</sample>
    <sample id="1435">the benefit of this is that we get really specific stereotypes and patterns without having to rely on any specific lexicon</sample>
    <sample id="1436">so the marked words method draws upon the sociolinguistic concept of markedness which states that there is an unmarked default and any group that differs from that default is linguistically marked</sample>
    <sample id="1437">so for instance the word man or sorry the word warrior is usually associated with men so when people are describing a warrior who is a woman they'll usually actually specify woman warrior and mark the term with woman</sample>
    <sample id="1438">and more broadly dominant groups in society are both linguistically and socially unmarked while the marginalized groups are usually marked</sample>
    <sample id="1439">quindi nel nostro metodo prima designiamo quali sono i gruppi non segnati e segnati.</sample>
    <sample id="1440">and then we compare the personas using the fighting words method which is basically using weighted log odds ratios to distinguish the top words for each marked group</sample>
    <sample id="1441">so for instance for the personas of black women we would do fighting words and compare the law odds ratios against both white personas and man personas because those are the two corresponding unmarked groups</sample>
    <sample id="1442">and now for some results so first we use alexicon of stereotypes and we find that the generated personas contain a lot more stereotypes than the human-written ones</sample>
    <sample id="1443">tuttavia quando esaminiamo effettivamente la distribuzione delle parole nel lessico, troviamo cose molto diverse.</sample>
    <sample id="1444">so while the generated personas have much higher rates of the luxon words the human written ones have a much wider distribution of words while the stereotype words that are in the generated personas are really just the words tall and athletic</sample>
    <sample id="1445">so really just only the positive or at least non-negative ones</sample>
    <sample id="1446">and in fact this lexicon doesn't really capture many of the harmful patterns that we saw in the earlier slides well at all so instead to do that we'll turn to the results from our marked words method to show how these positive seeming words facilitate stereotypes and essentializing narratives</sample>
    <sample id="1447">in our analysis, we reveal how these seemingly positive portrayals reflect horrible patterns.</sample>
    <sample id="1448">first from mark groups the top words include things like culture tradition proud and exotic and these words define these groups only by their relationship to their identity and distinguish them as different from the white norm</sample>
    <sample id="1449">this contributes to a long legacy of discrimination and othering for these groups.</sample>
    <sample id="1450">furthermore there's a lot of common tropes that are reflected in these words especially for women of color so for example the words describing latina women include things like vibrant and curvaceous</sample>
    <sample id="1451">which connects to a trope of tropicalism for asian women the words are things like petite and delicate and silky</sample>
    <sample id="1452">which connects to a long history of asian women being hypersexualized seen as very docile and submissive and so on</sample>
    <sample id="1453">finally for black women we see that some of the top words are things like strong and resilient</sample>
    <sample id="1454">this connects to an archetype that people have called the strong black woman archetype and while it sounds like positive at first glance</sample>
    <sample id="1455">and there's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles</sample>
    <sample id="1456">rather than actually working towards changing those obstacles it puts pressure on those people to overcome them which leads to very negative health outcomes for these people among other harms</sample>
    <sample id="1457">and more broadly we find that the words for each marked group pretty much just reflect very essentializing narratives</sample>
    <sample id="1458">based on these patterns we conclude with three recommendations for model owners</sample>
    <sample id="1459">first we should as researchers be addressing positive stereotypes and essentializing narratives we should also be using intersectional lens to study biases and harms because there's a lot of things that might be overlooked if we don't do that</sample>
    <sample id="1460">e infine dovrebbe esserci una maggiore trasparenza riguardo ai metodi di mitigazione del bias.</sample>
    <sample id="1461">because for instance like these positive stereotypes we don't know if it's because there's some sort of like weird</sample>
    <sample id="1462">overly excessive value alignment going on or maybe some other like anti-stereotyping methods that are resulting in these pernicious patterns</sample>
    <sample id="1463">non possiamo fare alcuna ipotesi o studiare ulteriormente senza più trasparenza.</sample>
    <sample id="1464">thank you so much for listening have a good time</sample>
    <sample id="1465">hello everyone my name is jingwei yi from the university of science and technology of china</sample>
    <sample id="1466">it's my pleasure to give a short advertisement video of our paper i will copy my model protecting the copyright of large language models for embedding and services will back the watermark</sample>
    <sample id="1467">introduciamo prima il contesto sull'integrazione dei nostri servizi.</sample>
    <sample id="1468">currently large language models such as gpt-3 lama palm are exceptional in natural language understanding and generation</sample>
    <sample id="1469">embedding as services is one of the services built upon large language models to assist various nlp tasks</sample>
    <sample id="1470">for example openai offers a gpt-based embedding api</sample>
    <sample id="1471">however recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services therefore it is necessary to protect the copyright of embedding as services</sample>
    <sample id="1472">to protect the copyright of embedding services, one of the solutions is to embed a watermark in the provider's service and detect whether another service contains the watermark.</sample>
    <sample id="1473">the watermark method needs to meet the following properties first the method should be applicable to embedding services second the watermark should not degrade the utility of the provided embedding services</sample>
    <sample id="1474">third the watermark should be covert enough to the attacker or the attacker can remove the watermark easily</sample>
    <sample id="1475">finally the watermark needs to be transferable to the attacker's services during the model extraction process</sample>
    <sample id="1476">le opere esistenti possono essere ampiamente classificate in quattro categorie.</sample>
    <sample id="1477">tuttavia, questi metodi non sono applicabili all'embedding di servizi o mancano di trasferibilità.</sample>
    <sample id="1478">in this paper, we propose embedding marker, which is a backdoor-based watermark method applicable to embedding services.</sample>
    <sample id="1479">then let me introduce the details of our embedding marker embedding marker contains two main steps watermark injection and copyright verification</sample>
    <sample id="1480">prima di questi passaggi principali, selezioniamo prima un set di trigger, il set di trigger è un gruppo di parole in un intervallo di frequenza moderato.</sample>
    <sample id="1481">assumiamo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole in esso.</sample>
    <sample id="1482">in watermark injection we first define a target embedding when a user sends a sentence to the provider's service the provider counts the trigger number in the sentence</sample>
    <sample id="1483">the provided embedding is a weighted summation of the target embedding and the original embedding.</sample>
    <sample id="1484">the weight of the target embedding is proportional to the number of triggers in the sentence when the number of triggers in the sentence is greater than m the provided embedding is exactly equal to the target embedding</sample>
    <sample id="1485">la verifica del copyright è per rilevare se un modello dietro un altro servizio contiene il marchio d'acqua.</sample>
    <sample id="1486">we first construct a backdoor and benign dataset backdoor dataset contains sentences of which all words belong to the trigger set while all words in the sentences of benign dataset do not belong to the trigger set</sample>
    <sample id="1487">il fornitore richiede l'embedding dal servizio stealer con il dataset.</sample>
    <sample id="1488">the cosine and l2 similarity between the requested embedding and the target embedding are computed we compute the similarity difference between the nine and the backdoor dataset which is defined as delta cosine and the delta l2</sample>
    <sample id="1489">nel frattempo, applichiamo anche il test di ks e utilizziamo il suo valore p come terza matrice.</sample>
    <sample id="1490">we conduct experiments on four datasets: agnews mind sst2 and erasmus we assume the provider applied wikitext to the dataset to count word frequencies</sample>
    <sample id="1491">the results on four datasets show that our embedded marker can have great detection performance while keeping great utility for downstream tasks</sample>
    <sample id="1492">we also validate the covertness of the provided embedding by visualizing the embedding of sentences unfolded as a bipca the legend of the figures means the number of triggers in each sentence.</sample>
    <sample id="1493">as shown in the figures it's hard to distinguish between the factored embeddings and normal embeddings</sample>
    <sample id="1494">that's all thank you welcome to discuss with us</sample>
    <sample id="1495">we call this approach annotating behaviors in chat or abc eval in short we developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature</sample>
    <sample id="1496">our conclusion is that for good generalization we would need a better model architecture larger model size as well as more fine tuning examples and these goes hand in hand we can't just have one ingredient but throw out the others at the same time we also found that the performance drop here is caused by temporal drift and kind of surprisingly it is not caused by adaptive overfitting even though conll 2003 has been used for over 20 years so going back to the question that we raised in the title of our paper do conll 2003 taggers still work in 2023 and we found that the answer is actually a resounding yes we hope our paper calls for more research on how to improve generalizations of the models</sample>
    <sample id="1497">hello my name is vasudha and i'm a computer science phd candidate at stony brook university i would like to present our work accepted into acl 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge</sample>
    <sample id="1498">we begin by defining cognitive dissonance and why it is an important problem to study in language simply put cognitive dissonance is two beliefs or actions that are inconsistent</sample>
    <sample id="1499">such as this example where a person states i know that cigarettes could kill me and then goes on to say i grabbed a couple of smokes after the meeting this belief and action are inconsistent and they are in dissonance</sample>
    <sample id="1500">further mentioning that i don't think i could keep my job without them justifies the second occurrence and they have a consonance relationship</sample>
    <sample id="1501">while dissonance is a very common phenomenon we experience in daily decision-making they are really rare to find expressed in language among other kinds of discourse relations</sample>
    <sample id="1502">so why does this matter studying cognitive dissonance can help us understand the effects of disagreement among people track trends in belief values and attitude changes in population</sample>
    <sample id="1503">alta dissonanza cognitiva è anche correlata ai disturbi d'ansia e può aiutare a comprendere meglio la salute mentale delle persone.</sample>
    <sample id="1504">studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups</sample>
    <sample id="1505">finally cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision-making processes better</sample>
    <sample id="1506">to the goal of creating a cognitive dissonance resource we conducted a large-scale annotation of dissonance relations we used a dissonance first approach as seen in the flowchart here</sample>
    <sample id="1507">tweets were passed using a pdtb parser and pairs of discourse units were annotated according to the guidelines that are described in our paper</sample>
    <sample id="1508">as can be seen here resonance was only found in 35 of the annotated pairs</sample>
    <sample id="1509">on collecting around 1000 examples of discourse unit pairs we ran training for an initial classifier trained only on 43 examples of dissonance to no surprise the classifier performed not much better than chance</sample>
    <sample id="1510">given the low occurrence of dissonance and the absence of any prior such dataset we are facing the problem of absolute rarity</sample>
    <sample id="1511">to alleviate this we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation runs lowering the overall annotation cost while improving dissonance detection</sample>
    <sample id="1512">since the initial model was not able to capture the dissonance class at all we start the active learning process by transferring weights from closely related tasks</sample>
    <sample id="1513">we transfer from two different tasks topic independent dissonance stance classification a task that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic</sample>
    <sample id="1514">called debate here and on binary classification of expansion and comparison classes of purity b since these two are closely related to the conception of consonance and dissonance and we call them ce here</sample>
    <sample id="1515">we find that on transferring the zero-shot performance on the annotated dataset is already much better than chance with the best with auc 062</sample>
    <sample id="1516">further on iteratively fine-tuning on both tasks we find that fine-tuning of ce tasks followed by further fine-tuning on debate yields a much better zero-shot performance thus this is the model that we use to co-start the active learning</sample>
    <sample id="1517">next we determine the best method to update a model with new data from each round of active learning and annotations cumulative accumulates all the data collected from active annotations so far whereas iterative updates the model by training on the latest set of data collected</sample>
    <sample id="1518">tra le diverse strategie abbiamo scoperto che quella cumulativa ha performato allo stesso modo o meglio di quella iterativa.</sample>
    <sample id="1519">next to improve the number of dissonance examples we use a probability of rare class strategy prc to select mostly examples that are highly likely to be dissonant by the current model at any round of air</sample>
    <sample id="1520">we compared this to the other state-of-the-art strategies that are commonly used in the community</sample>
    <sample id="1521">we find that the proposed prc strategy works better than other state-of-the-art strategies although the difference is small note that the performance is significantly lower for random</sample>
    <sample id="1522">on further rounds of ale with two best strategies we improved dissonance classification auc to 075 which is the best performance that we have on the task so far</sample>
    <sample id="1523">we also checked the feasibility of each strategy for annotation quality and costs to annotators we find that prc has the highest percentage of dissonance and works best for rare class however the annotators also find the examples difficult</sample>
    <sample id="1524">in summary we find that prc is a simple al strategy for rare class acquisition and cold starting al with appropriately designed transfer learning tasks can help significantly</sample>
    <sample id="1525">we also find that iterative update is useful for transfer learning from a different domain whereas in-domain active annotations benefit from cumulative update</sample>
    <sample id="1526">these are the links to our code dataset and our paper feel free to get in touch with us if you have any questions thank you</sample>
    <sample id="1527">hi my name is mathias lendeman and today i'm going to give you a brief introduction to our paper on compositional generalization without trees using multi-set tagging and latent permutations this is joint work with my advisors alexander cola and ivan titov</sample>
    <sample id="1528">hi i'm sijun from the university i'm here to introduce our work distinguishing script knowledge from large language models for constraint language planning</sample>
    <sample id="1529">hello my name is kayo yen and i will be presenting our work titled when does translation require context a data-driven multilingual exploration this work was done in collaboration with patrick frenoux emilie yu andrew ft martins and graham newbigg respond in a concise manner to the following question given the content english</sample>
    <sample id="1530">and we compare with popular strategies that are also applied to offline models that are the witness strategy and the local equipment and we compare also with the state-of-the-art architecture specifically tailored for simultaneous speech translation</sample>
  </task>
</testset>