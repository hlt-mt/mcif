<testset name="IWSLT2025" type="output">
  <task track="short" text_lang="it">
    <sample id="0">I modelli linguistici vengono addestrati sui grandi dataset web raccolti da fonti come New York Times, Los Angeles Times, The Guardian e Washington Post.</sample>
    <sample id="1">L'autore principale, Makshata, è un collaboratore con l'università di McGill e Microsoft Research.</sample>
    <sample id="2">Ciao! Benvenuti alla nostra presentazione di 'deplane', un nuovo corpus per la classificazione dei testi in tedesco a livello di documento e di frase.</sample>
    <sample id="3">Il mio nome è Regina Stodden e guiderò voi nella prima parte della presentazione. Innanzitutto, definiamo 'testo semplificato'.</sample>
    <sample id="4">La semplificazione del testo è il processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo specifico di destinatari, come persone che hanno difficoltà nella lettura o non sono madrelingua.</sample>
    <sample id="5">Per addestrare un modello di classificazione dei testi, abbiamo bisogno di pareoli di testo, ad esempio di documenti o di frasi.</sample>
    <sample id="6">Ecco un esempio qui di un paio di frasi congiunte parallele in tedesco e la loro traduzione in italiano.</sample>
    <sample id="7">Il contenuto in inglese è stato tradotto in italiano come segue: 'Sono possibile diverse tecniche, come ad esempio la sostituzione lessicale, la delazione di clausole, la riascoltatura delle clausole o l'insérzione di parole.'</sample>
    <sample id="8">Oggi proponiamo la nostra nuova piana di città因为在 gli ultimi anni ci sono stati alcuni problemi con l'esistente piano. Ad esempio, questi quartieri qui sono troppo piccoli per addestrare un modello di classificazione dei taxonomici.</sample>
    <sample id="9">I tre modelli proposti negli ultimi anni sono tutti automaticamente allineati, il che significa che possono essere sempre esposti a errori di allineamento.</sample>
    <sample id="10">Quindi proponiamo il nostro nuovo corpus 'dplane', che è diviso in due sottocorpi: 'dplane API' e 'dplane Web'. 'dplane API' si basa sui testi news.</sample>
    <sample id="11">Nel file API piano, abbiamo assegnato manualmente circa mille trecento documenti, che si traduce in approssimativamente trentamila migliaia di paresi parzialmente sovrapposti.</sample>
    <sample id="12">Per DeepFaceWeb, questo corpus include diversi domini e assegna tutti i documenti di questi settecento e cinquantadue manualmente e dall'altra parte con metodi di allineamento automatici.</sample>
    <sample id="13">Il totale è di 30.450 paragrafi.</sample>
    <sample id="14">Abbiamo analizzato i nostri paresi di frase un po' più da vicino, ad esempio sul tipo di identificazione dei nomi.</sample>
    <sample id="15">Come puoi vedere qui, i testi della Bibbia sono molto più forti e semplificati rispetto ad esempio ai testi del Nuovo Testamento o ai testi per apprendere la lingua.</sample>
    <sample id="16">Su tutti i livelli, ad esempio per quanto riguarda la semantizzazione lessicale, la semantizzazione strutturata e l'intero processo di semantizzazione.</sample>
    <sample id="17">Inoltre, puoi vedere che il nostro corpus di esempio ha una alta percentuale di trasformazioni di differenziazione. Ad esempio, nel corpus di esempio di piano, abbiamo molte più ordinazioni e aggiunte di testo rispetto al corpus di esempio web.</sample>
    <sample id="18">D'altra parte, nel corpus Web abbiamo molto più riferimenti.</sample>
    <sample id="19">Quindi, vediamo ora cosa possiamo fare con questo corso. Ciao, sono Omer e ora parlerò degli utilizzi del nostro dataset Deepplane. Quindi per il primo caso d'uso possiamo valutare i metodi di allineamento automatici.</sample>
    <sample id="20">Nel recente periodo ci sono state molte tecniche di adattamento, ma nel contesto delle traduzioni automatiche.</sample>
    <sample id="21">Il contenuto in inglese è stato tradotto in italiano come segue:

Dove abbiamo due documenti paragonabili scritti in lingue diverse e vogliamo estrarre le alineazioni di frasi nei documenti postali?</sample>
    <sample id="22">Nel nostro caso d'uso, stiamo cercando di estrarre le alineamenti tra le frasi di due documenti parzialmente diversi, con lo stesso linguaggio, con lo stesso contenuto, ma a livelli di complessità diversi.</sample>
    <sample id="23">E ora abbiamo il nostro dataset di esempi, che hanno been manualmente allineati. possiamo utilizzare questi esempi come riferimenti standard per valutare alcuni dei metodi proposti di allineamento.</sample>
    <sample id="24">Ecco la traduzione in italiano del contenuto originale: "Abbiamo apportato alcune adattazioni ai metodi proposti e abbiamo pubblicato tutte queste adattazioni e i codici per eseguire i nostri esperimenti nella nostra rivista."</sample>
    <sample id="25">Alla fine, abbiamo concluso che il metodo di allineamento automatico最好 utilizzato per la semplificazione dei testi tedeschi è il metodo di massimo allineamento.</sample>
    <sample id="26">Il contenuto in inglese è stato tradotto in italiano come segue: 'E puoi trovare anche il codice per eseguire questo metodo sui tuoi documenti personali nella pagina.'</sample>
    <sample id="27">Il secondo caso d'uso che abbiamo mostrato nel nostro documento è quello della semplificazione automatica del testo.</sample>
    <sample id="28">Il contenuto in inglese è stato tradotto in italiano come segue: 'Sfruttando i modelli di lingua adattati, si possono generare testi semplificati da un testo complesso.'</sample>
    <sample id="29">I abbiamo due modelli fin-tuning diversi e abbiamo anche un modello di fine-tuning per l'elaborazione di documenti.</sample>
    <sample id="30">Il contenuto in inglese è stato tradotto in italiano come segue: 'Inoltre, siamo anche in grado di regolare la lunghezza della base normale per produrre semplificazioni a livello di frase.'</sample>
    <sample id="31">Il contenuto in italiano è: 'Puoi trovare anche tutti i checkpoint e puoi vedere dettagli più approfonditi sui punteggi e le valutazioni delle nostre esperimenti nella relazione.'</sample>
    <sample id="32">Abbiamo concluso che questa regolazione fine base potrebbe produrre o ottenere punteggi migliori rispetto ai punteggi di riferimento.</sample>
    <sample id="33">Ecco la traduzione in italiano: 'E propone quei risultati come un punto di riferimento, un punto di riferimento base per il problema della semplificazione del testo automatico nel futuro.'</sample>
    <sample id="34">Grazie per la vostra attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie.</sample>
    <sample id="35">Il nome della relatrice è Kay O'Yen.</sample>
    <sample id="36">Il modello T5 large è stato utilizzato per ottenere l'accuratezza dell'82%-87%.</sample>
    <sample id="37">Sì, i tagger CoNLL-2003 funzionano ancora.</sample>
    <sample id="38">Il metodo proposto riduce la soggettività dell'valutazione umana attraverso l'annotazione esplicita delle risposte dei modelli che esprimono comportamenti specifici, come l'utilizzo di informazioni non pertinenti o la contraddizione tra loro.</sample>
    <sample id="39">Il successo dell'approccio scarsamente supervisionato si basa in larga misura sulla disponibilità di campioni di validazione puliti.</sample>
    <sample id="40">I progressi che possono essere fatti per migliorare il punteggio includono l'aggiunta di informazioni supplementari utili sull'entità specificata nella domanda originale.</sample>
    <sample id="41">Quattro.</sample>
    <sample id="42">Ciao, mi chiamo Adam Skurkowski e questo talk è sulla struttura di dipendenza della coordinazione.</sample>
    <sample id="43">Come sapete, diverse teorie e approcci corpi forniscono struttture di dipendenza diverse. Ad esempio, le dipendenze universali sono la struttura delle coordinate di coordinazione Lisa, Bart e Maggie.</sample>
    <sample id="44">Il contenuto in inglese è stato tradotto in italiano come segue: 'È vero che il primo congiuntivo è la testa della struttura delle coordinate, quindi in questo caso Lisa.'</sample>
    <sample id="45">Un approccio simile viene假设 in teoria dei testi di Igor Miltuchov, dove di nuovo tutta la struttura dei coordinate è guidata dal primo congiuntivo. Quindi questi due approcci sono isometrici, giusto? Hanno selezionato uno dei congiuntivi.</sample>
    <sample id="46">Oggi ci sono anche approcci simmetrici alle strutture coordinate, come l'approccio di Praga, gli approcci di connessione, l'approccio di Heisenberg e la dipendenza delle tre basi, dove le strutture coordinate sono guidate dalla connessione.</sample>
    <sample id="47">Quindi otteniamo dipendenze da 'a' a tutti i congiunti.</sample>
    <sample id="48">E infine, c'è anche un approccio a tre vie che viene utilizzato, ad esempio, nel grammatica del linguaggio di programmazione Catégorica.</sample>
    <sample id="49">Dove si dice che tutti i comportamenti sono capi della struttura coordinata, quindi otteniamo dipendenze dal governatore qui sopra. A sinistra, tutti i comportamenti sono separati. Questo è il modello di Barten.</sample>
    <sample id="50">Il paper di Amedeo è stato scritto per produrre un nuovo argomento per le strutture simmetriche di coordinazione, come queste due, e contro le strutture asimmetriche di coordinazione come queste.</sample>
    <sample id="51">Il contenuto in italiano è: 'Ok, l'argomento si basa sul principio di dipendenza selezionata che sarà spiegato sulla base di questi esempi.'</sample>
    <sample id="52">In inglese, come saprai probabilmente, gli oggetti diretti vengono preferiti vicino al verbo, mentre gli oggetti adjacente possono essere più lontani. Quindi 'March read it yesterday' è corretto perché 'it' è un oggetto diretto.</sample>
    <sample id="53">Il contenuto in inglese 'while march read yesterday it is much worse right because ah here between the verb and the direct object there is an adjective i yesterday.' si traduce in italiano come: 'Mentre Marche ha letto ieri, è molto peggio perché qui tra il verbo e l'oggetto diretto c'è un adjectivo "i" di ieri.'</sample>
    <sample id="54">Il contenuto in inglese è stato tradotto in italiano come segue: 'Tuttavia, questo effetto può essere ammorbidito quando l'oggetto diretto è molto pesante e lungo, perché allora può essere spostato nella posizione dopo l'aggiunta.'</sample>
    <sample id="55">Il contenuto inglese è stato tradotto in italiano come segue: 'Questo è illustrato qui, quindi entrambe queste frasi sono corrette. March ha letto un libro estremamente interessante sulla BCC ieri. È okay替代it, abbiamo questo lungo np.'</sample>
    <sample id="56">Ma è anche okay dire 'Marche Fredi ieri, questo libro assolutamente affascinante sulla vita degli uccelli'.</sample>
    <sample id="57">Il motivo qui è che questo è possibile perché, anche se questa frase viola il principio grammaticale generale che prevede che gli oggetti diretti debbano essere accanto al verbo, ciò è possibile poiché la grammatica inglese non ha alcuna regola che vieti esplicitamente l'uso di un pronome come oggetto diretto in una frase.</sample>
    <sample id="58">Soddisfa il principio della minimizzazione della lunghezza dipendente, che dice che le dipendenze più corte sono preferite.</sample>
    <sample id="59">Quindi, questi due alberi mostrano solo la lunghezza delle dipendenze cruciali che non sono costanti tra le due strutture.</sample>
    <sample id="60">Quindi abbiamo la dipendenza da 'rad' al 'aggiuntivo di lunghezza sette', misurata in parole, e da 'rad' al 'libro di lunghezza quattro'. Quindi insieme è undici.</sample>
    <sample id="61">Quando si sposta, quando si swap questi due componenti, la somma di queste due dipendenze diventa sei, giusto? Invece di undici, sei molto più corta. Questo è perché sembra abbastanza okay, giusto? Violava un principio, ma soddisfa un altro.</sample>
    <sample id="62">Okay, allora abbiamo estratto alcune statistiche sulla coordinazione dall'aggiornata versione di PanTREBANK e abbiamo visto il paper perché non usiamo le dipendenze universali.</sample>
    <sample id="63">E queste statistiche confermano l'osservazione fatta molte volte prima che i congiuntivi di sinistra tendano ad essere più corti? Sì, sale, pepe e noce di pepe misurati in sillabe.</sample>
    <sample id="64">Il contenuto in inglese diventa: 'E anche l'osservazione che è stata fatta nel passato, che questa tendenza cresce con la differenza di lunghezza.'</sample>
    <sample id="65">Quindi, quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto più corto preferisce essere il primo, giusto? Quindi, la proporzione è maggiore per i congiunti di sinistra corti.</sample>
    <sample id="66">Il contenuto del paper è che abbiamo osservato questa tendenza solo quando i governatori sul lato sinistro erano assenti.</sample>
    <sample id="67">Il governatore è sulla sinistra in questo esempio.</sample>
    <sample id="68">Il contenuto in inglese 'it's absent in the second example, Homer came and sneezed. Here we have coordination of two verbs and there is no outside external governor. Right? So in such cases the left conjunct prefers to be shortened than also the big difference between the two.' viene tradotto in italiano come: 'È assente nel secondo esempio, Homer è venuto e ha snobbato. Qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno. Giusto? Quindi in tali casi il congiuntivo sinistro preferisce essere accorciato piuttosto che anche la grande differenza tra i due.'</sample>
    <sample id="69">Il contenuto originale in inglese è: 'Tuttavia, quando il governo di destra governa la coordinazione della rete, l'effetto scompare.'</sample>
    <sample id="70">Quindi abbiamo mostrato che misurando la lunghezza in caratteri, la prima colonna è in sillabe, la colonna centrale in parole e la colonna destra in lettere. Quindi mi concentrerò sulla colonna destra.</sample>
    <sample id="71">Cosa vediamo qui è che quando il regolatore di sinistra è impostato su...</sample>
    <sample id="72">La tendenza per i congiuntivi di sinistra ad essere più corti cresce costantemente con la differenza assoluta di parole, e lo stesso si osserva quando ci sono governatori che hanno 'no' come prefisso nelle frasi, ma quando il governatore ha 'il' come prefisso, questa tendenza scompare.</sample>
    <sample id="73">Nel paper mostriamo come questo fornisca un argomento contro le strutture asimmetriche di coordinazione, come queste due e quattro strutture simmetriche, come queste due.</sample>
    <sample id="74">Visualizza il documento per l'accordo completo e l'argomento dell'argomentazione, e poi parla con noi della sessione successiva. Grazie.</sample>
    <sample id="75">I due.</sample>
    <sample id="76">I domini risultano più semplificati sono quelli del testo 'news' e del testo per apprendimento della lingua.</sample>
    <sample id="77">La preferenza per i congiunti a sinistra più brevi si verifica nell'uso di 'so', 'and' e 'not' invece che 'salt' e 'pepper'.</sample>
    <sample id="78">Sì, è possibile utilizzare i modelli pre-trainati disponibili sul sito ufficiale e le script di addestramento sono anche disponibili in un repository GitHub.</sample>
    <sample id="79">DEplain-apa contiene solo testi accademici e non include documenti del web.</sample>
    <sample id="80">Una migliore architettura di modello, dimensioni maggiori e esempi più finemente adattati contribuiscono alla buona generalizzazione.</sample>
    <sample id="81">La lunghezza dei congiuntivi a sinistra è stata misurata in caratteri, confrontando la prima colonna (congiuntivi in sillabe) con la seconda colonna (congiuntivi in parole) e la terza colonna (congiuntivi in lettere).</sample>
    <sample id="82">Gli esperimenti sono stati progettati misurando la lunghezza in caratteri delle colonne con governatori a sinistra, al centro e a destra, per concentrarsi sulle differenze tra loro.</sample>
    <sample id="83">Il classificatore base performa molto peggio di quanto ci si aspetterebbe addestrato solo su pochi esempi di dissenso.</sample>
    <sample id="84">Un solo autore, Shangbin Jiang.</sample>
    <sample id="85">I personaggi della conversazione sono Bob e Alice.</sample>
    <sample id="86">I modelli di MT sensibili al contesto migliorano rispetto a quelli indipendenti dal contesto per quanto riguarda l'accuratezza nei confronti di alcuni fenomeni specifici del discorso, come la formalezza e la coesione lessicale.</sample>
    <sample id="87">L'autore principale è Costruce Senna, con contributi da John Gehrke, Aaron Muller, Kanishka Misra, Karin Fengtian, Roger Levy e Atina Williams.</sample>
    <sample id="122">Il framework utilizza l'identificazione di diverse annotazioni su dataset per quantificare la posizione.</sample>
    <sample id="155">Il risultato dello studio è stato che i soggetti umani erano anche in grado di表面are stereotipi razziali.</sample>
    <sample id="156">Il contenuto inglese menziona l'utilizzo della versione estesa di PanTREBANK e del paper per l'estrazione delle statistiche sulla coordinazione. Tuttavia, non specifica altre fonti di dati utilizzate nel studio.</sample>
    <sample id="157">Un solo autore, Adam Skurkowski.</sample>
    <sample id="158">Le attività strettamente correlate alla dissonanza cognitiva sono l'espansione e la comparazione di classi di PNTB.</sample>
    <sample id="159">Due.</sample>
    <sample id="160">Un solo autore, Vasudha.</sample>
    <sample id="161">Il framework differisce dagli studi precedenti perché confronta gli utenti finali con modelli e dati, previsioni e etichette, invece di concentrarsi solo sul concetto di accordo degli etichettatori.</sample>
    <sample id="162">La configurazione 'lexicon of stereotypes' si sovrappone maggiormente al lessico degli stereotipi.</sample>
    <sample id="163">I sistemi commerciali comparati sono DeepL e Google Translate per la traduzione di livello documentale.</sample>
    <sample id="164">Ciao, sono Shangbin, studente di dottorato in scienze politiche all'Università di Washington. Oggi sto presentando il nostro lavoro, dal data pre-training alle modelli linguistici fino alle attività di downscaling, tracciare le traiettorie della corruzione politica che portano a modelli NLP iniqui.</sample>
    <sample id="165">I modelli di lingua sono addestrati sui dati di rete web di grande scala.</sample>
    <sample id="166">I media politici sono ben coperti dai loro dati di addestramento pre-trainato, secondo un'indagine della C4 corona. possiamo vedere che il New York Times, Los Angeles Times, il Guardian, Huffington Post, ecc. sono ben coperti nei dati di addestramento linguistico pre-trainato.</sample>
    <sample id="167">Questo ha creato un problema di incertezza per le applicazioni dei modelli linguistici.</sample>
    <sample id="168">Così, da un lato, sono stati in grado di imparare da diverse prospettive, che celebra la democrazia e la pluralità delle idee. Dall'altro lato, queste diverse opinioni politiche sono inherentemente socialmente influenzate e possono portare a potenziali problemi di parità nella selezione dei candidati.</sample>
    <sample id="169">Per questo, proponiamo di indagare il flusso del pipistrello politico dalla raccolta dei dati all'elaborazione del linguaggio fino alle attività inferiori, specificamente chiedendo le seguenti domande.</sample>
    <sample id="170">Il contenuto inglese descrive come valutare il orientamento politico di modelli linguistici e l'impatto che potrebbe avere sull'analisi dei dati. In italiano, si dice:
"Prima di tutto, come valutiamo l'orientamento politico dei modelli linguistici e quali sono le conseguenze che questo può avere sulla nostra analisi dei dati?"</sample>
    <sample id="171">In secondo luogo, come funzionano effettivamente i modelli linguistici con differenti politiche di NLP su task down stream e se ciò potrebbe causare problemi di parità nelle applicazioni NLP?</sample>
    <sample id="172">In particolare, siamo stati i primi a proporre modelli di lingua prompt con diversi formati prompt utilizzando le questionari politici, come il test Political Compass, che ci assicura di valutare automaticamente la letteratura scientifica politica.</sample>
    <sample id="173">Alcuni risultati preliminari dimostrano che i modelli di lingua hanno orientamenti politici diversi e occupano tutti e quattro i quadranti sullo spazio politico.</sample>
    <sample id="174">Il contenuto inglese descrive che il modello linguistico GPT-4 è quello più liberale tra tutti e che le teorie GPT sono generalmente più sociali di quelle di BERT e delle sue varianti. La traduzione italiana sarebbe: 'Inoltre, possiamo vedere che il modello linguistico GPT-4 è quello più liberale tra tutti e che le teorie GPT sono generalmente più sociali di quelle di BERT e delle sue varianti.'</sample>
    <sample id="175">In secondo luogo, miriamo a indagare fino a che punto le distorsioni politiche nei modelli di lingua sono effettivamente prese in carico dai dati di addestramento.</sample>
    <sample id="176">Quindi possiamo condurre un esperimento controllato utilizzando punti di controllo del modello linguistico pre-trainati su sei diversi corpi politici separati in notizie e social media, ulteriormente suddivisi per loro politica.</sample>
    <sample id="177">L'audio descrive che 'Pre-trainando ulteriormente i modelli di lingua su tali parti del corpus, possiamo vedere che anche le coordinate ideologiche dei modelli di lingua si spostano corrispondentemente.' In italiano, questo significa che addestrando i modelli di linguaggio su porzioni specifiche di un grande insieme di testi (noto come corpus), è possibile influenzare le loro posizioni ideologiche o punti di vista. Questo può essere fatto per adattare i modelli alla cultura, alla società o alle opinioni della comunità specifica per cui sono stati progettati.</sample>
    <sample id="178">Per esempio, per Robert, che ha ricevuto un'ulteriore formazione e addestramento sulla corrente di sinistra del Reddit, possiamo vedere un sostanziale cambiamento di orientamento politico verso il liberalismo.</sample>
    <sample id="179">In termini di distorsioni politiche.</sample>
    <sample id="180">Stiamo anche cercando di indagare se i modelli linguistici possono raccogliere la polarizzazione che è prevalente nella nostra società moderna.</sample>
    <sample id="181">Quindi dividiamo il pretraining copora in 'prima della cinquantacinquesima presidenza degli Stati Uniti' e 'dopo la cinquantacinquesima presidente degli Stati Uniti'. Separatamente, modelli di lingua pre-trainati su due copora temporali diversi.</sample>
    <sample id="182">Il contenuto inglese descrive come i modelli di lingua generalmente abbiano una tendenza politica che si allontana dal centro dopo il 2017. In italiano, questo significa che anche i modelli di lingua possono raccogliere l' polarizzazione nella nostra società.</sample>
    <sample id="183">Quindi, per concludere, abbiamo valutato modelli di lingua con significativi orientamenti politici sulla detezione del discorso haine e della falsa notizia nella ricerca su applicazioni NLP che spesso coinvolgono i modelli di lingua e potrebbero avere implicazioni molto importanti.</sample>
    <sample id="184">Quindi vediamo che se esaminiamo il rendimento per categoria, cioè se separiamo il rendimento in due categorie diverse, beh...</sample>
    <sample id="185">Il contenuto inglese descrive come, ad esempio, i modelli di rilevamento del discorso haine siano migliori per le lingue di sinistra. In italiano potremmo dire qualcosa del tipo: 'Su diversi fronti demografici o politici, possiamo notare un pattern, ad esempio, nel caso della detezione del discorso haine, dove i modelli basati sulle lingue di sinistra sono più efficienti.'</sample>
    <sample id="186">Il contenuto inglese descrive un processo di rilevamento del discorso haine che mira a gruppi socialmente minoritari. La traduzione italiana sarebbe: 'Nel rilevamento del discorso haine che mira ai gruppi socialmente minoritari.'</sample>
    <sample id="187">Il contenuto inglese descrive che i lavori di rilevamento del discorso incitazionista mirano più potenti gruppi nella nostra società. La traduzione italiana è: 'Tuttavia, i lavori per rilevare il discorso incitazionista mirano a gruppi più potenti nella nostra società.'</sample>
    <sample id="188">E viceversa, i modelli di lingua sono meglio adatti alla detenzione del discorso haine mirato ai bianchi e agli uomini, tuttavia peggio nello rilevamento del discorso haine mirato alle persone di colore, omosessuali, transgender ed altre minoranze.</sample>
    <sample id="189">Tendenze simili si verificano anche per la rilevazione di notizie false, dove vediamo che i modelli di lingua left-leaning sono meglio adatti a rilevare informazioni false dalle loro posizioni politiche opposte e viceversa.</sample>
    <sample id="190">Questo significa che mostreremo molti esempi qualitativi per vedere che i modelli di linguaggio con significati politici diversi sono...</sample>
    <sample id="191">Istruzioni per l'utilizzo: fate riferimento alla tabella per visualizzare le previsioni diverse per il linguaggio del dissenso e gli esempi di bufale sulla base delle loro categorie sociali. Ci sono molti altri esempi nell'appendice per evidenziare ulteriormente questo punto.</sample>
    <sample id="192">Questo indica che c'è un problema di equità molto preoccupante riguardo alle tendenze politiche dei modelli linguistici.</sample>
    <sample id="193">Per esempio, se i modelli di lingua a destra dovessero essere selezionati per il controllo dell'odio nei discorsi o della diffusione di false informazioni e così via, e poi essere distribuiti su una popolare piattaforma sociale.</sample>
    <sample id="194">Questo significa che le persone con opinioni politiche opposte potrebbero essere marginalizzate e il discorso di odio mirato ai gruppi minoritari potrebbe simply reagire in modo diffuso senza alcuna controllo effettivo.</sample>
    <sample id="195">Questo segnala l'allarme per noi di riconoscere e affrontare le questioni di equità causate dalle politiche linguistiche dei modelli di lingua.</sample>
    <sample id="196">Quindi, un po' di discussione, vorremmo anche sottolineare che esponiamo il dilemma unico riguardante le politiche linguistiche multietniche, è come tra la Siria e il Libano.</sample>
    <sample id="197">Quindi, se non sanificiamo le opinioni politiche nei dati di addestramento del modello linguistico, il pregiudizio si propaga dal data pretraining al modello linguistico e alle attività downstream, creando infine problemi di parità.</sample>
    <sample id="198">Se proviamo a pulire in qualche modo, rischieremo anche il censura o l'esclusione e è incredibilmente difficile determinare cosa sia effettivamente neutrale e ciò che dovrebbe essere mantenuto nella raccolta dei dati della lingua. È un po' come il problema dell'elettrostatica.</sample>
    <sample id="199">Okay, great. I think that's pretty much all I have for today. Thank you for your time.</sample>
    <sample id="200">Due.</sample>
    <sample id="201">fino a mille token di lunghezza del contesto.</sample>
    <sample id="202">Il set di dati include 'example.com' e 'example.org'.</sample>
    <sample id="203">La posizionalità è semplicemente le prospettive che le persone hanno come risultato delle loro demografia, identità e esperienze di vita.</sample>
    <sample id="204">Il relatore del video si chiama David.</sample>
    <sample id="205">Sì, è possibile utilizzare un modello ST offline già esistente senza rieducarlo o adattarlo per la classificazione di emoji. Si consiglia di utilizzare solo uno modello per ogni regola di latenza e di gestire la latenza attraverso parametri specifici.</sample>
    <sample id="206">Due.</sample>
    <sample id="207">No, il modello non funziona bene sulla suite di test.</sample>
    <sample id="208">Le tre varianti di KITMUS sono: setting 1 con conoscenza pre-train disponibile solo durante il tempo di addestramento; setting 2 con conoscenza pre- e post-train disponibili sia durante il tempo di addestramento che in quello di influenzazione; setting 3 con conoscenza pre- e post-train disponibile solo durante il tempo di influenza.</sample>
    <sample id="209">Ispirato all'audio 1, potremmo dire: "Gli autori dell'articolo sono Javad Hosseini, Philip Radinsky, Sylvia Perretti e Anne Lewis."</sample>
    <sample id="210">La domanda di ricerca finale è se sia necessario utilizzare solo campioni puliti per la validazione o ci siano altre tecniche migliori per utilizzare i dati non puliti.</sample>
    <sample id="211">La sensibilità della metrica misura la capacità del modello di generare sempre gli stessi output per lo stesso compito, indipendentemente dalla variazione minima nell'input.</sample>
    <sample id="212">La relatrice si chiama Jing Wei Yi.</sample>
    <sample id="213">Una maggiore sensibilità indica generalmente una performance del modello migliore, in quanto significa che il modello è in grado di rilevare anche piccoli cambiamenti nell'input. Tuttavia, questo può anche dipendere dal contesto specifico e dalle prestazioni degli altri parametri del modello.</sample>
    <sample id="214">I modelli vengono messi a disposizione di un contesto linguistico di addestramento composto da una grande quantità di testo multilingua, inclusi sottotitoli di film, traduzioni automatiche e altro materiale di addestramento.</sample>
    <sample id="215">In generale, si impiega solo un campione di validazione pulito per ottenere prestazioni elevate in WSL.</sample>
    <sample id="216">I due autori dell'articolo sono Esen Durmus e Dan Jurafsky.</sample>
    <sample id="217">I nuovi metodi sono necessari perché i modelli linguistici attuali mostrano che esiste un orientamento politico variabile nei loro risultati, occupando tutti e quattro i quadranti del compasso politico.</sample>
    <sample id="218">La relatrice è Makshata.</sample>
    <sample id="219">L'infrastruttura di propagazione dei bias politici include pre-training data, modelli linguistici e task down stream.</sample>
    <sample id="220">Il processo di semplificazione è più ampio nella copia DEplan-APA, con molte più rimesse e modifiche del testo, mentre la copia Web ha maggiori riformulazioni.</sample>
    <sample id="221">Sì, è disponibile pubblicamente.</sample>
    <sample id="222">La filigrana viene inserita nel testo utilizzando l'inserimento di parole chiave (keyword injection). Il processo prevede che un utente invii una frase al servizio del provider, il quale controlla il numero di trigger presenti nella frase. La filigrana viene calcolata come somma ponderata della target embedding e dell'embedding originale, con il peso della target embedding proporzionale al numero di trigger nella frase. Se il numero di trigger è maggiore di M, allora la filigrana fornita corrisponde esattamente a quella calcolata.</sample>
    <sample id="223">Il primo autore è Lucas Zhang del Dipartimento di Informatica della Pennsylvania State University.</sample>
    <sample id="224">Sì, i modelli codificatore-decodificatore come mt5 possono migliorare con l'addestramento su una combinazione di lingue.</sample>
    <sample id="225">Un esempio di pianificazione linguistica vincolata è quella che mira a raggiungere obiettivi specifici, come ad esempio ' Scrivere una lettera di presentazione.'</sample>
    <sample id="226">Gli autori verificano la segretezza del loro metodo utilizzando il embedding di frasi su un dizionario di 40mila parole.</sample>
    <sample id="227">Il lavoro utilizza tre modelli di addestramento basati su pretraining esistenti per analizzare l'impatto delle strategie di pretraining.</sample>
    <sample id="228">GPT-4 non è meno allineato ad alcun paese in particolare, ma mostra una maggiore affinità con i paesi che parlano inglese.</sample>
    <sample id="229">La relatrice mostra l'utilizzo del meccanismo dell'attenzione nella frase 'e leverage the knowledge already acquired by the model through the tension mechanism between audio input and textual output.'</sample>
    <sample id="230">La quantità di attività aumenta, migliorando le prestazioni del modello e riducendo la sensibilità al tempo medio.</sample>
    <sample id="231">Gli autori confrontano il loro metodo con altri modelli a tre meno riferimenti sulla base del benchmark dei corgi.</sample>
    <sample id="232">I due coautori sono advisor di primo autore.</sample>
    <sample id="233">Il primo autore di PaLM non è stato specificato nella voce fornita.</sample>
    <sample id="234">Cari amici, sono Jennie, un studente di primo anno al Carnegie Mellon University e oggi presenterò il mio lavoro su 'Annotazione posizionale caratterizzante progettazione basata su dati acquisiti con una telecamera'.</sample>
    <sample id="235">Questa opera è stata realizzata in collaborazione con alcuni colleghi dell'Università di Washington e l'Istituto Allen per l'AI, ovvero Sebastian Santi, Ronan Le Bras, Katerina Ryn娜ka e Martin Saps.</sample>
    <sample id="236">Quindi, cominciamo immaginando di lavorare per un giornale e di筛选 i commenti sotto l'articolo news cercando di rimuovere il contenuto offensivo.</sample>
    <sample id="237">Il contenuto in inglese si traduce in italiano come: 'Potresti rivolgerti a un API popolare, come l'API di Prospettiva per la detezione della tossicità. Questo funziona molto bene se sei Carl Jones, dove l'API di Prospettiva è in grado di rilevare correttamente le sostanze tossiche.'</sample>
    <sample id="238">Ma questo non è really il caso per Aditi Sharma, dove l'API prospettiva non è davvero sensibile ai termini offensivi che sono più comuni nei contesti indiani.</sample>
    <sample id="239">Questo è un esempio di pregiudizio progettato dove vediamo differenze sistematiche nella prestazione delle tecnologie tra le popolazioni.</sample>
    <sample id="240">I disegni di distorsione come quello che abbiamo visto prima potrebbero verificarsi a causa della posizione degli studiosi e dei sviluppatori dell'NLP. La posizione è semplicemente le prospettive che le persone hanno come risultato delle loro demografia, identità e esperienze di vita.</sample>
    <sample id="241">Questo è un concetto ampiamente utilizzato negli studi critici, soprattutto nelle accademie di femministe e queer.</sample>
    <sample id="242">Inoltre, come ricercatore, la posizione può influire sul processo di ricerca e sui suoi risultati perché può cambiare le decisioni che i ricercatori prendono.</sample>
    <sample id="243">Il contenuto in inglese dice: 'And so one question that people might ask is do data sets and models have positionality?' In italiano, questo sarebbe: 'E quindi una domanda che potrebbero fare le persone è: i set di dati e i modelli hanno posizione?'</sample>
    <sample id="244">Non stiamo cercando di dire che i modelli, le cellule e gli dataset hanno identità demografiche e esperienze della vita, ma che essi aggregano giudizi e opinioni su persone reali e possono rappresentare certe posizioni mediche rispetto ad altre.</sample>
    <sample id="245">Il lavoro pregresso ha suggerito alcune evidenze aneddotiche di posizione, come distorsioni culturali nei modelli e nei set di dati, nonché definizioni teoriche della posizione del modello.</sample>
    <sample id="246">Tuttavia, queste opere non esaminano veramente la comparazione degli utenti con i dati set e modelli loro stessi.</sample>
    <sample id="247">Il contenuto in inglese si traduce in italiano come: 'L'utilizzo di modelli di analisi e posizione del dataset diventa sempre più importante poiché i test di ELISA diventano sempre più sensibili e socialmente orientati.'</sample>
    <sample id="248">Il contenuto in inglese dice: 'E' difficile caratterizzare come queste posizioni vengano spostate, perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API.' In italiano si dice: 'È difficile caratterizzare come queste posizioni vengano spostate, poiché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro le API.'</sample>
    <sample id="249">Quindi, per studiare la posizione dei dati nel modello, effettivamente confrontiamo le annotazioni con gli utenti reali e con i set di dati esistenti e modelli.</sample>
    <sample id="250">Il contenuto in inglese si traduce in italiano come: 'Lavoriamo su questo attraverso il nostro framework di posizione NLP.'</sample>
    <sample id="251">Il nostro framework funziona in due fasi principali.</sample>
    <sample id="252">Il primo passo è re-entrire i set di dati con diversi etichettatori.</sample>
    <sample id="253">Ecco il contenuto in italiano: 'E dovremmo fare questo guardando le demografie dei set di dati originali, degli etichettatori perché solitamente solo pochi etichettatori etichetano ogni esempio e perché le demografie sono raramente raccolte e condivise.'</sample>
    <sample id="254">Così, dobbiamo ricorsivamente etichettare i dati per ottenere molti etichette e un set di dati demografici ricco.</sample>
    <sample id="255">Quindi, prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i set di dati utilizzando la valutazione della corrispondenza di Rendice.</sample>
    <sample id="256">Il nostro quadro di riferimento differisce effettivamente dalla letteratura sull'annotazione degli errori in quanto confronta gli utenti finali con modelli e set di dati, previsioni e etichette, invece di guardare solo all'agreement annotatore o alla modellazione delle distribuzioni annotatore.</sample>
    <sample id="257">Il nostro framework è largely enabled through Lab in the Wild, un platform di crowd-sourcing online former HCI collaboratore.</sample>
    <sample id="258">In Live on the Wild è un platforma di sperimentazione online dove possiamo reclutare volontari di diverse categorie, rispetto ai platform come Mturk che hanno principalmente partecipanti dagli Stati Uniti o dall'India. Inoltre, Live on the Wild è in grado di ottenere dati di alta qualità.</sample>
    <sample id="259">Noi ospitiamo due task cell lab nel mondo, uno dei quali si occupa della socialità accettabile. Il funzionamento di questo laboratorio è che i partecipanti leggono una situazione dal database di chimica sociale e poi scrivono come sia socialmente accettabile questa situazione.</sample>
    <sample id="260">Per restare coinvolti nello studio, possono confrontare le loro risposte con quelle di un AI e degli altri.</sample>
    <sample id="261">Inglese: We then compared these annotations with social chemistry, Delphi, and GP4.
Italiano: Abbiamo quindi confrontato queste annotazioni con la chimica sociale, Delphi e GP4.</sample>
    <sample id="262">Il contenuto in inglese è stato tradotto in italiano come segue: 'Quindi abbiamo replicato un setup molto simile per la task di rilevamento del discorso di intolleranza e odio, dove dovrebbero leggere un esempio da "Dinah hate" e scrivere se pensano che sia un esempio di discorso di odio.'</sample>
    <sample id="263">Il contenuto in inglese si traduce in italiano come: 'Nel nostro studio, abbiamo raccolto più di sedici migliaia di annotazioni da oltre mille esperti di vari paesi.'</sample>
    <sample id="264">Quindi, ora siamo equipaggiati per rispondere: chi sono i set di dati NLP e modelli che si adattano meglio? Abbiamo scoperto che ci è posizionalità nell'NLP.</sample>
    <sample id="265">Per esempio, troviamo che i set di dati e i modelli sono maggiormente associati ai paesi che parlano inglese. Ad esempio, per l'analisi della socialità accettabile del GPDR, siamo portati a credere che sia maggiormente associato alla filosofia confuciana e ai paesi che parlano inglese. Troviamo anche che il disprezzo per le persone nere è anche più associato ai paesi che parlano inglese.</sample>
    <sample id="266">Il contenuto in inglese dice: 'Inoltre, troviamo una maggiore concordanza con le persone che hanno un'istruzione universitaria. Per la tarefa di accessibilità sociale di GPT-4, abbiamo scoperto che è più adatta alle persone con un'istruzione universitaria o al diploma di laurea.'</sample>
    <sample id="267">Ecco il contenuto in italiano: 'E lo troviamo anche per Donna Hare, dove è più associato alle persone con un'istruzione universitaria.'</sample>
    <sample id="268">Tuttavia, quando i modelli e i set di dati sono associati a popolazioni specifiche, alcuni sono inevitabilmente lasciati indietro.</sample>
    <sample id="269">Un esempio di questo è che i set di dati e i modelli sono meno associati alle persone non bioniche rispetto ai loro omologhi maschili e femminili. Lo troviamo nel compito 'Social Acceptability Task' del GPT-4, così come nell'analisi del compito 'DINAE-HATE'.</sample>
    <sample id="270">Quindi, dato che c'è un posto disponibile in AFDLP, cosa possiamo fare?</sample>
    <sample id="271">Hai alcuni consigli per questo? Il primo è quello di tenere traccia di tutte le scelte progettuali rilevanti durante tutto il processo della ricerca e l'altro è quello di fare una ricerca NLP dal punto di vista del relativismo.</sample>
    <sample id="272">La nostra terza raccomandazione è quella di costruire set di dati e modelli specializzati all'interno di quattro comunità specifiche, e un buon esempio di questo è l'iniziativa Masa Kani. vogliamo sottolineare che l'accesso inclusivo alle tecnologie non significa solo rendere tutte le tecnologie utili per tutti.</sample>
    <sample id="273">Ecco il contenuto in italiano: 'Quindi, questo include la presentazione, ma se vuoi imparare di più, non esitare a consultare il nostro dashboard per i risultati dell'analisi più aggiornati e il nostro paper. Grazie.'</sample>
    <sample id="274">La relatrice menziona che i modelli di SimulST sono solitamente addestrati con architetture specifiche, l'aggiunta di moduli aggiuntivi per essere ottimizzati comporta lunghe e complesse procedure di addestramento, come l'utilizzo di obiettivi di ottimizzazione diversi, e l'mantenimento di diversi modelli per raggiungere differenti regole di latenza può comportare l'addestramento di più modelli.</sample>
    <sample id="275">Non esiste una soluzione semplice o universale per questo problema, poiché dipende dalle sfide specifiche del contesto e dalla natura dei dati stessi. Tuttavia, alcuni possibili approcci includono la raccolta e l'utilizzo di dati più rappresentativi della popolazione, l'adozione di tecniche di pre-processamento dei dati per rimuovere le informazioni personali e sensitive, e l'utilizzo di algoritmi di correzione degli errori per minimizzare gli effetti del bias nella generazione di output. Inoltre, è importante mantenere una trasparenza e una responsabilità significative nell'elaborazione dei dati e nell'utilizzo dei modelli sviluppati.</sample>
    <sample id="276">Il contenuto inglese dice: 'Ciao, sono Siyuan Yu dall'Università di Fudan. Sono qui per introdurre il nostro lavoro, che distingue la conoscenza del script da modelli di grande linguaggio per pianificazione della lingua con restrizioni.'</sample>
    <sample id="277">Nel corso della vita quotidiana, gli esseri umani pianificano spesso le loro azioni seguendo istruzioni passo per passo in forma di script previsti.</sample>
    <sample id="278">Il contenuto inglese descrive come i modelli di linguaggio precedenti abbiano esplorato l'utilizzo di modelli per pianificare azioni astratte di attività stereotipiche, come fare una torta, e dimostrare che i grandi modelli di linguaggio possono decomporre le azioni in fasi. In italiano, questo sarebbe: 'I modelli di linguaggio precedenti hanno esplorato l'uso di modelli per pianificare azioni astratte di attività stereotipiche, come fare una torta, e mostrare che i grandi modelli di linguaggio possono decomporre le azioni in fasi.'</sample>
    <sample id="279">Il contenuto inglese descrive che i lavori precedenti si concentrano principalmente sul pianificare obiettivi astratti per attività teoriche, mentre pianificare obiettivi con vincoli specifici, come fare una torta al cioccolato, rimane ancora poco studiato. In italiano, questo sarebbe: 'I lavori precedenti si sono concentrati principalmente sulla pianificazione di obiettivi astratti per attività teoriche, mentre la pianificazione di obiettivi con vincoli specifici, come quella di fare una torta al cioccolato, rimane ancora poco studiata.'</sample>
    <sample id="280">In questo articolo, definiamo il problema della pianificazione del linguaggio vincolato.</sample>
    <sample id="281">Il contenuto inglese dice che 'Ogni obiettivo ha vincoli diversi e un piano di azione può essere adattato per soddisfare diversi obiettivi con vincoli realistici specifici. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e flessibili ai vincoli.'</sample>
    <sample id="282">In questo articolo, valutiamo e miglioriamo in primo luogo la capacità di pianificazione della lingua vincolata dei modelli di grande linguaggio.</sample>
    <sample id="283">Il contenuto inglese dice: 'Non esiste alcuna prova fuori dalla speculazione che ci sia una vita extraterrestre.'</sample>
    <sample id="284">Il contenuto inglese si traduce in italiano come: 'Acquisiamo queste regole di base e, mostrando la tabella, estendiamo le regole astratte con restrizioni specifiche per l'utilizzo dell'applicazionecpd nella raccolta dei dati del look-up.'</sample>
    <sample id="285">Esampliamo mille donne specifiche e valutiamo le scripte generate da grandi modelli.</sample>
    <sample id="286">Il contenuto inglese relativo alla tabella riporta che tutti i modelli di rimozione lineare raggiungono risultati soddisfacenti per quanto riguarda la pianificazione specifica delle donne.</sample>
    <sample id="287">Allora condurremo un'analisi dettagliata per investigare perché i modelli di apprendimento automatico falliscono.</sample>
    <sample id="288">I risultati nella figura mostrano che la completezza semantica nei script generati è accettabile, ma non si può garantire fedeltà alle restrizioni.</sample>
    <sample id="289">Il contenuto inglese descrive come le prestazioni di pianificazione degli insegnanti possono variare considerevolmente per diverse categorie di studenti, mostrato dal tassello principale della figura. In italiano, questo sarebbe: 'Le prestazioni di pianificazione degli insegnanti variano notevolmente per le diverse categorie di studenti, come mostra la mappa chiave della figura.'</sample>
    <sample id="290">I risultati precedenti hanno mostrato che la qualità dell'output dei modelli di linguaggio varia notevolmente, causando prestazioni basse. Di conseguenza, abbiamo adottato l'idea di un filtro di generazione overriden per migliorare la qualità della generazione.</sample>
    <sample id="291">In primo luogo mostriamo i tipi di vincoli con esempi per l'ICPCT e otteniamo obiettivi specifici basati su tali obiettivi astratti.</sample>
    <sample id="292">Il contenuto inglese si traduce in italiano come: 'Quindi, istruisci GPT over generatori di descrizioni per ogni girl.'</sample>
    <sample id="293">Il contenuto inglese riportato in italiano è: 'Successivamente, viene sviluppato un modello di filtro per selezionare gli script più fatti.'</sample>
    <sample id="294">Il contenuto inglese descrive come convertire script e goi in istruzioni GPB per calcolare la相似度 di similità usando cosine similarity e score di相似ità. In italiano, questo sarebbe: 'Convertiamo i script e gli goi in istruzioni GPB e calcoliamo la similità di similità utilizzando la similità di senso e le valutazioni di相似ità.'</sample>
    <sample id="295">Inoltre, identifichiamo la riga di script che contiene le parole chiave del vincolo target. Siamo interessati solo alle righe di script se il valore del ghost del target è il più alto nella finestra dei ghost.</sample>
    <sample id="296">Il contenuto inglese descrive un metodo che utilizza insieme a IPB per generare fili di alta qualità e migliorare la planificabilità, sia nella completezza semantica che nella fedeltà alle restrizioni. In italiano, questo sarebbe: "Con il nostro metodo, l'IPB può generare fili di alta qualità e migliorare la planificabilità, sia nella completezza semantica che nella fedeltà alle restrizioni."</sample>
    <sample id="297">I modelli di lingua sono costosi da eseguire, quindi è essenziale consentire la capacità di pianificazione della lingua ai modelli più piccoli e specializzati. La creazione del dataset è un passo importante per raggiungere questo obiettivo.</sample>
    <sample id="298">Il contenuto inglese dice: 'Tuttavia, gli studi precedenti non pianificano in anticipo per obiettivi specifici e l'annotazione manuale del dataset è costosa.'</sample>
    <sample id="299">Il contenuto inglese descrive l'idea di una 'distillazione simbolica della conoscenza', che consiste nel separare i dati di pianificazione del linguaggio da modelli di linguaggio generale per ottenere informazioni più rilevanti e utili. In italiano, si dice: 'Si segue l'idea di una 'distillazione simbolica della conoscenza', che consiste nel separare i dati di pianificazione del linguaggio da modelli di linguaggio generale per ottenere informazioni più rilevanti e utili.'</sample>
    <sample id="300">Applicheremo il nostro metodo per la costruzione di un set di dati di pianificazione del linguaggio connesso, chiamato CoScript.</sample>
    <sample id="301">In totale, generiamo cinquantamila esempi specifici con script. Per garantire la qualità della validazione e dei siti di test, chiediamo ai lavoratori del crowdsource di revisionare gli esempi errati o corrotti.</sample>
    <sample id="302">Questo grafico mostra la distribuzione del codice sottoposto a vincoli. Ciò significa che i codici hanno una bassa variabilità e mostrano un alto clonaggio. CoS script è stato utilizzato per generare questo risultato specifico. Con CoS script, possiamo utilizzare modelli più piccoli ma specializzati per pianificazione del linguaggio con vincoli.</sample>
    <sample id="303">Il contenuto inglese dice che 'T5L-1200 con una velocità di esecuzione di 1200 può generare script di qualità della pelle migliore rispetto ai modelli più grandi, indicando che i modelli più piccoli possono essere sostenuti da modelli più grandi se addestrati correttamente sui dati adatti.' In italiano, questo significa che il modello T5L-1200, con la sua velocità di esecuzione di 1200, è in grado di creare script di qualità della pelle migliore rispetto ai modelli più grandi. Ciò suggerisce che i modelli più piccoli possono essere utilizzati come base per modelli più grandi, a condizione che siano adeguatamente addestrati sui dati appropriati.</sample>
    <sample id="304">In sintesi, abbiamo stabilito il problema della pianificazione del linguaggio con restrizioni, abbiamo valutato l'abilità di modelli di grande lingua nella pianificazione del linguaggio con restrizioni e sviluppato un metodo di filtro di sovrapposizione generale per i modelli di grande lingua.</sample>
    <sample id="305">Utilizziamo grandi modelli di linguaggio per generare un set di dati di script di alta qualità, CoSQL, per la pianificazione della lingua. Speriamo che il set di dati di CoSQL possa diventare una risorsa valiosa per la ricerca sulla pianificazione della lingua.</sample>
    <sample id="306">Grazie per il tuo tempo, ci vorrebbe maggiori dettagli sul codice sorgente nel nostro documento.</sample>
    <sample id="307">La fluidità di PaLM è comparabile alla qualità degli altri sistemi di artificiale intelligenza.</sample>
    <sample id="308">Il metodo di filigrana deve essere applicabile agli impianti di servizio, non degradare l'utilizzo degli impianti forniti, essere abbastanza nascosto per renderlo difficile da rimuovere per gli attaccanti e trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello.</sample>
    <sample id="309">I discorsi TED in inglese sono stati tradotti in 14 lingue diverse.</sample>
    <sample id="310">Solo poche istanze vengono campionate da un set di dati per la riannotazione.</sample>
    <sample id="311">La differenza tra i set di dati benigni e backdoor viene misurata utilizzando la metrica di相似ità Cosine.</sample>
    <sample id="312">I modelli basati su codificatori multilingue sono stati valutati su due gruppi di modelli, tra cui encoder PDR che si riferisce a encoder multilingue con decodificatori basati sui punti, e encoder-decoder modeli, che sono modelli multilingua tratti da encoder-decoditori.</sample>
    <sample id="344">Gli autori selezionano un set di parole a frequenza moderata scegliendo un gruppo di parole in un intervallo di frequenza moderato e suppongono che il fornitore possa raccogliere un corpus generale di testo per contare la frequenza delle parole.</sample>
    <sample id="345">Ciao a tutti, il mio nome è Zhuoheng. Oggi vi presenterò il nostro paper "Do Conll 2003 named entity tagger still work well in 2023?". Andiamo avanti.</sample>
    <sample id="346">Il nostro paper ha esplorato il problema della generalizzazione utilizzando il compito di riconoscimento degli entità chiamato anche task NER.</sample>
    <sample id="347">I modelli sono stati utilizzati per sviluppare l'NER da oltre venti anni ed ovviamente questo ha portato a diversi problemi. In primo luogo, questi modelli possono essere generalizzati ai dati più recenti?</sample>
    <sample id="348">Nel momento in cui sviluppiamo nuovi tag, cosa è necessario per una buona generalizzazione?</sample>
    <sample id="349">Nel frattempo, se osserviamo una cattiva generalizzazione, cosa causa la diminuzione delle prestazioni dei modelli?</sample>
    <sample id="350">Per indagare su questi problemi, abbiamo sviluppato l'insieme di dati Connel Plus Plus. Questo è un insieme di dati che abbiamo raccolto da Reuters News dal 2020 e poi annotato con le stesse linee guida di annotazione del 2023.</sample>
    <sample id="351">L'audio descrive come siamo stati in grado di ottimizzare oltre venti modelli su Convolv层 2003, valutandoli sia sui set di dati di test Convolv3 che su quelli di Convolv+Plus.</sample>
    <sample id="352">In ogni caso, abbiamo calcolato la variazione percentuale di F1 per valutare la generalizzazione di ogni modello.</sample>
    <sample id="353">Così, cosa serve per una buona generalizzazione? Dopo molti esperimenti, abbiamo scoperto che ci sono tre ingredienti principali necessari.</sample>
    <sample id="354">Il primo è l'architettura del modello. I nostri esperimenti hanno dimostrato che i modelli di trasformatore generalizzano通常 meglio ai nuovi dati.</sample>
    <sample id="355">Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che generalmente i modelli più grandi conducono a una migliore generazione.</sample>
    <sample id="356">Il contenuto in inglese è stato tradotto in italiano come segue:
'Inoltre, non dimentichiamo che il numero di esempi di regolazione fine direttamente influisce sulla prestazione delle attività di downstream. Di qui, abbiamo anche scoperto che più esempi di regolazione fine in realtà conducono anche a una migliore generalizzazione.'</sample>
    <sample id="357">Al prossimo quesito, cosa causa la diminuzione delle prestazioni di alcuni modelli?</sample>
    <sample id="358">Abbiamo due ipotesi. La prima è l'adattamento overfitting, che si verifica quando si utilizza lo stesso set di test più e più volte, causando la diminuzione delle prestazioni su nuovi dati. Questo fenomeno viene solitamente manifestato come un calo dei risultati sul nuovo set di test.</sample>
    <sample id="359">La seconda ipotesi è che 'tempo di drift', che è la diminuzione della prestazione causata dall'aumentare la differenza termica tra il treno e i dati di test.</sample>
    <sample id="360">Per la regola dell'adattamento, abbiamo visto che dalla grafica a destra, la linea rossa di adattamento ha un gradiente maggiore di uno.</sample>
    <sample id="361">Questo significa che ogni unità di miglioramento apportata su Connel due mila e tre si traduce in più di un'unità di miglioramento su Connel plus plus, il che significa che non ci sono riduzioni dei rendimenti.</sample>
    <sample id="362">Questo mostra che nell'ambito di questo caso non si osserva l'adattamento.</sample>
    <sample id="363">Quindi cosa succede al cambiamento climatico?</sample>
    <sample id="364">Per il drift termale, abbiamo eseguito un esperimento per riaddestrare o continuare ad addestrare alcuni modelli con dati più recenti e abbiamo scoperto che il rendimento decade con un intervallo temporale maggiore.</sample>
    <sample id="365">Questo conferma la nostra ipotesi che il principale fattore alla base della diminuzione delle prestazioni sia il drift temporale.</sample>
    <sample id="366">La conclusione è che per una buona generalizzazione, avremmo bisogno di un'architettura del modello migliore, dimensioni di modello più grandi e esempi più fini. Questi obiettivi vanno di mano in mano: non possiamo avere solo uno ingredientio, ma tutti gli altri.</sample>
    <sample id="367">Nel frattempo, abbiamo anche scoperto che la diminuzione delle prestazioni qui è causata da un cambiamento di temperatura e, sorprendentemente, non è causata dall'adattabilità all'ambientazione, anche se il modello Colonnello è stato utilizzato per più di venticinque anni.</sample>
    <sample id="368">Quindi, tornando al problema sollevato nella intestazione del nostro articolo, i taggati di Connel 2003 funzionano ancora nel 2023? Ebbene, abbiamo scoperto che la risposta è sorprendentemente positiva.</sample>
    <sample id="369">Speriamo che il nostro corso di paper incoraggi più ricerche sugli miglioramenti per le generazioni dei modelli.</sample>
    <sample id="370">E, per finire, assicurati di controllare il nostro giornale, il nostro dataset e se hai domande, non esitare a contattarmi. Grazie mille.</sample>
    <sample id="397">The solution uses a small segment approach.</sample>
    <sample id="398">Per identificare correttamente l'entità che il pronome 'he' fa riferimento, sono necessari due tipi di informazioni: la conoscenza specifica dell'entità (in questo caso, che Servin è un giudice) e le conoscenze generali (ad esempio, sulla posizione o attività di servizio).</sample>
    <sample id="399">La qualità dell'esempio è più importante della somiglianza con la frase sorgente.</sample>
    <sample id="400">L'articolo si concentra sui modelli linguistici GPT-4 e i suoi varianti, in particolare sulla loro posizione sul liberalismo sociale.</sample>
    <sample id="401">Il modello combina i punteggi di più livelli.</sample>
    <sample id="402">Inferenza diretta si riferisce all'utilizzo di informazioni immediate per dedurre una conoscenza o una verità senza passare attraverso l'induzione o la deduzione indiretta. Esempi di inferenza diretta includono il dire il nome di un brano musicale o la sua posizione nella lista degli album.</sample>
    <sample id="403">I membri dell'équipe sono appartenenti all'Università di Fudan.</sample>
    <sample id="404">Un solo autore, Yannick Lavaur.</sample>
    <sample id="405">Yes, using an automatic translation API to translate the source code to the target language before parsing it for semantic analysis has been considered a standard approach.</sample>
    <sample id="406">Il gruppo contrassegnato è 'woman warrior'.</sample>
    <sample id="407">Le architetture dei modelli trasformattori non generalizzano通常 in modo adeguato.</sample>
    <sample id="408">I nomi dei set di dati di test sono 'clean' e 'wsl'.</sample>
    <sample id="409">Due.</sample>
    <sample id="410">L'autore utilizza il testo per operare.</sample>
    <sample id="439">L'autore afferma che l'NLU si concentra troppo sui modelli basati su regole e non enough sull'utilizzo di conoscenza pre-train ed inferenza.</sample>
    <sample id="440">Inni, Michael, Qingyang e l'autore del post.</sample>
    <sample id="441">Sì, i test di qualità sono stati eseguiti sui coscript generati.</sample>
    <sample id="442">Le risorse esistenti per la traduzione dipendente dal contesto supportano solo un numero limitato di tipi di traduzioni e di lingue, poiché si basano generalmente sulle conoscenze di dominio e sulla creazione umana.</sample>
    <sample id="443">Ciao e sto per parlare del nostro lavoro sulla risoluzione di espressioni indirette per la selezione di entità, in cui introduciamo il concetto di score di identità.</sample>
    <sample id="444">Il mio nome è Javad Hosseini e questo è un lavoro congiunto con Philip Radoszynski, Sylvia Parity e Anne Lewis.</sample>
    <sample id="445">Il nostro obiettivo è capire il linguaggio degli utenti quando vogliono fare una scelta e consideriamo questa domanda alternativa: "Hai inteso 'facile da me' o 'ho avuto un senso?' Qui, l'utente vuole selezionare tra questi due siti.</sample>
    <sample id="446">Il più ovvio è utilizzare un riferimento diretto, ad esempio dicendo il nome della canzone 'Eminem' o la sua posizione nella classifica.</sample>
    <sample id="447">Ma a volte è più appropriato avere un'indirezione per una conversazione più naturale, questo potrebbe accadere quando l'utente non ricorda il nome della persona.</sample>
    <sample id="448">Tutte le pronunce sono troppo simili tra loro e difficili da distinguer.</sample>
    <sample id="449">Il contenuto inglese descrive come un utente possa specificare una preferenza direttamente, fornendo alcuni esempi di differenze espresse in modo diretto, come 'il nuovo one' o 'the sign that's not energetic'. In italiano potremmo tradurre come: 'O quando l'utente vuole specificare una preferenza, qui ci sono alcuni esempi di differenze espresse in modo diretto, ad esempio il "nuovo" o il "segno che non è energetico".</sample>
    <sample id="450">Questo è un problema importante nei sistemi di conversazione e anche per la classificazione degli enti LLM.</sample>
    <sample id="451">Non siamo a conoscenza di un set di dati pubblico a larga scala per questo compito, quindi abbiamo raccolto uno utilizzando la crowdourcing. Il nostro set di dati copre tre diversi domini: musica, libri e moda.</sample>
    <sample id="452">La nostra metodologia di raccolta dei dati enfatizza l'informalità utilizzando un set di completamento di cartone.</sample>
    <sample id="453">Il cartone ha tre punti di dialogo. Nel primo, Bob dice 'Ricorda quella canzone che stavamo ascoltando ieri'. Con questo, Bob fornisce il contesto della conversazione.</sample>
    <sample id="454">In questo secondo spazio di parola, Alice dice: 'Tu intendi "facile da fare" per me o ho avuto un'idea?'</sample>
    <sample id="455">Il contenuto inglese dice: 'Which is the alternative question? And in the third speech bubble, Bob uses an indirect reference to select one of these entities, for example, the New Earth.' La traduzione italiana sarebbe: 'Quale è la domanda alternativa? E nella terza bolla di discorso, Bob utilizza un riferimento indiretto per selezionare uno di questi entità, ad esempio, la Nuova Terra.'</sample>
    <sample id="456">Il primo e il secondo speech bubble vengono selezionati automaticamente, ma il terzo viene popolato dall'editor. Il primo speech bubble viene scelto da alcuni punti manuali per ogni pagina.</sample>
    <sample id="457">Il secondo punto, che è la domanda alternativa, viene generato come segue:</sample>
    <sample id="458">Noi sempre utilizziamo un semplice modello di template, tu intendi A o B? Ovvero A e B sono esempi prelevati da Wikipedia?</sample>
    <sample id="459">I diversi metodi di campionamento che abbiamo utilizzato diventano sempre più simili man mano che ci spostiamo verso il basso nell'elenco, e diventa generalmente più difficile effettuare l'analisi disgiuntiva.</sample>
    <sample id="460">Il primo è 'Trasformatore uniforme'.</sample>
    <sample id="461">Il secondo caso è quando le entità hanno titoli simili, ad esempio due libri con il nome 'The Retail'.</sample>
    <sample id="462">Il terzo è quando hanno descrizioni simili su Wikipedia e, infine, quando hanno informazioni o attributi simili su Wikipedia, ad esempio lo stesso genere per un film o lo stesso artista per un album.</sample>
    <sample id="463">Quando mostriamo questa domanda alternativa agli stakeholder, sanno il nome di queste entità, ma non necessariamente conoscono l'entità stessa.</sample>
    <sample id="464">Quindi, ciò che facciamo è mostrare alcune conoscenze di base sui tardi '2000 per le canzoni. Abbiamo semplicemente linkato ciascuna canzone a un ricerca Google.</sample>
    <sample id="465">Ecco un esempio dei risultati della ricerca in italiano: 'Quindi chiedi agli annotatori di ascoltare almeno alcuni brani di ogni canzone e di leggere su ogni canzone. Ecco un esempio dei risultati della ricerca in italiano:'</sample>
    <sample id="466">Per il dominio dei ricette e dei libri, mostriamo alcuni testi di sfondo da Wikipedia. Per le ricette, mostriamo inoltre le loro immagini nuovamente prese da Wikipedia, in modo che gli annotatori sappiano come devono apparire.</sample>
    <sample id="467">Quindi chiediamo agli etichettatori di scegliere uno degli entità, ad esempio l'ultimo elemento qui e descriverlo usando tre a cinque espressioni indirette.</sample>
    <sample id="468">Per esempio, il primo con la musica per pianoforte. Ecco alcuni esempi dal nostro set di dati. Ad esempio, quello senza parole, non quello con il ragazzo di dodici anni, quello fantasy o quello proveniente dall'Azerbaigian e così via.</sample>
    <sample id="469">Il corpus di entità ha sei migliaia di domande alternative su tre domini e ha quarantadue migliaia di espressioni indirette che si riferiscono ai risultati con il modello T5 large.</sample>
    <sample id="470">Se il modello linguistico ha accesso al medesimo background conoscenza degli annotatori, allora l'accuratezza è veramente alta, intorno al 92-95%. Ma questo non è realistico.</sample>
    <sample id="471">Se il modello linguistico ha accesso a alcune conoscenze di sfondo parzialmente sovrapposte, allora l'accuratezza è compresa tra il 82% e il 87%, che è più realistica. Ad esempio, quando il modello linguistico recupera le conoscenze di background.</sample>
    <sample id="472">Se il modello linguistico ha accesso solo ai nomi degli entità, allora l'accuratezza è del 60%. Ci sono quindi molte opportunità per migliorare. Abbiamo anche mostrato che i modelli sono generalizzabili per domini. Ecco un link al nostro set di dati. Grazie.</sample>
    <sample id="473">Con le politiche SimulST che vengono applicate anche ai modelli online.</sample>
    <sample id="474">I due autori dell'articolo sono Yannick Lavaur e Bertrand.</sample>
    <sample id="475">Jenny</sample>
    <sample id="476">Due.</sample>
    <sample id="477">Ciao, sono Sarah Papa da New York University di Toronto e sono la guida per il paper sulla traduzione del discorso simultaneo. È un lavoro congiunto con McTeague e Mark.</sample>
    <sample id="478">La traduzione simultanea del discorso è il processo di tradurre il linguaggio parlato in un testo in un'altra lingua in tempo reale, consentendo una comunicazione bidirezionale incrociata.</sample>
    <sample id="479">I problemi dei modelli di sintesi attuali sono che vengono spesso addestrati con architetture specifiche, il che significa che devono essere aggiunti moduli per essere ottimizzati.</sample>
    <sample id="480">Procedimenti di addestramento complessi e lunghi, ad esempio l'addestramento che coinvolge obiettivi di ottimizzazione diversi.</sample>
    <sample id="481">Il contenuto in inglese si traduce in italiano come: 'L'addestramento e la manutenzione di diversi modelli per raggiungere regimi di latenza diversi, ad esempio l'addestramento di un modello con una media di una seconda di latenza e un altro con due secondi di latenza e così via.'</sample>
    <sample id="482">La soluzione è: 'Quindi cosa è la tua soluzione?'</sample>
    <sample id="483">Il contenuto in inglese dice: 'First, use already existing off-the-shelf models without retraining or adopting specific architecture for symbol AI; use only one model for every latency regime and handle latency through specific parameters.'</sample>
    <sample id="484">L'utilizzo della conoscenza acquisita dal modello attraverso il meccanismo di attenzione tra input audio e output testuale, cioè il meccanismo di attenzione crociata, è descritto nella frase 'Leveraging knowledge already acquired by the model through the attention mechanism between audio input and textual output, that is the cross-attention mechanism'. In italiano, questo significa che si sta sfruttando la conoscenza già acquisita dal modello utilizzando il meccanismo di attenzione crociato tra l'input audio e l'output testuale.</sample>
    <sample id="485">La soluzione proposta consiste nell'aggiungere un punto o un codice di attenzione al testo, che rappresenta una strategia per decidere se emettere o meno una traduzione parziale basata sulle posizioni dell'attenzione nel testo originale.</sample>
    <sample id="486">Se la tensione non è concentrata, si emette un segnale di errore se il valore della somma è inferiore ad un certo threshold alpha rispetto ai ultimi n frame di speech. Ciò significa che i dati ricevuti sono abbastanza stabili.</sample>
    <sample id="487">Per esempio, se riceviamo un segmento di testo contenente 'Sono pronto a parlare di questo', e il nostro modello prevede una traduzione in tedesco.</sample>
    <sample id="488">Inglese: 'And we will look at the cross-section weights.' Italiano: 'E guarderemo le masse della sezione trasversale.'</sample>
    <sample id="489">Il contenuto in inglese dice: 'We will see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames.'</sample>
    <sample id="490">Questo significa che le prime due parole verranno eliminate.</sample>
    <sample id="491">Il contenuto inglese dice: 'Mentre la somma della tensione crociata è superiore a un certo valore di alfa, non lasceremo fuori le ultime parole e aspetteremo l'altro intervento.'</sample>
    <sample id="492">Se continuiamo e riceviamo un'altra frase, e il nostro modello prevede tre parole, guarderemo le loro pesi di attenzione crociata.</sample>
    <sample id="493">Inglese: We will see that no word points to the last lambda-lambdas speech frames.
Italiano: Vedremo che nessuna parola punta ai frame del discorso lambda-lambda ultimi.</sample>
    <sample id="494">Questo significa che queste tre parole verranno eliminate.</sample>
    <sample id="495">Se guardiamo i principali risultati di quel periodo.</sample>
    <sample id="496">Applicheremo i risultati della traduzione simultanea su grafici in cui abbiamo blu sul lato uno che misura la qualità della traduzione e media leggibile.</sample>
    <sample id="497">Il contenuto in inglese è: 'Quindi c'è la misura di latenza e consideriamo anche l'indice medio computazionale di ascolto, che rappresenta i tempi di calcolo del modello per produrre l'output.'</sample>
    <sample id="498">Quindi vogliamo che i nostri cibi siano il più salutari possibile su questo terreno.</sample>
    <sample id="499">Ma anche noi vogliamo che siano spostati a sinistra.</sample>
    <sample id="500">Inoltre, confrontiamo con strategie appropriate applicate a modelli online, come la strategia di Whitkeys e l'accordo locale, e confrontiamo anche l'architettura del sito Web specificamente progettata per la traduzione simultanea della pagina.</sample>
    <sample id="501">Questi sono i risultati della strategia di traduzione del discorso contemporaneo sulla Germania.</sample>
    <sample id="502">Il contenuto in inglese dice: 'Ecco cosa accade: l'auto si comporta meglio di tutte le strategie applicate ai modelli offline, poiché le curvate sono spostate verso sinistra.'</sample>
    <sample id="503">Inoltre, se consideriamo il tempo effettivo di esecuzione o il tempo di elaborazione computazionale, quella è la strategia più veloce.</sample>
    <sample id="504">Se vuoi scoprire più risultati, leggi il nostro articolo e abbiamo anche pubblicato il codice sorgente, modelli e output condivisi per rendere il nostro lavoro riproducibile. Grazie per l'attenzione!</sample>
    <sample id="505">Il contenuto inglese non specifica se il set di dati sia disponibile pubblicamente o meno.</sample>
    <sample id="506">Il contenuto in inglese dice: 'Hello everyone, my name is In and Myeong-Ki Choy and I will be presenting our research on multi-instructed learning by instruction tuning.'</sample>
    <sample id="507">Con l'avanzamento dei grandi modelli di lingua, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento per la riconoscenza del linguaggio utilizzando i modelli di lingua pre-trainati per diverse attività di calcolo parallelo nella parameterizzazione e nell'efficienza dei dati.</sample>
    <sample id="508">Nel主要内容 in inglese, si dice che recentemente molti studi hanno dimostrato che l'adattamento delle istruzioni consente ai modelli di grande lingua di svolgere compiti non noti in modo efficiente seguendo le istruzioni naturali. In italiano, questo sarebbe: "Recentemente, molti studi hanno mostrato che l'adattamento delle istruzioni consente ai modelli di grande lingua di svolgere compiti non noti in modo efficiente seguendo le istruzioni naturali."</sample>
    <sample id="509">Il contenuto inglese dice: 'Tuttavia, la maggior parte dei lavori precedenti sull'adattamento dell'instruzione si è concentrata sul miglioramento delle prestazioni di zero a zero sui compiti linguistici unici, mentre la visione computazionale e i modelli multietichettati sono stati lasciati fuori.'</sample>
    <sample id="510">Quindi, in questo lavoro, vogliamo indagare se l'adattamento delle istruzioni su modelli multilabel può effettivamente migliorare la generazione di risultati multi-modello per le attività multilabel.</sample>
    <sample id="511">Inoltre, durante la nostra ricerca, abbiamo scoperto una notevole disparità nell'accessibilità dei set di istruzioni tra LP e modelli multiModal.</sample>
    <sample id="512">Il contenuto in inglese dice: 'Esiste più di un migliaio e seicento task di istruzione univoca, tuttavia non esiste alcun set di addestramento multil modello di grande scala pubblicamente disponibile. Ciò ci demotiva a costruire un set di addestramento multil modello per la regolazione dell'istruzione.'</sample>
    <sample id="513">In questo articolo presentiamo il primo set di dati di confronto multietichettato per l'addestramento del modello multi-instruzione, che consiste di sessanta due compiti multietichettati diversi che coprono dieci categorie di etichette.</sample>
    <sample id="514">Questi compiti sono tratti da un set di dati open source esistente e ogni compito è fornito di cinque istruzioni scritte dagli esperti.</sample>
    <sample id="515">Per indagare sulla regolazione dell'instradamento multilivello, i nostri dati di base proposti sono i seguenti: utilizziamo un modello unificato multilivello come nostro modello di riferimento. Il modello unificato utilizza una lingua unica per i token di linguaggio, i token di immagine e le coordinate delle caselle di bound.</sample>
    <sample id="516">Nel nostro set di dati multi-inserimento ci sono alcune istanze di esempio.</sample>
    <sample id="517">Il contenuto in inglese dice: 'To unify the processing of various input and output data types.' In italiano, questo significa: 'Unificare il processamento di diversi tipi di dati d'input e di output.'</sample>
    <sample id="518">Il contenuto in inglese dice: 'We follow the method from OFA and formulate all tasks in a unified sequence-to-sequence format, in which the input text, images, instructions, and bounding boxes are represented in the same token space.' In italiano, questo significa: 'Seguiamo il metodo di OFA e formuliamo tutte le attività in un formato di sequenza unificato-a-sequenza, in cui il testo d'ingresso, le immagini, le istruzioni e i box di confine sono rappresentati nello stesso spazio di token.'</sample>
    <sample id="519">Ottimo, adesso parlo di regolazione dell'instradamento multi-modello.</sample>
    <sample id="520">Per il set di dati di addestramento, utilizziamo cinquantatré task dal gruppo di riferimento per l'addestramento e ne prendiamo esempi diecimila per la prova. Riserviamo interamente il gruppo di ragionamento comune per la prova e selezioniamo altrettante altre cinque task dal gruppo di VQ-VAE.</sample>
    <sample id="521">I utilizzi dell'intera istanza nel set di esercizi per ogni task. Inoltre, si estrae casualmente un campione di 20 task dal set di esercizi di istruzione naturale come singolo task LP.</sample>
    <sample id="522">Invece di utilizzare un modello pre-trainato come base, abbiamo deciso di creare uno da zero. Durante l'addestramento, ogni istanza viene combinata casualmente con una delle cinque istanze di riferimento.</sample>
    <sample id="523">Durante il testo per ogni task, eseguiamo un totale di cinque esperimenti valutando il modello utilizzando uno degli cinque istruzioni in ogni esperimento.</sample>
    <sample id="524">Il contenuto in inglese è stato tradotto in italiano come segue:

'Per ogni esperimento,报告平均值和最大值的性能以及标准偏差。'</sample>
    <sample id="525">Se la tarefa è una classificazione multimodale, si汇报 accuracy. Se è una generazione di modelli multimodali, si汇报 l'algoritmo used. Se è una tarefa di generazione di dati LP, si汇报 l'algoritmo used as well.</sample>
    <sample id="526">Abbiamo anche introdotto una metrica di valutazione supplementare chiamata sensibilità, che misura la capacità del modello di generare sempre gli stessi output per lo stesso compito, indipendentemente dalla variazione leggera nella direzione dell'istruzione.</sample>
    <sample id="527">Il nostro principale risultato è che l'adattamento delle istruzioni può migliorare significativamente le prestazioni di OFA su compiti multietichettati.</sample>
    <sample id="528">Il contenuto in inglese dice: 'Transfer learning da un set di dati di istruzione naturale può beneficiare dell'addestramento dell'instruzione.'</sample>
    <sample id="529">Il contenuto in inglese dice: 'Qui vediamo come l'aumento del numero di task aumenti le prestazioni del modello e riduca la sensibilità al tempo medio.'</sample>
    <sample id="530">Così abbiamo anche fatto un esperimento. Abbiamo utilizzato una istruzione contro cinque istruzioni. Come possiamo vedere, l'utilizzo di più istruzioni può migliorare le prestazioni del modello e ridurre la sua sensibilità molto.</sample>
    <sample id="531">Questo mostra l'effetto di diverse strategie di regolazione sulle sensibilità del modello. Come possiamo vedere, trasferendo l'apprendimento da un set di dati di istruzione naturale, il modello può ottenere una sensibilità molto migliore rispetto al modello OFA originale.</sample>
    <sample id="532">Il contenuto in inglese dice: 'We also can see transfer learning from natural instruction dataset can help OFA to achieve much better performance on the nitrogen instruct dataset.'</sample>
    <sample id="533">In sintesi, proponiamo il primo grande set di dati di addestramento multilivello per migliorare la capacità di rilevamento del segnale dell'IFV e esplorare diverse tecniche di apprendimento trasferito e mostrare i loro beneficrivibili vantaggi. Abbiamo progettato una nuova misura chiamata sensibilità.</sample>
    <sample id="534">Il contenuto in inglese dice: 'Questo è il codice QR per i nostri dati e il modello. Grazie.'</sample>
    <sample id="535">Gli autori dell'articolo sono Sarah Papa e Bruno Kassler, entrambi appartenenti all'Università di Toronto.</sample>
    <sample id="536">Jabot Hossaini</sample>
    <sample id="562">Ciao a tutti, sono il prof. Cosimo Sessa e sono lieto di accogliervi nel nostro talk sul nostro articolo ACER 2023 intitolato "Valutazione dell'adeguatezza dei modelli linguistici: non sempre robusti ai contesti".</sample>
    <sample id="563">Il lavoro congiunto è stato realizzato con John Goughery, Aaron Miller, Kanishka Mishra, Karen Fuentes, Roger Levy e Atina Williams.</sample>
    <sample id="564">In questo lavoro, esploriamo il paradigma del minor numero di bit.</sample>
    <sample id="565">Il minimal pair è fondamentalmente un metodo per valutare i modelli linguistici sulla base dei giudizi di compatibilità, che possono includere anche fattori come la grammaticità (ad esempio 'pontificate' e ' pontificare'), o l'aderenza ai tipi di testo ('scrittori' e 'scrittore').</sample>
    <sample id="566">Nel minimalismo lessicale, il modo tipico per valutare i modelli linguistici è mostrare un frase accettabile o grammaticale, quindi una frase inaccettabile o non grammaticale.</sample>
    <sample id="567">La speranza è che il modello abbia aumentato la probabilità per i valori accettabili.</sample>
    <sample id="568">Il flusso attuale del MPMP non ci consente di valutare l'aderenza dei modelli alle frasi più lunghe.</sample>
    <sample id="569">Nel corso degli ultimi tempi, i grandi modelli di lingua stanno emergendo con finestre di contesto sempre più lunghe e lunghe. È quindi essenziale valutare l'adeguatezza dei modelli per tutto il contesto.</sample>
    <sample id="570">Ecco ciò che stiamo cercando di fare qui: stiamo cercando di ripetere il pipeline Pp attraverso chiedere al modello di valutare l'adeguatezza su sequenze sempre più lunghe.</sample>
    <sample id="571">Quindi, questo è l'approccio che seguiamo. Ciò che facciamo è visitare i dati stessi e poi ricreare le frasi scegliendo, ad esempio, frasi accettabili o inaccettabili da quei dati.</sample>
    <sample id="572">Per esempio, qui abbiamo scelto un paio di caratteristiche tipiche della variabile 'dramatismo' dal set di dati BIM dell'isola adjacente.</sample>
    <sample id="573">Cosa facciamo è che per creare sequenze più lunghe e accettabili con la stessa struttura grammaticale, estraiamo le frasi grammaticali da un elenco di parole.</sample>
    <sample id="574">E poi lo aggiungiamo come prefisso sia alla query accettabile che all'query non accettabile.</sample>
    <sample id="575">In italiano, il contenuto dell'audio sarebbe: 'Possiamo fare la stessa cosa selezionando frasi inaccettabili dal matching e questo potrebbe anche essere utilizzato per testare l'adeguatezza del modello.'</sample>
    <sample id="576">Ecco la traduzione in italiano: 'E possiamo anche fare lo stesso selezionando frasi da un insieme di differenza o da un altro set di dati, questo è ciò che chiamiamo scenario di mismatch.'</sample>
    <sample id="577">Quindi, qui i sintomi sono ancora provenienti da set di dati pertinenti, ma non dallo stesso set di dati che stiamo valutando, e possiamo fare lo stesso per casi di inaccettabilità.</sample>
    <sample id="578">Infine, possiamo scegliere delle frasi da un dominio completamente non relazionato, come Wikipedia.</sample>
    <sample id="579">Questo ci dirà se i giudizi di adattabilità del modello sono effettivamente influenzati da qualunque contesto.</sample>
    <sample id="580">Se il contesto proviene da un sottotipo diverso di set di dati o se è completamente irrilevante rispetto alla frase corrente che stiamo esaminando.</sample>
    <sample id="581">Quindi, come fa il modello? Innanzitutto, esaminiamo le frasi di Wikipedia che sono completamente inutili per la coppia di query corrente e lì scopriamo che i giudizi MPP sono per lo più robusti per contesti arbitrari.</sample>
    <sample id="582">Abbiamo aumentato il valore di lunghezza di contesto verso l'alto fino a 2024 per massimizzare i modelli OMP e GPT2, e abbiamo visto qui nella linea orizzontale gialla che le valutazioni MPP sono relativamente stabili.</sample>
    <sample id="583">Adesso, cosa succede quando scegliamo frasi dallo stesso set di dati?</sample>
    <sample id="584">Quindi, qui stiamo scegliendo o creando frasi da domini accettabili e inaccettabili dallo stesso set di sintassi BERT.</sample>
    <sample id="585">Ecco cosa accade: le valutazioni dell'MPP aumentano o diminuono significativamente se si aggiungono prefissi ammissibili o prefissi non ammissibili.</sample>
    <sample id="586">Ma quando si confrontano le strutture, ovvero quando si sceglie di prendere le stesse frasi dai medesimi fenomeni nella prova di colpa, è possibile individuare delle differenze significative tra i due gruppi.</sample>
    <sample id="587">vediamo un aumento significativo o una diminuzione significativa della valutazione MPP per il modello, a seconda whether the chosen prefix è accettabile o inaccettabile.</sample>
    <sample id="588">Ora, questo è molto grande. Questo effetto aumenta attraverso l'intero link di contesto e probabilmente avrà un impatto sui modelli di lingua più recenti che hanno un ampio finestre di contesto.</sample>
    <sample id="589">Quindi, perché il prefisso "match" influenza così tanto la valutazione del modello linguistico?</sample>
    <sample id="590">Abbiamo eseguito una serie di analisi dove abbiamo cercato di mantenere la struttura rilevante dell'input, aggiungendo rumore all'input. Dopo aver eseguito diverse di queste perturbazioni,</sample>
    <sample id="591">Non riscontriamo che questi rumori stiano effettivamente modificando il percorso del modello nella selezione dei pagine da mostrare.</sample>
    <sample id="592">In sostanza, scopriamo che i modelli sono sensibili alle permutazioni di sillabe e alle frasi simili.</sample>
    <sample id="593">Quindi, quando permutiamo le frasi nel dominio accettabile, vediamo un aumento simile in tutti i permutamenti e, quando permutiamo le frasi nel dominio non accettabile, vediamo una diminuzione dei giudizi di MPP in modo simile.</sample>
    <sample id="594">Il contenuto inglese descrive che 'I principali punti chiave del nostro lavoro sono che i modelli di linguaggio sono sensibili ai caratteristiche sintattiche e semantiche latenti condivise tra le frasi.' In italiano, questa frase si traduce come: 'I principali punti chiave del nostro lavoro sono che i modelli di linguaggio sono sensibili alle caratteristiche sintattiche e semantiche latenti condivise tra le frasi.'</sample>
    <sample id="595">La valutazione MPP attuale, che utilizza input corti e un centro unico di elaborazione, potrebbe non catturare completamente il conoscenza astratta delle modelli linguistici all'interno dell'interfaccia finestra contesto.</sample>
    <sample id="596">Per maggiori dettagli sui nostri esperimenti, vi preghiamo di leggere il nostro articolo. Grazie per l'ascolto.</sample>
    <sample id="597">Il primo passaggio del metodo mappa i token di input li etichetta con un set multiplo non ordinato di token che appaiono nella output.</sample>
    <sample id="598">Cinquecentoventisei script sono rappresentati in Coscript.</sample>
    <sample id="626">Il miglior metodo di allineamento per DEplain è il mass alignment utilizzando il codice fornito.</sample>
    <sample id="627">I vantaggi dell'apprendimento scarsamente supervisionato includono la capacità di adattarsi ai dati non corazzati, l'accesso a grandi quantità di dati non etichettati e la possibilità di utilizzare algoritmi less costosi.</sample>
    <sample id="628">I documenti in DEplan Web sono stati allineati sia manualmente che automaticamente.</sample>
    <sample id="629">Il set di dati CoNLL++ è stato creato utilizzando un dataset di notizie raccolte da Reuters dal 2020 e successivamente annotato con le stesse linee guida di annotazione del 2023.</sample>
    <sample id="630">Ciao a tutti, mi chiamo Jiaxin Zhang e sono dell'Università dello stato di Pennsylvania. Oggi vorrei presentare il mio lavoro sugli esempi di sintassi crosslinguistica in più lingue naturali e rappresentazioni mentali.</sample>
    <sample id="631">Il parsing semantico è un compito che consiste nel creare rappresentazioni semantiche di richieste degli utenti, come SQL e calcoli lambda.</sample>
    <sample id="632">Il contenuto in inglese si traduce in italiano come: 'L'analisi semantica crosslinguistica è il compito di tradurre le query in diverse lingue naturali in rappresentazioni significative multiple.'</sample>
    <sample id="633">Il contenuto in inglese dice: 'Sono stato mostrato nel grafico, abbiamo bisogno di tradurre la query in molte lingue naturali utilizzando modelli neurali, SQL, Lambda, funzione Python e così via.'</sample>
    <sample id="634">I modelli di sintassi semantica crociata esistenti sono proposti e valutati separatamente sui dati set di esempi limitati e applicazioni, ad esempio.</sample>
    <sample id="635">Il contenuto inglese si traduce in italiano come: 'Ci sono delle lacune di copertura per alcune lingue naturali, il cinese manca.'</sample>
    <sample id="636">I clienti hanno la copertura su alcune minacce specifiche.</sample>
    <sample id="637">Il calcolo di Lambda è mancante.</sample>
    <sample id="638">oppure vengono valutati solo su alcuni modelli più recenti, ad esempio ci sono solo un modello per valutare loro.</sample>
    <sample id="639">Quindi, per questo scopo, proponiamo un esempio di dataset uniforme che fornisce un set di dati omogeneo per la traduzione automatica di una persona in più lingue naturali e rappresentazioni.</sample>
    <sample id="640">Contiene nove dizionari in vari domini, cinquantasei parti di testo in Texas, otto milioni di rappresentazioni e ventiquattro lingue naturali in quindici famiglie di lingua.</sample>
    <sample id="641">Per valutare meglio il riferimento, consideriamo i sei setting per addestramento e valutazione.</sample>
    <sample id="642">Il primo è 'Traduci testo', che utilizzeremo per tradurre il testo di partenza nella lingua target. Successivamente, utilizzeremo un modello monolingue per addestrare e valutare.</sample>
    <sample id="643">E, per esempio, abbiamo addestrato un modello di inglese su query in inglese e durante l'infrazione, utilizziamo l'API per tradurre la query tedesca in inglese, quindi utilizziamo il modello addestrato per prevedere la risposta SQL.</sample>
    <sample id="644">Ecco la traduzione in italiano: 'Inoltre, testiamo anche il modello monolingue.'</sample>
    <sample id="645">In questo setting, la lingua di partenza è uguale alla lingua di destinazione, ad esempio tedesco a tedesco o inglese a inglese.</sample>
    <sample id="646">Inoltre, abbiamo testato la configurazione di esecuzione monolinguale utilizzando modelli multilingue con solo il 10% dei dati di addestramento.</sample>
    <sample id="647">Il contenuto in inglese dice: 'E abbiamo creato un modello monolingua multilingue, che abbiamo addestrato per tutte le lingue.'</sample>
    <sample id="648">Per esempio, abbiamo unito le query tedesche, inglesi e cinesi per addestrare un modello multilingue e, durante l'infrazione, possiamo utilizzare questo modello.</sample>
    <sample id="649">Il contenuto in inglese si traduce in italiano come: 'Per tradurre query tedesche o cinesi o altro.'</sample>
    <sample id="650">Inoltre, considereremo anche la traslitterazione di zero a zero e il trasferimento di tre a quattro caratteri tra lingue diverse. Addestriamo un modello su una sola lingua di partenza e lo trasferiamo ad un'altra lingua.</sample>
    <sample id="651">Durante la formazione, si addestrerà sulle query in inglese o sulla combinazione di query in inglese e tedesco per addestrare un modello multilingua per prevedere l'output SQL.</sample>
    <sample id="652">Ecco la traduzione in italiano: 'Ed anche abbiamo trovato molti risultati interessanti riguardo all'analisi di modelli monolingui. Abbiamo valutato due gruppi di modelli.'</sample>
    <sample id="653">Incluso encoder PDR, che sta per encoder multilingua a trasmissione di codici con decodificatori basati sui punti, come XLR + PDR e BERT + PDR.</sample>
    <sample id="654">E valutiamo anche i modelli di encoder-decoder multilingua, che sono modelli di codifica/decodifica multilingua come M-BART e MT5.</sample>
    <sample id="655">Il contenuto in inglese dice: 'Sono stati trovati che encoder e decoder ottengono il massimo rendimento su tutti i nove set di dati.'</sample>
    <sample id="656">E valutiamo m5 e esempi di XLMR più PDR in configurazione multilingua.</sample>
    <sample id="657">Il contenuto in inglese dice: 'Senza di esso, il codice sorgente o il decodificatore del codice QR può essere migliorato attraverso l'addestramento in una miscela di vari linguaggi.'</sample>
    <sample id="658">Ecco la traduzione in italiano: 'Si è scoperto che ciò accade perché la maggior parte dei principali linguaggi naturali può ottenere un aumento delle prestazioni, tranne l'inglese, il cui rendimento diminuisce in sette dataset e aumenta solo in tre dataset.'</sample>
    <sample id="659">Questo è conosciuto come 'curva di multilinguismo'.</sample>
    <sample id="660">Inoltre, abbiamo anche confrontato il divario di prestazioni tra i diversi linguaggi di programmazione.</sample>
    <sample id="661">In questo grafico, la linea blu indica il trasferimento di funzione crosslinguistico, la linea arancione indica il trasferimento di funzione zero-shot crosslinguistico, mentre la verde indica il setting della lingua modello.</sample>
    <sample id="662">Hanno scoperto che confrontando la linea verde e arancione, abbiamo trovato che il setting di corto-circuito zero ha un differenza significativa nel rendimento della trasmissione crosslinguaggio, mentre confrontando la linea blu e arancione, abbiamo trovato che il differenziale di trasmissione si è accorciato rapidamente per il setting di breve-circuito.</sample>
    <sample id="663">Inoltre, abbiamo trovato alcuni altri risultati interessanti, ad esempio l'encoder-decoder di performance, il lavoro svolto o i risultati ottenuti sono comparabili per quanto riguarda la traduzione automatica dell'inglese al naturale e significativamente migliorano le prestazioni di FusedBert su lingue target naturali diverse.</sample>
    <sample id="664">Il contenuto in inglese dice: 'E abbiamo trovato che i modelli di lingua multipli, come Codex e Blue, sono ancora adeguati per molte tipologie di parsing.'</sample>
    <sample id="665">Il contenuto in inglese dice: 'To summarize, we propose the exemplar-based unified benchmark for cross-lingual sentiment parsing with multiple natural languages and multiple representations.' In italiano, questo significa: 'Riassumendo, proponiamo un riferimento unificato basato sull'esempio per la 解析 del sentimento tra lingue diverse con più lingue naturali e rappresentazioni multiple.'</sample>
    <sample id="666">Conduce un'indagine di riferimento esaustiva su tre rappresentanti di tipi di modelli di lingua multilingue e i nostri risultati mostrano molte interessanti scoperte, etc. Ecco il benvenuto al nostro paper e al codice. Grazie per l'ascolto.</sample>
    <sample id="667">I lavori esistenti possono essere classificati in quattro categorie.</sample>
    <sample id="668">Sì, gli LLM multilingue come Codex o Bloom sono ancora adeguati per il CLSP.</sample>
    <sample id="695">Il metodo induce l'allineamento come parte del training per risolvere l'ambiguità delle permutazioni.</sample>
    <sample id="696">L'equità di un modello NLP a valle viene definita come la sua capacità di non discriminare o marginaleizzare i gruppi sociali sulla base delle loro opinioni politiche diverse, evitando l'autoinculpazione e la diffusione del hate speech targeting minoranze.</sample>
    <sample id="697">La relatrice si chiama Yannick Lavaur.</sample>
    <sample id="698">Coesteban</sample>
    <sample id="699">La relatrice si chiama Myra.</sample>
    <sample id="700">Il termine 'tropicalismo' indica un atteggiamento o uno stile associato ai paesi tropicali, caratterizzato da elementi come la vivacità, l'attrattiva, la morbidezza e la delicatezza.</sample>
    <sample id="701">Gli autori hanno definito i loro gruppi target in base alla relazione con l'identità, distinguevano tra di loro e il 'modello bianco'.</sample>
    <sample id="702">Il lavoro utilizza cxmi per misurare l'utilizzo del contesto.</sample>
    <sample id="703">DrBERT ha 7 GB di natura, mentre ChuBERT ne ha 4 GB. ChuBERT contiene inoltre 4 GB di testi presi da Wikipedia.</sample>
    <sample id="751">Due.</sample>
    <sample id="752">Il trasferimento iterativo dell'apprendimento è un metodo che consente di utilizzare le informazioni apprese da una fonte per migliorare l'apprendimento in un'altra. In questo caso, il modello viene aggiornato con i dati raccolti durante l'aggiunta di nuove annotazioni in modo da migliorarne continuamente la precisione.</sample>
    <sample id="753">Il set di dati ha lo scopo di comprendere il linguaggio degli utenti quando desiderano prendere una decisione.</sample>
    <sample id="754">Un utente malintenzionato può estrarre i parametri del modello attraverso un EaaS (Environment as a Service) utilizzando tecniche di reverse engineering o eseguendo attacchi di tipo SQL injection o cross-site scripting.</sample>
    <sample id="755">Due, McTeague e Mark Durkeri.</sample>
    <sample id="756">Il numero di annotatori utilizzati per creare il set di dati iniziale non è stato specificato nella descrizione fornita.</sample>
    <sample id="757">Gli autori dell'articolo sono Sevastian Santi, Ronan Le Bras, Katerina Raneva e Martin Schmid.</sample>
    <sample id="758">Il governatore in esempio è Lisa.</sample>
    <sample id="759">I modelli all'avanguardia nei sistemi di dialogo includono l'IA basata su intelligenza artificiale (AI) e la conversational AI, che utilizzano tecniche di apprendimento automatico per interagire con gli utenti in modo naturale e semplificato.</sample>
    <sample id="760">Perché i grandi modelli linguistici stanno diventando sempre più lunghi e complessi, è essenziale valutare l'accettabilità del modello nell'intera finestra di contesto per garantire che funzioni correttamente in tutte le situazioni possibili.</sample>
    <sample id="761">Sì, secondo quanto afferma l'intervistato, la formazione attraverso la modalità multilingua ha causato un calo delle prestazioni rispetto al modello inglese monolingue in sette dei nove dataset esaminati.</sample>
    <sample id="762">Sì, gli annotatori conoscono l'entità in anticipo.</sample>
    <sample id="763">Le metriche di valutazione utilizzate sono le precisione, recall e F1-score.</sample>
    <sample id="764">Sì, i modelli più grandi generalizzano通常 meglio.</sample>
    <sample id="765">La posizionalità è importante perché influisce sull'accuratezza e sulla comprensione del significato delle parole all'interno di un contesto specifico. In NLP, l'analisi della posizione delle parole aiuta a identificare con maggiore precisione i significati e le relazioni tra le parole, migliorando l'efficacia dei modelli di elaborazione del linguaggio.</sample>
    <sample id="766">I LLM multilingue come BLOOM sono stati affinati sia con adattatori che con una messa a punto integrale.</sample>
    <sample id="767">Il modello utilizzato per il trasferimento dell'apprendimento è noto come CE here, che si riferisce all'uso di dati etichettati per la classificazione binaria dell'estensione e della comparazione delle classi di PNTB.</sample>
    <sample id="768">I recenti set di test utilizzati per valutare le capacità di PaLM includono il test di generazione di testo di Common Sense, il test di sintesi di audio di Common Sense, e il test di risoluzione di problemi di Common Sense.</sample>
    <sample id="769">Gli autori hanno proposto tre suggerimenti agli proprietari di modelli.</sample>
    <sample id="770">Il metodo proposto mostra un aumento significativo delle prestazioni rispetto al metodo di riferimento in termini di tempo di esecuzione.</sample>
    <sample id="771">Il relatore si chiama Shuheng Zhuo.</sample>
    <sample id="772">Sì, i risultati e il set di dati dell'articolo possono essere utilizzati come parametri di riferimento per il problema della semplificazione del testo automatico in futuro.</sample>
    <sample id="773">L'articolo menziona che i modelli più piccoli possono essere in grado di sostenere modelli più grandi se addestrati correttamente sui dati appropriati. Tuttavia, non ci sono specifici numeri o dimensioni dei modelli menzionati nell'articolo.</sample>
    <sample id="774">Il modello di base utilizzato per analizzare l'ottimizzazione delle istruzioni multimodali è OFA, che utilizza un'unica lingua, immagini e coordinate di bounding box.</sample>
    <sample id="833">L'autore dell'articolo ha lavorato con i suoi colleghi di Google Translate.</sample>
    <sample id="834">Waseem Badri è un candidato al dottorato di ricerca in scienze informatiche presso l'università di Stony Brook.</sample>
    <sample id="835">L'articolo ha analizzato le coppie linguistiche 'state-of-the-art' e 'NLP metrics'.</sample>
    <sample id="836">Il relatore si chiama Shangbin Jiang.</sample>
    <sample id="837">I modelli di lunga e normale base sono stati studiati durante gli esperimenti.</sample>
    <sample id="838">Tutte le 62 attività sono utilizzate per scopi di addestramento e test.</sample>
    <sample id="839">Due.</sample>
    <sample id="840">I ricercatori hanno eseguito i test su quattro set di dati: AGnews, MNLI, SST-2 e MNLI-Full.</sample>
    <sample id="876">NACHOS è un insieme di dati medici raccolti dal web.</sample>
    <sample id="877">Il relatore si chiama Ali Bilad.</sample>
    <sample id="878">La strategia del prompting ha un'influenza significativa sui risultati della traduzione LLM.</sample>
    <sample id="879">L'autore principale dell'articolo, Kay O Yen, ha collaborato con Patrick Frenneaux, MEY Liu, Andrew FM Martinez e Graham Newby.</sample>
    <sample id="880">Istruzioni non fornite nella trascrizione originale dell'audio.</sample>
    <sample id="881">Gli autori propongono un task di risoluzione di riferimenti condivisi progettato per valutare la capacità di utilizzare conoscenza disponibile in diverse fonti.</sample>
    <sample id="882">Ciao a tutti, mi chiamo Ali Bilal e vi darò un breve riassunto del paper 'Pruning patterns for machine translation', che esplora le strategie e i risultati di prestazione. Questo è un lavoro condiviso con i miei colleghi di Google Translate.</sample>
    <sample id="883">Il modello di lingua PAMM è un modello di grandi dimensioni con 540 miliardi di parametri che è stato presentato l'anno scorso ed è basato su una grande raccolta di testi che comprende 1,8 miliardi di token.</sample>
    <sample id="884">Nel campo della fabbricazione, è stato raggiunto lo stato dell'arte in centinaia di task LP.</sample>
    <sample id="885">In questo lavoro, presentiamo la prima ricerca sistematica sul promemoria di modelli di grande lingua per la traduzione formale.</sample>
    <sample id="886">Abbiamo valutato la trasparenza dei modelli di ricerca utilizzando le migliori pratiche della comunità AMT. Ciò include l'utilizzo degli ultimi set di test per evitare un sovrapposizione dei dati di test con i dati di addestramento del modello di lingua.</sample>
    <sample id="887">Ecco la traduzione in italiano: 'Ecco un confronto tra due stati di sistema, quindi i sistemi con il migliore rendimento, basato sull'valutazione WMT.'</sample>
    <sample id="888">L'utilizzo di misure di stato dell'arte e URM è stato descritto, nonché i risultati dell'valutazione umana eseguita da esperti. Infine, sono state fornite alcune raccomandazioni per le strategie di selezione dei promettenti.</sample>
    <sample id="889">Il promettente ha un grande impatto sulla prestazione delle LLMs per la traduzione, come possiamo vedere in un semplice esperimento dove utilizziamo uno sharding del promettente e forniamo due promesse diverse per una frase data.</sample>
    <sample id="890">La maggior parte delle frasi, su un totale di mille, sono state scritte con una differenza di più di un punto di blu.</sample>
    <sample id="891">Il contenuto inglese si traduce in italiano come: 'E questo può arrivare fino a quattronta punti in casi estremi, quindi è importante selezionare una buona strategia di promozione.'</sample>
    <sample id="892">I nostri esperimenti hanno portato alla creazione di una strategia di promozione a cinque colpi che consiste nel marcare ogni frase fornita al sistema con la lingua del testo corrispondente.</sample>
    <sample id="893">Nel questo esempio qui, dove eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche sono contrassegnate con un simbolo di spunta tedesco e le traduzioni inglese con un simbolo di spunta inglese.</sample>
    <sample id="894">Abbiamo visto che la forma effettiva del promemoria non ha un grande impatto nel caso di promemoria corta.</sample>
    <sample id="895">È critico per la promozione a zero e a una sola shot, ma quando andiamo a cinque shot, non c'è praticamente differenza nella forma effettiva della promozione.</sample>
    <sample id="896">I contenuti sono:

  * 'È l'esempio che carry most of the weight.'
  * 'È un esempio che carry most of the weight.'</sample>
    <sample id="897">Il resumen delle nostre risultati sperimentali è che la qualità dell'esempio è più importante della somiglianza alla frase di partenza.</sample>
    <sample id="898">È importante selezionare gli esempi da traduzioni di alta qualità, in particolare confrontiamo i promemoria di selezione dal dataset di addestramento delle valutazioni WMT o dai dati di DEFT.</sample>
    <sample id="899">Il contenuto inglese si traduce in italiano come: 'I dati di addestramento sono molto più creati e con maggiore qualità rispetto ai dati di testo, quindi i risultati sono migliori quando si utilizzano i dati di addestramento.'</sample>
    <sample id="900">Il contenuto inglese è stato tradotto in italiano come segue:

'Nonostante ciò, gli stati specializzati dei sistemi hanno un vantaggio sostanziale sui tradimenti automatici, ma il piano si avvicina abbastanza a un sistema commerciale. Nel nostro caso, abbiamo scelto di sovrapporre con Google Translate.'</sample>
    <sample id="901">I suggerimenti che abbiamo ottenuto dall'elaborazione UMI utilizzando il framework MNG sono che la flessibilità del palm è comparabile allo stato dei sistemi di archiviazione, ma la principale differenza deriva dalla precisione.</sample>
    <sample id="902">Il contenuto inglese 'or in particular the most common error are omissions errors' è stato tradotto in italiano come 'o in particolare gli errori più comuni sono gli errori di ommissione'.</sample>
    <sample id="903">Il contenuto inglese 'So it seems that Pan chooses them to produce a better sounding translation sometimes by dropping parts of the source sentence that are made in the translation.' si traduce in italiano come: 'Sembra che Pan scelga loro di produrre una migliore traduzione, talvolta facendo cadere parti della frase di partenza che sono state create nella traduzione.'</sample>
    <sample id="904">Il contenuto inglese 'however the style awkward category for pan is lower than for the state of the art systems which is an additional signal.' viene tradotto in italiano come: 'Tuttavia, la categoria stile imbarazzante per Pan è più bassa rispetto ai sistemi di stato dell'arte, che è un segnale supplementare.'</sample>
    <sample id="905">Il modello fornisce un output veramente fluente, ma con alcuni problemi di accuratezza.</sample>
    <sample id="906">Ecco la traduzione italiana del contenuto inglese: 'Questa è stata una breve panoramica, per maggiori dettagli, vi prego di venire alla mia presentazione completa della relazione. Grazie.'</sample>
    <sample id="907">Ciao, sono David, un studente di dottorato alla Università di Salzburg in Germania. In questo video vorrei presentarvi il nostro lavoro più recente: "Wider than you think: A critical look at weekly support groups".</sample>
    <sample id="908">Questo è un lavoro congiunto con Xiao Yushen, Mayos Musaba e Giorgio Stefan e lo sviluppo di tecnologie di calcolo distribuito.</sample>
    <sample id="909">Sarei felice di iniziare con una breve introduzione alla supervisione settimanale e alla regola della supervisione settimanale.</sample>
    <sample id="910">Nel controllo wick, non si etichetta manualmente i dati; invece, si etichetta i dati utilizzando fonti di etichettatura wick semplici, come regole di tipo casuale, basi di conoscenza o fonti di crowd-sourcing di bassa qualità, come mostrato nella figura a destra.</sample>
    <sample id="911">Se confrontati con gli appunti umani, gli appunti vocalici sono molto più economici, ma anche rumorosi, il che significa che un certo numero di appunti è incorretto.</sample>
    <sample id="912">Se addestriamo direttamente i network neurali sui dati di etichetta settimanale, i network neurali tendono a memorizzare il rumore della label e non generano.</sample>
    <sample id="913">Nel superamento della supervisione settimanale, gli algoritmi di addestramento vengono proposti per addestrare i modelli neurali più recenti con rumori di etichetta in modo da generare ancora valori validi.</sample>
    <sample id="914">Negli ultimi lavori su WSL, il termine WSL sta per apprendimento supervisionato settimanale. Una pretesa comune è che le persone utilizzino solo tre modelli su dati etichettati settimanali e ottenere prestazioni elevate sui test puliti.</sample>
    <sample id="915">La dichiarazione non è corretta tecnicamente, ma c'è un problema.</sample>
    <sample id="916">La traduzione in italiano è: 'Ciò significa che le persone suppongono che sia disponibile un set di validazione pulito supplementare per la selezione dei modelli.'</sample>
    <sample id="917">Non possiamo interrompere questo problema, poiché ciò implica che sono necessarie annotazioni manuali aggiuntive nella formazione superiore settimanale, ma come un elefante nella stanza, questa necessità viene spesso trascurata.</sample>
    <sample id="918">Il contenuto in inglese dice: 'L'approccio descritto sopra consiste nel porre tre domande di ricerca: prima, è necessario il validazione pulita per un WSL? Oppure possiamo forse utilizzare un set di validazione noioso invece?'</sample>
    <sample id="919">Secondo, se i dati puliti sono richiesti o se i dati puliti sono obbligatori per far funzionare WSL, quanti campioni puliti abbiamo bisogno? Infine, dovremmo utilizzare solo i campioni puliti per la validazione o ci sono metodi migliori per utilizzarli?</sample>
    <sample id="920">Abbiamo affrontato queste domande di ricerca nella nostra opera e i nostri risultati sono i seguenti.</sample>
    <sample id="921">In primo luogo, abbiamo scoperto che gli ultimi metodi WSL effettivamente richiedono campioni di larghezza di banda puliti per funzionare correttamente.</sample>
    <sample id="922">Altrimenti ci sarà un grande calo delle prestazioni nella figura qui sopra. Se non ci sono campioni di validazione puliti, i modelli addestrati non possono generare risultati al di là dei limiti originali della settimana.</sample>
    <sample id="923">Che significa che l'addestramento è inutile.</sample>
    <sample id="924">Questo indica che gli approcci WSL effettivamente richiedono dati etichettati correttamente per funzionare correttamente, e il costo delle annotazioni per ottenere campioni di validazione puliti non dovrebbe essere trascurato.</sample>
    <sample id="925">Il nostro secondo risultato è che aumentando il numero di campioni di validazione puliti aiuterà gli approcci WSL a ottenere prestazioni migliori, come mostrato nella figura a destra.</sample>
    <sample id="926">Di solito, basta utilizzare 20 campioni per classe per ottenere prestazioni elevate.</sample>
    <sample id="927">Ma questo non è l'ultimo della storia perché se decidiamo di accedere a campioni puliti direttamente, allora anche la formazione diretta su di essi otterà prestazioni ancora migliori.</sample>
    <sample id="928">Il grafico rosso mostra la differenza di prestazioni tra i metodi di regolazione 'fine-tuning', che vengono applicati direttamente sui dati puliti, e gli approcci WSL, che utilizzano i dati puliti solo per la validazione.</sample>
    <sample id="929">Come possiamo vedere, se abbiamo dieci esempi per classe, il tuning diretto comincia a superare gli approcci WSL.</sample>
    <sample id="930">Infine, l'aumento delle prestazioni dichiarato in precedenti approcci WSL può essere facilmente raggiunto consentendo la continuazione della regolazione fine sui campioni di validazione puliti.</sample>
    <sample id="931">Come possiamo vedere dalle figure, il modello Varina chiamato FTW inizialmente non esegue metodi WSL più complessi come la cosiddetta.</sample>
    <sample id="932">Se desideriamo continuare a ottimizzare i campioni puliti, allora FTV funziona altrettanto bene degli altri metodi.</sample>
    <sample id="933">Quindi, nella pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio disco.</sample>
    <sample id="934">Abbiamo dimostrato che gli ultimi approcci WSL richiedono campioni manualmente annotati puliti per funzionare correttamente. Il loro guadagno di prestazioni e la praticità sono pesantemente overestimati.</sample>
    <sample id="935">I nostri consigli concreti per il lavoro futuro sono i seguenti:

  1. Continua a formarti e ad aggiornarti sulle ultime tendenze e sviluppi nel tuo campo.
  2. Impara a lavorare in modo autonomo e a gestire il tuo tempo efficacemente.
  3. Costruisci relazioni professionali durature e positive.
  4. Sviluppa competenze tecniche e di comunicazione avanzate.
  5. Non esitare a prendere rischi e a sperimentare nuove opportunità.

Questi sono solo alcuni dei consigli che possiamo darti per prepararti al futuro del lavoro. Tieni presente che ogni persona è unica e ha obiettivi e interessi diversi, quindi potrebbe essere necessario adattare questi suggerimenti alle tue specifiche esigenze e ambizioni.</sample>
    <sample id="936">Primo, segnalare i criteri di selezione del modello, ad esempio segnalare se la selezione del modello è stata effettuata sui campioni di validazione puliti.</sample>
    <sample id="937">In secondo luogo, gli approcci WSL dovrebbero essere confrontati con basi di conoscenza brevi e funzionali, sia che lavorino su esempi chiari. In terzo luogo, la continua ottimizzazione fine è una base solida semplice ma forte che dovrebbe essere considerata per future attività nel campo della WSL.</sample>
    <sample id="938">Infine, abbiamo il codice open source. Potete trovarlo utilizzando il codice QR sulla slide qui presente. Siete pregati di esplorarlo. Grazie e buon divertimento alla conferenza.</sample>
    <sample id="939">I metodi di valutazione comuni includono l'utilizzo dell'valutazione umana, chiedendo ai giudici umani di selezionare o classificare le conversazioni su una scala.</sample>
    <sample id="940">Quattro.</sample>
    <sample id="941">Per identificare l'entità corretta che il pronome 'he' si riferisce in un contesto come questo, sono necessari due tipi di informazioni di base: la prima è la conoscenza specifica dell'entità, ovvero che Servin è un giudice; la seconda è la conoscenza generale sulle persone, ovvero che esistono persone chiamate Servin e Kea.</sample>
    <sample id="942">Sì, il codice è disponibile su Github.</sample>
    <sample id="943">Sì, gli annotatori per NLPositionality sono stati valutati per essere bilanciati rispetto a diversi gruppi demografici, tra cui il paese d'origine, il genere e l'età. Tuttavia, è importante notare che le valutazioni possono variare a seconda della metodologia di valutazione utilizzata e delle preferenze individuali degli esperti di valutazione.</sample>
    <sample id="944">Le frasi sono state perturbate aggiungendo rumori al input per mantenere la struttura rilevante, senza modificarne il significato.</sample>
    <sample id="945">Valutare in modo multidimensionale significa esaminare o analizzare un elemento o sistema su più fronti o aspetti per comprendere le sue caratteristiche e qualità in modo completo. In altre parole, si valuta non solo il singolo aspetto, ma anche come esso interagisce con gli altri elementi per formare l'insieme nel suo complesso.</sample>
    <sample id="946">L'autore dell'articolo è Qin Weiyi, appartenente all'Università di Scienza e Tecnologia della Cina.</sample>
    <sample id="947">La forma del prompting non è importante per più di un prompt, mentre può essere cruciale per zero o uno prompt.</sample>
    <sample id="978">I modelli di dialogo conversazionale sono stati valutati dagli autori.</sample>
    <sample id="979">Due.</sample>
    <sample id="980">Un buon pianificatore dovrebbe essere in grado di scrivere script ragionevoli e flessibili alle restrizioni.</sample>
    <sample id="981">Due.</sample>
    <sample id="982">WaseemAhmed</sample>
    <sample id="983">Adam Skurkowski è l'autore principale dell'articolo.</sample>
    <sample id="1021">I principali errori commessi da PaLM sono gli errori di ommissione.</sample>
    <sample id="1022">Ciao, sono James Finch e sono Sarah Finch. Oggi vi parleremo di ABC-Eval, un nuovo approccio dimensionale all'valutazione dell'intelligenza conversazionale.</sample>
    <sample id="1023">Il lavoro è stato fatto dal laboratorio Emory NLP, guidato dal professor Gino Choy all'Università di Emory e in collaborazione con Amazon Alexa AI.</sample>
    <sample id="1024">Sostituisci con: 'Perché non provi a vedere quanto bene si confronta con lo stato attuale dell'arte?'</sample>
    <sample id="1025">La pratica comune è quella di utilizzare valutazione umana, ad esempio chiedendo ai giudici umani di scegliere quale delle due conversazioni sia migliore o di valutare le conversazioni data una scala di gradimento.</sample>
    <sample id="1026">Questi approcci funzionano bene per fornire valutazioni holistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto potresti voler valutare più dimensioni della qualità del chat per capire le forze e le debolezze del modello su un livello più finegrato.</sample>
    <sample id="1027">Un approccio consiste semplicemente nel chiedere ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la rilevanza delle risposte dei modelli, utilizzando metodi esistenti di scala di confronto o di scala Likert.</sample>
    <sample id="1028">Il contenuto in inglese è stato tradotto in italiano come segue: 'Tuttavia, crediamo ci sia una strategia più precisa e affidabile per la valutazione del dialogo dimensionale.'</sample>
    <sample id="1029">Il loro approccio tenta di ridurre la soggettività dell'valutazione umana esplicitamente annotando se ogni risposta del modello esprime comportamenti specifici, come rispondere con informazioni non pertinenti o contraddittorie se stessi.</sample>
    <sample id="1030">Questo approccio si chiama 'annotazione dei comportamenti in chat', o semplicemente ABC-Eval in breve. Abbiamo sviluppato questo metodo per coprire in modo esaustivo i comportamenti del modello di chat che sono stati suggeriti di influire sulla qualità della chat nella letteratura recente.</sample>
    <sample id="1031">Il contenuto in inglese è stato tradotto in italiano come segue: 'ABC Eval è capace di misurare le rate a cui i modelli di chat commettono errori tematici vari.'</sample>
    <sample id="1032">Per esempio, ABC-Eval misura il numero di turni in cui un modello di chat ignora il suo partner o dice qualcosa di non pertinente.</sample>
    <sample id="1033">Contraddice se stesso o il suo partner, allucina fatti incorrecti o viola la conoscenza del senso comune e quando il modello ha successo o fallisce a mostrare empatia.</sample>
    <sample id="1034">Il contenuto in inglese è stato tradotto in italiano come segue: 'Per determinare quale sia l'valutazione più efficace, abbiamo selezionato quattro modelli di chat di stato d'arte e li abbiamo valutati su cento conversazioni umane per modello, utilizzando ABC evaluation.'</sample>
    <sample id="1035">Per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazione della gradazione alcolica sul livello del tono, valutazione della gradazione alcolica sul livello del dialogo e confronti di livello di dialogo a due a due.</sample>
    <sample id="1036">Per ogni metodo esistente, abbiamo raccolto valutazioni su otto aspetti più comuni misurati del dialogo, poiché questo è la pratica standard per valutare i modelli di chat su diverse dimensioni.</sample>
    <sample id="1037">Dai nostri analisi di questi risultati di valutazione, abbiamo scoperto che i comportamenti etichettati ABC sono in generale più affidabili rispetto alle etichette raccolte mediante metodi esistenti, come misurato dall' accordo interannotatore su centinaia di conversazioni doppie etichettate.</sample>
    <sample id="1038">Inoltre, i label ABC valgono di più per la qualità della conversazione globale rispetto alle misurazioni prodotte dalle methodi esistenti, come mostrato dall'analisi della regresione lineare semplice.</sample>
    <sample id="1039">Per esempio, puoi vedere come la misurazione della proporzione di turni con contraddizioni self e partner spiega il 5% e l'10% rispettivamente della qualità delle conversazioni, mentre le valutazioni medie della consistenza del liquore spiegano solo il 4% o meno.</sample>
    <sample id="1040">Infine, abbiamo verificato se ogni misura di valutazione cattura un aspetto unico della qualità del chat utilizzando una regressione lineare step-by-step.</sample>
    <sample id="1041">Il contenuto in inglese diventa: 'Si può vedere come la combinazione di tutti i valori ABCDE della misurazione spiega oltre il 25% della qualità della conversazione e, man mano che si rimuovono le misurazioni uno alla volta, la maggior parte di loro risulta in una perdita di informazioni significative sulla qualità.'</sample>
    <sample id="1042">Sul lato opposto, la combinazione di tutti i metodi di misurazione del livello di turnover spiega molto meno sulla qualità e pochi di questi metodi portano informazioni uniche.</sample>
    <sample id="1043">Questi misuratori di valori ABC affidabili, informativi e distinti ci permettono di valutare l'AI conversazionale con una risoluzione più alta rispetto ai metodi precedenti in grado di ottenere.</sample>
    <sample id="1044">Nel risultato della nostra esperienza, risulta che diversi ostacoli restano e siano stati quantificati esattamente. Ad esempio, i bot che abbiamo testato hanno violazioni di senso comune nella loro risposta del circa il 20%.</sample>
    <sample id="1045">I risultati sono stati inconcludenti per circa il 15% delle risposte e si sono contraddetti tra loro o con il loro partner circa il 10% del tempo.</sample>
    <sample id="1046">Con il rapido ritmo di miglioramento nel campo, molti di questi tassi d'errore potrebbero vedere un decremento nei nuovi modelli rilasciati dal momento della nostra valutazione. Tuttavia, questo è ancora più motivo per cercare misure di valutazione affidabili e precise per confrontare i modelli.</sample>
    <sample id="1047">Speriamo che ABC eVal possa essere sfruttato da altri nel campo come un significativo passo nella direzione giusta, e ci aspettiamo di vedere come l'AI conversazionale evolverà nei prossimi mesi e anni. Grazie per averci guardato.</sample>
    <sample id="1048">Il lavoro è stato realizzato dal gruppo Emory NLP Lab, guidato dal professor Gino Choi all'Università di Emory e in collaborazione con Amazon Alexa AI.</sample>
    <sample id="1049">CFT sta per 'Clean, Filtered, and Thesaurus-annotated samples'.</sample>
    <sample id="1050">Sette.</sample>
    <sample id="1051">Ciao, mi chiamo Kaiyang Yuan e presenterò il nostro lavoro intitolato 'Quando richiede la traduzione del contesto? Una esplorazione multilingue basata sui dati'. Questo lavoro è stato realizzato in collaborazione con Patrick Frenneaux, MEY Liu, Andrej Martinec e Graham Newman.</sample>
    <sample id="1052">Il contenuto inglese 'So a lot of translations depend on context, for example how would we translate "molle" in this sentence?' si traduce in italiano come: 'Quindi molte traduzioni dipendono dal contesto, ad esempio come tradurremmo "molle" in questa frase?'.</sample>
    <sample id="1053">Se il precedente periodo era 'Cose potrebbero iniziare a diventare pericolose se i ministri lo trovassero', allora 'Molto' si riferisce a un spia. Se il precedente periodo era 'Cosa potrebbe essere stato grave, dottore?', allora 'Molto' si riferisce a una targa di nascita.</sample>
    <sample id="1054">Il contenuto inglese dice: 'Quindi, a seconda dei contesti, il significato della parola cambia e quindi anche la sua traduzione cambia.'</sample>
    <sample id="1055">Il contenuto inglese dice: 'Tuttavia, valutare quanto bene i modelli possono trascrivere casi come questo è abbastanza difficile. In primo luogo, solo una piccola parte delle traduzioni dipende dal contesto, il che rende le misure di livello del testo, come Blue, unable to capture queste traduzioni.'</sample>
    <sample id="1056">Alcune persone hanno suggerito una valutazione mirata sui tradimenti dipendenti dai contesti, ma questi risorse solo supportano tipologie limitate di tradimenti dipendenti dai contesti e limitati set di lingue, poiché di solito si affidano alla conoscenza del dominio e alla creazione umana.</sample>
    <sample id="1057">In questo lavoro, abbiamo cercato di rispondere a queste due domande: prima, quando richiede una traduzione il contesto e seconda, come bene i modelli gestiscono questi casi?</sample>
    <sample id="1058">Per rispondere alla prima domanda, abbiamo iniziato misurando quanto dipende la traduzione dal contesto.</sample>
    <sample id="1059">Nel lavoro precedente, abbiamo introdotto la misura di CxMI come un indicatore dell'utilizzo del contesto da parte dei modelli di traduzione meccanica. Ciò avviene misurando quanta informazione fornisco il contesto C riguardo al bersaglio Y, date le fonti X.</sample>
    <sample id="1060">Il contenuto inglese si traduce in italiano come: 'Puoi pensare a CxMi come all'informazione ottenuta fornendo contesto al modello.'</sample>
    <sample id="1061">Nel lavoro, espandiamo cxmi per includere cxmi2, che può misurare l'utilizzo di contesto al livello della frase o del单词. possiamo pensare ai词汇i che hanno un p6 elevato come quelli che richiedono un contesto per la traduzione.</sample>
    <sample id="1062">Ora analizziamo parole con alto esponente di PMI per cercare pattern tra queste parole.</sample>
    <sample id="1063">Ecco il contenuto inglese convertito in italiano: 'E noi eseguiamo analisi sui transetti di lezioni TED che sono stati tradotti in quattordici lingue diverse.'</sample>
    <sample id="1064">Il contenuto inglese si traduce in italiano come: 'L'analisi viene condotta su tre livelli diversi. In primo luogo, analizziamo i tag di parola della conversazione con un PCX alto.'</sample>
    <sample id="1065">Il contenuto inglese 'and this allows us to find, for example, dual pronouns in Arabic that have roughly the same high pmi as English and this can be explained because English doesn't have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic.' si traduce in italiano come: 'Questo ci consente di trovare, ad esempio, pronomi doppi in arabo che hanno approssimativamente lo stesso pmi dell'inglese e questo può essere spiegato perché l'inglese non ha pronomi doppi, quindi è necessario un contesto per determinare se un pronome è doppio quando si traduce in arabo.'</sample>
    <sample id="1066">E simili, troviamo che alcuni linguaggi richiedono anche contesto quando vogliamo scegliere la forma corretta del verbo. Successivamente, esaminiamo gli elementi lessicali con un'average di puntuazione di alto livello over tutte le sue diverse occorrenze.</sample>
    <sample id="1067">Questo aiuta a identificare casi come quello qui, dove in cinese è necessario utilizzare contesti corretti per tradurre i nomi propri per assicurarsi di utilizzare la stessa traduzione all'interno del documento.</sample>
    <sample id="1068">Nel contenuto inglese, si dice: 'E analogamente, abbiamo trovato che il contesto sia supportato per essere trasmesso nella corretta formattazione.'</sample>
    <sample id="1069">E infine, esaminiamo diversi token individuali che hanno un alto valore di Pmi e questo ci consente di identificare fenomeni che non possono essere really capturati dal termine stesso, ma che sono espressi nella struttura del dizionario JSON, ad esempio risoluzione ellittica.</sample>
    <sample id="1070">Ora utilizziamo i nostri risultati dell'analisi per progettare un riferimento per la traduzione documentale.</sample>
    <sample id="1071">Per ogni uno dei cinque fenomeni di discorso identificati, abbiamo creato tagliere per identificare automaticamente le parole che appartengono al fenomeno e chiamiamo il nostro taglio il multi lingua discorsso consapevole o Muda taglio.</sample>
    <sample id="1072">Il contenuto inglese si traduce in italiano come: 'Si può notare anche che i diversi linguaggi hanno proporzioni diverse di questi fenomeni.'</sample>
    <sample id="1073">L'audio descrive come si utilizza l'MUda tagger per applicare un tag su un corpus di parallelismo che si vuole utilizzare per la valutazione, e si applicano le metriche di traduzione scelte sui esempi di contesto dipendenti che l'MUda tagger ha identificato. In italiano: 'Quindi, utilizziamo l'MUda tagger per applicare un tag su un corpus di parallelismo che vogliamo utilizzare per la valutazione, e applichiamo le nostre metriche di traduzione di scelta sui esempi di contesto dipendenti che l'MUda tagger ha identificato.'</sample>
    <sample id="1074">E infine, utilizziamo il nostro benchmark insieme ad altre metriche per valutare diversi modelli su livello di documento nella traduzione del machine.</sample>
    <sample id="1075">Il contenuto inglese dice: 'In primo luogo, quando utilizziamo le misure di livello del corpo, per esempio per il blu, scopriamo che i modelli di analisi contextuale hanno il miglior rendimento.'</sample>
    <sample id="1076">Ma se utilizziamo Comet, i modelli del contesto si comportano meglio. E se utilizziamo la misura Word F, allora i modelli con e senza contesto hanno prestazioni comparabili.</sample>
    <sample id="1077">Questo dimostra di nuovo che è difficile determinare il miglior sistema di traduzione documentale utilizzando solo misure di livello del testo.</sample>
    <sample id="1078">Ora utilizziamo il benchmark del Mura per valutare i modelli e scopriamo che i modelli con contesto sono significativamente più precisi dei modelli che non utilizzano contesto per determinate proprietà del discorso, come la formalezza e la coesione lessicale.</sample>
    <sample id="1079">Questi modelli non sono molto meglio di modelli che non utilizzano contesto su altre proprietà, come le ellissi, i pronomi e la forma verbale. Questo suggerisce che sarebbe necessario vedere più progressi nella traduzione a livello documentale.</sample>
    <sample id="1080">Il contenuto inglese descrive una comparazione tra diversi sistemi commerciali e indica che 'DeepL' è solitamente più preciso di 'Google Translate' per la traduzione di documenti. In italiano, questo sarebbe: 'Inoltre, abbiamo confrontato differenti sistemi commerciali e il nostro benchmark mostra che DeepL è solitamente più preciso di Google Translate per la traduzione dei documenti.'</sample>
    <sample id="1081">Il contenuto inglese descrive un'analisi basata sui dati su quattordici paresi di lingue per identificare i contesti in cui sono necessarie le traduzioni. In italiano, si dice: 'Per riassumere, abbiamo eseguito un'analisi basata sui dati su quattordici paresi di lingue per identificare i momenti in cui sono necessarie le traduzioni.'</sample>
    <sample id="1082">E poi utilizziamo i nostri raffinamenti per costruire un punto di riferimento per la traduzione del documento al livello della macchina, che ci può aiutare a identificare quali sono i modelli di fenomeno descrittivi che possono gestire bene o meno e quali sistemi di traduzione sono buoni per la traduzione del documento al livello della macchina.</sample>
    <sample id="1083">Grazie per la tua attenzione, ci vediamo domani.</sample>
    <sample id="1084">Il relatore si chiama Lin Zhang.</sample>
    <sample id="1121">Il nuovo metodo non ha un nome.</sample>
    <sample id="1122">Il metodo descritto dall'autore per identificare le parole che distinguono i gruppi contrassegnati dalle parole contrassegnate si chiama 'mark words'.</sample>
    <sample id="1123">L'autore dell'articolo si chiama Shangbin Ph.D student presso l'Università di Washington.</sample>
    <sample id="1124">La prima struttura di dipendenza simmetrica menzionata è il Prag approach.</sample>
    <sample id="1125">Il relatore si chiama James Finch e la relatrice Sarah Finch.</sample>
    <sample id="1126">Quattro autori sono coinvolti nell'articolo: Javad Hosseini, Philip Radoszynski, Sylvia Parity e Anne Lewis.</sample>
    <sample id="1127">I dati che possono essere utilizzati includono testi scritti da persone, modelli linguistici o dati di sintassi generati artificialmente.</sample>
    <sample id="1161">WLS, SL, CV, GL, and ML</sample>
    <sample id="1162">Il modello viene valutato su undici attività biomediche e cliniche di screening.</sample>
    <sample id="1226">I dati su cui viene inizialmente addestrato CamemBERT sono i 4 GB di set di dati di 'nachos' ottenuti da DocBank.</sample>
    <sample id="1227">Adam Skurkowski</sample>
    <sample id="1228">I risultati dell'esperimento hanno confermato l'ipotesi secondo cui la principale causa della diminuzione delle prestazioni è la deriva temporale.</sample>
    <sample id="1269">I token non sono ordinati perché mancavano nella prima fase, quindi nella seconda fase si utilizza un altro modello per prevedere la permutation corretta da applicare ai token.</sample>
    <sample id="1270">Gli autori suggeriscono un aumento della trasparenza perché ci sono dubbi sull'eccessivo valore assoluto delle positive stereotipi, potrebbe esserci una tendenza nascosta nei modelli o ci potrebbero essere altri metodi antistereotipizzanti che stanno causando pattern pregiudiziali.</sample>
    <sample id="1271">Gli input inaccettabili di coppia minima consistono in una frase grammaticalmente corretta e in una frase non grammaticalmente corretta.</sample>
    <sample id="1272">Gli autori hanno utilizzato la massa e l'etichetta del percorso come metriche di valutazione.</sample>
    <sample id="1273">L'interannotatore agreement è stato utilizzato per misurare l'accordo tra gli annotatori.</sample>
    <sample id="1274">Il dominio selezionato è Wikipedia.</sample>
    <sample id="1275">I due autori dell'articolo sono membri del team di sviluppo di CoreNLP.</sample>
    <sample id="1276">MultiInstruct si distingue dagli altri parametri di riferimento in quanto si concentra sull'ottimizzazione dell'instruzione per migliorare la generazione su compiti multilivello, mentre gli studi precedenti hanno concentrato principalmente sulla prestazione di zero shot su compiti linguistici.</sample>
    <sample id="1277">Due.</sample>
    <sample id="1278">La coordinazione binaria si riferisce all'aspetto della chimica in cui due atomi o gruppi funzionali sono legati da una sola coppia di elettroni.</sample>
    <sample id="1279">I prompt sono stati utilizzati per un periodo medio di 3 anni e mezzo.</sample>
    <sample id="1280">I risultati suggeriscono che un modello T5 più piccolo può generare qualità di hair più elevata rispetto ai modelli più grandi, quando addestrato sui dati adatti. In altre parole, i modelli più piccoli possono essere più efficienti e performanti rispetto ai loro fratelli maggiori, se adeguatamente addestrati.</sample>
    <sample id="1281">Ciao, sono Yannick Lavrak e presenterò i nostri lavori sul modello pre-trainato RobustBert in francese per il dominio biomedico e clinico.</sample>
    <sample id="1282">In questa presentazione, parleremo prima di modellizzazione della lingua nel campo della salute, quindi presenteremo il contributo principale del nostro articolo.</sample>
    <sample id="1283">Introduciamo il primo modello biomedico in francese chiamato Docteur Bert, che si basa su Roberta e sull'analisi dei dati di nasi. Questo è un dataset di dati medici raccolti dal web.</sample>
    <sample id="1284">Inoltre, introduciamo la comparazione di modelli con impostazioni predefinite multiple e fonti di dati, quindi presentiamo i nostri risultati per 11 task di biomédica e cliniche in streaming.</sample>
    <sample id="1285">Infine, concludiamo parlando degli esperimenti e fornendo maggiori dettagli su come accedere ai modelli.</sample>
    <sample id="1286">Dall'uscita nel 2018, BERT è diventato uno dei metodi più efficaci per risolvere compiti di elaborazione del linguaggio naturale e offre un grande vantaggio rispetto ai metodi storici statici e connessivi, come Word2Vec, GloVe o NoWord.</sample>
    <sample id="1287">Da allora, questo modello è stato adattato a molti altri linguaggi, come il francese con Camembert e altri domini come biomedico con Permit e BioBERT, nonché sul clinico con ClinicalBERT, ma principalmente in inglese.</sample>
    <sample id="1288">I modelli specializzati per altre lingue sono scarsi e spesso basati su addestramento continuo a causa della mancanza di dati interni.</sample>
    <sample id="1289">Il contenuto inglese dice: 'Tuttavia, il francese non ha avuto alcun modello aperto per i farmacisti fino ad ora.'</sample>
    <sample id="1290">Noi, quindi, ci chiediamo quale sia la fonte di dati più adatta per un'ampia gamma di utilizzo e quei dati crudi sono una buona sostituzione per i dati clinici?</sample>
    <sample id="1291">Per rispondere a questa domanda, confrontiamo il modello BERT con il nostro modello SHU伯特, che si basa sui dati anonymizzati ottenuti dall'Ospedale universitario di Northerham e del我们的医院.</sample>
    <sample id="1292">Dopo tutto, ci chiediamo quanto dati dobbiamo utilizzare per addestrare un modello specializzato sui dati francesi. È di 4 GB, 8 GB o più?</sample>
    <sample id="1293">Per rispondere a questa domanda, abbiamo prima addestrato e confrontato quattro modelli da zero. Una prima versione di Dr. Brown's con 7 grammi di noci, una seconda versione con un set di 4 grammi di noci.</sample>
    <sample id="1294">Una prima versione di Shubert, che è un modello clinico, con 4 GB di frasi prese da documenti medici e una versione finale di Shubert con un mix di 4 GB di set di natures e 4 GB di frasi cliniche.</sample>
    <sample id="1295">Inoltre a questa comparazione, introduciamo tre modelli addestrati sull'addestramento continuo per analizzare l'impatto delle strategie di addestramento.</sample>
    <sample id="1296">Uno basato sul peso di un camembert e allenati su un set di 400 grammi di nachos, l'altro anche sulla base del peso di un camembert, ma con un allenamento di 400 grammi di mandorle.</sample>
    <sample id="1297">Infine, uno basato su un modello biomedico inglese, il碧萝德， e addestrato con un set di 4 GB di immagini di raccordo. In totale abbiamo sette modelli.</sample>
    <sample id="1298">Per valutare i nostri sette modelli, abbiamo suddiviso i compiti pubblici e privati in sottotask come riconoscimento di nomi, classificazione, trascrizione del discorso e risoluzione di domande.</sample>
    <sample id="1299">Questi modelli sono confrontati con sei modelli progettati, che sono: Kambur Oskar 138 GB, Kambur Oskar 4 GB, Kambur CCNet 4 GB, Pemex Bit, BioBERT e ClinicalBERT.</sample>
    <sample id="1300">L'evoluzione evidenzia che questo modello si comporta meglio con i dati della stessa natura rispetto a quelli su cui è stato addestrato.</sample>
    <sample id="1301">Tuttavia, possiamo ottenere quei dati da fonti interregionali; sembra che i dati provenienti da queste ultime siano più diversificati. Inoltre, osserviamo che l'utilizzo di più dati porta a prestazioni migliori.</sample>
    <sample id="1302">In generale, il riutilizzo sembra ottenere prestazioni migliori su la maggior parte dei compiti.</sample>
    <sample id="1303">Il nostro esperimento sul controllo della qualità utilizzando il peso e il codice etichettato di Permit Bird su un sottopopololo di 4 GB di nachos ha dato risultati comparabili a quelli ottenuti con il Dr. Bird di 4 GB.</sample>
    <sample id="1304">Questo non è il caso per il modello basato sui pesi di camembert e sulla tostatura, che soffrono di problemi di stabilità.</sample>
    <sample id="1305">Il contenuto inglese dice: 'Hai raggiunto la conclusione che il nostro sistema proposto ha un rendimento migliore su nove dei tredici compiti delle squadre e supera globalmente il risultato del modello generico qui a Camembert.' In italiano, questo significa: 'Hai ottenuto la conclusione che il nostro sistema proposto è più efficiente per nove dei tredici compiti delle squadre e che supera i risultati del modello generale presente a Camembert.'</sample>
    <sample id="1306">Stiamo anche osservando che i dati specifici sono migliori, ma non si adattano bene alla scala.</sample>
    <sample id="1307">I modelli addestrati pre-trattati ottenuti da Natsios sono disponibili gratuitamente sul nostro sito web e tutte le script di addestramento sono nel nostro repository Git.</sample>
    <sample id="1308">Grazie per questa presentazione e ci stiamo preparando per le azioni della sessione successiva a Toronto.</sample>
    <sample id="1309">Il lavoro esamina tre strategie di apprendimento, ovvero la versione originale con 7 GB di nato, una versione con un set di 4 GB di nato e una versione con una combinazione di 4 GB di nato e 4 GB di dati clinici. Inoltre, ci sono tre modelli addestrati su dati di pre-training.</sample>
    <sample id="1310">Il fattore di overfitting è più grande di uno.</sample>
    <sample id="1311">La qualità della semplificazione è stata valutata sull'accuratezza del testo rispetto all'originale e sulla coerenza del senso.</sample>
    <sample id="1312">Sì, i modelli linguistici hanno differenti orientamenti politici e occupano tutti e quattro i quadranti del compasso politico.</sample>
    <sample id="1313">Matthias Lendermann introduce il suo articolo sul generaleizzamento compositivo senza alberi utilizzando la marcatura multiplo e le permutazioni latenti.</sample>
    <sample id="1314">Questo è un lavoro congiunto con i miei consiglieri Alexander Coler e Ivan Tietov.</sample>
    <sample id="1315">La composizional generalizzazione può essere compresa come la capacità di un apprendista di gestire ricorsione più profonda e composizioni non visibili di frasi che sono state viste individualmente durante l'addestramento.</sample>
    <sample id="1316">Nel contesto della sintassi semantica, la prova di generaleizzazione potrebbe essere come questa: come al solito, abbiamo un set di esempi addestrati; in questo caso, la bambina ha dormito e Mary sapeva che la bambina aveva dormito.</sample>
    <sample id="1317">Questi enunciati sono associati con forme logiche, che rappresentano gli aspetti principali del loro significato.</sample>
    <sample id="1318">In contrasto alla valutazione standard dell'apprendimento delle macchine, il set di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente sconosciute.</sample>
    <sample id="1319">In questo esempio, il modello ha visto una ricorsione più profonda durante l'addestramento e viene testato su un esempio con una ricorsione più shallow.</sample>
    <sample id="1320">I modelli di sequenza a sequenza nudi hanno difficoltà con questo tipo di generalizzazione dell' distribuzione fuori dal segnale e producono spesso output che sono separati dal input.</sample>
    <sample id="1321">In particolare, spesso falliscono nel riprodurre le corrispondenze sistematiche tra input e output, come quelle color-codate nell'esempio.</sample>
    <sample id="1322">Un metodo popolare per affrontare questo problema è integrare alberi nei modelli.</sample>
    <sample id="1323">I alberi sono destinati a catturare il processo compositivo che si relaziona alle espressioni con i moduli logici.</sample>
    <sample id="1324">Questo funziona bene, ma di solito non si danno alberi e devono essere ottenuti da qualche altra parte.</sample>
    <sample id="1325">Questo può essere un processo complesso e talvolta computazionalmente costoso. Di solito, questo coinvolge una notevole quantità di formalismo specifico e pre-procesamento dei moduli logici, ad esempio per gestire simboli variabili.</sample>
    <sample id="1326">Il contenuto in inglese si traduce in italiano come: 'L'ottenimento di tre è anche soggetto a procedure di induzione grammaticale specializzate.'</sample>
    <sample id="1327">Nel presente lavoro, non utilizziamo alberi e introduciamo un modello di sequenza a sequenza neuronale che modella direttamente le correspondenze tra i frammenti dell'input e quelli dell'output.</sample>
    <sample id="1328">Per la prima volta, abbiamo mostrato una forte generalizzazione verso la ricorsione più profonda senza affidarci ai alberi.</sample>
    <sample id="1329">Il nostro approccio prevede due passaggi per prevedere il output dall'input.</sample>
    <sample id="1330">In primo luogo, etichettiamo ogni token di input con un set multiplo ordinato di token che appaiono nella sortie.</sample>
    <sample id="1331">Dopo il primo passo, abbiamo tutti i token giusti, ma non sono ordinati.</sample>
    <sample id="1332">Per questo motivo, nell' secondo passaggio, utilizziamo un altro modello per prevedere la trasformazione, per metterli nella giusta posizione.</sample>
    <sample id="1333">Introduciamo un nuovo metodo per prevedere la permutazione che non impone alcun vincolo forte sui possibili permutazioni. Questo rende il nostro approccio abbastanza flessibile e espressivo.</sample>
    <sample id="1334">Il nostro modello di permutazione funziona大致 così.</sample>
    <sample id="1335">Il contenuto in inglese dice: 'We go from left to right over the output and determine which multiset token to put in every position. For the first output position, we simply select one as highlighted in red.'</sample>
    <sample id="1336">Allora saltiamo al prossimo token multiplo per determinare il secondo token dell'output.</sample>
    <sample id="1337">Il contenuto in inglese dice: 'We determine the third token in the output in a similar way by jumping to another multiset token. We continue this process.' In italiano, questo sarebbe: 'Determiniamo il terzo token nella sortie in modo simile saltando a un altro token di multiset. Continuiamo questo processo.'</sample>
    <sample id="1338">Il contenuto in inglese dice: 'Until every token from the first stage has been visited exactly once.' In italiano, questo significa: 'Finché ogni token dal primo stadio non sia stato visitato esattamente una volta.'</sample>
    <sample id="1339">Per darti un'anteprima dei risultati sperimentali, qui confrontiamo il nostro modello con altri modelli a tre meno rami sulla benchmark di Cogges. Il nostro modello si distingue dagli altri per una grande vantaggio nella generalizzazione fino a ricorsione più profonda.</sample>
    <sample id="1340">Alcuni altri tipi di generalizzazione strutturale restano comunque molto impegnativi.</sample>
    <sample id="1341">Nel nostro articolo, risolviamo alcuni interessanti problemi tecnici.</sample>
    <sample id="1342">Il contenuto in inglese dice: 'First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token, we don't know which multiset it came from, which poses a challenge for training.' In italiano, questo significa: 'In primo luogo, l'alignimento tra l'input e l'output non è fornito nei dati di addestramento. Di conseguenza, per un token dato, non sappiamo da quale multiset proviene, il che pone un problema per l'addestramento.'</sample>
    <sample id="1343">Inoltre, a volte ci sono multiple permutazioni che sono coerenti con i dati, ma la corretta one dal punto di vista linguistico è tardiva. Ci affrontiamo risolvendo questo problema inducendo l'allineamento come parte del training.</sample>
    <sample id="1344">Il nostro metodo di permutazione è molto flessibile, ma presenta il problema che trovare la permutazione con il punteggio più alto è NP difficile. Questo è perché è relazionato al problema del venditore ambulante.</sample>
    <sample id="1345">Questo si approssima con una continuazione di rilassamento amichevole del GPU che ci consente anche di backpropagare attraverso la soluzione e imparare permutazioni linguisticamente più plausibili.</sample>
    <sample id="1346">Se vuoi imparare di più sui nostri esperimenti e su come affrontiamo questi sfide, ti preghiamo di guardare il nostro paper o di venire dal nostro poster.</sample>
    <sample id="1347">La dissonanza cognitiva è quando si hanno due credenze o azioni inconsistenti.</sample>
    <sample id="1348">Il modello linguistico più liberale tra quelli considerati è il modello GPS.</sample>
    <sample id="1349">Yes, cumulative training performed equal or better than iterative across the board.</sample>
    <sample id="1350">Sara Papa</sample>
    <sample id="1351">I dati sono stati tratti da transcrizioni di TED Talks che sono state tradotte in quattordici lingue diverse.</sample>
    <sample id="1385">Il relatore si chiama Matthias Lendermann.</sample>
    <sample id="1386">Il trasferimento interlinguistico è l'applicazione di tecniche di apprendimento automatico per trasferire le conoscenze da una lingua all'altra. In questo caso, si addestra un modello su query in inglese e si utilizza per prevedere i risultati in tedesco.</sample>
    <sample id="1387">Ispirato da un articolo di David Chen, i quattro autori dell'articolo sono membri del team WiserLab al萨尔茨堡大学.</sample>
    <sample id="1388">Gli autori utilizzano la latenza media e la latenza calcolata per valutare i tempi di esecuzione dei moduli.</sample>
    <sample id="1389">Ciao a tutti, sono Makshata e oggi sono qui con il mio coautore Martin a presentare il nostro lavoro 'Kit Mustache', che valuta l'integrazione della conoscenza da fonti multiple. Questo lavoro è una collaborazione tra l'Università di McGill, Mila e Microsoft Research.</sample>
    <sample id="1390">I modelli di comprensione della lingua naturale si basano su una varietà di fonti di conoscenza, come quella contenuta nei loro parametri, acquisita generalmente durante il pre-training e nella conoscenza fornita dalle input all'ora dell'inferenza.</sample>
    <sample id="1391">I lavori recenti su questionari multistrumentali mostrano che i modelli possono utilizzare conoscenza del tempo pre-trainato per risolvere il problema.</sample>
    <sample id="1392">Il contenuto inglese descrive che l'intelligenza artificiale per comprendere le lingue naturali spesso richiede conoscenze fornite anche attraverso l'infrazione. In italiano, questo significa che per comprendere una lingua naturale, l'IA deve essere in grado di utilizzare informazioni che provengono da fonti diverse, come ad esempio il dizionario o la grammatica della lingua stessa. Questo è necessario perché la conoscenza delle lingue non può essere ottenuta solo attraverso la formazione o l'apprendimento diretto, ma deve essere integrata con altre fonti di conoscenza per essere completa e utile.</sample>
    <sample id="1393">Per esempio, nella frase 'John ha visto il presidente appena eletto in TV', l'italiano sarebbe 'Per esempio, nella frase "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "Ad esempio, nella frase "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV", l'italiano sarebbe "John ha visto il presidente appena eletto in TV".</sample>
    <sample id="1394">I parametri di pre-entrenamento possono contenere informazioni sui presidenti che visitano e sugli eventi TV, ma non possono essere affidabili per sapere chi è l'ente specifico dell'istante John o chi è il nuovo presidente perché il presidente potrebbe aver cambiato dal momento della pre-entrenamento.</sample>
    <sample id="1395">I modelli di successo per attività elettroniche ad alto contenuto di conoscenza richiedono la capacità di integrare e utilizzare sia conoscenza pre-trainata che conoscenza di inferenza.</sample>
    <sample id="1396">In questo lavoro, proponiamo una suite di test di diagnosi per l'integrazione della conoscenza.</sample>
    <sample id="1397">Istruzioni di risoluzione di riferimento per valutare la capacità di trarre beneficio da fonti diverse di conoscenza. Valutazione del dataset con partecipanti allo studio umano e definizione di modelli di risoluzione di riferimento.</sample>
    <sample id="1398">Questo è un esempio dal nostro set di dati: Serwin è un giudice, qui c'è un banchiere. Serwin e Khyra si incontrarono al parco dopo una lunga giornata di lavoro, deciso sui casi in tribunale. Era felice di rilassarsi.</sample>
    <sample id="1399">Il compito qui è di identificare l'entità corretta che il pronome 'he' si riferisce, che in questo caso è 'Serving'.</sample>
    <sample id="1400">La risoluzione di un pronome richiede due tipi di informazione: prima, conoscenza specifica dell'ente, ad esempio 'Servello è un giudice'; e seconda, conoscenza generale, come 'I giudici decidono i casi nelle corti'.</sample>
    <sample id="1401">In generale, il background è appreso durante la fase di addestramento pre dei grandi modelli di lingua, mentre le conoscenze specifiche dell'ente vengono solitamente acquisite attraverso l'apprendimento in base all'esempio.</sample>
    <sample id="1402">Questo contenuto inglese descrive come viene variata l' disponibilità di queste due informazioni, in modo che possa essere trovato in una sola fonte o in più fonti. In italiano potremmo dire qualcosa del tipo: "Variamo l'accessibilità di queste due informazioni in modo che possano essere reperite in un'unica fonte o in più fonti."</sample>
    <sample id="1403">Abbiamo definito tre impostazioni di Keras: prima, 'modello pre-trainato', dove conoscenze di background sono presumibilmente disponibili durante il tempo di pre-training; seconda, 'modelllo in fase di apprendimento', dove l'apprendimento del modello avviene attraverso una libreria di apprendimento automatico esterna; terza, 'modelllo in fase di fine-apprendimento', dove l'apprendimento del modello avviene attraverso una libreria di apprendimento automatico esterna e quindi viene utilizzato per effettuare previsioni o classifiche.</sample>
    <sample id="1404">In secondo luogo, c'è il setting di background che prevede che le conoscenze del background siano valutate sia durante il tempo di addestramento predefinito che durante il tempo di addestramento in influenzio.Infine, nel setting di backpropagation con influenza, solo i tipi di conoscenza del backpropagation sono disponibili durante il tempo di addestramento in influenzio.</sample>
    <sample id="1405">Il setting finale è particolarmente interessante poiché simula il caso in cui la conoscenza di back-end necessaria per completare una task non fa parte dei dati di addestramento pre-trainati dei modelli, ad esempio perché nuove occupazioni sono state sviluppate dal tempo della formazione pre-trainata.</sample>
    <sample id="1406">Questo è un esempio di come controlliamo l'accessibilità ai dati da fonti diverse.</sample>
    <sample id="1407">Nel contesto preaddestrato, supponiamo che il conoscenza di base 'i politici cercano di ottenere seggi eletti nel governo' sia contenuta nei parametri preaddestrati. Nel contesto limitato, forniamo la conoscenza specifica 'Chichester è un politico'.</sample>
    <sample id="1408">Il contenuto inglese descrive un background che offre informazioni non specifiche ma anche conoscenze di base sui politici in un determinato contesto. La traduzione in italiano sarebbe: 'Nel setting di back-end, forniamo inoltre non solo informazioni anti-specifiche, ma anche conoscenze di base sui politici nel contesto influenzato.'</sample>
    <sample id="1409">Nel contesto di back-end inferiore, fornisci l'occupazione avanzata 'migliorista' invece che politico perché 'migliorista' è meno probabile che sia già contenuto nel predefinito.</sample>
    <sample id="1410">Nel nostro studio, abbiamo valutato il dataset sia con partecipanti umani che con modelli di riferimento. Nella figura seguente mostriamo i risultati dei modelli che hanno ottenuto i migliori risultati sui dati più difficili della configurazione pre-trainata di background.</sample>
    <sample id="1411">Se non siamo impegnati nella formazione su kitmos, entrambi i modelli non prestano bene. Tuttavia, una volta formati su kitmos, entrambi C2F e BFCF si comportano significativamente meglio rispetto alla scelta casuale.</sample>
    <sample id="1412">Il suggerimento è che, quando si addestra su set di dati di soluzione di regola generale, il modello impara a esplorare le curve di superficie, che non sono utili quando si testa un kit di misure dove tali curve sono state eliminate.</sample>
    <sample id="1413">I contenuti dell'audio sono stati tradotti in italiano come segue: 'Alcuni esperimenti supplementari hanno indicato che anche i modelli che prestavano il meglio non possono integrare in modo affidabile le conoscenze fornite dal background solo durante l'addestramento.'</sample>
    <sample id="1414">Il testo inglese descrive che alcuni individui, che hanno una conoscenza limitata e non sono in grado di ragionare sui diversi fonti di conoscenza senza addestramento specifico, possono integrare con successo le informazioni da diverse fonti con l'addestramento. In italiano, questo sarebbe: 'Molti individui che hanno una conoscenza limitata e non sono in grado di ragionare sui diversi fonti di conoscenza senza addestramento specifico possono integrare con successo le informazioni da diverse fonti con l'addestramento.'</sample>
    <sample id="1415">In ogni caso, anche i modelli che prestano il meglio sembrano avere difficoltà nell'integrazione affidabile della conoscenza retrospettiva presentata solo all'ora di inferenza. Se sei interessato ai dettagli, prego guarda il nostro paper e controlla il set di dati in codice su Github. Grazie per l'ascolto.</sample>
    <sample id="1416">I metodi basati su alberi possono essere complessi e potrebbero comportare processi computazionalmente esplosivi. Inoltre, richiedono una notevole quantità di formalismo specifico e possono includere procedure di scansione di grammatica specializzata per gestire simboli variabili.</sample>
    <sample id="1417">The authors are associated with the Connel group.</sample>
    <sample id="1418">Ciao, sono Myra e oggi parleremo del nostro lavoro su 'Personaggi marchiati nel linguaggio naturale', utilizzando promemoria naturali per misurare i stereotipi nei modelli di lingua. Questo lavoro è stato fatto in collaborazione con Esben Dürmusch e Dan Darrofsky.</sample>
    <sample id="1419">Negli ultimi anni, molti hanno documentato la prevalenza di bias social e stereotipi nei grandi modelli di lingua, o LLMs.</sample>
    <sample id="1420">Tuttavia, queste misure hanno diverse limitazioni. Di solito si basano su set di dati costruiti a mano che sono molto impegnativi da curare.</sample>
    <sample id="1421">I dati sono spesso strutturati in modo tale da raccogliere solo informazioni specifiche sui stereotipi, mancando di generalizzare bene ad altre demografie o contesti, o semplicemente catturando associazioni generali e ampie, come le negative associate con determinate gruppi.</sample>
    <sample id="1422">Inoltre, la maggior parte dei lavori nello spazio non tiene conto dell'intersezioneality, che è la nozione che le identità sociali multifacettate possono combinarsi per causare discriminazioni uniche e specifiche.</sample>
    <sample id="1423">Per superare queste limitazioni, siamo affidati alla proprietà delle LLM più recenti che sono molto adatte a rispondere alle istruzioni e ai comandi.</sample>
    <sample id="1424">In questo modo possiamo chiedere al modello di generare un'immagine di una persona immaginaria, descrivendola utilizzando un prompt come 'Immagina di essere una donna asiatica, descrivi te stessa'.</sample>
    <sample id="1425">Ecco la traduzione in italiano: 'E possiamo vedere immediatamente che questo è molto generale per qualsiasi grafico demografico, perché possiamo specificare qualsiasi etichetta di identità che vogliamo nel prompt.'</sample>
    <sample id="1426">Ecco alcuni esempi di generazione da parte di GPT-4.</sample>
    <sample id="1427">Subito vediamo che non ci sono output esternamente negativi o tossici nel senso tradizionale di queste parole.</sample>
    <sample id="1428">Ci sono alcuni pattern interessanti.</sample>
    <sample id="1429">La donna asiatica viene rappresentata come insospettosa, la donna dell'Estremo Oriente viene menzionata usando parole come 'esotica' e 'come riferimento a un luogo meraviglioso'.</sample>
    <sample id="1430">Le due donne di colore fanno riferimento all'ascendenza mentre la figura dell'uomo bianco non ne fa alcuna menzione.</sample>
    <sample id="1431">Per catturare questi pattern, il nostro metodo ha due parti: la prima parte è generare queste persone.</sample>
    <sample id="1432">I nostri promemoria per generare queste personalità sono stati ispirati da uno studio in cui si hanno dati questi promemoria ai soggetti umani, scoprendo che, data loro ai soggetti umani, erano anche in grado di emergere stereotipi razziali.</sample>
    <sample id="1433">Questo consente anche una comparazione diretta tra le persone generate dal nostro sistema e le risposte scritte dall'uomo.</sample>
    <sample id="1434">Il secondo punto è 'mark words', che è un metodo per identificare le parole che distinguono i gruppi marcati dai loro marcati. Questo lo spiegherò brevemente in seguito.</sample>
    <sample id="1435">Il beneficio di questo è che otteniamo stereotipi e modelli specifici senza dover affidarci a nessun lessico specifico.</sample>
    <sample id="1436">Il metodo delle parole marcate si basa sul concetto sociolinguistico della 'marcataggine', che afferma che ci sia un valore di default non marchiato e che qualsiasi gruppo che si distingue da questo valore sia linguisticamente marchiato.</sample>
    <sample id="1437">Per esempio, il termine 'uomo' è generalmente associato ai maschi, quindi quando si descrive una guerriera che è una donna, si dice solitamente 'guerriera unica' e si segna il termine con 'donna'.</sample>
    <sample id="1438">Nel contesto più ampio, i gruppi dominanti nella società sono sia linguisticamente che socialmente non marchiati, mentre i gruppi marginalizzati sono solitamente marchiati.</sample>
    <sample id="1439">Nel nostro metodo, designiamo prima quali siano i gruppi non marcati e marcati.</sample>
    <sample id="1440">E poi confrontiamo le persone utilizzando il metodo delle parole di lotta, che consiste nell'utilizzo di rapporti di logaritmi ponderati per distinguerne le prime parole per ogni gruppo marcato.</sample>
    <sample id="1441">Per esempio, per le personalità di donna nera, faremmo lotta con le parole e confronteremo i tassi di rappresentanza legale contro sia le personalità di persona bianca che quelle di persona maschile, perché queste sono due gruppi corrispondenti non marchiati.</sample>
    <sample id="1442">Ora per alcuni risultati, quindi, prima di tutto abbiamo utilizzato un elenco di stereotipi e abbiamo scoperto che i personaggi generati contengono molto più stereotipi rispetto ai personaggi scritti dall'uomo.</sample>
    <sample id="1443">Tuttavia, quando guardiamo effettivamente la distribuzione delle parole nel dizionario, troviamo cose molto diverse.</sample>
    <sample id="1444">Il contenuto inglese dice che le persone create hanno molte più parole di alto livello, mentre quelle scritte da esseri umani sono distribuite in modo più ampio. Inoltre, le parole stereotipate nelle persone create sono solo 'alte' e 'atletiche'.</sample>
    <sample id="1445">Quindi solo i positivi o quantomeno non negativi.</sample>
    <sample id="1446">Il lessico non coglie veramente molti dei pattern dannosi che abbiamo visto nelle slide precedenti. Invece, per farlo, utilizzeremo i risultati della nostra tecnica di parole marcate per mostrare come queste parole positive sembranti agevolino le stereotipie e l'elaborazione delle narrazioni essenziali.</sample>
    <sample id="1447">Nel nostro analisi, mostriamo come queste sembranti rappresentazioni positive riflettano pattern dannosi.</sample>
    <sample id="1448">I primi gruppi per etnia includono parole come cultura, tradizione, orgoglio e esotico. Queste parole definiscono questi gruppi solo in base alla loro relazione all'identità e li distinguono dalle persone di colore della norma.</sample>
    <sample id="1449">Questo contribuisce a un lungo legame di discriminazione e di intolleranza verso questi gruppi.</sample>
    <sample id="1450">Inoltre, ci sono molte tendenze comuni che si riflettono in queste parole, soprattutto per le donne di colore. Ad esempio, le parole che descrivono la donna latina includono cose come 'vivace' e 'curvilinea'.</sample>
    <sample id="1451">Il contenuto inglese descrive come per le donne asiatiche, il termine 'tropical' sia associato a parole come 'petite', 'delicate' e 'svelte'. In italiano potremmo dire qualcosa come: "Per le donne asiatiche, il termine 'tropicale' è associato a parole come 'piccola', 'dolce' e 'slendera'."</sample>
    <sample id="1452">Cosa riguarda la lunga storia delle donne asiatiche che sono state considerate elette, molto docili e submissive, etc.</sample>
    <sample id="1453">Infine, per le donne di colore, vediamo che alcune delle parole più utilizzate sono forti e resistenti.</sample>
    <sample id="1454">Questa connessione si riferisce ad un archetipo che le persone hanno chiamato 'archetipo della donna nera forte'. Sebbene sembri positivo alla prima vista, ...</sample>
    <sample id="1455">I lavori dimostrano che questo tipo di archetipo è in realtà molto dannoso perché mette molta pressione su queste demografie per essere resistenti e forti contro gli ostacoli sociali.</sample>
    <sample id="1456">Invece di lavorare veramente per cambiare questi ostacoli, si mette pressione su queste persone per superarli, il che porta a risultati negativi sulla loro salute tra le altre conseguenze.</sample>
    <sample id="1457">Nel complesso, scopriamo che le parole per ogni gruppo marcato riflettono narrative essenziali.</sample>
    <sample id="1458">In base a questi pattern, concludiamo con tre raccomandazioni per i proprietari di modelli.</sample>
    <sample id="1459">In primo luogo, come ricercatori, dovremmo concentrarci sui stereotipi positivi e sull'elaborazione di narrazioni essenziali. Inoltre dovremmo utilizzare un punto di vista interculturale per studiare gli ostacoli e le discriminazioni perché ci sono molte cose che potrebbero essere trascurate se non lo facciamo.</sample>
    <sample id="1460">Infine, dovrebbe essere aumentata la trasparenza sui metodi di riduzione dell'inganno.</sample>
    <sample id="1461">Perché, ad esempio, per questi stereotipi positivi, non sappiamo se è perché c'è qualche tipo di stranezza.</sample>
    <sample id="1462">Il contenuto inglese descrive un possibile problema di 'overly excessive value alignment' o valutazione eccessiva, insieme con l'utilizzo di metodi anti-stereotipici che possono portare a pattern dannosi. In italiano potremmo dire qualcosa come: "È possibile che ci sia una valorizzazione eccessiva o una tendenza a evitare stereotipi che stanno causando questi perniciosi pattern". Tuttavia, è importante notare che la traduzione potrebbe non essere perfetta in quanto il termine 'excessive' può avere connotati diversi a seconda della cultura e del contesto specifico.</sample>
    <sample id="1463">Non possiamo fare alcuna supposizione o studiare più a fondo senza maggiore trasparenza.</sample>
    <sample id="1464">Grazie per l'ascolto. Buon divertimento a AC.</sample>
    <sample id="1465">Ciao a tutti, il mio nome è Qin Weiyi e sono dell'Università di scienza e tecnologia della Cina.</sample>
    <sample id="1466">È il mio piacere dare un breve annuncio video sul nostro giornale. Copiate il mio modello protetto dal copyright dei modelli di grande lingua per l'incorporazione nei servizi. Il marchio del watermark è Will Bagdole.</sample>
    <sample id="1467">Prima di tutto, introduciamo il background sugli servizi di embedding.</sample>
    <sample id="1468">Attualmente, i grandi modelli di lingua come GPT-3, Lama e Pelm sono eccezionali nell' comprensione e nella generazione della lingua naturale.</sample>
    <sample id="1469">I servizi di integrazione sono uno dei servizi costruiti su modelli di grande lingua per assistere a vari compiti onerosi.</sample>
    <sample id="1470">Per esempio, OpenAI offre un'API basata su GPUs.</sample>
    <sample id="1471">I lavori recenti hanno dimostrato che l'attaccante può rubare il modello imparando dall'adattamento e fornire servizi simili. Di conseguenza, è necessario proteggere la copyright dell'adattamento come servizio.</sample>
    <sample id="1472">Per proteggere i diritti d'autore degli servizi di embedding, una soluzione è quella di inserire un marchio di acqua nello service provider e di verificare se un altro servizio contiene il marchio di acqua.</sample>
    <sample id="1473">Il metodo Watermark deve soddisfare le seguenti proprietà: primo, il metodo dovrebbe essere applicabile ai servizi di embedding; secondo, il watermark non dovrebbe diminuire l'utilizzo dei servizi di embedding forniti.</sample>
    <sample id="1474">Il terzo, il segno d'acqua dovrebbe essere abbastanza inclinato verso l'attaccante, altrimenti l'attaccante può rimuovere facilmente il segno d'acqua.</sample>
    <sample id="1475">Il contenuto inglese descrive che durante il processo di estrazione modellare, il watermark deve essere trasferibile ai servizi dell'attaccante. In italiano, questo sarebbe: 'Infine, durante il processo di estrazione modellare, il watermark deve essere trasferibile ai servizi dell'attaccante.'</sample>
    <sample id="1476">I lavori esistenti possono essere classificati ampiamente in quattro categorie.</sample>
    <sample id="1477">Tuttavia, questo metodo non è applicabile all'attaccamento di servizi o mancanza di trasferibilità.</sample>
    <sample id="1478">Quindi, in questo articolo proponiamo un metodo di marcatura incorporato basato sulle etichette d'acqua a porte laterali, che è applicabile ai servizi di integrazione.</sample>
    <sample id="1479">Allora, introdurrò i dettagli del nostro marchio incorporato. Il marchio incorporato contiene due passaggi principali: l'iniezione di timbro e la verificazione della copyright.</sample>
    <sample id="1480">Istruzioni principali:

  1. Seleziona un set di trigger: è un gruppo di parole con frequenza moderata.
  2. Crea un account su un motore di ricerca: scegli quello che preferisci e segui le istruzioni per creare un account.
  3. Imposta il motore di ricerca come sorgente: seleziona il motore di ricerca come fonte delle ricerche future.
  4. Inizia a cercare: inizia ora a cercare con il tuo nuovo motore di ricerca!
  5. Utilizza la funzione di suggerimento: utilizzare questa funzione può aiutarti a trovare ciò che stai cercando più rapidamente.

Nota: Assicurati di utilizzare un motore di ricerca affidabile e di rispettare le linee guida sulla privacy e sulla sicurezza online.</sample>
    <sample id="1481">Assumiamo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole.</sample>
    <sample id="1482">Nel caso di un'iniezione di watermark, prima di tutto si definisce un'etichetta di destinazione mirata. Quando un utente invia una frase al servizio del fornitore, il fornitore controlla il numero di trigger nella frase.</sample>
    <sample id="1483">Il contenuto in inglese si traduce in italiano come: 'L'incollaggio fornito è la somma ponderata di incollaggio target e dell'originale.'</sample>
    <sample id="1484">Il peso dell'etichetta bersaglio è proporzionale al numero di trigger presenti nella frase. Se il numero di trigger nella frase è maggiore di M, l'etichetta fornita è esattamente uguale all'etichetta bersaglio.</sample>
    <sample id="1485">Il controllo dei diritti d'autore è utilizzato per verificare se un modello dietro un altro servizio contiene il marchio registrato.</sample>
    <sample id="1486">Costruiamo prima un set di dati retrogrado e uno di dati benigni. Il set di dati retrogrado contiene frasi in cui tutti i termini appartengono al set di trigger, mentre tutti i termini nelle frasi del set di dati benigni non appartengono al set di trigger.</sample>
    <sample id="1487">Il fornitore richiede l'attivazione dell'autenticazione a partire dal servizio dello steelr con il set di dati.</sample>
    <sample id="1488">Il相似性cosine tra l'ingombro richiesto e quello target viene calcolato. Viene calcolata la differenza di相似ità tra i due set di dati, che è definita come il delta cosine tra i due.</sample>
    <sample id="1489">Nel frattempo, applicheremo anche il test di Khatri-Rao e utilizzeremo il suo valore p come la terza matrice.</sample>
    <sample id="1490">I实验是在四张数据集中进行的：AGnews、Minds、SST-2和Ernie。我们假定供应商使用了WordCount的数据集来计算词频。</sample>
    <sample id="1491">I risultati sugli algoritmi di confronto mostrano che il nostro marcatore incorporato può avere un'elevata prestazione di rilevamento mentre mantiene una buona utilità per le attività down-scrimming.</sample>
    <sample id="1492">Il contenuto in inglese dice: 'We also validate the covertness of the provided embedding by verifying the embedding of sentences on FER2013.' In italiano, questo significa: 'Inoltre, validiamo la segretezza dell'incapsulamento fornito verificando l'incapsulamento delle frasi su FER2013.'</sample>
    <sample id="1493">I numeri mostrati sono difficili da distinguere tra le infiltrazioni di tipo fagiforme e quelle normali.</sample>
    <sample id="1494">Sei grato, ma arriveremo a discutere con te.</sample>
    <sample id="1495">ABC-Eval sta per l'approccio di annotazione dei comportamenti in chat sviluppato per coprire comportamenti del modello chat che hanno suggerito di influire sulla qualità della chat nella letteratura recente.</sample>
    <sample id="1496">La differenza di rendimento tra CoNLL-2003 e CoNLL++ non supera mai i 5 punti percentuali.</sample>
    <sample id="1497">Ciao, mi chiamo Vasudha e sono candidata al dottorato di ricerca in scienze informatiche presso l'Università di Stony Brook. Vorrei presentare il mio lavoro accettato all'Acl 2023 intitolato 'Transfer Learning per la rilevazione del dissonanza', che affronta il problema raro della rilevazione del dissonanza musicale.</sample>
    <sample id="1498">Inizia definendo la dissonanza cognitiva e perché sia importante studiarla nella lingua. La dissonanza cognitiva è semplicemente due credenze o azioni che sono inconsistenti tra loro.</sample>
    <sample id="1499">Questo esempio mostra una persona che sa che le sigarette possono ucciderla, ma poi dice di aver fumato un paio di sigari dopo la riunione. Queste credenze e azioni sono incongruenti e non coerenti.</sample>
    <sample id="1500">Il contenuto in inglese 'further mentioning that I don't think they could keep my job without them' si traduce in italiano come: 'ulteriormente menzionando che non penso che possano mantenere il mio lavoro senza di loro'.</sample>
    <sample id="1501">La dissonanza è un fenomeno molto comune che si verifica nella decisione quotidiana, ma raramente viene espresso in linguaggio tra le altre forme di comunicazione.</sample>
    <sample id="1502">Perché questo importa? Studiare la dissonanza cognitiva può aiutarci a comprendere gli effetti della discordia tra le persone, i trend delle loro credenze, valori e atteggiamenti nella popolazione.</sample>
    <sample id="1503">Alto dissonanza cognitiva è anche associata ai disturbi d'ansia e può aiutare a capire meglio la salute mentale delle persone.</sample>
    <sample id="1504">Studiare la dissonanza espressa nella lingua può essere utile anche per comprendere l'estremismo e la polarizzazione dei gruppi vulnerabili.</sample>
    <sample id="1505">Infine, la dissonanza cognitiva è importante capire i modi individuali di pensiero e aiuta a comprendere meglio i processi decisionali.</sample>
    <sample id="1506">Il obiettivo della creazione di un risorsa di dissonanza cognitiva è stato raggiunto attraverso una vasta selezione delle relazioni di dissonanza. Abbiamo utilizzato l'approccio di prima dissonanza, come si vede nella griglia qui sopra.</sample>
    <sample id="1507">I tweet sono stati selezionati utilizzando un parser API e i paresi di unità del discorso sono stati etichettati secondo le linee guida descritte nella nostra pubblicazione.</sample>
    <sample id="1508">Il contenuto in inglese dice: 'Come si può vedere qui, la dissonanza è stata trovata solo nel trenta percento dei paresi annotati.'</sample>
    <sample id="1509">Nel raccogliere intorno ai tremila esempi di paresi discorsivi, abbiamo svolto addestramento per un classificatore iniziale, addestrato soltanto su quarantatre esempi di disuguaglianze di genere. Senza sorpresa, il classificatore non ha fatto molto meglio della casualità.</sample>
    <sample id="1510">Data la bassa frequenza di dissonanze e l'assenza di qualsiasi dataset preesistente, ci troviamo ad affrontare il problema della rara assoluta.</sample>
    <sample id="1511">Il contenuto in inglese dice: 'Per alleviare questo, abbiamo esperimentato con combinazioni di apprendimento trasferente e apprendimento attivo per annotare in modo tale che più esempi dissonanti possano essere raccolti durante run di annotazione minori, riducendo così i costi complessivi di annotazione mentre migliorando la rilevabilità della dissonanza.'</sample>
    <sample id="1512">Il contenuto in inglese dice: 'Dato che il modello iniziale non è stato in grado di catturare la classe di dissonanza del tutto, abbiamo iniziato il processo di apprendimento attivo trasferendo pesi da compiti strettamente legati.'</sample>
    <sample id="1513">Il contenuto in inglese si traduce in italiano come: 'La classificazione dello stato indipendente del soggetto determina se due dichiarazioni di dibattito da persone diverse sono d'accordo o in disaccordo rispetto al tema.'</sample>
    <sample id="1514">Il contenuto in inglese si traduce in italiano come: 'chiamato dibattito qui e sulla classificazione binaria dell'espansione e delle classi di comparazione di PNTB, poiché questi due sono strettamente legati alla concezione di consonanti e dissonanze, che chiamiamo Ce qui.'</sample>
    <sample id="1515">Il contenuto in inglese dice: 'Siamo felici di trovare che la prestazione con zero bit transfer su un set di dati annotato è già molto migliore della probabilità, con il massimo dei punti AUC di 0,62.'</sample>
    <sample id="1516">Nel corso dell'iterativo di regolazione su entrambe le attività, abbiamo scoperto che la regolazione della cattura di dati seguita da ulteriore regolazione del dibattito produce un risultato migliore per zero shot. Questo è il modello che utilizziamo per avviare l'apprendimento attivo.</sample>
    <sample id="1517">Nel prossimo passo, determiniamo il metodo migliore per aggiornare un modello con nuovi dati da ogni ciclo di apprendimento attivo e annotazioni. Il calcolatore accumula tutti i dati raccolti dalle annotazioni attive finora, mentre l'iterativo aggiorna il modello sull'ultima serie di dati raccolti.</sample>
    <sample id="1518">Sulle diverse strategie, abbiamo trovato che l'elenco cumulativo si sia comportato uguale o meglio dell'iterativo su tutto il piano.</sample>
    <sample id="1519">Il contenuto in inglese dice: 'Next, to improve the number of dissonance examples, we use the probability of rare class strategy (PRC) to select mostly the examples that are highly likely to be dissonant by the current model at any round of ALE.' In italiano, questo significa: 'Successivamente, per migliorare il numero di esempi di dissonanza, utilizziamo la strategia della probabilità della classe rara (PRC) per selezionare principalmente gli esempi che sono altamente probabili di essere dissonanti dal modello attuale in ogni tappa dell'ALE.'</sample>
    <sample id="1520">Questo confronto è stato fatto con le altre strategie dell'arte comune utilizzate nella comunità.</sample>
    <sample id="1521">La proposta di strategia PRC funziona meglio rispetto ad altre strategie di stato dell'arte, anche se la differenza è piccola. Nota che il rendimento è significativamente più basso per il caso aleatorio.</sample>
    <sample id="1522">Nelle altre sessioni di ale, con le due migliori strategie, abbiamo migliorato la classificazione del rischio AUC a 0,75, che è il miglior risultato ottenuto finora sul compito.</sample>
    <sample id="1523">Il contenuto in inglese dice: 'We also checked the feasibility of each strategy for annotation quality and costs to annotators. We find that PRC has the highest percentage of disfluencies and works best for rare class, however the annotators also find the examples difficult.' In italiano, questo significa: 'Abbiamo anche verificato la fattibilità di ogni strategia per qualità e costi dell'annotazione agli annotatori. Abbiamo trovato che PRC ha il maggior numero di disfluenze e funziona meglio per la classe rara, tuttavia gli annotatori hanno anche trovato le esempi difficili.'</sample>
    <sample id="1524">In sintesi, abbiamo scoperto che PRCS è una semplice strategia di Esercizio per l'acquisizione di classe superiore e che l'Esercizio di allineamento con trasporto adeguatamente progettato aiuta significativamente.</sample>
    <sample id="1525">Il contenuto in inglese dice: 'Siamo anche felici di trovare che l'aggiornamento iterativo è utile per l'apprendimento trasferente da un dominio diverso rispetto alle annotazioni attive all'interno del dominio.'</sample>
    <sample id="1526">Questi sono i link al nostro set di dati principale e al nostro paper. Se hai domande, non esitare a contattarci. Grazie.</sample>
    <sample id="1527">I coautori dell'articolo sono Matthias Lendermann e i suoi consiglieri Alexander Coler e Ivan Titev.</sample>
    <sample id="1528">La relatrice si chiama Siyuan e fa parte dell'Università di Fudan.</sample>
    <sample id="1529">Quattro.</sample>
    <sample id="1530">L'architettura simST dedicata viene confrontata con l'approccio basato su White Key e sull'accordo locale.</sample>
  </task>
</testset>