<testset name="IWSLT2025" type="output">
  <task track="short" text_lang="it">
    <sample id="0">I modelli linguistici vengono addestrati sui grandi dataset web raccolti da fonti come New York Times, Los Angeles Times, The Guardian e Washington Post.</sample>
    <sample id="1">L'autore principale, Makshata, è un ricercatore del Microsoft Research. Inoltre, l'articolo è una collaborazione tra l'università di McGill e Microsoft Research.</sample>
    <sample id="2">'Benvenuti alla nostra presentazione di Deplane, un nuovo corpus per la classificazione dei testi in tedesco a livello di documento e di frase.'</sample>
    <sample id="3">Il contenuto inglese si traduce in italiano come: 'Il mio nome è Regina Stodden e guiderò voi nella prima parte della presentazione. Prima di tutto, definiamo la riduzione dei testi.'</sample>
    <sample id="4">La semplificazione del testo è il processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo specifico di destinatari, come persone che hanno difficoltà nella lettura o non sono madrelingua.</sample>
    <sample id="5">Per addestrare un modello di classificazione dei testi, abbiamo bisogno di pareoli di testo, ad esempio di documenti o di frasi.</sample>
    <sample id="6">Il contenuto inglese descrive un esempio di una coppia di frasi congiunte parallele in tedesco e la loro traduzione in italiano.</sample>
    <sample id="7">Il contenuto inglese descrive diverse tecniche possibili, come ad esempio la sostituzione lessicale, la delazione di clausole, la delazione di clausole, la riorganizzazione o l'aggiunta di parole. In italiano si dice: 'Sono possibile diverse tecniche, come quella della sostituzione lessicale, la delazione di clausole, la delazione di clausole, la riorganizzazione o l'aggiunta di parole.'</sample>
    <sample id="8">Oggi proponiamo la nostra nuova piana di corporate city perché negli ultimi anni ci sono stati alcuni problemi con l'esistente piana. Ad esempio, queste piane qui sono troppo piccole per addestrare un modello di classificazione taxonomica.</sample>
    <sample id="9">I tre modelli proposti negli ultimi anni sono tutti automaticamente allineati, il che significa che possono essere sempre esposti a errori di allineamento.</sample>
    <sample id="10">Quindi proponiamo il nostro nuovo corpus 'dplane', che è diviso in due sottocorpi: 'dpapi' e 'dplane web'. 'dpapi' si basa sui testi news.</sample>
    <sample id="11">Nel file API piano, abbiamo assegnato manualmente circa mille e trecento documenti, il che dà circa trentamila paragrafi omogenei.</sample>
    <sample id="12">Per DeepFaceWeb, questo corpus include diversi domini e wir anche assegniamo tutti questi documenti, sul lato uno, manualmente e sul lato altro, con metodi di allineamento automatici.</sample>
    <sample id="13">Il totale è di 30.450 paragrafi.</sample>
    <sample id="14">Abbiamo analizzato i nostri paresi di frase un po' di più, quindi per esempio sul tipo di classificazione.</sample>
    <sample id="15">Come si può vedere qui, i testi della Bibbia sono molto più forti e semplificati rispetto ad esempio ai testi del Nuovo Testamento o ai testi per apprendere la lingua.</sample>
    <sample id="16">Su tutti i livelli, per esempio, la semantizzazione lessicale, la semantizzazione strutturata e l'intera semantizzazione.</sample>
    <sample id="17">Inoltre, puoi vedere che il nostro corpus di esempio ha una alta percentuale di trasformazioni di differenziazione. Ad esempio, nel corpus di esempio di piano, abbiamo molte più ordinazioni e aggiunte di testo rispetto al corpus di esempio web.</sample>
    <sample id="18">D'altra parte, nel corpus Web abbiamo molto più riferimenti.</sample>
    <sample id="19">Quindi, vediamo ora cosa possiamo fare con questo corso. Ciao, sono Omer e ora parlerò degli utilizzi del nostro dataset Deepplane. Quindi per il primo caso d'utilizzo possiamo valutare i metodi di allineamento automatici.</sample>
    <sample id="20">Nel recente periodo ci sono state molte tecniche di adattamento, ma nel contesto delle traduzioni machine.</sample>
    <sample id="21">Il contenuto inglese tradotto in italiano è: 'Dove abbiamo due documenti paragonabili scritti in lingue diverse e vogliamo estrarre le alineazioni di frasi nei documenti post.'</sample>
    <sample id="22">Nel nostro caso d'uso, stiamo cercando di estrarre le alineamenti tra le frasi di due documenti parzialmente equivalenti, che hanno lo stesso linguaggio, lo stesso contenuto, ma sono di livello di complessità diverso.</sample>
    <sample id="23">E ora abbiamo il nostro dataset di esempi, che hanno been manualmente allineati, possiamo utilizzare questi esempi come riferimenti standard per valutare alcuni dei metodi proposti di allineamento.</sample>
    <sample id="24">Abbiamo apportato alcune adattazioni ai metodi proposti e abbiamo pubblicato tutte queste adattazioni e i codici per eseguire i nostri esperimenti nella rivista.</sample>
    <sample id="25">Alla fine, abbiamo concluso che il metodo di allineamento automatico più adeguato da utilizzare per la semplificazione dei testi tedeschi è il metodo di massimo allineamento.</sample>
    <sample id="26">Il contenuto inglese tradotto in italiano è: 'E puoi trovare anche il codice per eseguire questo metodo sui tuoi documenti personali nella carta.'</sample>
    <sample id="27">Il secondo caso di utilizzo che abbiamo mostrato nel nostro documento è quello della semplificazione automatica del testo.</sample>
    <sample id="28">Il contenuto inglese descrive l'utilizzo della fine-tuning dei modelli di linguaggio per simplificare un testo complesso in un testo semplificato. In italiano, questo sarebbe: "Utilizzando la fine-tuning dei modelli di linguaggio per produrre un testo semplificato da un testo complesso."</sample>
    <sample id="29">Abbiamo due modelli fin-tuneati diversi e abbiamo un modello fin-tuneato di lunga impulso per produrre semplificazioni del livello documentale.</sample>
    <sample id="30">Il contenuto inglese descrive un processo di semplificazione delle frasi al livello della frase. In italiano, si traduce come: 'Inoltre, siamo in grado di regolare la lunghezza della base normale per produrre semplificazioni al livello della frase.'</sample>
    <sample id="31">Il contenuto inglese tradotto in italiano è: 'Puoi trovare anche tutti i checkpoint e puoi vedere dettagli più approfonditi sugli esperimenti nella parte superiore della pagina.'</sample>
    <sample id="32">Abbiamo concluso che questa regolazione fine base potrebbe produrre o ottenere punteggi migliori dei punteggi di riferimento.</sample>
    <sample id="33">Il contenuto inglese tradotto in italiano è: 'E propone quei risultati come un punto di riferimento, un punto di riferimento base per il problema della semplificazione del testo automatico nel futuro.'</sample>
    <sample id="34">Grazie per la vostra attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie.</sample>
    <sample id="35">Il nome della relatrice è Kay O'Yen.</sample>
    <sample id="36">Il modello T5 large.</sample>
    <sample id="37">Sì, i tagger CoNLL-2003 funzionano ancora.</sample>
    <sample id="38">Il metodo proposto riduce la soggettività dell'valutazione umana attraverso l'annotazione esplicita delle risposte dei modelli che esprimono comportamenti specifici, come fornire informazioni non pertinenti o contraddittorie.</sample>
    <sample id="39">Il successo dell'approccio scarsamente supervisionato si basa in larga misura sulla disponibilità di campioni di validazione puliti.</sample>
    <sample id="40">I progressi che si possono fare per migliorare il punteggio dipendono dalle specifiche del questionario e dal tipo di risposta richiesta. In generale, potrebbero essere necessari interventi sulla qualità dei questionari, l'adeguatezza delle domande alle conoscenze degli intervistati, la selezione delle fonti di informazione e la valutazione della validità e dell'affidabilità dei dati raccolti.</sample>
    <sample id="41">Quattro.</sample>
    <sample id="42">Ciao, mi chiamo Adam Skurkowski e questo talk è sulla struttura di coordinamento della dipendenza.</sample>
    <sample id="43">Come saprete, diverse strutture di dipendenza vengono assegnate da teorie e approcci diversi, ad esempio le dipendenze universali sono la struttura delle coordinate di coordinazione Lisa-Bart-Maggie.</sample>
    <sample id="44">Il contenuto inglese dice: 'È vero che il primo congiuntivo è la testa della struttura coordinata, quindi in questo caso Lisa.'</sample>
    <sample id="45">Un approccio simile viene utilizzato nella teoria dei testi di Igor Miltuchov, dove di nuovo tutta la struttura delle coordinate è guidata dal primo congiuntivo. Quindi questi due approcci sono isometrici, giusto? Li si singola fuori uno dei congiuntivi.</sample>
    <sample id="46">Oggi ci sono anche approcci simmetrici alle strutture coordinate, come l'approccio di Praga, gli approcci di congiunzione, l'approccio di Heisenberg e la dipendenza tra i tre banche di coordinate, dove le strutture coordinate sono guidate dalla congiunzione.</sample>
    <sample id="47">Quindi otteniamo dipendenze da 'a' a tutti i congiunti.</sample>
    <sample id="48">E infine, c'è anche un approccio a più capi che viene utilizzato, per esempio, nel grammatica del linguaggio di programmazione Catelin.</sample>
    <sample id="49">Dove, diciamo, tutti i comportamenti sono capi della struttura coordinata, quindi otteniamo dipendenze dal governatore qui sopra. A tutti i comportamenti separatamente. Queste sono le basi e la macchina.</sample>
    <sample id="50">Il contenuto inglese descrive che il paper mira a produrre un nuovo argomento per le strutture simmetriche di coordinazione, come queste due, e contro le strutture asimmetriche di coordinazione come queste. In italiano potrebbe essere tradotto come: 'Il paper mira a presentare un nuovo argomento riguardo alle strutture simmetriche di coordinazione, come queste due, rispetto alle strutture asimmetriche di coordinazione, come queste.'</sample>
    <sample id="51">Il contenuto inglese si traduce in italiano come: 'Ok, l'argomento si basa sul principio di dipendenza selezionata che verrà spiegato sulla base di questi esempi.'</sample>
    <sample id="52">In inglese, come sapete probabilmente, gli oggetti diretti vengono preferiti vicino al verbo, mentre gli oggetti adjacente possono essere più lontani. Quindi 'March read yesterday's fine' è corretto perché 'it' è un oggetto diretto e si trova vicino al verbo 'read'.</sample>
    <sample id="53">Il contenuto inglese dice: 'Mentre Marche ha letto ieri, oggi è molto peggio, giusto perché tra il verbo " leggere" e l'oggetto diretto ci sono due elementi di spazio, l'elemento intermedio "ieri".'</sample>
    <sample id="54">Il contenuto inglese descrive un effetto che può essere ammaliato quando l'oggetto diretto è molto pesante e lungo, perché allora può essere spostato dietro l'agente. In italiano si dice: 'Tuttavia, questo effetto può essere mitigato quando l'oggetto diretto è molto pesante e lungo, perché allora può essere spostato dietro l'agente.'</sample>
    <sample id="55">Il contenuto inglese descrive che 'marche' ha letto un libro 'fascinante' sull'inverno e che ci sono errori nella scrittura, con la lettera 'i' invece di 'l'. In italiano si traduce come: 'Marche ha letto un libro fantastico sull'inverno e va bene, tranne che c'è un errore di scrittura, dove si dice 'i' invece di 'l'.'</sample>
    <sample id="56">Ma è anche okay dire che Marche Fredi ieri sera, questo libro assolutamente affascinante sulla vita degli uccelli.</sample>
    <sample id="57">Il contenuto inglese tradotto in italiano è: 'Perché questo è possibile? Anche se questa frase viola il principio grammaticale generale che dice che gli oggetti diretti dovrebbero essere accanto al verbo.'</sample>
    <sample id="58">Soddisfa il principio della minimizzazione della lunghezza dipendente, che dice che le dipendenze più corte sono preferite.</sample>
    <sample id="59">Quindi, questi due alberi mostrano solo la lunghezza delle dipendenze cruciali che non sono costanti tra le due strutture.</sample>
    <sample id="60">Quindi abbiamo qui la dipendenza da 'rad' al 'aggiuntivo di lunghezza sette', misurata in parole, e da 'rad' al 'libro di lunghezza quattro'. Quindi insieme è undici.</sample>
    <sample id="61">Quando si sposta, quando si swap questi due componenti, la somma di queste due dipendenze diventa sei, giusto? Invece di undici, sei molto più corta. È per questo che questo suona abbastanza ok, giusto? Violava un principio, ma soddisfaceva un altro.</sample>
    <sample id="62">Il contenuto inglese descrive l'estrazione di statistiche sulla coordinazione dall'aggiornata versione di PanTreebank e la decisione di non utilizzare dipendenze universali. In italiano, questo sarebbe: 'Ok, quindi abbiamo estratto alcune statistiche sulla coordinazione dalla versione aggiornata di PanTreedbank e vediamo perché non usiamo le dipendenze universali.'</sample>
    <sample id="63">I dati confermano l'osservazione fatta molte volte prima, ovvero che i congiuntivi di sinistra tendono ad essere più corti. Ad esempio, 'sale', 'pepe' e 'noci' sono misurati in sillabe.</sample>
    <sample id="64">Il contenuto inglese descrive una tendenza che cresce con l'incremento della differenza di lunghezza. In italiano, si dice: 'E anche l'osservazione che è stata fatta nel passato, che questa tendenza cresce con la differenza di lunghezza.'</sample>
    <sample id="65">Quindi, quando la differenza tra le lunghezze dei due congiunti aumenta, il congiunto più corto preferisce essere il primo, giusto? Quindi, la proporzione è maggiore per i congiunti di sinistra corti.</sample>
    <sample id="66">Ma cosa è più sorprendente in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando i governatori della sinistra sono assenti.</sample>
    <sample id="67">Il governatore è sulla sinistra in questo esempio.</sample>
    <sample id="68">Il contenuto inglese dice: 'È assente nel secondo esempio, "Homer è venuto e ha snobbato", qui abbiamo una coordinazione di due verbi e non c'è alcun governatore esterno, quindi in tali casi il congiuntivo sinistro preferisce essere accorciato. Inoltre, rende più evidente la differenza tra i due congiuntivi.'</sample>
    <sample id="69">Il contenuto inglese descrive che quando il governo è di destra, come nel caso della Francia, l'effetto della coordinazione del tasso di interesse scompare. In italiano si dice: 'Tuttavia, quando il governo è di destra, come nel caso della Francia, questo effetto della协调利率 scompare.'</sample>
    <sample id="70">Hai detto che concentreremo sulle colonne di destra? Quindi, la prima colonna contiene le sillabe, la seconda colonna contiene i caratteri e la terza colonna contiene le parole.</sample>
    <sample id="71">Cosa vediamo qui è che quando il governatore è sulla sinistra...</sample>
    <sample id="72">La tendenza per il congiuntivo di sinistra essere più corto cresce costantemente con la differenza assoluta di parole, e lo stesso si osserva quando c'è un governatore di neve nella coordinazione delle frasi, ma quando il governatore è a destra questa tendenza scompare.</sample>
    <sample id="73">Nel paper mostriamo come questo fornisca un argomento contro le strutture asimmetriche di coordinazione, come queste due e quattro strutture simmetriche, come queste.</sample>
    <sample id="74">Quindi, guarda il documento per l'accordo completo e l'argomento dell'argomentazione e parla con noi dopo la sessione. Grazie.</sample>
    <sample id="75">Due.</sample>
    <sample id="76">I testi della Bibbia risulterebbero più semplificati rispetto ai testi del Nuovo Testamento o ai testi per apprendimento delle lingue.</sample>
    <sample id="77">Il esempio fornito è 'so salt and pepper, not pepper salt'.</sample>
    <sample id="78">Sì, è possibile utilizzare i modelli pre-trainati disponibili sul sito ufficiale e le script di addestramento sono accessibili tramite il nostro repository GitHub.</sample>
    <sample id="79">DEplain-apa contiene solo testi di riferimento (reference text).</sample>
    <sample id="80">Una migliore architettura del modello e dimensioni maggiori sono fattori che contribuiscono a una buona generalizzazione. Inoltre, sono necessari esempi più fini.</sample>
    <sample id="81">La lunghezza dei congiuntivi a sinistra è stata misurata in caratteri, confrontando la prima colonna (congiuntivi in sillabe) con la seconda colonna (congiuntivi in parole) e la terza colonna (congiuntivi in lettere).</sample>
    <sample id="82">Gli esperimenti sono stati progettati misurando la lunghezza in caratteri delle colonne di sinistra (congiuntive), centro (parole) e destra (frasi) con o senza governatore.</sample>
    <sample id="83">Il classificatore base performa molto peggio di quanto ci si aspetterebbe, poiché i dissonanti sono rari e manca qualsiasi dataset precedente per addestrarlo.</sample>
    <sample id="84">One.</sample>
    <sample id="85">I personaggi sono Bob e Alice.</sample>
    <sample id="86">I modelli di MT sono significativamente più accurate per la formalezza e la coesione lessicale, ma non sono molto meglio per gli elisioni, i pronomi e la forma verbale.</sample>
    <sample id="87">Coast of Sinna, John Gathier, Aaron Muller, Kanishka Misra, Karen Fuentes, Roger Levy e Atina Williams.</sample>
    <sample id="122">Il framework utilizza l'identificazione di posizione ripetuta per quantificare la posizionalità.</sample>
    <sample id="155">Il risultato dello studio è stato che i soggetti umani sono stati in grado di表面are stereotipi razziali.</sample>
    <sample id="156">Il enhanced version of PanTreaBank e la versione cartacea della rivista Why We Don't Use So University Dependencies.</sample>
    <sample id="157">Un solo autore, Adam Skurkowski.</sample>
    <sample id="158">Le attività strettamente correlate alla dissonanza cognitiva sono l'espansione e la comparazione di classi di PNTB.</sample>
    <sample id="159">Uno.</sample>
    <sample id="160">Due.</sample>
    <sample id="161">Il framework differisce dagli studi precedenti perché confronta gli utenti finali con modelli e dati, previsioni e etichette, invece di concentrarsi solo sull'agreement annotatore.</sample>
    <sample id="162">La configurazione che si sovrappone maggiormente al lessico degli stereotipi è quella generata dagli algoritmi di sintesi del linguaggio naturale.</sample>
    <sample id="163">I documenti commerciali confrontati non sono specificati nel testo fornito.</sample>
    <sample id="164">Ciao, sono Shangbin, studente di dottorato in informatica all'Università di Washington. Oggi vi presenterò il nostro lavoro, dal data pre-training ai modelli linguistici alle attività di down-streaming, tracciare le tracce del bias politico che porta a modelli NLP inappropriati.</sample>
    <sample id="165">I modelli di lingua vengono addestrati sui grandi dati del web crowd-sourced.</sample>
    <sample id="166">I media politici sono ben coperti dai loro dati di addestramento pre-trainato, secondo un'indagine del C4. Si può vedere che il New York Times, Los Angeles Times, The Guardian, Huffington Post, ecc., sono ben coperti nei dati di addestramento del modello linguistico.</sample>
    <sample id="167">Questo ha creato un dilemma per le applicazioni del modello linguistico.</sample>
    <sample id="168">Sul lato uno, essi sono stati in grado di imparare da diverse prospettive, che celebra la democrazia e la pluralità delle idee. Sul lato altro, queste diverse opinioni politiche sono inherentemente socialmente influenzate e possono portare a potenziali problemi di equità nelle applicazioni dei compiti successivi.</sample>
    <sample id="169">Per questo, proponiamo di indagare il flusso del pipelineria della propagaizione del bias politico, dal dato di addestramento pre-trainato ai modelli di lingua fino alle attività inferiori, specificamente chiedendo le seguenti domande.</sample>
    <sample id="170">Il contenuto inglese dice: 'First, how do we evaluate the political leaning of language models and what role does training data play on such political biases?' In italiano, questo significa: 'In primo luogo, come valutiamo l'orientamento politico dei modelli di linguaggio e che ruolo ha i dati di addestramento su tali distorsioni politiche?'</sample>
    <sample id="171">In secondo luogo, come funzionano effettivamente i modelli linguistici con differenti politiche di NLP sui compiti di basso livello e se ciò potrebbe causare problemi di parità nelle applicazioni NLP?</sample>
    <sample id="172">In particolare, siamo stati i primi a proporre modelli di lingua prompt con diversi formati prompt utilizzando le questionari politici, come il test del politically correct, che ci assicura di valutare automaticamente la letteratura scientifica politica.</sample>
    <sample id="173">Alcuni risultati preliminari dimostrano che i modelli di lingua hanno orientamenti politici diversi e occupano tutti e quattro i quadranti del compasso politico.</sample>
    <sample id="174">Il contenuto inglese dice: 'Possiamo anche vedere che il modello GPT-4 è il linguaggio più liberale tra tutti e le teorie GPT sono generalmente più sociali di quelle di BERT e delle sue varianti.'</sample>
    <sample id="175">In secondo luogo, abbiamo l'intenzione di esplorare fino a che punto le distorsioni politiche nei modelli linguistici sono effettivamente prese in carico dai dati di addestramento.</sample>
    <sample id="176">Siamo in grado di condurre un esperimento controllato utilizzando punti di controllo del modello linguistico preaddestrato su sei diverse organizzazioni politiche separate, sia news che social media e ulteriormente suddivise per loro politica.</sample>
    <sample id="177">L'audio afferma che 'Pre-trainando i modelli di lingua su tali parti del corpo, possiamo vedere che anche gli assi ideologici dei modelli di lingua si spostano corrispondentemente.'</sample>
    <sample id="178">Per esempio, per Robert, che ha ricevuto un'ulteriore formazione e addestramento sulla corrente di sinistra del Reddit, possiamo vedere un cambiamento sostanziale verso il liberalismo.</sample>
    <sample id="179">In termini di distorsioni politiche.</sample>
    <sample id="180">Stiamo anche cercando di indagare se i modelli linguistici possono cogliere la polarizzazione che è presente nella nostra società moderna.</sample>
    <sample id="181">Dividiamo quindi il pre-training in due periodi: prima e dopo la quarantacinquesima presidenza degli Stati Uniti. separatamente pre-trainiamo modelli di lingua sui due diversi periodi temporali.</sample>
    <sample id="182">I modelli di lingua hanno generalmente una tendenza politica che si allontana dal centro dopo il 2017, quindi questo indica che i modelli di lingua possono anche raccogliere la polarizzazione nella nostra società.</sample>
    <sample id="183">Quindi, per concludere, abbiamo valutato modelli di lingua con significativi orientamenti politici sulla detezione del discorso haine e della fake news, applicazioni che spesso coinvolgono i modelli di lingua e possono avere implicazioni molto importanti.</sample>
    <sample id="184">Quindi vediamo che se esploriamo il rendimento per categoria, cioè se dividiamo il rendimento in due o più categorie, vediamo che...</sample>
    <sample id="185">Differenti demografie o orientamenti politici nei media di notizie, possiamo vedere un modello che, ad esempio, per la rilevazione del discorso d'odio, i modelli di lingua di sinistra sono migliori.</sample>
    <sample id="186">Il contenuto inglese descrive un processo di rilevamento del discorso haine che mira a gruppi socialmente minoritari. In italiano, si traduce come: 'Nel rilevamento del discorso haine che mira ai gruppi socialmente minoritari.'</sample>
    <sample id="187">Il contenuto inglese descrive che i lavori di rilevamento del discorso del odio sono più efficaci nel targeting gruppi più potenti della nostra società. In italiano, questo sarebbe: 'Tuttavia, i lavori di rilevamento del discorso del odio sono più efficaci nel targeting gruppi più potenti della nostra società.'</sample>
    <sample id="188">E viceversa, i modelli di lingua sono meglio adatti alla rilevazione del discorso haine mirato ai bianchi e agli uomini, tuttavia peggio nella rilevazione del discorso haine mirato ai neri, agli omosessuali, alle persone con orientamento sessuale non binario e altre minoranze.</sample>
    <sample id="189">I trend simili si verificano anche per la rilevazione di notizie false, dove vediamo che i modelli di lingua left-leaning sono meglio adatti a rilevare informazioni false dai loro linguaggi politicamente opposti e viceversa.</sample>
    <sample id="190">Questo significa che mostreremo molti esempi qualitativi per vedere che i modelli di linguaggio con significati politici diversi sono...</sample>
    <sample id="191">Istruzioni per l'utilizzo di un motore di ricerca avanzato (parte 2)</sample>
    <sample id="192">Questo indica che c'è un problema di equità molto preoccupante riguardo alle tendenze politiche dei modelli linguistici.</sample>
    <sample id="193">Per esempio, se i modelli di lingua a destra dovessero essere selezionati per l'eliminazione del discorso offensivo o della falsa informazione e quindi distribuiti su una popolare piattaforma sociale.</sample>
    <sample id="194">Questo significa che le persone con opinioni politiche opposte potrebbero essere marginalizzate e la diffusione della propaganda di odio mirata ai gruppi minoritari potrebbe diventare sempre più diffusa senza alcuna controllo.</sample>
    <sample id="195">Questo segnala l'allarme per noi di riconoscere e affrontare le questioni di equità causate dalle politiche linguistiche dei modelli di lingua.</sample>
    <sample id="196">Quindi, un po' di discussione, vorremmo anche sottolineare che esponiamo il dilemma unico riguardante le politiche linguistiche modali, che è tra Cile e Corea del Sud.</sample>
    <sample id="197">Quindi, se non sanalizziamo le opinioni politiche nei dati di addestramento del modello linguistico, la propensione all'errore si propagherà dalle informazioni pre-addestrate ai modelli linguistici e alle attività di down-stream, creando infine problemi di parità.</sample>
    <sample id="198">Se proviamo a sanificare in qualche modo, rischieremo anche la censura o l'esclusione e è incredibilmente difficile determinare cosa sia effettivamente neutrale e ciò che dovrebbe essere mantenuto nella raccolta dei dati della lingua monitorata. È un po' come il problema dell'elettroscopio elettrico.</sample>
    <sample id="199">Okay, great. I think that's pretty much all I have for today. Thank you for your time.</sample>
    <sample id="200">Due.</sample>
    <sample id="201">Sono state eseguite valutazioni MPP su fino a 2024 token di lunghezza del contesto.</sample>
    <sample id="202">Il loro set di dati include 'example.com' e 'example.org'.</sample>
    <sample id="203">Posizionamento è semplicemente le prospettive che le persone hanno come risultato delle loro demografia, identità e esperienze di vita.</sample>
    <sample id="204">Il relatore del video si chiama David.</sample>
    <sample id="205">Sì, è possibile utilizzare un modello ST offline già esistente senza rieducarlo o adattarlo per i requisiti di多样本学习.</sample>
    <sample id="206">Un solo autore, Jiaxin Zhang.</sample>
    <sample id="207">No, il modello non funziona bene sulla suite di test.</sample>
    <sample id="208">Le tre varianti di KITMUS sono: setting 1 con conoscenza background pre-train, setting 2 con conoscenza background disponibile sia durante la fase di pre-training che in quella di addestramento, e setting 3 con conoscenza background disponibile solo durante la fase di addestramento.</sample>
    <sample id="209">Il lavoro è un joint work con Philipp Radinski, Silvia Perretti e Anne Luise.</sample>
    <sample id="210">Se si devono utilizzare solo i campioni puliti per la validazione, o ci sono altre tecniche migliori per utilizzarli?</sample>
    <sample id="211">La sensibilità della metrica misura la capacità del modello di generare sempre gli stessi output per lo stesso compito, indipendentemente dalla variazione minima nell'input.</sample>
    <sample id="212">La relatrice si chiama Jing Wei Yi.</sample>
    <sample id="213">Una maggiore sensibilità indica generalmente una performance del modello migliore, poiché significa che il modello è in grado di rilevare anche piccoli cambiamenti nell'input. Tuttavia, senza informazioni specifiche sulla natura e qualità dei dati di addestramento utilizzati, non è possibile dire con certezza se la maggiore sensibilità sia un segno positivo o negativo per il modello.</sample>
    <sample id="214">I modelli vengono addestrati utilizzando un corpus di testo di grande dimensione, che include una vasta gamma di esempi di lingua naturale. Questi esempi rappresentano diverse tipologie di testo, come articoli, discorsi, e-mail, conversazioni, e così via, per assicurare che i modelli siano in grado di gestire una vasta gamma di situazioni linguistiche durante l'addestramento.</sample>
    <sample id="215">In generale, si impiega solo un campione di validazione pulito per ottenere prestazioni elevate in WSL.</sample>
    <sample id="216">I primi due autori sono in collaborazione con Esben Dürmusch e Dan Darrofsky.</sample>
    <sample id="217">I nuovi metodi sono necessari perché i modelli linguistici hanno tendenze politiche variabili e occupano tutti gli spazi sul compasso politico, mostrando che i bias dell'informazione esistono e sono significativi.</sample>
    <sample id="218">La relatrice è Makshata.</sample>
    <sample id="219">L'infrastruttura di propagazione dei bias politici include tutto, dal pre-training dei dati fino ai modelli linguistici e alle applicazioni di basso livello, passando per la selezione delle questioni da porre durante le indagini.</sample>
    <sample id="220">Sì, il corpus DEplan-apa ha una maggioranza di trasformazioni di semplicitizzazione diverse rispetto al corpus DEplan-web, che ha più riformulazioni. Inoltre, il corpus Web ha molte più riformulazioni rispetto al corpus APA.</sample>
    <sample id="221">Sì, è disponibile pubblicamente.</sample>
    <sample id="222">La filigrana viene inserita nel testo in base al numero di trigger presenti nella frase inviata dall'utente al servizio del provider, che calcola il peso della filigrana sottraendo il peso dell'originale e moltiplicando per il numero di trigger. Se il numero di trigger è maggiore di un valore specifico M, allora il peso fornito dal provider è esattamente uguale alla filigrana.</sample>
    <sample id="223">Il primo autore è Jason Zhang e fa parte del Dipartimento di Informatica presso l'Università dello Utah.</sample>
    <sample id="224">Sì, i modelli codificatore-decodificatore come mt5 possono migliorare con l'addestramento su una combinazione di lingue.</sample>
    <sample id="225">Un esempio di pianificazione linguistica vincolata è la pianificazione della produzione di una torta al cioccolato, che tiene conto di determinate restrizioni specifiche.</sample>
    <sample id="226">Ispirandosi al metodo di verificazione utilizzato per i file binari, gli autori validano la segretezza dell'incorporamento fornito eseguendo una verifica delle frasi nascoste su un dizionario di controllo di 40.000 parole.</sample>
    <sample id="227">Il lavoro utilizza tre modelli di addestramento su pre-training esistenti per analizzare l'impatto delle strategie di pre-training.</sample>
    <sample id="228">Con il Paese GPT-4 non ci sono informazioni specifiche su chiaramente identificabili come paese con cui sia meno allineato. Tuttavia, si dice che i dati e modelli sono maggiormente associati ai paesi di lingua inglese.</sample>
    <sample id="229">La relatrice mostra l'utilizzo del meccanismo dell'attenzione nella frase 'and leverage the knowledge already acquired by the model through the tension mechanism between audio input and textual output, that is the cross attention mechanism.'</sample>
    <sample id="230">La quantità di attività aumenta, il modello raggiunge una migliore prestazione e una maggiore sensibilità nel tempo medio.</sample>
    <sample id="231">Gli autori confrontano il loro metodo con altri modelli a tre meno riferimenti sulle prestazioni di generalizzazione e ricorsione.</sample>
    <sample id="232">I due coautori sono advisor di primo autore.</sample>
    <sample id="233">Il primo autore di PaLM non è stato specificato nel contenuto inglese fornito.</sample>
    <sample id="234">Cari amici, sono Jennie, un studente di primo anno al Carnegie Mellon University e oggi presenterò il mio lavoro e la mia posizione personale riguardo il design basato sui dati sensibili.</sample>
    <sample id="235">Questa opera è stata realizzata in collaborazione con alcuni colleghi dell'Università di Washington e l'Istituto Allen per l'AI, ovvero Sebastian Santi, Ronan Le Bras, Katerina Rinica e Martin Sap.</sample>
    <sample id="236">Quindi, cominciamo immaginando di lavorare per un giornale e di srotolare i commenti sotto l'articolo news cercando di eliminare il contenuto offensivo.</sample>
    <sample id="237">Il contenuto inglese descrive come potresti rivolgerti a un'API popolare, come l'API Perspective per la detezione della tossicità, e come funziona bene per chi ha nome Carl Jones, dove l'API Perspective è in grado di rilevare correttamente le sostanze tossiche. In italiano, questo sarebbe: 'Potresti rivolgerti ad un'API popolare, come l'API Perspective per la detezione della tossicità, e questo funzionerà molto bene se sei Carl Jones, dove l'API Perspective è in grado di rilevare correttamente le sostanze tossiche.'</sample>
    <sample id="238">Ma questo non è really il caso per Aditi Sharma, dove l'API prospettiva non è veramente sensibile ai termini offensivi che sono più comuni nei contesti indiani.</sample>
    <sample id="239">Questo è un esempio di disegno di pregiudizio, dove vediamo differenze sistematiche nella prestazione delle tecnologie tra le popolazioni.</sample>
    <sample id="240">I disegni di tendenza come quello che abbiamo visto prima potrebbero verificarsi a causa della posizione degli studiosi e dei sviluppatori NLP. La posizione è semplicemente le prospettive che le persone hanno a causa delle loro demografia, identità e esperienze di vita.</sample>
    <sample id="241">Questo è un concetto ampiamente utilizzato negli studi critici, soprattutto nelle accademie femministe e queer.</sample>
    <sample id="242">Come ricercatore, la posizione può influire sul processo di ricerca e sui suoi risultati perché può cambiare le decisioni che i ricercatori prendono.</sample>
    <sample id="243">Il contenuto inglese si traduce in italiano come: 'Quindi una domanda che potrebbero fare è: i set di dati e i modelli hanno posizione?'</sample>
    <sample id="244">Non stiamo dicendo che i modelli, le cellule e gli dataset hanno identità demografiche e esperienze di vita, ma che essi aggregano giudizi e opinioni di persone reali e possono rappresentare certe posizioni mediche rispetto ad altre.</sample>
    <sample id="245">Il lavoro pregresso ha suggerito alcune evidenze aneddotiche di posizione, come distorsioni culturali nei modelli e nei set di dati, nonché definizioni teoriche della posizione del modello.</sample>
    <sample id="246">Tuttavia, queste opere non esaminano veramente la comparazione degli utenti con i dati set e modelli stessi.</sample>
    <sample id="247">Il modello di sostituzione e la posizione del codice diventano sempre più importanti perché i test dell'NLP diventano sempre più mirati e orientati alla società.</sample>
    <sample id="248">Il contenuto inglese descrive come sia difficile caratterizzare queste posizioni perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API. In italiano, questo sarebbe: 'E' difficile caratterizzare queste posizioni poiché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro alle API.'</sample>
    <sample id="249">Quindi, per studiare la posizione dei dati e della modellazione, verifichiamo effettivamente le annotazioni con gli utenti reali e con i set di dati esistenti e con i modelli.</sample>
    <sample id="250">Questo lo facciamo attraverso il nostro framework NLP Positionality.</sample>
    <sample id="251">Il nostro framework funziona in due fasi principali.</sample>
    <sample id="252">Il primo passo è ricodificare i set di dati con diversi etichettatori.</sample>
    <sample id="253">Ecco la traduzione in italiano del contenuto inglese: 'E dovremmo farlo guardando le demografie dei set di dati originali, degli etichettatori perché solitamente solo pochi etichettatori etichetano ogni esempio e perché le demografie sono raramente raccolte e condivise.'</sample>
    <sample id="254">Così, dobbiamo ricorsivamente etichettare i dati per ottenere molti etichette e un set di dati demografici ricco.</sample>
    <sample id="255">Quindi, prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i set di dati utilizzando la valutazione della corrispondenza di apparienza.</sample>
    <sample id="256">Il nostro quadro di riferimento differisce effettivamente dalla letteratura sull'annotazione degli errori in quanto confronta gli utenti finali con modelli e set di dati, previsioni e etichette, invece di guardare solo all'agreement annotatore o alla modellazione delle distribuzioni annotatore.</sample>
    <sample id="257">Il nostro framework è largely enabled through Lab in the Wild, un platform di crowd-sourcing online former HCI collaboratore.</sample>
    <sample id="258">In Live on the Wild è un platforma di sperimentazione online dove possiamo reclutare volontari di diverse categorie, rispetto ai platform come Mturk che hanno principalmente partecipanti dagli Stati Uniti o dall'India. Inoltre, Live on the Wild riesce ancora a ottenere dati di alta qualità.</sample>
    <sample id="259">Iscriviti al nostro corso di laboratorio su due task online nel mondo, uno dei quali è "Social Acceptability". Il funzionamento di questo progetto è che i partecipanti leggeranno una situazione dal database di chimica sociale e poi scriveranno come sarebbe accettabile socialmente.</sample>
    <sample id="260">Dopo di che, per restare coinvolti nello studio, possono confrontare le loro risposte con quelle degli altri.</sample>
    <sample id="261">Avremmo quindi confrontato queste annotazioni con la chimica sociale, DeltaF e GPT4.</sample>
    <sample id="262">L'audio descrive un setup simile per la detezione del linguaggio di odio e della tossicità, dove le persone leggono un esempio da "Dinah hate" e scrivono se pensano che sia un esempio di linguaggio di odio. In italiano: "Allora abbiamo ripetuto lo stesso setup per la detezione del linguaggio di odio e della tossicità, dove le persone leggono un esempio da 'Dinah hate' e scriveranno se pensano che sia un esempio di linguaggio di odio."</sample>
    <sample id="263">Il contenuto inglese descrive un processo di confronto di annotazioni, che vengono utilizzate per analizzare grandi quantità di dati. In italiano, la traduzione sarebbe: 'Nel nostro studio, abbiamo raccolto più di sedici migliaia di annotazioni da oltre mille annotatori in ottanta sette paesi.'</sample>
    <sample id="264">Quindi, ora siamo equipaggiati per rispondere: chi sono i set di dati NLP e modelli che si adattano meglio? Troviamo che ci sia posizionalità nei set di dati NLP.</sample>
    <sample id="265">Per esempio, troviamo che i set di dati e i modelli sono maggiormente associati ai paesi che parlano inglese. Ad esempio, per l'analisi della socialità accettabile del GPT-4, riscontriamo che è maggiormente associato alla cultura cinese e ai paesi che parlano inglese. Troviamo anche che il disprezzo per le persone trans è anche più associato ai paesi che parlano inglese.</sample>
    <sample id="266">Inoltre, abbiamo anche trovato un maggiore allineamento con le persone che hanno una formazione universitaria. Per la tarefa di accessibilità sociale di GPT-4, abbiamo scoperto che è più adattata alle persone con un'istruzione universitaria o al college.</sample>
    <sample id="267">E lo troviamo anche per Donna, dove è più associato alle persone con un'istruzione universitaria.</sample>
    <sample id="268">Tuttavia, quando i modelli e i dati set sono associati a popolazioni specifiche, alcuni sono inevitabilmente lasciati indietro.</sample>
    <sample id="269">Un esempio di questo è che i set di dati e i modelli sono meno associati alle persone non bioniche rispetto ai loro omologhi maschili e femminili. Lo troviamo nella task socialmente accettabile del GPT-4, così come nell'analisi della prova di disprezzo per le donne.</sample>
    <sample id="270">Quindi, dato che c'è un posto disponibile in lady D e LP, cosa possiamo fare?</sample>
    <sample id="271">abbiamo alcune raccomandazioni per questo. la prima è quella di tenere traccia di tutte le scelte progettuali rilevanti durante tutto il processo di ricerca e l'altra è quella di fare una ricerca NLP con un'ottica di prospettiva.</sample>
    <sample id="272">La nostra terza raccomandazione è quella di costruire set di dati e modelli specializzati all'interno di quattro comunità specifiche, e un buon esempio di questo è l'iniziativa Musashi. Vorrei sottolineare che l'accesso inclusivo alle tecnologie non significa solo rendere tutte le tecnologie utili per tutti.</sample>
    <sample id="273">Ecco quindi la presentazione, ma se vuoi imparare di più, non esitare a consultare il nostro elenco dei risultati dell'analisi e il nostro paper. Grazie!</sample>
    <sample id="274">La relatrice menziona tre problemi principali associati ai modelli di SimulST: l'introduzione di nuovi moduli per essere ottimizzati, procedure di addestramento lunghe e complesse, e la necessità di mantenere diversi modelli per raggiungere differenti regole di latenza.</sample>
    <sample id="275">Non esiste una soluzione semplice o universale per questo problema, poiché dipende dalle sfide specifiche del contesto e dalla natura dei dati stessi. Tuttavia, alcuni possibili approcci includono la raccolta e l'utilizzo di dati più rappresentativi della popolazione, l'adozione di tecniche di pre-processing dei dati per identificare e rimuovere le informazioni tendenziose, l'utilizzo di algoritmi di correzione degli errori e l'introduzione di meccanismi di monitoraggio e valutazione per tenere traccia dell'impatto del bias nelle prestazioni dei modelli.</sample>
    <sample id="276">Ciao, sono Siyuan Yu dall'Università di Fudan. Sono qui per introdurre il nostro lavoro, che distingue la conoscenza del script da modelli di grande lingua per pianificazione della lingua con vincoli.</sample>
    <sample id="277">Nel corso della vita quotidiana, gli esseri umani pianificano spesso le loro azioni seguendo istruzioni passo per passo in forma di script granted.</sample>
    <sample id="278">Il contenuto inglese descrive come sono stati utilizzati i modelli di linguaggio per pianificare azioni astratte o stereotipiche, come fare una torta, dimostrando che i grandi modelli di linguaggio possono decomporre le azioni in fasi. In italiano, questo sarebbe: 'I modelli di lingua precedenti hanno esplorato l'utilizzo di modelli di linguaggio per pianificare azioni astratte o stereotipiche, come fare una torta, e mostrano che i grandi modelli di linguaggio possono decomporre le azioni in fasi.'</sample>
    <sample id="279">Il contenuto inglese descrive che i lavori precedenti si concentravano principalmente sulla pianificazione di obiettivi astratti per attività tipiche, mentre la pianificazione di obiettivi con vincoli specifici, come fare una torta al cioccolato, rimane ancora poco studiata. In italiano, questo sarebbe: 'Tuttavia, i lavori precedenti si sono concentrati principalmente sul pianificare obiettivi astratti per attività tipiche, mentre la pianificazione di obiettivi con vincoli specifici, come fare una torta al cioccolato, rimane ancora poco studiata.'</sample>
    <sample id="280">In questo articolo, definiamo il problema della pianificazione del linguaggio vincolato.</sample>
    <sample id="281">Il testo descrive che ogni obiettivo ha delle restrizioni diverse e un piano di azione può essere adattato per soddisfare diverse situazioni reali con molte restrizioni. Un buon pianificatore dovrebbe scrivere script ragionevoli e flessibili alle restrizioni.</sample>
    <sample id="282">Nel presente lavoro, valuteremo e miglioreremo innanzitutto la capacità di pianificazione della lingua vincolata dei grandi modelli linguistici.</sample>
    <sample id="283">Nessun altro dato, tranne quelli specifici delle persone, esiste per la nostra ricerca.</sample>
    <sample id="284">Dobbiamo acquisire queste regole per primi e, mostrando la tabella, estendiamo le regole astratte con restrizioni specifiche per l'utilizzo dell'acquisizione di dati di look-up per gli utenti.</sample>
    <sample id="285">Esampliamo mille donne specifiche e valutiamo le scripte generate da grandi modelli.</sample>
    <sample id="286">Il contenuto inglese descrive che tutti i modelli di rimozione di linee del modello raggiungono risultati soddisfacenti per quanto riguarda la pianificazione specifica delle linee. In italiano, questo sarebbe: 'I modelli di rimozione delle linee del modello raggiungono risultati soddisfacenti per quanto riguarda la pianificazione delle linee specifiche.'</sample>
    <sample id="287">Quindi condurremo un'analisi dettagliata per investigare perché i modelli di apprendimento automatico falliscono.</sample>
    <sample id="288">I risultati della figura mostrano che la completezza semantica nei script generati è accettabile, ma non si può garantire fedeltà alle restrizioni.</sample>
    <sample id="289">Nel nostro studio, abbiamo esplorato le restrizioni funzionali più gravi definite in Wikipedia per quanto riguarda i risultati del piano. Il mappa principale mostra che il rendimento del piano varia considerevolmente tra le diverse categorie di istruzioni.</sample>
    <sample id="290">I risultati degli studi precedenti hanno dimostrato che la qualità dell'output dei modelli di rappresentazione a livello di linguaggio varia notevolmente, causando prestazioni inferiori. Di conseguenza, abbiamo adottato l'idea di un filtro di generazione overriden per migliorare la qualità della generazione.</sample>
    <sample id="291">Mostriamo prima i tipi di vincoli con esempi per l'ICPCT e otteniamo obiettivi specifici basati su tali obiettivi astratti.</sample>
    <sample id="292">Inserisci descrizioni dei dati per le tue quattro ragazze specifiche nella tabella GP天生。</sample>
    <sample id="293">Il contenuto inglese descrive come viene sviluppato un modello di filtro per selezionare gli script più fatti. In italiano, questo sarebbe: "Successivamente, viene sviluppato un modello di filtro per selezionare gli script più validi."</sample>
    <sample id="294">converto script e gozze in istruzioni GPB, e calcolo la somiglianza di senso come punteggio di somiglianza.</sample>
    <sample id="295">Inoltre, identifichiamo la riga di script che contiene le parole chiave del vincolo target. Siamo interessati solo alle righe di script se il valore del ghost del target ha ottenuto il massimo punteggio nella schermata dei ghost.</sample>
    <sample id="296">Con la nostra tecnica, l'insight CPB può generare fili di alta qualità. La nostra tecnica miglioramenti notevolmente la planificabilità, sia nella completezza semantica che nella fedeltà alle restrizioni.</sample>
    <sample id="297">I modelli di lingua sono costosi da eseguire, quindi è essenziale consentire la capacità di pianificazione della lingua dei modelli più piccoli e specializzati. La creazione del dataset è un passo importante per raggiungere questo obiettivo.</sample>
    <sample id="298">I precedenti studi, tuttavia, non prevedono pianificazione specifica per obiettivi precisi e l'annotazione manuale del database è costosa.</sample>
    <sample id="299">Questo è il contenuto tradotto in italiano: 'Si segue l'idea di distillazione simbolica della conoscenza, per estrarre i dati di pianificazione del linguaggio da modelli di lingua a larga scala.'</sample>
    <sample id="300">Applicheremo il nostro metodo per la costruzione di un set di dati di pianificazione del linguaggio connesso, chiamato CoScript.</sample>
    <sample id="301">In totale, generiamo cinquantamila esempi specifici con script per garantire la qualità della validazione e dei siti di test. Chiediamo ai lavoratori esterni di revisionare gli esempi corrotti o incorrecti.</sample>
    <sample id="302">Questo grafico mostra la distribuzione vincolata del codice sorgente. Abbiamo visto che il codice sorgente mostra un'elevata aspettativa di adozione nei generati specifici goals. Con il codice sorgente, possiamo utilizzare modelli più piccoli e specializzati per pianificare il linguaggio vincolato.</sample>
    <sample id="303">Abbiamo scoperto che il modello T5L-12x1024 su larga scala può generare script di qualità della pelle migliore rispetto ai modelli più grandi, indicando che i modelli più piccoli possono essere sostenuti da modelli più grandi quando addestrati correttamente sui dati adatti.</sample>
    <sample id="304">In sintesi, abbiamo stabilito il problema della pianificazione del linguaggio con restrizioni, abbiamo valutato l'abilità di pianificazione del linguaggio con restrizioni dei modelli di lingua large e sviluppato un metodo di filtro di sovrappopolamento per i modelli di lingua large.</sample>
    <sample id="305">Utilizziamo grandi modelli di linguaggio per generare un set di dati di script scritto di alta qualità, CoScript, per la pianificazione della lingua. Speriamo che il set di dati di CoScript possa diventare una risorsa valiosa per la ricerca sulla pianificazione della lingua.</sample>
    <sample id="306">Grazie per il tuo tempo, cerca di trovare maggiori dettagli sul codice sorgente nella tua carta.</sample>
    <sample id="307">La fluidità di PaLM è comparabile alle altre systeme di arte.</sample>
    <sample id="308">Il metodo di filigrana deve essere applicabile agli impianti di servizio, non degradare l'utilizzo degli impianti forniti, essere abbastanza flessibile per essere rimosso dall'attaccante e trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello.</sample>
    <sample id="309">I discorsi TED in inglese sono stati tradotti in 14 lingue diverse.</sample>
    <sample id="310">Solo poche istanze vengono campionate da un set di dati per la riannotazione.</sample>
    <sample id="311">La metrica utilizzata è Delta Cosine similarity.</sample>
    <sample id="312">I modelli basati su codificatori multilingue sono stati valutati su due gruppi di modelli, tra cui encoder PDR che si riferisce a encoder multilingue con decodificatori basati sui punti, come XLM-R + PDR e BERT + PDR. Inoltre, ci sono state valutazioni anche dei modelli di encoder-decoditore multilingue.</sample>
    <sample id="344">Gli autori selezionano un set di parole a frequenza moderata scegliendo un gruppo di parole in un intervallo di frequenza moderato, supponendo che il fornitore possa raccogliere un corpus generale di testo e contare la frequenza delle parole.</sample>
    <sample id="345">Ciao a tutti, il mio nome è Zhuoheng. Oggi vi presenterò il nostro paper "Do Conll 2003 named entity tagger still work well in 2023?". Andiamo avanti.</sample>
    <sample id="346">Il nostro paper ha esplorato il problema della generalizzazione utilizzando il compito di riconoscimento degli entità chiamato 'NER task'.</sample>
    <sample id="347">I modelli sono stati utilizzati per sviluppare l'NER da oltre venti anni ed ovviamente questo ha portato a diversi problemi. In primo luogo, questi modelli possono essere generalizzati ai dati più recenti?</sample>
    <sample id="348">Il contenuto inglese dice: 'E quando sviluppiamo nuovi tag, cosa serve per una buona generalizzazione?'</sample>
    <sample id="349">Nel contempo, se osserviamo una cattiva generalizzazione, cosa causa la diminuzione delle prestazioni dei modelli?</sample>
    <sample id="350">Per indagare su questi problemi, abbiamo sviluppato l'insieme di dati Connel Plus Plus. Questo è un insieme di dati che abbiamo raccolto da Reuters News dal 2020 e poi ha annotato con le stesse linee guida di annotazione del 2023.</sample>
    <sample id="351">L'audio descrive come siamo stati in grado di ottimizzare oltre venti modelli su Convolv层 2003, valutandoli sia sui set di dati di test Convolv3 che sui set di dati di test Convolv++.</sample>
    <sample id="352">In ogni caso, abbiamo calcolato la variazione percentuale del modello F-1 per valutare la generalizzazione di ogni modello.</sample>
    <sample id="353">Così, cosa serve per una buona generalizzazione? Dopo tutti gli esperimenti, abbiamo scoperto che ci sono tre ingredienti principali necessari.</sample>
    <sample id="354">Il primo è l'architettura del modello. Nel nostro esperimento, abbiamo scoperto che i modelli di trasformatore generalizzano通常 meglio ai nuovi dati.</sample>
    <sample id="355">Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che generalmente i modelli più grandi conducono a una migliore generazione.</sample>
    <sample id="356">In ultima analisi, tutti noi sappiamo che il numero di esempi di regolazione fine influenza direttamente le prestazioni di un compito di downstream. Qui abbiamo anche scoperto che più esempi di regolazione fine in realtà conducono anche a una better generalizzazione.</sample>
    <sample id="357">Il contenuto inglese descrive che ci sono alcuni modelli che hanno una diminuzione delle prestazioni. In italiano, si dice: 'Ci sono alcuni modelli che hanno una diminuzione delle prestazioni.'</sample>
    <sample id="358">Abbiamo due ipotesi. La prima è l'adattamento overfitting, che si verifica quando si utilizza lo stesso set di test più e più volte, causando una diminuzione dei valori sul nuovo set di test. Questo fenomeno viene solitamente manifestato come la diminuzione del tasso di errore sul nuovo set di test.</sample>
    <sample id="359">La seconda ipotesi è che 'tempo di drift', che è la diminuzione della prestazione causata dall'aumentare il divario termico tra il treno e i dati di test.</sample>
    <sample id="360">Per la sovrapposizione adattativa, abbiamo visto che dalla grafica a destra, la linea rossa di massima adattabilità ha un gradiente maggiore di uno.</sample>
    <sample id="361">Questo significa che ogni unità di miglioramento apportata su Connel due mila e tre si traduce in più di una unità di miglioramento su Connel plus plus, il che significa che non ci sono riduzioni dei rendimenti.</sample>
    <sample id="362">Questo dimostra che l'adattamento non è stato osservato in questo caso.</sample>
    <sample id="363">Quindi che succede al cambiamento climatico?</sample>
    <sample id="364">Per il drift termale, abbiamo eseguito un esperimento per riaddestrare o continuare ad addestrare alcuni modelli con dati più recenti e abbiamo scoperto che il rendimento decade con un maggior intervallo temporale.</sample>
    <sample id="365">Questo conferma la nostra ipotesi secondo cui il principale motivo della diminuzione delle prestazioni è il drift termico.</sample>
    <sample id="366">La conclusione è che per una buona generalizzazione, avremmo bisogno di un'architettura del modello migliore, dimensioni di modello più grandi e anche esempi più raffinati. E questi obiettivi vanno di mano in mano: non possiamo avere solo uno ingredientio, ma tutti gli altri.</sample>
    <sample id="367">Nel contempo, abbiamo anche scoperto che la diminuzione delle prestazioni qui è causata da un cambiamento di temperatura e, sorprendentemente, non è causata dall'adattabilità all'ambientazione, anche se il modello di colonnello del 2003 è stato utilizzato per più di venti anni.</sample>
    <sample id="368">Quindi, tornando al problema sollevato nella intestazione del nostro articolo, i taggati di Connel 2003 funzionano ancora nel 2023? Ebbene, abbiamo scoperto che la risposta è effettivamente sì.</sample>
    <sample id="369">Speriamo che il nostro corso di studio richieda più ricerche su come migliorare la generalizzazione dei modelli.</sample>
    <sample id="370">E, per finire, assicurati di controllare il nostro giornale, il nostro dataset e se hai qualsiasi domanda, non esitare a contattarmi. Grazie mille.</sample>
    <sample id="397">30 secondi</sample>
    <sample id="398">Per rispondere alla domanda, è necessario prima di tutto identificare l'entità specifica menzionata, in questo caso 'Servin'. Successivamente, si richiede la conoscenza generale sulle persone per riconoscere che 'judge' (giudice) è un titolo o una posizione.</sample>
    <sample id="399">La qualità dell'esempio è più importante della somiglianza con la frase sorgente.</sample>
    <sample id="400">L'articolo si concentra sui modelli linguistici GPT-4 e i suoi varianti, nonché su BERT.</sample>
    <sample id="401">Il modello combina i punteggi di più livelli.</sample>
    <sample id="402">I nomi degli album o posizione nella lista.</sample>
    <sample id="403">I membri dell'équipe sono appartenenti all'Università di Fudan.</sample>
    <sample id="404">Un solo autore, Yannick Lavaur.</sample>
    <sample id="405">Sì, l'utilizzo di una traduzione automatica per la query in linguaggio naturale prima del parsing semantico è stato considerato un approccio standard.</sample>
    <sample id="406">Il gruppo contrassegnato è 'woman warrior'.</sample>
    <sample id="407">Le architetture dei modelli trasformattori non generalizzano通常 in modo adeguato.</sample>
    <sample id="408">I nomi dei set di dati di test sono 'clean' e 'wsl'.</sample>
    <sample id="409">Due.</sample>
    <sample id="410">L'autore utilizza il testo solo.</sample>
    <sample id="439">L'autore afferma che l'NLU si concentra troppo sui modelli basati su regole e non abbastanza sui modelli basati sull'intelligenza artificiale estesa (comprende anche l'apprendimento automatico supervisato e non supervisionato).</sample>
    <sample id="440">Inni, Kaylee e Zhengyang.</sample>
    <sample id="441">Sì, i script sono stati generati e revisionati da lavoratori esterni per garantire la qualità della validazione e dei test.</sample>
    <sample id="442">Le risorse esistenti per la traduzione dipendente dal contesto supportano solo un numero limitato di tipi di traduzioni e di lingue, poiché si basano generalmente sulle conoscenze di dominio e sulla creazione umana.</sample>
    <sample id="443">"Oggi parlerò del nostro lavoro sulla risoluzione di espressioni indirette per la selezione di entità, in cui introducemo l'identificatore d'entità."</sample>
    <sample id="444">Il contenuto inglese si traduce in italiano come: 'Ecco il mio nome, Javad Hosseini, e questo è un lavoro condiviso con Philip Radoszynski, Sylvia Parity e Anne Lewis.'</sample>
    <sample id="445">Il nostro obiettivo è capire la lingua degli utenti quando vogliono fare una scelta e consideriamo questa domanda alternativa: "Hai inteso 'facile per me' o 'ho avuto un'impressione?' Qui, l'utente vuole selezionare tra questi due siti.</sample>
    <sample id="446">Il contenuto inglese dice: 'La cosa più ovvia è utilizzare una riferenza diretta, ad esempio dicendo il nome della canzone o la sua posizione nella lista.'</sample>
    <sample id="447">Ma a volte è più appropriato avere una conversazione più naturale con un amico indiretto. Questo potrebbe accadere quando l'utente non ricorda il nome dell'amico.</sample>
    <sample id="448">Tutte le pronuncia sono troppo simili tra loro e difficili da distinguer.</sample>
    <sample id="449">Il contenuto inglese descrive come l'utente possa specificare una preferenza direttamente, fornendo alcuni esempi di differenze dirette, come 'il nuovo one' o 'the song that's not energetic'. In italiano, si traduce come segue: 'oppure quando l'utente vuole specificare una preferenza, ecco alcuni esempi di differenze dirette, per esempio il nuovo one o la canzone che non è energetica.'</sample>
    <sample id="450">Questo è un problema importante nei sistemi di conversazione e anche per la classificazione degli elementi di testo.</sample>
    <sample id="451">Non siamo a conoscenza di un set di dati pubblico a grande scala per questo compito, quindi abbiamo raccolto uno utilizzando la censura delcrowd. Il nostro set di dati copre tre diversi domini: musica, libri e moda.</sample>
    <sample id="452">La metodologia della raccolta dei dati enfatizza l'informalità utilizzando un set di completamento di cartone.</sample>
    <sample id="453">Il cartone ha tre punti di dialogo. Nel primo punto, Bob dice: 'Ricorda quella canzone che stavamo ascoltando ieri?' E con questo, Bob fornisce il contesto del dialogo.</sample>
    <sample id="454">In secondo luogo, Alice dice: 'Significa per me o ho ottenuto un'idea?'</sample>
    <sample id="455">Il contenuto inglese si traduce in italiano come: 'Questa è la domanda alternativa e nell'ultimo periodo di conversazione, Bob usa un riferimento indiretto per selezionare una di queste entità, ad esempio il nuovo mondo.'</sample>
    <sample id="456">Il contenuto inglese descrive come vengano selezionati automaticamente i primi due speech bubble, mentre il terzo viene popolato dall'editor. In italiano si dice: 'Il primo e il secondo speech bubble vengono selezionati automaticamente, mentre il terzo viene popolato dall'editor.'</sample>
    <sample id="457">Il secondo, che è la domanda alternativa, viene generato come segue:</sample>
    <sample id="458">Il contenuto inglese si traduce in italiano come: 'Noi sempre utilizziamo un semplice modello di template, vuoi dire A o B? Ecco alcuni esempi da Wikipedia.'</sample>
    <sample id="459">I diversi metodi di campionamento che abbiamo utilizzato diventano sempre più simili man mano che ci muoviamo verso la fine dell'elenco, e diventa spesso più difficile effettuare l'analisi disgiuntiva.</sample>
    <sample id="460">Il primo è 'Trasmissione uniforme'.</sample>
    <sample id="461">Il secondo caso è quando le entità hanno titoli simili, ad esempio due libri con il nome 'The Retail'.</sample>
    <sample id="462">Il terzo è quando hanno descrizioni simili su Wikipedia e, infine, quando hanno informazioni o attributi simili su Wikipedia, ad esempio lo stesso genere per un libro o lo stesso artista per una canzone.</sample>
    <sample id="463">Quando mostriamo questa domanda alternativa agli stakeholder, sanno il nome di queste entità, ma non necessariamente conoscono l'entità stessa.</sample>
    <sample id="464">Quindi, ciò che facciamo è mostrare alcune conoscenze di base sui tardi '2000 per le canzoni. Abbiamo semplicemente mostrato un link di ricerca Google per ogni canzone.</sample>
    <sample id="465">Ecco un esempio dei risultati della ricerca in italiano: 'Quindi chiedi agli annotatori di ascoltare almeno alcuni brani di ogni canzone e leggere su ogni canzone.'</sample>
    <sample id="466">Per i domaini di ricette e libri, mostriamo alcuni testi di sfondo da Wikipedia. Per le ricette, abbiamo inoltre mostrato le loro immagini di nuovo da Wikipedia, in modo che gli annotatori sappiano come devono apparire.</sample>
    <sample id="467">Quindi chiediamo agli etichettatori di scegliere una delle entità, ad esempio qui la prima e descriverla usando tre a cinque espressioni indirette.</sample>
    <sample id="468">Per esempio, quello con la musica per pianoforte. Ecco alcuni esempi dal nostro set di dati: non quello senza parole, non quello con il ragazzo dodicesimo anno, quello fittizio o quello proveniente dall'Azerbaigian e così via.</sample>
    <sample id="469">Il corpus di entità ha sei migliaia di domande alternative su tre domini e ha quarantadue migliaia di espressioni indirette che si riferiscono ai risultati con il modello T5 large.</sample>
    <sample id="470">Il contenuto inglese dice: 'Se il modello linguistico ha accesso al medesimo background di conoscenza degli annotatori, allora l'accuratezza è veramente alta, intorno al novanta percento. Ma questo non è realistico.'</sample>
    <sample id="471">Se il modello linguistico ha accesso a alcune conoscenze di sfondo parzialmente sovrapposte, allora l'accuratezza è compresa tra il 82% e il 87%, che è più realistica. Ad esempio, quando il modello linguistico recupera le conoscenze di background.</sample>
    <sample id="472">Se il modello linguistico ha accesso solo ai nomi degli entità, allora l'accuratezza è del 60%. C'è quindi molto spazio per migliorare. Abbiamo anche mostrato che i modelli sono generalizzabili per domini. Ecco un link al nostro set di dati. Grazie.</sample>
    <sample id="473">Con le politiche SimulST che vengono applicate anche ai modelli online.</sample>
    <sample id="474">I due autori dell'articolo sono Yannick Lavaur e Bertrand.</sample>
    <sample id="475">Jenny</sample>
    <sample id="476">Due.</sample>
    <sample id="477">Ciao, sono Sarah Papa da New York University di Toronto e sono stata selezionata come guida per la relazione "Bilinguismo e traduzione simultanea". È un lavoro congiunto con McTeague Nagle e Marco Turco.</sample>
    <sample id="478">La traduzione simultanea del testo o SRT è il processo di tradurre il linguaggio parlato in un testo in un altro idioma in tempo reale, consentendo una comunicazione bidirezionale fluente tra i partecipanti.</sample>
    <sample id="479">I problemi attuali dei modelli di sintesi del simbolo sono che vengono utilizzate architetture specifiche che sono generalmente addestrate, introducendo moduli aggiuntivi per essere ottimizzati.</sample>
    <sample id="480">Procedure di addestramento complesse e lunghe, per esempio l'addestramento che coinvolge obiettivi di ottimizzazione diversi.</sample>
    <sample id="481">Il contenuto inglese descrive la necessità di addestrare e mantenere diversi modelli per raggiungere regimi di latenza diversi, ad esempio addestrando un modello con una media di latenza di uno secondo e un altro con due secondi di latenza e così via. In italiano si traduce come: "L'addestramento e il mantenimento di alcuni modelli per raggiungere differenti livelli di latenza, ad esempio addestrando un modello con una media di latenza di un secondo e un altro con due secondi di latenza e così via."</sample>
    <sample id="482">La soluzione è...</sample>
    <sample id="483">In primo luogo, utilizzare modelli di LSTMs esistenti senza rieducarli o adottare un'architettura specifica per l'SST. Utilizzare solo uno modello per ogni regime di latenza e trasmettere la latenza attraverso parametri specifici.</sample>
    <sample id="484">L'utilizzo della conoscenza acquisita dal modello attraverso il meccanismo di attenzione tra l'input audio e l'output testuale, cioè il meccanismo di attenzione crociata, è mostrato sull'esempio a destra.</sample>
    <sample id="485">La nostra soluzione è proporre un punto o un codice di attenzione, e questa è una strategia per cui decidiamo se emettere o meno una traduzione parziale basata sul luogo dove si trova l'attenzione.</sample>
    <sample id="486">Se la tensione non è concentrata, viene emesso un carattere 'E' se il somma delle tre cifre precedenti è inferiore ad un certo valore di soglia (alpha). Ciò significa che le informazioni ricevute sono abbastanza stabili.</sample>
    <sample id="487">Per esempio, se riceviamo un segmento di testo contenente 'Sono pronto a parlare di questo', e il nostro modello prevede una traduzione in tedesco.</sample>
    <sample id="488">Il contenuto inglese tradotto in italiano è: 'Ecco cosa faremo: guarderemo alla tensione di cross e al peso.'</sample>
    <sample id="489">Ierariamente, le prime due parole indicano i primi frame di speech ricevuti, mentre l'ultima parola indica i frame di speech meno recentemente ricevuti, come i frame di speech 'lambda'.</sample>
    <sample id="490">Questo significa che le prime due parole verranno eliminate.</sample>
    <sample id="491">Il contenuto inglese descrive una situazione in cui, se la somma delle tensioni crociate è superiore a un certo limite (che viene specificato come 'alfa'), non si procederà all'espulsione dell'ultimo membro della crew e si aspetterà l'intervento di un altro membro della squadra per parlare. In italiano, questo sarebbe traducibile come: 'Mentre, poiché la somma delle tensioni crossate è superiore ad un certo limite (che viene specificato come "alfa"), non si procederà all'espulsione dell'ultimo membro della crew e si aspetterà l'intervento di un altro membro della squadra per parlare.'</sample>
    <sample id="492">Se continuiamo e riceviamo un altro segmento di discorso, e il nostro modello prevede tre parole, guarderemo le loro attenzioni crociate.</sample>
    <sample id="493">Vedremo che nessun segno di spunta punta verso il frame di linguaggio less.</sample>
    <sample id="494">Questo significa che questi tre termini verranno omessi.</sample>
    <sample id="495">Se guardiamo i principali risultati di quel periodo, ...</sample>
    <sample id="496">abbiamo tracciato i risultati della traduzione simultanea su grafici in cui abbiamo blu da un lato che misura la qualità della traduzione e media like Instagram.</sample>
    <sample id="497">Questo è il misuratore di latenza e consideriamo anche la media ponderata della durata computazionale, che rappresenta i tempi di calcolo del modello per produrre l'output.</sample>
    <sample id="498">Quindi vogliamo che i nostri curativi siano il più alti possibile su questo plotto.</sample>
    <sample id="499">Ma anche noi vogliamo che siano spostati a sinistra.</sample>
    <sample id="500">Ecco il traduzione in italiano del testo inglese: 'E confrontiamo con strategie appropriate applicate anche ai modelli online, come la strategia di Whitkeys e l'accordo locale, e confrontiamo anche con l'architettura del sito specificamente progettata per la traduzione simultanea della pagina.'</sample>
    <sample id="501">Questi sono i risultati della strategia di traduzione del testo simultaneo sull'italiano.</sample>
    <sample id="502">Ecco la traduzione in italiano del contenuto inglese: 'Ed ecco che il modello outperforms tutte le strategie applicate ai modelli offline, poiché le curvate sono spostate verso sinistra.'</sample>
    <sample id="503">Inoltre, se consideriamo il tempo effettivo di esecuzione o il tempo di elaborazione computazionale, la risposta è falsa.</sample>
    <sample id="504">Se vuoi scoprire più risultati, leggi il nostro articolo e abbiamo anche pubblicato il codice sorgente, modelli e output paralleli per facilitare la riproducibilità del nostro lavoro. Grazie per l'attenzione.</sample>
    <sample id="505">Sì, il paper e il dataset sono disponibili pubblicamente.</sample>
    <sample id="506">Hello everyone, my name is In and Myeong-Ki Choyang, and I will be presenting our research on multi-instrumental learning by instruction.</sample>
    <sample id="507">Così, con gli avanzamenti nei grandi modelli di lingua, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento per riutilizzare i modelli di lingua pre-trainati per diverse tassonomie di sotto-grammi in modo efficiente sui dati.</sample>
    <sample id="508">In molti studi recenti è stato dimostrato che l'adattamento delle istruzioni consente ai modelli di grande lingua di svolgere compiti non noti in modo efficiente seguendo le istruzioni naturali.</sample>
    <sample id="509">Il contenuto inglese descrive che la maggior parte dei lavori precedenti sull'adattamento dell'instruzione si è concentrata sul miglioramento delle prestazioni su compiti linguistici unici, mentre i modelli di visione computazionale e multietichettatura sono stati lasciati fuori. In italiano potremmo dire qualcosa del tipo: 'La maggior parte degli studi precedenti sulla regolazione dell'instruzione si è concentrata sui miglioramenti delle prestazioni sui compiti linguistici unici, mentre i modelli di visione computazionale e multietichettatura sono stati esclusi.'</sample>
    <sample id="510">Quindi, in questo lavoro, vogliamo indagare se l'adattamento delle istruzioni su modelli multilayer può effettivamente migliorare la generazione di dati per le attività multilayer.</sample>
    <sample id="511">Inoltre, durante le nostre ricerche, abbiamo scoperto una notevole disparità nell'accessibilità dei set di istruzioni tra LP e modelli multiModal.</sample>
    <sample id="512">Il contenuto inglese tradotto in italiano è: 'Esistono più di mille e seicento task di istruzione unici, tuttavia non esiste alcuna grande raccolta di task di istruzione multilivello pubblicamente disponibile. Ciò ci demotiva a costruire un set di addestramento multilivello per l'addestramento degli strumenti.'</sample>
    <sample id="513">Qui presentiamo il primo set di dati di riferimento per l'addestramento multi-modello, che consiste di 62 compiti multietichettati diversi che coprono 10 categorie di board.</sample>
    <sample id="514">Questi compiti sono tratti da un set di dati open source esistente e ogni compito è fornito di cinque istruzioni scritte dagli esperti.</sample>
    <sample id="515">Per indagare sulla regolazione dell'inserimento multietichettato, i nostri dati di riferimento proposti sono: utilizziamo un modello unico multi-modello come nostro modello di base. OFA utilizza una lingua unica per i token di linguaggio, i token di immagine e le coordinate degli etichette di bound.</sample>
    <sample id="516">Istruzioni per l'utilizzo del motore di ricerca interno:

  1. Clicca sul pulsante "Cerca" situato nella barra laterale della home page.
  2. Inserisci il tuo termine di ricerca nella casella di ricerca centrale.
  3. Seleziona un'opzione di ricerca avanzata dal menu a tendina accanto alla casella di ricerca.
  4. Utilizza i filtri e le opzioni di selezione per limitare la tua ricerca.
  5. Clicca sul pulsante "Cerca" situato nella parte inferiore della pagina per eseguire la ricerca.

Nota: Assicurati di utilizzare una fonte di caratteri adeguata per leggere correttamente il contenuto dell'instruction manual.</sample>
    <sample id="517">Il contenuto inglese descrive un processo di elaborazione che può gestire diversi tipi di input e output. In italiano, si dice: "Unifica il trattamento di diversi tipi di input e output."</sample>
    <sample id="518">Il contenuto inglese descrive un processo di formattazione di una sequenza a sequenza unificata, dove vengono rappresentati tutti gli elementi come token nello spazio di token unico. In questo formato, le etichette di input, immagini, istruzioni e caselle di selezione sono rappresentate nello stesso spazio di token.</sample>
    <sample id="519">Ottimo, ora parlo di regolazione dell'instradamento multi-modello.</sample>
    <sample id="520">Per il set di dati di addestramento, utilizziamo cinquantatré task dal gruppo di Naive Bayes per l'addestramento e ne prendiamo esempi diecimila per la prova. Riserviamo interamente il gruppo di elaborazione del comune per la prova e selezioniamo altrettante altre cinque task dal gruppo di Word2Vec e dal gruppo di Mikrosilvia.</sample>
    <sample id="521">Nel testo si dice che: 'Per ogni task, utilizziamo tutte le istanze nella coda di lavoro. Inoltre, selezioniamo casualmente 20 task dalla coda di lavoro di istruzione naturale come task singolo NLP.'</sample>
    <sample id="522">Il contenuto inglese descrive un processo di addestramento di una grande rete pre-trainata utilizzando un modello come base. Durante l'addestramento, ogni istanza viene combinata casualmente con uno dei cinque modelli di istruzione della rete. In italiano, questo sarebbe descritto come: 'Per addestrare la grande rete pre-trainata, si utilizza un modello di base. Ogni istanza durante l'addestramento viene combinata casualmente con uno dei cinque modelli di istruzione della rete.'</sample>
    <sample id="523">Durante il testo per ogni task, eseguiamo un totale di cinque esperimenti valutando il modello utilizzando uno degli insegnamenti delle cinque istruzioni in ogni esperimento.</sample>
    <sample id="524">Il contenuto inglese descrive come la media e il massimo delle prestazioni, nonché la deviazione standard, sono stati calcolati per tutti gli esperimenti. In italiano, si dice: "Abbiamo calcolato la media e il massimo delle prestazioni e la deviazione standard per tutti gli esperimenti."</sample>
    <sample id="525">Se la tarefa è una classificazione multietichettata, si汇报 accuracy. Se è una generazione di modelli multietichettati, si汇报 accettazione. Se è una tarefa di generazione di modelli a tre etichette, si汇报 accettazione.</sample>
    <sample id="526">Abbiamo anche introdotto una metrica di valutazione supplementare chiamata sensibilità, che misura la capacità del modello di generare sempre gli stessi output per lo stesso compito, indipendentemente dalla variazione leggera nella direzione dell'istruzione.</sample>
    <sample id="527">Il nostro principale risultato è che l'adattamento delle istruzioni può migliorare significativamente le prestazioni di OFA su compiti multietichettati.</sample>
    <sample id="528">Il contenuto inglese tradotto in italiano è: 'Anche il trasferimento di apprendimento da un set di dati di istruzione naturale può beneficiare dell'addestramento dell'instruzione.'</sample>
    <sample id="529">Il contenuto inglese descrive come l'aumento del numero di task aumenti le prestazioni del modello e riduce la sensibilità al tempo medio. In italiano, si dice: 'Qui vediamo che aumentando il numero di task, il modello raggiunge prestazioni migliori e ha una maggiore sensibilità al tempo medio.'</sample>
    <sample id="530">Il contenuto inglese descrive un esperimento condotto utilizzando una istruzione invece di cinque istruzioni, mostrando che l'utilizzo di più istruzioni può migliorare le prestazioni del modello e ridurre la sua sensibilità. In italiano si dice: 'Inoltre, abbiamo anche fatto uno sperimento in cui abbiamo utilizzato una istruzione invece di cinque istruzioni. Come possiamo vedere, l'utilizzo di più istruzioni può migliorare le prestazioni globali del modello e ridurre la sua sensibilità molto.'</sample>
    <sample id="531">Questo mostra l'effetto di diverse strategie di regolazione sulle sensibilità del modello, come possiamo vedere, la trasferenza apprendimento dal set di dati di istruzione naturale, il modello può ottenere una sensibilità molto migliore rispetto al modello OFA originale.</sample>
    <sample id="532">Il contenuto inglese descrive che 'nessun altro dataset di istruzione naturale può aiutare a raggiungere prestazioni molto migliori sul set di dati di istruzione naturale.' In italiano, questo significa che nessun altro set di dati di istruzione naturale può migliorare le prestazioni del set di dati di istruzione naturale.</sample>
    <sample id="533">In sintesi, proponiamo il primo grande set di dati di addestramento multietichettato per migliorare la capacità di rilevamento del segnale dell'OFA e esplorare diverse tecniche di apprendimento trasferito mostrando i loro beneficrivibili vantaggi. Abbiamo progettato una nuova metrica chiamata sensibilità.</sample>
    <sample id="534">Il contenuto inglese tradotto in italiano è: 'Questo è il codice QR per i nostri dati e il modello. Grazie.'</sample>
    <sample id="535">Gli autori dell'articolo sono Sarah Papa e Franz Bruno Kassler, entrambi appartenenti all'Università di Toronto.</sample>
    <sample id="536">Jabot Hossaini</sample>
    <sample id="562">Ciao a tutti, sono Coos of Sinna e sono lieto di accogliervi nel nostro talk sul nostro articolo ACER 2023 intitolato "La giudizio di adattabilità dei modelli linguistici non è sempre robusta ai contesti".</sample>
    <sample id="563">Il lavoro congiunto è stato fatto con John Gower, Aaron Miller, Kanishka Mishra, Karen Fuentes, Roger Levy e Atina Williams.</sample>
    <sample id="564">In questo lavoro, esploriamo il paradigma del minimo numero di bit.</sample>
    <sample id="565">Il contenuto inglese descrive che il minimal pairling valuta i modelli linguistici sulla base dei giudizi di compatibilità, che possono includere fattori come la grammaticità (ad esempio 'pontificate' o 'pontificare'), l'accuratezza lessicale e la conformità ai tipi di testo standardizzati ('scritte corrette'). In italiano si dice: 'Il minimal pairling valuta i modelli linguistici sulla base dei giudizi di compatibilità, che possono includere fattori come la grammaticità (ad esempio "pontificate" o "pontificare"), l'accuratezza lessicale e la conformità ai tipi di testo standardizzati ("scritte corrette").'</sample>
    <sample id="566">Nel paradigma minimalista, la tipica maniera per valutare i modelli di linguaggio consiste nel mostrare un frase accettabile o una frase grammaticale e poi una frase inaccettabile o non grammaticale.</sample>
    <sample id="567">Il contenuto inglese descrive l'aspettativa di un modello che \"assegni maggiore probabilità al settaggio accettabile\". In italiano, si dice: "Ecco l'aspettativa del modello che assegna maggior probabilità al settaggio accettabile".</sample>
    <sample id="568">Il flusso attuale del MPMP non ci consente di valutare l'aderenza dei modelli alle frasi più lunghe.</sample>
    <sample id="569">Nel corso degli ultimi tempi, i grandi modelli di lingua stanno emergendo con finestre di contesto sempre più lunghe. È quindi fondamentale valutare l'adeguatezza dei modelli nel contesto globale.</sample>
    <sample id="570">Ecco ciò che stiamo cercando di fare qui: stiamo cercando di ripetere il pipeline Pp by chiedendo al modello di valutare l'adeguatezza su sequenze sempre più lunghe.</sample>
    <sample id="571">Il contenuto inglese descrive un approccio per simulare lunghe sequenze di dati ripetendo gli stessi dataset e ricreando le frasi scegliendo, ad esempio, frasi accettabili o inaccettabili da quelli. In italiano potrebbe essere tradotto come: 'Quindi, l'approccio consiste nel visitare i dataset stessi ripetutamente e poi nella creazione di frasi selezionando, ad esempio, frasi accettabili o inaccettabili da quei dataset.'</sample>
    <sample id="572">Per esempio, qui abbiamo scelto un paio di caratteristiche tipiche della variabile 'dramatismo' dal set di dati BIM dell'isola adjacente.</sample>
    <sample id="573">Il contenuto inglese descrive un processo di ricreazione di sequenze più lunghe e accettabili con la stessa struttura grammaticale estratte da un documento in formato JSON. In italiano, questo sarebbe: "Ecco cosa facciamo: per creare sequenze più lunghe e accettabili che hanno la stessa struttura grammaticale, estraiamo le frasi grammaticali da un file in formato JSON."</sample>
    <sample id="574">E poi lo aggiungiamo come prefisso sia alla query accettabile che all'query non accettabile.</sample>
    <sample id="575">Il contenuto inglese dice: 'Possiamo fare la stessa cosa selezionando frasi inaccettabili dallo stesso matching e questo potrebbe anche essere utilizzato per testare l'adeguatezza del modello.'</sample>
    <sample id="576">Ecco la traduzione in italiano: 'E possiamo anche fare lo stesso selezionando delle frasi da un insieme di differenza o da un altro set di dati, questo è ciò che chiamiamo scenario di mismatch.'</sample>
    <sample id="577">Il contenuto inglese descrive che 'I dati qui sono ancora provenienti da set di dati pertinenti, ma non dallo stesso set di dati che stiamo valutando'. In italiano potremmo dire qualcosa del tipo: 'I dati qui provengono ancora da un set di dati pertinenti, ma non dal medesimo set di dati su cui stiamo lavorando.'</sample>
    <sample id="578">Infine, possiamo scegliere delle frasi da un dominio del tutto non relazionato come Wikipedia.</sample>
    <sample id="579">Questo ci dirà se i giudizi di adattabilità del modello sono effettivamente influenzati da qualunque contesto.</sample>
    <sample id="580">Il contenuto inglese descrive se il contesto proviene da un sottotipo diverso del set di dati o se è completamente irrilevante rispetto alla frase corrente che stiamo esaminando. In italiano, si traduce come segue: 'Come ad esempio, se il contesto proviene da un sottotipo diverso del set di dati o se è completamente irrilevante rispetto alla frase corrente che stiamo esaminando.'</sample>
    <sample id="581">Il modello fa quanto segue: "In primo luogo, esaminiamo le frasi di Wikipedia che sono completamente inutili rispetto al paio di query corrente e ci accorgiamo che i giudizi MPP sono per lo più robusti per contesti arbitrari."</sample>
    <sample id="582">abbiamo aumentato la lunghezza del contesto verso l'alto fino a 2024 per massimizzare opzione p e gpt2 modelli e abbiamo visto qui nella linea verde, le valutazioni mpp sono relativamente stabili.</sample>
    <sample id="583">Quando scegliamo frasi dallo stesso dataset, cosa accade?</sample>
    <sample id="584">In questo caso, stiamo creando frasi da domini accettabili e inaccettabili dallo stesso set di sintassi BERT.</sample>
    <sample id="585">Il contenuto inglese descrive come i giudizi dell'MPP aumentino o diminuiscano significativamente quando vengono aggiunti prefissi accettabili o prefissi non accettabili. In italiano, questo sarebbe: 'Ecco che vediamo che i giudizi dell'MPP aumentino o diminuiscano significativamente quando si aggiungono prefissi accettabili o prefissi non accettabili.'</sample>
    <sample id="586">Ma quando si confrontano le strutture, ovvero quando si sceglie di prendere le frasi dallo stesso fenomeno nella prova di colpa, Jim...</sample>
    <sample id="587">Il contenuto inglese descrive un aumento o una diminuzione significativo della valutazione MPP per il modello, a seconda del fatto che il prefisso scelto sia accettabile o inaccettabile. In italiano, si dice: ' vediamo un aumento o una diminuzione significativi nella valutazione MPP per il modello, a seconda se il prefisso selezionato è accettabile o inaccettabile.'</sample>
    <sample id="588">Ora, questo è molto grande. Questo effetto aumenta attraverso l'intero link di contesto e probabilmente avrà un impatto sui modelli di lingua più recenti, che hanno una finestra di contesto grande.</sample>
    <sample id="589">Il prefisso 'match' influisce così tanto sulla valutazione del modello linguistico perché determina la natura e l'intensità delle relazioni tra le parole. Ad esempio, se si utilizza il prefisso 'un-', il modello potrebbe riconoscere una parola come 'un sorriso' piuttosto che 'sorriso'. Queste relazioni sono importanti per il modello linguistico perché aiutano a comprendere il significato delle parole in un contesto specifico.</sample>
    <sample id="590">Abbiamo eseguito una serie di analisi dove abbiamo cercato di preservare la struttura del testo d'ingresso aggiungendo rumore all'input. Dopo aver eseguito diverse di queste perturbazioni,</sample>
    <sample id="591">Non riscontriamo che questi rumori effettivamente modifichino il percorso del modello o la sua selezione dei valori di pagamento.</sample>
    <sample id="592">Il contenuto inglese descrive che i modelli sono sensibili alle frasi e alle parole simili. In italiano, si dice: 'Peraltro, abbiamo scoperto che i modelli sono sensibili alle frasi e alle parole simili.'</sample>
    <sample id="593">Il contenuto inglese descrive come, quando si perde di alcuni caratteri nelle frasi accettabili, ci sia un aumento simile nella maggior parte delle perturbazioni, mentre, quando si perde di alcuni caratteri nelle frasi non accettabili, ci sia una diminuzione dei giudizi di MPP in modo simile. In italiano potrebbe essere tradotto come: 'Quando si perdono alcuni caratteri nelle frasi accettabili, vediamo un aumento analogo in tutti i tipi di distorsioni e, quando si perdono alcuni caratteri nelle frasi non accettabili, vediamo una diminuzione dei giudizi di MPP in modo analogo.'</sample>
    <sample id="594">Il contenuto inglese descrive che 'I principali punti chiave del nostro lavoro sono che i modelli di lingua sono sensibili ai caratteristiche sintattiche e semantiche latenti comuni a tutte le frasi.' In italiano, questo significa che i modelli di linguaggio sono in grado di riconoscere e analizzare le strutture grammaticali e le idee semantiche comuni a più di una frase o a un testo intero. Questa capacità è essenziale per la comprensione della lingua naturale e per l'utilizzo delle tecniche di elaborazione del linguaggio avanzate, come la generazione automatica di testo o la traduzione automatica.</sample>
    <sample id="595">L'valutazione MPP attuale, che utilizza input corti e singoli centri di elaborazione, potrebbe non catturare completamente il sapere astratto del modello linguistico nell'ambito dell'intero finestra di contesto.</sample>
    <sample id="596">Per maggiori dettagli sui nostri esperimenti, leggete il nostro articolo. Grazie per l'ascolto.</sample>
    <sample id="597">Un multi-set non ordinato dei token che appaiono nella output.</sample>
    <sample id="598">Codic/script è un termine generico che non riferisce direttamente a un numero di script specifici. Invece, potrebbe essere utilizzato per descrivere una raccolta di script o di codice in generale. Tuttavia, se si assume che 'fifty five thousand' sia un numero di script, allora ci sarebbero cinquantamila script rappresentati in Coscript.</sample>
    <sample id="626">Il miglior metodo di allineamento per DEplain è il method of mass align.</sample>
    <sample id="627">I vantaggi dell'apprendimento scarsamente supervisionato includono la capacità di adattarsi ai dati non etiquetati, l'automatizzazione del processo di etichettatura dei dati e la possibilità di utilizzare una quantità limitata di dati per formare algoritmi di alta qualità.</sample>
    <sample id="628">I documenti in DEplan Web sono stati allineati sia manualmente che automaticamente.</sample>
    <sample id="629">Il set di dati CoNLL++ è stato creato esplorando problemi, sviluppando quindi il dataset CoNLL++.</sample>
    <sample id="630">Ciao a tutti, mi chiamo Yun Zhang dallo University of Pennsylvania. Oggi vi presenterò il mio lavoro sull'interpretazione semantica dei comandi in più lingue naturali e rappresentazioni mentali.</sample>
    <sample id="631">Il contenuto inglese descrive che la sintassi è un compito che consiste nel creare rappresentazioni semantiche di richieste degli utenti, come SQL e lambda calcolo. In italiano si dice: 'La sintassi è il compito di costruire rappresentazioni semantiche delle richieste degli utenti, come ad esempio SQL e lambda calcolo.'</sample>
    <sample id="632">Il traduttore automatico è il compito di tradurre le query in diverse lingue naturali in rappresentazioni significative multiple.</sample>
    <sample id="633">Il contenuto inglese si traduce in italiano come: 'Si tratta di mostrare nella figura l'elaborazione del query in molte lingue naturali utilizzando modelli neurali, SQL, Lambda, funzione Python e così via.'</sample>
    <sample id="634">I modelli di sintassi semantica cross linguaggio esistenti sono proposti e valutati separatamente sui dati set di esempi limitati e delle applicazioni, per esempio.</sample>
    <sample id="635">Il contenuto inglese tradotto in italiano è: 'Ci sono delle lacune di copertura per alcune lingue naturali, la cinese manca.'</sample>
    <sample id="636">I clienti hanno una copertura su alcune minacce specifiche.</sample>
    <sample id="637">Il calcolo del lambda è mancante.</sample>
    <sample id="638">oppure vengono valutati solo su alcuni modelli più recenti, ad esempio ci sono solo un modello per valutare loro.</sample>
    <sample id="639">Quindi, per questo scopo, proponiamo un esempio di dataset uniforme che fornisce una rappresentazione standardizzata di una persona con più lingue naturali e rappresentazioni diverse.</sample>
    <sample id="640">Contiene nove dizionari in vari domini, cinquantasei parti del testo, otto milioni di rappresentazioni e ventiquattro lingue naturali in quindici famiglie di lingua.</sample>
    <sample id="641">Per valutare meglio i benchmark, consideriamo i sei setting per addestramento e valutazione.</sample>
    <sample id="642">Il primo è 'traduci testo', che utilizzeremo per tradurre dal linguaggio di origine al target. Successivamente, utilizzeremo un modello monolingua per addestrare e valutare.</sample>
    <sample id="643">E, per esempio, abbiamo addestrato un modello inglese su query inglese e durante l'induzione, utilizziamo l'API per tradurre la query tedesca in inglese, quindi utilizziamo il modello addestrato per prevedere le risposte SQL.</sample>
    <sample id="644">Ecco la traduzione in italiano: 'Inoltre, testeremo anche il modello monolingue.'</sample>
    <sample id="645">In questo setting, la lingua di partenza è uguale alla lingua di destinazione, ad esempio tedesco a tedesco o inglese a inglese.</sample>
    <sample id="646">Inoltre, abbiamo testato la configurazione di esecuzione monolinguale utilizzando modelli multilingue con solo il 10% dei dati di addestramento.</sample>
    <sample id="647">E connesso un modello monolingua multilingue, che abbiamo addestrato per tutte le lingue.</sample>
    <sample id="648">Per esempio, abbiamo messo insieme le query tedesche, inglesi e cinesi per addestrare un modello multilingua e durante l'infrazione possiamo utilizzare questo modello.</sample>
    <sample id="649">Il contenuto inglese tradotto in italiano è: 'Per tradurre query tedesche o cinesi, ecc.'</sample>
    <sample id="650">E considereremo anche la traduzione crosslinguistica di zero shot e transfer shot. Siamo addestrati su una lingua di partenza e trasferiamo a un'altra lingua.</sample>
    <sample id="651">Durante la formazione, si addestrerà su query in inglese o sulla combinazione di query in inglese e Germania. Questo per addestrare un modello multilingua per prevedere le uscite SQL.</sample>
    <sample id="652">Ecco il contenuto tradotto in italiano: 'E anche abbiamo trovato molti risultati interessanti, quindi riguardo all'analisi di modelli monolingui, li abbiamo valutati su due gruppi di modelli.'</sample>
    <sample id="653">Incluso encoder PDR, che sta per encoder multilingua pretrattato, con decodificatori basati sui punti, come XLR + PDR e BERT + PDR.</sample>
    <sample id="654">E valutiamo anche i modelli di encoder-decoder multilingua, che sono modelli di codifica/decodifica multilingua come M-BART e MT5.</sample>
    <sample id="655">Il contenuto inglese dice: ' Abbiamo trovato che encoder, decoder ottiene i migliori risultati su tutti e nove set di dati.'</sample>
    <sample id="656">E valutiamo m5 e esempi di XLR+PDR multilingua.</sample>
    <sample id="657">Il contenuto inglese dice: 'Senza di esso, l'encoder-decodeur o encoder PDR può essere migliorato con l'addestramento in una miscela di vari linguaggi.'</sample>
    <sample id="658">Il contenuto inglese tradotto in italiano è: 'Ecco cosa abbiamo scoperto: la maggior parte dei principali linguaggi naturali può ottenere un aumento delle prestazioni, tranne l'inglese che ha una diminuzione nelle prestazioni nei sette dataset e solo guadagna in tre dataset.'</sample>
    <sample id="659">Questo è conosciuto come 'curva di multilinguismo'.</sample>
    <sample id="660">Inoltre, abbiamo confrontato il divario di prestazioni tra i diversi linguaggi di programmazione.</sample>
    <sample id="661">In questo grafico, la linea blu indica il trasferimento di celle a zero bit cross linguaggio, la linea arancione indica il trasferimento di celle a zero bit cross linguaggio, mentre la verde indica il setting del modello linguistico.</sample>
    <sample id="662">Hanno scoperto che confrontando la linea verde e l'arancione, abbiamo trovato che il setting con zero shot è significativamente più veloce, mentre confrontando la linea blu e l'arancione, abbiamo trovato che il setting con few shot si accorcia rapidamente.</sample>
    <sample id="663">Inoltre, abbiamo trovato alcuni altri risultati interessanti, ad esempio encoder-decoder, algoritmi di performance, progressi o risultati comparabili per la traduzione automatica del linguaggio naturale inglese che notevolmente migliora le prestazioni di Fused-Transformer su lingue target naturali diverse.</sample>
    <sample id="664">Abbiamo trovato che i modelli di lingua del linguaggio di programmazione multipla, come Codice e Blue, sono ancora adeguati per molte espressioni di sintassi.</sample>
    <sample id="665">Il contenuto inglese descrive un esempio di una riferenza di benchmark unificata per la parola chiave 'cross angle' con più lingue naturali e rappresentazioni. In italiano, questo sarebbe: 'Per riassumere, si tratta di un esempio di una riferenza di benchmark unificata per la parola chiave "cross angle" con diverse lingue naturali e rappresentazioni.'</sample>
    <sample id="666">Conduce una ricerca di riferimento approfondita su tre rappresentanti di tipi di modelli di lingua multilingue e i nostri risultati mostrano molte interessanti scoperte, etc. Ecco la versione italiana della traduzione: 'Conduci una ricerca di riferimento approfondita su tre rappresentanti di tipi di modelli di lingua multilingue e i nostri risultati mostrano molte interessanti scoperte, ecco.'</sample>
    <sample id="667">I lavori esistenti possono essere ampiamente classificati in quattro categorie.</sample>
    <sample id="668">Sì, gli LLM multilingue come Codex o Bloom sono ancora adeguati per il CLSP.</sample>
    <sample id="695">Il metodo induce l'allineamento come parte del training per risolvere l'ambiguità delle permutazioni.</sample>
    <sample id="696">L'equità di un modello NLP a valle viene definita come la sua capacità di non discriminare o ridurre le opportunità per i gruppi under-representati nella raccolta e nell'utilizzo dei dati.</sample>
    <sample id="697">La relatrice si chiama Yannick Lavaur.</sample>
    <sample id="698">Coast of Sinna</sample>
    <sample id="699">La relatrice è Myra.</sample>
    <sample id="700">Il termine 'tropicalismo' indica un atteggiamento o uno stile associato ai paesi tropicali, caratterizzato da elementi come la vivacità, l'attrattiva e la delicatezza.</sample>
    <sample id="701">Gli autori hanno suddiviso i loro gruppi target in base alle parole chiave come cultura, tradizione, orgoglio e esotico, utilizzando queste parole per definire i gruppi in base alla loro relazione all'identità e per distinguerli dalla norma bianca.</sample>
    <sample id="702">Il lavoro utilizza la metrica 'cxmi' per misurare l'utilizzo del contesto.</sample>
    <sample id="703">DrBERT ha 7 GB di natura, mentre ChuBERT ne ha solo 4 GB. Inoltre, ChuBERT contiene 4 GB di sentence taken from clinical notes.</sample>
    <sample id="751">Due.</sample>
    <sample id="752">Il trasferimento iterativo dell'apprendimento è un metodo che consente di utilizzare le informazioni apprese da una fonte per migliorare l'apprendimento in un'altra. In questo caso, si tratta di aggiornare continuamente un modello con nuove annotazioni e dati raccolti durante l'apprendimento attivo.</sample>
    <sample id="753">Il set di dati ha lo scopo di comprendere il linguaggio degli utenti quando desiderano fare una scelta.</sample>
    <sample id="754">Un utente malintenzionato può estrarre i parametri del modello attraverso un EaaS (Environment as a Service) utilizzando tecniche di reverse engineering o di attacco APT (Advanced Persistent Threat). Queste tecniche consentono di accedere al codice sorgente del modello e quindi di comprendere come esso funziona e di estenderne le capacità o di compromettere la sicurezza.</sample>
    <sample id="755">Due.</sample>
    <sample id="756">Due.</sample>
    <sample id="757">Sevastian Santi, Ronan Le Bras, Katerina Rynika e Martin Saftarec sono gli autori dell'articolo.</sample>
    <sample id="758">Il governatore in esempio è Lisa.</sample>
    <sample id="759">I modelli all'avanguardia nei sistemi di dialogo includono l'IA con intelligenza artificiale (AI) e la conversational AI, che utilizzano tecniche di apprendimento automatico per interagire con gli utenti in modo naturale e semplificato.</sample>
    <sample id="760">Perché i grandi modelli linguistici stanno diventando sempre più lunghi e complessi, è essenziale valutare l'accettabilità del modello nell'intera finestra di contesto per garantire che funzioni correttamente.</sample>
    <sample id="761">Sì, secondo quanto afferma il contenuto inglese, la formazione attraverso la modalità multilingua ha causato un calo delle prestazioni rispetto al modello inglese monolingue.</sample>
    <sample id="762">Sì, gli annotatori conoscono l'entità in anticipo.</sample>
    <sample id="763">Non è possibile rispondere alla domanda poiché il testo inglese non fornisce informazioni su quali metriche di MT siano state utilizzate per la valutazione.</sample>
    <sample id="764">Sì, i modelli più grandi generalizzano通常 meglio ai NER.</sample>
    <sample id="765">La posizionalità è importante perché influisce sull'interpretazione del significato delle parole e frase, determinando l'intenzione comunicativa e la relazione tra le parole.</sample>
    <sample id="766">LLMs multilingue come BLOOM sono stati affinati sia con adattatori che con una messa a punto integrale.</sample>
    <sample id="767">Il modello utilizzato per il trasferimento dell'apprendimento è noto come CE here, che si riferisce all'uso di algoritmi di classificazione binaria per l'estensione e la comparazione delle classi di consonanti e dissonanze.</sample>
    <sample id="768">I recenti set di test utilizzati per valutare le capacità di PaLM includono benchmarks su diverse attività, tra cui generazione di testo, risoluzione di problemi matematici e analisi di dati.</sample>
    <sample id="769">Gli autori hanno proposto tre suggerimenti per i proprietari di modelli.</sample>
    <sample id="770">Il metodo proposto mostra un aumento del 30% sulle prestazioni del metodo di riferimento.</sample>
    <sample id="771">Il relatore si chiama Shuheng Zhuo.</sample>
    <sample id="772">Sì, i risultati e il set di dati dell'articolo possono essere utilizzati come parametri di riferimento per il problema della semplificazione del testo automatico in futuro.</sample>
    <sample id="773">L'articolo utilizza la parola 'smaller models' per descrivere i modelli che vengono utilizzati, ma non specifica esattamente quante siano. Tuttavia, si dice che i modelli più piccoli sono in grado di superare i modelli più grandi quando addestrati su dati appropriati.</sample>
    <sample id="774">Il modello di base utilizzato per analizzare l'ottimizzazione delle istruzioni multimodali è OFA (un modello unificato di rappresentazione multimodale).</sample>
    <sample id="833">I colleghi dell'autore sono di Google Translate.</sample>
    <sample id="834">Waseem Badar è un candidato al dottorato di ricerca in scienze informatiche presso l'università di Stony Brook.</sample>
    <sample id="835">L'articolo ha analizzato le coppie linguistiche 'state-of-the-art' e 'NLP metrics'.</sample>
    <sample id="836">Il relatore si chiama Shangbin Ph.D. student all'Università di Washington.</sample>
    <sample id="837">I modelli di lunga e normale base sono stati studiati.</sample>
    <sample id="838">Tutte le 62 attività utilizzate in MultiInstruct vengono utilizzate per scopi di addestramento e test.</sample>
    <sample id="839">Un solo autore, Regina Stodden.</sample>
    <sample id="840">I ricercatori hanno eseguito i test su quattro set di dati: AG news, MNIST, SStoXX e Yahoo Finance.</sample>
    <sample id="876">NACHOS è un acronimo che sta per 'Notre Anatomie, Chimie et Pathologie Orale, Santé' e rappresenta una banca dati medica francese online.</sample>
    <sample id="877">Il relatore si chiama Ali Bilad.</sample>
    <sample id="878">La strategia del prompting ha un'influenza significativa sui risultati delle traduzioni LLM.</sample>
    <sample id="879">I coautori dell'articolo sono Patrick Frennance, MEY Liu, Andrej F. Martínez e Graham Neubig.</sample>
    <sample id="880">Istruzioni non fornite. Il testo inglese non specifica le 5 istruzioni scritte dagli esperti.</sample>
    <sample id="881">Gli autori propongono un task di risoluzione di riferimenti costruita per valutare la capacità di utilizzare conoscenza disponibile in diverse fonti.</sample>
    <sample id="882">Ciao a tutti, mi chiamo Ali Bilal e vi darò un breve sguardo al paper "Pruning patterns for machine translation", che esplora le strategie e i risultati di prestazione. Questo è un lavoro condiviso con i miei colleghi di Google Translate.</sample>
    <sample id="883">Il modello di lingua PAMM è un modello di grandi dimensioni con 540 miliardi di parametri che è stato presentato l'anno scorso ed è basato su una grande raccolta di testi che comprende 1,8 miliardi di token.</sample>
    <sample id="884">Nel campo della fabbricazione, è stato raggiunto lo stato dell'arte in centinaia di attività LSP.</sample>
    <sample id="885">In questo lavoro, presentiamo la prima ricerca sistematica sul promettente utilizzo di modelli di grande lingua per la traduzione assistita.</sample>
    <sample id="886">Abbiamo valutato la compatibilità della trasmissione di tali modelli utilizzando le migliori pratiche della comunità AMT, che include l'utilizzo dei set di test più recenti per evitare sovrapposizioni tra i dati di test e i dati di addestramento del modello di lingua.</sample>
    <sample id="887">Stiamo parlando di due stati di sistema, quindi del sistema con il rendimento migliore tra quelli valutati con la valutazione WMT.</sample>
    <sample id="888">L'utilizzo di misure di stato dell'arte e URM è descritto, nonché risultati di valutazione umana eseguiti da esperti. Infine, sono fornite alcune raccomandazioni per le strategie di selezione del prometeo.</sample>
    <sample id="889">Il promettente ha un grande impatto sulla prestazione delle traduzioni LLM, come possiamo vedere in un semplice esperimento dove utilizziamo uno short prompt e forniamo due promesse diverse per la frase corrispondente.</sample>
    <sample id="890">La maggior parte delle frasi, su un totale di mille, sono state scritte con una differenza di più di un punto di blu.</sample>
    <sample id="891">Questo può arrivare fino a quattronta punti di vantaggio in casi estremi, quindi è importante selezionare una buona strategia di promozione.</sample>
    <sample id="892">I nostri esperimenti hanno portato alla creazione di una strategia di promozione a cinque tappe che consiste nel marcare ogni frase fornita al sistema con la lingua del testo corrispondente.</sample>
    <sample id="893">In questo esempio, dove eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche sono contrassegnate con un simbolo di apertura maiuscola 'G', mentre le traduzioni in inglese sono contrassegnate con un simbolo di apertura minuscola 'g'.</sample>
    <sample id="894">Abbiamo visto che la forma effettiva del promemoria non ha un grande impatto nel caso di promemoria corta.</sample>
    <sample id="895">È critico per la promozione a zero e a una sola shot, ma quando andiamo a cinque shot, non c'è praticamente differenza nella forma effettiva della promozione.</sample>
    <sample id="896">I contenuti sono i seguenti: 'Sono gli esempi che portano la maggior parte del peso.'</sample>
    <sample id="897">Il resumen delle nostre risultati sperimentali è che la qualità dell'esempio è più importante della somiglianza al testo di partenza.</sample>
    <sample id="898">È importante selezionare gli esempi dalle traduzioni di alta qualità. In particolare, confrontiamo i promemoria di selezione dal set di addestramento delle valutazioni WMT o dei dati di riferimento.</sample>
    <sample id="899">Il contenuto inglese descrive che i dati di addestramento sono stati creati con una maggiore qualità rispetto ai dati di testo, ottenendo così risultati migliori nell'utilizzo dei dati di addestramento. In italiano si traduce come segue: 'I dati di addestramento sono stati creati con una maggior qualità rispetto ai dati di testo, ottenendo così risultati migliori nell'utilizzo dei dati di addestramento.'</sample>
    <sample id="900">I traduttori automatici sono spesso più lenti che i traduttori umani, ma forniscono risultati molto buoni e sono utili per alcune applicazioni. Inoltre, alcuni traduttori automatici sono progettati per essere specializzati in determinate lingue o campi di conoscenza, il che significa che possono fornire prestazioni ancora migliori rispetto ai traduttori generici. Tuttavia, nonostante le loro limitazioni, i traduttori automatici stanno diventando sempre più sofisticati e utili, e rappresentano una parte importante della tecnologia di traduzione contemporanea.</sample>
    <sample id="901">I suggerimenti che abbiamo ottenuto dall'elaborazione UMI utilizzando il framework M5 sono che la velocità di esecuzione del algoritmo PAM è comparabile con quella degli altri sistemi di archiviazione, ma la principale differenza deriva dalla precisione.</sample>
    <sample id="902">In particolare, gli errori più comuni sono gli errori di ommissione.</sample>
    <sample id="903">In sintesi, seeme che Pan scelga di produrre una traduzione migliore del testo originale, talvolta omettendo parti della frase d'origine che non sono state tradotte nella traduzione.</sample>
    <sample id="904">Il contenuto inglese tradotto in italiano è: 'Tuttavia, la categoria "outward style" per Pan è più lenta rispetto allo stato dei sistemi di aria, che è un segnale supplementare.'</sample>
    <sample id="905">Il modello fornisce un output veramente fluente, ma con alcuni problemi di accuratezza.</sample>
    <sample id="906">Ecco la fine di questa breve panoramica. Per maggiori dettagli, vi prego di venire alla mia presentazione completa del paper. Grazie mille.</sample>
    <sample id="907">In questo video vorrei presentare il nostro lavoro più recente, intitolato 'Migliorare di più di quanto pensi: una visione critica della fornitura settimanale'.</sample>
    <sample id="908">Questo è un lavoro congiunto con Xiao Yunshen, Maios Musbach, Jia Stephen e Detlef Klacko.</sample>
    <sample id="909">Sarei felice di iniziare con una breve introduzione alla supervisione settimanale e alla durata della supervisione settimanale.</sample>
    <sample id="910">Nel controllo wick, non si etichetta manualmente i dati. Invece, si etichetta i dati utilizzando fonti di etichettatura wick semplici, come regole di tipo casuale, basi di conoscenza o sorgenti di crowd-sourcing di bassa qualità, come illustrato nella figura a destra.</sample>
    <sample id="911">I segni di vettura sono molto più economici rispetto ai segni di punte, ma sono anche rumorosi, il che significa che un certo numero di segni di annotazione è incorretto.</sample>
    <sample id="912">Se addestriamo direttamente i network neurali sui dati di etichetta settimanale, i network neurali tendono a memorizzare il rumore della label e non generano.</sample>
    <sample id="913">Nel superamento della supervisione settimanale, gli algoritmi di addestramento vengono proposti per addestrare i modelli neurali più recenti con rumori di etichetta in modo da generare ancora risultati validi.</sample>
    <sample id="914">Nelle ultime ricerche su WSL, WSL sta per apprendimento supervisionato settimanale. Una pretesa comune è che le persone utilizzino solo tre modelli sulle date di lavoro settimanali e raggiungano un alto rendimento sui test puliti.</sample>
    <sample id="915">Il contenuto inglese tradotto in italiano è: 'Tecnicamente, questa affermazione non è sbagliata, ma c'è un catch.'</sample>
    <sample id="916">Il contenuto inglese tradotto in italiano è: 'Ciò significa che le persone suppongono che ci sia un set di validazione pulito supplementare disponibile per la selezione del modello.'</sample>
    <sample id="917">Il contenuto inglese tradotto in italiano è: 'Siamo fermi su questo problema di impostazione, ma ciò implica che sono necessarie annotazioni manuali aggiuntive nella formazione superiore settimanale, ma come un elefante nella stanza, questa necessità viene spesso trascurata.'</sample>
    <sample id="918">Il contenuto inglese si traduce in italiano come segue: 'L'approccio menzionato sopra prevede di porre tre domande di ricerca: prima, è necessario validare i dati puliti per un SLW o possiamo forse utilizzare un set di validazione noioso invece?'</sample>
    <sample id="919">In secondo luogo, se si richiede o se è obbligatorio avere dati puliti per far funzionare WSL, quanti campioni puliti abbiamo bisogno? Infine, dovremmo utilizzare solo i campioni puliti per la validazione o ci sono metodi migliori per utilizzarli?</sample>
    <sample id="920">Abbiamo affrontato queste domande di ricerca nella nostra opera e i nostri risultati sono i seguenti.</sample>
    <sample id="921">In primo luogo, abbiamo scoperto che gli ultimi metodi WSL effettivamente richiedono campioni di larghezza di banda puliti per funzionare correttamente.</sample>
    <sample id="922">Altrimenti ci sarebbe una grande diminuzione delle prestazioni nella figura qui sopra se non ci sono campioni di validazione puliti. In questo caso, i modelli addestrati non possono generare risultati al di là dei limiti originali della settimana.</sample>
    <sample id="923">Il contenuto inglese significa: 'Che l'addestramento del delfino è inutile.'</sample>
    <sample id="924">Questo indica che gli approcci WSL effettivamente richiedono dati etichettati correttamente per funzionare correttamente, e i costi di annotazione per ottenere campioni di validazione puliti non dovrebbero essere trascurati.</sample>
    <sample id="925">Il nostro secondo risultato è che aumentando il numero di campioni di validazione puliti aiuterà gli approcci WSL a ottenere prestazioni migliori, come mostrato nella figura a destra.</sample>
    <sample id="926">In generale, basta utilizzare 20 campioni per classe per ottenere prestazioni elevate.</sample>
    <sample id="927">Ma questo non è l'ultimo della storia perché se decidiamo di accedere a campioni puliti, allora anche la formazione diretta su di essi otterà prestazioni ancora migliori.</sample>
    <sample id="928">Il grafico rosso mostra la differenza di prestazioni tra approcci di ottimizzazione del punteggio, che vengono applicati direttamente sui dati puliti e gli approcci WSL, che utilizzano i dati puliti solo per la validazione.</sample>
    <sample id="929">Come possiamo vedere, se abbiamo dieci esempi per classe, la rettifica diretta comincia a superare gli approcci WSL.</sample>
    <sample id="930">In sintesi, l'aumento delle prestazioni dichiarato in precedenti approcci WSL può essere facilmente raggiunto consentendo la continua regolazione fine dei campioni di validazione puliti.</sample>
    <sample id="931">Come possiamo vedere dalle figure, il modello Valina chiamato FTW inizialmente esegue metodi WSL più complessi come la cosiddetta.</sample>
    <sample id="932">Se desideriamo continuare a sottoporre i campioni puliti a un'ulteriore messa a punto, allora FTTW si comporta altrettanto bene degli altri metodi.</sample>
    <sample id="933">In pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio disco.</sample>
    <sample id="934">Hanno mostrato che gli ultimi approcci WSL richiedono campioni manualmente annotati puliti per funzionare correttamente, il loro guadagno di prestazioni e la praticità sono pesantemente overestimati.</sample>
    <sample id="935">I nostri consigli concreti per le prossime ore di lavoro sono i seguenti:

  1. Assicurati di avere tutto il materiale di cui hai bisogno prima di iniziare.
  2. Stabilisci un obiettivo chiaro e realistico per la tua giornata di lavoro.
  3. Prenditi pause regolari per rilassarti e ricaricare le batterie.
  4. Evita di multitasking e concentri-te su una attività alla volta.
  5. Non esitare a chiedere aiuto se hai bisogno di aiuto o supporto durante il giorno.

Speriamo che questi consigli ti siano utili nella tua prossima sessione di lavoro!</sample>
    <sample id="936">Primo, segnalare i criteri di selezione del modello, ad esempio segnalare se la selezione del modello è stata effettuata sui campioni di validazione puliti.</sample>
    <sample id="937">In secondo luogo, le approssimazioni WSL dovrebbero essere confrontate con basi di conoscenza brevi e semplici, che funzionino entrambi sui campioni di testo. In terzo luogo, la regolazione continua è una base forte e semplice che dovrebbe essere considerata per futuri lavori su WSL.</sample>
    <sample id="938">Infine, abbiamo il codice open source. Potete trovarlo utilizzando il codice QR sulla slide qui presente. Siete pregati di esplorarlo. Grazie e buon divertimento alla conferenza.</sample>
    <sample id="939">I metodi di valutazione comuni includono l'utilizzo dell'valutazione umana, come chiedere ai giudici umani di selezionare quale delle due conversazioni sia migliore o di valutare le conversazioni su una scala liquorata.</sample>
    <sample id="940">Quattro.</sample>
    <sample id="941">Per rispondere all'esempio con Servin e Kea, è necessario sapere che 'servin' è un giudice e che 'kea' è un banchiere. Inoltre, si ha bisogno di conoscere il contesto in cui i due personaggi si incontrarono al parco dopo una lunga giornata di lavoro.</sample>
    <sample id="942">Sì, il codice è disponibile su Github.</sample>
    <sample id="943">Sì, gli annotatori per NLPositionality sono stati valutati per essere bilanciati rispetto a diversi gruppi demografici, tra cui il paese d'origine, il genere e l'età. Tuttavia, è importante notare che la maggior parte degli studi su NLPositionality sono stati condotti su campioni di testo inglese, pertanto potrebbero esserci differenze nella validità della valutazione dei dati per altri contesti linguistici o culture.</sample>
    <sample id="944">Le frasi sono state perturbate aggiungendo rumori al input per mantenere la struttura rilevante, senza alterare significativamente il significato della frase originale.</sample>
    <sample id="945">Valutazione dimensionale significa valutare un sistema o processo su più parametri o aspetti per comprendere le sue caratteristiche e prestazioni in modo completo. In altre parole, si analizzano più elementi insieme invece che separatamente.</sample>
    <sample id="946">L'autore dell'articolo si chiama Jing Wei Yi e fa parte dell'Università di Scienza e Tecnologia della Cina.</sample>
    <sample id="947">La forma del prompting non è importante per il short prompting (zero e uno shot), mentre diventa cruciale per il long prompting (5 shot).</sample>
    <sample id="978">I modelli di dialogo conversazionale.</sample>
    <sample id="979">One.</sample>
    <sample id="980">Un buon pianificatore dovrebbe essere in grado di scrivere script ragionevoli e flessibili alle restrizioni.</sample>
    <sample id="981">Due.</sample>
    <sample id="982">Wasudha</sample>
    <sample id="983">Adam Skurkowski è l'autore principale dell'articolo.</sample>
    <sample id="1021">I principali errori commessi da PaLM sono gli errori di ommissione.</sample>
    <sample id="1022">Ciao, sono James Finch e sono Sarah Finch. Oggi vi parleremo di ABC-Eval, un nuovo approccio dimensionale all'valutazione dell'intelligenza conversazionale.</sample>
    <sample id="1023">Il lavoro è stato fatto dal laboratorio Emory NLP, guidato dal professor Gino Choy all'Università di Emory e in collaborazione con Amazon Alexa AI.</sample>
    <sample id="1024">Sostituisci l'audio 1 con il seguente testo in italiano: 'Però supponiamo che hai appena sviluppato un modello di dialogo e vuoi vedere quanto bene si confronta con lo stato attuale dell'arte.'</sample>
    <sample id="1025">La pratica comune è quella di utilizzare valutazione umana, ad esempio chiedendo ai giudici umani di scegliere quale delle due conversazioni sia migliore o di valutare le conversazioni data una scala di gradimento.</sample>
    <sample id="1026">Questi approcci funzionano bene per fornire valutazioni holistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto potresti voler valutare più dimensioni della qualità del chat per capire le forze e le debolezze del modello su un livello più finegrato.</sample>
    <sample id="1027">Un approccio consiste semplicemente nel chiedere ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la rilevanza delle risposte dei modelli, utilizzando metodi esistenti di scala comparativa o Likert.</sample>
    <sample id="1028">Il contenuto inglese tradotto in italiano è: 'Tuttavia, crediamo ci sia una strategia più precisa e affidabile per la valutazione del dialogo dimensionale.'</sample>
    <sample id="1029">Il loro approccio tenta di ridurre la soggettività dell'valutazione umana esplicitamente annotando se ogni risposta del modello esprime comportamenti specifici, come rispondere con informazioni non pertinenti o contraddittorie se stessi.</sample>
    <sample id="1030">Questo approccio si chiama 'annotazione dei comportamenti nel chat', o semplicemente ABC-Eval in breve. Abbiamo sviluppato questo metodo per coprire in modo esaustivo i comportamenti del modello di chat che sono stati suggeriti di influire sulla qualità del chat nella letteratura recente.</sample>
    <sample id="1031">L'elaborazione ABC è capace di misurare le rate a cui i modelli di chat commettono errori tematici vari.</sample>
    <sample id="1032">Per esempio, ABC-Eval misura il numero di turni in cui un modello di chat ignora il suo partner o dice qualcosa di non relevant.</sample>
    <sample id="1033">Se autocontraddice se stesso o il suo partner, allucina fatti incorrecti o viola la conoscenza del senso comune e quando il modello ha successo o fallisce a mostrare empatia.</sample>
    <sample id="1034">Il contenuto inglese descrive un processo di valutazione dei modelli di chatbot di stato avanzato. In italiano, si dice: 'abbiamo selezionato quattro modelli di chatbot di stato avanzati e li abbiamo valutati su cento conversazioni umane per modello, utilizzando l'evaluazione ABCDE.'</sample>
    <sample id="1035">Per una comparazione, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni di lickerit sul livello del tono, valutazioni di lickerit sul livello del dialogo e confronti di livello di dialogo a due a due.</sample>
    <sample id="1036">Per ogni metodo esistente, abbiamo raccolto valutazioni su otto aspetti più comuni misurati del dialogo, poiché questo è la pratica standard per valutare i modelli di chat su diverse dimensioni.</sample>
    <sample id="1037">Dalle nostre analisi di questi risultati di valutazione, abbiamo scoperto che i comportamenti etichettati ABC sono in generale più affidabili rispetto alle etichette raccolte tramite metodi esistenti, come misurato dall' accordo interannotatore su centinaia di conversazioni doppie etichettate.</sample>
    <sample id="1038">Inoltre, i label ABC valgono di più per la qualità della conversazione globale rispetto alle misurazioni prodotte dalle methodi esistenti, come mostrato dall'analisi della regresione lineare semplice.</sample>
    <sample id="1039">Per esempio, puoi vedere come la misurazione della proporzione di turni con contraddizioni self e partner spiega il 5% e l'10% rispettivamente della qualità delle conversazioni, mentre le valutazioni medie della consistenza del liquore spiegano solo il 4% o meno.</sample>
    <sample id="1040">Infine, abbiamo verificato se ogni misura di valutazione cattura un aspetto unico della qualità del chat utilizzando una regressione lineare step-by-step.</sample>
    <sample id="1041">Il contenuto inglese descrive come l'insieme delle metriche ABCDE valutative spiega oltre il 25% della qualità della conversazione, e come l'eliminazione di una metrica alla volta porta a perdere una quantità decente di informazioni sulla qualità. In italiano si dice: 'Si può vedere come la combinazione di tutte le metriche ABCDE valutative spiega oltre il 25% della qualità della conversazione e come l'eliminazione di una metrica alla volta porta a perdere una quantità decente di informazioni sulla qualità.'</sample>
    <sample id="1042">Sul lato opposto, la combinazione di tutti i metodi di misurazione del livello di turnover spiega molto meno sulla qualità e pochi di questi metodi portano informazioni uniche.</sample>
    <sample id="1043">Queste misurazioni ABC evl affidabili, informative e distinte ci permettono di valutare l'AI conversazionale con una risoluzione più alta di quanto siano state in grado di ottenere i metodi precedenti.</sample>
    <sample id="1044">Nel risultato del nostro esperimento, si può vedere che alcuni ostacoli restano e sono stati quantificati esattamente. Ad esempio, i bot che abbiamo testato hanno violazioni di senso comune nella loro risposta al大约20% delle richieste.</sample>
    <sample id="1045">I rispondenti producono informazioni non pertinenti in circa il fifteen percento delle loro risposte e si contraddicono a se stessi o al loro partner intorno al dieci percento del tempo.</sample>
    <sample id="1046">Con il rapido miglioramento nel campo, molti di questi tassi d'errore potrebbero vedere una diminuzione nei nuovi modelli rilasciati dal momento della nostra valutazione. Tuttavia, questo è ancora più motivo per cercare metodi di valutazione affidabili e precisi per confrontare i modelli.</sample>
    <sample id="1047">Speriamo che ABC eVal possa essere sfruttato da altri nel campo come un significativo passo nella direzione giusta, e ci aspettiamo di vedere come l'AI conversazionale evolverà nei prossimi mesi e anni. Grazie per averci guardato.</sample>
    <sample id="1048">L'articolo è stato realizzato dal gruppo Emory NLP Lab, guidato dal professor Gino Choi all'Università di Emory e in collaborazione con Amazon Alexa AI.</sample>
    <sample id="1049">CFT sta per 'Clean, Filtered, and Thesaurus-annotated samples'.</sample>
    <sample id="1050">Sette.</sample>
    <sample id="1051">Ciao, mi chiamo Kai Ouyang e presenterò il nostro lavoro intitolato 'Quando richiede la traduzione del contesto? Una esplorazione multilingue basata sui dati'. Questo lavoro è stato realizzato in collaborazione con Patrick Frenneaux, MEY Liu, Andrej F. Martínez e Graham Newman.</sample>
    <sample id="1052">Molte traduzioni dipendono dal contesto, ad esempio, come tradurremmo 'molto' in questa frase?</sample>
    <sample id="1053">Se il precedente periodo era 'Cose potrebbero iniziare a diventare pericolose se i ministri lo trovassero', allora 'moore' si riferisce a un spia. Se invece il periodo precedente era 'Cosa potrebbe essere stato grave, dottore?', allora 'moore' si riferisce a una targa di nascita.</sample>
    <sample id="1054">Il contenuto inglese dice: 'Quindi, a seconda dei contesti, il significato della parola cambia e quindi anche la sua traduzione cambia.'</sample>
    <sample id="1055">Il contenuto inglese dice: 'Tuttavia, valutare quanto bene i modelli possono trapiantare casi come questo è abbastanza difficile. In primo luogo, solo una piccola parte delle traduzioni dipende dal contesto, il che rende le misure di livello del testo, come Blue, incapaci di catturare queste traduzioni.'</sample>
    <sample id="1056">Alcune persone hanno suggerito una valutazione mirata sui tradimenti dipendenti dai contesti, ma questi risorse solo supportano tipologie limitate di tradimenti dipendenti dai contesti e limitati set di lingue, poiché di solito si affidano alla conoscenza del dominio e alla creazione umana.</sample>
    <sample id="1057">In questo lavoro, abbiamo cercato di rispondere a queste due domande: prima, quando richiede una traduzione contesto e seconda, come bene i modelli gestiscono questi casi?</sample>
    <sample id="1058">Per rispondere alla prima domanda, abbiamo iniziato misurando quanto dipende la traduzione dal contesto.</sample>
    <sample id="1059">Nel lavoro precedente, abbiamo introdotto la misura di CxMI come indice dell'utilizzo del contesto da parte dei modelli di traduzione automatica e questo viene fatto misurando quanta informazione forniscia il contesto C sulla destinazione Y, dato il testo sorgente X.</sample>
    <sample id="1060">Il contenuto inglese descrive come si può pensare a CxMi come all'informazione ottenuta fornendo contesto al modello. In italiano, questo sarebbe: 'Si può pensare a CxMi come all'informazione ottenuta fornendo contesto al modello.'</sample>
    <sample id="1061">In questo lavoro, estendiamo cxmi all'oggetto puntato cxmi, che può misurare l'utilizzo di contesto al livello della frase o del单词. possiamo pensare ai词汇i che hanno un p6 alto come quelli che richiedono un contesto per la traduzione.</sample>
    <sample id="1062">Ora analizziamo parole con alto esponente della funzione di probabilità inversa (PSI) per cercare pattern tra queste parole.</sample>
    <sample id="1063">Ecco la traduzione in italiano del contenuto inglese: 'E noi eseguiamo analisi sui trascritti di lezioni TED che sono stati tradotti in quattordici lingue diverse.'</sample>
    <sample id="1064">Il contenuto inglese descrive un'analisi condotta su tre livelli diversi: si esaminano i tag di testo del discorso che hanno un significato medio elevato. In italiano, la traduzione sarebbe: 'L'analisi è stata condotta su tre livelli diversi: si sono esaminati i tag di testo del discorso che hanno un significato medio elevato.'</sample>
    <sample id="1065">Questo ci consente di trovare, ad esempio, pronomi dubbi arabi che hanno la radice 'p-hy-p'. E questo può essere spiegato perché l'inglese non ha pronomi dubbi, quindi è necessario un contesto per determinare se un pronome è doppio durante la traduzione in arabo.</sample>
    <sample id="1066">Nel contenuto inglese, si dice che 'similmente, troviamo che alcuni linguaggi richiedono anche contesti quando vogliamo scegliere l'forma corretta del verbo appropriato.' In italiano, questo significa: 'Similmente, scopriamo che alcuni linguaggi richiedono anche contesti quando vogliamo scegliere la forma corretta del verbo adatto.'</sample>
    <sample id="1067">Questo aiuta a identificare casi come quello qui, dove in cinese è necessario utilizzare contesti corretti per tradurre i nomi propri per assicurarsi di utilizzare la stessa traduzione all'interno del documento.</sample>
    <sample id="1068">In modo simile, abbiamo trovato che il contesto è supportato per essere trasmesso nella corretta formattazione.</sample>
    <sample id="1069">E infine, esaminiamo diversi token individuali che hanno un alto valore di Pmi e questo ci consente di identificare fenomeni che non possono essere really capturati dal termine stesso, ma che sono espressi nella struttura del dizionario. Ad esempio, risoluzione degli occhi.</sample>
    <sample id="1070">Ora utilizziamo i nostri risultati dell'analisi per progettare un punto di riferimento per la traduzione documentale automatica.</sample>
    <sample id="1071">Per ogni uno dei cinque fenomeni di discorso identificati, abbiamo creato tagliere per identificare automaticamente le parole che appartengono al fenomeno e chiamiamo il nostro taglio il multi lingua discorsso consapevole o Muda taglio.</sample>
    <sample id="1072">Il contenuto inglese si traduce in italiano come: 'Si può notare anche che i diversi linguaggi hanno proporzioni diverse di questi fenomeni.'</sample>
    <sample id="1073">In seguito, utilizziamo l'attaccante Muda applicando il tatuaggio sul corpus paralelo che vogliamo utilizzare per la valutazione e applichiamo le nostre metriche di traduzione scelte sui esempi di contesto dipendenti che l'attaccante Muda ha identificato.</sample>
    <sample id="1074">E infine, utilizziamo il nostro benchmark insieme ad altre metriche per valutare diversi modelli su livello di documento nella traduzione automatica.</sample>
    <sample id="1075">In primo luogo, quando utilizziamo le misure di livello del corpo, scopriamo che i modelli di analisi esplorativa hanno il miglior rendimento per il blu.</sample>
    <sample id="1076">Ma se utilizziamo Comet, i modelli di riferimento si comportano meglio. E se misuriamo la parola F, allora i modelli con e senza contesti hanno prestazioni comparabili.</sample>
    <sample id="1077">Questo dimostra di nuovo che è difficile determinare il miglior sistema di traduzione documentale utilizzando solo misure di livello del testo.</sample>
    <sample id="1078">Ora utilizziamo il benchmark del MoDa per valutare i modelli e scopriamo che i modelli con contesto sono significativamente più precisi dei modelli che non utilizzano contesto per determinate proprietà del discorso, come la formalezza e la coesione lessicale.</sample>
    <sample id="1079">Questi modelli non sono molto meglio di quelli che non utilizzano contesto su altre proprietà, come le ellissi, i pronomi e la forma verbale. Questo suggerisce che sarebbe necessario vedere più progressi nella traduzione a livello documentale.</sample>
    <sample id="1080">Il contenuto inglese descrive una comparazione tra diversi sistemi commerciali e indica che DeepL è solitamente più preciso di Google Translate per la traduzione di documenti. In italiano, questo sarebbe: 'Abbiamo anche confrontato differenti sistemi commerciali e il nostro benchmark mostra che DeepL è solitamente più preciso di Google Translate per la traduzione di documenti.'</sample>
    <sample id="1081">Il contenuto inglese descrive un'analisi basata sui dati su quattordici paresi di lingue per identificare i contesti in cui sono necessarie traduzioni. In italiano, questo sarebbe: 'Per riassumere, abbiamo eseguito un'analisi basata sui dati su quattordici paresi di lingue per identificare i momenti in cui sono necessarie le traduzioni.'</sample>
    <sample id="1082">Il contenuto inglese descrive come vengono utilizzate le raffinatezze per costruire un punto di riferimento per la traduzione del documento automatica, che può aiutare a identificare quali sono i modelli di fenomeno descrittivi che possono gestire bene o meno e quali sistemi di traduzione sono buoni per la traduzione del documento. In italiano, questo sarebbe: 'Allora usiamo le nostre raffinatezze per costruire un punto di riferimento per la traduzione del documento automatica, che ci può aiutare a identificare quali sono i modelli di fenomeno descrittivi che possono gestire bene o meno e quali sistemi di traduzione sono buoni per la traduzione del documento.'</sample>
    <sample id="1083">Grazie per la tua attenzione, ci vediamo domani.</sample>
    <sample id="1084">L'oratore si chiama Jiaxin Zhang.</sample>
    <sample id="1121">Se il nuovo metodo ha un nome, lo indichi; altrimenti dica che ne è privo.</sample>
    <sample id="1122">Il metodo descritto dall'autore per identificare le parole che distinguono i gruppi contrassegnati dalle parole contrassegnate si chiama "mark words".</sample>
    <sample id="1123">L'autore dell'articolo si chiama Shangbin e ha un PhD all'università di Washington.</sample>
    <sample id="1124">Il nome della prima struttura di dipendenza simmetrica menzionata è il Praga approach.</sample>
    <sample id="1125">Il relatore si chiama James Finch e la relatrice Sarah Finch.</sample>
    <sample id="1126">Quattro.</sample>
    <sample id="1127">I dati che possono essere utilizzati includono testi scritti da persone, dati di conversazione, o altri tipi di input linguistico.</sample>
    <sample id="1161">WLS, SL, CV, GL, and ML</sample>
    <sample id="1162">Il modello viene valutato su 11 attività biomediche e cliniche di screening.</sample>
    <sample id="1226">I dati su cui viene inizialmente addestrato CamemBERT sono i 4 GB di subset dei dati di 'nachos'.</sample>
    <sample id="1227">Adam Skurkowski</sample>
    <sample id="1228">I risultati dell'esperimento hanno confermato l'ipotesi secondo cui la principale causa della diminuzione delle prestazioni è la deriva temporale.</sample>
    <sample id="1269">I token non sono ordinati nella sequenza corretta dopo il primo passaggio, quindi vengono utilizzati un altro modello per prevedere la permutation corretta da applicare ai token per ottenere la sequenza desiderata.</sample>
    <sample id="1270">Gli autori suggeriscono un aumento della trasparenza perché ci sono dubbi sull'eccessivo valore assoluto delle positive stereotipi e su altri possibili metodi antistereotipizzanti che possono generare pattern pregiudiziali.</sample>
    <sample id="1271">Gli input inaccettabili di coppia minima sono una frase accettabile e una frase non grammaticale.</sample>
    <sample id="1272">Gli autori hanno utilizzato la massa e l'etichetta del percorso come metriche di valutazione.</sample>
    <sample id="1273">L'interannotatore agreement è stato utilizzato per misurare l'accordo tra gli annotatori.</sample>
    <sample id="1274">Il dominio selezionato è Wikipedia.</sample>
    <sample id="1275">I due autori dell'articolo sono membri del team di sviluppo di CoreNLP.</sample>
    <sample id="1276">MultiInstruct si distingue dagli altri parametri di riferimento perché si concentra sull'ottimizzazione dell'instruzione per migliorare la generazione su compiti multilivello, mentre gli studi precedenti hanno concentrato principalmente sulla prestazione di zero shot su compiti linguistici.</sample>
    <sample id="1277">Due.</sample>
    <sample id="1278">La coordinazione binaria si riferisce all'aspetto binario di una coordinazione, dove due sostanze o atomi sono legati da un legame chimico.</sample>
    <sample id="1279">The prompts were used for an average of 30 minutes.</sample>
    <sample id="1280">I risultati suggeriscono che un modello T5 più piccolo può generare qualità di hair equal o superiore ai grandi modelli, se addestrato correttamente sui dati adatti.</sample>
    <sample id="1281">Ciao, sono Yannick Lavaur e presenterò i nostri lavori sul modello pre-trainato RobustBert per domini biomedici e clinici.</sample>
    <sample id="1282">In questa presentazione, parleremo prima di modellizzazione del linguaggio nella sanità, quindi presenteremo il contributo principale del nostro articolo.</sample>
    <sample id="1283">Introduciamo il primo modello biomedico in francese chiamato Docteur Bert, che si basa su Roberta e sull'elenco dei pazienti di Natsos, che è un insieme di dati medici raccolti dal web.</sample>
    <sample id="1284">Inoltre, introduciamo una comparazione di modelli con impostazioni predefinite multiple e fonti di dati, quindi presentiamo i nostri risultati per 11 task di biologia medica e clinica in streaming.</sample>
    <sample id="1285">In sintesi, concludiamo parlando degli esperimenti e fornendo maggiori dettagli su come accedere ai modelli.</sample>
    <sample id="1286">Dall'uscita nel 2018, BERT è diventato uno dei metodi più efficaci per risolvere compiti di elaborazione del linguaggio naturale e offre un enorme vantaggio rispetto ai metodi storici statici e connessivi, come Word2Vec, GloVe o NoWord.</sample>
    <sample id="1287">Da allora, questo modello è stato adattato a molti altri linguaggi, come il francese con Camembert e altri domini come biomedico con Permit e BioBERT, nonché sul clinico con ClinicalBERT, ma principalmente in inglese.</sample>
    <sample id="1288">I modelli specializzati per altre lingue sono scarsi e spesso basati su addestramento continuo a causa della mancanza di dati interni.</sample>
    <sample id="1289">Il contenuto inglese dice: 'Tuttavia, non esiste alcun modello aperto per i farmaci biologici fino ad ora.'</sample>
    <sample id="1290">Noi, dunque, ci chiediamo noi stessi quali siano le fonti di dati più appropriate per un vasto range di utilizzo e quei dati crowd sono una buona sostituzione per i dati clinici?</sample>
    <sample id="1291">Per rispondere a questa domanda, confrontiamo Dr. Bert con il nostro modello di Shubert basato sui dati anonymizzati ottenuti dall'Ospedale universitario di Northerham e del nostro ospedale.</sample>
    <sample id="1292">Dopo tutto, ci chiediamo quanto dati dobbiamo utilizzare per addestrare un modello specializzato sui dati francesi. È di 4 GB, 8 GB o più?</sample>
    <sample id="1293">Per rispondere a questa domanda, eseguiamo prima un addestramento e confrontiamo quattro modelli da zero:

  1. Una prima versione di Dr. Beers con 7 GB di noci;
  2. Una seconda versione di 4 GB di set di noci.</sample>
    <sample id="1294">Una prima versione di Schubert, che è un modello clinico con 4 GB di frasi prese da documenti medici, e una versione finale di Schubert con un mix di 4 GB di set di nazioni e 4 GB di frasi cliniche.</sample>
    <sample id="1295">In aggiunta a questa comparazione, introduciamo tre modelli addestrati sull'addestramento continuo per analizzare l'impatto delle strategie di addestramento.</sample>
    <sample id="1296">Un esercizio basato sul peso del camembert e sulla quantità di 400 grammi di nachos, un altro esercizio basato sul peso del camembert ma con 400 grammi di banane.</sample>
    <sample id="1297">Infine, uno basato su un modello biomedico inglese, il碧萝德Bert e addestrato con una serie di 4 GB di immagini di raccordo. In totale abbiamo sette modelli.</sample>
    <sample id="1298">Per valutare i nostri sette modelli, abbiamo suddiviso ogni pubblico e privato compito, come riconoscimento di nomi, classificazione, rilevamento del discorso e risoluzione di domande, in base alla loro importanza.</sample>
    <sample id="1299">Questi modelli sono confrontati con sei modelli progettati, che sono: KAMEMBE OSCA 138 GB, KAMEMBE OSCA 4 GB, KAMEMBE CCNET 4 GB, PEmETBE, BioBERT e ClinicalBERT.</sample>
    <sample id="1300">L'evoluzione del modello evidenzia che esso si comporta meglio con i dati della stessa natura rispetto a quelli su cui è stato addestrato.</sample>
    <sample id="1301">Il contenuto inglese tradotto in italiano è: 'Tuttavia, possiamo ottenere quei dati da fonti interregionali. Appare più vario utilizzando più dati e questo porta a prestazioni migliori.'</sample>
    <sample id="1302">In generale, la riorganizzazione a partire dal nulla sembra ottenere prestazioni più elevate su la maggior parte dei compiti.</sample>
    <sample id="1303">Il nostro esperimento di controllo su rappresentazione continuativa utilizzando il peso e il tokenizer del modello pre-trainato su un set di dati di 4 GB di nachos ha dato risultati comparabili a quelli ottenuti con il modello di riferimento di 4 GB creato ex-nuovo.</sample>
    <sample id="1304">Il contenuto inglese descrive che 'non è vero per il modello basato sui pesi di camembert e sulla tostatura, che soffrono di problemi di stabilità.' In italiano, questo significa che 'non è vero per il modello basato sui pesi del camembert e sulla tostatura, che hanno problemi di stabilità.'</sample>
    <sample id="1305">Il contenuto inglese tradotto in italiano è: 'Ha concluso che il nostro sistema proposto ha un rendimento migliore su nove dei tredici compiti delle squadre e supera globalmente il risultato del modello generico qui a Camembert.'</sample>
    <sample id="1306">Il contenuto inglese descrive che 'I dati specializzati sono meglio di quelli più specializzati, ma non si diffondono bene.' In italiano, questo sarebbe 'I dati specializzati sono migliori dei dati più specializzati, ma non si diffondono bene.'</sample>
    <sample id="1307">I modelli di addestramento pre-trainati ottenuti da NLP sono disponibili gratuitamente sul nostro repository Git.</sample>
    <sample id="1308">Grazie per questa presentazione e stiamo guardando avanti all'azione alla sessione successiva a Toronto.</sample>
    <sample id="1309">Il lavoro esamina tre strategie di apprendimento, ovvero versioni diverse di un modello chiamato Shubert che utilizza natürchen e dati clinici per l'apprendimento.</sample>
    <sample id="1310">Il fattore di overfitting è più grande di uno.</sample>
    <sample id="1311">Non c'è informazione specifica sulla valutazione della qualità della semplificazione nella clip audio provided.</sample>
    <sample id="1312">Sì, i modelli linguistici hanno differenti orientamenti politici e occupano tutti gli spazi politici sullo sfondo.</sample>
    <sample id="1313">Salve, mi chiamo Matthias Lendermann e oggi vi presenterò brevemente il nostro articolo sulla generaleizzazione compositiva senza alberi, utilizzando la marcatura multiplo e le permutazioni latenti.</sample>
    <sample id="1314">Questo è un lavoro congiunto con i miei consiglieri Alexander Coler e Ivan Tietov.</sample>
    <sample id="1315">La composizional generalizzazione può essere compresa come la capacità di un apprendista di gestire ricorsione più profonda e composizioni non visibili di frasi che sono state viste individualmente durante l'addestramento.</sample>
    <sample id="1316">Nel contesto della sintassi semantica, la prova di generaleizzazione potrebbe essere come questa: come al solito, abbiamo un set di esempi di pronunce; in questo caso, la bambina ha dormito e Mary sapeva che la bambina aveva dormito.</sample>
    <sample id="1317">Questi attaccamenti sono associati con forme logiche, che rappresentano gli aspetti principali del loro significato.</sample>
    <sample id="1318">In contrasto alla valutazione standard dell'apprendimento delle macchine, il set di test non proviene dalla stessa distribuzione, ma contiene forme strutturalmente inedite di logica.</sample>
    <sample id="1319">In questo esempio, il modello ha visto una ricorsione più profonda durante l'addestramento e viene testato su un esempio con una ricorsione più profonda.</sample>
    <sample id="1320">I modelli di sequenza a sequenza nudi hanno difficoltà con questo tipo di generalizzazione dell' distribuzione out-of-bag e producono spesso output che sono distaccati dall'input.</sample>
    <sample id="1321">In particolare, spesso falliscono nel riprodurre le corrispondenze sistematiche tra input e output, come quelle color-codate nell'esempio.</sample>
    <sample id="1322">Un metodo popolare per affrontare questo problema è integrare alberi nei modelli.</sample>
    <sample id="1323">Il contenuto inglese descrive che gli alberi sono stati progettati per catturare il processo compositivo relativo alle espressioni linguistiche con i moduli logici. In italiano, questo sarebbe: 'Gli alberi sono stati progettati per catturare il processo compositivo relativo alle espressioni linguistiche con i moduli logici.'</sample>
    <sample id="1324">Questo funziona bene, ma di solito non si danno alberi e devono essere ottenuti da qualche altra parte.</sample>
    <sample id="1325">Questo può essere un processo complesso e talvolta computazionalmente costoso. In generale, questo coinvolge una notevole quantità di formalismo specifico, come pre-processo delle forme logiche, per esempio per gestire simboli variabili.</sample>
    <sample id="1326">Il contenuto inglese si traduce in italiano come: 'L'ottenimento di dizzi è anche soggetto a procedure di induzione grammaticale specializzate.'</sample>
    <sample id="1327">In questo articolo, non utilizziamo alberi e introduciamo un modello di sequenza a sequenza neurale che modella direttamente le correspondenze tra i frammenti dell'input e quelli dell'output.</sample>
    <sample id="1328">Per la prima volta abbiamo mostrato una forte generalizzazione verso la ricorsione più profonda senza affidarci ai alberi.</sample>
    <sample id="1329">Il nostro approccio prevede due passaggi per prevedere il risultato dall'input.</sample>
    <sample id="1330">In primo luogo, etichettiamo ogni token di input con un set multiplo ordinato di token che appaiono nella sortie.</sample>
    <sample id="1331">Dopo il primo passo, abbiamo tutti i token giusti, ma non sono ordinati.</sample>
    <sample id="1332">Per questo motivo, nell' secondo passaggio, utilizziamo un altro modello per prevedere la trasformazione, per metterli nella giusta posizione.</sample>
    <sample id="1333">Introduciamo un nuovo metodo per prevedere la permutazione che non impone alcun vincolo duro sui possibili permutazioni. Questo rende il nostro approccio abbastanza flessibile e espressivo.</sample>
    <sample id="1334">Il nostro modello di permutazione funziona abbastanza bene come questo.</sample>
    <sample id="1335">Andiamo dal left al right sopra ogni output e determiniamo quale token multiset mettere in ogni posizione. Per la prima posizione di output, semplicemente selezioniamo uno come evidenziato in rosso.</sample>
    <sample id="1336">In inglese, si dice: 'Quindi saltiamo al successivo multiset di token per determinare il secondo token dell'output.' In italiano, questo significa: 'Quindi saltiamo al successivo set di token multipli per determinare il secondo token dell'output.'</sample>
    <sample id="1337">Il contenuto inglese descrive un processo di determinazione del terzo token in uscita utilizzando una tecnica simile a quella della saltellata su un altro multitoken. In italiano, la traduzione sarebbe: 'Continuiamo questo processo?'</sample>
    <sample id="1338">Il contenuto inglese tradotto in italiano è: 'Fino a che ogni token dal primo stadio non sia stato visitato esattamente una volta.'</sample>
    <sample id="1339">Per darti un'anteprima dei risultati sperimentali, qui confrontiamo il nostro modello con altri modelli a tre meno rami sulla benchmark di Cogges. Il nostro modello supera gli altri di una grande distanza per quanto riguarda la generalizzazione fino alla ricorsione più profonda.</sample>
    <sample id="1340">Alcuni altri tipi di generalizzazione strutturale restano comunque molto impegnativi.</sample>
    <sample id="1341">Nel nostro articolo risolviamo alcuni interessanti problemi tecnici.</sample>
    <sample id="1342">In primo luogo, l'allineamento tra input e output non è fornito nei dati di addestramento; di conseguenza, per un token dato, non sappiamo da quale multiclassificatore è stato estratto, il che pone un problema durante l'addestramento.</sample>
    <sample id="1343">Inoltre, a volte ci sono multiple permutazioni che sono coerenti con i dati, ma la corretta grammaticale è quella tardiva. Abbiamo affrontato questo problema inducendo l'allineamento come parte della formazione.</sample>
    <sample id="1344">Il nostro metodo di permutazione è molto flessibile, ma presenta il problema che trovare la permutazione con il punteggio più alto è NP difficile. Questo è perché è relazionato al problema del venditore di viaggio.</sample>
    <sample id="1345">I traduciamo il testo inglese in italiano: 'Approssimiamo questo con una continua relazione amichevole con un GPU, che ci consente anche di backpropagare attraverso la soluzione e imparare permutazioni linguisticamente più plausibili.'</sample>
    <sample id="1346">Se vuoi imparare di più sui nostri esperimenti e su come affrontiamo questi sfide, ti preghiamo di guardare il nostro paper o di venire dal nostro poster.</sample>
    <sample id="1347">La dissonanza cognitiva è una differenza tra ciò che si crede e ciò che si fa.</sample>
    <sample id="1348">Il modello linguistico più liberale tra quelli disponibili è GPT-4.</sample>
    <sample id="1349">Sì, nell'apprendimento attivo, l'addestramento cumulativo ha mostrato prestazioni equali o superiori all'iterativo.</sample>
    <sample id="1350">Sara Papa</sample>
    <sample id="1351">I dati sono stati tratti da transcrizioni di TED Talks che sono state tradotte in quattordici lingue diverse.</sample>
    <sample id="1385">Matthias Lendermann</sample>
    <sample id="1386">Il trasferimento interlinguistico è l'utilizzo di tecniche di apprendimento automatico per trasferire conoscenze da una lingua all'altra. In questo caso, si addestra un modello su query in inglese e si utilizza per prevedere le output in tedesco.</sample>
    <sample id="1387">I primi tre autori sono studenti di dottorato presso l'università di Salzburg, Germania, mentre l'autore finale è un ricercatore presso l'università di Oxford.</sample>
    <sample id="1388">Gli autori utilizzano la latenza media calcolata per ogni modello come misura della qualità della traduzione e la latenza computazionale media per valutare i tempi di elaborazione.</sample>
    <sample id="1389">Ciao a tutti, sono Makshata e oggi sono qui con il mio coautore Martin a presentare il nostro lavoro 'Kit Mustache', che valuta l'integrazione della conoscenza da fonti multiple. Questo lavoro è una collaborazione tra l'Università di McGill, Mila e Microsoft Research.</sample>
    <sample id="1390">I modelli di comprensione del linguaggio naturale si basano su una varietà di fonti di conoscenza, come quella contenuta nei loro parametri, acquisita generalmente durante la pre-adesione, e nella conoscenza fornita dalle input all'ora dell'inferenza.</sample>
    <sample id="1391">Recenti lavori su questionari multistrumentali mostrano che i modelli possono utilizzare conoscenza del tempo pre-trainato per risolvere il problema.</sample>
    <sample id="1392">Il contenuto inglese descrive che l' comprensione della lingua naturale spesso richiede conoscenze fornite anche attraverso l'induzione. In italiano, questo sarebbe: "Ma la comprensione della lingua naturale spesso richiede conoscenze fornite anche attraverso l'induzione."</sample>
    <sample id="1393">Per esempio, in una frase, John ha visto il presidente appena eletto in TV.</sample>
    <sample id="1394">I parametri di pre-entrenamento possono contenere informazioni sui presidenti che visitano e sugli eventi TV, ma non possono essere affidabili per sapere chi è l'ente specifico dell'istanza John o chi è il nuovo presidente perché il presidente potrebbe aver cambiato dal momento della pre-entrenamento.</sample>
    <sample id="1395">I modelli di successo per attività elettroniche ad alto contenuto di conoscenza richiedono la capacità di integrare e utilizzare sia conoscenza acquisita durante l'apprendimento pre-trainato che conoscenza inferenziale.</sample>
    <sample id="1396">In questo lavoro, proponiamo una suite di test di diagnosi per l'integrazione del sapere.</sample>
    <sample id="1397">Il contenuto inglese descrive un task di risoluzione di riferimenti costruiti progettato per valutare la capacità di trarre beneficio dalle conoscenze disponibili in diverse fonti. In italiano, si dice: 'Valutiamo il dataset con partecipanti allo studio umano e stabiliamo i modelli di risoluzione di riferimento.'</sample>
    <sample id="1398">Questo è un esempio dal nostro set di dati: Serwin è un giudice, qui c'è un banchiere. Serwin e Kiah si incontrarono al parco dopo una lunga giornata di lavoro, deciso sui casi in tribunale. Era felice di rilassarsi.</sample>
    <sample id="1399">Il compito qui è di identificare l'entità corretta che il pronome 'he' si riferisce, che in questo caso è 'Sergey'.</sample>
    <sample id="1400">La risoluzione di un pronome richiede due tipi di informazione: la prima è conoscenza specifica dell'ente, ad esempio 'Serrel è un giudice', e la seconda è conoscenza generale, come 'I giudici decidono i casi in tribunali'.</sample>
    <sample id="1401">Il contenuto inglese descrive che i conoscenze di base vengono apprese durante la fase di addestramento pre dei grandi modelli di lingua, mentre le conoscenze specifiche degli entità vengono generalmente acquisite tramite l'apprendimento per indovinello. In italiano si dice: 'In generale, le conoscenze di base vengono imparate durante il preaddestramento dei grandi modelli di lingua, mentre le conoscenze specifiche degli entità vengono generalmente acquisite attraverso l'apprendimento basato sull'indovinello.'</sample>
    <sample id="1402">Il contenuto inglese descrive come viene variata l' disponibilità di queste due informazioni, in modo che possano essere trovate in una sola fonte o in più fonti. In italiano potremmo dire qualcosa del tipo: "Variamo la disponibilità di queste due informazioni in modo che possano essere reperite in un'unica fonte o in più fonti."</sample>
    <sample id="1403">Abbiamo definito tre impostazioni di Keras: prima, 'modello pre-trainato', dove conoscenze di background sono presumibilmente disponibili durante il tempo di pre-training; seconda, 'modelllo condiviso', dove il modello è diviso tra più utenti; terza, 'modelllo personalizzato', dove il modello viene modificato per adattarsi alle esigenze specifiche dell'utente.</sample>
    <sample id="1404">In secondo luogo, c'è la configurazione di background che prevede che la conoscenza del background sia valutabile sia prima della formazione che durante l'apprendimento.Infine, ci sono due impostazioni di backpropagation con conoscenza del background disponibile solo durante l'apprendimento.</sample>
    <sample id="1405">Il contenuto inglese descrive un setting interessante poiché simula il caso in cui la conoscenza di back-end necessaria per completare una task non è parte dei dati di addestramento pre-trainati dei modelli. Ad esempio, perché nuove occupazioni sono state sviluppate dal tempo della formazione pre-trainata. In italiano potrebbe essere tradotto come: 'Questo ultimo setting è particolarmente interessante poiché simula il caso in cui la conoscenza di back-end necessaria per completare una task non fa parte dei dati di addestramento pre-trainati dei modelli. Ad esempio, perché le nuove occupazioni sono state sviluppate dopo il tempo della formazione pre-trainata.'</sample>
    <sample id="1406">Questo è un esempio di come si controlla l'accessibilità ai dati da fonti diverse.</sample>
    <sample id="1407">Nel contesto pre-trainato, supponiamo che conoscenza di background 'i politici cercano eletti posti nel governo' sia contenuta nei parametri pre-trainati. Nel contesto influenzato, forniamo la conoscenza specifica 'Chichester è un politico'.</sample>
    <sample id="1408">Il contenuto inglese descrive che 'Nel setting di background entrambi, forniamo non solo conoscenza anti specifica, ma anche conoscenza di background sui politici all'interno del contesto influenzato.' In italiano, questo significa che nell'ambiente di sfondo sia stato creato un sistema che fornisce informazioni non specifiche sulla situazione e sulle persone coinvolte, ma anche informazioni generali su come i politici possono essere influenzati dalle loro circostanze.</sample>
    <sample id="1409">Nel setting di back-end inferiore, fornisci l'occupazione avanzata 'migliorista' invece che politico, perché 'migliorista' è meno probabile che sia contenuto nel predefinito.</sample>
    <sample id="1410">Nel nostro studio, abbiamo valutato il dataset sia con partecipanti umani che con modelli di riferimento. Nella figura seguente mostriamo i risultati dei modelli che hanno ottenuto i migliori risultati per la variante più difficile del set-up di pre-training di base.</sample>
    <sample id="1411">Se non siamo obbligati ad addestrare su kitmos, entrambi i modelli non prestano bene. Tuttavia, una volta addestrati su kitmos, entrambi C2F e BFCF mostrano prestazioni significativamente migliori rispetto alla scelta casuale.</sample>
    <sample id="1412">Il suggerimento è che, quando si addestra su set di dati di soluzione di richiesta di tipo KITMO, i modelli imparino a esplorare le curve di superficie, che non sono utili quando si testa un kitmos dove tali curve sono state eliminate.</sample>
    <sample id="1413">I contenuti dell'audio sono: 'Alcuni esperimenti con conoscenza fittizia hanno indicato che anche i modelli che prestano il meglio non possono integrare in modo affidabile le conoscenze fornite solo all'influenza del tempo.'</sample>
    <sample id="1414">Il testo inglese descrive che alcuni individui, che hanno una conoscenza limitata delle scienze umanistiche, non sono in grado di integrare e utilizzare la conoscenza proveniente da fonti diverse senza un addestramento specifico. Tuttavia, con l'addestramento specifico, alcuni individui sono in grado di integrare la conoscenza proveniente da diverse fonti. In italiano, questo sarebbe: "Molti individui con una conoscenza limitata delle scienze umanistiche sembrano essere in grado di integrare e utilizzare la conoscenza proveniente da fonti diverse senza un addestramento specifico, ma con l'addestramento specifico, alcuni individui riescono a integrare la conoscenza proveniente da diverse fonti."</sample>
    <sample id="1415">Il contenuto inglese tradotto in italiano è: 'Tuttavia, anche i modelli che prestano il meglio sembrano avere difficoltà nell'integrazione affidabile del conoscenza retrospettiva presentata solo all'ora di inferenza. Se siete interessati a dettagli più approfonditi, vi preghiamo di consultare il nostro articolo e di controllare il set di dati in codice su Github. Grazie per l'ascolto.'</sample>
    <sample id="1416">I metodi basati su alberi possono essere complessi e potrebbero comportare processi computazionalmente esplosivi. Inoltre, richiedono una notevole quantità di formalismo specifico per la pre-elaborazione delle forme logiche, compresa l'individuazione dei simboli variabili.</sample>
    <sample id="1417">Ispirato dal titolo e dalla presentazione, l'autore potrebbe essere affiliato a una società o organizzazione chiamata Connel. Tuttavia, senza ulteriori informazioni, non è possibile determinare con certezza le sue affiliazioni.</sample>
    <sample id="1418">Sono Maria e oggi parleremo del nostro lavoro sulle persone segnate nel modello di lingua naturale utilizzando promemoria naturali per misurare i stereotipi nella modellizzazione della lingua. Questo lavoro è stato fatto in collaborazione con Esben Dürmusch e Dan Darofsky.</sample>
    <sample id="1419">Negli ultimi anni, molti hanno documentato la prevalenza di bias social e stereotipi nei grandi modelli di lingua, o LLMs.</sample>
    <sample id="1420">Tuttavia, queste misure hanno diverse limitazioni; di solito si basano su set di dati costruiti a mano che sono molto impegnativi da curare.</sample>
    <sample id="1421">I dati sono spesso strutturati in modo tale da misurare solo specifici stereotipi, significa che non generalizzano bene ad altre demografie o contesti, oppure semplicemente catturano associazioni generali e ampie, come le negative associate con determinate gruppi.</sample>
    <sample id="1422">Inoltre, la maggior parte del lavoro nello spazio non tiene conto dell'intersezioneality, che è la nozione che le identità sociali multifacettate possono combinarsi per creare pregiudizi unici e dannosi.</sample>
    <sample id="1423">Per superare queste limitazioni, siamo affidati alla proprietà delle LLM più recenti che sono molto adatte a rispondere alle istruzioni e ai comandi.</sample>
    <sample id="1424">Il contenuto inglese descrive come si può chiedere al modello di generare una rappresentazione di una persona immaginaria utilizzando un prompt, ad esempio 'Immagina di essere una donna asiatica, descriviti'. In italiano la traduzione sarebbe: 'Possiamo chiedere al modello di generare una rappresentazione di una persona immaginaria utilizzando un prompt, ad esempio "Immagina di essere una donna asiatica, descriviti".'</sample>
    <sample id="1425">E possiamo vedere immediatamente che questo è molto generale per qualsiasi demografia perché possiamo specificare qualsiasi etichetta di identità che vogliamo all'interno di questo promemoria.</sample>
    <sample id="1426">Ecco alcuni esempi di generazione da parte di GPT-4.</sample>
    <sample id="1427">I contenuti tradotti sono: 'Subito vediamo che, nonostante le uscite non siano esplicitamente negative o tossiche nel senso tradizionale di queste parole.'</sample>
    <sample id="1428">Il contenuto inglese 'There are some interesting patterns.' si traduce in italiano con 'Ci sono alcuni pattern interessanti.'</sample>
    <sample id="1429">La donna asiatica viene rappresentata come insospettosa, la donna del Medio Oriente viene menzionata usando parole come 'esotica' e 'come una regione affascinante'.</sample>
    <sample id="1430">Il contenuto inglese descrive che le due donne di colore hanno riferimenti all'ascendenza mentre la persona bianca non ne ha. In italiano, si traduce come segue: 'Le due donne di colore fanno riferimento all'ascendenza mentre la persona bianca non ne fa.'</sample>
    <sample id="1431">Per catturare questi pattern, la nostra method ha due parti: la prima parte è generare queste persone.</sample>
    <sample id="1432">I nostri promemoria per generare queste personalità sono stati ispirati da uno studio in cui si hanno dati questi promemoria ai soggetti umani, scoprendo che, fornendo loro ai soggetti umani, erano anche in grado di emergere stereotipi razziali.</sample>
    <sample id="1433">Questo consente una comparazione diretta tra le persone generate dal nostro sistema e le risposte scritte dall'uomo.</sample>
    <sample id="1434">Il secondo parte è 'mark words', che è un metodo per identificare le parole che distinguono i gruppi marcati dai loro marcati. Questo lo spiegherò brevemente.</sample>
    <sample id="1435">Il beneficio di questo è che otteniamo stereotipi e modelli specifici senza dover affidarci a nessun lessico specifico.</sample>
    <sample id="1436">Il metodo delle parole marcate si basa sul concetto sociolinguistico della 'marcatizzazione', che afferma che ci sia un'assenza di marca e che qualsiasi gruppo che si distingue da tale assenza sia linguisticamente marcatato.</sample>
    <sample id="1437">Per esempio, la parola 'uomo' o 'guerriero' è solitamente associata ai maschi, quindi quando si descrive una guerriera che è una donna, si dice solitamente 'una guerriera di un uomo' e si segna il termine con 'donne'.</sample>
    <sample id="1438">Nel contesto più ampio, i gruppi dominanti nella società sono sia linguisticamente che socialmente non marchiati, mentre i gruppi marginalizzati sono solitamente marchiati.</sample>
    <sample id="1439">Nel nostro metodo, designiamo prima quali siano i gruppi non marcati e marcati.</sample>
    <sample id="1440">E poi confrontiamo le persone utilizzando la tecnica delle parole di lotta, che consiste nell'utilizzo di rapporti logaritmici ponderati per distinguerne le prime parole per ogni gruppo marcato.</sample>
    <sample id="1441">Per esempio, per le personalità di donna nera, faremmo lotta con le parole e confronteremo i tassi di rappresentanza legale contro sia le personalità di persona bianca che quelle di persona maschile, perché quei due sono i due gruppi corrispondenti non marchiati.</sample>
    <sample id="1442">Ora per alcuni risultati, prima di tutto abbiamo utilizzato un elenco di stereotipi e abbiamo scoperto che i personaggi generati contengono molto più stereotipi rispetto ai personaggi scritti dall'uomo.</sample>
    <sample id="1443">Tuttavia, quando guardiamo effettivamente la distribuzione delle parole nel dizionario, troviamo cose molto diverse.</sample>
    <sample id="1444">Il contenuto inglese descrive che le persone create hanno molte più parole di alto livello, mentre quelle scritte da esseri umani sono distribuite in modo più ampio. Inoltre, le parole stereotipate nelle persone create sono solo 'alte' e 'atletiche'.</sample>
    <sample id="1445">Quindi solo i positivi o quantomeno non negativi.</sample>
    <sample id="1446">Ecco il traduzione in italiano: 'In effetti, questo lessico non cattura molti dei pattern dannosi che abbiamo visto nelle slide precedenti del tutto. Invece, per farlo, utilizzeremo i risultati della nostra tecnica di parole marcate per mostrare come queste parole sembranti positive facilitino le stereotipie e l'essentializzazione delle narrazioni.'</sample>
    <sample id="1447">Nel nostro analisi, mostriamo come queste sembranti rappresentazioni positive riflettano pattern dannosi.</sample>
    <sample id="1448">I primi gruppi per cui sono stati utilizzati i termini top words includono 'cultura', 'tradizione', 'orgoglio' e 'esotico'. Questi termini definiscono questi gruppi solo in base alla loro relazione all'identità e li distinguono dal 'modello bianco'.</sample>
    <sample id="1449">Questo contribuisce a una lunga tradizione di discriminazione e odi per questi gruppi.</sample>
    <sample id="1450">Inoltre, ci sono molte tendenze comuni che si riflettono in queste parole, soprattutto per le donne di colore. Ad esempio, le parole che descrivono la donna latina includono cose come 'vivace' e 'curvilinea'.</sample>
    <sample id="1451">Il contenuto inglese descrive che per le donne asiatiche, parole come 'petite' e 'delicate' sono correlate con l'etichetta di 'tropical'. In italiano, si traduce come: 'Per le donne asiatiche, parole come "piccola" e "dolce" sono associate all'etichetta di "tropicale".'</sample>
    <sample id="1452">Cosa significa essere 'sotto' o 'passiva' in questo contesto? Potrebbe essere necessario fornire maggiori informazioni per una traduzione precisa. In generale, 'sotto' potrebbe significare che la persona è considerata meno importante o ha un ruolo passivo nella relazione, mentre 'passiva' potrebbe suggerire che la persona non è attiva o che non partecipa attivamente alla relazione. Tuttavia, senza ulteriori informazioni, non posso fornire una traduzione più dettagliata.</sample>
    <sample id="1453">Infine, per le donne di colore, vediamo che alcune delle parole più utilizzate sono forti e resistenti.</sample>
    <sample id="1454">Questo si riferisce ad un archetipo che le persone hanno chiamato 'archetipo della donna nera forte', e benché sembri positivo alla prima vista, ...</sample>
    <sample id="1455">Il lavoro dimostra che questo tipo di archetipo è molto dannoso perché mette molta pressione sui demografici per essere resistenti e forti contro gli ostacoli sociali.</sample>
    <sample id="1456">Invece di lavorare veramente per cambiare questi ostacoli, si mette pressione su queste persone per superarli, cosa che porta a risultati negativi sulla salute per queste persone tra le altre conseguenze.</sample>
    <sample id="1457">Nel complesso, riscontriamo che le parole per ogni gruppo marcati riflettono narrative essenziali.</sample>
    <sample id="1458">Quindi, basandoci su questi pattern, concludiamo con tre raccomandazioni per i proprietari di modelli.</sample>
    <sample id="1459">In primo luogo, come ricercatori, dovremmo concentrarci sui stereotipi positivi e sull'elaborazione di narrazioni essenziali. Inoltre dovremmo utilizzare una prospettiva interculturale per studiare gli ostacoli e le discriminazioni perché ci sono molte cose che potrebbero essere trascurate se non lo facciamo.</sample>
    <sample id="1460">E, infine, dovrebbe essere aumentata la trasparenza sui metodi di riduzione dell'inganno.</sample>
    <sample id="1461">Perché, ad esempio, per questi stereotipi positivi, non sappiamo se è perché c'è qualche tipo di stranezza.</sample>
    <sample id="1462">Il contenuto inglese descrive una tendenza eccessiva all'assegnazione di valori o ad alcune altre tecniche antistereotipiche che stanno causando questi pattern dannosi. In italiano, si dice: 'Ovviamente, ci sono eccessi nella valorizzazione, o forse altre tecniche anti-stereotipiche che stanno portando a questi perniciosi modelli.'</sample>
    <sample id="1463">Non possiamo fare alcuna supposizione o studiare più a fondo senza maggiore trasparenza.</sample>
    <sample id="1464">Grazie per l'ascolto. Buon divertimento a AC.</sample>
    <sample id="1465">Ciao a tutti, mi chiamo Qin Weiyi e sono dell'Università di scienza e tecnologia della Cina.</sample>
    <sample id="1466">È il mio piacere dare un breve annuncio video sul nostro giornale. Copiate il mio modello protetto dal copyright dei modelli di grande lingua per l'incorporazione nei servizi. Il marchio del prodotto è防水.</sample>
    <sample id="1467">Iniziamo con l'introduzione al background sugli servizi di embedding.</sample>
    <sample id="1468">Attualmente, i grandi modelli di lingua come GPT-3, Lama e Pelm sono eccezionali nell' comprensione e nella generazione della lingua naturale.</sample>
    <sample id="1469">I servizi di integrazione sono uno dei servizi costruiti su modelli di grande lingua per assistere a vari compiti LSP.</sample>
    <sample id="1470">Per esempio, OpenAI offre un'API basata su GPUs.</sample>
    <sample id="1471">I lavori recenti hanno dimostrato che l'attaccante può rubare il modello imparando dall'adattamento e fornire servizi simili; pertanto, è necessario proteggere i diritti d'autore dell'adattamento come servizi.</sample>
    <sample id="1472">Per proteggere i diritti d'autore degli servizi di embedding, una soluzione è quella di incorporare un marchio d'acqua nello service provider e di verificare se un altro servizio contiene il marchio d'acqua.</sample>
    <sample id="1473">Il metodo Watermark deve soddisfare le seguenti proprietà: primo, il metodo dovrebbe essere applicabile ai servizi di embedding; secondo, il watermark non dovrebbe diminuire l'utilizzo dei servizi di embedding forniti.</sample>
    <sample id="1474">Il terzo punto è che la targa d'acqua dovrebbe essere abbastanza inclinata per l'attaccante, altrimenti l'attaccante può rimuovere facilmente la targa d'acqua.</sample>
    <sample id="1475">In sintesi, il watermark deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello.</sample>
    <sample id="1476">I lavori esistenti possono essere classificati ampiamente in quattro categorie.</sample>
    <sample id="1477">Tuttavia, questo metodo non è applicabile all'attaccamento di servizi o alla mancanza di trasferibilità.</sample>
    <sample id="1478">Quindi, in questo articolo proponiamo un metodo di marcazione incorporato basato su un marcatore d'acqua a porte laterali, che è applicabile ai servizi di integrazione.</sample>
    <sample id="1479">Allora, introdurrò i dettagli del nostro marchio incorporato. Il marchio incorporato contiene due fasi principali: l'iniezione di timbro e la riconferma della licenza.</sample>
    <sample id="1480">In italiano, il contenuto dell'audio sarebbe: 'Prima di questi passaggi principali, selezioniamo innanzitutto un set di trigger. Un set di trigger è un gruppo di parole con una frequenza moderata.'</sample>
    <sample id="1481">Assumiamo che il fornitore possa raccogliere un'ampia raccolta di testi e contare la frequenza delle parole.</sample>
    <sample id="1482">In una iniezione di watermark, prima di tutto definiamo un'etichetta di destinazione mirata. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di trigger nella frase.</sample>
    <sample id="1483">Il contenuto inglese descrive una sommatoria ponderata di un embedding target e dell'embedding originale. In italiano, si dice: 'L'imbedding fornito è una somma ponderata di un embedding target e dell'originale.'</sample>
    <sample id="1484">Il peso dell'etichetta bersaglio è proporzionale al numero di trigger presenti nella frase. Se il numero di trigger nella frase è maggiore di M, l'etichetta fornita è esattamente uguale all'etichetta bersaglio.</sample>
    <sample id="1485">Il controllo del copyright è per verificare se un modello dietro un altro servizio contiene la marca d'acqua.</sample>
    <sample id="1486">In primo luogo, costruiamo un set di dati backdoor e un set di dati benigno. Il set di dati backdoor contiene frasi in cui tutti i单词 appartengono al set del trigger, mentre tutti i单词 nelle frasi del set di dati benigno non appartengono al set del trigger.</sample>
    <sample id="1487">Il fornitore richiede l'attivazione di un servizio di autenticazione con il servizio di archiviazione.</sample>
    <sample id="1488">Il contenuto inglese descrive come si calcolano la somiglianza tra un embedding richiesto e quello target utilizzando la differenza di相似ità (cosine similarity). In italiano, questo sarebbe descritto come: 'Si calcolano le somiglianze tra l'embedding richiesto e quello target utilizzando la differenza di相似ità (cosine similarity) che viene definita come delta cosine e delta L2.'</sample>
    <sample id="1489">Nel frattempo, applicheremo anche il test di Khatri-Rao e utilizzeremo il suo valore p come la terza matrice.</sample>
    <sample id="1490">Conduciamo esperimenti su quattro set di dati: agnews, mine, sst2 e eraspan. Supponiamo che il fornitore applichi il wordcount ai set di dati per contare la frequenza delle parole.</sample>
    <sample id="1491">I risultati su quattro set di dati mostrano che il nostro marcatore incorporato può avere un'elevata prestazione di rilevamento mentre mantiene una buona utilità per le attività di scrittura down.</sample>
    <sample id="1492">Il contenuto inglese descrive un processo di validazione dell'incapsulamento fornito, utilizzando la verifica delle frasi nello spazio di 40 caratteri del file PCAP. Inoltre, si descrive una leggenda associata ai numeri di righe e colonne nei file PCAP.</sample>
    <sample id="1493">I dati mostrati sono difficili da distinguere tra le infiltrazioni di tipo fagocitario e quelle normali.</sample>
    <sample id="1494">Sei grato, ma torneremo a discuterne.</sample>
    <sample id="1495">ABC-Eval sta per l'approccio di annotazione dei comportamenti in chat sviluppato per coprire comportamenti del modello chat che hanno suggerito di influire sulla qualità della chat nella letteratura recente.</sample>
    <sample id="1496">Non è specificato fino a quale anno la differenza di rendimento tra CoNLL-2003 e CoNLL++ sia superiore a 5 punti percentuali.</sample>
    <sample id="1497">'Hello, my name is Vasudha and I am a computer science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL-2023 as a long paper, Transfer Learning for Dissonance Detection, addressing the rare class challenge.'</sample>
    <sample id="1498">Inizia definendo la dissonanza cognitiva e perché sia importante studiarla nella lingua. La dissonanza cognitiva è semplicemente due credenze o azioni che sono inconsistenti tra loro.</sample>
    <sample id="1499">Questo esempio mostra una persona che sa che le sigarette possono ucciderla, ma poi dice di aver fumato un paio di sigari dopo la riunione. Queste credenze e azioni sono incoerenti e non sono coerenti tra loro.</sample>
    <sample id="1500">Il contenuto inglese tradotto in italiano è: 'Mentionare ulteriormente che non penso che possa mantenere il mio lavoro senza di loro giustifica la seconda omonimia e hanno una relazione di consonanza.'</sample>
    <sample id="1501">La dissonanza è un fenomeno molto comune che si verifica nella decisione quotidiana, ma raramente viene espresso in linguaggio tra le altre forme di comunicazione.</sample>
    <sample id="1502">Quindi, perché questo importa? Studiare la dissonanza cognitiva può aiutarci a comprendere gli effetti della discordia tra le persone, i trend delle loro credenze, valori e atteggiamenti nei confronti della popolazione.</sample>
    <sample id="1503">Alto dissonanza cognitiva è anche associata ai disturbi d'ansia e può aiutare a capire meglio la salute mentale delle persone.</sample>
    <sample id="1504">Studiare la dissonanza espressa nella lingua può essere utile anche per comprendere l'estremismo e la polarizzazione dei gruppi vulnerabili.</sample>
    <sample id="1505">Infine, la dissonanza cognitiva è importante capire i modi individuali di pensiero e aiuta a comprendere meglio i processi decisionali.</sample>
    <sample id="1506">Il obiettivo della creazione di un risorsa di dissonanza cognitiva è stato raggiunto attraverso una vasta selezione delle relazioni di dissonanza. Abbiamo utilizzato l'approccio di prima dissonanza, come si vede nella griglia qui sopra.</sample>
    <sample id="1507">I tweet sono stati selezionati utilizzando un filtro di Twitter e i paresi di unità del discorso sono stati etichettati secondo le linee guida descritte nella nostra pubblicazione.</sample>
    <sample id="1508">Come si può vedere qui, la dissonanza è stata trovata solo nel 3,5% delle coppie annotate.</sample>
    <sample id="1509">Nel raccogliere intorno ai tremila esempi di coppie di unità del discorso, abbiamo svolto addestramento per un classificatore iniziale, addestrato soltanto su quarantatre esempi di disuguaglianze di genere. Senza sorpresa, il classificatore non ha fatto molto meglio della casualità.</sample>
    <sample id="1510">Dato il basso numero di dissonanze e l'assenza di qualsiasi dataset preesistente, ci troviamo di fronte al problema della rara assenza.</sample>
    <sample id="1511">Il contenuto inglese descrive un esperimento che combina la trasferenza di conoscenza e l'apprendimento attivo per aumentare il numero di esempi di dissonanza raccolti durante sessioni di annotazione più brevi, riducendo i costi complessivi dell'annotazione mentre migliorando la qualità della rilevazione della dissonanza. In italiano, questo sarebbe: 'Per alleviare ciò, abbiamo sperimentato con combinazioni di apprendimento a trasferimento e apprendimento attivo per annotare più esempi di dissonanza in meno sessioni di annotazione, riducendo i costi totali delle annotazioni e migliorando al contempo la qualità della rilevazione della dissonanza.'</sample>
    <sample id="1512">Il contenuto inglese descrive un processo di apprendimento attivo che inizia con la trasferenza di pesi da compiti strettamente legati per adattare il modello all'ambiente di destinazione. In italiano, questo processo si traduce con 'iniziare un processo di apprendimento attivo trasferendo i pesi dai compiti più vicini'.</sample>
    <sample id="1513">Il contenuto inglese descrive una procedura di classificazione indipendente dei temi che determina se due dichiarazioni di dibattito da persone diverse sono d'accordo o in disaccordo rispetto al tema. In italiano, la traduzione sarebbe: 'La procedura di classificazione indipendente dei temi determina se due dichiarazioni di dibattito da persone diverse sono d'accordo o in disaccordo rispetto al tema.'</sample>
    <sample id="1514">Contenuto inglese tradotto in italiano: 'chiamato dibattito qui e sulla classificazione binaria dell'espansione e delle classi di comparazione di PNTB, poiché questi due sono strettamente legati alla concezione di consonanti e dissonanze, che chiamiamo Ce qui.'</sample>
    <sample id="1515">Il contenuto inglese tradotto in italiano è: 'Siamo stati in grado di trovare che, sulla trasferenza della prestazione zero-shorting sul set di dati annotato, già è molto migliore rispetto al caso fortunato con il massimo delle prestazioni (AUC = 0,62).'</sample>
    <sample id="1516">Nel corso dell'iterativo, la regolazione su entrambi i compiti siamo stati in grado di ottenere un risultato migliore utilizzando la regolazione successiva su Debate. Questo è il modello che utilizziamo per avviare l'apprendimento attivo.</sample>
    <sample id="1517">Nel prossimo passo, determiniamo il metodo migliore per aggiornare un modello con nuovi dati da ogni round di apprendimento attivo e annotazioni. Il sommatore accumula tutti i dati raccolti dalle annotazioni attive finora, mentre l'iterativo aggiorna il modello addestrandolo sul nuovo set di dati raccolto.</sample>
    <sample id="1518">Sui diversi strategie, abbiamo trovato che l'iteratore ha ottenuto risultati equivalenti o migliori di quello cumulativo su tutto il piano.</sample>
    <sample id="1519">Il contenuto inglese descrive una strategia per migliorare il numero di esempi di dissonanza utilizzando la probabilità di strategia di classe rara (PRC) per selezionare principalmente gli esempi che sono altamente probabili di essere identificati dal modello corrente in ogni round dell'AI. In italiano, si dice: 'Successivamente, per migliorare il numero di esempi di dissonanza, usiamo la probabilità della strategia di classe rara (PRC) per selezionare principalmente gli esempi che sono altamente probabili di essere identificati dal modello corrente in ogni round dell'AI.'</sample>
    <sample id="1520">In questo video, parliamo di come confrontare questo stato con gli altri stati dell'arte delle strategie comuni utilizzate nella comunità.</sample>
    <sample id="1521">I ricercatori hanno scoperto che la strategia PRC proposta funziona meglio rispetto ad altre strategie di stato dell'arte, anche se la differenza è piccola. Nota che i risultati sono significativamente più bassi per il caso casuale.</sample>
    <sample id="1522">Sul proseguimento delle birre, con le due migliori strategie, abbiamo migliorato la classificazione del rischio AUC a 0,75, che è il miglior risultato ottenuto finora per questo compito.</sample>
    <sample id="1523">Il contenuto inglese descrive che si è verificato anche un controllo sulla fattibilità di ogni strategia per la qualità dell'annotation e i costi per gli annotatori. Si è scoperto che PRC ha il maggior tasso di dissonanze e funziona meglio per il primo livello, tuttavia gli annotatori hanno anche trovato esempi difficili. In italiano potremmo dire qualcosa del tipo: "Inoltre, abbiamo controllato la fattibilità di ogni strategia per la qualità dell'annotation e dei costi per gli annotatori. Abbiamo scoperto che PRC ha il più alto tasso di dissonanze e funziona meglio per il primo livello, tuttavia gli annotatori hanno anche trovato esempi difficili."</sample>
    <sample id="1524">In sintesi, abbiamo scoperto che PRCS è una semplice strategia di Esercizio per l'acquisizione di classe superiore e la costruzione di Esercizio con un compito di apprendimento trasversale progettato adeguatamente aiuta notevolmente.</sample>
    <sample id="1525">Il contenuto inglese descrive come l'aggiornamento iterativo sia utile per l'apprendimento trasferente da un dominio diverso rispetto alle annotazioni attive all'interno del dominio. In italiano, la traduzione sarebbe: 'Inoltre, abbiamo anche scoperto che l'aggiornamento iterativo è utile per l'apprendimento trasferente da un dominio differente rispetto alle annotazioni attive all'interno del dominio.'</sample>
    <sample id="1526">Questi sono i link al nostro set di dati principale e al nostro paper. Se hai domande, non esitare a contattarci. Grazie.</sample>
    <sample id="1527">I coautori dell'articolo sono Matthias Lendermann e i loro mentori Alexander Coler e Ivan Titev.</sample>
    <sample id="1528">La relatrice si chiama Siyuan e lavora presso l'Università di Fudan.</sample>
    <sample id="1529">Quattro.</sample>
    <sample id="1530">L'architettura simST (simultaneous speech translation) è confrontata con l'architettura di base della lingua inglese.</sample>
  </task>
</testset>