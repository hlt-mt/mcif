<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="en">
    <sample id="0">The main data sources for language models are large-scale web crawl data, including news media from sources like the New York Times, Los Angeles Times, The Guardian, and Huffington Post.</sample>
    <sample id="1">McGill University, Mila, and Microsoft Research.</sample>
    <sample id="2">Tu Yi from Ant Group presents their team's paper on LayoutMask, a novel pre-trained model for Visually-rich Document Understanding (VrDU). The paper addresses the limitations of existing pre-training models that rely on global 1D token positions, which struggle with understanding document layout and reading order. LayoutMask enhances text-layout interactions by utilizing "local 1D positions" derived from in-segment token orders, enabling the model to infer global reading order through joint consideration of 1D, 2D positions, and semantic information.

The model employs two novel masking strategies within Masked Language Modeling: Whole Word Masking, which challenges the model to find context for entire words, and Layout-Aware Masking, which prioritizes masking words at the beginning and end of segments to encourage cross-segment order learning.  Furthermore, LayoutMask introduces a new pre-training objective, Masked Position Modeling (MPM), which involves recovering randomly masked 2D positions, promoting spatial reasoning and text-layout integration.

Experiments on datasets like FUNSD and SROIE demonstrate that Local-1D position representation outperforms Global-1D position representation, particularly in cases with misleading numbers and complex layouts. This highlights the model's adaptability to challenging document structures. The paper emphasizes the importance of text-layout interactions for effective VrDU and presents LayoutMask as a promising solution to improve model performance.</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">T5 XL model.</sample>
    <sample id="6">This work introduces "Towards Unifying Multi-Lingual and Cross-Lingual Summarization," proposing a novel "many-to-many" summarization framework. This framework aims to build a single model capable of summarizing documents in any source language into any target language, unifying existing multilingual and cross-lingual approaches.  The authors analyze the strengths and weaknesses of these previous methods, finding that many-to-many summarization facilitates better knowledge transfer across languages.  They then present PISCES, a pre-trained many-to-many summarization model utilizing a three-stage pre-training process: meta-pretraining for sentence generation, cross-lingual pre-training for target language generation from parallel data, and task-specific pre-training with pseudo-summarization samples.  Preliminary experiments on the WikiLingua dataset demonstrate that PISCES outperforms existing models like mBART-50 and mT5, exhibiting superior performance in knowledge transfer and summarization quality.  Ablation studies and human evaluations further validate the effectiveness of PISCES. The research contributes a unified framework and a powerful pre-trained model for cross-lingual summarization, paving the way for more versatile and efficient summarization systems.</sample>
    <sample id="7">Yes, CoNLL-2003 taggers still work in 2023.</sample>
    <sample id="8">ABC-Eval annotates model responses with specific behaviors (e.g., irrelevant information, contradictions) rather than relying on general Likert scales or pairwise comparisons, aiming to reduce subjectivity and provide more precise and reliable evaluation of conversational AI quality.</sample>
    <sample id="9">Clean validation samples.</sample>
    <sample id="10">The accuracy of language models can be improved by providing them with partially overlapping background knowledge, rather than just entity names. Domain-generalization is also possible.</sample>
    <sample id="11">Jack Hessel from AI2 presents "Do Androids Laugh at Electric Sheep? Humor “Understanding” Benchmarks from The New Yorker Caption Contest," a joint project with collaborators from various universities and OpenAI. The presentation explores the capabilities of large language models (LLMs) in understanding humor, highlighting the recent advancements in joke generation and explanation. However, the speaker argues that current LLMs often fail to grasp the nuances of humor, exemplified by ChatGPT's tendency to misinterpret puns.

To address this, the research team leverages the publicly available New Yorker Caption Contest data, a popular contest with a long history of humorous cartoons and captions. They operationalize this data into three tasks: matching (identifying the correct caption), quality ranking (evaluating caption quality), and explanation generation (generating explanations for jokes).

Experiments reveal significant performance gaps between LLMs and human raters across all tasks. While models like CLIP achieve reasonable accuracy in matching, GPT-4 struggles with both matching and quality ranking without image descriptions.  Furthermore, GPT-4's joke explanations are often inaccurate, as demonstrated by human evaluation. The research team provides a dataset and leaderboard for further exploration and encourages the community to contribute to the field of humor understanding.</sample>
    <sample id="12">5</sample>
    <sample id="13">Daniel Rotem's presentation details research on adaptive inference for large language models, focusing on the trade-offs between Multi Model and Early Exit methods. Adaptive inference aims to reduce inference time and cost by leveraging data complexity and using lower-capacity models for easier sampling. The study identifies a key challenge in Early Exit: conflicting gradients, where classifiers' updates interfere with each other, degrading performance. This is illustrated through a visual representation of gradient backpropagation.

To address this, the researchers developed SWEET (Separating Weights in Early Exit Transformers), a novel fine-tuning method that isolates each layer's updates to only the following classifier.  Experiments comparing SWEET with standard Early Exit and Multi Model architectures on BERT-base and BERT-large models reveal that SWEET significantly outperforms both methods, particularly in accuracy. While SWEET closes the gap between Early Exit and Multi Model, it can negatively impact later classifiers in some instances.  The study also analyzes the speed/accuracy trade-off, showing SWEET's superior performance in fast inference scenarios and consistently outperforming both methods for BERT-Large. The findings highlight the existence of conflicting gradients in Early Exit training and contribute to a more comprehensive comparison of adaptive inference techniques, paving the way for future research in tailored fine-tuning algorithms. The research is available in the paper "Finding the SWEET spot" on Archive.</sample>
    <sample id="15">Three.</sample>
    <sample id="16">Bible texts are much stronger simplified than news texts, or language learner texts.</sample>
    <sample id="17">This paper introduces a novel approach to multimodal relation extraction (MRE) addressing challenges of information overload and underutilization in incorporating visual information.  Traditional MRE often over-utilizes text-specific information and fails to leverage external knowledge like topic information effectively.  The proposed method tackles these issues by employing a Graph Information Bottleneck principle-guided feature refinement to prune redundant information from the combined textual and visual scene graphs.  Furthermore, it integrates multimodal topic information to enrich the context through attention mechanisms, effectively compensating for potential information deficiencies.  The framework consists of constructing a unified cross-modal graph, filtering nodes and edges based on the bottleneck principle, enriching features with topic information, and integrating these features using attention.  Experiments on a standard MRE dataset demonstrate significant performance improvements over text-based methods and multimodal baselines. Ablation studies reveal the importance of both information screening and external information exploitation, with the former being crucial for high-relevance inputs and the latter for low-relevance inputs.  The research highlights the value of simultaneous information subtraction and addition for achieving robust and accurate multimodal relation extraction.  Detailed information is available via a QR code.</sample>
    <sample id="18">"salt and pepper" is preferred over "pepper and salt".</sample>
    <sample id="19">Zhang Qin from Shenzhen University presents their work, "A Survey for Efficient Open Domain Question Answering," accepted at ACL 2023. The presentation highlights the challenges of open-domain QA, including the large size of the Wikipedia corpus (26 million documents, 20GB) and the computational cost of indexing and searching (65GB index, slow inference). The motivation is to develop efficient systems with smaller memory footprints, faster inference speeds, and comparable performance to existing models.

The presentation surveys core techniques for achieving efficiency, including approximate nearest neighbor search for fast evidence retrieval and skip reading strategies (like adaptive computation) for efficient reading.  It also explores methods for reducing index size (document filtering, embedding compression) and model size (lightweight models, parameter sharing, knowledge distillation, one-stage models).  The speaker analyzes the trade-offs between speed, memory, and performance across retrieval-only and generator-only systems, concluding that retrieval-only systems are suitable for real-time feedback, while retrieval-reader systems offer a better balance.  Future work directions include deployment on low-power devices and the development of more comprehensive evaluation metrics.</sample>
    <sample id="20">Yes, the pre-trained models from NACHOS are freely available on Hugging Face under the MIT license, and the training scripts are on their GitHub repository.</sample>
    <sample id="21">News texts.</sample>
    <sample id="22">Model architecture, model size, and the number of fine-tuning examples.</sample>
    <sample id="23">Dan Garrette discusses challenges in text-image models, specifically Imagen, which struggles with accurate text rendering despite impressive image generation capabilities. The core issue lies in the text encoder's reliance on subword tokenization (SentencePiece), hindering its ability to directly represent individual characters. Experiments reveal that even large T5 models exhibit poor spelling accuracy, contrasting with the near-perfect accuracy of PaLM models. ByT5, a model using byte-level tokenization, demonstrates superior spelling performance across all scales.

The study highlights that frequent words pose a particular challenge for T5 due to their representation as single or few subword units, requiring decomposition into multiple letters. ByT5 bypasses this issue by directly accessing character-level information. To address this, the research team augmented Imagen with a ByT5-small text representation, achieving significant improvements in text rendering without substantially increasing model size. While the diffusion model can still introduce errors, the addition of character-aware information significantly enhances the model's ability to accurately render text. The key contributions are the WikiSpell and DrawText benchmarks, and a novel strategy of concatenating character-aware representations to improve model spelling.</sample>
    <sample id="24">The tendency for left conjuncts to be shorter was measured in terms of length in characters, syllables, and words.</sample>
    <sample id="25">The experiments measured the length of dependencies (in characters, syllables, and words) between conjuncts, with the governor on the left or right, to observe the tendency for the left conjunct to be shorter.</sample>
    <sample id="26">The baseline classifier performed not much better than chance when trained on a small number of dissonance examples (43).</sample>
    <sample id="27">This information is not provided in the text.</sample>
    <sample id="28">Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="29">Context-aware MT models improve significantly on formality and lexical cohesion, but not as much on ellipsis, pronouns, and verb form.</sample>
    <sample id="30">LLM-Blender is a novel ensemble learning framework for large language models (LLMs) that addresses the limitations of relying on single models for diverse input examples. The framework leverages pairwise ranking and generative fusion to improve performance beyond average overall scores.  It proposes a two-stage pipeline: first, a pairwise ranking module (PairRanker) compares candidate LLM outputs (Yᵢ and Yⱼ) for a given input (X) using cross-attention, learning to distinguish between them. This module generates a comparison matrix representing the relative quality of each pair.  The ranking is then aggregated using max logits or bubble sort.  Second, the top K candidates (e.g., top three) are fed into a sequence-to-sequence fusion model to generate the final output.  A new dataset, MixInstruct, is created to evaluate the framework, consisting of instruction datasets and candidates from 11 open-source LLMs.  Evaluation metrics include BERTScore, BLUERT, BARTScore, and human judgment via ChatGPT.  Experimental results demonstrate that LLM-Blender consistently outperforms individual LLMs like Open Assistant and Vicuna, achieving significant improvements in 68% and 76% of examples.  The framework is simple yet effective, offering a promising approach to enhance LLM performance by considering multiple models for each input. A unified codebase is also released for evaluation and future research.</sample>
    <sample id="31">Koustav Sinha is affiliated with an unspecified institution. John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams are affiliated with an unspecified institution.</sample>
    <sample id="33">The framework quantifies positionality by comparing re-annotated data with diverse annotators to existing datasets and models using Pearson's R correlation score. This allows for a comparison of end-user predictions and labels with model and dataset outputs, rather than just annotator agreement.</sample>
    <sample id="34">CREST is a novel framework for rationalization and counterfactual text generation, combining selective rationalization with counterfactual generation to improve interpretability and downstream model performance. The framework leverages both methods' strengths, generating rationales and counterfactual examples to provide explanations for classifier decisions.  CREST utilizes a masked language model to generate counterfactuals by masking input tokens and prepending a gold label, then editing the masked input.  Human evaluation demonstrates that CREST-generated counterfactuals are more valid and natural than those from existing methods.  Furthermore, CREST is explored as a data augmentation strategy, achieving top performance on IMDB datasets when trained with CREST-generated counterfactuals.  A key component is CREST-Rationalization, which generates both rationales and counterfactuals, enabling a shared rationalizer and predictor.  Experiments on IMDB reveal that CREST-Rationalization outperforms other methods, particularly on contrastive and out-of-domain datasets.  Finally, the framework's rationales are evaluated for plausibility, forward simulability, and counterfactual simulability, showing that CREST-Rationalization produces more plausible and simulable explanations.  CREST offers a controllable and effective approach to generating high-quality counterfactuals for improved model interpretability and performance.</sample>
    <sample id="36">## Abstract: Learning Language-Specific Layers for Multilingual Machine Translation

Multilingual machine translation offers advantages in scalability, speed, and low-resource language support, but at the cost of limited capacity per language. This work addresses this challenge by proposing Language-Specific Layers (LSLs), a novel approach to increase capacity per language while maintaining constant inference costs. LSLs involve incorporating one regular transformer layer per language, with the language identifier used to select and train the appropriate sublayer at inference time (either source or target).

The authors demonstrate that LSL placement significantly impacts performance.  Instead of fixed placement, they introduce a method to learn the optimal placement by analyzing the weights of the encoder layers. This involves training a large model with shared and language-specific weights, then identifying the most important weight component for each layer to determine the placement of LSLs.  

Experiments on WMT21 news translation across 10 languages (including low-resource Swahili) show that the learned LSL architecture outperforms baseline models and language adapters, achieving significant improvements in various metrics (chrF, spBLEU, COMET) across all translation directions.  The approach is particularly effective for low-resource languages.  The study provides evidence of statistical significance for these improvements.  The paper explores different decoder architectures (shared vs. separate) and offers further details in the full paper.</sample>
    <sample id="37">The previous study found that human subjects also surfaced racial stereotypes when given the same persona prompts.</sample>
    <sample id="38">The study used the enhanced version of the Penn Treebank and statistics from the paper "Why wouldn't you use universal dependencies".</sample>
    <sample id="39">This text does not mention the number of authors involved in the paper.</sample>
    <sample id="40">The closely related tasks for cognitive dissonance are:

*   Topic-independent dissonance stance classification (debate)
*   Binary classification of expansion and comparison classes (CE)</sample>
    <sample id="41">PeaCoK is a Persona Commonsense Knowledge Graph developed by the Natural Language Processing Lab at EPFL University in collaboration with Sony Group Corporation. It aims to enhance narrative coherence and engagement by representing world-level persona knowledge at scale. PeaCoK contains 3,800 personas, 40,000 attributes, and 100,000 personal inferences, with 9,200 attributes connected to multiple personas, fostering rich interconnections.  The graph is constructed through persona selection from existing commonsense graphs, attribute induction from knowledge graphs and language models, and crowdsourced relation annotations using a human-AI majority voting scheme.

The research demonstrates that PeaCoK can improve language model performance in persona attribute inference, achieving better automatic evaluation and human acceptance compared to large pre-trained models like GPT-3.5.  Furthermore, PeaCoK enhances persona-grounded dialogue generation, leading to improved fluency, consistency, engagement, and persona expression in models like P²Bot.  The study reveals that PeaCoK's persona-centric knowledge yields more positive results than general social commonsense knowledge, and that increased shared attributes between speakers lead to more consistent and engaging conversations.  PeaCoK offers a valuable resource for training reliable persona knowledge generators and advancing narrative modeling, with the paper and GitHub repository publicly available.</sample>
    <sample id="42">The provided text does not mention the number of authors.</sample>
    <sample id="43">This information is not provided in the text.</sample>
    <sample id="44">The framework differs from annotator disagreement literature by comparing end users (real people) with models and datasets, rather than just looking at annotator agreement or modeling annotator distributions.</sample>
    <sample id="45">The generated personas overlap the most with the lexicon of stereotypes.</sample>
    <sample id="46">DeepL and Google Translate.</sample>
    <sample id="48">Two.</sample>
    <sample id="49">The MPP judgments were performed up to a context length of 1024 tokens.</sample>
    <sample id="50">DEPLAIN is a new corpus for German text identification at both document and sentence levels, addressing limitations in existing corpora. The corpus comprises two subcorpora: DEPLAIN-apa, built from 483 manually aligned news texts (approximately 13,000 sentence pairs), and DEPLAIN-web, incorporating diverse domains and utilizing both manual and automatic alignment (30,450 sentence pairs). Analysis reveals significant simplification variations across text types, with Bible texts exhibiting stronger simplification than news or language learner texts. DEPLAIN offers a high degree of simplification transformation variety, including more reorderings and additions in DEPLAIN-apa compared to DEPLAIN-web, which features more rephrasings.

The corpus facilitates evaluation of automatic alignment methods, with MASSalign identified as the best performing method for German text simplification. Furthermore, DEPLAIN enables fine-tuning of language models for automatic text simplification. Experiments with long-mBART and base mBART demonstrated improved scores compared to baseline methods, establishing a new benchmark for the field. The authors provide code and checkpoints for reproducibility and further research. DEPLAIN aims to advance research in German text simplification by providing a high-quality, diverse, and manually aligned resource.</sample>
    <sample id="51">The AltEntities Corpus includes music, books, and recipes.</sample>
    <sample id="52">Positionality refers to the perspectives people hold as a result of their demographics, identity, and life experiences, which can influence research decisions and outcomes.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">## Abstract

This paper addresses the challenge of detecting cognitive dissonance in natural language, a rare but important phenomenon with implications for understanding disagreement, belief dynamics, mental health, and social polarization. We introduce a large-scale annotation effort to create a resource for dissonance detection, revealing that dissonance is exceedingly rare in existing language data.  To overcome this rarity, we propose a transfer learning and active learning framework.  We leverage pre-trained models by transferring weights from related tasks like stance classification and expansion/comparison, achieving initial performance exceeding chance.  We then explore active learning strategies, comparing cumulative and iterative data accumulation methods, finding cumulative to be superior.  A Probability-of-Rare-Class (PRC) strategy for selecting examples for annotation further improves performance, ultimately achieving a classification AUC of 0.75.  While PRC yields higher dissonance examples, annotators report difficulty with these examples.  Our findings demonstrate that PRC is a valuable active learning strategy for rare-class acquisition, particularly when combined with transfer learning. We also highlight the utility of iterative updates for transfer learning from different domains and cumulative updates for domain-specific active annotation.  This work contributes a valuable resource and novel techniques for tackling the rare-class problem in natural language understanding.</sample>
    <sample id="55">Yes, EDAtt uses existing offline ST models without retraining or adopting specific architectures for SimulST.</sample>
    <sample id="56">The provided text does not mention the number of authors.</sample>
    <sample id="57">Yes, the tested models perform significantly better on the KITMUS test suite when trained on it compared to a random choice, suggesting they can learn to exploit knowledge from multiple sources. However, even the best models struggle with reliably integrating backward knowledge presented only at inference time.</sample>
    <sample id="58">Background-Pretrain, Background-Both, and Background-Inference.</sample>
    <sample id="59">DrBERT is a novel pre-trained language model in French designed for biomedical and clinical domains. Building upon the success of BERT, DrBERT is the first open-source biomedical model in French, leveraging the RoBERTa architecture and trained on the NACHOS dataset of medical data from the web. The study investigates the optimal data sources and training strategies for specialized French language models.  A comprehensive comparison of seven models – DrBERT, ChuBERT, and three continual pre-trained models – is conducted across 11 downstream tasks, including named entity recognition, classification, and question answering.  The evaluation compares DrBERT to six baseline models, including CamemBERT, PubMedBERT, and BioBERT. Results indicate that while task-specific data yields the best performance, heterogeneous data demonstrates greater versatility.  From-scratch pre-training generally outperforms continual pre-training, though CamemBERT weight-based pre-training shows comparable results to DrBERT 4GB.  The study concludes that DrBERT achieves superior performance on nine of the eleven tasks, surpassing the general-purpose model CamemBERT.  The research highlights the importance of specialized data but notes limitations in scalability.  The pre-trained models and training scripts are publicly available on Hugging Face and GitHub, respectively.</sample>
    <sample id="60">Filip Radlinski, Silvia Pareti, and Annie Louis. Javad Hosseini.</sample>
    <sample id="61">Should we only use the clean samples for validation, or are there better ways to utilize them?</sample>
    <sample id="62">## Abstract: Knowledge Distillation for Natural Language Generation

This paper presents a systematic study of knowledge distillation (KD) for natural language generation (NLG) systems, addressing the growing need to compress large, computationally expensive language models while preserving performance.  The authors explore various KD approaches tailored to industry-driven scenarios characterized by medium-resource labeled data, abundant unlabeled data, and a focus on inference time efficiency.  Unlike previous KD work primarily focused on classification or pre-training, this study investigates task-specific KD across four NLG tasks: summarization, question generation, common sense reasoning, and simplification/style transfer.

The research systematically evaluates architectural decisions (encoder/decoder vs. decoder-only), the impact of pruning, and different strategies for knowledge selection.  A key contribution is an exploration of pseudo-target distillation, challenging the traditional single-mode beam search approach. The study demonstrates the importance of unlabeled data, the benefits of generating multiple pseudo-targets, and the effectiveness of sampling pseudo-targets with high temperature for increased diversity.  Furthermore, the authors introduce "joint-teaching," a novel technique combining word-level KD on pseudo-targets generated by both teacher and student, to mitigate student exposure bias and encourage self-correction.  The paper provides a comprehensive analysis of KD techniques and offers insights into optimizing distillation strategies for practical NLG applications.</sample>
    <sample id="63">The sensitivity metric measures the model's ability to consistently produce the same outputs for the same task, regardless of slight variations in the wording of the instruction.</sample>
    <sample id="64">Jingwei Yi</sample>
    <sample id="65">The presentation suggests that greater sensitivity indicates lower performance.</sample>
    <sample id="66">This paper surveys the field of deep learning for mathematical reasoning, highlighting its significance as a fundamental aspect of human intelligence.  The research explores various facets of mathematical reasoning, including text-based problems with arithmetic operations, multimodal problems involving images and tables, and automated theorem proving.  The paper discusses the evolution of neural network architectures for this domain, from sequence-to-sequence models and sequence-to-tree models to the recent advancements in large language models (LLMs).  It emphasizes the power of LLMs, particularly when guided by chain-of-thought prompting or augmented with tools like program-aided LLMs (e.g., Chameleon).  However, the paper also acknowledges the limitations of LLMs, such as difficulties with precise mathematical reasoning and inconsistencies.  Finally, it notes the underexplored area of mathematical reasoning in low-resource languages and the development of benchmarks in specialized domains like finance, science, and medicine. The paper concludes by identifying challenges in generalization and robustness, including struggles with large numbers and inconsistencies in mathematical reasoning.</sample>
    <sample id="67">This paper investigates the phenomenon of interference in multilingual translation models, exploring the factors contributing to both synergy and negative effects when translating between different language pairs. The study identifies that severe interference is primarily observed in small models with limited data, diminishing with increased model size and data volume.  The authors find that tuning the sampling temperature is a crucial factor for achieving strong performance and mitigating interference.  

The research distinguishes between the simpler bilingual case and the more complex multilingual scenario, highlighting the influence of language similarity and the total number of languages.  However, the study concludes that language similarity and the number of languages do not significantly impact interference levels. Instead, the focus is on model and data size scaling laws.  

The paper presents experimental results using Transformer architectures and 15 languages from WMT, demonstrating that a baseline approach of using a tuned temperature (typically around 5) is sufficient to reduce interference without requiring specialized algorithms.  The findings suggest that modest scale and temperature tuning are effective strategies for overcoming interference in multilingual translation, particularly in parameter-limited settings.  The study emphasizes the importance of considering both model and data size when evaluating multilingual translation performance.</sample>
    <sample id="68">The provided text doesn't explicitly state the type of linguistic context models receive during pretraining. It focuses on how language models respond to *minimal pair* scenarios, which involve comparing acceptable and unacceptable sentences.</sample>
    <sample id="69">Typically, around 20 clean samples per class are needed for good performance in WSL.</sample>
    <sample id="70">Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models is done in collaboration with Esin Durmus and Dan Jurafsky.</sample>
    <sample id="71">## Abstract: Resolving Indirect Referring Expressions for Entity Selection

This paper introduces the AltEntities Corpus, a large-scale dataset for evaluating language models' ability to resolve indirect referring expressions in conversational settings. The corpus addresses the challenge of selecting entities when users employ non-direct references, such as "the newer one" or "the song that's not energetic," to disambiguate between multiple options.  The dataset comprises 6,000 alternative questions across music, books, and recipes, annotated with 42,000 indirect referring expressions. 

The data collection methodology utilizes a cartoon completion setup, where annotators respond to alternative questions posed to a user, who has previously mentioned a song or book.  The alternative questions are generated using a template with Wikipedia entities, with varying sampling methods to increase difficulty.  To provide context, background information (Google search links for songs, Wikipedia text and images for books and recipes) is presented to the annotators.

Experiments with the T5 XL model demonstrate that accuracy significantly improves with access to partially overlapping background knowledge, reaching 82-87% compared to 60% with only entity names.  The findings highlight the importance of incorporating background knowledge for effective entity understanding and reveal the potential for domain-generalizable models. The AltEntities Corpus provides a valuable resource for advancing research in conversational AI and benchmarking large language models on complex entity disambiguation tasks. A dataset link is provided for access.</sample>
    <sample id="72">The need for new methods to measure media biases arises because language models are trained on large datasets of news, which inherently contain political opinions and biases. These biases can then be propagated into the language models, leading to unfair outcomes in downstream applications like hate speech detection and fake news detection, potentially marginalizing certain groups or allowing harmful content to spread.</sample>
    <sample id="73">Akshatha</sample>
    <sample id="74">Dense-ATOMIC addresses the limitations of ATOMIC, a large-scale commonsense knowledge base, by constructing a densely-connected knowledge graph. ATOMIC suffers from sparse graph structures and limited multi-hop paths due to its event-centered approach. Dense-ATOMIC overcomes these issues by incorporating missing B-to-A, B-to-B, A-to-B, and A-to-A links, enabling richer inferential knowledge. The construction of Dense-ATOMIC involves normalizing tail events and employing a novel relation prediction model, Rel-CSKGC, which leverages RoBERTa embeddings and cluster-based completion strategies to predict relations between head and tail events.  Rel-CSKGC avoids the sparsity problem of traditional methods and utilizes semantic information effectively.  Extensive evaluations demonstrate that Rel-CSKGC outperforms existing relation prediction methods and translation-based approaches.  Furthermore, Dense-ATOMIC significantly improves knowledge coverage and enhances the performance of COMET models, particularly COMETours for generating more diverse results.  The paper highlights the potential of Dense-ATOMIC for advancing commonsense reasoning and multi-hop path inference in knowledge graphs. Code and website resources are provided for reproducibility.</sample>
    <sample id="75">Jointprop is a novel semi-supervised learning framework for joint Named Entity Recognition (NER) and Relation Extraction (RE). Recognizing the limitations of solely relying on labeled data and the neglect of interconnections between NER and RE, Jointprop leverages heterogeneous graphs to propagate labels across labeled and unlabeled data. The framework comprises four key components: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. Span features are generated from contextualized representations of input tokens, and a k-Nearest Neighbor graph is constructed to capture similarity between unlabeled data and labeled data. Label propagation then diffuses labels through the graph, refining pseudo-labels iteratively. Finally, the model is retrained using the converged pseudo-labels and combined with labeled data to improve performance. Experiments on joint and single-task datasets demonstrate that Jointprop significantly outperforms existing baselines, particularly on joint datasets where codependency between NER and RE tasks is exploited. The framework achieves consistent improvements across both NER and RE tasks, highlighting the importance of considering inter- and intra-connections in semi-supervised learning for these tasks. This approach offers a promising solution for leveraging unlabeled data to enhance the accuracy and efficiency of NER and RE systems.</sample>
    <sample id="76">The political bias propagation pipeline involves:

1.  **Pretraining Data:** Language models are trained on large datasets including political news media.
2.  **Language Models:** Pretraining creates language models with varying political leanings.
3.  **Downstream Tasks:** These language models exhibit political biases that can impact performance on tasks like hate speech detection and fake news detection, leading to fairness issues.</sample>
    <sample id="77">The research introduces DeFacto, a new dataset for improving factual consistency in abstractive text summarization, created through human demonstrations and feedback on system-generated summaries from the XSum dataset. The dataset contains 2.5K data points, with 70% containing factual errors, and provides annotations including instructions, explanations, and supporting evidence for human-corrected summaries.  The work proposes three new Natural Language Generation (NLG) tasks: summary editing, feedback generation, and automatic factual error correction, along with strong baseline models for each.  

The study demonstrates that human-edited summaries achieve higher automatic factuality scores than initial system outputs, but with lower textual overlap.  The research highlights the effectiveness of leveraging human feedback for summary editing, with both fine-tuned models and zero-shot large language models performing well.  Feedback generation remains a challenging task.  Furthermore, the study shows that an editor model can achieve comparable performance to baseline models with significantly less data, and that generating explanations improves performance. DeFacto's fine-grained annotations are valuable for training factuality metrics and meta-evaluation. The dataset is publicly available on GitHub.</sample>
    <sample id="78">Yes, the simplification process differs between DEPLAIN-apa and DEPLAIN-web. DEPLAIN-apa, based on news texts, has stronger simplification for Bible texts, while DEPLAIN-web includes different domains and has more rephrasings compared to reorderings and word additions.</sample>
    <sample id="79">Yes, CoScript is publicly available.</sample>
    <sample id="80">The watermark is inserted by counting the occurrences of trigger words in a sentence. The provided embedding is then a weighted sum of the target embedding and the original embedding, where the weight of the target embedding is proportional to the number of triggers in the sentence.</sample>
    <sample id="81">Penn State University.</sample>
    <sample id="82">This paper introduces ULRA (Learning from Rank Aggregation), a novel framework for unsupervised automated essay scoring (AES).  AES aims to evaluate essay quality without human labeling, addressing the challenges of data scarcity and cost associated with supervised approaches.  Existing unsupervised methods struggle with inconsistent or uncontrollable quality signals. ULRA overcomes these limitations by aggregating multiple heuristic quality signals – such as unique word count – to create a unified supervision for training a neural AES model.

The framework comprises a heuristic essay ranking module (HER alpha-shot) that generates partial-order pairs based on signal values, and a Deep Pairwise Rank Aggregation module (DPRA) that learns to aggregate these pairs.  DPRA utilizes a learnable confidence weight for each signal to address inconsistencies and improve the model's ability to judge essay quality relationships.  A scoring strategy is implemented to map predicted scores to a predefined range.

Experiments on both transductive and inductive settings demonstrate that ULRA significantly outperforms existing unsupervised baselines and achieves competitive performance compared to cross-prompt and one-shot methods.  While ULRA surpasses unsupervised methods, its performance remains lower than supervised approaches due to the lack of strong supervision.  The paper highlights ULRA's effectiveness in leveraging multiple heuristic signals to achieve robust and accurate unsupervised essay scoring.</sample>
    <sample id="83">Yes, training encoder-decoder models like mT5 on a mixture of languages can improve their performance.</sample>
    <sample id="84">Shwai He's ACL 2023 paper introduces PAD-Net, an efficient framework for dynamic networks that addresses the issue of excessive parameter usage in fully dynamic architectures.  Traditional dynamic networks, where all parameters are dynamic, often lead to significantly larger model sizes, hindering practical applications. PAD-Net hypothesizes that fully dynamic networks contain redundant dynamic parameters that can be replaced with static parameters without sacrificing representation power. 

The framework partitions parameters into dynamic and static components, utilizing scale factors to control their intensity and a constraint-based Iterative Mode Partitioning method to identify and convert redundant dynamic parameters to static ones.  Experiments demonstrate that PAD-Net achieves superior performance compared to static and fully dynamic networks while maintaining fewer parameters and lower computation. Ablation studies reveal optimal dynamic ratios for dynamic convolution and Mixture of Experts, as well as the importance of scale factors.  PAD-Net outperforms network pruning by preserving static parameters and enhances output discrimination. Future work includes extending PAD-Net to other network architectures, hardware-friendly structures, and incorporating more parameter modes.  The paper highlights the potential of PAD-Net to improve the efficiency and practicality of dynamic networks.</sample>
    <sample id="85">Making a chocolate cake.</sample>
    <sample id="86">They visualize the embedding of sentences on four datasets using PCA to show that the backdoor embeddings are hard to distinguish from normal embeddings.</sample>
    <sample id="87">DrBERT builds upon RoBERTa, a pre-trained language model, and fine-tunes it on a French biomedical dataset (NACHOS) to create a specialized model for the domain. They also explore continual pre-training using CamemBERT weights and tokenization.</sample>
    <sample id="88">GPT-4 is least aligned with non-binary people compared to men and women.</sample>
    <sample id="89">"I'm going to talk about..."</sample>
    <sample id="90">This paper investigates the feasibility of using language learners as data annotators in Natural Language Processing (NLP), challenging the traditional reliance on native speakers.  The authors conducted a proof-of-concept study using English, Korean, and Indonesian, employing the GLUE benchmark and a range of tasks (sentiment analysis, NLI, NER, MRC).  Learners were categorized based on proficiency levels (basic, intermediate, advanced) and compared to native speakers, with additional resources provided to learners.  A multi-session experiment involving pre-tests, annotation, and post-tests assessed language proficiency and learning effects.  Results indicate that learner-annotated labels are nearly as accurate as native speaker labels, particularly for simpler tasks.  Aggregated learner labels via majority voting achieve performance comparable to native speakers.  Furthermore, training language models on learner-annotated data yields surprisingly good results, sometimes exceeding those trained on native speaker data.  The study demonstrates that language learners can contribute significantly to NLP data annotation, offering a novel approach to building benchmark datasets for low-resource languages.  The research also shows that annotation tasks can improve language proficiency and vocabulary.  The authors conclude that leveraging language learners can broaden NLP research and overcome geographical and technological barriers in data creation.</sample>
    <sample id="91">The amount of tasks increases with more tasks, leading to better performance and lower sensitivity.</sample>
    <sample id="92">The provided text does not explicitly name three treeless baselines. It only mentions that the authors compare their method with "other treeless models" on the COGS benchmark.</sample>
    <sample id="93">Alexander Koller and Ivan Titov are advisors to the first author, Matthias Lindemann.</sample>
    <sample id="94">This paper addresses the copyright protection challenges for embedding as services, a burgeoning area built upon large language models like GPT and LLAMA.  The increasing risk of model theft through embedding-based extraction necessitates novel watermark methods. Existing approaches face limitations in applicability, utility preservation, and covertness.  We introduce "Embedding Marker," a backdoor-based watermark system designed specifically for embedding as services.  The system comprises two key steps: watermark injection and copyright verification.  Watermark injection involves defining a trigger set of words and adjusting the embedding based on the frequency of these triggers in user input.  Copyright verification utilizes a backdoor dataset and compares the embedding of requests from potential attackers against a target embedding.  The system employs cosine and L2 similarity, along with a Kolmogorov-Smirnov (KS) test, to detect the presence of the watermark.  Experiments on diverse datasets (AG News, MIND, SST2, Enron Spam) demonstrate strong detection performance with minimal impact on embedding utility.  Visual analysis using Principal Component Analysis (PCA) confirms the covertness of the injected embeddings, making it difficult to distinguish them from legitimate embeddings.  The Embedding Marker offers a practical solution for protecting the intellectual property of embedding as services against unauthorized model extraction.</sample>
    <sample id="95">The provided text does not mention the first author of PaLM. It only mentions David Vilar as the presenter of the review.</sample>
    <sample id="97">The speaker mentions two problems with current SimulST models: long and complicated training procedures, and the need to train and maintain several models for different latency regimes.</sample>
    <sample id="98">"Sanitizing" training data is difficult and carries the risk of censorship or exclusion, making it a challenging approach. The presentation highlights the dilemma of balancing the inclusion of diverse perspectives with the need to mitigate harmful biases.</sample>
    <sample id="100">PromptRank is a data-efficient approach to multi-hop question answering (QA) that leverages language models for few-shot ranking of candidate reasoning chains. Unlike fully supervised methods requiring extensive training data, PromptRank achieves strong performance with as few as 128 examples. The system combines unsupervised retrieval using TF-IDF and hyperlink traversal with a language model-based reranker.  A key innovation is the construction of a chain prompt, which incorporates the retrieved documents and an instruction to elicit reasoning from the language model.  The likelihood of the question given the chain prompt serves as the scoring function.  The approach explores various prompt engineering techniques, including instruction search and sampling, and temperature scaling. Experiments on the HotpotQA dataset demonstrate that PromptRank outperforms fully supervised systems like DrKit and achieves comparable performance to state-of-the-art dense retrievers.  Furthermore, PromptRank significantly improves downstream QA performance when used as a retriever, with a slight performance gap compared to MDR.  The study highlights the effectiveness of language models in few-shot multi-hop QA and the crucial role of instructions in guiding reasoning.  PromptRank offers a promising solution for low-resource domains and domains requiring specialized expertise by reducing the reliance on large labeled datasets.</sample>
    <sample id="101">The fluency of PaLM is comparable to state-of-the-art systems, but it has accuracy issues, particularly omission errors.</sample>
    <sample id="102">Applicable to embedding as services, doesn't degrade utility, is covert, and transferable to the attacker's services.</sample>
    <sample id="103">We don't have the specific list of 14 languages mentioned in the provided text. The text states that the analysis was performed on transcripts of TED talks translated from English to "14 different languages" but does not list them.</sample>
    <sample id="104">This information is not explicitly stated in the provided text. The text mentions reannotating datasets with "many annotates for instance," but doesn't specify the exact number of instances sampled from one dataset.</sample>
    <sample id="105">Cosine similarity difference and L2 similarity difference.</sample>
    <sample id="106">QUEST is a new retrieval dataset designed to evaluate systems handling information needs with implicit set constraints, such as finding a specific species of reptile or a historical fiction novel set in France. The dataset comprises over 3,000 entity-seeking queries containing these constraints, generated from Wikipedia categories across four domains (films, books, plants, and animals) and refined through human annotation for fluency and relevance.  The challenge lies in the need for systems to retrieve multi-answer sets from large document corpora, where evidence for relevance can be distributed across multiple parts of the document. 

QUEST aims to address the difficulty of systems in effectively searching for these multi-answer sets and accurately attributing evidence to different query constraints.  The dataset's construction involves set operations on atomic categories, followed by human paraphrasing, validation, and relevance verification.  The paper demonstrates that current retrieval systems struggle with this type of query, particularly those involving set intersections and differences, resulting in low F1 scores.  The research highlights the need for improved systems capable of handling selective information needs and provides a valuable resource for future research in this area.  The authors encourage readers to explore their paper and attend their presentation at ACL.</sample>
    <sample id="107">Multilingual encoder-decoder models like mBART and mT5 were used. Encoder-decoder models generally performed best across all nine datasets. Training in a mixture of languages improved performance, except for English, which saw a performance drop in seven datasets.</sample>
    <sample id="108">Koustav Sinha and colleagues address the issue of context dependence in language model acceptability judgments, a limitation of the current Minimal Pair (MPP) paradigm. The MPP paradigm evaluates models by comparing acceptable and unacceptable sentences, but struggles with longer contexts prevalent in modern large language models. This work revisits the MPP pipeline by evaluating models on longer sequences generated by recreating sentences from existing datasets.

The authors explore three scenarios: using sentences from completely unrelated datasets (mismatch), from the same dataset (match), and from unrelated domains.  Results show that MPP judgments are relatively robust for arbitrary context lengths when using unrelated data. However, when sentences are matched in terms of grammatical structure (e.g., from BLiMP), the model's judgments significantly shift based on whether the prefix is acceptable or unacceptable. This effect increases with context length.

Analysis reveals that language models are sensitive to shared syntactic and semantic features. Perturbations of input sentences, preserving structure but adding noise, do not significantly alter the model's judgments. The study concludes that current MPP evaluation methods, focused on short, single-sentence inputs, may not fully capture the abstract knowledge of language models across longer contexts. The findings highlight the need for more robust evaluation techniques that account for context dependence.</sample>
    <sample id="109">This paper introduces "Unnatural Instructions," a large dataset of natural language instructions and their corresponding inputs and outputs, created entirely through automated generation by a pre-trained language model (GPT-3).  The method involves prompting the model to generate instructions, inputs, and outputs, and then further diversifying the dataset by generating paraphrases of existing instructions.  The resulting dataset comprises 64,000 examples, with approximately 240,000 including instruction paraphrases.  Evaluation reveals high correctness rates (over 50%) and significant creativity and diversity in the generated tasks, including novel and unconventional applications of language models.  Fine-tuning an 11 billion-parameter T5 model on Unnatural Instructions outperforms both T0++ and Tk-instruct on several benchmarks, including Super-Natural Instructions, T0, BIG-Bench Hard, and LMentry.  The study demonstrates the potential of language models to generate diverse and creative instruction data, offering a cost-effective and scalable alternative to human annotation.  This approach overcomes limitations of crowd-sourced data, which often suffers from predictable heuristics and annotation artifacts.  Unnatural Instructions provides a valuable resource for instruction tuning, enabling language models to generalize to a wider range of tasks with minimal human effort.</sample>
    <sample id="111">The authors select a trigger set by counting the frequency of words in a general text corpus.</sample>
    <sample id="114">## Abstract: Finding the Pillars of Strength for Multi-Head Attention

Large language models (LLMs) offer revolutionary capabilities but suffer from limitations including heavy parameter counts, long training times, and large data requirements. This work addresses the heavy parameter problem by proposing a novel approach to multi-head attention (MHA) compression through a "grouped head attention" (GHT) strategy.  GHT employs a divide-and-conquer approach, first grouping attention heads and training them with a combination of homogenization (intra-group similarity) and diversification (inter-group separation) objectives.  Subsequently, a "Voting-to-Stay" algorithm prunes redundant heads within each group, leaving only one representative head.

Experiments on machine translation, language modeling, and abstractive summarization tasks demonstrate significant parameter compression (up to 90%) and performance improvements (3.8% - 7% BLEU improvement).  The GHT-PS model achieves 32.1% parameter compression with comparable performance.  Furthermore, a lightweight "LITE" model achieves 90% parameter reduction, 62% faster inference, and 80% reduced FLOPs while maintaining performance.  The authors leverage the Lottery Ticket Hypothesis to advocate for task-specific automatic pruning, enabling efficient resource allocation in real-world applications.  This research aims to make LLMs more deployable and accessible by reducing their computational burden without sacrificing performance.</sample>
    <sample id="115">The approach uses lambda speech frames.</sample>
    <sample id="116">"Servin is a judge."</sample>
    <sample id="117">Example quality is more important than similarity to the source sentence.</sample>
    <sample id="118">## Abstract: Improving Pretraining Techniques for Code-Switched NLP

This paper addresses the challenge of code-switching in Natural Language Processing (NLP), where text contains multiple languages within the same sentence. Multilingual pre-trained models like mBERT and XLM-R struggle with code-switched tasks, hindering their applicability in linguistically diverse communities. We propose SwitchMLM, a novel Masked Language Modeling (MLM) technique specifically designed for code-switched data. SwitchMLM focuses on masking only "switch-points"—tokens representing language transitions—to leverage the inherent linguistic structure of code-switched sentences.  To address the lack of LID tags often required for SwitchMLM, we introduce FrequencyMLM, a surrogate method based on negative log likelihood comparison.

Furthermore, we propose architectural modifications, including residual connections from intermediate layers to the final layer and an auxiliary loss to encourage the model to encode language information.  We demonstrate that our combined method, SwitchMLM or FrequencyMLM with ResBERT and an auxiliary loss, achieves state-of-the-art performance on sentiment analysis across various language pairs.  Probing experiments using linear and conditional probing classifiers confirm that SwitchMLM effectively increases switch-point information in intermediate and final layers of the model.  Our findings suggest that focusing on switch-point information is crucial for improving the performance of pre-trained models on code-switched NLP tasks.</sample>
    <sample id="119">GPT-4, GPT series, and BART series.</sample>
    <sample id="120">The model uses attention scores from the cross-attention mechanism between audio input and textual output.</sample>
    <sample id="121">"Easy on Me" or "I Gotta Feeling" (by name).</sample>
    <sample id="122">Fudan University.</sample>
    <sample id="123">MultiInstruct addresses the gap in multi-modal instruction tuning by introducing the first large-scale benchmark dataset of 62 diverse multi-modal tasks. This dataset, built from 21 existing datasets and augmented with five expert-written instructions per task, aims to improve generalization to unseen multi-modal tasks using the OFA model.  The research investigates the effectiveness of instruction tuning on OFA, a unified multi-modal pre-trained model, and explores different fine-tuning strategies.  The study demonstrates that instruction tuning significantly enhances OFA's performance on seen multi-modal tasks, with performance improving with increased task diversity.  Furthermore, transfer learning from natural instruction datasets improves model sensitivity and overall performance.  A novel "sensitivity" metric is introduced to measure the model's consistency across instruction variations.  The findings highlight the benefits of instruction tuning and transfer learning for multi-modal learning.  The authors plan to release an expanded dataset with approximately 150 additional vision-language tasks.  The presentation concludes with a QR code linking to the dataset and model.</sample>
    <sample id="124">Tan Qingyu from NUS and Alibaba introduces their work "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models." The research addresses the challenge of temporal reasoning in LLMs, dividing it into three levels: time-to-time, time-to-event, and event-to-event reasoning.  Prior work often focused on the second level, but this study aims for a more comprehensive approach.

A preliminary experiment reveals biases in year prediction, with models favoring the 2000-2020 period.  ChatGPT shows promise but struggles with month prediction. To address this, they propose the TempReason dataset, encompassing all three reasoning levels and long temporal coverage, constructed from Wikidata and Wikipedia.  They evaluate temporal reasoning using Closed Book QA, Open Book QA (with Wikipedia context), and a novel Reasoning QA setting requiring temporal knowledge.

A new training strategy, Temporal span extraction pre-training and time-sensitive reinforcement learning, is introduced to enhance temporal reasoning.  The TempT5 model, trained with this strategy, outperforms existing LLMs like ChatGPT and FLAN-T5-L, particularly in Open Book QA and Reasoning QA.  The study highlights temporal reasoning biases in LLMs, demonstrating ChatGPT's inconsistent performance across time periods.  Future work will focus on mitigating these biases. The research provides a benchmark dataset and a training paradigm to improve LLMs' temporal reasoning capabilities.</sample>
    <sample id="125">The provided text does not mention the number of authors.</sample>
    <sample id="126">Yes, translating the natural language query using a machine translation model before semantic parsing was considered as a baseline.</sample>
    <sample id="127">## Abstract: Large Language Models Are Reasoning Teachers

This paper introduces "Large Language Models Are Reasoning Teachers," a novel approach to transfer complex reasoning abilities from large language models (LLMs) to smaller, more accessible models.  Chain-of-thought (CoT) prompting, while effective for LLMs, is computationally expensive and limits its deployment to massive models.  Our method addresses this by leveraging LLMs as "reasoning teachers" to generate step-by-step solutions for complex tasks, which are then used to fine-tune smaller models.  A key innovation is "Diverse Reasoning," which generates multiple reasoning samples from the teacher using stochastic sampling, leading to improved student performance compared to single-sample approaches.  

Experiments on 12 tasks demonstrate that our method achieves notable performance, particularly in text-based tasks, and outperforms prompt-based baselines.  Diverse Reasoning further enhances performance, with a significant increase in accuracy on tasks like Multi-Arithmetic.  The method is highly scalable, with performance improving with larger datasets, better teacher models, and larger student models.  However, the approach involves trade-offs between development costs (teacher model complexity) and inference costs (student model size).  We provide open-source code and data, including access to OpenAI inference, to facilitate further research.  This work offers a promising path towards democratizing access to advanced reasoning capabilities in smaller, more practical models.</sample>
    <sample id="128">The KITMUS test suite evaluates knowledge integration in natural language understanding (NLU) models, addressing the challenge of leveraging both pretraining and inference-time knowledge.  This work highlights the critical need for models to effectively combine these knowledge sources for knowledge-intensive tasks.  The KITMUS test suite introduces a coreference resolution task designed to probe for this ability, varying the availability of entity-specific and background knowledge.  Three settings are defined: "Background-Pretrain" (background knowledge available during pretraining), "Background-Both" (available during pretraining and inference), and "Background-Inference" (available only at inference time).  The "Background-Inference" setting simulates scenarios where background knowledge is not present in pretraining data, such as with evolving occupations.  Experiments with human participants and established coreference resolution models demonstrate that while task-specific training improves performance, even the best models struggle with reliably integrating inference-time knowledge.  The findings suggest that many models rely on surface-level cues rather than deeper knowledge integration.  The KITMUS test suite provides a valuable tool for assessing and advancing knowledge integration capabilities in NLU models, emphasizing the limitations of current approaches and the need for further research in this area.  The dataset and code are publicly available on GitHub.</sample>
    <sample id="129">"Asian woman" was given as an example of a marked group.</sample>
    <sample id="130">The paper doesn't explicitly state which model architectures *do not* generalize well. It focuses on transformer models generalizing better than other architectures.</sample>
    <sample id="131">The provided text does not mention the names of the testing datasets.</sample>
    <sample id="132">Three.</sample>
    <sample id="133">Multi-modal.</sample>
    <sample id="135">ABC-Eval is a novel dimensional approach to evaluating conversational AI, developed by the Emory NLP Lab and Amazon Alexa AI. It addresses the limitations of existing methods like human ratings and pairwise comparisons by explicitly annotating model responses for specific behaviors, such as irrelevant information, contradictions, and lack of empathy. This approach aims to reduce subjectivity and provide a more precise assessment of dialogue quality across multiple dimensions.

The researchers evaluated four state-of-the-art chat models using ABC-Eval on 100 human-bot conversations, comparing it to Likert ratings and pairwise comparisons. Results showed that ABC-Eval behavior labels are more reliable and predictive of overall conversation quality than existing methods, as indicated by higher inter-annotator agreement and better performance in linear regression analysis.  ABC-Eval metrics also demonstrate distinctiveness, explaining a larger proportion of conversation quality than turn-level Likert scores.

The study quantified common challenges in conversational AI, revealing error rates of approximately 20% for common sense violations, 15% for irrelevant information, and 10% for contradictions.  The authors emphasize the importance of reliable evaluation metrics for tracking progress in the rapidly evolving field of conversational AI and hope ABC-Eval will be a valuable tool for future research.</sample>
    <sample id="136">## FERMAT: An Alternative to Accuracy for Numerical Reasoning

This presentation introduces FERMAT, a novel evaluation framework for assessing numerical reasoning capabilities in large language models (LLMs). The work addresses the limitations of existing benchmarks, which primarily focus on accuracy and fail to provide insights into the underlying mathematical abilities of these models.  

FERMAT utilizes a flexible evaluation set constructed from arithmetic-type questions extracted from Illinois and CommonCore curricula.  The questions are modified to include a wider range of number representations (integers, decimals) and mathematical operations, moving beyond simple number setting.  A zero-shot evaluation reveals poor performance across these aspects, highlighting the inadequacy of traditional benchmarks.

To improve performance, the research employs fine-tuning using human-written templates generating 200,000 diverse questions.  This fine-tuning demonstrates promising results, with performance improvements across various mathematical operations and number types.  Further analysis reveals that models often fail to accurately recall specific mathematical expressions during testing, suggesting a reliance on memorization rather than true understanding.  Finally, the study highlights the importance of incorporating language and mathematical diversity from datasets like GSM8K and AQUA to enhance model performance.  FERMAT offers a more informative alternative to accuracy-based evaluations, providing a deeper understanding of LLMs' numerical reasoning strengths and weaknesses.</sample>
    <sample id="137">Tell2Design is a novel dataset and approach for language-guided floor plan generation, addressing the need for design automation that aligns with user-specified requirements. Unlike text-to-image models focused on artistic generation, Tell2Design tackles the more constrained task of generating functional floor plans from natural language instructions detailing room types, shapes, and relationships. The dataset comprises 5,051 human-annotated instructions and 76,000 artificially generated ones, representing a diverse range of design scenarios.

The core challenge lies in generating designs under strict constraints, interpreting complex textual descriptions, and handling ambiguity.  The proposed solution employs a sequence-to-sequence model based on a transformer architecture, initialized with the T5 language model, to reconstruct room bounding boxes from the input instructions.  This approach allows for handling varying instruction lengths and room numbers.  Evaluation demonstrates that the Tell2Design model achieves high Intersection over Union (IoU) scores, outperforming text-conditional image generation baselines.  The study highlights a performance gap between artificial and human instructions, but also shows the benefit of using artificial data for pre-training.  Tell2Design provides a valuable resource for advancing research in language-guided design and enabling users to interact with design processes more effectively.</sample>
    <sample id="138">The authors claim that many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training, and even the best-performing models struggle with reliably integrating backward knowledge presented only at inference time.</sample>
    <sample id="139">Ying and Zhiyang.</sample>
    <sample id="140">Yes, crowd-sourced workers found and revised incorrect samples to ensure the quality of the validation and test sets.</sample>
    <sample id="141">The current resources for context-dependent translation are limited by supporting only a limited types of context-dependent translations and limited sets of languages, often relying on domain knowledge and human curation.</sample>
    <sample id="143">Wait-k strategy and Local Agreement.</sample>
    <sample id="144">The provided text does not mention the affiliations of the authors.</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">Yicheng from Fudan University presents a research paper addressing the critical issue of omission in dialogue summarization. While large language models excel at generating fluent summaries, they frequently omit key information, leading to factual inaccuracies and incomplete summaries. This paper systematically analyzes the prevalence of omission, revealing a high rate (around 70%) even in state-of-the-art models and a random distribution of omitted information across dialogue positions.

To tackle this challenge, the authors propose the OLDS dataset, a high-quality dataset of dialogue summaries with omission labels, built upon five existing benchmarks. They develop an automatic method for generating these labels and validate their quality through human evaluation.  The paper explores three model architectures – pair-wise classification, sequence labeling, and pointer networks – for omission detection and evaluates their performance using metrics like Precision, Recall, and F1-score, as well as the Word-level Omission Recall (WR) score.  The results highlight the difficulty of the task and the presence of label imbalance.

Furthermore, the research investigates the potential of using detected omissions to refine summaries. A post-editing method that incorporates omission content as input significantly improves summary quality. The findings suggest that omission detection is a valuable task for enhancing the accuracy and completeness of dialogue summarization, paving the way for more robust and reliable summarization systems. The dataset is publicly available.</sample>
    <sample id="147">Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models is done in collaboration with Esin Durmus and Dan Jurafsky.</sample>
    <sample id="149">Yes, the CoNLL++ dataset is publicly available.</sample>
    <sample id="150">MeetingQA is a novel extractive question-answering dataset designed for meeting transcripts, addressing the gap in NLP research focusing on the inherently significant question-asking component within these long, domain-specific documents.  The dataset comprises 7.7K questions and answers extracted from public meeting transcripts, with a high inter-annotator agreement.  Questions are open-ended and designed to elicit detailed responses, often involving multiple speakers and complex sentence structures.  The data collection process leverages the AMI corpus and involves annotating answer spans.

The paper introduces various methods for question answering, including context retrieval, single-span models, and multi-span models.  Experiments demonstrate that while fine-tuned models achieve high performance (F1 of 84.6), significant gaps remain compared to human performance.  Short-context models outperform long-context models, and multi-span models show comparable performance to single-span models.  Data augmentation with silver annotations improves zero-shot performance.  Error analysis reveals challenges in identifying rhetorical questions and speaker attribution, particularly in zero-shot settings.  MeetingQA presents a challenging task for existing QA models, highlighting the need for further research in understanding and leveraging the conversational nature of meeting discussions.</sample>
    <sample id="152">## Abstract: Exploring Large Language Models for Classical Philology

This presentation introduces a project focused on developing novel large language models (LLMs) tailored for classical philology, specifically Ancient Greek and Latin. Recognizing the limitations of existing BERT and multilingual models lacking sufficient pre-training data for these languages, the research team created GreBERTa, GreTa, PhilBERTa, and PhilTa – monolingual and multilingual models, respectively. 

A key innovation was the development of a high-quality pre-training corpus for Ancient Greek, leveraging the Internet Archive and a novel method for identifying Greek texts amidst noisy OCR data.  The models were rigorously benchmarked on tasks including part-of-speech tagging, dependency parsing, and lemmatization, achieving state-of-the-art performance for both languages.  Analysis revealed unique behavior of the T5 encoder and significant improvements in lemmatization accuracy. 

Furthermore, the study investigated the capabilities of the models in semantic understanding and world knowledge, finding that multilingual models did not consistently outperform monolingual counterparts.  The project provides a foundation for advancing the application of LLMs to classical texts, offering a robust toolkit for researchers seeking to unlock new insights from ancient literature and historical sources.  The research emphasizes the importance of native tokenization and high-quality pre-training data for effective language modeling in specialized domains.</sample>
    <sample id="153">This presentation details research on resolving ambiguities in text-to-image generative models. The work addresses the challenge of user intention not aligning with model interpretations of ambiguous prompts, hindering image generation fidelity. A key focus is developing frameworks for both disambiguating prompts and evaluating image faithfulness to user intent.

The research introduces a benchmark dataset based on the LAVA corpus, encompassing various ambiguity types. A prompt disambiguation framework leverages in-context learning to generate clarifying questions or alternative visual interpretations, allowing users to refine the prompt.  These disambiguated prompts are then fed into text-to-image models for image generation.

A novel automatic evaluation framework utilizes a Visual Question Answering (VQA) model to assess image faithfulness.  The VQA model receives both the generated image and a human-defined intention question, determining if the image satisfies the user's intent.  Findings demonstrate that ambiguity resolution varies across prompt types, disambiguation improves image fidelity, and the proposed evaluation framework correlates with human judgment. The research aims to improve the reliability and accuracy of text-to-image models by addressing prompt ambiguity.</sample>
    <sample id="154">University of Trento and Foundazione Bruno Kessler.</sample>
    <sample id="155">Javad Hosseini</sample>
    <sample id="157">The research introduces "Dialogue Summarization with Static-Dynamic Structure Fusion Graph" (SDDS), a novel approach to automatically generate concise summaries of multi-turn dialogues. Existing methods struggle with reliance on external linguistic tools and static graph structures, limiting adaptability and accuracy. SDDS addresses these drawbacks by employing a hybrid graph representation.

The model comprises an Utterance Encoder to convert dialogue utterances into vector representations, followed by a Static-Dynamic Graph module. This module constructs static graphs using heuristic methods like Discourse Parsing Graph (based on keyword co-occurrence) and speaker interaction frequency matrices, capturing dialogue structure and speaker relationships.  A Dynamic Graph module utilizes multi-head attention to learn semantic relationships between utterances based on their deep vector representations.  These static and dynamic graphs are then fused using a dual cross-attention mechanism, incorporating graph information into the summary generation process.

The SDDS model achieves improved summary quality by dynamically adapting to the dialogue context and leveraging both structural and semantic information. The code and data are publicly available on GitHub. This work offers a more robust and adaptable solution for dialogue summarization compared to traditional methods.</sample>
    <sample id="158">This presentation introduces "Dual Cache for Long Document Neural Coreference Resolution," addressing the challenge of coreference resolution in long documents where mentions of entities are scattered across a wide text range. Traditional methods suffer from quadratic complexity, while cache-based methods offer linear complexity but struggle with high cache misses in long documents due to topic shifts.

The proposed solution employs a dual cache architecture, combining a local cache (LRU eviction) for local entities and a global cache (LFU eviction) for global entities. This approach allows the model to efficiently manage entities with varying frequencies and improves performance by reducing cache misses.  

Experiments on benchmark datasets (LitBank, OntoNotes, WikiCoref) demonstrate that the dual cache outperforms baseline methods, even with unbounded memory, and achieves faster processing times.  A book-level evaluation further highlights the significant performance gap between the dual cache and single cache methods.  The dual cache also demonstrates a substantial reduction in cache misses compared to single cache approaches.  The presentation concludes that the dual cache offers the best balance of performance and cost-effectiveness for long document coreference resolution.</sample>
    <sample id="160">Unordered multisets of tokens that will appear in the output.</sample>
    <sample id="161">55,000</sample>
    <sample id="163">MASSalign.</sample>
    <sample id="164">Weak supervision offers a cheaper alternative to manual labeling, using weaker labeling sources like heuristics or crowdsourcing.</sample>
    <sample id="165">This paper introduces LiPoR, an unsupervised learning method for abductive reasoning that aims to identify plausible explanations for a given outcome, given a context and a set of candidate explanations.  Abductive reasoning, exemplified by the scenario of "Emily was stuck in traffic" leading to "Emily made it to her flight," seeks to bridge the gap between context and outcome by selecting the most likely explanation.  Traditional supervised methods for abductive reasoning rely on annotated plausible explanations, which are often noisy and subjective, as demonstrated by crowd worker disagreement.  LiPoR addresses this challenge by learning abductive reasoning without explicit plausibility annotations.

The method treats explanations as latent variables and maximizes the marginal likelihood of the outcome given the context, while incorporating a regularizer based on mutual exclusivity among explanations. This regularizer, denoted as Omega, balances the likelihood of the outcome with the entropy of the probability distribution over explanations, effectively preferring a subset of plausible explanations.  The LiPoR objective function combines these two components.  Experiments on the AlphaNLI dataset demonstrate that LiPoR outperforms zero-shot models and previous unsupervised approaches by over 4 absolute points in accuracy.  The paper's findings suggest that unsupervised learning can effectively enable abductive reasoning without relying on human-annotated plausibility. The paper is available at tinyurl.com/zhao-lipor.</sample>
    <sample id="166">This paper introduces "A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text" to address the challenge of image retrieval from long, complex text descriptions, where typical visual language models struggle. The proposed framework leverages the Divide-and-Conquer strategy and Dual-Process Theory, inspired by human cognitive systems. It posits that visual language models (System 1) excel at analogical reasoning but falter with complexity, while System 2, representing logical reasoning, is better suited for complex tasks.

The framework comprises three key modules: a Proposition Generator to decompose complex text into simple propositions, a Visual-Linguistic Interactor (System 1) to interact with visual and proposition information, and a Neural-Symbolic Reasoner (System 2) to integrate reasoning states and results for final solution generation. The Neural-Symbolic Reasoner utilizes negation and conjunction operations.  Experimental results demonstrate that the proposed method (NDCR) outperforms existing baselines, with ablation studies validating the individual module's effectiveness.  The system also exhibits interoperability by presenting inference states and results throughout the reasoning process. The authors suggest that integrating neural symbolic calculation with Divide-and-Conquer and Dual-Process Theory holds promise for enhancing compositional reasoning and planning in large language models.</sample>
    <sample id="167">The documents in DEPLAIN-web were aligned both manually and with automatic alignment methods.</sample>
    <sample id="168">The CoNLL++ dataset was created by collecting Reuters News from 2020 and annotating it with the same CoNLL-2003 annotation guidelines.</sample>
    <sample id="169">This paper presents a systematic study of prompting strategies for machine translation using the PaLM large language model (540 billion parameters). PaLM, trained on 780 billion tokens, demonstrates state-of-the-art performance on various NLP tasks. The authors evaluated PaLM's translation capabilities by comparing it to state-of-the-art systems, including Google Translate, using WMT evaluation metrics and human evaluation.

The study highlights the significant impact of prompting on LLM translation performance, with even small changes in prompt design leading to substantial differences (up to 40 BLEURT points).  While prompt form is crucial for zero and one-shot prompting, the quality of the provided examples is paramount for five-shot prompting.  The examples should be high-quality translations, ideally sourced from curated datasets like the WMT development set.

Human evaluation using the MQM framework revealed that PaLM achieves comparable fluency to state-of-the-art systems but struggles with accuracy, particularly omission errors.  The model tends to prioritize fluent output over complete accuracy, resulting in some instances of missing parts of the source sentence.  Despite these accuracy challenges, PaLM's fluency is notably better than other systems, and it performs competitively with commercial translation services. The paper concludes with recommendations for prompt selection strategies and encourages further exploration of the topic.</sample>
    <sample id="171">Existing works are broadly classified into four categories, but they either are not applicable to embedding as services or lack transferability.</sample>
    <sample id="172">No, multilingual LLMs like Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="174">Thea from ArgAnalysis35K introduces a novel dataset for argument quality analysis, addressing limitations of existing datasets.  ArgAnalysis35K boasts the largest dataset of 35,000 argument-analysis pairs, prioritizing high-quality arguments sourced from expert and intermediate debaters, with a smaller portion from novice users.  Unlike datasets tied to specific motions, ArgAnalysis35K utilizes 24 themes, capturing diverse arguments relevant to parliamentary debate.  A key innovation is the introduction of "analysis," encompassing claims, premises, and their combinations, providing a more comprehensive understanding of argument strength.  The dataset incorporates instance-based annotator reliability, mitigating bias by focusing on individual argument reliability rather than entire annotator groups.  Finally, a relevance model assigns scores to arguments' relevance to different themes, capturing broader applicability beyond single motions.  This combination of features aims to provide a more robust, diverse, and reliable resource for NLP research in argument quality assessment. The authors encourage further feedback on the dataset and its methodology.</sample>
    <sample id="175">The method addresses the ambiguity of permutations by inducing the alignment as part of the training and approximating the NP-hard permutation search with a GPU-friendly continuous relaxation that allows for learning linguistically plausible permutations.</sample>
    <sample id="176">Left-leaning language models are better at detecting hate speech targeting socially minority groups, while right-leaning models are better at detecting hate speech targeting white and men. Similar trends exist for fake news detection, with left-leaning models better at detecting misinformation from their opposite political leaning and vice versa. These differences in performance based on social categories indicate a fairness issue.</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustav Sinha</sample>
    <sample id="179">## Abstract: Minding Language Models’ (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker

This presentation introduces SymbolicToM, a novel inference-time method to enhance Theory of Mind (ToM) reasoning in large language models (LLMs). ToM is the ability to understand mental states, crucial for tasks like reading comprehension involving multiple characters, particularly assessed through false-belief questions. Current LLMs often struggle with these tasks, highlighting a need for improved reasoning capabilities.

SymbolicToM addresses this by employing explicit graphical representations of mental states, specifically belief graphs, for all character combinations up to a defined level. These graphs are computed using off-the-shelf Natural Language Inference (NLI) and OpenIE models, enabling efficient question answering. The method involves detecting entities in the question, retrieving relevant belief graphs, performing recursive reasoning over the question, and finally feeding the results to a language model for a final answer.

Experiments demonstrate significant performance gains across various LLMs, including GPT-3 and Flan-T5, compared to out-of-the-box performance on the ToMi dataset. Furthermore, SymbolicToM exhibits robust generalization capabilities on novel datasets designed to test storage structure and linguistic diversity, outperforming supervised baselines while maintaining benefits even with more advanced models like GPT-4.  SymbolicToM offers a plug-and-play solution that avoids overfitting, provides interpretable reasoning, and significantly improves LLM performance in ToM tasks.</sample>
    <sample id="180">Myra</sample>
    <sample id="181">This paper addresses the challenge of constrained language planning, extending the work on abstract goal planning for stereotypical activities to scenarios with specific constraints.  While large language models (LLMs) demonstrate promise in decomposing goals, their performance on constrained planning remains unsatisfactory.  The authors define this problem and explore the limitations of current LLMs in generating faithful scripts given multi-faceted constraints.

To address this, they propose an "over-generate-then-filter" method, leveraging InstructGPT to generate numerous scripts for specific goals and then employing a filter model based on semantic similarity and constraint keyword presence.  This approach significantly improves script quality in both semantic completeness and constraint faithfulness.  Furthermore, the paper tackles the data scarcity problem by creating the CoScript dataset, a large-scale dataset of 55,000 constrained language planning examples generated by LLMs and refined through human review.  

The CoScript dataset exhibits high diversity in goal types.  The authors demonstrate that fine-tuning smaller models like T5 on CoScript yields superior performance compared to larger LLMs, highlighting the potential for specialized models in constrained language planning.  The paper concludes by establishing the constrained language planning problem, evaluating LLM performance, and presenting a method and dataset to advance research in this area.</sample>
    <sample id="182">"Tropicalism" indicates a trope of hyper-sexualization and being seen as very docile and submissive, specifically connecting to the words describing Latina women.</sample>
    <sample id="183">The authors used prompts like "Imagine you are an Asian woman. Describe yourself." to generate personas, inspired by a study where these prompts were given to human subjects.</sample>
    <sample id="184">CXMI (Contextualized Word Meaning Information) was extended to Pointwise CXMI to measure context usage at the sentence or word level.</sample>
    <sample id="185">DrBERT is a pre-trained biomedical model in French based on RoBERTa, trained on the NACHOS dataset. ChuBERT is a clinical model based on anonymized data from the Nantes University Hospital data warehouse.</sample>
    <sample id="187">Ying and Zhiyang.</sample>
    <sample id="188">Iterative transfer learning updates the model by training on the latest set of data collected during active learning.</sample>
    <sample id="189">The goal of the dataset is to understand users' language when they want to make a choice, specifically focusing on indirect referring expressions for entity selection in conversational systems and benchmarking LLMs' entity understanding.</sample>
    <sample id="190">An attacker can extract model parameters by learning from the embedding provided by the EaaS and then replicating the service to provide similar embeddings.</sample>
    <sample id="191">Three.</sample>
    <sample id="192">This presentation introduces CAME (Confidence-guided Adaptive Memory Efficient Optimization), a novel optimizer designed to address the trade-off between fast convergence and low memory usage in large language model training. Existing memory-efficient optimizers like Adafactor often sacrifice performance, while traditional optimizers like Adam suffer from high memory consumption. CAME builds upon the principles of non-negative matrix factorization (NMF) to reduce memory requirements, aiming for a more efficient approach.

The core innovation lies in addressing erroneous updates inherent in NMF-based optimizers. CAME utilizes the residual between predicted and actual updates to dynamically adjust the denominator of the momentum term, effectively mitigating instability and improving convergence speed. Experiments on BookCorpus and English Wikipedia, using BERT, GPT-2, and T5, demonstrate CAME's superior performance compared to Adam and Adafactor, achieving significant validation accuracy improvements and reduced memory costs, particularly with larger batch sizes.  CAME also outperforms Adam in pre-training very large models.  Furthermore, CAME achieves comparable performance to baseline models on downstream tasks while significantly reducing memory footprint.  The proposed optimizer offers a promising solution for efficient training of large language models, especially in scenarios involving large batch sizes.</sample>
    <sample id="193">The provided text does not specify the number of annotators used to create the initial dataset. It only mentions that a large-scale annotation was conducted.</sample>
    <sample id="194">Jenny is a first-year PhD student at Carnegie Mellon University. The authors are also affiliated with the University of Washington and the Allen Institute for AI.</sample>
    <sample id="195">## Abstract: Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering

This paper introduces RoHT, a novel framework for explainable question answering (XQA) that addresses the limitations of existing approaches. While neuro-symbolic methods struggle with incomplete knowledge bases and decompose-based methods face challenges with natural language diversity, RoHT leverages hierarchical question decomposition to integrate knowledge from heterogeneous sources effectively. 

RoHT builds a Hierarchical Question Decomposition Tree (HQDT) to understand the compositional structure of complex questions, breaking them down into atomic questions.  A probabilistic reasoning process then fuses knowledge from knowledge bases (KBs) and text corpora at different levels of the tree, considering both string generation and answering probabilities.  The reasoning proceeds recursively, selecting appropriate knowledge sources (KB, text, or recursive calls) for each node, obtaining answers with associated probabilities, and aggregating them to produce top-ranked answers.

Evaluated on challenging datasets KQA Pro and Musique, RoHT demonstrates significant performance improvements over existing methods, particularly when integrating knowledge from both KBs and text.  On KQA Pro, RoHT outperforms KB QA methods and TransferNet, highlighting the benefits of explicit decomposition. On Musique, RoHT significantly improves F1 scores compared to state-of-the-art text models.  The results indicate that RoHT effectively leverages hierarchical question decomposition to enhance XQA performance by integrating diverse knowledge sources and providing explainability through its tree-based structure.</sample>
    <sample id="196">"I saw Bart and Lisa"</sample>
    <sample id="197">The text mentions four state-of-the-art chat models that were evaluated using ABC-Eval.</sample>
    <sample id="198">The current minimal pair paradigm doesn't allow evaluation of models' acceptability across longer sentences, which are becoming increasingly common with large language models and their larger context windows.</sample>
    <sample id="199">Yes, training in a mixture of various languages caused a performance drop in English on seven of the nine datasets.</sample>
    <sample id="200">The annotators know the names of the entities but not necessarily about them.</sample>
    <sample id="201">BLEURT and human evaluation (MQM framework).</sample>
    <sample id="202">The provided text does not discuss whether the regress in generalization impacts specific NER types. It focuses on general model performance and the factors influencing generalization.</sample>
    <sample id="203">NLPositionality highlights that NLP datasets and models can reflect the perspectives of their creators and the populations they are trained on, leading to systematic performance differences across different groups. This can result in some groups being unfairly disadvantaged or excluded.</sample>
    <sample id="204">The presentation states that multilingual language models like Codex and BLOOM are "still inadequate" for cross-lingual semantic parsing tasks, implying they were not fine-tuned with adapters or full fine-tuning in a way that yielded satisfactory results.</sample>
    <sample id="205">## Abstract: Political Biases in Language Models

This work investigates the propagation of political biases from pretraining data to language models and their impact on downstream NLP tasks. Leveraging the C4 corpus, the study reveals that language models exhibit varying political leanings, with GPT-4 leaning more liberal than other models.  Experiments demonstrate that pretraining on partisan corpora significantly shifts the ideological coordinates of language models, indicating the potential for models to absorb societal polarization.  Furthermore, the study analyzes language models with different political leanings on hate speech and fake news detection, revealing disparate performance across demographic groups and political affiliations. Left-leaning models show strength in detecting hate speech targeting minority groups, while right-leaning models excel at identifying hate speech targeting dominant groups.  These findings highlight a critical fairness issue: language models can perpetuate and amplify existing societal biases. The research concludes by discussing the ethical dilemma of mitigating political biases – balancing the risk of fairness issues with the potential for censorship and the difficulty of defining neutrality in data collection.  The study underscores the urgent need to address political biases in language models to prevent discriminatory outcomes in NLP applications.</sample>
    <sample id="206">Transfer learning from topic-independent dissonance stance classification (debate) and binary classification of expansion and comparison (CE) classes of PDTB.</sample>
    <sample id="207">The paper uses the latest test sets to avoid overlap with the training data of the language model.</sample>
    <sample id="208">The authors proposed three recommendations.</sample>
    <sample id="209">The proposed method using CoScript and T5 fine-tuned on it generates scripts of higher quality than most large language models, surpassing them in quality.</sample>
    <sample id="210">Shuheng</sample>
    <sample id="211">Yes, the results and dataset in the paper can be used as a benchmark.</sample>
    <sample id="212">Two: T5 and InstructGPT.</sample>
    <sample id="213">OFA</sample>
    <sample id="215">This paper investigates the dependency structure of coordination, arguing against asymmetric structures like those proposed by universal dependencies and Igor Mel'čuk's meaning text theory, and advocating for symmetric structures. The authors propose that dependency length minimization is a key principle driving coordination structure. They demonstrate this principle using examples like "Marge read it yesterday" and "Marge read yesterday this absolutely fascinating book about bees," showing that shorter dependencies are preferred, even if it violates typical grammatical rules.

The study analyzes coordination statistics from the enhanced Penn Treebank, revealing a tendency for left conjuncts to be shorter, particularly when the governor is on the left or absent. This tendency increases with the length difference between conjuncts.  Crucially, the paper finds that this length preference only occurs when the governor is on the left or absent; it disappears when the governor is on the right.  By measuring length in words, syllables, and characters, the authors provide empirical evidence supporting the symmetric structure of coordination. The findings suggest that dependency length minimization, influenced by the governor's position, is a fundamental principle governing how coordination is structured, thus providing an argument for symmetric coordination structures over asymmetric ones. The paper offers a novel perspective on coordination, highlighting the importance of dependency length in shaping its syntactic organization.</sample>
    <sample id="217">This work addresses the challenge of compositional generalization in multi-attribute controllable dialogue generation (CDG). Existing methods often focus on single attributes or rely on limited annotated data, hindering the ability to generate coherent dialogues with unseen attribute combinations. We introduce DCG, a Disentangled Controllable Generation model, which learns attribute concepts from seen values using a disentanglement loss.  A key contribution is the development of MAE, a unified reference-free evaluation framework for various attribute granularities.  We establish two benchmarks and demonstrate the effectiveness of DCG and MAE through experiments on DailyDialog-CG.

Our model utilizes a compositional prompt module, employing both attribute-oriented and task-oriented prompts to guide the generation process.  Attribute-oriented prompts focus on specific attribute values, while task-oriented prompts guide the model towards global features.  A disentanglement loss further enhances compositional generalization.  Experiments show that DCG outperforms baselines in both attribute controllability and text equality, even with minimal performance drop on established metrics.  MAE demonstrates strong correlation with human judgments across both discrete and continuous attributes.  Visualizations of concatenated prompt embeddings confirm the disentanglement of attribute combinations and the learning of attribute relationships.  The proposed approach enables effective generalization from seen to unseen attribute combinations, offering a promising solution for more flexible and controllable dialogue generation.</sample>
    <sample id="218">Google Translate.</sample>
    <sample id="219">Jia-Huei Ju from Academia Sinica presents their research on a compare-and-contrast multistage pipeline for uncovering financial signals in financial reports, specifically using the Form 10-K. The work addresses the challenge of extracting valuable information from annual reports, which often require significant human effort. The research is motivated by the observation of high text similarity between yearly reports and the yearly-dependent nature of content.

The proposed approach introduces a highlighting task focused on identifying the rationale (words) that explain the relationship between a target report and its previous year's report. This involves predicting word importance to measure highlighting performance. The pipeline consists of document segmentation, relation recognition, and out-of-domain and in-domain fine-tuning.  The relation recognition stage classifies report pairs into three types: highly similar, revised (syntactically similar but semantically different), and mismatched (new information).

Fine-tuning utilizes the eSNLI dataset for out-of-domain learning and revised pairs for in-domain training, employing soft labeling techniques to mitigate issues with low-quality pseudo-labels. The evaluation uses both eSNLI and a newly released FINAL dataset, with performance measured using precision and PCC. The research demonstrates that the domain-adaptive highlighting model achieves strong performance and generalizes well to the eSNLI dataset. Future work includes exploring improvements in effectiveness and incorporating additional features. The research is available on the paper and GitHub.</sample>
    <sample id="220">Vasudha is a Computer Science PhD candidate at Stony Brook University.</sample>
    <sample id="221">German to English.</sample>
    <sample id="222">## Abstract: Data Interventions for Domain Adaptation in Open-Domain Question Answering

This work addresses the challenge of domain adaptation in open-domain question answering (QA), where models trained on general-purpose corpora like Wikipedia struggle to generalize to specialized domains. We investigate data interventions to enable out-of-domain generalization, focusing on understanding the type of dataset shift encountered and identifying effective intervention strategies.

We evaluate seven target datasets across six domains, comparing a Wikipedia-based source model with interventions using both zero-shot and few-shot learning. Few-shot methods, leveraging manually constructed fact-based questions, significantly improve retriever and reader performance (8% and 11% respectively). Zero-shot techniques explore controlling the interaction of question, answer, and context variables, finding cloze-style questions to be more amenable to adaptation.

We characterize dataset shifts using a taxonomy, differentiating between no shift, concept shift, covariate shift, and full shift. A compatibility measure quantifies the alignment between the source model and the target domain. Our findings reveal that few-shot interventions are effective across all shift types, while zero-shot methods are particularly beneficial for concept and covariate shifts.  We conclude that targeted data interventions, informed by dataset shift analysis, can substantially improve reader performance in open-domain QA, highlighting the importance of domain-specific adaptation strategies.</sample>
    <sample id="223">Shangbin</sample>
    <sample id="224">MASSalign and fine-tuned long-mBART and base mBART models.</sample>
    <sample id="225">The training dataset consists of 53 tasks, and the testing dataset consists of 5 tasks.</sample>
    <sample id="226">The provided text does not mention the number of authors involved in the paper.</sample>
    <sample id="227">This paper addresses the challenge of grounded language understanding (GLU), where natural language is mapped to executable plans or programs in a specific environment. While large language models (LLMs) show promise in GLU, current approaches often rely on autoregressive generation, which can produce invalid or non-executable plans. The core problem is the lack of grounding during pre-training, creating a gap between pre-training and downstream application.

To overcome this, the authors propose "Pangu," a novel framework that shifts the focus from generation to discrimination. Pangu utilizes a symbolic agent to interact with the environment and generate candidate plans, while a language model scores and ranks these candidates. This approach avoids the language model's responsibility for plan validity and grammar.

Experiments on knowledge-based question answering demonstrate Pangu's effectiveness across various LLMs (BERT, T5, Codex) and training methods (fine-tuning, in-context learning). Pangu achieves strong performance and sample efficiency, even with limited examples. Notably, Pangu exhibits robustness to non-independent and identically distributed (non-i.i.d.) data, contrasting with autoregressive models that tend to overfit.

The key takeaway is that discrimination, rather than generation, offers a more effective strategy for leveraging LLMs in grounded language understanding. The framework's ability to achieve strong results with minimal data and robustness to diverse environments positions it as a significant advancement in the field.</sample>
    <sample id="228">AG News, MIND, SST2, and Enron Spam.</sample>
    <sample id="229">Gabriella Skitalinskaya and Henning Wachsmuth present a work on detecting improvable claims in argumentative writing, aiming to address a challenge for novice writers: determining when a claim is sufficiently phrased. The paper introduces two new tasks: Suboptimal Claim Detection (identifying claims needing revision) and Claim Improvement Suggestion (selecting quality issues for revision). The authors explore the challenges of using revision-based data, particularly in the context of argumentative text, focusing on collaborative online debate platforms like Kialo.

The work identifies four key challenges: Representativity and Reliability (ensuring the dataset accurately reflects high-quality claims), Model Complexity and Architecture (optimizing models for revision sensitivity), Contextual Information (determining relevant contextual factors for quality assessment), and Topical and User Bias (mitigating noise and biases in revision histories).  The paper details strategies for addressing these challenges and presents a systematic comparison of approaches for the two tasks.  Experiments demonstrate that revision-based data is effective for claim assessment, modeling the distance between claim versions aids in identifying suboptimal claims, and contextual information's impact depends on the task and identified quality issues. The authors conclude that these findings offer valuable insights for improving argumentative writing support tools.</sample>
    <sample id="231">NACHOS is a dataset of medical crawled data from the web.</sample>
    <sample id="232">David Vilar</sample>
    <sample id="233">Simultaneous Speech Translation (SimuST) aims to translate spoken language into text in real-time, enabling cross-language communication. Current SimuST models face challenges including long training procedures, optimization of multiple architectures for varying latency, and maintaining several models for different latency regimes.

The paper introduces EDAtt (Encoder-Decoder Attention), a strategy for achieving low-latency SimuST by leveraging existing offline ST models and the attention mechanism. EDAtt decides whether to emit a partial translation based on the attention distribution between the audio input and the textual output. Specifically, it emits a word if the attention weights are not concentrated towards the last few speech frames, indicating stable information. This allows for handling latency by emitting partial translations and waiting for subsequent speech chunks.

Experiments on German demonstrate that EDAtt outperforms existing strategies like Wait-k and Local Agreement, achieving faster translation speeds while maintaining high translation quality (measured by BLEU score).  The computational-aware latency metric further highlights EDAtt's efficiency. The authors release open-source code and models to promote reproducibility.  The paper emphasizes the potential of EDAtt to facilitate real-time cross-lingual communication by optimizing for both translation quality and latency.</sample>
    <sample id="234">The prompting strategy has a significant impact, with one-shot prompting showing a difference of over one BLEURT point and up to 40 BLEURT points in extreme cases. However, for five-shot prompting, the form of the prompt is less important, and the quality of the examples is more crucial.</sample>
    <sample id="235">Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig.</sample>
    <sample id="236">The presentation does not specify the content of the 5 expert-written instructions. It only states that each task in MultiInstruct is equipped with five expert-written instructions.</sample>
    <sample id="237">A coreference resolution task designed to probe for the ability to draw on knowledge available in different sources.</sample>
    <sample id="238">MeetingBank is a new benchmark dataset for meeting summarization, created by the University of Central Florida. Addressing the challenges of high-quality summaries and accessible public meeting data, the dataset comprises 1,366 City Council meetings with transcripts, reference summaries, and URLs. Data collection involved using the Speechmatics API for transcript conversion and identifying meeting segments using unique MeetingIDs. The dataset includes statistics on meeting duration, speaker count, and meeting periods, alongside summarization instances and average sentence/token counts.

Analysis reveals varying levels of abstraction in summaries, with coverage and density scores indicating the extent to which summaries include verbatim points versus extracted references.  Extractive summarization systems like Oracle and LEAD show promising ROUGE-2 scores, while DialogLM achieves the highest ROUGE-2 among abstractive models.  GPT-3 demonstrates strong fluency and coherence but weaker informativeness and factuality according to automatic metrics.  Human evaluation using five criteria (informativeness, factuality, fluency, coherence, redundancy) reveals GPT-3 achieves the highest overall scores, particularly in fluency and coherence.

MeetingBank is intended to facilitate research in meeting summarization and offers insights into City Council decision-making. The dataset is publicly available for download and use. The research highlights the need for improved automatic evaluation metrics that better align with human preferences and emphasizes the importance of capturing key discussion points in meeting summaries.</sample>
    <sample id="241">Ethan, along with co-authors Yang Chen, Wei Xu, and Alan Ritter from Georgia Tech, presents their paper on "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments." The paper addresses limitations in existing automated misinformation detection systems, which often rely on unrealistic evaluations and fail to account for the complexities of real-world social media data and human moderation.

The authors propose an evaluation framework for developing more effective and human-centric systems. Their system incorporates human feedback at various stages, from claim extraction using a T5 model to policy violation verification using a BERT-based stance classification model.  The system focuses on early detection of unapproved treatments before they are debunked, operationalized as the detection of such claims before their appearance in news articles.  

Evaluation reveals a 65% accuracy in policy violation detection, with the system identifying 124.2 policy violations per human hour. The framework emphasizes an end-to-end approach, aligning system outputs with human content moderators. The paper concludes by advocating for consistent evaluation methods and motivating future research in human-in-the-loop misinformation detection, offering a valuable perspective for the field.</sample>
    <sample id="242">Common evaluation methods for dialogue systems include human judges selecting better conversations or rating conversations using Likert scales.</sample>
    <sample id="243">Sebastian Santy, Ronan Le Bras, Katharina Reinecke, and Maarten Sap.</sample>
    <sample id="244">The background knowledge needed is that "Judges decide cases in law courts."</sample>
    <sample id="245">This paper introduces a two-step pipeline for identifying high-agreement workers on Amazon Mechanical Turk (MTurk) for summarization tasks, addressing the limitations of automatic metrics and the need for better MTurk recruitment practices. The pipeline employs pre-task qualifications, including a qualification task assessing evaluation abilities and an endurance task testing workload capacity.  Workers are categorized into gold, silver, bronze, and block tiers, with gold and silver workers demonstrating high inter-annotator agreement (IAA) exceeding that of experts.  A reference-based task further evaluates general performance, yielding a Krippendorff's Alpha of 0.534 for pipeline workers.  The study compares the pipeline to Baseline MTurk workers (using MACE) and CloudResearch MTurk workers, finding the pipeline to achieve comparable quality at a lower cost and with higher agreement.  Analysis of correctness across annotation sources reveals strong correlation between pipeline and CloudResearch workers, while GPT models correlate well with expert judgments. The authors conclude that this pipeline offers a cost-effective and efficient method for identifying high-quality annotators for large-scale summarization projects, with future work focusing on expanding the pipeline's applicability and addressing limitations related to language, task types, and correctness guarantees.</sample>
    <sample id="246">Yes, the code and dataset are available on GitHub.</sample>
    <sample id="247">FACTKG is a novel dataset for fact verification leveraging knowledge graphs (KGs) as evidence, addressing the gap in existing datasets that primarily rely on text or tables.  The dataset comprises claims in both written and colloquial styles, extracted from DBpedia, and labeled as either SUPPORTED or REFUTED.  The task involves retrieving evidence from the KG and verifying the claim using reasoning across five types: one-hop, conjunction, existence, multi-hop, and negation.  

The dataset's structure allows for diverse reasoning scenarios, enabling more reliable fact verification compared to text- or table-based approaches.  The inclusion of colloquial claims enhances practical applicability for dialogue systems and consistency checks between KG and natural language.  

To facilitate data creation, we employed a colloquial style transfer model and presupposition templates.  We also constructed baseline models, including claim-only baselines and a GEAR model utilizing graph evidence.  Our results demonstrate that all baselines outperform a majority class baseline (51%), with the GEAR model achieving the highest performance by leveraging KG evidence.  FACTKG provides a valuable resource for advancing fact verification research and applications in knowledge-intensive domains.  The dataset is publicly available for download.</sample>
    <sample id="248">The annotators for NLPositionality are not perfectly balanced across all demographics. The study found that datasets and models are more aligned with English-speaking countries and individuals with a college education, and less aligned with non-binary people compared to men and women.</sample>
    <sample id="249">"We find that when we perturb the sentences in the acceptable domain, we see similar increase in all the perturbations..."</sample>
    <sample id="250">It means evaluating conversational AI based on multiple, distinct aspects of chat quality, rather than relying on a single overall score or a limited set of dimensions.</sample>
    <sample id="251">University of Science and Technology of China.</sample>
    <sample id="252">U-CREAT is a novel approach to unsupervised case retrieval, addressing the challenge of identifying relevant precedents in legal contexts. The research introduces the Indian Legal Prior Case Retrieval (IL-PCR) dataset, a benchmark for PCR tasks containing 7,070 cases with a high average citation count, offering a more comprehensive test bed than existing datasets like COLIEE’21.  The core of U-CREAT is the U-CREAT pipeline, which leverages unsupervised learning and an event-based approach.  It extracts key events from query and candidate documents using dependency parsing, forming subject-verb-object triplets.  These events are then used to compute an interaction matrix, highlighting commonalities between the query and candidate documents.  

The pipeline incorporates various retrieval models, including count-based, transformer-based, and event-based approaches. While transformer models showed limited performance, event-based models significantly outperformed baselines, achieving higher F1 scores and lower inference times.  The Event Filtered Documents model, which filters the corpus based on matching events, demonstrates the best performance.  U-CREAT's approach generalizes well across Indian and Canadian legal systems without requiring specific tuning.  The research contributes a new benchmark dataset and a highly efficient, unsupervised pipeline for prior case retrieval, paving the way for advancements in legal information retrieval.</sample>
    <sample id="253">DisorBERT is a novel double domain adaptation model designed for detecting signs of mental disorders in social media posts. The research team, comprised of researchers from Mexico and Spain, addresses the challenge of insufficient annotated data by leveraging knowledge from a related domain – Reddit – to improve model performance.  The model utilizes a base language model (BERT) and integrates information from Reddit and a lexicon to specialize in mental health language.  A guided masking strategy further focuses the model on important words during training.  Experiments on the eRisk dataset demonstrate that DisorBERT achieves a balanced performance, exhibiting good precision and recall across different dimensions compared to baseline models like BERT and MentalBERT.  Analysis of generated words reveals that DisorBERT is more likely to predict words associated with mental disorders, such as "focus," "talk," and "sleep," compared to BERT.  Visualization techniques highlight the importance of words related to anxiety and medication in identifying depression.  The study concludes that the combination of double domain adaptation and guided masking is effective for detecting mental disorders in social media, surpassing the performance of existing models. Future work will explore incorporating different lexical resources and clinical data to further enhance the model's capabilities.</sample>
    <sample id="254">This research presents a novel framework for document-level distant relation extraction (DS-RE) that addresses the noise problem in distant supervision (DS) data. DS-RE aims to extract relationships between entities within a document, but DS data often contains inaccurate labels due to the lack of human annotation.  Current methods mitigate this noise with pseudo-labeling, but false positives can still occur, leading to incorrect relation extraction.

Our framework employs uncertainty-guided label denoising to improve the quality of DS data. It first trains a pre-denoising DocRE model on both DS and human-annotated data to generate pseudo-labels.  Crucially, we introduce instance-level uncertainty estimation to assess the reliability of each prediction, particularly for overlapping relations.  A dynamic class uncertainty threshold is then used to filter out low-confidence pseudo-labels.  Finally, we employ a multi-phase training strategy to iteratively refine the DS data.

We leverage Monte Carlo dropout to model uncertainty in the pre-denoising DocRE model.  Our approach overcomes limitations of previous uncertainty estimation methods by adapting it to handle overlapping relations and dynamically adjusting thresholds for long-tail classes.  Experimental results on public datasets demonstrate significant performance improvements compared to existing baselines, highlighting the effectiveness of our uncertainty-guided denoising approach.  The key contributions are a framework for denoising DS data, an instance-level uncertainty estimation method, a dynamic thresholding strategy, and substantial performance gains.</sample>
    <sample id="255">The form of prompting is crucial for zero and one-shot prompting, but has little influence on five-shot prompting.</sample>
    <sample id="257">The authors evaluated four state-of-the-art chat models.</sample>
    <sample id="258">Chiang Cheng-Han introduces a new work exploring the feasibility of using large language models (LLMs) as an alternative to human evaluation in natural language processing (NLP). The research proposes instructing LLMs to rate text quality based on predefined criteria like grammar, coherence, likability, and relevance, mirroring the process of human evaluation.  The novelty lies in this direct LLM evaluation approach, as prior work primarily focused on LLMs following instructions for other tasks.

To validate this approach, the study evaluates stories generated by GPT-2 and human writers using four LLMs: T0, InstructGPT (curie &amp; davinci), and ChatGPT. Human English teachers provide ground-truth ratings. The results show that human raters generally prefer human-written stories, with Davinci and ChatGPT exhibiting a stronger preference.  The research investigates factors influencing LLM evaluation, including agreement between LLMs and human raters, instruction wording, sampling methods, and potential benefits and costs compared to human evaluation. The paper concludes by highlighting the potential of LLMs as a valuable tool for automated text quality assessment and invites further exploration of this topic.</sample>
    <sample id="259">XSemPLR is a new benchmark for cross-lingual semantic parsing, addressing the lack of comprehensive datasets for multiple natural languages and meaning representations. The benchmark includes nine datasets across various domains, five semantic parsing tasks, eight meaning representations, and 22 languages spanning 15 language families.  The study explores six evaluation settings: Translate-Test, Monolingual, Monolingual Few-shot, Multilingual, Cross-lingual Zero-shot, and Cross-lingual Few-shot transfer.

The research compares the performance of monolingual models (Encoder-PTR and Encoder-Decoder) and multilingual models (mBART, mT5, XLM-R + PTR). Encoder-Decoder models consistently achieve the best performance across all datasets.  The study highlights the "Curse of Multilinguality," observing that while training on a mixture of languages improves performance, English performance can decline.  Cross-lingual transfer performance is significantly lower in zero-shot settings but improves rapidly with few-shot learning.  The findings suggest that pretraining on English can boost few-shot performance, and that current multilingual models like Codex and BLOOM are insufficient for this task. XSemPLR provides a valuable resource for advancing research in cross-lingual semantic parsing.</sample>
    <sample id="260">The provided text does not mention the number of authors.</sample>
    <sample id="261">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="262">The provided text does not explicitly state the number of authors. It only mentions Siyu Yuan from Fudan University.</sample>
    <sample id="263">This work addresses instability and bias in in-context learning, a popular method for utilizing large language models. Prior research highlights search instability and the introduction of biases due to design choices like example order.  We propose a typology of label biases, identifying a novel "domain-label bias" arising from the task corpus influencing model predictions.  We demonstrate that random in-domain words can significantly bias model preferences, particularly on tasks with strong domain-label bias, often leading to performance degradation even with prior calibration methods.

To mitigate these biases, we introduce "domain-context calibration," a method using random in-domain words as content-free text to estimate and correct for bias.  This approach improves in-context learning performance across various datasets, with greater benefit on tasks exhibiting higher domain-label bias.  Our experiments show that domain-context calibration leads to better decision boundaries and outperforms previous calibration techniques, which often rely on single predefined tokens.  We conclude that domain-context calibration offers a holistic and effective solution for addressing label biases in in-context learning, significantly enhancing the performance of large language models.</sample>
    <sample id="264">Lin Wang from Zhejiang University presents "TAVT: Towards Transferable Audio-Visual Text Generation," addressing the challenges of multimodal text generation with limited data and domain shifts. Existing methods struggle with varying conditions across domains, particularly in audio-visual tasks. The proposed TAVT framework tackles this by learning a unified audio semantic space to align visual concepts across domains.  The framework consists of an audio-visual meta-mapper network, a transformer-based encoder and generator, and Dual Counterfactual Contrastive Learning (DCLL). The meta-mapper network uses audio clustering and learnable visual prefixes to map visual concepts to the audio space. The transformer model incorporates an alpha to evaluate modality contributions and utilizes DCLL to optimize visual-textual alignment.  TAVT employs a meta-learning approach similar to MAML, using support and query sets for rapid adaptation to new domains.  Experiments on MSVD and MSR-VTT benchmarks demonstrate that TAVT significantly outperforms state-of-the-art RNN and transformer-based models, especially in low-resource scenarios. Ablation studies confirm the importance of audio features.  The research highlights the potential of transferable audio-visual text generation for applications with limited labeled data and varying domain characteristics.</sample>
    <sample id="265">Vasudha</sample>
    <sample id="266">This text does not mention the affiliations of the authors.</sample>
    <sample id="268">Omission errors.</sample>
    <sample id="270">Emory NLP Lab, Emory University and Amazon Alexa AI.</sample>
    <sample id="271">CFT stands for Fine-Tuning.</sample>
    <sample id="272">John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams.</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">Ananya and Vignesh present "IndicMT Eval," a dataset and evaluation framework for machine translation metrics in Indian languages. Recognizing the understudied nature of evaluating translations into Indian languages, this work focuses on five languages from Dravidian (Tamil, Malayalam) and Indo-Aryan (Hindi, Marathi, Gujarati) families.  They created 7,000 translation samples using seven translation models and employed bilingual expert annotators to meticulously evaluate these translations, capturing error types (from MQM) and severity.  The study analyzes the correlation between these human-annotated errors and various automatic metrics, including overlap-based (chrF) and embedding-based (LabSE, BERTscore, MuRIL) approaches, as well as COMET variants.  Results reveal that while chrF correlates best with overlap-based metrics, they perform poorly overall. Embedding-based metrics show better correlations, particularly when using multilingual models. COMET-metric variants demonstrate the highest overall correlations, but exhibit skewed score ranges.  Fine-tuning COMET on the annotated data further improves its performance, achieving better correlations than baseline COMET models across multiple languages.  Finally, the study assesses the zero-shot transferability of IndicCOMET and its robustness on the ACES Translation Accuracy Challenge Sets, demonstrating its superior performance compared to baseline metrics. The dataset and code are publicly available.</sample>
    <sample id="277">The new method is called "Multiset Tagging and Latent Permutations".</sample>
    <sample id="278">The "marked words" method identifies words that distinguish marked groups (e.g., women of color) from unmarked groups (e.g., white men) by analyzing the frequency of specific terms and their association with stereotypes. It draws upon the sociolinguistic concept of "markedness" to reveal how dominant groups are linguistically and socially unmarked, while marginalized groups are often marked.</sample>
    <sample id="279">Shangbin is a PhD student at the University of Washington.</sample>
    <sample id="280">Shi Tao's presentation introduces MultiEMO, a novel attention-based framework for emotion recognition in conversations (ERC). The research addresses challenges in existing ERC methods, including inadequate multimodal fusion, poor performance on minority emotions, and difficulty distinguishing semantically similar emotions. MultiEMO proposes a visual feature extractor (VisExtNet) that focuses on facial expressions without redundant scene information.  A key component is MultiAttn, a multimodal fusion network utilizing bidirectional multi-head cross-attention layers to integrate textual, audio, and visual features.  The framework employs a Sample-Weighted Focal Contrastive Loss to improve performance on minority classes and differentiate similar emotions.  Experiments on MELD and IEMOCAP demonstrate state-of-the-art results, particularly in challenging scenarios involving asynchronous emotional cues and minority classes.  Limitations include VisExtNet's inability to distinguish between speakers and background individuals, the need for large batch sizes on MELD for the SWFC loss, and continued performance gaps in minority emotion recognition compared to majority classes. The work contributes a more effective and robust approach to multimodal emotion recognition in conversational settings.</sample>
    <sample id="281">This work investigates the crucial role of context in machine translation, addressing the challenge of evaluating models on context-dependent translations.  The authors extend the Contextualized Word Mover's Distance (CXMI) measure to a pointwise level, enabling analysis of word context usage across 14 language pairs using TED talks.  They identify patterns in word dependencies based on part-of-speech tags, vocabulary items, and individual tokens, revealing that context is vital for translating dual pronouns, verb forms, and proper nouns, as well as resolving ellipses. 

To quantify these context-dependent phenomena, they develop the Multilingual Discourse-Aware (MuDA) tagger, which automatically identifies words related to five discourse phenomena.  A benchmark is then created using MuDA to evaluate document-level machine translation models.  The study finds that while corpus-level metrics like BLEU favor context-agnostic models, context-aware models excel on phenomena like formality and lexical cohesion. However, their performance is comparable to context-agnostic models on phenomena like ellipsis and pronouns.  Furthermore, DeepL demonstrates superior accuracy in document-level translation compared to Google Translate.  The research highlights the limitations of current evaluation methods and provides a framework for future progress in building more accurate and contextually aware machine translation systems.</sample>
    <sample id="282">StoryTrans is a novel approach to non-parallel story author-style transfer, addressing the challenge of imitating authorial linguistic preferences at the discourse level. Existing methods primarily focus on token or sentence-level style transfer, lacking the ability to capture the complex narrative structures and style-specific content associated with authorial writing.  The research proposes a generation model that learns discourse representations from source texts and combines them with learnable style embeddings to generate texts in target styles. A new training objective is introduced to disentangle stylistic features from content, and a two-stage generation process enhances content preservation. The first stage focuses on reconstructing the input and disentangling style and content, while the second stage fills in style-specific content.  Experiments on new datasets in Chinese and English demonstrate StoryTrans's superior performance in style control and content preservation compared to strong baselines, as validated by both automatic and manual evaluations. Style visualization confirms alignment with the golden text's style features.  StoryTrans effectively supplements storylines with relevant phrases and rewrites sentences to match the target style while maintaining semantic meaning. The model's data and code are publicly available.</sample>
    <sample id="283">The Prague approach.</sample>
    <sample id="284">This paper introduces FSUIE, a novel framework for enhancing universal information extraction (UIE) models. Current UIE models heavily rely on precise span boundaries, which are often ambiguous in annotation. FSUIE addresses this by proposing a fuzzy span mechanism, learning span boundaries as continuous probability distributions rather than discrete points.  To overcome the mismatch between transformer feature extraction and span length, we introduce adaptive attention, allowing the model to focus on relevant span boundaries.  A fuzzy span attention mask dynamically adjusts the attention range and linearly decays the attention distribution, guiding the model to prioritize semantic information within a limited context.

FSUIE is evaluated on named entity recognition, relationship extraction, and aspect sentiment triplet extraction tasks. Experiments demonstrate significant performance improvements compared to baseline models, particularly on small-scale datasets.  FSUIE achieves state-of-the-art results on relationship extraction datasets and demonstrates strong generalization capabilities. Ablation studies show that the combination of fuzzy span loss and fuzzy span attention yields the best results, improving convergence speed and information extraction capability. Visualizations confirm that the fuzzy span attention layer focuses on semantic information within a limited range.  FSUIE offers a simple yet effective approach to improve UIE performance across various information extraction tasks.</sample>
    <sample id="285">This work addresses the challenge of factual errors in dialogue summarization, a critical issue often overlooked in existing research. While two main approaches exist – factuality-aware training and Factual Error Correction (FEC) models – the authors argue that current FEC evaluation methods are flawed, leading to a misdirection of research. Existing metrics like FactCC and DAE provide overall scores, lacking granularity and failing to distinguish between genuine error correction and simple summary rewriting. Furthermore, they blur the line between FEC and factuality-aware training.

To address these issues, the authors propose a new evaluation framework based on manually annotated reference corrections, emphasizing minimal edits for fluency and non-redundancy. This provides more valuable data for training FEC models and enables a more comprehensive assessment of their performance.  A novel taxonomy of factual errors, categorized as content-based and form-based, is introduced. The framework leverages ERRANT, encompassing alignment, classification, and comparison steps. Experiments reveal that training FEC models with reference summaries significantly improves performance when compared to unreliable metrics. The study highlights the need for revised evaluation methods and suggests combining human-annotated and synthetic data for improved results.  Current FEC models struggle with specific error types like additions and attribute errors, underscoring the need for further research in this area.</sample>
    <sample id="286">James Finch and Sarah Finch.</sample>
    <sample id="287">Filip Radlinski, Silvia Pareti, Annie Louis, and Javad Hosseini.</sample>
    <sample id="288">What datasets can be used to test syntactic phenomena?

BLiMP and SyntaxGym.</sample>
    <sample id="290">FTw, COSINE</sample>
    <sample id="291">The models are evaluated on public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="294">CamemBERT is initially trained on a 4 GB set of NACHOS data.</sample>
    <sample id="295">Adam Przepiórkowski</sample>
    <sample id="296">This work explores irony detection in natural language processing, addressing the limitations of purely data-driven approaches that rely on a single ground truth. The authors developed the English Perspectivist Irony Corpus (EPIC), a dataset of 300 short conversations collected from social media sources over 1.5 years, annotated by 74 annotators across five English varieties.  They employed crowdsourcing via Prolific, aiming for more informative model outputs than simple binary classification.  The annotation interface was designed to be simple, focusing on the question "Is the reply ironic?" with respect to the context.  

The study investigates how annotator perspectives – including gender, age, nationality, and geographical distribution – influence annotation consistency.  They developed "perspective-aware models" by fine-tuning pre-trained language models on datasets split by annotator groups.  While raw performance showed no significant trends, perspective-aware models demonstrated higher confidence in their predictions compared to standard models.  Further analysis revealed that generational differences and geographical variations in annotator backgrounds correlate with greater disagreement in irony perception.  The findings suggest that incorporating annotator perspectives can improve the reliability and accuracy of irony detection models.</sample>
    <sample id="297">This project investigates coded rhetoric, specifically dogwhistles – terms that convey a hidden meaning to an in-group while appearing innocuous to an out-group. The study addresses the challenge of understanding dogwhistles, which are context-dependent and often used to evade content moderation. Researchers developed a typology and glossary of over 340 dogwhistle terms and symbols, categorized by register, type, and persona, drawing from academic, Wikipedia, and other sources. A case study of historical U.S. political speeches revealed a correlation between racial dogwhistles and the Republican Southern Strategy, with increasing association with conservatism over time.  Experiments with language models, particularly GPT-3, demonstrated varying levels of success in surfacing and identifying dogwhistles, with performance influenced by prompting strategies and the formality of the language.  Furthermore, the project explored how dogwhistles can bypass content moderation by altering toxicity detection scores when replacing explicit slurs with coded terms. The findings highlight the importance of understanding dogwhistles for NLP and linguistics, emphasizing their role in political influence, hateful rhetoric, and the complexities of automated content moderation.</sample>
    <sample id="298">The red best fit line in the graph showed a gradient greater than one, indicating diminishing returns on new test sets, which ruled out adaptive overfitting. Retraining models with more recent data showed performance degradation with larger temporal gaps, confirming temporal drift as the main cause of performance loss.</sample>
    <sample id="299">This work addresses the vulnerability of Natural Language Inference (NLI) models to shortcuts, spurious correlations in training data that lead to strong in-distribution performance but poor generalization to out-of-distribution data. Existing shortcut mitigation methods often require domain-specific knowledge or auxiliary models, limiting their applicability.  The authors propose a novel training method based on minimax optimization to reduce reliance on shortcuts.  The core idea is to emphasize under-represented "hard" training instances – those that contradict dominant "easy" examples – which are crucial for robust generalization.  The learner model minimizes the NLI loss, while the auxiliary model maximizes it, incentivizing the learner to focus on hard examples.  This minimax objective allows the learner to prioritize learning from these counteracting examples without requiring prior knowledge of shortcut types.  The method is evaluated on MNLI, FEVER, and QQP, along with adversarial test sets, demonstrating improved out-of-distribution performance compared to standard ERM training and existing shortcut mitigation techniques, while maintaining high in-distribution accuracy.  The study also investigates the effect of pre-training, auxiliary model size, and the transferability of the method to larger models and synthetic shortcuts.  The authors conclude that this approach offers a promising solution for building more robust NLI models.</sample>
    <sample id="300">## Interactive Dictation: A New Approach to Voice-Enabled Document Editing

This work introduces interactive dictation, a novel task enabling users to dictate and edit documents in a natural, conversational manner. Unlike current speech-to-text systems focused solely on dictation, interactive dictation allows for seamless interleaving of dictation and command utterances, facilitating intuitive corrections and modifications.  The system supports open-ended natural language commands, avoiding the need for rigid template commands found in existing solutions.

The research formalizes interactive dictation as a four-step process: ASR recognition, utterance segmentation, command normalization, and sequential execution.  A new data collection interface and dataset are designed to support this task.  A baseline system is developed, employing T5 and GPT-3 architectures for ASR repair and state prediction.  Experiments demonstrate a trade-off between runtime and accuracy, with GPT-3 achieving higher accuracy but slower performance.  Predicting the final state directly yields better results than predicting intermediate programs.  The authors release code and encourage further research in this promising area.  The key contribution is the formalization of interactive dictation and the development of a system to address the limitations of existing speech-to-text technologies.</sample>
    <sample id="302">The output tokens are not ordered after the first step, so a permutation model is needed to put them in the correct order.</sample>
    <sample id="303">The authors recommend increased transparency about bias mitigation methods because they don't know if positive stereotypes are a result of value alignment or other methods, and thus cannot study them further without more transparency.</sample>
    <sample id="304">Minimal-pair unacceptable inputs are sentences chosen from the same data set (like BLiMP or SyntaxGym) but are considered ungrammatical or unacceptable by the model.</sample>
    <sample id="305">This presentation introduces "Weaker Than You Think: A Critical Look at Weakly Supervised Learning," a joint work by Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. The work challenges the common claim that weakly supervised learning (WSL) methods achieve high performance solely on weakly labeled data, often relying on an additional clean validation set. The authors investigate the necessity of clean validation data, the optimal number of clean samples, and the best utilization of clean data.

Their findings reveal that recent WSL methods *require* clean validation samples for proper generalization, with performance drops observed without them.  Increasing the number of clean validation samples improves performance, and direct fine-tuning on clean data yields even better results than WSL approaches.  Furthermore, the performance gains often attributed to WSL can be achieved by allowing continued fine-tuning on clean validation samples, rendering more complex WSL methods computationally less attractive.

The authors conclude that the performance benefits and practicality of current WSL approaches are often overestimated. They recommend reporting model selection criteria, comparing WSL with few-shot learning, and considering continuous fine-tuning as a baseline. The code for the work is open-sourced.</sample>
    <sample id="306">Sebastian Schuster and Najoung Kim present research on entity tracking in large language models (LLMs), a crucial ability for understanding discourse. The paper addresses the challenge of evaluating this capability, highlighting potential pitfalls such as reliance on common pre-training data, heuristic associations, and memorization of entity states through fine-tuning. To overcome these issues, they designed a novel task involving boxes and objects, where models predict the contents of boxes after state-changing operations.

Experiments with Flan-T5 and GPT-3/3.5 demonstrate that most models primarily copy the initial state, achieving high accuracy on the "copy" cases. However, text-davinci-003 exhibits non-trivial tracking, suggesting a capacity for state evolution. Further analysis reveals that models trained on code show stronger entity tracking abilities, while fine-tuning can enable smaller models to learn the task. The research underscores the importance of pre-training in developing entity tracking capabilities in LLMs, but questions remain about the generalizability of these findings. The authors encourage readers to consult their paper on arXiv for detailed results and analysis, and to contact them with questions.</sample>
    <sample id="307">Named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="308">## NLPositionality: Characterizing Design Biases in NLP Datasets and Models

This work investigates design biases in Natural Language Processing (NLP) datasets and models, focusing on the role of positionality – the influence of demographics, identity, and life experiences on research.  The study addresses the concern that NLP technologies may exhibit systematic performance differences across populations, exemplified by the varying sensitivity of toxicity detection APIs to different cultural contexts. 

To characterize this positionality, the researchers developed the NLPositionality framework. This framework involves re-annotating datasets with diverse annotators, collecting demographic data, and comparing these annotations to existing datasets and models using correlation scores.  The study leverages Lab in the Wild, an online crowdsourcing platform, to recruit a large and diverse pool of annotators from 87 countries.

The analysis reveals that NLP datasets and models tend to align most strongly with English-speaking countries and individuals with college education. However, this alignment leaves certain populations behind, such as non-binary individuals.  The findings highlight the importance of considering positionality in NLP research and development.  The study recommends documenting design choices, adopting a perspectivist approach, and building specialized datasets and models tailored to specific communities.  The research underscores the need for inclusive NLP that benefits all users, not just those within dominant demographics.</sample>
    <sample id="309">Doubly-labeled conversations.</sample>
    <sample id="310">Wikipedia.</sample>
    <sample id="311">The provided text does not mention the affiliations of the authors.</sample>
    <sample id="312">MultiInstruct is the first large-scale multi-modal instruction tuning dataset, unlike existing language-only instruction datasets. It contains 62 diverse multi-modal tasks covering 10 categories, derived from 21 open-source datasets, and includes five expert-written instructions per task.</sample>
    <sample id="313">Two.</sample>
    <sample id="314">"Binary coordination" refers to the coordination of two elements, such as two conjuncts in a sentence, where the relationship between them is governed by a coordinating element like a conjunction.</sample>
    <sample id="315">This information is not provided in the text.</sample>
    <sample id="316">The findings suggest that smaller, specialized models like T5 can outperform larger language models when trained on a suitable dataset like CoScript, highlighting the potential for efficient language planning with smaller resources.</sample>
    <sample id="317">CodeIE addresses the challenge of mismatched outputs in large language models (LLMs) for information extraction (IE) tasks. Traditional LLMs like T5 and GPT-3, trained on text-to-text formats, struggle to generate structured outputs required for IE, often requiring extensive structured training data and specialized decoding. CodeIE proposes transforming IE into a structure-to-structure code generation task using code-based LLMs like Codex. This approach allows for seamless conversion between text input and structured output during inference, improving alignment and performance.

The method involves designing prompts that instruct the LLM to generate code for extracting and structuring information.  Experiments on named entity recognition and relation extraction datasets demonstrate that CodeIE significantly outperforms traditional baselines (T5, UIE, GPT-3) and other code-based models (CodeT5, code-davinci-002).  Analysis reveals that code-based prompts lead to lower perplexity, fewer structural errors, and better recall, particularly in relation to information extraction.  Furthermore, Codex consistently outperforms GPT-3 across various IE tasks.  The study highlights the benefits of aligning LLMs with the structured nature of IE and suggests that code-based prompting is a promising direction for improving performance and reducing the reliance on large structured datasets. The paper and code are publicly available.</sample>
    <sample id="319">The work investigates the impact of different pre-training strategies, including:

*   From-scratch pre-training
*   Continual pre-training using CamemBERT weights and tokenization
*   Pre-training with different data sources (NACHOS, clinical notes, PubMedBERT)</sample>
    <sample id="320">The paper found that adaptive overfitting, caused by reusing the same test set over and over again, is not the main cause of performance drop in this case. The gradient of the best fit line is greater than one, indicating diminishing returns, which is not characteristic of adaptive overfitting.</sample>
    <sample id="321">The quality of simplification was evaluated using manually aligned sentence pairs as a gold standard for evaluating automatic alignment methods.</sample>
    <sample id="322">Enrico's presentation at ACL 23 explores how text classifiers learn about morality, addressing the limitations of treating morality as a single, subjective scale. He argues that morality is pluralistic and context-dependent, influenced by social theories like the Moral Foundation Theory, which posits five distinct ways humans perceive morality and prioritize different foundations. 

The presentation focuses on understanding what language models learn when tasked with understanding morality in text.  Enrico's team utilizes explainable AI techniques and a dataset of 35,000 tweets from seven domains (e.g., #AllLivesMatter, #BlackLivesMatter) to investigate whether models can discern differing moral expressions across these domains.  The research demonstrates that language models recognize variations in moral language, such as the differing connotations of "subversion" in #AllLivesMatter versus #BlackLivesMatter. 

The study highlights the danger of using a single model for diverse domains, as it can lead to misunderstandings of morality.  The findings underscore the importance of considering context and domain-specific nuances when developing language models for moral reasoning.  The research aims to provide insights into the fine-grained differences in moral expression and the potential pitfalls of simplistic approaches to moral understanding in NLP.</sample>
    <sample id="323">This paper introduces Dynamic Heterogeneous-Graph Reasoning (DHLK) for Commonsense Question Answering (QA), addressing limitations in existing knowledge-augmented language models. Current approaches often suffer from noisy entity retrieval, isolated encoding of knowledge and text, and a lack of semantic relationship modeling. DHLK tackles these issues by constructing a highly optimized heterogeneous knowledge graph (HKG) using a two-stage pruning strategy and knowledge representation learning (KRL).  The HKG incorporates entity and relation embeddings derived from multiple knowledge bases (ConceptNet, WordNet, Wiktionary) and is further refined using TransE.  A key innovation is the use of Relation Mask Self-Attention (RMSA) to model the subgraph, inspired by RGAT, allowing for dynamic entity removal based on attention weights.  The HKG path information is then integrated with the QA context through path enhancement.  Finally, a multi-layer perceptron (MLP) predicts the answer probability based on the HKG embedding, path information, and QA context embedding.  Experiments on CommonsenseQA and OpenBookQA demonstrate that DHLK achieves competitive results compared to existing language model and HKG methods, highlighting the effectiveness of its dynamic reasoning and knowledge integration approach.</sample>
    <sample id="324">Language models do have varying political leanings, occupying all four quadrants on the political spectrum, with GPT-4 being the most liberal and GPT series generally more socially liberal than BART series.</sample>
    <sample id="326">Cognitive dissonance is the inconsistency between two beliefs or actions, such as stating one belief and then acting in a way that contradicts it. It's a common phenomenon in daily decision-making but rare in language, making it a valuable area for study to understand various aspects of human behavior, including mental health, extremism, and decision-making processes.</sample>
    <sample id="327">ManagerTower is a novel vision-language (VL) model architecture designed to improve cross-modal representation learning by aggregating insights from multiple unimodal experts. Building upon BridgeTower, ManagerTower introduces "managers" within each cross-modal layer to adaptively combine information from pre-trained visual and textual encoders at different levels of abstraction. This allows for more comprehensive cross-modal alignment and fusion, addressing the limitations of previous architectures that treat all unimodal layers uniformly.

The proposed architecture utilizes RoBERTa and CLIP-ViT as unimodal encoders and demonstrates superior performance on VL tasks, achieving 39.15% accuracy on the Wikivideo test standard, significantly outperforming BridgeTower. ManagerTower's effectiveness is validated through visualization of aggregation weights, revealing that adaptive managers dynamically adjust their influence based on the specific cross-modal layer and unimodal expert, leading to more nuanced and effective knowledge integration.  The model's performance is comparable to, and in some cases surpasses, models trained with larger datasets and more parameters.  ManagerTower's adaptability and ability to leverage diverse unimodal semantic knowledge make it a promising advancement in the field of vision-language representation learning. The paper, code, and model are publicly available on Archive and GitHub.</sample>
    <sample id="328">GPT-4 is the most liberal language model.</sample>
    <sample id="329">Minghang Zheng from Peking University presents "Generating Structured Pseudo Labels for Noise-resistant Zero-shot Video Sentence Localization." This work addresses the challenge of zero-shot video sentence localization, aiming to identify relevant video segments based on natural language queries. Traditional methods rely on manual annotations, which are costly. The proposed approach generates structured pseudo-labels to train models without manual annotation, mitigating the drawbacks of existing zero-shot methods. These drawbacks include simplistic pseudo-queries and a lack of alignment between pseudo-queries and events, as well as ignoring label noise.

The proposed method generates complex pseudo-queries using a pre-trained image caption model, leveraging its strong zero-shot generalization capabilities. It then models the temporal structure of events to generate pseudo-events, ensuring high relevance within events and low relevance outside.  A sliding window approach identifies the most relevant pseudo-events based on similarity between video frame features and query text features.  To address label noise, the method estimates label noise based on model confidence and IoU, weighting noisy samples and refining predictions in subsequent training rounds. Experiments on ActivityNet Captions and Charades-STA demonstrate superior performance compared to existing zero-shot methods, achieving state-of-the-art results. The code is publicly available via a QR code.</sample>
    <sample id="330">Cumulative training performed equal or better than iterative when doing active learning.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">TED talks.</sample>
    <sample id="333">INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation addresses the limitations of neural machine translation (NMT) models, particularly their non-smooth representation spaces that hinder generalization. The core idea of kNN-MT is to smooth predictions by leveraging nearest neighbors in the representation space. However, kNN-MT suffers from slow retrieval and static datastore updates.  INK overcomes these drawbacks by introducing an iterative training loop that injects kNN knowledge into the NMT model.  The framework utilizes an adapter to adjust representations guided by kNN data, with updated representations asynchronously refreshing the datastore.  This process aligns contextualized representations with token embeddings, kNN token embeddings, and target token representations to address sparsity.  Experiments on the WMT’19 German-English news translation task demonstrate that INK significantly outperforms state-of-the-art kNN-MT, achieving a 1.99 COMET score and 1.0 BLEU score gain.  INK achieves this with less memory and faster inference speed.  The results suggest that jointly applying adapters and datastores further refines the representation space.  INK offers a novel approach to iteratively refine NMT model representations, leading to improved translation performance and efficiency.</sample>
    <sample id="335">Matthias Lindemann.</sample>
    <sample id="336">Cross-lingual transfer refers to the ability of a model trained on one language to perform well on another language, especially when there is limited data available for the target language. The presentation highlights that the performance gap between zero-shot and few-shot cross-lingual transfer is significant, but the gap is reduced with few-shot learning.</sample>
    <sample id="337">This paper introduces a novel approach to learning word embeddings for out-of-vocabulary (OOV) words by leveraging word formation and association. The proposed method constructs a Word Relationship Graph, representing words and wordpieces as nodes connected by relationships reflecting lexical rules.  An OOV word is tokenized and associated with relevant words, creating a two-level graph.  A self-attention network assigns node attributes based on character information, and Graph Attention Networks are used to refine node representations. A readout block generates a graph-level representation.  Contrastive learning is employed in the loss function to align OOV words with relevant neighbors and background embeddings, encouraging proximity in the embedding space.  Experiments demonstrate superior performance compared to baselines on both intrinsic and extrinsic tasks. The model benefits both static and contextual embedding models.  The authors discuss the potential for extending the model to other languages, particularly agglutinative languages, while acknowledging the challenges posed by fusional languages. The core contribution lies in the ability of the graph-based approach to effectively handle complex word formations and learn meaningful representations for OOV words.</sample>
    <sample id="338">Bingsheng presents research on evaluating the helpfulness of human natural language explanations for machine learning models. The work addresses the challenge of objectively assessing explanations, which are often subjective and task-dependent, unlike labels. The research introduces a unified data format to standardize tasks and facilitate comparison across datasets like CoS-E, ECQA, e-SNLI, and ComVE.  Experiments analyze the utility of explanations by varying fine-tuning and inference settings, revealing that explanations can improve model performance, particularly during fine-tuning. 

A novel evaluation metric, TREU, is proposed to extend the existing simulatability score by specifically assessing explanation helpfulness during fine-tuning.  The study evaluates TREU and simulatability scores on T5 and BART models across five datasets. Results demonstrate that TREU outperforms simulatability in reflecting the helpfulness of explanations, especially for tasks like natural language inference where negation and counterfactuals are crucial. The research highlights the task-dependent nature of explanation utility and suggests that future work should incorporate quality checks for human annotation to ensure high-quality human-model collaboration. The findings contribute to a more robust and reliable evaluation of human explanations in NLP.</sample>
    <sample id="339">Saarland University, Germany.</sample>
    <sample id="340">ParaAMR is a novel, large-scale paraphrase dataset created using AMR back-translation. Addressing the limitations of existing datasets like MRPC and the lack of syntactic diversity in automatically generated datasets, ParaAMR leverages Abstract Meaning Representations (AMR) graphs to generate paraphrases. The method involves modifying the AMR graph by randomly sampling a node as the new root, altering edges and labels, and then using an AMR graph-to-text generator to produce text. This process ensures semantic similarity while introducing syntactic variation.

ParaAMR contains approximately 15 million source sentences, with 6.9 paraphrases per sentence, offering a significant scale compared to previous work. Quantitative analysis reveals that ParaAMR achieves high semantic similarity scores with other back-translation datasets but demonstrates superior syntactic diversity.  The dataset benefits various NLP applications, including sentence embedding learning, syntactic control paraphrase generation, and data augmentation for few-shot learning.  Experiments show that sentence embeddings trained on ParaAMR outperform those trained on other datasets, and paraphrase generators trained on ParaAMR exhibit better syntactic control.  Furthermore, ParaAMR leads to improved performance in few-shot learning scenarios. The authors conclude that ParaAMR provides a valuable resource for advancing paraphrase generation and related NLP tasks. The dataset is publicly available.</sample>
    <sample id="341">BLEU score and average lagging (and computational-aware average lagging).</sample>
    <sample id="342">LiveChat is a novel, large-scale, personalized dialogue dataset constructed from live streaming videos on Chinese platforms like TikTok and Douyin. Addressing the limitations of existing text-based dialogue datasets, LiveChat incorporates video sources, enabling more realistic conversational data. The dataset is built through three key steps: video scraping, audio extraction and transcription, and dialogue construction using a reply-to-whom matching method.  Persona information is extracted through manual labeling and rule-based/classifier approaches, enabling personalized dialogue generation. 

The paper evaluates LiveChat on response modeling and addressee recognition tasks, demonstrating that persona profiles and longer average session lengths are beneficial for learning personalized responses. Experiments with pre-trained dialogue models, including BART, reveal the distinctiveness of LiveChat's domain.  While LLMs show strong performance, particularly with in-context learning, performance degrades with excessive demonstrations due to noise.  The research highlights the importance of video-sourced data and personalized information for developing more realistic and effective dialogue systems, particularly in the context of virtual assistants and multi-party conversations. Future work will focus on efficient transfer learning techniques for large language models on the LiveChat dataset.</sample>
    <sample id="344">Tree-based methods require obtaining trees, which can be complicated, computationally expensive, and involve formalism-specific pre-processing or grammar-induction procedures.</sample>
    <sample id="345">This paper introduces a novel neural sequence-to-sequence model for compositional generalization in semantic parsing, achieving strong performance without relying on explicit tree structures. Compositional generalization refers to the ability to handle unseen combinations of phrases seen during training, a crucial capability for robust semantic parsing.  Traditional tree-based approaches require tree construction, which can be complex and computationally expensive.  Our method directly models the correspondence between input and output fragments using a multiset tagging approach.  The model first tags input tokens with a multiset of potential output tokens, and then a separate permutation model predicts the order of these tokens.  This permutation model avoids hard constraints, offering flexibility and expressiveness.  The paper addresses the challenge of unknown alignment between input and output tokens and the difficulty of identifying the linguistically correct permutation, which is often latent.  We tackle these issues by inducing alignment during training and approximating the NP-hard permutation problem with a GPU-friendly continuous relaxation.  Experimental results on the COGS benchmark demonstrate significant generalization to deeper recursion compared to other treeless models.  The paper highlights the technical challenges of alignment and permutation prediction and offers solutions for training a flexible and effective model for compositional generalization.</sample>
    <sample id="346">The provided text does not mention the affiliations of the authors.</sample>
    <sample id="348">This paper investigates the prevalence of stereotypes in large language models (LLMs) by prompting them to generate personas of individuals from various demographic groups.  Unlike traditional methods relying on curated datasets, the authors leverage the instruction-following capabilities of LLMs to generate diverse personas, allowing for a more generalizable assessment of bias.  The study employs a "Marked Words" method, drawing on sociolinguistic "markedness" theory, to identify words that distinguish marked groups (e.g., women of color) from unmarked groups (e.g., white men).  

The research reveals that while LLMs generate personas with a high frequency of stereotypical language, the analysis of the word distributions highlights subtle, often positive-seeming, stereotypes.  These patterns, such as "culture," "tradition," and "exotic," contribute to harmful narratives of othering and essentialization.  The study further analyzes tropes associated with different groups, including "tropicalism" for Latina women, hyper-sexualization for Asian women, and the "Strong Black Woman" archetype.  The authors conclude with recommendations for model owners, emphasizing the need for research to address positive stereotypes, adopt an intersectional lens, and increase transparency regarding bias mitigation methods to better understand and combat harmful biases in LLMs.</sample>
    <sample id="350">This paper investigates the reliability of leaderboard scores in Natural Language Understanding (NLU), particularly concerning the meaning of "superhuman performance" achieved by large language models (LLMs) on benchmark datasets like SuperGLUE and SQuAD. The authors argue that current leaderboard-based evaluations are insufficient for assessing true understanding, as they are susceptible to spurious correlations, lack robustness, and fail to account for the complexities of human reasoning.

The study reveals that while LLMs often outperform humans on these benchmarks, the comparisons are often unfair due to differences in evaluation sets, flawed ground-truth answers, and unreliable human baselines.  The paper highlights issues such as small human evaluation subsets, errors in data annotation, and inconsistent compensation for human annotators.  Furthermore, the lack of transparency regarding annotator pools undermines the validity of human-to-system comparisons.

The authors conclude that claims of superhuman performance are not yet scientifically meaningful and advocate for more rigorous and comprehensive benchmark design. They propose recommendations to address these limitations and construct more reliable evaluations that better reflect true NLU capabilities. The paper emphasizes the need to move beyond simple performance metrics and consider the underlying reasoning and knowledge involved in language understanding.</sample>
    <sample id="351">This paper investigates the generalization capabilities of CoNLL-2003 named entity taggers in 2023, a task developed nearly two decades ago. The study addresses the question of whether these models can still perform well on modern data and what factors contribute to good generalization. To evaluate this, the authors created the CoNLL++ dataset, a collection of Reuters News data annotated with the original CoNLL-2003 guidelines. Over 20 models were fine-tuned on CoNLL-2003 and evaluated on both datasets, measuring performance changes using F1 scores.

The research identifies three key ingredients for good generalization: advanced model architectures (specifically transformer models), larger model sizes, and a sufficient number of fine-tuning examples.  The authors ruled out adaptive overfitting as the primary cause of performance degradation by observing a consistent improvement in CoNLL++ with each improvement on CoNLL-2003.  Instead, they concluded that temporal drift, caused by the increasing gap between training and testing data, is the main driver of performance drops.  The study concludes that CoNLL-2003 taggers remain effective in 2023, but further research is needed to improve their generalization. The authors encourage further investigation and provide access to the dataset and paper for those interested.</sample>
    <sample id="352">Annotating Behaviors in Chat.</sample>
    <sample id="353">## Abstract

This paper addresses the challenge of input underspecification in code generation, a critical issue in program synthesis. State-of-the-art methods struggle with scenarios where natural language descriptions lack crucial details, particularly at the operation level. To tackle this, we introduce a novel approach: code generation by asking clarification questions (CQ-driven code generation). We hypothesize that interactive clarification can help gather missing specifications.

We propose CodeClarQA, a synthetic dataset of clarification questions for key operations, and a pipeline for code generation that leverages these questions. Our method identifies missing key operations by comparing natural language descriptions to operation documentation using schema similarity. We demonstrate that this approach effectively identifies missing operations, with MPNet achieving strong performance.

We analyze the results, highlighting challenges such as distinguishing aligned operations requiring clarification and the use of documentation versus argument values.  Our pipeline, comprising a Clarification Need Predictor, Question Selector, and Code Generator, improves code generation performance, particularly when more high-ranked clarification questions are addressed. However, the pipeline still lags behind model-only training, reflecting the difficulty of the task.  We conclude that clarified key operations contribute to better code generation, though the task remains challenging due to the inclusion of novel clarification questions in the evaluation. We encourage further research and feedback on this work.</sample>
    <sample id="354">The performance delta between CoNLL-2003 and CoNLL++ is higher than 5 percentage points until the performance degrades with larger temporal gaps, confirming temporal drift. The paper doesn't specify an exact year, but the experiments were conducted with data from 2020 onwards, implying the performance delta is higher until at least 2020.</sample>
    <sample id="356">Alexander Koller and Ivan Titov.</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig.</sample>
    <sample id="359">State-of-the-art architecture specifically tailored for simultaneous pre-translation.</sample>
    <sample id="361">CounterComp addresses the challenge of compositional generalization in multi-step quantitative reasoning, specifically question answering over financial tables. State-of-the-art models struggle with tasks requiring more than two reasoning steps due to memorization of spurious patterns, such as associating repeated tokens with common operations.  The CounterComp approach leverages counterfactual scenarios to mitigate this issue.  It treats training samples as anchors and mines positive and negative examples by intervening in the question to assess output changes.  An auxiliary metric learning loss is then added to the training procedure, dynamically adjusting its margin based on the extent of intervention. This loss encourages the model to attend to more meaningful tokens related to operations in the output.  Experiments demonstrate that CounterComp consistently improves performance on in-distribution samples, particularly for tasks with more than two reasoning steps.  Crucially, it also enhances performance on out-of-distribution samples, indicating improved compositional generalization.  Qualitative analysis further supports the effectiveness of CounterComp by showing improved token attention during training.  The research aims to avoid costly human supervision by leveraging the inherent interchangeability of components in questions and outputs.  The findings suggest a promising avenue for building more robust and generalizable models for complex reasoning tasks.</sample>
  </task>
</testset>