<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="zh">
    <sample id="0">语言模型主要的数据来源是大型Web爬取数据，其中包含了大量的政治新闻媒体内容。</sample>
    <sample id="1">McGill University, Mila, and Microsoft Research.</sample>
    <sample id="2">Tu Yi from Ant Group presented their team's paper on Visually-rich Document Understanding (VrDU), focusing on addressing the limitations of existing pre-training models for documents. These models often rely on global 1D token positions, which struggle with understanding document layout and reading order.

The proposed solution, LayoutMask, introduces "local 1D positions" based on in-segment token orders, aiming to enhance text-layout interactions.  LayoutMask differs from previous work in its choice of 1D position, masking strategy, and pre-training objectives.  It utilizes Masked Language Modeling (MLM) with two novel masking strategies: Whole Word Masking (challenging, promotes context understanding) and Layout-Aware Masking (prioritizes masking at segment boundaries, fostering cross-segment order learning).  Furthermore, LayoutMask incorporates a new pre-training objective, Masked Position Modeling (MPM), which involves recovering randomly masked 2D positions, encouraging spatial reasoning and improving layout representation.

Experiments on datasets like FUNSD and SROIE demonstrate that Local-1D outperforms Global-1D, particularly in cases with entities like "Total" that are difficult to identify based on a simple global reading order.  The performance gap is observed across different datasets, with Local-1D showing better results on SROIE, especially when dealing with entities that have both vertical and horizontal layout.

The paper highlights the importance of incorporating text and layout information for better document understanding. LayoutMask's approach to local 1D positions and novel masking strategies effectively promote text-layout interactions, leading to improved performance in VrDU tasks. The team encourages further discussion and provides contact information for inquiries.</sample>
    <sample id="3">你好！欢迎参加我们关于 DEPLAIN 的演示，DEPLAIN 是一个用于文档级别和句子级别德语文本识别的新语料库。我的名字是 Regina Stodden，我将引导您通过演示的第一部分。首先，我们来定义文本简化。文本简化是指调整文本以提高其针对特定目标群体（例如，有阅读障碍或非母语人士）的理解能力的过程。为了训练文本简化模型，我们需要对文本进行对齐的对等文本对，例如文档或句子。这里，您可以看到一个对齐的句子对，其中一个复杂的德语句子及其翻译成简单语言的版本。为了简化句子，可以使用不同的技术，例如词汇替换、子句删除、重新排序或插入单词，正如您在这里看到的。因此，我们提出我们的新语料库 DEPLAIN，因为近年来，现有语料库存在一些问题。例如，这些语料库太小，无法训练文本简化模型。近年来提出的其他三个模型都是自动对齐的，这意味着它们可能存在错误。因此，我们提出我们的新语料库 DEPLAIN，它分为两个子语料库：DEPLAIN-apa 和 DEPLAIN-web。DEPLAIN-apa 基于新闻文本。在 DEPLAIN-apa 中，我们对 483 个文档进行了手动对齐，这产生了大约 13,000 对对等句子。对于 DEPLAIN-web，这个语料库包含各种领域，我们还对这 750 个文档进行了手动和自动对齐。总共有 30,450 对句子。我们对这些句子进行了进一步分析，例如，在简化类型方面，可以看到圣经文本比新闻文本或语言学习文本更强地简化。在所有级别上，例如词汇简化、结构简化，以及整体简化水平方面，我们都观察到这一点。此外，您还可以看到我们的 DEPLAIN 语料库具有多种不同的简化转换。例如，在 DEPLAIN-apa 语料库中，我们有更多的重新排序和单词添加，而在 DEPLAIN-web 语料库中则有更多的重新表述。现在，让我们看看我们可以用这个语料库做什么。你好，我是 Omar，现在我将介绍 DEPLAIN 数据集的用例。对于第一个用例，我们可以评估自动对齐方法。近年来，已经出现了很多对齐方法，但在机器翻译的背景下，当我们有两份用不同的语言书写的平行文档，并且想提取两个文档中句子之间的对齐时，我们的用例是尝试提取两个具有相同语言但复杂度不同的平行文档中句子之间的对齐。现在，由于我们拥有 DEPLAIN 数据集，其中包含手动对齐的句子，我们可以使用这些句子作为金标准对齐来评估一些提出的对齐方法。我们对这些方法进行了一些修改，并将所有修改后的代码发布在论文中。最终，我们得出结论，对于德语文本简化，最佳的自动对齐方法是 MASSalign 方法。您也可以在论文中找到运行此方法的代码。第二个用例是我们论文中展示的一个自动文本简化案例，即通过使用语言模型对复杂输入文本生成简化文本。我们对两个不同的模型进行了微调。我们微调了 long-mBART 模型以生成文档级别的简化，并且我们还微调了 normal base mBART 模型以生成句子级别的简化。您也可以在论文中找到所有检查点以及有关实验分数和评估指标的更多详细信息。我们得出结论，这种基本的微调可以产生优于基线分数的结果，并将其作为未来自动文本简化问题的基准提出。感谢您的关注，我们希望在会议上见到您。谢谢。</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">T5 XL model.</sample>
    <sample id="6">Jiaan and colleagues present their work "Towards Unifying Multi-Lingual and Cross-Lingual Summarization," introducing a new framework called many-to-many summarization. This approach aims to build a single model capable of summarizing documents in any source language into any target language, unifying previous multilingual and cross-lingual summarization methods.

The paper analyzes the differences between these approaches, highlighting that multilingual models summarize in the same language as the input, cross-lingual models summarize in a different language, and many-to-many models encompass both.  They demonstrate that many-to-many summarization facilitates better knowledge transfer across languages compared to the other two.

To achieve this, they propose PISCES, a pre-trained many-to-many summarization model. PISCES utilizes a three-stage pre-training process: meta-pretraining for generating original sentences from noisy counterparts, cross-lingual pre-training for generating target language sentences from noisy parallel sentences, and task-specific pre-training using pseudo many-to-many summarization samples.

Preliminary experiments on the WikiLingua dataset show that the many-to-many model trained with PISCES outperforms multilingual and cross-lingual models. Ablation studies and human evaluations further validate PISCES's effectiveness. The authors encourage readers to consult their paper for more details.</sample>
    <sample id="7">是的，根据论文结论，CoNLL-2003 标注器在 2023 年仍然有效。</sample>
    <sample id="8">ABC-Eval 通过明确标注模型响应中表达的具体行为（如忽略对话伙伴、说无关内容、自相矛盾等），从而减少了人工评估的 субъективность，并提供更精确和可靠的对话质量评估方法。</sample>
    <sample id="9">现有弱监督方法的成功在很大程度上依赖于**使用干净的验证样本**。</sample>
    <sample id="10">*   **提供更全面的背景知识：**  让语言模型获得与标注者相同的背景知识，例如通过提供 Google 搜索链接（歌曲）或维基百科文本（书籍、食谱）。
*   **利用更丰富的背景信息：**  除了实体名称，提供更详细的背景信息，例如图像（食谱）或信息框（歌曲）。
*   **使用更具代表性的采样方法：**  在生成替代问题时，使用更贴近实体相似性的采样方法，例如基于标题、描述或属性的相似度。
*   **考虑模型检索到的知识：**  评估模型仅使用实体名称进行知识检索时的准确性，并努力提高其准确性。
*   **探索模型泛化能力：**  进一步研究模型在不同领域的泛化能力。</sample>
    <sample id="11">Jack Hessel from AI2 presents research on humor understanding using The New Yorker Caption Contest data. The study investigates whether large language models (LLMs) truly "understand" humor, contrasting their performance with human capabilities.

The research utilizes The New Yorker Caption Contest, a popular contest with a long history, to create three tasks: matching (identifying the correct caption), quality ranking (judging caption quality), and explanation generation (explaining why a caption is funny).

The matching task shows a significant gap between LLMs and humans, with a CLIP-fine-tuned model achieving 62% accuracy compared to human-level 94%.  GPT-4, when given a textual description of the cartoon, still lags behind humans.

The explanation generation task reveals further limitations. GPT-4 often produces inaccurate explanations, as demonstrated by its incorrect interpretation of the "He'll be back" caption. Human evaluations consistently prefer human-authored explanations over GPT-4's.

The study highlights that while LLMs can generate jokes and even attempt to explain them, they lack true humor understanding.  The research provides a valuable dataset and leaderboard for further exploration of humor in AI. The findings underscore the challenges of replicating human-like understanding of humor in artificial intelligence. The dataset and models are publicly available.</sample>
    <sample id="12">5</sample>
    <sample id="13">Daniel Rotem's presentation details research on adaptive inference for large language models, aiming to reduce inference time and cost. The study compares two common methods: Multi Model and Early Exit. Multi Model stores multiple models with classifiers, sequentially running them until a classifier decides to stop, offering versatility but at the cost of storage and overhead. Early Exit employs classifiers at intermediate transformer layers, enabling faster inference but potentially suffering from conflicting gradients – where classifiers' updates interfere with each other, degrading performance.

The research hypothesizes that conflicting gradients are a key issue in Early Exit, and experiments confirm this, showing that Multi Model outperforms Early Exit, especially with earlier classifiers.  The study then introduces SWEET (Separating Weights in Early Exit Transformers), a novel fine-tuning method that addresses conflicting gradients by training each layer to receive updates only from the subsequent classifier.  

Experiments demonstrate that SWEET significantly reduces the performance gap between Early Exit and Multi Model, particularly for later classifiers. While SWEET sometimes negatively impacts later layers, it generally outperforms both methods in terms of speed and accuracy, especially for BERT-Large.

The key takeaways are the existence of conflicting gradients in Early Exit training, the first comprehensive comparison of these methods, and the introduction of SWEET as a promising fine-tuning approach. The research motivates further exploration of tailored algorithms for Early Exit architectures. The presentation concludes with a call to visit the paper on Archive for more information.</sample>
    <sample id="14">Adam Przepiórkowski 先生的演讲是关于协调的依赖结构。正如您可能知道的，不同的理论和语料库方法假设了不同的依赖结构。例如，在通用依赖中，协调结构（Lisa, Bart, and Maggie）的结构是这样，即第一个连词是整个协调结构的头。在这种情况下，第一个连词是 Lisa。一个类似的观点也适用于 Igor Mel'čuk 的意义文本理论，同样，整个协调结构由第一个连词构成。这两者都是非对称的。现在，这些是关于协调结构的非对称方法，例如布拉格方法，它假设协调结构由连词构成。因此，我们从每个连词的左右两侧都得到依赖关系。最后，还有一种多头方法，例如 Hudson 的词法语法，它认为所有连词都是协调结构的头。因此，我们从协调的支配者得到所有连词的依赖关系：Lisa, Bart, and Maggie。本文旨在为协调的对称结构（如上述两种）提供一个新论证，并反对协调的非对称结构（如上述两种）。该论证基于依赖长度最小化的原则，我将在这些例子的基础上解释。在英语中，正如您可能知道的，直接宾语更喜欢靠近动词，而修饰语则可能更靠后。例如，“Marge read it yesterday”是正确的，因为直接宾语靠近动词，而“Marge read yesterday it”则不正确，因为介于动词和直接宾语之间的是一个修饰语“yesterday”。然而，当直接宾语非常重且很长时，这种效果可能会得到缓解。这在以下例子中有所说明：“Marge read this absolutely fascinating book about bees yesterday.”这在某种程度上是正确的，我们可以用“this absolutely fascinating book about bees”代替“it”。这里的推理是，即使句子违反了直接宾语应该靠近动词的普遍语法原则，它也满足了依赖长度最小化的原则，即更短的依赖关系更受欢迎。这些树只显示了关键依赖的长度，这些依赖在这些结构中没有变化。在这里，从“read”到修饰语的依赖长度为 7 个单词，从“read”到“book”的依赖长度为 4 个单词，总共为 11 个单词。如果将这两个成分互换，那么这两个依赖的长度之和变为 6 个单词，这比 11 个单词要短得多，因此看起来还算可以。这违反了一个原则，但满足了另一个原则。好的，我们提取了各种协调的统计数据，这些数据来自增强版本的 Penn Treebank，并且在论文“Why wouldn't you use universal dependencies”中得到了证实，这已经多次被观察到，左侧连词通常更短。“salt and pepper”而不是“pepper and salt”，以音节为单位衡量，以及在句法分析中观察到的，这种趋势随着长度差异的增加而增加。当两个连词长度的差异增大时，更短的连词更喜欢位于最前面，更强。因此，左侧短连词的比例更高。但是，本文的新之处在于，我们观察到这种趋势仅在支配者位于左侧或不存在时才会发生。例如，“I saw Bart and Lisa”中支配者位于左侧，而“Homer came and sneezed.”中支配者位于右侧，没有外部支配者。在这种情况下，左侧短连词更喜欢位于最前面；最长两个连词之间的差异。然而，当支配者位于右侧时，这种趋势消失了。我们通过测量长度（以字符为单位、以音节为单位和以单词为单位）来展示这一点。我将重点关注以单词为单位的长度。在这里，当支配者位于左侧时，左侧连词更短的趋势随着绝对长度差异的增加而稳步增长，同样的情况也观察到当没有支配者时，协调句子。然而，当支配者位于右侧时，这种趋势消失了。我们展示了这如何为协调的非对称结构（如上述两种）提供一个论证，并为协调的对称结构（如上述两种）提供一个论证。请查看论文以获取完整的论证。并在海报会期间与我们交流。谢谢。</sample>
    <sample id="15">三位作者。</sample>
    <sample id="16">Bible texts are much stronger simplified than news texts or language learner texts.</sample>
    <sample id="17">Shengqiong Wu, a PhD student at NUS, introduces their research on multimodal relation extraction, a task aiming to identify relationships between entities in text, enhanced by visual information.  The research addresses two key problems: over-utilization of internal textual information and under-exploitation of external information like topic data.

Their proposed method, termed "CMG," builds upon graph information bottleneck principles to refine features by filtering nodes and edges in a cross-modal graph created from textual and visual scene graphs. This process aims to prioritize relevant information.  Furthermore, they integrate multimodal topic information to enrich the context, retrieving relevant keywords and using attention mechanisms to incorporate them into the feature representation.

Experiments on a standard MRE dataset demonstrate that leveraging visual features significantly improves performance compared to text-based methods, and all proposed methods achieve state-of-the-art results. Ablation studies reveal that both information screening and external information exploitation are crucial for performance. Scene graphs are also found to be beneficial for structural modeling.

The research further investigates when internal and external information are most helpful.  Instances with high text-vision relevance benefit more from internal information screening (GENE), while instances with lower relevance benefit more from external information exploitation (LAMO).

In conclusion, the work introduces a novel approach combining information subtraction (internal screening) and addition (external enrichment) guided by graph information bottleneck principles and multimodal topic modeling.  The system achieves significant performance gains over existing models.  Further details are available via a QR code.</sample>
    <sample id="18">"salt and pepper" 和 "pepper and salt" 。</sample>
    <sample id="19">Zhang Qin, a master's student from Shenzhen University, presented their work "A Survey for Efficient Open Domain Question Answering" at ACL 2023. The paper addresses the challenges of open-domain question answering, particularly the large size of the Wikipedia corpus (26 million documents, 20GB) and the computational cost of indexing and searching (65GB index file). These issues hinder real-time applications on resource-constrained devices.

The work explores efficient approaches to address these challenges, contrasting the traditional two-stage model (retrieval + reader) with one-stage retrieval-only and generator-only systems.  The paper highlights techniques for faster evidence retrieval (approximate nearest neighbor search), efficient reading (skip reading, adaptive computation), and index size reduction (document filtering, embedding compression, dimension completion, product quantization).  Furthermore, it discusses model size reduction strategies like lightweight models, parameter sharing, and model compression.

The study analyzes the trade-offs between speed, memory, and performance across different QA models. Retrieval-only systems are fast but require large indexes, while generator-only systems are efficient but often rely on large, less performant models.

The paper concludes with recommendations for resource-constrained scenarios (generator-only, embedding compression) and real-time requirements (retrieval-only).  Future work directions include deploying QA systems on low-power devices and developing more comprehensive evaluation metrics. The presentation emphasizes the importance of balancing efficiency and performance in open-domain question answering systems.</sample>
    <sample id="20">是的，你可以使用这些模型。所有预训练模型都已在 Hugging Face 上免费提供，并根据 MIT 许可发布。所有训练脚本也已在 GitHub 仓库中提供。</sample>
    <sample id="21">DEPLAIN-apa 包含新闻文本。</sample>
    <sample id="22">*   **模型架构** (Model architecture)
*   **模型大小** (Model size)
*   **微调示例数量** (Number of fine-tuning examples)</sample>
    <sample id="23">Dan Garrette介绍了他们团队在文本图像模型中改进视觉文本渲染方面的研究。尽管文本图像模型在生成高质量图像方面取得了显著进展，但它们在表示文本方面表现不佳，尤其是在处理包含单词的简单文本输入时。

他们重点研究了Imagen模型，该模型使用T5-XXL编码器将文本编码为表示，然后使用扩散模型生成图像。研究发现，即使是简单的文本输入，T5模型也难以准确渲染单词。

研究表明，T5模型在单词拼写方面存在严重不足，即使是最大的XXL模型也只能达到70%的准确率。相比之下，PaLM模型在拼写方面表现更好，但其规模和训练数据量都很大，难以应用于许多场景。

ByT5模型则通过使用字符级别Tokenization，能够更好地理解单词的拼写，并在拼写准确率方面表现出色，不受单词频率的影响。

为了解决这个问题，他们将ByT5-small模型的文本表示与Imagen模型的文本表示进行拼接，结果表明，这种方法显著提高了Imagen模型在文本渲染方面的能力。虽然扩散模型仍然可能引入错误，但通过添加字符级别信息，模型在拼写方面取得了显著改善。

研究成果包括WikiSpell和DrawText两个benchmark，以及一种高效的策略，即通过拼接字符级别信息来提高模型在文本渲染方面的能力。</sample>
    <sample id="24">左并列词是否更短，可以通过以下三种方式衡量：

*   **字符数**
*   **音节数**
*   **词数**

论文中主要关注的是词数，并观察到当主语（governor）位于左侧或不存在时，左侧并列词的词数倾向于更短。</sample>
    <sample id="25">我无法直接回答如何设计实验，因为原文主要论证了依赖长度最小化原则在协调结构中的作用，并使用了现有数据（Penn Treebank）进行分析。原文并没有详细描述实验设计。

但是，根据原文内容，可以推断出实验设计可能包括以下几个方面：

1. **数据收集：** 使用增强的 Penn Treebank 数据集，并提取协调结构中的依赖关系（长度、音节数、词数等）。
2. **变量定义：**
    * **独立变量：** 协调结构中的支配词位置（左侧或右侧，以及是否有支配词）。
    * **依赖变量：** 协调结构中左右两个 conjunct 的长度（词数、音节数）。
3. **实验设计：**
    * **对比实验：**  比较不同支配词位置和是否有支配词的协调结构，分析 conjunct 长度之间的关系。
    * **统计分析：** 使用统计方法（例如，相关性分析、回归分析）来量化 conjunct 长度之间的关系，并验证长度最小化原则的有效性。
4. **数据分析：** 分析收集到的数据，观察左侧 conjunct 是否倾向于更短，以及这种倾向与支配词位置和是否有支配词的关系。

总而言之，实验设计需要收集协调结构数据，定义变量，并使用统计方法分析数据，以验证长度最小化原则在协调结构中的作用。</sample>
    <sample id="26">基线分类器在不平衡数据上的训练效果非常差，仅能达到随机猜测的水平（AUC约为0.62）。</sample>
    <sample id="27">This text does not mention the number of authors. It only identifies Shangbin as a PhD student at the University of Washington presenting the work.</sample>
    <sample id="28">Bob and Alice.</sample>
    <sample id="29">在正式性和词汇连贯性等话语现象上，语境感知 MT 模型比语境无关模型更有优势。</sample>
    <sample id="30">LLM-Blender is a novel ensemble learning framework for large language models (LLMs), addressing the issue of inconsistent performance across different input examples.  While average performance metrics exist, the optimal model selection varies significantly.  The paper proposes a two-stage approach: PairRanker and GenFuser.

PairRanker compares candidate LLM outputs (Yᵢ and Yⱼ) for a given input X using cross-attention, encoding both the input and the candidate outputs to identify subtle differences in quality. This pairwise comparison yields a ranking matrix, which is then aggregated using max logits or bubble sort to determine the optimal order of candidates.  The authors claim PairRanker outperforms existing ranking methods by better correlating with the "oracle" ranking.

To evaluate the framework, the authors created MixInstruct, a dataset combining existing instruction datasets and candidates from 11 open-source LLMs.  They used BERTScore, BLUERT, and BARTScore as automatic metrics, alongside human evaluation by ChatGPT.  Experimental results demonstrate that LLM-Blender consistently outperforms Open Assistant and Vicuna, particularly in 68% and 76% of examples for Open Assistant and Vicuna respectively.

The key takeaway is that LLM-Blender offers a simple yet effective way to improve LLM performance by leveraging multiple models and intelligently selecting the best ones for each input. The framework comprises a pairwise comparison module (PairRanker) and a generative fusion module (GenFuser), along with a new evaluation dataset (MixInstruct).  The authors also released a unified codebase for further research.</sample>
    <sample id="31">This information is not provided in the text. The text only mentions the authors' names.</sample>
    <sample id="33">框架通过比较由不同背景的标注者（re-annotators）提供的标注与数据集和模型（如 GPT-4, Dynahate, Perspective API 等）的预测和标签之间的 Pearson's R 相关系数来量化立场。</sample>
    <sample id="34">Marcos Treviso presented "CREST: A Joint Framework for Rationalization and Counterfactual Text Generation," a collaborative work with Alexis Ross, Nuno Guerreiro, and André Martins. CREST combines selective rationalization and counterfactual text generation to improve decision explanations.

The core idea is to generate counterfactual examples by masking parts of the input and prepending a gold label, then using a masked language model to fill in the masked tokens.  Human evaluation showed that CREST-generated counterfactuals were more valid and natural than those from other methods, particularly MiCE.  CREST also showed promise for data augmentation.

CREST-Generation further enhances this by producing both rationalizations and counterfactual examples, allowing for a shared rationalizer and predictor.  A regularization term encourages the rationales to be similar to those generated by CREST-Generation, focusing on factual and counterfactual reasoning.  Experiments on IMDB, SNLI, and other datasets demonstrated that CREST-Rationalization achieved top results on IMDB, performed comparably to human-generated counterfactuals on contrastive datasets, and outperformed other methods on out-of-domain data.

The framework also assesses the interpretability of generated rationales through plausibility, forward simulability, and a new metric called counterfactual simulability.  CREST-Rationalization produced more plausible rationales and significantly higher counterfactual simulability, indicating that the explanations focus on the contrasting parts of the input.

In summary, CREST offers a controllable way to generate valid, fluent, and diverse counterfactuals, leading to more plausible explanations that highlight the contrasting aspects of the input. The paper and code are available for further exploration.</sample>
    <sample id="36">ACL 论文“Learning Language-Specific Layers for Multilingual Machine Translation”介绍了针对多语言机器翻译的改进方法。多语言翻译具有可扩展性、速度和低资源语言支持等优势，但也存在语言容量限制和推理成本问题。

该研究提出了一种名为语言特定层 (LSL) 的解决方案，旨在提高每种语言的容量，同时保持推理成本不变。LSL 的核心思想是为每个语言设计一个独立的 Transformer 层，并在推理时根据输入语言选择相应的子层。

研究人员首先探索了 LSL 的放置位置，发现将 LSL 放置在编码器中效果更好。他们设计了一种学习 LSL 放置位置的方法，通过训练一个包含共享权重、源权重和目标权重的模型，并分析不同权重的重要性，从而确定最佳的 LSL 组合。

实验结果表明，提出的 LSL 架构在各种语言对上的性能都优于传统的语言适配器和最大的基线模型，尤其是在低资源语言上表现出色。此外，该方法在推理速度上具有显著优势。

论文还展示了共享和分离解码器架构的实验结果，并进行了统计显著性测试，验证了 LSL 改进的有效性。研究人员认为，LSL 是一种有效的方法，可以显著提升多语言机器翻译的性能，并为低资源语言提供更好的支持。</sample>
    <sample id="37">在之前的研究中，当人类受试者被给予相同的人格化提示时，他们也能够揭示种族刻板印象。</sample>
    <sample id="38">根据内容，此研究使用了增强版本的 Penn Treebank 数据来源。</sample>
    <sample id="39">This text does not mention the number of authors.</sample>
    <sample id="40">以下是与认知失调密切相关的任务：

*   **辩论 (Debate):** 评估两个不同人提出的论点是否一致或不一致。
*   **CE (Expansion and Comparison):**  评估文本中扩展和比较的类别的关系，这些类别与认知失调的共存和矛盾有关。</sample>
    <sample id="41">Silin from EPFL's Natural Language Processing Lab introduces PeaCoK, a Persona Commonsense Knowledge Graph developed in collaboration with Sony. The work addresses the challenge of creating coherent and engaging narratives by understanding speaker personas and their connections to the real world.

PeaCoK contains 3,800 personas and 40,000 attributes, representing 100,000 personal inferences.  It leverages existing commonsense graphs and pre-trained language models to build this knowledge base, with human-AI collaboration ensuring high-quality relation annotations.  The AI, InstructGPT-3, assists human annotators, improving efficiency and accuracy.

The research demonstrates that PeaCoK can enhance language models' ability to generate persona-aware text.  A BART-based common knowledge generator trained on PeaCoK outperforms large language models like GPT-3.5 in natural language generation metrics and human evaluations.

Furthermore, PeaCoK improves dialogue generation by linking relevant facts from the knowledge graph to speakers' personas.  Models augmented with PeaCoK exhibit better fluency, consistency, engagement, and persona expression compared to baseline models.  The study reveals that the impact of PeaCoK is stronger when speakers share common attributes, highlighting the importance of interconnected persona knowledge for creating consistent and engaging conversations.

In conclusion, PeaCoK provides a valuable resource for building reliable persona knowledge and advancing narrative modeling, leading to more natural and engaging interactions. The research is publicly available on the paper and GitHub.</sample>
    <sample id="42">The provided text does not mention the number of authors. It only identifies Shuheng as the presenter.</sample>
    <sample id="43">This text does not mention the number of authors. It only identifies Vasudha as a Computer Science PhD candidate at Stony Brook University and states that the work is accepted into ACL 2023 as a long paper.</sample>
    <sample id="44">框架与以前的研究不同之处在于，它**将最终用户（end users）与数据集和模型进行比较，而不是仅仅关注标注者（annotators）的同意程度或模型标注者分布**。 换句话说，它直接评估了模型和数据集对不同人群的偏好程度，而不仅仅是评估标注者之间的共识。</sample>
    <sample id="45">我无法直接从文本中确定哪个设置与刻板词汇的重叠最多。文本主要介绍了“Marked Personas”方法，以及它如何通过生成不同群体的角色来识别语言模型中的刻板印象。它提到了三个比较设置：白人男性、亚洲女性和黑人女性。

文本中提到，使用刻板词汇的分析结果显示，**亚洲女性**的描述中出现了一些刻板印象，例如“不引人注目”、“异域风情”等。此外，文本还指出，黑人女性的描述中出现了一些与“坚强”、“有韧性”相关的词语，这些词语反映了“强壮的黑人女性”这一刻板印象。

然而，文本并没有明确指出哪个设置与刻板词汇的重叠最多。它更侧重于展示不同群体角色中出现的刻板印象，以及这些刻板印象如何反映了社会偏见。

因此，我无法根据给定的文本给出明确的答案。</sample>
    <sample id="46">DeepL 和 Google Translate。</sample>
    <sample id="47">Shangbin，华盛顿大学博士生，今天我来介绍我们的工作“从预训练数据到语言模型到下游任务：追踪导致不公平 NLP 模型的政治偏见”。我们研究的语言模型是基于大规模网页爬取数据训练的。政治新闻媒体在预训练数据中占据重要地位。根据 C4 数据集调查，纽约时报、洛杉矶时报、英国卫报、Huffington Post 等媒体被广泛纳入语言模型训练数据。这为语言模型应用带来了双重影响。一方面，它们能够从多元化的视角学习，这有助于促进民主和各种观点。另一方面，这些不同的政治观点本身就带有社会偏见，可能导致 NLP 应用中出现不公平问题。为此，我们旨在研究政治偏见传播的管道，从预训练数据到语言模型到下游任务，具体来说，我们提出了以下问题：首先，如何评估语言模型的政治倾向，预训练数据在其中扮演什么角色？其次，语言模型具有不同的政治倾向，在下游任务中表现如何，这是否可能导致 NLP 应用中的不公平问题？为此，我们首先建议使用政治问卷，如政治会议测试，为语言模型提供提示，以确保我们进行自动评估具有政治科学文献的可靠性。初步结果表明，语言模型确实具有不同的政治倾向，它们占据了政治光谱的四个象限。我们还发现，GPT-4 是最自由的语言模型，GPT 系列通常比 BART 系列及其变体更具社会自由性。其次，我们旨在调查语言模型政治偏见是否实际来源于训练数据。为此，我们进行了一个受控实验，通过进一步预训练语言模型检查点，在 6 种不同的党派语料库上，这些语料库分别由新闻和社交媒体组成，并进一步根据政治倾向进行划分。通过在这些党派语料库上进一步预训练语言模型，我们可以看到语言模型的意识形态坐标相应地发生变化。例如，RoBERTa 在左派 Reddit 语料库上进一步预训练后，在政治偏见方面呈现出显著的自由化趋势。我们还试图调查语言模型是否能够捕捉到现代社会普遍存在的极化现象。我们将预训练语料库划分为美国前 45 任总统和美国后 45 任总统的时期。我们分别在两个不同的时间语料库上对语言模型进行预训练。我们看到，语言模型总体上具有政治倾向，在 2017 年之后更加远离中心。这表明语言模型也能够捕捉到我们社会中的极化现象。最后，我们评估了具有不同政治倾向的语言模型在仇恨言论检测和虚假新闻检测等 NLP 应用中的表现。我们看到，如果根据不同类别进行性能分析，即根据新闻媒体的政治倾向划分，我们可以看到模式。例如，对于仇恨言论检测，左派语言模型在检测针对社会弱势群体的仇恨言论方面表现更好，但在检测针对更强大群体的仇恨言论方面表现较差，反之亦然。类似趋势也适用于虚假新闻检测，我们看到，左派语言模型在检测其对立政治倾向的虚假信息方面表现更好，反之亦然。我们还展示了许多定性例子，以进一步说明语言模型具有不同政治倾向时，对仇恨言论和虚假信息示例做出不同的预测，这些预测基于社会类别。在附录中还有许多更多例子，以进一步强调这一点，这表明语言模型政治偏见是一个非常严峻的问题。例如，如果右派语言模型被用于微调仇恨言论或虚假信息，并部署到流行的社交媒体平台，这将意味着，具有不同政治观点的人可能会被边缘化，而针对少数群体的仇恨言论可能会不受控制地蔓延。这引起了我们对语言模型政治偏见造成的公平问题的警惕和解决的呼吁。因此，我们还想强调，我们揭示了语言模型政治偏见面临的独特困境。这就像斯基拉和卡里布斯之间的选择。如果我们不净化预训练数据中的政治观点，偏见将从预训练数据传播到语言模型到下游任务，最终导致公平问题。如果我们试图净化，我们也会面临审查或排除的风险，很难确定什么是真正中立，应该保留的语言监控数据。这有点像电车难题。</sample>
    <sample id="48">David Vilar 和他的 Google Translate 同事。</sample>
    <sample id="49">MPP 评估最多涵盖了 1024 个词元的上下文长度。</sample>
    <sample id="50">DEPLAIN is a new corpus designed for German text identification at both document and sentence levels, addressing limitations in existing corpora. The corpus aims to improve text simplification model training by providing high-quality, manually aligned parallel sentence pairs.

DEPLAIN is divided into two subcorpora: DEPLAIN-apa, based on news texts with 483 manually aligned documents (approximately 13,000 sentence pairs), and DEPLAIN-web, encompassing diverse domains with 750 documents aligned both manually and automatically (totaling 30,450 sentence pairs). Analysis reveals that simplification types vary across texts, with Bible texts being significantly more simplified than news or language learner texts. DEPLAIN also exhibits a high variety of simplification transformations, such as more reorderings and word additions in DEPLAIN-apa compared to rephrasings in DEPLAIN-web.

The presentation highlights two key use cases for DEPLAIN. First, it serves as a gold standard for evaluating automatic alignment methods. By leveraging manually aligned sentences, researchers can assess the performance of various alignment techniques, with MASSalign identified as the best performing method for German text simplification.

Second, DEPLAIN facilitates automatic text simplification through fine-tuning language models. The presentation details fine-tuning long-mBART for document-level simplification and base mBART for sentence-level simplification, demonstrating that these methods can achieve scores exceeding baseline performance. The results are presented as a benchmark for future research in automatic text simplification. The authors encourage further exploration of DEPLAIN and its applications.</sample>
    <sample id="51">他们的数据集中包含音乐、书籍和食谱三个领域。</sample>
    <sample id="52">Positionality 指的是人们基于其人口统计学、身份和生活经历所持有的视角和观点。它会影响研究过程和结果，因为研究者可能会因为自己的立场而做出不同的决策。</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">Vasudha from Stony Brook University presents their ACL 2023 paper, "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge." The paper introduces cognitive dissonance – the inconsistency between beliefs and actions – and highlights its underrepresentation in language, emphasizing its importance for understanding societal trends, mental health, and individual cognitive styles.

To build a resource for dissonance detection, the authors conducted a large-scale annotation of dissonance relations in tweets, employing a dissonance-first approach. However, they found dissonance instances were rare (3.5% of annotated pairs), leading to poor initial classifier performance.

To address this rarity, they explored transfer learning and active learning. They leveraged transfer learning from related tasks like debate (agreement/disagreement) and PDTB expansion/comparison (CE), finding significant performance improvements.  They then experimented with different active learning update strategies: Cumulative and Iterative.  Cumulative proved superior across various rounds.

The paper further investigates a Probability-of-Rare-Class (PRC) strategy for selecting examples for annotation, comparing it to other active learning methods. PRC outperformed state-of-the-art strategies, although annotators found the examples challenging.

The study concludes that PRC is a simple and effective active learning strategy for acquiring data for rare classes, particularly dissonance.  Iterative updates are beneficial for transfer learning from different domains, while cumulative updates are more suitable for domain-specific active annotations. The paper also assesses the feasibility of each strategy for annotation quality and cost.  The authors provide links to their dataset and paper for further information.</sample>
    <sample id="55">是的，EDAtt 适应了现有的离线 ST 模型，它不要求重新训练或采用特定的 SimulST 架构，而是利用已有的模型，并使用特定的参数来处理延迟。</sample>
    <sample id="56">The provided text does not mention the number of authors. It only names Yusen Zhang from Penn State University as the presenter.</sample>
    <sample id="57">是的，被测模型在测试套件上运行，并且在经过任务特定训练后，性能显著提升。</sample>
    <sample id="58">KITMUS 有三个变体：

1.  **Background-Pretrain**
2.  **Background-Both**
3.  **Background-Inference**</sample>
    <sample id="59">Yanis Labrak's presentation introduces DrBERT, a robust pre-trained language model in French for biomedical and clinical domains. The talk begins by discussing the importance of language modeling in healthcare, highlighting the gap in specialized French models compared to English counterparts like BERT, PubMedBERT, and ClinicalBERT.

DrBERT is the first open-source biomedical model in French, built upon RoBERTa and trained on the NACHOS dataset, a collection of medical data crawled from the web. The presentation details a comparison of DrBERT with the ChuBERT model (based on anonymized hospital data) and explores the impact of data size on model performance.  Seven models were trained – from-scratch DrBERT and ChuBERT, and continual pre-training models based on CamemBERT and PubMedBERT.

The models were evaluated on 11 downstream tasks (NER, classification, POS tagging, QA) against six baseline models (CamemBERT, CamemBERT OSCAR, CamemBERT CCNET, PubMedBERT, BioBERT, ClinicalBERT).  The results indicate that models trained on data similar to the training data performed best, but heterogeneous data showed greater versatility.  More data generally led to better performance, and from-scratch pre-training yielded superior results.  Control pre-training using CamemBERT weights showed comparable performance to DrBERT 4GB.

The presentation concludes that DrBERT outperforms generic models like CamemBERT on nine of the eleven tasks.  While more specialized data is beneficial, it doesn't scale effectively.  The pre-trained models from NACHOS are freely available on Hugging Face, and training scripts are on GitHub.  The researchers are available for discussion at the Toronto poster session.</sample>
    <sample id="60">This information is not provided in the text.</sample>
    <sample id="61">最后一个研究问题是：是否应该只使用干净的样本进行验证，还是有更好的方法？</sample>
    <sample id="62">Nitay Calderon introduces their ACL paper, "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training," a collaborative effort with Amir and Subhabrata from Microsoft and advisor Roi. The paper addresses the growing need to compress large language models (LLMs) in natural language generation (NLG) to reduce computational costs.

The core goal is to find an effective "recipe" for NLG compression, balancing model size reduction with performance preservation. The study explores knowledge distillation, a technique where a smaller "student" model learns from a larger "teacher" model.  They focus on task-specific distillation in NLG, contrasting it with previous work in classification, pre-training, and general NLG tasks.

The research adopts a systematic approach, considering industry-driven setups characterized by medium-resource labeled data, large unlabeled data, medium-sized off-the-shelf models, and a focus on inference time efficiency.  They investigate four NLG tasks: summarization, question generation, common sense reasoning, and simplification/style transfer.

The paper outlines eight stages of investigation, including architectural decisions (encoder/decoder vs. decoder-only), the impact of pruning, and different knowledge selection approaches.  A key contribution is an exploration of pseudo-targets, where the teacher model generates output text used to augment the student's training data.  They challenge traditional sequence-level distillation by demonstrating the benefits of generating multiple pseudo-targets, sampling them instead of using beam search, and employing high-temperature sampling for increased diversity.

Furthermore, they propose "joint-teaching," a novel technique combining word-level knowledge distillation on pseudo-targets generated by both the teacher and student, to address student exposure bias and encourage self-correction. The paper provides details on methods and motivation in the full paper, accessible via a QR code. Nitay invites attendees to visit their poster for further discussion.</sample>
    <sample id="63">指标灵敏度衡量模型在给定任务时，即使指令措辞略有不同，也能始终如一地产生相同输出的能力。</sample>
    <sample id="64">Jingwei Yi</sample>
    <sample id="65">更高灵敏度表明模型性能得到了提高。</sample>
    <sample id="66">This paper discusses the field of deep learning for mathematical reasoning, highlighting its importance as a fundamental aspect of human intelligence. The research explores various facets of mathematical reasoning, including solving math word problems (text-based and multimodal, like images and tables), automated theorem proving, and leveraging large language models (LLMs).

The paper outlines different approaches to mathematical reasoning, from sequence-to-sequence models that map input sequences (like math problems) to output sequences (like equations or proofs), to sequence-to-tree models that explicitly model the tree structure of mathematical expressions.  It emphasizes the recent advancements in LLMs and their potential for solving math problems through techniques like chain-of-thought prompting.

However, the paper also acknowledges the limitations of LLMs, particularly their lack of precise mathematical reasoning and potential inconsistencies.  To address these limitations, the research explores methods like self-consistency (sampling multiple reasoning paths) and program-aided LLMs, which augment LLMs with external tools to enhance their capabilities.  

The paper also notes the underexplored area of mathematical reasoning in low-resource settings and the development of benchmarks for specialized domains like finance, science, and medicine.  Finally, it points out challenges in generalization and robustness, including difficulties with large numbers and inconsistencies in mathematical reasoning. The paper concludes by emphasizing the ongoing research and development in this rapidly evolving field.</sample>
    <sample id="67">This paper investigates interference in multilingual translation models, exploring the factors that contribute to it and the effectiveness of mitigation strategies. The research identifies that severe interference occurs when models are very small relative to the available data.  The study finds that tuning the sampling temperature is crucial for strong performance, often outperforming tuned baselines.

The authors analyze the relationship between interference and various factors, including language similarity and the number of languages. They discover that language similarity and the total number of languages have a limited impact on interference levels.  Specifically, they find that language similarity is not a dominant factor, and the number of languages doesn't significantly affect the problem.

The paper presents experiments using Transformer architectures and 15 languages from WMT, varying model and data sizes. They define interference as the relative difference between the loss of a bilingual model and a multilingual model.  The findings suggest that interference is largely mitigated by scaling up model and data sizes.  

The authors propose that a simple and effective approach to controlling interference is to use temperature sampling, with values greater than 1 allowing for more training examples from lower-resource languages.  They observe that uncalibrated temperature settings, even for larger models, can be detrimental.  

In conclusion, the study highlights that modest model and data scaling, combined with appropriately tuned temperature, can significantly reduce interference in multilingual translation without requiring specialized algorithms. The research emphasizes the importance of considering model and data size as primary factors influencing interference, while other factors have a comparatively smaller impact.</sample>
    <sample id="68">根据内容，模型在预训练期间接收到的语言上下文是**不明确的**。论文作者认为，当前的最小对范例（MPP）pipeline无法评估模型在长文本上下文中的接受度，因为模型在预训练时可能没有接触到长文本上下文。论文旨在解决这个问题，并探索在长文本上下文中评估模型接受度的方法。</sample>
    <sample id="69">根据视频内容，通常只需要 20 个干净的验证样本就能获得良好的表现。但是，直接在干净的验证样本上进行训练，效果会更好。</sample>
    <sample id="70">Dan Jurafsky and Esin Durmus are collaborators on this paper. The paper is done in collaboration with them. The provided text does not explicitly state the authors' institutions. However, it mentions Dan Jurafsky is affiliated with Stanford University.</sample>
    <sample id="71">Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis's work focuses on "Resolving Indirect Referring Expressions for Entity Selection," introducing the AltEntities Corpus. This corpus addresses the challenge of understanding user language when they need to choose between entities, often using indirect references instead of direct names.

The study highlights the importance of this problem for conversational systems and evaluating Large Language Models (LLMs) in entity understanding.  A key issue is the lack of large-scale public datasets for this task. To address this, they created the AltEntities Corpus, a dataset of 6,000 alternative questions across music, books, and recipes, containing 42,000 indirect referring expressions.

The dataset uses a cartoon completion setup where annotators respond to a dialogue context with an indirect reference to select an entity.  The alternative questions are generated using a template ("Do you mean A or B?") with entities sampled from Wikipedia using various similarity metrics (random, similar titles, similar descriptions, etc.).

To ensure annotators have sufficient background knowledge, they provide Google search links for songs and Wikipedia text for books and recipes, along with images for recipes. Annotators then generate three to five indirect referring expressions to describe the entities.

Experiments with the T5 XL model show that accuracy increases with the amount of overlapping background knowledge the model possesses.  With access to the same background knowledge (92-95% accuracy), accuracy is high.  However, with partially overlapping knowledge (82-87%), it's more realistic.  Accessing only entity names results in a significantly lower accuracy (60%).

The research demonstrates the domain-generalizability of the models and provides a valuable resource for advancing LLMs in understanding and resolving indirect referring expressions. The dataset is publicly available via a provided link.</sample>
    <sample id="72">在语言模型训练数据中包含来自不同政治新闻媒体的内容，这使得模型学习到了各种政治观点。虽然这有助于模型学习到多样化的视角，但也可能导致模型继承并放大这些社会偏见，从而在下游任务中产生不公平的结果。因此，需要开发新的方法来衡量媒体偏见，以确保语言模型不会因为政治偏见而产生不公平的后果。</sample>
    <sample id="73">Akshatha and Martin.</sample>
    <sample id="74">The paper introduces Dense-ATOMIC, a new commonsense knowledge base built upon ATOMIC, aiming to address its limitations in knowledge coverage and multi-hop paths. ATOMIC, while high-quality, suffers from sparse graph structures and limited multi-hop reasoning due to the lack of B-to-B, A-to-B, and A-to-A links.

Dense-ATOMIC overcomes these issues by constructing a densely connected knowledge graph. The construction process involves normalizing tail events (converting them to head event equivalents), training a relation prediction model (Rel-CSKGC) using RoBERTa embeddings and MaxPooling, and then constructing the Dense-ATOMIC graph. Rel-CSKGC predicts relations between head and tail events, leveraging semantic information and mitigating sparsity.  A cluster-based completion strategy is employed for efficient inference.

The paper demonstrates that Dense-ATOMIC significantly improves knowledge coverage, particularly in 1-hop, 2-hop, and 3-hop paths.  Furthermore, Dense-ATOMIC enhances the performance of COMET models, leading to more diverse results.  The study also validates the utility of multi-hop paths within Dense-ATOMIC, showcasing its potential for commonsense reasoning.  Extensive evaluations show that Dense-ATOMIC outperforms existing relation prediction methods and translation-based approaches. The authors provide code and a website for access.</sample>
    <sample id="75">Zheng Yandan and colleagues present Jointprop, a novel joint semi-supervised learning framework for Named Entity Recognition (NER) and Relation Extraction (RE). The motivation stems from the neglect of interconnections between NER and RE tasks, which can lead to missed label alignments.  Fully exploiting these connections allows for more accurate label inference.

Jointprop addresses this by propagating labels across heterogeneous graphs constructed from labeled and unlabeled data. The framework consists of four key components:

1. **Span Feature Generation:**  Contextualized representations of tokens are used to generate initial span and span pair representations for both labeled and unlabeled data.
2. **Heterogeneous Graph Construction:**  A k-Nearest Neighbor graph is built to capture similarity between unlabeled data and labeled data. Entity and relation nodes are automatically linked based on their representations.
3. **Joint Label Propagation:** Labels are propagated through the graph, refining pseudo-labels for entity and relation candidates in the unlabeled data. This process iteratively improves label accuracy.
4. **Model Optimization:**  The refined pseudo-labels are used to retrain the classification model, combining high-confidence pseudo-labels with labeled data.

Experiments on joint and single-task datasets demonstrate that Jointprop significantly outperforms existing baselines.  On joint datasets, the framework benefits from the codependency between NER and RE.  On single-task datasets, it consistently improves performance for both NER and RE.  The framework leverages the interconnectedness of NER and RE to achieve more robust and accurate label predictions, offering a promising approach to semi-supervised learning in information extraction.</sample>
    <sample id="76">以下是根据内容总结的政治偏见传播流程：

1. **预训练数据：** 大型Web爬取数据，包括政治新闻媒体的报道，导致语言模型学习到带有政治倾向的信息。
2. **语言模型：** 预训练后的语言模型继承了预训练数据中的政治偏见，表现出不同的政治倾向（例如，GPT-4更倾向于自由派，BART系列更倾向于保守派）。
3. **下游任务：** 语言模型在各种下游任务（如仇恨言论检测、虚假新闻检测）中表现出不同的政治偏见，并且这些偏见会影响对不同社会群体（例如，少数族裔、弱势群体）的判断。

总而言之，政治偏见从预训练数据通过语言模型传播到下游任务，并可能导致不公平的NLP应用。</sample>
    <sample id="77">The video introduces the research paper "On Improving Summarization Factual Consistency from Natural Language Feedback," a joint effort from Yale University and Microsoft Research. The work focuses on enhancing factual consistency in abstractive text summarization by introducing a new dataset called DeFacto.

DeFacto contains human demonstrations and feedback on system-generated summaries, aiming to improve the accuracy of summaries by identifying and correcting factual errors. The dataset comprises around 2,500 data points, with 70% containing factual errors, collected from the XSum dataset using the Pegasus model as a baseline. Annotators provide labels indicating factual consistency, along with human-corrected summaries and detailed feedback including instructions, explanations, and supporting evidence.

The research proposes three new Natural Language Generation (NLG) tasks to address factual consistency: summary editing, feedback generation, and automatic factual error correction.  The paper presents strong baseline models for each task.  

The summary editing task involves using human feedback to refine initial summaries.  Both fine-tuned models and zero-shot large language models can effectively utilize this feedback. Feedback generation, where a critic model generates feedback for editing, remains a challenging task.  Automatic factual error correction, coupled with explanation generation, demonstrates promising results with fewer training data points compared to baseline models. The explanation generation enhances model performance.

DeFacto's fine-grained annotations offer valuable insights for training factuality metrics and meta-evaluation. The dataset is publicly available on GitHub. The research highlights the importance of factual consistency in summarization and provides a valuable resource for advancing the field. The study demonstrates that human feedback can significantly improve factual consistency, even with limited data.</sample>
    <sample id="78">是的，DEPLAIN-apa 和 DEPLAIN-web 的简化过程有所不同。DEPLAIN-apa 主要基于新闻文本，简化程度较高，而 DEPLAIN-web 包含不同领域，简化程度较低，并且在重述方面更多。</sample>
    <sample id="79">是的，CoScript 数据集公开可用。</sample>
    <sample id="80">水印是通过计算句子中触发词的数量来插入的。当句子中触发词的数量超过某个阈值（m）时，提供的嵌入向量就等于目标嵌入向量。</sample>
    <sample id="81">Penn State University.</sample>
    <sample id="82">This video introduces ULRA (Learning from Rank Aggregation), a novel framework for unsupervised Automated Essay Scoring (AES). AES aims to automatically evaluate essay quality without human scoring, a crucial application of natural language processing. Traditional supervised AES relies on labeled data, which is expensive to obtain, especially for new prompts. Unsupervised AES offers a promising alternative.

The video highlights limitations of existing unsupervised AES approaches.  Early methods, like Chen's work (2010), used a single heuristic signal (unique terms) but lacked control over the clustering process. Zhang and Litman's (2021) approach used word count as weak supervision, but direct regression proved ineffective.

ULRA addresses these shortcomings by leveraging multiple heuristic quality signals as a form of pseudo-groundtruth. It consists of two main modules: HER alpha-shot (Heuristic Essay Ranking) and DPRA (Deep Pairwise Rank Aggregation). HER alpha-shot generates partial order pairs by ranking essays based on various quality signals (e.g., readability, vocabulary diversity). DPRA then aggregates these partial order pairs using a deep learning model, incorporating a learnable confidence weight for each signal to handle inconsistencies.

The video also includes a Scoring Strategy to map the model's predicted scores to a predefined score set. Experiments on both transductive and inductive settings demonstrate ULRA's superior performance compared to existing unsupervised baselines and cross-prompt/one-shot methods.  While ULRA performs better than unsupervised methods, it still lags behind supervised methods due to the lack of strong supervision.

In conclusion, ULRA provides a robust framework for unsupervised AES by aggregating partial order knowledge from multiple heuristic signals, effectively addressing signal conflicts and achieving improved essay scoring accuracy.</sample>
    <sample id="83">是的，像 mT5 这样的编码器-解码器模型可以通过混合语言的训练来改进。研究发现，在多语种设置中，混合训练可以提高 Encoder-Decoder 或 Encoder-PTR 模型（如 mT5 和 XLM-R + PTR）的性能。</sample>
    <sample id="84">Shwai He's ACL 2023 paper introduces PAD-Net, an efficient framework for dynamic networks that addresses the issue of excessive parameter usage in fully dynamic architectures. Traditional networks are static, while dynamic networks adapt their architecture or parameters based on input, offering potential performance gains. However, fully dynamic networks often contain redundant parameters, leading to significant model size increases, as exemplified by the 5x size increase in BERT-Base with Mixture of Experts.

PAD-Net tackles this by partitioning parameters into dynamic and static components, utilizing two scale factors to control their intensity. The Iterative Mode Partition method identifies and converts redundant dynamic parameters to static ones, aiming to minimize the impact on the loss value. Experiments demonstrate that PAD-Net outperforms static and fully dynamic networks while maintaining fewer parameters and less computation.

Ablation studies reveal optimal dynamic ratios for Dynamic Convolution and Mixture of Experts, as well as the importance of scale factors for both dynamic and static parameters. PAD-Net's performance surpasses network pruning due to its preservation of static parameters. Furthermore, PAD-Net enhances output discrimination, contributing to improved performance.

Future work includes extending PAD-Net to other network architectures, exploring hardware-friendly structures, and incorporating more parameter modes. The paper highlights the potential of PAD-Net to improve efficiency and performance in dynamic networks, addressing a key limitation of current approaches.</sample>
    <sample id="85">“Make a chocolate cake” 是受限语言规划的一个示例。</sample>
    <sample id="86">他们通过可视化嵌入来验证隐蔽性，并发现后门嵌入与正常嵌入难以区分。</sample>
    <sample id="87">研究通过在 NACHOS 数据集上训练基于 RoBERTa 的 DrBERT 模型，以及通过比较不同预训练设置和数据来源的模型，展示了如何使用现有的预训练语言模型 (PLM) 构建新的 PLM。他们还探讨了使用不同数据来源（如 NACHOS 和临床笔记）训练 PLM 的影响，并发现从头开始预训练通常能获得更好的性能。此外，他们还研究了使用 CamemBERT 的权重和分词器进行持续预训练，发现这种方法在某些情况下可以达到与从头开始预训练相当的性能，但存在稳定性问题。</sample>
    <sample id="88">GPT-4 在社会可接受性分析任务中，与非二元性别群体（non-binary people）的立场最不一致。</sample>
    <sample id="89">演讲者在 "If we receive a speech chunk containing "I'm going to talk about..." and our model predicts the translation in German, and we will look at the cross-attention weights, we'll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames. This means that the first two words will be emitted while since the sum of the cross-attention is above a certain threshold alpha, we will not emit the last word and we wait for another speech chunk." 这个句子上展示了模型如何利用注意力机制所学的知识。</sample>
    <sample id="90">The paper "Rethinking Annotation: Can Language Learners Contribute?" investigates the feasibility of using language learners as data annotators in Natural Language Processing (NLP).  It challenges the traditional reliance on native speakers, particularly for languages with limited native speakers. The authors conducted a proof-of-concept study using English, Korean, and Indonesian, focusing on sentiment analysis, NLI, and NER tasks from the GLUE benchmark.

They categorized learners into basic, intermediate, and advanced levels based on CFR criteria and compared their performance to native speakers.  Experiments involved a three-step process: pre-test (proficiency assessment), annotation (labeling tasks with varying difficulty), and post-test (proficiency assessment).  Learners were provided with additional resources (dictionaries, machine translation) and their annotations were aggregated using majority voting.

The study found that language learners produced nearly accurate labels, especially for simpler tasks and easier questions.  Their performance was comparable to native speakers when labels were aggregated.  Furthermore, the paper demonstrates that language learners can contribute to NLP by training language models on their annotations, achieving performance close to ground truth, and sometimes surpassing models trained on native speaker data.

The research highlights that using language learners can offer a novel approach to data construction for low-resource languages, overcoming geographical and technological barriers.  The annotation process itself also leads to improvements in learners' language proficiency.  The authors conclude that language learners can be valuable contributors to NLP data annotation, suggesting a more accessible and scalable method for building benchmark datasets. They emphasize the importance of considering control variables to understand the impact on annotation performance.</sample>
    <sample id="91">任务数量增加，模型性能会更好，同时敏感性会降低。</sample>
    <sample id="92">The paper compares the proposed method with other treeless models on the COGS benchmark. The text mentions that the model outperforms the others by a large margin on generalization to deeper recursion, but it doesn't explicitly list the three treeless baselines used for comparison.</sample>
    <sample id="93">Alexander Koller and Ivan Titov are advisors to the first author, Matthias Lindemann.</sample>
    <sample id="94">Jingwei Yi from the University of Science and Technology of China presents a paper on protecting copyright for embedding as services, a burgeoning area built upon large language models like GPT, LLAMA, and PALM.  The core problem is that attackers can potentially steal models by learning from embeddings and replicating the service.  The paper proposes "Embedding Marker," a backdoor-based watermark method to address this.

Embedding Marker involves two key steps: watermark injection and copyright verification.  Watermark injection uses a trigger set – a group of frequently occurring words – to modify the embedding provided by the service. The weight of the target embedding (the watermark) is proportional to the number of triggers in the input sentence.  If a sentence contains a threshold number of triggers, the embedding becomes identical to the target embedding.

Copyright verification detects if a model behind another service contains the watermark.  This is achieved by creating a backdoor dataset (containing only trigger words) and a benign dataset (without trigger words). The provider service requests embeddings from the suspected service using the backdoor dataset.  The similarity between the requested embedding and the target embedding is calculated using cosine and L2 distance.  The difference between the benign and backdoor datasets, along with a Kolmogorov-Smirnov (KS) test p-value, are used as detection metrics.

Experiments on AG News, MIND, SST2, and Enron Spam datasets demonstrate that Embedding Marker achieves high detection performance while maintaining good utility for downstream tasks.  Visualizations using PCA show that the backdoor embeddings are difficult to distinguish from normal embeddings, highlighting the covertness of the watermark. The paper concludes that Embedding Marker offers a practical and effective solution for protecting the copyright of embedding as services.</sample>
    <sample id="95">David Vilar</sample>
    <sample id="96">Jenny，一位卡内基梅隆大学的一届博士生，今天将介绍他们的工作“NLPositionality：特征化数据集和模型的设计偏见”。这项工作与华盛顿大学和人工智能研究所的塞巴斯蒂安·桑蒂、罗南·勒布拉斯、卡塔琳娜·雷内克和马特·萨普合作完成。

想象一下，你是一名报纸编辑，正在审查新闻文章下的评论，试图删除有害内容。你可能会转向流行的 API，如 Prospective API，用于检测毒性。如果 Carl Jones 是你的话，那么 Prospective API 在检测到有害实例方面效果很好。但对于 Aditya Sharma 来说，它并非如此。Prospective API 在检测到针对印度语背景的冒犯性词语方面表现不佳。这是一种设计偏见，即技术在不同人群中的系统性性能差异。

像我们刚才看到的这种设计偏见可能源于 NLP 研究人员和模型开发者的立场。立场指的是人们基于他们的人口统计学、身份和生活经历所持有的观点。这个概念在批判性研究中，特别是女性主义和酷儿学术领域中广泛使用。作为研究人员，立场会影响研究过程和结果，因为它会改变研究人员的决策。因此，一个问题可能是：数据集和模型是否具有立场？我们并非认为模型本身或数据集本身具有人口统计学身份和生活经历，但它们确实汇总了真实人的意见和观点，因此可以代表某些立场而代表其他立场。

此前的一些工作暗示了数据集和模型可能具有立场，例如文化差距和模型/数据集，以及模型立场等理论定义。然而，这些工作并未比较最终用户与数据集和模型本身，而是关注模型和数据集的立场。研究数据集和模型立场日益重要，因为 NLP 任务变得越来越主观和具有社会性，并且难以量化这些偏见，因为并非所有决策都会被记录，许多模型隐藏在 API 背后。

因此，为了研究数据集和模型立场，我们比较了标注与真实用户，并使用现有数据集和模型。我们的框架名为 NLPositionality。我们的框架分为两个主要步骤。第一步是重新标注数据集，使用来自不同背景的标注者。我们必须避免考虑原始数据集标注者的人口统计学特征，因为通常只有少数标注者标注每个实例，而且人口统计学数据很少被收集和共享。因此，我们选择重新标注数据以获得大量标注和丰富的人口统计学数据。然后，我们根据人口统计学进行标注，并将其与模型和数据集进行比较，使用 Pearson's R 相关系数。因此，我们的框架与标注者意见不一致的文献不同，它将最终用户与模型和数据集的预测和标签进行比较，而不是仅仅关注标注者之间的意见一致性或模型标注者分布。我们的框架很大程度上依赖于 Lab in the Wild 和 HCI 合作平台上的在线众包平台。Lab in the Wild 是一个在线实验平台，我们可以招募来自不同背景的志愿者。与像 M Turk 这样的平台相比，M Turk 主要来自美国或印度，而 Lab in the Wild 仍然能够获得高质量的数据。我们托管了两个任务在 Lab in the Wild，其中一个为社会可接受性任务，该任务的运作方式是，参与者阅读 Social Chemistry 数据集中的一个情境，然后写下该情境的社会可接受程度。为了保持参与度，他们可以比较自己的回答与 AI 和其他人的回答。然后，我们比较了这些标注与 Social Chemistry、Delphi 和 GPT-4。我们随后以类似的方式复制了毒性检测任务，参与者阅读 Dynahate 数据集中的一个实例，并判断它是否为仇恨言论。然后，我们比较了这些标注与 Dynahate、Perspective API、Rewire API、Hate Roberta 和 GPT-4。我们最终收集了超过 16,000 条标注，来自超过 1000 名标注者，来自 87 个国家。现在，我们更清楚地了解 NLP 数据集和模型与哪些群体最匹配。我们发现，数据集和模型在很大程度上与英语国家相关联。例如，对于 GPT-4 的社会可接受性分析，我们发现它与儒家文化和英语国家相关联。我们发现 Dynahate 也与英语国家相关联。我们还发现，与大学教育背景的人们相比，模型和数据集的匹配度更高。对于 GPT-4 的社会可接受性任务，我们发现它与拥有大学或研究生教育背景的人们最匹配，我们对于 Dynahate 的任务分析也发现相同的情况。然而，当模型和数据集与特定群体对齐时，必然会留下一些人被忽视。例如，我们发现数据集和模型与非双性人相比，对双性人表现不佳。我们发现这在 GPT-4 的社会可接受性任务以及 Dynahate 任务分析中都有。

那么，既然 NLP 中存在立场，我们该如何做呢？因此，我们提出了几个建议：首先，记录研究过程中的所有相关设计选择；其次，以视角主义的视角进行 NLP 研究；第三，在特定社区内构建专门的数据集和模型，例如 Masakhani 倡议。我们想强调的是，包容性 NLP 不仅仅是让所有技术都能为所有人服务。

这就是今天的总结。如果您想了解更多信息，请随时查看我们的仪表板以获取最新的分析结果和我们的论文。谢谢。</sample>
    <sample id="97">演讲者提到 SimulST 的几个问题是：

*   需要训练特定的架构，引入额外的模块进行优化。
*   训练过程长且复杂，涉及不同的优化目标。
*   需要训练和维护多个模型以达到不同的延迟（latency）水平。</sample>
    <sample id="98">根据演讲内容，减轻数据集中的社会和政治偏见没有一个简单的有效方法，而是面临着“斯基拉和卡里布斯”的困境。

*   **不清理数据：** 允许偏见在训练过程中传播，最终导致公平问题。
*   **清理数据：** 可能会导致审查或排除某些观点，难以确定什么是“中立”的。

因此，这是一个非常困难的问题，需要仔细权衡。</sample>
    <sample id="99">大家好，我是复旦大学的Siyu Yuan。我们团队的论文是“从大型语言模型中提炼脚本知识以用于约束型语言规划”。在日常生活中，人类通常通过遵循逐步说明的形式来规划行动，这些说明被称为目标导向的脚本。之前的研究利用语言模型来规划具有典型活动抽象目标的脚本，例如“制作蛋糕”。展示了大型语言模型能够有效地将目标分解为步骤。然而，之前的研究主要关注的是规划具有典型活动抽象目标的脚本。规划具有特定约束的目标，例如“制作巧克力蛋糕”，仍然研究不足。在本文中，我们定义了约束型语言规划问题，该问题对规划目标施加了不同的约束。一个抽象目标可以被不同的具体目标继承，这些目标具有多方面约束。一个好的规划者应该编写既合理又与约束相符的脚本。在本文中，我们首先评估并改进了大型语言模型在约束型语言规划方面的能力。由于没有特定目标的数据集来支持我们的研究，因此我们必须首先获取这些目标。如图所示，我们通过使用 InstructGPT 进行人工参与的数据获取，扩展了抽象目标并添加了多方面约束。我们采样了 100 个具体目标并评估了由大型语言模型生成的脚本。本表报告了结果的总体准确性。我们发现所有语言模型在规划具体目标方面都表现不佳。然后，我们进行详细分析以调查学习模型失败的原因。图中的结果表明，生成的脚本的语义完整性可以接受，但无法保证与约束的忠实度。我们深入研究了 wikiHow 中定义的约束的更细粒度类别。图中的热图显示，InstructGPT 的规划性能对不同类别目标具有显著差异。之前的研究表明，语言模型的输出质量存在高方差，导致性能不佳。因此，我们采用“过度生成然后过滤”的想法来提高生成质量。首先，我们展示 InstructGPT 对约束类型和示例，并根据种子抽象目标生成特定目标。接下来，InstructGPT 过度生成 K 个脚本用于特定目标。然后，开发一个过滤器模型来选择忠实于约束的脚本。我们将脚本和目标转换为 InstructGPT 嵌入，并计算余弦相似度作为相似度分数来衡量语义相似度。此外，我们奖励包含目标约束关键词的脚本。只有当目标目标得分最高时，我们才保留脚本。通过我们的方法，InstructGPT 可以生成更高质量的脚本。我们的方法显著提高了规划能力，既在语义完整性方面，也在约束的忠实度方面。由于大型语言模型成本高昂，因此需要启用小型和专门化模型的语言规划能力。创建数据集是这一目的的关键步骤。然而，之前的研究并未启用对特定目标的规划，并且人工数据注释成本高昂。因此，我们遵循符号知识蒸馏的想法，从大型语言模型中蒸馏约束型语言规划数据集。我们应用我们的方法构建了一个约束型语言规划数据集，命名为 CoScript。总共有 55,000 个具体目标和脚本。为了确保验证和测试集的质量，我们请众包工作者查找并修改不正确的样本。图显示了 CoScript 中约束的分布。我们发现 CoScript 具有高度的多元化。有了 CoScript，我们可以尝试使用更小但专门化的模型进行约束型语言规划。我们发现，使用 CoScript 微调的 T5 模型生成的脚本质量优于大多数大型语言模型，这表明，如果正确训练，小型模型可以超越大型模型。总而言之，我们确立了约束型语言规划问题。我们评估了大型语言模型在约束型语言规划方面的能力，并开发了一种“过度生成然后过滤”方法。我们使用大型语言模型生成高质量的脚本数据集 CoScript 用于约束型语言规划。我们希望 CoScript 数据集能够成为推进语言规划研究的宝贵资源。感谢您的时间。有关 CoScript 的更多详细信息，请参阅我们的论文。</sample>
    <sample id="100">PromptRank is a data-efficient approach to multi-hop question answering (QA), aiming to address the need for large training datasets typically required for state-of-the-art multi-hop retrievers. The method combines unsupervised retrieval (TF-IDF and hyperlink traversal) with a few-shot language model-based reranker.

The core idea is to construct a chain prompt that incorporates the retrieved documents, an indicator token, and an instruction to guide the language model's reasoning. The language model then scores each chain based on the likelihood of the question given the chain prompt. This scoring function is found to be more effective than the reverse likelihood.

PromptRank achieves strong performance with as few as 128 training examples, outperforming fully supervised systems like DrKit and performing comparably to state-of-the-art dense retrievers.  Ablation studies confirm the importance of each component.  Furthermore, PromptRank demonstrates good downstream QA performance when used as a retriever, achieving results close to MDR.

The approach explores various techniques to optimize the chain prompt, including instruction search and sampling, and temperature scaling.  Experiments using GPT2-XL and T5-XL on the HotpotQA dataset show promising results.  The key takeaways are that language models can effectively rank candidate paths for multi-hop QA, and the likelihood of the question given the chain is a superior scoring function.  Instruction design is crucial for eliciting reasoning from language models.  The paper highlights the potential of PromptRank for low-resource domains and expert-knowledge-intensive tasks.</sample>
    <sample id="101">PaLM 的流畅度与最先进系统相当。</sample>
    <sample id="102">水印方法需要满足以下属性：

1.  **适用性：** 适用于 embedding as services。
2.  **不影响效用：** 不会降低提供的嵌入的效用。
3.  **隐蔽性：** 足够隐蔽，难以被攻击者发现或移除。
4.  **可转移性：** 可以转移到攻击者服务中的模型。</sample>
    <sample id="103">TED 英语演讲已被翻译成 14 种不同的语言。</sample>
    <sample id="104">该研究使用了超过 16,000 个实例用于重新注释。</sample>
    <sample id="105">Cosine similarity difference and L2 similarity difference.</sample>
    <sample id="106">The QUEST dataset, developed in collaboration with Google DeepMind, addresses the challenge of information retrieval with implicit set constraints. The paper highlights how users often express information needs with multiple preferences, such as finding a red reptile under 12 inches long in Costa Rica (Jane) or historical fiction novels set in France (Austin). These queries inherently involve set operations (intersection, complement).

QUEST is a retrieval dataset containing over 3,000 entity-seeking queries with these implicit set operations.  The dataset includes verified answer entities and attributable spans within documents, indicating where the evidence for each constraint resides.  The dataset was constructed by performing set operations on Wikipedia categories (films, books, plants, animals) and then having human annotators paraphrase and validate queries for fluency and relevance. Annotators also marked the relevant spans in documents for each constraint.

The paper demonstrates the difficulty of retrieving multi-answer sets from large document corpora with these constraints.  Baseline systems, including sparse and dense retrievers and a T5-based reranker, show limited performance, particularly for queries involving set intersection and set difference.  The MRecall@100 scores indicate a significant room for improvement in retriever performance.  End-to-end system performance is also low, reflecting the complexity of handling such queries.

The authors conclude that QUEST provides a valuable resource for researchers to develop more effective systems for handling information needs with selective constraints, enabling users like Jane and Austin to find the information they seek more efficiently. They encourage readers to explore the paper and attend their presentation at ACL.</sample>
    <sample id="107">在这项任务中，基于编码器的多语言模型（如mBART和mT5）在所有九个数据集上都取得了最佳性能。通过在混合多种语言的数据上进行训练，可以提高编码器-解码器模型（如mT5和XLM-R + PTR）的性能。</sample>
    <sample id="108">Koustav Sinha and colleagues at ACL 2023 presented a paper revisiting minimal pair paradigms for evaluating language models, focusing on longer context windows. The traditional MPP pipeline struggles with longer sequences, so they developed a method to recreate longer sequences by selecting acceptable/unacceptable sentences from existing datasets and adding them as prefixes to the query pairs. This approach allows for testing model acceptability across varying context lengths and domains.

Experiments using the BLiMP dataset showed that MPP judgments remained relatively stable even with context lengths up to 1024 tokens. However, when using sentences from the same dataset (acceptable or unacceptable), the MPP judgments exhibited significant changes, increasing or decreasing depending on the prefix. This effect was observed across longer context lengths, suggesting a potential issue with current evaluation methods.

Further analysis revealed that language models are sensitive to shared latent syntactic and semantic features within sentences. Perturbations of input sentences, while altering the surface form, did not significantly change the model's MPP judgments. This indicates that the model's sensitivity to context is driven by underlying structural and semantic information.

The key takeaway is that current MPP evaluation, often based on short, single-sentence inputs, may not fully capture the language models' abstract knowledge and contextual understanding across longer sequences. The research highlights the need for more robust evaluation methods that account for the complexities of language models' internal representations and their sensitivity to shared linguistic features. The authors encourage readers to consult their paper for further details.</sample>
    <sample id="109">The presentation introduces "Unnatural Instructions," a novel dataset for instruction tuning language models, created entirely through automated generation without human annotation. The core idea is to leverage pre-trained language models, specifically GPT-3 variants, to generate instruction-input-output triplets. The process involves prompting the model to generate an instruction and input, followed by prompting it to generate a corresponding output.  To enhance diversity, the dataset also includes paraphrased versions of each instruction.

The resulting dataset comprises 64,000 instruction-input pairs and approximately 240,000 with paraphrases.  The authors evaluated the quality of the generated data, finding a high degree of correctness (over 50%) and significant creativity and diversity in the tasks covered.  Examples include tasks like scientific experiment design and word invention, showcasing the model's ability to generate instructions beyond typical NLP benchmarks.

To demonstrate the utility of Unnatural Instructions, the researchers fine-tuned an 11 billion-parameter T5 model on this dataset and compared its performance to models trained on existing datasets like Super-Natural Instructions and T0++.  The results showed that the Unnatural Instructions-trained model outperformed both T0++ and Tk-instruct across multiple benchmarks, and, when considering the cost of data generation, it surpassed the baseline model trained on Super-Natural Instructions.

The authors highlight the advantages of this fully automated approach over human annotation, noting that language models can produce more creative and diverse data, avoiding the predictability and potential artifacts often found in crowd-sourced annotations.  Unnatural Instructions offers a faster and cheaper alternative to traditional instruction tuning data collection, paving the way for more versatile and capable language models.</sample>
    <sample id="111">作者假设提供商可以收集一个通用文本语料库并统计单词频率来确定中等频率的单词。</sample>
    <sample id="112">大家好，我的名字是舒恒。今天我将介绍我们的论文《CoNLL-2003命名实体识别器在2023年是否仍然有效？》。让我们开始吧。我们的论文研究了泛化问题，即命名实体识别任务（NER任务）。我们观察到，CoNLL-2003模型近20年来一直用于开发NER，这自然引发了许多问题。首先，这些模型是否能够泛化到现代数据？其次，当我们开发新的标签器时，需要什么来保证良好的泛化？与此同时，如果我们观察到泛化不良，那么性能下降的原因是什么？为了解决这些问题，我们开发了CoNLL++数据集。这是一个我们从2020年Reuters新闻中收集的数据集，然后使用与CoNLL-2003标注指南相同的标注方法进行标注。然后，我们对20个模型进行了微调，并在CoNLL-03测试集和CoNLL++数据集上进行了评估。最后，我们计算了每个模型F1值的百分比变化，以评估其泛化能力。那么，什么对于良好的泛化是必要的呢？在实验过程中，我们发现有三个主要要素是必要的。第一个是模型架构。通过我们的实验，我们发现Transformer模型通常能够更好地泛化到新数据。第二个要素是模型大小。我们发现，通常较大的模型能够带来更好的泛化。最后，我们都知道，微调示例的数量直接影响下游任务的性能。在这里，我们还发现，更多的微调示例实际上也能够带来更好的泛化。针对我们的下一个问题，性能下降的原因是什么？我们提出了两个假设。第一个假设是自适应过拟合，即过拟合成本通过重复使用相同的测试集来增加，这通常表现为在新测试集上改进的收益越来越小。第二个假设是时间漂移，即性能下降是由训练数据和测试数据之间时间差距增加引起的。对于数据过拟合，我们观察到，右侧图中的红色最佳拟合线具有大于1的斜率。这意味着，我们对CoNLL-2003的每一点改进，在CoNLL++上都能带来超过一倍的改进，这表明在这种情况下没有 diminishing returns。这表明，在这种情况下，自适应过拟合并非观察到。那么，时间漂移呢？为了研究时间漂移，我们对一些模型进行了重新训练或继续预训练，使用更多最近的数据，我们发现，随着时间差距的增大，性能会下降，这证实了我们关于时间漂移是性能下降的主要原因的假设。我们的结论是，为了良好的泛化，我们需要更好的模型架构、更大的模型大小以及更多的微调示例。这三者相互关联，不能只选择其中一个而放弃其他两者。与此同时，我们还发现，性能下降的原因是时间漂移，而且令人惊讶的是，这并非由自适应过拟合引起，即使CoNLL-2003已经使用了20多年。回到我们论文标题中提出的问题《CoNLL-2003标签器在2023年是否仍然有效？》，我们发现答案是肯定的。我们希望我们的论文能够促使更多研究来改进模型的泛化能力。最后，请务必查看我们的论文和数据集，如果您有任何问题，请随时与我联系。非常感谢。</sample>
    <sample id="114">The presentation introduces "Finding the Pillars of Strength for Multi-Head Attention," a work from Nanyang Technological University, addressing the limitations of large language models (LLMs) – namely, their heavy parameter count, long training times, and large data requirements. The core problem is the redundancy within multi-head attention, where some attention heads can be pruned without significant performance loss.

Existing approaches to address this redundancy have faced limitations: homogenization-based methods sacrifice performance, diversification-based methods lack parameter efficiency, and scoring-based methods still leave considerable redundancy. The proposed solution is a grouped head attention (GHT) strategy employing a divide-and-conquer approach. It consists of two stages: group-constrained training and a voting-to-stay algorithm.

Group-constrained training divides attention heads into groups, encouraging intra-group similarity and inter-group separation. This is supervised by an unsupervised hidden unit discovery system. The voting-to-stay algorithm then prunes redundant heads within each group based on a score determined by evaluators.

The GHT and GHT-PS models achieved significant performance improvements on machine translation, language modeling, and abstractive summarization tasks, with GHT-PS achieving up to 32.1% parameter compression while maintaining comparable performance.  Efficiency analysis shows the LITE model achieves 90% parameter pruning, 62% faster inference, and 80% FLOPs reduction compared to the original model.

The researchers believe task-specific automatic pruning, guided by the Lottery Ticket Hypothesis, is a promising future direction. They argue that LLMs are often redundant in real-world scenarios, where only a subset of tasks is needed.  Pruning these redundant parameters, similar to uninstalling unused apps, can significantly reduce model size and improve efficiency without sacrificing performance. The team invites attendees to their poster session for further details.</sample>
    <sample id="115">该方法使用 lambda 个语音帧的语音片段。</sample>
    <sample id="116">Servin 是一个法官。</sample>
    <sample id="117">示例质量比与源句子的相似度更为重要。</sample>
    <sample id="118">The presentation introduces "Improving Pretraining Techniques for Code-Switched NLP," a paper addressing the challenges of natural language processing in multilingual environments where languages are mixed within a single sentence (code-switching). The authors highlight that standard multilingual pre-trained models like mBERT and XLM-R often underperform on code-switched tasks.

The core contribution is **SwitchMLM**, a novel Masked Language Modeling (MLM) technique specifically designed for code-switching.  The key idea is to focus masking on "switch-points" – tokens that mark the transition between languages.  The paper proposes two methods to implement SwitchMLM: **FrequencyMLM**, which uses the negative log likelihood of words in monolingual corpora to estimate LID tags (language identification tags), and **SwitchMLM**, which masks only the switch-points.  FrequencyMLM is presented as a practical alternative when LID tagging data is unavailable.

To further enhance performance, the authors introduce **ResBERT**, a method involving residual connections between intermediate layers of BERT and the final layer.  They also incorporate an **auxiliary LID-based loss** to encourage the intermediate layers to encode language information.

Experimental results demonstrate that the combined SwitchMLM (or FrequencyMLM) with ResBERT and the auxiliary loss achieves the best performance on sentiment analysis across various language pairs.

The presentation also includes probing experiments using linear and conditional probing to verify the claim that SwitchMLM increases switch-point information in BERT's intermediate and final layers.  These experiments show that SwitchMLM representations contain more switch-point information compared to standard MLM representations, and that adding residual connections can further improve this effect.

In conclusion, the paper proposes SwitchMLM and ResBERT, along with an auxiliary loss, to improve pretraining for code-switched NLP.  The methods are designed to enhance the representation of switch-points, leading to better performance on code-switched tasks. The authors verify their claims through probing experiments, demonstrating the increased switch-point information captured by their proposed techniques.</sample>
    <sample id="119">GPT-4, GPT series, BART series and its variants, RoBERTa.</sample>
    <sample id="120">该模型使用注意力机制（cross-attention mechanism）来处理不同层之间的信息，并根据注意力分数来决定是否发出部分翻译。</sample>
    <sample id="121">以下是根据英文内容中提到的直接推断示例：

*   说歌曲的名字，例如 "Easy on Me"。
*   提到歌曲的位置，例如 "the first one"。</sample>
    <sample id="122">Fudan University.</sample>
    <sample id="123">Ying and Zhiyang's research focuses on improving multi-modal zero-shot learning using instruction tuning, addressing a gap in existing literature that primarily concentrates on language-only tasks. They introduce MultiInstruct, the first large-scale multi-modal instruction tuning dataset, comprising 62 diverse tasks across 10 categories, derived from 21 existing datasets and each equipped with five expert-written instructions. Their base model is OFA, a unified multi-modal pre-trained model.

The research details the training and evaluation process, using a pre-trained OFA model and employing a unified sequence-to-sequence format to represent input (text, images, instructions, bounding boxes) in the same token space.  They evaluate the model on seen and unseen multi-modal tasks, reporting accuracy for classification and Rouge-L for generation.  A novel "sensitivity" metric is introduced to measure the model's consistency across different instruction variations.

Key findings demonstrate that instruction tuning significantly enhances OFA's performance on seen multi-modal tasks, and transfer learning from natural instruction datasets further improves results.  Increasing the number of instructions during fine-tuning leads to better performance and reduced sensitivity.  Transfer learning from natural instruction datasets also improves the model's sensitivity on natural instruction datasets.

The authors conclude that MultiInstruct provides a valuable resource for multi-modal instruction tuning, significantly improving OFA's capabilities and highlighting the benefits of various transfer learning strategies.  They plan to expand MultiInstruct with approximately 150 additional vision-language tasks and will release the dataset and model.</sample>
    <sample id="124">Tan Qingyu from NUS and Alibaba presented their work "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models." The research focuses on temporal reasoning, a crucial aspect of understanding the real world, categorized into three levels: time-to-time (e.g., "What is the year after 2010?"), time-to-event (e.g., "What team did Lionel Messi play for in 2010?"), and event-to-event (e.g., "What team did Lionel Messi play for after FC Barcelona?").  The study highlights that existing temporal reasoning research often prioritizes the second level, neglecting the broader scope.

The researchers conducted preliminary experiments on year prediction, finding that T5-L and FLAN-T5-L exhibited a bias towards the 2000-2020 time period, likely due to term frequency in their training data. ChatGPT showed promise for year prediction but struggled with month prediction.

To address this, they created the TempReason dataset, encompassing all three temporal reasoning levels and a wide range of time periods, leveraging Wikidata and Wikipedia. They evaluated temporal reasoning using Closed Book QA, Open Book QA (with Wikipedia articles), and a novel "Reasoning QA" setting requiring temporal knowledge.

A new training strategy, TempT5, was proposed, combining temporal span extraction pre-training and time-sensitive reinforcement learning to reward correct temporal predictions and penalize errors.  Experiments on TempReason showed that TempT5 significantly outperformed existing models like FLAN-T5-L and ChatGPT, particularly in Open Book QA and Reasoning QA.  ChatGPT demonstrated inconsistent performance across different time periods, indicating a flaw in its temporal reasoning.

The study concludes by analyzing temporal reasoning biases in LLMs, introducing the TempReason benchmark, and proposing the TempT5 training paradigm to improve their temporal reasoning capabilities.  Future work will focus on mitigating potential biases related to training data imbalance.</sample>
    <sample id="125">The provided text does not mention the number of authors. It only names Yanis Labrak as the presenter.</sample>
    <sample id="126">是的，在语义解析之前，使用 Google Translate API 将自然语言查询翻译成目标语言，然后使用单语模型进行训练和评估。</sample>
    <sample id="127">Namgyu Ho from KAIST AI introduces their research paper, "Large Language Models Are Reasoning Teachers," a collaborative effort with Laura Schmid and Se-Young Yun. The paper addresses the limitation of chain-of-thought (CoT) reasoning, a technique enabling LLMs to solve complex tasks, which currently restricts applicability to massive models like GPT-3 and PaLM due to high computational costs.

The core idea is to leverage these large models as "reasoning teachers" to transfer their reasoning abilities to smaller, more accessible models.  The proposed method involves prompting large models to generate step-by-step solutions for complex tasks, then using these solutions as training data for smaller models.  A key innovation is "Diverse Reasoning," which generates multiple reasoning samples from the teacher model using stochastic temperature sampling, leading to more robust and effective training.

The research demonstrates that fine-tuned CoT models, using Diverse Reasoning, achieve notable performance on 12 tasks, outperforming prompt-based baselines and significantly improving upon vanilla fine-tuning, even with small student models (0.3 billion parameters).  The performance scales favorably with increased dataset size, better teacher models, and larger student models, but also presents trade-offs between development costs (teacher model complexity) and inference costs (student model size).

The paper highlights the potential for distilling reasoning abilities from large LLMs to smaller ones, opening avenues for deploying advanced reasoning capabilities in resource-constrained environments.  The researchers provide open-source code and data, including access to OpenAI inference, encouraging further exploration and application of their method. They emphasize the accessibility and scalability of their approach, while acknowledging the necessary trade-offs. The paper also delves into the emergence of reasoning in smaller models.</sample>
    <sample id="128">Akshatha and Martin's work, "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources," explores the challenge of natural language understanding (NLU) models utilizing both pre-training and inference-time knowledge.  The research highlights that while models can leverage knowledge embedded in their parameters (acquired during pre-training), they often lack the ability to access and integrate knowledge relevant to specific, instance-specific situations encountered during inference. This is exemplified by the difficulty in resolving coreferences when the entity being referred to is novel or has changed since pre-training.

To address this, the authors introduce KITMUS, a diagnostic test suite designed to probe for knowledge integration capabilities.  The test suite centers around a coreference resolution task, requiring models to identify the correct entity referred to by a pronoun, drawing upon both entity-specific and background knowledge.  The test includes three settings: "Background-Pretrain" (background knowledge available during pre-training), "Background-Both" (background knowledge available both during pre-training and inference), and "Background-Inference" (background knowledge only available at inference time). The "Background-Inference" setting is particularly crucial as it simulates scenarios where background knowledge is not present in the model's pre-training data.

Experiments with human participants and established coreference resolution models reveal that while models trained on generic datasets tend to rely on surface-level cues, KITMUS requires task-specific training to effectively integrate knowledge from multiple sources.  Even the best-performing models struggle with reliably integrating knowledge available only at inference time.  The study concludes that many coreference resolution models are unable to reason over knowledge from different sources without task-specific training, although some models demonstrate success with it.  The authors emphasize the ongoing challenge of reliably integrating backward knowledge presented only at inference time.  The KITMUS dataset and code are available on GitHub.</sample>
    <sample id="129">作者给出的“显性群体”(marked group) 的示例包括：

*   亚洲女性 (Asian woman)
*   中东女性 (Middle-Eastern woman)
*   黑人女性 (Black woman)
*   白人男性 (White man)
*   拉丁裔女性 (Latina woman)
*   亚裔女性 (Asian woman)</sample>
    <sample id="130">根据演讲内容，没有明确指出哪些模型架构泛化能力较差。演讲主要探讨了transformer模型在泛化方面的优势，并认为transformer模型通常比其他模型泛化更好。</sample>
    <sample id="131">Clean validation set.</sample>
    <sample id="132">三位作者。</sample>
    <sample id="133">MultiInstruct 采用了多种模态，包括文本、图像和 bounding box。</sample>
    <sample id="135">This presentation introduces ABC-Eval, a novel dimensional approach to evaluating conversational AI, developed by the Emory NLP Lab and Amazon Alexa AI.  Traditional methods rely on human judgment for overall quality, but ABC-Eval aims for more granular, reliable assessments by explicitly annotating model behaviors like irrelevant responses, contradictions, and factual errors.

The study evaluated four state-of-the-art chat models using ABC-Eval on 100 human-bot conversations, comparing it to existing methods: Likert ratings at turn and dialogue levels, and pairwise comparisons.  The results showed ABC-Eval labels are more reliable and predictive of overall conversation quality than existing methods, as measured by inter-annotator agreement and linear regression analysis.  Specifically, metrics like self and partner contradictions explain a significant portion of conversation quality, while Likert consistency scores explain less.

ABC-Eval metrics also demonstrate distinctiveness, with the combination of all metrics explaining over 25% of conversation quality, whereas turn-level Likert metrics explain far less.  The study quantified common challenges in conversational AI, revealing that bots exhibit common sense violations, irrelevant information, and contradictions at rates of approximately 20%, 15%, and 10% respectively.

The researchers emphasize the importance of reliable and precise evaluation metrics for comparing conversational AI models, especially given the rapid advancements in the field. They hope ABC-Eval will be a valuable tool for the community to further advance the development of conversational AI. The study highlights the need for more detailed analysis of model behaviors to improve the overall quality of dialogue systems.</sample>
    <sample id="136">Jasivan and Nafise from the University of Sheffield presented "FERMAT: An Alternative to Accuracy for Numerical Reasoning," addressing the limitations of current numerical reasoning benchmarks. The motivation stems from the real-world application of numerical reasoning in tasks like fact-checking, where models often struggle with mathematical operations despite achieving high accuracy.

The paper introduces FERMAT, a flexible evaluation set based on arithmetic types extracted from Illinois and CommonCore curricula. FERMAT tests models on number understanding, mathematical operations (easy vs. combined), and training dependency.  The dataset includes questions with varied number representations (e.g., 5.0, large integers, decimals) and operations.

A baseline zero-shot evaluation reveals poor performance across all aspects, suggesting existing benchmarks are insufficient. Fine-tuning models with human-written templates (replacing numbers with placeholders) generated 200,000 examples, leading to performance improvements.  The study found that models often fail to memorize exact expressions, highlighting the importance of linguistic nuances like the presence of "increases" vs. "another."

Furthermore, the impact of training templates was investigated.  Using diverse templates from GSM8K and AQUA, alongside mathematical diversity, significantly improved performance compared to using only the original templates.

The key conclusions are that existing benchmarks are unrepresentative and single scores are inadequate. FERMAT offers a more informative alternative by considering language and mathematical diversity.  The research also points to the need for improvements in number encoding and tokenization. The presentation concludes with a reminder of the provided QR code and links for further exploration.</sample>
    <sample id="137">Sicong from the Singapore University of Technology and Design introduces "Tell2Design," a dataset and model for language-guided floor plan generation, published in ACL 2023.  The paper addresses the need for design generation that adheres to specific requirements outlined in natural language, contrasting with the current focus on image generation from text.

The core problem is enabling users to "tell" instructions for floor plan design, a task requiring understanding of semantics, geometry, and topology within a structured layout.  Tell2Design comprises 5,051 human-annotated instructions and 76,000 artificially generated ones, totaling over 10 sentences per floor plan.

The research tackles three key challenges: strict constraints compared to image generation, understanding complex, unstructured text, and handling ambiguity in instructions.  Unlike traditional methods, Tell2Design uses a sequence-to-sequence model (encoder-decoder) to reconstruct room bounding boxes into a structured sequence, allowing for variable instruction lengths.  The model leverages a pre-trained T5 language model and a language modeling objective.

Evaluation shows Tell2Design achieves high IoU scores (Micro IoU 54, Macro IoU 53), outperforming text-conditional image generation baselines. This success is attributed to the model's ability to focus on salient information from the language instructions.  Text-conditional image models struggle because they prioritize high-level visual concepts over detailed instruction adherence.  The study also reveals a language distribution gap between artificial and human instructions, but indicates that combining both during training improves performance.

The paper concludes by establishing Tell2Design as a foundation for future research in language-guided design generation, demonstrating the potential for users to interact with design processes through natural language.</sample>
    <sample id="138">作者认为 NLU 中研究不足的领域是：

1.  **如何有效地整合预训练时知识和推理时知识。**
2.  **如何可靠地整合仅在推理时提供的知识（即“反向知识”）。**
3.  **许多核心指代消解模型在没有任务特定训练的情况下，无法从不同来源推理知识。**
4.  **即使是最好的模型，也难以可靠地整合仅在推理时提供的知识。**</sample>
    <sample id="139">Ying and Zhiyang.</sample>
    <sample id="140">是的，CoScript 经过了质量检查。论文中提到，为了确保验证和测试集的质量，他们请众包工作者查找并修改错误样本。</sample>
    <sample id="141">现有资源在评估依赖上下文的翻译方面存在以下局限性：

1.  **覆盖范围有限：** 仅支持有限类型的上下文依赖翻译，且仅针对有限的语言。
2.  **依赖领域知识和人工标注：**  这些资源通常依赖于领域知识和人工标注，难以推广到更广泛的场景。
3.  **无法捕捉部分依赖上下文的翻译：** 由于只有少量翻译依赖上下文，因此传统的语料库级别指标（如 BLEU）无法捕捉到这些翻译。</sample>
    <sample id="142">Javad Hosseini 和 Filip Radlinski、Silvia Pareti 和 Annie Louis 合作研究了“解决间接指代表达式以进行实体选择”问题，并介绍了 AltEntities 数据集。我们的目标是理解用户在选择时使用的语言。例如，“你指的是 ‘Easy on Me’ 还是 ‘I Gotta Feeling’？” 在这里，用户想要选择这两首歌中的一个。最明显的方法是使用直接引用，例如说歌曲名称“Easy on Me”或其位置“第一个”。但有时间接引用更合适，以便进行更自然的对话。这可能发生在用户记不住歌曲名称时，或者歌曲的音高太相似，难以区分时，或者用户想要指定偏好时。以下是一些间接引用的例子，例如“更新的那个”或“不是充满活力的那个”。这是一个在对话系统以及评估大型语言模型实体理解方面都非常重要的问题。我们意识到目前没有更大的公共数据集来解决这个问题，因此我们收集了一个通过众包标注的数据集。该数据集涵盖了三个不同的领域：音乐、书籍和食谱。我们数据集的收集方法强调非正式性，采用卡通完成设置。卡通包含三个对话气泡。在第一个气泡中，鲍勃说：“请记住我们昨天正在听的歌？” 这样就设置了对话背景。在第二个气泡中，艾丽斯说：“你指的是 ‘Easy on Me’ 还是 ‘I Gotta Feeling’？” 这是替代问题。在第三个气泡中，鲍勃使用间接引用来选择这两个实体，例如，“更新的那个”。我们自动生成第一个和第二个气泡，但第三个气泡由标注者填写。第一个气泡从多个人工提示中选择，这些提示来自不同的领域。第二个气泡，即替代问题，通过以下模板生成：“你指的是 A 还是 B？” 其中 A 和 B 是维基百科上的样本。我们使用的不同采样方法如下：数值越高，实体之间的相似度越高，通常越难进行歧义消除。第一个方法是随机均匀采样。第二个方法是在实体标题相似的情况下进行采样，例如，两个名为“The Return”的书籍。第三个方法是在维基百科上实体描述相似的情况下进行采样。例如，对于歌曲，我们简单地提供每个歌曲的 Google 搜索链接，然后要求标注者听至少部分每首歌曲并阅读每首歌曲。对于食谱和书籍领域，我们提供维基百科上的背景文本。对于食谱，我们还显示维基百科上的图片，以便标注者了解它们的外观。然后，我们要求标注者选择一个实体，例如，这里是第一个，并用三到五个间接指代表达式描述它。例如，带有钢琴音乐的那一个。“没有歌词的那一个”、“不是那个有 12 岁男孩的那一个”、“虚构的那一个”、“来自阿塞拜疆”等等。AltEntities 数据集包含 6,000 个替代问题，涵盖三个领域，并包含 42,000 个间接指代表达式。使用 T5 XL 模型的结果总结如下。如果语言模型能够获得与标注者相同的背景知识，那么准确率将非常高，约为 92% 到 95%。但这在现实中是不可能的。如果语言模型能够获得一些部分重叠的背景知识，那么准确率将介于 82% 到 87% 之间，这更接近现实。例如，如果语言模型检索到实体名称，那么准确率仅为 60%，还有很大的改进空间。我们还证明了模型具有领域泛化能力。这里有一个数据集的链接。谢谢。</sample>
    <sample id="143">该方法与 Wait-k 策略、Local Agreement 策略以及专门针对同步预翻译定制的 state-of-the-art 架构进行了比较。</sample>
    <sample id="144">The provided text does not explicitly state the authors' affiliated institution. It only mentions Yanis Labrak.</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">Yicheng from Fudan University introduces a research paper on dialogue summarization, focusing on the problem of omission – where critical information is missing from generated summaries. While large language models produce fluent summaries, they often suffer from factual errors, including omissions, which significantly impact summary quality.

The paper highlights the severity of omission, revealing that even state-of-the-art models have a high omission rate (around 70%) across various domains.  Omissions are distributed randomly within dialogues, indicating challenges in identifying key information.

To address this, the researchers propose the OLDS dataset, a high-quality dataset of dialogue summaries with omission labels, built upon existing benchmarks. They developed an automatic method to generate these labels and ensured their quality through human evaluation. The dataset includes 10 candidate summaries per dialogue, generated by different models and decoding strategies.

The paper explores three baseline frameworks for omission detection: pair-wise classification, sequence labeling, and pointer networks.  They evaluate these models using Precision, Recall, and F1-score, and also calculate the Word-level Omission Recall (WR) score. The results show a significant label imbalance, indicating the task's difficulty.

Furthermore, the research investigates the potential of using detected omissions to refine summaries. They propose a post-editing method that concatenates candidate summaries with omission content as input to a sequence-to-sequence model.  The results demonstrate that incorporating omission information significantly improves summary quality, suggesting that omission detection is a valuable task for enhancing dialogue summarization. The dataset is publicly available.</sample>
    <sample id="147">这篇论文由 Myra, Esin Durmus 和 Dan Jurafsky 三位作者共同完成。</sample>
    <sample id="148">你好，我是特伦托大学和布鲁诺·科斯克尔基金会 Sara Papi，我将简要介绍论文“注意力作为同步语音翻译的指南”，这是一项与 Matteo Negri 和 Marco Turchi 合作的研究。同步语音翻译（SimulST）是指将口语翻译成目标语言的文本，从而实现跨语言交流。而当前 SimulST 模型存在哪些问题？通常会使用特定的架构进行训练，引入额外的模块进行优化。长而复杂的训练过程，例如涉及不同优化目标的训练。以及训练和维护多个模型以达到不同的延迟水平。例如，训练一个平均为一秒延迟的模型，另一个延迟为两秒的模型，以此类推。那么我们的解决方案是什么？首先，使用现有的离线 ST 模型，而无需重新训练或采用专门为 SimulST 设计的架构。仅使用一个模型来处理每个延迟水平，并通过特定的参数来处理延迟。并利用模型已获得的知识，通过音频输入和文本输出之间的注意力机制（交叉注意力机制）来实现。您可以在右侧看到一个示例。我们的解决方案是提出 EDAtt，即编码器-解码器注意力策略，它决定是否发出部分翻译，这取决于注意力点在哪里。一个词会被发出，如果注意力没有集中，即其总和低于一定的阈值 alpha，相对于最后 lambda 个语音帧，意味着接收到的信息足够稳定。例如，如果我们接收到包含“我将谈论……”的语音片段，我们的模型预测德语翻译，我们会查看交叉注意力权重，我们会看到第一个两个词指向最早接收的语音帧，而最后一个词指向最后接收的 lambda 个语音帧。这意味着第一个两个词会被发出，因为由于交叉注意力权重之和高于一定的阈值 alpha，我们将不会发出最后一个词，而是等待另一个语音片段。如果我们继续接收到另一个语音片段，并且我们的模型预测了另外三个词，我们会查看这些交叉注意力权重。我们会看到没有词指向最后 lambda 个语音帧。这意味着这三个词会被发出。如果我们查看 EDAtt 的主要结果，我们将绘制在图表上的同步语音翻译结果，其中 BLEU 值表示翻译质量，平均滞后表示延迟量，我们还考虑了计算成本相关的平均滞后，这考虑了模型预测输出的计算时间。因此，我们希望我们的曲线尽可能高，并且向左移动。我们还将其与针对离线模型的流行策略进行比较，即 Wait-k 策略和 Local Agreement。我们还将其与专门为同步预翻译设计的 state-of-the-art 架构进行比较。所有这些都是针对德语的同步语音翻译策略的结果。我们看到，这些策略优于所有针对离线模型的策略，因为曲线向左移动。我们还看到，如果考虑实际的延迟时间或计算成本相关的延迟时间，那么这是最快的策略。如果您想了解更多结果，请阅读我们的论文。我们还开源了代码和模型以及同步输出，以方便重现我们的工作。感谢您的聆听。</sample>
    <sample id="149">是的，数据集公开。演讲中提到“we also found that the performance drop here is caused by temporal drift and kind of surprisingly, it is not caused by adaptive overfitting even though CoNLL-2003 has been used for over 20 years. And lastly, please make sure to check out our paper, our data set...” 这表明数据集是公开的。</sample>
    <sample id="150">Archiki from Adobe Research and UNC Chapel Hill presented their ACL paper "MEETINGQA," focusing on extractive question-answering in meeting transcripts. They highlight the unique challenge of meeting data – long, domain-specific, and information-rich – which is currently underutilized for QA compared to summarization and action item extraction.

MEETINGQA introduces a new dataset comprising questions asked during meetings and their corresponding answers. These questions are longer, more open-ended, and designed to elicit discussions. The dataset includes diverse answer scenarios, such as multiple speakers, discontinuous sentences, and rhetorical questions.

The data was collected from public meeting transcripts (AMI corpus) through question selection and annotation by human annotators, achieving a high inter-annotator agreement (Krippendorff's alpha of 0.73). The dataset contains 7.7K questions, with 30% unanswerable, 40% with multispan answers, and 48% with multi-speaker answers. Question types vary, with a majority being yes/no or opinion-seeking, and 20% rhetorical questions.

The paper explores various methods for meeting QA, including context retrieval, single-span models, and multi-span models (token classification). They also leverage data augmentation using silver annotations from the MediaSum dataset.

Results show a significant gap (over 25 F1 points) between fine-tuned models and human performance. Short-context models outperformed long-context models. Multi-span models showed slightly better performance than single-span models. Zero-shot performance is also significantly lower, but data augmentation improves it. Larger instruction-tuned models like FLAN-T5 achieve comparable zero-shot results.

Error analysis reveals challenges in identifying rhetorical questions and irrelevant sentences in single-span models.  Models also struggle to identify speaker attribution, particularly in zero-shot settings.

In conclusion, MeetingQA presents a challenging dataset for QA, requiring advanced models to handle the complexities of real-world meeting discussions. The research highlights the limitations of current QA models and the need for further development in this domain.</sample>
    <sample id="151">大家好，我的名字是 Ying，我的同事 Zhiyang，我们今天将介绍我们的研究，主题是 MultiInstruct 如何通过指令微调提高多模态零样本学习能力。随着大型语言模型的不断发展，许多研究开始探索利用预训练语言模型为不同下游任务进行参数和数据高效复用的新学习范式。最近，许多研究表明，指令微调能够使大型语言模型在没有见过的情况下，通过遵循自然指令执行任务，实现零样本性能。然而，此前关于指令微调的大部分工作都集中在提高语言任务的零样本性能上，而计算机视觉和多模态任务则被忽略了。因此，在我们的工作中，我们希望研究指令微调多模态预训练模型是否能够提高对未见多模态任务的泛化能力。此外，在我们的研究初期，我们发现在 NLP 和多模态任务之间存在相当大的指令数据集可用性差异。目前，有超过 1600 个语言任务指令数据集。然而，尚不存在大型公开的多模态指令任务数据集。因此，这促使我们构建了一个多模态指令微调数据集。我们现在介绍 MultiInstruct，这是第一个多模态指令微调基准数据集，包含 62 种多样化的多模态任务，涵盖 10 个广泛的类别。这些任务是从 21 个现有开源数据集派生的，每个任务都配备了五位专家撰写的指令。为了研究多模态指令微调，我们使用 OFA，一个统一的多模态预训练模型作为基础模型。OFA 使用统一的词汇表来处理语言、图像令牌和边界框的坐标。在这里，我们展示了 MultiInstruct 数据集的一些示例实例，以统一各种输入和输出数据类型的处理。我们遵循 OFA 的方法，将所有任务都格式化为统一的序列到序列格式。在此中，输入文本、图像、指令和边界框都表示在相同的令牌空间中。好的，现在我将谈论多模态指令微调。因此，对于训练数据集，我们使用 9 个组中的 53 个任务进行训练，并对每个任务采样 10,000 个实例。对于测试，我们保留了常识推理组的整个常见推理组用于测试，并选择额外的 5 个任务来自 VQ 和 Miscellaneous 组。我们使用测试集中的所有实例对每个任务进行使用。此外，我们随机采样 20 个来自自然指令的测试集任务作为 NLP 的未见任务。因此，我们使用预训练的 OFA 大型模型作为基础模型。在训练过程中，我们将所有任务的实例混合在一起。每个实例都随机组合到一个指令模板中。因此，在测试中，对于每个任务，我们进行总共 5 次实验，通过使用不同的 5 个指令来评估模型。在每次实验中，我们报告性能的最小值和最大值以及性能在所有 5 个实验中的标准差。如果任务是多模态分类任务，我们报告准确率。如果是多模态生成任务，我们报告 Rouge-L。对于 NLP 任务，我们也报告 Rouge-L。我们还引入了一个额外的评估指标，称为敏感性。该指标衡量模型在不改变指令措辞的情况下，对相同任务产生相同输出的能力。现在，我们的主要结果是，指令微调可以显著提高 OFA 在已见多模态任务上的性能。此外，从自然指令数据集进行迁移学习可以为指令微调带来好处。正如我们所看到的，随着任务数量的增加，模型能够实现更好的性能，同时降低敏感性。因此，我们还进行了实验。我们使用一个指令与 5 个指令进行比较。正如我们所看到的，使用更多的指令可以提高模型的整体性能并大大降低其敏感性。这表明了不同微调策略对模型敏感性的影响。正如我们所看到的，从自然指令数据集进行迁移学习可以使模型比原始 OFA 模型实现更好的敏感性。我们还看到，从自然指令数据集进行迁移学习可以帮助 OFA 在自然指令数据集上取得更好的性能。总而言之，我们提出了第一个大型多模态指令微调数据集，它显著提高了 OFA 的短能力，并探索了不同的迁移学习技术及其益处。我们设计了一个新的指标，称为敏感性。此外，我们正在收集一个更大的多模态指令微调数据集，其中包含大约 150 个额外的视觉语言任务，我们将发布它们。这是我们数据的 QR 代码和模型。谢谢。</sample>
    <sample id="152">Frederick Riemenschneider and his team are exploring the use of Large Language Models (LLMs) for Classical Philology, specifically Ancient Greek and Latin. They address the limitations of existing models, which are primarily monolingual BERT models trained on limited datasets. Their project focuses on creating new, more capable models for these languages, exploring different architectures and multilingual capabilities.

The team developed several models: GreBERTa (monolingual RoBERTa for Ancient Greek), GreTa (monolingual encoder-decoder T5 for Ancient Greek), PhilBERTa (multilingual model for Greek, Latin, and English), and PhilTa (multilingual encoder-decoder model).  They tackled data challenges by creating a high-quality pre-training corpus for Ancient Greek using the Internet Archive, identifying and correcting OCR errors.  For multilingual models, they leveraged existing corpora for Latin and related English texts.

The models were benchmarked on tasks like part-of-speech tagging, dependency parsing, and lemmatization using datasets like Universal Dependencies for Greek and EvaLatina 2022 for Latin.  Results show significant performance improvements over existing state-of-the-art.  They investigated the behavior of the T5 encoder and found that while initially poor, it improves with training.  Their encoder-decoder models excel at lemmatization, achieving a 5% performance gain for Ancient Greek.

Furthermore, they probed the models' semantic and world knowledge capabilities, finding that they outperform previous models in these areas.  Interestingly, they found no significant performance difference between monolingual and multilingual models.  The project emphasizes the importance of native tokenizers and high-quality pre-training data for achieving robust performance in Classical Philology.  The team's work provides a foundation for future research and applications in this field.</sample>
    <sample id="153">Ninareh Mehrabi from Amazon Alexa AI's Responsible AI team presented their work on resolving ambiguities in text-to-image generative models. The core problem addressed is that ambiguous prompts can lead to inaccurate image generation, hindering the model's ability to fulfill user intent.

The research focuses on two key aspects: prompt disambiguation and evaluation of generated images. They developed a benchmark dataset based on LAVA, encompassing various ambiguity types.  A central component is a prompt disambiguation framework. This framework employs two methods: first, a language model generates clarifying questions based on the ambiguous prompt, and the user answers these questions to refine the prompt. Second, the language model generates multiple possible visual interpretations, and the user selects the one that best aligns with their intention.  The final, disambiguated prompt is then used with a text-to-image model.

To assess the faithfulness of generated images to user intent, the researchers introduced an automatic evaluation framework. This framework compares the original ambiguous prompt with the disambiguated prompt, feeding both into a Visual Question Answering (VQA) model. The VQA model then assesses whether the image accurately reflects the user's intended meaning.

The findings demonstrate that ambiguity resolution varies across different types of ambiguities.  The proposed disambiguation framework generally improves image fidelity.  Furthermore, the automatic evaluation framework aligns with human evaluation, making it a reliable tool for assessing text-to-image models. The paper also includes additional insights and discussions.

In summary, the work provides a comprehensive approach to tackling prompt ambiguity in text-to-image models, offering both methods for resolving ambiguities and a reliable automatic evaluation system. The researchers encourage readers to refer to their paper for further details.</sample>
    <sample id="154">University of Trento and Foundazione Bruno Kessler.</sample>
    <sample id="155">Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="157">The presentation introduces "Dialogue Summarization with Static-Dynamic Structure Fusion Graph" (SDDS), a new approach to automatically generating concise summaries of dialogues. The research addresses the limitations of existing methods that rely on pre-computed static graphs derived from external linguistic tools, which can be inaccurate and inflexible.

SDDS employs a four-component model: an Utterance Encoder to convert dialogue utterances into vector representations, a Static-Dynamic Graph module, a Summary Generator, and a pre-trained language model. The Static-Dynamic Graph module combines static graphs built using heuristic methods like Discourse Parsing Graph (based on keyword co-occurrence and speaker interaction frequency) and utterance position information.  These static graphs capture the dialogue structure.

The Dynamic Graph module leverages multi-head attention to learn semantic relationships between utterances based on their deep vector representations, adapting to the dialogue's flow.  The static and dynamic graphs are then fused using a unified graph, Gu, and incorporated into the summary generation process via a graph attention layer.

The authors emphasize the model's ability to dynamically adapt to the dialogue context and overcome the limitations of static graph-based approaches.  The code and data are publicly available on GitHub. The research aims to provide a more accurate and flexible solution for dialogue summarization, enabling users to quickly grasp the key points of multi-participant conversations.</sample>
    <sample id="158">Qipeng Guo from AWS introduces their work on "Dual Cache for Long Document Neural Coreference Resolution." Coreference resolution aims to identify mentions referring to the same entity within a document, a task hindered by the quadratic complexity of traditional methods. Cache-based methods offer linear complexity but suffer from high cache misses in long documents due to topic shifts and scattered mentions.

The proposed dual cache utilizes both a local cache (LRU eviction) for local entities and a global cache (LFU eviction) for global entities.  The model scans the document, classifying mentions as new or existing, and adds them to the appropriate cache based on frequency.  Cache eviction triggers when full.

Experiments on LitBank, OntoNotes, and WikiCoref demonstrate that the dual cache outperforms baselines, even with unbounded memory, and is faster than single-cache methods.  Performance gaps are larger for book-level documents, highlighting the benefits of the dual cache's ability to handle long texts.  The dual cache significantly reduces cache misses compared to single-cache approaches.

The study concludes that the dual cache achieves the best performance-to-cost ratio among cache-based methods. It effectively separates local and global entities, reduces cache misses, and offers a cost-effective solution for long document coreference resolution. The dual cache's architecture addresses the limitations of single-cache methods, making it a promising approach for coreference resolution in long texts.</sample>
    <sample id="159">Koustav Sinha 先生和 John Gauthier、Aaron Mueller、Kanishka Misra、Karen Fences、Roger Levy 和 Adina Williams 合作，今天我们来谈谈 ACL 2023 论文。语言模型对可接受性的判断并非总是对上下文稳定的。这项工作是基于最小对偶范式。最小对偶范式基本上通过评估语言模型在可接受性判断上的表现，例如 BLiMP、SyntaxGym 或基于样板的判断（如 CrowS pairs）。在这种范式中，通常是将一个可接受的句子和一个不接受的句子展示给模型，然后期望模型更倾向于可接受的句子。当前的 MPP 管道并不允许我们评估模型在更长句子上的接受性。如今，大型语言模型拥有越来越长的上下文窗口。因此，评估模型在整个上下文窗口上的接受性至关重要，而这就是我们试图做到的。我们试图重新审视 MPP 管道，要求模型在更长序列上评估可接受性。这就是我们的方法。因此，我们通过重新审视数据集本身来模拟这些更长的序列，并选择可接受或不接受的句子。例如，我们选择了 BLiMP 数据集中的 Adjunct Island 案例的典型可接受性对偶。我们提取 Adjunct Island 中的语法句子，并将其作为可接受查询和不接受查询的前缀添加。我们可以对同一匹配的句子进行操作，也可以从不同的子集或不同的数据集选择句子。这就是我们称之为“异构场景”的方法。在这里，句子仍然来自相关的子集，但不是来自评估时使用的相同数据集。我们还可以选择完全无关的领域，例如维基百科。这将表明模型的可接受性判断是否受到任何上下文的影响，即上下文是否来自不同的子集，还是完全与当前正在查看的句子无关。那么，模型表现如何？首先，我们查看了维基百科的句子，这些句子与当前查询对完全无关，而在那里我们发现 MPP 判断在任意上下文长度下都相当稳定。然后，当我们选择来自相同数据集的句子时，即从 BLiMP 或 SyntaxGym 数据集中的可接受和不接受领域选择句子时，MPP 判断会显著增加或减少。然而，当我们匹配结构时，即选择来自同一现象的句子时，我们看到 MPP 判断会大幅增加或大幅减少，这取决于我们选择的前缀是可接受的还是不接受的。这种效应随着上下文长度的增加而变得非常大，这可能会影响具有大型上下文窗口的较新的语言模型。为什么匹配前缀会如此强烈地影响语言模型的判断？因此，我们进行了一系列分析，试图通过对输入句子进行扰动，同时保留相关的结构并向输入添加噪声来扰动输入句子。在进行了多次这样的扰动后，我们发现这些噪声实际上并没有让模型改变其在 MPP 判断上的表现方式。也就是说，当我们扰动可接受领域的句子时，我们会看到所有扰动都以相似的方式增加 MPP 判断，当我们扰动不接受领域的句子时，我们会看到 MPP 判断以相似的方式减少。因此，我们的工作的主要结论是，语言模型对共享于句子中的潜在语法和语义特征非常敏感。而目前以短句和单个句子为基础的 MPP 评估方法可能无法完全捕捉语言模型在整个上下文窗口上的抽象知识。请阅读我们的论文以获取更多实验细节。谢谢您的聆听。</sample>
    <sample id="160">该方法的第一步将输入词元映射到**一个无序多重集（unordered multiset）的词元**。</sample>
    <sample id="161">55,000</sample>
    <sample id="163">MASSalign。</sample>
    <sample id="164">弱监督学习的好处在于，它使用弱标签数据进行训练，这比完全人工标注数据更便宜。然而，弱标签数据通常存在噪声，直接在弱标签数据上训练神经网络会导致模型记忆噪声，无法泛化。弱监督学习的目标是提出训练算法，使其能够有效地在噪声标签数据上训练神经网络，从而实现良好的泛化性能。</sample>
    <sample id="165">Wenting Zhao from Cornell University presented their paper "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations," focusing on unsupervised abductive reasoning. Abductive reasoning aims to find the most plausible explanation for an observed outcome given a context. The paper addresses the challenge of learning abductive reasoning without labeled plausible explanations, a common issue in supervised methods where such annotations are often noisy.

The authors introduce LiPoR (Likelihood Learning with Posterior Regularization), an unsupervised method that treats explanations as latent variables. LiPoR maximizes the likelihood of the outcome given the context, effectively learning to predict the most probable explanation. However, this approach doesn't inherently favor plausible explanations. To address this, LiPoR incorporates a regularizer based on the mutual exclusivity of explanations.  The paper leverages the fact that explanations are mutually exclusive, meaning one cannot be true while another is false.

The LiPoR objective function combines maximizing the likelihood of the outcome with minimizing the entropy of the probability distribution over explanations, effectively preferring a subset of plausible explanations.  The regularizer, denoted by Omega, balances the likelihood of the outcome with the preference for a limited number of plausible explanations.

The paper evaluated LiPoR on the AlphaNLI dataset, a widely used abductive reasoning benchmark.  The results demonstrate that LiPoR outperforms all existing zero-shot models and the previous best unsupervised approach by over 4 absolute points in accuracy. This highlights the effectiveness of the proposed method in learning abductive reasoning without relying on labeled plausibility. The paper is available at tinyurl.com/zhao-lipor.</sample>
    <sample id="166">Yunxin from Harbin Institute of Technology, Shenzhen, introduces their new work, "A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text." The research addresses the challenge of image retrieval from long, complex text descriptions, where typical visual language models struggle.

The proposed framework draws inspiration from Divide-and-Conquer strategy and Dual-Process Theory.  Dual-Process Theory posits that human cognition involves two systems: System 1 (analogical reasoning) and System 2 (logical reasoning).  Pre-trained visual language models primarily rely on System 1, leading to performance degradation with complex tasks.  The framework aims to integrate System 2 logical reasoning to enhance complex retrieval.

The system comprises three main modules:

1.  **Proposition Generator:** Decomposes complex text into simpler propositions, using BART to generate corresponding sentences.
2.  **Visual-Linguistic Interactor (System 1):** Performs visual-proposition interaction, generating matching scores and reasoning states for propositions and images.
3.  **Neural-Symbolic Reasoner (System 2):** Integrates reasoning states from simple propositions to obtain the final solution. It uses a negation executor and conjunction operation for logical reasoning.

The research demonstrates that the proposed Neural Divide-and-Conquer Reasoning (NDCR) framework outperforms existing baselines, with ablation experiments confirming the effectiveness of each module.  The system also exhibits interoperability by presenting inference states and results at intermediate steps.

The authors suggest that neural symbolic calculation can improve compositional reasoning and planning in large language models.  The framework's Divide-and-Conquer approach is likened to chain-of-thought reasoning, both aiming to decompose complex problems into simpler ones. Integrating Dual-Process Theory with Divide-and-Conquer is seen as a promising avenue for enhancing complex reasoning capabilities.</sample>
    <sample id="167">DEPLAIN-web 包括不同领域，其中 750 个文档手动对齐，其余文档使用自动对齐方法进行对齐。</sample>
    <sample id="168">CoNLL++ 数据集是从 2020 年的 Reuters 新闻数据中收集的，然后使用与 CoNLL-2003 标注指南相同的标注方法进行标注。</sample>
    <sample id="169">David Vilar from Google Translate presents a review of the paper "Prompting PaLM for Translation: Assessing Strategies and Performance." The paper investigates the effectiveness of prompting large language models (LLMs) like PaLM (a 540 billion parameter model trained on 780 billion tokens) for machine translation. PaLM achieved state-of-the-art results in various NLP tasks prior to its release in 2022.

The study is the first systematic evaluation of LLM prompting in MT, utilizing best practices and comparing PaLM to state-of-the-art systems like WMT.  Neural MT metrics and human evaluation (using MQM framework) were employed.  The research highlights the significant impact of prompting on translation performance, demonstrating that even small prompt variations can lead to substantial differences (BLEURT scores of 1+ to 40+ points).

The study found that the quality of the examples in the prompt is more crucial than their similarity to the source sentence, especially for 5-shot prompting.  Prompting strategies like marking source and target sentences with their respective languages (e.g., "German colon" for German sentences) proved effective.  The quality of the development data (dev data) was also more influential than the training data.

While PaLM's translation quality is close to commercial systems like Google Translate, it still exhibits accuracy issues, particularly omission errors.  Human evaluation revealed that PaLM produces fluent translations but sometimes sacrifices accuracy for stylistic reasons.  The paper concludes by recommending prompt selection strategies based on example quality and highlighting the importance of careful prompt design.  The full paper provides more detailed insights.</sample>
    <sample id="170">大家好，我是来自宾夕法尼亚州立大学的Yusen Zhang。今天我将介绍我们的工作“XSemPLR：跨语言语义解析在多个自然语言和意义表示中的研究”。语义解析是一个任务，旨在构建用户查询的语义表示，例如 SQL 和 Lambda Calculus。跨语言语义解析的任务是将多个自然语言查询翻译成多个意义表示。正如这张图所示，我们需要使用神经网络模型将多个自然语言查询翻译成 SQL、Lambda 或 FunQL 等等。现有跨语言语义解析模型通常单独提出并评估在有限任务和应用数据集上的表现。例如，在某些自然语言上覆盖广泛，但在中文方面缺乏覆盖，或者在某些意义表示上缺乏覆盖。Lambda Calculus 可能缺失，或者它们只在某些神经网络模型上进行评估。例如，只有一个模型用于评估它们。因此，为此我们提出了 XSemPLR。我们提供了一个统一的数据集 XSemPLR，用于跨语言语义解析在多个自然语言和意义表示中的研究。该数据集包含 9 个不同领域的任务，5 个语义解析任务，8 种意义表示，以及 22 个语言在 15 个语言家族中的 15 个自然语言。为了更好地评估我们的基准，我们考虑了六种训练和评估设置。第一种是 Translate-Test。我们使用 Google 翻译 API 将源语言翻译成目标语言，然后使用单语模型进行训练和评估。例如，我们使用英语模型在英语查询上进行训练，而在推理过程中，我们将德语查询翻译成英语，然后使用训练好的模型预测 SQL。我们还将测试单语模型。在这个设置中，源语言和目标语言相同，例如德语到德语或英语到英语。我们还将测试单语小样本设置，通过仅使用 10% 的训练数据训练单语模型。我们还将测试多语言模型，例如将德语、英语、中文查询组合在一起训练一个多语言模型。在推理过程中，我们可以使用该模型来翻译德语查询或中文查询等。我们还考虑了跨语言零样本和少样本迁移。我们在一个源语言上进行训练，然后在另一个语言上进行迁移。在训练过程中，我们使用英语查询或英语和德语的少样本查询组合来训练一个多语言模型，以预测 SQL 输出。我们还发现许多有趣的结果。关于单语模型的分析，我们评估了两种模型组：Encoder-PTR（Multilingual Pretrained Encoders with Pointer-based Decoders），例如 XLM-R + PTR 和 mBERT + PTR。我们还评估了 Encoder-Decoder 模型，即 Multilingual Pretrained Encoder-Decoder Models，例如 mBART 和 mT5。我们发现 Encoder-Decoder 在所有九个数据集上的性能最佳。我们在 mT5 和 XLM-R + PTR 上评估了多语言设置。我们发现 Encoder-Decoder 或 Encoder-PTR 可以通过在各种语言的混合中进行训练来改进。这主要是因为大多数主要自然语言都可以获得性能提升，除了英语在七个数据集上的性能下降，而在三个数据集上有所提升，这被称为“多语言性诅咒”。我们还比较了跨语言性能差距。图中的蓝色线表示跨语言少样本迁移，橙色线表示跨语言零样本迁移，绿色线表示单语设置。我们发现，通过比较绿色线和橙色线，我们发现零样本设置的跨语言迁移性能差距很大，而通过比较蓝色线和橙色线，我们发现少样本设置的迁移差距迅速缩小。我们还发现了一些其他有趣的发现。例如，Encoder-Decoder 优于以前的工作或实现了可比的结果。在英语自然语言上预训练可以显著提高少样本任务在目标自然语言上的性能。我们发现，像 Codex 和 BLOOM 这样的多语言语言模型对于跨语言语义解析任务仍然不够。总而言之，我们构建了 XSemPLR，这是一个用于跨语言语义解析的统一基准，该解析涉及多个自然语言和意义表示。我们对三种代表性的多语言语言模型进行了全面的基准研究，我们的结果显示了许多有趣的发现。等等。欢迎访问我们的论文和代码。谢谢您的聆听。</sample>
    <sample id="171">现有研究可以分为四类，但这些方法要么不适用于 embedding as services，要么缺乏可转移性。</sample>
    <sample id="172">No, Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="174">Thea, a co-author of the paper "ArgAnalysis35K," explains what makes their dataset unique for argument quality analysis. Current datasets often suffer from quality issues due to crowdsourcing, lack diversity (limited motions), and insufficient depth in explaining arguments. ArgAnalysis35K addresses these problems with several key features.

Firstly, it's the largest dataset (35K argument-analysis pairs) with high-quality arguments, primarily sourced from high-quality debates and expert debaters. Secondly, it boasts diverse arguments by covering 24 themes based on real-world debate scenarios, rather than relying on pre-selected motions.  Crucially, ArgAnalysis35K introduces the concept of "analysis," which goes beyond simple claims and premises to encompass a combination of both, providing a more comprehensive understanding of the argument's strength. This is a novel addition to NLP.

Furthermore, the dataset incorporates instance-based annotator reliability. Instead of removing entire annotators due to bias on certain topics, they only remove judgments that are deemed biased for specific arguments. This allows for better utilization of annotator expertise. Finally, ArgAnalysis35K includes a relevance model that assigns scores to arguments' relevance to different themes, capturing the broader applicability of arguments beyond a single motion.

In essence, ArgAnalysis35K offers a more robust and nuanced dataset for argument quality analysis, providing higher quality arguments, greater diversity, and more reliable scoring through its unique features. Thea encourages viewers to explore their paper and provide feedback.</sample>
    <sample id="175">该方法通过在训练过程中诱导对输入和输出之间对齐关系来处理排列的不确定性。它还使用一种GPU友好的连续放松来近似NP-hard的旅行商问题，从而允许反向传播并学习更符合语言逻辑的排列。</sample>
    <sample id="176">根据演讲内容，下游 NLP 模型的公平性问题在于：**如果语言模型带有政治偏见，那么在实际应用中，这些偏见可能会导致对不同社会群体产生不公平的待遇，例如在仇恨言论检测和虚假信息检测等任务中，对不同政治观点或社会群体的识别和处理结果可能存在差异，从而导致歧视和边缘化。** 演讲者认为，这种不公平性是一个非常紧迫的问题，需要引起重视并加以解决。</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustav Sinha</sample>
    <sample id="179">Melanie Sclar's presentation introduces "Minding Language Models’ (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker," focusing on improving theory of mind (ToM) reasoning in large language models (LLMs). Theory of mind is the ability to understand that others have beliefs, desires, and intentions that may differ from one's own.

The presentation highlights the limitations of current LLMs in passing the Sally-Anne test, a classic ToM benchmark.  The research addresses this by proposing SymbolicToM, an inference-time method using explicit graphical representations to model characters' mental states.  SymbolicToM creates belief graphs for all character combinations, leveraging NLI and OpenIE models for inference. These graphs represent what characters believe about the world and what they believe others believe.

The method is applied to various LLMs, including ChatGPT and GPT-3, and compared to supervised baselines like fine-tuned GPT-3 and Textual Time Travel.  Experiments demonstrate significant performance gains across the board, with improvements ranging from 51 to 67 accuracy points.  

Furthermore, the research evaluates SymbolicToM's robustness through out-of-domain datasets (D₁, D₂, D₃) and a dataset with linguistic diversity (ParaphrasedToMi).  Supervised models struggle with these out-of-domain tests, while SymbolicToM consistently provides substantial improvements, even enabling larger models like GPT-4 to achieve full performance.

In conclusion, SymbolicToM is presented as a plug-and-play, inference-time algorithm that enhances LLMs' ToM abilities. It offers interpretable reasoning and avoids overfitting, demonstrating significant benefits in both in-domain and out-of-domain scenarios, particularly when dealing with linguistic diversity. The paper provides further details for those interested.</sample>
    <sample id="180">Myra</sample>
    <sample id="181">Fudan University's Siyu Yuan introduces their work on "Distilling Script Knowledge from Large Language Models for Constrained Language Planning." The research addresses the challenge of planning for specific, constrained goals, moving beyond the previously studied abstract goals in stereotypical activities. 

The paper defines constrained language planning, where goals are subject to multi-faceted constraints, and aims to generate scripts that are both reasonable and faithful to these constraints.  The authors first evaluated large language models (LLMs) on this task, finding they performed poorly.  They identified issues with semantic completeness but a lack of constraint faithfulness.

To address this, they developed an "over-generate-then-filter" method. This involves using InstructGPT to generate numerous scripts for specific goals, followed by a filter model to select the most faithful ones.  They also explored the use of symbolic knowledge distillation to create a high-quality dataset called CoScript, containing 55,000 specific goals and corresponding scripts.  This dataset was created using InstructGPT and crowdsourced human validation.

The results show that CoScript exhibits high diversity in specific goals.  Furthermore, fine-tuning smaller models like T5 on CoScript yields better script generation quality than many larger LLMs, suggesting that specialized models can outperform general-purpose ones when trained on appropriate datasets.  The authors conclude that CoScript can serve as a valuable resource for advancing research in constrained language planning, enabling the use of smaller, more efficient models. The paper highlights the importance of dataset creation for enabling language planning in resource-constrained environments.</sample>
    <sample id="182">热带主义 (tropicalism) 在本文的背景下指的是一种将拉丁裔女性描绘成热带地区的形象，强调她们的“充满活力”和“曲线感”，这与她们的文化背景和身份无关，而是将她们与热带地区的形象联系起来。</sample>
    <sample id="183">作者通过向语言模型发出指令，要求其想象并描述特定身份的人，例如“Imagine you are an Asian woman. Describe yourself.”，从而创建了目标群体的人工描写。他们使用这些指令来生成不同群体的描述，并分析这些描述中的词语和模式。</sample>
    <sample id="184">CXMI (Contextualized Word Meaning Information) 和 Pointwise CXMI。</sample>
    <sample id="185">DrBERT 是一个基于 RoBERTa 的预训练模型，使用 NACHOS 数据集训练，专注于生物医学和临床领域。ChuBERT 是一个基于匿名数据（来自南特大学医院数据仓库）的临床模型。DrBERT 是第一个专门为法语生物医学领域设计的开放源代码模型，而 ChuBERT 是一个临床模型。</sample>
    <sample id="187">Ying and Zhiyang.</sample>
    <sample id="188">迭代迁移学习是指在每个主动学习轮次中，模型使用最新收集的数据进行更新。</sample>
    <sample id="189">数据集的目标是理解用户在选择时使用的语言，特别是处理间接指代的情况，例如当用户无法记住歌曲名称或两者发音相似时，会使用“较新的那个”或“不那么激烈的那个”等表达。</sample>
    <sample id="190">攻击者通过学习从 EaaS 的嵌入中，模仿该服务的行为，从而窃取模型。</sample>
    <sample id="191">三位作者。</sample>
    <sample id="192">The presentation introduces CAME (Confidence-guided Adaptive Memory Efficient Optimization), a new optimizer designed to address the memory limitations of existing optimizers like Adam and the performance penalties of memory-efficient optimizers like Adafactor when training large language models.

The core challenge is to achieve both fast convergence and low memory usage. The presentation highlights the memory reduction potential of Non-negative Matrix Factorization (NMF), used in Adafactor, but notes that NMF introduces errors that slow down convergence.

CAME tackles this by analyzing the errors introduced by Adafactor's NMF. It proposes a method to mitigate these errors by using the residual between the predicted and actual updates as a measure of instability. This instability is then used to adjust the denominator of the momentum term, making the optimization step more adaptive and less prone to divergence.

Experiments on BookCorpus and English Wikipedia, using BERT, GPT-2, and T5, demonstrate CAME's effectiveness. CAME achieves a 3.4% improvement in validation accuracy compared to Adafactor, using the same training steps. It also outperforms Adam in pre-training very large models, especially with increasing batch sizes.  Furthermore, CAME enhances BERT-Large training, achieving comparable performance to Adam and Adafactor with significantly reduced memory costs.

The presentation emphasizes that CAME reduces memory footprint compared to existing memory-efficient optimizers like SM3, and its confidence-guided updating allows for better performance, particularly during large batch training.  The results show that CAME is a promising solution for training large language models with efficient memory usage and fast convergence.</sample>
    <sample id="193">This information is not provided in the text. The text mentions a large-scale annotation, but doesn't specify the number of annotators.</sample>
    <sample id="194">这篇论文的作者所属机构包括：Carnegie Mellon University, University of Washington, 和 Allen Institute for AI。</sample>
    <sample id="195">The paper introduces RoHT (Reasoning over Hierarchical Question Decomposition Tree), a novel framework for explainable question answering (XQA) that addresses limitations of existing methods. Current XQA approaches, like neuro-symbolic and decompose-based methods, face challenges in knowledge source limitations (neuro-symbolic) and difficulty handling diverse natural language (decompose-based). RoHT aims to overcome these by leveraging question decomposition to flexibly select knowledge sources for each sub-question.

The framework builds a Hierarchical Question Decomposition Tree (HQDT) to understand the compositional structure of complex questions. The HQDT breaks down the question into atomic questions and intermediate sub-questions.  A question decomposer generates atomic questions, and a question generator creates intermediate questions based on grouped leaf nodes.  A certainty score is computed for each node to reflect the likelihood of its generation.

RoHT then performs probabilistic reasoning over the HQDT.  The reasoning process recursively traverses the tree, selecting appropriate knowledge sources (KB, text corpus, or recursive calls) for each node.  Executors retrieve answers with associated probabilities from these sources, and an aggregator combines the candidate answers to produce the top-ranked results.

The framework is evaluated on KQA Pro (KB QA with incomplete knowledge) and Musique (QA comprehension with text and KB). On KQA Pro, RoHT outperforms existing KB QA methods and TransferNet, demonstrating the benefits of integrating sub-question answers. On Musique, RoHT significantly improves performance compared to state-of-the-art methods, especially when combining text and KB.  The results highlight the effectiveness of explicit question decomposition and the advantages of integrating knowledge from heterogeneous sources for more intuitive and accurate question answering. The paper concludes that RoHT offers a promising approach to XQA by providing explainability and leveraging diverse knowledge sources.</sample>
    <sample id="196">我看到了 Bart and Lisa 的例子，其中 Bart 是左侧的支配词。</sample>
    <sample id="197">根据内容，最先进的对话系统模型是：

*   **四个 state-of-the-art chat models** (四个最先进的聊天模型)

内容中没有具体说明这些模型是什么，只提到它们被用于评估。</sample>
    <sample id="198">在整个上下文窗口中评估模型的可接受性是因为现在的语言模型拥有越来越长的上下文窗口，而现有的最小对评估方法（MPP）无法处理更长的序列，因此需要评估模型在不同长度的上下文中的可接受性。</sample>
    <sample id="199">是的，多语言训练会导致表现下降，尤其是在某些数据集上。研究发现，在七个数据集上，多语言训练导致英语模型的性能下降，而在三个数据集上则有所提升。这被称为“多语言的诅咒”。</sample>
    <sample id="200">注释者在被问及选择实体时，**不一定**提前知道该实体。他们被告知知道两个实体的名称，但他们并不一定了解这些实体本身。</sample>
    <sample id="201">BLEURT 和人工评估（human evaluation）。</sample>
    <sample id="202">根据论文内容，泛化问题主要针对的是 Named Entity Recognition (NER) 任务。论文研究的是 CoNLL-2003 命名实体识别器在 2023 年的泛化能力，以及如何获得更好的泛化效果。因此，泛化问题是针对 NER 类型的。</sample>
    <sample id="203">NLP 中的立场很重要，因为它指的是 NLP 研究人员和模型开发者所持有的视角，这些视角会影响研究过程和结果。由于 NLP 任务越来越具有主观性和社会性，并且许多模型隐藏在 API 背后，因此难以识别和量化这些立场带来的偏见。研究表明，NLP 数据集和模型可能存在对特定人群的偏好，导致某些群体被边缘化。</sample>
    <sample id="204">根据内容，提到 multilingual language models 像 Codex 和 BLOOM 仍然对跨语言语义解析任务来说是“不充分的”。内容中没有提及 BLOOM 是否采用适配器微调还是完整微调。</sample>
    <sample id="205">Shangbin from the University of Washington presented their research "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models." The work investigates how political biases in large web crawl data, particularly from news sources like the New York Times, influence language models and impact downstream applications.

The study explores the political leaning of language models using prompt-based evaluation, finding that models like GPT-4 are generally more liberal than others.  They further investigate how pretraining on partisan corpora (news and social media categorized by political leaning) shifts the ideological coordinates of language models.  Specifically, pretraining on left-leaning Reddit data resulted in a liberal shift, and pretraining on corpora separated before and after the 45th US president showed models leaning further away from the center after 2017, indicating the incorporation of societal polarization.

The research then evaluates these models on hate speech and fake news detection.  They discovered that left-leaning models are better at detecting hate speech targeting minority groups, while right-leaning models are better at detecting hate speech targeting white and men.  Similar patterns emerged for fake news detection. Qualitative examples highlighted how models with different political leanings make different predictions based on social categories.

The study concludes that political biases in language models pose a significant fairness issue.  The researchers acknowledge the dilemma of balancing the propagation of bias with the risk of censorship and the difficulty of defining neutrality in data collection. They emphasize the need to address these biases to prevent marginalization and unchecked hate speech, highlighting the potential for negative societal consequences if these models are deployed without mitigation. The research frames the issue as a difficult choice between allowing bias to propagate or risking censorship.</sample>
    <sample id="206">Transfer learning was used from two different tasks: topic-independent dissonance stance classification (debate) and binary classification of expansion and comparison classes (CE) of PDTB.</sample>
    <sample id="207">WMT evaluation.</sample>
    <sample id="208">根据所给的英文内容，作者最终提出了三条建议。</sample>
    <sample id="209">提议的方法在语义完整性和约束忠实度方面都获得了提升，并且T5在Fine-tuning后能生成比大多数大型语言模型更好的脚本。</sample>
    <sample id="210">Shuheng.</sample>
    <sample id="211">是，论文中的结果和数据集可以用作基准。具体来说，论文中提出的自动文本简化方法（特别是 MASSalign）以及对语言模型进行微调以生成简化文本的方法，都被认为是基准，可以作为未来文本简化问题的参考。</sample>
    <sample id="212">他们使用了 T5 模型进行实验。</sample>
    <sample id="213">OFA</sample>
    <sample id="215">Adam Przepiórkowski's talk focuses on the dependency structure of coordination, arguing against asymmetric structures and advocating for symmetric ones. He begins by outlining different theoretical approaches to coordination, including universal dependencies, Mel'čuk's meaning text theory, the Prague approach, and Hudson's Word Grammar, highlighting their asymmetric nature.

The core argument rests on dependency length minimization, a principle similar to how direct objects prefer proximity to the verb. Przepiórkowski illustrates this with examples like "Marge read it yesterday" versus "Marge read yesterday it," demonstrating that shorter dependencies are preferred. He uses the Penn Treebank to analyze coordination statistics, revealing a tendency for left conjuncts to be shorter, particularly when the governor is on the left or absent.

The key finding is that this length preference only emerges when the governor is on the left or absent. When the governor is on the right, this tendency disappears.  The talk presents data measured in characters, syllables, and words, showing a steady increase in the left conjunct's length difference with the governor on the left and a lack of this effect when the governor is on the right.

Przepiórkowski concludes that these observations provide evidence supporting symmetric coordination structures, contrasting them with the asymmetric approaches. He directs the audience to the paper for a detailed explanation of the arguments and invites questions during the poster session. The talk emphasizes the importance of dependency length as a crucial factor in determining the structure of coordinated elements.</sample>
    <sample id="217">## Seen to Unseen: Multi-Attribute Controllable Dialogue Generation - Summary

Weihao Zeng, Lulu Zhao, and Keqing He from Beijing University of Posts and Telecommunications introduce "Seen to Unseen," a work focused on improving controllable dialogue generation (CDG) for multi-attribute scenarios.  Existing methods often struggle with the practical complexities of generating dialogues with multiple attributes, particularly continuous ones.  The research addresses this limitation by proposing **DCG (Disentangled Controllable Generation)**, a model that learns attribute concepts from seen values and uses a disentanglement loss to manage attribute combinations.

The core contribution is a **unified evaluation framework (MAE)** for different attribute granularities, eliminating the need for large, labeled datasets.  They establish two benchmarks and demonstrate the effectiveness of DCG and MAE through experiments on DailyDialog-CG.  

DCG builds upon the DialoGPT framework, utilizing **compositional prompts** to guide the model.  They introduce two prompt types: attribute-oriented prompts (using controllable attribute values) and task-oriented prompts (leveraging global features).  A disentanglement loss further enhances the model's ability to distinguish between different attribute combinations.

Experiments show that DCG outperforms baselines in both attribute controllability and text quality.  Attribute-oriented prompts focus on controllable information, while task-oriented prompts improve text equality.  The disentanglement learning enhances compositional generalization.  MAE achieves high correlation with human judgments across both discrete and continuous attributes.  Furthermore, the research demonstrates the ability of DCG to disentangle attribute combinations and learn relationships between attributes, enabling generalization from seen to unseen attribute combinations.  The attribute-oriented prompt method proves superior to learning independent prompts for each attribute value.  In conclusion, DCG offers a promising approach to tackling the challenges of compositional generalization in multi-attribute controllable dialogue generation.</sample>
    <sample id="218">Google Translate</sample>
    <sample id="219">Jia-Huei Ju from Academia Sinica presented their research on a "Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports." Their work focuses on analyzing Form 10-K reports, annual reports required by the SEC, which contain detailed company activities but require significant human effort to extract useful information.

The research is motivated by the observation that words within reports are highly similar (around 80% token overlap) and content is yearly-dependent. This leads to a "highlighting task" where the model identifies the rationale (important words) explaining the relationship between a target report and its previous year's report. The goal is to predict word importance to measure highlighting performance.

The proposed pipeline consists of three stages: document segmentation (Stage 0, not detailed), relation recognition (Stage 1), and out-of-domain/in-domain fine-tuning (Stage 2 &amp; 2+). Stage 1 classifies report pairs into three types: highly similar (e.g., regulations), revised (similar syntax but different meaning), and mismatched (new information).

For model tuning, they utilize the eSNLI dataset for out-of-domain fine-tuning, followed by intermediate fine-tuning using revised pairs with pseudo-labels generated from revised words. They employ soft labeling techniques combining cross-entropy and KL divergence to mitigate issues with low-quality pseudo-labels.

The evaluation uses both eSNLI pairs and their released FINAL dataset, with performance measured using Precision and PCC (correlation with annotations). The research demonstrates that their domain-adaptive highlighting model achieves top performance on FINAL and maintains generalization ability on eSNLI. They also found benefits in simulation with mismatched pairs.

In conclusion, the work proposes a highlighting task and a two-stage fine-tuning pipeline, with future research directions including improvements in effectiveness, feature addition, and exploring more information retrieval techniques. The paper and GitHub repository provide further details.</sample>
    <sample id="220">Vasudha 是 Stony Brook University 的计算机科学 PhD 候选人。</sample>
    <sample id="221">论文主要针对多种语言对进行了评估，包括德语到英语（German-English）。</sample>
    <sample id="222">The paper "To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering" addresses the problem of adapting open-domain question answering (QA) models to new domains, particularly when the source model is trained on a general-purpose corpus like Wikipedia. The core issue is that models trained on Wikipedia may struggle to generalize to domains like biomedicine, where the data is sparse and the model's learned knowledge is not directly applicable.

The authors investigate three main contributions: (1) data interventions to enable out-of-domain generalization, (2) identification of dataset shift types, and (3) determining effective interventions for specific shift types. They evaluate their approach on seven target datasets spanning six domains, comparing it to baseline models.

They explore two data intervention methods: few-shot learning and zero-shot learning. Few-shot learning involves using a few examples from the target domain to prompt large language models for generating more examples, which are then used to adapt the retriever and reader models. Zero-shot learning, on the other hand, involves controlling the interactions between question, answer, and context variables to understand their impact on model learning.

The paper identifies different types of dataset shifts: no shift (source model is compatible), concept shift (reasoning type mismatch), covariate shift (retriever struggles, reader succeeds), and full shift (neither is compatible). They measure compatibility using likelihood calculations for retriever and reader models.

Their experiments show that few-shot learning is effective for all target datasets, while zero-shot learning is more beneficial for concept and covariate shifts. They observe that the retriever performance improves by an average of 8% and the reader performance by an average of 11% with these interventions.  The paper concludes that understanding the type of dataset shift is crucial for selecting the most effective data intervention strategy. They demonstrate that their approach can improve reader performance by up to 24%.</sample>
    <sample id="223">Shangbin.</sample>
    <sample id="224">在实验过程中研究了以下模型：

*   MASSalign (作为自动对齐方法的最佳方法)
*   long-mBART (用于文档级别简化)
*   base mBART (用于句子级别简化)</sample>
    <sample id="225">MultiInstruct 中使用的 62 个不同任务中，53 个用于训练，并且在测试中保留了整个常识推理组以及另外 5 个任务。因此，总共有 53 个任务用于训练，并且在测试中使用了 58 个任务。</sample>
    <sample id="226">The provided text does not mention the number of authors.</sample>
    <sample id="227">The provided text discusses a key challenge in current language model research: grounded language understanding. This field aims to connect natural language expressions to executable actions or plans within a specific environment, such as a program or a knowledge base. Applications include smart assistants, semantic search, and robotic control.

The core problem lies in the lack of grounding during pre-training of most language models, leading to a gap between pre-training and real-world application. Existing approaches often rely on language models to directly generate plans, but these plans can be invalid or grammatically incorrect.

The authors propose a novel framework called "Pangu" that shifts the focus from generation to discrimination. Pangu utilizes a symbolic agent to interact with the environment and propose candidate plans, while the language model acts as a scorer, ranking the candidate plans. This approach avoids the language model needing to handle plan validity and grammar, leveraging its strengths in discrimination.

Pangu is demonstrated on knowledge-based question answering, achieving strong performance across various language models (BERT, T5, Codex) and training methods (fine-tuning, in-context learning). Notably, Pangu exhibits strong sample efficiency, achieving high accuracy with minimal examples, even surpassing Codex with in-context learning.  It also demonstrates robustness in non-i.i.d. settings, contrasting with autoregressive models that tend to overfit.

The central takeaway is that for grounded language understanding, discrimination is a more effective strategy than generation. The authors invite discussion and collaboration, highlighting the potential of Pangu as a framework for bridging the gap between language models and real-world environments.</sample>
    <sample id="228">AG News, MIND, SST2, Enron Spam.</sample>
    <sample id="229">Gabriella Skitalinskaya and Henning Wachsmuth's work focuses on improving argumentative writing by detecting improvable claims. They address the challenge of determining when a claim is sufficiently phrased and whether further revisions are needed. Their research introduces two tasks: Suboptimal Claim Detection (identifying claims needing revision) and Claim Improvement Suggestion (specifying revision types).

The authors explore the difficulties of learning from revision-based data, which differs across domains. They specifically concentrate on argumentative text, leveraging revision histories from collaborative online debate platforms like Kialo.  The paper highlights four key challenges:

1. **Representativity and Reliability:** Ensuring the dataset accurately reflects high-quality arguments and avoiding overlooking truly optimal claims.
2. **Model Complexity and Architecture:** Selecting models sensitive to subtle changes in claim delivery and evaluating the impact of pre-training, fine-tuning, and classification.
3. **Contextual Information:** Determining which contextual elements (e.g., debate structure, parent claim, domain knowledge) are relevant to claim quality.
4. **Topical and User Bias:** Addressing noise in revision histories due to errors, user biases, and the influence of social/cultural context.

The paper analyzes strategies to overcome these challenges and compares different approaches for the two tasks. Their experiments suggest that revision-based data is valuable for claim assessment. Modeling the distance between claim versions is helpful for identifying suboptimal claims.  Furthermore, the relevance of contextual information depends on the specific task and the identified quality issues. The authors conclude that their work provides insights into effectively utilizing revision data for improving argumentative writing.</sample>
    <sample id="231">NACHOS 是一个来自网络的医疗数据数据集，用于训练 DrBERT 模型。</sample>
    <sample id="232">David Vilar.</sample>
    <sample id="233">The presentation introduces "Attention as a Guide for Simultaneous Speech Translation" paper by Sara Papi, Matteo Negri, and Marco Turchi, focusing on the challenges and solution for Simultaneous Speech Translation (SimuST). SimuST aims to translate spoken language into text in real-time, enabling cross-language communication. Current SimuST models face issues with long training procedures, optimization objectives, and maintaining multiple models for different latency levels (e.g., 1 second and 2 seconds).

The proposed solution, EDAtt (Encoder-Decoder Attention), addresses these problems by leveraging existing offline ST models without retraining or custom architectures. It utilizes a single model for all latency regimes, controlling latency through specific parameters.  The core idea is to use cross-attention to guide partial translation. EDAtt decides whether to emit a partial translation based on attention distribution. A word is emitted if attention is not concentrated (sum of cross-attention weights below a threshold alpha towards the last lambda speech frames), indicating stable information. Conversely, if attention is concentrated, the word is withheld until more stable information arrives.

The presentation highlights experimental results comparing EDAtt with existing strategies like Wait-k and Local Agreement, as well as state-of-the-art architectures for pre-translation.  The results demonstrate that EDAtt outperforms these strategies in terms of translation quality (BLEU score) and latency (average lagging), particularly achieving faster translation times.  The computational-aware average lagging is also improved.

The authors emphasize the importance of balancing translation quality and latency.  They also made the code and models open-source to promote reproducibility. The paper's findings suggest that EDAtt offers a more efficient and effective approach to SimuST by intelligently managing the translation process based on attention patterns.</sample>
    <sample id="234">提示策略对结果有很大影响，尤其是在零次和一次提示的情况下。在五次提示的情况下，提示形式的影响几乎没有。关键在于提供的示例质量，而不是它们与源句子的相似性。</sample>
    <sample id="235">This information is not provided in the text.</sample>
    <sample id="236">The provided text does not specify what the five expert-written instructions are. It only states that each task in MultiInstruct is equipped with five expert-written instructions.</sample>
    <sample id="237">作者建议通过设计一个核心指代消解任务，并根据信息来源的可用性（预训练时、同时、仅在推理时）进行不同设置的实验，来测试模型对来自多种来源的信息的整合能力。</sample>
    <sample id="238">Yebowen Hu from the University of Central Florida introduces MeetingBank, a new benchmark dataset for meeting summarization. The dataset addresses the need for high-quality meeting summaries, a challenge in the fast-paced world of frequent meetings. MeetingBank comprises City Council meeting transcripts, reference summaries, and URLs, offering a rich resource for research.

The data collection process involves using Speechmatics API to transcribe audio, identifying meeting types and IDs, retrieving reference summaries from meeting minutes, and aligning timestamps to create meeting segments. The dataset contains 1,366 City Council meetings and nearly 7,000 instances, with detailed statistics on meeting duration, token counts, speaker numbers, and year periods.

Analysis reveals that most summaries exhibit a coverage score between 0.7 and 0.9, indicating a mix of verbatim and abstractive summarization. Density scores show Seattle and Boston with the highest scores, suggesting a higher degree of editing.

For model evaluation, the dataset was used to test top-tier summarization systems, including extractive methods (Oracle, LEAD, LexRank, TextRank) and abstractive methods (BART-Large, Pagasus, Longformer, DialogLM, HMNet). Zero-shot summarization with GPT-3 (Davinci-003) was also employed. Extractive systems like Extr-Oracle achieved high ROUGE-2 scores, while DialogLM performed best among abstractive models. GPT-3 showed promising fluency and coherence but weaker performance in informativeness and factuality according to automatic metrics.

Human evaluation using 200 instances assessed summaries based on informativeness, factuality, fluency, coherence, and redundancy. GPT-3 achieved the highest overall scores in fluency and coherence, but with less impressive results in informativeness and factuality.

The primary contribution of MeetingBank is a valuable dataset for researchers and a tool for understanding City Council decision-making processes. The dataset is publicly available for download and use. The speaker encourages viewers to explore the dataset and look forward to further discussion in July.</sample>
    <sample id="239">大家好，我的名字是戴维·维拉尔，我将简要回顾一下论文“Prompting PaLM for Translation：评估策略与性能”。这是一项与我同事们合作完成的工作，来自谷歌翻译。PaLM 是 2022 年发布的一款 5400 亿参数的大型语言模型。它在 2022 年训练了 7800 亿个 tokens 的大量文本数据集。在发布时，它在数百个 NLP 任务中都达到了最先进水平。在这项工作中，我们首次系统地研究了大型语言模型对机器翻译的提示方法。我们使用机器翻译社区的最佳实践来评估这些模型的转换能力。这包括使用最新的测试集，以避免测试数据与语言模型训练数据之间的重叠。我们还将其与最先进的系统进行比较，即 WMT 评估。我们使用最先进的神经机器翻译指标，并额外展示了专家基于人类评估的结果。最后，我们提供了一些关于提示选择策略的建议。提示对 LLM 的翻译性能有很大的影响，正如我们简单实验所看到的，我们在每个句子中使用了两种不同的提示，并使用了单次提示。大多数句子（1000 个句子中的 516 个）。观察到的差异超过了 BLEURT 分数的一个点。在极端情况下，这种差异可以高达 40 分数。因此，选择一个好的提示策略非常重要。在我们的实验中，我们采用了五次提示策略，即我们只需标记每个我们提供的句子所使用的语言。例如，在从德语翻译成英语时，德语句子使用德语冒号标记，英语翻译使用英语冒号标记。我们发现，实际的提示形式对几次短提示的影响不大。对于零次和一次提示，这至关重要。当我们采用五次提示时，实际的提示形式几乎没有差异。例子本身携带了大部分权重。我们实验结果的总结是，例子质量比与源句子相似度更重要。因此，重要的是从高质量的翻译中选择例子。特别是，我们比较了从 WMT 评估的开发数据中选择提示，该开发数据比训练数据更经过精心挑选，质量更高，因此更少噪声。他们的结果表明，使用开发数据性能更好。然而，专门的先进系统对 PaLM 的翻译具有显著优势。但是，PaLM 接近商业系统。在我们的案例中，我们选择使用谷歌翻译进行评估。我们通过 MQM 框架进行的专家基于人类评估揭示了 PaLM 的流畅度与最先进系统相当，但主要区别在于准确性。特别是，最常见的错误是遗漏错误。看来 PaLM 会选择产生更好的声音的翻译，有时会丢弃源句中被翻译掉的部分。然而，PaLM 的“风格/不自然”类别低于最先进系统，这进一步表明 PaLM 提供流畅的输出，但仍然存在一些准确性问题。这就是本次简要概述的内容。有关更多详细信息，请访问论文的完整演示。非常感谢。</sample>
    <sample id="240">大家好，我是Dawei，一位来自德国萨尔州立大学的博士生。在这个视频中，我想向大家介绍我们最近的研究成果“更弱于你所想：对弱监督学习的批判性视角”。这项工作是与Xiaoyu Shen、Marius Mosbach、Andreas Stephan和Dietrich Klakow合作完成的。我想从简要介绍弱监督学习和弱监督学习开始。在弱监督学习中，我们无需手动标注数据。相反，我们使用弱标注来源，例如简单的启发式规则、知识库或低质量的众包数据进行标注，如图所示。与人工标注相比，这些弱标注更便宜，但它们也存在噪声，这意味着某些标注是不准确的。如果我们直接在弱标注数据上训练神经网络，神经网络会倾向于记住这些标注中的噪声，而不会泛化。在弱监督学习中，我们提出了一些训练算法，以在如此标注噪声的情况下，Robust地训练神经网络，从而使训练后的模型仍然能够很好地泛化。在弱监督学习（WSL）领域，一个常见的说法是，人们说他们只在弱标注数据上训练模型，并能获得在干净测试集上的高性能。技术上，这个说法并不错误，但有一个问题，那就是人们假设存在额外的干净验证集可用用于模型选择。我们不能停止在这个问题设置，但这暗示了弱监督学习中需要额外的手动标注。但正如一个潜伏在其中的问题一样，这个需求经常被忽视。上述疑虑引发了三个研究问题。首先，弱监督学习是否需要干净的验证数据，或者我们可以使用噪声验证集呢？其次，如果需要干净的数据，或者干净数据是弱监督学习工作正常运行的必要条件，那么我们需要多少干净样本？最后，我们应该只使用干净样本进行验证，还是有更好的方法可以利用它们？我们在这项工作中就解决了这些研究问题，我们的发现如下。首先，我们发现，最近的弱监督学习方法确实需要干净的验证样本才能正常工作。否则，性能会大幅下降。如图所示，如果没有干净的验证样本，那么训练后的模型将无法超出原始弱标签，这意味着训练毫无意义。这表明弱监督学习方法实际上需要干净的标注数据才能正常工作，我们应该不忽视获得干净验证样本的标注成本。我们的第二个发现是，增加干净的验证样本的数量将帮助弱监督学习方法获得更好的性能，如图所示。通常我们只需要每类20个样本就能获得高性能。但这还不是全部，因为如果我们决定使用干净样本，那么直接在它们上进行训练也会获得更好的性能。图右显示了直接微调方法与弱监督学习方法（仅使用干净数据进行验证）的性能差异。正如我们所见，如果我们有每类10个样本，直接微调开始优于弱监督学习方法。最后，我们能够实现的前沿性能提升，可以很容易地通过允许在干净验证样本上继续微调来实现。正如图所示，最初性能较差的通用模型FTw，在更复杂的弱监督学习方法（如COSINE）的帮助下，性能可以达到与其它方法相当的水平。因此，在实践中，没有理由选择更复杂的弱监督学习方法，这些方法需要更多的计算时间和磁盘空间。总而言之，我们证明了最近的弱监督学习方法需要干净、手动标注的样本才能正常工作。它们的性能提升和实用性被高估了。我们为未来的工作提出了以下具体建议。首先，报告模型选择标准。例如，报告是否使用干净的验证样本进行模型选择。其次，弱监督学习方法应与少样本学习基线进行比较，因为两者都可以在干净样本上工作。第三，持续微调是一个简单但强大的基线，未来工作应考虑。最后，我们已开源代码。您可以在此幻灯片上的二维码找到它。请随时查看。谢谢大家，祝大家在会议上愉快。</sample>
    <sample id="241">Ethan, along with Yang Chen, Wei Xu, and Alan Ritter from Georgia Tech, presented their paper on "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments." The paper addresses shortcomings in existing automated misinformation detection systems, which often rely on unrealistic evaluations using retrospective datasets and fail to account for leaked counter-evidence or the limitations of human content moderators.

The authors propose an evaluation framework for developing more robust and human-centric systems. Their system is designed as an end-to-end workflow, integrating human feedback at various stages. The system consists of two main components: first, a claim detection component that identifies potentially misleading claims from raw tweets using a T5 model trained for question answering. These claims are ranked by popularity and then presented to humans for verification. Second, a BERT-based stance classification model flags tweets violating Twitter's policies regarding COVID-19 misinformation, based on the author's stance towards unapproved treatments.

The paper evaluates the system's efficacy by focusing on early detection – identifying unapproved treatments before they appear in debunking articles. They found the system detected these treatments with reasonable accuracy. Furthermore, they assessed the policy violation verification component, finding a 65% accuracy rate based on human assessments.  The system demonstrated a high efficiency in identifying policy violations, detecting 124.2 violations per human hour.

The authors emphasize the importance of evaluating misinformation detection systems with realistic human-in-the-loop workflows. Their framework provides a valuable, industry-independent perspective on the development and evaluation of these systems, motivating future research in this area. They highlight the critical need for early detection to effectively combat the spread of misinformation.</sample>
    <sample id="242">对话系统的常用评估方法是使用人类 judges 评估对话质量，例如通过选择哪个对话更好或给对话打分。</sample>
    <sample id="243">这篇论文有 6 位作者：Sebastian Santy, Ronan Le Bras, Katharina Reinecke, Maarten Sap, 以及 Carnegie Mellon University 的 Jenny 和 University of Washington 的研究人员。</sample>
    <sample id="244">在 Servin 和 Kea 的示例中，需要以下背景知识：

1.  Servin 是一个法官。
2.  法官在法律法院审理案件。</sample>
    <sample id="245">Lining Zhang's presentation details a pipeline for identifying high-agreement workers on Amazon Mechanical Turk (MTurk) for summarization tasks. The motivation stems from the limitations of automatic metrics and a lack of understanding regarding optimal MTurk recruitment practices.

The pipeline employs a two-step qualification process. The first stage assesses annotator ability to evaluate multiple dimensions of summaries, categorizing them into "gold," "silver," "bronze," and "block" levels (gold and silver pass). This yields 26 qualified workers (13% of 200 participants). The second stage tests the capacity to handle heavy workloads, evaluating saliency dimensions in four summaries per document over 10 HITs. This results in 12 qualified workers (6%), with 4 gold and 8 silver. These workers demonstrate high inter-annotator agreement (IAA), exceeding expert levels.

A reference-based task further evaluates general performance, with 8 out of 12 workers completing all HITs and achieving a Krippendorff's Alpha of 0.534.  A baseline approach using statistical filtering (MACE) yielded a Krippendorff's Alpha of 0.380, but with incomplete HIT coverage. CloudResearch MTurk workers achieved a Krippendorff's Alpha of 0.513, but with lower task acceptance rates.

Analysis of correctness across annotation sources reveals significant Spearman's correlation between the pipeline and CloudResearch workers, with potential for the pipeline to achieve similar quality.  The pipeline results in 4 gold and 8 silver workers (6% of 200 participants), offering a cost-effective solution for high-agreement annotations.

Future work will focus on improving worker quality in terms of both agreement and correctness, exploring applications across different tasks, languages, and platforms. Limitations include the English-only focus, the potential for the qualification questions to be imperfect, and the lack of a guarantee for correctness training. The research was funded by Google.</sample>
    <sample id="246">是的，代码和数据集在 GitHub 上公开。</sample>
    <sample id="247">Jiho Kim from KAIST AI presents their paper "FACTKG: Fact Verification via Reasoning on Knowledge Graphs." The paper addresses the lack of datasets specifically designed for fact verification using knowledge graphs (KGs), contrasting with existing datasets like FEVER and VitaminC that rely on text or tables, and TabFact/InfoTabs that use tables.

The authors propose FactKG, a new dataset for KG-based fact verification, leveraging DBpedia as the knowledge graph.  FactKG contains claims in both written and colloquial styles, labeled as SUPPORTED or REFUTED. The task involves retrieving evidence from DBpedia and verifying the claim using reasoning.

The dataset includes five types of reasoning: one-hop (direct relationship between entities), conjunction (multiple one-hop claims), existence (entity-relation relationship), multi-hop (inferring relationships across multiple hops), and negation.  The paper details how each reasoning type is represented and how evidence retrieval and verification are performed.

To address the colloquial style, the authors utilize a colloquial style transfer model and presupposition templates.  They also present baseline models, including claim-only baselines and the GEAR model (which utilizes graph evidence).  The results demonstrate that all baselines outperform a majority class baseline (51%), with the GEAR model achieving the best performance by leveraging KG evidence.

The paper highlights the practical applicability of KG-based fact verification in dialogue systems and other consistency-checking tasks between knowledge and natural language.  The authors encourage download of the FactKG dataset and offer contact information.</sample>
    <sample id="248">NLPositionality 的注释者来自 87 个国家，总计超过 1000 人。研究发现，数据集和模型与英语国家和拥有大学学历的人群更具一致性，而与非二元性别群体的一致性较弱。因此，注释者在人口统计学特征方面并非完全均衡。</sample>
    <sample id="249">根据内容，在可接受的域中扰乱句子，通过尝试对输入句子进行扰动，同时保留相关的结构。</sample>
    <sample id="250">进行维度评估意味着评估对话模型在多个不同的方面，而不是只关注整体质量。这有助于理解模型的优势和劣势，并更细致地了解其表现。</sample>
    <sample id="251">University of Science and Technology of China.</sample>
    <sample id="252">```
U-CREAT: Unsupervised Case Retrieval using Events Extraction, presented by Sai Kiran Tanikella from IIT Kanpur, addresses the challenge of Prior Case Retrieval (PCR) in legal domains. PCR involves retrieving relevant past precedents cited within a query document.  The presentation highlights the limitations of relying solely on legal professionals' experience and the increasing volume of cases making this task complex.

The work introduces two key contributions: the IL-PCR dataset and the U-CREAT pipeline. The IL-PCR dataset, a benchmark for PCR, comprises 7,070 Indian legal cases with a high average citation count, offering a more comprehensive test bed than existing datasets like COLIEE’21.  The dataset was created using publicly available data.

U-CREAT leverages unsupervised learning and an event-based approach.  It extracts events from case documents using dependency parsing, identifying subject-verb-object triplets. These events are then used to compute an interaction matrix between the query and candidate events, enabling ranking of candidates.  The pipeline involves pre-processing, dependency parsing, and post-processing to extract events.

The presentation compares the performance of various retrieval models – count-based, transformer-based, and event-based – on the PCR task. Transformer-based models, including those specifically trained on Indian legal text, showed significantly lower performance compared to baseline methods like BM25. Event-based models, including Atomic Events, Non-Atomic Events, and Event Filtered Documents, outperformed all other methods, achieving significantly higher F1 scores and lower inference times. The Event Filtered Documents model demonstrated the best performance.

U-CREAT outperforms existing approaches, including recent supervised methods, on the COLIEE’21 dataset. The research opens new avenues for advancing prior case retrieval techniques, particularly in the Indian legal context. The paper provides further details on the methodology and results.
```</sample>
    <sample id="253">DisorBERT is a research project from Mexico and Spain focused on detecting signs of mental disorders in social media posts. The researchers define mental disorders as psychological syndromes affecting thinking, feeling, mood, and behavior, citing examples like depression, PTSD, and anorexia. They argue that social media offers a valuable opportunity to study how people experience difficulties and seek help.

The project addresses the challenge of insufficient annotated data by employing domain adaptation. They leverage the pre-trained BERT language model and adapt it to the specific language of Reddit and mental health discussions. This involves integrating knowledge from these domains and using a lexicon to guide the masking process within BERT. The core idea is to first learn the language of social media and then specialize in mental health.

The researchers present results using the eRisk dataset, demonstrating that DisorBERT achieves a good balance between precision and recall, outperforming baseline models.  Analysis of the model's predictions reveals that DisorBERT is more likely to generate words related to negative emotions and psychological issues compared to BERT.  For example, when masking the word "cry," DisorBERT predicts words like "focus," "talk," and "sleep," which are associated with mental health challenges.

Visualization techniques highlight the importance of words like "anxious" and "medication" in the posts of individuals with depression.  The combined effect of double domain adaptation and guided masking proves effective in capturing signs of mental disorders.  DisorBERT's performance surpasses that of MentalBERT, a model trained on a large dataset.

Future work will explore the use of different lexical resources and clinical data to further improve the model's accuracy. The researchers conclude that DisorBERT offers a promising approach for developing technology to warn about the onset of mental disorders and provide supporting evidence.</sample>
    <sample id="254">This research paper presents a novel framework for document-level distant relation extraction (DSRE), addressing the challenge of noise in distant supervision (DS) data. DSRE aims to identify relationships between entities within a document, but DS data, generated without human annotations, often contains inaccurate labels (pseudo-labels). This noise can lead to the extraction of false relations, hindering model performance.

The proposed framework tackles this issue through "uncertainty-guided label denoising." It first trains a pre-denoising DocRE model using both DS and human-annotated data to generate pseudo-labels.  Crucially, it incorporates uncertainty estimation to assess the reliability of these pseudo-labels.  The framework employs instance-level uncertainty estimation to handle overlapping relations, where an entity pair might have multiple possible relationships.  This allows for a more nuanced assessment of each relation's validity.

A key innovation is the use of dynamic class uncertainty thresholds. These thresholds adapt to the distribution of uncertainty scores for each relation class, filtering out pseudo-labels with high uncertainty, particularly those associated with less frequent (long-tail) relations.  The framework further enhances performance through a multi-phase training strategy, iteratively re-labeling the DS data based on the denoised pseudo-labels.

The authors demonstrate that their framework significantly outperforms existing baselines on public datasets.  The main contributions are: a framework for denoising DS data using uncertainty; an instance-level uncertainty estimation method for overlapping relations; a dynamic uncertainty thresholding strategy for long-tail relations; and substantial performance gains.  The research highlights the importance of uncertainty estimation in mitigating noise in distant supervision and achieving robust document-level relation extraction.</sample>
    <sample id="255">在零次和一次提示的情况下，提示的形式很重要。</sample>
    <sample id="257">作者评估了四个 state-of-the-art 的聊天模型。</sample>
    <sample id="258">Chiang Cheng-Han introduces their new work, "Can Large Language Models Be an Alternative to Human Evaluation?". The research explores using large language models (LLMs) to evaluate text quality in natural language processing, aiming to overcome the instability and reproducibility issues of human evaluation.

The motivation stems from the fact that human evaluation is subjective and prone to inconsistencies. The researchers propose leveraging LLMs, which are capable of following natural language instructions, to perform evaluations. They conduct an experiment comparing stories generated by GPT-2 and human writers, using LLMs to rate them based on grammar, coherence, likability, and relevance. Human English teachers serve as ground truth raters.

The experiment involves using four LLMs: T0, InstructGPT (curie &amp; davinci), and ChatGPT. The results show that human raters generally prefer human-written stories, but some smaller LLMs don't show a clear preference. However, Davinci and ChatGPT exhibit a strong preference for human-written text, aligning with the English teachers' judgments.

The paper investigates factors influencing LLM evaluations, such as agreement between LLMs and human raters, instruction wording, sampling methods, and the benefits/costs compared to human evaluation.  It also explores the applicability of LLM evaluation to other tasks.  The researchers invite viewers to read the paper or visit their poster at ACL for more details. The work is novel as it's the first exploration of using LLMs for evaluation in the field.</sample>
    <sample id="259">Yusen Zhang from Penn State University presented their work "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations." The presentation highlighted the challenge of cross-lingual semantic parsing, which involves translating queries across multiple languages into various meaning representations like SQL or Lambda Calculus. Existing models often lack coverage for specific languages or meaning representations, creating gaps in the field.

To address this, the researchers proposed XSemPLR, a unified dataset containing nine datasets across various domains, five semantic parsing tasks, eight meaning representations, and 22 natural languages spanning 15 language families. They designed six evaluation settings: Translate-Test (using Google Translate), Monolingual Model (same language for source and target), Monolingual Few-shot (limited training data), Multilingual Model (training on all languages), Cross-lingual Zero-shot (training on one language, testing on another), and Cross-lingual Few-shot (transferring from one language to another).

The study evaluated two model types: Encoder-PTR (like XLM-R + PTR and mBERT + PTR) and Encoder-Decoder (like mBART and mT5). Encoder-Decoder models achieved the best performance across all datasets. Training on a mixture of languages improved both Encoder-Decoder and Encoder-PTR models, though English performance declined in some datasets, a phenomenon known as the "Curse of Multilinguality."

The presentation also analyzed the performance gap between zero-shot and few-shot cross-lingual transfer. Zero-shot transfer showed a significant gap compared to monolingual settings, while few-shot transfer rapidly reduced this gap.  The researchers found that pretraining on English significantly boosted few-shot performance on target languages, and that large multilingual models like Codex and BLOOM were not yet sufficient for cross-lingual semantic parsing.

In conclusion, XSemPLR provides a comprehensive benchmark for cross-lingual semantic parsing, and the study revealed valuable insights into the strengths and limitations of different multilingual language models. The researchers encourage readers to explore their paper and code.</sample>
    <sample id="260">The provided text does not mention the number of authors. It only names Jingwei Yi from the University of Science and Technology of China.</sample>
    <sample id="261">一个优秀的规划器应该写出合理且忠于约束的脚本。</sample>
    <sample id="262">The provided text does not mention the number of authors. It only names Siyu Yuan from Fudan University as the presenter.</sample>
    <sample id="263">This work addresses instability issues in in-context learning, a popular method for utilizing large language models. The authors argue that the choice and order of in-context examples introduce biases, hindering model performance.  They propose a systematic categorization of label biases, identifying a new bias type: domain-label bias, stemming from the task corpus influencing model predictions.

To validate this, they experiment with random words from the task corpus versus random English words, finding that in-domain words significantly bias predictions, especially on tasks with strong domain-label bias.  This bias severely limits the performance of language models, often barely outperforming chance baselines.

The authors introduce "domain-context calibration," a novel method to mitigate all types of label biases. This method uses random in-domain words as content-free text to estimate bias and then calibrates the model's predictions accordingly.  They demonstrate that domain-context calibration significantly improves in-context learning performance across various datasets, particularly on those with higher domain-label bias.  Furthermore, it leads to better decision boundaries in the model's predictions.

The study reveals that pre-defined content-free tokens are also biased, and using a wider range of random words yields further improvements.  Finally, using random in-domain words is more effective than random English words in mitigating domain-label bias.  The authors conclude that their work provides a comprehensive understanding of label biases in in-context learning and a practical calibration method to enhance the performance of large language models.</sample>
    <sample id="264">Lin Wang from Zhejiang University presents their paper "TAVT: Towards Transferable Audio-Visual Text Generation." The presentation addresses the challenge of audio-visual text generation, which suffers from data scarcity and domain shifts, unlike the flourishing field of uni-modal text generation. The core problem is the varying visual styles, audio characteristics, and their impact on understanding the same event.

The proposed solution, TAVT, aims to overcome these constraints by learning a unified audio semantic space to align visual concepts across domains. The framework consists of three key components: an audio-visual meta-mapper network, an audio-visual encoder and language model generator, and counterfactual contrastive learning.

The audio-visual meta-mapper network maps diverse visual concepts to a common audio semantic space using k-means clustering of audio clips from the Flickr dataset, enhanced with learnable visual prefixes inspired by prompt learning. The audio-visual encoder and language model generator utilizes a transformer architecture with an alpha parameter to assess the contribution of each modality to word generation.

A crucial innovation is Dual Counterfactual Contrastive Learning (DCLL), which generates fine-grained supervision signals from counterfactual scenarios to directly optimize visual-textual alignment, bypassing the need for randomly selected negative samples.  The model employs a meta-learning approach similar to MAML, using a support set of K-1 domains and a query set of the remaining domain for fast adaptation.

Experimental results on MSVD and MSR-VTT benchmarks demonstrate that TAVT significantly outperforms state-of-the-art RNN and transformer-based models, especially in low-resource domains. Ablation studies confirm the importance of audio features for performance. The paper highlights TAVT's ability to achieve strong performance even with limited labeled data, marking a significant advancement in transferable audio-visual text generation.</sample>
    <sample id="265">Vasudha</sample>
    <sample id="266">This text does not mention the author's institution.</sample>
    <sample id="268">PaLM 最常见的错误是遗漏错误 (omission errors)。</sample>
    <sample id="269">大家好，我是詹姆斯·芬奇，我是莎拉·芬奇。今天我们将向大家介绍ABC-Eval，一种新的维度方法来评估对话式人工智能。这项工作由埃莫里NLP实验室的Jinho Choi教授领导，与亚马逊Alexa AI合作完成。那么，假设你已经开发了一个对话模型，你想看看它与当前最先进的模型相比如何。常见的做法是使用人类评估，例如通过让人类评判员选择哪个对话更好或根据Likert量表对对话进行评分。这些方法对于提供对整体对话质量的全面评估效果很好，但对话质量有很多方面。因此，你可能想评估对话质量的多个维度，以便更细致地了解模型的优势和劣势。一种方法是简单地让人类评判员评估对话质量的多个维度，例如模型回复的相关性，使用现有的比较或Likert量表方法。然而，我们认为有一种更精确和可靠的策略来评估维度对话。我们的方法尝试减少人类评估的主观性，通过明确注释模型回复是否表达了某些行为，例如回复不相关信息或自相矛盾。我们称这种方法为聊天行为注释，简称ABC-Eval。我们开发这种方法是为了全面涵盖近年来被认为会影响聊天质量的聊天模型行为。ABC-Eval能够测量聊天模型在各种主题错误方面的发生率。例如，ABC-Eval测量聊天模型在多少个回合中忽略了对话伙伴或说了不相关的东西，自相矛盾或与对话伙伴矛盾，产生不正确的错误事实或违反常识知识，以及模型在展示同理心方面的成功或失败。为了确定哪种评估最有效，我们选择了四个最先进的聊天模型，并使用ABC-Eval对每个模型进行了100个人类-机器人对话的评估。为了比较，我们还使用三种现有的方法对这些对话进行了评估：回合级Likert评分、对话级Likert评分和对话级配对比较。对于每种现有方法，我们收集了八个最常用的对话质量方面的评估。由于这是评估聊天模型多维度的标准做法。通过对这些评估结果的分析，我们发现ABC-Eval的行为标签比现有方法收集的标签更可靠，这通过100个双标记对话上的互信度分析得到了证实。此外，ABC-Eval标签对整体对话质量的预测能力也比现有方法指标更高，正如这个简单的线性回归分析所展示的那样。例如，你可以看到，衡量回合中自我和对话伙伴矛盾的比例解释了对话质量的5%，而平均Likert一致性评分仅解释了4%或更少。最后，我们检查了每个评估指标是否捕捉了对话质量的独特方面，通过逐步线性回归。你可以看到，所有ABC-Eval指标的组合解释了超过25%的对话质量，而当逐个移除指标时，大多数指标都损失了相当多的信息。另一方面，所有回合级Likert指标的组合解释的质量远少于，并且较少的指标携带独特的 معلومات。这些可靠、信息丰富且独特的ABC-Eval指标使我们能够以比以前方法更高的分辨率评估对话式人工智能。正如我们在实验结果中看到的，仍然存在几个挑战，并且这些挑战已被精确量化。例如，我们测试的机器人模型在他们的回复中存在常识性错误约20%的概率。他们产生不相关信息约15%的概率，并且他们自相矛盾或与对话伙伴矛盾约10%的概率。在领域快速进步的情况下，许多这些错误率在发布新模型后可能会下降。然而，这正是我们追求可靠和精确评估指标的原因。我们希望ABC-Eval可以被其他人用于该领域，作为这一方向上的一个有意义的步骤。我们期待着看到对话式人工智能在未来几个月和几年中的发展。谢谢观看。</sample>
    <sample id="270">Emory NLP Lab, led by Professor Jinho Choi at Emory University, in collaboration with Amazon Alexa AI.</sample>
    <sample id="271">在本文中，CFT 代表 FTw 模型。</sample>
    <sample id="272">这篇论文有 7 位作者。</sample>
    <sample id="273">Kayo Yin 老师今天将介绍我们的工作，题目是“何时需要上下文进行翻译？一个数据驱动、多语言探索”。这项工作由 Patrick Fernandes、Emmy Liu、André F. T. Martins 和 Graham Neubig 合作完成。因此，很多翻译都依赖于上下文。例如，如果我们说“mole”这个词，在第一个句子是“如果部长们发现事情开始变得危险”，那么“mole”指的是间谍。但如果第一个句子是“医生，这到底有多严重？”那么“mole”指的是胎记。根据上下文，单词的意思会发生变化，因此翻译也会发生变化。然而，评估模型在处理这些情况方面的能力非常困难。首先，只有少量翻译依赖于上下文，这使得基于语料库的指标，如 BLEU，无法捕捉到这些翻译。有些人建议对上下文依赖的翻译进行有针对性的评估，但这些资源仅支持有限类型的上下文依赖翻译，并且仅限于有限的语言对，因为它们通常依赖于领域知识和人工整理。在本文中，我们试图回答这两个问题。首先，何时需要上下文进行翻译？其次，模型如何处理这些情况？为了回答第一个问题，我们首先测量单词在翻译过程中依赖上下文的程度。在之前的项目中，我们引入了 CXMI 作为衡量机器翻译模型上下文使用的指标。这通过测量给定源文本 X 和目标文本 Y 时，上下文 C 为目标文本 Y 提供的信息量来衡量。可以认为 CXMI 是模型获得上下文信息的一种方式。在本文中，我们扩展了 CXMI 为 Pointwise CXMI，它可以衡量上下文在句子级别或单词级别上的使用情况。我们可以认为那些具有高 P-CXMI 的单词，在翻译过程中需要上下文。然后，我们分析具有高 P-CXMI 的单词，以寻找这些单词之间的模式。我们对来自 TED 演讲的 14 种不同语言的翻译文本进行分析。我们分别在三个不同的级别上进行分析。首先，我们查看具有高平均 P-CXMI 的词性标签。这可以帮助我们找到，例如，阿拉伯语中具有相对较高 P-CXMI 的双重代词，因为英语没有双重代词，因此需要上下文来确定代词是否为双重，从而进行翻译。类似地，我们发现某些语言在选择适当的动词形式时也需要上下文。然后，我们查看在所有不同出现中对高 P-CXMI 进行平均的词汇项。这有助于我们识别出，例如，中文在翻译专有名词时需要上下文，以确保在文档中使用相同的翻译。类似地，我们发现上下文对于翻译正确的礼貌程度也很重要。最后，我们查看不同的单个词语，这些词语具有高 P-CXMI。这允许我们识别无法通过单词本身捕捉到的现象，而是在句子结构中表达的现象，例如省略句的解析。现在，我们使用我们的发现来设计一个用于文档级别翻译的基准。对于我们识别出的五个不同的语境现象，我们创建了标签器来自动识别涉及这些现象的单词。我们称我们的标签器为 Multilingual Discourse-Aware (MuDA) 标签器。我们还可以注意到，不同语言有不同的语境现象比例。然后，我们使用 MuDA 标签器，通过在用于评估的平行语料库上应用标签器，并使用我们选择的翻译指标来评估上下文依赖的例子，最后，我们使用我们的基准以及其他指标来评估不同的模型在文档级别机器翻译方面的表现。首先，当我们使用基于语料库的指标时，例如 BLEU，上下文无关的模型表现最好。但是，如果我们使用 COMET，上下文感知模型表现最好。如果我们使用词语 F-measure，那么模型和没有上下文的模型在性能上具有可比性。这再次表明，如果我们仅使用基于语料库的指标，很难确定最佳的文档级别翻译系统。现在，我们使用 MuDA 基准来评估模型，我们发现上下文感知模型在处理某些不同的语境现象方面比不使用上下文的模型更准确，例如礼貌和词汇连贯性。但是，在其他现象方面，例如省略、代词和动词形式，这些模型并没有明显优于不使用上下文的模型。这表明我们还需要在文档级别翻译方面取得更多进展。我们还比较了不同的商业系统，我们的基准显示 DeepL 通常比 Google 翻译在文档级别翻译方面更准确。总而言之，我们通过对 14 种不同语言对的数据驱动分析，识别了何时需要上下文进行翻译，然后我们使用我们的发现构建了一个用于文档级别机器翻译的基准，它可以帮助我们识别模型在处理哪些不同的语境现象方面的能力，以及哪些翻译系统在文档级别翻译方面表现良好。谢谢您的聆听。我们下次在多伦多见。</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">Ananya and Vignesh's work addresses the understudied area of evaluating machine translation metrics for Indian languages, focusing on a dataset called "IndicMT Eval."  They collected 7,000 translation samples across five Indian languages (Tamil, Malayalam, Hindi, Marathi, Gujarati) using seven translation models and obtained human annotations detailing errors (type and severity) and overall scores, based on the MQM framework.

The study analyzed the correlation between various metrics (chrF, LabSE embedding, BERTscore, MuRIL, COMET variants) and human evaluations.  While overlap-based metrics like chrF showed the highest correlation overall, they performed poorly. Embedding-based metrics like LabSE and BERTscore showed better correlations, especially when using multilingual models. COMET-metric variants exhibited the strongest correlations across all languages, but scores were often skewed.  Accuracy errors showed a stronger correlation with human scores than fluency errors.

To improve metric performance, they fine-tuned the COMET metric using their MQM dataset, creating "IndicCOMET."  This fine-tuned metric outperformed COMET baselines on three of the five languages and showed higher correlations across all languages.  Furthermore, IndicCOMET demonstrated zero-shot translation accuracy on an unseen language and exhibited better robustness compared to the original COMET metric on the ACES Translation Accuracy Challenge Sets.  The authors make their dataset publicly available.</sample>
    <sample id="277">该方法没有名称。</sample>
    <sample id="278">“显性词汇”(marked words) 方法基于社会语言学的“标记性”(markedness) 概念。它认为，社会上默认的、未标记的群体通常被认为是“标记”的，因为它们与默认的特征不同。该方法首先确定标记和未标记群体，然后通过使用加权对数似然比来比较生成的文本，以识别与这些群体相关的特定词语。</sample>
    <sample id="279">University of Washington.</sample>
    <sample id="280">Shi Tao's presentation introduces "MultiEMO," a novel attention-based framework for emotion recognition in conversations (ERC). The core problem addressed is accurately predicting emotion labels for each utterance, considering textual, audio, and visual modalities. Existing methods struggle with multimodal complementarity, minority emotion classes, and distinguishing semantically similar emotions.

MultiEMO tackles these challenges with four key components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification.  A key contribution is VisExtNet, a visual feature extractor that focuses on facial expressions of interlocutors, avoiding redundant scene information.  The framework utilizes MultiAttn, a multimodal fusion network employing bidirectional multi-head cross-attention layers to integrate textual, audio, and visual features.  This allows for cross-modal correlation learning.

To improve performance on minority classes and semantically similar emotions, MultiEMO introduces a Sample-Weighted Focal Contrastive Loss (SWFC). This loss assigns higher weights to difficult examples and encourages distinct inter-class distances.  Experimental results on MELD and IEMOCAP demonstrate MultiEMO achieving state-of-the-art performance, particularly in challenging scenarios.

The presentation acknowledges limitations, including VisExtNet's inability to distinguish between speakers and background individuals, the SWFC loss's batch size requirement on MELD, and the continued performance gap for minority emotions compared to majority classes.  The overall goal of MultiEMO is to improve the accuracy and robustness of emotion recognition in conversational settings by effectively leveraging multimodal information.</sample>
    <sample id="281">Kayo Yin and colleagues presented their work "When Does Translation Require Context? A Data-driven, Multilingual Exploration," focusing on the challenges of evaluating machine translation (MT) that relies heavily on context. The research addresses the difficulty of assessing context-dependent translations using standard metrics like BLEU, which fail to capture these nuances.

The study extends the CXMI (Contextualized Machine Translation Information) measure to Pointwise CXMI, analyzing how much context a word requires for accurate translation.  They analyzed TED talks translated into 14 languages, examining part-of-speech tags, vocabulary items, and individual tokens to identify patterns.  They found that context is crucial for translating dual pronouns in Arabic, verb forms in certain languages, and proper nouns in Chinese.  Furthermore, context is important for translating formality and resolving ellipses.

Based on these findings, they developed the Multilingual Discourse-Aware (MuDA) tagger to automatically identify discourse phenomena requiring context.  They then used this tagger to evaluate MT models on a parallel corpus, comparing performance using various metrics.  The results showed that while context-aware models excel in handling phenomena like formality and lexical cohesion, they don't significantly outperform context-agnostic models on phenomena like ellipsis and pronouns.

Finally, they compared DeepL and Google Translate, finding DeepL generally performs better for document-level translation.  The work highlights the need for more progress in document-level MT, particularly in handling context-dependent phenomena. The MuDA benchmark provides a valuable tool for identifying model strengths and weaknesses in this area.</sample>
    <sample id="282">Xuekai Zhu's ACL 2023 paper introduces StoryTrans, a new model for non-parallel story author-style transfer. The work addresses the challenge of transferring writing styles at the story level, going beyond token or sentence-level approaches.  The core problem lies in imitating author-specific linguistic preferences, particularly discourse structures and style-specific content, which are difficult to transfer between styles.

StoryTrans tackles this by learning discourse representations from source texts and combining them with learnable style embeddings. A novel training objective reduces stylistic features from discourse representations, promoting consistency.  The generation process is split into two stages: first, masking style-specific content keywords in the source text and reconstructing it; second, generating the complete text by incorporating these keywords.  The training framework utilizes self-reconstruction, disentanglement, sentence order, and style classifier losses in the first stage, and a separate stage for content filling and mask removal.

Experiments on new datasets in Chinese and English demonstrate StoryTrans's effectiveness in transferring styles while preserving content. Automatic and manual evaluations show superior style control and content preservation compared to baselines. Style visualization confirms alignment with the golden text's style features.  StoryTrans also excels at enriching storylines with relevant phrases and maintaining semantic accuracy during rewriting. The paper includes data and code in a public repository.</sample>
    <sample id="283">The name of the first mentioned symmetric dependency structure is "universal dependencies".</sample>
    <sample id="284">Peng Tianshuo from Wuhan University presented "FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction" at ACL Main Conference 4,915. The paper addresses the limitations of current span-based Universal Information Extraction (UIE) models, which heavily rely on precise span boundaries, often ambiguous in annotation.

The proposed solution introduces a "fuzzy span mechanism" to learn span boundaries as continuous distributions of correct probabilities, represented by R-min and R-max. This addresses the ambiguity in golden span boundaries and overcomes the mismatch between transformer feature extraction and span length.  A fuzzy span attention mask is introduced to dynamically adjust the attention span range and linearly decay the attention distribution, rather than truncating it.

FSUIE's core components are a fuzzy span loss (FSL) and a fuzzy span attention (FSA). FSL alleviates the reliance on precise boundaries, while FSA adapts the attention span for better information extraction. The model predicts a continuous boundary distribution, which is then converted to discrete values for loss calculation using Binary Cross Entropy (BCE) loss with the golden boundary and KL-divergence with supplementary information.

Experiments on named entity recognition, relationship extraction, and aspect sentiment triplet extraction demonstrate FSUIE's effectiveness. FSUIE-base achieved significant performance improvements compared to UIE-base, especially on small-scale datasets.  It achieved new state-of-the-art results on relationship extraction datasets (ACE2004, 2005, ADE) and AST-V2 dataset (14lap, 15res, 16res). Ablation studies show that FSA improves convergence speed and FSL enhances information extraction capability when combined. Visualizations reveal that the fuzzy span attention focuses on semantic information within a limited range.

In conclusion, FSUIE offers a novel approach to UIE by learning fuzzy span boundaries and adapting attention spans, leading to improved performance across various information extraction tasks.</sample>
    <sample id="285">Mingqi Gao from Peking University introduces their work "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework." The presentation highlights the challenge of factual errors in dialogue summarization and proposes a novel evaluation framework for Factual Error Correction (FEC) models.

The core problem is that current factuality metrics like FactCC and DAE provide a vague overall score, failing to capture the nuances of error correction and blurring the distinction between FEC models and other factuality-enhancing techniques.  The research argues that these metrics are insufficient and that manual reference corrections are needed for a more accurate assessment.

The proposed framework focuses on manually annotated reference corrections, which provide valuable data for training FEC models and enable a more comprehensive evaluation.  A new taxonomy of factual errors, categorized as content-based (based on part-of-speech and dependencies) and form-based (based on addition, deletion, and substitution), is introduced.

The evaluation framework builds upon ERRANT, consisting of alignment, classification, and comparison steps. Experiments with FEC models in different training modes reveal that training with reference summaries from dialogue datasets yields the best results when using unreliable factuality metrics.  

Key findings include the urgent need to revise FEC model evaluation methods, the benefit of incorporating human-corrected summaries during training, and the potential of combining human and synthetic data.  The research also identifies limitations of current FEC models in addressing specific error types like addition, attribute errors, modality errors, and link errors.  The work emphasizes the importance of fine-grained evaluation to improve the performance of FEC models in dialogue summarization.</sample>
    <sample id="286">James Finch 和 Sarah Finch。</sample>
    <sample id="287">Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="288">以下数据集可用于测试句法现象：

*   BLiMP (Adjunct Island case)
*   SyntaxGym</sample>
    <sample id="290">WSL (Weakly Supervised Learning)</sample>
    <sample id="291">该模型在命名实体识别、分类、词性标注和问答等 11 个生物医学和临床下游任务上进行了评估。</sample>
    <sample id="294">CamemBERT 最初是在 NACHOS 数据集上训练的。</sample>
    <sample id="295">Adam Przepiórkowski.</sample>
    <sample id="296">Valerio Basile and the University of Turin, in collaboration with Amazon Alexa, explored irony detection using a new dataset called EPIC (English Perspectivist Irony Corpus).  The research challenges the traditional assumption of a single "ground truth" in natural language understanding, focusing instead on the varying perspectives of annotators.

The EPIC corpus comprises 300 short conversations collected from social media sources (Reddit, Twitter) over 1.5 years, with annotations from 74 annotators across five English varieties.  Annotators were asked to label each reply as "ironic" or "not ironic," with an average of five annotations per conversation.  A simple, chat-like interface was used to ensure consistency.

The study found that inter-annotator agreement varied significantly depending on annotator characteristics like gender, age, nationality, and geographical location.  To address this, they developed "perspective-aware models" by fine-tuning pre-trained language models on datasets split by annotator perspectives.

While raw performance didn't show a clear trend, perspective-aware models demonstrated significantly higher confidence in their irony predictions compared to standard models.  Analysis revealed that generational differences and geographical distribution of annotators were correlated with variations in annotation agreement.  Specifically, annotators from the UK and Ireland showed the most disagreement.

The research highlights the limitations of relying solely on aggregated annotations and emphasizes the importance of considering individual perspectives in natural language understanding.  The findings suggest that perspective-aware models can improve the reliability and confidence of irony detection systems. The team plans to discuss these findings further at a poster session.</sample>
    <sample id="297">The research project "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models" investigates how coded language, or dogwhistles, are used in political discourse and how language models can be used to detect them. The project addresses the challenge of understanding dogwhistles, which communicate a hidden meaning to an in-group while appearing innocuous to an out-group, often evading content moderation.

The study begins with a comprehensive typology and glossary of over 340 dogwhistle terms and symbols, categorized by register (formal/informal) and type (implicature vs. covert persona signaling).  This glossary is built from diverse academic, online, and other sources, focusing primarily on US-centric terms related to racism, transphobia, and antisemitism.

A case study of historical US political speeches reveals a correlation between the frequency of racial dogwhistles and the Republican Southern Strategy, highlighting the evolution of dogwhistle usage over time and its association with conservatism.

The project then evaluates the ability of language models, specifically GPT-3, to recognize dogwhistles. Experiments show that GPT-3 can identify many dogwhistles, particularly those in formal registers, but performs poorly on informal and transphobic dogwhistles.  Prompting strategies, including providing definitions and secret cues, significantly improve performance.

Finally, the research examines how dogwhistles can evade content moderation by analyzing toxicity detection scores.  The study demonstrates that replacing explicit slurs and group labels with dogwhistles can reduce the perceived toxicity of hateful sentences, suggesting a vulnerability in current content moderation systems.  The project concludes by emphasizing the importance of understanding dogwhistles for NLP and linguistics, highlighting their impact on political influence and the difficulty of detecting them.</sample>
    <sample id="298">在 CoNLL-2003 数据集上进行实验后，发现性能下降的主要原因是**时间漂移**，即训练数据和测试数据之间的时间差距越来越大。具体来说，通过重新训练或继续预训练一些模型，发现性能会随着时间差距的增大而下降，这证实了时间漂移的假设。</sample>
    <sample id="299">Michalis Korakakis and Andreas Vlachos's work addresses the issue of NLI (Natural Language Inference) models' vulnerability to shortcuts, which are spurious correlations between input attributes and labels. While NLI models achieve state-of-the-art results on standard benchmarks, their performance degrades significantly when tested on out-of-distribution adversarial datasets.

Existing shortcut mitigation methods often require auxiliary models trained to exploit shortcuts, which introduces domain-specific knowledge and can be computationally expensive. Furthermore, these methods may not effectively address the issue of shortcuts, as the learner might not naturally exploit the same shortcuts as the auxiliary model.

This paper proposes a novel training method based on minimax training. The core idea is that NLI models struggle with under-represented "hard" training instances that contradict the dominant "easy" examples, which are crucial for robust generalization. The method aims to emphasize these hard examples by optimizing a weight distribution that prioritizes them.

In the minimax training objective, the learner minimizes the NLI loss, while the auxiliary model maximizes the learner's loss by generating example weights that incentivize the learner to focus on hard examples. Both models are trained iteratively using standard optimization algorithms. At test time, the learner can make predictions without relying on the auxiliary model.

The proposed method doesn't require any assumptions about the type of shortcuts in the dataset and relies on the learner's training dynamics to generate example weights. A feed-forward network is used to model the auxiliary model.

Experiments on MNLI, FEVER, and QQP, along with adversarial test sets, demonstrate that the minimax training objective consistently improves out-of-distribution performance while maintaining high in-distribution accuracy, outperforming ERM training and existing shortcut mitigation methods. The paper also investigates the effect of pre-training the learner, the size of the auxiliary model, and the transferability of the improvements to larger models, synthetic shortcuts, and out-of-domain test sets. The authors invite questions during the poster session.</sample>
    <sample id="300">Belinda's presentation introduces "interactive dictation," a new task aiming to bridge the gap between traditional speech-to-text and more intuitive human-computer interaction. Unlike current systems that primarily support dictation or limited vocal command templates, interactive dictation allows users to seamlessly switch between dictating and editing text using natural language.

The core idea is to enable users to correct errors mid-sentence and issue commands to modify the text, all without requiring rigid trigger words. The presentation formalizes interactive dictation as a four-step process: speech recognition, utterance segmentation (dictation vs. command), command normalization and error correction, and finally, execution of the sequence of dictation and command utterances.

To address the lack of existing data, Semantic Machines developed a new data collection interface and a corresponding dataset. A baseline system was then built, comprising separate models for each of the four steps. The models were evaluated using exact match of the predicted final document state against the desired state.

The presentation highlights a trade-off between runtime and accuracy, with GPT-3 models achieving higher accuracy but slower performance. Predicting the final state directly yielded better results for GPT-3 compared to predicting intermediate programs. T5 models showed a less pronounced difference, with program prediction offering efficiency gains.

The authors conclude that significant room for improvement exists and have released code for future research. The paper provides further details on the task, data collection, and system evaluation.</sample>
    <sample id="302">因为在模型预测输出时，首先预测的是输入序列中每个词元可能出现的词元集合（multiset），但这些词元集合没有顺序。因此，需要一个额外的步骤来预测这些词元集合的排列顺序，才能得到最终的输出序列。</sample>
    <sample id="303">作者建议模型所有者提高偏见缓解方法的透明度，因为我们不知道导致这些积极的刻板印象产生的原因，可能是由于过度价值对齐或其他反刻板印象方法，而缺乏透明度使得我们无法进一步研究和解决这些问题。</sample>
    <sample id="304">最小对不可接受输入（Minimal Pair Acceptability Judgments, MPP）是一种评估语言模型是否能正确判断句子可接受性的方法。它通过展示一个可接受的句子和一个不可接受的句子，让模型预测哪个句子更可能被接受。这种方法可以评估模型对语法、风格和潜在的偏见等方面的理解。</sample>
    <sample id="305">This video presents the research of Dawei, a PhD student at Saarland University, on "Weaker Than You Think: A Critical Look at Weakly Supervised Learning." The work, conducted in collaboration with several researchers, challenges the common claim that weakly supervised learning (WSL) methods achieve high performance solely on weakly labeled data, often relying on a clean validation set.

The research investigates three key questions: Is a clean validation set necessary for WSL? How many clean samples are required? And should clean samples be used for validation or other purposes?

The findings reveal that recent WSL methods *require* clean validation samples to generalize effectively. Without them, models fail to learn beyond the weak labels, rendering the training process pointless.  Furthermore, increasing the number of clean validation samples leads to improved performance, with even direct fine-tuning on clean data outperforming WSL approaches in some cases.  The study demonstrates that the performance gains often attributed to WSL are easily achievable by allowing models to continue fine-tuning on the clean validation set.  Even simpler models like FTw can achieve comparable performance to more complex WSL methods with this approach.

The authors conclude that the claimed benefits and practicality of many WSL approaches are overestimated. They offer several recommendations for future research, including transparent reporting of model selection criteria (specifically, whether clean validation is used), comparison with few-shot learning baselines, and the inclusion of continuous fine-tuning as a strong baseline.  The code for the research is also open-sourced. The overall message is a critical re-evaluation of the current state of WSL, highlighting the importance of clean data and suggesting that simpler approaches can often achieve comparable results.</sample>
    <sample id="306">Sebastian Schuster and Najoung Kim presented research on entity tracking in language models, a crucial ability for understanding discourse. They argue that current language models haven't been systematically evaluated on this task, and their work aims to address this gap.

The core research question is: to what extent can large language models track entities?  The authors highlight challenges in designing evaluation tasks: preventing models from relying on common entity states in pre-training data, avoiding simple heuristic associations between words and entity states, and mitigating the risk of memorization or heuristic application through fine-tuning or in-context demonstrations.

To address these challenges, they designed a task involving boxes and objects.  The input provides the initial contents of each box, and the model must predict the contents after state-changing operations (like moving objects).  They implemented measures to prevent reliance on heuristics.  Flan-T5 and GPT-3/3.5 were tested using 2-shot in-context learning.

Results show that most models simply repeat the initial state, indicating a lack of true entity tracking.  Text-davinci-003 exhibited non-trivial tracking, while other models performed below a random baseline.  Analysis of GPT-3.5 models revealed that those trained on code demonstrate better entity tracking capabilities compared to models with less code in their pre-training.  Smaller models like T5-base can learn tracking through direct fine-tuning, but randomly initialized models struggle.

The authors conclude that pre-training on code appears to be a key factor in enabling entity tracking in language models.  They acknowledge the limited generalizability of their findings and invite further discussion. The paper is available on arXiv.</sample>
    <sample id="307">作者使用了以下评估指标：命名实体识别、分类、词性标注、问答。</sample>
    <sample id="308">Jenny, a PhD student at Carnegie Mellon University, presents "NLPositionality: Characterising Design Biases of Datasets and Models." The work, a collaboration with the University of Washington and the Allen Institute for AI, investigates the presence of "positionality" in NLP datasets and models – the influence of researchers' demographics and life experiences on their development.

The presentation highlights how design biases, like the disparity in toxicity detection accuracy between Prospective API and models sensitive to Indian contexts, arise from this positionality.  NLPositionality aims to quantify these biases by comparing annotations from diverse user groups with existing datasets and models.  

The framework involves re-annotating datasets with a large, diverse group of annotators and then comparing their responses to models like GPT-4, Perspective API, and Dynahate using Pearson's R correlation.  The study analyzed over 16,000 annotations from 1000+ annotators across 87 countries.

Key findings reveal positionality in NLP, with datasets and models showing higher alignment with English-speaking countries and individuals with college education.  However, this alignment leaves non-binary individuals underrepresented.  

The presentation concludes with recommendations for mitigating these biases: documenting design choices, adopting a perspectivist approach to research, and building specialized datasets and models for specific communities, referencing the Masakhani initiative as an example.  The work emphasizes that inclusive NLP requires considering the diverse perspectives of all users.</sample>
    <sample id="309">Inter-annotator agreement.</sample>
    <sample id="310">选择 Wikipedia。</sample>
    <sample id="311">The provided text does not mention the authors' affiliated institution.</sample>
    <sample id="312">MultiInstruct 是第一个大规模多模态指令微调基准数据集，它包含 62 种不同的多模态任务，涵盖 10 个广泛的类别。与以往的指令微调工作不同，MultiInstruct 专注于多模态任务，而之前的研究主要集中在语言任务上。此外，MultiInstruct 旨在解决 NLP 和多模态任务指令数据集可用性的差距，它构建了一个包含 21 个现有开源数据集的指令数据集，每个任务都配备了五种由专家撰写的指令。</sample>
    <sample id="313">这篇论文由 James Finch, Sarah Finch, Jinho Choi 和 Amazon Alexa AI 共同完成。</sample>
    <sample id="314">根据内容，二进制协调是指两个或多个词语或短语以某种方式连接在一起，形成一个整体。内容中提到了不同的依赖结构理论和语料库方法来描述二进制协调，并讨论了这些结构是否对称或非对称。</sample>
    <sample id="315">This paper doesn't specify the average length of the prompts used. It focuses on the *type* of prompts – "Imagine you are an Asian woman. Describe yourself." – rather than a specific length.</sample>
    <sample id="316">这些发现表明，通过在 CoScript 数据集上进行微调，T5 模型可以生成比大多数大型语言模型更高质量的脚本，这表明在适当训练的合适数据集上，较小的模型可以超越大型模型。</sample>
    <sample id="317">Peng Li from Fudan University presented their work "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors." The presentation focused on information extraction, a core NLP task involving extracting structured information from unstructured text, including named entity recognition (NER) and relation extraction (RE).

Traditional approaches utilize pre-trained language models like T5 and GPT-3, which are trained in a text-to-text manner. However, during inference, the output is linearized into a plan sequence, creating a mismatch between input and output formats. This makes it difficult for models to generate correct structured outputs, often requiring large structured training datasets and specialized decoding strategies.

CodeIE addresses this by transforming the text-to-structured task into a structure-to-structure code generation task, leveraging code-based large language models like Codex. This allows for easy conversion between text and structured formats, ensuring alignment between input and output.

The researchers designed prompts for NER and RE, using few-shot in-context demonstrations to guide the code generation process. They evaluated their method on various datasets, comparing it to T5, UIE, GPT-3, and Codex.  The results showed that CodeIE significantly outperformed traditional baselines, particularly when using code-style prompts.

Analysis revealed that code-based models like Codex better align with the information extraction task, exhibiting fewer structural errors compared to GPT-3 with text-style prompts. Codex also demonstrated superior performance overall, especially in terms of recall.  The study highlighted that using code format prompts consistently yielded better results than text format prompts.

The researchers conclude that transforming information extraction into a code generation task using code-based models is a promising approach.  They made their paper and code publicly available.</sample>
    <sample id="318">Yanis Labrak 先生，您将向我们展示你们在“DrBERT：用于生物医学和临床领域的鲁棒预训练模型”上的工作。在本次演示中，我们将首先讨论医疗保健中的语言建模。然后，我们将介绍我们文章的主要贡献。我们介绍了第一个法语生物医学模型 DrBERT，它基于 RoBERTa 并使用 NACHOS 数据集进行训练，NACHOS 是一个来自网络的医疗爬取数据集合。我们还介绍了使用多种预训练设置和数据来源的模型比较。最后，我们将展示我们在 11 个法语生物医学和临床下游任务上的结果。此外，我们将对实验进行总结，并提供更多关于如何访问这些模型的细节。自 2018 年以来，BERT 已成为解决自然语言处理任务的最有效方法之一，并且与历史上的静态和上下文感知方法（如 Word2vec、fastText 或更早的模型）相比，具有巨大的性能提升。此后，该模型已适应许多其他语言，如 CamemBERT（法语），以及生物医学领域（PubMedBERT 和 BioBERT）和临床领域（ClinicalBERT），但主要集中在英语上。针对其他语言的专业模型稀少，并且通常依赖于持续预训练，因为缺乏特定领域的训练数据。然而，法语在生物医学领域没有开源模型。因此，我们提出了一个问题：哪些数据来源最适合广泛使用？爬取的数据是临床数据的良好替代品吗？为了回答这个问题，我们将 DrBERT 与我们的 ChuBERT 模型进行比较，ChuBERT 是一个基于 Nantes 大学医院数据仓库中匿名数据的临床模型。接下来，我们思考：我们需要多少数据来训练一个针对法语数据的专业模型？是 4 GB、8 GB 还是更多？为了回答这个问题，我们首先训练并比较了四个从零开始的模型：DrBERT 的第一个版本，使用 7 GB 的 NACHOS 数据；第二个版本，使用 4 GB 的 NACHOS 数据集；ChuBERT 的第一个版本，这是一个临床模型，使用 4 GB 的来自临床笔记的句子；以及 ChuBERT 的最后一个版本，使用 4 GB 的 NACHOS 数据集和 4 GB 的临床笔记的混合。此外，我们还引入了三个基于持续预训练的模型，以分析预训练策略的影响。一个基于 CamemBERT 的权重，使用 4 GB 的 NACHOS 数据集进行训练。另一个也基于 CamemBERT，但使用 4 GB 的临床笔记进行训练。还有一个基于英语生物医学模型 PubMedBERT，使用 4 GB 的 NACHOS 数据集进行训练。总共有七个模型。为了评估我们的七个模型，我们收集了用于公共和私有下游任务的数据，如命名实体识别、分类、词性标注和问答。这些模型与六个基线模型进行比较，分别是 CamemBERT OSCAR 138 GB、CamemBERT OSCAR 4 GB、CamemBERT CCNET 4 GB、PubMedBERT、BioBERT 和 ClinicalBERT。评估结果表明，模型在数据性质与模型训练数据相同的情况下表现最佳。然而，我们观察到，来自异构数据来源的模型更具通用性。我们还观察到，使用更多数据可以提高性能。总的来说，从零开始的预训练在大多数任务上都能获得更高的性能。然而，我们在控制预训练中，使用 CamemBERT 的权重和分词器，在 4 GB 的 NACHOS 数据集上训练的 DrBERT 4 GB 模型与我们实验中获得的性能相当，这并非 CamemBERT 权重和分词器模型所见。此外，我们的系统在九个下游任务中表现更好，并且在全球范围内超越了通用模型 CamemBERT。我们还观察到，更专业的训练数据效果更好，但无法扩展。所有从 NACHOS 数据集中预训练的模型均已免费提供在 Hugging Face 上，并附有 MIT 许可，所有训练脚本均已上传到我们的 GitHub 仓库。因此，感谢本次演示，我们期待在多伦多会议期间进行交流。</sample>
    <sample id="319">论文研究了以下学习策略：

*   **从零开始预训练 (From-scratch pre-training)**
*   **持续预训练 (Continual pre-training)**
*   **使用 CamemBERT 的权重和分词器 (Using CamemBERT weights and tokenizer)**</sample>
    <sample id="320">根据演讲内容，由于测试重复使用而导致的过拟合因素**不是**主要的。演讲者通过实验发现，CoNLL-2003数据集的重复使用并没有导致模型性能的下降，而是存在“temporal drift”（时间漂移）现象，即随着训练和测试数据之间的时间差距增大，模型性能会下降。</sample>
    <sample id="321">根据内容，评估简化质量的方法是使用**手动对句子进行标注的对齐（gold standard alignments）**来评估**自动对齐方法**。此外，还通过**评估模型在生成简化文本方面的性能**来衡量简化质量，并将其作为未来自动文本简化问题的基准。</sample>
    <sample id="322">Enrico from ACL 23 presents on how text classifiers learn about morality. He begins by defining morality as a subjective internal compass guiding us towards right and wrong, crucial for societal understanding. While NLP has explored morality in text, it often treats it as a single scale, which is insufficient given its subjective nature (e.g., differing views on abortion).

Enrico introduces the Moral Foundation Theory, proposing five fundamental ways humans perceive morality, each influencing judgment. He notes that language models *do* somewhat understand morality, and his paper aims to understand *how* they learn this.

The research utilizes explainable AI techniques on language models trained on morality in text, specifically the Moral Foundation Twitter Corpus (35,000 tweets from seven domains like #AllLivesMatter and #BlackLivesMatter). The goal is to determine if models recognize the domain-specific nuances in moral expression.

The presentation highlights the example of #AllLivesMatter and #BlackLivesMatter. While both address similar topics, they employ different rhetorical strategies regarding moral subversion (rebellion against authority). The research found language models recognize that subversion is negatively framed in #AllLivesMatter and more positively in #BlackLivesMatter.

Enrico emphasizes that morality is expressed differently across domains, and using a single model for multiple domains can lead to dangerous misunderstandings. The paper explores various levels of understanding, warning against the limitations of a one-size-fits-all approach to morality in NLP. He concludes by inviting attendees to ACL in Toronto.</sample>
    <sample id="323">Yujie Wang from Shanxi University introduces DHLK, a novel approach for Commonsense Question Answering (QA) that addresses limitations in existing methods. Current approaches often suffer from noisy entity retrieval, isolated encoding of knowledge and text, and a lack of semantic relationship understanding. DHLK tackles these issues by building a highly optimized Heterogeneous Knowledge Graph (HKG) using a two-stage pruning strategy and Knowledge Representation Learning (KRL).

The method enhances the HKG by incorporating paraphrases of key entities from WordNet and Wiktionary, creating additional nodes and improving knowledge representation.  It then utilizes RoBERTa and Mask Self-Attention to encode and fuse the question context and entities, dynamically removing irrelevant entities based on attention weights.  Entity and relation embeddings are optimized using TransE and Relation Mask Self-Attention (RMSA), inspired by RGAT.

The HKG's path information is integrated with the question context through path enhancement, and a Multi-Layer Perceptron (MLP) predicts the answer probability.  Experiments on CommonsenseQA and OpenBookQA, using ConceptNet, WordNet, and Wiktionary, demonstrate that DHLK achieves strong performance compared to existing Language Model (LM) and HKG methods. The study highlights the importance of semantic relationship understanding and dynamic entity removal for effective Commonsense QA.</sample>
    <sample id="324">语言模型确实有不同的政治偏见。研究表明，语言模型在不同的政治立场上表现出差异，并且这些偏见会随着训练数据的变化而变化。</sample>
    <sample id="325">大家好！我叫马蒂亚斯·林德曼，今天我将简要介绍我们论文“无需树结构的成分泛化：使用多重标签和隐式排列”的工作。这是一项与我的导师亚历山大·科勒和伊万·提托共同完成的研究。成分泛化是指学习者能够处理更深层次的递归和训练数据中看到的句子短语的未见结构组合的能力。在语义解析的语境下，测试成分泛化可能如下所示。我们通常有一个包含句子的训练集。“女孩在睡觉。”和“玛丽知道女孩在睡觉。”这些句子与表示它们核心含义的逻辑形式对齐。与标准的机器学习评估不同，测试集并非来自相同的分布，而是包含结构上未见的逻辑形式。在这个例子中，模型在训练中看到了浅层递归，而在测试中则面对更深层次的递归。传统的序列到序列模型难以处理这种离散分布的泛化，并且常常产生与输入不相关的输出。特别是，它们经常无法重现输入和输出之间的系统对应关系，例如在例子中颜色编码的对应关系。一种流行的解决此问题的办法是集成树结构到模型中。这些树结构旨在捕捉句子短语之间关系所进行的成分过程。这通常有效，但树结构需要以某种方式获得，这可能很复杂，有时会很耗时。这通常涉及对逻辑形式的相当大的形式化预处理，例如处理变量符号。获得树结构也可能涉及专门的语法推断程序。在本文中，我们不使用树结构，而是一种神经网络序列到序列模型，该模型直接建模输入和输出片段之间的对应关系。我们首次证明了无需树结构即可实现对更深层次递归的强大泛化。我们的方法通过两步预测输出。首先，我们为输入中的每个标记分配一个包含输出中出现的标记的无序多重标签。在第一步之后，我们拥有所有正确的标记，但它们尚未排序。因此，在第二步中，我们使用另一个模型来预测一个排列，将这些标记放入正确的顺序。我们引入了一种新的方法来预测排列，该方法不施加任何硬约束。这使得我们的方法非常灵活和具有表达力。从概念上讲，我们的排列模型大致如下工作。我们从左到右遍历输出，确定要在每个位置放置哪个多重标签标记。对于输出中的第一个位置，我们简单地选择一个，如图中红色高亮所示。然后跳到下一个多重标签标记，以确定输出中的第二个标记。我们以类似的方式确定输出中的第三个标记，跳到另一个多重标签标记。我们继续这个过程，直到所有从第一阶段访问的标记都只访问了一次。为了给您一个关于实验结果的预览，我们使用 COGS 基准将我们的方法与其他无树模型进行比较。我们的模型在更深层次递归的泛化方面比其他模型都表现出显著优势。然而，仍然存在一些其他类型的结构泛化非常具有挑战性。在本文中，我们解决了几个有趣的实际挑战。首先，输入和输出之间的对齐关系在训练数据中未给出。因此，对于给定的标记，我们不知道它来自哪个多重标签集合，这给训练带来了挑战。此外，有时存在多个与数据一致的排列，但语言上正确的排列是隐式的。我们通过将对齐关系作为训练的一部分来解决这个问题。我们的排列方法非常灵活，但它带来了一个挑战，即找到最高分排列是 NP-hard 的。这是因为这与“旅行商问题”相关。我们通过一个 GPU 友好的连续放松来近似它，该放松还允许我们通过解决方案进行反向传播，并学习更符合语言逻辑的排列。如果您想了解更多关于我们的实验以及我们如何解决这些挑战的信息，请查看我们的论文或来我们的海报。</sample>
    <sample id="326">认知失调是指两个信念或行为不一致的情况，例如一个人知道吸烟会致癌，但仍然吸烟。这种不一致会导致心理上的不适，并可能通过其他行为来缓解，例如通过说服自己“我需要吸烟来维持工作”。认知失调在语言中非常罕见，但研究它有助于理解社会现象、趋势、态度变化、心理健康以及极端化和极化等问题。</sample>
    <sample id="327">The presentation introduces ManagerTower, a novel vision-language (VL) model architecture designed to improve visual question answering (VQA) performance. The core problem addressed is that existing VL models, like BridgeTower, often treat unimodal encoders (e.g., METER, CLIP-ViT) uniformly, ignoring the varying levels of semantic knowledge present at different layers.

ManagerTower builds upon BridgeTower by introducing "managers" within each cross-modal layer. These managers adaptively aggregate insights from multiple unimodal representations at different levels, allowing for more comprehensive cross-modal alignment and fusion. The architecture utilizes RoBERTa and CLIP-ViT base encoders.

The key innovation lies in the adaptive aggregation of unimodal knowledge.  ManagerTower demonstrates superior performance compared to BridgeTower, particularly on the Wikivideo dataset, achieving a 39.15% accuracy improvement.  This improvement is attributed to the ability of managers to effectively exploit different levels of semantic knowledge.

The presentation provides visual evidence of this adaptation, showing that static managers exhibit consistent aggregation patterns, while adaptive managers show diverse and context-dependent weight distributions. This suggests that the need for different unimodal knowledge varies across cross-modal layers.

ManagerTower's effectiveness is demonstrated by its performance on various downstream tasks, surpassing models trained with more data or parameters. The authors emphasize that ManagerTower maintains the same pre-training and fine-tuning settings as other models, highlighting its efficiency.  The paper, code, and model are publicly available on Archive and Github. The researchers hope their work will contribute to the advancement of VL representation learning.</sample>
    <sample id="328">GPT-4 是最倾向于自由派的语言模型。</sample>
    <sample id="329">Minghang Zheng from Peking University presents their work on "Generating Structured Pseudo Labels for Noise-resistant Zero-shot Video Sentence Localization." This research addresses the challenge of video sentence localization – identifying relevant video segments based on natural language queries – a task crucial for video retrieval and summarization.  Traditional methods rely on manual annotations, which are costly.  The proposed work aims to achieve zero-shot localization without manual annotations by generating pseudo-labels.

Existing zero-shot methods generate pseudo-queries based on pseudo-events, but suffer from drawbacks. Pseudo-queries are often simplistic, and the approach struggles to differentiate between relevant and irrelevant video segments. Furthermore, they fail to account for label noise, leading to inaccurate training.

The proposed solution, SPL (Structured Pseudo-Label generation), tackles these issues. It leverages a pre-trained image caption model to generate complex, free-form pseudo-queries.  Then, it uses another pre-trained model to assess the relevance between video frames and these queries, creating pseudo-events that guarantee high relevance within the event and low relevance outside.  Finally, SPL reduces the influence of noisy samples through sample re-weighting and label refinement.

The method involves densely sampling video frames and using a BLIP model to generate pseudo-queries.  It then models the temporal structure of events to generate pseudo-events, focusing on high similarity within events and low similarity outside.  The quality of pseudo-events is determined by the average similarity within and outside the event.  Noisy samples are identified based on model confidence and IoU with pseudo-labels, and their contribution is reduced.  High-confidence, high-IoU predictions are used as new pseudo-labels for subsequent training rounds.

Experiments on ActivityNet Captions and Charades-STA demonstrate that SPL outperforms existing zero-shot methods on key evaluation metrics like R@M and mIoU. The code is available via a QR code.  The research offers a robust approach to zero-shot video sentence localization, mitigating the impact of label noise and achieving state-of-the-art performance.</sample>
    <sample id="330">Cumulative 训练在主动学习中比迭代训练更有效。</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">MuDA基准的数据是从一个平行语料库获得的。</sample>
    <sample id="333">Nanjing University's Wenhao introduces their work "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation." The research addresses the issue of non-smooth representation spaces in neural machine translation (NMT) models, which limits generalization.  The core idea is to use k-Nearest Neighbors (kNN) to smooth predictions by leveraging nearest neighbor information in the representation space. However, this approach faces challenges: the large datastore required for kNN-MT is time-consuming to maintain, and updating representations is difficult.

INK proposes a novel training framework to overcome these limitations. It employs a two-step training loop: first, kNN knowledge is extracted from the datastore to guide an adapter in adjusting the representation.  Then, updated representations are asynchronously used to refresh the datastore.  The adapter uses a combined learning objective involving aligning contextualized representations with token embeddings, kNN token embeddings, and representations of the same target token to address sparsity.

Experiments on the WMT'19 German-English news translation task demonstrate that INK outperforms state-of-the-art kNN-MT systems, even surpassing the performance of the WMT winner model.  INK achieves higher BLEU scores with less memory usage.  The research explores the impact of adapter size and the combined use of adapters and datastores, finding that joint application further improves prediction smoothness.  The INK framework offers a novel approach to iteratively refine the NMT model's representation space using kNN knowledge, leading to significant gains in translation performance, memory efficiency, and inference speed.  The study concludes that further improvements to the framework could reveal even greater benefits from a smoother representation space.</sample>
    <sample id="335">Matthias Lindemann.</sample>
    <sample id="336">在跨语言语义解析中，跨语言转移是指在一种语言上训练的模型，然后将其应用于另一种语言的任务。具体来说，在XSemPLR的实验中，他们通过在一种语言上训练模型，然后将其应用于另一种语言的语义解析任务，来评估模型的跨语言能力。</sample>
    <sample id="337">This presentation introduces "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning," a novel approach to handling out-of-vocabulary (OOV) words in natural language processing. OOV words pose a challenge for embedding-based models, so the research focuses on inferring their meaning based on word formation and association.

The core idea is to construct a Word Relationship Graph, mimicking lexical rules, around OOV words.  Each word or wordpiece becomes a node, and its embedding represents the node's attribute.  A two-level graph is created: the first layer retains complete wordpiece information, while the second layer samples nodes for training to reduce noise.

To address the challenge of assigning node attributes to OOV nodes, a self-attention network is used to leverage character information.  Graph Attention Networks are then applied twice, concatenated and fused with the initial input, to generate node-level representations. A readout block generates a graph-level representation summarizing the word formation.

The model utilizes a Graph Convolutional Network for processing the graph and incorporates contrastive learning with NT-XENT loss. Positive samples include two-hop relevant neighbors, synonyms, and the OOV word itself, encouraging proximity between related words and separation from unrelated ones.

Experiments demonstrate superior performance compared to baselines in both intrinsic and extrinsic tasks. The model benefits both static and contextual embedding models.  The research also explores the applicability to other languages, particularly agglutinative languages, noting that the model's success with English depends on reasonable word segmentation.  The graph-based approach is presented as a versatile method for handling complex word formations.</sample>
    <sample id="338">Bingsheng from Rensselaer Polytechnic Institute, Northeastern University, and IBM Research presented their research on evaluating the helpfulness of human-generated natural language explanations for machine learning models. The paper, titled "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations," addresses the challenge of objectively assessing explanations, which are often subjective and task-dependent, unlike labels used for training.

The research highlights that traditional metrics like BLEU and ROUGE, which focus on word similarity, are insufficient for evaluating explanations. The "simulatability score," which measures performance changes with and without explanations, also has limitations in considering task differences and explanation utility during different model stages.

To address these shortcomings, the authors introduced a unified data format that converts various tasks into a consistent multiple-choice format, allowing for seamless application of different tasks and explanation types. They conducted experiments on five datasets (CoS-E, ECQA, e-SNLI, and ComVE) using T5 and BART models, analyzing the utility of explanations in both fine-tuning and inference settings.

The results showed that fine-tuning with explanations doesn't necessarily teach models new knowledge, but rather encourages reliance on the explanation part of the input. CoS-E explanations were less helpful than ECQA explanations on baseline models, aligning with previous findings.  Fine-tuning with explanations led to substantial performance improvements.

The authors proposed a novel evaluation metric called TREU, which extends the simulatability score by evaluating explanation helpfulness during fine-tuning.  TREU consistently ranked datasets higher than the simulatability score, and showed that the helpfulness of explanations depends on the task and explanation format.  For example, explanations in e-SNLI were more helpful for entailment than for neutral or contradiction classes.

The research concludes that their work provides a foundation for high-quality human collaboration in annotation and recommends future research to perform similar quality checks. The paper demonstrates that TREU outperforms the simulatability score in evaluating explanation helpfulness.</sample>
    <sample id="339">Saarland University in Germany.</sample>
    <sample id="340">Kuan-Hao Huang from UCLA presents "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation." The presentation highlights the importance of paraphrase generation for NLP tasks like question answering and chatbots, noting the limitations of existing datasets (MRPC, PAN, Quora) in scale and syntactic diversity.

The core idea of ParaAMR is to leverage AMR (Abstract Meaning Representation) graphs for generating more diverse paraphrases.  The process involves using a pre-trained AMR parser to create AMR graphs from source sentences.  Then, the focus of the graph is randomly changed (a node becomes the new root), and the corresponding edges and labels are modified.  An AMR graph-to-text generator then produces paraphrases based on these modified graphs, preserving semantic similarity while introducing syntactic variation.

ParaAMR contains approximately 15 million source sentences, with 6.9 paraphrases per sentence.  Quantitative analysis shows that ParaAMR achieves high semantic similarity scores comparable to other back-translation datasets, but significantly higher syntactic diversity scores.

The presentation demonstrates the benefits of ParaAMR across various NLP applications.  Specifically, sentence embeddings learned from ParaAMR outperform those learned from other datasets on the STS benchmark.  ParaAMR enables better syntactic control in paraphrase generation.  Furthermore, ParaAMR improves performance in few-shot learning scenarios when used for data augmentation.

In conclusion, ParaAMR is presented as a valuable resource for paraphrase generation, offering a large scale and syntactically diverse dataset that enhances performance in multiple NLP tasks. The dataset is publicly available.</sample>
    <sample id="341">作者使用了 BLEU (衡量翻译质量) 和平均延迟 (latency) 以及计算成本相关的平均延迟 (computationally-aware average lagging) 来测量延迟。</sample>
    <sample id="342">Gao Jingsheng's presentation introduces "LiveChat," a large-scale, personalized dialogue dataset constructed from live streaming videos on Chinese platforms like TikTok and Douyin. The paper addresses the limitations of existing dialogue datasets, which are predominantly text-based and often rely on manual annotation, hindering the development of realistic and personalized conversational AI.

The presentation outlines the dataset construction process, which involves scraping videos, transcribing audio using Automatic Speech Recognition (ASR), and then using a reply-to-whom matching method to generate dialogues from audience comments.  Persona information is extracted through manual labeling and rule-based methods, aiming to capture individual conversational styles.

LiveChat distinguishes itself from other datasets through its video-sourced nature, larger scale, and longer average conversation sessions.  Experiments on response modeling and addressee recognition demonstrate the benefits of persona profiles and extended conversation lengths for learning personalized responses.  While single-stream BERT performs better in addressee recognition, BART shows superior performance in response modeling, highlighting the distinct characteristics of LiveChat's domain.

The presentation also explores the performance of pre-trained dialogue models like BART and investigates in-context learning with demonstrations.  The results suggest that while LLMs benefit from demonstrations, excessive demonstrations can introduce noise.  

In conclusion, LiveChat provides a valuable resource for developing personalized dialogue systems, particularly in Chinese, and future work will focus on efficient transfer learning of large language models for this dataset. The research addresses key barriers in existing dialogue datasets, paving the way for more realistic and engaging conversational AI applications.</sample>
    <sample id="343">大家好，我是Akshatha，和我共同作者Martin今天将介绍我们的工作“KITMUS测试：评估多源知识整合”。这项工作由麦吉尔大学、Mila和微软研究合作完成。自然语言理解模型从各种知识来源中获取信息，例如存储在参数中的知识，通常是在预训练过程中获得的，以及在推理时提供的知识。最近在问答任务中，研究表明模型可以利用预训练时期的知识来解决任务。但是，自然语言理解通常需要在推理时提供的知识。例如，在句子“John saw the newly elected president on TV”中，预训练参数可能包含关于总统做什么以及电视是什么的信息，但它们无法可靠地知道这个特定实体“John”是谁，或者新总统是谁，因为总统可能会在预训练后发生变化。因此，知识密集型自然语言理解任务的成功模型需要能够整合和使用预训练时间和推理时期的知识。在本文中，我们提出了一种知识整合的诊断测试套件。我们引入了一个核心指代消解任务，旨在测试模型在不同知识来源之间提取知识的能力。我们使用人类研究参与者和已建立的核心指代消解模型评估数据集。这里有一个来自我们数据集的例子。Servin是一位法官。Kea是一位面包师。Servin和Kea在公园见面了。在一天工作结束后，在法庭上处理案件，他感到很开心，放松了。任务是识别代词“he”指代的正确实体，在这种情况下，指的是Servin。解决一个代词需要两种类型的信息。首先，实体特定的知识，例如“Servin是一位法官”。其次，背景知识，例如“法官在法庭上审理案件”。通常，背景知识是在大型语言模型预训练过程中学习的，而实体特定的知识通常是在推理时观察到的。我们以不同的方式控制两种知识类型的可用性，使其可能只存在于一个来源，或存在于多个来源。我们定义了三个KITMUS设置。首先是典型的设置：“背景-预训练”，在这种设置中，背景知识被假定是在预训练期间可用的。第二个是“背景-两者”，在这种设置中，背景知识在预训练期间和推理时都可用。最后是“背景-推理”，在这种设置中，两种知识类型都只在推理时可用。最后一个设置尤其有趣，因为它模拟了背景知识不足以解决任务的情况，因为新职业在预训练时间之后已经发展。这里有一个如何控制真源事实可用性的例子。在“背景-预训练”设置中，我们假设背景知识“政治家寻求在政府中当选”包含在预训练参数中，而在推理时，我们提供实体特定的知识“Chichester是一位政治家”。在“背景-两者”设置中，我们额外提供政治家背景知识，以及在推理时关于政治家的知识。在“背景-推理”设置中，我们提供虚构的职业“mirituer”而不是政治家，因为“mirituer”不太可能包含在预训练数据中。我们使用人类研究参与者和已建立的核心指代消解模型评估数据集。在下面的图表中，我们展示了最佳性能模型在“背景-预训练”设置中最困难的变体上的结果。如果没有针对KITMUS的任务特定训练，两种模型都表现不佳。然而，经过针对KITMUS的训练后，两种模型C2F和BERT4Coref都比随机选择表现出显著的改进。这表明，当使用通用指代消解数据集训练时，大多数模型学会了利用表面线索，而这些线索在KITMUS中已被删除。额外的虚构知识实验表明，即使是最具表现的模型，也无法可靠地整合仅在推理时提供的后向知识。总结我们论文的主要结论是，许多核心指代消解模型似乎无法在不同来源的知识之间推理，而没有针对任务的特定训练。然而，通过针对任务的特定训练，一些模型能够成功地整合来自多个来源的知识。然而，即使是最具表现的模型，也似乎在可靠地整合仅在推理时呈现的后向知识方面仍然存在困难。如果您对更多细节感兴趣，请查看我们的论文并查看数据集和代码。感谢您的聆听。</sample>
    <sample id="344">基于树的方法通常需要获取树结构，这可能比较复杂且耗时，例如需要对逻辑形式进行大量的预处理或进行语法推断。</sample>
    <sample id="345">Matthias Lindemann introduces a paper on "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations," co-authored with Alexander Koller and Ivan Titov. The paper addresses the challenge of compositional generalization in semantic parsing, where models need to handle unseen combinations of phrases. Traditional methods using tree structures for compositional understanding face difficulties in obtaining these trees, which can be complex and computationally expensive.

The proposed approach avoids trees by using a neural seq2seq model that directly models the relationship between input and output fragments. The model first tags each input token with a multiset of potential output tokens. Then, a separate model predicts a permutation of these tokens to arrange them in the correct order. This permutation prediction is done without hard constraints, offering flexibility and expressiveness. The permutation model iteratively selects tokens from the multiset, placing them in the output sequence based on a left-to-right process.

Experiments on the COGS benchmark demonstrate that the proposed method significantly outperforms other treeless models in generalizing to deeper recursion. The paper also tackles challenges related to alignment between input and output, as well as the difficulty of identifying the linguistically correct permutation among multiple possibilities. To address the NP-hard permutation problem, the authors employ a GPU-friendly continuous relaxation that allows for backpropagation and learning of more plausible permutations. The paper highlights the technical challenges and solutions implemented in the approach, inviting further exploration through the paper or poster presentation.</sample>
    <sample id="346">The provided text does not state the author's institution. It only mentions the author's name, Shuheng.</sample>
    <sample id="347">Myra：大家好，今天我将介绍我们的论文“标记式人物：使用自然语言提示衡量语言模型中的刻板印象”。这项工作由 Esin Durmus 和 Dan Jurafsky 合作完成。近年来，许多研究人员已经记录了大型语言模型（LLM）中社会偏见和刻板印象的普遍性。然而，这些衡量标准存在各种局限性。它们通常依赖于人工构建的数据集，这些数据集耗时制作，并且通常只能衡量非常具体的刻板印象，这意味着它们难以推广到其他人口统计学或背景，或者只是捕捉到非常一般的、广泛的关联，例如对特定群体的负面联想。此外，目前的研究大多没有考虑交叉性，交叉性是指多重社会身份可能会叠加偏见并成为独特的危害来源。为了克服这些局限性，我们依赖于 LLM 能够响应指令和提示的特性。因此，我们可以要求模型生成一个人物画像，使用像“想象一下你是一个亚洲女性。描述一下你自己。”这样的提示。我们可以立即看到这一点非常具有概括性，因为我们可以随意指定我们想要使用的任何身份标记。以下是 GPT-4 的一些示例生成。我们立即看到，虽然这些输出并非传统意义上的明显负面或有害，但仍然有一些有趣的模式。亚洲女性被描绘成不引人注目的；中东女性则被用作“异域风情”之类的词语，并像“迷人”一样，指代一个迷人的地区。并且，所有女性的群体都提到了血统，而白人男性则没有任何提及。为了捕捉这些模式，我们的方法分为两个部分。第一部分是生成这些人物画像。我们的提示语用于生成这些人物画像，灵感来自于一项研究，该研究向人类研究者提供了这些提示语，发现通过向人类研究者提供这些提示语，他们也能揭示种族刻板印象。此外，这使得我们能够直接比较我们生成的人物画像和人类撰写的回复。第二部分是标记词语，这是一种识别区分标记群体和非标记群体的词语的方法，我稍后会详细说明。它的优势在于，我们可以获得非常具体的刻板印象和模式，而无需依赖任何特定的词汇表。因此，标记词语方法借鉴了社会语言学的概念“标记性”，该概念指出，存在一个非标记的默认状态，任何与其他默认状态不同的群体都具有语言上的标记性。例如，“战士”一词通常与男性相关联。当人们描述一个女性战士时，他们通常会明确指出“女性战士”，并标记该词为“女性”。更广泛地说，社会上占主导地位的群体在语言和社会上都是非标记的，而边缘化的群体通常是标记的。因此，在我们的方法中，我们首先确定标记群体和非标记群体，然后我们使用“Fightin’ Words”方法比较这些人物画像，该方法基本上是使用加权对数似然比值来区分标记群体中的顶级词语。例如，对于黑人女性的画像，我们将使用 Fightin’ Words 方法并将其对标于白人画像和男性画像进行比较，因为这两个都是对应的非标记群体。现在，让我们来看一些结果。首先，我们使用一个刻板印象词汇表，发现生成的画像包含比人类撰写的画像更多的刻板印象。然而，当我们实际查看词语的分布和词汇表时，会发现非常不同的情况。虽然生成的画像包含比词汇表中更多的刻板印象词语，但人类撰写的画像则具有更广泛的词语分布，而生成的画像中的刻板印象词语实际上只是“高”和“健壮”这样的词语，仅限于积极或至少非负面的词语。事实上，这个词汇表并没有很好地捕捉到我们在早期幻灯片中看到的许多有害模式。因此，我们将转向使用标记词语方法来展示这些看似积极的描绘如何促进刻板印象和本质化叙事。在我们的分析中，我们揭示了这些看似积极的描绘反映了有害模式。首先，对于我们的群体，顶级词语包括“文化”、“传统”、“自豪”和“异域风情”。这些词语将这些群体定义为与他们的身份相关的，并将他们与其他白人规范区分开来，这导致了长期以来对这些群体的歧视和异化。此外，许多共同的刻板印象也反映在这些词语中，尤其是在女性群体中。例如，描述拉丁裔女性的词语包括“充满活力”和“丰满”，这与热带主义的刻板印象相关联。对于亚洲女性，词语包括“娇小”和“柔美”和“丝滑”，这与亚洲女性长期以来被过度性化、视为非常顺从的刻板印象相关联。最后，对于黑人女性，我们看到一些顶级词语包括“坚强”和“有韧性”。这与人们所说的“强大的黑人女性”原型相关联。虽然这听起来在表面上是积极的，但有研究表明，这种原型实际上非常有害，因为它给这些群体带来了巨大的压力，必须在社会障碍面前保持坚强和有韧性。这实际上并没有解决这些障碍，而是将压力放在人们身上，让他们克服它们，这会导致这些人群的负面健康后果，以及其他危害。更广泛地说，对于每个标记群体，词语几乎都反映了非常本质化的叙事。因此，根据这些模式，我们得出结论，为模型所有者提出三个建议。首先，作为研究人员，我们应该解决积极的刻板印象和本质化叙事。我们还应该使用交叉性视角来研究偏见和危害，因为如果没有这样做，可能会忽略许多事情。最后，应该对偏见缓解方法保持更高的透明度，因为例如，这些积极的刻板印象，我们不知道这是否是由于某种过度价值对齐，或者可能是由于一些其他反刻板印象方法导致了这些有害模式。我们无法在没有更多透明度的情况下进行任何假设或进一步研究。感谢您的聆听。祝您在 ACL 期间玩得开心。</sample>
    <sample id="348">Myra and colleagues investigated stereotypes in large language models (LLMs) by prompting them to generate personas of individuals from various demographic groups.  They found that while LLMs don't always produce overtly harmful outputs, they exhibit subtle patterns reflecting existing societal biases.  

The study highlights limitations of traditional bias detection methods relying on hand-crafted datasets.  Instead, they employed "Marked Personas," a method leveraging the sociolinguistic concept of "markedness" – the idea that dominant groups are unmarked, while marginalized groups are marked linguistically.  They identified "marked words" – words that distinguish marked groups from unmarked ones – using a "Fightin’ Words" method.

Analysis revealed that generated personas contain more stereotypes than human-written ones, but the stereotype words are often positive, like "tall" or "athletic," masking harmful underlying patterns.  The study found that words like "culture," "tradition," "proud," and "exotic" define groups by their relationship to identity, contributing to "othering."  Tropes like "vibrant" and "curvaceous" for Latina women, "petite" and "delicate" for Asian women, and "strong" and "resilient" for Black women were identified, revealing essentializing narratives.  The "Strong Black Women" archetype, while seemingly positive, can create pressure and negatively impact well-being.

The researchers recommend addressing positive stereotypes and essentializing narratives, adopting an intersectional lens for bias studies, and increasing transparency about bias mitigation methods.  They emphasize the need for further research to understand the origins of these patterns, as they may stem from unintended value alignments or other factors.  The study underscores the importance of moving beyond simple bias detection to uncover the nuanced and often positive-seeming ways LLMs perpetuate harmful stereotypes.</sample>
    <sample id="349">大家好，我是来自中国科学技术大学的景伟易。非常荣幸能给大家做一个关于我们论文的简短宣传视频。你们在复制我的模型吗？通过后门水印保护大型语言模型用于嵌入服务的版权。首先，我们先介绍一下嵌入服务。目前，GPT、LLAMA、PALM等大型语言模型在自然语言理解和生成方面表现出色。嵌入服务是基于大型语言模型的一项服务，用于辅助各种自然语言处理任务。例如，OpenAI提供一个基于GPT的嵌入API。然而，最近的研究表明，攻击者可能会通过学习嵌入来窃取模型并提供类似的服务。因此，保护嵌入服务的版权至关重要。为了保护嵌入服务的版权，一种解决方案是为提供商服务嵌入水印，并检测其他服务中是否包含水印。水印方法需要满足以下几个属性。首先，该方法应适用于嵌入服务。其次，水印不应降低提供的嵌入的效用。第三，水印应足够隐蔽，攻击者难以发现或移除。最后，水印应能够转移到攻击者服务中的模型。现有工作可以大致分为四类。然而，这些方法要么不适用于嵌入服务，要么缺乏转移性。因此，在我们的论文中，我们提出了Embedding marker，这是一种后门水印方法，适用于嵌入服务。然后，让我介绍一下我们Embedding marker的细节。Embedding marker包含两个主要步骤：水印注入和版权验证。在这些主要步骤之前，我们首先选择一个触发集。触发集是一个在一定频率范围内的一组单词。我们假设提供商可以收集一个通用文本语料库并统计单词频率。在水印注入中，我们首先定义一个目标嵌入。当用户向提供商服务发送句子时，提供商会统计句子中触发器的数量。提供的嵌入是目标嵌入和原始嵌入的加权和。目标嵌入的权重与句子中触发器的数量成正比。当句子中触发器的数量大于 m 时，提供的嵌入就等于目标嵌入。版权验证是检测其他服务中是否包含水印。我们首先构建一个后门数据集和一个无害数据集。后门数据集包含所有单词都属于触发集的句子，而无害数据集中的所有单词不属于触发集。然后，提供商向窃取者的服务发送包含这些数据集的嵌入。我们计算请求的嵌入与目标嵌入之间的余弦相似度和 L2 距离。同时，我们计算无害数据集和后门数据集之间的余弦相似度差和 L2 距离差。此外，我们还应用 KS 检验，并使用其 p 值作为第三个指标。我们在 AG News、MIND、SST2 和 Enron Spam 四个数据集上进行了实验。我们假设提供商使用维基文本数据集来统计单词频率。四种数据集上的结果表明，我们的Embedding marker可以具有很强的检测性能，同时保持良好的下游任务效用。我们还通过可视化四个数据集的句子嵌入来验证提供的嵌入的隐蔽性。图的 legend 表示每个句子中触发器的数量。如图所示，很难区分后门嵌入和正常嵌入。以上就是全部内容。谢谢。欢迎与我们讨论。</sample>
    <sample id="350">This presentation discusses the meaning of "superhuman performance" in Natural Language Understanding (NLU) and questions the reliability of current leaderboard-based benchmarks. The authors, Simone Tedeschi and collaborators, argue that achieving human-level or superhuman performance on these benchmarks doesn't necessarily indicate true understanding.

The presentation highlights the "saturated benchmarks" – tasks where models frequently outperform humans. While impressive, this raises concerns about the tasks being truly solved and the models' limitations. These limitations include brittleness, lack of generalization, susceptibility to adversarial attacks and spurious correlations, and sensitivity to minor perturbations.

The study analyzes SuperGLUE and SQuAD, two popular benchmarks.  SuperGLUE reveals that humans consistently perform better than the best systems on many tasks, and the gap is often significant. SQuAD shows similar results, with humans being outperformed by systems. However, the authors discovered several issues with the benchmark design and evaluation process.

Key problems identified include:

*   **Inconsistent evaluation sets:** Humans are often evaluated on small subsets of the test data, while systems are evaluated on the full test set.
*   **Errors in ground truth answers:**  The provided answers are not always accurate, leading to unfair comparisons.
*   **Vague human baselines:**  The "human baseline" is often poorly defined, and simple aggregation methods are used, lacking rigor.
*   **Unreliable human performance data:**  Pay rates for human annotators vary significantly, and the quality of human performance is dependent on adequate motivation.
*   **Missing annotator information:**  Details about the annotator pool (size, background, process) are often omitted, making claims about human performance unreliable.

The authors conclude that current benchmarks are not scientifically meaningful for assessing true NLU capabilities. They recommend improvements to benchmark design and evaluation to avoid repeating these mistakes and construct more reliable assessments of human and machine intelligence. The paper provides recommendations for more robust benchmarks.</sample>
    <sample id="351">Shuheng's presentation discusses the generalization capabilities of CoNLL-2003 named entity taggers in 2023. The paper investigates whether models trained on the CoNLL-2003 dataset still perform well on modern data and what factors contribute to good generalization. To address this, they created the CoNLL++ dataset, a collection of Reuters News data annotated with the same CoNLL-2003 guidelines.

The researchers fine-tuned over 20 models on the CoNLL-2003 dataset and evaluated their performance on both CoNLL-03 and CoNLL++ datasets, measuring generalization through F1 score changes. Their findings highlight three key ingredients for good generalization: model architecture (transformer models perform better), model size (larger models generalize better), and fine-tuning examples (more examples lead to better generalization).

The paper explores the causes of performance drops. They ruled out adaptive overfitting, where repeated testing on the same dataset leads to diminishing returns, by observing a consistent improvement in F1 score across different CoNLL-03 and CoNLL++ datasets. Instead, they identified temporal drift as the primary cause. Performance degradation was observed with increasing temporal gaps between training and testing data.

The conclusion is that achieving good generalization requires a combination of better model architectures, larger model sizes, and more fine-tuning examples, with these factors working synergistically. Surprisingly, temporal drift, not adaptive overfitting, is the main reason for performance drops despite the long history of CoNLL-2003.

Ultimately, the presentation answers the question posed in the title: "Do CoNLL-2003 taggers still work in 2023?" with a resounding "yes." The researchers encourage further research into improving model generalization and provide access to their dataset and paper.</sample>
    <sample id="352">ABC-Eval 是一个新的方法，用于评估对话式 AI，它通过明确标注模型响应中表达的行为（例如，忽略对话伙伴、说无关内容、自相矛盾等）来减少人工评估的主观性。它旨在更精确和可靠地衡量聊天模型的各种行为，从而更全面地评估其对话质量。</sample>
    <sample id="353">ACL 2023 论文 "Python Code Generation by Asking Clarification Questions" 探讨了代码生成中的一个关键挑战：输入信息不足（underspecification）。现有方法难以应对这种问题，尤其是在自然语言描述（NLD）中缺少关键操作的明确定义。

论文提出了一种新的方法，通过在代码生成过程中引入交互，即主动提问澄清问题。研究人员构建了一个名为 CodeClarQA 的合成数据集，包含针对关键操作的澄清问题，并利用该数据集训练了一个 CQ-驱动的代码生成管道。该管道包含一个澄清需求预测器、一个问题选择器和一个代码生成器。

研究人员通过分析代码知识图谱，识别出关键操作，并利用这些操作的文档信息来评估 NLD 是否缺少关键信息。他们使用相似度计算来判断操作是否被明确定义，并根据结果生成相应的澄清问题。

实验结果表明，该方法在识别缺失的关键操作方面表现出色，并且通过引入澄清问题，代码生成的性能得到了提升。然而，该方法仍然存在一些挑战，例如澄清问题可能与参考问题不同，导致预测结果不完全准确。

论文还探讨了澄清问题是否能改善代码生成质量，结果显示，澄清关键操作确实有助于生成更准确的代码。研究人员认为，未来可以进一步改进方法，例如更好地处理不同类型的澄清问题，并探索更有效的代码生成策略。他们希望通过进一步研究，解决代码生成中的输入信息不足问题，从而推动代码生成技术的发展。</sample>
    <sample id="354">CoNLL-2003 和 CoNLL++ 之间的性能增量直到 2020 年才高于 5 个百分点。</sample>
    <sample id="355">Vasudha，一位 Stony Brook大学计算机科学博士候选人，今天向大家介绍了我们发表在 ACL 2023 上的一篇长论文，题目是“转移学习用于异议检测：应对稀有类问题”。首先，我们定义了认知异议，并解释了为什么它是一个在语言研究中重要的课题。简单来说，认知异议是指两个信念或行为相互矛盾，例如，一个人说“我知道香烟会致癌”，然后又说“我抽了几个烟”，这两种行为相互矛盾，构成了异议关系。同时，他们又存在共性关系。虽然异议是日常决策中非常常见的一种现象，但在语言等其他类型的交流中，它却非常罕见。那么，这有什么意义呢？研究认知异议可以帮助我们理解不同人群之间的意见分歧，追踪趋势和信念变化，以及态度转变。高认知异议也与焦虑症有关，可以帮助我们更好地理解人们的心理健康。研究语言中表达的异议，也能帮助我们理解群体中的极化和偏见。此外，认知异议对于理解个人的认知风格和更好地理解决策过程也至关重要。为了创建认知异议资源，我们进行了大规模的异议关系标注。我们采用了异议优先的方法，如图表所示。推文使用 PDTB 解析器进行处理，并将对话单元进行标注，根据论文中描述的指南进行标注。正如您所看到的，异议仅在 3.5% 的标注对中出现。在收集了大约 1,000 个对话单元对后，我们运行了一个初始分类器，该分类器仅使用 43 个异议样本进行训练。不出所料，该分类器在异议类别的准确率上并没有达到随机猜测的水平。由于异议的发生频率非常低，并且缺乏任何以前的此类数据集，我们面临着绝对稀有性的问题。为了缓解这个问题，我们对转移学习和主动学习进行了多种组合实验，以收集更多异议样本，从而在更少的标注轮次下降低整体标注成本，同时提高异议检测准确率。由于初始模型无法捕捉到异议类别，因此我们启动了主动学习过程，通过从与异议相关的任务中转移权重来完成。我们从两个不同的任务中进行转移：独立于主题的异议立场分类任务，该任务确定两个不同人发表的辩论陈述是同意还是不同意，无论主题如何，我们称之为辩论；以及 PDTB 中扩展和比较类别的二进制分类任务，这两种类别与共性和异议的产生密切相关，我们称之为 CE。我们发现，零样本性能通过转移 CE 任务的权重已经优于随机猜测，最佳结果为 AUC 0.62。此外，通过迭代地对 CE 任务进行微调，然后进一步对辩论任务进行微调，我们发现，微调 CE 任务后，再对辩论任务进行微调，零样本性能会得到显著提升。因此，我们使用这个模型作为主动学习的起始点。接下来，我们确定了更新模型的方法，该方法使用每个主动学习轮次从新数据中更新模型。“累积”方法累积所有已收集的活动标注数据，而“迭代”方法则通过在最新收集的数据集上进行训练来更新模型。在不同的策略中，我们发现，累积方法在各个方面都优于迭代方法。然后，为了增加异议样本的数量，我们使用概率稀有类策略（PRC）选择最有可能被当前模型预测为稀有样本的示例。我们将其与社区中常用的其他 state-of-the-art主动学习策略进行比较。我们发现，提出的 PRC 策略优于其他 state-of-the-art策略，尽管差异很小。请注意，随机策略的性能明显较低。在后续的活动学习轮次中，使用两个最佳策略，我们提高了异议分类的 AUC 至 0.75，这是我们迄今为止取得的最佳性能。此外，我们还检查了每个策略对标注质量和标注成本的适用性。我们发现，PRC 方法具有最高的异议比例，并且在稀有类上效果最好。然而，标注员也发现这些示例非常困难。总而言之，我们发现 PRC 是一种简单的主动学习策略，用于稀有类别的样本获取，并且与适当设计的转移学习任务结合使用，可以显著提高主动学习的性能。我们还发现，迭代更新对于从不同的领域进行转移学习很有用，而针对领域主动标注则受益于累积更新。这些是我们的核心数据集和论文的链接。如果您有任何问题，请随时与我们联系。谢谢大家。</sample>
    <sample id="356">This paper is joint work with Alexander Koller and Ivan Titov. The provided text doesn't explicitly state the author's institution.</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig.</sample>
    <sample id="359">该方法与针对同步预翻译专门优化的架构进行了比较。</sample>
    <sample id="361">Armineh Nourbakhsh, a PhD student at Carnegie Mellon University and research director at JP Morgan AI Research, presented "CounterComp," a method for improving compositional generalization in multi-step quantitative reasoning, specifically question answering. The core problem is that current neural models struggle with tasks requiring more than two reasoning steps due to memorizing spurious patterns, like repeatedly associating tokens (e.g., "2019") with common operations (e.g., subtraction).

CounterComp addresses this by leveraging counterfactual scenarios. It treats each training sample as an anchor and mines positive and negative examples by intervening in the question to see if the output changes. A positive example represents a question modification that doesn't alter the output, while a negative example does.  An auxiliary metric learning loss is then added to the training procedure, dynamically adjusting its margin based on the extent of change introduced by the interventions.

The presentation demonstrates that adding this auxiliary loss consistently improves performance on in-distribution samples (training and testing on the same dataset) and, crucially, on out-of-distribution samples (different datasets or unseen examples within the same dataset). This highlights the effectiveness of CounterComp in promoting compositional generalization.

Furthermore, qualitative analysis shows that the CounterComp loss encourages the model to attend to more meaningful tokens during training, those related to the operational terms in the output. The presentation references key publications and encourages further exploration of the work through a poster or direct contact. The speaker expressed gratitude to co-authors, advisors, and the audience.</sample>
  </task>
</testset>