<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">Il contenuto indica che i modelli linguistici sono addestrati su grandi quantità di dati web crawl, che includono notizie da fonti come il New York Times, il Los Angeles Times, The Guardian e Huffington Post.</sample>
    <sample id="1">McGill University, Mila, and Microsoft Research.</sample>
    <sample id="2">This paper introduces LayoutMask, a novel pre-trained model for Visually-rich Document Understanding (VrDU) that addresses the limitations of existing models in handling document layout. While previous approaches utilize global 1D token positions, LayoutMask leverages "local 1D positions" derived from in-segment token orders, enabling the model to infer global reading order through joint consideration of 1D, 2D positions, and semantic information.

To enhance text-layout interactions, LayoutMask employs two novel masking strategies within the Masked Language Modeling (MLM) objective: Whole Word Masking and Layout-Aware Masking. Whole Word Masking challenges the model to find context for masked words, promoting interaction between text and layout. Layout-Aware Masking prioritizes masking the first and last words of each segment, encouraging the model to consider context across segments.  Furthermore, the paper introduces Masked Position Modeling (MPM), a symmetric pre-training objective where the model recovers randomly masked 2D positions, fostering spatial reasoning and improving layout representation.

Experiments on datasets like FUNSD and SROIE demonstrate that Local-1D position outperforms Global-1D position, particularly in scenarios with complex layouts and misleading numerical entities. This highlights the effectiveness of LayoutMask in adapting to diverse document structures. The findings suggest that incorporating local layout information during pre-training significantly improves VrDU performance by promoting deeper text-layout interactions and enhancing the model's ability to understand visually rich documents. The paper provides insights into the benefits of local 1D positions and novel masking strategies for improving document understanding.</sample>
    <sample id="3">Ecco la traduzione del contenuto inglese in italiano:

Buongiorno! Benvenuti alla presentazione di DEPLAIN, un nuovo corpus per l'identificazione di testi in tedesco a livello di documento e di frase. Il mio nome è Regina Stodden e vi guiderò nella prima parte della presentazione. Definiamo prima la semplificazione del testo. La semplificazione del testo è un processo di adattamento di un testo per migliorarne la comprensione per un determinato gruppo di destinatari, come persone con difficoltà di lettura o madrelingua non tedeschi. Per addestrare un modello di semplificazione del testo, abbiamo bisogno di coppie di testi paralleli, ad esempio documenti o frasi. L'esempio qui, potete vedere una coppia di frasi parallele di una complessa frase tedesca e la sua traduzione in linguaggio semplice. Per semplificare la frase, sono possibili diverse tecniche, come potete vedere nell'esempio, come la sostituzione lessicale, la cancellazione di clausole, il riordinamento o l'inserimento di parole. Ora proponiamo il nostro nuovo corpus, DEPLAIN, perché negli ultimi anni ci sono stati alcuni problemi con i corpora esistenti. Ad esempio, questi corpora qui sono troppo piccoli per addestrare un modello di semplificazione del testo. Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere errati nei loro allineamenti. Pertanto, proponiamo il nostro nuovo corpus DEPLAIN, che è diviso in due sottocorpora: DEPLAIN-apa e DEPLAIN-web. DEPLAIN-apa è basato su testi di notizie. In DEPLAIN-apa, abbiamo allineato 483 documenti manualmente, il che si traduce in circa 13.000 coppie di frasi parallele. Per DEPLAIN-web, questo corpus include diversi domini e abbiamo anche allineato tutti e 750 documenti, sia manualmente che con metodi di allineamento automatico. In totale, otteniamo 30.450 coppie di frasi. Abbiamo analizzato un po' di più le nostre coppie di frasi, ad esempio, sul tipo di semplificazione. Come potete vedere qui, i testi biblici sono molto più semplificati rispetto, ad esempio, ai testi di notizie o ai testi per studenti di lingua. A tutti i livelli, in particolare per quanto riguarda la semplificazione lessicale, la semplificazione strutturale e il livello complessivo di semplificazione. Inoltre, potete vedere che il nostro corpus DEPLAIN ha una grande varietà di diverse trasformazioni di semplificazione. Ad esempio, nel corpus DEPLAIN-apa abbiamo molto più riordinamento e aggiunta di parole rispetto al corpus DEPLAIN-web. D'altra parte, nel corpus web abbiamo molto più riformulazione. Ora vediamo cosa possiamo fare con questo corpus. Ciao, sono Omar e ora parlerò dei casi d'uso del nostro dataset DEPLAIN. Per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico. Negli ultimi anni, ci sono stati molti metodi di allineamento, ma nel contesto delle traduzioni automatiche, dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre allineamenti di frasi in entrambi i documenti. Ma nel nostro caso d'uso, stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli che hanno la stessa lingua, hanno lo stesso contenuto, ma sono a un livello di complessità diverso. E ora, poiché abbiamo il dataset DEPLAIN, che ha allineamenti manuali delle frasi, possiamo utilizzare queste frasi come standard di riferimento per valutare alcuni dei metodi di allineamento proposti. Abbiamo apportato alcune modifiche ai metodi proposti e abbiamo pubblicato tutti gli adattamenti e i codici per eseguire i nostri esperimenti nel paper. Alla fine, abbiamo concluso che il metodo di allineamento automatico migliore da utilizzare per la semplificazione del testo in tedesco è il metodo MASSalign. E potete anche trovare il codice per eseguire questo metodo sui vostri documenti in paper. Il secondo caso d'uso che abbiamo mostrato nel nostro paper è un caso di semplificazione automatica del testo tramite l'ottimizzazione di modelli linguistici per produrre testo semplificato dal testo di input complesso. Abbiamo ottimizzato due diversi modelli. Abbiamo ottimizzato il modello long-mBART per produrre semplificazioni a livello di documento e abbiamo anche ottimizzato il modello base normalizzato mBART per produrre semplificazioni a livello di frase. Potete anche trovare tutti i checkpoint e potete approfondire i dettagli dei punteggi e delle metriche di valutazione dei nostri esperimenti nel paper. Abbiamo concluso che questa ottimizzazione di base poteva produrre o ottenere punteggi migliori rispetto ai punteggi di riferimento e abbiamo proposto questi risultati come benchmark di base per il problema della semplificazione automatica del testo in futuro. Grazie mille per la vostra attenzione e speriamo di incontrarvi durante la conferenza. Grazie.</sample>
    <sample id="4">Kayo Yin.</sample>
    <sample id="5">T5 XL model.</sample>
    <sample id="6">This work introduces a novel framework for multi-lingual and cross-lingual summarization termed "many-to-many summarization," aiming to build a single model capable of summarizing documents in any source language into any target language.  The authors analyze the differences between existing approaches – multilingual summarization (same input and output language), cross-lingual summarization (different input and output languages), and their proposed many-to-many approach.  A preliminary experiment on the WikiLingua dataset demonstrates that many-to-many summarization facilitates better knowledge transfer across languages compared to the other methods.

The authors propose PISCES, a pre-trained many-to-many summarization model utilizing a three-stage pre-training strategy: meta-pretraining for sentence generation from noisy data, cross-lingual pre-training for target language generation from noisy parallel data, and task-specific pre-training on pseudo many-to-many summarization samples.  Experimental results show PISCES outperforms baselines like mBART-50 and mT5.  Ablation studies and human evaluations further validate the effectiveness of each pre-training stage and the superiority of PISCES.  The paper highlights the potential of many-to-many summarization to address the challenges of cross-lingual summarization and offers a powerful pre-trained model for this task.</sample>
    <sample id="7">Sì, i tagger CoNLL-2003 funzionano ancora bene nel 2023.</sample>
    <sample id="8">Il metodo di valutazione umana proposto, ABC-Eval, riduce la soggettività dell'annotazione umana annotando esplicitamente se i modelli esprimono comportamenti specifici come informazioni irrilevanti, contraddizioni o errori di conoscenza. Questo approccio è considerato più affidabile e predittivo della qualità della conversazione rispetto ai metodi esistenti.</sample>
    <sample id="9">Il successo dell'attuale approccio scarsamente supervisionato si basa in larga misura sull'utilizzo di un set di dati di validazione pulito.</sample>
    <sample id="10">Il punteggio può essere migliorato se il modello linguistico ha accesso a una conoscenza di background parzialmente sovrapposta a quella degli annotatori, ottenendo un'accuratezza tra l'82% e l'87%. L'accuratezza scende al 60% se il modello ha solo accesso ai nomi delle entità.</sample>
    <sample id="11">```
This research investigates the capabilities of large language models (LLMs) in understanding humor, using the publicly available New Yorker Caption Contest dataset.  The study addresses the growing excitement around LLMs generating and explaining jokes, highlighting the discrepancy between their apparent ability and actual understanding.  The research operationalizes the contest data into three tasks: caption matching (identifying the correct caption), quality ranking (evaluating caption quality), and explanation generation (providing explanations for jokes).

A CLIP-fine-tuned model achieves 62% accuracy on caption matching, significantly lower than human performance (94%).  Even with human-authored image descriptions, GPT-4 struggles on these tasks, with human explanations consistently preferred.  The study demonstrates that while LLMs can generate text resembling jokes and even attempt explanations, they often lack true understanding, exhibiting factual errors and misinterpretations.  The research provides a valuable dataset and leaderboard for further exploration of humor understanding in LLMs, emphasizing the need for more nuanced evaluation methods beyond simple generation. The findings suggest that current LLMs are not yet capable of genuinely "understanding" humor, despite their impressive text generation abilities.
```</sample>
    <sample id="12">5</sample>
    <sample id="13">## Abstract

This presentation details research on adaptive inference methods for reducing the inference time of large language models (LLMs), specifically focusing on Multi Model and Early Exit approaches. Adaptive inference aims to optimize inference costs (time and money) by leveraging data complexity and utilizing low-capacity models for efficient sampling.  Multi Model stores multiple models with classifiers, sequentially running them until a classifier decides to halt. Early Exit employs classifiers at intermediate transformer layers, halting computation when a classifier predicts.

The study investigates the pros and cons of each method. Multi Model offers versatility but suffers from storage overhead and unnecessary computation of previous models. Early Exit provides faster inference and memory efficiency but faces a critical issue: conflicting gradients.  Each classifier updates model weights, potentially interfering with each other and degrading overall performance.  This "conflicting gradients" phenomenon is illustrated through a visual representation of backpropagation.

To address this, the research introduces SWEET (Separating Weights in Early Exit Transformers), a novel fine-tuning method that trains Early Exit architectures where each layer receives updates solely from the following classifier.  This separation avoids conflicting gradients.  Experiments comparing SWEET with both standard Early Exit and Multi Model demonstrate that SWEET significantly outperforms them, particularly in accuracy. While SWEET can negatively impact later classifiers in some instances, it consistently outperforms both methods in speed/accuracy trade-offs, especially for BERT-Large.  The findings highlight the existence of conflicting gradients in Early Exit training and provide a first comprehensive comparison of these adaptive inference techniques.  The SWEET method motivates future research into tailored fine-tuning algorithms for Early Exit architectures.</sample>
    <sample id="14">Adam Przepiórkowski e questo discorso riguarda la struttura di dipendenza della coordinazione. Come sapete, esistono diverse strutture di dipendenza ipotizzate da diverse teorie e approcci al corpus. Ad esempio, nelle dipendenze universali, la struttura della coordinazione "Lisa, Bart e Maggie" tale che il primo congiunto sia il capo dell'intera struttura coordinata. In questo caso, Lisa. Un approccio simile è assunto nella teoria del significato di Igor Mel'čuk, dove di nuovo, l'intera struttura coordinata è governata dal primo congiunto. Questi due approcci sono asimmetrici. Giusto? Individuano un congiunto. Ora, questi sono approcci asimmetrici alla struttura coordinata, come l'approccio praghese, l'approccio alla coordinazione governata dal congiunto nell'analisi delle dipendenze di Praga, dove le strutture coordinate sono governate dal congiunto. Quindi, otteniamo dipendenze dall'esterno a tutti i congiunti. E infine, c'è anche un approccio multi-capo, utilizzato, ad esempio, nella grammatica del linguaggio di Hudson, dove affermano che tutti i congiunti sono capi della struttura coordinata. Quindi, otteniamo dipendenze dal governatore a tutti i congiunti separatamente: Lisa, Bart e Maggie. L'obiettivo di questo articolo è quello di produrre un nuovo argomento a favore delle strutture simmetriche della coordinazione, come queste due, e contro le strutture asimmetriche della coordinazione, come queste due. L'argomento è basato sul principio della minimizzazione della lunghezza della dipendenza, che spiegherò sulla base di questi esempi. In inglese, come sapete, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli avverbi possono essere più lontani. "Marge ha letto ieri 'esso'". È corretto perché l'oggetto diretto è vicino al verbo, mentre "Marge ha letto ieri esso" è molto peggiore. Perché qui tra il verbo e l'oggetto diretto c'è un avverbio: "ieri". Tuttavia, questo effetto può essere mitigato quando l'oggetto diretto è molto pesante e molto lungo. Perciò, entrambi questi enunciati sono corretti. "Marge ha letto questo assolutamente affascinante libro sui bachi ieri". È ok il modo in cui, invece di "esso", abbiamo questo lungo NP. Ma è anche ok dire "Marge ha letto ieri questo assolutamente affascinante libro sui bachi". Il ragionamento qui è che questo è possibile perché anche se questa frase viola il principio grammaticale generale che gli oggetti diretti debbano essere vicini al verbo, soddisfa il principio della minimizzazione della lunghezza della dipendenza, che afferma che le dipendenze più corte sono preferite. Quindi, queste due alberi mostrano solo la lunghezza delle dipendenze cruciali, quelle che non sono costanti tra queste due strutture. Qui abbiamo una dipendenza da "ha letto" all'avverbio di lunghezza 7 misurata in parole e da "ha letto" a "libro" di lunghezza 4, quindi insieme sono 11. Quando si scambiano questi due costituenti, la somma di queste due dipendenze diventa 6. Quindi, invece di 11, 6 è molto più corto. Ecco perché suona abbastanza corretto. Viola un principio, ma soddisfa un altro. Ok, quindi ciò che abbiamo fatto è aver estratto varie statistiche sulla coordinazione dall'enhanced version del Penn Treebank e abbiamo visto l'articolo "Why wouldn't you use universal dependencies" e queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti di sinistra tendano ad essere più brevi. "Sale e pepe" e non "pepe e sale", misurati in sillabe, e anche l'osservazione fatta nell'analisi che questa tendenza cresce con la differenza di lunghezza. Quando la differenza tra le lunghezze dei due congiunti aumenta, il congiunto più corto preferisce essere il primo, più forte, giusto? Quindi, la proporzione è maggiore del congiunto di sinistra più corto. Ma ciò che è nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando il governatore è a sinistra o assente. Giusto? Il governatore è a sinistra in questo esempio "Ho visto Bart e Lisa", quindi se il governatore è a sinistra. È assente nel secondo esempio "Homer è venuto e ha starnutito". Abbiamo una coordinazione di due verbi e non c'è un esterno governatore. Quindi, in questi casi, il congiunto di sinistra preferisce essere più corto; la maggior parte della differenza più grande tra i due congiunti. Tuttavia, quando il governatore è a destra, come qui, "ha esclamato" governa la coordinazione Ted e Ned, questo effetto scompare. Abbiamo dimostrato che misurando la lunghezza in caratteri, nella prima colonna, in sillabe nella colonna centrale e in parole nella colonna destra. Quindi mi concentrerò sulla destra. Ciò che vediamo qui è che quando il governatore è a sinistra, la tendenza per il congiunto di sinistra a essere più corto cresce costantemente, con la differenza assoluta in parole, e la stessa è osservata quando non c'è un governatore, come nella coordinazione di frasi. Tuttavia, quando il governatore è a destra questa tendenza scompare. E dimostriamo nell'articolo come questo fornisca un argomento contro le strutture asimmetriche della coordinazione, come queste due, e a favore delle strutture simmetriche, come queste due. Quindi guardate l'articolo per l'intero argomento. E parlate con noi alla sessione poster. Grazie.</sample>
    <sample id="15">Tre.</sample>
    <sample id="16">I testi biblici risultano più semplificati rispetto ai testi di notizie o ai testi per studenti di lingua.</sample>
    <sample id="17">This paper introduces a novel approach to multimodal relation extraction (MRE) addressing limitations of existing methods. While MRE leverages visual information to enhance relation detection, it often suffers from over-utilization of text-specific information and under-exploitation of external knowledge like topic information.  The proposed method tackles these issues by employing a Graph Information Bottleneck (GIB) principle to guide fine-grained information pruning within a unified cross-modal graph (CMG) constructed from textual and visual scene graphs. This pruning focuses on filtering nodes and edges to optimize the CMG structure.  Furthermore, the method incorporates multimodal topic information to enrich the context by retrieving and integrating relevant topic keywords using attention mechanisms. 

Experiments on a standard MRE dataset demonstrate significant performance improvements over text-based methods and multimodal baselines. Ablation studies reveal the importance of both information screening and external knowledge exploitation.  The analysis of performance based on text-vision relevance indicates that internal information screening is crucial for high-relevance inputs, while external knowledge exploitation is more beneficial for low-relevance inputs.  The proposed framework, combining GIB-guided pruning and multimodal topic enrichment, offers a more robust and effective solution for MRE, achieving state-of-the-art results while addressing key challenges in the field.  The paper concludes with a call to action for further exploration of this approach.</sample>
    <sample id="18">"Salt and pepper" è preferibile a "pepper and salt" in termini di lunghezza delle parole.</sample>
    <sample id="19">Zhang Qin from Shenzhen University presents their work, "A Survey for Efficient Open Domain Question Answering," accepted at ACL 2023. The presentation focuses on the challenges of open-domain question answering, particularly the large size of the Wikipedia corpus (26 million documents, 20 GB) and the computational cost of indexing and searching (65 GB index).  The motivation is to develop efficient systems with smaller memory footprint, faster inference, and comparable performance to existing models.

The presentation surveys existing approaches, including two-stage models (retrieval and reader) and one-stage models (retrieval-only and generator-only).  It highlights techniques for efficient evidence retrieval (approximate nearest neighbor search), fast reading (skip reading), and index size reduction (document filtering, embedding compression).  Furthermore, it discusses model size reduction strategies like lightweight models, parameter sharing, and knowledge distillation, and the possibility of combining retrieval and reading into a single model.

The presentation analyzes the trade-offs between speed, memory, and performance across different model architectures. Retrieval-only systems are fast but require large indexes, while generator-only systems are efficient but often rely on large models and lower performance.  The conclusion suggests that resource constraints favor generator-only systems or embedding compression, while real-time feedback benefits from retrieval-only systems.  Future work directions include deployment on low-power devices and the development of more comprehensive evaluation metrics.</sample>
    <sample id="20">Sì, i modelli pre-addestrati da DrBERT sono disponibili gratuitamente su Hugging Face, sotto licenza MIT, e i script di addestramento sono disponibili sul repository GitHub.</sample>
    <sample id="21">DEPLAIN-apa contiene documenti di notizie.</sample>
    <sample id="22">*   **Architettura del modello:** I modelli Transformer generalizzano meglio.
*   **Dimensione del modello:** Modelli più grandi generalizzano meglio.
*   **Numero di esempi di fine-tuning:** Più esempi di fine-tuning portano a una migliore generalizzazione.</sample>
    <sample id="23">This paper investigates the challenges text-to-image models face in rendering visual text, despite recent advancements in image generation. The research focuses on the Imagen model, which uses a T5-XXL encoder to represent text before feeding it to a diffusion model for image generation.  A key finding is that even simple textual inputs requiring text rendering often fail, highlighting limitations in the text encoder's ability to accurately spell words.

The study reveals that T5, utilizing SentencePiece tokenization, struggles with spelling, particularly for less frequent words, due to its subword representation.  PaLM models demonstrate superior spelling accuracy but at the cost of increased size and training data requirements.  ByT5, which operates at the character level, excels at spelling across all scales.

The research proposes an efficient strategy to improve text rendering by augmenting the Imagen model with a text representation derived from the smaller ByT5 model. This approach, which concatenates ByT5's character-level information to the existing text representation, significantly enhances the model's spelling ability without substantially increasing computational cost. While the diffusion model can still introduce errors, the combined approach leads to improved text rendering capabilities. The paper introduces the WikiSpell and DrawText benchmarks to evaluate text-only and text-to-image models, respectively, and presents a novel method for enhancing model spelling accuracy.</sample>
    <sample id="24">Il testo indica che la tendenza dei congiunti a sinistra a essere più brevi è stata misurata in base alla lunghezza in parole, sillabe e caratteri. La tendenza è più forte quando il governo è a sinistra o assente, e svanisce quando il governo è a destra.</sample>
    <sample id="25">Il progetto sperimentale ha coinvolto la misurazione della lunghezza delle dipendenze (in caratteri, sillabe e parole) per le coordinate in due scenari: quando il governatore è sulla sinistra e quando è sulla destra (o quando manca). I risultati hanno mostrato che la tendenza per il primo congiunto di essere più corto aumenta con la differenza di lunghezza quando il governatore è sulla sinistra e svanisce quando è sulla destra.</sample>
    <sample id="26">Il classificatore base, addestrato su dati non bilanciati, ha prestazioni inferiori a quelle casuali.</sample>
    <sample id="27">Shangbin è l'autore principale.</sample>
    <sample id="28">Javad Hosseini, Filip Radlinski, Silvia Pareti, Annie Louis, Bob, Alice.</sample>
    <sample id="29">MuDA benchmark mostra che i modelli sensibili al contesto sono significativamente più accurati per i fenomeni di formalità e coesione lessicale, ma non molto migliori per fenomeni come ellissi, pronomi e forma verbale.</sample>
    <sample id="30">LLM-Blender è un framework di ensemble learning semplice ed efficace per modelli linguistici di grandi dimensioni (LLM). Il framework si basa su un approccio a due fasi: PairRanker e GenFuser. PairRanker confronta pairwise diversi LLM per un dato input, utilizzando cross-attention per distinguere la qualità di ciascun modello. Questo permette di ottenere un ranking dei modelli in base alla loro performance sull'input specifico. GenFuser seleziona le prime tre posizioni del ranking e le utilizza come input a un modello di generazione sequenza-a-sequenza per produrre l'output finale.

A differenza dei metodi esistenti, PairRanker codifica le coppie di modelli insieme all'input, consentendo un'analisi più approfondita delle loro differenze. I risultati sperimentali dimostrano che PairRanker è significativamente correlato con il ranking corretto e supera altri metodi di ranking.  Per valutare l'efficacia di LLM-Blender, è stato creato un nuovo dataset chiamato MixInstruct, composto da istruzioni esistenti e modelli open-source.  L'utilizzo di metriche automatiche (BERTScore, BLUERT, BARTScore) e giudizio umano (ChatGPT) conferma che LLM-Blender migliora la performance rispetto ai modelli individuali, in particolare Open Assistant e Vicuna, in un'ampia percentuale di casi.  Il codice sorgente e il dataset MixInstruct sono disponibili pubblicamente per la ricerca futura.</sample>
    <sample id="31">John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams.</sample>
    <sample id="33">Il framework NLPositionality quantifica la posizionalità confrontando le annotazioni di utenti reali con i modelli e i dataset utilizzando il Pearson's R correlation score. Questo differisce dalla letteratura sugli annotatori che si concentra sull'accordo degli annotatori, invece di confrontare gli utenti con i modelli e i dataset.</sample>
    <sample id="34">```
CREST: A Joint Framework for Rationalization and Counterfactual Text Generation

This work introduces CREST, a novel framework combining selective rationalization and counterfactual text generation to improve interpretability and downstream model performance.  CREST addresses the challenge of explaining classifier decisions by leveraging both faithful rationalization and human-aligned counterfactual examples. The framework consists of a counterfactual generation component that masks input text and prepends a gold label to generate counterfactual examples, followed by an editor model to fill in the masked portions.  Human evaluation demonstrates that CREST-generated counterfactuals are more valid and natural than those from existing methods.

Furthermore, CREST-Generation extends the framework by generating both rationalizations and counterfactual examples, enabling a shared rationalization process.  This approach, trained on diverse datasets, achieves state-of-the-art results on IMDB, with strong performance on contrastive and out-of-domain data.  The framework's rationales are evaluated for plausibility, forward simulability, and counterfactual simulability, revealing that CREST-Rationalization produces more plausible and simulable explanations.  The ability to leverage counterfactual examples during training leads to explanations that focus on the contrasting aspects of the input, enhancing model interpretability and potentially improving downstream performance.  CREST offers a controllable and effective approach to generating high-quality counterfactuals for enhanced model understanding.
```</sample>
    <sample id="36">## Abstract

This paper introduces Language-Specific Layers (LSLs) for improving multilingual machine translation (MMT) while maintaining constant inference costs. MMT offers advantages like scalability, speed, and improved performance for low-resource languages, but also suffers from limited capacity per language and increased training complexity.  LSLs address these limitations by incorporating a regular transformer layer per language, enabling selective activation at inference time.  

The core idea is to learn the optimal placement of these LSLs within the encoder, rather than relying on fixed positions.  A novel approach involves training a large model with shared and language-specific weights, allowing the model to learn the importance of each weight component for different layers.  This learned placement is then used to construct a final architecture.  Experiments on WMT21 news translation across 10 languages demonstrate that LSLs significantly outperform language adapters and even a large baseline model, achieving substantial improvements in translation quality, particularly for low-resource languages.  The learned architecture also maintains constant inference costs.  The paper provides detailed analysis of weight distributions and statistical validation of the improvements across a wide range of translation directions.  The findings suggest that LSLs offer a promising solution for enhancing MMT performance without compromising efficiency.</sample>
    <sample id="37">Lo studio precedente ha dimostrato che i soggetti umani, quando hanno ricevuto gli stessi prompt che venivano forniti all'LLM, hanno anche potuto rivelare stereotipi razziali.</sample>
    <sample id="38">The study used statistics extracted from the enhanced version of the Penn Treebank.</sample>
    <sample id="39">Adam Przepiórkowski.</sample>
    <sample id="40">Ecco le attività strettamente correlate alla dissonanza cognitiva menzionate nel testo:

*   **Stanze di accordo (consonance)**
*   **Espansione**
*   **Confronto**</sample>
    <sample id="41">PeaCoK is a novel Persona Commonsense Knowledge Graph developed by the Natural Language Processing Lab at EPFL University in collaboration with Sony Group Corporation. It addresses the challenge of sustaining coherent and engaging narratives by providing a structured representation of real-world personas and their interconnected knowledge.  PeaCoK contains 3,800 personas, 40,000 attributes, and 100,000 personal inferences, with 9,200 attributes linked to multiple personas, fostering rich interconnections.

The knowledge graph is constructed through three steps: persona selection from existing commonsense graphs, attribute induction from knowledge graphs and language models, and crowdsourced relation annotations using a human-AI majority voting scheme.  This approach achieves high-quality annotations with strong accuracy.

PeaCoK is used to train a BART-based common knowledge generator for persona attribute inference, outperforming large-scale pre-trained language models like GPT-3 and GPT-3.5 in automatic evaluation and human assessment.  Furthermore, PeaCoK enhances downstream narrative modeling by augmenting dialogue systems with persona-grounded knowledge.  Experiments demonstrate improved fluency, consistency, engagement, and persona expression in dialogue generation, particularly when leveraging shared attributes between speakers.  The study highlights the importance of interconnected persona knowledge for creating more consistent and engaging narratives.  PeaCoK offers a valuable resource for training reliable persona knowledge generators and advancing narrative modeling. The paper and GitHub repository are publicly available.</sample>
    <sample id="42">L'articolo è presentato da Shuheng.</sample>
    <sample id="43">```
The provided text does not explicitly state the number of authors involved in the paper. It only mentions Vasudha as a Computer Science PhD candidate at Stony Brook University. 
```</sample>
    <sample id="44">Il framework NLPositionality differisce dai lavori precedenti perché confronta gli utenti finali con i modelli e i dataset stessi, piuttosto che con solo gli annotatori. Questo approccio si concentra sulle previsioni e le etichette dei modelli e dei dataset, a differenza della letteratura precedente che si concentra sull'accordo degli annotatori o sulla distribuzione degli annotatori.</sample>
    <sample id="45">Il lessico degli stereotipi si sovrappone maggiormente alle generazioni di personas.</sample>
    <sample id="46">DeepL e Google Translate.</sample>
    <sample id="47">Shangbin, dottorando presso l'Università di Washington. Oggi presentiamo il nostro lavoro "Dal dati di pre-addestramento ai modelli linguistici alle attività a valle: tracciare le tracce dei pregiudizi politici che portano a modelli NLP ingiusti". I modelli linguistici sono addestrati su grandi quantità di dati web. I media di notizie politici sono ampiamente coperti nei dati di pre-addestramento. Secondo un sondaggio sul corpus C4, i media New York Times, Los Angeles Times, The Guardian, Huffington Post, ecc. sono ampiamente coperti nei dati di addestramento dei modelli linguistici. Ciò ha creato una sorta di benedizione e maledizione per le applicazioni dei modelli linguistici. Da un lato, sono stati in grado di imparare da diverse prospettive, celebrando la democrazia e la pluralità delle idee. Dall'altro, queste diverse opinioni politiche sono intrinsecamente socialmente influenzate e potrebbero portare a potenziali problemi di equità nelle applicazioni a valle. A tal fine, proponiamo di indagare sul percorso di propagazione dei pregiudizi politici dai dati di pre-addestramento ai modelli linguistici alle attività a valle, ponendoci le seguenti domande: Innanzitutto, come valutiamo il pregiudizio politico dei modelli linguistici e qual è il ruolo dei dati di pre-addestramento in tali pregiudizi? In secondo luogo, come si comportano i modelli linguistici con diversi pregiudizi politici nelle attività a valle e se ciò potrebbe portare a problemi di equità nelle applicazioni NLP? In particolare, abbiamo proposto di fornire ai modelli linguistici diversi formati di prompt utilizzando questionari politici come il Political Conference Test. Ciò garantisce che la nostra valutazione automatica sia ben fondata nella letteratura scientifica. I risultati preliminari dimostrano che innanzitutto i modelli linguistici hanno diversi pregiudizi politici. Occupano tutti e quattro i quadranti del panorama politico. Possiamo anche vedere che GPT-4 è il modello linguistico più liberale di tutti, e la serie GPT è generalmente più socialmente liberale rispetto alla serie BART e alle sue varianti. In secondo luogo, miriamo a indagare fino a che punto i pregiudizi politici dei modelli linguistici sono effettivamente acquisiti dai dati di addestramento. Quindi conduciamo un esperimento controllato ri-addestrando i modelli linguistici su sei diversi corpora partitici separati in notizie e social media, ulteriormente divisi in base al loro orientamento politico. Ri-addestrando i modelli linguistici su tali corpora partitici possiamo vedere che le coordinate ideologiche dei modelli linguistici cambiano di conseguenza. Ad esempio, per RoBERTa ri-addestrato su un corpus Reddit di orientamento sinistro possiamo vedere un significativo spostamento di pregiudizio politico verso il liberismo. E proviamo anche a indagare se i modelli linguistici possono acquisire la polarizzazione prevalente nella nostra società. Dividiamo i corpora di pre-addestramento in prima e dopo la presidenza del 45° presidente degli Stati Uniti e ri-addestriamo separatamente i modelli linguistici sui due diversi corpora temporali. Possiamo vedere che i modelli linguistici hanno generalmente un orientamento politico che è più lontano dal centro dopo il 2017. Ciò indica che i modelli linguistici possono anche acquisire la polarizzazione nella nostra società. Infine, valutiamo i modelli linguistici con diversi pregiudizi politici su applicazioni NLP che spesso coinvolgono modelli linguistici e che possono avere implicazioni molto significative. Vediamo che se valutiamo le prestazioni per categoria, cioè se separiamo le prestazioni in base alle diverse demografie o all'orientamento politico dei media di notizie, possiamo vedere un modello. Ad esempio, per la rilevazione di discorsi d'odio, i modelli linguistici di orientamento sinistro sono migliori nel rilevare i discorsi d'odio rivolti a gruppi minoritari socialmente, ma sono peggiori nel rilevare i discorsi d'odio rivolti a gruppi più potenti nella nostra società. Viceversa, i modelli linguistici di orientamento destro sono migliori nel rilevare i discorsi d'odio rivolti a bianchi e uomini, ma peggiori nel rilevare i discorsi d'odio rivolti a neri, LGBTQ+ e altri gruppi minoritari. Simili tendenze si verificano anche per la rilevazione di notizie false, dove vediamo che i modelli linguistici di orientamento sinistro sono migliori nel rilevare le disinformazioni provenienti dal loro opposto orientamento politico e viceversa. Mostriamo anche molti esempi qualitativi per far vedere che i modelli linguistici con diversi pregiudizi politici danno previsioni diverse su esempi di discorsi d'odio e disinformazione in base alle loro categorie sociali. Ci sono molti altri esempi nell'appendice per evidenziare che ciò indica che esiste un problema di equità molto pressante relativo ai pregiudizi politici dei modelli linguistici. Ad esempio, se i modelli linguistici di orientamento destro venissero ri-addestrati su discorsi d'odio o disinformazione o qualsiasi altra cosa e distribuiti su una popolare piattaforma di social media, ciò significherebbe che le persone con opinioni opposte potrebbero essere marginalizzate e i discorsi d'odio rivolti a gruppi minoritari potrebbero diffondersi senza alcun controllo. Ciò solleva un allarme per noi per riconoscere e affrontare i problemi di equità derivanti dai pregiudizi politici dei modelli linguistici. Quindi un breve commento. Vorremmo anche sottolineare il dilemma unico relativo ai pregiudizi politici dei modelli linguistici. È come tra Scylla e Charybdis. Se non sanifichiamo le opinioni politiche nei dati di addestramento dei modelli linguistici, il pregiudizio si propagherà dai dati di pre-addestramento ai modelli linguistici alle attività a valle, creando ultimately problemi di equità. Se proviamo a sanificare in qualche modo, rischieremo anche la censura o l'esclusione. È incredibilmente difficile determinare cosa sia effettivamente neutrale e che debba essere mantenuto nei dati di monitoraggio linguistico. È un po' come il problema del carrello elettrico. Ok, ottimo. Penso che questo sia tutto quello che ho da dire oggi. Grazie per il vostro tempo.</sample>
    <sample id="48">3</sample>
    <sample id="49">OPT e GPT-2 modelli sono stati valutati fino a 1024 token di lunghezza del contesto.</sample>
    <sample id="50">DEPLAIN is a new corpus for German text identification at both document and sentence levels, addressing limitations in existing corpora. The corpus comprises two subcorpora: DEPLAIN-apa, built from 483 manually aligned news texts (approximately 13,000 sentence pairs), and DEPLAIN-web, incorporating diverse domains with both manual and automatically aligned data (30,450 sentence pairs). Analysis reveals significant simplification variations across text types, with Bible texts exhibiting stronger simplification than news or language learner texts. DEPLAIN demonstrates a high variety of simplification transformations, including more reorderings and additions in DEPLAIN-apa compared to DEPLAIN-web, which features more rephrasings.

The corpus facilitates evaluation of automatic alignment methods for parallel German texts, with MASSalign identified as the best performing method. Furthermore, DEPLAIN enables fine-tuning of language models for automatic text simplification. Experiments with long-mBART and base mBART demonstrate that fine-tuning can achieve scores exceeding baseline performance, establishing a new benchmark for the field. DEPLAIN provides a valuable resource for researchers aiming to improve text simplification techniques in German, offering a diverse and high-quality dataset for both alignment and model training.</sample>
    <sample id="51">AltEntities Corpus includes data from music, books, and recipes domains.</sample>
    <sample id="52">La posizionalità è semplicemente le prospettive che le persone hanno a causa delle loro demografie, identità e esperienze di vita.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">## Abstract

This paper addresses the challenge of detecting cognitive dissonance in natural language, a phenomenon crucial for understanding human behavior, attitudes, and mental health. While dissonance is prevalent in daily life, it's rare to observe in language, hindering research into its linguistic expression. We introduce a large-scale annotation effort to create a cognitive dissonance resource, employing a dissonance-first approach. Initial experiments reveal a low detection rate, highlighting the problem of data scarcity.

To overcome this, we explore transfer learning and active learning strategies. We demonstrate that transferring weights from related tasks – stance classification and expansion/comparison – significantly improves zero-shot performance.  We then evaluate different active learning update methods, finding that cumulative updates outperform iterative updates.  Finally, we introduce a Probability-of-Rare-Class (PRC) strategy for selecting data for annotation, which proves effective in increasing the number of dissonance examples, although annotators report difficulty.  

Our results show that PRC is a simple yet effective active learning strategy for rare-class acquisition, particularly when combined with transfer learning. We also identify the benefits of cumulative updates for active annotations within the same domain.  These findings contribute to a more robust understanding of cognitive dissonance in language and provide valuable insights for future research in natural language processing and cognitive science.</sample>
    <sample id="55">Sì, EDAtt adatta un modello ST offline esistente, utilizzando solo un modello per ogni regime di latenza e gestendo la latenza tramite parametri.</sample>
    <sample id="56">L'articolo è presentato da Yusen Zhang di Penn State University. Non viene menzionato il numero di autori.</sample>
    <sample id="57">Sì, il modello testato funziona sulla suite di test KITMUS.</sample>
    <sample id="58">Le tre varianti di KITMUS sono:

1.  Background-Pretrain
2.  Background-Both
3.  Background-Inference</sample>
    <sample id="59">DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains

This presentation introduces DrBERT, the first open-source biomedical language model in French, built upon the RoBERTa architecture and pre-trained on the NACHOS dataset of medical crawled data.  The work addresses the scarcity of specialized language models for French in the biomedical field, a gap exacerbated by the lack of in-domain data.  We compare DrBERT with the ChuBERT model, a clinical model trained on anonymized hospital data, to investigate optimal data sources for pre-training.  Furthermore, we explore the impact of data quantity through a series of four-from-scratch models, varying in training data size (4GB, 7GB) and combining NACHOS and clinical notes.  We also analyze the effectiveness of continual pre-training using CamemBERT and PubMedBERT as base models.

A total of seven models were evaluated on 11 downstream tasks (named entity recognition, classification, POS tagging, question answering) against six baseline models (CamemBERT, PubMedBERT, BioBERT, ClinicalBERT).  Results indicate that task-specific data yields the best performance, but heterogeneous data demonstrates greater versatility.  From-scratch pre-training generally outperforms continual pre-training, with CamemBERT weight-based models exhibiting stability issues.  DrBERT achieved superior performance on nine of the eleven downstream tasks, surpassing the general-purpose CamemBERT model.  The study highlights the importance of specialized data, but notes that its benefits diminish with increasing data volume.  The pre-trained models and training scripts are publicly available on Hugging Face and GitHub, respectively.</sample>
    <sample id="60">Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="61">Qual è l'ultima domanda di ricerca?

Should we only use the clean samples for validation, or are there better ways to utilize them?</sample>
    <sample id="62">## Abstract

This paper presents a systematic study of knowledge distillation for natural language generation (NLG) systems, addressing the growing need to compress large language models without sacrificing performance.  The authors explore various approaches to NLG compression, focusing on realistic industry-driven scenarios characterized by medium-resource labeled data, abundant unlabeled data, and a priority on inference time efficiency.  The study investigates architectural decisions (encoder/decoder vs. decoder-only), the impact of pruning techniques, and different knowledge selection strategies.

A key contribution is a comprehensive analysis of pseudo-target training in NLG.  The paper challenges traditional sequence-level distillation using beam search, demonstrating the crucial role of unlabeled data and the benefits of generating multiple, diverse pseudo-targets through sampling.  Furthermore, the authors introduce "joint-teaching," a novel technique combining word-level knowledge distillation on pseudo-targets generated by both the teacher and student. This approach aims to mitigate student exposure bias, promote grounded learning, and enable the student to correct its own errors.  

The study evaluates the effectiveness of these techniques across four NLG tasks: summarization, question generation, common sense reasoning, and simplification/style transfer.  By conducting a systematic investigation, the authors provide valuable insights into optimizing knowledge distillation for practical NLG applications, ultimately offering a "recipe" for effective model compression. The paper emphasizes the importance of considering task-specific characteristics and leveraging unlabeled data to achieve high compression rates while maintaining strong performance.</sample>
    <sample id="63">La sensibilità misura la capacità del modello di produrre consistentemente gli stessi output per la stessa attività, indipendentemente da lievi variazioni nella formulazione dell'istruzione.</sample>
    <sample id="64">Jingwei Yi.</sample>
    <sample id="65">```
Una maggiore sensibilità indica una performance del modello peggiore.
```</sample>
    <sample id="66">This paper surveys the field of deep learning for mathematical reasoning, highlighting its significance as a fundamental aspect of human intelligence.  The research explores various facets of mathematical reasoning, including text-based problems with arithmetic operations, multimodal problems involving images and tables, and automated theorem proving.  The paper discusses the evolution of neural network architectures for this domain, from sequence-to-sequence models and sequence-to-tree models to the recent advancements in large language models (LLMs).  

LLMs show promise in solving math word problems, particularly when guided by chain-of-thought prompting. However, they face limitations in precise mathematical reasoning.  The paper introduces techniques like self-consistency and program-aided LLMs to address these challenges.  Furthermore, it explores the development of datasets for low-resource languages and specialized domains like finance, science, and medicine.  The paper concludes by noting the ongoing challenges in generalization and robustness, particularly concerning large numbers and consistency issues within LLMs.  The work underscores the potential of deep learning to advance mathematical reasoning, while acknowledging the need for continued research to overcome existing limitations.</sample>
    <sample id="67">This paper investigates the phenomenon of interference in multilingual translation models, exploring the factors contributing to both synergy and negative effects when translating between different language pairs. The study identifies that severe interference is primarily observed in small models when data scarcity limits their capacity.  A key finding is the crucial role of temperature scaling during decoding, with tuned temperature values significantly improving performance. 

The research analyzes the impact of various factors, including language similarity and the number of languages, finding that language similarity has a minimal influence on interference levels.  The study demonstrates that the effect of the number of languages is also relatively weak, with performance stabilizing as the number of languages increases.  

The authors propose that a simple yet effective strategy for mitigating interference is to utilize temperature sampling, particularly with values greater than 1, to encourage the model to consider a wider range of training examples.  They argue that a baseline performance is achieved through modest model scale and appropriately tuned temperature, suggesting that specialized algorithms are not always necessary to address interference in multilingual translation. The findings highlight the importance of model and data size in controlling interference, while emphasizing the value of temperature scaling as a practical solution.</sample>
    <sample id="68">Il contesto linguistico messo a disposizione dei modelli durante il pre-addestramento è principalmente costituito da dati di testo, come quelli presenti in Wikipedia.</sample>
    <sample id="69">In genere, si hanno bisogno di circa 20 campioni di convalida puliti per ottenere buone prestazioni in WSL.</sample>
    <sample id="70">Esin Durmus e Dan Jurafsky.</sample>
    <sample id="71">```
The AltEntities Corpus is a novel dataset for evaluating and advancing entity selection in conversational systems, particularly focusing on indirect referring expressions. This work addresses the challenge of users employing non-direct references when making choices, such as "the newer one" or "the song that's not energetic," which are crucial for natural and nuanced conversations.  The dataset comprises 6,000 alternative question-answer pairs across music, books, and recipes domains, annotated with 42,000 indirect referring expressions. 

The dataset utilizes a cartoon completion setup, where annotators respond to a dialogue context involving an alternative question between two entities.  The alternative questions are generated using a template based on Wikipedia entities, with varying sampling methods to promote disambiguation. To ensure annotator understanding, background knowledge is provided for each entity, including Google search links for songs and Wikipedia text and images for books and recipes. Annotators are then tasked with generating three to five indirect referring expressions to describe the entities.

Experiments with the T5 XL model demonstrate that accuracy significantly improves with access to partially overlapping background knowledge, reaching 82-87% compared to 60% with only entity names.  The findings highlight the importance of incorporating background knowledge for effective entity understanding in conversational AI. Furthermore, the dataset exhibits domain-generalizability, suggesting its applicability beyond specific domains.  The AltEntities Corpus provides a valuable resource for researchers aiming to improve entity selection and natural language understanding in conversational systems.
```</sample>
    <sample id="72">Il problema è che i modelli linguistici imparano dai dati web, che contengono opinioni politiche. Questi bias possono poi essere riprodotti e amplificati dai modelli, portando a risultati ingiusti in applicazioni come la rilevazione di discorsi d'odio o la verifica di notizie false. È difficile definire cosa sia "neutro" e rimuovere completamente i bias senza censurare o escludere determinate prospettive.</sample>
    <sample id="73">Akshatha</sample>
    <sample id="74">Dense-ATOMIC addresses the limitations of ATOMIC, a large-scale commonsense knowledge base, by constructing a densely-connected knowledge graph. ATOMIC suffers from sparse graph structures and limited multi-hop paths due to the B-to-A link structure, resulting in unsatisfactory knowledge coverage. Dense-ATOMIC overcomes these issues by incorporating B-to-B, A-to-B, and A-to-A links, enabling the creation of multi-hop paths.

The construction of Dense-ATOMIC involves normalizing tail events and training a relation prediction model, Rel-CSKGC. Rel-CSKGC predicts relations between head and tail events using RoBERTa embeddings and MaxPooling, addressing the sparsity problem and leveraging semantic information.  A novel Intra- and Inter-Cluster Completion Strategy is employed for training.

Extensive evaluations demonstrate that Rel-CSKGC outperforms existing relation prediction methods and translation-based methods in both automatic and human evaluations.  Dense-ATOMIC exhibits significantly higher knowledge coverage, particularly in 1-hop, 2-hop, and 3-hop paths. Furthermore, Dense-ATOMIC enhances the performance of COMET models, leading to more diverse results.  The paper also highlights the potential of Dense-ATOMIC for commonsense reasoning by showcasing the high aggregate of multi-hop paths achievable through random sampling.  The authors provide code and a website for further exploration.</sample>
    <sample id="75">JointProp è un nuovo framework di apprendimento semi-supervisionato per l'estrazione di entità denominate (NER) e l'estrazione di relazioni (RE). Il framework affronta la limitazione dei modelli supervisionati che richiedono ampie annotazioni e la mancanza di integrazione tra NER e RE negli approcci semi-supervisionati esistenti. JointProp propone di modellare le attività NER e RE propagando le etichette attraverso grafi eterogenei che rappresentano le interconnessioni tra dati etichettati e non etichettati.

Il framework è composto da quattro componenti principali: generazione di caratteristiche a livello di span, costruzione di grafi eterogenei, propagazione joint delle etichette e ottimizzazione del modello. La generazione di caratteristiche a livello di span utilizza rappresentazioni contestuali per span e span pairs. La costruzione del grafo eterogeneo sfrutta la vicinanza tra dati non etichettati e le relazioni tra dati etichettati. La propagazione joint delle etichette diffonde le etichette attraverso il grafo, raffinando le etichette candidate non etichettate. Infine, il modello viene ottimizzato utilizzando le etichette pseudo-etichettate e le prestazioni migliorate vengono valutate su quattro dataset, inclusi dataset joint-task. I risultati sperimentali dimostrano che JointProp supera i modelli di riferimento sia nei dataset joint-task che in quelli single-task, evidenziando i benefici della codependenza tra NER e RE.</sample>
    <sample id="76">L'infrastruttura di propagazione dei bias politici è il percorso da cui i bias politici iniziano a partire dai dati di pre-addestramento fino all'utilizzo dei modelli linguistici per compiti a valle. Questo percorso include:

1.  **Dati di pre-addestramento:** I modelli linguistici vengono addestrati su grandi quantità di dati web, che spesso includono una rappresentazione di diverse prospettive politiche.
2.  **Modelli linguistici:** I modelli linguistici stessi assorbono i bias presenti nei dati di pre-addestramento, sviluppando una tendenza politica.
3.  **Compiti a valle:** Quando i modelli linguistici vengono utilizzati per compiti specifici (come la rilevazione di discorsi d'odio o la rilevazione di notizie false), i bias politici possono manifestarsi, portando a risultati iniqui.</sample>
    <sample id="77">This paper introduces DeFacto, a new dataset for improving factual consistency in abstractive text summarization. The dataset comprises human demonstrations and feedback on system-generated summaries, addressing the critical challenge of ensuring summaries accurately reflect the source documents.  The work proposes three novel Natural Language Generation (NLG) tasks: summary editing, feedback generation, and automatic factual error correction, providing strong baseline models for each.  These tasks leverage human feedback to refine summaries and improve factual accuracy.

The dataset was created by annotators correcting initial summaries from the Pegasus model on the XSum dataset, a widely used summarization benchmark.  Annotations include factual error labels, human-edited summaries, and detailed instructions, explanations, and evidence for the corrections.  Analysis reveals that a significant portion of XSum summaries contain factual errors, and human-edited summaries often exhibit lower textual overlap with the original summaries.

Experiments demonstrate that both fine-tuned and zero-shot large language models can effectively utilize human feedback for summary editing.  Feedback generation remains a challenging task.  Furthermore, the paper shows that an editor model can achieve competitive performance with fewer data points when trained to generate explanations alongside factual corrections.  DeFacto's fine-grained annotations offer valuable resources for training factuality metrics and meta-evaluation. The dataset is publicly available on GitHub.</sample>
    <sample id="78">Sì, DEPLAIN-apa e DEPLAIN-web differiscono nel tipo di semplificazione. DEPLAIN-apa mostra una semplificazione più forte, soprattutto per i testi biblici, mentre DEPLAIN-web mostra una maggiore varietà di trasformazioni, come riordinamenti e aggiunte di parole, rispetto a riformulazioni.</sample>
    <sample id="79">No, CoScript is not publicly available. The paper focuses on creating and using the dataset, but it doesn't provide access to the dataset itself.</sample>
    <sample id="80">La filigrana viene inserita nel testo calcolando il numero di parole nel set di trigger presenti nella frase dell'utente. Questo numero viene utilizzato come peso per sommare l'embedding target all'embedding originale, creando così un embedding "filigranato".</sample>
    <sample id="81">Yusen Zhang è affiliato alla Penn State University.</sample>
    <sample id="82">This paper introduces ULRA (Learning from Rank Aggregation), a novel framework for unsupervised automated essay scoring (AES).  AES aims to evaluate essay quality without human labeling, addressing the challenges of data scarcity and cost associated with supervised approaches.  Existing unsupervised methods struggle with inconsistent or uncontrollable quality signals. ULRA tackles this by aggregating multiple heuristic quality signals – such as unique word count – into a unified supervision for training a neural AES model.

The framework comprises a heuristic essay ranking module (HER alpha-shot) that generates partial-order pairs by ranking essays based on these signals.  A deep pairwise rank aggregation module (DPRA) then learns to aggregate these partial-order pairs, addressing inconsistencies between signals using a learnable confidence weight.  Finally, a scoring strategy transforms the model's predicted scores to fit a predefined score set.

Experiments on both transductive and inductive settings demonstrate that ULRA significantly outperforms existing unsupervised baselines, achieving competitive performance compared to cross-prompt and one-shot methods.  While ULRA surpasses unsupervised methods, its performance remains lower than supervised approaches due to the lack of strong supervision.  The paper highlights the effectiveness of ULRA in leveraging multiple heuristic signals to achieve robust and accurate unsupervised essay scoring.  The proposed framework provides a promising approach for practical applications where labeled data is limited.</sample>
    <sample id="83">Il modello codificatore-decodificatore, come mT5, può migliorare con l'addestramento su una combinazione di lingue.</sample>
    <sample id="84">PAD-Net: An Efficient Framework for Dynamic Networks addresses the issue of excessive parameter usage in fully dynamic networks, which limits their practical application. The paper investigates whether fully dynamic networks contain redundant parameters and explores the benefits of combining static and dynamic components. The authors hypothesize that partially dynamic networks can maintain or surpass the representation power of static networks.

PAD-Net introduces a framework that partitions network parameters into dynamic and static components, utilizing scale factors to control their intensity. An Iterative Mode Partition method is employed to identify and convert redundant dynamic parameters to static ones, minimizing their impact on the loss function. Experimental results demonstrate that PAD-Net achieves superior performance compared to static and fully dynamic networks while significantly reducing the number of parameters and computational cost.

Ablation studies reveal optimal dynamic ratios for Dynamic Convolution and Mixture of Experts, highlighting the importance of scale factors for both dynamic and static parameters. PAD-Net outperforms network pruning due to its preservation of static parameters and enhances output discrimination. Future work includes extending PAD-Net to other network architectures, hardware-friendly structures, and incorporating more parameter modes. The paper concludes that PAD-Net offers a promising approach to building efficient and effective dynamic networks.</sample>
    <sample id="85">"Make a chocolate cake" è un esempio di pianificazione linguistica vincolata.</sample>
    <sample id="86">Gli autori dimostrano la segretezza del loro metodo visualizzando le rappresentazioni degli embedding delle frasi su quattro dataset utilizzando PCA. Le figure mostrano che è difficile distinguere gli embedding con backdoor da quelli normali.</sample>
    <sample id="87">Il lavoro utilizza RoBERTa come base per costruire DrBERT, un nuovo modello linguistico pre-addestrato in francese per i domini biomedici e clinici.</sample>
    <sample id="88">GPT-4 è meno allineato con le persone non binarie rispetto a uomini e donne.</sample>
    <sample id="89">"If we receive a speech chunk containing "I'm going to talk about..." and our model predicts the translation in German, and we will look at the cross-attention weights, we'll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames."</sample>
    <sample id="90">This paper investigates the feasibility of using language learners as data annotators in Natural Language Processing (NLP), challenging the traditional reliance on native speakers.  The authors argue that, given the scarcity of native speakers for many languages, leveraging language learners offers a viable alternative. A controlled study was conducted across English, Korean, and Indonesian, utilizing sentiment analysis, NLI, and NER tasks from the GLUE benchmark.  Learners were categorized into basic, intermediate, and advanced levels based on CFR criteria, and compared to native speakers.  Participants completed a pre-test, annotated data with varying levels of support (dictionaries, machine translation), and then took a post-test to assess learning effects.

Results indicate that language learners produce nearly accurate labels, particularly for simpler tasks and questions.  Aggregated learner labels via majority voting achieve performance comparable to native speakers.  Furthermore, training language models on learner-annotated data yields performance close to ground truth, and in some cases, surpasses models trained on native speaker data.  The study demonstrates that data annotation can improve language proficiency and vocabulary/grammar for learners.  The authors conclude that language learners can significantly contribute to NLP data annotation, offering a novel approach to building benchmark datasets for low-resource languages.  This work aims to broaden NLP research by overcoming geographical and technological barriers.</sample>
    <sample id="91">Il modello migliora la performance con un numero maggiore di attività, e in questo modo la sensibilità diminuisce.</sample>
    <sample id="92">*   Standard seq2seq models
*   Tree-based models
*   Other treeless models on the COGS benchmark</sample>
    <sample id="93">Alexander Koller e Ivan Titov sono i supervisori del primo autore.</sample>
    <sample id="94">This paper addresses the critical issue of copyright protection for embedding-as-a-service platforms, which are built upon large language models like GPT, LLAMA, and PALM.  The increasing accessibility of these models makes it vulnerable for attackers to replicate services by learning from embeddings.  The proposed solution, "Embedding Marker," introduces a backdoor-based watermark method designed for embedding services. 

The method involves two key steps: watermark injection and copyright verification.  Watermark injection utilizes a trigger set of words, commonly found in general text corpora, to modulate the embedding.  The weight of the target embedding, representing the watermark, is proportional to the number of triggers present in a user's input sentence.  Copyright verification then detects the presence of the watermark in other services by comparing the embeddings of input sentences against a target embedding derived from the watermark.  A backdoor dataset, consisting of sentences containing only trigger words, is used to assess the similarity between the requested embedding and the target embedding.  The paper evaluates the effectiveness of Embedding Marker on four datasets (AG News, MIND, SST2, and Enron Spam) using cosine similarity, L2 distance, and a Kolmogorov-Smirnov test.  Experimental results demonstrate strong detection performance with minimal impact on embedding utility.  Furthermore, the covertness of the watermark is validated through visualization of embedding distributions using PCA, showing that the watermark is difficult to distinguish from normal embeddings.  The paper concludes that Embedding Marker offers a practical and effective approach to protect the copyright of embedding-as-a-service platforms.</sample>
    <sample id="95">The provided text does not mention the first author of PaLM. It only states that David Vilar is giving a review of the paper.</sample>
    <sample id="96">Ciao a tutti. Sono Jenny, una studentessa del dottorato di primo anno alla Carnegie Mellon University e oggi presenterò il nostro lavoro NLPositionality: caratterizzazione dei bias di progettazione di dataset e modelli. Questo lavoro è stato svolto in collaborazione con alcuni colleghi del University of Washington e dell'Allen Institute for AI, in particolare Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap. Quindi iniziamo immaginando che lavoriate per una testata giornalistica e stiate setacciando i commenti sotto un articolo di notizie per rimuovere contenuti tossici. Potreste rivolgervi a un'API popolare come Prospective API per il rilevamento della tossicità, e questo funziona davvero bene se siete Carl Jones. Dove Prospective API è in grado di rilevare correttamente istanze tossiche. Ma questo non è il caso di Aditya Sharma. Dove Prospective API non è affatto così sensibile a termini offensivi più comuni nei contesti indiani. Questo è un esempio di un bias di progettazione dove osserviamo differenze di prestazioni sistematiche tra tecnologie e popolazioni. I bias di progettazione come quello che abbiamo appena visto possono verificarsi a causa della posizione degli NLP researchers e degli sviluppatori di modelli. Posizione è semplicemente le prospettive che le persone hanno come risultato delle loro demografie, identità e esperienze di vita. Questo è un concetto ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femministi e queer. E come ricercatrice, la posizione può influenzare il processo di ricerca e i suoi risultati e risultati perché può cambiare le decisioni che i ricercatori prendono. E quindi una domanda che le persone potrebbero porsi è, i dataset e i modelli hanno posizione? Non intendiamo dire che i modelli stessi o i dataset stessi hanno identità demografiche o esperienze di vita, ma aggregano giudizi e opinioni di persone reali e possono quindi rappresentare determinate posizioni rispetto ad altre. Tuttavia, precedenti lavori hanno suggerito alcune prove aneddotiche di avere posizione, come i divari culturali tra modelli e dataset, nonché definizioni teoriche di posizione dei modelli. Tuttavia, questi lavori non studiano realmente il confronto tra gli utenti finali e i dataset e i modelli stessi, e lo studio della posizione dei modelli e dei dataset è sempre più importante poiché le attività NLP diventano più soggettive e socialmente orientate, ed è difficile caratterizzare come queste posizioni siano distorte perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API. Quindi, per studiare la posizione dei dataset e dei modelli, confrontiamo le annotazioni con gli utenti reali con i dataset e i modelli esistenti. Lo facciamo attraverso il nostro framework NLPositionality. Il nostro framework funziona in due fasi principali. La prima fase è la ri-annotazione dei dataset con annotatori diversi. E dovremmo farlo escludendo le demografie degli annotatori originali dei dataset, perché di solito solo pochi annotatori annotano ogni istanza e le demografie raramente vengono raccolte e condivise. E quindi optiamo per ri-annotare i dataset per ottenere molti annotatori per ogni istanza e per ottenere un ricco set di dati demografici. Quindi prendiamo le annotazioni in base alle demografie e le confrontiamo con i modelli e i dataset utilizzando un punteggio di correlazione di Pearson, e quindi il nostro framework differisce dalla letteratura sull'inconsenso degli annotatori confrontando gli utenti finali con le previsioni e le etichette dei modelli e dei dataset, invece di guardare solo all'accordo degli annotatori o alle distribuzioni degli annotatori. Il nostro framework è in gran parte abilitato da Lab in the Wild e da una piattaforma di crowdsourcing online per HCI collaborator. Live in the Wild è una piattaforma di sperimentazione online dove possiamo reclutare volontari diversificati. Rispetto alle piattaforme come M Turk che in gran parte hanno partecipanti dagli Stati Uniti o dall'India e ulteriormente Live in the Wild è ancora in grado di ottenere dati di alta qualità. Ospitiamo 2 task su Lab in the Wild, uno dei quali è l'accettabilità sociale, e il modo in cui funziona è che i partecipanti leggeranno una situazione dal dataset Social Chemistry e poi scriveranno quanto una situazione sia socialmente accettabile. Successivamente, per rimanere coinvolti nello studio, possono confrontare le loro risposte con quelle di un'IA e di altri. Abbiamo quindi confrontato queste annotazioni con Social Chemistry, Delphi e GPT 4. Abbiamo quindi replicato un setup molto simile per il task di rilevamento della tossicità e del linguaggio d'odio, dove i partecipanti leggeranno un'istanza da Dynahate e scriveranno se pensano che sia un'istanza di linguaggio d'odio. Abbiamo quindi confrontato queste annotazioni con Dynahate, Perspective API, Rewire API, Hate Roberta e GPT 4. Il nostro studio ha raccolto oltre 16.000 annotazioni da oltre 1.000 annotatori provenienti da 87 paesi. Quindi ora siamo meglio equipaggiati per rispondere chi si allinea maggiormente con i dataset e i modelli NLP. Troviamo che esiste una posizione negli NLP. Ad esempio, troviamo che i dataset e i modelli sono più allineati ai paesi di lingua inglese. Per GPT 4, l'analisi dell'accettabilità sociale, troviamo che è più allineato ai paesi confuciani e di lingua inglese. Troviamo che Dynahate è anche più allineato ai paesi di lingua inglese. Troviamo anche un allineamento maggiore con le persone che hanno un titolo universitario. Quindi per GPT 4, nel task di accettabilità sociale, troviamo che è più allineato con le persone con un titolo universitario o un dottorato di ricerca e troviamo lo stesso per Dynahate che è più allineato con le persone con un titolo universitario. Tuttavia, quando i modelli e i dataset sono allineati a specifiche popolazioni, alcuni sono inevitabilmente lasciati indietro. Un esempio di ciò è che i dataset e i modelli sono meno allineati con le persone non binarie rispetto ai loro counterpart maschili e femminili. Lo troviamo nell'analisi dei task di accettabilità sociale di GPT 4 e Dynahate come bene. Dato che esiste una posizione negli NLP, cosa possiamo fare al riguardo? Quindi abbiamo alcune raccomandazioni per questo. La prima è tenere traccia di tutte le scelte di progettazione pertinenti durante il processo di ricerca. E la seconda è condurre la ricerca NLP con la lente del perspectivismo. La nostra terza raccomandazione è costruire dataset e modelli specializzati all'interno di 4 specifiche comunità. E un buon esempio di ciò è l'iniziativa Masakhani. Voglio enfatizzare che l'inclusione NLP non significa solo che tutte le tecnologie funzionino per tutti. Quindi questo conclude la nostra presentazione. Ma se volete saperne di più, non esitate a controllare il nostro dashboard per i risultati di analisi più aggiornati e il nostro paper. Grazie.</sample>
    <sample id="97">La relatrice menziona i seguenti problemi associati a SimulST:

*   Lunghe e complesse procedure di addestramento.
*   Addestramento e manutenzione di diversi modelli per diversi regimi di latenza.</sample>
    <sample id="98">```
Il documento evidenzia la difficoltà di mitigare i bias sociali e politici nei set di dati per l'addestramento dei modelli di NLP. L'approccio proposto è quello di affrontare il problema attraverso un'attenta analisi del flusso di bias, dalla raccolta dei dati di pre-addestramento al modello finale, e di valutare le conseguenze di diversi approcci di "sanificazione". Tuttavia, il documento sottolinea che la "sanificazione" dei dati può portare a rischi di censura o esclusione, rendendo difficile definire cosa sia "neutro". In sintesi, non esiste una soluzione semplice e la mitigazione dei bias è un problema complesso e delicato.
```</sample>
    <sample id="99">Ciao, sono Siyu Yuan da Fudan University. Sono qui per presentare il nostro lavoro "Distillare la conoscenza dello script dai grandi modelli linguistici per la pianificazione linguistica vincolata". Nella vita quotidiana, gli umani pianificano spesso le proprie azioni seguendo istruzioni passo-passo nella forma di script orientati agli obiettivi. Lavori precedenti hanno sfruttato i modelli linguistici per pianificare per obiettivi astratti di attività stereotipate come "preparare una torta". Hanno dimostrato che i grandi modelli linguistici possono decomporre efficacemente gli obiettivi in passaggi. Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione per obiettivi astratti di attività stereotipate. La pianificazione per obiettivi con vincoli specifici, come "preparare una torta al cioccolato", rimane poco studiata. In questo articolo, definiamo il problema della pianificazione linguistica vincolata che impone diversi vincoli agli obiettivi di pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici reali con molteplici vincoli. Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli. In questo articolo, valutiamo e miglioriamo la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici. Poiché non esiste un dataset di obiettivi specifici per supportare il nostro studio, dobbiamo acquisire questi obiettivi. Come mostrato nel grafico, estendiamo gli obiettivi astratti con vincoli multi-faccettati tramite dati acquisiti dall'uomo tramite InstructGPT. Campioniamo 100 obiettivi specifici e valutiamo gli script generati dai grandi modelli linguistici. Questa tabella riporta la precisione complessiva dei risultati. Scopriamo che tutti i modelli linguistici raggiungono risultati insoddisfacenti nella pianificazione per obiettivi specifici. Conduciamo un'analisi dettagliata per indagare sul perché i modelli imparano a fallire. I risultati mostrati nel grafico mostrano che la completezza semantica degli script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita. Esploriamo un argomento più specifico di categorie di vincoli definiti da wikiHow. Il heatmap nel grafico mostra che le prestazioni di pianificazione di InstructGPT variano notevolmente per gli obiettivi di diverse categorie. Studi precedenti hanno dimostrato che la qualità dell'output dei modelli linguistici presenta una grande variabilità, portando a prestazioni scadenti. Pertanto, adottiamo l'idea di "over-generare-then-filter" per migliorare la qualità della generazione. Innanzitutto, mostriamo i tipi di vincoli con esempi per InstructGPT e otteniamo obiettivi specifici in base agli obiettivi astratti di base. Quindi, InstructGPT genera K script per obiettivi specifici. Successivamente, viene sviluppato un modello di filtro per selezionare gli script fedeli. Convertiamo gli script e gli obiettivi in embedding di InstructGPT e calcoliamo i punteggi di similarità coseno come punteggi di similarità semantica. Inoltre, premiamo lo script che contiene le parole chiave dei vincoli di destinazione. Conserviamo solo lo script che ha il punteggio più alto per l'obiettivo di destinazione nel set di obiettivi. Con il nostro metodo, InstructGPT può generare script di qualità superiore. Il nostro metodo migliora notevolmente la capacità di pianificazione sia in termini di completezza semantica che di fedeltà ai vincoli. Poiché i grandi modelli linguistici sono costosi da distribuire, è essenziale consentire la capacità di pianificazione linguistica di modelli più piccoli e specializzati. La creazione del dataset è un passo essenziale per questo fine. Tuttavia, studi precedenti non consentono la pianificazione per obiettivi specifici e l'annotazione manuale dei dati è costosa. Pertanto, seguiamo l'idea della distillazione del conoscenza simbolica per distillare i dataset di pianificazione linguistica vincolata dai grandi modelli linguistici. Applichiamo il nostro metodo per costruire un dataset di pianificazione linguistica vincolata, chiamato CoScript. In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità del set di validazione e test, chiediamo a lavoratori crowdsourcing di trovare e rivedere i campioni errati. Il grafico mostra la distribuzione dei vincoli in CoScript. Scopriamo che CoScript mostra un'alta pluralità degli obiettivi specifici generati. Con CoScript possiamo provare modelli più piccoli ma specializzati per la pianificazione linguistica vincolata. Scopriamo che T5 fine-tunato su CoScript genera script di qualità superiore rispetto alla maggior parte dei grandi modelli linguistici, indicando che i modelli più piccoli possono superare i modelli più grandi quando adeguatamente addestrati su dataset appropriati. In sintesi, stabiliamo il problema della pianificazione linguistica vincolata. Valutiamo la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici e sviluppiamo un metodo "over-generare-then-filter" per i grandi modelli linguistici. Usiamo i grandi modelli linguistici per generare un dataset di script di alta qualità, CoScript, per la pianificazione linguistica vincolata. Speriamo che il dataset CoScript possa essere una risorsa preziosa per far progredire la ricerca sulla pianificazione linguistica. Grazie per il vostro tempo. Si trovano maggiori dettagli su CoScript nel nostro articolo.</sample>
    <sample id="100">PromptRank è un approccio data-efficient per il retrieval di chain in Multi-Hop Question Answering (QA).  Si differenzia dai metodi tradizionali che richiedono grandi quantità di dati di training, raggiungendo prestazioni competitive con soli 128 esempi. L'idea chiave è combinare un retrieval unsupervised (TF-IDF e hyperlink traversal) con un reranker basato su modelli linguistici a pochi esempi.

Il sistema costruisce un prompt che inserisce i documenti della chain, un token indicatore e un'istruzione (es. "Read the previous documents and ask a question").  L'istruzione guida il modello linguistico a ragionare sui documenti della chain.  Vengono esplorate tecniche come la ricerca di istruzioni ottimali e l'aggregazione di punteggi con diverse istruzioni.

PromptRank supera i sistemi fully supervised come DrKit e si avvicina alle prestazioni dei dense retrievers state-of-the-art.  L'ablatio conferma l'importanza di ciascun componente.  L'utilizzo di PromptRank come retriever migliora significativamente le prestazioni downstream, con un leggero svantaggio rispetto a MDR.  La likelihood della domanda data la chain si rivela un buon punteggio, superiore al reverse likelihood.  In sintesi, PromptRank dimostra l'efficacia dei modelli linguistici per il ranking a pochi esempi di path multi-hop, offrendo un'alternativa data-efficiente ai metodi tradizionali.</sample>
    <sample id="101">La fluidità di PaLM è comparabile ai sistemi all'avanguardia.</sample>
    <sample id="102">Un metodo di filigrana deve essere:

1. Applicabile ai servizi di embedding.
2. Non degradare l'utilità degli embedding forniti.
3. Sufficientemente nascosto per evitare la rimozione da parte degli attaccanti.
4. Trasferibile ai servizi degli attaccanti durante il processo di estrazione del modello.</sample>
    <sample id="103">14 diverse lingue.</sample>
    <sample id="104">Il set di dati viene riannotato con oltre 16.000 istanze da oltre 1000 annotatori da 87 paesi.</sample>
    <sample id="105">Cosine similarity difference e L2 similarity difference.</sample>
    <sample id="106">QUEST is a new retrieval dataset designed to evaluate systems handling information needs with implicit set constraints, exemplified by queries like finding a specific reptile species or historical fiction novels set in France. The dataset comprises over 3,000 entity-seeking queries containing these constraints, verified for relevance and annotated with attributable spans for different query conditions.  The creation of QUEST involved generating queries from Wikipedia categories across four domains (films, books, plants, and animals) and employing human annotators to paraphrase, validate fluency, and assess the relevance of answer entities and supporting evidence within documents.

The dataset presents a challenging retrieval problem, requiring systems to search large corpora for multi-answer sets where evidence for relevance can be distributed across document spans.  The paper demonstrates a significant gap in retriever performance, particularly for queries involving set intersection and difference, as measured by MRecall@100.  End-to-end system performance remains low, highlighting the difficulty in effectively addressing these complex information needs.  The research underscores the need for improved systems capable of handling selective information needs and provides a valuable resource for future research in this area. The authors encourage readers to explore their paper and attend their presentation at ACL.</sample>
    <sample id="107">I modelli basati su codificatori multilingue, come mBART e mT5, hanno ottenuto le migliori prestazioni su tutti e nove i dataset. Tuttavia, l'addestramento in un mix di varie lingue ha migliorato sia gli encoder-decoder che gli encoder-PTR, con alcune eccezioni in cui l'inglese ha subito un calo di prestazioni.</sample>
    <sample id="108">## Abstract

This paper investigates the robustness of language model acceptability judgments within the context of minimal pair paradigms, a method for evaluating language models based on acceptability judgments, including grammaticality and stereotypical acceptability.  Current minimal pair pipelines struggle to evaluate models across longer sequences, a crucial consideration given the increasing context window sizes of large language models.  

To address this, we revisit the minimal pair paradigm by simulating longer sequences through the creation of artificial sentence pairs. We explore three scenarios: using sentences from different datasets (mismatch), using sentences from the same dataset (match), and using sentences from completely unrelated domains (irrelevant).  

Our results demonstrate that language models exhibit relatively robust acceptability judgments when evaluating sentences from completely irrelevant domains, even with context lengths up to 1024 tokens. However, when sentences are drawn from the same dataset, the model's judgments significantly shift based on whether the prefix is acceptable or unacceptable. This effect is consistent across context lengths and suggests that models are sensitive to shared syntactic and semantic features. 

We further analyze the sensitivity of the model to input perturbations, finding that the model's response remains consistent regardless of the type of perturbation.  The key takeaway is that current minimal pair evaluation methods may not fully capture the abstract knowledge of language models across extended contexts.  This work highlights the need for more sophisticated evaluation techniques that account for the model's ability to leverage shared linguistic information.</sample>
    <sample id="109">## Abstract

This paper introduces "Unnatural Instructions," a novel dataset of natural language instructions and their corresponding inputs and outputs, generated entirely through a fully automatic process without human annotation. Addressing the limitations of existing instruction tuning data, which are often restricted to academic benchmarks or require costly human annotation, Unnatural Instructions leverages a pre-trained language model (GPT-3 variant) to generate a diverse set of instructions and examples. 

The data generation pipeline involves prompting the model to create instructions and inputs, followed by generating alternative formulations of each instruction to enhance diversity. The resulting dataset comprises 64,000 examples, expanding to approximately 240,000 with paraphrases.  Evaluation reveals a high degree of correctness (over 50%) and the presence of highly creative and diverse tasks, extending beyond traditional NLP benchmarks. 

Fine-tuning an 11 billion-parameter T5 model on Unnatural Instructions demonstrates superior performance compared to existing instruction tuning datasets like T0++ and Tk-instruct across multiple benchmarks, including Super-Natural Instructions, T0, BIG-Bench Hard, and LMentry.  Furthermore, amortized training costs show that Unnatural Instructions outperforms a baseline model trained on Super-Natural Instructions.  This work highlights the potential of language models to generate high-quality, diverse instruction data, offering a more efficient and cost-effective alternative to human annotation while mitigating the predictability often found in crowd-sourced data.</sample>
    <sample id="111">Gli autori selezionano un insieme di parole a frequenza moderata (trigger set) analizzando un corpus di testo generale per contare la frequenza delle parole.</sample>
    <sample id="112">Ciao a tutti, mi chiamo Shuheng. Oggi presenterò il nostro articolo "I tagger CoNLL-2003 funzionano ancora bene nel 2023?". Iniziamo. Il nostro articolo ha indagato il problema della generalizzazione nell'ambito del Task di Named Entity Recognition (NER). Osserviamo che i modelli hanno utilizzato i dati CoNLL-2003 per sviluppare il NER per quasi 20 anni, il che solleva diverse problematiche. Innanzitutto, questi modelli generalizzano ancora ai dati moderni? E quando sviluppiamo nuovi tagger, cosa serve per una buona generalizzazione? Inoltre, se osserviamo una scarsa generalizzazione, quale causa la diminuzione delle prestazioni di questi modelli? Per indagare questi problemi, abbiamo sviluppato il dataset CoNLL++. Questo è un dataset che abbiamo raccolto da Reuters News dal 2020 e che abbiamo annotato utilizzando le stesse linee guida di annotazione CoNLL-2003. Abbiamo quindi fine-tunato oltre 20 modelli sui dati CoNLL-2003. Abbiamo valutato questi modelli sia sui test set CoNLL-03 che sul CoNLL++. Infine, abbiamo calcolato la percentuale di variazione dell'F1 per valutare la generalizzazione di ciascun modello. Quindi, cosa serve per una buona generalizzazione? Durante gli esperimenti abbiamo scoperto che ci sono tre ingredienti principali necessari. Il primo è l'architettura del modello. Dai nostri esperimenti abbiamo scoperto che i modelli transformer generalizzano normalmente meglio ai nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che in genere i modelli più grandi portano a una migliore generalizzazione. E, ultimo ma non meno importante, sappiamo che il numero di esempi di fine-tuning influisce direttamente sulle prestazioni di un task a valle. Qui abbiamo anche scoperto che più esempi di fine-tuning portano effettivamente a una migliore generalizzazione. Per la nostra prossima domanda, quale causa la diminuzione delle prestazioni di alcuni modelli? Abbiamo due ipotesi. La prima è l'overfitting adattivo, che è l'overfitting causato dalla riproduzione dello stesso test set ripetutamente e questo si manifesta in genere come un calo dei rendimenti su un nuovo test set. La seconda ipotesi è il drift temporale, che è la degradazione delle prestazioni causata dall'aumento del divario temporale tra i dati di addestramento e i dati di test. Per l'overfitting dei dati, abbiamo osservato che dal grafico sulla destra, la linea di adattamento migliore rossa ha una pendenza maggiore di uno. Ciò significa che ogni unità di miglioramento ottenuto sui dati CoNLL-2003 si traduce in più di un'unità di miglioramento sui dati CoNLL++ e questo indica che non si osserva il calo dei rendimenti. Ciò dimostra che in questo caso l'overfitting adattivo non è osservato. E per il drift temporale, abbiamo condotto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti e abbiamo scoperto che le prestazioni peggiorano con un divario temporale maggiore e questo conferma la nostra ipotesi che la principale causa della diminuzione delle prestazioni è il drift temporale. La nostra conclusione è che, per una buona generalizzazione, avremmo bisogno di un'architettura del modello migliore, di una dimensione del modello maggiore e di più esempi di fine-tuning. E questi vanno di pari passo, non possiamo avere solo un ingrediente e ignorare gli altri. Inoltre, abbiamo anche scoperto che la diminuzione delle prestazioni è causata dal drift temporale e, sorprendentemente, non è causata dall'overfitting adattivo, anche se i dati CoNLL-2003 sono stati utilizzati per oltre 20 anni. Tornando alla domanda che abbiamo sollevato nel titolo del nostro articolo "I tagger CoNLL-2003 funzionano ancora bene nel 2023?", e abbiamo scoperto che la risposta è un sonoro sì. Speriamo che il nostro articolo stimoli ulteriori ricerche su come migliorare la generalizzazione dei modelli. E, infine, vi invitiamo a consultare il nostro articolo, il nostro dataset e, se avete domande, non esitate a contattarmi. Grazie mille.</sample>
    <sample id="114">## Abstract: Finding the Pillars of Strength for Multi-Head Attention

Large language models (LLMs) offer revolutionary capabilities but suffer from limitations including heavy parameter counts, long training times, and large data requirements. This work addresses the parameter-intensive problem of LLMs by proposing a novel approach to optimize multi-head attention (MHA) through a "grouped head attention" strategy.  

Our method employs a divide-and-conquer approach, first grouping attention heads and training them with a combination of homogenization (intra-group similarity) and diversification (inter-group separation) objectives. This group-constrained training is followed by a "Voting-to-Stay" algorithm to prune redundant heads within each group, ultimately retaining only one head per group.

Extensive evaluations on machine translation, language modeling, and abstractive summarization tasks demonstrate significant performance gains (up to 4.4% BLEU improvement) and substantial parameter compression (up to 90% reduction).  Furthermore, we achieve a 32.1% parameter compression with comparable performance in the GHT-PS model.  Efficiency analysis reveals a "LITE" model achieving 90% parameter reduction, 62% faster inference, and 80% reduced FLOPs while maintaining performance.

We believe that task-specific automatic pruning, guided by the Lottery Ticket Hypothesis, holds great promise for further reducing the size and computational cost of LLMs.  This approach allows for selective pruning of redundant parameters, enabling more efficient and deployable LLMs tailored to specific application needs.</sample>
    <sample id="115">Il segmento parlato utilizzato dall'approccio è di lambda speech frames.</sample>
    <sample id="116">Servin è un giudice.</sample>
    <sample id="117">La qualità degli esempi è più importante della somiglianza con la frase sorgente.</sample>
    <sample id="118">```
This paper addresses the challenge of code-switching in NLP, where text contains a mixture of languages, a common phenomenon in linguistically diverse communities. Multilingual pre-trained models like mBERT and XLM-R often struggle with code-switched tasks. We propose SwitchMLM, a novel Masked Language Modeling (MLM) technique specifically designed for code-switching, focusing on identifying and masking switch-points – tokens that mark language transitions.  We offer FrequencyMLM as a surrogate method when LID tagging data is unavailable.

To further enhance performance, we introduce architectural modifications including residual connections leveraging switch-point information encoded in intermediate BERT layers, and an auxiliary loss to encourage language information encoding in these layers.  We evaluate SwitchMLM and its variants on sentiment analysis tasks, demonstrating superior performance compared to standard MLM.

We validate our claims through probing experiments using linear and conditional probing classifiers.  These experiments show that SwitchMLM effectively increases the amount of switch-point information in intermediate and final layers of BERT.  Specifically, we demonstrate that incorporating residual connections from intermediate layers to the final layer can further improve switch-point information content.  Our work contributes to improved computational models for code-switching, highlighting the importance of explicitly modeling language transitions.
```</sample>
    <sample id="119">GPT-4, GPT series, BART series, RoBERTa.</sample>
    <sample id="120">Il modello utilizza i punteggi di attenzione tra l'audio di input e l'output testuale, ovvero il meccanismo di cross-attenzione.</sample>
    <sample id="121">```
The text states that the most obvious thing is to use a direct reference, for example by saying the name of the song "Easy on Me" or its position, "the first one".
```</sample>
    <sample id="122">Siyu Yuan è affiliata all'Università di Fudan.</sample>
    <sample id="123">MultiInstruct presenta il primo dataset di instruction tuning multi-modale, composto da 62 task diversificate che coprono 10 categorie, derivate da 21 dataset open-source esistenti. Ogni task è accompagnata da cinque istruzioni scritte da esperti, fornendo una base per l'instruction tuning di modelli pre-addestrati multi-modali come OFA. L'obiettivo è indagare l'efficacia dell'instruction tuning su task multi-modali, un'area meno esplorata rispetto al linguaggio puro.

L'approccio prevede l'utilizzo di OFA, un modello pre-addestrato che gestisce dati testuali, immagini e bounding box in un unico spazio token. Il dataset MultiInstruct viene utilizzato per l'instruction tuning di OFA, con un'attenzione particolare alla diversità delle istruzioni.  Viene valutata l'efficacia dell'instruction tuning e del transfer learning da dataset di istruzioni naturali, misurando performance (accuratezza per task di classificazione, Rouge-L per task di generazione) e sensibilità del modello.

I risultati dimostrano che l'instruction tuning migliora significativamente le performance di OFA su task multi-modali e che il transfer learning da istruzioni naturali ne aumenta ulteriormente la sensibilità.  L'introduzione del metrico di sensibilità permette di valutare la robustezza del modello alle variazioni nelle istruzioni.  Il team prevede di ampliare il dataset con circa 150 task aggiuntive, rendendolo una risorsa preziosa per la ricerca in questo campo.</sample>
    <sample id="124">Tan Qingyu from NUS and Alibaba introduces their work "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models." The research addresses the fundamental challenge of temporal reasoning in LLMs, dividing it into three levels: time-to-time (year prediction), time-to-event (event grounding to time), and event-to-event (multiple event grounding).  Prior work has focused primarily on the second level, leading to a more comprehensive study.

The authors developed the TempReason dataset, encompassing all three levels of temporal reasoning and long temporal coverage, constructed from Wikidata and Wikipedia. They evaluated LLMs like T5-L, FLAN-T5-L, and ChatGPT on this dataset, revealing biases towards the 2000-2020 time period and ChatGPT's performance degradation with month prediction.

To enhance temporal reasoning, they propose a training strategy combining temporal span extraction pre-training and time-sensitive reinforcement learning.  Their model, TempT5, demonstrates significant performance improvements over zero-shot LLMs and fine-tuned models on TempReason, particularly in Open Book QA and Reasoning QA settings.  ChatGPT exhibits inconsistent performance across time periods, highlighting a flaw in its temporal reasoning.  The research concludes by analyzing temporal reasoning biases in LLMs and proposing TempReason as a benchmark and a training paradigm to improve their temporal reasoning capabilities.</sample>
    <sample id="125">The provided text does not mention the number of authors involved in the article.</sample>
    <sample id="126">XSemPLR ha considerato la traduzione della query in linguaggio naturale utilizzando un modello di traduzione automatica prima del parsing semantico come un approccio standard.</sample>
    <sample id="127">## Abstract: Large Language Models Are Reasoning Teachers

This paper introduces "Large Language Models Are Reasoning Teachers," a novel approach to transfer complex reasoning abilities from large language models (LLMs) to smaller, more accessible models.  Chain-of-thought (CoT) prompting, while effective for LLMs, is computationally expensive and limits its deployment to massive models.  Our method addresses this by leveraging LLMs as "reasoning teachers" to generate step-by-step solutions for complex tasks, which are then used to fine-tune smaller models.  

A key innovation is "Diverse Reasoning," which generates multiple reasoning samples from the teacher model using stochastic sampling, leading to improved performance compared to single-sample approaches.  Experiments on 12 tasks demonstrate that fine-tuned CoT models achieve notable performance, particularly in text-based tasks, and outperform vanilla fine-tuning even with small student models (0.3 billion parameters).  

The study highlights the scalability of the method, with performance improving with larger datasets, better teacher models, and larger student models. However, it also acknowledges the trade-offs between development costs (teacher model complexity) and inference costs (student model size).  The paper provides accessible code and data, including OpenAI inference credits, to encourage further research in this area.  The findings suggest that simple distillation of reasoning abilities from LLMs can enable the development of more capable and deployable AI systems.</sample>
    <sample id="128">The KITMUS test suite evaluates knowledge integration in natural language understanding (NLU) models, addressing the challenge of leveraging both pretraining and inference-time knowledge.  This work highlights the critical need for models to effectively combine knowledge from different sources to solve knowledge-intensive NLU tasks, exemplified by coreference resolution.  The KITMUS test suite introduces a coreference resolution task designed to probe for the ability to draw on knowledge available in various sources.  The test suite features three settings: "Background-Pretrain," "Background-Both," and "Background-Inference," varying the availability of entity-specific and background knowledge.  The "Background-Inference" setting simulates scenarios where background knowledge is not present in pretraining.  Experiments with human participants and established coreference resolution models reveal that while task-specific training can improve performance, even the best models struggle with reliably integrating knowledge available only at inference time.  This suggests that current models often rely on surface-level cues rather than deeper knowledge integration.  The findings underscore the limitations of models trained on generic datasets and emphasize the need for task-specific training to effectively leverage knowledge from multiple sources.  The KITMUS dataset and code are publicly available on GitHub.</sample>
    <sample id="129">"Asian woman" e "Middle-Eastern woman".</sample>
    <sample id="130">Il documento non specifica quali architetture dei modelli non generalizzano adeguatamente. Si concentra su modelli Transformer che generalizzano meglio e su modelli che generalizzano peggio a causa del "temporal drift".</sample>
    <sample id="131">Il video non menziona i nomi dei set di dati di test.</sample>
    <sample id="132">Due.</sample>
    <sample id="133">OFA, il modello base utilizzato, utilizza un vocabolario unificato per testo, immagini e coordinate di bounding box.</sample>
    <sample id="135">ABC-Eval è un nuovo approccio dimensionale per valutare i modelli di dialogo di intelligenza artificiale conversazionale. Sviluppato dall'Emory NLP Lab, ABC-Eval mira a superare le limitazioni delle valutazioni tradizionali basate sull'uomo, che spesso sono soggettive e non catturano la complessità della qualità del dialogo. ABC-Eval si concentra sull'annotazione esplicita dei comportamenti dei modelli, come risposte irrilevanti, contraddizioni, allucinazioni e mancanza di empatia.

L'approccio è stato testato su quattro modelli di dialogo all'avanguardia, confrontato con metodi esistenti basati su valutazioni Likert a livello di turno e dialogo, e confronti pairwise. I risultati dimostrano che le etichette ABC-Eval sono più affidabili e predittive della qualità complessiva del dialogo rispetto ai metodi tradizionali.  ABC-Eval identifica metriche distinte che spiegano una porzione significativa della qualità del dialogo, evidenziando l'importanza di un'analisi più granulare.

L'analisi rivela che i modelli testati presentano ancora problemi significativi, come violazioni della conoscenza comune, risposte irrilevanti e contraddizioni. ABC-Eval fornisce una base per valutare in modo più preciso e completo i modelli di dialogo, consentendo di quantificare i punti di forza e di debolezza e di guidare il progresso nel campo dell'intelligenza artificiale conversazionale.</sample>
    <sample id="136">## Abstract

This presentation introduces FERMAT, a novel evaluation framework for numerical reasoning designed to address limitations of existing benchmarks.  The work investigates why large language models (LLMs) struggle with numerical reasoning, particularly at the 3-billion parameter scale, despite their overall performance.  Current benchmarks primarily rely on accuracy scores, offering limited insight into the underlying mathematical abilities of these models.

FERMAT overcomes this by constructing a flexible evaluation set based on arithmetic types extracted from Illinois and CommonCore curricula.  The dataset includes word problems, with numbers adapted to represent real-world values and varying complexities (integers, decimals).  The evaluation assesses performance across different mathematical operations and training dependencies.

A zero-shot evaluation reveals poor performance across all aspects, highlighting the inadequacy of existing benchmarks.  Fine-tuning LLMs on 200,000 FERMAT examples, generated with human-written templates, significantly improves performance, demonstrating the importance of diverse numerical representations and operations.  Further analysis reveals that models may not fully "memorize" exact expressions, suggesting the role of linguistic cues.  Finally, incorporating templates from GSM8K and AQUA, known for their mathematical diversity, further enhances performance.

The research concludes that existing benchmarks are unrepresentative of real-world numerical reasoning skills. FERMAT provides a more informative alternative, emphasizing the importance of language and mathematical diversity, and identifying areas for improvement in number encoding and tokenization.  The findings suggest that a more nuanced evaluation approach is crucial for understanding and advancing the capabilities of LLMs in numerical reasoning.</sample>
    <sample id="137">```
This paper introduces Tell2Design, a novel dataset and a sequence-to-sequence model for language-guided floor plan generation.  Unlike text-conditional image generation, which focuses on high-level visual concepts, this work addresses the practical need to generate designs that satisfy specific requirements outlined in natural language. The task involves generating 2D floor plans from text instructions detailing semantics, geometry, and topology of rooms.  A large-scale dataset of 5,051 human-annotated instructions and 76,000 artificially generated instructions is created using Amazon Mechanical Turk and pre-defined templates.

The core challenge lies in generating designs under stricter constraints and interpreting complex, potentially ambiguous instructions.  The proposed approach utilizes a sequence-to-sequence model based on a transformer architecture, initialized with the T5 language model, to reconstruct room bounding boxes from the input text.  The model is trained using a language modeling objective.  Evaluation demonstrates that the Tell2Design model achieves high IoU scores (Micro IoU: 54, Macro IoU: 53), outperforming text-conditional image generation baselines.  This success is attributed to the model's ability to effectively control the target bounding box sequence based on extracted information from the instructions.  The study also highlights the language distribution gap between artificial and human instructions, suggesting a potential benefit of combining both during training.  The paper concludes by positioning Tell2Design as a foundation for future research in language-guided design generation.
```</sample>
    <sample id="138">La capacità dei modelli di NLU di integrare conoscenza da diverse fonti (pre-training e inferenza) è poco studiata.</sample>
    <sample id="139">Ying e Zhiyang.</sample>
    <sample id="140">Sì, i campioni del set di validazione e test di CoScript sono stati revisionati da lavoratori della comunità per individuare e correggere i campioni errati.</sample>
    <sample id="141">Le risorse esistenti per la traduzione dipendente dal contesto hanno limitazioni perché supportano solo un piccolo numero di traduzioni che dipendono dal contesto, e le risorse esistenti supportano solo un numero limitato di tipi di traduzioni dipendenti dal contesto e un numero limitato di set di lingue, in quanto si basano spesso sulla conoscenza del dominio e sulla curatela umana.</sample>
    <sample id="142">Siamo qui per parlare del nostro lavoro su "Risoluzione di espressioni di riferimento indirette per la selezione di entità", in cui introduciamo il corpus AltEntities. Il mio nome è Javad Hosseini e questo è un lavoro con Filip Radlinski, Silvia Pareti e Annie Louis. Il nostro obiettivo è comprendere il linguaggio degli utenti quando vogliono fare una scelta. Consideriamo la seguente domanda alternativa: "Hai inteso 'Easy on Me' o 'I Gotta Feeling'?" In questo caso, un utente vuole scegliere tra una di queste due canzoni. L'ovvio è usare un riferimento diretto, ad esempio dicendo il nome della canzone "Easy on Me" o la sua posizione, "la prima". Ma a volte un riferimento indiretto è più appropriato per una conversazione più naturale. Questo può accadere quando l'utente non ricorda il nome della canzone. Oppure le pronunce sono troppo simili per essere disambiguate facilmente. Oppure quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di riferimenti indiretti, ad esempio "quella più recente" o "quella che non è energica". Questo è un problema importante nei sistemi conversazionali e anche per il benchmarking dei modelli linguistici di grandi dimensioni (LLM) nella comprensione delle entità. Non siamo a conoscenza di un dataset pubblico più ampio per questo compito, quindi ne abbiamo creato uno utilizzando l'annotazione da parte di crowd. Il nostro dataset copre tre domini diversi: musica, libri e ricette. La nostra metodologia di raccolta dati enfatizza l'informalità utilizzando un setup di completamento di cartoon. Il cartoon ha tre bolle di dialogo. Nella prima bolla, Bob dice: "Ricordate quella canzone che stavamo ascoltando ieri?". E con questo, Bob stabilisce il contesto del dialogo. Nella seconda bolla, Alice dice: "Vuoi dire 'Easy on Me' o 'I Gotta Feeling'?" Che è la domanda alternativa. E nella terza bolla, Bob usa un riferimento indiretto per selezionare una delle entità, ad esempio, "quella più recente". Forniamo le prime e seconde bolle automaticamente, ma la terza è compilata dall'annotatore. La prima bolla è scelta da una serie di prompt manuali per dominio. La seconda, che è la domanda alternativa, è generata come segue. Usiamo sempre un semplice template: "Vuoi dire A o B?", dove A e B sono campioni da Wikipedia. Abbiamo utilizzato diversi metodi di campionamento. Man mano che ci si sposta verso l'alto nella lista, le entità diventano più simili tra loro e la disambiguazione è generalmente più difficile. Il primo è casuale. Il secondo è quando le entità hanno titoli simili, ad esempio due libri con lo stesso nome "The Return". Il terzo è quando hanno descrizioni simili su Wikipedia. E infine quando hanno informazioni simili su Wikipedia, ad esempio lo stesso genere o lo stesso artista per una canzone. Quando mostriamo questa domanda alternativa agli annotatori, sanno il nome delle entità, ma non necessariamente le conoscono. Quindi ciò che facciamo è che mostriamo alcune informazioni di background sulle due entità. Per le canzoni, mostriamo semplicemente un link di ricerca su Google per ciascuna canzone e poi chiediamo agli annotatori di ascoltare almeno alcune delle canzoni e di leggere su ciascuna canzone. Ecco, ad esempio, il risultato della ricerca su Google per la canzone "Easy on Me". Per i domini delle ricette e dei libri, mostriamo alcuni testi da Wikipedia. Per le ricette, mostriamo anche le immagini, ancora una volta da Wikipedia, in modo che gli annotatori sappiano come appaiono. Quindi abbiamo chiesto agli annotatori di scegliere una di queste entità e di descriverla usando tre a cinque espressioni di riferimento indirette. Ad esempio, quella con la musica a pianoforte. Ecco alcuni esempi dal nostro dataset. Ad esempio, "quella senza parole", "non quella con il bambino di 12 anni", o "quella fittizia", o "viene dall'Azerbaigian", e così via. Il corpus AltEntities ha 6.000 domande alternative su tre domini e contiene 42.000 espressioni di riferimento indirette. I risultati con il modello T5 XL sono riassunti di seguito. Se il modello linguistico ha accesso alle stesse conoscenze di background degli annotatori, l'accuratezza è molto alta, intorno al 92-95%. Ma questo non è realistico. Se il modello linguistico ha accesso a conoscenze di background parzialmente sovrapposte, l'accuratezza è compresa tra l'82 e l'87%, il che è più realistico. Ad esempio, quando il modello linguistico recupera le conoscenze di background. Se il modello linguistico ha accesso solo ai nomi delle entità, l'accuratezza è solo del 60%, quindi c'è molto spazio per il miglioramento. Abbiamo anche dimostrato che i modelli sono generalizzabili a dominio. Ecco un link al nostro dataset. Grazie.</sample>
    <sample id="143">Wait-k strategy e Local Agreement.</sample>
    <sample id="144">The provided text does not mention the affiliations of the authors.</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">Yicheng from Fudan University presents a research paper addressing the critical issue of omission in dialogue summarization. While large language models excel at generating fluent summaries, they frequently omit key information, leading to factual inaccuracies and incomplete summaries. This presentation highlights the severity of the omission problem, revealing a high omission rate (around 70%) across various domains and models.

To systematically analyze and address this challenge, the researchers propose the OLDS dataset, a high-quality dataset of dialogue summaries with explicit omission labels. This dataset is built upon existing benchmarks and utilizes automatic and human evaluation to ensure label quality. The paper explores three model architectures – pairwise classification, sequence labeling, and pointer networks – to build omission detection models, evaluating their performance using metrics like Precision, Recall, and F1-score.  A key finding is the significant label imbalance in the dataset, indicating the need for more advanced detection models.

Furthermore, the research investigates the potential of using detected omissions to refine summaries. A post-editing method, concatenating the candidate summary with omission content, demonstrates a substantial performance boost, suggesting that omission detection is a valuable step towards improving dialogue summarization quality. The paper concludes by emphasizing the challenging nature of the task and the promising direction of omission-based refinement.</sample>
    <sample id="147">Esin Durmus, Dan Jurafsky e Myra.</sample>
    <sample id="148">Sara Papi del Dipartimento di Informatica dell'Università di Trento e della Fondazione Bruno Kessler, presenta brevemente il lavoro di ricerca "Attention as a Guide for Simultaneous Speech Translation" in collaborazione con Matteo Negri e Marco Turchi.

Cosa è la traduzione simultanea del parlato (SimulST)? La SimulST è il processo di traduzione di un linguaggio parlato in testo in un altro linguaggio in tempo reale, consentendo la comunicazione interlinguistica.

Quali sono i problemi dei modelli SimulST attuali? Le architetture specifiche sono solitamente addestrate, introducendo moduli aggiuntivi da ottimizzare. Procedure di addestramento lunghe e complesse, ad esempio, addestramento che coinvolge diversi obiettivi di ottimizzazione. E addestramento e manutenzione di diversi modelli per raggiungere diversi regimi di latenza. Ad esempio, addestrare un modello con una latenza media di un secondo e un altro modello con due secondi di latenza, e così via.

Qual è la nostra soluzione? Innanzitutto, utilizzare modelli ST offline esistenti senza riaddestramento o l'adozione di architetture specifiche per la SimulST. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza tramite parametri specifici. E sfruttare la conoscenza già acquisita dal modello attraverso il meccanismo di attenzione tra l'input audio e l'output testuale. Questo è il meccanismo di cross-attenzione, e potete vedere un esempio a destra. La nostra soluzione è proporre EDAtt, o Encoder-Decoder Attention, che è una strategia per la quale decidiamo di emettere o meno una traduzione parziale in base a dove l'attenzione punta. Una parola viene emessa se l'attenzione non è concentrata, cioè la somma è inferiore a una certa soglia alfa rispetto alle ultime lambda frame audio, il che significa che le informazioni ricevute sono sufficientemente stabili. Ad esempio, se riceviamo un blocco di parlato contenente "I'm going to talk about...", e il nostro modello prevede la traduzione in tedesco, e guardiamo i pesi di cross-attenzione, vedremo che le prime due parole puntano alle prime frame audio ricevute, mentre l'ultima parola punta alle ultime lambda frame audio. Ciò significa che le prime due parole verranno emesse, mentre poiché la somma dei pesi di cross-attenzione è superiore a una certa soglia alfa, non emetteremo l'ultima parola e aspetteremo un altro blocco di parlato. Se andiamo avanti e riceviamo un altro blocco di parlato, e il nostro modello prevede altre tre parole, e guardiamo quei pesi di cross-attenzione, vedremo che nessuna parola punta alle ultime lambda frame audio. Ciò significa che queste tre parole verranno emesse.

Inoltre, i principali risultati di EDAtt, visualizzati in grafici in cui BLEU (che misura la qualità della traduzione) è su un lato e la latenza media (la misura della latenza) è sull'altro, e consideriamo anche la latenza computazionalmente consapevole che tiene conto dei tempi di calcolo del modello per prevedere l'output. Vogliamo che le nostre curve siano il più alte possibile su questo grafico, ma anche che siano spostate verso sinistra. E confrontiamo con le strategie popolari applicate ai modelli offline, come la strategia Wait-k e la Local Agreement. Confrontiamo anche con l'architettura state-of-the-art specificamente progettata per la pre-traduzione simultanea. Questi sono tutti i risultati della strategia di traduzione simultanea del parlato su tedesco. E vediamo che supera tutte le strategie applicate ai modelli offline poiché le curve sono spostate verso sinistra. E vediamo anche che se si considera il tempo effettivo trascorso o il tempo computazionalmente consapevole, che è la strategia più veloce,

Se volete scoprire altri risultati, leggete il nostro articolo. E abbiamo anche rilasciato il codice e i modelli aperti per facilitare la riproducibilità del nostro lavoro. Grazie per la vostra attenzione.</sample>
    <sample id="149">Sì, il set di dati è disponibile pubblicamente.</sample>
    <sample id="150">MeetingQA is a novel extractive question-answering dataset designed for meeting transcripts, addressing the gap in NLP research concerning the significant question-asking component within these long, domain-specific documents. Unlike existing datasets focused on summarization and action item extraction, MeetingQA captures the open-ended and discussion-driven nature of meeting discussions. The dataset comprises 7.7K questions and corresponding answers extracted from public meeting transcripts (AMI corpus), annotated with high inter-annotator agreement.

The dataset features diverse question types, including yes/no, opinion-seeking, rhetorical, and multi-speaker questions, with a notable percentage of unanswerable questions and answers spanning multiple sentences or speakers.  The length of questions and answers typically ranges from 12 and 35 words, respectively.

The paper explores various approaches to extractive QA on MeetingQA, including context retrieval, single-span and multi-span models, and data augmentation using silver annotations.  Experiments demonstrate a significant performance gap between fine-tuned models and human performance, with short-context models outperforming long-context models. Multi-span models show comparable performance to single-span models. Zero-shot performance is substantially lower, but data augmentation with silver annotations improves it. Error analysis reveals challenges in identifying rhetorical questions and speaker attribution, particularly in zero-shot settings.  MeetingQA presents a challenging task for current QA models, highlighting the need for further research in handling the complexities of real-world meeting discourse.</sample>
    <sample id="151">Ciao a tutti, mi chiamo Ying e il mio collega Zhiyang e presenteremo la nostra ricerca su MultiInstruct che migliora l'apprendimento zero-shot multi-modale tramite l'instruction tuning. Con i recenti progressi dei modelli linguistici di grandi dimensioni, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento riutilizzando i modelli linguistici pre-addestrati per diversi compiti downstream in modo parametrico e dati-efficiente. Recentemente, molti studi hanno dimostrato che l'instruction tuning consente ai modelli linguistici di grandi dimensioni di eseguire compiti sconosciuti in modo zero-shot seguendo istruzioni naturali. Tuttavia, molti lavori precedenti sull'instruction tuning si sono concentrati sul miglioramento delle prestazioni zero-shot sui compiti linguistici, mentre computer vision e compiti multi-modali sono stati lasciati di lato. Pertanto, in questo lavoro vogliamo indagare se l'instruction tuning di modelli pre-addestrati multi-modali possa effettivamente migliorare la generalizzazione a compiti multi-modali sconosciuti. Inoltre, al momento della nostra ricerca, abbiamo riscontrato una notevole discrepanza nella disponibilità di dataset di instruction tra NLP e multi-modali. Esistono più di 1600 task di instruction solo per il linguaggio. Tuttavia, non esiste un dataset multi-modale di instruction di grandi dimensioni pubblicamente disponibile. Pertanto, questo ci motiva a costruire un dataset di instruction tuning multi-modale. Qui presentiamo MultiInstruct, il primo benchmark di dataset di instruction tuning multi-modale che consiste in 62 task multi-modali diverse che coprono 10 categorie ampie. Queste task sono derivate da 21 dataset open-source esistenti e ogni task è dotato di cinque istruzioni scritte da esperti. Per indagare sull'instruction tuning multi-modale sul nostro dataset proposto, prendiamo OFA, un modello pre-addestrato multi-modale unificato, come nostro modello di base. OFA utilizza un vocabolario unificato per token di linguaggio, immagini e coordinate di bounding box. Qui mostriamo alcuni esempi di istanze dal nostro dataset MultiInstruct, per unificare l'elaborazione di diversi tipi di input e output dati. Seguiamo il metodo da OFA e formuliamo tutte le task in un formato di sequenza-a-sequenza unificato. In cui il testo di input, le immagini, le istruzioni e le bounding box sono rappresentati nello stesso spazio di token. Ok, ora parlerò dell'instruction tuning multi-modale. Quindi, per il dataset di training, utilizziamo 53 task da 9 gruppi per l'addestramento e campioniamo 10.000 istanze per task. Per la valutazione, riserviamo l'intero gruppo di ragionamento comune per la valutazione e selezioniamo ulteriori 5 task dai gruppi VQ e Miscellaneous. Utilizziamo tutte le istanze nel split di test per ogni task. Inoltre, campioniamo casualmente 20 task dal split di test delle istruzioni naturali come task sconosciuto per NLP. Quindi utilizziamo il modello OFA pre-addestrato di grandi dimensioni come modello di base. Durante l'addestramento, mescoliamo tutte le istanze per tutte le task. Ogni istanza viene casualmente combinata con una delle sue cinque istruzioni. Quindi durante la valutazione per ogni task, conduciamo un totale di 5 esperimenti valutando il modello utilizzando una delle cinque istruzioni. In ogni esperimento, riportiamo la performance minima e massima e la deviazione standard della performance su tutte le 5 esecuzioni. Se la task è un task di classificazione multi-modale, riportiamo l'accuratezza. Se è un task di generazione multi-modale, riportiamo Rouge-L. Per i task NLP, riportiamo Rouge-L come bene. Introduciamo anche un'ulteriore metrica di valutazione chiamata sensitivity. Questa misura la capacità del modello di produrre costantemente gli stessi output per la stessa task indipendentemente da lievi variazioni nella formulazione dell'istruzione. Ecco il nostro risultato principale. Come possiamo vedere, l'instruction tuning può migliorare significativamente le prestazioni di OFA sui task multi-modali visti. Inoltre, il transfer learning da dataset di istruzioni naturali può beneficiare l'instruction tuning. Qui possiamo vedere, man mano che la quantità di task aumenta, il modello ottiene prestazioni migliori e al contempo una sensibilità inferiore. Quindi abbiamo anche condotto un esperimento. Utilizziamo una sola istruzione contro 5 istruzioni. Come possiamo vedere, utilizzare più istruzioni può migliorare l'intera performance del modello e ridurre notevolmente la sua sensibilità. Ciò dimostra l'effetto delle diverse strategie di fine-tuning sul modello sensitivity. Come possiamo vedere dai transfer learning da dataset di istruzioni naturali, il modello può ottenere molto migliori sensibilità rispetto al modello OFA originale. Possiamo anche vedere che il transfer learning da dataset di istruzioni naturali può aiutare OFA ad ottenere prestazioni molto migliori sul dataset di istruzioni naturali. In generale, proponiamo il primo dataset di instruction tuning multi-modale di grandi dimensioni con un miglioramento significativo della capacità di OFA e esploriamo diverse tecniche di transfer learning e i loro benefici. Progettiamo una nuova metrica chiamata sensitivity. Quindi un'altra cosa, stiamo raccogliendo un dataset di instruction tuning multi-modale molto più grande con circa 150 task di visione-linguaggio aggiuntivi e lo rilasceremo. Questa è una QR code per il nostro dataset e modello. Grazie.</sample>
    <sample id="152">## Abstract: Exploring Large Language Models for Classical Philology

This presentation introduces a project focused on developing novel large language models (LLMs) tailored for classical philology, specifically Ancient Greek and Latin. Recognizing the limitations of existing BERT and multilingual models, we have created GreBERTa and GreTa (monolingual RoBERTa and T5 encoder-decoder models for Greek, respectively), alongside PhilBERTa and PhilTa (multilingual models encompassing Greek, Latin, and English). Our work addresses key challenges in this domain, including the lack of robust evaluation and the need for models capable of handling both languages.

We tackled data scarcity by developing a high-quality pre-training corpus for Ancient Greek from the Internet Archive, leveraging a novel method to identify and clean Greek texts amidst OCR errors.  Benchmarking against existing state-of-the-art models, our models demonstrate significant performance improvements in part-of-speech tagging, dependency parsing, and lemmatization for both languages.  Notably, our encoder-decoder models excel in lemmatization, achieving a 5% performance gain. 

Furthermore, we investigated the behavior of the T5 encoder and explored the implications of multilinguality. While multilingual models show comparable performance to monolingual models in semantic and world knowledge tasks, our findings suggest that the benefits of multilinguality are not consistently realized.  This research provides a foundation for advancing the application of LLMs to classical philology, offering new tools for text analysis, interpretation, and knowledge discovery.  A detailed report is available in the accompanying paper.</sample>
    <sample id="153">## Abstract

This work addresses the challenge of ambiguities in text-to-image generative models, aiming to improve the faithfulness of generated images to user intent. We investigate various ambiguities in prompts, such as those related to spatial relationships and object placement, and propose a framework to mitigate these ambiguities. Our approach involves a benchmark dataset built upon the LAVA corpus, encompassing diverse ambiguity types.  We introduce two disambiguation strategies: one leveraging in-context learning to generate clarifying questions, and another employing visual interpretation generation.  The user responds to these prompts, providing a disambiguated prompt that reflects their intended meaning.

To evaluate the faithfulness of generated images, we propose an automatic evaluation framework utilizing a Visual Question Answering (VQA) model.  The VQA model assesses whether the generated image aligns with the user's stated intention, considering both the original ambiguous prompt and the disambiguated prompt.  Our findings demonstrate that ambiguity resolution varies across different types of ambiguities, but that our framework generally improves image fidelity.  Furthermore, our automatic evaluation framework exhibits strong agreement with human evaluation, suggesting its reliability for assessing text-to-image model performance.  This research contributes to advancing the robustness and user-friendliness of text-to-image generation, paving the way for more accurate and intuitive image creation.</sample>
    <sample id="154">Sara Papi è affiliata alla University of Trento e alla Foundazione Bruno Kessler. Matteo Negri e Marco Turchi sono affiliati a queste stesse istituzioni.</sample>
    <sample id="155">Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="157">The paper introduces SDDS, a novel model for dialogue summarization that aims to extract key information from multi-turn conversations. Existing methods often rely on pre-computed static graphs derived from linguistic tools, which are susceptible to errors and lack adaptability. SDDS addresses these limitations by employing a Static-Dynamic Structure Fusion Graph.

The model comprises four main components: an Utterance Encoder for vector representation, a Static Graph module for constructing a static graph based on discourse parsing and speaker interaction, a Static-Dynamic Graph module for fusing static and dynamic graph representations using multi-head attention, and a Summary Generator based on a pre-trained language model.

To build the static graph, the model utilizes Discourse Parsing Graph, Key Co-occurrence, and Speaker Interaction Frequency Matrix to capture discourse structure, semantic relationships, and speaker dynamics. A Dynamic Graph module leverages multi-head attention to model semantic relationships based on utterance embeddings.  The static and dynamic graphs are then fused using a graph attention layer and a dual cross-attention mechanism.

The authors demonstrate the effectiveness of SDDS on dialogue summarization tasks. The code and data are publicly available on GitHub. This work offers a more robust and adaptable approach to dialogue summarization by combining the strengths of static and dynamic graph representations.</sample>
    <sample id="158">This presentation introduces "Dual Cache for Long Document Neural Coreference Resolution," addressing the challenge of coreference resolution in long documents where mentions of entities are scattered across a wide text range. Traditional methods suffer from quadratic complexity, while cache-based methods offer linear complexity but struggle with high cache misses in long documents due to topic shifts.

The proposed solution employs a dual cache architecture, combining a local cache (LRU eviction) for local entities and a global cache (LFU eviction) for global entities. This allows the model to efficiently manage entities with varying frequencies and improve performance in long documents.  The model scans the document, classifying mentions as new or belonging to the cache, and adding them to either the local or global cache based on frequency.

Experiments on benchmark datasets (LitBank, OntoNotes, WikiCoref) demonstrate that the dual cache outperforms baseline methods, even with unbounded memory, and achieves faster processing times.  A book-level evaluation further highlights the significant performance gap between the baseline and dual cache.  The dual cache also significantly reduces cache misses compared to single-cache approaches.  The study concludes that the dual cache offers the best performance-to-cost ratio among cache-based methods, effectively addressing the limitations of single-cache approaches in long document coreference resolution.</sample>
    <sample id="159">Ciao a tutti. Sono Koustav Sinha e sono lieto di darvi il benvenuto al nostro talk sul paper di ACL 2023. I giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto. Questo è un lavoro congiunto con John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy e Adina Williams. Quindi, in questo lavoro, rivisito i paradigmi dei minimal pair. Il paradigma dei minimal pair valuta i modelli linguistici basandosi sui giudizi di accettabilità, che possono includere anche la correttezza grammaticale come BLiMP, SyntaxGym o l'accettabilità in termini di stereotipi come CrowS pairs. In questo, il modo tipico per valutare i modelli linguistici nel paradigma dei minimal pair è mostrare una frase accettabile o grammaticale e poi mostrare una frase accettabile o non grammaticale. L'idea è che il modello, in pratica, dia più probabilità alla frase accettabile. Il pipeline MPP attuale non consente di valutare l'accettabilità verso frasi più lunghe. Negli ultimi tempi, i grandi modelli linguistici stanno diventando sempre più lunghi e con finestre di contesto sempre più ampie. Pertanto, è fondamentale valutare l'accettabilità dei modelli in tutto il finestrino di contesto e questo è ciò che stiamo cercando di fare qui. Stiamo cercando di rivisitare il pipeline MPP ponendo al modello il compito di valutare l'accettabilità su sequenze sempre più lunghe. Questo è il nostro approccio. Quindi, ciò che facciamo è che per simulare queste sequenze più lunghe, rivisito i dataset stessi e ricreo le frasi scegliendo frasi accettabili o non accettabili da quei dataset. Ad esempio, qui abbiamo scelto un tipico paio di correttezza grammaticale dal dataset BLiMP dal caso dell'Isola degli Adjunct. E ciò che facciamo è che per ricreare sequenze più lunghe e che abbiano lo stesso accoppiamento della struttura grammaticale, estraggo frasi grammaticali dall'Isola degli Adjunct e le aggiungo come prefisso sia alla query accettabile che alla query non accettabile. Possiamo fare lo stesso scegliendo frasi da un altro sottoinsieme o da un altro dataset. Questo è ciò che chiamiamo scenario di mismatch. Quindi, qui le frasi vengono ancora da dataset rilevanti, ma non dallo stesso dataset con cui si sta valutando. Possiamo anche fare lo stesso scegliendo frasi da un dominio completamente irrilevante, come Wikipedia. Questo ci dirà se i giudizi di accettabilità dei modelli sono effettivamente influenzati da qualsiasi contesto, ovvero se il contesto proviene da un sottoinsieme diverso dei dataset o se è completamente irrilevante per la frase che stiamo guardando. Come funziona il modello? Quindi, inizialmente, guardiamo le frasi di Wikipedia, che sono completamente irrilevanti per la query corrente, e lì troviamo che i giudizi MPP sono per lo più robusti per qualsiasi lunghezza di contesto. Aumentiamo la lunghezza del contesto fino a 1024 per ottimizzare OPT e GPT-2 e vediamo qui, nella linea puntata arancione, che i giudizi MPP sono relativamente stabili. Ora, cosa succede quando scegliamo frasi dallo stesso dataset? Qui stiamo scegliendo o creando frasi da domini accettabili e non accettabili dallo stesso dataset BLiMP o SyntaxGym. E lì vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o non accettabili. Ma quando si corrisponde alla struttura, ovvero quando si scelgono frasi dallo stesso fenomeno in BLiMP o SyntaxGym, vediamo un aumento o una diminuzione massiccia dei giudizi MPP del modello, a seconda che il prefisso scelto sia accettabile o non accettabile. E questo è molto grande, come questo effetto, aumenta lungo la lunghezza del contesto e probabilmente influenzerà i nuovi modelli linguistici con finestre di contesto ampie. Perché il prefisso di corrispondenza influisce così tanto sul giudizio del modello? Quindi, abbiamo condotto una serie di analisi cercando di perturbare la frase modificando il suo contenuto mantenendo la struttura rilevante, aggiungendo come rumore all'input. Dopo aver eseguito diversi di questi perturbazioni, troviamo che nessuna di queste perturbazioni sta effettivamente facendo cambiare al modello il suo corso in termini di come ci mostra il giudizio MPP. In pratica, troviamo che i modelli sono sensibili alle frasi perturbate in modo simile. In altre parole, quando perturbiamo le frasi nel dominio accettabile, vediamo un aumento simile in tutte le perturbazioni e quando perturbiamo le frasi nel dominio non accettabile, vediamo una diminuzione dei giudizi MPP in modo simile. Quindi, i principali risultati del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche latenti sintattiche e semantiche condivise tra le frasi e il modo in cui valutiamo l'accettabilità con input brevi e singole frasi potrebbe non catturare pienamente la conoscenza astratta dei modelli linguistici in tutto il finestrino di contesto. Leggete il nostro paper per maggiori dettagli sui nostri esperimenti. Grazie per aver ascoltato.</sample>
    <sample id="160">Il primo passaggio del metodo mappa ogni token di input a un multiset di token che appariranno nell'output.</sample>
    <sample id="161">55,000</sample>
    <sample id="163">MASSalign.</sample>
    <sample id="164">L'apprendimento scarsamente supervisionato è vantaggioso perché utilizza dati etichettati in modo meno accurato o meno completo rispetto all'apprendimento supervisionato tradizionale. Questo riduce i costi di etichettatura, ma può portare a modelli meno accurati. L'apprendimento scarsamente supervisionato cerca di mitigare questo problema addestrando modelli che siano robusti al rumore nelle etichette.</sample>
    <sample id="165">This paper introduces LiPoR, a novel unsupervised learning method for abductive reasoning, addressing the challenge of learning to identify plausible explanations without labeled plausibility data. Abductive reasoning aims to bridge the gap between a context and an outcome, given a set of candidate explanations. Current supervised methods struggle with noisy and subjective annotations. LiPoR tackles this by treating explanations as latent variables and maximizing the marginal likelihood of the outcome given the context, effectively learning from the data itself.

A key innovation is the introduction of a regularizer, Omega, that leverages the mutual exclusivity characteristic of explanations.  Omega balances maximizing the likelihood of the outcome with penalizing high entropy among explanations, effectively preferring a subset of plausible explanations. This approach is implemented as a combined objective function, incorporating both likelihood maximization and the regularizer.

Experiments on the AlphaNLI dataset demonstrate LiPoR's superior performance compared to zero-shot models and existing unsupervised methods, including a strong GPT-3 baseline. LiPoR achieves an accuracy improvement of over 4 absolute points.  The paper highlights the potential of unsupervised learning for abductive reasoning, paving the way for more robust and data-efficient systems capable of reasoning in complex scenarios without relying on human-annotated plausibility judgments. The paper is available at tinyurl.com/zhao-lipor.</sample>
    <sample id="166">This paper introduces a novel neural framework, Neural Divide-and-Conquer Reasoning (NDCR), for image retrieval from linguistically complex text. The task presents challenges due to image similarity and lengthy descriptions, hindering the performance of traditional visual-language models. NDCR addresses this by integrating Divide-and-Conquer strategy with Dual-Process Theory, inspired by human cognitive systems. It posits that complex reasoning requires both analogical (System 1) and logical (System 2) processing.

The framework comprises three key modules: a Proposition Generator to decompose complex text into simple propositions, a Visual-Linguistic Interactor (System 1) to interact visual and proposition information, and a Neural-Symbolic Reasoner (System 2) to perform logical reasoning on the propositions. The Reasoner utilizes negation and conjunction operations to integrate reasoning states and produce a final solution.

Experimental results demonstrate that NDCR outperforms existing baselines, with ablation studies validating the individual module contributions. The system's ability to present inference states and results at intermediate steps highlights its interoperability. The authors suggest that neural symbolic calculation holds promise for enhancing compositional reasoning in large language models, aligning with the effectiveness of Divide-and-Conquer and Dual-Process Theory in tackling complex problems. NDCR offers a promising approach to improve image retrieval from linguistically complex text by leveraging the strengths of both analogical and logical reasoning.</sample>
    <sample id="167">In DEPLAIN-web, i documenti sono stati allineati sia manualmente che con metodi di allineamento automatico.</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato annotando i dati di Reuters News del 2020 con le stesse linee guida di annotazione CoNLL-2003.</sample>
    <sample id="169">## Abstract: Prompting PaLM for Translation: Assessing Strategies and Performance

This paper presents a systematic study of large language model (LLM) prompting for machine translation (MT), focusing on Google's PaLM, a 540 billion-parameter model trained on 780 billion tokens.  The research evaluates the transition capability of PaLM using best practices in MT, including utilizing recent test sets and comparing performance against state-of-the-art systems like WMT.  Neural MT metrics and human evaluation (MQM framework) are employed to assess translation quality.

The study reveals that prompting strategy significantly impacts PaLM's translation performance, with one-shot and zero-shot prompting yielding substantial improvements (up to 40 BLEURT points) compared to baseline models.  However, for five-shot prompting, the form of the prompt becomes less critical, with example quality proving to be the most influential factor.  High-quality translations are prioritized over prompt similarity.

While PaLM demonstrates fluency comparable to state-of-the-art systems, accuracy remains a key challenge.  Common errors include omission errors, suggesting PaLM sometimes prioritizes stylistic coherence over complete sentence preservation.  Human evaluation indicates a lower "Style/Awkward" category for PaLM compared to other systems, indicating a strong balance between fluency and accuracy.  The findings suggest that careful selection of high-quality examples is crucial for maximizing PaLM's translation capabilities, and that while PaLM approaches commercial-level performance, further improvements in accuracy are needed.  The paper provides recommendations for prompt selection strategies and highlights the importance of evaluating LLMs with curated datasets.</sample>
    <sample id="170">Buongiorno a tutti, mi chiamo Yusen Zhang della Penn State University. Oggi presenterò il nostro lavoro "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations". Il semantic parsing è un compito che mira a costruire rappresentazioni semantiche di query degli utenti come SQL e Lambda Calculus. E il Cross-Lingual Semantic Parsing è il compito di tradurre le query in più lingue naturali in più rappresentazioni semantiche. Come mostrato in questa figura, dobbiamo tradurre le query in più lingue naturali utilizzando modelli neurali in SQL, Lambda o FunQL, eccetera. I modelli di cross-lingual semantic parsing esistenti sono proposti e valutati separatamente su dataset di task e applicazioni limitati. Ad esempio, c'è molta copertura su determinate lingue, ma la cinese è assente e manca di copertura su determinate rappresentazioni semantiche. Il Lambda calculus è assente, o sono valutati solo su determinati modelli neurali. Ad esempio, c'è solo un singolo modello per valutarli. Quindi, a tal fine, proponiamo XSemPLR. Forniamo un dataset uniforme XSemPLR per il cross-lingual semantic parsing in più lingue naturali e rappresentazioni semantiche. Contiene 9 dataset in vari domini, 5 task di semantic parsing, 8 rappresentazioni semantiche e 22 lingue naturali in 15 famiglie linguistiche. E per valutare meglio il nostro benchmark, consideriamo sei impostazioni per l'addestramento e la valutazione. La prima è Translate-Test. Usiamo l'API di Google Translate per tradurre la sorgente in destinazione, quindi usiamo un modello monolingue per l'addestramento e la valutazione. Ad esempio, addestriamo il modello inglese su query in inglese e durante l'inferenza traduciamo la query tedesca usando l'API in inglese e quindi usiamo il modello addestrato per prevedere l'SQL. Testeremo anche il modello monolingue. In questa impostazione, la lingua sorgente è la stessa della lingua di destinazione, ad esempio tedesco-tedesco o inglese-inglese. Testeremo anche l'impostazione Monolingual Few-shot addestrando modelli monolingue con solo il 10% dei dati di addestramento. E testeremo il modello Multilingue addestrando un modello multilingue su tutte le lingue. Ad esempio, mettiamo insieme le query tedesche, inglesi e cinesi per addestrare un modello multilingue. Durante l'inferenza possiamo utilizzare questo modello per tradurre query tedesche o cinesi, eccetera. E consideriamo anche il transfer cross-linguale zero-shot e few-shot. Addestriamo su una lingua sorgente e trasferiamo a un'altra lingua. Quindi, durante l'addestramento, addestriamo un modello multilingue per prevedere l'output SQL utilizzando query inglesi o combinazioni di query inglesi e tedesche few-shot. E troviamo molti risultati interessanti. Riguardo all'analisi dei modelli monolingue, li valutiamo su due gruppi di modelli: Encoder-PTR, che sta per Multilingual Pretrained Encoders with Pointer-based Decoders, come XLM-R + PTR e mBERT + PTR. Valutiamo anche i modelli Encoder-Decoder, che sono Multilingual Pretrained Encoder-Decoder Models, come mBART e mT5. Abbiamo scoperto che i modelli Encoder-Decoder ottengono le migliori prestazioni su tutti e nove i dataset. Valutiamo mT5 e XLM-R + PTR sulla modalità multilingue. Abbiamo scoperto che sia Encoder-Decoder che Encoder-PTR possono essere migliorati addestrandoli su una miscela di varie lingue. Abbiamo scoperto che la maggior parte delle principali lingue naturali può ottenere un miglioramento delle prestazioni, ad eccezione dell'inglese che perde prestazioni su sette dataset e guadagna su tre. Penso che questo sia noto come la "Maledizione della Multilingua". Confrontiamo anche la performance cross-linguale. Questa figura mostra la Cross-lingual Few-shot transfer, la Cross-lingual Zero-shot transfer e la Monolingual Setting. Abbiamo scoperto che, confrontando la linea verde e la linea arancione, la performance di trasferimento cross-lingua zero-shot è significativa e, confrontando la linea blu e la linea arancione, abbiamo scoperto che con l'impostazione few-shot la performance di trasferimento si riduce rapidamente. Abbiamo anche scoperto alcune altre interessanti scoperte. Ad esempio, Encoder-Decoder supera i lavori precedenti o ottiene risultati comparabili. Il pre-addestramento su inglese naturale può migliorare significativamente le prestazioni few-shot su lingue naturali di destinazione e abbiamo scoperto che i modelli linguistici multilingue come Codex e BLOOM sono ancora inadeguati per i task di semantic parsing cross-lingua. Per riassumere, costruiamo XSemPLR, un benchmark unificato per il cross-lingual semantic parsing con più lingue naturali e rappresentazioni semantiche. Conduciamo uno studio di benchmark completo su tre tipi rappresentativi di modelli linguistici multilingue. E i nostri risultati mostrano molti risultati interessanti. E eccetera. E vi invitiamo a visitare il nostro paper e il nostro codice. Grazie per l'attenzione.</sample>
    <sample id="171">I lavori connessi sono classificati in quattro categorie, ma nessuno di essi è applicabile a embedding as services o manca di trasferibilità.</sample>
    <sample id="172">No, LLM multilingue come Codex e BLOOM sono ancora inadeguati per le attività di parsing semantico cross-lingua.</sample>
    <sample id="174">```
ArgAnalysis35K è un nuovo dataset per l'analisi della qualità degli argomenti, progettato per superare le limitazioni dei dataset esistenti. A differenza di quelli basati su crowdsourcing, che spesso presentano bassa qualità e scarsa diversità, ArgAnalysis35K offre 35.000 coppie argomento-analisi di alta qualità, derivanti principalmente da competizioni di dibattito di alto livello. Il dataset è strutturato attorno a 24 temi, garantendo una maggiore diversità rispetto alla selezione di soli 30-40 argomenti.

Un elemento distintivo è l'introduzione del concetto di "analisi", che va oltre la semplice identificazione di affermazioni e premesse, comprendendo combinazioni complesse per spiegare la validità di un argomento.  Inoltre, ArgAnalysis35K incorpora un modello di affidabilità annotatore a livello di istanza, che permette di sfruttare al meglio le valutazioni degli annotatori, mitigando i bias individuali.

Infine, il dataset include un modello di rilevanza che assegna un punteggio a ciascun argomento in base alla sua rilevanza per diversi temi, catturando la molteplicità di applicazioni di un singolo argomento.  ArgAnalysis35K mira a fornire un dataset più completo, diversificato e affidabile per la ricerca nell'analisi della qualità degli argomenti, offrendo una migliore comprensione della struttura e della validità degli argomenti in contesti di dibattito.
```</sample>
    <sample id="175">Il metodo affronta l'ambiguità delle permutazioni inducendo l'allineamento come parte dell'addestramento.</sample>
    <sample id="176">Il modello NLP a valle viene valutato per l'equità analizzando le sue prestazioni in compiti come il rilevamento di discorsi d'odio e la rilevazione di notizie false, separando i risultati per diverse demografie e orientamenti politici. Si scopre che i modelli con orientamenti politici diversi mostrano prestazioni diverse in questi compiti, con potenziali conseguenze di emarginazione e diffusione di discorsi d'odio.</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustav Sinha.</sample>
    <sample id="179">## Abstract

This presentation introduces SymbolicToM, a novel inference-time method to enhance Theory of Mind (ToM) reasoning in large language models (LLMs). ToM is the ability to understand that others have beliefs that may differ from one's own, crucial for interpreting narrative and social situations. While LLMs struggle with ToM tasks like false-belief questions, this research aims to improve their performance.

SymbolicToM leverages explicit graphical representations – belief graphs – to model the mental states of characters in stories. These graphs, representing the current and anticipated beliefs of characters, are computed for all combinations of characters up to a defined level of complexity. An inference-time algorithm utilizes off-the-shelf Natural Language Inference (NLI) and Open Information Extraction (OpenIE) models to efficiently generate these graphs.

The method then uses the pre-computed graphs to answer questions about character beliefs, recursively querying the graphs and feeding the results to a language model. Experiments with various LLMs, including GPT-3 and Textual Time Travel, demonstrate significant performance gains across the board, with accuracy improvements ranging from 51 to 65 points.

Furthermore, SymbolicToM exhibits strong generalization capabilities, outperforming supervised baselines on out-of-domain datasets designed to test storage structure and linguistic diversity.  The method avoids overfitting and provides more interpretable reasoning.  In conclusion, SymbolicToM offers a plug-and-play approach to significantly improve LLM performance on ToM tasks, offering a robust and generalizable solution for enhancing their understanding of human psychology.</sample>
    <sample id="180">Esin Durmus e Dan Jurafsky.</sample>
    <sample id="181">This paper addresses the challenge of constrained language planning, extending the capabilities of large language models (LLMs) beyond abstract goal planning to scenarios with specific constraints, such as "make a chocolate cake." While LLMs demonstrate proficiency in decomposing abstract goals, their performance on constrained goals remains limited. We define constrained language planning and tackle the problem by first evaluating and improving LLM performance using a human-in-the-loop data acquisition approach with InstructGPT.  Our experiments reveal that current LLMs struggle to reliably adhere to constraints in generated scripts.

To address this, we introduce an "over-generate-then-filter" method leveraging InstructGPT to generate multiple scripts for specific goals, followed by a filter model to select faithful scripts based on semantic similarity and constraint keywords. This approach significantly enhances the quality of generated scripts, improving both semantic completeness and constraint faithfulness.  Furthermore, we tackle the challenge of creating a high-quality dataset for constrained language planning. We develop CoScript, a dataset of 55,000 specific goals and corresponding scripts, using LLMs and crowd-sourced validation.  Our results show that CoScript exhibits high diversity in goal types.  Finally, we demonstrate that fine-tuning smaller models like T5 on CoScript can achieve superior performance compared to larger LLMs, highlighting the potential of specialized models trained on targeted datasets.  We aim to make CoScript a valuable resource for advancing research in constrained language planning.</sample>
    <sample id="182">Il tropicalismo, nel contesto di questo articolo, si riferisce a un tropo che associa le donne latine a immagini di tropicalità, come "vibrante" e "curvaceous".</sample>
    <sample id="183">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target generando personas utilizzando prompt specifici e analizzando le parole utilizzate per descrivere queste personas. Hanno scoperto che, sebbene le descrizioni generate possano sembrare positive, spesso riflettono stereotipi e narrazioni essenzialistiche, come la "Strong Black Woman" o l'associazione di caratteristiche fisiche a gruppi specifici.</sample>
    <sample id="184">CXMI (Contextualized Machine Translation Information) è stato utilizzato per misurare l'utilizzo del contesto in questo lavoro.</sample>
    <sample id="185">DrBERT è un modello pre-addestrato in francese basato su RoBERTa, addestrato su NACHOS (dati medici web). ChuBERT è un modello basato su un modello clinico (dati clinici) e addestrato su dati clinici.</sample>
    <sample id="187">Ying e Zhiyang.</sample>
    <sample id="188">Il trasferimento iterativo dell'apprendimento aggiorna il modello addestrandolo sulla più recente serie di dati raccolti in ogni round di apprendimento attivo.</sample>
    <sample id="189">Il set di dati mira a comprendere il linguaggio degli utenti quando desiderano fare una scelta, in particolare quando utilizzano riferimenti indiretti invece di diretti.</sample>
    <sample id="190">Un utente malintenzionato può estrarre i parametri del modello imparando dai dati di embedding forniti dal servizio EaaS e poi riproducendo il modello con questi dati.</sample>
    <sample id="191">Tre.</sample>
    <sample id="192">CAME (Confidence-guided Adaptive Memory Efficient Optimization) affronta la sfida di ottimizzare l'addestramento di modelli linguistici di grandi dimensioni, bilanciando velocità di convergenza e efficienza di memoria. Gli ottimizzatori tradizionali come Adam soffrono di elevati requisiti di memoria, mentre quelli memory-efficient come Adafactor sacrificano prestazioni. CAME si basa su principi di non-negative matrix factorization (NMF) per ridurre la memoria, ma introduce un meccanismo per mitigare gli errori intrinseci presenti in Adafactor, che possono portare a convergenza lenta.

CAME introduce un'innovativa tecnica che utilizza il residuo tra l'aggiornamento previsto e quello effettivo per calcolare un fattore di instabilità. Questo fattore viene poi utilizzato per adattare l'aggiornamento, migliorando la stabilità del processo di ottimizzazione.  Gli esperimenti su BookCorpus e Wikipedia dimostrano che CAME supera Adam e Adafactor, ottenendo un miglioramento della precisione di validazione del 3.4% e una significativa riduzione dei requisiti di memoria, specialmente con batch size elevati.  CAME offre prestazioni comparabili a quelle di Adam e Adafactor su compiti downstream, pur riducendo l'utilizzo di memoria.  Inoltre, CAME si dimostra efficace anche con batch size ridotti, superando gli ottimizzatori memory-efficient esistenti.  In sintesi, CAME rappresenta un'ottimizzazione promettente per l'addestramento di modelli linguistici di grandi dimensioni, offrendo un compromesso ottimale tra velocità e efficienza di memoria.</sample>
    <sample id="193">Il testo non specifica il numero di annotatori impiegati per creare il set di dati iniziale.</sample>
    <sample id="194">Jenny è una studentessa di dottorato all'Università di Carnegie Mellon. Il lavoro è stato collaborativo con la University of Washington e l'Allen Institute for AI, con Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap.</sample>
    <sample id="195">## Abstract

This paper introduces RoHT (Reasoning over Hierarchical Question Decomposition Tree), a novel framework for explainable question answering (XQA) that addresses limitations of existing approaches. While neuro-symbolic methods struggle with incomplete knowledge bases and decompose-based methods face challenges with natural language diversity, RoHT leverages hierarchical question decomposition to integrate knowledge from heterogeneous sources effectively.

RoHT builds a Hierarchical Question Decomposition Tree (HQDT) to understand the compositional structure of complex questions, breaking them down into atomic questions.  A probabilistic reasoning process then fuses knowledge from knowledge bases (KBs) and text corpora at different levels of the tree.  The framework employs a scheduler to select appropriate knowledge sources for each node, executors to retrieve answers with associated probabilities, and an aggregator to combine candidate answers.  

Experiments on challenging datasets KQA Pro (with incomplete KB and Wikipedia) and Musique (with Wikipedia and Wikidata) demonstrate RoHT's superior performance compared to existing methods like TransferNet and EX(SA).  On KQA Pro, RoHT outperforms KB QA methods and shows significant gains with the addition of Wikipedia.  On Musique, RoHT significantly improves F1 scores compared to state-of-the-art text models.  The results highlight the effectiveness of explicit question decomposition and probabilistic reasoning for achieving robust and explainable XQA, particularly when integrating diverse knowledge sources.  RoHT's hierarchical approach allows for flexible knowledge utilization and addresses the challenges of both knowledge scarcity and natural language complexity.</sample>
    <sample id="196">"I saw Bart and Lisa"</sample>
    <sample id="197">I quattro modelli all'avanguardia valutati in questo studio sono stati selezionati per l'analisi.</sample>
    <sample id="198">La valutazione dell'accettabilità dei modelli nell'intera finestra di contesto è necessaria perché i modelli linguistici moderni hanno finestre di contesto sempre più lunghe e la pipeline MPP attuale non le gestisce, quindi è cruciale valutare l'accettabilità attraverso l'intera finestra di contesto.</sample>
    <sample id="199">La formazione attraverso la modalità multilingue ha causato un calo delle prestazioni in sette dataset, mentre ha migliorato le prestazioni in tre dataset. Questo fenomeno è noto come la "Maledizione della Multilingua".</sample>
    <sample id="200">Gli annotatori conoscono il nome delle entità, ma non necessariamente le entità stesse.</sample>
    <sample id="201">Le metriche MT utilizzate sono state le metriche neurali state-of-the-art e la valutazione umana basata su esperti.</sample>
    <sample id="202">Sì, il regresso nella generalizzazione influisce su specifici tipi di NER. Il nostro studio ha dimostrato che la performance di alcuni modelli diminuisce a causa del regresso temporale, ovvero la performance peggiora con una maggiore distanza temporale tra i dati di addestramento e quelli di test.</sample>
    <sample id="203">La posizionalità nella NLP è importante perché i dataset e i modelli possono riflettere e amplificare le prospettive di determinati gruppi demografici, portando a performance differenziali tra popolazioni e a escludere o penalizzare i gruppi sottorappresentati.</sample>
    <sample id="204">Gli LLM multilingue come Codex e BLOOM sono stati ancora inadeguati per le attività di parsing semantico cross-lingua.</sample>
    <sample id="205">## Abstract

This work investigates the propagation of political biases from pretraining data to language models and their impact on downstream NLP tasks. Leveraging the C4 corpus, we demonstrate that language models exhibit varying political leanings, with GPT-4 leaning more liberal than other models like BART.  We further explore how these biases are amplified through further pretraining on partisan corpora, revealing a shift towards more polarized ideological coordinates, particularly after 2017. 

To assess the implications of these biases, we evaluate language models on hate speech and fake news detection.  Our results show that left-leaning models exhibit stronger performance in detecting hate speech targeting minority groups, while right-leaning models are better at identifying hate speech directed towards specific demographics.  These findings highlight significant fairness issues, with models potentially exacerbating existing societal biases. 

We conclude that mitigating political biases in language models presents a complex dilemma.  Simply removing political opinions risks propagating biases, while attempting to sanitize data could lead to censorship or exclusion.  This work underscores the urgent need to address the fairness implications of language model political leanings, acknowledging the inherent challenges in defining neutrality and mitigating bias in large-scale datasets.  The research highlights the potential for these biases to have significant real-world consequences, particularly in applications like hate speech detection and fake news identification.</sample>
    <sample id="206">Transfer learning from debate and expansion/comparison (CE) tasks.</sample>
    <sample id="207">I test set più recenti utilizzati per valutare le capacità di PaLM sono i set di test WMT.</sample>
    <sample id="208">Ecco tre raccomandazioni proposte dagli autori alla fine:

1. Affrontare i positivi stereotipi e le narrazioni essenzialistiche.
2. Utilizzare un approccio intersezionale per studiare i pregiudizi e i danni.
3. Aumentare la trasparenza sui metodi di mitigazione dei pregiudizi.</sample>
    <sample id="209">Il metodo proposto migliora la qualità della generazione di script sia in termini di completezza semantica che di fedeltà alle restrizioni, superando le prestazioni dei modelli linguistici di grandi dimensioni (LLM) e dimostrando che i modelli più piccoli e specializzati possono eccellere quando addestrati su dataset appropriati.</sample>
    <sample id="210">Shuheng.</sample>
    <sample id="211">Sì, i risultati e il set di dati nell'articolo possono essere utilizzati come parametri di riferimento.</sample>
    <sample id="212">Il modello T5 viene utilizzato nell'articolo.</sample>
    <sample id="213">OFA</sample>
    <sample id="215">This paper argues for the validity of symmetric dependency structures in coordination, contrasting them with asymmetric approaches. The authors propose that dependency length minimization, a principle of syntactic processing, favors shorter dependencies. They demonstrate this principle through empirical analysis of coordination structures in the Penn Treebank, focusing on the length of conjunct dependencies.

The study reveals that a tendency for the left conjunct to be shorter is observed when the governor (the coordinating element) is positioned on the left or absent. This effect is more pronounced with larger length differences between the conjuncts. Conversely, when the governor is on the right, this length-minimizing tendency disappears. 

The authors further support their argument by analyzing statistics on conjunct lengths (syllables and characters) and observing that left conjuncts are generally shorter in coordination, particularly when the governor is on the left.  The paper concludes that these findings provide evidence against asymmetric dependency structures, such as those proposed by Mel'čuk and Hudson, and support the idea of symmetric structures where the coordination is headed by a single element, like a conjunction. The study highlights how dependency length minimization, a fundamental principle in syntactic processing, plays a crucial role in determining the preferred structure of coordination.</sample>
    <sample id="217">"Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation" addresses the limitations of existing controllable dialogue generation (CDG) methods, which primarily focus on single attributes and struggle with multi-attribute scenarios. The work highlights the lack of generation capability in current models and the challenges posed by limited annotated data and a need for unified evaluation metrics.

The authors introduce DCG, a Disentangled Controllable Generation model built upon the DialoGPT framework, that learns attribute concepts from seen values using a disentanglement loss.  A key innovation is the introduction of a unified reference-free evaluation framework, MAE, applicable to various attribute granularities.  Two benchmarks are established to demonstrate the effectiveness of DCG and MAE.

DCG utilizes compositional prompts, combining attribute-oriented and task-oriented prompts to guide the model. Attribute-oriented prompts focus on specific attribute values, while task-oriented prompts guide the model with global features. A disentanglement loss further enhances the model's ability to distinguish between different attribute combinations.

Experiments on DailyDialog-CG show that DCG outperforms baselines in both attribute controllability and text equality, even with minimal performance drop on established metrics.  The study demonstrates DCG's ability to generalize from seen attributes to unseen combinations, and the MAE framework correlates well with human judgments.  The research concludes that DCG effectively tackles compositional generalization in multi-attribute CDG, offering a promising approach for more flexible and controllable dialogue generation.</sample>
    <sample id="218">David Vilar, Google Translate.</sample>
    <sample id="219">This research addresses the challenge of extracting financial signals from annual reports (Form 10-K) by leveraging the inherent similarity between yearly reports. The work introduces a novel compare-and-contrast multistage pipeline for highlighting relevant information within these reports. The core idea is to identify the rationale (words) that explain the relationship between a target report and its previous year's report. This is achieved through a highlighting task where the model predicts the importance of words in context.

The pipeline consists of document segmentation, relation recognition, and out-of-domain/in-domain fine-tuning.  The relation recognition stage classifies report pairs into three types: highly similar (e.g., regulations), revised (syntactically similar but semantically different), and mismatched (new information).  Fine-tuning utilizes a combination of external (eSNLI) and in-domain data, employing soft labeling techniques to mitigate issues with low-quality pseudo-labels.  The model is trained with a mix of cross-entropy and KL divergence losses.

Evaluation is performed on both eSNLI and a newly released FINAL dataset, using precision and PCC as metrics. The results demonstrate that the proposed domain-adaptive highlighting model achieves state-of-the-art performance on FINAL and maintains strong generalization capabilities on eSNLI.  Furthermore, the model shows promise in benefiting from simulation with mismatched pairs. The research concludes with a discussion of future work, including enhancements to the model and exploration of additional techniques for information retrieval.  The full paper and GitHub repository are available for further details.</sample>
    <sample id="220">Vasudha è una dottoranda in informatica presso la Stony Brook University.</sample>
    <sample id="221">German to English.</sample>
    <sample id="222">## Abstract

This work addresses the challenge of domain adaptation in open-domain question answering (QA), where models trained on general-purpose corpora like Wikipedia struggle to generalize to specialized domains. We investigate data interventions to enable out-of-domain generalization, focusing on the types of dataset shifts encountered when adapting to new domains. Our experiments utilize a Wikipedia-based source domain and evaluate performance on seven target datasets spanning six domains, including biomedical and specialized news.

We explore two data intervention methods: few-shot learning and zero-shot learning. Few-shot interventions leverage a small number of target domain examples to prompt large language models for generating additional training data, leading to performance improvements in both retriever and reader models. Zero-shot interventions control the interactions between question, answer, and context to understand their impact on model learning. We find that cloze-style questions are more effective than standard WH questions for zero-shot adaptation.

Furthermore, we identify the type of dataset shift – no shift, concept shift, covariate shift, and full shift – using a likelihood-based compatibility measure for retriever and reader models. This allows us to map target datasets onto a 2D grid, revealing that few-shot interventions are effective for all shift types, while zero-shot interventions are more beneficial for concept and covariate shifts.  We conclude that understanding the nature of dataset shift is crucial for selecting appropriate data interventions to maximize performance in open-domain QA. Our experiments demonstrate that data interventions can improve reader performance by up to 24%, highlighting the importance of targeted adaptation strategies.</sample>
    <sample id="223">Shangbin.</sample>
    <sample id="224">Durante gli esperimenti sono stati studiati i seguenti modelli: MASSalign e long-mBART, base mBART.</sample>
    <sample id="225">Il dataset MultiInstruct contiene 62 attività diverse. 53 di queste vengono utilizzate per l'addestramento e 5 vengono utilizzate per il test.</sample>
    <sample id="226">Il testo menziona due autori: Regina Stodden e Omar.</sample>
    <sample id="227">The field of language models has achieved remarkable progress in natural language processing, but a key challenge remains: grounded language understanding. This involves translating natural language into executable plans or programs within a specific environment, crucial for applications like smart assistants, semantic search, and robotic control. Current approaches often rely on language models for direct plan generation, but these plans can be grammatically incorrect or invalid, failing to execute correctly.

This work introduces a novel framework, Pangu, that shifts the focus from generation to discrimination. Pangu utilizes a symbolic agent to interact with the environment and propose candidate plans, while a language model is employed solely to score and rank these candidates. This approach avoids the language model's responsibility for plan validity and grammar, leveraging its strengths in discrimination.

Experiments on knowledge-based question answering demonstrate Pangu's effectiveness across various language models (BERT, T5, Codex) and training paradigms (fine-tuning, in-context learning). Pangu achieves strong performance and sample efficiency, even with limited examples. Notably, Pangu exhibits robustness to non-independent and identically distributed (non-i.i.d.) data, contrasting with autoregressive models that tend to overfit.

The central takeaway is that discrimination offers a more promising strategy for grounded language understanding than generation, potentially unlocking the full potential of language models in real-world applications. Pangu's framework provides a flexible and effective approach to bridging the gap between language understanding and environmental execution.</sample>
    <sample id="228">AG News, MIND, SST2 e Enron Spam.</sample>
    <sample id="229">## Abstract

This paper addresses the challenge of automatically detecting improvable claims in argumentative writing, a crucial aspect of text revision. We introduce two novel tasks: Suboptimal Claim Detection (determining if a claim requires revision) and Claim Improvement Suggestion (identifying specific quality issues for revision).  We explore the complexities of leveraging revision-based data, focusing on argumentative text extracted from collaborative online debate platforms like Kialo. 

Our work highlights four key challenges: Representativity and Reliability of the dataset, Model Complexity and Architecture for capturing subtle revisions, the dependence of argument quality on contextual information (including domain knowledge and social context), and the presence of Topical and User Bias within revision histories. We investigate these challenges through experiments and propose strategies for addressing them. 

We demonstrate that revision-based data can be effectively utilized for claim assessment, particularly in detecting suboptimal claims. Furthermore, modeling the distance between claim versions proves beneficial.  The impact of contextual information is task-dependent and varies based on the identified quality issues.  Our findings contribute to the development of more accurate and nuanced argumentative writing support systems. We conclude that a deeper understanding of revision patterns, coupled with careful consideration of data characteristics, is essential for building effective claim improvement tools.</sample>
    <sample id="231">NACHOS è un dataset di dati medici estratti dal web.</sample>
    <sample id="232">David Vilar.</sample>
    <sample id="233">Simultaneous Speech Translation (SimuST) aims to translate spoken language into text in real-time, enabling cross-lingual communication. Current SimuST models face challenges with long training procedures, optimization objectives, and the need to maintain multiple models for varying latency requirements.

This paper introduces EDAtt (Encoder-Decoder Attention), a strategy for achieving low-latency SimuST by leveraging existing offline ST models. EDAtt utilizes cross-attention to determine when to emit partial translations. It decides to emit a word if the attention weights are not concentrated towards the last few speech frames, indicating sufficient stability in the received information. This allows for handling latency by emitting partial translations based on attention patterns.

Experiments on German demonstrate that EDAtt outperforms existing strategies like Wait-k and Local Agreement, achieving faster translation speeds while maintaining high translation quality (measured by BLEU score).  The computational-aware latency metric further highlights EDAtt's efficiency.  The authors release open-source code and models to promote reproducibility.  EDAtt's approach allows for a single model to handle different latency regimes, simplifying the development and deployment of SimuST systems. The key innovation lies in intelligently managing the translation process based on attention distribution, enabling real-time translation with reduced latency.</sample>
    <sample id="234">La strategia di prompting ha un impatto significativo sulla performance dei modelli linguistici di grandi dimensioni per la traduzione. In particolare, la differenza di performance può essere di più di un punto BLEURT, e in casi estremi può arrivare fino a 40 punti. Tuttavia, per le strategie di prompting più brevi (uno o zero-shot), la forma esatta del prompt non ha un grande impatto, ma la qualità dei dati di esempio è cruciale. Per le strategie di prompting più lunghe (come il cinque-shot), la qualità degli esempi è più importante della loro somiglianza con la frase di input.</sample>
    <sample id="235">Patrick Fernandes, Emmy Liu, André F. T. Martins, Graham Neubig.</sample>
    <sample id="236">```
Le 5 istruzioni scritte da esperti sono incluse in ogni task del dataset MultiInstruct.
```</sample>
    <sample id="237">Gli autori propongono un test diagnostico basato su un compito di risoluzione della coreferenza, che valuta la capacità dei modelli di utilizzare informazioni provenienti da diverse fonti (pre-training e contesto di inferenza). Il test introduce tre impostazioni: "Background-Pretrain", "Background-Both" e "Background-Inference", che variano la disponibilità di conoscenza di background ed entità specifiche.</sample>
    <sample id="238">MeetingBank is a new benchmark dataset for meeting summarization, addressing the need for high-quality, publicly available meeting data. The dataset comprises 1,366 City Council meetings, including transcripts, reference summaries, and URLs, collected using the Speechmatics API and Boston City Council meeting websites.  The data includes meeting statistics like duration, speaker count, and token counts, alongside summarization instances with average sentence and token lengths.  Coverage and density scores are used to analyze the abstraction level of summaries, revealing a tendency towards verbatim reporting.

The dataset was evaluated using a range of summarization systems, including extractive methods (Oracle, LEAD, LexRank, TextRank) and abstractive models (BART-Large, Pagasus, Longformer, DialogLM, HMNet), as well as GPT-3.  Extractive systems showed promising ROUGE-2 scores, while DialogLM achieved the highest ROUGE-2 among abstractive models. GPT-3 performed well in fluency and coherence but less so in informativeness and factuality according to automatic metrics.  Human evaluation using five criteria (informativeness, factuality, fluency, coherence, redundancy) revealed that GPT-3 achieved the highest overall scores, particularly in fluency and coherence.

MeetingBank is intended to facilitate research in meeting summarization and provide insights into City Council decision-making. The dataset is publicly available for download and use. The research highlights the need for improved automatic evaluation metrics that better align with human preferences and emphasizes the importance of capturing key discussion points in meeting summaries.</sample>
    <sample id="239">Buongiorno a tutti, mi chiamo David Vilar e presenterò una breve revisione del paper "Prompting PaLM for Translation: Assessing Strategies and Performance". Questo è un lavoro congiunto con i miei colleghi di Google Translate. PaLM è un modello linguistico di grandi dimensioni con 540 miliardi di parametri presentato l'anno scorso, nel 2022. È stato addestrato su una vasta collezione di testo, che comprende 780 miliardi di token. Al momento della pubblicazione, ha raggiunto risultati all'avanguardia in centinaia di task di NLP. In questo lavoro, presentiamo il primo studio sistematico sull'utilizzo di prompting per i modelli linguistici di grandi dimensioni per la traduzione automatica. Abbiamo valutato la capacità di transizione di tali modelli utilizzando le migliori pratiche della comunità MT. Ciò comporta l'utilizzo dei test set più recenti per evitare sovrapposizioni tra i dati di test e i dati di addestramento del modello linguistico. E abbiamo confrontato con i sistemi all'avanguardia, quindi il sistema con le migliori prestazioni, ovvero la valutazione WMT. Utilizziamo metriche di traduzione MT neurali all'avanguardia e, inoltre, mostriamo risultati di valutazione umana basati su esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione del prompting. Il prompting ha un grande impatto sulle prestazioni dei LLM per la traduzione, come possiamo vedere in un semplice esperimento, in cui abbiamo utilizzato il prompting one-shot e fornito due diverse istruzioni per ogni frase. La maggior parte delle frasi, 516 su 1.000, la differenza osservata è superiore di più di un BLEURT point. E questo può andare, in casi estremi, fino a 40 BLEURT points. Quindi, è importante selezionare una buona strategia di prompting. Nei nostri esperimenti, ci siamo accontentati di una strategia di prompting a 5-shot, in cui abbiamo semplicemente contrassegnato ogni frase che forniamo al sistema con la lingua in cui è scritta. In questo esempio qui, in cui effettuiamo la traduzione dal tedesco all'inglese, le frasi tedesche, le frasi sorgente, sono contrassegnate con il colon tedesco e le traduzioni in inglese sono contrassegnate con il colon inglese. Abbiamo visto che la forma effettiva del prompting non ha un grande impatto nel caso di poche istruzioni brevi. È cruciale per il prompting zero e one-shot. E quando passiamo, come nel nostro caso, al prompting a cinque istruzioni, c'è quasi nessuna differenza nella forma effettiva del prompting. Sono i campioni che pesano di più. Il riepilogo dei nostri risultati sperimentali è che la qualità dei campioni è più importante della somiglianza con la frase sorgente. Quindi, è importante selezionare i campioni da traduzioni di alta qualità. In particolare, confrontiamo la selezione dei prompt dai dati di addestramento per le valutazioni WMT sui dati di sviluppo. I dati di sviluppo sono molto più curati e di qualità superiore rispetto ai dati di addestramento, quindi sono più rumorosi. E i loro risultati sono migliori quando si utilizza il dato di sviluppo. Tuttavia, i sistemi all'avanguardia specializzati hanno un vantaggio sostanziale rispetto alle traduzioni PaLM. Ma PaLM si avvicina abbastanza a un sistema commerciale. Nel nostro caso, abbiamo scelto di valutare con Google Translate. Gli approfondimenti che abbiamo ottenuto dalla valutazione umana che abbiamo condotto utilizzando il framework MQM hanno mostrato che la fluidità di PaLM è comparabile ai sistemi all'avanguardia, ma la principale differenza deriva dall'accuratezza. In particolare, gli errori più comuni sono gli errori di omissione. Sembra che PaLM scelga di produrre una traduzione più scorrevole, a volte eliminando parti della frase sorgente che sono state tradotte. Tuttavia, la categoria "Stile/Inadeguato" per PaLM è inferiore rispetto ai sistemi all'avanguardia, il che è un ulteriore segnale che PaLM fornisce output davvero fluenti, ma con ancora alcuni problemi di accuratezza. E questo è tutto per questa breve panoramica. Per maggiori dettagli, si prega di venire alla presentazione completa del paper. Grazie mille.</sample>
    <sample id="240">Ciao, sono Dawei, dottorando presso l'Università di Saarland in Germania. In questo video, vorrei presentare il nostro recente lavoro "Weaker Than You Think: A Critical Look at Weakly Supervised Learning". Questo è un lavoro congiunto con Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow. Vorrei iniziare con una breve introduzione al weakly supervised learning e al learning supervisionato debole. Nel learning supervisionato debole, non si etichettano i dati manualmente. Invece, etichettiamo i dati utilizzando fonti di etichette deboli, come semplici regoleuristiche, knowledge base o crowdsourcing a bassa qualità, come illustrato nella figura sulla destra. Rispetto alle annotazioni umane, le annotazioni più deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità delle annotazioni è errata. Quando si addestrano direttamente le reti neurali su dati etichettati debolmente, le reti neurali tendono a memorizzare il rumore delle etichette e non generalizzano. Nel learning supervisionato debole, vengono proposte algoritmi di addestramento per addestrare robustamente le reti neurali sotto tale rumore delle etichette in modo che i modelli addestrati generalizzino ancora bene. Negli ultimi lavori di WSL, WSL sta per Weakly Supervised Learning, una comune affermazione è che le persone dicono che si addestrano solo sui dati etichettati debolmente e si ottengono prestazioni elevate sui set di test puliti. Tecnicamente, questa affermazione non è sbagliata, ma c'è un intoppo, che è che si assume che esista un set di validazione pulito disponibile per la selezione del modello. Non possiamo fermarci in questo problema, ma questo implica che sono necessarie ulteriori annotazioni manuali nel learning supervisionato debole. Ma come un elefante in una stanza, questa necessità viene spesso trascurata. La suddetta incertezza pone tre domande di ricerca. Prima, i dati di validazione puliti sono necessari per WSL o possiamo usare un set di validazione rumoroso? Secondo, se sono richiesti dati puliti, o se i dati puliti sono obbligatori per il funzionamento di WSL, quanti campioni puliti dobbiamo avere? Infine, dovremmo utilizzare solo i campioni puliti per la validazione, o ci sono modi migliori per utilizzarli? Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti. Innanzitutto, troviamo che i metodi WSL recenti richiedono effettivamente set di validazione puliti per funzionare correttamente. Altrimenti, c'è un grande calo delle prestazioni. Come mostra questa figura, se non ci sono set di validazione puliti, i modelli addestrati non possono generalizzare oltre le etichette deboli originali, il che rende l'addestramento inutile. Ciò indica che gli approcci WSL richiedono effettivamente dati etichettati puliti per funzionare correttamente e il costo di annotazione per ottenere set di validazione puliti non dovrebbe essere trascurato. Il nostro secondo risultato è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci WSL a ottenere prestazioni migliori, come mostra la figura sulla sinistra. Tipicamente, abbiamo bisogno solo di 20 campioni per classe per ottenere prestazioni elevate. Ma non è finita qui, perché se decidiamo di accedere ai dati puliti, allora l'addestramento sui dati puliti da soli raggiungerà prestazioni ancora migliori. La figura sulla destra mostra la differenza di prestazioni tra gli approcci di fine-tuning e gli approcci WSL, che utilizzano i dati puliti per la validazione. Come possiamo vedere, se abbiamo 10 campioni per classe, il fine-tuning diretto inizia a superare gli approcci WSL. Infine, l'aumento delle prestazioni dichiarato nei precedenti approcci WSL può essere facilmente ottenuto consentendo di continuare il fine-tuning sui set di validazione puliti. Come possiamo vedere dalle figure, il modello vanilla, denominato FTw, inizialmente sottoperforma rispetto agli approcci WSL più complessi, come COSINE. Tuttavia, se consentiamo di continuare il fine-tuning sui campioni puliti, allora FTw raggiunge prestazioni equivalenti ad altri metodi. Quindi in pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio su disco. In sintesi, abbiamo dimostrato che gli approcci WSL recenti richiedono dati manualmente annotati puliti per funzionare correttamente. Il loro guadagno di prestazioni e la loro praticità sono fortemente sovrastimati. Le nostre raccomandazioni concrete per i lavori futuri sono le seguenti. In primo luogo, segnalare i criteri di selezione del modello. Ad esempio, segnalare se la selezione del modello è effettuata tramite set di validazione puliti. In secondo luogo, gli approcci WSL dovrebbero essere confrontati con le baseline di few-shot learning, poiché entrambi operano su dati puliti. In terzo luogo, il fine-tuning continuo è una baseline semplice ma potente che dovrebbe essere presa in considerazione nei lavori futuri di WSL. Infine, abbiamo open-sourced il nostro codice. Puoi trovarlo tramite il codice QR su questa slide. Ti invito a dare un'occhiata. Grazie e goditi la conferenza.</sample>
    <sample id="241">## Abstract

This paper addresses the limitations of current automated misinformation detection systems, which often suffer from unrealistic evaluation methods and a lack of human-centric design.  Existing systems frequently rely on retrospective datasets and are vulnerable to leaked counter-evidence, failing to capture the dynamic nature of misinformation spread. Furthermore, they often neglect the crucial role of human content moderators, either excluding them entirely or relegating them to the final verification step.

We propose a novel evaluation framework for developing more effective and human-aligned misinformation detection systems. Our framework emphasizes an end-to-end approach, integrating human feedback throughout the process, from raw tweet analysis to actionable outputs for human review. We concretely implement and evaluate this framework using a case study of COVID-19 treatment misinformation.

Our system comprises two main components: claim extraction, leveraging a T5 model to identify and rank potentially misleading claims, and policy violation verification, utilizing a BERT-based stance classification model to flag tweets violating Twitter's policies.  We operationalize early detection as the identification of unapproved treatments before their public debunking.  Evaluation reveals a 65% accuracy in policy violation detection and a significant human workload reduction, with 124.2 policy violations confirmed per human hour.  

This work provides a more realistic and human-centric approach to evaluating misinformation detection systems, offering valuable insights for future research and development in this critical area.  It also provides an outside perspective on the development and evaluation of misinformation detection systems.</sample>
    <sample id="242">I metodi di valutazione comuni per i sistemi di dialogo includono valutazioni a livello di turno (Likert scale) e valutazioni a livello di dialogo (comparazioni pairwise).</sample>
    <sample id="243">Il lavoro è stato condotto in collaborazione con Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap.</sample>
    <sample id="244">Servin è un giudice e Kea è un panettiere.</sample>
    <sample id="245">This paper introduces a two-step pipeline for identifying high-agreement workers on Amazon Mechanical Turk (MTurk) for summarization tasks. The motivation stems from the limitations of automatic metrics and the need for better MTurk recruitment practices. The pipeline employs pre-task qualifications, including a qualification task assessing annotator ability to evaluate summaries across six dimensions and an endurance task testing workload capacity.  Workers are categorized into gold, silver, bronze, and block tiers, with gold and silver workers passing both stages.  The pipeline achieves high inter-annotator agreement (IAA) compared to experts, with a best Krippendorff's Alpha of 0.443.  A reference-based task further validates the pipeline's performance, yielding a Krippendorff's Alpha of 0.534.  The study compares the pipeline to Baseline MTurk workers (using a statistical filter) and CloudResearch MTurk workers, finding that the pipeline offers comparable quality at a lower cost and with higher agreement.  The analysis of correctness across annotation sources reveals strong correlation between pipeline and CloudResearch workers, while also noting potential limitations in guaranteeing correctness.  The authors conclude that the pipeline provides a cost-effective and efficient method for identifying high-quality annotators for large-scale summarization projects, with future work focusing on expanding the pipeline's applicability to different languages, tasks, and platforms.</sample>
    <sample id="246">Sì, il codice e il dataset sono disponibili su GitHub.</sample>
    <sample id="247">## FACTKG: Fact Verification via Reasoning on Knowledge Graphs

This paper introduces FACTKG, a novel dataset and task for fact verification leveraging knowledge graphs (KGs) as evidence. Existing fact verification datasets primarily rely on text or tables, lacking the direct reasoning capabilities offered by KGs. FACTKG addresses this gap by providing a collection of claims and corresponding evidence extracted from DBpedia, a large-scale KG. The dataset includes claims in both written and colloquial styles, with labels indicating whether the claim is supported or refuted.

The task requires retrieving relevant evidence from DBpedia and verifying the claim through reasoning, encompassing five reasoning types: one-hop, conjunction, existence, multi-hop, and negation.  The dataset's structure facilitates the representation of claims and evidence using triples, enabling efficient reasoning.  

To enhance practical applicability, FACTKG incorporates colloquial claim styles, achieved through transfer learning and presupposition templates.  We evaluate the effectiveness of FACTKG by constructing baseline models, including claim-only baselines and a GEAR model utilizing graph evidence.  Our results demonstrate that all baselines outperform a majority class baseline, with the GEAR model achieving state-of-the-art performance by leveraging KG evidence.  FACTKG offers a valuable resource for advancing fact verification research and enabling consistency checks between knowledge and natural language in various applications, such as dialogue systems.  The dataset is publicly available for download.</sample>
    <sample id="248">Il team NLPositionality ha utilizzato un approccio di ri-annotazione per ottenere un set di dati più ampio e diversificato di annotatori, con l'obiettivo di bilanciare le rappresentazioni demografiche. Tuttavia, i risultati indicano che i modelli e i dataset tendono ad allinearsi maggiormente con i paesi di lingua inglese e le persone con un'istruzione universitaria, lasciando indietro i gruppi demografici meno rappresentati.</sample>
    <sample id="249">Non sono state perturbate le frasi nel dominio accettabile. L'analisi ha dimostrato che le perturbazioni delle frasi nel dominio accettabile hanno prodotto risultati simili alle perturbazioni delle frasi nel dominio inaccettabile.</sample>
    <sample id="250">Una valutazione dimensionale di un modello di conversazione AI significa valutare il modello su diversi aspetti della qualità della conversazione, come la rilevanza delle risposte, la coerenza, l'empatia, ecc., invece di affidarsi a una singola valutazione complessiva. Questo permette di identificare i punti di forza e di debolezza del modello in modo più preciso.</sample>
    <sample id="251">Jingwei Yi from the University of Science and Technology of China.</sample>
    <sample id="252">```
U-CREAT: Unsupervised Case Retrieval using Events Extraction, presented by Sai Kiran Tanikella from IIT Kanpur, addresses the challenge of Prior Case Retrieval (PCR) in legal domains.  PCR aims to retrieve relevant past precedents cited within a query document.  The work introduces the IL-PCR dataset, a benchmark for PCR tasks containing 7,070 Indian legal cases with a high average citation count, offering a more comprehensive test bed than existing datasets like COLIEE’21.

The core contribution is the U-CREAT pipeline, an unsupervised approach leveraging event extraction.  Event extraction, utilizing dependency parsing and verb category analysis, identifies subject-verb-object triplets representing events within documents.  The pipeline computes an interaction matrix between query and candidate events, identifying common events and enabling ranking of candidate documents.

Experiments with various models – count-based, transformer-based, and event-based – demonstrate U-CREAT's effectiveness. While transformer models show limited performance, event-based models significantly outperform baselines, achieving higher F1 scores and lower inference times.  The Event Filtered Documents model achieves the best performance.  U-CREAT surpasses existing approaches, including recent supervised methods, on the COLIEE’21 dataset.  The research highlights the importance of event-based techniques for efficient and effective prior case retrieval, particularly in the context of Indian legal systems.  The work opens new avenues for research and development in this area.
```</sample>
    <sample id="253">DisorBERT is a novel double domain adaptation model designed for detecting signs of mental disorders in social media posts. The research addresses the challenge of insufficient annotated data by leveraging knowledge from a related domain (Reddit) and a lexicon to improve model performance. The approach combines a base language model (BERT) with domain-specific information, utilizing guided masking to encourage the model to focus on semantically important words related to mental health. 

The study utilizes the eRisk dataset to evaluate DisorBERT against baseline models, demonstrating a balanced performance with good precision and recall. Analysis of generated words reveals that DisorBERT exhibits a stronger bias towards words associated with mental disorders compared to BERT, indicating a more specialized understanding of the domain. Visualization techniques highlight the importance of words like "anxious" and "medication" in identifying depression. 

The results indicate that DisorBERT effectively captures subtle indicators of mental disorders in social media interactions, achieving superior performance compared to existing models like MentalBERT. Future work will focus on exploring different lexical resources and incorporating clinical data to further enhance the model's accuracy and robustness. This research contributes to the development of technology capable of early detection and support for individuals experiencing mental health difficulties.</sample>
    <sample id="254">This research presents a novel framework for document-level distant relation extraction (DS-RE) that addresses the noise problem in distant supervision (DS) data. DS-RE aims to identify relationships between entities within a document, but DS data often contains inaccurate labels due to the lack of human annotations.  Current methods mitigate this noise with pseudo-labeling, but false positives can still occur, leading to incorrect relation predictions.

Our approach employs uncertainty-guided label denoising to improve the quality of DS data. We first train a pre-denoising DocRE model using both DS and human-annotated data to generate pseudo-labels.  Crucially, we introduce instance-level uncertainty estimation to assess the reliability of each prediction, particularly for overlapping relations.  This is achieved through Monte Carlo dropout, modified to account for the challenges of overlapping relations.  We then utilize dynamic class uncertainty thresholds to filter out pseudo-labels with high uncertainty.  Finally, we employ a multi-phase training strategy to iteratively refine the DS data, further boosting performance.

Experiments on public datasets demonstrate that our framework significantly outperforms existing baselines.  The key contributions are: a framework for uncertainty-guided label denoising, an instance-level uncertainty estimation method for overlapping relations, a dynamic uncertainty thresholding strategy for long-tail classes, and substantial performance gains.  This work offers a practical solution to improve the reliability of DS data, paving the way for more accurate and robust document-level relation extraction.</sample>
    <sample id="255">La forma del prompting è importante solo per i casi di prompting zero e one-shot.</sample>
    <sample id="257">Gli autori hanno valutato quattro modelli di dialogo state-of-the-art.</sample>
    <sample id="258">Chiang Cheng-Han presenta il lavoro "Can Large Language Models Be an Alternative to Human Evaluation?". L'articolo propone di utilizzare i modelli linguistici di grandi dimensioni (LLM) per valutare la qualità del testo in ambito NLP, fornendo loro istruzioni e campioni da valutare. L'idea è che, se gli LLM comprendono le istruzioni, possano fornire valutazioni significative.

Il lavoro si distingue per essere stato uno dei primi a esplorare questa possibilità, in contrasto con la crescente adozione di metodi di valutazione basati su LLM. Per validare l'approccio, viene condotto un esperimento su storie generate da GPT-2 e da umani, valutate da LLM su quattro attributi: grammatica, coerenza, gradevolezza e rilevanza. Le valutazioni degli LLM vengono confrontate con quelle di valutatori umani esperti.

I risultati indicano che, in generale, i valutatori umani preferiscono le storie scritte a mano, anche se alcuni modelli più piccoli non mostrano una preferenza chiara. Tuttavia, i modelli Davinci e ChatGPT mostrano una preferenza significativa per il testo umano. L'articolo esplora anche la coerenza tra le valutazioni degli LLM e degli umani, l'impatto delle variazioni nelle istruzioni e nei campioni, e i vantaggi e gli svantaggi dell'utilizzo di LLM per la valutazione rispetto all'approccio umano.  Il lavoro offre una panoramica completa sull'utilizzo degli LLM come alternativa alla valutazione umana, con ulteriori dettagli disponibili nella pubblicazione completa.</sample>
    <sample id="259">XSemPLR presenta un nuovo benchmark completo per la semantic parsing cross-linguale, affrontando la mancanza di risorse e la limitata copertura di lingue e rappresentazioni semantiche in studi precedenti. Il dataset XSemPLR include nove dataset in vari domini, cinque task di semantic parsing, otto rappresentazioni semantiche e ventidue lingue in quindici famiglie linguistiche.  Viene proposto un approccio di valutazione a sei livelli: Translate-Test, Monolingual, Monolingual Few-shot, Multilingual, Cross-lingual Zero-shot e Cross-lingual Few-shot transfer.

L'analisi confronta modelli Encoder-PTR (come XLM-R + PTR e mBERT + PTR) e Encoder-Decoder (come mBART e mT5) su diversi dataset. I risultati indicano che i modelli Encoder-Decoder ottengono le migliori prestazioni su tutti i dataset. L'addestramento con un mix di lingue migliora le prestazioni sia per Encoder-Decoder che per Encoder-PTR, con un'eccezione in cui l'inglese mostra un calo di performance.

La valutazione delle prestazioni cross-linguali rivela un significativo divario tra zero-shot e few-shot transfer, che si riduce con l'utilizzo di few-shot transfer.  Si evidenzia che i modelli multilingue come Codex e BLOOM non sono ancora sufficientemente performanti per la semantic parsing cross-linguale.  XSemPLR fornisce una piattaforma per la ricerca e lo sviluppo di modelli cross-linguali semanticamente più efficaci.</sample>
    <sample id="260">Un solo autore, Jingwei Yi.</sample>
    <sample id="261">Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli.</sample>
    <sample id="262">L'articolo è stato presentato da Siyu Yuan da Fudan University. Non viene specificato il numero totale di autori.</sample>
    <sample id="263">## Abstract

This work addresses the instability of in-context learning in large language models (LLMs), a popular paradigm for utilizing LLMs. While prior research identifies search instability as a root cause, it lacks a systematic categorization of bias problems and mitigation strategies. We propose a typology of label biases in in-context learning, identifying a novel bias: domain-label bias, arising from the influence of the task corpus on model predictions.

We demonstrate that random in-domain words can significantly bias model predictions, particularly on tasks with strong domain-label bias, often leading to performance degradation even with prior calibration methods. To address this, we introduce domain-context calibration, a novel calibration method that utilizes random in-domain words as content-free text to estimate and mitigate biases across different label types. 

Our experiments on diverse datasets and models show that domain-context calibration significantly improves in-context learning performance, especially on tasks exhibiting higher domain-label bias.  We further analyze the effectiveness of domain-context calibration compared to existing methods, revealing that single predefined tokens are insufficient and that using random in-domain words yields superior results.  Our findings highlight the importance of considering domain-specific biases in in-context learning and demonstrate the efficacy of domain-context calibration as a robust mitigation strategy.  This work provides a systematic approach to understanding and addressing label biases in in-context learning, paving the way for more reliable and effective LLM applications.</sample>
    <sample id="264">TAVT (Towards Transferable Audio-Visual Text Generation) affronta la sfida della generazione di testo multimodale audio-visiva, dove l'annotazione dati è costosa e le soluzioni esistenti soffrono di scarsa generalizzazione tra domini. L'approccio proposto mira a superare queste limitazioni attraverso un framework modulare basato su un audio-visual meta-mapper, un encoder audio-visivo e un generatore linguistico, e un apprendimento contrastivo controfattuale.

Il meta-mapper mappa i concetti visivi tra domini in uno spazio semantico audio unificato, sfruttando l'analisi di cluster audio e token visivi. L'encoder audio-visivo utilizza un transformer con un meccanismo di valutazione del contributo di ciascuna modalità, e il contrastivo controfattuale ottimizza l'allineamento tra testo e audio.

Il framework TAVT utilizza un approccio di meta-apprendimento simile a MAML, selezionando un supporto e un query set per l'adattamento a nuovi domini.  I risultati sperimentali su benchmark come MSVD e MSR-VTT dimostrano che TAVT supera significativamente i modelli state-of-the-art, inclusi quelli basati su RNN e transformer, in cross-dataset e cross-domain settings, specialmente in domini a bassa risorse. L'analisi degli effetti delle caratteristiche audio conferma l'importanza dell'audio per le prestazioni. TAVT offre un'alternativa promettente per la generazione di testo multimodale audio-visiva, con particolare attenzione alla sua capacità di adattarsi a nuovi domini con dati limitati.</sample>
    <sample id="265">Vasudha</sample>
    <sample id="266">Adam Przepiórkowski.</sample>
    <sample id="268">Gli errori più comuni di PaLM sono omissioni, ovvero la rimozione di parti della frase originale durante la traduzione.</sample>
    <sample id="269">Ciao, sono James Finch. E sono Sarah Finch. E oggi vi parleremo di ABC-Eval, un nuovo approccio dimensionale per valutare i modelli di conversazione AI. Questo lavoro è stato svolto dal Emory NLP Lab guidato dal Professor Jinho Choi dell'Università Emory e in collaborazione con Amazon Alexa AI. Quindi, diciamo che hai sviluppato un modello di dialogo e vuoi vedere quanto è bravo a confrontarsi con lo stato dell'arte attuale. La pratica comune è utilizzare l'valutazione umana, come chiedere a giudici umani di selezionare quale tra due conversazioni è migliore o di valutare le conversazioni in base a una scala Likert. Questi approcci funzionano bene per fornire valutazioni olistiche della qualità del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto, potresti voler valutare molteplici dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più granulare. Un approccio è semplicemente chiedere ai giudici umani di valutare diverse dimensioni della qualità della chat, come la rilevanza delle risposte del modello utilizzando metodi comparativi o scale Likert esistenti. Tuttavia, crediamo che esista un approccio più preciso e affidabile per l'valutazione dimensionale del dialogo. Il nostro approccio tenta di ridurre la soggettività dell'valutazione umana annotando esplicitamente se ogni risposta del modello esprime determinati comportamenti, come rispondere con informazioni irrilevanti o contraddire se stesso. Chiamiamo questo approccio annotando i comportamenti nella chat o ABC-Eval in breve. Abbiamo sviluppato questo metodo per coprire in modo completo i comportamenti dei modelli di chat che sono stati suggeriti per influenzare la qualità della chat nella letteratura recente. ABC-Eval è in grado di misurare le percentuali con cui i modelli di chat commettono vari errori tematici. Ad esempio, ABC-Eval misura il numero di turni in cui un modello di chat ignora il suo partner o dice qualcosa di irrilevante, contraddice se stesso o il suo partner, allucina fatti errati o viola la conoscenza comune, e quando il modello riesce o fallisce a mostrare empatia. Per determinare quale tipo di valutazione è più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su 100 conversazioni uomo-bot per modello utilizzando ABC-Eval. Per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: scale Likert sui turni, scale Likert sui dialoghi e confronti pairwise a livello di dialogo. Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questo è il metodo standard per l'valutazione dei modelli di chat su molteplici dimensioni. Dall'analisi dei risultati di queste valutazioni, abbiamo scoperto che i label dei comportamenti ABC-Eval sono complessivamente più affidabili dei label raccolti dai metodi esistenti, come misurato dall'accordo inter-annotatore su 100 conversazioni doppiamente etichettate. Inoltre, i label ABC-Eval sono più predittivi della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come dimostrato da questa semplice analisi di regressione lineare. Ad esempio, puoi vedere come la proporzione di turni con contraddizioni tra se stesso e il partner spiega il 5% e il 10% della qualità della conversazione, mentre i punteggi di coerenza Likert a livello di turno spiegano solo il 4% o meno. Infine, abbiamo verificato se ciascuna metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare passo-passo. Puoi vedere come la combinazione di tutte le metriche ABC-Eval spiega oltre il 25% della qualità della conversazione, e man mano che le metriche vengono rimosse una alla volta, la maggior parte di esse comporta la perdita di una quantità decente di informazioni sulla qualità. D'altra parte, la combinazione di tutte le metriche Likert a livello di turno spiega molto meno della qualità complessiva, e poche di queste metriche portano informazioni uniche. Questi metriche ABC-Eval affidabili, informative e distinte ci consentono di valutare l'AI conversazionale con una risoluzione superiore rispetto a quanto siano in grado di raggiungere i metodi precedenti. Puoi vedere che nei risultati del nostro esperimento che diversi problemi rimangono ancora e sono stati quantificati con precisione. Ad esempio, i bot che abbiamo testato hanno violazioni della conoscenza comune in circa il 20% delle loro risposte. Producono informazioni irrilevanti in circa il 15% delle risposte e contraddicono se stessi o il loro partner circa il 10% delle volte. Con il rapido ritmo di miglioramento nel campo, molte di queste percentuali di errore potrebbero diminuire nei nuovi modelli rilasciati da nostro esperimento. Tuttavia, questo è proprio il motivo per cui è ancora più importante perseguire metriche di valutazione affidabili e precise per confrontare i modelli. Speriamo che ABC-Eval possa essere sfruttato da altri nel campo come un passo significativo in questa direzione. E ci auguriamo di vedere come l'AI conversazionale avanzerà nei prossimi mesi e anni. Grazie per aver guardato.</sample>
    <sample id="270">Emory NLP Lab, Emory University, in collaborazione con Amazon Alexa AI.</sample>
    <sample id="271">In this paper, CFT stands for "Fine-tuning".</sample>
    <sample id="272">John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams.</sample>
    <sample id="273">Ciao, mi chiamo Kayo Yin e presenterò il nostro lavoro intitolato "Quando la traduzione richiede contesto? Un'esplorazione basata sui dati, multilingue". Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig. Quindi, molte traduzioni dipendono dal contesto. Ad esempio, come tradurremmo "mole" in questa frase? Beh, se la frase precedente era "Potrebbe iniziare a diventare pericoloso se i ministri scoprissero", allora "mole" si riferisce a uno spia. Ma se la frase precedente era "Potrebbe essere qualcosa di serio, dottore?", allora "mole" si riferisce a una macchia di nascita. Quindi, a seconda del contesto, il significato della parola cambia e quindi anche la traduzione. Tuttavia, valutare quanto bene i modelli possono tradurre casi del genere è piuttosto difficile. Innanzitutto, solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus come BLEU incapaci di catturare queste traduzioni. E alcune persone hanno suggerito valutazioni mirate sulle traduzioni dipendenti dal contesto, ma questi risorse supportano solo un numero limitato di tipi di traduzioni dipendenti dal contesto e un numero limitato di lingue poiché in genere si basano sulla conoscenza del dominio e sulla curatela umana. In questo lavoro, cerchiamo di rispondere a due domande. Prima, quando la traduzione richiede contesto? E seconda, quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipende dal contesto durante la traduzione. In lavori precedenti, abbiamo introdotto CXMI come misura dell'utilizzo del contesto dai modelli di traduzione automatica. Questo è fatto misurando quanto informazioni il contesto C fornisce sulla target Y, dato lo source X. Si può pensare a CXMI come l'informazione acquisita dal fornire contesto al modello. In questo lavoro, estendiamo CXMI a Pointwise CXMI, che può misurare l'utilizzo del contesto a livello di frase o a livello di parola. Si può pensare alle parole che hanno un alto P-CXMI come quelle che richiedono contesto per la traduzione. Ora analizziamo le parole con un alto P-CXMI per cercare modelli tra queste parole. E conduciamo il nostro analisi sui trascrizioni dei discorsi TED tradotti dall'inglese in 14 lingue diverse. Conduciamo il nostro analisi a tre diversi livelli. Primo, esaminiamo i tag di parte del discorso che hanno un alto P-CXMI. Questo ci permette di trovare, ad esempio, pronomi duali in arabo che hanno relativamente un alto P-CXMI. Questo può essere spiegato perché l'inglese non ha pronomi duali, quindi è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo. Allo stesso modo, troviamo che alcune lingue richiedono contesto quando vogliamo scegliere la forma verbale appropriata. Quindi, esaminiamo anche gli elementi del vocabolario che hanno un alto P-CXMI mediato su tutte le loro diverse occorrenze. Questo aiuta a identificare casi come quello qui, in cinese è necessario il contesto per tradurre i nomi propri per assicurarsi di utilizzare la stessa traduzione all'interno del documento. Allo stesso modo, troviamo che il contesto è importante per tradurre in la forma di cortesia appropriata. Infine, esaminiamo diversi token individuali che hanno un alto P-CXMI. Questo ci permette di identificare fenomeni che non possono essere catturati dalla parola stessa, ma piuttosto espressi dalla struttura della frase, come la risoluzione degli ellissi. Ora utilizziamo i nostri risultati dall'analisi per progettare un benchmark per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni discorsivi identificati, creiamo tagger per identificare automaticamente le parole che si riferiscono a tale fenomeno. E abbiamo chiamato il nostro tagger MuDA, Multilingual Discourse-Aware. Possiamo anche notare che diverse lingue hanno diverse proporzioni di questi fenomeni discorsivi. Quindi, utilizziamo il tagger MuDA, applicando il tagger su un corpus parallelo che vogliamo utilizzare per la valutazione e applichiamo le nostre metriche di traduzione di scelta sui esempi dipendenti dal contesto che il tagger MuDA ha identificato. Infine, utilizziamo il nostro benchmark così come altre metriche per valutare diversi modelli di traduzione a livello di documento. Innanzitutto, quando utilizziamo metriche a livello di corpus: quindi per BLEU, troviamo che i modelli context-agnostic hanno le migliori prestazioni. Ma poi, se utilizziamo COMET, i modelli context-aware hanno le migliori prestazioni. E se utilizziamo word f-measure, allora i modelli con e senza contesto hanno prestazioni comparabili. Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se si utilizzano metriche a livello di corpus da soli. Ora, utilizziamo il benchmark MuDA per valutare i modelli e troviamo che i modelli context-aware sono significativamente più accurati dei modelli che non utilizzano il contesto per determinati fenomeni discorsivi come la formalità e la coesione lessicale. Tuttavia, questi modelli non sono molto migliori dei modelli che non utilizzano il contesto su altri fenomeni come pronomi, forma verbale ed ellissi. Questo suggerisce dove dovremmo vedere più progressi per la traduzione a livello di documento. Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che DeepL è in genere più accurato di Google Translate per la traduzione a livello di documento. Per riassumere, conduciamo un'analisi basata sui dati su 14 coppie di lingue per identificare quando la traduzione richiede contesto e quindi utilizziamo i nostri risultati per costruire un benchmark per la traduzione a livello di documento che può aiutarci a identificare quali fenomeni discorsivi i modelli possono gestire bene o male e quali sistemi di traduzione sono bravi nella traduzione a livello di documento. Grazie mille per la vostra attenzione. Ci vediamo a Toronto.</sample>
    <sample id="274">Yusen Zhang.</sample>
    <sample id="276">```
This work addresses the understudied area of machine translation metric evaluation for Indian languages, focusing on a dataset called IndicMT Eval.  The study investigates five Indian languages (Tamil, Malayalam, Hindi, Marathi, and Gujarati) by evaluating translations generated by seven different translation models.  Human expert annotators provide detailed error assessments, classifying errors by type (accuracy, fluency, special) and severity, and assigning overall scores.  A total of 7,000 samples were annotated, yielding 1,400 segments per language.

The research analyzes the correlation between various metrics (including overlap-based, embedding-based, and COMET-based) and human scores.  While overlap-based metrics show the highest overall correlation, they perform poorly. Embedding-based metrics like LabSE embedding and BERTscore with multilingual embeddings demonstrate better correlations. COMET-based metrics exhibit the highest correlations overall, but suffer from skewed score distributions.  

The study further investigates the impact of error type, finding that metrics correlate better with human scores when focusing on accuracy errors.  A fine-tuned COMET metric, IndicCOMET MQM, outperforms baseline COMET metrics on three of the five languages and demonstrates improved correlation across all languages.  Furthermore, IndicCOMET MQM exhibits better robustness compared to the original COMET metric on the ACES Translation Accuracy Challenge Sets.  The dataset and code are publicly available to facilitate further research in this area.
```</sample>
    <sample id="277">Multiset Tagging and Latent Permutations.</sample>
    <sample id="278">Il metodo delle "parole contrassegnate" identifica le parole che distinguono i gruppi contrassegnati (ad esempio, donne asiatiche, donne mediorientali, donne nere) dai gruppi non contrassegnati (ad esempio, uomini, persone bianche). Questo viene fatto confrontando le descrizioni generate dal modello per i gruppi contrassegnati e non contrassegnati, e identificando le parole che sono più frequentemente associate ai gruppi contrassegnati.</sample>
    <sample id="279">Shangbin è uno studente di dottorato presso l'Università del Washington.</sample>
    <sample id="280">```
This paper introduces MultiEMO, a novel attention-based correlation-aware multimodal fusion framework for emotion recognition in conversations (ERC).  ERC aims to predict the emotion label of each utterance, leveraging textual, audio, and visual modalities. Existing methods often lack effective multimodal integration, struggle with minority emotion classes, and fail to distinguish semantically similar emotions.

MultiEMO addresses these challenges by proposing a novel visual feature extractor, VisExtNet, which focuses on facial expressions without redundant scene information.  It then employs a multimodal fusion model, MultiAttn, based on bidirectional multi-head cross-attention layers to effectively integrate textual, audio, and visual features.  The framework utilizes sample-weighted focal contrastive loss to improve performance on minority classes and differentiate semantically similar emotions.

Experiments on MELD and IEMOCAP demonstrate that MultiEMO achieves state-of-the-art results, particularly in challenging scenarios involving asynchronous emotional cues and minority classes.  The proposed approach improves emotion recognition accuracy and robustness.  Limitations include VisExtNet's inability to distinguish between speakers and background individuals, the requirement for large batch sizes on MELD for the SWFC loss, and continued performance gaps in minority emotion recognition compared to majority classes.
```</sample>
    <sample id="281">## Abstract

This work investigates the crucial role of context in machine translation, addressing the challenge of evaluating models on context-dependent translations. We explore when translation necessitates contextual information and assess how well current models handle these cases.  Building upon the CXMI framework, we extend it to Pointwise CXMI to measure context usage at word and sentence levels across 14 language pairs, analyzing TED talks.  Our analysis reveals patterns in context dependence based on part-of-speech tags (e.g., dual pronouns), vocabulary items (e.g., proper nouns), and individual tokens (e.g., ellipses).  

We then develop the Multilingual Discourse-Aware (MuDA) tagger, which automatically identifies discourse phenomena like formality and lexical cohesion, enabling a document-level translation benchmark.  Evaluating models using MuDA and various metrics (BLEU, COMET, word f-measure), we find that context-aware models significantly outperform context-agnostic models on phenomena like formality and lexical cohesion. However, performance gaps remain for phenomena like ellipsis and pronouns.  Furthermore, our benchmark reveals DeepL's superior accuracy compared to Google Translate for document-level translation.  This research highlights the limitations of corpus-level metrics in capturing context-dependent translations and provides a valuable tool for identifying strengths and weaknesses of machine translation systems in document-level scenarios.</sample>
    <sample id="282">StoryTrans è un nuovo modello di generazione di storie proposto a ACL 2023 per il trasferimento di stile non parallelo, un compito cruciale nella generazione di linguaggio naturale. A differenza dei modelli esistenti che si concentrano su token o frasi, StoryTrans affronta il trasferimento di stile a livello di storia e discourse, un aspetto fondamentale per emulare lo stile di un autore. Il modello risolve le sfide legate alle complesse preferenze linguistiche degli autori, come le strutture discorsive, e alla loro stretta correlazione con specifici argomenti.

StoryTrans impara rappresentazioni discorsive dai testi di input e le combina con embedding di stile addestrabili per generare testi nello stile desiderato. Introduce un nuovo obiettivo di addestramento per ridurre le caratteristiche stilistiche dalle rappresentazioni discorsive, avvicinando le rappresentazioni derivate da testi diversi nello spazio latente. Per migliorare la conservazione del contenuto, il modello separa la generazione in due fasi: prima, maschera le parole chiave specifiche dello stile nei testi di input, e poi genera l'intero testo incorporando queste parole chiave esplicitamente.

L'addestramento avviene in due fasi: una fase di advisory con perdita di auto-ricostruzione, perdita di disentanglement e perdita di ordine delle frasi, e una fase di generazione che si concentra sulla riempimento delle parole chiave specifiche dello stile e sulla rimozione dei token di maschera. I risultati sperimentali, sia automatici che manuali, dimostrano che StoryTrans supera i modelli di riferimento in termini di controllo dello stile e conservazione del contenuto. L'analisi visiva dello stile conferma che i testi generati da StoryTrans si allineano con le caratteristiche stilistiche dei testi di riferimento. Il codice e i dati sono disponibili nel repository.</sample>
    <sample id="283">Dependency Structure of Coordination.</sample>
    <sample id="284">FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction

This paper introduces FSUIE, a novel framework for Universal Information Extraction (UIE) that addresses the limitations of existing span-based models. Current UIE models heavily rely on precise span boundaries, which are often ambiguous in annotation. FSUIE proposes a fuzzy span mechanism, learning span boundaries as continuous probability distributions rather than discrete points. This is achieved through a fuzzy span loss function that combines Binary Cross Entropy (BCE) with a KL-divergence term to encourage a continuous boundary distribution.  A fuzzy span attention module dynamically adjusts the attention span using an optimizable parameter and linearly decays the attention distribution, enabling the model to focus on relevant semantic information within a limited range.

Experiments on named entity recognition, relationship extraction, and aspect sentiment triplet extraction demonstrate FSUIE's effectiveness. FSUIE-base achieves significant performance improvements over UIE-base, particularly on small-scale datasets.  It achieves state-of-the-art results on relationship extraction datasets (ACE2004, 2005, ADE) and AST-V2 datasets. Ablation studies show that the fuzzy span attention module improves convergence speed and enhances information extraction capability.  The visualization of the attention distribution confirms that the module focuses on semantic information within a limited context. FSUIE offers a unified structure for information extraction, demonstrating strong generalization capabilities across diverse domains.  The proposed fuzzy span mechanism and attention module significantly enhance UIE performance, leading to excellent results across various IE tasks.</sample>
    <sample id="285">This paper addresses the limitations of current evaluation methods for Factual Error Correction (FEC) models in dialogue summarization. While existing metrics like FactCC and DAE assess overall factuality, they lack granularity and fail to distinguish between true error correction and simple rephrasing. The authors argue that these metrics blur the line between approaches to factual correctness, potentially hindering the progress of FEC models.

To address these issues, the paper proposes a new evaluation framework based on manually annotated reference corrections, emphasizing the importance of minimal edits for fluency and non-redundancy. This framework, built upon ERRANT, involves alignment, classification, and comparison steps.  A novel taxonomy of factual errors, categorized as content-based (based on part-of-speech and dependencies) and form-based (based on addition, deletion, and substitution), is introduced.

Experiments with FEC models trained on dialogue summarization datasets demonstrate that training with reference summaries yields the best results when evaluated using unreliable metrics. The study highlights the need for a shift in evaluation methodologies and suggests combining human-annotated data with synthetic data as a promising future direction.  Furthermore, the paper identifies limitations of current FEC models in handling specific error types like additions and attribute/modality/link errors. The findings underscore the critical need for more comprehensive and accurate evaluation of FEC models to advance the field of dialogue summarization.</sample>
    <sample id="286">Professor Jinho Choi.</sample>
    <sample id="287">Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="288">Il dataset BLiMP e SyntaxGym possono essere utilizzati per testare i fenomeni sintattici.</sample>
    <sample id="290">COSINE, FTw.</sample>
    <sample id="291">Il modello viene valutato su 11 attività downstream pubbliche e private, tra cui riconoscimento di entità nominate, classificazione, tagging delle parti del discorso e question answering.</sample>
    <sample id="294">CamemBERT viene inizialmente addestrato su un set di dati di 4 GB di NACHOS.</sample>
    <sample id="295">Adam Przepiórkowski.</sample>
    <sample id="296">This work investigates irony detection in natural language processing, moving beyond binary classification to explore more informative outputs. The research focuses on irony within social media data collected from Reddit and Twitter over 1.5 years, resulting in the English Perspectivist Irony Corpus (EPIC).  A crowdsourcing study using Prolific involved 74 annotators, with 200 conversations annotated for irony.  The annotation interface was designed as a simple chat-like system.

The study highlights that inter-annotator agreement varies across annotator groups (gender, age, nationality), leading to the development of "perspective-aware models." These models are trained by fine-tuning pre-trained language models on datasets split by annotator perspectives.  While raw performance showed no significant trends, perspective-aware models demonstrated significantly higher confidence in their irony predictions compared to standard aggregated models.

Further analysis revealed that generational differences and geographical distribution of annotators correlate with variations in annotation. Specifically, annotators from the UK and Ireland exhibited the most disagreement.  The research suggests that individual perspectives significantly influence irony detection, and understanding these perspectives is crucial for building more robust and reliable natural language understanding systems. The findings contribute to a deeper understanding of the complexities of irony and the limitations of traditional data-driven approaches.</sample>
    <sample id="297">## Abstract

This project investigates the phenomenon of coded rhetoric, specifically dogwhistles – terms that convey a hidden, often controversial, meaning to an in-group while appearing innocuous to an out-group.  The study addresses the challenges of understanding dogwhistles, which are context-dependent and often used to evade content moderation.  A core component is the development of a typology and glossary of over 340 dogwhistle terms and symbols, compiled from diverse sources and categorized by register, persona, and type.  

The research employs a case study of historical U.S. political speeches, revealing a correlation between the frequency of racial dogwhistles and the Republican Southern Strategy.  Furthermore, experiments with language models, particularly GPT-3, demonstrate varying levels of success in recognizing dogwhistles, with performance being significantly impacted by the formality of the language and the specific prompt strategies used.  The study also explores how dogwhistles can circumvent content moderation by analyzing the impact on toxicity detection scores when standard group labels are replaced with coded terms.  The findings highlight the importance of understanding dogwhistles for NLP and linguistics, emphasizing their role in political influence, persuasion, and the perpetuation of harmful rhetoric.  The project aims to provide a framework for better identifying and mitigating the effects of dogwhistles in online discourse.</sample>
    <sample id="298">L'esperimento con la ri-allenamento/pre-allenamento di alcuni modelli con dati più recenti ha dimostrato che le prestazioni peggiorano con una maggiore distanza temporale tra i dati di addestramento e di test, confermando l'ipotesi che la deriva temporale sia la causa principale della perdita di prestazioni.</sample>
    <sample id="299">This work addresses the vulnerability of Natural Language Inference (NLI) models to shortcuts, spurious correlations in training data that lead to strong in-distribution performance but poor generalization to out-of-distribution data.  Existing shortcut mitigation methods often require domain-specific knowledge or auxiliary models, limiting their applicability.  We propose a novel training method based on minimax optimization to reduce reliance on shortcuts and improve out-of-distribution performance.

Our approach leverages the observation that NLI models struggle with under-represented "hard" training instances that contradict dominant "easy" examples.  We train a learner model and an auxiliary model in an alternating minimax objective. The learner minimizes the NLI loss, while the auxiliary maximizes it, incentivizing the learner to focus on hard examples. This allows the learner to prioritize learning from instances that counteract shortcut-driven patterns.  The auxiliary model is a feed-forward network.

We evaluate our method on MNLI, FEVER, and QQP, using both in-distribution and adversarial out-of-distribution test sets. Results demonstrate that our minimax training consistently improves out-of-distribution performance compared to standard ERM training and existing shortcut mitigation techniques, while maintaining high in-distribution accuracy.  We also investigate the effect of pre-training the learner, the size of the auxiliary model, and the transferability of the learned weight distribution to larger models and different datasets.  The findings suggest that our method offers a promising approach to building more robust NLI models.</sample>
    <sample id="300">## Abstract

This work introduces the task of interactive dictation, a novel approach to document creation that combines voice dictation with real-time editing capabilities. Unlike existing speech-to-text systems that primarily support dictation, interactive dictation allows users to seamlessly correct and modify their spoken text through intuitive vocal commands, fostering a more natural and efficient writing experience. 

The research formalizes interactive dictation as a four-step process: ASR recognition, utterance segmentation, command extraction and normalization, and sequential execution of dictation and command utterances. A new data collection interface is designed and a dataset is built to support this task. A baseline system is then developed, employing T5 and GPT-3 architectures for ASR repair and state prediction. Experiments demonstrate a trade-off between runtime and accuracy, with GPT-3 achieving higher accuracy but slower performance. Predicting the final state directly yields better results than predicting intermediate programs. 

The study highlights the potential of interactive dictation to improve user productivity and comfort when creating documents. The authors release code and provide further details in the accompanying paper, encouraging future research in this area. This work represents a significant step towards a more human-centered approach to speech-based document creation.</sample>
    <sample id="302">Il modello prevede prima un multiset di token per ogni token di input, ma non l'ordine. Quindi, per ottenere la sequenza di output corretta, è necessario permutare questi token.</sample>
    <sample id="303">Gli autori hanno suggerito di aumentare la trasparenza sui metodi di mitigazione dei bias perché non si sa se i risultati positivi che si osservano derivino da un'eccessiva allineamento dei valori o da altri metodi anti-stereotipici che potrebbero generare pattern dannosi.</sample>
    <sample id="304">Il modello di coppia minima (MPP) valuta i modelli linguistici mostrando loro una frase accettabile e una frase inaccettabile. Il modello deve quindi prevedere quale delle due frasi è più probabile.</sample>
    <sample id="305">This presentation introduces the research paper "Weaker Than You Think: A Critical Look at Weakly Supervised Learning," co-authored with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. The work challenges the common claim that weakly supervised learning (WSL) methods achieve high performance solely on weakly labeled data, often relying on an additional clean validation set for model selection.

The paper investigates the necessity of clean validation data in WSL, the optimal number of clean samples, and the best utilization of clean data. Findings reveal that recent WSL methods *require* clean validation samples for proper generalization, with performance drops observed without them.  Increasing the number of clean samples improves performance, and direct fine-tuning on clean data often surpasses WSL approaches.  Furthermore, the performance gains often attributed to WSL can be achieved by allowing continuous fine-tuning on clean validation samples, rendering more complex WSL methods computationally less attractive.

The authors conclude that the performance benefits and practicality of current WSL methods are often overestimated.  They recommend future research to report model selection criteria, compare WSL with few-shot learning, and consider continuous fine-tuning as a baseline. The paper also highlights the importance of open-sourcing code for reproducibility.  The research questions addressed are: Is clean validation data necessary for WSL? How many clean samples are needed? And should clean samples be used for validation or other purposes? The paper provides a critical analysis of WSL, emphasizing the need for clean data and suggesting practical improvements for future research.</sample>
    <sample id="306">Sebastian Schuster and Najoung Kim present research on entity tracking in large language models (LLMs), a crucial ability for understanding discourse. The paper addresses the challenge of evaluating this capability, highlighting potential pitfalls like reliance on common pre-training data, heuristic associations, and memorization. To overcome these issues, they designed a task involving boxes and objects, where models predict the contents of boxes after state-changing operations.

The study tested Flan-T5 and GPT-3/3.5 models using 2-shot in-context learning. Results indicate that most models primarily copy the initial state, achieving high accuracy on the "copy" cases. However, text-davinci-003 demonstrates non-trivial entity tracking, while models with limited code pre-training show no such ability.  Further analysis reveals that GPT-3.5 models, trained on substantial code, exhibit stronger tracking capabilities. While smaller models can learn through fine-tuning, pre-training is essential for learning the task. The authors conclude that pre-training on code appears to be a key factor in enabling entity tracking in LLMs. They acknowledge the limited generalizability of their findings and invite further discussion. The full paper is available on arXiv.</sample>
    <sample id="307">Gli autori hanno utilizzato metriche per task pubblici e privati come Named Entity Recognition, Classification, Part-of-Speech tagging e Question Answering.</sample>
    <sample id="308">## NLPositionality: Characterizing Design Biases in NLP Datasets and Models

This work investigates the presence of design biases in Natural Language Processing (NLP) datasets and models, arguing that they reflect the positionality of researchers and the populations they represent.  The study addresses the issue of systematic performance differences across populations, exemplified by the varying sensitivity of toxicity detection APIs to different linguistic contexts. 

To characterize this positionality, the researchers developed the NLPositionality framework, which compares annotations from diverse annotators with existing datasets and models.  This involves re-annotating datasets with a large, demographically diverse pool of annotators and correlating their judgments with model predictions using Pearson's R.  The study leverages Lab in the Wild, an online crowdsourcing platform, to recruit volunteers from 87 countries.

The analysis reveals that NLP datasets and models exhibit strong alignment with English-speaking countries and individuals with higher levels of education.  However, this alignment leaves underrepresented groups, such as non-binary individuals, behind.  The findings highlight the need for greater awareness of positionality in NLP research and development. 

The research proposes several recommendations, including documenting design choices, adopting a perspectivist approach to research, and building specialized datasets and models tailored to specific communities.  Ultimately, NLPositionality aims to promote more inclusive and equitable NLP technologies.</sample>
    <sample id="309">Inter-annotator agreement su 100 conversazioni doppiamente etichettate.</sample>
    <sample id="310">Wikipedia.</sample>
    <sample id="311">L'articolo è presentato da Regina Stodden, e Omar è il relatore. Non sono specificate le affiliazioni degli autori.</sample>
    <sample id="312">MultiInstruct è il primo benchmark di instruction tuning multi-modale, che comprende 62 task diversificate che coprono 10 categorie ampie, derivate da 21 dataset open-source esistenti. Si distingue per la sua attenzione ai task multi-modali, che sono stati trascurati in precedenza, e per la sua creazione di un dataset di instruction tuning multi-modale di grandi dimensioni.</sample>
    <sample id="313">James Finch, Sarah Finch, Jinho Choi e Amazon Alexa AI.</sample>
    <sample id="314">La coordinazione binaria è la struttura in cui due elementi (conjunct) sono collegati da un elemento di coordinazione (congiunzione) e sono strutturati in modo che il primo conjunct sia il capo della struttura coordinata.</sample>
    <sample id="315">Il testo non specifica il tempo medio di utilizzo dei prompt.</sample>
    <sample id="316">I risultati mostrano che i modelli più piccoli, come T5, possono superare i modelli più grandi quando vengono adeguatamente addestrati su dataset appropriati, come CoScript.</sample>
    <sample id="317">CodeIE affronta la sfida di migliorare l'estrazione di informazioni (IE) utilizzando modelli linguistici di grandi dimensioni (LLM). L'IE, che mira a estrarre informazioni strutturate da testo non strutturato, è un compito fondamentale nell'elaborazione del linguaggio naturale. I modelli pre-addestrati come T5 e GPT-3, pur eccellendo nella generazione di testo, presentano difficoltà nell'estrazione di strutture complesse, richiedendo dati strutturati e strategie di decodifica specializzate.

CodeIE risolve questo problema trasformando l'IE in un compito di generazione di codice, sfruttando modelli linguistici di codice come Codex. Questo approccio consente di convertire facilmente il testo in un formato strutturato durante l'input e di garantire strutture coerenti durante l'output.  L'estrazione di entità nominate e l'estrazione di relazioni sono state implementate utilizzando prompt basati su codice, che guidano il modello a generare codice per l'estrazione di entità e la loro relazione.

I risultati sperimentali su diversi dataset dimostrano che CodeIE, utilizzando modelli di codice come Codex, supera significativamente i modelli basati su testo come T5 e GPT-3, sia in termini di accuratezza che di coerenza strutturale. L'analisi approfondita rivela che i modelli di codice si allineano meglio con la natura stessa dell'IE, producendo meno errori strutturali e output più pertinenti.  In sintesi, CodeIE offre un approccio promettente per migliorare l'IE sfruttando la potenza dei modelli linguistici di codice.</sample>
    <sample id="318">Yanis Labrak e il suo team presenteranno il loro lavoro su "DrBERT: un modello pre-addestrato robusto in francese per i domini biomedici e clinici". In questa presentazione, discuteranno prima della modellazione linguistica in ambito sanitario. Successivamente, presenteranno il contributo principale del loro articolo: l'introduzione del primo modello biomedico in francese chiamato DrBERT, basato su RoBERTa e addestrato su NACHOS, un dataset di dati medici estratti da internet. Inoltre, presenteranno un confronto tra diversi modelli con diverse impostazioni di pre-addestramento e fonti di dati. Infine, presenteranno i loro risultati su 11 task di downstream in ambito biomedico e clinico in francese. Concluderanno gli esperimenti e forniranno maggiori dettagli su come accedere ai modelli.

Dato che BERT è diventato un approccio molto efficace per risolvere i task di elaborazione del linguaggio naturale dalla sua pubblicazione nel 2018, offrendo notevoli miglioramenti rispetto ai metodi statici e contestuali precedenti come Word2vec, fastText o modelli più recenti, questo modello è stato adattato a molti altri linguaggi, come in francese con CamemBERT, e anche a domini come il biomedico con PubMedBERT e BioBERT e il clinico con ClinicalBERT, sebbene prevalentemente in inglese. I modelli specializzati per altre lingue sono scarsi e spesso si basano su pre-addestramento continuo a causa della mancanza di dati specifici per il dominio. Tuttavia, il francese non aveva ancora un modello open source per il settore biomedico. Quindi, si pongono la domanda su quali siano le fonti di dati più appropriate per un'ampia gamma di utilizzi e se i dati estratti siano una buona sostituzione dei dati clinici. Per rispondere a questa domanda, confrontano DrBERT con il loro modello ChuBERT, basato su dati anonimizzati ottenuti dal data warehouse dell'Ospedale Universitario di Nantes. Successivamente, si chiedono quanti dati siano necessari per addestrare un modello specializzato in francese. È sufficiente 4 gigabyte, 8 gigabyte o di più? Per rispondere a questa domanda, addestrano e confrontano quattro modelli da zero: una prima versione di DrBERT con 7 GB di NACHOS, una seconda versione con 4 GB di NACHOS, una prima versione di ChuBERT, un modello clinico con 4 GB di frasi estratte da note cliniche, e una versione finale di ChuBERT con una combinazione di 4 GB di NACHOS e 4 GB di note cliniche. Oltre a questo confronto, introducono tre modelli addestrati con pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento. Uno basato sul peso e sulla tokenizzazione di CamemBERT addestrato su un set di 4 GB di NACHOS, un altro basato anch'esso su CamemBERT ma addestrato su 4 GB di note cliniche, e infine uno basato sul modello biomedico in inglese PubMedBERT addestrato su 4 GB di NACHOS. In totale, hanno sette modelli. Per valutare i loro sette modelli, raccolgono dati per task di downstream pubblici e privati come Named Entity Recognition, classificazione, part-of-speech tagging e question answering. Questi modelli vengono confrontati con sei modelli di baseline: CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT e ClinicalBERT. L'analisi evidenzia che i modelli che hanno ottenuto i migliori risultati hanno funzionato meglio sui task con dati della stessa natura di quelli su cui sono stati addestrati. Tuttavia, si osserva che i dati provenienti da fonti eterogenee appaiono più versatili. Si osserva inoltre che l'utilizzo di più dati tradotti porta a risultati migliori. In generale, il pre-addestramento da zero sembra ottenere prestazioni superiori nella maggior parte dei task. Tuttavia, il loro esperimento sul pre-addestramento controllato utilizzando il peso e la tokenizzazione di CamemBERT addestrato sul set di 4 GB di NACHOS ha prodotto risultati comparabili a quelli ottenuti con DrBERT 4 GB da zero. Questo non è il caso del modello basato sui pesi e sulla tokenizzazione di CamemBERT, che presenta problemi di stabilità. Infine, concludono che il loro sistema proprietario ha ottenuto prestazioni migliori su nove dei 11 task di downstream e ha superato globalmente i risultati del modello generico, CamemBERT. Si osserva inoltre che più dati specializzati sono migliori, ma non scalano bene. Tutti i modelli pre-addestrati da NACHOS sono disponibili gratuitamente su Hugging Face e sotto la licenza MIT, e tutti i script di addestramento sono presenti sul loro repository GitHub. Quindi ringraziano per la presentazione e si aspettano di scambiare idee alla sessione poster a Toronto.</sample>
    <sample id="319">Le strategie di apprendimento esaminate nel lavoro includono:

*   Pre-training da zero (DrBERT, ChuBERT)
*   Continual pre-training (CamemBERT, PubMedBERT, BioBERT)
*   Utilizzo di dati di diverse fonti (NACHOS, note cliniche)
*   Diversi set di dati (4 GB, 8 GB, ecc.)</sample>
    <sample id="320">Il fattore di overfitting dovuto al riutilizzo del test non è osservato.</sample>
    <sample id="321">La qualità della semplificazione è stata valutata utilizzando le coppie di frasi semplificate e non semplificate, con le coppie di frasi semplificate considerate come standard di riferimento (gold standard).</sample>
    <sample id="322">This paper investigates how text classifiers learn about morality, addressing the limitations of treating morality as a single, subjective scale. The research explores the Moral Foundation Theory, proposing that human morality is rooted in five distinct foundations, each influencing how individuals perceive and judge actions.  The study utilizes explainable AI techniques on language models trained to understand morality in text, focusing on domain-specific expression of moral concepts.  A dataset of 35,000 tweets from seven domains (e.g., #AllLivesMatter, #BlackLivesMatter) is employed to assess whether language models can discern variations in moral expression across different contexts.  The findings reveal that language models recognize differences in how morality is articulated in domains like ALM and BLM, with subversion being framed differently in each.  This suggests that morality is not monolithic but rather context-dependent. The paper highlights the potential dangers of using a single model for diverse domains, as it could lead to misunderstandings of morality.  The research underscores the importance of understanding the nuances of moral expression and the role of domain-specific knowledge in language models to avoid misinterpretations and ensure responsible AI development.  The study contributes to a deeper understanding of how language models internalize and represent complex moral concepts.</sample>
    <sample id="323">This paper introduces DHLK, a novel approach for Commonsense Question Answering (QA) that addresses limitations in existing methods combining language models and knowledge graphs (KGs). Current approaches often suffer from noisy entity retrieval, isolated encoding of text and knowledge, and a lack of semantic relationship modeling. DHLK tackles these issues by first constructing a highly optimized heterogeneous knowledge graph (HKG) using a two-stage pruning strategy and knowledge representation learning (KRL).  This HKG incorporates paraphrased entities retrieved from WordNet and Wiktionary, enriching the graph with semantic connections.

The core of DHLK lies in fusing the language model and the HKG. RoBERTa and Mask Self-Attention are used to encode both the question context and the entities within the HKG.  A key innovation is the use of Relation Mask Self-Attention (RMSA) to model the subgraph, incorporating relationship information and iteratively updating entity and relation embeddings.  The HKG path information is then integrated with the question context through path enhancement. Finally, a multi-layer perceptron (MLP) predicts the answer probability based on the graph embedding, path information, and question context embedding.

Experiments on CommonsenseQA and OpenBookQA demonstrate that DHLK achieves competitive results compared to existing language model and KG-based methods. The approach effectively leverages external knowledge, models semantic relationships, and mitigates noise in entity retrieval, leading to improved Commonsense QA performance. The paper provides detailed results and leaderboards for both datasets.</sample>
    <sample id="324">I modelli linguistici presentano bias politici diversi, come dimostrato dal nostro studio. I modelli mostrano inclinazioni politiche che variano a seconda della loro formazione e possono influenzare le prestazioni e la correttezza in applicazioni downstream.</sample>
    <sample id="325">Ciao! Il mio nome è Matthias Lindemann e oggi vi presento brevemente il nostro articolo su "Generalizzazione composizionale senza alberi utilizzando etichette multiset e permutazioni latenti". Questo è un lavoro congiunto con i miei supervisori Alexander Koller e Ivan Titov. La generalizzazione composizionale può essere compresa come la capacità di un apprendista di gestire una recursion più profonda e composizioni inattese di frasi che sono state viste individualmente durante l'addestramento. Nel contesto della semantica della traduzione, testare la generalizzazione composizionale potrebbe essere così: abbiamo un set di dati di frasi. In questo caso, "La ragazza dormiva" e "Mary sapeva che la ragazza dormiva". Queste frasi sono abbinate a forme logiche che rappresentano aspetti fondamentali del loro significato. A differenza della valutazione standard dei modelli di machine learning, il set di test non proviene dalla stessa distribuzione ma contiene forme logiche strutturalmente invisibili. In questo esempio, il modello ha visto una recursion superficiale durante l'addestramento e viene testato su un esempio con una recursion più profonda. I modelli seq2seq naïvi faticano con questo tipo di generalizzazione fuori distribuzione e spesso producono output che sono scollegati dall'input. In particolare, spesso falliscono nel riprodurre le corrispondenze sistematiche tra input e output, come quelle colorate nell'esempio. Un metodo popolare per affrontare questo problema è integrare gli alberi nei modelli. Gli alberi sono intesi per catturare il processo compositivo che relaziona le frasi di input con le forme logiche. Questo funziona bene, ma gli alberi devono essere ottenuti in qualche modo. Questo può essere complicato e talvolta un processo computazionalmente costoso. Tipicamente, questo comporta una considerevole pre-elaborazione formale dei formi logici, ad esempio per gestire simboli variabili. Ottenere gli alberi può anche comportare procedure di induzione di grammatica specializzate. Nel nostro articolo, non utilizziamo gli alberi e introduciamo un modello seq2seq neurale che modella direttamente le corrispondenze tra frammenti di input e frammenti di output. Per la prima volta, dimostriamo una forte generalizzazione a recursion più profonda senza fare affidamento sugli alberi. Il nostro approccio prevede la predizione dell'output dall'input in due fasi. In primo luogo, etichettiamo ogni token di input con un multiset non ordinato di token che appariranno nell'output. Dopo la prima fase, abbiamo tutti i token corretti, ma non sono ordinati. Ecco perché nella seconda fase utilizziamo un altro modello per prevedere una permutazione per metterli nell'ordine corretto. Introduciamo un nuovo metodo per prevedere la permutazione che non impone alcuna restrizione dura alle possibili permutazioni. Ciò rende il nostro approccio piuttosto flessibile ed espressivo. Concettualmente, il nostro modello di permutazione funziona in modo simile: procediamo da sinistra a destra sull'output e determiniamo quale multiset token inserire in ogni posizione. Per la prima posizione di output, selezioniamo semplicemente uno, come evidenziato in rosso. Quindi saltiamo alla successiva multiset token per determinare il secondo token nell'output. Determiniamo il terzo token nell'output in modo simile saltando a un altro multiset token. Continuiamo questo processo fino a quando ogni token dalla prima fase non è stato visitato esattamente una volta. Per darvi un'idea del risultato sperimentale, qui confrontiamo il nostro metodo con altri modelli treeless sul benchmark COGS. Il nostro modello supera gli altri di gran lunga nella generalizzazione a recursion più profonda. Rimangono comunque molto impegnativi altri tipi di generalizzazione strutturale. Nel nostro articolo, risolviamo alcuni problemi tecnici interessanti. Innanzitutto, l'allineamento tra input e output non è fornito nei dati di addestramento. Di conseguenza, per un dato token non sappiamo da quale multiset proviene, il che pone una sfida per l'addestramento. Inoltre, a volte ci sono più permutazioni che sono coerenti con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo inducendo l'allineamento come parte dell'addestramento. Il nostro metodo di permutazione è molto flessibile, ma comporta la sfida che trovare la permutazione con il punteggio più alto è NP-hard. Ciò è correlato al problema del "Traveling Salesman". Lo approssimiamo con una rilassazione continua compatibile con GPU che consente anche di retropropagare attraverso la soluzione e di imparare le permutazioni più plausibili dal punto di vista linguistico. Se volete saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, date un'occhiata al nostro articolo o venite al nostro poster.</sample>
    <sample id="326">La dissonanza cognitiva è l'incongruenza tra due o più credenze o azioni, come ad esempio affermare di sapere che i fumo può uccidere, ma continuare a fumare. Questa incongruenza crea un senso di disagio e può essere risolta cercando di giustificare l'azione incoerente.</sample>
    <sample id="327">ManagerTower è una nuova architettura di visione-linguaggio che migliora i modelli esistenti come BridgeTower, focalizzandosi sull'aggregazione dei contributi di esperti unimodali a diversi livelli.  I modelli VL attuali spesso utilizzano un'architettura a torri, con encoder testuali, visivi e cross-modali. ManagerTower introduce "manager" in ogni livello cross-modale, che raccolgono e combinano le rappresentazioni unimodali pre-addestrate.  Questi manager adattivamente sfruttano diverse sfumature di conoscenza semantica, consentendo un allineamento e una fusione cross-modali più completi.

A differenza di BridgeTower, che utilizzava connessioni dirette tra livelli unimodali, ManagerTower permette una maggiore flessibilità e scalabilità. L'architettura è applicabile a qualsiasi encoder visivo, testuale o cross-modale.  Testando su VQAv2, ManagerTower ottiene prestazioni superiori rispetto a modelli di base pre-addestrati su 4 milioni di immagini, superando anche modelli addestrati con più dati o parametri, con un miglioramento del 39.15% nell'accuratezza.

L'analisi delle rappresentazioni aggregate dei manager rivela che i manager adattivi mostrano distribuzioni di peso significativamente diverse rispetto a quelle statiche, indicando una capacità di adattamento a diversi livelli di conoscenza semantica.  La disponibilità del codice e del paper su Archive e Github rende ManagerTower accessibile alla comunità.  L'obiettivo è creare sistemi di intelligenza artificiale in grado di comprendere sia immagini che testo in modo più efficace.</sample>
    <sample id="328">GPT-4 è il modello linguistico più liberale.</sample>
    <sample id="329">Minghang Zheng from Peking University presents "Generating Structured Pseudo Labels for Noise-resistant Zero-shot Video Sentence Localization." This work addresses the challenge of zero-shot video sentence localization, aiming to identify relevant video segments based on natural language queries. Traditional methods rely on manual annotations, which are costly. The proposed approach generates structured pseudo-labels to train video sentence localization models without manual annotation, mitigating the drawbacks of existing zero-shot methods.

The core innovation lies in a noise-resistant pseudo-label generation process.  It leverages pre-trained image caption models to create complex, free-form pseudo-queries, going beyond simple event-based queries.  A pre-trained model then measures the relevance between video frames and these pseudo-queries, generating pseudo-events that guarantee high relevance within the event and low relevance outside.  Furthermore, the method employs a sliding window approach to select the most relevant pseudo-events based on similarity scores.

To address label noise, the approach estimates label noise based on model confidence and IoU with pseudo-labels. Noisy samples are down-weighted, and high-confidence, high-IoU predictions are refined as new pseudo-labels for subsequent training rounds.  Experiments on ActivityNet Captions and Charades-STA demonstrate that the proposed method outperforms existing zero-shot approaches on key metrics like R@M and mIoU. The research offers a robust and efficient solution for zero-shot video sentence localization, achieving state-of-the-art performance while being resilient to label noise. Code is available via a QR code.</sample>
    <sample id="330">Cumulative performs equal or better than Iterative across the board.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">Il parametro di riferimento MuDA è stato costruito utilizzando un corpus parallelo che è stato utilizzato per l'evaluazione.</sample>
    <sample id="333">INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation, presentato da Wenhao di Nanjing University, affronta la limitazione delle rappresentazioni non-lisce nei modelli di traduzione automatica (NMT) che ne ostacolano la generalizzazione. L'approccio kNN-MT mira a mitigare questo problema incorporando il conoscenza dei vicini più prossimi per smussare le previsioni. Tuttavia, l'implementazione kNN-MT presenta svantaggi legati al costo computazionale dell'accesso al datastore e alla difficoltà di aggiornamento delle rappresentazioni.

INK introduce un nuovo framework di addestramento iterativo che "iniettare" conoscenza kNN nel modello NMT. Il framework prevede due fasi: l'estrazione della conoscenza kNN dal datastore per guidare l'adattamento dell'adattatore e l'aggiornamento asincrono delle rappresentazioni per rinfrescare il datastore.  L'addestramento utilizza tre tipi di rappresentazioni, allineando le rappresentazioni contestualizzate con token, token kNN e rappresentazioni di token simili per affrontare la dispersione.

Gli esperimenti, basati sul modello WMT'19 German-English news translation, dimostrano che INK supera il modello kNN-MT e altri modelli state-of-the-art.  INK ottiene un miglioramento significativo delle prestazioni, con un guadagno medio di 1.99 COMET score e 1.0 BLEU score.  Il framework INK offre anche un miglioramento delle prestazioni con meno memoria e velocità di inferenza più elevate.  In conclusione, INK fornisce un approccio innovativo per migliorare la generalizzazione e le prestazioni dei modelli NMT attraverso l'integrazione iterativa della conoscenza kNN.</sample>
    <sample id="335">Matthias Lindemann.</sample>
    <sample id="336">Il trasferimento interlinguistico è la capacità di migliorare le prestazioni di un modello di semantic parsing in una lingua target utilizzando dati e modelli pre-addestrati in un'altra lingua.</sample>
    <sample id="337">This paper introduces a novel approach to learning word embeddings for out-of-vocabulary (OOV) words by leveraging word formation and association. The proposed method constructs a Word Relationship Graph, representing words and wordpieces as nodes connected by relationships reflecting lexical rules.  An OOV word is tokenized and associated with relevant words, creating a two-level graph.  A self-attention network assigns node attributes based on character information, and Graph Attention Networks are used to refine node representations. A readout block generates a graph-level representation.  A Graph Convolutional Network is then applied to summarize the graph information. Contrastive learning is employed in the loss function, utilizing NT-XENT positive samples like relevant neighbors, synonyms, and the OOV word itself, to align OOV words with background embeddings.  Experiments demonstrate superior performance compared to baselines on both intrinsic and extrinsic tasks. The model benefits both static and contextual embedding models.  The paper also discusses the potential for extending the model to other languages, particularly agglutinative languages, while acknowledging the challenges posed by fusional languages. The core contribution lies in effectively handling OOV words by modeling their formation and relationships within a graph structure, leading to improved embedding quality and downstream task performance.</sample>
    <sample id="338">```
This research addresses the critical question of how to objectively evaluate the helpfulness of human-annotated natural language explanations for training language models. While human annotations are widely used to generate explanations for improved model performance, the subjective nature of these explanations makes their evaluation challenging. Existing metrics like BLEU and ROUGE focus on lexical similarity, while the simulability score measures performance changes with and without explanations, but lacks task-specific considerations and explanation utility across different stages of model development.

This work introduces a unified data format to standardize tasks and facilitate comparison across diverse datasets (CoS-E, ECQA, e-SNLI, ComVE) and models (T5, BART).  A series of experiments analyze the utility of explanations by varying data subsets and fine-tuning/inference settings.  The findings reveal that fine-tuning with explanations doesn't necessarily transfer new knowledge, but rather encourages reliance on the explanation input.  CoS-E explanations are less helpful than ECQA explanations for baseline models, highlighting task-dependent explanation quality.

To address the limitations of existing metrics, we propose TREU, a novel evaluation metric that extends the simulability score to specifically assess explanation helpfulness during fine-tuning.  TREU demonstrates improved performance compared to the simulability score across the five datasets, particularly for tasks like entailment.  The results suggest that explanation helpfulness is task-dependent and influenced by explanation format.  The work establishes a foundation for more reliable evaluation of human-annotated explanations and recommends future research in this area.
```</sample>
    <sample id="339">Dawei è uno studente di dottorato presso l'Università di Saarland in Germania. Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow sono coautori dell'articolo.</sample>
    <sample id="340">ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation

Paraphrase generation is crucial for NLP tasks like question answering and chatbots, but existing datasets suffer from limited scale and syntactic diversity. This work introduces ParaAMR, a large-scale paraphrase dataset generated via AMR back-translation, addressing these limitations.  We leverage Abstract Meaning Representations (AMR) graphs to create syntactically diverse paraphrases.  The process involves parsing source sentences into AMR graphs, randomly modifying the graph's focus node and edges, and then generating text using an AMR graph-to-text generator. This approach ensures semantic similarity while promoting syntactic variation.

ParaAMR comprises approximately 15 million source sentences, each with 6.9 paraphrases.  Quantitative analysis reveals that ParaAMR achieves comparable semantic similarity to other back-translation datasets but demonstrates significantly higher syntactic diversity.  We demonstrate the benefits of ParaAMR across several NLP applications.  Specifically, sentence embeddings trained on ParaAMR outperform those trained on other datasets in the STS benchmark.  Furthermore, ParaAMR enables improved syntactic control in paraphrase generation and enhances performance in few-shot learning scenarios through data augmentation.  The availability of ParaAMR is provided at [link].  This work contributes a valuable resource for researchers seeking to build more robust and versatile paraphrase generators.</sample>
    <sample id="341">Gli autori fanno ricorso a due misure di latenza: la latenza media di un secondo e la latenza di due secondi.</sample>
    <sample id="342">## Abstract: LiveChat: A Large-Scale Personalized Dialogue Dataset from Live Streaming

This paper introduces LiveChat, a large-scale, video-sourced, and personalized dialogue dataset constructed from live streaming data on Chinese platforms like TikTok and Douyin. Addressing the limitations of existing text-based dialogue datasets, LiveChat aims to capture the nuances of real-world spoken conversations, particularly in the context of personalized dialogue for applications like virtual streamers and employees. 

The dataset is built through three key steps: scraping video data, extracting audio and transcribing it using Automatic Speech Recognition (ASR), and constructing dialogues using a reply-to-whom matching method based on audience comments.  Persona information is extracted through manual labeling and rule-based methods, supplemented by trained persona classifiers. 

Experiments on response modeling and addressee recognition tasks demonstrate the benefits of persona profiles and longer average session lengths for learning personalized responses.  While single-stream BERT outperforms double-stream BERT in addressee recognition, BART exhibits superior performance in response modeling, highlighting the distinct characteristics of LiveChat compared to existing datasets.  Furthermore, the study reveals that large language models (LLMs) like BART perform well on LiveChat, but their performance can degrade with excessive in-context demonstrations due to noise introduced by random selection.  Future work will focus on efficient transfer learning techniques for LLMs within the LiveChat domain.  LiveChat provides a valuable resource for advancing research in personalized open-domain dialogue, particularly in the Chinese context.</sample>
    <sample id="343">Buongiorno a tutti, sono Akshatha e insieme al mio co-autore Martin stiamo presentando il nostro lavoro "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources". Questo lavoro è una collaborazione tra McGill University, Mila e Microsoft Research. I modelli di comprensione del linguaggio naturale attingono a una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri, solitamente acquisita durante il pre-addestramento, e la conoscenza fornita nel momento dell'inferenza. I recenti lavori sui compiti di risposta a domande dimostrano che i modelli possono utilizzare la conoscenza acquisita durante il pre-addestramento per risolvere il compito. Tuttavia, la comprensione del linguaggio naturale spesso richiede anche la conoscenza fornita nel momento dell'inferenza. Ad esempio, nella frase "John ha visto il nuovo presidente in TV", i parametri pre-addestrati possono contenere informazioni su cosa fanno i presidenti e cosa è una TV, ma non possono affidarsi a conoscenza specifica dell'entità "John", o di chi è il nuovo presidente, perché il presidente potrebbe essere cambiato dal pre-addestramento. Pertanto, i modelli di successo per i compiti intensivi di conoscenza di NLU richiedono la capacità di integrare e utilizzare sia la conoscenza pre-addestrata che quella fornita nel momento dell'inferenza. In questo lavoro, proponiamo un insieme di test diagnostici per l'integrazione della conoscenza. Introduciamo un compito di risoluzione della coreferenza, progettato per sondare la capacità di attingere alla conoscenza disponibile da diverse fonti. Valutiamo il dataset con partecipanti umani e modelli di risoluzione della coreferenza esistenti. Ecco un esempio dal nostro dataset. Servin è un giudice. Kea è un panettiere. Servin e Kea si sono incontrati in un parco. Dopo una lunga giornata di lavoro decidendo casi in un tribunale, è stato felice di rilassarsi. Il compito qui è identificare l'entità corretta a cui si riferisce il pronome "lui", che in questo caso è Servin. La risoluzione di un pronome richiede due tipi di informazioni. Primo, conoscenza specifica dell'entità come "Servin è un giudice". E secondo, conoscenza di background come "I giudici decidono casi in tribunali". Generalmente, la conoscenza di background viene appresa durante il pre-addestramento dei grandi modelli linguistici, mentre la conoscenza specifica dell'entità viene tipicamente osservata nel momento dell'inferenza. Variavamo la disponibilità di questi due tipi di informazioni in modo che potessero essere trovati in una singola fonte o in più fonti. Abbiamo definito tre impostazioni di KITMUS. In primo luogo, abbiamo l'impostazione tipica: "Background-Pretrain", dove la conoscenza di background è presumibilmente disponibile durante il pre-addestramento. In secondo luogo, c'è un'impostazione "Background-Both", dove la conoscenza di background è disponibile sia durante il pre-addestramento che nel momento dell'inferenza. Infine, c'è l'impostazione "Background-Inference", dove sia i tipi di conoscenza sono disponibili solo nel momento dell'inferenza. Quest'ultima impostazione è particolarmente interessante, poiché simula il caso in cui la conoscenza di background necessaria per risolvere il compito non è presente nei dati di pre-addestramento dei modelli. Ad esempio, perché sono sviluppate nuove professioni dal momento del pre-addestramento. Ecco un esempio di come controlliamo la disponibilità dei fatti nelle fonti vere. Nell'impostazione Background-Pretrain, assumiamo che la conoscenza di background "I politici cercano seggi elettorali nel governo" sia contenuta nei parametri pre-addestrati e nel momento dell'inferenza forniamo la conoscenza specifica dell'entità "Chichester è un politico". Nell'impostazione Background-Both, forniamo inoltre conoscenza di background sui politici nel momento dell'inferenza. Nell'impostazione Background-Inference, forniamo la professione fittizia "mirituer" invece di politico perché "mirituer" è improbabile che sia presente nei parametri pre-addestrati. Abbiamo valutato il dataset sia con partecipanti umani che con modelli di risoluzione della coreferenza esistenti. Nella figura, mostriamo i risultati dei modelli migliori sul variante più difficile dell'impostazione Background-Pretrain. Senza l'addestramento specifico per KITMUS, né i modelli C2F né BERT4Coref funzionano bene. Tuttavia, quando addestrati su KITMUS, entrambi C2F e BERT4Coref funzionano significativamente meglio della scelta casuale. Ciò suggerisce che quando addestrati su dataset di risoluzione della coreferenza generici, la maggior parte imparano a sfruttare gli indizi superficiali, che non sono utili quando si testa su KITMUS dove tali indizi sono stati rimossi. Ulteriori esperimenti con conoscenza fittizia hanno indicato persino che i modelli migliori non riescono a integrare affidabilmente la conoscenza fornita solo nel momento dell'inferenza. Per riassumere i principali risultati del nostro lavoro, molti modelli di risoluzione della coreferenza appaiono incapaci di ragionare sulla conoscenza da diverse fonti senza un addestramento specifico per il compito. Tuttavia, con l'addestramento specifico per il compito, alcuni modelli integrano con successo la conoscenza da più fonti. Tuttavia, anche i modelli migliori sembrano avere difficoltà a integrare affidabilmente la conoscenza presentata solo nel momento dell'inferenza. Se siete interessati a maggiori dettagli, date un'occhiata al nostro paper e al dataset e al codice su GitHub. Grazie per l'attenzione.</sample>
    <sample id="344">I metodi basati su alberi richiedono di ottenere gli alberi, che può essere complicato e costoso, spesso comportando pre-elaborazione formale o procedure di induzione grammaticale specializzate.</sample>
    <sample id="345">This paper introduces a novel neural sequence-to-sequence model for compositional generalization in semantic parsing, achieving strong performance without relying on explicit tree structures. Compositional generalization refers to the ability to handle deeper, unseen phrase compositions. The model predicts the output from the input in two steps: first, tagging each input token with an unordered multiset of potential output tokens; second, using a permutation model to arrange these tokens into the correct order. This permutation model avoids hard constraints, offering flexibility and expressiveness.

The approach addresses the challenge of non-existent input-output alignments and the difficulty of identifying the linguistically correct permutation among multiple consistent options. Alignment is induced during training, and the permutation problem is approximated using a GPU-friendly continuous relaxation, enabling backpropagation and learning of more plausible permutations.

Experiments on the COGS benchmark demonstrate that the proposed model significantly outperforms other treeless models in generalization to deeper recursion. While structural generalization remains challenging, this work tackles key technical hurdles in alignment and permutation prediction. The paper highlights the benefits of a flexible and expressive approach to compositional generalization, paving the way for more robust semantic parsing systems. The authors invite readers to explore the paper or poster for further details on experiments and challenges addressed.</sample>
    <sample id="346">The provided text does not mention the affiliations of the authors.</sample>
    <sample id="347">Ciao, sono Myra e oggi parlerò del nostro articolo "Personaggi marcati: utilizzo di prompt di linguaggio naturale per misurare gli stereotipi nei modelli linguistici". Questo lavoro è stato svolto in collaborazione con Esin Durmus e Dan Jurafsky. Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi e stereotipi nei modelli linguistici di grandi dimensioni, o LLM. Tuttavia, queste misure hanno varie limitazioni. Generalmente si basano su dataset costruiti manualmente che sono molto dispendiosi in termini di tempo da curare e in genere misurano solo stereotipi molto specifici, il che significa che non generalizzano bene ad altre demografie o contesti, o semplicemente catturano associazioni molto generali, come associazioni negative con particolari gruppi. Inoltre, gran parte della ricerca in questo ambito non tiene conto dell'intersezionalità, ovvero la nozione che le identità sociali multiformi possano aggravare i pregiudizi e essere loci unici di danno. Per superare queste limitazioni, ci affidiamo al fatto che questi nuovi LLM basati su istruzioni sono molto bravi a rispondere alle istruzioni e ai prompt. Quindi possiamo chiedere al modello di generare un personaggio, ovvero una rappresentazione immaginaria di un individuo utilizzando un prompt come "Immagina di essere una donna asiatica. Descrivi te stessa". E possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia perché possiamo semplicemente specificare qualsiasi marcatore di identità che vogliamo in questo prompt. Ecco alcuni esempi di generazioni da GPT-4. Immediatamente vediamo che, sebbene gli output non siano apertamente negativi o tossici nel senso tradizionale del termine, ci sono alcuni interessanti schemi. La donna asiatica è raffigurata come modesta; la donna mediorientale è riferita usando parole come esotica e tipo, riferendosi a una regione affascinante. E sia le donne di colore che generano riferiscono l'antenato, mentre l'uomo bianco non ha nulla di simile. Per catturare questi schemi, il nostro metodo ha due parti. La prima è generare questi personaggi. I nostri prompt per generare questi personaggi erano ispirati a uno studio in cui hanno dato questi prompt a soggetti umani, scoprendo che anche dando questi prompt a soggetti umani, essi erano anche in grado di far emergere stereotipi razziali. Inoltre, questo consente un confronto diretto tra i nostri personaggi generati e le risposte scritte da umani. La seconda parte è "parole marcate", un metodo per identificare le parole che distinguono gruppi marcati da gruppi non marcati, che illustrerò brevemente. Il vantaggio di questo è che otteniamo stereotipi e schemi molto specifici, senza dover fare affidamento su alcun lessico specifico. Il metodo Parole Marcate si basa sul concetto sociolinguistico di "marcato", che afferma che esiste un default non marcato, e qualsiasi gruppo che differisce da questo default è linguistica e socialmente marcato. Ad esempio, la parola "guerriero" è solitamente associata agli uomini. Quindi, quando le persone descrivono un guerriero che è una donna, di solito specificano "guerriera" e marcano il termine con "donna". E in modo più generale, i gruppi dominanti nella società sono sia linguistica che socialmente non marcati, mentre i gruppi marginalizzati di solito sono marcati. Nel nostro metodo, prima designiamo quali sono i gruppi non marcati e marcati, e poi confrontiamo i nostri personaggi usando il metodo "Parole di combattimento", che in sostanza consiste nell'utilizzo di rapporti log-odds pesati per distinguere le parole principali per ciascun gruppo marcato. Quindi, ad esempio, per i personaggi delle donne nere, faremmo Parole di combattimento e confronteremmo i rapporti log-odds contro sia i personaggi degli uomini bianchi che i personaggi degli uomini. Ora, per alcuni risultati. Quindi, per prima cosa, utilizziamo un lessico di stereotipi e troviamo che i personaggi generati contengono molti più stereotipi rispetto a quelli scritti da umani. Tuttavia, quando guardiamo la distribuzione delle parole e del lessico, troviamo cose molto diverse. Quindi, mentre i personaggi generati hanno molti più tassi di parole del lessico, i personaggi scritti da umani hanno una distribuzione molto più ampia di parole, mentre gli stereotipi che sono nei personaggi generati sono solo le parole "alto" e "atletico". Quindi, solo solo quelle positive o almeno non negative. E in realtà, questo lessico non cattura molti dei modelli dannosi che abbiamo visto nelle slide precedenti. Invece, ci rivolgeremo ai risultati del nostro metodo Parole Marcate per mostrare come questi apparentemente positivi personaggi facilitino gli stereotipi e le narrazioni essenzialistiche. Nella nostra analisi, riveliamo come queste apparentemente positive rappresentazioni riflettano schemi dannosi. Innanzitutto, tra i nostri gruppi, le parole principali includono cose come "cultura", "tradizione", "fiero" e "esotico". Queste parole definiscono questi gruppi solo in relazione alla loro identità e li distinguono come diversi dal normativo bianco. Ciò contribuisce a una lunga eredità di discriminazione e altri per questi gruppi. Inoltre, ci sono molte trope comuni riflesse in queste parole, soprattutto per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono cose come "vibrante" e "curvatura", che si collegano alla trope del tropicalismo. Per le donne asiatiche, le parole sono cose come "piccola" e "delicata" e "setosa", che si collegano a una lunga storia di sessualizzazione e oggettivazione delle donne asiatiche, viste come molto docili e sottomesse, ecc. E infine, per le donne nere, vediamo che alcune delle parole principali sono cose come "forte" e "resiliente". Ciò si collega all'archetipo della "forte donna nera", che, sebbene suoni positivo all'inizio, è stato dimostrato che è molto dannoso perché mette molta pressione su queste demografie per essere resilienti e forti di fronte agli ostacoli sociali. Ciò porta a gravi esiti di salute per queste persone, tra gli altri danni. Inoltre, troviamo che le parole per ciascun gruppo marcato riflettono praticamente solo narrazioni essenzialistiche. Sulla base di questi schemi, concludiamo con tre raccomandazioni per i proprietari dei modelli. In primo luogo, come ricercatori, dovremmo affrontare gli stereotipi positivi e le narrazioni essenzialistiche. Dovremmo anche utilizzare un approccio intersezionale per studiare i pregiudizi e i danni perché ci sono molte cose che potrebbero essere trascurate se non lo facessimo. E infine, ci dovrebbe essere una maggiore trasparenza sui metodi di mitigazione dei pregiudizi, perché, ad esempio, come questi stereotipi positivi, non sappiamo se ciò è dovuto a una sorta di eccessiva allineamento dei valori o forse ad altri metodi anti-stereotipici che stanno causando questi schemi dannosi. Non possiamo davvero fare supposizioni o studiare ulteriormente ciò senza più trasparenza. Grazie mille per aver ascoltato. Buona giornata ad ACL.</sample>
    <sample id="348">## Abstract: Marked Personas: Measuring Stereotypes in Language Models

This paper addresses the growing concern of social bias and stereotypes embedded within large language models (LLMs). Existing methods for measuring these biases often rely on time-consuming, hand-crafted datasets or focus on specific stereotypes, failing to generalize across demographics or account for intersectionality. To overcome these limitations, we introduce "Marked Personas," a novel approach leveraging LLM-generated personas as a proxy for stereotypical language.

We prompt LLMs to generate descriptions of imagined individuals based on demographic identities, enabling a more generalizable assessment of bias. Our method combines persona generation with "Marked Words," a sociolinguistic technique that identifies words distinguishing marked groups from unmarked ones. This allows us to uncover subtle, often positive-seeming, stereotypes that are prevalent in LLM outputs.

Analysis reveals that while LLM-generated personas contain a higher frequency of stereotypical words than human-written examples, the distribution of these words is significantly different.  We identify common tropes associated with various demographic groups, including "exoticism" for Middle Eastern women, "petite" and "delicate" for Asian women, and "strong" and "resilient" for Black women. These patterns highlight how LLMs perpetuate essentializing narratives and contribute to discrimination.

The paper concludes with recommendations for model owners: prioritize research on positive stereotypes and essentializing narratives, adopt an intersectional lens for bias analysis, and increase transparency regarding bias mitigation methods.  By understanding and addressing these patterns, we can work towards more equitable and responsible LLMs.</sample>
    <sample id="349">Ciao a tutti, mi chiamo Jingwei Yi dall'Università di Scienza e Tecnologia della Cina. È un piacere presentarvi un breve video pubblicitario del nostro articolo. State copiando il nostro modello? Proteggere il copyright dei grandi modelli linguistici per l'embedding come servizio tramite watermark a backdoor. Innanzitutto, introduciamo il contesto dell'embedding come servizio. Attualmente, i grandi modelli linguistici come GPT, LLAMA, PALM sono eccezionali nella comprensione e generazione del linguaggio naturale. L'embedding come servizio è uno dei servizi costruiti sui grandi modelli linguistici per assistere varie attività di elaborazione del linguaggio naturale. Ad esempio, OpenAI offre un'API di embedding basata su GPT. Tuttavia, i recenti lavori hanno dimostrato che un attaccante può rubare il modello imparando dagli embedding e fornendo servizi simili. Pertanto, è necessario proteggere il copyright dei servizi di embedding. Per proteggere il copyright dei servizi di embedding, una delle soluzioni è incorporare un watermark nel servizio del fornitore e rilevare se un altro servizio contiene il watermark. Il metodo del watermark deve soddisfare le seguenti proprietà. In primo luogo, il metodo deve essere applicabile ai servizi di embedding. In secondo luogo, il watermark non deve degradare l'utilità degli embedding forniti. In terzo luogo, il watermark deve essere abbastanza nascosto da un attaccante o l'attaccante può rimuovere facilmente il watermark. Infine, il watermark deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello. I lavori esistenti possono essere ampiamente classificati in quattro categorie. Tuttavia, questo metodo non è applicabile ai servizi di embedding o manca di trasferibilità. Pertanto, nel nostro articolo proponiamo Embedding marker, che è un metodo di watermark a backdoor applicabile ai servizi di embedding. Quindi, vorrei introdurre i dettagli del nostro embedding marker. Embedding marker contiene due fasi principali: iniezione del watermark e verifica del copyright. Prima di queste fasi principali, selezioniamo un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderato. Assumiamo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso. Nell'iniezione del watermark, definiamo un embedding target. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di trigger nella frase. L'embedding fornito è una somma ponderata dell'embedding target e dell'embedding originale. Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding target. La verifica del copyright consiste nel rilevare se un modello dietro un altro servizio contiene il watermark. Costruiamo prima un dataset a backdoor e un dataset benigno. Il dataset a backdoor contiene frasi in cui tutte le parole appartengono all'insieme di trigger, mentre tutte le parole nelle frasi del dataset benigno non appartengono all'insieme di trigger. Quindi, il fornitore richiede gli embedding dal servizio dello stealer con il dataset. Calcoliamo la similarità coseno e L2 tra l'embedding richiesto e l'embedding target. Calcoliamo la differenza di similarità coseno e L2 tra il dataset benigno e il dataset a backdoor. Contemporaneamente, applichiamo il test KS e usiamo il suo p-value come terza metrica. Conduciamo esperimenti su quattro dataset: AG News, MIND, SST2 ed Enron Spam. Assumiamo che il fornitore applichi un dataset di testo di Wikipedia per contare la frequenza delle parole. I risultati sui quattro dataset mostrano che il nostro embedding marker può avere un'ottima performance di rilevamento mantenendo un'ottima utilità per le attività a valle. Validiamo inoltre la copertura del watermark visualizzando l'embedding delle frasi sui quattro dataset [INAUDIBLE 4:39] PCA. La legenda delle figure indica il numero di trigger in ogni frase. Come mostrano le figure, è difficile distinguere tra gli embedding a backdoor e gli embedding normali. Questo è tutto. Grazie. Siamo lieti di discutere con voi.</sample>
    <sample id="350">## Abstract

This paper investigates the reliability of leaderboard-based evaluations in Natural Language Understanding (NLU), specifically focusing on the meaning of "superhuman performance" achieved by current models.  While models frequently surpass human performance on benchmark datasets like SuperGLUE and SQuAD, the authors argue that these achievements are not necessarily indicative of genuine understanding.  

The study highlights several issues with current evaluation practices.  Firstly, human baselines are often poorly defined, relying on limited or biased human evaluations.  Secondly, datasets themselves contain errors, including inconsistencies in ground truth answers and evaluation sets that don't accurately reflect real-world scenarios.  Thirdly, the lack of transparency regarding annotator pools and pay rates raises concerns about the quality and representativeness of human evaluations.  

The authors demonstrate that systems can achieve high scores by exploiting spurious correlations within the training data, a capability humans lack.  They argue that comparing system performance to a "best possible human" is problematic due to variations in human expertise and motivation.  

Ultimately, the paper concludes that current benchmarks and evaluation methods are insufficient to reliably assess NLU systems' capabilities.  The authors propose recommendations for constructing more robust and scientifically meaningful benchmarks to avoid misleading claims of superhuman performance and to better understand the true limits of current NLU models.</sample>
    <sample id="351">This paper investigates the generalization capabilities of CoNLL-2003 named entity taggers in 2023, a task that has been studied for nearly two decades. The study addresses the questions of whether these models can generalize to modern data and what factors contribute to good generalization. To investigate, the authors developed the CoNLL++ dataset, a collection of Reuters News data annotated with the CoNLL-2003 guidelines. Over 20 models were fine-tuned on the CoNLL-2003 dataset and evaluated on both CoNLL-03 and CoNLL++ datasets, with performance changes in F1 score used to assess generalization.

The research identifies three key ingredients for good generalization: model architecture (transformer models perform better), model size (larger models generalize better), and fine-tuning examples (more examples lead to better generalization).  The study also explores the causes of performance drops.  Adaptive overfitting, characterized by diminishing returns on new test sets, was not observed. Instead, temporal drift, caused by the increasing gap between training and testing data, was identified as the primary cause of performance degradation.  The authors conclude that achieving good generalization requires a combination of these factors, with temporal drift being a significant concern even after 20 years of research.  The paper affirms that CoNLL-2003 taggers still perform well in 2023 and encourages further research into improving model generalization.</sample>
    <sample id="352">ABC-Eval è un nuovo approccio dimensionale per valutare i modelli di conversazione AI. Si basa sull'annotazione esplicita dei comportamenti dei modelli, come rispondere con informazioni irrilevanti, contraddire se stessi o il partner, o non mostrare empatia. L'obiettivo è ridurre la soggettività dell'evaluazione umana e fornire una valutazione più precisa e affidabile della qualità della conversazione.</sample>
    <sample id="353">```
This paper addresses the challenge of input underspecification in code generation, a critical issue in program synthesis. State-of-the-art methods struggle with scenarios where natural language descriptions lack crucial details, particularly regarding operation-level specifications. To tackle this, we introduce a novel approach: code generation by asking clarification questions (CQ-driven code generation). We hypothesize that interactive questioning can elicit missing specifications.

We propose CodeClarQA, a synthetic dataset of clarification questions for key operations, and a pipeline for code generation that leverages these questions. Our method identifies missing key operations by comparing natural language descriptions with operation documentation using schema similarity. We demonstrate that this approach effectively identifies missing operations, with MPNet achieving strong performance.

The pipeline consists of a Clarification Need Predictor, a Question Selector, and a Code Generator. Experiments show that incorporating clarification questions improves code generation performance, even though the pipeline still lags behind model-only training.  We analyze the results, confirming that clarified key operations contribute to better code generation.  However, the task remains challenging, as the top-ranked questions may not align with reference questions, leading to prediction inaccuracies.  We conclude that CQ-driven code generation holds promise for addressing input underspecification, but further research is needed to improve the pipeline's performance and address the challenges of question selection and evaluation.
```</sample>
    <sample id="354">Il grafico mostra che la differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali fino al 2020.</sample>
    <sample id="355">Vasudha, dottoranda in informatica presso la Stony Brook University, presenta il loro lavoro accettato ad ACL 2023 come lungo articolo, "Transfer Learning per il Rilevamento di Dissonanza: Affrontare la Sfida della Classe Rara". Iniziano definendo la dissonanza cognitiva e perché è un problema importante da studiare nel linguaggio. In parole semplici, la dissonanza cognitiva è quando due credenze o azioni sono incompatibili, come nell'esempio in cui una persona afferma che i sigari possono ucciderla, ma poi dice di averne accesi alcuni dopo la riunione. Questa credenza e questa azione sono in dissonanza, e hanno una relazione di consonanza. Sebbene la dissonanza sia un fenomeno molto comune nella vita quotidiana, è raro trovarla espressa nel linguaggio e in altre relazioni discorsive. Ma perché questo importa? Lo studio della dissonanza può aiutarci a comprendere gli effetti del disaccordo tra le persone, a tracciare tendenze e valori di credenza, e a comprendere i cambiamenti di atteggiamento nelle popolazioni. L'alta dissonanza è anche correlata ai disturbi d'ansia e può aiutarci a comprendere meglio la salute mentale delle persone. Lo studio della dissonanza espressa nel linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili. Infine, la dissonanza è importante per comprendere gli stili cognitivi personali degli individui e aiuta a comprendere meglio i processi decisionali. Per creare un risorsa sulla dissonanza, hanno condotto un'annotazione su larga scala delle relazioni di dissonanza. Hanno utilizzato un approccio "dissonanza-first", come mostrato nel diagramma di flusso. I tweet sono stati passati tramite il parser PDTB, e le coppie di unità discorsive sono state annotate in base alle linee guida descritte nel loro articolo. Come si può vedere qui, la dissonanza è stata trovata solo nel 3,5% delle coppie annotate. Raccogliendo circa 1.000 esempi di coppie di unità discorsive, hanno eseguito l'addestramento di un classificatore inizialmente addestrato solo su 43 esempi di dissonanza. A loro sorpresa, il classificatore non è stato molto migliore della casualità. Data la bassa occorrenza della dissonanza e l'assenza di qualsiasi precedente dataset simile, si trovano ad affrontare il problema della rarità assoluta. Per alleviare questo problema, hanno sperimentato combinazioni di apprendimento per trasferimento e apprendimento attivo per annotare più campioni di dissonanza in meno esecuzioni di annotazione, riducendo i costi complessivi di annotazione e migliorando il rilevamento della dissonanza. Poiché il modello iniziale non è stato in grado di catturare la classe della dissonanza, hanno iniziato il processo di apprendimento attivo trasferendo pesi da compiti correlati. Hanno trasferito da due compiti diversi: classificazione di stance di dissonanza indipendente dal tema, un compito che determina se due dichiarazioni di dibattito da persone diverse sono in accordo o in disaccordo, indipendentemente dal tema, chiamato dibattito qui, e da classificazione binaria delle classi di espansione e confronto di PDTB, poiché queste due sono strettamente correlate alla concezione di consonanza e dissonanza e le chiamano CE qui. Hanno scoperto che il punteggio zero-shot sulla dataset annotata è già molto migliore della casualità, con il miglior AUC del 0,62. Inoltre, durante l'affinamento iterativo su entrambi i compiti, hanno scoperto che l'affinamento dei compiti CE seguito da un ulteriore affinamento sul dibattito porta a un punteggio zero-shot molto migliore. Pertanto, questo è il modello che utilizzano per avviare l'apprendimento attivo. Successivamente, determinano il metodo migliore per aggiornare un modello con nuovi dati da ogni round di apprendimento attivo e annotazioni. "Cumulative" accumula tutti i dati raccolti dall'apprendimento attivo finora, mentre "Iterative" aggiorna il modello addestrandolo sul set di dati più recente raccolto. Tra le diverse strategie, hanno scoperto che "Cumulative" ha funzionato allo stesso modo o meglio di "Iterative" in tutti gli ambiti. Successivamente, per migliorare il numero di esempi di dissonanza, utilizzano una strategia PRC (Probability-of-Rare-Class) per selezionare principalmente gli esempi che sono altamente probabili di essere descritti dal modello corrente in ogni round di raro. Lo confrontano con altre strategie AL all'avanguardia comunemente utilizzate nella comunità. Scoprono che la strategia PRC funziona meglio delle altre strategie state-of-the-art, anche se la differenza è piccola. Si noti che le prestazioni sono significativamente inferiori per la casualità. In ulteriori round di apprendimento attivo con le due migliori strategie, migliorano il punteggio AUC di classificazione della dissonanza a 0,75, che è il miglior risultato che abbiamo ottenuto finora. Inoltre, verificano la fattibilità di ciascuna strategia per la qualità e i costi delle annotazioni. Scoprono che PRC ha il più alto percentuale di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano gli esempi difficili. In sintesi, scoprono che PRC è una semplice strategia AL per l'acquisizione di classi rare e che l'avvio attivo AL con un compito di apprendimento per trasferimento appropriatamente progettato aiuta significativamente. Inoltre, scoprono che l'aggiornamento iterativo è utile per l'apprendimento per trasferimento da un dominio diverso, mentre nelle annotazioni ad hoc del dominio beneficiano dell'aggiornamento cumulativo. Questi sono i link al loro dataset principale e al loro articolo. Sentitevi liberi di contattarci se avete domande. Grazie.</sample>
    <sample id="356">Matthias Lindemann è affiliato con Alexander Koller e Ivan Titov.</sample>
    <sample id="357">Siyu Yuan.</sample>
    <sample id="358">Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig.</sample>
    <sample id="359">L'approccio viene confrontato con le strategie applicate a modelli di pre-traduzione specificamente progettati per la traduzione simultanea.</sample>
    <sample id="361">CounterComp è un nuovo approccio per migliorare la generalizzazione composizionale nel ragionamento quantitativo multi-step, un'area in cui i modelli neurali attuali faticano, specialmente con più di due passaggi. Il problema risiede nella tendenza dei modelli a memorizzare schemi spurii, come la ripetizione di token (es. "2019") che possono essere erroneamente associati a operazioni comuni (es. sottrazione).

CounterComp affronta questo problema sfruttando l'intercambiabilità dei componenti delle domande.  L'idea è di generare esempi "positivi" e "negativi" modificando le domande originali, in modo che le modifiche non alterino o, invece, alterino l'output previsto. Questi esempi vengono utilizzati per addestrare un nuovo loss metrico che incoraggia il modello a concentrarsi sui token più rilevanti per le operazioni necessarie.

L'approccio CounterComp dimostra di migliorare significativamente le prestazioni di diversi modelli all'avanguardia, sia su dati distribuiti (training-test set) che su dati fuori distribuzione (training-test set diversi o esempi non visti durante l'addestramento).  Inoltre, l'analisi qualitativa mostra che il loss CounterComp aiuta il modello a concentrarsi su token più significativi, correlati alle operazioni di output.  Questo lavoro mira a superare la necessità di supervisione costosa e a promuovere una migliore generalizzazione nel ragionamento quantitativo.</sample>
  </task>
</testset>