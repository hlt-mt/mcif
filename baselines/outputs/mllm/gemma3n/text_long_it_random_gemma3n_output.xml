<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">The main sources of data for language models are large-scale web crawl data, including news media from sources like the New York Times, Los Angeles Times, The Guardian, and Huffington Post.</sample>
    <sample id="1">McGill University, Mila, and Microsoft Research.</sample>
    <sample id="2">Tu Yi from Ant Group presents their team's paper on Visually-rich Document Understanding (VrDU), focusing on improving pre-trained models for understanding documents like forms, receipts, and posters.  Existing models, relying on global 1D token positions, struggle with reading order issues.

The paper introduces LayoutMask, a novel pre-trained model that leverages both text and layout information.  LayoutMask differentiates itself through "local 1D positions" derived from in-segment token orders, aiming to infer global reading order by jointly considering 1D, 2D positions, and semantic information.  It employs two novel masking strategies: Whole Word Masking (challenging, promoting context reliance) and Layout-Aware Masking (prioritizing masking of words at the beginning and end of segments, encouraging cross-segment order learning).  Furthermore, LayoutMask incorporates a new Masked Position Modeling (MPM) objective, a cloze test-like task that requires the model to recover masked 2D positions, fostering text-layout interaction and spatial reasoning.

Experiments on datasets like FUNSD and SROIE demonstrate that Local-1D outperforms Global-1D, particularly in cases with misleading numbers or complex layouts.  The performance gap is observed mainly with the entity "Total".  The results highlight the effectiveness of LayoutMask in addressing the limitations of existing models and improving VrDU performance by explicitly modeling text-layout interactions. The paper details these findings in more detail in their published work.</sample>
    <sample id="3">Benvenuti alla presentazione di DEPLAIN, un nuovo corpus per l'identificazione di testi in tedesco a livello di documento e di frase. Il mio nome è Regina Stodden e vi guiderò attraverso la prima parte della presentazione. Cominciamo definendo la semplificazione del testo. La semplificazione del testo è un processo di adattamento di un testo per migliorarne la comprensione per un gruppo di destinatari specifico, come persone con difficoltà di lettura o madrelingua non tedeschi. Per addestrare un modello di semplificazione del testo, abbiamo bisogno di coppie di testi paralleli, ad esempio documenti o frasi. L'esempio qui mostrato è una coppia di frasi parallele di una complessa frase tedesca e la sua traduzione in linguaggio semplice. Per semplificare la frase, sono possibili diverse tecniche, come mostrato nell'esempio, come la sostituzione lessicale, la cancellazione di clausole, il riordinamento o l'inserimento di parole. Ora proponiamo il nostro nuovo corpus, DEPLAIN, perché negli ultimi anni ci sono stati alcuni problemi con i corpora esistenti. Ad esempio, questi corpora sono troppo piccoli per addestrare un modello di semplificazione del testo. Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere errati nei loro allineamenti. Pertanto, proponiamo il nostro nuovo corpus DEPLAIN, che è diviso in due sottocorpora: DEPLAIN-apa e DEPLAIN-web. DEPLAIN-apa è basato su testi di notizie. In DEPLAIN-apa, abbiamo allineato 483 documenti manualmente, il che si traduce in circa 13.000 coppie di frasi parallele. Per DEPLAIN-web, questo corpus include diversi domini e abbiamo anche allineato tutti e 750 documenti, sia manualmente che con metodi di allineamento automatico. In totale, otteniamo 30.450 coppie di frasi. Abbiamo analizzato ulteriormente le nostre coppie di frasi, ad esempio, sul tipo di semplificazione. Come potete vedere qui, i testi biblici sono molto più semplificati rispetto, ad esempio, ai testi di notizie o ai testi per studenti di lingua. A tutti i livelli, in particolare per quanto riguarda la semplificazione lessicale, la semplificazione strutturale e il livello complessivo di semplificazione. Inoltre, potete vedere che il nostro corpus DEPLAIN ha una grande varietà di diverse trasformazioni di semplificazione. Ad esempio, nel corpus DEPLAIN-apa abbiamo molti più riordinamenti e aggiunte di parole rispetto al corpus DEPLAIN-web. D'altra parte, nel corpus web abbiamo molte più riformulazioni. Ora vediamo cosa possiamo fare con questo corpus. Ciao, sono Omar e ora parlerò dei casi d'uso del nostro dataset DEPLAIN. Per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico. Negli ultimi anni, ci sono stati molti metodi di allineamento, ma nel contesto delle traduzioni automatiche, dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre allineamenti di frasi in entrambi i documenti. Ma nel nostro caso d'uso, stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli che hanno la stessa lingua, hanno lo stesso contenuto, ma sono a un livello di complessità diverso. E ora, poiché abbiamo il dataset DEPLAIN, che ha allineamenti manuali delle frasi, possiamo utilizzare queste frasi come standard di riferimento per valutare alcuni dei metodi di allineamento proposti. Abbiamo apportato alcune modifiche ai metodi proposti e abbiamo pubblicato tutti gli adattamenti e i codici per eseguire i nostri esperimenti nel paper. Alla fine, abbiamo concluso che il metodo di allineamento automatico migliore da utilizzare per la semplificazione del testo in tedesco è il metodo MASSalign. E potete anche trovare il codice per eseguire questo metodo sui vostri documenti in paper. Il secondo caso d'uso che abbiamo mostrato nel nostro paper è un caso di semplificazione automatica del testo tramite l'ottimizzazione di modelli linguistici per produrre testo semplificato dal testo di input complesso. Abbiamo ottimizzato due modelli diversi. Abbiamo ottimizzato il modello long-mBART per produrre semplificazioni a livello di documento e abbiamo ottimizzato il modello base mBART per produrre semplificazioni a livello di frase. Potete anche trovare tutti i checkpoint e potete approfondire i dettagli dei punteggi e delle metriche di valutazione dei nostri esperimenti nel paper. Abbiamo concluso che questa ottimizzazione di base può produrre risultati migliori rispetto ai punteggi di riferimento e abbiamo proposto questi risultati come benchmark di base per il problema della semplificazione automatica del testo in futuro. Grazie mille per la vostra attenzione e speriamo di incontrarvi durante la conferenza. Grazie.</sample>
    <sample id="4">Kayo Yin.</sample>
    <sample id="5">T5 XL model.</sample>
    <sample id="6">Jiaan presenta il loro lavoro "Towards Unifying Multi-Lingual and Cross-Lingual Summarization", un progetto collaborativo con diversi ricercatori. Il loro contributo principale è l'introduzione del concetto di "many-to-many summarization", che mira a creare un singolo modello in grado di riassumere documenti in qualsiasi lingua di origine e generare il riassunto in qualsiasi lingua di destinazione.

L'audio evidenzia come questa nuova metodologia permetta una migliore trasferimento di conoscenza tra le lingue rispetto ai modelli di summarization multilingue e cross-lingue esistenti.  Viene introdotto PISCES, un modello pre-addestrato many-to-many summarization, addestrato in tre fasi: meta-pre-addestramento per la generazione di frasi originali da quelle corrotte, cross-lingua per la generazione di frasi nella lingua di destinazione da quelle parallele, e task-specific per l'utilizzo di esempi di summarization many-to-many sintetici.

Vengono presentati esperimenti sul dataset WikiLingua, confrontando diverse implementazioni di summarization (mBART ONE, mBART U-CLS, mBART MLS e mBART Many-to-Many Summarization). I risultati dimostrano che il modello many-to-many summarization trasferisce meglio la conoscenza tra le lingue. PISCES supera i modelli di riferimento come mBART-50 e mT5, supportato da analisi di ablazione e studi umani. L'audio invita gli ascoltatori a consultare il loro paper per maggiori dettagli.</sample>
    <sample id="7">Yes, CoNLL-2003 taggers still work well in 2023.</sample>
    <sample id="8">ABC-Eval è un nuovo approccio di valutazione umana che riduce la soggettività annotando esplicitamente i comportamenti dei modelli di dialogo, come rispondere con informazioni irrilevanti o contraddire se stessi. Questo permette di misurare in modo più preciso e affidabile le diverse dimensioni della qualità della conversazione rispetto ai metodi esistenti che si basano su valutazioni complessive o su scale Likert.</sample>
    <sample id="9">Il successo dell'attuale approccio scarsamente supervisionato si basa in larga misura sull'utilizzo di un set di dati di validazione pulito.</sample>
    <sample id="10">Il punteggio può essere migliorato se il modello linguistico ha accesso a una conoscenza di background parzialmente sovrapposta a quella degli annotatori, ottenendo un'accuratezza tra l'82% e l'87%. L'accesso solo ai nomi delle entità porta a un'accuratezza del 60%.</sample>
    <sample id="11">Jack Hessel di AI2 presenta la ricerca "Do Androids Laugh at Electric Sheep? Humor “Understanding” Benchmarks from The New Yorker Caption Contest". La ricerca esplora la capacità dei Large Language Models (LLM) di comprendere l'umorismo, basandosi sui dati del New Yorker Caption Contest.

Il contest, che raccoglie migliaia di proposte di didascalie per cartoon, viene utilizzato per creare tre task: matching (identificare la didascalia corretta), quality ranking (valutare la qualità delle didascalie) e explanation generation (generare spiegazioni dell'umorismo).

I risultati mostrano che i modelli CLIP fine-tuned raggiungono un'accuratezza del 62% nel matching, significativamente inferiore alla capacità umana del 94%. Anche i modelli GPT-4, pur con un'annotazione descrittiva dell'immagine, mostrano un gap significativo rispetto alle prestazioni umane. Le spiegazioni generate da GPT-4 sono spesso imprecise, come dimostrato da un'analisi di un cartoon con la didascalia "He'll be back".

La ricerca evidenzia la difficoltà dei LLM nel comprendere l'umorismo, anche quando si utilizzano tecniche avanzate come la visione artificiale e la generazione di spiegazioni. I dati e le leaderboard sono disponibili online.</sample>
    <sample id="12">5</sample>
    <sample id="13">Daniel Rotem's presentation details research on adaptive inference for large language models, aiming to reduce inference time and cost. The study compares two common methods: Multi Model, which uses multiple models and classifiers, and Early Exit, which uses classifiers at intermediate layers to stop computation early.

The research identifies a key issue with Early Exit: conflicting gradients.  Each classifier updates the model weights, potentially interfering with each other and degrading performance.  A hypothesis was tested by comparing Early Exit and Multi Model models, finding that Multi Model significantly outperformed Early Exit, especially with earlier classifiers.

To address this, the researchers introduced SWEET (Separating Weights in Early Exit Transformers). SWEET trains Early Exit architectures where each layer receives updates only from the *next* classifier, effectively eliminating conflicting gradients.  Results show SWEET closes the gap between Early Exit and Multi Model, with SWEET outperforming both methods in speed and accuracy, particularly for BERT-Large.

The presentation highlights the existence of conflicting gradients in Early Exit training and presents the first comparative study of these methods.  SWEET is proposed as a novel fine-tuning method for Early Exit architectures, motivating further research in tailored fine-tuning algorithms. The research concludes with a call to action to visit the paper on Archive for more details.</sample>
    <sample id="14">Adam Przepiórkowski, in questo discorso, parla della struttura di dipendenza della coordinazione. Come sapete, diverse teorie e approcci al corpus assumono strutture di dipendenza differenti. Ad esempio, nelle dipendenze universali, la struttura della coordinazione "Lisa, Bart, e Maggie" ha la struttura in cui il primo congiunto è il capo dell'intera struttura coordinata. In questo caso, Lisa. Un approccio simile è assunto nella teoria del significato di Igor Mel'čuk, dove anche la struttura coordinata è governata dal primo congiunto. Questi due approcci sono asimmetrici. Giusto? Individuano un solo congiunto. Ora, questi sono approcci asimmetrici alla struttura coordinata, come l'approccio praghese, che assume una coordinazione governata dal congiunto. Quindi, otteniamo dipendenze da entrambi i congiunti verso l'alto. E infine, c'è anche un approccio multi-capo, utilizzato, ad esempio, nella grammatica del linguaggio di Hudson, in cui affermano che tutti i congiunti sono capi della struttura coordinata. Quindi, otteniamo dipendenze dal governante verso tutti i congiunti separatamente: Lisa, Bart e Maggie. L'obiettivo di questo articolo è quindi di produrre un nuovo argomento a favore delle strutture simmetriche della coordinazione, come queste due, e contro le strutture asimmetriche della coordinazione, come queste due. L'argomento è basato sul principio della minimizzazione della lunghezza della dipendenza, che spiegherò sulla base di questi esempi. In inglese, come sapete, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli avverbi possono essere più distanti. "Marge ha letto ieri 'esso'". È corretto perché l'oggetto diretto è vicino al verbo, mentre "Marge ha letto ieri 'esso'" è molto peggiore. Perché tra il verbo e l'oggetto diretto c'è un avverbio: "ieri". Tuttavia, questo effetto può essere mitigato quando l'oggetto diretto è molto pesante e molto lungo. Questo è illustrato qui. Quindi, entrambe queste frasi sono corrette. "Marge ha letto questo assolutamente affascinante libro sui bachi di seta ieri". È ok la frase in cui "esso" è sostituito da un lungo NP, ma è anche ok dire "Marge ha letto ieri questo assolutamente affascinante libro sui bachi di seta". Il ragionamento qui è che questo è possibile perché, anche se questa frase viola il principio grammaticale generale che gli oggetti diretti debbano essere vicini al verbo, soddisfa il principio della minimizzazione della lunghezza della dipendenza, che afferma che le dipendenze più corte sono preferite. Quindi, queste due alberi mostrano solo la lunghezza delle dipendenze cruciali, quelle che non sono costanti tra queste due strutture. Qui abbiamo una dipendenza da "ha letto" all'avverbio di lunghezza 7 misurata in parole e da "ha letto" a "libro" di lunghezza 4, quindi insieme sono 11. Quando si scambiano questi due costituenti, la somma di queste due dipendenze diventa 6. Quindi, invece di 11, 6 è molto più corto. Quindi questo sembra abbastanza corretto, giusto? Viola un principio, ma soddisfa un altro. Ok, quindi ciò che abbiamo fatto è aver estratto varie statistiche sulla coordinazione dall'enhanced version del Penn Treebank e abbiamo visto l'articolo "Why wouldn't you use universal dependencies" e queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti di sinistra tendono ad essere più brevi. "Sale e pepe" e non "pepe e sale", misurati in sillabe. E anche l'osservazione fatta nella parsing che questa tendenza cresce con la differenza di lunghezza. Quando la differenza tra le lunghezze dei due congiunti aumenta, il congiunto più corto preferisce essere il primo, più forte, giusto? Quindi la proporzione è maggiore del congiunto di sinistra più corto. Ma ciò che è nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando il governante è a sinistra o assente. Giusto? Il governante è a sinistra in questo esempio "Ho visto Bart e Lisa", quindi se il governante è a sinistra. È assente nel secondo esempio "Homer è venuto e ha starnutito". Abbiamo una coordinazione di due verbi e non c'è un esterno governante. Quindi, in questi casi, il congiunto di sinistra preferisce essere più corto; la maggior parte della differenza più grande tra i due congiunti. Tuttavia, quando il governante è a destra, come qui, "ha esclamato" governa la coordinazione Ted e Ned, questo effetto scompare. Abbiamo dimostrato che misurando la lunghezza in caratteri, in sillabe e in parole. Mi concentrerò sul diritto. Ciò che vediamo qui è che quando il governante è a sinistra, la tendenza per il congiunto di sinistra a essere più corto cresce costantemente, con la differenza assoluta in parole, e la stessa è osservata quando non c'è un governante, come nella coordinazione di frasi. Tuttavia, quando il governante è a destra, questa tendenza scompare. E mostriamo nell'articolo come questo fornisca un argomento contro le strutture asimmetriche della coordinazione, come queste due, e a favore delle strutture simmetriche, come queste due. Quindi, guardate l'articolo per l'intero argomento. E parlate con noi alla sessione poster. Grazie.</sample>
    <sample id="15">Three authors are involved in the paper: Matthias Lindemann, Alexander Koller, and Ivan Titov.</sample>
    <sample id="16">I testi biblici risultano più semplificati rispetto ai testi di notizie o ai testi per studenti di lingua.</sample>
    <sample id="17">Shengqiong Wu, uno studente di dottorato presso NUS, presenta un nuovo approccio per l'estrazione di relazioni multimodale. La ricerca affronta le sfide dell'estrazione di relazioni in contesti reali, dove i dati includono informazioni visive oltre al testo.  Il problema principale è l'uso eccessivo di informazioni interne (text-only) e la sottoutilizzazione di informazioni esterne (come il contesto tematico).

La proposta introduce un framework basato su un grafo informativo (CMG) che combina scene graph testuali e visive.  Un principio di bottleneck informativo guida la selezione e la rimozione di nodi ed edge nel grafo, ottimizzando le rappresentazioni.  Successivamente, vengono aggiunte informazioni tematiche multimodali per arricchire il contesto.

I risultati sperimentali su un dataset MRE dimostrano che l'utilizzo di informazioni visive migliora significativamente le prestazioni rispetto ai metodi basati solo sul testo. L'analisi dell'abbandono rivela che la selezione delle informazioni interne è cruciale per input con alta rilevanza tra testo e visione, mentre l'esplorazione delle informazioni esterne è più utile per input con bassa rilevanza.

In conclusione, il lavoro introduce un approccio innovativo che combina la rimozione e l'aggiunta di informazioni per migliorare l'estrazione di relazioni multimodale.  Il sistema proposto supera i modelli esistenti e offre un'ottimizzazione strutturale tramite scene graph.</sample>
    <sample id="18">"Salt and pepper" è preferibile a "pepper and salt" in termini di lunghezza sillabica.</sample>
    <sample id="19">Zhang Qin, a master's student from Shenzhen University, presented their work "A Survey for Efficient Open Domain Question Answering" accepted at ACL 2023. The presentation focused on the challenges and solutions for efficient open-domain question answering, building upon the two-stage model framework (retrieval and reader) proposed by Danqi Chen.

The core challenges highlighted include the large size of the Wikipedia corpus (26 million documents, 20GB), the bottleneck of index searching (65GB, 65GB), and the computational demands of large language models. The motivation is to achieve systems with smaller memory footprint, faster inference, and comparable performance.

The presentation explored one-stage frameworks like retrieval-only and generator-only systems.  Efficient tactics were discussed across three aspects: fast evidence retrieval (approximate nearest neighbor search), fast reading (skip reading/adaptive computation), and reducing index size (document filtering, embedding compression).  Model size reduction strategies included lightweight models, parameter sharing, and one-stage models.

The analysis of existing models revealed that retrieval-only systems are fast but require large indexes, while generator-only systems are performant but resource-intensive.  The conclusion suggests that resource constraints favor generator-only systems or embedding compression, while real-time feedback benefits from retrieval-only systems.  Future work will focus on deployment on low-power devices and developing more comprehensive evaluation metrics.</sample>
    <sample id="20">Sì, i modelli pre-addestrati da DrBERT sono disponibili gratuitamente su Hugging Face, sotto licenza MIT, e i script di addestramento sono disponibili sul repository GitHub.</sample>
    <sample id="21">DEPLAIN-apa contiene documenti di notizie.</sample>
    <sample id="22">*   Model architecture (transformer models generally generalize better)
*   Model size (larger models generally lead to better generalization)
*   Number of fine-tuning examples (more fine-tuning examples lead to better generalization)</sample>
    <sample id="23">Dan Garrette discute delle sfide che i modelli di text-to-image affrontano nella rappresentazione accurata del testo, in particolare evidenziando i limiti dei modelli basati su T5. I modelli T5, che utilizzano la tokenizzazione subword, mostrano una scarsa capacità di "spelling" delle parole, con accuratezza che scende sotto il 70% anche per i modelli più grandi. Questo perché la tokenizzazione subword richiede al modello di scomporre le parole in unità più piccole, rendendo difficile la rappresentazione delle singole lettere.

In contrasto, i modelli ByT5, che utilizzano la tokenizzazione a livello di byte, eccellono nella spelling, poiché hanno accesso diretto alle singole lettere. L'analisi dei risultati mostra che i modelli T5 hanno difficoltà con le parole più frequenti, poiché queste vengono rappresentate da un numero inferiore di unità subword.

Per superare queste limitazioni, il team ha sviluppato una strategia per migliorare la spelling nei modelli di text-to-image. Hanno aggiunto una rappresentazione testuale derivata da un modello ByT5 più piccolo alla rappresentazione originale di T5. Questo approccio ha migliorato significativamente la capacità del modello di generare testo accurato, anche se la generazione finale può ancora contenere errori dovuti al processo di diffusione. I risultati del lavoro includono i benchmark WikiSpell e DrawText, e una nuova strategia efficiente per migliorare la spelling nei modelli di text-to-image.</sample>
    <sample id="24">Il testo indica che la tendenza dei congiunti a sinistra a essere più brevi è stata misurata in termini di lunghezza in caratteri, sillabe e parole. La tendenza aumenta con la differenza assoluta di lunghezza tra i congiunti, soprattutto quando il governo è sulla sinistra o assente.</sample>
    <sample id="25">Il progetto sperimentale ha coinvolto la misurazione della lunghezza delle dipendenze (in caratteri, sillabe e parole) tra i costituenti coordinati, in base alla posizione del governatore. Sono state analizzate coordinazioni con e senza governatore sulla sinistra, e si è osservato che la tendenza per il primo (e più corto) congiunto di essere posizionato all'inizio aumenta con la differenza di lunghezza tra i due conjunct quando il governatore è sulla sinistra o assente. Questa tendenza svanisce quando il governatore è sulla destra.</sample>
    <sample id="26">Il classificatore base, addestrato su dati non bilanciati, ha prestazioni inferiori al caso.</sample>
    <sample id="27">This presentation is by Shangbin, a PhD student at the University of Washington. The presentation is about their work, "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models." Therefore, there is only one author mentioned in the presentation.</sample>
    <sample id="28">Javad Hosseini, Filip Radlinski, Silvia Pareti, Annie Louis, Bob, Alice.</sample>
    <sample id="29">MuDA tagger identifica che i modelli di MT sensibili al contesto migliorano in fenomeni come formalità e coesione lessicale.</sample>
    <sample id="30">LLM-Blender è un framework di ensemble learning semplice ed efficace per modelli linguistici di grandi dimensioni (LLM). Si basa su un approccio a due fasi: PairRanker e GenFuser. PairRanker confronta pairwise diversi LLM per un dato input, utilizzando cross-attention per determinare quale modello è migliore. Questo permette di ottenere un ranking dei modelli. GenFuser seleziona i primi tre modelli e li utilizza come input a un modello di generazione sequenza-a-sequenza per produrre l'output finale.

A differenza dei metodi esistenti, PairRanker codifica le coppie di modelli insieme all'input, analizzando le sottili differenze tra di loro. I risultati del ranking vengono aggregati tramite max logits o bubble sort. Gli esperimenti dimostrano che PairRanker è più correlato al ranking corretto rispetto ad altri metodi.

Per valutare il framework, è stato creato il dataset MixInstruct, composto da istruzioni esistenti e 11 LLM open-source. Sono state utilizzate metriche automatiche (BERTScore, BLUERT, BARTScore) e ChatGPT come giudici. I risultati mostrano che LLM-Blender supera Open Assistant e Vicuna in molteplici metriche, con un miglioramento significativo in 68% e 76% dei casi.

In sintesi, LLM-Blender è un framework promettente per l'ensemble learning, che migliora le prestazioni sfruttando il confronto pairwise e la generazione di output combinati. Il codice sorgente e il dataset MixInstruct sono disponibili per la ricerca futura.</sample>
    <sample id="31">John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams.</sample>
    <sample id="33">Il framework quantifica la posizionalità confrontando le annotazioni di end-user con le prestazioni di dataset e modelli utilizzando il Pearson's R correlation score.</sample>
    <sample id="34">Marcos Treviso presenta CREST, un nuovo framework per la generazione di spiegazioni e controfattuali testuali. CREST combina la selezione razionale con la generazione di controfattuali, sfruttando i punti di forza di entrambi gli approcci. Il framework prevede una componente di generazione di controfattuali che maschera l'input originale e prepende l'etichetta corretta, poi utilizza un modello linguistico per generare un controfattuale. La qualità dei controfattuali viene valutata tramite valutazioni umane, che li considerano più validi e naturali rispetto a quelli generati da altri metodi.

CREST-Generation estende il framework includendo sia la generazione di spiegazioni razionali che di controfattuali.  Un modello condiviso elabora sia l'input originale che quello controfattuale, con un meccanismo di regolarizzazione per garantire la similarità tra le spiegazioni e quelle originali.  I risultati su dataset IMDB mostrano che CREST-Rationalization supera altri metodi, specialmente su dataset contrastivi e out-of-domain.

CREST-Rationalization produce spiegazioni più plausibili e con una maggiore simulabilità controfattuale, ovvero la capacità di influenzare la decisione del classificatore tramite modifiche guidate dall'esplicazione. In sintesi, CREST offre un modo controllato per generare controfattuali validi e significativi, migliorando le spiegazioni e le prestazioni dei modelli di machine learning.</sample>
    <sample id="36">## Abstract

This paper introduces Language-Specific Layers (LSLs) for improving multilingual machine translation (MMT) while maintaining constant inference costs. MMT offers advantages like scalability, speed, and improved performance for low-resource languages, but also suffers from limited capacity per language and increased training complexity.  LSLs address these limitations by incorporating a regular transformer layer per language, enabling selective activation at inference time.  

The core idea is to learn the optimal placement of these LSLs within the encoder, rather than relying on fixed positions.  The authors propose a training approach where each encoder layer has shared, source, and target weights, and the model learns to activate the appropriate LSL based on the largest weight component. This allows for a flexible architecture that can be adapted to different language pairs.

Experiments on WMT21 news translation across 10 languages demonstrate that LSLs significantly outperform baseline models and language adapters, achieving substantial improvements in translation quality, particularly for low-resource languages.  The learned architecture also maintains constant inference costs, a key advantage of MMT.  Statistical significance tests confirm the robustness of these improvements across a wide range of translation directions.  The paper highlights the potential of LSLs to enhance MMT performance without sacrificing efficiency, paving the way for more effective and scalable multilingual translation systems.</sample>
    <sample id="37">The study found that human subjects also surfaced racial stereotypes when given the same prompts as the language model.</sample>
    <sample id="38">The study used the enhanced version of the Penn Treebank.</sample>
    <sample id="39">Adam Przepiórkowski.</sample>
    <sample id="40">Ecco le attività strettamente correlate alla dissonanza cognitiva menzionate nel testo:

*   **Stanze di accordo (consonance)**
*   **Espansione**
*   **Confronto**</sample>
    <sample id="41">PeaCoK è un nuovo grafo di conoscenza basato sulle persone, sviluppato dal Natural Language Processing Lab di EPFL in collaborazione con Sony. L'obiettivo è migliorare la coerenza e l'engagement nelle narrazioni, considerando le caratteristiche uniche delle persone coinvolte.

PeaCoK contiene circa 3.800 persone, 40.000 attributi distintivi e 100.000 inferenze basate sul senso comune, con collegamenti tra gli attributi che ne aumentano la ricchezza.  Le relazioni tra persone e attributi sono classificate in tre dimensioni: relazioni principali, interattività e distintività.

Il grafo è stato costruito attraverso tre fasi: selezione di persone da grafi esistenti, induzione di attributi da fonti diverse e annotazione delle relazioni tramite un sistema di voto umano-AI. L'AI, InstructGPT-3, ha aiutato a risolvere le controversie tra gli annotatori umani, migliorando l'accuratezza.

PeaCoK viene utilizzato per addestrare modelli linguistici come BART, che generano attributi di persone in base a relazioni specifiche. I risultati dimostrano che questi modelli, addestrati su PeaCoK, superano i modelli pre-addestrati più grandi in termini di generazione di linguaggio naturale.

Inoltre, PeaCoK migliora la generazione di dialoghi, arricchendo i profili dei partecipanti con informazioni pertinenti.  I risultati indicano che il grafo di conoscenza basato sulle persone di PeaCoK è più efficace rispetto ai grafi di conoscenza sociali generali, soprattutto quando le persone hanno attributi in comune.  La coerenza e l'engagement nei dialoghi aumentano con la condivisione di attributi tra i partecipanti. PeaCoK rappresenta un passo avanti per la creazione di narrazioni più realistiche e coinvolgenti.</sample>
    <sample id="42">The provided text does not mention the number of authors involved in the paper.</sample>
    <sample id="43">This information is not provided in the text.</sample>
    <sample id="44">Il framework differisce dai lavori precedenti perché confronta gli utenti finali con i modelli e i dataset stessi, invece di concentrarsi solo sull'accordo degli annotatori o sulla distribuzione degli annotatori.</sample>
    <sample id="45">Il lessico degli stereotipi si sovrappone maggiormente alle generazioni di personas create con il metodo "Marked Words".</sample>
    <sample id="46">DeepL e Google Translate.</sample>
    <sample id="47">Ciao, sono Shangbin, dottorando all'Università di Washington. Oggi presento il nostro lavoro "Dall'insieme di dati di pre-addestramento ai modelli linguistici alle attività a valle: tracciare le tracce dei pregiudizi politici che portano a modelli NLP ingiusti". I modelli linguistici sono addestrati su grandi quantità di dati web. I media di notizie politici sono ben coperti nei dati di pre-addestramento. Secondo un sondaggio sul corpus C4, i media come New York Times, Los Angeles Times, The Guardian, Huffington Post, ecc. sono ampiamente rappresentati nei dati di addestramento dei modelli linguistici. Questo ha creato una sorta di benedizione e maledizione per le applicazioni dei modelli linguistici. Da un lato, hanno imparato da diverse prospettive, celebrando la democrazia e la pluralità delle idee. Dall'altro, queste diverse opinioni politiche sono intrinsecamente socialmente influenzate e potrebbero portare a potenziali problemi di equità nelle applicazioni a valle. A tal fine, proponiamo di indagare sul percorso di propagazione dei pregiudizi politici dai dati di pre-addestramento ai modelli linguistici alle attività a valle, ponendoci le seguenti domande: Innanzitutto, come valutiamo il pregiudizio politico dei modelli linguistici e qual è il ruolo dei dati di pre-addestramento in tali pregiudizi? In secondo luogo, come si comportano i modelli linguistici con diversi pregiudizi politici nelle attività a valle e se ciò potrebbe portare a problemi di equità nelle applicazioni NLP? In particolare, abbiamo proposto di fornire ai modelli linguistici diversi formati di prompt utilizzando questionari politici come il Political Conference Test. Ciò garantisce che la nostra valutazione automatica sia ben fondata nella letteratura scientifica. I risultati preliminari dimostrano che i modelli linguistici hanno effettivamente diversi pregiudizi politici. Occupano tutti e quattro i quadranti del panorama politico. Possiamo anche vedere che GPT-4 è il modello linguistico più liberale di tutti, e la serie GPT è generalmente più socialmente liberale rispetto alla serie BART e alle sue varianti. In secondo luogo, miriamo a indagare fino a che punto i pregiudizi politici dei modelli linguistici sono effettivamente acquisiti dai dati di addestramento. Quindi conduciamo un esperimento controllato ri-addestrando i modelli linguistici su sei diversi corpora partitici separati in notizie e social media, ulteriormente suddivisi in base al loro orientamento politico. Ri-addestrando i modelli linguistici su tali corpora partitici possiamo vedere che le coordinate ideologiche dei modelli linguistici cambiano di conseguenza. Ad esempio, per RoBERTa ri-addestrato su un corpus Reddit di orientamento di sinistra, possiamo vedere un significativo spostamento di pregiudizio politico verso il liberismo. Cerchiamo anche di indagare se i modelli linguistici possono acquisire la polarizzazione prevalente nella nostra società. Dividiamo i corpora di pre-addestramento in prima e dopo la presidenza del 45° presidente degli Stati Uniti e ri-addestriamo separatamente i modelli linguistici su questi due corpora temporali diversi. Possiamo vedere che i modelli linguistici hanno generalmente un orientamento politico che è più lontano dal centro dopo il 2017. Ciò indica che i modelli linguistici possono anche acquisire la polarizzazione nella nostra società. Infine, valutiamo i modelli linguistici con diversi pregiudizi politici su applicazioni NLP che spesso coinvolgono modelli linguistici e che possono avere implicazioni molto significative. Vediamo che se valutiamo le prestazioni per categoria, ovvero se separiamo le prestazioni in base alle diverse demografie o all'orientamento politico dei media di notizie, possiamo vedere un modello. Ad esempio, per la rilevazione di discorsi d'odio, i modelli linguistici di orientamento di sinistra sono migliori nel rilevare i discorsi d'odio rivolti a gruppi minoritari socialmente, ma sono peggiori nel rilevare i discorsi d'odio rivolti a gruppi più potenti nella nostra società. Viceversa, i modelli linguistici di orientamento di destra sono migliori nel rilevare i discorsi d'odio rivolti a bianchi e uomini, ma peggiori nel rilevare i discorsi d'odio rivolti a neri, LGBTQ+ e altri gruppi minoritari. Simili tendenze si verificano anche per la rilevazione di notizie false, dove vediamo che i modelli linguistici di orientamento di sinistra sono migliori nel rilevare le disinformazioni provenienti dal loro opposto orientamento politico e viceversa. Mostriamo anche molti esempi qualitativi per far vedere che i modelli linguistici con diversi pregiudizi politici danno previsioni diverse su esempi di discorsi d'odio e disinformazione in base alle loro categorie sociali. Ci sono molti altri esempi nell'appendice per evidenziare che ciò indica che esiste un problema di equità molto pressante relativo ai pregiudizi politici dei modelli linguistici. Ad esempio, se i modelli linguistici di orientamento di destra venissero ri-addestrati su discorsi d'odio o disinformazione o qualsiasi altra cosa e distribuiti su una popolare piattaforma di social media, ciò significherebbe che le persone con opinioni opposte potrebbero essere marginalizzate e i discorsi d'odio rivolti a gruppi minoritari potrebbero diffondersi senza alcun controllo. Ciò solleva un allarme per noi per riconoscere e affrontare i problemi di equità derivanti dai pregiudizi politici dei modelli linguistici. Quindi, un po' di discussione. Vorremmo anche sottolineare il dilemma unico relativo ai pregiudizi politici dei modelli linguistici. È come tra Scylla e Charybdis. Se non sanifichiamo le opinioni politiche nei dati di pre-addestramento, il pregiudizio si propagherà dai dati di pre-addestramento ai modelli linguistici alle attività a valle, creando infine problemi di equità. Se cerchiamo di sanificare in qualche modo, rischieremo anche la censura o l'esclusione. È incredibilmente difficile determinare cosa sia effettivamente neutrale e che debba essere mantenuto nei dati di monitoraggio linguistico. È un po' come il problema del carrello elettrico. Ok, ottimo. Penso che questo sia tutto quello che ho da dire oggi. Grazie per il vostro tempo.</sample>
    <sample id="48">3</sample>
    <sample id="49">OPT e GPT-2 modelli sono stati valutati fino a 1024 token di lunghezza del contesto.</sample>
    <sample id="50">The presentation introduces DEPLAIN, a new corpus for German text identification at both document and sentence levels, addressing limitations in existing corpora. DEPLAIN is split into DEPLAIN-apa (news texts, manually aligned, ~13,000 sentence pairs) and DEPLAIN-web (various domains, manually and automatically aligned, ~30,450 sentence pairs). Analysis reveals varying simplification types across text types, with Bible texts being significantly more simplified than news or learner texts. DEPLAIN exhibits a high diversity of simplification transformations, with more reorderings/additions in DEPLAIN-apa and more rephrasings in DEPLAIN-web.

The presentation highlights two key use cases. First, DEPLAIN serves as a gold standard for evaluating automatic alignment methods, particularly for aligning sentences of different complexity levels within the same language.  The authors found MASSalign to be the best performing method and provide code for its use. Second, DEPLAIN facilitates fine-tuning language models for automatic text simplification. They fine-tuned long-mBART for document-level simplification and base mBART for sentence-level simplification, achieving results exceeding baseline scores and establishing a new benchmark. The authors conclude that DEPLAIN offers valuable resources for research in German text simplification.</sample>
    <sample id="51">AltEntities Corpus includes music, books, and recipes domains.</sample>
    <sample id="52">La posizionalità è semplicemente le prospettive che le persone hanno come risultato delle loro demografie, identità e esperienze di vita.</sample>
    <sample id="53">Dawei.</sample>
    <sample id="54">Vasudha, a Computer Science PhD candidate at Stony Brook University, presents their work on "Transfer Learning for Dissonance Detection," accepted to ACL 2023. The paper addresses the challenge of detecting cognitive dissonance in language, a rare but important phenomenon related to disagreement, attitude change, mental health, and understanding cognitive styles.

The research involved creating a large-scale annotated dataset of dissonance relations from tweets, finding that dissonance was present in only 3.5% of annotated pairs.  Due to the rarity of dissonance, initial models performed poorly. To overcome this, they explored transfer learning and active learning. They successfully transferred weights from related tasks like stance classification and PDTB expansion/comparison, achieving a zero-shot AUC of 0.62. Fine-tuning on these tasks further improved performance.

The study compared different active learning strategies for data selection, finding that the "Probability-of-Rare-Class" (PRC) strategy, which prioritizes examples the model is most likely to misclassify, performed better than other state-of-the-art methods.  They also found that "Cumulative" model updates were more effective than "Iterative" updates for active annotation in this context.  While PRC improved dissonance classification AUC to 0.75, annotators found the examples challenging.  The research concludes that PRC is a valuable active learning strategy for rare class acquisition, particularly when combined with transfer learning, and that cumulative updates are beneficial for domain-specific active annotation.</sample>
    <sample id="55">Sì, EDAtt adatta un modello ST offline esistente, utilizzando solo un modello per ogni regime di latenza e gestendo la latenza tramite specifici parametri.</sample>
    <sample id="56">L'articolo è presentato da Yusen Zhang di Penn State University.</sample>
    <sample id="57">Sì, il modello testato funziona sulla suite di test KITMUS.</sample>
    <sample id="58">Le tre varianti di KITMUS sono:

1.  Background-Pretrain
2.  Background-Both
3.  Background-Inference</sample>
    <sample id="59">Yanis Labrak presenta il lavoro di DrBERT, un modello pre-addestrato in francese per domini biomedici e clinici. Il lavoro inizia con una panoramica della modellazione linguistica in ambito sanitario, evidenziando la necessità di modelli specializzati in francese, dato il divario con i modelli esistenti in inglese come PubMedBERT e ClinicalBERT.

DrBERT è il primo modello biomedico in francese basato su RoBERTa, addestrato su NACHOS, un dataset di dati medici web-crawled.  Viene confrontato con ChuBERT, un modello basato su dati clinici del Nantes University Hospital.  L'esperimento valuta l'impatto della dimensione del dataset (4GB, 8GB, ecc.) e delle diverse strategie di pre-addestramento, includendo modelli basati su CamemBERT e PubMedBERT.

Sono stati valutati sette modelli su 11 task downstream (NER, classificazione, POS tagging, QA) rispetto a sei baseline. I risultati indicano che i modelli addestrati da zero tendono a performare meglio, ma l'uso di dati eterogenei risulta più versatile.  L'utilizzo di più dati tradotti migliora le performance.  Un controllo pre-addestramento con i pesi di CamemBERT ha ottenuto risultati comparabili a DrBERT 4GB da zero.

DrBERT ha superato le performance di CamemBERT su nove delle 11 task.  I modelli addestrati su NACHOS sono disponibili su Hugging Face e i training scripts su GitHub.</sample>
    <sample id="60">Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="61">Should we only use the clean samples for validation, or are there better ways to utilize them?</sample>
    <sample id="62">Nitay Calderon presenta il loro articolo ACL "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training". Il paper affronta la crescente necessità di comprimere i modelli di linguaggio per ridurre costi e dimensioni, mantenendo al contempo le prestazioni.

La ricerca si concentra sulla knowledge distillation (distillazione della conoscenza) per l'NLG, un approccio che trasferisce la conoscenza da un modello "insegnante" (grande) a un modello "studente" (piccolo).  A differenza di lavori precedenti focalizzati su classificazione o pre-training, questo studio esamina la distillazione specifica per task di NLG, utilizzando dataset realistici e a basso costo di annotazione.

Il lavoro esplora diverse architetture (encoder/decoder vs. decoder-only), l'impatto del pruning (rimozione di parti del modello) e diverse strategie per la generazione di pseudo-target (testo generato dal modello insegnante).  Si evidenzia come l'uso di dati non etichettati sia cruciale, la generazione di più pseudo-target migliori, e l'utilizzo di campionamento casuale (con temperature elevate) aumenti la diversità della conoscenza trasmessa.

Il contributo principale è la proposta di "joint-teaching", una tecnica che combina la distillazione a livello di parola su pseudo-target generati sia dal teacher che dallo student, per mitigare il bias di esposizione e permettere allo student di correggere i propri errori.  L'articolo presenta un approccio sistematico e completo alla distillazione per l'NLG, con quattro task di esempio (summarization, question generation, common sense reasoning, simplification e style transfer).</sample>
    <sample id="63">La sensibilità misura la capacità del modello di produrre consistentemente gli stessi output per la stessa attività, indipendentemente da lievi variazioni nella formulazione dell'istruzione.</sample>
    <sample id="64">Jingwei Yi</sample>
    <sample id="65">```
Una maggiore sensibilità indica una performance del modello peggiore.
```</sample>
    <sample id="66">The paper "Deep Learning for Mathematical Reasoning" explores the field of AI and NLP focused on enabling machines to solve math problems and prove theorems. Mathematical reasoning, encompassing text-based problems, multimodal data (images, tables), and automated theorem proving, is a crucial area. 

The paper discusses various approaches, including sequence-to-sequence models and sequence-to-tree models, which map input problems to output solutions. Large Language Models (LLMs) have shown promise, particularly when guided by chain-of-thought prompting, but still struggle with precision.  Self-consistency and program-aided LLMs are presented as solutions to improve performance.

The research highlights the lack of resources for mathematical reasoning in low-resource languages and the need for benchmarks in specialized domains like finance, science, and medicine.  The paper concludes by noting challenges with generalization and robustness, including difficulties with large numbers and inconsistencies in mathematical reasoning.  The work aims to address these limitations through novel architectures and techniques.</sample>
    <sample id="67">This paper investigates interference in multilingual translation models, exploring how synergy between language pairs can be disrupted. The study identifies that severe interference occurs when models are small relative to the data size, and that adjusting the sampling temperature is crucial for strong performance.

The research focuses on the relative difference in loss between a bilingual and a multilingual model, defining interference as this difference.  They examine the impact of language similarity and the number of languages, finding that language similarity has a minimal effect, especially with sufficient data.  The number of languages also shows a similar trend.

Experiments reveal that interference is largely mitigated by scaling up model and data size.  The paper highlights that a baseline approach of tuning the sampling temperature (T &gt; 1) is effective in reducing interference, with a value of 5 being commonly used.  The study concludes that modest model and data scaling, combined with temperature tuning, can significantly reduce interference without requiring specialized algorithms.  The findings suggest that the focus should be on model size and data quantity rather than language similarity or the number of languages when addressing interference in multilingual translation.</sample>
    <sample id="68">Il contesto linguistico messo a disposizione dei modelli durante il pre-addestramento è quello di dataset come BLiMP e SyntaxGym, che contengono frasi accettabili e non accettabili, e anche dati provenienti da altre fonti come Wikipedia.</sample>
    <sample id="69">In genere, si hanno bisogno di circa 20 campioni di convalida puliti per ottenere buone prestazioni in WSL.</sample>
    <sample id="70">Esin Durmus e Dan Jurafsky.</sample>
    <sample id="71">Javad Hosseini, in collaborazione con Filip Radlinski, Silvia Pareti e Annie Louis, presenta il "AltEntities Corpus", un dataset di 6.000 domande alternative e 42.000 espressioni di riferimento indirette, creato per studiare come gli utenti si esprimono quando devono scegliere tra entità, come canzoni, libri o ricette. Il dataset mira a risolvere il problema delle espressioni di riferimento indirette, che sono comuni nelle conversazioni naturali ma difficili per i modelli linguistici di grandi dimensioni (LLM) da comprendere.

Il dataset è stato creato tramite annotazione umana, utilizzando un approccio informale basato su un setup di completamento di cartoon.  Le domande alternative sono generate automaticamente usando un template "Do you mean A or B?", con A e B prese da Wikipedia.  La scelta delle entità alternative varia in base a diversi criteri di similarità, come titoli, descrizioni e attributi.

Per preparare gli annotatori, vengono forniti background knowledge sulle entità, come link a Google Search o brevi testi da Wikipedia, immagini (per ricette e libri) e informazioni sull'entità. Gli annotatori devono poi descrivere le entità usando tre a cinque espressioni di riferimento indirette.

I risultati mostrano che l'accuratezza dei modelli linguistici varia a seconda della quantità e della qualità del background knowledge fornito. Con accesso a un background knowledge completo, l'accuratezza è alta (92-95%), mentre con un background knowledge parziale o solo con i nomi delle entità, l'accuratezza è significativamente inferiore (60%).  Il dataset dimostra inoltre la generalizzabilità dei modelli a diversi domini. Il dataset è disponibile al link fornito.</sample>
    <sample id="72">Il problema è che i modelli linguistici vengono addestrati su dati web che includono opinioni politiche, e questi bias possono propagarsi dai dati di addestramento ai modelli e influenzare le prestazioni in compiti a valle, portando a problemi di equità. È difficile determinare cosa sia "neutro" e cosa debba essere mantenuto nei dati di addestramento, e qualsiasi tentativo di "sanificare" i dati potrebbe portare a censura o esclusione.</sample>
    <sample id="73">Akshatha</sample>
    <sample id="74">Dense-ATOMIC è un nuovo knowledge graph per il commonsense, costruito su ATOMIC, un knowledge base esistente. ATOMIC ha lacune nella copertura del knowledge graph a causa della sua struttura basata su link B-to-A, che limita i percorsi multi-hop. Dense-ATOMIC risolve questo problema completando le lacune nei link B-to-B, A-to-B e A-to-A, aggiungendo anche percorsi multi-hop.

Il metodo di costruzione di Dense-ATOMIC prevede la normalizzazione degli eventi finali, la formazione di un modello di previsione delle relazioni e la costruzione del knowledge graph.  Un nuovo metodo, Rel-CSKGC, utilizza modelli linguistici pre-addestrati (RoBERTa) per prevedere le relazioni tra eventi, superando le limitazioni dei metodi tradizionali basati su grafici. Rel-CSKGC sfrutta anche l'elaborazione dei cluster di eventi per migliorare l'efficienza computazionale.

I risultati dimostrano che Dense-ATOMIC ha una copertura del knowledge graph significativamente superiore e percorsi multi-hop più lunghi rispetto ad ATOMIC. Inoltre, Dense-ATOMIC migliora le prestazioni di modelli di ragionamento come COMETours.  L'analisi dei percorsi multi-hop suggerisce che Dense-ATOMIC può essere utilizzato per inferire ragionamenti complessi.  Il paper presenta il codice e il sito web per l'accesso al dataset e al codice sorgente.</sample>
    <sample id="75">JointProp is a novel semi-supervised learning framework for Named Entity Recognition (NER) and Relation Extraction (RE) that leverages the interconnectedness between these tasks.  Traditional approaches to NER and RE rely heavily on labeled data, which is expensive and time-consuming to obtain. JointProp addresses this by recognizing that NER and RE are related, for example, through syntactic similarities like "used to" and "use in."

The framework consists of four key components: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization.  Span features are generated from contextualized representations of tokens. A heterogeneous graph is built using k-Nearest Neighbors, connecting unlabeled data based on similarity in both unlabeled and labeled data representations.  Label propagation then spreads labels across the graph, refining pseudo-labels in unlabeled data. Finally, the model is retrained using the converged pseudo-labels and a confidence filter to improve performance.

Experiments on joint and single-task datasets demonstrate that JointProp significantly outperforms existing baselines.  On joint datasets, the framework benefits from the codependency between NER and RE, leading to improved performance.  On single-task datasets, JointProp consistently achieves significant improvements over all baselines for both NER and RE.  The framework aims to fully exploit the relationships between labeled and unlabeled data to achieve more robust and accurate NER and RE models with less labeled data.</sample>
    <sample id="76">L'infrastruttura di propagazione dei bias politici è il percorso da cui i bias politici iniziano a partire dai dati di pre-addestramento fino all'utilizzo dei modelli linguistici per compiti a valle. Questo percorso include:

1.  **Dati di pre-addestramento:** I modelli linguistici vengono addestrati su grandi quantità di dati web, che spesso includono notizie da diverse fonti politiche.
2.  **Modelli linguistici:** I modelli linguistici stessi assorbono i bias presenti nei dati di pre-addestramento.
3.  **Compiti a valle:** I modelli linguistici vengono utilizzati per compiti come la rilevazione di discorsi d'odio e la rilevazione di notizie false, e questi compiti possono perpetuare i bias politici.</sample>
    <sample id="77">This video introduces "DeFacto," a new dataset for improving factual consistency in abstractive text summarization, developed jointly by Yale University and Microsoft Research. The work builds upon the author's internship at Microsoft Research and aims to address the challenge of ensuring summaries accurately reflect the source documents.

DeFacto contains human demonstrations and feedback on system-generated summaries, focusing on identifying and correcting factual errors. Annotators provide labels for factual consistency, human-corrected summaries with instructions, explanations, and supporting evidence from the original text. The dataset is based on the XSum dataset, a common benchmark for summarization, and initial system outputs were generated by the Pegasus model.

The research introduces three new Natural Language Generation (NLG) tasks: summary editing, feedback generation, and automatic factual error correction.  The video highlights findings from the summary editing task, demonstrating that models can effectively utilize human feedback to improve factual accuracy, although at the cost of reduced textual overlap with reference summaries.  The feedback generation task remains challenging, and automatic factual error correction shows promising results with fewer training data points, especially when combined with explanation generation.

DeFacto's fine-grained annotations offer valuable insights for training factuality metrics and meta-evaluation. The dataset is publicly available on GitHub, with further details in the paper. The work provides a testbed for NLG tasks and contributes to advancing factual consistency in summarization models.</sample>
    <sample id="78">Sì, DEPLAIN-apa e DEPLAIN-web differiscono nel tipo di semplificazione. DEPLAIN-apa mostra una semplificazione più forte, in particolare per i testi biblici, mentre DEPLAIN-web mostra una maggiore varietà di trasformazioni, come riordinamenti e aggiunte di parole, rispetto a rielaborazioni.</sample>
    <sample id="79">The paper mentions that CoScript is a dataset generated by the authors. However, it does not explicitly state whether CoScript is publicly available. It refers to the dataset in the paper and suggests it can be a valuable resource for research.</sample>
    <sample id="80">La filigrana viene inserita nel testo calcolando il numero di parole nel set di trigger in una frase. L'embedding fornito dal servizio è quindi una somma pesata dell'embedding di destinazione e dell'embedding originale, con il peso dell'embedding di destinazione proporzionale al numero di trigger nella frase.</sample>
    <sample id="81">Yusen Zhang è affiliato alla Penn State University.</sample>
    <sample id="82">Questo video presenta ULRA (Learning from Rank Aggregation), un nuovo framework per l'Automated Essay Scoring (AES) non supervisionato. L'AES mira a valutare la qualità della scrittura senza intervento umano, un'applicazione importante dell'elaborazione del linguaggio naturale. I metodi AES tradizionali si basano su dati etichettati, che sono costosi da ottenere. ULRA affronta questo problema aggregando più segnali di qualità (heuristic signals) per creare una supervisione più robusta.

Il framework ULRA include un modulo di ranking di segnali (HER alpha-shot) che genera coppie di ordine parziale basate sui valori dei segnali di qualità. Questo modulo trasforma i ranking in coppie di ordine parziale. Un modulo di aggregazione di ranking pairwise profondo (DPRA) utilizza queste coppie per addestrare un modello neurale AES, gestendo le incoerenze tra i segnali di qualità attraverso una funzione di perdita di ranking pairwise profonda.  Un ulteriore modulo di scoring trasforma i punteggi predetti dal modello in un intervallo predefinito.

Gli esperimenti dimostrano che ULRA supera i metodi non supervisionati esistenti e si avvicina alle prestazioni dei metodi supervisionati, che sono inferiori a causa della mancanza di supervisione forte. ULRA sfrutta il conoscenza di ranking contenuta in più segnali di qualità per un'AES non supervisionata efficace.</sample>
    <sample id="83">Il modello codificatore-decodificatore, come mT5, può migliorare con l'addestramento su una combinazione di lingue.</sample>
    <sample id="84">Shwai He's ACL 2023 paper introduces PAD-Net, a framework for efficient dynamic networks. Traditional networks are static, while dynamic networks adapt based on input, offering potential performance gains. However, fully dynamic networks, which make all parameters dynamic, often lead to excessive parameter usage, like the 5x size increase in BERT-Base with Mixture of Experts.

PAD-Net addresses this by partitioning parameters into dynamic and static components, using scale factors to control their intensity. The Iterative Mode Partition method identifies and converts redundant dynamic parameters to static ones, minimizing computational cost and improving performance. Experiments show PAD-Net outperforms static and fully dynamic networks while maintaining fewer parameters.

Ablation studies reveal optimal dynamic ratios for Dynamic Convolution and Mixture of Experts, and the importance of scale factors. PAD-Net surpasses network pruning due to its preservation of static parameters and enhances output discrimination. Future work includes extending PAD-Net to other network architectures, hardware-friendly structures, and incorporating more parameter modes. The core hypothesis is that fully dynamic networks contain redundant parameters that can be replaced with static ones without significant loss in performance.</sample>
    <sample id="85">"Make a chocolate cake" è un esempio di pianificazione linguistica vincolata.</sample>
    <sample id="86">Gli autori hanno visualizzato l'embedding delle frasi su quattro dataset utilizzando PCA per dimostrare che è difficile distinguere gli embedding con backdoor da quelli normali.</sample>
    <sample id="87">DrBERT, il nuovo modello pre-addestrato in francese, è stato costruito basandosi su RoBERTa. È stato pre-addestrato su NACHOS, un dataset di dati medici web crawled, e poi è stato confrontato con ChuBERT, un modello basato su dati clinici. Sono stati anche introdotti tre modelli pre-addestrati con continui pre-addestramento, basati su CamemBERT e PubMedBERT.</sample>
    <sample id="88">GPT-4 è meno allineato con le persone non binarie rispetto a uomini e donne.</sample>
    <sample id="89">"If we receive a speech chunk containing "I'm going to talk about..." and our model predicts the translation in German, and we will look at the cross-attention weights, we'll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames."</sample>
    <sample id="90">This paper investigates the feasibility of using language learners as data annotators in Natural Language Processing (NLP), challenging the traditional reliance on native speakers.  The authors argue that, given the scarcity of native speakers for many languages, leveraging language learners offers a viable alternative. A proof-of-concept study was conducted across English, Korean, and Indonesian, utilizing the GLUE benchmark and four task types (sentiment analysis, NLI, NER, and MRC).  Learners were categorized into basic, intermediate, and advanced levels based on CFR criteria, and compared to native speakers.

Experiments involved a three-step process: pre-test (proficiency assessment), annotation (labeling tasks with varying difficulty), and post-test (proficiency assessment).  The study found that language learners produced nearly accurate labels, particularly for simpler tasks and questions.  Aggregated labels from multiple learners achieved performance comparable to native speakers through majority voting.  Furthermore, training language models on learner-annotated data yielded results close to ground truth, and in some cases, surpassed those trained on native speaker data.  The study also demonstrated that annotation tasks led to improvements in learners' language proficiency and vocabulary.  The findings suggest that language learners can significantly contribute to NLP data annotation, offering a novel approach to building benchmark datasets for low-resource languages and potentially broadening NLP research globally.  The paper highlights the importance of carefully controlling variables during the annotation process.</sample>
    <sample id="91">The model achieves better performance and lower sensitivity as the amount of tasks increases.</sample>
    <sample id="92">Gli autori confrontano il loro metodo con i seguenti approcci di riferimento:

1.  Modelli seq2seq tradizionali.
2.  Modelli basati su alberi.
3.  Altri modelli treeless.</sample>
    <sample id="93">Alexander Koller e Ivan Titov sono i supervisori del primo autore.</sample>
    <sample id="94">Jingwei Yi from the University of Science and Technology of China presents a paper on protecting copyright for embedding as services, a growing area built upon large language models like GPT and LLAMA.  The concern is that attackers could steal models by learning from embeddings and replicating services.

The paper proposes "Embedding Marker," a backdoor-based watermark method. This involves two steps: watermark injection and copyright verification.  Watermark injection uses a trigger set of words (common in general text) to modify the embedding.  The weight of the trigger embedding is proportional to the number of triggers in the input sentence.  If the trigger count exceeds a threshold, the embedding becomes identical to the target embedding.

Copyright verification detects if a model behind another service contains the watermark.  A backdoor dataset (containing only trigger words) and a benign dataset (without trigger words) are used.  The provider service requests embeddings from the suspected service and compares the requested embedding to the target embedding using cosine and L2 similarity, along with a KS test.

Experiments on AG News, MIND, SST2, and Enron Spam datasets demonstrate strong detection performance with minimal impact on embedding utility. Visualizations using PCA show that the backdoor embeddings are difficult to distinguish from normal embeddings. The paper aims to provide a practical and effective solution for protecting the copyright of embedding as services.</sample>
    <sample id="95">The provided text does not mention the first author of PaLM. It only states that David Vilar is giving a review of a paper on prompting PaLM for translation, co-authored with colleagues from Google Translate.</sample>
    <sample id="96">Hi everyone. I'm Jenny, a first-year PhD student at Carnegie Mellon University, and today I'll be presenting our work, NLPositionality: characterizing design biases of datasets and models. This work was done in collaboration with some folks at the University of Washington and the Allen Institute for AI, namely Sebastian Santy, Ronan Le Bras, Katharina Reinecke, and Maarten Sap. So let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content. You might turn towards a popular API like Perspective API for toxicity detection, and this works really well if you're Carl Jones. Where Perspective API is able to detect correctly toxic instances. But that's not really the case for Aditya Sharma. Where Perspective API is really not as sensitive to offensive terms that are more common in Indian contexts. This is an example of a design bias where we see systematic performance differences of technology between populations. Design biases like the one that we just saw before might occur due to the positionality of the NLP researchers and model developers. Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences. This is a concept widely used in critical studies, specifically in feminist and queer academic spaces. And as a researcher, positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make. And so one question that people might ask is, do datasets and models have positionality? And we're not trying to say that models themselves, or datasets themselves, have demographic identities and life experiences, but they do aggregate judgments and opinions of real people, and can thus represent certain positionalities over others. So prior work has suggested some anecdotal evidence of having positionality, such as cultural gaps in models and datasets, as well as theoretical definitions of model positionality. However, these works really don't look at comparing end users with the datasets and models themselves, and studying model and dataset positionality is increasingly important as NLP tasks become more subjective and socially oriented, and it's challenging to characterize how these positionalities are skewed because not all decisions are documented and many models are hidden behind APIs. So to study dataset and model positionality, we actually compare the annotations with real users with existing datasets and models. We do this through our framework NLPositionality. Our framework works in two main steps. The first step is to re-annotate datasets with diverse annotators. And we ought to do this overlooking the demographics of original dataset annotators, because usually only a few annotators annotate each instance and because demographics are rarely collected and shared. And so we opt to re-annotate data to get many annotates for instance and to get a rich set of demographic data. We then take the annotations by demographic and compare them to the models and datasets using a Pearson's R correlation score, and thus our framework actually differs from annotator disagreement literature by comparing end users with models and datasets, predictions and labels, as opposed to looking at just annotator agreement or modeling annotator distributions. Our framework is largely enabled through Lab in the Wild and online crowdsourcing platform for where HCI collaborator. Live in the Wild is an online experimentation platform where we can recruit diverse volunteers. Compared to the platforms like Mturk which largely have participants from the US or India, and further Lab in the Wild still is able to get high-quality data. We host 2 tasks on Lab in the Wild, one of them being social acceptability, and the way this works is that participants will read a situation from the Social Chemistry dataset and then they'll write how socially acceptable a situation is. Afterwards, they can compare their responses to an AI and others. We've then compared these annotations with Social Chemistry, Delphi, and GPT-4. We then replicate a very similar setup for the toxicity and hate speech detection task, where they'll read an instance from Dynahate and write whether they think it's an instance of hate speech. We then compared these annotations with Dynahate, Perspective API, Rewire API, Hate Roberta, and GPT-4. We then amassed over 16,000 annotations from over 1000 annotators from 87 countries. So now we're better equipped to answer who do NLP datasets and models align with the most. We find that there is positionality in NLP. For example, we find that datasets and models are most aligned to English-speaking countries. For the GPT-4 social acceptability analysis, we find that it's most aligned to Confucian and English-speaking countries. We find that Dynahate is also most aligned to English-speaking countries. We also find most additional alignment with people who have a college education. So for GPT-4, in the social acceptability task, we find that it's most aligned to people with a college or Graduate School education, and we find the same for Dynahate where it's most aligned to people with a college education. However, when models and datasets are aligned to specific populations, some are inevitably left behind. An example of this is that datasets and models are less aligned to non-binary people compared to men and women counterparts. We find this in the GPT-4 social acceptability task as well as the Dynahate task analysis as well. So, given that there is positionality in NLP, what can we do about it? So we have a few recommendations for this. First one is keep a record of all relevant design choices throughout the research process. And the other is to do NLP research with the lens of perspectivism. Our third recommendation is to build specialized datasets and models within 4 specific communities. A good example of this is the Masakhani initiative. I mean, we want to emphasise that inclusive NLP isn't just making, you know, all technologies work for everyone. So that concludes our presentation. But if you'd like to learn more, feel free to check out our dashboard for the most updated analysis results and our paper. Thank you.</sample>
    <sample id="97">The presenter mentions the following problems with current SimulST models:

*   Long and complicated training procedures.
*   Training and maintaining several models for different latency regimes.</sample>
    <sample id="98">```
Il documento evidenzia la difficoltà di mitigare i bias sociali e politici nei set di dati per l'addestramento dei modelli di NLP. Non esiste una soluzione semplice, poiché la rimozione dei bias potrebbe portare a censura o esclusione, e definire la neutralità è complesso. L'approccio proposto è quello di affrontare il problema attraverso un'attenta analisi del bias nella fase di pre-addestramento e di valutare l'impatto dei bias nei modelli durante le applicazioni downstream.
```</sample>
    <sample id="99">Sono Siyu Yuan da Fudan University. Sono qui per presentare il nostro lavoro "Distilling Script Knowledge from Large Language Models for Constrained Language Planning". Nella vita quotidiana, gli umani pianificano le proprie azioni seguendo istruzioni passo dopo passo nella forma di script orientati agli obiettivi. Lavori precedenti hanno sfruttato i modelli linguistici per pianificare per obiettivi astratti di attività stereotipate come "preparare una torta". Hanno dimostrato che i grandi modelli linguistici possono decomporre efficacemente gli obiettivi in passaggi. Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione per obiettivi astratti di attività stereotipate. La pianificazione per obiettivi con vincoli specifici, come "preparare una torta al cioccolato", rimane poco studiata. In questo articolo, definiamo il problema della pianificazione linguistica vincolata, che impone diversi vincoli agli obiettivi di pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici reali con molteplici vincoli. Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli. In questo articolo, valutiamo e miglioriamo la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici. Poiché non esiste un dataset di obiettivi specifici per supportare il nostro studio, dobbiamo acquisire questi obiettivi. Come mostrato nel grafico, estendiamo gli obiettivi astratti con vincoli multi-faccettati tramite dati acquisiti dall'uomo tramite InstructGPT. Campioniamo 100 obiettivi specifici e valutiamo gli script generati dai grandi modelli linguistici. Il grafico riporta la precisione complessiva dei risultati. Scopriamo che tutti i modelli linguistici raggiungono risultati insoddisfacenti nella pianificazione per obiettivi specifici. Conduciamo un'analisi dettagliata per indagare sul perché i modelli imparano a fallire. I risultati mostrati nel grafico indicano che la completezza semantica degli script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita. Esploriamo ulteriormente le categorie di vincoli più specifiche definite in wikiHow. Il heatmap nel grafico mostra che le prestazioni di pianificazione di InstructGPT variano notevolmente per gli obiettivi di diverse categorie. Studi precedenti hanno dimostrato che la qualità dell'output dei modelli linguistici presenta una grande variabilità, portando a prestazioni scadenti. Pertanto, adottiamo l'idea di "over-generate-then-filter" per migliorare la qualità della generazione. Innanzitutto, mostriamo i tipi di vincoli con esempi per InstructGPT e otteniamo obiettivi specifici in base agli obiettivi astratti di base. Quindi, InstructGPT genera K script per obiettivi specifici. Successivamente, viene sviluppato un modello di filtro per selezionare gli script fedeli. Convertiamo gli script e gli obiettivi in embedding di InstructGPT e calcoliamo i punteggi di similarità coseno come punteggi di similarità semantica. Inoltre, premiamo lo script che contiene le parole chiave dei vincoli di destinazione. Conserviamo solo lo script che ha il punteggio più alto per l'obiettivo di destinazione nel set di obiettivi. Con il nostro metodo, InstructGPT può generare script di qualità superiore. Il nostro metodo migliora notevolmente la capacità di pianificazione sia in termini di completezza semantica che di fedeltà ai vincoli. Poiché i grandi modelli linguistici sono costosi da distribuire, è essenziale consentire la capacità di pianificazione linguistica di modelli più piccoli e specializzati. La creazione del dataset è un passo essenziale per questo fine. Tuttavia, studi precedenti non consentono la pianificazione per obiettivi specifici e l'annotazione manuale dei dati è costosa. Pertanto, seguiamo l'idea della distillazione del conoscenza simbolica per distillare i dataset di pianificazione linguistica vincolata dai grandi modelli linguistici. Applichiamo il nostro metodo per costruire un dataset di pianificazione linguistica vincolata, denominato CoScript. In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità del set di validazione e test, chiediamo a lavoratori crowdsourcing di trovare e rivedere i campioni errati. Il grafico mostra la distribuzione dei vincoli in CoScript. Scopriamo che CoScript mostra un'alta pluralità degli obiettivi specifici generati. Con CoScript possiamo provare modelli più piccoli ma specializzati per la pianificazione linguistica vincolata. Scopriamo che T5 fine-tunato su CoScript genera script di qualità superiore rispetto alla maggior parte dei grandi modelli, indicando che i modelli più piccoli possono superare i modelli più grandi quando adeguatamente addestrati su dataset appropriati. In sintesi, abbiamo stabilito il problema della pianificazione linguistica vincolata. Abbiamo valutato la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici e sviluppato un metodo "over-generate-then-filter" per i grandi modelli linguistici. Abbiamo utilizzato i grandi modelli linguistici per generare un dataset di script di alta qualità, CoScript, per la pianificazione linguistica vincolata. Speriamo che il dataset CoScript possa essere una risorsa preziosa per far progredire la ricerca sulla pianificazione linguistica. Grazie per il vostro tempo. Si trovano maggiori dettagli su CoScript nel nostro articolo.</sample>
    <sample id="100">PromptRank è un approccio data-efficient per il retrieval di chain in Multi-Hop QA, che mira a rispondere a domande che richiedono ragionamenti multipli basati su documenti. A differenza dei metodi tradizionali che richiedono grandi quantità di dati di training, PromptRank utilizza solo 128 esempi.

Il sistema combina un retrieval unsupervised (TF-IDF e hyperlink traversal) con un reranker basato su modelli linguistici a pochi colpi.  La chiave è la costruzione di un prompt che inserisce i documenti della chain, un token indicatore e un'istruzione (es. "Leggi i documenti precedenti e rispondi alla domanda"). L'istruzione guida il modello linguistico a ragionare sui documenti.

PromptRank utilizza la probabilità della domanda data la chain come funzione di scoring.  Vengono esplorate tecniche come la ricerca di istruzioni ottimali, il sampling di istruzioni e la temperature scaling per migliorare la qualità del ranking.

I risultati dimostrano che PromptRank supera i sistemi fully supervised e si avvicina alle performance dei dense retrievers state-of-the-art.  L'ablation study conferma l'importanza di ogni componente.  Utilizzando PromptRank come retriever, un reader model (ELECTRA-Large) ottiene performance elevate in downstream QA, con una leggera perdita rispetto a MDR. In sintesi, PromptRank dimostra l'efficacia dei modelli linguistici per il ranking di chain in Multi-Hop QA, con un'elevata performance e un basso costo di training.</sample>
    <sample id="101">La fluidità di PaLM è comparabile ai sistemi all'avanguardia.</sample>
    <sample id="102">The watermark method needs to be:

1.  Applicable to embedding as services.
2.  Not degrade the utility of the provided embeddings.
3.  Covert enough to the attacker or easily removable.
4.  Transferable to the attacker's services during the model extraction process.</sample>
    <sample id="103">14 diverse lingue.</sample>
    <sample id="104">The study amassed over 16,000 annotations from over 1000 annotators.</sample>
    <sample id="105">Cosine similarity difference e L2 similarity difference.</sample>
    <sample id="106">QUEST è un nuovo dataset per la ricerca di informazioni che presenta sfide significative per i sistemi di retrieval. Il dataset contiene oltre 3.000 query che richiedono la ricerca di entità, spesso con vincoli impliciti definiti da operazioni sui set (intersezione, complementazione, differenza).  Esempi come la ricerca di una specie di rettile sconosciuta o di un libro di genere specifico illustrano questa necessità di gestire vincoli complessi.

QUEST è stato creato utilizzando categorie da Wikipedia (film, libri, piante, animali) e attraverso un processo di annotazione umana che include la creazione di query, la verifica della loro naturalezza e la valutazione della rilevanza delle risposte.  Gli annotatori identificano anche le porzioni di testo nel documento che supportano le diverse condizioni della query.

L'obiettivo è valutare come i sistemi di retrieval gestiscono queste query complesse, in cui la rilevanza di un documento può essere supportata da diverse parti del testo.  Il dataset include anche la valutazione delle prestazioni di diversi approcci, come retriever sparsi e densi, e un reranker basato su T5.

I risultati mostrano che la performance dei retriever è limitata, soprattutto per le query che coinvolgono operazioni sui set.  L'analisi approfondita evidenzia che le query con intersezioni e differenze di set sono particolarmente difficili. QUEST mira a stimolare la ricerca di sistemi di retrieval più avanzati in grado di gestire le esigenze informative complesse degli utenti.</sample>
    <sample id="107">I modelli basati su codificatori multilingue, come mBART e mT5, hanno ottenuto le migliori prestazioni su tutti e nove i dataset. L'addestramento in un mix di varie lingue ha migliorato le prestazioni sia per i modelli basati su codificatori che su codificatori-decodificatori, con alcune eccezioni in cui l'inglese ha subito un calo di prestazioni.</sample>
    <sample id="108">Koustav Sinha e il suo team hanno presentato un lavoro su come i modelli linguistici valutano l'accettabilità delle frasi, evidenziando che questa valutazione non è sempre robusta al contesto. Hanno rivisto il paradigma dei minimal pair, che valuta i modelli confrontando frasi accettabili e inaccettabili. Il problema è che il paradigma attuale non gestisce bene le frasi lunghe, cruciali per i modelli con finestre di contesto sempre più ampie.

Per affrontare questo problema, hanno sviluppato un nuovo approccio: simulare sequenze lunghe ricostruendo frasi accettabili e inaccettabili da dataset esistenti, aggiungendo prefissi a queste frasi. Hanno testato la robustezza dei giudizi di accettabilità su frasi provenienti da dataset diversi (inclusi dataset irrilevanti come Wikipedia) e su frasi dello stesso dataset.

I risultati mostrano che i giudizi di accettabilità sono relativamente stabili quando si usano frasi provenienti da dataset completamente diversi. Tuttavia, quando si usano frasi dello stesso dataset, i giudizi di accettabilità variano significativamente in base al prefisso utilizzato (accettabile o inaccettabile), e questo effetto aumenta con la lunghezza della sequenza.

L'analisi rivela che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche condivise tra le frasi, e che il paradigma dei minimal pair attuale potrebbe non catturare appieno questa conoscenza. In sintesi, il lavoro suggerisce che i modelli linguistici mostrano una sensibilità a caratteristiche strutturali e semantiche comuni, e che il paradigma dei minimal pair necessita di essere rivisto per valutare accuratamente le loro capacità in contesti più ampi.</sample>
    <sample id="109">The presentation introduces "Unnatural Instructions," a dataset of natural language instructions and their corresponding inputs and outputs, created entirely through automated generation by a pre-trained language model (GPT-3). This approach bypasses the need for expensive and time-consuming human annotation, a common bottleneck in instruction tuning for language models.

The method involves prompting the model to generate instructions, inputs, and outputs, and then further diversifying the dataset by generating paraphrases of existing instructions.  The resulting dataset contains 64,000 examples, with approximately 240,000 including paraphrases.

Analysis reveals that over 50% of the generated examples are correct, and even incorrect examples offer valuable insights.  Unnatural Instructions boasts highly creative and diverse tasks, extending beyond traditional NLP benchmarks.  

The utility of the dataset is demonstrated by fine-tuning an 11 billion-parameter T5 model on Unnatural Instructions, which outperforms both T0++ and Tk-instruct on several benchmarks.  Furthermore, training on Unnatural Instructions proves more cost-effective than training on a baseline dataset of Super-Natural Instructions.

In conclusion, Unnatural Instructions showcases the potential of language models to generate diverse and creative data for instruction tuning, offering a faster and cheaper alternative to human annotation while overcoming the limitations of crowd-sourced data.</sample>
    <sample id="111">Gli autori selezionano un insieme di parole a frequenza moderata (trigger set) analizzando un corpus di testo generale per contare la frequenza delle parole.</sample>
    <sample id="112">Buongiorno a tutti, mi chiamo Shuheng. Oggi presenterò il nostro articolo "Do CoNLL-2003 named entity taggers still work well in 2023?". Iniziamo. Il nostro articolo ha indagato il problema della generalizzazione nell'ambito del Task di Named Entity Recognition (NER). Osserviamo che i modelli basati su CoNLL-2003 sono stati utilizzati per sviluppare NER per quasi 20 anni e questo solleva diverse problematiche. Innanzitutto, questi modelli riescono a generalizzare a dati moderni? E quando sviluppiamo nuovi tagger, cosa serve per una buona generalizzazione? Inoltre, se osserviamo una scarsa generalizzazione, quale causa la diminuzione delle prestazioni di questi modelli? Per indagare questi problemi, abbiamo sviluppato il dataset CoNLL++. Questo è un dataset che abbiamo raccolto da Reuters News dal 2020 e poi lo abbiamo annotato utilizzando le stesse linee guida di annotazione CoNLL-2003. Abbiamo quindi fine-tunato oltre 20 modelli su CoNLL-2003. Abbiamo valutato questi modelli sia sui test set CoNLL-03 che su CoNLL++. Infine, abbiamo calcolato la percentuale di variazione dell'F1 per valutare la generalizzazione di ciascun modello. Quindi, cosa serve per una buona generalizzazione? Durante gli esperimenti abbiamo scoperto che ci sono tre ingredienti principali necessari. Il primo è l'architettura del modello. Attraverso i nostri esperimenti abbiamo scoperto che i modelli transformer generalizzano normalmente meglio a nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che in genere modelli più grandi portano a una migliore generalizzazione. E, ultimo ma non meno importante, sappiamo che il numero di esempi di fine-tuning influisce direttamente sulle prestazioni di un compito a valle. Qui abbiamo anche scoperto che più esempi di fine-tuning portano effettivamente a una migliore generalizzazione. Per la nostra prossima domanda, quale causa la diminuzione delle prestazioni di alcuni modelli? Abbiamo due ipotesi. La prima è l'overfitting adattivo, che è l'overfitting causato dalla riproduzione dello stesso test set ripetutamente e questo si manifesta in genere come un calo dei rendimenti su un nuovo test set. La seconda ipotesi è il drift temporale, che è la degradazione delle prestazioni causata dalla crescente distanza temporale tra i dati di addestramento e i dati di test. Per l'overfitting dei dati, abbiamo osservato che dal grafico sulla destra, la linea di adattamento rosso ha una pendenza maggiore di uno. Ciò significa che ogni unità di miglioramento ottenuto su CoNLL-2003 si traduce in più di un'unità di miglioramento su CoNLL++ e questo indica che non si osserva il calo dei rendimenti. Ciò dimostra che in questo caso l'overfitting adattivo non è osservato. E per il drift temporale, abbiamo condotto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti e abbiamo scoperto che le prestazioni peggiorano con una maggiore distanza temporale e questo conferma la nostra ipotesi che la principale causa della diminuzione delle prestazioni è il drift temporale. La nostra conclusione è che, per una buona generalizzazione, avremmo bisogno di un'architettura del modello migliore, di una dimensione del modello maggiore e di più esempi di fine-tuning. E questi vanno di pari passo; non possiamo avere solo un ingrediente e ignorare gli altri. Inoltre, abbiamo anche scoperto che la diminuzione delle prestazioni è causata dal drift temporale e, sorprendentemente, non è causata dall'overfitting adattivo, anche se CoNLL-2003 è stato utilizzato per oltre 20 anni. Tornando alla domanda che abbiamo sollevato nel titolo del nostro articolo "Do CoNLL-2003 taggers still work well in 2023?", e abbiamo scoperto che la risposta è un sonoro sì. Speriamo che il nostro articolo stimoli ulteriori ricerche su come migliorare la generalizzazione dei modelli. E, infine, vi invitiamo a consultare il nostro articolo, il nostro dataset e, se avete domande, non esitate a contattarmi. Grazie mille.</sample>
    <sample id="114">Il lavoro presentato a ACL 2023, "Finding the Pillars of Strength for Multi-Head Attention", affronta il problema della pesantezza dei modelli linguistici di grandi dimensioni (LLM), che richiedono risorse computazionali elevate per l'addestramento e l'inferenza.  L'articolo propone un approccio innovativo chiamato "grouped head attention" per ottimizzare l'attenzione multi-testa.

Il metodo si basa su una strategia di "divide et impera" che divide le teste di attenzione in gruppi, promuovendo la similarità all'interno dei gruppi e la diversità tra di essi.  Questo viene realizzato attraverso un addestramento a gruppo vincolato e un algoritmo di "Voting-to-Stay" che seleziona le teste più performanti per ogni gruppo, permettendo la compressione dei parametri.

I risultati sperimentali su machine translation, language modeling e abstractive summarization dimostrano che il metodo GHT e GHT-PS (la versione compressa) raggiungono prestazioni competitive con i modelli all'avanguardia, con una significativa riduzione del numero di parametri (fino al 90% e 32.1% rispettivamente).  Inoltre, l'inferenza è notevolmente più veloce e i requisiti di memoria sono ridotti.

L'articolo conclude sottolineando il potenziale di un approccio di pruning automatico basato sulla ipotesi del "Lottery Ticket", che suggerisce l'esistenza di subnetworks efficienti all'interno dei modelli.  Questo approccio mira a ridurre la complessità dei LLM, rendendoli più adatti all'utilizzo in scenari specifici e con risorse limitate.</sample>
    <sample id="115">L'approccio utilizza un segmento parlato di lambda speech frames.</sample>
    <sample id="116">Servin è un giudice.</sample>
    <sample id="117">La qualità degli esempi è più importante della somiglianza con la frase sorgente.</sample>
    <sample id="118">This presentation introduces "Improving Pretraining Techniques for Code-Switched NLP," a paper submitted to ACL 2023. The work addresses the challenge of code-switching, where sentences contain multiple languages (e.g., English and Hindi), a common phenomenon in linguistically diverse communities.  Multilingual pre-trained models like mBERT and XLM-R struggle with code-switched tasks.

The authors propose SwitchMLM, a novel Masked Language Modeling (MLM) technique specifically designed for code-switching.  Switch-points are defined as tokens that mark language transitions (e.g., English to Hindi).  SwitchMLM masks only these switch-points, unlike standard MLM which masks all tokens uniformly.  To address the lack of LID tag availability, they introduce FrequencyMLM, a surrogate method based on negative log likelihood of words in monolingual corpora.

Architectural modifications include residual connections from intermediate layers to the final layer, leveraging the fact that certain layers encode more switch-point information.  An auxiliary loss is also introduced to encourage the intermediate layer to encode language information.

Experiments demonstrate that the combined SwitchMLM/FrequencyMLM with ResBERT and the auxiliary loss achieves the best performance on sentiment analysis across various language pairs.  Probing experiments using linear and conditional probing classifiers confirm that the proposed methods increase switch-point information in intermediate and final layers of the model.  The authors conclude by highlighting the importance of incorporating switch-point information for effective code-switched NLP models.</sample>
    <sample id="119">GPT-4, GPT series, BART series, RoBERTa.</sample>
    <sample id="120">Il modello utilizza i punteggi di attenzione tra l'audio di input e l'output testuale, ovvero il meccanismo di cross-attenzione.</sample>
    <sample id="121">```
"Easy on Me" o "I Gotta Feeling"
```</sample>
    <sample id="122">Siyu Yuan è affiliata all'Università di Fudan.</sample>
    <sample id="123">Ying and Zhiyang present their research on MultiInstruct, a novel multi-modal instruction tuning dataset designed to improve zero-shot learning in computer vision and multi-modal tasks. They address the lack of publicly available instruction datasets for multi-modal models, noting the disparity compared to the abundance of language-only instruction datasets.

MultiInstruct comprises 62 diverse multi-modal tasks, derived from 21 existing datasets, each equipped with five expert-written instructions. They utilize OFA, a unified multi-modal pre-trained model, as their base model and formulate all tasks in a unified sequence-to-sequence format, representing input and output data in the same token space.

Their training involves mixing instances with randomly sampled instruction templates, and they evaluate the model using five experiments per task, reporting accuracy (for classification) or Rouge-L (for generation). They introduce a new metric, sensitivity, to measure the model's consistency across different instruction variations.

The results demonstrate that instruction tuning significantly enhances OFA's performance on seen multi-modal tasks, and transfer learning from natural instruction datasets further improves performance and reduces sensitivity.  Increasing the number of instructions leads to better overall performance and lower sensitivity.  The research also highlights the benefits of transfer learning from natural instruction datasets.  Finally, they plan to release a larger dataset with approximately 150 additional vision-language tasks.</sample>
    <sample id="124">Tan Qingyu del National University of Singapore e Alibaba presenta il loro lavoro "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models". L'obiettivo è analizzare e migliorare la capacità dei modelli linguistici di ragionamento temporale, un aspetto fondamentale per la comprensione del mondo reale.

Il lavoro suddivide il ragionamento temporale in tre livelli: tempo-a-tempo (inferenza di date), tempo-a-evento (relazione tra eventi e tempo) ed evento-a-evento (relazione tra eventi nel tempo).  Si evidenzia come le ricerche precedenti si concentrino principalmente sul secondo livello, trascurando il terzo.

Viene proposto il dataset TempReason, che copre tutti e tre i livelli di ragionamento e un ampio arco temporale, costruito su Wikidata e Wikipedia.  Vengono valutati modelli come T5, FLAN-T5 e ChatGPT in diverse modalità: Closed Book QA (solo domanda), Open Book QA (con contesto da Wikipedia) e Reasoning QA (con conoscenza temporale esplicita).

I risultati mostrano che ChatGPT presenta debolezze nel ragionamento temporale, in particolare per le domande di previsione di mesi.  Il modello TempT5, sviluppato con pre-training e reinforcement learning specifici per il ragionamento temporale, supera significativamente i modelli precedenti, migliorando le prestazioni in Open Book QA e Reasoning QA.  Si notano anche alcune fluttuazioni nelle prestazioni di TempT5 a seconda del periodo temporale, suggerendo la necessità di affrontare i bias nei dati di training.

In conclusione, il lavoro identifica i bias nel ragionamento temporale dei modelli linguistici e propone un dataset e una strategia di training per migliorare tale capacità.</sample>
    <sample id="125">The provided text does not mention the number of authors involved in the article.</sample>
    <sample id="126">XSemPLR proposes a uniform dataset for cross-lingual semantic parsing in multiple natural languages and meaning representations. It considers six settings for training and evaluation, including Translate-Test, where the source language is translated to the target language using Google Translate API before semantic parsing. The paper also tests monolingual models, multilingual models, and cross-lingual transfer settings. The results show that the Translate-Test setting is a standard approach, but it has limitations, such as the "Curse of Multilinguality" and significant cross-lingual performance gaps in zero-shot transfer.</sample>
    <sample id="127">Here's a summary of the presentation in English, around 200 words:

Namgyu Ho from KAIST AI introduces their research, "Large Language Models Are Reasoning Teachers," a project exploring how to transfer reasoning abilities from large language models (LLMs) to smaller, more deployable models. The core problem is that chain-of-thought reasoning, a technique enabling complex task solving, is computationally expensive and memory-intensive, limiting its use to massive models like GPT-3.

The proposed solution is to use these large models as "reasoning teachers" to train smaller models.  They introduce "Diverse Reasoning," a novel technique that generates multiple reasoning solutions from the teacher model using stochastic sampling, leading to better performance.  The research demonstrates that fine-tuning smaller models with these teacher-generated solutions significantly improves their ability to solve complex tasks, even outperforming prompt-based baselines on various benchmarks.

The study shows that the method is highly scalable, with performance improving as dataset size, teacher model quality, and student model size increase. However, it also highlights trade-offs between development costs (teacher model complexity) and inference costs (student model size). The researchers provide open-source code and data, including access to OpenAI inference, encouraging further exploration of this approach for transferring emergent abilities to smaller models. The paper details the reasoning emergence in smaller models and results on open-source models.</sample>
    <sample id="128">Akshatha e Martin presentano il loro lavoro "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources", una collaborazione tra McGill University, Mila e Microsoft Research. Il loro studio affronta la sfida di valutare la capacità dei modelli di linguaggio di integrare conoscenza pre-addestrata e conoscenza disponibile durante l'inferenza.

Il problema nasce dal fatto che i modelli, pur imparando durante il pre-addestramento, potrebbero non avere accesso a informazioni specifiche sull'istanza (es. chi è "John" in "John saw the newly elected president") o a conoscenze di background rilevanti per il compito.

KITMUS è un test diagnostico basato su un task di risoluzione di coreferenze, che richiede l'identificazione del referente di un pronome, basandosi su conoscenza di entità specifiche e conoscenza di background. Il test presenta tre configurazioni: "Background-Pretrain" (conoscenza di background pre-addestrata), "Background-Both" (conoscenza di background pre-addestrata e durante l'inferenza) e "Background-Inference" (conoscenza di background disponibile solo durante l'inferenza).

I risultati mostrano che, sebbene i modelli generalisti non performino bene su KITMUS senza addestramento specifico, l'addestramento su KITMUS migliora significativamente le prestazioni. Tuttavia, anche i modelli più performanti faticano a integrare conoscenza di background disponibile solo durante l'inferenza.

In sintesi, il lavoro evidenzia la difficoltà dei modelli di linguaggio di ragionare su conoscenza proveniente da diverse fonti e sottolinea la necessità di addestramento specifico per sfruttare appieno la conoscenza integrata.</sample>
    <sample id="129">"Asian woman" e "Middle-Eastern woman" sono stati forniti come esempi di gruppi contrassegnati.</sample>
    <sample id="130">Il paper suggerisce che le architetture dei modelli tradizionali non generalizzano adeguatamente. I modelli transformer generalizzano meglio.</sample>
    <sample id="131">Il video non menziona i nomi dei set di dati di test.</sample>
    <sample id="132">Tre.</sample>
    <sample id="133">OFA, il modello base utilizzato, utilizza un vocabolario unificato per testo, immagini e coordinate di bounding box.</sample>
    <sample id="135">ABC-Eval è un nuovo approccio dimensionale per valutare i modelli di dialogo di intelligenza artificiale (AI). Sviluppato dall'Emory NLP Lab, si differenzia dai metodi tradizionali basati sull'analisi umana (valutazione di singoli turni o conversazioni) per concentrarsi sull'annotazione esplicita dei comportamenti dei modelli, come risposte irrilevanti, contraddizioni, allucinazioni e mancanza di empatia.

ABC-Eval mira a ridurre la soggettività umana e a fornire una valutazione più precisa e affidabile della qualità del dialogo. È stato testato su quattro modelli di chat all'avanguardia, confrontato con metodi esistenti (valutazione Likert a livello di turno o dialogo, confronti pairwise). I risultati mostrano che le etichette di comportamento di ABC-Eval sono più affidabili e predittive della qualità complessiva della conversazione rispetto ai metodi tradizionali.

ABC-Eval identifica metriche distintive che catturano diversi aspetti della qualità del dialogo, spiegando una percentuale maggiore della qualità complessiva rispetto alle metriche Likert.  Sebbene i modelli testati presentino ancora errori comuni (circa il 20% di violazioni della common sense, il 15% di informazioni irrilevanti e il 10% di contraddizioni), ABC-Eval fornisce una base solida per la valutazione e il confronto dei modelli di AI. L'obiettivo è sviluppare metriche precise e affidabili per guidare il progresso nel campo della conversazione AI.</sample>
    <sample id="136">Jasivan e Nafise della University of Sheffield presentano FERMAT, un nuovo approccio per valutare le capacità di ragionamento numerico nei modelli linguistici. Il lavoro nasce dalla necessità di migliorare le attuali metriche di accuratezza, che non forniscono informazioni sufficienti sulle reali capacità matematiche dei modelli, specialmente quelli di dimensioni inferiori a 10 miliardi di parametri.

FERMAT è un set di dati basato su domande matematiche estratte da Illinois e CommonCore, con numeri rappresentati in vari formati (interi, decimali, grandi numeri) e con diverse operazioni. L'obiettivo è testare la comprensione dei numeri, le operazioni matematiche e la dipendenza dal training.

Un'analisi zero-shot rivela che i modelli linguistici performano scarsamente. Un fine-tuning con domande generate da esperti di matematica, che includono diverse operazioni e formati numerici, migliora significativamente le prestazioni. Tuttavia, l'accuratezza rimane bassa anche quando le domande corrispondono a quelle presenti nel training, suggerendo che i modelli non memorizzano le informazioni, ma le interpretano linguisticamente.

L'introduzione di template linguistici diversificati, provenienti da dataset come GSM8K e AQUA, porta a un miglioramento ancora maggiore delle prestazioni. In conclusione, FERMAT offre una valutazione più completa e informativa delle capacità di ragionamento numerico dei modelli linguistici, evidenziando l'importanza della diversità linguistica e matematica e la necessità di migliorare la rappresentazione numerica.</sample>
    <sample id="137">Sicong from the Singapore University of Technology and Design presents "Tell2Design: A Dataset for Language-Guided Floor Plan Generation," published in ACL 2023. The paper addresses the need for generating designs from natural language instructions, a crucial step in bridging the gap between user needs and expert design. Unlike text-to-image models focused on artistic generation, Tell2Design tackles the more constrained task of floor plan generation, where instructions specify semantics, geometry, and topology.

The research introduces a novel sequence-to-sequence model, leveraging a transformer-based architecture and pre-trained language models like T5, to generate floor plan layouts from these instructions.  A large-scale dataset of 5,051 human-annotated instructions and 76,000 artificially generated ones is created to train and evaluate the model.  The model achieves high IoU scores (Micro and Macro) compared to text-to-image baselines, demonstrating its ability to follow complex instructions.

The study highlights the challenges of this task, including strict constraints, understanding unstructured text, and handling ambiguity in human language.  The paper also explores the effectiveness of combining artificial and human-written instructions during training.  The findings suggest that while language distribution gaps exist, a blended approach can significantly improve performance.  Tell2Design and the proposed sequence-to-sequence model offer a foundation for future research in language-guided design generation.</sample>
    <sample id="138">Secondo gli autori, l'area della NLU che è poco studiata è la capacità dei modelli di integrare e utilizzare sia la conoscenza acquisita durante la pre-addestramento che quella fornita al momento dell'inferenza.</sample>
    <sample id="139">Ying e Zhiyang.</sample>
    <sample id="140">Yes, the CoScript dataset was reviewed and revised by crowd-sourced workers to ensure the quality of the validation and test sets.</sample>
    <sample id="141">First, only a small portion of translations depend on context, making corpus-level metrics like BLEU unable to capture these translations. Secondly, existing resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation.</sample>
    <sample id="142">Javad Hosseini, insieme a Filip Radlinski, Silvia Pareti e Annie Louis, presenta il loro lavoro su "Resolving Indirect Referring Expressions for Entity Selection", in cui introducono il dataset AltEntities. Il loro obiettivo è comprendere il linguaggio degli utenti quando desiderano fare una scelta. Ad esempio, nella domanda alternativa "Did you mean 'Easy on Me' or 'I Gotta Feeling'?", un utente vuole scegliere tra queste due canzoni. L'obvio sarebbe usare una citazione diretta, come dire il nome della canzone o la sua posizione. Tuttavia, a volte una citazione indiretta è più appropriata per una conversazione naturale. Questo può accadere quando l'utente non ricorda il nome della canzone, o quando le pronunce sono troppo simili per essere disambiguate, o quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di citazioni indirette, come "la più recente" o "quella che non è energica". Questo è un problema importante nei sistemi conversazionali e anche per il benchmarking dei modelli linguistici di grandi dimensioni (LLM) nella comprensione degli entità. Non siamo a conoscenza di un dataset pubblico più ampio per questo compito, quindi ne abbiamo creato uno utilizzando l'annotazione da parte di crowd. Il nostro dataset copre tre domini diversi: musica, libri e ricette. La nostra metodologia di raccolta dati enfatizza l'informalità utilizzando un setup di completamento di cartoon. Il cartoon ha tre bolle di dialogo. Nella prima bolla, Bob dice: "Ricordi quella canzone che stavamo ascoltando ieri?". Con questo, Bob stabilisce il contesto del dialogo. Nella seconda bolla, Alice dice: "Vuoi dire 'Easy on Me' o 'I Gotta Feeling'?", che è la domanda alternativa. E nella terza bolla, Bob usa una citazione indiretta per selezionare una delle entità, ad esempio, "la più recente". Forniamo le prime e seconde bolle automaticamente, ma la terza è compilata dall'annotatore. La prima bolla è scelta da diverse domande manuali per dominio. La seconda, che è la domanda alternativa, è generata come segue. Usiamo sempre un semplice template: "Vuoi dire A o B?", dove A e B sono campioni da Wikipedia. Abbiamo utilizzato diversi metodi di campionamento. Man mano che ci si sposta nella lista, le entità diventano più simili tra loro e la disambiguazione è generalmente più difficile. Il primo è un campionamento uniforme casuale. Il secondo è quando le entità hanno titoli simili, ad esempio due libri con lo stesso nome, "The Return". Il terzo è quando hanno descrizioni simili su Wikipedia. E infine, quando hanno informazioni simili su Wikipedia, ad esempio lo stesso genere o lo stesso artista per una canzone. Quando mostriamo questa domanda alternativa agli annotatori, sanno il nome delle entità, ma non necessariamente le conoscono. Quindi, ciò che facciamo è mostrare alcuni background knowledge sulle due entità. Per le canzoni, mostriamo semplicemente un link di ricerca su Google per ciascuna canzone e poi chiediamo agli annotatori di ascoltare almeno alcune delle canzoni e di leggere su ciascuna. Ecco, ad esempio, il risultato della ricerca su Google per la canzone "Easy on Me". Per i domini delle ricette e dei libri, mostriamo alcuni testi da Wikipedia. Per le ricette, mostriamo anche le immagini, anch'esse da Wikipedia, in modo che gli annotatori sappiano come appaiono. Quindi, chiediamo agli annotatori di scegliere una delle entità e di descriverla usando tre a cinque espressioni di riferimento indirette. Ad esempio, quella con la musica a pianoforte. Ecco alcuni esempi dal nostro dataset. Ad esempio, "quella senza parole", "non quella con il bambino di 12 anni", o "quella fittizia", o "viene dall'Azerbaigian", e così via. Il dataset AltEntities ha 6.000 domande alternative su tre domini e contiene 42.000 espressioni di riferimento indirette. I risultati con il modello T5 XL sono riassunti di seguito. Se il modello linguistico ha accesso alle stesse conoscenze di background degli annotatori, l'accuratezza è molto alta, intorno al 92-95%. Ma questo non è realistico. Se il modello linguistico ha accesso a conoscenze di background parzialmente sovrapposte, l'accuratezza è compresa tra l'82 e l'87%, che è più realistico. Ad esempio, quando il modello linguistico recupera le conoscenze di background. Se il modello linguistico ha accesso solo ai nomi delle entità, l'accuratezza è solo del 60%, quindi c'è molto spazio per il miglioramento. Abbiamo anche dimostrato che i modelli sono generalizzabili a dominio. Ecco un link al nostro dataset. Grazie.</sample>
    <sample id="143">L'approccio viene confrontato con le strategie Wait-k e Local Agreement, nonché con l'architettura state-of-the-art specificamente progettata per la pre-traduzione simultanea.</sample>
    <sample id="144">The provided text does not mention the affiliations of the authors.</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">Yicheng from Fudan University presents a research paper addressing the critical issue of omission in dialogue summarization. While large language models excel at generating fluent summaries, they frequently omit key information, leading to factual inaccuracies and incomplete summaries. This presentation highlights the severity of the omission problem, revealing a high omission rate (around 70%) across various domains and models.

To systematically analyze and address this challenge, the researchers propose the OLDS dataset, a high-quality dataset of dialogue summaries with explicit omission labels. This dataset is built upon existing benchmarks and utilizes automatic and human evaluation to ensure label quality. The paper explores three model architectures – pairwise classification, sequence labeling, and pointer networks – to build omission detection models, evaluating their performance using metrics like Precision, Recall, and F1-score.  A key finding is the significant label imbalance in the dataset, indicating the need for more advanced detection models.

Furthermore, the research investigates the potential of using detected omissions to refine summaries. A post-editing method that incorporates omission content as input demonstrates a substantial performance boost, suggesting that omission detection is a valuable step towards improving the quality of dialogue summarization. The paper concludes by emphasizing the challenging nature of the task and the promising direction of omission-based refinement.</sample>
    <sample id="147">Esin Durmus, Dan Jurafsky e Myra.</sample>
    <sample id="148">La traduzione simultanea del parlato (SimuST) è il processo di traduzione di un linguaggio parlato in testo in un altro linguaggio in tempo reale, consentendo la comunicazione interlinguistica. Quali sono i problemi dei modelli SimulST attuali? Le architetture specifiche sono solitamente addestrate, introducendo moduli aggiuntivi da ottimizzare. Lunghe e complesse procedure di addestramento, ad esempio, addestramento che coinvolge diversi obiettivi di ottimizzazione. E addestramento e mantenimento di diversi modelli per raggiungere diversi regimi di latenza. Ad esempio, addestrare un modello con una latenza media di un secondo e un altro con due secondi, e così via. Qual è la nostra soluzione? Innanzitutto, utilizzare modelli ST offline esistenti senza riaddestramento o adozione di architetture specifiche per SimulST. Utilizzare solo un modello per ogni regime di latenza e gestire la latenza tramite parametri specifici. E sfruttare la conoscenza già acquisita dal modello attraverso il meccanismo di attenzione tra l'input audio e l'output testuale. Questo è il meccanismo di cross-attenzione, e potete vedere un esempio a destra. La nostra soluzione è proporre EDAtt, o Encoder-Decoder Attention, che è una strategia per la quale decidiamo di emettere o meno una traduzione parziale in base a dove l'attenzione punta. Una parola viene emessa se l'attenzione non è concentrata, cioè la somma è inferiore a una certa soglia alfa rispetto alle ultime lambda frame audio, il che significa che le informazioni ricevute sono sufficientemente stabili. Ad esempio, se riceviamo un blocco di parlato contenente "Sto per parlare di...", e il nostro modello prevede la traduzione in tedesco, e visualizziamo i pesi di cross-attenzione, vedremo che le prime due parole puntano alle prime frame audio ricevute, mentre l'ultima parola punta alle ultime lambda frame audio. Ciò significa che le prime due parole verranno emesse, mentre poiché la somma dei pesi di cross-attenzione è superiore a una certa soglia alfa, non emetteremo l'ultima parola e aspetteremo un altro blocco di parlato. Se andiamo avanti e riceviamo un altro blocco di parlato, e il nostro modello prevede altre tre parole, visualizzeremo quei pesi di cross-attenzione e vedremo che nessuna parola punta alle ultime lambda frame audio. Ciò significa che queste tre parole verranno emesse. Se guardiamo i principali risultati di EDAtt, tracciamo i risultati della traduzione simultanea su grafici in cui abbiamo BLEU su un lato che misura la qualità della traduzione, e la latenza media che è la misura della latenza, e consideriamo anche la latenza computazionalmente consapevole che tiene conto dei tempi di calcolo del modello per prevedere l'output. Vogliamo che le nostre curve siano il più alte possibile su questo grafico. Ma vogliamo anche che siano spostate verso sinistra. E confrontiamo con le strategie popolari applicate ai modelli offline che sono anche la strategia Wait-k e la strategia Local Agreement. Confrontiamo anche con l'architettura state-of-the-art specificamente progettata per la pre-traduzione simultanea. Questi sono tutti i risultati della strategia di traduzione simultanea su tedesco. E vediamo che supera tutte le strategie applicate ai modelli offline poiché le curve sono spostate verso sinistra. E vediamo anche che se si considera il tempo effettivo trascorso o il tempo computazionalmente consapevole, che è la strategia più veloce, è la strategia più veloce. Se volete scoprire altri risultati, leggete il nostro articolo. E abbiamo anche rilasciato il codice e i modelli aperti per facilitare la riproducibilità del nostro lavoro. Grazie per la vostra attenzione.</sample>
    <sample id="149">Sì, il set di dati è disponibile pubblicamente.</sample>
    <sample id="150">Archiki presenta il paper "MEETINGQA", un nuovo dataset per l'estrazione di risposte a domande in trascrizioni di riunioni. Il dataset, basato su domande poste durante le riunioni e le relative risposte, si distingue per la sua natura lunga, aperta e orientata alla discussione, a differenza dei dataset esistenti che si concentrano su riassunti e azioni.

Il dataset MeetingQA contiene 7.7K domande, con un'alta coerenza inter-annotatore (Krippendorff's alpha di 0.73). Le domande sono spesso lunghe, multi-speaker e includono elementi come domande retoriche.

L'articolo esplora diverse metodologie, tra cui il retrieval di contesto, modelli a singolo span e modelli a più span (token classification).  I risultati mostrano un significativo divario tra le prestazioni dei modelli e quelle umane, soprattutto in zero-shot. I modelli hanno difficoltà a identificare domande retoriche e a distinguere tra risposte di diversi partecipanti. L'utilizzo di dati di aumento con "silver annotations" migliora le prestazioni in zero-shot.

In sintesi, MeetingQA offre una sfida interessante per i modelli di QA, evidenziando la complessità delle domande in contesti di discussione e la necessità di approcci specifici per questo tipo di dati.</sample>
    <sample id="151">Hello everyone, my name is Ying and my colleague Zhiyang and we will be presenting our research on MultiInstruct improving Multi-Modal Zero-Shot Learning via Instruction Tuning. So with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data-efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions. However, most previous works on instruction tuning focused on improving the zero-shot performance on language only tasks, while computer vision and multi-modal tasks have been left out. Therefore, in this work we want to investigate whether instruction tuning a multi-modal pre-trained models can actually improve generalisation to unseen multi-modal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in the availability of instructional datasets between NLP and multi-modal. There exist more than 1600 language-only instruction tasks. However, there is no large-scale publicly-available multi-modal instruction task. Therefore, this motivates us to build a multi-modal instruction tuning dataset. Here we present MultiInstruct, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source dataset and each task is equipped with five expert written instructions. For investigating multi-modal instruction tuning on our proposed dataset, we take OFA, a unified multi-modal pre-trained model, as our base model. OFA uses a unified vocabulary for language, image tokens and the coordinates of a bounding box. Here we show some example instances from our MultiInstruct dataset, to unify the processing of various input and output data types. We follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format. In which the input text, images, instructions and bounding boxes are represented in the same token space. Ok, now I'm going to talk about multi-modal instruction tuning. So for the training dataset, we use 53 tasks from 9 groups for training and we sample 10,000 instances per task. For testing, we reserve the entire common sense reasoning group for testing, and we select additional 5 tasks from VQ and Miscellaneous groups. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. So we use pre-trained OFA large model as a base model. During training, we mix all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates. So during test for each task, we conduct a total of 5 experiments by evaluating the model using one of the five instructions. In each experiment, we report the min and max performance and the standard deviation of the performance across all 5 experiments. If the task is a multi-model classification task, we report accuracy. If it's a multi-modal generation task, we report Rouge-L. For NLP task, we report Rouge-L as well. We also introduce an additional evaluation metric called sensitivity. This measures the model's ability to consistently produce the same outputs for the same task regardless of the slight variation in the wording of the instruction. Here is our main result. As we can see, instruction tuning can significantly improve OFA's performance on seen multi-modal tasks. Also, transfer learning from natural instruction dataset can benefit instruction tuning. Here we can see, as the amount of task increases, the model achieves better performance and in the meantime, lower sensitivity. So we also did one experiment. We use one instruction versus 5 instruction. As we can see, using more instructions can improve the model's overall performance and reduce its sensitivity a lot. So this shows the effect of different fine-tuning strategies on the model sensitivity. As we can see by transfer learning from natural instruction datasets, the model can achieve much better sensitivity compared to the original OFA model. We also can see transfer learning from natural instruction datasets can help OFA to attain much better performance on the natural instruct dataset. So overall, we propose the first large scale multi-model instruction tuning dataset with significantly improved their short capability of OFA, and we explore different transfer learning technique and show their benefits. We design a new metric called sensitivity. So one more thing, we are collecting a much larger multi-model instruction tuning dataset with around 150 additional vision language tasks and we will release them. So this is a QR code for our data and model. Thank you.</sample>
    <sample id="152">Frederick Riemenschneider presenta il lavoro del suo team sull'utilizzo di modelli linguistici di grandi dimensioni (LLM) per la filologia classica. Il progetto mira a migliorare le capacità di analisi di testi in greco antico e latino, superando i limiti dei modelli BERT monolinguali esistenti.

Sono stati sviluppati nuovi modelli: GreBERTa (RoBERTa monolinguale per il greco), GreTa (encoder-decoder T5 per il greco), PhilBERTa e PhilTa (modelli multilingue per greco, latino e inglese).  La creazione di questi modelli ha comportato la raccolta di dati di pre-addestramento, inclusi nuovi corpora derivati dall'Internet Archive, corretti tramite l'identificazione e la correzione di errori di trascrizione.

I modelli sono stati valutati su task come tagging POS, parsing della dipendenza e lemmatizzazione, ottenendo risultati superiori allo stato dell'arte.  L'analisi del comportamento dell'encoder T5 ha rivelato differenze significative rispetto ai modelli encoder-only tradizionali.  La lemmatizzazione ha visto un miglioramento notevole grazie all'approccio encoder-decoder.

Nonostante l'utilizzo di modelli multilingue, non si è riscontrata una differenza significativa nelle prestazioni rispetto ai modelli monolingue per quanto riguarda la comprensione semantica e la conoscenza del mondo.  Il lavoro si conclude con l'introduzione di modelli linguistici avanzati e di un dataset di pre-addestramento di alta qualità per il greco antico, aprendo nuove prospettive per la ricerca in filologia classica.</sample>
    <sample id="153">Ninareh Mehrabi presenta il lavoro del suo team su Amazon Alexa AI, "Resolving Ambiguities in Text-to-Image Generative Models". Il team si concentra sulla risoluzione delle ambiguità nei prompt testuali utilizzati per generare immagini con modelli di intelligenza artificiale. L'obiettivo è migliorare la fedeltà delle immagini generate all'intenzione dell'utente.

Il lavoro si basa su un dataset curato, una versione modificata di LAVA, che copre diversi tipi di ambiguità. Il framework proposto prevede due approcci principali per la disambiguazione: l'utilizzo di un modello linguistico per generare domande chiarificatrici, o la generazione di diverse interpretazioni visive. L'utente risponde alle domande o seleziona le interpretazioni, fornendo così un prompt disambiguato.

Per valutare la fedeltà delle immagini generate, viene proposto un framework di valutazione automatica basato su un modello VQA (Visual Question Answering). Il modello VQA riceve come input le immagini generate e la descrizione dell'intenzione dell'utente, valutando se l'immagine soddisfa tale intenzione.

I risultati mostrano che la risoluzione delle ambiguità ha un impatto positivo sulla generazione di immagini fedeli all'intenzione, e che il framework di valutazione automatica è in grado di concordare con le valutazioni umane. In conclusione, il lavoro presenta un approccio per affrontare le sfide poste dalle ambiguità nei prompt testuali, migliorando la qualità e la fedeltà delle immagini generate dai modelli di intelligenza artificiale.</sample>
    <sample id="154">Sara Papi from the University of Trento and Foundazione Bruno Kessler, Matteo Negri, and Marco Turchi.</sample>
    <sample id="155">Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="157">Shen Gao from Shandong University introduces their work, "Dialogue Summarization with Static-Dynamic Structure Fusion Graph," a joint project with several researchers. The research addresses the challenge of creating concise summaries from dialogues, aiming to capture key information efficiently. Existing methods rely on pre-computed static graphs, which are prone to errors and don't adapt to the task.

Their SDDS model employs a four-component architecture. First, an Utterance Encoder converts dialogue utterances into vector representations. Then, a Static-Dynamic Graph module constructs static graphs based on discourse parsing, key co-occurrence, and speaker interaction frequency, capturing structural information.  A Dynamic Graph module uses multi-head attention to learn semantic relationships between utterances based on their vector representations. Finally, a pre-trained language model generates the summary by fusing the static and dynamic graph information.

The model utilizes heuristic methods for static graph construction, including Discourse Parsing Graph, Key Co-occurrence, and Speaker Interaction Frequency Matrix, along with relative distance as edge features.  The Dynamic Graph module leverages multi-head attention for relationship modeling.  A fusion method combines the static and dynamic graphs, and a graph attention layer integrates the graph representation into the summary generation process. The code and data are available on GitHub.</sample>
    <sample id="158">Qipeng Guo from AWS introduces "Dual Cache for Long Document Neural Coreference Resolution." The coreference resolution task aims to identify mentions of the same entity in a document, addressing the challenge of quadratic complexity of traditional methods. Cache-based methods offer linear complexity but suffer from high cache misses in long documents due to topic shifts and the LRU eviction policy.

The proposed dual cache utilizes a local cache (LRU) for local entities and a global cache (LFU) for global entities, working together.  The model scans the document, classifying mentions as new or existing, and adds them to the appropriate cache based on frequency.  Eviction policies manage cache fullness.

Experiments on LitBank, OntoNotes, and WikiCoref demonstrate that the dual cache outperforms baselines, even with unbounded memory, and is faster than single-cache methods.  Performance gaps are larger for book-level documents, highlighting the benefit of the dual cache's ability to handle long texts.  The dual cache significantly reduces cache misses compared to single-cache approaches.

The dual cache achieves a high performance/cost ratio, making it a cost-effective solution for long document coreference resolution.  It effectively separates local and global entities, leading to improved performance and reduced cache misses.</sample>
    <sample id="159">Ciao a tutti. Sono Koustav Sinha e sono lieto di darvi il benvenuto alla nostra presentazione del paper di ACL 2023. I giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto. Questo è un lavoro congiunto con John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy e Adina Williams. Quindi, in questo lavoro, rivisito i paradigmi delle coppie minimali. Il paradigma delle coppie minimali valuta i modelli linguistici basandosi sui giudizi di accettabilità, che possono includere anche la correttezza grammaticale come BLiMP, SyntaxGym o l'accettabilità in termini di stereotipi come CrowS pairs. In questo, il paradigma delle coppie minimali, il modo tipico per valutare i modelli linguistici è che mostri una frase accettabile o grammaticale e poi mostri una frase accettabile o non grammaticale. L'idea è che il modello, in pratica, dia più probabilità alla frase accettabile. Il pipeline MPP attuale non consente di valutare l'accettabilità verso frasi più lunghe. Negli ultimi anni, i grandi modelli linguistici stanno diventando sempre più lunghi e con finestre di contesto sempre più ampie. Pertanto, è fondamentale valutare l'accettabilità dei modelli in tutto il finestrino di contesto e questo è ciò che stiamo cercando di fare qui. Stiamo cercando di rivisitare il pipeline MPP ponendo al modello il compito di valutare l'accettabilità su sequenze sempre più lunghe. Questo è il nostro approccio. Quindi, ciò che facciamo è che per simulare queste sequenze più lunghe, rivisito i dataset stessi e ricreo le frasi scegliendo frasi accettabili o non accettabili da quei dataset. Ad esempio, qui abbiamo scelto una tipica coppia di correttezza grammaticale dal dataset BLiMP dal caso dell'Isola degli Adjunct. E ciò che facciamo è che per ricreare sequenze più lunghe e che abbiano la stessa corrispondenza della struttura grammaticale, estraggo frasi grammaticali dall'Isola degli Adjunct e le aggiungo come prefisso sia alla query accettabile che alla query non accettabile. Possiamo fare lo stesso scegliendo frasi da un altro sottoinsieme o da un altro dataset. Questo è ciò che chiamiamo scenario di mismatch. Quindi, qui le frasi vengono ancora dai dataset pertinenti, ma non dallo stesso dataset con cui si sta valutando. Possiamo anche fare lo stesso scegliendo frasi da un dominio completamente diverso, come Wikipedia. Questo ci dirà se i giudizi di accettabilità dei modelli sono effettivamente influenzati da qualsiasi contesto, ovvero se il contesto proviene da un sottoinsieme diverso dei dataset o se è completamente irrilevante per la frase che stiamo guardando. Come si comporta il modello? Quindi, inizialmente, guardiamo le frasi di Wikipedia, che sono completamente irrilevanti per la query corrente, e lì troviamo che i giudizi MPP sono per lo più robusti per qualsiasi lunghezza di contesto. Aumentiamo la lunghezza del contesto fino a 1024 per esaurire OPT e GPT-2 e qui vediamo che la linea puntata arancione indica che i giudizi MPP sono relativamente stabili. Cosa succede quando scegliamo frasi dallo stesso dataset? Qui stiamo scegliendo o creando frasi da domini accettabili e non accettabili dallo stesso dataset BLiMP o SyntaxGym. E qui vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o non accettabili. Ma quando si corrisponde alla struttura, ovvero quando si scelgono frasi dallo stesso fenomeno in BLiMP o SyntaxGym, vediamo un aumento o una diminuzione massiccio dei giudizi MPP del modello, a seconda che il prefisso scelto sia accettabile o non accettabile. E questo è molto grande, questo effetto, aumenta lungo la lunghezza del contesto e probabilmente influenzerà i modelli linguistici più recenti che hanno finestre di contesto ampie. Perché il prefisso corrispondente influisce così tanto sul giudizio del modello? Quindi, abbiamo condotto una serie di analisi cercando di perturbare la frase modificando il suo contenuto mantenendo la struttura rilevante aggiungendo rumore alla frase di input. Dopo aver eseguito diversi di questi perturbazioni, troviamo che nessuna di queste perturbazioni sta effettivamente facendo cambiare al modello il suo modo di esprimere il giudizio MPP. In pratica, troviamo che i modelli sono sensibili alle frasi perturbate in modo simile. In altre parole, quando perturbiamo le frasi nel dominio accettabile, vediamo un aumento simile in tutte le perturbazioni e quando perturbiamo le frasi nel dominio non accettabile, vediamo una diminuzione dei giudizi MPP in modo simile. Quindi, i principali risultati del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche latenti sintattiche e semantiche condivise tra le frasi e il modo in cui valutiamo l'accettabilità con input brevi e singole frasi potrebbe non catturare pienamente la conoscenza astratta dei modelli linguistici in tutto il finestrino di contesto. Leggete il nostro paper per maggiori dettagli sui nostri esperimenti. Grazie per aver ascoltato.</sample>
    <sample id="160">Il primo passaggio del metodo mappa ogni token di input a un multiset di token che appariranno nell'output.</sample>
    <sample id="161">55,000</sample>
    <sample id="163">Il metodo di allineamento migliore per DEPLAIN è MASSalign.</sample>
    <sample id="164">L'apprendimento scarsamente supervisionato è più economico rispetto all'annotazione manuale dei dati, ma i dati etichettati sono rumorosi.</sample>
    <sample id="165">Wenting Zhao presenta il suo lavoro "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations". Il paper affronta il problema dell'abduzione, ovvero dedurre la migliore spiegazione per un evento osservato (Outcome) a partire da un contesto (Context). L'obiettivo è identificare una spiegazione plausibile che colmi il divario tra i due.

I metodi tradizionali per l'abduzione si basano su dati annotati, ma questi dati sono spesso rumorosi e soggettivi. L'approccio di Zhao propone un metodo di apprendimento non supervisionato chiamato LiPoR (Likelihood Learning with Posterior Regularization). LiPoR considera le spiegazioni come variabili latenti e ottimizza la probabilità dell'Outcome dato il Context, marginalizzando le altre spiegazioni.

Per migliorare la preferenza verso spiegazioni plausibili, LiPoR introduce un regularizer basato sulla mutualità esclusiva delle spiegazioni.  Questo regularizer, denotato come Omega, penalizza le spiegazioni con alta entropia (cioè, con alta probabilità di essere selezionate) e preferisce un sottoinsieme di spiegazioni.

I risultati sperimentali su AlphaNLI, un dataset di abduzione, dimostrano che LiPoR supera significativamente i modelli zero-shot e l'approccio precedente, ottenendo un miglioramento di oltre 4 punti di accuratezza.  Il paper è disponibile su tinyurl.com/zhao-lipor.</sample>
    <sample id="166">This paper introduces a novel neural framework, Neural Divide-and-Conquer Reasoning (NDCR), for image retrieval from linguistically complex text. The task presents challenges due to image similarity and lengthy descriptions, hindering the performance of traditional visual-language models. NDCR addresses this by integrating Divide-and-Conquer strategy with Dual-Process Theory, inspired by human cognitive systems. It posits that complex reasoning requires both analogical (System 1) and logical (System 2) processing.

The framework comprises three key modules: a Proposition Generator to decompose complex text into simple propositions, a Visual-Linguistic Interactor (System 1) to interact visual and proposition information, and a Neural-Symbolic Reasoner (System 2) to perform logical reasoning on the propositions. The Reasoner utilizes negation and conjunction operations to integrate reasoning states and produce a final solution.

Experimental results demonstrate that NDCR outperforms existing baselines, with ablation studies validating the individual module contributions. The system's ability to present inference states and results mid-process highlights its interoperability. The authors suggest that neural-symbolic approaches hold promise for enhancing compositional reasoning in large language models, drawing parallels with chain-of-thought prompting and emphasizing the potential of combining Divide-and-Conquer with Dual-Process Theory for complex problem-solving.</sample>
    <sample id="167">In DEPLAIN-web, the documents were aligned both manually and with automatic alignment methods.</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato annotando i dati di Reuters News del 2020 con le stesse linee guida di annotazione CoNLL-2003.</sample>
    <sample id="169">David Vilar presenta una revisione del paper "Prompting PaLM for Translation: Assessing Strategies and Performance", in collaborazione con Google Translate. Il paper analizza l'utilizzo di PaLM, un modello linguistico di grandi dimensioni (LLM) con 540 miliardi di parametri, per la traduzione automatica.

L'obiettivo è valutare la capacità di PaLM di tradurre, confrontandolo con i sistemi all'avanguardia, come WMT. L'analisi si concentra sulle strategie di prompting, evidenziando come la scelta del prompt influenzi significativamente la performance, con differenze che possono arrivare a 40 BLEURT points.

I risultati indicano che la qualità degli esempi nel prompt è più importante della loro somiglianza con la frase originale, soprattutto per i prompting a cinque esempi.  L'utilizzo dei dati di valutazione (dev set) è cruciale per ottenere risultati migliori rispetto ai dati di addestramento.

Nonostante PaLM raggiunga una performance vicina a quella di un sistema commerciale come Google Translate, presenta ancora delle lacune in termini di accuratezza, con errori di omissione comuni. La fluidità della traduzione di PaLM è comparabile a quella dei sistemi all'avanguardia, ma a volte a scapito della precisione.  Il paper suggerisce che la selezione di prompt di alta qualità è fondamentale per ottenere risultati soddisfacenti.</sample>
    <sample id="170">Buongiorno a tutti, mi chiamo Yusen Zhang da Penn State University. Oggi presenterò il nostro lavoro "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations". Il semantic parsing è un compito che mira a costruire rappresentazioni semantiche di query degli utenti come SQL e Lambda Calculus. E il Cross-Lingual Semantic Parsing è il compito di tradurre le query in più lingue naturali in più rappresentazioni semantiche. Come mostrato in questa figura, dobbiamo tradurre le query in più lingue naturali in SQL, Lambda o FunQL, ecc. I modelli di cross-lingual semantic parsing esistenti sono proposti e valutati separatamente su dataset di task e applicazioni limitati. Ad esempio, c'è molta copertura su alcune lingue, ma la cinese è mancante e manca di copertura su alcune rappresentazioni semantiche. Il Lambda calculus è mancante, o sono valutati solo su alcuni modelli neurali. Ad esempio, c'è solo un singolo modello per valutarli. Quindi, a tal fine, proponiamo XSemPLR. Forniamo un dataset uniforme XSemPLR per il cross-lingual semantic parsing in più lingue naturali e rappresentazioni semantiche. Contiene 9 dataset in vari domini, 5 task di semantic parsing, 8 rappresentazioni semantiche e 22 lingue naturali in 15 famiglie linguistiche. E per valutare meglio il nostro benchmark, consideriamo sei impostazioni per l'addestramento e la valutazione. La prima è Translate-Test. Usiamo l'API di Google Translate per tradurre la sorgente in destinazione, quindi usiamo un modello monolingue per l'addestramento e la valutazione. Ad esempio, addestriamo il modello inglese su query in inglese e durante l'inferenza traduciamo la query tedesca usando l'API in inglese e quindi usiamo il modello addestrato per prevedere l'SQL. Testeremo anche il modello monolingue. In questa impostazione, la lingua sorgente è la stessa della lingua di destinazione, ad esempio tedesco-tedesco o inglese-inglese. Testeremo anche l'impostazione Monolingual Few-shot addestrando modelli monolingue con solo il 10% dei dati di addestramento. Testeremo anche il modello Multilingue addestrando un modello multilingue su tutte le lingue. Ad esempio, mettiamo insieme le query tedesche, inglesi e cinesi per addestrare un modello multilingue. Durante l'inferenza possiamo usare questo modello per tradurre query tedesche o cinesi, ecc. Considereremo anche il transfer cross-linguale zero-shot e few-shot. Addestriamo su una lingua sorgente e trasferiamo a un'altra lingua. Durante l'addestramento, addestriamo un modello multilingue per prevedere l'output SQL usando query inglesi o combinazioni di query inglesi e tedesche few-shot. Inoltre, troviamo molti risultati interessanti. Riguardo all'analisi dei modelli monolingue, li valutiamo su due gruppi di modelli: Encoder-PTR (Multilingual Pretrained Encoders with Pointer-based Decoders), come XLM-R + PTR e mBERT + PTR. Valutiamo anche i modelli Encoder-Decoder, ovvero Multilingual Pretrained Encoder-Decoder Models, come mBART e mT5. Abbiamo scoperto che i modelli Encoder-Decoder ottengono le migliori prestazioni su tutti e nove i dataset. Valutiamo mT5 e XLM-R + PTR sulla modalità multilingue. Abbiamo scoperto che sia Encoder-Decoder che Encoder-PTR possono essere migliorati addestrandoli su una miscela di varie lingue. Abbiamo scoperto che la maggior parte delle principali lingue naturali può ottenere un miglioramento delle prestazioni, ad eccezione dell'inglese che perde prestazioni su sette dataset e guadagna su tre. Penso che questo sia noto come la "Maledizione della Multilingua". Confrontiamo anche la performance cross-linguale. Questa figura mostra la Cross-lingual Few-shot transfer, la Cross-lingual Zero-shot transfer e la Monolingual Setting. Abbiamo scoperto che, confrontando la linea verde e la linea arancione, la performance di cross-lingual transfer nella modalità zero-shot è significativa e che, confrontando la linea blu e la linea arancione, abbiamo scoperto che con l'impostazione few-shot la performance di transfer diminuisce rapidamente. Abbiamo anche scoperto alcune altre interessanti scoperte. Ad esempio, Encoder-Decoder supera i lavori precedenti o ottiene risultati comparabili. Il pre-addestramento su inglese può migliorare significativamente le prestazioni few-shot su lingue naturali di destinazione. Abbiamo scoperto che i modelli linguistici multilingue come Codex e BLOOM sono ancora inadeguati per i task di semantic parsing cross-lingua. Per riassumere, abbiamo costruito XSemPLR, un benchmark unificato per il cross-lingual semantic parsing con più lingue naturali e rappresentazioni semantiche. Abbiamo condotto uno studio di benchmark completo su tre tipi rappresentativi di modelli linguistici multilingue. I nostri risultati mostrano molti risultati interessanti. E eccetera. E vi invitiamo a visitare il nostro paper e il nostro codice. Grazie per l'attenzione.</sample>
    <sample id="171">I lavori connessi sono classificati in quattro categorie, ma nessuno di essi è applicabile a embedding as services o manca di trasferibilità.</sample>
    <sample id="172">No, the paper finds that multilingual language models like Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="174">Thea, co-author of the paper "ArgAnalysis35K," explains what makes their dataset unique for argument quality analysis. Current datasets often suffer from quality issues due to crowdsourcing, lack of diversity, and insufficient depth in argument explanation. ArgAnalysis35K addresses these problems by being the largest dataset (35K argument-analysis pairs) with high-quality arguments sourced from expert and intermediate debaters, rather than just novice users.

The dataset boasts a diverse range of arguments, covering 24 themes based on parliamentary debate, unlike the pre-selected motions found in other datasets.  Crucially, ArgAnalysis35K introduces the concept of "analysis," which goes beyond simple claims and premises to encompass a combination of both, providing a more comprehensive understanding of the argument's reasoning.

Furthermore, the dataset incorporates instance-based annotator reliability, allowing for the utilization of annotator judgments even when they exhibit biases towards certain topics. Finally, a relevance model assigns scores to arguments' relevance to different themes, capturing the broader applicability of arguments beyond a single motion.  The goal is to provide a more diverse, reliable, and nuanced dataset for NLP research in argument analysis.</sample>
    <sample id="175">Il metodo affronta l'ambiguità delle permutazioni inducendo l'allineamento come parte dell'addestramento.</sample>
    <sample id="176">Il modello NLP a valle viene valutato per l'equità analizzando le prestazioni di modelli con diverse inclinazioni politiche su compiti come il rilevamento di discorsi d'odio e la rilevazione di notizie false, considerando come le prestazioni variano tra diversi gruppi demografici e inclinazioni politiche.</sample>
    <sample id="177">Yanis Labrak.</sample>
    <sample id="178">Koustav Sinha.</sample>
    <sample id="179">Melanie Sclar presenta "Minding Language Models’ (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker". La presentazione affronta la difficoltà dei modelli linguistici di comprendere la teoria della mente, ovvero la capacità di ragionare sulle credenze degli altri. Il test classico di teoria della mente è il test di Sally-Anne, che valuta se un individuo o un modello può comprendere che un'altra persona può avere credenze diverse dalla realtà.

La ricerca si concentra sull'aumento delle capacità di teoria della mente nei modelli linguistici di grandi dimensioni (LLM). Viene introdotto SymbolicToM, un metodo di inferenza-time che utilizza rappresentazioni grafiche esplicite per modellare le credenze dei personaggi. Queste rappresentazioni grafiche, BBob e BBob,Alice, vengono generate per tutte le combinazioni di personaggi e vengono utilizzate per rispondere alle domande sulla teoria della mente.

Vengono presentati risultati sperimentali che dimostrano che SymbolicToM migliora significativamente le prestazioni degli LLM su compiti di teoria della mente, superando i modelli supervisionati. Vengono inoltre valutate le capacità di generalizzazione del metodo su nuovi dataset, evidenziando la sua robustezza e interpretabilità. In conclusione, SymbolicToM è presentato come un metodo efficace e flessibile per migliorare le capacità di teoria della mente negli LLM, evitando l'overfitting e fornendo una comprensione più approfondita del ragionamento.</sample>
    <sample id="180">Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models.</sample>
    <sample id="181">This paper addresses the challenge of constrained language planning, extending the work on abstract goal planning for stereotypical activities to scenarios with specific constraints, such as "make a chocolate cake."  While large language models (LLMs) demonstrate promise in decomposing goals, their performance on constrained planning remains unsatisfactory.  The authors investigate the limitations of LLMs in this domain, finding issues with both semantic completeness and constraint faithfulness.  

To overcome these limitations, they propose an "over-generate-then-filter" method, leveraging InstructGPT to generate numerous scripts for specific goals and then employing a filter model based on semantic similarity and constraint keywords to select high-quality, constraint-faithful scripts.  This approach significantly improves the quality of generated scripts.  

Furthermore, the paper tackles the scarcity of datasets for constrained language planning by utilizing symbolic knowledge distillation.  They create a large dataset of 55,000 specific goals and corresponding scripts, named CoScript, using InstructGPT and crowd-sourced validation.  The CoScript dataset exhibits high diversity in goal types.  Finally, the authors demonstrate that fine-tuning smaller models like T5 on CoScript can achieve performance comparable to, and sometimes surpass, larger LLMs, highlighting the potential of specialized models for constrained language planning.  The CoScript dataset is presented as a valuable resource for advancing research in this area.</sample>
    <sample id="182">Il tropicalismo si riferisce a un tropo che associa le donne latinoamericane a immagini di tropicalità, come essere esotiche e seducenti.</sample>
    <sample id="183">The authors used natural language prompts to generate personas for various demographic groups (Asian women, Middle-Eastern women, women of color, white men). They then analyzed the generated personas using the "Marked Words" method to identify words that distinguish marked groups from unmarked ones, revealing patterns of stereotypes and essentializing narratives.</sample>
    <sample id="184">CXMI (Contextualized Machine Translation Information) è stato utilizzato per misurare l'utilizzo del contesto in questo lavoro.</sample>
    <sample id="185">DrBERT is a pre-trained model in French based on RoBERTa, trained on the NACHOS dataset of medical crawled data. ChuBERT is a clinical model based on anonymized data from the Nantes University Hospital data warehouse.</sample>
    <sample id="187">Ying e Zhiyang.</sample>
    <sample id="188">Il trasferimento iterativo dell'apprendimento aggiorna il modello addestrandolo sulla più recente serie di dati raccolti in ogni round di apprendimento attivo.</sample>
    <sample id="189">The goal of the AltEntities Corpus is to understand how users express choices when they use indirect referring expressions. It aims to address the problem of resolving these expressions in conversational systems and benchmarking LLMs' entity understanding.</sample>
    <sample id="190">Un utente malintenzionato può estrarre i parametri del modello imparando da un embedding e fornendo servizi simili.</sample>
    <sample id="191">3</sample>
    <sample id="192">Yang Luo presenta CAME: Confidence-guided Adaptive Memory Efficient Optimization, un nuovo ottimizzatore per l'addestramento di modelli linguistici di grandi dimensioni. L'obiettivo di CAME è bilanciare la velocità di convergenza con l'efficienza di memoria, un problema comune con gli ottimizzatori tradizionali come Adam e con quelli memory-efficient come Adafactor.

L'audio spiega come l'ottimizzatore NMF (Non-negative Matrix Factorization) riduca significativamente i requisiti di memoria, ma che Adafactor, basato su NMF, soffra di errori di aggiornamento che rallentano l'addestramento. CAME affronta questo problema introducendo un meccanismo di "confidenza guidata" che utilizza la differenza tra l'aggiornamento previsto e quello effettivo per regolare l'aggiornamento.

I risultati sperimentali su BookCorpus e English Wikipedia dimostrano che CAME supera Adam e Adafactor in termini di accuratezza e memoria, specialmente per modelli molto grandi e batch size elevati. CAME offre un miglioramento significativo in termini di accuratezza e riduzione della memoria, mantenendo prestazioni comparabili a quelle degli ottimizzatori di riferimento.  Inoltre, CAME si dimostra efficace anche per l'addestramento di modelli BERT-Large, riducendo i requisiti di memoria senza compromettere le prestazioni. In sintesi, CAME rappresenta un'evoluzione promettente per l'addestramento efficiente di modelli linguistici di grandi dimensioni.</sample>
    <sample id="193">The paper states that around 1,000 examples of discourse unit pairs were collected for the initial annotation. It doesn't specify the number of annotators.</sample>
    <sample id="194">Jenny is a first-year PhD student at Carnegie Mellon University. The authors are also affiliated with the University of Washington and the Allen Institute for AI. Specifically, the collaborators mentioned are Sebastian Santy, Ronan Le Bras, Katharina Reinecke, and Maarten Sap.</sample>
    <sample id="195">RoHT (Reasoning over Hierarchical Question Decomposition Tree) è un nuovo framework per l'Explainable Question Answering (XQA) che mira a fornire risposte a domande complesse e spiegazioni del perché tali risposte sono state scelte.  Il problema principale è la difficoltà di gestire domande complesse, che spesso richiedono la decomposizione in sotto-domande.

RoHT affronta questa sfida costruendo un albero di decomposizione gerarchico (HQDT) per rappresentare la struttura compositiva della domanda. Questo albero identifica le domande atomiche e le sotto-domande.  Il framework utilizza quindi il ragionamento probabilistico sull'HQDT per integrare conoscenza da basi di conoscenza (KB) e corpus testuali a diversi livelli di elaborazione.

Il processo di ragionamento avviene ricorsivamente, con un scheduler che seleziona le fonti di conoscenza appropriate (KB, testo o ragionamento sui figli), un executor che estrae le risposte con probabilità e un aggregator che combina le risposte candidate per produrre la risposta finale.

RoHT è stato valutato su due dataset complessi: KQA Pro (KB QA con dati incompleti) e Musique (QA comprehension con KB supplementare).  I risultati dimostrano che RoHT supera i metodi esistenti, specialmente quando si integra conoscenza da KB e testo.  In particolare, RoHT ottiene prestazioni superiori a TransferNet, un approccio end-to-end.  RoHT-mix, che combina KB e testo, mostra miglioramenti significativi su Musique.  In sintesi, RoHT offre un approccio promettente per l'XQA, sfruttando la decomposizione gerarchica delle domande e il ragionamento probabilistico per integrare diverse fonti di conoscenza.</sample>
    <sample id="196">I saw Bart and Lisa.</sample>
    <sample id="197">I am sorry, but the provided text does not mention any specific state-of-the-art dialogue models. It only states that four state-of-the-art chat models were evaluated using ABC-Eval.</sample>
    <sample id="198">La valutazione dell'accettabilità dei modelli nell'intera finestra di contesto è necessaria perché i modelli linguistici moderni hanno finestre di contesto sempre più lunghe e la pipeline MPP attuale non le gestisce, quindi è cruciale valutare l'accettabilità attraverso l'intera finestra di contesto.</sample>
    <sample id="199">No, la formazione multilingue ha portato a un calo delle prestazioni solo in sette dataset, mentre ha migliorato le prestazioni in tre dataset. Questo è noto come il "Curse of Multilinguality".</sample>
    <sample id="200">Gli annotatori conoscono il nome delle entità, ma non necessariamente le entità stesse.</sample>
    <sample id="201">Le metriche MT utilizzate sono state le metriche neurali state-of-the-art e la valutazione umana basata su esperti.</sample>
    <sample id="202">Sì, il regresso nella generalizzazione influisce su specifici tipi di NER.</sample>
    <sample id="203">NLPositionality highlights that NLP datasets and models can reflect the perspectives of those who create and use them, leading to systematic performance differences across populations. This is important because it can perpetuate biases and exclude certain groups, making NLP technologies less inclusive and equitable.</sample>
    <sample id="204">No, gli LLM multilingue come BLOOM sono stati ritenuti inadeguati per le attività di parsing semantico cross-lingua.</sample>
    <sample id="205">Shangbin, PhD student at the University of Washington, presented their work "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models." The research investigates how political biases in large language models (LLMs) propagate from pretraining data to downstream applications, highlighting potential fairness issues.

The study analyzed LLMs like GPT-4, RoBERTa, and BART, finding they exhibit varying political leanings, with GPT-4 being the most liberal.  They demonstrate that pretraining on partisan corpora (news and social media categorized by political leaning) shifts the ideological coordinates of the models.  Furthermore, the research shows that LLMs increasingly lean towards a more polarized viewpoint after 2017.

The team evaluated these models on hate speech and fake news detection, revealing that left-leaning models are better at detecting hate speech targeting minority groups, while right-leaning models are better at detecting hate speech targeting specific demographics.  Qualitative examples further illustrate how different political leanings lead to varied predictions on hate speech and misinformation.

The research concludes that LLMs exhibit significant political biases, posing a fairness risk in NLP applications.  The study acknowledges the dilemma of balancing the propagation of bias with the risk of censorship, emphasizing the difficulty of defining neutrality in data collection.  The work calls for addressing these biases to mitigate unfair outcomes.</sample>
    <sample id="206">Transfer learning from topic-independent dissonance stance classification and binary classification of expansion and comparison classes (CE).</sample>
    <sample id="207">The latest test sets were used to avoid overlap of the test data with the training data of the language model.</sample>
    <sample id="208">Ecco tre suggerimenti che gli autori hanno proposto alla fine:

1. Affrontare i positivi stereotipi e le narrazioni essenzialistiche.
2. Utilizzare un approccio intersezionale per studiare i pregiudizi e i danni.
3. Aumentare la trasparenza sui metodi di mitigazione dei pregiudizi.</sample>
    <sample id="209">Il metodo proposto migliora la qualità dei script generati dai modelli linguistici di grandi dimensioni, sia in termini di completezza semantica che di fedeltà alle restrizioni, rispetto ai modelli linguistici di grandi dimensioni senza il metodo di filtraggio. Inoltre, il metodo consente di creare un dataset di addestramento (CoScript) per modelli più piccoli e specializzati, superando le prestazioni dei modelli linguistici di grandi dimensioni in questo contesto.</sample>
    <sample id="210">Shuheng.</sample>
    <sample id="211">Yes, the results and dataset in the article can be used as a benchmark. The authors state that their fine-tuning of language models produced scores better than baseline scores, and they proposed these results as a base benchmark for the problem of automatic text simplification.</sample>
    <sample id="212">L'articolo utilizza T5 fine-tunato su CoScript come modello più piccolo.</sample>
    <sample id="213">OFA</sample>
    <sample id="215">Adam Przepiórkowski presenta un lavoro che sostiene la validità delle strutture di coordinazione simmetriche, contrapponendole alle strutture asimmetriche. L'argomento si basa sul principio di minimizzazione della lunghezza delle dipendenze, simile a come gli oggetti diretti preferiscono essere vicini al verbo.

L'analisi utilizza dati dal Penn Treebank enhanced e statistiche sulla lunghezza delle dipendenze tra i conjuncts. Si osserva che, quando il verbo è il governante (on the left), la conjunct sinistra tende ad essere più corta, soprattutto quando la differenza di lunghezza tra i conjuncts aumenta. Questo fenomeno è più marcato quando non c'è un governante esterno.

Al contrario, quando il governante è sulla destra, questa tendenza diminuisce o scompare.  L'esperimento dimostra che questa preferenza per conjuncts più corti sulla sinistra è una caratteristica distintiva delle strutture di coordinazione simmetriche.

In sintesi, il lavoro evidenzia come la minimizzazione della lunghezza delle dipendenze, in particolare quando il verbo è il governante, supporti l'idea di strutture di coordinazione simmetriche, fornendo un argomento a favore di queste strutture rispetto alle alternative asimmetriche.</sample>
    <sample id="217">"Seen to Unseen" esplora la generazione di dialoghi controllabili con più attributi, un'area trascurata dai metodi esistenti che si concentrano su attributi singoli o combinazioni semplici. L'obiettivo è superare la limitazione della dipendenza dai dati etichettati e sviluppare un metodo robusto per la generazione di dialoghi controllabili.

La proposta principale è DCG (Disentangled Controllable Generation), un modello che impara i concetti degli attributi dai valori visti e utilizza una perdita di disentanglement per separare le diverse combinazioni di attributi.  Viene introdotto MAE, un framework di valutazione senza dati etichettati, per diversi livelli di granularità degli attributi.

Il modello DCG si basa su DialoGPT e utilizza prompt composizionali, sia orientati agli attributi che orientati alla task, per guidare il modello.  Un ulteriore disentanglement loss aiuta a distinguere le diverse combinazioni di attributi.

I risultati sperimentali su DailyDialog-CG dimostrano che DCG supera i modelli di riferimento in termini di controllo degli attributi e qualità del testo, con un impatto minimo sulla precisione.  Il modello è in grado di generalizzare da attributi visti a combinazioni non viste.  MAE, il framework di valutazione, supera le valutazioni umane, specialmente per attributi continui.  La visualizzazione delle rappresentazioni dei prompt conferma la capacità del modello di disentanglement e apprendimento delle relazioni tra gli attributi. In sintesi, DCG offre un approccio promettente per la generazione di dialoghi controllabili con più attributi, con un'elevata capacità di generalizzazione.</sample>
    <sample id="218">David Vilar è affiliato a Google Translate.</sample>
    <sample id="219">Jia-Huei Ju presenta il lavoro di ricerca "A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports". Il lavoro, condotto con Yu-Shiang Huang, Cheng-Wei Lin e i loro supervisori, si concentra sull'analisi delle relazioni tra i report annuali (Form 10-K) delle aziende, che spesso contengono informazioni cruciali ma richiedono notevoli sforzi umani per l'estrazione.

L'obiettivo è sfruttare la somiglianza semantica e temporale dei contenuti dei report, che spesso presentano parole e concetti simili tra anni consecutivi. Per questo, viene introdotta una "task di highlighting" che mira a identificare le relazioni tra un report di interesse e il suo precedente anno, evidenziando le parole chiave che ne giustificano la rilevanza.

Il lavoro propone un pipeline a due fasi: la classificazione delle relazioni (Type β, Revised pairs, Mismatched pairs) e la fine-tuning del modello. La fine-tuning avviene utilizzando dati esterni (eSNLI) e dati sintetici (Revised pairs), con l'utilizzo di tecniche di soft labeling per migliorare la qualità delle etichette.

La valutazione si basa su dataset pubblici (eSNLI e FINAL) e metriche come Precision e PCC. I risultati dimostrano che il modello proposto ottiene prestazioni elevate, mantenendo anche una buona generalizzazione. Il lavoro conclude proponendo un pipeline semplice e una task di highlighting, con prospettive future per ulteriori miglioramenti e applicazioni nell'information retrieval.</sample>
    <sample id="220">Vasudha è una dottoranda in informatica presso la Stony Brook University.</sample>
    <sample id="221">German to English.</sample>
    <sample id="222">## Abstract

This work addresses the challenge of domain adaptation in open-domain question answering (QA), where models trained on general-purpose corpora like Wikipedia struggle to generalize to specialized domains. We investigate data interventions to enable out-of-domain generalization, focusing on zero-shot and few-shot approaches.  We demonstrate that few-shot learning, leveraging carefully crafted fact-based prompts generated by large language models, significantly improves both retriever and reader performance.  We also explore zero-shot techniques by controlling the interactions between question, answer, and context, finding that cloze-style questions are more amenable to adaptation than standard WH questions.

Furthermore, we analyze the type of dataset shift encountered during domain adaptation, identifying four categories: no shift, concept shift, covariate shift, and full shift. We develop a compatibility measure based on likelihood of context assignment by the retriever and answer likelihood by the reader, allowing us to map target datasets onto a 2D grid representing their shift type.  Our findings reveal that few-shot interventions are effective across all shift types, while zero-shot methods are particularly beneficial for concept and covariate shifts.  We conclude that understanding the nature of dataset shift is crucial for selecting appropriate data interventions to maximize performance gains in open-domain QA.  Our experiments demonstrate that data interventions can improve reader performance by up to 24%, highlighting the importance of targeted adaptation strategies.</sample>
    <sample id="223">Shangbin.</sample>
    <sample id="224">Durante gli esperimenti sono stati studiati i modelli MASSalign e long-mBART e base mBART.</sample>
    <sample id="225">The presentation states that 53 tasks from 9 groups are used for training and the entire common sense reasoning group is used for testing, plus 5 additional tasks from VQ and Miscellaneous groups. This totals 58 tasks used for training and testing.</sample>
    <sample id="226">The provided text does not mention the number of authors involved in the article. It only mentions Regina Stodden as the presenter.</sample>
    <sample id="227">The research addresses the challenge of grounded language understanding, where natural language is mapped to executable plans or programs in a specific environment. Current language models struggle with this due to a lack of grounding during pre-training, leading to generated plans that may be invalid or grammatically incorrect.

The proposed framework, named Pangu, shifts the focus from generation to discrimination. It utilizes a symbolic agent to interact with the environment and propose candidate plans, while a language model scores and ranks these candidates. This approach avoids the language model's responsibility for plan validity and grammar.

Experiments with BERT, T5, and Codex demonstrate Pangu's strong performance across fine-tuning and in-context learning settings, achieving high accuracy and sample efficiency. Pangu outperforms baseline models like ArcaneQA, particularly in in-context learning with Codex.  A key finding is Pangu's robustness to non-i.i.d. data, suggesting it generalizes better than autoregressive models.

The core takeaway is that discrimination, rather than generation, is a more effective strategy for language models in grounded language understanding. This framework offers a promising path towards building more reliable and adaptable AI systems capable of executing natural language instructions in real-world scenarios.</sample>
    <sample id="228">AG News, MIND, SST2, and Enron Spam.</sample>
    <sample id="229">Gabriella Skitalinskaya and Henning Wachsmuth present a work on detecting improvable claims in argumentative writing, aiming to assist novice writers. The paper addresses the question of when a claim is well-phrased and requires revision, focusing specifically on argumentative text. They introduce two tasks: Suboptimal Claim Detection (identifying claims needing revision) and Claim Improvement Suggestion (selecting quality issues for revision).

The authors explore the challenges of using revision-based data, which differs from directly learning from human revision patterns. They focus on argumentative text, leveraging revision histories from collaborative online debate platforms like Kialo.  Four key challenges are identified: Representativity and Reliability (ensuring the dataset accurately reflects high-quality claims), Model Complexity and Architecture (selecting models sensitive to small changes in claims), Contextual Information (determining relevant contextual factors for quality assessment), and Topical and User Bias (addressing noise and biases in revision histories).

The paper details how they tackle these challenges and presents a systematic comparison of approaches for the two tasks. Their experiments suggest that revision-based data is effective for claim assessment, modeling the distance between claim versions is helpful for detecting suboptimal claims, and contextual information's impact depends on the task and quality issues. The authors invite readers to consult their paper for a detailed analysis of their strategies and findings.</sample>
    <sample id="231">NACHOS è un dataset di dati medici estratti dal web.</sample>
    <sample id="232">David Vilar.</sample>
    <sample id="233">Simultaneous Speech Translation (SimuST) aims to translate spoken language to text in real-time. Current SimuST models face challenges with long training procedures, optimization objectives, and maintaining multiple models for different latency levels.

The proposed solution, EDAtt (Encoder-Decoder Attention), addresses these issues by leveraging existing offline ST models without retraining or custom architectures. It utilizes a single model for all latency regimes, controlling latency through parameters. EDAtt employs cross-attention to decide whether to emit a partial translation. A word is emitted if its cross-attention weights are below a threshold, indicating stable information. This allows the system to wait for more stable speech chunks before emitting a word.

Experiments on German demonstrate that EDAtt outperforms existing strategies like Wait-k and Local Agreement, achieving higher BLEU scores and lower average lagging times.  The computational-aware average lagging shows EDAtt as the fastest strategy. The paper also includes open-source code and models to promote reproducibility.  The goal is to improve the quality and speed of real-time speech translation by intelligently managing the translation process based on attention patterns.</sample>
    <sample id="234">La strategia di prompting ha un impatto significativo sulla performance, con differenze che possono variare da pochi a decine di BLEURT points, specialmente per i prompting zero e one-shot. Per i prompting più numerosi, la forma esatta del prompt ha un impatto minore, e la qualità degli esempi (le traduzioni di alta qualità) è più importante della loro somiglianza con la frase originale.</sample>
    <sample id="235">Patrick Fernandes, Emmy Liu, André F. T. Martins, Graham Neubig.</sample>
    <sample id="236">```
We present MultiInstruct, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source dataset and each task is equipped with five expert written instructions.
```</sample>
    <sample id="237">They propose a diagnostic test suite for knowledge integration, introducing a coreference resolution task designed to probe for the ability to draw on knowledge available in different sources. They define three settings of KITMUS: "Background-Pretrain," "Background-Both," and "Background-Inference," varying the availability of pretrain-time and inference-time knowledge.</sample>
    <sample id="238">MeetingBank è un nuovo dataset per la creazione di sistemi di summarization di meeting, creato dalla University of Central Florida. Il dataset contiene trascrizioni di riunioni del consiglio comunale, riassunti di riferimento e altre risorse utili. La creazione del dataset ha affrontato le sfide di ottenere riassunti di alta qualità e risorse affidabili per le riunioni pubbliche.

Il processo di raccolta dati include la conversione audio in trascrizioni tramite Speechmatics API, l'identificazione del tipo di riunione, la localizzazione dei riassunti di riferimento e l'estrazione dei segmenti di meeting con timestamp. Il dataset include 1.366 riunioni e quasi 7.000 istanze, con statistiche dettagliate su durata, numero di token, numero di oratori e periodo di raccolta.

L'analisi del dataset rivela che i riassunti di maggior successo tendono a includere punti chiave (coverage) e a essere composti da riferimenti (density). Sono stati valutati diversi modelli di summarization, tra cui sistemi estrattivi (Oracle, LEAD, LexRank, TextRank) e modelli astrattivi (BART-Large, Pagasus, Longformer, DialogLM, HMNet).

I risultati della valutazione mostrano che i sistemi estrattivi come Extr-Oracle ottengono buoni risultati, mentre DialogLM si distingue per la sua capacità di generare riassunti lunghi. GPT-3, pur non performando bene con le metriche automatiche, ottiene i punteggi più alti nella valutazione umana per fluidità e coerenza, ma con risultati inferiori in termini di informatività e accuratezza.

MeetingBank è un prezioso strumento per la ricerca e offre spunti interessanti sul processo decisionale dei consigli comunali.</sample>
    <sample id="239">Buongiorno a tutti, mi chiamo David Vilar e presenterò una breve revisione del paper "Prompting PaLM for Translation: Assessing Strategies and Performance". Questo è un lavoro congiunto con i miei colleghi di Google Translate. PaLM è un modello linguistico di grandi dimensioni con 540 miliardi di parametri presentato l'anno scorso, nel 2022. È stato addestrato su una vasta collezione di testo, che comprende 780 miliardi di token. Al momento della pubblicazione, ha raggiunto prestazioni all'avanguardia in centinaia di attività NLP. In questo lavoro, presentiamo il primo studio sistematico sull'utilizzo di modelli linguistici di grandi dimensioni per la traduzione automatica. Abbiamo valutato la capacità di transizione di tali modelli utilizzando le migliori pratiche della comunità MT. Ciò comporta l'utilizzo dei test set più recenti per evitare sovrapposizioni tra i dati di test e i dati di addestramento del modello linguistico. E abbiamo confrontato con i sistemi all'avanguardia, ovvero il sistema WMT di riferimento. Utilizziamo metriche di traduzione MT neurali all'avanguardia e, inoltre, mostriamo risultati di valutazione umana basati su esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione dei prompt. Il prompting ha un grande impatto sulle prestazioni dei LLM per la traduzione, come possiamo vedere in un semplice esperimento, in cui abbiamo utilizzato il prompting one-shot e fornito due prompt diversi per ogni frase. La maggior parte delle frasi, 516 su 1.000, mostra una differenza superiore a un punto BLEURT. E in casi estremi, questa differenza può arrivare fino a 40 punti BLEURT. Quindi, è importante selezionare una buona strategia di prompting. Nei nostri esperimenti, ci siamo accontentati di una strategia di prompting a 5-shot, in cui abbiamo semplicemente contrassegnato ogni frase che forniamo al sistema con la lingua in cui è scritta. In questo esempio qui, dove effettuiamo la traduzione dal tedesco all'inglese, le frasi tedesche, le frasi sorgente, sono contrassegnate con il colon tedesco e le traduzioni in inglese sono contrassegnate con il colon inglese. Abbiamo osservato che la forma effettiva del prompting non ha un grande impatto nel caso di pochi prompt. È cruciale per il prompting one-shot e zero-shot. E quando passiamo, come nel nostro caso, al prompting a 5-shot, c'è quasi nessuna differenza nella forma effettiva del prompting. Sono i campioni che hanno più peso. Il riepilogo dei nostri risultati sperimentali è che la qualità dei campioni è più importante della somiglianza con la frase sorgente. Quindi, è importante selezionare i campioni da traduzioni di alta qualità. In particolare, confrontiamo la selezione dei prompt dai dati di addestramento per le valutazioni WMT sui dati di sviluppo. I dati di sviluppo sono molto più curati e di qualità superiore rispetto ai dati di addestramento, quindi sono più rumorosi. E i loro risultati mostrano una migliore prestazione quando si utilizzano i dati di sviluppo. Tuttavia, i sistemi all'avanguardia specializzati hanno un vantaggio significativo rispetto alle traduzioni PaLM. Ma PaLM si avvicina abbastanza a un sistema commerciale. Nel nostro caso, abbiamo scelto di valutare con Google Translate. Gli approfondimenti che abbiamo ottenuto dalla valutazione umana che abbiamo condotto utilizzando il framework MQM hanno mostrato che la fluidità di PaLM è comparabile ai sistemi all'avanguardia, ma la principale differenza deriva dall'accuratezza. In particolare, gli errori più comuni sono gli errori di omissione. Sembra che PaLM scelga di produrre una traduzione più fluida, a volte eliminando parti della frase sorgente che sono state tradotte. Tuttavia, la categoria "Stile/Inadeguato" per PaLM è inferiore rispetto ai sistemi all'avanguardia, il che è un ulteriore segnale che PaLM fornisce output davvero fluenti, ma con alcuni problemi di accuratezza. E questo è tutto per questa breve panoramica. Per maggiori dettagli, vi invitiamo a partecipare alla presentazione completa del paper. Grazie mille.</sample>
    <sample id="240">Ciao, sono Dawei, dottorando presso l'Università di Saarland in Germania. In questo video, vorrei presentare il nostro recente lavoro "Weaker Than You Think: A Critical Look at Weakly Supervised Learning". Questo è un lavoro congiunto con Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow. Vorrei iniziare con una breve introduzione al weakly supervised learning e al learning supervisionato debole. Nel learning supervisionato debole, non si etichettano i dati manualmente. Invece, etichettiamo i dati utilizzando fonti di etichette deboli, come semplici regoleuristiche, knowledge base o crowdsourcing a bassa qualità, come illustrato nella figura sulla destra. Rispetto alle annotazioni umane, le annotazioni più deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità delle annotazioni è errata. Quando si addestrano direttamente le reti neurali su dati etichettati debolmente, le reti neurali tendono a memorizzare il rumore delle etichette e non generalizzano. Nel learning supervisionato debole, vengono proposte tecniche di addestramento che consentono alle reti neurali di generalizzare bene sotto il rumore delle etichette. Negli ultimi lavori di WSL, WSL sta per Weakly Supervised Learning, una comune affermazione è che si addestrano i modelli solo sui dati etichettati debolmente e si ottengono prestazioni elevate sui set di test puliti. Tecnicamente, questa affermazione non è sbagliata, ma c'è un intoppo, ovvero si assume che esista un set di validazione pulito disponibile per la selezione del modello. Non possiamo fermarci a questo problema, ma questo implica che sono necessarie ulteriori annotazioni manuali nel learning supervisionato debole. Ma come un elefante in una stanza, questa necessità viene spesso trascurata. La suddetta incertezza pone tre domande di ricerca. Prima, i dati di validazione puliti sono necessari per il WSL o possiamo usare un set di validazione rumoroso? Secondo, se sono richiesti dati puliti, o se i dati puliti sono essenziali per il funzionamento del WSL, quanti campioni puliti dobbiamo avere? Infine, dovremmo utilizzare solo i campioni puliti per la validazione, o ci sono modi migliori per utilizzarli? Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti. Innanzitutto, scopriamo che i metodi WSL recenti richiedono effettivamente set di validazione puliti per funzionare correttamente. Altrimenti, c'è un grande calo delle prestazioni. Come mostra questa figura, se non ci sono set di validazione puliti, i modelli addestrati non possono generalizzare oltre le etichette deboli originali, il che rende l'addestramento inutile. Ciò indica che gli approcci WSL richiedono effettivamente dati etichettati puliti per funzionare correttamente e il costo di annotazione per ottenere set di validazione puliti non dovrebbe essere trascurato. Il nostro secondo risultato è che aumentare il numero di campioni di validazione puliti aiuta gli approcci WSL a ottenere prestazioni migliori, come mostra la figura sulla sinistra. Tipicamente, ne abbiamo solo bisogno di 20 campioni per classe per ottenere prestazioni elevate. Ma non è finita qui, perché se decidiamo di accedere a dati puliti, l'addestramento diretto su di essi raggiungerà persino prestazioni migliori. La figura sulla destra mostra la differenza di prestazioni tra gli approcci di fine-tuning e gli approcci WSL, che utilizzano i dati puliti per la validazione. Come possiamo vedere, se abbiamo 10 campioni per classe, il fine-tuning diretto inizia a superare gli approcci WSL. Infine, l'aumento delle prestazioni dichiarato nei precedenti approcci WSL può essere facilmente ottenuto consentendo di continuare il fine-tuning sui set di validazione puliti. Come possiamo vedere dalle figure, il modello vanilla, denominato FTw, inizialmente sottoperforma rispetto agli approcci WSL più complessi, come COSINE. Tuttavia, se consentiamo di continuare il fine-tuning sui campioni puliti, allora FTw raggiunge prestazioni equivalenti ad altri metodi. Quindi, in pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio su disco. In sintesi, abbiamo dimostrato che gli approcci WSL recenti richiedono dati manualmente annotati puliti per funzionare correttamente. Il loro guadagno di prestazioni e la loro praticità sono fortemente sovrastimati. Le nostre raccomandazioni concrete per i lavori futuri sono le seguenti. In primo luogo, segnalare i criteri di selezione del modello. Ad esempio, segnalare se la selezione del modello è effettuata tramite set di validazione puliti. In secondo luogo, gli approcci WSL dovrebbero essere confrontati con le baseline di few-shot learning, poiché entrambi operano su dati puliti. In terzo luogo, il fine-tuning continuo è una baseline semplice ma potente che dovrebbe essere presa in considerazione nei lavori futuri di WSL. Infine, abbiamo open-sourced il nostro codice. Puoi trovarlo tramite il codice QR su questa slide. Ti invito a dare un'occhiata. Grazie e goditi la conferenza.</sample>
    <sample id="241">## Abstract

This paper addresses the limitations of current automated misinformation detection systems, which often suffer from unrealistic evaluation methods and a lack of human-centric design.  Existing systems frequently rely on retrospective datasets and are vulnerable to leaked counter-evidence, failing to capture the dynamic nature of misinformation spread. Furthermore, they often neglect the crucial role of human content moderators, either excluding them entirely or relegating them to the final verification step.

We propose a novel evaluation framework for developing more effective and human-aligned misinformation detection systems. Our framework emphasizes an end-to-end approach, integrating human feedback throughout the process, from raw tweet analysis to actionable outputs for human review. We concretely implement and evaluate this framework using a case study of COVID-19 treatment misinformation.

Our system comprises two main components: claim extraction, leveraging a T5 model to identify and rank potentially misleading claims, and policy violation verification, utilizing a BERT-based stance classification model to flag tweets violating Twitter's policies.  We operationalize early detection as the identification of unapproved treatments before their public debunking.  Evaluation reveals a 65% accuracy in policy violation detection and a significant increase in confirmed policy violations per human hour compared to traditional methods.

This work provides a more realistic and human-centric approach to evaluating misinformation detection systems, offering valuable insights for future research and development in this critical area.  It highlights the importance of integrating human expertise into the loop to effectively combat the spread of misinformation.</sample>
    <sample id="242">I valutazione comune per i sistemi di dialogo include l'utilizzo di giudici umani per selezionare quale tra due conversazioni è migliore o per valutare le conversazioni su una scala Likert.</sample>
    <sample id="243">Sebastian Santy, Ronan Le Bras, Katharina Reinecke, Maarten Sap.</sample>
    <sample id="244">Servin è un giudice.</sample>
    <sample id="245">Lining Zhang presenta il loro lavoro "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization". L'obiettivo è migliorare la qualità delle annotazioni su MTurk, spesso problematica per i metodi automatici e la scarsa comprensione delle migliori pratiche di reclutamento.

Il loro pipeline di due fasi qualifica i lavoratori: una fase iniziale con domande di valutazione di diverse dimensioni e una fase successiva con un carico di lavoro più intenso. Questo processo identifica "gold", "silver", "bronze" e "block" workers, con solo i "gold" e "silver" che superano le fasi di qualifica.

I risultati mostrano un'alta inter-annotator agreement (IAA) tra i lavoratori qualificati, superiore a quella degli esperti.  L'analisi delle prestazioni sui task di riferimento rivela una correlazione tra i lavoratori del pipeline e quelli di CloudResearch, ma con la necessità di garantire la corretta formazione.

Il pipeline offre un approccio efficiente per reclutare annotatori ad alta qualità, con un costo inferiore rispetto a CloudResearch, pur raggiungendo un livello di accuratezza simile.  Le limitazioni includono la focalizzazione sull'inglese su MTurk e la mancanza di garanzia di corretta formazione.  Prossimi passi includono l'esplorazione di metodi per migliorare sia l'accordo che l'accuratezza, e l'applicazione del pipeline a diverse lingue e piattaforme.</sample>
    <sample id="246">Sì, il codice e il dataset sono disponibili su GitHub.</sample>
    <sample id="247">Jiho Kim from KAIST AI presents their paper "FACTKG: Fact Verification via Reasoning on Knowledge Graphs." The paper introduces FactKG, a new dataset for fact verification that leverages knowledge graphs (KGs) instead of traditional text or table-based evidence.  This approach offers advantages in reliability and practicality, particularly for applications like dialogue systems.

FactKG utilizes DBpedia as the KG and contains claims in both written and colloquial styles, labeled as SUPPORTED or REFUTED. The task involves retrieving evidence from the KG and verifying the claim through reasoning, encompassing five types: one-hop, conjunction, existence, multi-hop, and negation.

The dataset includes examples requiring different reasoning steps, such as checking relationships between entities (one-hop), verifying multiple related facts (conjunction), and inferring connections across the KG (multi-hop).  Negation requires additional inference to determine the opposite of a fact.

The paper explores two methods for generating colloquial claims: a colloquial style transfer model and presupposition templates.  The authors also present baseline models, including claim-only baselines and a GEAR model utilizing graph evidence.  The results demonstrate that the GEAR model, which leverages KGs, significantly outperforms all other baselines, achieving a verification accuracy exceeding 50%. The paper concludes by offering the dataset for download and encouraging contact for further inquiries.</sample>
    <sample id="248">```
No, the annotators for NLPositionality are not perfectly balanced across all demographic groups. The study found that datasets and models are most aligned with English-speaking countries and individuals with a college education, while being less aligned with non-binary people.
```</sample>
    <sample id="249">Non sono state perturbate le frasi nel dominio accettabile. L'analisi ha dimostrato che le perturbazioni delle frasi nel dominio accettabile hanno prodotto risultati simili alle perturbazioni delle frasi nel dominio inaccettabile.</sample>
    <sample id="250">Una valutazione dimensionale di un modello di conversazione AI significa valutare il modello su diversi aspetti della qualità della conversazione, come la rilevanza delle risposte, la coerenza, l'empatia e la capacità di evitare errori comuni. Questo approccio mira a fornire una comprensione più approfondita dei punti di forza e di debolezza del modello rispetto alle valutazioni tradizionali basate su giudizi umani o su scale Likert.</sample>
    <sample id="251">Jingwei Yi è affiliato all'University of Science and Technology of China.</sample>
    <sample id="252">Sai Kiran Tanikella, uno studente di master all'IIT Kanpur, presenta il lavoro "U-CREAT: Unsupervised Case Retrieval using Events extraction". Il lavoro affronta la sfida della ricerca di casi precedenti rilevanti in ambito legale, un compito cruciale per avvocati e giudici.

Il team ha sviluppato due contributi principali: il dataset IL-PCR, un nuovo benchmark per la ricerca di casi precedenti con 7.070 casi e un'alta percentuale di citazioni, e il pipeline U-CREAT, un approccio basato sull'estrazione di eventi per la ricerca di casi. U-CREAT utilizza tecniche di apprendimento non supervisionato e un approccio basato sugli eventi per migliorare l'efficienza e la generalizzabilità rispetto ai modelli tradizionali.

Il pipeline U-CREAT estrae eventi dalle query e dai documenti candidati, creando una matrice di interazione tra eventi comuni. Vengono valutati diversi modelli, tra cui modelli basati su parole, modelli transformer e modelli basati sugli eventi. I modelli basati sugli eventi, in particolare il modello "Event Filtered Documents", si dimostrano significativamente superiori rispetto ai modelli tradizionali, offrendo tempi di inferenza inferiori e un'alta accuratezza.

U-CREAT supera i metodi esistenti, inclusi i modelli transformer più recenti, e si presenta come un'evoluzione promettente per la ricerca di casi precedenti, con potenziali applicazioni in diversi sistemi legali.</sample>
    <sample id="253">DisorBERT is a new model developed by a team from Mexico and Spain to detect signs of mental disorders in social media posts. The research addresses the challenge of insufficient annotated data by leveraging domain adaptation, using a pre-trained language model like BERT and fine-tuning it with data from Reddit and mental health resources. This allows DisorBERT to better understand the specific language used in discussions about mental health.

The model employs guided masking, a technique that encourages the model to focus on important words during training, leading to more accurate predictions.  Experiments on the eRisk dataset show that DisorBERT achieves a good balance between precision and recall, outperforming baseline models like BERT.  Analysis of the model's predictions reveals a tendency to generate words related to mental health issues, such as "focus," "talk," and "sleep," compared to BERT's more general responses.

Visualization tools highlight the importance of words like "anxious" and "medication" in identifying depression.  The study concludes that the combination of double domain adaptation and guided masking is effective for detecting mental disorders in social media.  Future work will explore incorporating different lexical resources and clinical data to further improve the model's performance.</sample>
    <sample id="254">Sun Qi from Nanjing University of Science and Technology presents research on "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction." The research addresses the challenge of noise in distant supervision (DS) data for document-level relation extraction, which relies on large human annotations.  Current methods using pseudo-labels are susceptible to false positives, leading to incorrect relation extraction.

The proposed framework employs uncertainty-guided label denoising to improve label quality. It first trains a pre-denoising DocRE model on DS and human-annotated data to generate pseudo-labels.  Uncertainty estimation is then used to determine the trustworthiness of model predictions, particularly for overlapping relations.  A novel instance-level uncertainty estimation method captures uncertainty scores for each relation class.  A re-labeling strategy with dynamic class uncertainty thresholds filters out low-confidence pseudo-labels.  Finally, a multi-phase training strategy iteratively refines the DS data.

The research introduces Monte Carlo dropout for uncertainty modeling in the DocRE task, addressing limitations of previous methods that didn't handle overlapping relations effectively.  The study demonstrates improved performance compared to existing baselines on public datasets.  Key contributions include the uncertainty-guided denoising framework, instance-level uncertainty estimation for overlapping relations, dynamic class uncertainty thresholds, and significant performance gains.</sample>
    <sample id="255">La forma del prompting è importante solo per zero e one-shot prompting.</sample>
    <sample id="257">Gli autori hanno valutato quattro modelli di dialogo state-of-the-art.</sample>
    <sample id="258">Chiang Cheng-Han presenta il suo lavoro "Can Large Language Models Be an Alternative to Human Evaluation?". L'articolo propone di utilizzare i modelli linguistici di grandi dimensioni (LLM) per valutare la qualità del testo in ambito NLP, fornendo loro istruzioni e campioni da valutare. L'idea è che, se gli LLM comprendono le istruzioni, possano fornire valutazioni significative.

Pur riconoscendo l'esistenza di lavori simili, l'originalità del loro approccio risiede nel primo tentativo di utilizzare LLM per la valutazione, un'area allora inesplorata. La motivazione è la fragilità e l'imprevedibilità delle valutazioni umane.

L'esperimento consiste nel valutare storie generate da GPT-2 e da umani, utilizzando quattro attributi: grammatica, coerenza, gradevolezza e rilevanza.  Vengono utilizzati diversi LLM (T0, InstructGPT, ChatGPT) e si confrontano le valutazioni degli LLM con quelle degli esperti (insegnanti di inglese).

I risultati indicano che, in generale, gli insegnanti preferiscono le storie scritte a mano, ma alcuni LLM (Davinci e ChatGPT) mostrano una preferenza più marcata. L'articolo affronta anche domande sulla coerenza tra LLM e valutatori umani, sull'impatto delle istruzioni e del campionamento, sui costi e benefici rispetto alla valutazione umana, e sull'applicabilità a diverse task.  Chiang invita a leggere il paper o a visitare il loro stand all'ACL.</sample>
    <sample id="259">Yusen Zhang from Penn State University presents "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations." The work addresses the lack of comprehensive benchmarks for cross-lingual semantic parsing, particularly for Chinese and Lambda Calculus.

XSemPLR provides a unified dataset containing 9 datasets across various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 languages.  The study explores six evaluation settings: Translate-Test (using Google Translate), Monolingual, Monolingual Few-shot, Multilingual, Cross-lingual Zero-shot, and Cross-lingual Few-shot transfer.

The research compares monolingual models (XLM-R + PTR, mBERT + PTR) and Encoder-Decoder models (mBART, mT5). Encoder-Decoder models achieve the best performance across all datasets. Training on a mixture of languages improves performance, but the "Curse of Multilinguality" causes English performance to drop in some cases.

Cross-lingual transfer performance gaps are significant in zero-shot settings but narrow with few-shot learning. The study finds that pretraining on English significantly boosts few-shot performance on target languages.  Multilingual models like Codex and BLOOM are found to be inadequate for cross-lingual semantic parsing.

In conclusion, XSemPLR offers a valuable benchmark for cross-lingual semantic parsing, and the study highlights the strengths and limitations of different multilingual language models. The paper is available for review and code access.</sample>
    <sample id="260">Un autore.</sample>
    <sample id="261">Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli.</sample>
    <sample id="262">L'articolo è stato scritto da Siyu Yuan e altri.</sample>
    <sample id="263">"Mitigating Label Biases for In-context Learning" affronta la questione della stabilità e dei bias negli in-context learning, una tecnica popolare per sfruttare i grandi modelli linguistici. La ricerca identifica tre tipi di bias: vanilla-label bias (preferenza per i nomi delle etichette), context-label bias (influenza del contesto) e, in nuova scoperta, domain-label bias (effetto del corpus di addestramento sulla previsione).

Gli esperimenti dimostrano che il domain-label bias può compromettere significativamente le prestazioni, specialmente in task con un forte bias di dominio. Per mitigare questi bias, viene proposto il "domain-context calibration". Questo metodo utilizza parole casuali dal corpus di addestramento come "content-free text" per stimare e correggere le previsioni del modello, superando i limiti dei metodi di calibrazione precedenti che utilizzano token predefiniti.

I risultati mostrano che il domain-context calibration migliora significativamente le prestazioni in in-context learning, soprattutto in task con un elevato domain-label bias. L'analisi delle distribuzioni di previsione rivela che la calibrazione porta a decision boundaries più accurate.  La ricerca evidenzia che i token predefiniti possono essere anch'essi soggetti a bias e che l'utilizzo di più parole casuali e, in particolare, di parole casuali dal dominio, porta a risultati migliori. In sintesi, il lavoro offre un approccio sistematico per comprendere e mitigare i bias nei modelli linguistici durante l'in-context learning.</sample>
    <sample id="264">Lin Wang, una studentessa di dottorato presso la Zhejiang University, presenta il suo lavoro "TAVT: Towards Transferable Audio-Visual Text Generation". Il suo studio affronta la sfida della generazione di testo multimodale audio-visiva, dove l'annotazione dei dati è costosa e le soluzioni esistenti soffrono di scarsa generalizzazione tra domini diversi.

La proposta TAVT introduce un nuovo task e si concentra sui cambiamenti multimodali, come lo stile visivo e l'energia audio, che possono influenzare la comprensione degli eventi. L'idea chiave è utilizzare uno spazio semantico audio unificato per allineare i concetti visivi tra domini.

Il framework TAVT è modulare e comprende: un network meta-mapper per mappare i concetti visivi in uno spazio semantico audio unificato; un encoder audio-visivo e un generatore di linguaggio; e un apprendimento contrastivo controfattuale.  Il meta-mapper utilizza l'apprendimento di token visivi per creare un audio semantico unificato. L'encoder e il generatore utilizzano un meccanismo alpha per valutare il contributo di ciascuna modalità.

Il lavoro presenta risultati promettenti, dimostrando che TAVT supera i modelli state-of-the-art in cross-dataset e cross-domain, specialmente in domini a bassa risorse.  Gli esperimenti dimostrano la capacità di TAVT di adattarsi rapidamente a nuovi domini con dati limitati.</sample>
    <sample id="265">Vasudha</sample>
    <sample id="266">This text doesn't explicitly state the affiliations of the authors. It only mentions Adam Przepiórkowski as the speaker.</sample>
    <sample id="268">The most common errors PaLM makes are omission errors, where it drops parts of the source sentence during translation.</sample>
    <sample id="269">Hello, I'm James Finch. And I'm Sarah Finch. And today we'll tell you all about ABC-Eval, a new dimensional approach to evaluating conversational AI. This work was done by the Emory NLP Lab led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI. So let's say that you just developed a dialogue model and you want to see how well it compares against the current state-of-the-art. The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale. These approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level. One approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses using existing comparative or Likert scale methods. However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation. Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. We call this approach annotating behaviors in chat or ABC-Eval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature. ABC-Eval is capable of measuring the rates at which chat models will commit various thematic errors. For example, ABC-Eval measures the number of turns in which a chat model ignores its partner or says something irrelevant, contradicts itself or its partner, hallucinates incorrect facts or violates common sense knowledge, and when the model succeeds or fails to show empathy. To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC-Eval. For comparison, we also evaluated these conversations using three existing methods: Likert ratings on the turn-level, Likert ratings on the dialogue-level, and dialogue-level pairwise comparisons. For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue, since this is the standard practice for evaluating chat models along multiple dimensions. From our analysis of these evaluation results, we found that ABC-Eval behavior labels are overall more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly-labeled conversations. In addition, ABC-Eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis. For example, you can see how measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality, respectively, while the average Likert consistency scores explain only 4% or less. Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression. You can see how the combination of all ABC-Eval metrics explains over 25% of conversation quality, and as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality. On the other hand, the combination of all turn-level Likert metrics explains far less of the quality, and fewer of these metrics carry unique information. These reliable, informative, and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve. You can see that in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common sense violations in around 20% of their responses. They produce irrelevant information in around 15% of the responses, and they contradict themselves or their partner around 10% of the time. With the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted. However, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models. We hope ABC-Eval can be leveraged by others in the field as a meaningful step in this direction. And we look forward to seeing how conversational AI will advance in the coming months and years. Thank you for watching.</sample>
    <sample id="270">The authors of the article are from the Emory NLP Lab led by Professor Jinho Choi at Emory University, in collaboration with Amazon Alexa AI.</sample>
    <sample id="271">CFT stands for Fine-tuning.</sample>
    <sample id="272">John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams.</sample>
    <sample id="273">Ciao, mi chiamo Kayo Yin e presenterò il nostro lavoro intitolato "Quando la traduzione richiede contesto? Un'esplorazione basata sui dati, multilingue". Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig. Quindi, molte traduzioni dipendono dal contesto. Ad esempio, come tradurremmo "mole" in questa frase? Beh, se la frase precedente era "Potrebbe iniziare a diventare pericoloso se i ministri scoprissero", allora "mole" si riferisce a uno spia. Ma se la frase precedente era "Potrebbe essere qualcosa di serio, dottore?", allora "mole" si riferisce a una macchia di nascita. Quindi, a seconda del contesto, il significato della parola cambia e quindi anche la traduzione. Tuttavia, valutare quanto bene i modelli possono tradurre casi del genere è piuttosto difficile. Innanzitutto, solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus come BLEU incapaci di catturare queste traduzioni. E alcune persone hanno suggerito valutazioni mirate sulle traduzioni dipendenti dal contesto, ma questi risorse supportano solo un numero limitato di tipi di traduzioni dipendenti dal contesto e un numero limitato di lingue poiché in genere si basano sulla conoscenza del dominio e sulla curazione umana. In questo lavoro, cerchiamo di rispondere a due domande. Prima, quando la traduzione richiede contesto? E seconda, quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipende dal contesto durante la traduzione. In lavori precedenti, abbiamo introdotto CXMI come misura dell'utilizzo del contesto dai modelli di traduzione automatica. Questo è fatto misurando quanto informazioni il contesto C fornisce sulla target Y, dato lo source X. Si può pensare a CXMI come l'informazione acquisita dal fornire contesto al modello. In questo lavoro, estendiamo CXMI a Pointwise CXMI, che può misurare l'utilizzo del contesto a livello di frase o a livello di parola. Si può pensare alle parole che hanno un alto P-CXMI come quelle che richiedono contesto per la traduzione. Ora analizziamo le parole con un alto P-CXMI per cercare modelli tra queste parole. E conduciamo il nostro analisi sui trascrizioni dei discorsi TED tradotti dall'inglese in 14 lingue diverse. Conduciamo il nostro analisi a tre diversi livelli. Innanzitutto, esaminiamo i tag di parte del discorso che hanno un alto P-CXMI. Questo ci permette di trovare, ad esempio, pronomi duali in arabo che hanno relativamente un alto P-CXMI. Questo può essere spiegato perché l'inglese non ha pronomi duali, quindi è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo. Allo stesso modo, troviamo che alcune lingue richiedono contesto quando vogliamo scegliere la forma verbale appropriata. Quindi, esaminiamo anche gli elementi del vocabolario che hanno un alto P-CXMI mediato su tutte le loro diverse occorrenze. Questo aiuta a identificare casi come quello qui, in cinese è necessario il contesto per tradurre i nomi propri per assicurarsi di utilizzare la stessa traduzione all'interno del documento. Allo stesso modo, troviamo che il contesto è importante per tradurre in la forma di cortesia appropriata. Infine, esaminiamo diversi token individuali che hanno un alto P-CXMI. Questo ci permette di identificare fenomeni che non possono essere catturati dalla parola stessa, ma piuttosto espressi dalla struttura della frase, come la risoluzione degli ellissi. Ora utilizziamo i nostri risultati dall'analisi per progettare un benchmark per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni discorsivi identificati, creiamo tagger per identificare automaticamente le parole che si riferiscono a tale fenomeno. E abbiamo chiamato il nostro tagger MuDA (Multilingual Discourse-Aware). Possiamo anche notare che diverse lingue hanno diverse proporzioni di questi fenomeni discorsivi. Quindi, utilizziamo il tagger MuDA, applicando il tagger su un corpus parallelo che vogliamo utilizzare per la valutazione e applichiamo le nostre metriche di traduzione di scelta sui esempi dipendenti dal contesto che il tagger MuDA ha identificato. Infine, utilizziamo il nostro benchmark così come altre metriche per valutare diversi modelli di traduzione a livello di documento. Innanzitutto, quando utilizziamo metriche a livello di corpus: quindi per BLEU, troviamo che i modelli context-agnostic hanno le migliori prestazioni. Ma poi, se utilizziamo COMET, i modelli context-aware hanno le migliori prestazioni. E se utilizziamo word f-measure, allora i modelli con e senza contesto hanno prestazioni comparabili. Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se si utilizzano metriche a livello di corpus da soli. Ora, utilizziamo il benchmark MuDA per valutare i modelli e troviamo che i modelli context-aware sono significativamente più accurati dei modelli che non utilizzano il contesto per determinati fenomeni discorsivi come la formalità e la coesione lessicale. Tuttavia, questi modelli non sono molto migliori dei modelli che non utilizzano il contesto su altri fenomeni come pronomi, forma verbale ed ellissi. Questo suggerisce dove dovremmo vedere più progressi per la traduzione a livello di documento. Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che DeepL è in genere più accurato di Google Translate per la traduzione a livello di documento. Per riassumere, conduciamo un'analisi basata sui dati su 14 coppie di lingue per identificare quando la traduzione richiede contesto e quindi utilizziamo i nostri risultati per costruire un benchmark per la traduzione a livello di documento che può aiutarci a identificare quali fenomeni discorsivi i modelli possono gestire bene o male e quali sistemi di traduzione sono bravi nella traduzione a livello di documento. Grazie mille per la vostra attenzione. Ci vediamo a Toronto.</sample>
    <sample id="274">Yusen Zhang.</sample>
    <sample id="276">Ananya and Vignesh present "IndicMT Eval," a dataset for meta-evaluating machine translation metrics for Indian languages.  They address the understudied area of evaluating translations *into* Indian languages, highlighting the unique linguistic characteristics of these languages compared to English.

Their work involves translating 200 sentences from the Flores dataset into English using seven translation models, generating 1,400 candidate translations per language, resulting in 7,000 samples.  These translations are then evaluated by bilingual expert annotators who provide detailed error annotations, categorized by MQM error types (accuracy, fluency, special errors) and severity.

The study analyzes the correlation between various metrics (chrF, LabSE embedding, BERTscore, MuRIL, COMET) and human scores across five Indian languages (Tamil, Malayalam, Hindi, Marathi, Gujarati).  They find that overlap-based metrics perform poorly, while embedding-based metrics show better correlations, particularly when using BERTscore with multilingual embeddings. COMET-metric variants demonstrate the highest overall correlations.

The research also investigates the impact of error type.  Metrics correlate better with human scores when focusing on accuracy errors.  Finally, they fine-tune the best-performing metric, COMET, using their annotated dataset, creating IndicCOMET MQM, which outperforms COMET baselines and demonstrates zero-shot translation capabilities on unseen languages.  IndicCOMET MQM also exhibits greater robustness on the ACES Translation Accuracy Challenge Sets. The dataset is publicly available for use.</sample>
    <sample id="277">Multiset Tagging and Latent Permutations.</sample>
    <sample id="278">Il metodo delle "parole contrassegnate" si basa sul concetto sociolinguistico di "contrassegnazione", che indica che i gruppi dominanti sono linguistically e socialmente non contrassegnati, mentre i gruppi emarginati sono linguistically e socialmente contrassegnati. Il metodo identifica le parole che distinguono i gruppi contrassegnati dai gruppi non contrassegnati e le utilizza per analizzare le descrizioni generate dai modelli linguistici.</sample>
    <sample id="279">Shangbin è uno studente di dottorato presso l'Università del Washington.</sample>
    <sample id="280">```
This paper introduces MultiEMO, a novel attention-based correlation-aware multimodal fusion framework for emotion recognition in conversations (ERC).  ERC aims to predict the emotion label of each utterance, leveraging textual, audio, and visual modalities. Existing methods often lack effective multimodal integration, struggle with minority emotion classes, and fail to distinguish semantically similar emotions.

MultiEMO addresses these challenges by proposing a novel visual feature extractor, VisExtNet, which focuses on facial expressions without redundant scene information.  It then employs a multimodal fusion model, MultiAttn, based on bidirectional multi-head cross-attention layers to effectively integrate textual, audio, and visual features.  The framework utilizes sample-weighted focal contrastive loss to improve performance on minority and similar emotion classes.

Experiments on MELD and IEMOCAP demonstrate that MultiEMO achieves state-of-the-art results, particularly in challenging scenarios involving asynchronous emotional cues and minority classes.  The proposed approach enhances the model's ability to capture subtle emotional nuances across modalities.  Limitations include VisExtNet's inability to distinguish between speakers and background individuals, the need for large batch sizes on MELD for the SWFC loss, and continued performance gaps in minority emotion recognition compared to majority classes.
```</sample>
    <sample id="281">Kayo Yin e il team hanno presentato il loro lavoro "When Does Translation Require Context?", che esplora quando e come il contesto influisce sulla traduzione automatica. L'obiettivo è superare le limitazioni delle metriche tradizionali come BLEU, che non catturano le traduzioni dipendenti dal contesto.

Il team ha esteso la misura CXMI per includere Pointwise CXMI, analizzando l'uso del contesto a livello di parola e frase in 14 lingue, utilizzando trascrizioni di TED Talks. Hanno identificato che parole con alto P-CXMI richiedono contesto per la traduzione, analizzando part-of-speech, vocabolario e token individuali.

Basandosi su queste analisi, hanno sviluppato il MuDA tagger, un tagger multilingue progettato per identificare fenomeni discorsivi dipendenti dal contesto. Hanno poi creato un benchmark per la traduzione documentale, valutando modelli di traduzione con e senza contesto utilizzando metriche come BLEU, COMET e word f-measure.

I risultati mostrano che i modelli context-aware sono più accurati per fenomeni come formalità e coesione lessicale, ma non necessariamente per altri come ellissi e pronomi. Il benchmark rivela che DeepL è spesso più accurato di Google Translate per la traduzione documentale. In sintesi, il lavoro identifica quando il contesto è cruciale per la traduzione e fornisce uno strumento per valutare le prestazioni dei modelli di traduzione documentale.</sample>
    <sample id="282">Xuekai Zhu's ACL 2023 paper introduces StoryTrans, a new model for non-parallel story author-style transfer. The work addresses the challenge of transferring writing styles at the story level, going beyond token or sentence-level approaches.  The primary difficulty lies in imitating author-specific linguistic preferences, particularly discourse structures and style-specific content, which are often associated with specific topics.

StoryTrans tackles these issues by learning discourse representations from source texts and combining them with learnable style embeddings. A novel training objective reduces stylistic features from discourse representations, aiming to align representations across texts.  The generation process is split into two stages: first, masking style-specific content keywords in the source text and reconstructing it; second, generating the complete text by incorporating these keywords.

The training framework utilizes self-reconstruction, disentanglement, sentence order, and style classifier losses in the first stage. The second stage focuses on filling in style-specific content and removing the mask.  Experiments on new datasets in Chinese and English demonstrate StoryTrans's effectiveness in style control and content preservation, outperforming baselines. Style visualization confirms alignment with golden text in the style feature space.  StoryTrans also demonstrates the ability to enrich storylines with relevant phrases and maintain source semantics. The paper's data and code are publicly available.</sample>
    <sample id="283">Dependency Structure of Coordination.</sample>
    <sample id="284">Peng Tianshuo from Wuhan University presented "FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction" at ACL Main Conference 4,915. The paper addresses the limitations of current span-based Universal Information Extraction (UIE) models, which heavily rely on precise span boundaries, often ambiguous in annotation.

FSUIE proposes a novel approach by introducing a "fuzzy span" mechanism. Instead of precise boundaries, the model learns a fuzzy span, representing the target boundary as a continuous probability distribution. This is achieved through a fuzzy span attention module that dynamically adjusts the attention span and linearly decays the attention distribution, rather than truncating it.

The model utilizes a fuzzy span loss, combining Binary Cross Entropy (BCE) with the KL-divergence between the predicted fuzzy span and the golden boundary, augmented by supplementary information.  Experiments on named entity recognition, relationship extraction, and aspect sentiment triplet extraction demonstrate significant performance improvements, especially on smaller datasets. FSUIE achieves state-of-the-art results on relationship extraction datasets like ACE2004 and AST-V2.

Ablation studies show that the fuzzy span attention improves convergence speed and enhances information extraction capability. Visualizations reveal that the module focuses on semantic information within a limited range of preceding tokens.  FSUIE's unified structure and strong generalization capabilities make it a promising solution for various information extraction tasks. The combined effect of fuzzy span loss and attention leads to superior performance.</sample>
    <sample id="285">Mingqi Gao from Peking University presents their work "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization." The research addresses the issue of factual errors in dialogue summaries, a problem that has been largely overlooked in the field of factual error correction (FEC).

The paper argues that current evaluation methods for FEC models are flawed. Existing metrics like FactCC and DAE provide overall scores, which are unreliable and don't differentiate between genuine error correction and simply generating a factually correct summary without addressing the original summary's content.  Furthermore, these metrics don't account for the importance of minimizing the number of edits (substitutions, insertions, deletions) to maintain fluency and avoid redundancy.

To address these issues, the researchers propose a new evaluation framework based on ERRANT, incorporating manual annotations of reference corrections. This provides more valuable data for training FEC models and allows for a more comprehensive assessment of their performance. They introduce a taxonomy of factual errors categorized as content-based (based on part-of-speech and dependencies) and form-based (based on edit operations).

Experiments show that training FEC models with reference summaries from dialogue datasets yields the best results when using unreliable metrics. The study highlights the need for improved evaluation methods and suggests combining human-annotated data with synthetic data as a promising approach.  The research also identifies limitations of current FEC models in correcting specific error types like additions and attribute errors.</sample>
    <sample id="286">Professor Jinho Choi.</sample>
    <sample id="287">Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="288">Here are the datasets that can be used to test syntactic phenomena:

*   BLiMP (Adjunct Island case)
*   SyntaxGym
*   Wikipedia</sample>
    <sample id="290">COSINE, FTw.</sample>
    <sample id="291">Il modello viene valutato su attività di riconoscimento di entità nominate, classificazione, tagging delle parti del discorso e question answering, utilizzando dati pubblici e privati.</sample>
    <sample id="294">CamemBERT viene inizialmente addestrato su dati NACHOS.</sample>
    <sample id="295">Adam Przepiórkowski.</sample>
    <sample id="296">Valerio Basile presenta un lavoro di collaborazione tra l'Università di Torino e Amazon Alexa incentrato sull'ironia nel Natural Language Understanding (NLU). Il progetto, chiamato EPIC (English Perspectivist Irony Corpus), mira a superare i limiti delle tradizionali approcci basati su dati annotati manualmente, che presuppongono una singola verità.

L'EPIC è un corpus di 300 conversazioni raccolte da social media (Reddit, Twitter) su un periodo di 1½ anni, annotato da 74 annotatori divisi in cinque varietà di inglese.  Gli annotatori hanno valutato se la risposta in una conversazione fosse ironica rispetto al contesto.

L'analisi ha rivelato differenze nell'accordo tra gli annotatori, influenzate da fattori come genere, età, nazionalità. Per affrontare queste variazioni, sono stati sviluppati modelli "perspective-aware", addestrati su sottoinsiemi di dati relativi a gruppi di annotatori specifici.

I modelli perspective-aware hanno dimostrato una maggiore confidenza nelle loro previsioni rispetto ai modelli tradizionali, mostrando meno incertezza.  L'analisi delle cause delle differenze di annotazione ha evidenziato discrepanze tra generazioni e aree geografiche, in particolare tra annotatori del Regno Unito e dell'Irlanda.  Il lavoro suggerisce che la diversità di prospettive degli annotatori può influenzare la capacità dei modelli di NLU di rilevare l'ironia.</sample>
    <sample id="297">The research project "From Dogwhistles to Bullhorns" investigates coded rhetoric, specifically dogwhistles – terms that convey a hidden meaning to an in-group while appearing innocuous to an out-group. The study analyzes how dogwhistles function in political discourse, particularly in the context of anti-Semitism and other forms of prejudice.

The project develops a typology and glossary of over 340 dogwhistle terms and symbols, categorized by register (formal/informal) and type (implicature vs. covert persona).  A case study of historical U.S. political speeches reveals a correlation between the use of racial dogwhistles and the Republican Southern Strategy.  

The research evaluates the ability of language models, like GPT-3, to recognize dogwhistles.  While GPT-3 can identify some dogwhistles, its performance varies significantly, especially with informal and transphobic terms.  Adding definitions and cues to prompts improves accuracy.

Finally, the project demonstrates how dogwhistles can evade content moderation by altering toxicity detection scores.  The study concludes that understanding dogwhistles is crucial for NLP and linguistics, as they challenge our understanding of meaning and can be used to spread hateful rhetoric. The research highlights the difficulty of detecting dogwhistles due to their context-dependent nature and the fact that researchers are often within the out-group.</sample>
    <sample id="298">Il grafico mostra che l'aumento della distanza temporale tra i dati di addestramento e di test porta a una diminuzione delle prestazioni.</sample>
    <sample id="299">Michalis Korakakis e Andreas Vlachos presentano un nuovo metodo per migliorare la robustezza dei modelli NLI (Natural Language Inference) con l'utilizzo di un approccio di training minimax. I modelli NLI hanno ottenuto risultati all'avanguardia, ma sono vulnerabili a "shortcut", ovvero correlazioni spurie tra gli attributi di input e le etichette, che si manifestano durante la creazione dei dataset.

I metodi di mitigazione dei shortcut esistenti spesso richiedono conoscenze specifiche del dominio o del dataset, e possono non essere efficaci perché il modello di apprendimento potrebbe non sfruttare gli stessi shortcut del modello ausiliario.

Il metodo proposto si basa sull'idea che i modelli NLI hanno difficoltà con esempi "difficili" sottorappresentati, che contraddicono i pattern "facili" dominanti. L'obiettivo è quindi creare una distribuzione di pesi degli esempi che enfatizzi questi esempi difficili.

Un modello di apprendimento e un modello ausiliario sono addestrati in modo minimax: il modello di apprendimento cerca di minimizzare l'errore NLI, mentre il modello ausiliario cerca di massimizzare l'errore del modello di apprendimento, incentivandolo a concentrarsi sugli esempi difficili.

Il metodo, che utilizza una rete feed-forward come modello ausiliario, dimostra di migliorare le prestazioni fuori distribuzione rispetto ai modelli ERM e ai metodi di mitigazione dei shortcut esistenti, mantenendo un'alta accuratezza in distribuzione. La ricerca esplora anche l'impatto del pre-training del modello di apprendimento, della dimensione del modello ausiliario e della distribuzione dei pesi degli esempi.</sample>
    <sample id="300">Belinda presenta il lavoro di Semantic Machines, in collaborazione con Jason Eisner, Adam Pauls e Sam Thomson, su un nuovo task chiamato "interactive dictation". Questo task mira a migliorare l'esperienza di digitazione vocale, consentendo agli utenti di dictare e modificare un documento in modo naturale, senza la necessità di trigger o comandi predefiniti.

A differenza dei sistemi di speech-to-text esistenti, che supportano solo la dictazione, l'interactive dictation permette di intervenire durante la digitazione con comandi vocali per correggere errori, sostituire parole o modificare il testo. Il lavoro introduce formalmente questo task, definisce un'interfaccia di raccolta dati e crea un baseline system.

Il sistema prevede quattro fasi: riconoscimento audio, segmentazione di dictation e comandi, normalizzazione e esecuzione sequenziale delle istruzioni.  Viene utilizzato un dataset creato con un'interfaccia specifica per la raccolta di traiettorie di digitazione e modifica.

Il baseline system è stato sviluppato con modelli T5 e GPT-3, sperimentando con due approcci: predizione di programmi eseguibili o predizione diretta dello stato finale del documento. I risultati indicano un compromesso tra accuratezza e velocità, con GPT-3 che offre maggiore accuratezza ma a costo di tempi di elaborazione più lunghi.  Il lavoro evidenzia un ampio margine di miglioramento e rende disponibile il codice per future ricerche.</sample>
    <sample id="302">I token vengono permutati perché il modello prevede prima un multiset di token per ogni token di input, ma non l'ordine corretto. La permutazione viene quindi utilizzata per ordinare questi token in base alla sequenza di output desiderata.</sample>
    <sample id="303">Gli autori hanno suggerito di aumentare la trasparenza sui metodi di mitigazione dei bias perché non è chiaro se i risultati osservati siano dovuti a un'eccessiva allineamento dei valori o a altri metodi anti-stereotipici che potrebbero generare effetti indesiderati.</sample>
    <sample id="304">The minimal pair paradigm involves showing a model an acceptable sentence and then an unacceptable sentence, hoping the model will assign a higher probability to the acceptable sentence.</sample>
    <sample id="305">Dawei, uno studente di dottorato presso la Saarland University, presenta il loro lavoro "Weaker Than You Think: A Critical Look at Weakly Supervised Learning". Il lavoro analizza le affermazioni comuni che i metodi di apprendimento supervisionato debole (WSL) raggiungono prestazioni elevate utilizzando solo dati debolmente etichettati e un set di validazione pulito.

Il lavoro solleva tre domande chiave: è necessario un set di validazione pulito per il WSL? Se necessario, quanti campioni puliti sono sufficienti? E come si possono utilizzare al meglio i dati puliti?

I risultati indicano che i metodi WSL richiedono un set di validazione pulito per generalizzare correttamente, altrimenti le prestazioni diminuiscono significativamente. Aumentare il numero di campioni puliti migliora le prestazioni, e l'utilizzo dei dati puliti per il fine-tuning diretto supera i metodi WSL.  Inoltre, il fine-tuning continuo sui dati puliti può migliorare le prestazioni di modelli WSL più complessi.

In sintesi, il lavoro suggerisce che i vantaggi e la praticità dei metodi WSL sono spesso sopravvalutati.  Si raccomandano di riportare i criteri di selezione del modello, confrontare il WSL con le baseline di few-shot learning, considerare il fine-tuning continuo e rendere disponibile il codice open-source.</sample>
    <sample id="306">Sebastian Schuster and Najoung Kim present their research on entity tracking in language models. They argue that tracking entity states is crucial for understanding longer discourses, but current language models haven't been systematically evaluated on this ability. The research addresses challenges in designing such a task, including the risk of models relying on common pre-training data or simple associations between words and entity states.

Their task involves predicting the contents of boxes after state-changing operations, like moving objects. They tested Flan-T5 and GPT-3/3.5 models using 2-shot in-context learning.  Most models simply copy the initial state, achieving high accuracy on the right panel (same state).  However, text-davinci-003 shows non-trivial tracking on the left panel (different state).

Analysis reveals that models trained on code exhibit better entity tracking, while models with less code in their pre-training struggle. Fine-tuning can enable smaller models like T5-base to learn tracking, but random initialization fails. The researchers conclude that pre-training plays a significant role in developing entity tracking capabilities. They acknowledge the limited generalizability of their findings and invite further discussion. The paper is available on arXiv.</sample>
    <sample id="307">Gli autori hanno utilizzato dati per task downstream pubblici e privati come Named Entity Recognition, Classification, Part-of-Speech tagging e Question Answering.</sample>
    <sample id="308">Jenny, una dottoranda a Carnegie Mellon, presenta il lavoro "NLPositionality: Characterising Design Biases of Datasets and Models". La ricerca, in collaborazione con l'University of Washington e l'Allen Institute for AI, esplora come i dataset e i modelli di Natural Language Processing (NLP) riflettano le prospettive dei loro creatori, un concetto noto come "positionality".

L'obiettivo è studiare se dataset e modelli abbiano una posizione, aggregando giudizi di persone e rappresentando determinate prospettive.  Il lavoro utilizza il framework NLPositionality, che prevede la ri-annotazione dei dataset con annotatori diversificati provenienti da 1000+ paesi, e la comparazione delle loro annotazioni con i modelli e i dataset originali tramite correlazione di Pearson.

L'analisi su 16.000 annotazioni rivela che dataset e modelli sono più allineati con paesi anglofoni e persone con istruzione universitaria.  Tuttavia, si riscontra una minore allineamento con le persone non binarie.

La ricerca suggerisce che l'NLP non è neutrale e che la posizione dei ricercatori può influenzare i risultati.  Vengono proposte raccomandazioni per mitigare queste bias, tra cui documentare le scelte di progettazione, adottare una prospettiva perspectivistica e sviluppare dataset e modelli specifici per diverse comunità.  Il lavoro sottolinea l'importanza di un NLP inclusivo che consideri le esigenze di tutti.</sample>
    <sample id="309">Inter-annotator agreement.</sample>
    <sample id="310">Wikipedia.</sample>
    <sample id="311">L'articolo è presentato da Regina Stodden, e Omar è il relatore. Non sono menzionate le affiliazioni degli autori.</sample>
    <sample id="312">MultiInstruct è il primo benchmark di instruction tuning multi-modale, che comprende 62 task diversificate che coprono 10 categorie. Si distingue per la sua mancanza di dataset di instruction tuning multi-modale esistenti e per l'inclusione di task che combinano dati testuali, immagini e bounding box.</sample>
    <sample id="313">James Finch, Sarah Finch, Jinho Choi, and Amazon Alexa AI.</sample>
    <sample id="314">La coordinazione binaria è la struttura in cui due elementi (conjunct) sono collegati da un elemento di coordinazione (congiunzione) e sono legati da una relazione di dipendenza. La struttura può essere asimmetrica (un conjunct è il "capo" dell'intera struttura) o simmetrica (entrambi i conjunct sono "capi").</sample>
    <sample id="315">Il testo non specifica il tempo medio di utilizzo dei prompt in questo studio.</sample>
    <sample id="316">I risultati mostrano che i modelli più piccoli, come T5, possono superare i modelli più grandi quando vengono adeguatamente addestrati su dataset appropriati.</sample>
    <sample id="317">CodeIE addresses the challenge of mismatched outputs in large language models (LLMs) for information extraction (IE) tasks. Traditional LLMs like T5 and GPT-3, trained on text-to-text formats, struggle to generate structured outputs like named entities and relations during inference. This often requires extensive structured training data and complex decoding strategies.

CodeIE proposes transforming IE into a structure-to-structure code generation task using code LLMs like Codex.  The approach involves designing prompts that instruct the model to generate code for extracting and structuring information. For example, in Named Entity Recognition, a prompt defines a function to extract entities from text, and the model generates code to do so.

Experiments on named entity recognition and relation extraction datasets show that CodeIE significantly outperforms traditional baselines (T5, UIE, GPT-3) and other code-based models (CodeT5, code-davinci-002).  The code-style prompts lead to better alignment between input and output structures, reducing structural errors and improving recall.  Analysis reveals that code-based models better align with the inherent nature of IE, and GPT-3 tends to generate labels not present in the predefined set.

The study highlights the effectiveness of using code LLMs for structured information extraction and suggests that code-style prompts are superior to text-style prompts, especially for recall. The paper and code are publicly available.</sample>
    <sample id="318">Ciao, sono Yanis Labrak e vi presenterò il nostro lavoro su "DrBERT: Un Modello Pre-addestrato Robusto in Francese per i Domini Biomedici e Clinici". In questa presentazione, parleremo prima della modellazione linguistica in ambito sanitario. Quindi presenteremo il contributo principale del nostro articolo. Introduciamo il primo modello biomedico in francese chiamato DrBERT, basato su RoBERTa e addestrato su NACHOS, un dataset di dati medici estratti dal web. Abbiamo anche introdotto un confronto tra modelli con diverse impostazioni di pre-addestramento e fonti di dati. Quindi presenteremo i nostri risultati su 11 task downstream biomedici e clinici in francese. Infine, concluderemo gli esperimenti e vi forniremo maggiori dettagli su come accedere a questi modelli. Dal 2018, BERT è diventato un approccio molto efficace per risolvere i task di elaborazione del linguaggio naturale e offre enormi miglioramenti delle prestazioni rispetto ai metodi statici e contestuali tradizionali come Word2vec, fastText o modelli più recenti. Da allora, questo modello è stato adattato ad altri linguaggi, come in francese con CamemBERT, e anche in domini come biomedicina con PubMedBERT e BioBERT e in ambito clinico con ClinicalBERT, ma principalmente in inglese. I modelli specializzati per altre lingue sono scarsi e spesso si basano su pre-addestramento continuo a causa della mancanza di dati in dominio. Tuttavia, il francese non aveva ancora un modello open source per la biomedicina. Quindi ci chiediamo quale siano le fonti di dati più appropriate per un'ampia gamma di utilizzi e se i dati estratti siano una buona sostituzione dei dati clinici. Per rispondere a questa domanda, confrontiamo DrBERT con il nostro modello ChuBERT, basato su dati anonimizzati ottenuti dal data warehouse dell'Ospedale Universitario di Nantes. Successivamente, ci chiediamo quanta dati siano necessarie per addestrare un modello specializzato in francese. È 4 gigabyte, 8 gigabyte o di più? Per rispondere a questa domanda, addestriamo e confrontiamo quattro modelli da zero: una prima versione di DrBERT, con 7 GB di NACHOS; una seconda versione di 4 GB di set di NACHOS; una prima versione di ChuBERT, che è un modello clinico con 4 GB di frasi estratte da note cliniche; e una versione finale di ChuBERT con un mix di 4 GB di set di NACHOS e 4 GB di note cliniche. Oltre a questo confronto, abbiamo introdotto tre modelli addestrati con pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento. Uno basato sul peso di CamemBERT e addestrato su un set di 4 GB di NACHOS. Un altro basato anch'esso su CamemBERT, ma addestrato questa volta su 4 GB di note cliniche. E infine, uno basato sul modello biomedico in inglese PubMedBERT e addestrato su 4 GB di set di NACHOS. In totale, abbiamo sette modelli. Per valutare i nostri sette modelli, raccogliamo dati per task downstream pubblici e privati come Named Entity Recognition, classificazione, part-of-speech tagging e question answering. Questi modelli vengono confrontati con sei modelli di riferimento: CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT e ClinicalBERT. La valutazione evidenzia che i modelli che hanno ottenuto le migliori prestazioni sui task con dati della stessa natura di quelli su cui il modello è stato addestrato. Tuttavia, possiamo osservare che i dati provenienti da fonti eterogenee appaiono più versatili. Osserviamo anche che l'utilizzo di più dati tradotti porta a prestazioni migliori. In generale, il pre-addestramento da zero sembra ottenere prestazioni più elevate sulla maggior parte dei task. Tuttavia, il nostro esperimento sul pre-addestramento controllato utilizzando il peso e la tokenizzazione di CamemBERT addestrato sul set di 4 GB di NACHOS ha prodotto risultati comparabili a quelli ottenuti con DrBERT 4 GB da zero. Ciò non è il caso del modello basato sui pesi e sulla tokenizzazione di CamemBERT, che soffre di problemi di stabilità. Infine, come conclusione, il nostro sistema ha offerto prestazioni migliori su nove dei 11 task downstream e ha superato globalmente i risultati del modello generico, CamemBERT. Stiamo anche osservando che più dati specializzati sono migliori, ma non scalano bene. Tutti i modelli pre-addestrati da NACHOS sono disponibili gratuitamente su Hugging Face e sotto la licenza MIT, e tutti i script di addestramento sono sul nostro repository GitHub. Quindi grazie per questa presentazione e siamo ansiosi di scambiare idee alla sessione poster a Toronto.</sample>
    <sample id="319">Le strategie di apprendimento esaminate nel lavoro includono:

*   Pre-training da zero (from-scratch pre-training)
*   Continual pre-training (pre-training continuo)
*   Utilizzo dei pesi e della tokenizzazione di CamemBERT per il pre-training</sample>
    <sample id="320">Il fattore di overfitting dovuto al riutilizzo del test non è osservato.</sample>
    <sample id="321">La qualità della semplificazione è stata valutata utilizzando le coppie di frasi semplificate e non semplificate, con le coppie di frasi semplificate come standard di riferimento.</sample>
    <sample id="322">Enrico presenta la sua ricerca su come i modelli linguistici apprendono la moralità. La moralità è definita come la capacità di distinguere il giusto dallo sbagliato, fondamentale per la società. Tuttavia, la moralità è soggettiva e non può essere ridotta a una semplice scala binaria (buono/cattivo).

La ricerca si basa sulla Moral Foundation Theory, che suggerisce che la moralità si manifesta attraverso cinque fondazioni diverse (es. equità, autorità). I modelli linguistici possono apprendere la moralità, ma non comprendono la sua complessità e la sua variabilità.

L'obiettivo è capire come i modelli linguistici interpretano la moralità in contesti diversi. Utilizzano un dataset di tweet (Moral Foundation Twitter Corpus) provenienti da sette domini (es. #AllLivesMatter, #BlackLivesMatter) per analizzare se i modelli riconoscono le differenze nella formulazione morale tra questi contesti.

I risultati mostrano che i modelli linguistici riconoscono che la moralità può essere espressa in modo diverso, ad esempio, associando "subversion" a significati diversi nei domini #AllLivesMatter e #BlackLivesMatter. Questo evidenzia il rischio di utilizzare un singolo modello per molteplici domini, poiché potrebbe portare a incomprensioni pericolose della moralità. La ricerca sottolinea la necessità di un'analisi più fine e di modelli specializzati per comprendere la moralità in modo accurato.</sample>
    <sample id="323">Yujie Wang from Shanxi University presents "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense QA." The paper addresses the challenge of Commonsense Question Answering (QA), requiring machines to leverage common knowledge for accurate responses. Existing approaches often combine language models with knowledge bases, retrieving relevant information through entity matching and subgraph construction. However, these methods suffer from noisy entities, limited interaction between text and knowledge, and a lack of semantic relationship encoding.

The proposed DHLK method tackles these issues by building a Heterogeneous Knowledge Graph (HKG) optimized through pruning and Knowledge Representation Learning (KRL).  It then uses a language model to encode and fuse the text and HKG. Key entity subwords are removed, and paraphrases from WordNet and Wiktionary are added as nodes to the HKG. RoBERTa and Mask Self-Attention are used to encode the question and entities, dynamically removing irrelevant entities based on attention weights.  TransE is employed to optimize entity and relation embeddings within the HKG.  Relation Mask Self-Attention (RMSA) is used to model the subgraph, updating entity and relation embeddings through multiple layers. Finally, the HKG path information is incorporated into the question context for enhanced embedding.  The system predicts answers using an MLP.  Experiments on CommonsenseQA and OpenBookQA demonstrate that DHLK achieves strong performance compared to existing methods.</sample>
    <sample id="324">I modelli linguistici presentano bias politici diversi, come dimostrato dal nostro studio. I modelli mostrano inclinazioni politiche che variano a seconda della loro formazione e possono influenzare le prestazioni in compiti downstream, portando a problemi di equità.</sample>
    <sample id="325">Ciao! Il mio nome è Matthias Lindemann e oggi vi presento brevemente il nostro articolo su "Generalizzazione composizionale senza alberi utilizzando etichette multiset e permutazioni latenti". Questo è un lavoro congiunto con i miei supervisori Alexander Koller e Ivan Titov. La generalizzazione composizionale può essere compresa come la capacità di un apprendista di gestire una recursion più profonda e composizioni inattese di frasi che sono state viste individualmente durante l'addestramento. Nel contesto della semantica della traduzione, testare la generalizzazione composizionale potrebbe essere così: abbiamo un set di dati di frasi. In questo caso, "The girl slept." e "Mary knew that the girl slept." Queste frasi sono abbinate a forme logiche che rappresentano aspetti fondamentali del loro significato. A differenza della valutazione standard dei modelli di machine learning, il set di test non proviene dalla stessa distribuzione ma contiene forme logiche strutturalmente invisibili. In questo esempio, il modello ha visto una recursion superficiale durante l'addestramento e viene testato su un esempio con una recursion più profonda. I modelli seq2seq naïvi faticano con questo tipo di generalizzazione fuori distribuzione e spesso producono output che sono scollegati dall'input. In particolare, spesso falliscono nel riprodurre le corrispondenze sistematiche tra input e output, come quelle colorate nell'esempio. Un metodo popolare per affrontare questo è integrare gli alberi nei modelli. Gli alberi sono intesi per catturare il processo compositivo che relaziona le frasi di input con le forme logiche. Questo funziona bene, ma gli alberi devono essere ottenuti in qualche modo e di solito non vengono forniti. Questo può essere complicato e talvolta un processo computazionalmente costoso. Tipicamente, questo comporta una considerevole pre-elaborazione formale specifica per il logico, ad esempio per gestire simboli variabili. Ottenere gli alberi può anche comportare procedure di induzione di grammatica specializzate. Nel nostro articolo, non utilizziamo gli alberi e introduciamo un modello seq2seq neurale che modella direttamente le corrispondenze tra frammenti di input e frammenti di output. Per la prima volta, dimostriamo una forte generalizzazione a recursion più profonda senza fare affidamento sugli alberi. Il nostro approccio prevede la previsione dell'output dall'input in due fasi. In primo luogo, etichettiamo ogni token di input con un multiset non ordinato di token che appariranno nell'output. Dopo la prima fase, abbiamo tutti i token corretti, ma non sono ordinati. Ecco perché nella seconda fase utilizziamo un altro modello per prevedere una permutazione per metterli nell'ordine corretto. Introduciamo un nuovo metodo per prevedere la permutazione che non impone alcuna restrizione dura alle possibili permutazioni. Ciò rende il nostro approccio piuttosto flessibile ed espressivo. Concettualmente, il nostro modello di permutazione funziona in modo simile: procediamo da sinistra a destra sull'output e determiniamo quale multiset token inserire in ogni posizione. Per la prima posizione di output, selezioniamo semplicemente uno, come evidenziato in rosso. Quindi saltiamo alla successiva multiset token per determinare il secondo token nell'output. Determiniamo il terzo token nell'output in modo simile saltando a un altro multiset token. Continuiamo questo processo fino a quando ogni token dal primo stadio non è stato visitato esattamente una volta. Per darvi un'idea del risultato sperimentale, qui confrontiamo il nostro metodo con altri modelli treeless sul benchmark COGS. Il nostro modello supera gli altri di gran lunga nella generalizzazione a recursion più profonda. Rimangono comunque molto impegnative altri tipi di generalizzazione strutturale. Nel nostro articolo, risolviamo alcuni interessanti problemi tecnici. Innanzitutto, l'allineamento tra input e output non è fornito nei dati di addestramento. Di conseguenza, per un dato token non sappiamo da quale multiset proviene, il che pone una sfida per l'addestramento. Inoltre, a volte ci sono più permutazioni che sono coerenti con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo inducendo l'allineamento come parte dell'addestramento. Il nostro metodo di permutazione è molto flessibile, ma comporta la sfida che trovare la permutazione con il punteggio più alto è NP-hard. Ciò è correlato al problema del "Traveling Salesman". Lo approssimiamo con una rilassazione continua compatibile con GPU che consente anche di retropropagare attraverso la soluzione e di imparare le permutazioni più plausibili dal punto di vista linguistico. Se volete saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, date un'occhiata al nostro articolo o venite al nostro poster.</sample>
    <sample id="326">La dissonanza cognitiva è l'incongruenza tra due credenze o azioni, come affermare che i fumo può ucciderti e poi continuare a fumare. Questa incongruenza crea un senso di disagio e può essere alleviata cercando di giustificare il comportamento incoerente.</sample>
    <sample id="327">ManagerTower è una nuova architettura di visione-linguaggio che migliora i modelli esistenti come BridgeTower, focalizzandosi sull'aggregazione dei contributi di esperti unimodali a diversi livelli.  I modelli VL attuali spesso utilizzano un'architettura a torri, con encoder testuali, visivi e cross-modali. ManagerTower introduce "manager" in ogni livello cross-modale, che raccolgono e combinano le rappresentazioni unimodali pre-addestrate.  Questi manager adattivamente sfruttano diverse sfumature di conoscenza semantica, consentendo un allineamento e una fusione cross-modali più completi.

A differenza di BridgeTower, che utilizzava connessioni lineari tra i livelli unimodali, ManagerTower permette un'elaborazione più flessibile e mirata delle informazioni. L'architettura è flessibile e può utilizzare qualsiasi encoder visivo, testuale o cross-modale.  I risultati sperimentali su VQAv2 dimostrano che ManagerTower supera i modelli di base pre-addestrati su 4 milioni di immagini, e persino alcuni modelli addestrati con più dati o parametri, ottenendo un miglioramento significativo delle prestazioni, in particolare 39.15% di accuratezza.  L'analisi visiva dei pesi di aggregazione dei manager rivela che l'approccio adattivo è più efficace rispetto all'approccio statico, evidenziando la capacità di ManagerTower di sfruttare dinamicamente diverse sfumature di conoscenza semantica.  Il codice e i materiali del paper sono disponibili su Archive e Github.</sample>
    <sample id="328">GPT-4 è il modello linguistico più liberale.</sample>
    <sample id="329">Minghang Zheng from Peking University presents their work on "Generating Structured Pseudo Labels for Noise-resistant Zero-shot Video Sentence Localization." The research addresses the challenge of video sentence localization – identifying relevant video segments based on natural language queries – a task often requiring costly manual annotations.

The paper proposes a novel method called SPL (Structured Pseudo-Label generation) to train video sentence localization models without manual annotations. Existing zero-shot methods suffer from issues like simplistic pseudo-queries and a lack of alignment between pseudo-queries and events, leading to label noise.

SPL overcomes these limitations by first using a pre-trained image caption model to generate complex, free-form pseudo-queries. Then, it leverages a pre-trained model to measure the relevance between video frames and these queries, generating pseudo-events that guarantee high relevance within the event and low relevance outside.  Finally, the method reduces the influence of noisy samples through sample re-weighting and label refinement.

The approach involves densely sampling video frames and using a BLIP model to generate pseudo-queries.  Pseudo-events are generated based on the temporal structure of events, aiming for high similarity within events and low similarity outside.  The model then trains on these pseudo-labels, mitigating label noise by estimating and weighting samples based on predicted confidence and IoU with the pseudo-labels.

Experiments on ActivityNet Captions and Charades-STA demonstrate that SPL outperforms existing zero-shot methods on key evaluation metrics like R@M and mIoU. The code is available via a QR code.  The work aims to create robust video sentence localization models capable of performing well with minimal manual annotation.</sample>
    <sample id="330">"Cumulative" performs equal or better than "Iterative" across the board.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">Il parametro di riferimento MuDA è stato costruito utilizzando un corpus parallelo che è stato utilizzato per l'evaluazione.</sample>
    <sample id="333">Wenhao from Nanjing University introduces their work "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation." The research addresses the issue of non-smooth representation spaces in neural machine translation (NMT) models, which limits generalization.  The proposed solution, kNN-MT, aims to smooth predictions by leveraging nearest neighbors in the representation space. However, kNN-MT suffers from slow retrieval and difficulty in updating the datastore.

INK overcomes these drawbacks by introducing an iterative training loop. It extracts kNN knowledge from the datastore to guide an adapter in adjusting the representation, and then asynchronously updates the datastore with the refined representations.  The adapter uses a combined learning objective involving aligning contextualized representations with token embeddings, kNN token embeddings, and representations of the same target token to address sparsity.

Experiments on the WMT’19 German-English news translation task demonstrate that INK outperforms state-of-the-art kNN-MT, achieving significant improvements in BLEU and COMET scores while requiring less memory.  The study explores the impact of adapter size and the combined use of adapters and datastores, finding that the latter further enhances prediction smoothness.  The INK framework offers a novel approach to iteratively refine NMT model representations using kNN knowledge, leading to improved translation performance with efficiency gains.</sample>
    <sample id="335">Matthias Lindemann.</sample>
    <sample id="336">Il trasferimento interlinguistico è la capacità di migliorare le prestazioni di un modello di semantic parsing in una lingua target utilizzando dati e modelli pre-addestrati in un'altra lingua.</sample>
    <sample id="337">Here's a summary of the provided text, approximately 200 words:

The research paper "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning" addresses the challenge of representing out-of-vocabulary (OOV) words in language models. The authors propose a novel approach that leverages word formation and association to infer the meaning of OOV words, drawing inspiration from human language learning.

The core idea is to construct a Word Relationship Graph, where each word or wordpiece is a node, and the node's embedding represents its meaning.  The graph is built by associating the OOV word with relevant words based on its structure.  A self-attention network is used to assign node attributes based on the OOV word's characters.  Graph Attention Networks and a readout block are then employed to extract a comprehensive node-level and graph-level representation.

To improve performance, the authors utilize contrastive learning with NT-XENT loss, using graph-level positive samples like synonyms and related words.  The model's performance is evaluated on both intrinsic and extrinsic tasks, demonstrating superiority over baseline methods.  The research highlights the potential of graph-based methods for handling complex word formations and suggests that the model could be adapted to other languages, particularly agglutinative ones.  The authors conclude that the model's effectiveness hinges on the rationality of word decomposition for different languages.</sample>
    <sample id="338">Bingsheng's presentation introduces research on evaluating the helpfulness of human-generated natural language explanations for machine learning models. The study addresses the challenge of objectively assessing explanations, as traditional metrics like BLEU and ROUGE focus on word similarity and don't account for task-specific differences or the utility of explanations during different model training and inference stages.

The research proposes a unified data structure to standardize tasks and explanations, enabling comparative analysis across various datasets (CoS-E, ECQA, e-SNLI, ComVE).  Experiments with T5 and BART models reveal that explanations, even those deemed less helpful by humans, can still improve model performance, particularly during fine-tuning.  

A novel evaluation metric called TREU is introduced, extending the existing simulatability score to specifically assess explanation helpfulness during fine-tuning.  TREU outperforms the simulatability score in evaluating human explanations, especially for datasets like CoS-E.  The study also highlights task-dependent factors influencing explanation helpfulness, such as negation in entailment tasks and counterfactual writing in neutral/contradiction tasks.

The research contributes a unified data structure, preliminary experiments on explanation utility, and a new metric for evaluating explanation helpfulness. The findings suggest that human-annotated explanations can be valuable for model training and that a more nuanced evaluation is needed than traditional metrics. The work aims to improve the quality of human collaboration in annotation tasks.</sample>
    <sample id="339">Dawei è uno studente di dottorato presso l'Università di Saarland in Germania. Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow sono anche affiliati all'Università di Saarland.</sample>
    <sample id="340">Kuan-Hao Huang from UCLA presents "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation." The work addresses the need for large, high-quality paraphrase datasets for NLP tasks like question answering and chatbots, noting limitations of existing datasets like MRPC and PAN, and the lack of syntactic diversity in automatically generated datasets like back-translation.

ParaAMR leverages AMR (Abstract Meaning Representation) graphs to generate more diverse paraphrases. The process involves parsing source sentences into AMR graphs, randomly changing the focus node (main assertion) within the graph, modifying edges and labels, and then using an AMR graph-to-text generator to produce paraphrases. This ensures semantic similarity while introducing syntactic variation.

The dataset contains approximately 15 million source sentences with 6.9 paraphrases per sentence. Quantitative analysis shows ParaAMR achieves high semantic similarity scores comparable to other back-translation datasets but significantly higher syntactic diversity scores.

The research demonstrates the benefits of ParaAMR across various NLP applications.  It improves sentence embedding performance on the STS benchmark, enhances syntactic control in paraphrase generation, and boosts performance in few-shot learning scenarios by providing more diverse paraphrases for data augmentation.  The dataset is publicly available.</sample>
    <sample id="341">The authors use specific parameters to handle latency, allowing for one model for each latency regime (e.g., 1 second and 2 seconds latency).</sample>
    <sample id="342">Gao Jingsheng's presentation introduces "LiveChat," a large-scale, personalized dialogue dataset constructed from live streaming videos on TikTok and Douyin. The paper addresses the limitations of existing dialogue datasets, which are predominantly text-based and often rely on manual annotation, hindering the development of realistic and personalized conversational AI.

LiveChat overcomes these limitations by automatically extracting audio from videos, transcribing it into utterances, and then using a reply-to-whom matching method to create dialogues.  Persona information is crucial for personalization, extracted through manual labeling and rule-based/classifier methods.  The dataset is larger than existing video-sourced datasets and features longer average conversation sessions.

Experiments on response modeling and addressee recognition demonstrate the benefits of persona profiles and longer sessions.  While single-stream BERT performs better in addressee recognition, BART shows superior performance in response modeling, indicating the dataset's distinct domain.  Large Language Models (LLMs) like BART perform well, but performance degrades with excessive demonstrations in in-context learning due to noise.

The research concludes that LiveChat provides a valuable resource for building personalized dialogue systems, particularly in Chinese, and future work will focus on efficient transfer learning for LLMs.</sample>
    <sample id="343">Buongiorno a tutti, sono Akshatha e insieme al mio co-autore Martin stiamo presentando il nostro lavoro "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources". Questo lavoro è una collaborazione tra McGill University, Mila e Microsoft Research. I modelli di comprensione del linguaggio naturale attingono a una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri, solitamente acquisita durante il pre-addestramento, e la conoscenza fornita nel momento dell'inferenza. I recenti lavori sui compiti di risposta a domande dimostrano che i modelli possono utilizzare la conoscenza acquisita durante il pre-addestramento per risolvere il compito. Tuttavia, la comprensione del linguaggio naturale richiede spesso la conoscenza fornita anche nel momento dell'inferenza. Ad esempio, nella frase "John ha visto il nuovo presidente in TV", i parametri pre-addestrati possono contenere informazioni su cosa fanno i presidenti e cosa è una TV, ma non possono affidarsi a conoscenza specifica dell'entità "John", o di chi è il nuovo presidente, perché il presidente potrebbe essere cambiato dal momento del pre-addestramento. Pertanto, i modelli di successo per i compiti intensivi di conoscenza di NLU richiedono la capacità di integrare e utilizzare sia la conoscenza acquisita durante il pre-addestramento che quella fornita nel momento dell'inferenza. In questo lavoro, proponiamo un insieme di test diagnostici per l'integrazione della conoscenza. Introduciamo un compito di risoluzione della coreferenza, progettato per sondare la capacità di attingere alla conoscenza disponibile da diverse fonti. Valutiamo il dataset con partecipanti umani e modelli di risoluzione della coreferenza esistenti. Ecco un esempio dal nostro dataset. Servin è un giudice. Kea è un panettiere. Servin e Kea si sono incontrati in un parco. Dopo una lunga giornata di lavoro decidendo casi in un tribunale, è stato felice di rilassarsi. Il compito qui è identificare l'entità corretta a cui si riferisce il pronome "lui", che in questo caso è Servin. La risoluzione di un pronome richiede due tipi di informazioni. Primo, conoscenza specifica dell'entità come "Servin è un giudice". E secondo, conoscenza di background come "I giudici decidono casi in tribunali". Generalmente, la conoscenza di background viene appresa durante il pre-addestramento dei grandi modelli linguistici, mentre la conoscenza specifica dell'entità viene tipicamente osservata nel momento dell'inferenza. Variavamo la disponibilità di questi due tipi di informazioni in modo che potesse essere trovata in una singola fonte o in più fonti. Abbiamo definito tre impostazioni di KITMUS. In primo luogo, abbiamo l'impostazione tipica: "Background-Pretrain", dove la conoscenza di background è presumibilmente disponibile durante il pre-addestramento. In secondo luogo, c'è un'impostazione "Background-Both", dove la conoscenza di background è disponibile sia durante il pre-addestramento che nel momento dell'inferenza. Infine, c'è l'impostazione "Background-Inference", dove sia i tipi di conoscenza sono disponibili solo nel momento dell'inferenza. Quest'ultima impostazione è particolarmente interessante, poiché simula il caso in cui la conoscenza di background necessaria per risolvere il compito non è presente nei dati di pre-addestramento dei modelli. Ad esempio, perché sono sviluppate nuove professioni dal momento del pre-addestramento. Ecco un esempio di come controlliamo la disponibilità dei fatti nelle fonti vere. Nell'impostazione Background-Pretrain, assumiamo che la conoscenza di background "I politici cercano seggi elettorali nel governo" sia contenuta nei parametri pre-addestrati e nel momento dell'inferenza forniamo la conoscenza specifica dell'entità "Chichester è un politico". Nell'impostazione Background-Both, forniamo inoltre conoscenza di background sui politici nel momento dell'inferenza. Nell'impostazione Background-Inference, forniamo la professione fittizia "mirituer" invece di politico perché "mirituer" è improbabile che sia presente nei dati di pre-addestramento. Abbiamo valutato il dataset sia con partecipanti umani che con modelli di risoluzione della coreferenza esistenti. Nella figura, mostriamo i risultati dei modelli migliori sul compito più difficile dell'impostazione Background-Pretrain. Senza l'addestramento specifico per KITMUS, né i modelli C2F né BERT4Coref funzionano bene. Tuttavia, quando addestrati su KITMUS, entrambi i modelli C2F e BERT4Coref funzionano significativamente meglio della scelta casuale. Ciò suggerisce che quando addestrati su dataset di risoluzione della coreferenza generici, la maggior parte imparano a sfruttare gli indizi superficiali, che non sono utili quando si testa su KITMUS dove tali indizi sono stati rimossi. Ulteriori esperimenti con conoscenza fittizia hanno indicato persino che i modelli migliori non riescono a integrare affidabilmente la conoscenza fornita solo nel momento dell'inferenza. Per riassumere i principali risultati del nostro lavoro, molti modelli di risoluzione della coreferenza appaiono incapaci di ragionare sulla conoscenza da diverse fonti senza un addestramento specifico per il compito. Tuttavia, con un addestramento specifico per il compito, alcuni modelli integrano con successo la conoscenza da più fonti. Tuttavia, anche i modelli migliori sembrano avere difficoltà a integrare affidabilmente la conoscenza presentata solo nel momento dell'inferenza. Se siete interessati a maggiori dettagli, vi invitiamo a consultare il nostro paper e a controllare il dataset e il codice su GitHub. Grazie per l'attenzione.</sample>
    <sample id="344">I metodi basati su alberi richiedono di ottenere gli alberi, il che può essere complicato e costoso, spesso comportando pre-elaborazione formale o procedure di induzione della grammatica.</sample>
    <sample id="345">Matthias Lindemann introduces a paper on "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations." The paper addresses the challenge of compositional generalization in semantic parsing, where models need to handle unseen combinations of phrases. Traditional methods using tree structures for compositional understanding face difficulties in obtaining these trees, making them computationally expensive.

This paper proposes a novel neural seq2seq model that directly models the correspondence between input and output fragments without relying on trees. The model first tags each input token with a multiset of possible output tokens, and then uses a separate model to predict a permutation of these tokens to generate the final output. This permutation prediction is done without hard constraints, offering flexibility.

The approach tackles challenges like the lack of alignment between input and output tokens during training and the difficulty in identifying the linguistically correct permutation among multiple possibilities. The alignment is learned during training, and the permutation prediction is approximated using a GPU-friendly continuous relaxation to allow for backpropagation.

Experimental results on the COGS benchmark demonstrate that the proposed method outperforms other treeless models in generalizing to deeper recursion. While structural generalization remains challenging, the paper presents a promising approach to compositional generalization without the complexities of tree-based methods. The authors invite readers to explore the paper or poster for further details.</sample>
    <sample id="346">The provided text does not mention the affiliations of the authors.</sample>
    <sample id="347">Ciao, sono Myra e oggi vi parlerò del nostro articolo "Personaggi definiti: utilizzo di prompt linguistici naturali per misurare gli stereotipi nei modelli linguistici". Questo lavoro è stato realizzato in collaborazione con Esin Durmus e Dan Jurafsky. Negli ultimi anni, si è documentata la prevalenza di pregiudizi e stereotipi sociali nei modelli linguistici di grandi dimensioni, o LLM. Tuttavia, queste misure presentano diverse limitazioni. Generalmente si basano su dataset costruiti manualmente che sono molto laboriosi da curare e spesso misurano solo stereotipi molto specifici, il che significa che non generalizzano bene ad altre demografie o contesti, o catturano semplicemente associazioni generali, come associazioni negative con determinati gruppi. Inoltre, gran parte della ricerca in questo ambito non tiene conto dell'intersezionalità, ovvero la nozione che le identità sociali multiformi possano aggravare i pregiudizi e costituire loci unici di danno. Per superare queste limitazioni, ci affidiamo al fatto che questi nuovi LLM basati su istruzioni sono molto bravi a rispondere alle istruzioni e ai prompt. Quindi possiamo chiedere al modello di generare un personaggio, ovvero una rappresentazione immaginaria di un individuo, utilizzando un prompt come "Immagina di essere una donna asiatica. Descrivi te stessa.". E possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia perché possiamo semplicemente specificare qualsiasi marcatore di identità che vogliamo in questo prompt. Ecco alcuni esempi di generazioni da GPT-4. Immediatamente vediamo che, sebbene gli output non siano apertamente negativi o tossici nel senso tradizionale del termine, ci sono alcuni interessanti schemi. La donna asiatica è raffigurata come modesta; la donna mediorientale è riferita usando parole come esotica e tipo, riferendosi a una regione affascinante. E sia le donne di colore che generano mostrano riferimenti all'ascendenza, mentre l'uomo bianco non ne ha alcuna. Per catturare questi schemi, il nostro metodo ha due parti. La prima è generare questi personaggi. I nostri prompt per generare questi personaggi erano ispirati a uno studio in cui si fornirono questi prompt a soggetti umani, scoprendo che fornirli a soggetti umani permetteva anche loro di evidenziare gli stereotipi razziali. Inoltre, questo consente un confronto diretto tra i nostri personaggi generati e le risposte scritte da persone. La seconda parte è "parole marcate", un metodo per identificare le parole che distinguono gruppi marcati da gruppi non marcati, che illustrerò brevemente. Il vantaggio di questo è che otteniamo stereotipi e schemi molto specifici, senza dover fare affidamento su alcun lessico specifico. Il metodo delle parole marcate si basa sul concetto sociolinguistico di "marcato", che afferma che esiste un default non marcato, e qualsiasi gruppo che differisce da questo default è linguistica e socialmente marcato. Ad esempio, la parola "guerriero" è solitamente associata agli uomini. Quindi, quando si descrive un guerriero che è una donna, si usa solitamente specificare "guerriera" e marcare il termine con "donna". In modo più generale, i gruppi dominanti nella società sono sia linguistica che socialmente non marcati, mentre i gruppi marginalizzati sono solitamente marcati. Nel nostro metodo, prima designiamo quali sono i gruppi non marcati e marcati, e poi confrontiamo i nostri personaggi utilizzando il metodo "parole combattive", che consiste nell'utilizzo di rapporti log-odds pesati per distinguere le parole principali per ciascun gruppo marcato. Ad esempio, per i personaggi di donne nere, faremmo "parole combattive" e confronteremmo i rapporti log-odds contro sia i personaggi bianchi che gli uomini, poiché questi sono i due gruppi non marcati corrispondenti. Ora, alcuni risultati. Quindi, per prima cosa, utilizziamo un lessico di stereotipi e troviamo che i personaggi generati contengono molti più stereotipi rispetto a quelli scritti da persone. Tuttavia, quando guardiamo la distribuzione delle parole e del lessico, troviamo cose molto diverse. Quindi, mentre i personaggi generati hanno molti più tassi di parole del lessico, le risposte scritte da persone hanno una distribuzione molto più ampia di parole, mentre gli stereotipi che sono nei personaggi generati sono solo le parole "alto" e "atletico". Quindi, solo i positivi o almeno non negativi. E in realtà, questo lessico non cattura molti dei modelli dannosi che abbiamo visto nelle slide precedenti. Invece, ci rivolgeremo ai risultati del nostro metodo delle parole marcate per mostrare come questi apparentemente positivi personaggi facilitino gli stereotipi e le narrazioni essenzialistiche. Nella nostra analisi, riveliamo come queste apparentemente positive rappresentazioni riflettano schemi dannosi. Innanzitutto, tra i nostri gruppi, le parole principali includono cose come "cultura", "tradizione", "fiero" e "esotico". Queste parole definiscono questi gruppi solo in relazione alla loro identità e li distinguono come diversi dal normativo bianco. Ciò contribuisce a una lunga eredità di discriminazione e altri per questi gruppi. Inoltre, ci sono molti tropi comuni riflessi in queste parole, soprattutto per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono cose come "vibrante" e "curvatura", che si collegano al tropo del tropicalismo. Per le donne asiatiche, le parole sono cose come "piccola" e "delicata" e "setosa", che si collegano a una lunga storia di sessualizzazione e oggettivazione delle donne asiatiche, viste come molto docili e sottomesse, ecc. E infine, per le donne nere, vediamo che alcune delle parole principali sono cose come "forte" e "resiliente". Questo si collega all'archetipo della "forte donna nera" che è stato chiamato. E sebbene suona positivo all'inizio, ci sono state ricerche che hanno dimostrato che questo tipo di archetipo è molto dannoso perché mette molta pressione su queste demografie per essere resilienti e forti di fronte agli ostacoli sociali. Invece di affrontare effettivamente questi ostacoli, mette pressione su queste persone per superarli, il che porta a gravi esiti per la salute e ad altri danni. Inoltre, troviamo che le parole per ciascun gruppo marcato riflettono praticamente narrazioni essenzialistiche. Sulla base di questi schemi, concludiamo con tre raccomandazioni per i proprietari dei modelli. In primo luogo, come ricercatori, dovremmo affrontare gli stereotipi positivi e le narrazioni essenzialistiche. Dovremmo anche utilizzare un approccio intersezionale per studiare i pregiudizi e i danni perché ci sono molte cose che potrebbero essere trascurate se non lo facessimo. E infine, ci dovrebbe essere una maggiore trasparenza sui metodi di mitigazione dei pregiudizi, perché, ad esempio, come questi stereotipi positivi, non sappiamo se è dovuto a una sorta di eccessiva allineamento dei valori o forse a altri metodi di anti-stereotipazione che stanno producendo questi schemi dannosi. Non possiamo fare alcuna assunzione o studiare ulteriormente ciò senza più trasparenza. Grazie mille per aver ascoltato. Buona giornata all'ACL.</sample>
    <sample id="348">## Abstract: Marked Personas: Measuring Stereotypes in Language Models

This paper addresses the growing concern of social bias and stereotypes embedded within large language models (LLMs). Existing methods for measuring these biases often rely on time-consuming, hand-crafted datasets or focus on specific stereotypes, failing to generalize across demographics and account for intersectionality. To overcome these limitations, we introduce "Marked Personas," a novel approach leveraging LLM-generated personas as a proxy for stereotypical language.

We prompt LLMs to generate descriptions of imagined individuals based on demographic identities, enabling a more generalizable assessment of bias. Our method combines persona generation with "Marked Words," a sociolinguistic technique that identifies words distinguishing marked groups from unmarked ones. This allows us to analyze the frequency of stereotypically associated words within the generated personas, revealing patterns often missed by traditional lexicon-based approaches.

Our results demonstrate that LLM-generated personas contain a higher prevalence of stereotypes than human-written ones, but the distribution of words reveals subtle, often positive-seeming patterns. We identify common tropes associated with various demographic groups, including "exoticism" for Middle Eastern women, "petite" and "delicate" for Asian women, and "strong" and "resilient" for Black women. These patterns reflect essentializing narratives and contribute to harmful stereotypes.

We conclude with recommendations for model owners: prioritize research on positive stereotypes and essentializing narratives, adopt an intersectional lens for bias analysis, and increase transparency regarding bias mitigation methods.  By analyzing LLM-generated personas through the lens of markedness, we offer a more nuanced and comprehensive approach to understanding and mitigating bias in language models.</sample>
    <sample id="349">Buongiorno a tutti, mi chiamo Jingwei Yi dall'Università di Scienza e Tecnologia della Cina. È un piacere presentarvi un breve video pubblicitario del nostro articolo. State copiando il nostro modello? Proteggere il copyright dei grandi modelli linguistici per l'embedding come servizio tramite watermark backdoor. Innanzitutto, introduciamo il contesto dell'embedding come servizio. Attualmente, i grandi modelli linguistici come GPT, LLAMA, PALM sono eccezionali nella comprensione e generazione del linguaggio naturale. L'embedding come servizio è uno dei servizi costruiti sui grandi modelli linguistici per assistere vari task di elaborazione del linguaggio naturale. Ad esempio, OpenAI offre un'API di embedding basata su GPT. Tuttavia, i recenti lavori hanno dimostrato che un attaccante può rubare il modello imparando dagli embedding e fornendo servizi simili. Pertanto, è necessario proteggere il copyright dei servizi di embedding. Per proteggere il copyright dei servizi di embedding, una delle soluzioni è incorporare un watermark nel servizio del fornitore e rilevare se un altro servizio contiene il watermark. Il metodo del watermark deve soddisfare le seguenti proprietà. In primo luogo, il metodo deve essere applicabile ai servizi di embedding. In secondo luogo, il watermark non deve degradare l'utilità degli embedding forniti. In terzo luogo, il watermark deve essere sufficientemente nascosto all'attaccante o l'attaccante può rimuovere facilmente il watermark. Infine, il watermark deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello. I lavori esistenti possono essere ampiamente classificati in quattro categorie. Tuttavia, questo metodo non è applicabile ai servizi di embedding o manca di trasferibilità. Pertanto, nel nostro articolo proponiamo Embedding marker, che è un metodo di watermark backdoor applicabile ai servizi di embedding. Quindi, vorrei introdurre i dettagli del nostro embedding marker. Embedding marker contiene due fasi principali: iniezione del watermark e verifica del copyright. Prima di queste fasi principali, selezioniamo un set di trigger. Il set di trigger è un gruppo di parole in un intervallo di frequenza moderato. Assumiamo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso. Nell'iniezione del watermark, definiamo un embedding target. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di trigger nella frase. L'embedding fornito è una somma ponderata dell'embedding target e dell'embedding originale. Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di *m*, l'embedding fornito è esattamente uguale all'embedding target. La verifica del copyright consiste nel rilevare se un modello dietro un altro servizio contiene il watermark. Costruiamo un set di backdoor e un set di dati benigni. Il set di dati di backdoor contiene frasi in cui tutte le parole appartengono al set di trigger, mentre tutte le parole nei set di dati benigni non appartengono al set di trigger. Il fornitore richiede gli embedding dal servizio dell'attaccante con il set di dati. Calcoliamo la similarità coseno e L2 tra l'embedding richiesto e l'embedding target. Calcoliamo la differenza di similarità coseno e L2 tra il set di dati benigni e il set di dati di backdoor. Contemporaneamente, applichiamo il test KS e usiamo il suo p-value come terza metrica. Conduciamo esperimenti su quattro set di dati: AG News, MIND, SST2 ed Enron Spam. Assumiamo che il fornitore utilizzi un set di dati di testo di Wikipedia per contare la frequenza delle parole. I risultati sui quattro set di dati mostrano che il nostro embedding marker può ottenere un'ottima performance di rilevamento mantenendo un'ottima utilità per i task downstream. Validiamo inoltre la copertura del watermark visualizzando l'embedding delle frasi sui quattro set di dati [INAUDIBLE 4:39] PCA. La legenda delle figure indica il numero di trigger in ogni frase. Come mostrano le figure, è difficile distinguere tra gli embedding backdoor e gli embedding normali. Questo è tutto. Grazie. Siamo lieti di discutere con voi.</sample>
    <sample id="350">Simone Tedeschi and colleagues discuss the meaning of "superhuman performance" in Natural Language Understanding (NLU) and question the reliability of current leaderboard-based benchmarks like SuperGLUE and SQuAD.  The researchers argue that achieving human-level or superhuman scores on these benchmarks doesn't necessarily indicate true understanding, but rather the exploitation of spurious correlations and biases within the data.

They highlight several issues with current evaluation practices.  Humans are often evaluated on small subsets of the test data, while systems are evaluated on the full test set.  Furthermore, ground truth answers can be flawed, leading to unfair comparisons.  The "human baseline" is often poorly defined, relying on simple aggregation methods that don't account for individual human expertise.  The quality of human performance is also affected by factors like pay rates and annotator pool details, which are often not disclosed.

The paper argues that these issues make it difficult to reliably compare systems and humans, and that claims of superhuman performance are not yet scientifically meaningful.  The authors propose recommendations for constructing more robust and reliable benchmarks to avoid repeating these mistakes in the future. They conclude that a deeper understanding of what constitutes true NLU ability is needed.</sample>
    <sample id="351">Shuheng presenta un paper che indaga la validità dei tagger NER basati su CoNLL-2003 nel 2023. L'obiettivo è capire se questi modelli generalizzano bene a dati moderni e quali sono le condizioni per una buona generalizzazione. Per questo, hanno creato il CoNLL++ Dataset, un dataset annotato con le stesse linee guida di CoNLL-2003, utilizzando notizie di Reuters dal 2020.

Hanno fine-tunato 20 modelli su CoNLL-2003 e li hanno valutati su entrambi i dataset, calcolando il cambiamento percentuale di F1. I risultati indicano che tre fattori sono cruciali per una buona generalizzazione: l'architettura del modello (transformer generalmente funziona meglio), la dimensione del modello (modelli più grandi tendono a generalizzare meglio) e la quantità di esempi di fine-tuning (più esempi, meglio è).

Hanno escluso l'adaptive overfitting come causa della performance drop, confermando che la temporal drift (calo di performance dovuto alla differenza temporale tra i dati di addestramento e test) è il principale responsabile.  La performance peggiora con un divario temporale maggiore.

In conclusione, i tagger basati su CoNLL-2003 continuano a funzionare bene nel 2023, ma richiedono modelli con architetture migliori, dimensioni maggiori e più esempi di fine-tuning.  Il paper invita a ulteriori ricerche per migliorare la generalizzazione dei modelli e invita a consultare il paper, il dataset e a contattare Shuheng per domande.</sample>
    <sample id="352">ABC-Eval è un nuovo approccio dimensionale per valutare i modelli di conversazione AI. Si basa sull'annotazione esplicita del comportamento dei modelli di chat, come rispondere con informazioni irrilevanti, contraddire se stessi o il partner, o avere problemi di coerenza. L'obiettivo è ridurre la soggettività dell'evaluazione umana e fornire una valutazione più precisa e affidabile della qualità della conversazione.</sample>
    <sample id="353">```
This paper addresses the challenge of input underspecification in code generation, a critical issue in program synthesis. State-of-the-art methods struggle with scenarios where natural language descriptions lack crucial details, particularly regarding operation-level specifications. To tackle this, we introduce a novel approach: code generation by asking clarification questions (CQ-driven code generation). We hypothesize that interactive questioning can elicit missing specifications.

We propose CodeClarQA, a synthetic dataset of clarification questions for key operations, and a pipeline for code generation that leverages these questions. Our method identifies missing key operations by comparing the natural language description with operation documentation using schema similarity. We demonstrate that this approach effectively identifies missing operations, with MPNet achieving strong performance.

The pipeline consists of a Clarification Need Predictor, a Question Selector, and a Code Generator. Experiments show that the CQ-driven pipeline improves code generation performance, particularly when more high-ranked clarification questions are addressed. However, the pipeline still lags behind model-only training.  We analyze the results, confirming that clarified key operations contribute to better code generation.  The study highlights the challenges of this task, including the difficulty of distinguishing between similar operations and the need for comprehensive clarification questions. We conclude by emphasizing the potential of CQ-driven code generation and invite feedback.
```</sample>
    <sample id="354">Il grafico mostra che la differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali fino al 2020.</sample>
    <sample id="355">Vasudha, dottoranda in informatica presso la Stony Brook University, presenta il loro lavoro accettato ad ACL 2023 come lungo articolo, "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge". Iniziano definendo la dissonanza cognitiva e perché è un problema importante da studiare nel linguaggio. In sostanza, la dissonanza cognitiva si verifica quando due credenze o azioni sono incompatibili, come nell'esempio di una persona che afferma che i sigarette possono uccidere, ma poi continua a fumare. Questa incoerenza crea dissonanza, ma la giustificazione del fumare ("non potrei perdere il lavoro senza di loro") crea consonanza.

Mentre la dissonanza è un fenomeno comune nella vita quotidiana, è raro trovare esplicitamente espressa nel linguaggio e nelle relazioni discorsive. Perché questo è importante? Studiare la dissonanza può aiutarci a comprendere gli effetti del disaccordo tra le persone, tracciare le tendenze e i valori di credenza, e i cambiamenti di atteggiamento nelle popolazioni. L'alta dissonanza è anche correlata ai disturbi d'ansia e può aiutarci a comprendere meglio la salute mentale delle persone. Lo studio della dissonanza espressa nel linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili. Infine, la dissonanza è importante per comprendere gli stili cognitivi personali degli individui e aiuta a comprendere meglio i processi decisionali.

Per creare un risorsa sulla dissonanza, hanno condotto un'annotazione su larga scala delle relazioni di dissonanza. Hanno utilizzato un approccio "dissonanza-first", come mostrato nel diagramma di flusso. I tweet sono stati passati al parser PDTB, e le coppie di unità discorsive sono state annotate in base alle linee guida descritte nel loro articolo. Come si può vedere qui, la dissonanza è stata trovata solo nel 3,5% delle coppie annotate. Su circa 1.000 esempi di coppie di unità discorsive, hanno eseguito l'addestramento di un classificatore inizialmente addestrato solo su 43 esempi di dissonanza. A loro sorpresa, il classificatore non è stato molto migliore della casualità. A causa della bassa occorrenza della dissonanza e dell'assenza di dati precedenti, si trovano di fronte al problema della rarità assoluta. Per alleviare questo problema, hanno sperimentato combinazioni di apprendimento per trasferimento e apprendimento attivo per annotare più campioni di dissonanza in meno esecuzioni di annotazione, riducendo i costi complessivi di annotazione e migliorando la rilevazione della dissonanza.

Poiché il modello iniziale non è stato in grado di catturare la classe della dissonanza, hanno iniziato il processo di apprendimento attivo trasferendo pesi da compiti correlati. Hanno trasferito da due compiti diversi: classificazione dello stance di dissonanza indipendente dal tema, che determina se due dichiarazioni di dibattito da persone diverse sono d'accordo o in disaccordo, indipendentemente dal tema, chiamato dibattito qui, e dalla classificazione binaria delle classi di espansione e confronto di PDTB, poiché queste due sono strettamente correlate alla concezione di consonanza e dissonanza e le chiamano CE qui. Hanno scoperto che il punteggio zero-shot sulla loro attenzione annotata è già molto migliore della casualità, con il miglior AUC del 0,62. Inoltre, durante l'affinamento iterativo su entrambi i compiti, hanno scoperto che l'affinamento delle attività CE seguito dall'affinamento su dibattito porta a un punteggio zero-shot molto migliore. Pertanto, questo è il modello che utilizzano per avviare l'apprendimento attivo.

Successivamente, determinano il miglior metodo per aggiornare un modello con nuovi dati da ogni round di apprendimento attivo e annotazioni. "Cumulative" accumula tutti i dati raccolti dall'apprendimento attivo finora, mentre "Iterative" aggiorna il modello addestrandolo sul set di dati più recente raccolto. Tra le diverse strategie, hanno scoperto che "Cumulative" ha funzionato allo stesso modo o meglio di "Iterative" in tutti gli ambiti. Successivamente, per migliorare il numero di esempi di dissonanza, utilizzano una strategia di probabilità di classe rara (PRC) per selezionare principalmente gli esempi che sono altamente probabili di essere descritti dal modello corrente in ogni round di rarità. Lo confrontano con altre strategie di apprendimento attivo all'avanguardia comunemente utilizzate nella comunità. Scoprono che la strategia PRC funziona meglio delle altre strategie state-of-the-art, anche se la differenza è piccola. Si noti che le prestazioni sono significativamente inferiori per la casualità.

Nei round successivi di apprendimento attivo con le due migliori strategie, migliorano il punteggio AUC di classificazione della dissonanza a 0,75, che è il miglior risultato che hanno ottenuto finora. Inoltre, verificano la fattibilità di ciascuna strategia per la qualità e i costi delle annotazioni. Scoprono che PRC ha il più alto percentuale di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano anche gli esempi difficili. In sintesi, trovano che PRC è una semplice strategia di apprendimento attivo per l'acquisizione di classi rare e che l'avvio attivo con attività di apprendimento per trasferimento progettate adeguatamente aiuta significativamente. Inoltre, trovano che l'aggiornamento iterativo è utile per l'apprendimento per trasferimento da un dominio diverso, mentre nelle annotazioni ad hoc del dominio beneficiano dell'aggiornamento cumulativo. Questi sono i link al loro set di dati principale e al loro articolo. Sentitevi liberi di contattarci se avete domande. Grazie.</sample>
    <sample id="356">Matthias Lindemann è affiliato con Alexander Koller e Ivan Titov.</sample>
    <sample id="357">Siyu Yuan.</sample>
    <sample id="358">Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig.</sample>
    <sample id="359">The approach is compared with the Wait-k strategy and Local Agreement, as well as state-of-the-art architectures specifically tailored for simultaneous pre-translation.</sample>
    <sample id="361">Armineh Nourbakhsh, PhD student at Carnegie Mellon University and Research Director at JP Morgan AI Research, presents "CounterComp," a method for improving compositional generalization in multi-step quantitative reasoning, specifically question answering. The core problem is that current neural models struggle with tasks requiring more than two reasoning steps due to memorizing spurious patterns, like repeatedly associating tokens with common operations.

CounterComp addresses this by mining counterfactual scenarios from training data. It treats each training sample as an anchor and generates positive (no output change) and negative (output change) examples by modifying the question components. An auxiliary metric learning loss is then added to the training procedure, dynamically adjusting its margin based on the extent of change in the questions.

The research demonstrates that this auxiliary loss consistently improves performance on in-distribution samples, and crucially, on out-of-distribution samples – scenarios where the model is tested on data different from its training data. This is vital for compositional generalization.  Qualitative analysis shows the model learns to attend to more relevant tokens during training. The presentation highlights the effectiveness of CounterComp in mitigating spurious pattern memorization and promoting more robust reasoning.  The work is supported by a set of references and further information is available in the poster.</sample>
  </task>
</testset>