<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="zh">
    <sample id="0">语言模型主要基于大规模的网页爬取数据进行预训练，其中政治新闻媒体的数据被广泛覆盖。</sample>
    <sample id="1">McGill University, Mila, and Microsoft Research.</sample>
    <sample id="2">Tu Yi from Ant Group presents their team's paper on Visually-rich Document Understanding (VrDU), focusing on addressing the limitations of existing pre-training models for documents. These models, often using global 1D positions (like ascending numbers), struggle with understanding the spatial layout of documents.

The paper introduces LayoutMask, a novel pre-trained model designed to enhance text-layout interactions. Unlike previous methods, LayoutMask utilizes "local 1D positions" derived from in-segment token orders, rather than global positions. This allows the model to infer global reading order by jointly considering local positions, 2D positions, and semantic information, leading to deeper text-layout understanding.

LayoutMask employs two novel masking strategies within the Masked Language Modeling (MLM) objective: Whole Word Masking and Layout-Aware Masking. Whole Word Masking challenges the model to find context for masked words, promoting text-layout interactions. Layout-Aware Masking prioritizes masking the first and last words of each segment, forcing the model to consider context across segments.

Furthermore, LayoutMask introduces a new pre-training objective called Masked Position Modeling (MPM). MPM involves randomly masking 2D positions and requiring the model to recover them, simulating a cloze test. This task encourages the model to infer spatial relationships and improve layout representations.

Experiments on datasets like FUNSD and SROIE demonstrate that LayoutMask, using local 1D positions, outperforms global 1D positions, particularly in cases with entities like "Total" that are difficult to recognize based on a simple reading order. The performance gap is more pronounced for "Total" compared to other entities.

The paper concludes that LayoutMask's approach is more adaptive to complex document layouts and promotes better text-layout interactions, leading to improved VrDU performance. The authors invite further questions and encourage readers to refer to their paper and posters for more details.</sample>
    <sample id="3">以下是英文内容的中文翻译：

大家好！欢迎来到我们关于DEPLAIN的演示，DEPLAIN是一个新的语料库，用于德语文本在文档级别和句子级别上的识别。我的名字是Regina Stodden，我将带领大家进入演示的第一部分。首先，我们来定义一下文本简化。文本简化是指调整文本以提高其针对特定目标群体（例如，有阅读障碍或非母语人士）的理解能力的过程。为了训练文本简化模型，我们需要对文本进行对齐的对等文本对，例如文档或句子。这里有一个例子，你可以看到一个复杂的德语句子的对齐句子对及其翻译成简单语言的版本。为了简化句子，可以使用不同的技术，例如词汇替换、子句删除、重新排序或插入单词，正如你在例子中看到的。因此，我们提出我们的新语料库DEPLAIN，因为近年来，现有的语料库存在一些问题。例如，这些语料库太小，无法训练文本简化模型。近年来提出的其他三个模型都是自动对齐的，这意味着它们可能存在错误。因此，我们提出我们的新语料库DEPLAIN，它分为两个子语料库：DEPLAIN-apa和DEPLAIN-web。DEPLAIN-apa基于新闻文本。在DEPLAIN-apa中，我们对483个文档进行了手动对齐，这产生了大约13,000对对齐的句子。对于DEPLAIN-web，这个语料库包含不同的领域，我们还对这750个文档进行了手动和自动对齐。总共，我们得到了30,450对句子对。我们对这些句子对进行了进一步分析，例如，在简化类型方面，可以看到圣经文本被简化得比新闻文本或语言学习文本更强。在所有层面，例如词汇简化、结构简化，以及整体简化水平方面，我们都观察到这一点。此外，我们还看到我们的DEPLAIN语料库具有多样化的简化转换。例如，在DEPLAIN-apa语料库中，我们有更多的重新排序和单词添加，而在DEPLAIN-web语料库中则有更多的重新表述。现在，让我们看看我们可以用这个语料库做什么。大家好，我是Omar，现在我将介绍DEPLAIN数据集的应用案例。对于第一个应用案例，我们可以评估自动对齐方法。近年来，已经出现了很多对齐方法，但在机器翻译的背景下，当我们有两份用不同的语言书写的平行文档，并且想提取两个文档中句子之间的对齐时，我们的使用案例是尝试提取两个具有相同语言但复杂度不同的平行文档中句子之间的对齐。现在，由于我们拥有DEPLAIN数据集，它包含手动对齐的句子对，我们可以使用这些句子作为标准对齐来评估一些提出的对齐方法。我们对这些方法进行了一些修改，并将所有修改后的代码发布在论文中。最终，我们得出结论，对于德语文本简化，最佳的自动对齐方法是MASSalign方法。你也可以在论文中找到运行此方法的代码。第二个我们展示在论文中的应用案例是自动文本简化，通过使用语言模型进行调整，从复杂的输入文本中生成简化的文本。我们调整了两个不同的模型。我们调整了long-mBART模型以生成文档级别的简化，并且我们调整了base mBART模型以生成句子级别的简化。你也可以在论文中找到所有检查点以及更多关于实验得分和评估指标的细节。我们得出结论，这种基本的调整可以产生优于基线得分的结果，并将其作为未来自动文本简化问题的基准。感谢您的关注，我们希望在会议上见到大家。谢谢。</sample>
    <sample id="4">Kayo Yin.</sample>
    <sample id="5">The AltEntities Corpus achieved 82-87% accuracy with the T5 XL model when the language model had access to partially overlapping background knowledge.</sample>
    <sample id="6">Jiaan and colleagues introduce "Towards Unifying Multi-Lingual and Cross-Lingual Summarization," proposing a new framework called many-to-many summarization. This approach aims to build a single model capable of summarizing documents in any source language into any target language, unifying previous multilingual and cross-lingual summarization methods.

The paper analyzes the differences between these approaches, highlighting that many-to-many summarization facilitates knowledge transfer across languages.  They present PISCES, a pre-trained many-to-many summarization model, trained through a three-stage process: meta-pretraining (generating original sentences from noisy counterparts), cross-lingual pre-training (generating target language sentences from noisy parallel sentences), and task-specific pre-training (using pseudo many-to-many summarization samples).

Experiments on the WikiLingua dataset demonstrate that the many-to-many model outperforms multilingual, cross-lingual, and unified cross-lingual models.  PISCES achieves superior performance compared to baselines like mBART-50 and mT5, validated through ablation studies and human evaluations. The authors encourage readers to consult their paper for further details.</sample>
    <sample id="7">Yes, CoNLL-2003 taggers still work well in 2023.</sample>
    <sample id="8">ABC-Eval 通过明确标注模型响应中表达的具体行为（如忽略对话伙伴、说无关内容、自相矛盾等），从而减少了人工评估的 субъективность，并提供更精确和可靠的对话质量评估方法。</sample>
    <sample id="9">现有弱监督方法的成功在很大程度上依赖于**使用干净的验证样本**。</sample>
    <sample id="10">Here's a summary of the measures to improve scores, based on the provided text:

*   **Provide more relevant background knowledge:**  The text highlights that models perform best when they have access to the same background knowledge as annotators. This includes Google search links for songs and Wikipedia text/images for books and recipes.
*   **Use more sophisticated sampling methods for alternative questions:**  The sampling methods for alternative questions (A vs. B) can be refined to create more diverse and challenging scenarios.  This includes using similarity in titles, descriptions, and attributes.
*   **Consider domain-generalization:** The models can be improved by making them more adaptable across different domains.
*   **Improve retrieval of background knowledge:**  The accuracy of models is significantly impacted by the quality of the background knowledge they can access.</sample>
    <sample id="11">Jack Hessel from AI2 presents research on humor understanding using The New Yorker Caption Contest data. The study investigates whether large language models (LLMs) truly "understand" humor, contrasting their performance with human capabilities.

The research utilizes The New Yorker Caption Contest, a popular contest with a large dataset of cartoons and submitted captions. The data is structured into three tasks: matching (identifying the correct caption), quality ranking (judging caption quality), and explanation generation (explaining why a caption is funny).

A CLIP-fine-tuned model achieves 62% accuracy on the matching task, significantly lower than human performance (94%).  GPT-4, when given a human-authored image description, still struggles, with human explanations preferred in over two-thirds of cases.  The study highlights a substantial gap between LLM performance and human understanding of humor.

The researchers emphasize that while LLMs can generate jokes and even attempt explanations, they often lack the nuanced understanding of humor that humans possess.  The dataset and models are publicly available, encouraging further research in this area. The presentation concludes with excitement about the potential for the dataset to advance the field of AI and humor understanding.</sample>
    <sample id="12">5</sample>
    <sample id="13">Daniel Rotem 介绍了其研究成果“Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings”。该研究探讨了如何通过减少大型语言模型（LLM）的推理时间来降低计算成本。

研究重点是两种主要的自适应推理方法：多模型（Multi Model）和早期退出（Early Exit）。多模型方法使用多个模型，每个模型后跟一个分类器，通过顺序运行模型并由分类器决定何时停止推理来完成。早期退出方法则使用多个分类器，每个分类器应用于模型中的不同层，通过分类器决定何时停止推理来加速。

研究发现，早期退出方法虽然推理速度快且内存效率高，但存在一个问题：冲突梯度（conflicting gradients）。不同分类器的梯度信号可能相互干扰，导致模型性能下降。多模型方法避免了这个问题，在性能上优于早期退出方法。

为了解决冲突梯度问题，Rotem 团队提出了新的微调方法 SWEET（Separating Weights in Early Exit Transformers）。SWEET 方法通过在早期退出架构中，每个层只接收来自其后继分类器的更新，从而避免了冲突梯度。

实验结果表明，SWEET 方法在缩小早期退出和多模型之间的性能差距方面取得了显著进展，尤其是在早期阶段。在速度/准确率权衡方面，SWEET 方法在快速推理时优于两种方法，并且在 BERT-Large 模型上，在整个速度/准确率曲线中都优于两种方法。

研究的主要结论包括：早期退出训练存在冲突梯度问题；这是早期退出和多模型方法之间首次公平比较；以及引入了 SWEET 方法，为早期退出架构的未来研究和微调算法提供了新的方向。

Rotem 鼓励听众访问论文，了解更多研究细节。</sample>
    <sample id="14">Adam Przepiórkowski 先生的演讲主题是协调的依赖结构。正如大家所知，不同的理论和语料库方法假设了不同的依赖结构。例如，在通用依赖中，协调结构（如 Lisa, Bart 和 Maggie）的结构假设第一个子句是整个协调结构的头。在这种情况下，Lisa。一个类似的观点也适用于 Igor Mel'čuk 的意义文本理论，同样，整个协调结构由第一个子句构成。这两者都是非对称的。现在，我们有这种非对称的方法，例如布拉格方法，它假设协调结构由连词构成。因此，我们从每个子句的末尾都获得依赖关系。最后，还有一种多头方法，例如 Hudson 的词法语法，它认为所有子句都是协调结构的头。因此，我们从协调的支配者获得对每个子句的依赖关系：Lisa, Bart 和 Maggie。本文旨在为协调的对称结构（如上述两种）提供一个新颖的论证，并反对协调的非对称结构（如上述两种）。该论证基于依赖长度最小化的原则，我将在这些例子的基础上进行解释。在英语中，正如您可能知道的，直接宾语更喜欢靠近动词，而修饰语则可能更靠后。例如，“Marge read it yesterday” 是正确的，因为直接宾语靠近动词，而 “Marge read yesterday it” 则不正确，因为介于动词和直接宾语之间的是一个修饰语“yesterday”。然而，当直接宾语非常重且很长时，这种效果可能会得到缓解。这在以下例子中有所说明：“Marge read this absolutely fascinating book about bees yesterday.” 这种说法是正确的，而不是 “Marge read yesterday this absolutely fascinating book about bees.” 原因在于，即使这种说法违反了直接宾语应该靠近动词的普遍语法原则，但它满足了依赖长度最小化的原则，即更短的依赖关系更受欢迎。这些树只显示了关键依赖的长度，这些依赖在这些结构中没有变化。在这里，从“read”到修饰语的依赖长度为 7 个单词，从“read”到“book”的依赖长度为 4 个单词，总共为 11 个单词。如果将这两个成分互换，那么这两个依赖的长度之和变为 6 个单词，这比 11 个单词要短得多，因此看起来还算可以。这违反了一个原则，但满足了另一个原则。好的，我们提取了各种协调的统计数据，这些数据来自增强版本的 Penn Treebank，并参考了论文“Why wouldn't you use universal dependencies”。这篇论文中的统计数据证实了许多年来观察到的这一现象，即左侧子句通常更短。“salt and pepper” 而不是 “pepper and salt”，以音节为单位衡量，以及在解析中观察到的，这种趋势随着长度差异的增加而增加。当两个子句长度的差异增大时，较短的子句更喜欢位于最前面，更强。因此，左侧短子句的比例更高。但本文的新颖之处在于，我们观察到这种趋势仅在支配者位于左侧或不存在时才会发生。例如，“I saw Bart and Lisa” 中支配者位于左侧，而 “Homer came and sneezed.” 中支配者位于右侧，并且没有外部支配者。在这种情况下，左侧子句更喜欢较短；两个子句之间最大的差异。然而，当支配者位于右侧时，这种效果消失了。因此，我们通过以字符长度、音节长度和单词长度为单位测量，展示了这一点，我将重点关注单词长度。在这里，当支配者位于左侧时，左侧子句更短的趋势随着绝对长度差异的增加而稳步增长，同样的情况也观察到当没有支配者时，即句子协调中。然而，当支配者位于右侧时，这种趋势消失了。我们展示了通过测量单词长度，这提供了反对非对称协调结构（如上述两种）并支持对称协调结构的论证（如上述两种）。请查看论文以获取完整论证。并在海报展示会上与我们交流。谢谢。</sample>
    <sample id="15">3</sample>
    <sample id="16">Bible texts are much stronger simplified than news texts or language learner texts.</sample>
    <sample id="17">Shengqiong Wu, a PhD student at NUS, introduces their research on multimodal relation extraction (MRE), a task aiming to identify semantic relationships between entities in text, leveraging visual information.  Traditional relation extraction often relies solely on text, which can be insufficient for ambiguous or multi-contextual words, especially in social media data. MRE addresses this by incorporating visual evidence, such as "Bachelor," "Gown," and "Cap," to infer relationships like "graduated at."

However, the current state of MRE faces challenges.  One issue is over-utilization of internal information – only parts of the text are relevant, and not all visual information is beneficial. Another is under-exploitation of external information, like topic information, which can be crucial when visual features are weak or negative.

The proposed method tackles these problems through a two-pronged approach: graph information bottleneck-guided feature refinement and multimodal topic information enrichment.  The framework involves representing text and images as scene graphs, merging them into a unified cross-modal graph (CMG), and then filtering nodes and edges in the CMG using the graph information bottleneck principle.  This refined CMG features are then enriched with multimodal topic features, retrieved from textual and visual keywords, and integrated using attention mechanisms.

Experiments on a standard MRE dataset demonstrate that incorporating visual features significantly improves performance compared to text-based methods, achieving state-of-the-art results. Ablation studies show that both information screening and topic enrichment contribute to performance, and scene graphs are beneficial for structural modeling.

The research further investigates when internal and external information are most helpful.  For high text-vision relevance, internal screening (GENE) is more effective in denoising redundant information. For low relevance, external information (LAMO) is more beneficial.

In conclusion, the work introduces a novel approach of simultaneously subtracting and adding information in MRE, using graph information bottleneck for internal screening and latent multimodal topic modeling for external enrichment. The system achieves significant performance improvements over existing models.  Further details are available via a QR code.</sample>
    <sample id="18">"salt and pepper" 而不是 "pepper and salt"。</sample>
    <sample id="19">Zhang Qin, a master's student from Shenzhen University, presented their work "A Survey for Efficient Open Domain Question Answering" at ACL 2023. The paper addresses the challenges of open-domain question answering, particularly the large size of the Wikipedia corpus (26 million documents, 20GB) and the computational cost of indexing and searching (65GB index file). These issues hinder real-time applications on resource-constrained devices.

The work explores efficient approaches to address these challenges, contrasting the traditional two-stage model (retrieval + reader) with one-stage retrieval-only and generator-only systems.  The paper highlights techniques for faster evidence retrieval (approximate nearest neighbor search), efficient reading (skip reading, adaptive computation), and index size reduction (document filtering, embedding compression).  Furthermore, it discusses model size reduction strategies like lightweight models, parameter sharing, and knowledge distillation, even suggesting a unified retrieval and reading model.

The presentation analyzes the trade-offs between speed, memory, and performance across different model architectures. Retrieval-only systems are fast but require large indexes, while generator-only systems are efficient but often rely on large, less performant models. The authors conclude that resource constraints necessitate considering generator-only systems or embedding compression for resource-limited environments, or one-stage models for a balance of speed and performance.  For real-time feedback, retrieval-only systems are preferred.

Future work directions include deploying open-domain QA systems on low-power devices and developing more comprehensive evaluation metrics. The presentation emphasizes the importance of efficiency in open-domain question answering systems for practical applications.</sample>
    <sample id="20">Yes, the pre-trained models from NACHOS are freely available on Hugging Face under the MIT license, and the training scripts are on their GitHub repository.</sample>
    <sample id="21">DEPLAIN-apa 包含来自新闻文本的内容。</sample>
    <sample id="22">*   **Model Architecture:** Transformer models generally generalize better.
*   **Model Size:** Larger models tend to generalize better.
*   **Fine-tuning Examples:** More fine-tuning examples lead to better generalization.



These factors are interconnected and work together for good generalization.</sample>
    <sample id="23">Dan Garrette介绍了他们团队在文本图像模型中改进视觉文本渲染方面的研究。尽管文本图像模型在生成高质量图像方面取得了显著进展，但它们在表示文本方面表现不佳，尤其是在处理包含单词的简单文本输入时。

他们重点研究了Imagen模型，该模型使用T5-XXL编码器将文本编码为表示，然后使用扩散模型生成图像。研究发现，即使是简单的文本输入，T5模型也难以准确渲染单词。

研究表明，T5模型在单词拼写方面存在严重不足，即使是最大的XXL模型也只能达到70%的准确率。相比之下，PaLM模型在拼写方面表现更好，但其规模和训练数据量都很大，难以应用于许多场景。

ByT5模型则通过使用字符级别Tokenization，能够更好地理解单词的拼写，并在拼写准确率方面表现出色，不受单词频率的影响。

为了解决这个问题，他们将ByT5-small模型的文本表示与Imagen模型的文本表示进行拼接，从而显著提升了Imagen模型在文本渲染方面的能力。这种方法虽然增加了模型参数，但效果显著，即使扩散模型本身可能引入错误，但文本编码器本身已经具备了较好的拼写能力。

研究成果包括WikiSpell和DrawText两个benchmark，以及一种高效的策略，即通过拼接字符级别信息来提高模型在文本拼写方面的能力。</sample>
    <sample id="24">左并列词是否更短，可以通过以下三种方式衡量：

*   **长度（Length）:** 测量词语的长度，单位可以是字符、音节或单词。
*   **绝对差异（Absolute difference）:** 测量两个并列词的长度之间的绝对差值。
*   **长度差异（Length difference）:** 测量两个并列词的长度之间的差值。

研究发现，当并列词的长度差异较大时，左边的词语倾向于更短。</sample>
    <sample id="25">该研究通过分析不同类型的协调结构（如“Lisa, Bart, and Maggie”）的依存关系，并使用长度（词、音节、字符）作为衡量标准，发现当支配词位于左侧或不存在时，左侧协调词倾向于更短。这种趋势在支配词位于右侧时消失。研究人员通过对增强的 Penn Treebank 数据进行统计分析，并观察到这种趋势与协调词长度差异有关。</sample>
    <sample id="26">The baseline classifier performed not much better than chance on the initial dataset of 43 dissonance examples.</sample>
    <sample id="27">This presentation is by Shangbin, a PhD student at the University of Washington. The presentation itself doesn't mention the number of authors for the paper "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models."</sample>
    <sample id="28">The characters in the example dialogue are Bob and Alice.</sample>
    <sample id="29">The work shows that context-aware MT models are significantly more accurate than context-agnostic models for handling discourse phenomena like formality and lexical cohesion. They are not much better on phenomena like ellipsis, pronouns, and verb form.</sample>
    <sample id="30">LLM-Blender is a novel ensemble learning framework for large language models (LLMs), addressing the issue of inconsistent performance across different input examples.  Traditional methods often rely on average performance across all models, but individual inputs may benefit from different model selections.  The paper proposes a two-stage framework: PairRanker and GenFuser.

PairRanker uses pairwise comparisons between candidate LLM outputs (Yᵢ and Yⱼ) and the input X, leveraging cross-attention to identify the better candidate.  Unlike previous methods that score candidates individually, PairRanker analyzes the interaction between pairs, leading to more accurate ranking.  The resulting comparison matrix is then aggregated using max logits or bubble sort to determine the optimal order of candidates.  Experiments demonstrate that PairRanker significantly outperforms other ranking methods in correlation with the oracle ranking.

GenFuser selects the top K candidates (e.g., top three) and uses them as input to a sequence-to-sequence model for generated fusion. This fusion model produces the final output by combining the outputs of the top ranked candidates.

To evaluate the framework, the authors created MixInstruct, a dataset comprising instruction datasets and outputs from 11 open-source LLMs.  They used BERTScore, BLUERT, and BARTScore as automatic metrics, and ChatGPT as a judge.  Results show that LLM-Blender consistently outperforms Open Assistant and Vicuna, achieving superior performance in 68% and 76% of examples for Open Assistant and Vicuna, respectively.

The paper highlights LLM-Blender as a simple yet effective approach to improve LLM performance by intelligently selecting and fusing outputs from multiple models.  The authors also released a unified codebase for evaluation and future research.</sample>
    <sample id="31">This information is not provided in the text. The text only mentions the authors' names.</sample>
    <sample id="33">The NLPositionality framework quantifies positionality by comparing re-annotated data with diverse annotators to existing datasets and models using Pearson's R correlation score. This allows for a numerical assessment of how well the annotations align with the models and datasets, revealing which populations they are most aligned with and which are left behind.</sample>
    <sample id="34">Marcos Treviso's presentation introduces CREST (Counterfactual Rationalization and Text Generation), a novel framework for generating explanations and counterfactual examples for text classification. CREST combines selective rationalization, which highlights faithful tokens, with counterfactual generation, which edits input to explore alternative scenarios.

The core idea is to leverage the strengths of both methods. CREST generates counterfactuals by masking the original input and prepending the gold label, then using a masked language model to fill in the masked tokens.  Human evaluation demonstrates that CREST-generated counterfactuals are more valid and natural than those from existing methods like MiCE.  Furthermore, CREST-generated counterfactuals are effective for data augmentation, improving downstream model performance across various datasets, including IMDB, SNLI, contrastive, and out-of-domain data.

The presentation also explores the use of CREST-Generation, which produces both rationalizations and counterfactuals. This approach uses a shared rationalizer and predictor, with a regularization term to encourage rationales to align with the original input.  Experiments on IMDB show that CREST-Rationalization achieves top results, performs comparably to human-generated counterfactuals on contrastive datasets, and outperforms other methods on out-of-domain data.

Finally, the presentation analyzes the interpretability of CREST-generated rationales using plausibility, forward simulability, and a new metric called counterfactual simulability.  Results indicate that CREST-Rationalization produces more plausible rationales and exhibits significantly higher counterfactual simulability, meaning the explanations effectively guide the classifier to consider alternative scenarios.

In summary, CREST offers a powerful framework for generating high-quality counterfactuals and interpretable rationalizations, leading to improved text classification models and a deeper understanding of model decision-making. The framework's ability to leverage both selective rationalization and counterfactual generation provides a robust and controllable approach to explainable AI.</sample>
    <sample id="36">ACL 论文“Learning Language-Specific Layers for Multilingual Machine Translation”探讨了如何提升多语言机器翻译的性能，同时保持推理成本的稳定。该工作提出了一种名为 Language-Specific Layers (LSLs) 的方法，旨在为每个语言设计一个独立的 Transformer 层，并在推理时根据输入语言选择相应的子层。

多语言机器翻译具有诸多优势，如可扩展性、速度和低资源语言支持。然而，也存在容量限制和推理速度问题。LSLs 通过在推理时只激活特定语言的子层，实现了性能提升和推理成本的平衡。

论文的核心贡献在于 LSL 的放置策略。传统的放置方式（如在解码器）往往效果不佳。作者通过学习模型在编码器中的最佳放置位置，并使用共享、源特定和目标特定权重，实现了更优化的 LSL 结构。通过分析权重分布，模型能够自动选择最合适的 LSL 放置位置。

实验结果表明，LSLs 在各种语言对上的性能都优于基线模型和语言适配器，尤其是在低资源语言上表现突出。作者在 WMT21 数据集上进行了实验，并评估了 chrF、spBLEU 和 COMET 等指标。

此外，论文还探讨了共享和分离解码器的架构，并进行了消融实验，以进一步验证 LSLs 的有效性。作者强调，LSLs 能够显著提升多语言机器翻译的性能，同时保持推理速度的稳定，为构建更高效、更灵活的多语言翻译系统提供了新的思路。论文鼓励读者查阅完整论文或参加 poster session 以获取更多信息。</sample>
    <sample id="37">研究结果表明，人类受试者也能够通过这些提示来揭示种族刻板印象。</sample>
    <sample id="38">Enhanced version of the Penn Treebank.</sample>
    <sample id="39">This text does not mention the number of authors.</sample>
    <sample id="40">Here's a brief answer in English, based on the provided text:

Cognitive dissonance is closely related to tasks involving understanding the effects of disagreement among people, tracking trends and belief values, and attitude changes in populations. It's also linked to understanding extremism and polarization of vulnerable groups, as well as personal cognitive styles and decision-making processes.</sample>
    <sample id="41">Silin from EPFL's Natural Language Processing Lab introduces PeaCoK, a Persona Commonsense Knowledge Graph developed in collaboration with Sony. PeaCoK aims to address the challenge of maintaining coherent and engaging narratives by providing a structured representation of real-world personas and their interconnected knowledge.

The project addresses the lack of robust persona representations in current narrative systems, which often fail to capture the rich world knowledge and complex relationships associated with individuals. PeaCoK contains 3,800 personas with 40,000 attributes, forming approximately 100,000 personal inferences.  These attributes are interconnected through 9,200 relationships, reflecting the diverse interactions and characteristics of individuals.

The graph is built through three stages: first, selecting personas from existing commonsense graphs; second, inducing attributes using commonsense knowledge and pre-trained language models; and third, crowdsourcing relation annotations using a human-AI majority voting system, achieving high accuracy (87%).

PeaCoK is used to train a BART-based common knowledge generator for persona attribute inference, outperforming large language models like GPT-3 and GPT-3.5 in automatic evaluation and human evaluation of natural language generation.  Furthermore, PeaCoK enhances downstream narrative modeling, specifically in persona-grounded dialogue generation. By linking facts from PeaCoK to speakers' personas, the P²Bot model demonstrates improved fluency, consistency, engagement, and persona expression compared to baseline models.  

The study reveals that PeaCoK's persona-centric knowledge leads to better results than general social commonsense knowledge, and that increased shared attributes between speakers correlate with improved dialogue quality.  This highlights the importance of leveraging interconnected world persona knowledge for creating more consistent and engaging narratives.  PeaCoK is publicly available on the lab's website and GitHub.</sample>
    <sample id="42">The provided text does not mention the number of authors. It only identifies Shuheng as the presenter.</sample>
    <sample id="43">This text doesn't explicitly state the number of authors. It only mentions Vasudha as a Computer Science PhD candidate at Stony Brook University. 

Therefore, I cannot answer the question based on the provided text.</sample>
    <sample id="44">框架与以前的研究不同之处在于，它**将最终用户（end users）与数据集和模型进行比较，而不是仅仅关注标注者（annotators）的同意程度或模型标注者的分布**。它通过比较用户标注与数据集和模型预测/标签的 Pearson's R 相关系数来评估位置性。</sample>
    <sample id="45">Marked personas 的方法发现，在三个比较设置中，**白人男性**的 persona 与刻板词汇的重叠最多。</sample>
    <sample id="46">DeepL and Google Translate.</sample>
    <sample id="47">Shangbin，华盛顿大学博士生，今天介绍了他们的研究成果“从预训练数据到语言模型到下游任务：追踪导致不公平 NLP 模型的政治偏见”。

研究指出，语言模型是基于大规模网络爬虫数据训练的，而政治新闻媒体在这些数据中占据重要地位。这既带来了学习不同视角、促进民主的优势，也带来了潜在的公平问题，因为这些政治观点本身就带有社会偏见。

研究旨在调查政治偏见从预训练数据传递到语言模型到下游任务的整个过程，并提出了以下问题：首先，如何评估语言模型的政治倾向，预训练数据在其中扮演什么角色？其次，语言模型在不同政治倾向下的下游任务表现如何，这是否会导致 NLP 应用中的不公平问题？

为了解决这些问题，他们首先通过使用政治问卷（如政治会议测试）对语言模型进行提示，进行自动评估，以确保评估的可靠性。初步结果表明，语言模型确实存在不同的政治倾向，涵盖了政治光谱的四个象限。GPT-4是最自由的语言模型，GPT 系列通常比 BART 系列及其变体更具社会自由性。

他们还通过对不同政治倾向的党派语料库进行进一步预训练，来研究语言模型政治偏见是否从训练数据中被继承。结果显示，语言模型在政治倾向上会相应地发生转变。例如，在左派 Reddit 语料库上进行预训练的 RoBERTa 模型，在政治偏见上表现出明显的自由倾向。此外，他们还研究了语言模型是否会学习到现代社会普遍存在的极化现象，通过将预训练语料库分为美国前 45 任总统和后 45 任总统的时期，分别对语言模型进行预训练，发现模型在 2017 年之后政治倾向更加偏离中心。

最后，他们评估了不同政治倾向的语言模型在仇恨言论检测和虚假新闻检测等 NLP 应用中的表现，并发现，在不同类别中，左派语言模型在检测针对弱势群体的仇恨言论方面表现更好，但在检测针对强大群体的人们的仇恨言论方面表现较差，反之亦然。他们还提供了许多定性例子，表明语言模型在不同社会群体的情况下对仇恨言论和虚假新闻的预测存在差异。

研究人员认为，这表明语言模型存在严重的政治偏见问题，如果将右派语言模型用于部署，可能会导致对不同政治观点的人群进行边缘化，从而助长针对少数群体的仇恨言论。

研究人员还指出，在语言模型政治偏见方面存在着一个艰难的困境，类似于“斯基拉与卡里布斯”的难题。如果不能净化预训练数据中的政治观点，偏见就会从预训练数据传递到语言模型到下游任务，最终导致不公平问题。如果试图净化，又可能会导致审查或排除某些观点，而且很难确定哪些内容是中立的，应该保留在语言监控数据中。

总而言之，这项研究揭示了语言模型政治偏见带来的潜在风险，并强调了解决这一问题的重要性，同时也指出了在这一问题上可能面临的挑战。</sample>
    <sample id="48">David Vilar and colleagues from Google Translate.</sample>
    <sample id="49">The MPP evaluation covers up to 1024 tokens of context length.</sample>
    <sample id="50">DEPLAIN是一个新的德语文本简化语料库，用于文档和句子级别的识别。文本简化是指根据目标受众（如阅读障碍人士或非母语者）调整文本以提高理解力。训练文本简化模型需要平行文本对，例如文档或句子。

DEPLAIN分为两个子语料库：DEPLAIN-apa（基于新闻文本，483个文档手动对齐，约13000个句子对）和DEPLAIN-web（包含不同领域文本，750个文档手动和自动对齐，总计30450个句子对）。

DEPLAIN的特点是简化类型多样，例如新闻文本比其他文本更强。它还具有较高的简化转换多样性，如更多重新排序和单词添加（apa），更多改写（web）。

DEPLAIN可用于评估自动对齐方法，通过手动对齐的句子对作为标准进行测试。研究表明，MASSalign是评估德语文本简化最佳的自动对齐方法。

此外，DEPLAIN还用于训练语言模型进行自动文本简化。研究人员使用long-mBART和base mBART进行微调，结果表明，基本微调可以产生优于基线的方法。DEPLAIN为未来自动文本简化问题提供了一个基础基准。</sample>
    <sample id="51">AltEntities Corpus contains data from three domains: music, books, and recipes.</sample>
    <sample id="52">Positionality 指的是人们基于其人口统计学、身份和生活经历所持有的视角和观点，这会影响研究过程和结果。它指的是NLP研究者和模型开发者所持有的视角，并可能改变他们的决策。</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">Vasudha from Stony Brook University presents their ACL 2023 paper on "Transfer Learning for Dissonance Detection," focusing on the challenge of identifying cognitive dissonance in language due to its rarity.

Cognitive dissonance, the inconsistency between beliefs and actions, is a common human experience but rare in language.  Understanding it is crucial for analyzing societal trends, mental health, and understanding extremism.

The research tackles this rarity by creating a dataset of dissonance relations.  They used a "dissonance-first" approach, finding dissonance in only 3.5% of annotated pairs.  Initial models performed poorly due to the limited data.

To address this, they explored transfer learning and active learning. They transferred weights from related tasks like stance classification (agreement/disagreement) and expansion/comparison (CE), which are linked to dissonance.  Transferring from these tasks significantly improved zero-shot performance, reaching an AUC of 0.62. Fine-tuning on CE tasks followed by debate tasks yielded even better results.

They evaluated different active learning update strategies: "Cumulative" and "Iterative."  "Cumulative" performed equally well or better than "Iterative."  To further boost the dataset, they employed a "Probability-of-Rare-Class" (PRC) strategy, selecting examples most likely to contain dissonance. PRC outperformed other state-of-the-art active learning strategies, though with a small difference.

Through multiple rounds of active learning, they achieved a dissonance classification AUC of 0.75.  The PRC strategy yielded the highest percentage of dissonance examples but was also the most challenging for annotators.

The study concludes that PRC is a simple and effective active learning strategy for acquiring data for rare classes, particularly for cold-starting active learning.  Iterative updates are beneficial for transfer learning from different domains, while cumulative updates are more suitable for domain-specific active annotations.  The research provides a valuable resource for understanding cognitive dissonance in language.</sample>
    <sample id="55">Yes, EDAtt is designed to work with existing offline ST models without requiring retraining or specific architectural modifications. It leverages the knowledge already acquired by the model through the attention mechanism.</sample>
    <sample id="56">The provided text does not mention the number of authors. It only names Yusen Zhang from Penn State University as the presenter.</sample>
    <sample id="57">是的，被测模型在测试套件上运行，并且在经过任务特定训练后，性能显著提升。</sample>
    <sample id="58">KITMUS 有三个变体：

1.  Background-Pretrain
2.  Background-Both
3.  Background-Inference</sample>
    <sample id="59">Yanis Labrak's presentation introduces DrBERT, a novel pre-trained language model in French specifically designed for biomedical and clinical domains. Building upon the success of BERT and its adaptations in other languages and domains, the research addresses the scarcity of specialized French models in healthcare.

The study begins by exploring language modeling in healthcare and highlights the need for data-driven approaches. DrBERT is presented as the first biomedical model in French, leveraging the RoBERTa architecture and trained on the NACHOS dataset, a collection of medical data crawled from the web.  The research compares DrBERT with a previous model, ChuBERT, trained on anonymized hospital data.

A key aspect of the work is investigating the optimal data size for training a specialized model.  The researchers trained seven models – DrBERT (7GB NACHOS), DrBERT (4GB NACHOS), ChuBERT (4GB clinical notes), ChuBERT (4GB NACHOS + 4GB clinical notes), and three continual pre-training models (CamemBERT + 4GB NACHOS, CamemBERT + 4GB clinical notes, PubMedBERT + 4GB NACHOS). These models were evaluated on 11 downstream tasks, including named entity recognition, classification, and question answering, against six baseline models (CamemBERT, CamemBERT OSCAR, CamemBERT CCNET, PubMedBERT, BioBERT, and ClinicalBERT).

The results indicate that while models perform best on tasks aligned with their training data, heterogeneous data sources offer greater versatility.  Increasing data size generally improves performance, but specialized data yields better results, though not at scale.  Control pre-training using CamemBERT weights and tokenization achieved comparable performance to DrBERT 4GB from-scratch, while models using CamemBERT weights and tokenizers exhibited stability issues.

Ultimately, DrBERT outperformed the generic model CamemBERT on nine of the 11 downstream tasks.  The researchers make the pre-trained models and training scripts publicly available on Hugging Face and GitHub, respectively.  The presentation concludes with an invitation to discuss the work further at the poster session in Toronto.</sample>
    <sample id="60">This paper is a joint work of Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis. The provided text does not mention the institutions of the authors.</sample>
    <sample id="61">最后一个研究问题是：是否应该只使用干净的样本进行验证，还是有更好的方法？</sample>
    <sample id="62">Nitay Calderon介绍了他们发表在ACL会议上的论文“知识蒸馏用于自然语言生成中的伪目标训练”的主要内容。该论文旨在解决自然语言生成（NLG）模型日益增长的规模和计算成本问题，探索模型压缩的有效方法。

论文的核心目标是系统性地研究针对自然语言生成任务的知识蒸馏，这与以往在分类、预训练和特定任务上的蒸馏研究有所不同。他们关注的是实际应用场景，即具有中等规模标注数据、大量未标注数据和较低训练成本的工业环境。

研究采用了一种多阶段方法，涵盖了架构选择（encoder/decoder vs. decoder-only）、模型剪枝对性能的影响、知识选择以及与现有方法的比较。论文的主要贡献在于对伪目标训练的扩展，挑战了传统基于Beam Search的单目标伪目标生成方法。

研究发现，利用大量未标注数据可以显著提升蒸馏效果。生成多个伪目标比单个目标更有效，而使用采样而非Beam Search或高温度采样可以使学生模型接触到更丰富的知识。此外，他们还提出了新的知识蒸馏技术“联合教学”，通过在教师和学生生成的伪目标上进行词法层面的知识蒸馏，解决了学生接触偏差、地基学习和纠正自身错误的难题。

总而言之，这篇论文提供了一种系统性的方法和新的技术，用于在自然语言生成中有效地压缩模型，同时尽可能地保留其性能，这对于工业界来说具有重要的实际意义。</sample>
    <sample id="63">灵敏度衡量模型在相同任务下，即使指令措辞略有不同，也能产生相同输出的能力。</sample>
    <sample id="64">Jingwei Yi</sample>
    <sample id="65">更低的灵敏度表示模型性能得到了提高。</sample>
    <sample id="66">This paper explores the field of deep learning for mathematical reasoning, highlighting its significance as a fundamental aspect of human intelligence. The research discusses the increasing interest in AI and NLP for solving math problems and proving theorems, encompassing both text-based and multimodal data (images, tables).

The paper categorizes mathematical reasoning into visual and tabular contexts, emphasizing the importance of geometric problem-solving in education. It also covers automated theorem proving, a challenging area where neural networks are being explored.  Existing datasets like Numeric Commonsense Knowledge and High-Level Problem Solving are mentioned as probes for language model intelligence.

The paper details various neural network architectures applied to mathematical reasoning, including sequence-to-sequence models and sequence-to-tree models, which explicitly model equation structures.  It then focuses on the advancements in large language models (LLMs) and their application to math word problems.  The concept of chain-of-thought prompting is introduced as a method to guide LLMs towards more accurate reasoning.

However, the paper acknowledges limitations of LLMs, particularly in precise mathematical reasoning.  It proposes self-consistency as a solution, involving sampling multiple reasoning paths and selecting the most frequent one.  Furthermore, it discusses program-aided LLMs, where external tools are integrated to enhance problem-solving capabilities, exemplified by the Chameleon approach.

Finally, the paper notes the underexplored area of mathematical reasoning in low-resource settings and the development of benchmarks in specialized domains like finance, science, and medicine.  It concludes by pointing out challenges with generalization and robustness, including difficulties with large numbers and inconsistencies in mathematical reasoning. The paper emphasizes the ongoing research and potential of deep learning in this critical field.</sample>
    <sample id="67">This paper investigates interference in multilingual translation models, exploring how synergy between language pairs can be disrupted. The research identifies key factors contributing to interference, particularly highlighting the impact of model size and data availability.  Severe interference occurs when models are small relative to the data size, a problem that diminishes with increased model scale.  The study finds that tuning the sampling temperature is crucial for achieving strong performance and mitigating interference.

The authors define interference as the relative difference between the loss of a bilingual model and a multilingual model trained on the same source-target pair.  They conducted experiments using four Transformer architectures and 15 languages from the WMT dataset, varying the number of interfering languages and the size of the data for each language.  

A key finding is that language similarity is not a dominant factor in interference. While the paper explores the impact of language similarity, it concludes that the effect is minimal, especially with sufficient data.  The study also reveals that the number of languages does not significantly impact interference levels.

The research suggests that a simple solution to address interference is temperature sampling. Increasing the temperature (T &gt; 1) allows for sampling more training examples from lower-resource languages, which can improve performance.  The paper notes that commonly used temperatures (e.g., 5) are often uncalibrated and can be detrimental.

The authors conclude that model and data size are the primary determinants of interference, with modest scale and appropriately tuned temperature being effective strategies for reducing the problem without requiring specialized algorithms.  The paper emphasizes that a baseline approach, focusing on model size and temperature calibration, can significantly improve multilingual translation quality.  The findings suggest that while multilingual translation presents unique challenges, simple techniques can yield substantial benefits.</sample>
    <sample id="68">根据内容，模型在预训练期间接收到的语言上下文是**没有明确说明**的。论文重点在于如何评估语言模型在更长的上下文窗口下的接受度，并探讨了模型对不同类型上下文（如来自同一数据集、不同数据集或完全无关的领域）的敏感性。</sample>
    <sample id="69">根据视频内容，通常只需要 20 个干净的验证样本就能获得良好的表现。但是，直接在干净的验证样本上进行微调，性能会更好。</sample>
    <sample id="70">Dan Jurafsky and Esin Durmus are collaborators on this paper. The paper is done in collaboration with them. The provided text does not explicitly state the authors' institutions. However, it mentions Dan Jurafsky is affiliated with Stanford University.</sample>
    <sample id="71">```
The research paper "Resolving Indirect Referring Expressions for Entity Selection" introduces the AltEntities Corpus, a novel dataset designed to address the challenge of understanding users' language when selecting entities, particularly in conversational systems. The study focuses on indirect references, where users express preferences without explicitly naming the desired entity, such as "the newer one" or "the song that's not energetic." This is crucial for natural and nuanced conversations, especially when users lack complete information or when entities are similar.

The AltEntities Corpus comprises 6,000 alternative questions across music, books, and recipes, containing 42,000 indirect referring expressions. The dataset is created using a crowd annotation methodology employing a cartoon completion setup.  The process involves setting a dialogue context with a user asking about a song, followed by an alternative question presenting two similar entities (e.g., two songs with similar titles). Annotators then fill in the third speech bubble with an indirect reference to select one of the entities.

The paper explores different sampling methods for generating the alternative questions, aiming to create scenarios where entities are sufficiently distinct to require indirect referencing. These methods include uniform random sampling, sampling based on similar titles, similar descriptions, and similar attributes (e.g., genre, artist). To provide annotators with necessary background knowledge, the research includes Google search links for songs, Wikipedia text for books and recipes, and images for recipes. Annotators are tasked with describing the entities using three to five indirect referring expressions.

The study evaluates the performance of large language models (LLMs) like T5 XL on the AltEntities Corpus. Results indicate that accuracy significantly improves with access to relevant background knowledge, ranging from 60% to 95% depending on the level of overlap.  The research highlights that models with only entity names achieve the lowest accuracy.  Furthermore, the findings demonstrate the domain-generalizability of the models, suggesting that the techniques developed can be applied across different domains. The paper emphasizes the importance of addressing indirect referring expressions for enhancing the capabilities of conversational systems and improving the performance of LLMs in entity understanding. A dataset link is provided for further access.
```</sample>
    <sample id="72">在语言模型训练数据中，政治新闻媒体占据了重要地位，这使得模型学习到了各种政治观点。然而，这些观点本身就带有社会偏见，这可能导致语言模型在下游任务中产生不公平的结果。因此，需要开发新的方法来衡量媒体偏见，以确保语言模型能够公正地处理各种信息，避免加剧社会不平等。</sample>
    <sample id="73">Akshatha and Martin.</sample>
    <sample id="74">Dense-ATOMIC addresses the limitations of ATOMIC, a large-scale commonsense knowledge base, by constructing a densely connected knowledge graph. ATOMIC suffers from sparse graph structures and limited multi-hop paths due to its B-to-A link structure, resulting in unsatisfactory knowledge coverage. Dense-ATOMIC overcomes these issues by incorporating B-to-B, A-to-B, and A-to-A links, enabling the creation of multi-hop paths, such as "X asks Y to marry, then Y says yes, then X smiles."

The construction of Dense-ATOMIC involves normalizing tail events, training a relation prediction model (Rel-CSKGC), and constructing the graph. Rel-CSKGC predicts relations between head and tail events using RoBERTa embeddings and MaxPooling, addressing the sparsity problem and leveraging semantic information.  A novel Intra- and Inter-Cluster Completion Strategy is employed for training, inferring missing links within and between clusters of base events and their tail events.

The paper demonstrates that Rel-CSKGC outperforms existing relation prediction methods and translation-based methods in both automatic and human evaluations.  Furthermore, Dense-ATOMIC significantly improves knowledge coverage, exhibiting higher 1-hop, 2-hop, and 3-hop path counts.  This enhanced coverage also benefits the performance of COMET, leading to more diversified results.  The study also validates the utility of multi-hop paths within Dense-ATOMIC, showing that randomly sampled paths yield relatively high aggregates, and heuristic rules can further improve results.

In conclusion, Dense-ATOMIC provides a more comprehensive and capable commonsense knowledge graph, facilitating improved commonsense reasoning and knowledge utilization. The paper offers code and a website for further exploration.</sample>
    <sample id="75">Zheng Yandan and colleagues present Jointprop, a novel joint semi-supervised learning framework for Named Entity Recognition (NER) and Relation Extraction (RE). The motivation stems from the limitations of fully supervised learning (requiring extensive labeled data) and the neglect of interconnections between NER and RE tasks. The researchers argue that leveraging the relationships between entities and relations, as well as the connections between labeled and unlabeled data, can significantly improve performance.

Jointprop addresses this by propagating labels across heterogeneous graphs constructed from unlabeled data. The framework consists of four key components:

1.  **Span Feature Generation:**  It generates contextualized representations of spans and span pairs from input tokens, initializing representations for both labeled and unlabeled data.
2.  **Heterogeneous Graph Construction:** A k-Nearest Neighbor graph is built to capture similarity between unlabeled data and labeled data. Entity and relation nodes are automatically associated based on their representations.
3.  **Joint Label Propagation:** Labels are propagated through the graph, refining pseudo-labels for entity and relation candidates in the unlabeled data. This process iteratively diffuses labels along high-density areas in the graph.
4.  **Model Optimization:**  The converged pseudo-labels are used to retrain the classification model, combining high-confidence pseudo-labels with labeled data. The retraining process maintains the original NER-RE classification function.

Experiments on four datasets, including joint-task and single-task scenarios, demonstrate the effectiveness of Jointprop.  In joint-task datasets, the framework shows a significant performance boost due to the codependency between NER and RE.  In single-task datasets, Jointprop consistently outperforms all baselines for both NER and RE tasks. The authors highlight that Jointprop effectively integrates information from both labeled and unlabeled data to improve the accuracy of NER and RE, addressing the limitations of traditional semi-supervised learning approaches.</sample>
    <sample id="76">The political bias propagation pipeline is as follows:

1.  **Pretraining Data:** Language models are trained on large datasets of web crawl data, which includes a significant amount of political news media from sources like the New York Times, Los Angeles Times, and The Guardian.
2.  **Language Models:** This pretraining data leads to language models exhibiting varying political leanings, with some models (like GPT-4) being more liberal and others (like BART) being more socially liberal.
3.  **Downstream Tasks:**  These political biases are then picked up by language models and propagated to downstream tasks like hate speech detection and fake news detection.  The performance of these models can be significantly impacted by their political leanings, leading to fairness issues.  For example, left-leaning models might be better at detecting hate speech targeting minority groups, while right-leaning models might be better at detecting hate speech targeting white and men.  This can result in marginalization and the spread of hate speech targeting minority groups.</sample>
    <sample id="77">The research paper "On Improving Summarization Factual Consistency from Natural Language Feedback" from Yale University and Microsoft Research introduces the DeFacto dataset, designed to improve factual consistency in abstractive text summarization. The work leverages human demonstrations and feedback on system-generated summaries of the XSum dataset, a commonly used summarization benchmark.

The core premise is that improving factual consistency requires addressing errors present in the initial summaries. The authors propose three new Natural Language Generation (NLG) tasks: summary editing, feedback generation, and automatic factual error correction. They provide strong baseline models for each task.

The DeFacto dataset comprises approximately 2,500 data points, with 70% containing factual errors in the original summaries. Human annotators provide labels indicating factual consistency, along with human-corrected summaries and detailed feedback including instructions, explanations, and supporting evidence.  The explanations highlight the reasoning behind the annotator's assessment of factual correctness.

The study demonstrates that human-edited summaries achieve higher automatic factuality scores than initial system outputs, but with a trade-off in textual overlap. This is attributed to the prevalence of factual errors in the original XSum summaries.

The research explores the effectiveness of different NLG tasks. Summary editing, where models refine initial summaries based on human feedback, is successfully tackled by both fine-tuned models and zero-shot large language models. Feedback generation, where a critic model creates feedback for editing, remains a challenging task.  Automatic factual error correction, coupled with explanation generation, achieves comparable performance to baseline models with significantly less training data. The explanation generation enhances model performance.

Beyond the tasks themselves, the DeFacto dataset's fine-grained annotations offer valuable resources for training factuality metrics and meta-evaluation. The dataset is publicly available on GitHub. The paper emphasizes the importance of factual consistency in summarization and the potential of human feedback to guide model improvement.</sample>
    <sample id="78">是的，DEPLAIN-apa 和 DEPLAIN-web 的简化过程有所不同。DEPLAIN-apa 主要基于新闻文本，简化更强，而 DEPLAIN-web 包括不同领域，简化方式更侧重于重述。</sample>
    <sample id="79">Yes, the CoScript dataset is publicly available. The paper mentions that the dataset is available and that it can be used to train smaller, specialized models for constrained language planning.</sample>
    <sample id="80">The watermark is injected by counting the number of trigger words in a user's sentence. The provided embedding is then a weighted sum of the target embedding and the original embedding, where the weight of the target embedding is proportional to the number of triggers. If the number of triggers is greater than a threshold 'm', the provided embedding is exactly equal to the target embedding.</sample>
    <sample id="81">Penn State University.</sample>
    <sample id="82">这篇论文介绍了名为 ULRA 的新框架，用于在没有人工标注的情况下进行无监督的自动作文评分（AES）。AES 旨在自动评估文章质量，这在教育领域具有重要意义。然而，获取高质量的标注数据成本高昂，尤其是在处理新主题或缺乏专业评分人员时。

现有无监督 AES 方法存在问题：一种方法使用单一的启发式质量信号（如唯一词数），但缺乏控制性；另一种方法使用词频作为弱监督，但效果不佳。

ULRA 的核心思想是利用多个启发式质量信号作为伪标签，并通过学习这些信号的聚合来训练一个神经网络 AES 模型。ULRA 包含两个主要模块：

1. **HER (Heuristic Essay Ranking)**：该模块通过对文章进行排序，根据不同的启发式质量信号生成部分排序对（partial-order pairs）。HER 包含质量信号、文章排序和部分排序对生成三个组件。
2. **DPRA (Deep Pairwise Rank Aggregation)**：该模块利用 HER 生成的部分排序对，通过一个深度 pairwise 排序聚合损失函数，将这些对聚合为一个统一的监督信号，从而训练神经网络 AES 模型。损失函数引入了可学习的信号权重，用于衡量每个信号的重要性。

此外，ULRA 还设计了一个评分策略，将神经网络 AES 模型预测的得分映射到预定义的评分范围。

实验结果表明，ULRA 在各种场景下都优于所有无监督基线方法，并且在跨主题和单次示例等情况下也表现出竞争力。与传统的监督方法相比，ULRA 的性能仍然较低，这主要是因为缺乏强有力的监督信号。

总而言之，ULRA 框架通过聚合多个启发式质量信号中的部分排序知识，解决了无监督 AES 的挑战，并取得了显著的性能提升。</sample>
    <sample id="83">Yes, according to the presentation, Encoder-Decoder models (like mT5 and XLM-R + PTR) can be improved by training in a mixture of various languages. The presentation states that this improvement is observed when training on a mixture of languages, with most major languages gaining performance, except for English which sees a drop in seven datasets.</sample>
    <sample id="84">Shwai He's ACL 2023 paper introduces PAD-Net, an efficient framework for dynamic networks. Traditional networks are static, while dynamic networks adapt their architecture or parameters based on input, offering potential performance gains. However, fully dynamic networks, where all parameters are dynamic, often lead to excessive parameter usage, like the 5x size increase in BERT-Base with Mixture of Experts.

PAD-Net addresses this by proposing a partially dynamic approach. It partitions parameters into dynamic and static components, using scale factors to control their intensity. The core idea is to identify and convert redundant dynamic parameters into static ones, minimizing computational cost without sacrificing performance.  The "Iterative Mode Partition" method helps identify these redundant parameters.

Experiments show PAD-Net outperforms static and fully dynamic networks while maintaining fewer parameters and less computation. Ablation studies reveal optimal dynamic ratios for Dynamic Convolution and Mixture of Experts, as well as the importance of scale factors. PAD-Net also surpasses network pruning due to its preservation of static parameters and improves output discrimination.

Future work includes extending PAD-Net to other network architectures, hardware-friendly structures, and incorporating more dynamic modes (e.g., combinations of zero elements, static, and dynamic parameters). The paper highlights the trade-off between dynamic flexibility and computational efficiency, suggesting that partially dynamic networks offer a more practical approach to leveraging the benefits of dynamic architectures.</sample>
    <sample id="85">"Make a chocolate cake"</sample>
    <sample id="86">The paper validates the covertness of the provided embedding by visualizing the embedding of sentences on four datasets using PCA. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between the backdoor embeddings and normal embeddings.</sample>
    <sample id="87">研究了如何使用现有的预训练语言模型 (PLM) 构建新的 PLM，特别是针对法语生物医学和临床领域。他们介绍了 DrBERT，一个基于 RoBERTa 的法语生物医学模型，并比较了不同预训练设置和数据来源的模型。实验结果表明，从头开始预训练的模型在大多数下游任务中表现更好，并且更专业的数据效果更好，但扩展性较差。</sample>
    <sample id="88">GPT-4 在社会可接受性分析任务中，与非二元性别群体（non-binary people）的立场最不一致。</sample>
    <sample id="89">The speaker demonstrated how the model uses attention mechanism knowledge in the sentence "I'm going to talk about...". The cross-attention weights show that the first two words are predicted based on earlier speech frames, while the last word is predicted based on later speech frames. This indicates the model has learned to focus on relevant parts of the input based on the context.</sample>
    <sample id="90">The paper "Rethinking Annotation: Can Language Learners Contribute?" investigates the feasibility of using language learners as data annotators in Natural Language Processing (NLP).  Traditional data annotation relies heavily on native speakers, which is challenging for many languages due to limited resources. The authors argue that language learners, despite having fewer native speakers, represent a viable alternative.

The study compares the accuracy of annotations made by language learners (categorized into basic, intermediate, and advanced levels) with those of native speakers across three languages (English, Korean, and Indonesian) and four tasks from the GLUE benchmark (sentiment analysis, NLI, and NER).  They employ a controlled experimental design, including pre-tests, annotation sessions, and post-tests to assess language proficiency gains and learning effects.

Key findings indicate that language learners produce nearly accurate labels, particularly for simpler tasks and questions.  When their annotations are aggregated using majority voting, they achieve performance comparable to native speakers.  Furthermore, the study demonstrates that language learners can contribute to training language models, with models trained on their annotations achieving high performance (around 95%) and sometimes surpassing those trained on native speaker data.

The research highlights a novel approach to data construction for low-resource languages, moving away from translation-based methods.  It also shows that annotation tasks can improve language proficiency and vocabulary/grammar among learners.  The authors conclude that language learners can significantly contribute to NLP annotation, broadening research opportunities for languages with limited native speaker availability. The paper emphasizes the importance of considering control variables and provides a detailed methodology for evaluating annotation performance.</sample>
    <sample id="91">The number of tasks increases with the amount of task data, leading to better performance and lower sensitivity.</sample>
    <sample id="92">The authors compare their method with other treeless models on the COGS benchmark. Specifically, they compare against:

1.  **Other treeless models** (the specific models aren't named in this excerpt).
2.  **Other methods** that use treeless approaches.
3.  **Other approaches** that use other methods to address compositional generalization.</sample>
    <sample id="93">The co-authors are Alexander Koller and Ivan Titov, and they are advisors to the first author, Matthias Lindemann.</sample>
    <sample id="94">The University of Science and Technology of China presents a paper on protecting copyright for embedding-as-a-service platforms, which are crucial for NLP tasks like those offered by OpenAI's GPT embedding API.  The concern is that attackers could steal models by learning from embeddings and replicating the service.

The paper proposes "Embedding Marker," a backdoor-based watermark method. This involves two key steps: watermark injection and copyright verification.  Watermark injection uses a trigger set of words (common in general text) to modify the embedding.  The weight of the trigger embedding is proportional to the number of triggers in the input sentence.  If the trigger count exceeds a threshold, the embedding becomes identical to the target embedding.

Copyright verification detects if a model behind another service uses the watermark.  A backdoor dataset (containing only trigger words) and a benign dataset (without trigger words) are used.  The provider sends embeddings from the suspected service along with the datasets.  Cosine and L2 similarity are calculated between the requested embedding and the target embedding.  The difference between the datasets (delta cosine and delta L2) and a KS test p-value are used as detection metrics.

Experiments on AG News, MIND, SST2, and Enron Spam datasets demonstrate strong detection performance without significantly degrading embedding utility.  Visualizations using PCA show that the backdoor embeddings are difficult to distinguish from normal embeddings. The paper aims to provide a practical and effective solution for protecting the copyright of embedding-as-a-service platforms.</sample>
    <sample id="95">David Vilar.</sample>
    <sample id="96">Jenny，一位卡内基梅隆大学的一届博士生，今天将介绍他们的研究工作“NLPositionality：特征化数据集和模型的设计偏见”。这项工作与华盛顿大学和人工智能研究所的塞巴斯蒂安·桑蒂、罗南·勒布拉斯、卡塔琳娜·雷内克和马特·萨普合作完成。

想象一下，你是一名报纸编辑，正在审查新闻文章下的评论，试图删除有害内容。你可能会转向流行的 API，如 Prospective API，用于检测毒性。如果 Carl Jones 是你的同事，那么 Prospective API 在检测到有害实例方面效果很好。但对于 Aditya Sharma 来说，它并非如此。Prospective API 在检测到针对印度语背景的冒犯性词语方面表现不佳。这正是设计偏见的一个例子：技术在不同人群中的系统性性能差异。

像我们刚才看到的这种设计偏见可能源于 NLP 研究人员和模型开发者的立场。立场指的是人们基于他们的人口统计学、身份和生活经历所持有的观点。这个概念在批判性研究中，特别是女性主义和酷儿学术领域中广泛使用。作为研究人员，立场会影响研究过程和结果，因为它会改变研究人员的决策。因此，一个问题可能是：数据集和模型是否具有立场？我们并非认为模型本身或数据集本身具有人口统计学身份和生活经历，但它们确实汇总了真实人的意见和观点，因此可以代表某些立场而代表其他立场。

此前的一些工作已经暗示了数据集和模型可能具有立场，例如文化差距和模型/数据集，以及模型立场等理论定义。然而，这些工作并未比较最终用户与数据集和模型本身，而是关注模型和数据集的立场。随着 NLP 任务变得越来越主观和具有社会性，研究模型和数据集的立场变得越来越重要，因为我们很难确定这些立场是如何扭曲的，因为并非所有决策都是记录下来的，许多模型都隐藏在 API 背后。

因此，为了研究数据集和模型的立场，我们比较了标注与真实用户，并使用现有数据集和模型。我们使用我们的框架 NLPositionality来实现这一点。我们的框架分为两个主要步骤。第一步是重新标注数据集，使用来自不同背景的标注者。我们必须避免考虑原始数据集标注者的人口统计学特征，因为通常只有少数标注者标注每个实例，而且人口统计学数据很少被收集和共享。因此，我们选择重新标注数据以获得大量标注和丰富的人口统计学数据。然后，我们根据人口统计学进行标注，并将其与模型和数据集进行比较，使用 Pearson's R 相关系数。因此，我们的框架与标注者意见不一致的文献不同，它将最终用户与模型和数据集的预测和标签进行比较，而不是仅仅关注标注者之间的意见一致性或模型标注者分布。我们的框架很大程度上依赖于 Lab in the Wild 和 HCI 合作者提供的在线众包平台。Lab in the Wild 是一个在线实验平台，我们可以招募来自不同背景的志愿者。与像 M Turk 这样的平台相比，M Turk 主要来自美国或印度，而 Lab in the Wild 仍然能够获得高质量的数据。我们托管了两个任务在 Lab in the Wild 上，其中一个任务是社会可接受性，它的工作方式是：参与者阅读来自 Social Chemistry 数据集的场景，然后写下该场景的社会可接受程度。为了保持参与度，他们可以比较自己的答案与 AI 和其他人的答案。然后，我们比较这些标注与 Social Chemistry、Delphi 和 GPT-4。我们随后以类似的方式复制了毒性检测任务，参与者阅读 Dynahate 数据集中的实例，并判断它是否属于仇恨言论。然后，我们比较这些标注与 Dynahate、Perspective API、Rewire API、Hate Roberta 和 GPT-4。我们最终收集了超过 16,000 条标注，来自超过 1000 名标注者，来自 87 个国家。现在，我们更清楚地了解 NLP 数据集和模型与哪些群体最匹配。我们发现，数据集和模型最匹配英语国家。例如，对于 GPT-4 的社会可接受性分析，我们发现它最匹配的是儒家文化和英语国家。我们发现 Dynahate 也最匹配英语国家。我们还发现，对于 GPT-4，最匹配的是具有大学学历或研究生学历的人，对于 Dynahate 来说，也同样如此。然而，当模型和数据集与特定群体对齐时，必然会留下一些人被忽视。这在 GPT-4 的社会可接受性任务以及 Dynahate 分析中都有体现，即数据集和模型与非双性人相比，与男性和女性群体不太匹配。

那么，既然 NLP 中存在立场，我们该如何做呢？我们有几个建议：首先，记录研究过程中的所有相关设计选择。其次，以视角主义的视角进行 NLP 研究。第三，构建针对特定社区的专业数据集和模型，例如 Masakhani 倡议。我们想强调的是，包容性 NLP 不仅仅是让所有技术都能为所有人服务。

这就是我们今天的总结。如果您想了解更多信息，请随时查看我们的仪表板以获取最新的分析结果和我们的论文。谢谢。</sample>
    <sample id="97">The current SimulST models face problems with long and complicated training procedures, including training involving different optimization objectives, and training and maintaining several models to reach different latency regimes.</sample>
    <sample id="98">```
The presentation highlights the difficulty of mitigating political biases in language models. The core dilemma is balancing the benefits of diverse perspectives in training data with the risk of perpetuating societal biases. 

The presenter suggests that simply sanitizing the training data is problematic because it's hard to define "neutral" and could lead to censorship.  Instead, the research focuses on understanding *how* biases propagate and exploring potential mitigation strategies, rather than a single, easy solution.  The presentation doesn't offer a definitive answer to *how* to effectively reduce bias, but emphasizes the complexity of the problem and the need for careful consideration of the trade-offs.
```</sample>
    <sample id="99">大家好，我是复旦大学的Siyu Yuan。我们团队的论文是“从大型语言模型中提炼脚本知识以用于约束型语言规划”。在日常生活中，人类通常通过遵循逐步的指令来规划行动，这些指令的形式是目标导向的脚本。之前的研究利用语言模型来规划抽象目标，例如“制作蛋糕”等典型活动。这些研究表明，大型语言模型能够有效地将目标分解为步骤。然而，之前的研究主要关注的是规划抽象目标，而规划具有特定约束的目标，例如“制作巧克力蛋糕”，则鲜有研究。在本文中，我们定义了约束型语言规划问题，该问题在规划目标时施加了不同的约束。一个抽象目标可以被不同的具体目标继承，这些具体目标具有多方面约束。一个好的规划者应该编写既合理又与约束相符的脚本。因此，本文首先评估并改进了大型语言模型在约束型语言规划方面的能力。由于目前没有特定目标的数据集来支持我们的研究，因此我们需要首先获取这些目标。如图所示，我们通过使用 InstructGPT 进行人工参与的数据获取，扩展了抽象目标并添加了多方面约束。我们随机采样 100 个具体目标并评估由大型语言模型生成的脚本。本表报告了结果的整体准确率。我们发现，所有语言模型在规划具体目标方面都表现不佳。然后，我们进行详细分析，以调查学习模型失败的原因。图表显示，生成的脚本的语义完整性可以接受，但无法保证与约束的忠实性。我们深入研究了 wikiHow 中定义的约束的更细粒度类别。图表中的热图显示，InstructGPT 的规划性能对不同的目标类别差异很大。之前的研究表明，语言模型的输出质量存在高波动，导致性能不佳。因此，我们采用“过度生成然后过滤”的想法来提高生成质量。首先，我们展示 InstructGPT 对约束类型和示例，并根据种子抽象目标生成特定目标。接下来，InstructGPT 过度生成 K 个脚本用于特定目标。然后，我们开发了一个过滤模型来选择忠实于约束的脚本。我们将脚本和目标转换为 InstructGPT 嵌入，并计算余弦相似度作为相似度分数来衡量语义相似性。此外，我们奖励包含目标约束关键词的脚本。只有当目标得分最高时，我们才保留脚本。通过我们的方法，InstructGPT 可以生成更高质量的脚本。我们的方法显著提高了规划能力，既在语义完整性方面，也在约束的忠实性方面有所提升。由于大型语言模型成本高昂，因此需要能够使小型和专门化的模型具备语言规划能力。创建数据集是这一步骤的关键。然而，之前的研究并未实现对特定目标的规划，并且人工数据标注成本高昂。因此，我们遵循了符号知识蒸馏的想法，从大型语言模型中蒸馏约束型语言规划数据集。我们应用我们的方法构建了一个约束型语言规划数据集，命名为 CoScript。总共有 55,000 个具体目标及其脚本被生成。为了确保验证和测试集的质量，我们请众包工作者查找并修改不正确的样本。图表显示了 CoScript 中约束的分布。我们发现 CoScript 具有高度的多元化。有了 CoScript，我们可以尝试使用更小但专门化的模型进行约束型语言规划。我们发现，使用 CoScript 微调的 T5 模型生成的脚本质量优于大多数大型语言模型，这表明，如果正确训练，小型模型可以超越大型模型。总而言之，我们建立了约束型语言规划问题。我们评估了大型语言模型在约束型语言规划方面的能力，并开发了一种“过度生成然后过滤”方法。我们使用大型语言模型生成了一个高质量的脚本数据集，CoScript，用于约束型语言规划。我们希望 CoScript 数据集能够成为推进语言规划研究的宝贵资源。感谢您的时间。有关 CoScript 的更多详细信息，请参阅我们的论文。</sample>
    <sample id="100">PromptRank is a data-efficient approach to multi-hop question answering (QA) that leverages language models for reranking candidate answer chains. Unlike traditional methods requiring large datasets, PromptRank achieves strong performance with only 128 training examples.

The system combines unsupervised retrieval (TF-IDF and hyperlink traversal) to generate a pool of candidate chains, followed by a few-shot language model-based reranker.  A key innovation is the "chain prompt," which incorporates the candidate documents and an instruction to elicit reasoning from the language model. This prompt is designed to guide the model in understanding the relationship between the question and the chain of documents.

The likelihood of the question given the chain prompt is used as the scoring function, proving more effective than the reverse likelihood.  The effectiveness of different components (retrieval, prompt construction, instructions, temperature scaling) is validated through ablation studies.

Experiments on HotpotQA demonstrate that PromptRank outperforms fully supervised systems like DrKit and achieves comparable performance to state-of-the-art dense retrievers.  When used as a retriever in conjunction with an ELECTRA-Large reader model, PromptRank exhibits strong downstream QA performance, with only a slight performance gap compared to MDR.

In summary, PromptRank demonstrates the power of language models for few-shot ranking in multi-hop QA, offering a data-efficient alternative to traditional methods. The chain prompt and the likelihood-based scoring function are crucial for achieving strong performance.</sample>
    <sample id="101">PaLM 的流畅度与最先进系统相当。</sample>
    <sample id="102">水印方法需要满足以下属性：

1.  **适用性：** 适用于 embedding as services。
2.  **不影响效用：** 不会降低提供的嵌入的效用。
3.  **隐蔽性：** 足够隐蔽，难以被攻击者发现或移除。
4.  **可转移性：** 可以转移到攻击者服务中的模型。</sample>
    <sample id="103">We performed our analysis on transcripts of TED talks that have been translated from English to 14 different languages.</sample>
    <sample id="104">我们重新注释了 16,000 个实例。</sample>
    <sample id="105">The following distance metrics are used to measure the difference between benign and backdoor datasets:

*   **Cosine similarity**
*   **L2 distance**
*   **KS test p-value**</sample>
    <sample id="106">QUEST is a new retrieval dataset designed to challenge systems in handling information needs with multiple constraints, particularly implicit set operations. The dataset focuses on entity-seeking queries where users express preferences using combinations of sets, such as "red reptile, not more than 12 inches long, found in Costa Rica" (Jane) or "historical fiction novels set in France" (Austin).

The dataset comprises over 3,000 queries derived from Wikipedia category names across four domains: films, books, plants, and animals.  These categories are used to create queries with implicit set constraints, which are then paraphrased and validated for fluency and relevance by human annotators.  Annotators also mark the specific spans of text within documents that support the answer entities and indicate their relevance to different constraints.

QUEST presents a significant challenge for retrieval systems because it requires systems to search large document corpora to identify multi-answer sets, where evidence for relevance can be distributed across multiple parts of a document.  The dataset is used to evaluate retrieval systems, including sparse and dense retrievers, and a T5-based reranker.

The research demonstrates that current retrieval systems struggle with this type of information seeking.  While retriever performance is limited, end-to-end systems achieve relatively low F1 scores.  The study highlights that queries involving set intersections and set differences are particularly difficult.

The authors hope that QUEST will facilitate the development of more effective systems for handling complex information needs and improve the ability of systems to understand and respond to user queries with multiple constraints. The paper is available for reading, and the authors invite attendees to their presentation at ACL.</sample>
    <sample id="107">XSemPLR 使用 Encoder-Decoder 模型，例如 mBART 和 mT5。这些模型是预训练的多语言编码器-解码器模型，在多语言设置中表现良好。</sample>
    <sample id="108">Koustav Sinha and colleagues at ACL 2023 presented a paper addressing the issue of context dependence in language model acceptability judgments. They argue that current Minimal Pair (MPP) paradigms, which evaluate models by comparing acceptable and ungrammatical sentences, are not robust to longer contexts, a crucial consideration given the increasing context window sizes of large language models.

The authors revisit the MPP pipeline by simulating longer sequences. They achieve this by recreating sentences from existing datasets (like BLiMP and SyntaxGym) by adding acceptable or unacceptable prefixes to the query sentences, maintaining the same grammatical structure. They explore three scenarios: using sentences from completely unrelated datasets (to assess context independence), using sentences from the same dataset (to examine dataset-specific biases), and using sentences from different datasets (to investigate the impact of different linguistic phenomena).

Their experiments reveal that MPP judgments are relatively stable when using completely irrelevant Wikipedia sentences, suggesting robustness to arbitrary context length. However, when using sentences from the same dataset, the MPP judgments show significant variations based on the prefix's acceptability. Specifically, adding acceptable prefixes leads to increased judgments, while adding unacceptable prefixes leads to decreased judgments, and this effect increases with context length.

The authors hypothesize that this sensitivity stems from the models' reliance on latent syntactic and semantic features shared across sentences. They found that perturbing input sentences to preserve structure but add noise doesn't significantly alter the model's judgment, indicating that the model's sensitivity is consistent across different sentence types.

The key takeaway is that current MPP evaluation methods, particularly with short, single-sentence inputs, may not fully capture the language models' abstract knowledge and contextual understanding across longer sequences. The paper suggests that future evaluations need to account for these contextual dependencies to more accurately assess language model capabilities.</sample>
    <sample id="109">The presentation introduces "Unnatural Instructions," a novel dataset for instruction tuning language models, created entirely through automated generation without human annotation. The core problem addressed is the limited diversity of instruction tuning datasets, which are often restricted to existing academic benchmarks.  The proposed solution leverages pre-trained language models, specifically GPT-3 variants, to generate a large and diverse set of instructions, inputs, and outputs.

The process involves prompting the model to generate instructions and corresponding inputs, followed by generating outputs for those instructions.  To further enhance diversity, the model is prompted to generate paraphrases of existing instructions. This results in a dataset of 64,000 examples, expandable to approximately 240,000 with paraphrases.

The study assesses the quality of the generated data, finding a high degree of correctness (over 50%) and identifying valuable information even in incorrect examples.  The dataset exhibits creativity and diversity, encompassing tasks far beyond traditional NLP benchmarks, such as scientific experiment design and word invention.

To validate the dataset's utility, an 11 billion-parameter T5 model was fine-tuned on Unnatural Instructions.  The results demonstrate that the model outperforms both T0++ and Tk-instruct on several benchmarks, and training on Unnatural Instructions is more cost-effective than training on a baseline dataset of Super-Natural Instructions.

The key takeaway is that language models can effectively generate diverse and creative instruction data, offering a cheaper and faster alternative to human annotation. This approach overcomes the limitations of crowd-sourced annotation, which often leads to predictable and artifactual results. Unnatural Instructions provides a valuable resource for advancing instruction tuning and enabling language models to generalize to a wider range of tasks.</sample>
    <sample id="111">The author selects a trigger set as a group of words in a moderate frequency interval, which the provider can collect from a general text corpus and count the word frequency with.</sample>
    <sample id="112">大家好，我的名字是舒恒。今天我将介绍我们的论文《CoNLL-2003命名实体识别器在2023年是否仍然有效？》。让我们开始吧。我们的论文研究了泛化问题，即命名实体识别任务（NER任务）。我们观察到，CoNLL-2003模型近20年来一直被用于开发NER，这自然引发了许多问题。首先，这些模型是否能够泛化到现代数据？其次，当我们开发新的标签器时，需要什么来保证良好的泛化？与此同时，如果我们观察到泛化不良，那么性能下降的原因是什么？为了解决这些问题，我们开发了CoNLL++数据集。这是一个我们从2020年Reuters新闻中收集的数据集，然后使用与CoNLL-2003标注指南相同的标注方法进行标注。然后，我们对超过20个模型进行了微调，并在CoNLL-03测试集和CoNLL++数据集上进行了评估。最后，我们计算了每个模型F1值的百分比变化，以评估其泛化能力。那么，什么对于良好的泛化是必要的呢？在实验过程中，我们发现有三个主要要素是必要的。第一个是模型架构。通过我们的实验，我们发现Transformer模型通常能够更好地泛化到新数据。第二个要素是模型大小。我们发现，通常较大的模型能够带来更好的泛化效果。最后，我们都知道，下游任务的性能直接受到微调示例数量的影响。我们在这里也发现，更多的微调示例实际上也能够带来更好的泛化效果。针对我们的下一个问题，性能下降的原因是什么？我们提出了两个假设。第一个假设是自适应过拟合，即过拟合成本通过重复使用相同的测试集来增加，这通常表现为在新测试集上改进的收益越来越小。第二个假设是时间漂移，即性能下降是由训练数据和测试数据之间时间差距增加引起的。对于数据过拟合，我们观察到，右侧图中的红色最佳拟合线具有大于1的斜率。这意味着，我们在CoNLL-2003上取得的每一单位改进，在CoNLL++上就能带来超过一单位的改进，这表明在这种情况下没有 diminishing returns。这表明，在这种情况下，自适应过拟合并非观察到。那么，时间漂移呢？为了研究时间漂移，我们对一些模型进行了重新训练或持续预训练，并发现，随着时间差距增大，性能会下降，这证实了我们关于时间漂移是性能下降的主要原因的假设。我们的结论是，为了良好的泛化，我们需要更好的模型架构、更大的模型大小以及更多的微调示例。这三个要素相互关联，不能只选择其中一个而放弃其他两个。与此同时，我们还发现，性能下降的原因是时间漂移，而且令人惊讶的是，这并非由自适应过拟合引起，即使CoNLL-2003已经使用了20多年。回到我们论文标题中提出的问题《CoNLL-2003标签器在2023年是否仍然有效？》，我们发现答案是肯定的。我们希望我们的论文能够促使更多研究来改进模型的泛化能力。最后，请务必查看我们的论文、数据集，如果您有任何问题，请随时与我联系。非常感谢。</sample>
    <sample id="114">Nanyang Technological University's work at ACL 2023 addresses the issue of heavy parameters in large language models (LLMs), which hinder deployment and training. The core problem is that while multi-head attention allows models to attend to different aspects of input, some heads can be pruned without significant performance loss. Existing approaches to optimize multi-head attention – homogenization, diversification, and head scoring – have limitations like sacrificing performance, lacking parameter efficiency, or still leaving redundancy.

The proposed solution is "Grouped Head Attention (GHT)," a divide-and-conquer strategy for compressing multi-head attention. It employs a two-stage process:

1. **Group-constrained training:**  Heads are grouped, encouraging intra-group similarity and inter-group separation. This is supervised using unsupervised hidden unit discovery (e.g., K-means) on projected feature maps.  A homogenization term promotes similarity within groups, while a diversification term encourages separation between groups.
2. **Voting-to-Stay algorithm:**  After group-constrained training, heads are pruned based on a voting mechanism. Each batch acts as a voter, and heads receive votes based on their evaluator score. Heads with low votes are pruned, leaving only one head per group.

GHT achieves significant parameter compression, with extreme cases reaching 90% reduction.  Evaluations on machine translation, language modeling, and abstractive summarization tasks demonstrate performance improvements (BLEU scores) and compression rates.  GHT-PS, the pruned version, achieves 32.1% parameter compression with comparable performance.

Furthermore, the research explores efficiency gains, demonstrating a "LITE" model that achieves 90% parameter reduction, 62% faster inference, and 80% reduced FLOPs while maintaining performance.  The work leverages the Lottery Ticket Hypothesis, suggesting that pruning can be performed without performance loss.

The authors argue that LLMs are often redundant in real-world scenarios, where only a subset of tasks is needed.  Pruning can be likened to uninstalling unused apps on a smartphone, freeing up resources without impacting functionality.  The research encourages task-specific automatic pruning as a promising future direction.</sample>
    <sample id="115">The method uses speech chunks of lambda speech frames.</sample>
    <sample id="116">Servin 是一个法官。</sample>
    <sample id="117">示例质量比与源句子的相似度更重要。</sample>
    <sample id="118">The presentation introduces "Improving Pretraining Techniques for Code-Switched NLP," a paper addressing the challenges of natural language processing in multilingual environments where languages are mixed within a single sentence (code-switching). The authors highlight that standard multilingual pre-trained models like mBERT and XLM-R often underperform on code-switched tasks.

The core contribution is **SwitchMLM**, a novel Masked Language Modeling (MLM) technique specifically designed for code-switching.  The key idea is to focus masking on "switch-points" – tokens that mark the transition between languages.  The paper proposes two methods to implement SwitchMLM: **FrequencyMLM**, which uses the negative log likelihood of words in monolingual corpora to estimate LID tags (language identification tags), and **SwitchMLM**, which masks only the switch-points.  FrequencyMLM is presented as a practical alternative when LID tagging data is unavailable.

To further enhance performance, the authors propose **ResBERT**, incorporating residual connections from intermediate layers to the final layer of BERT. This leverages the fact that certain layers encode more switch-point information.  They also introduce an **auxiliary LID-based loss** to encourage the intermediate layers to learn language information.

Experimental results demonstrate that the combined method of SwitchMLM (or FrequencyMLM) with ResBERT and the auxiliary loss achieves the best performance on sentiment analysis across various language pairs.

The presentation also includes probing experiments using linear and conditional probing to verify the claim that SwitchMLM increases switch-point information in the intermediate and final layers of the model.  These experiments show that SwitchMLM representations contain more switch-point information compared to standard MLM representations, and that adding residual connections can further improve this effect.

In conclusion, the paper proposes SwitchMLM and ResBERT, along with an auxiliary loss, to improve pretraining for code-switched NLP.  The methods are designed to enhance the representation of switch-points, leading to better performance on code-switched tasks.</sample>
    <sample id="119">GPT-4, GPT series, BART series, and variants.</sample>
    <sample id="120">The model uses the attention mechanism between audio input and textual output, leveraging the knowledge already acquired by the model through this mechanism. It doesn't specify whether it uses attention scores from specific layers or combines scores from multiple layers.</sample>
    <sample id="121">The examples of direct references are using the song's name ("Easy on Me") or its position ("the first one").</sample>
    <sample id="122">Fudan University.</sample>
    <sample id="123">Ying and Zhiyang's research focuses on improving multi-modal zero-shot learning using instruction tuning, addressing a gap in existing research that primarily concentrated on language-only tasks. They introduce MultiInstruct, the first large-scale multi-modal instruction tuning dataset, comprising 62 diverse tasks across 10 categories, derived from 21 existing datasets and each equipped with five expert-written instructions. Their base model is OFA, a unified multi-modal pre-trained model.

The research investigates the effectiveness of instruction tuning on OFA, using a unified sequence-to-sequence format to process diverse input and output data types. They train the model on 53 tasks, sampling 10,000 instances per task, and evaluate it on a held-out test set, including a common sense reasoning group and additional tasks from VQ and Miscellaneous groups.  They employ five different instructions during testing, reporting the minimum, maximum, and standard deviation of performance metrics like accuracy (for classification tasks) and Rouge-L (for generation tasks).  A new metric, sensitivity, is introduced to measure the model's consistency across different instruction variations.

The results demonstrate that instruction tuning significantly enhances OFA's performance on seen multi-modal tasks.  Furthermore, transfer learning from natural instruction datasets improves instruction tuning effectiveness.  Increasing the number of instructions used during fine-tuning leads to better performance and reduced sensitivity. Transfer learning from natural instruction datasets also improves the model's sensitivity on natural instruction datasets.

The authors conclude that MultiInstruct provides a valuable resource for multi-modal instruction tuning, significantly improving OFA's capabilities and exploring the benefits of various transfer learning techniques. They plan to expand MultiInstruct with approximately 150 additional vision-language tasks and release the updated dataset and model.</sample>
    <sample id="124">Tan Qingyu from NUS and Alibaba presented their work "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models." The presentation focused on the limitations of current large language models (LLMs) in handling temporal reasoning, a crucial aspect of real-world understanding.

The work categorizes temporal reasoning into three levels: time-to-time (basic time axis understanding), time-to-event (grounding events to time), and event-to-event (reasoning based on multiple events over time).  Existing research primarily focuses on the second level, neglecting the more comprehensive approach the authors advocate.

To address this, they created the TempReason dataset, encompassing all three levels of temporal reasoning and covering a wide range of time periods.  They evaluated three LLMs – T5-L fine-tuned on Natural Questions, FLAN-T5-L, and ChatGPT – on this dataset.  The initial experiments revealed a bias towards the 2000-2020 time period in the first two models, likely due to term frequency in their pre-training data. ChatGPT showed promise in year prediction but struggled with month prediction.

The authors further explored temporal reasoning through three QA settings: Closed Book QA (question-only), Open Book QA (using Wikipedia articles), and Reasoning QA (providing temporal knowledge).  They also proposed a novel training strategy called TempT5, combining temporal span extraction pre-training and time-sensitive reinforcement learning to reward correct temporal predictions and penalize incorrect ones.

Results showed that TempT5 significantly outperformed other models, including ChatGPT, particularly in Open Book QA and Reasoning QA.  ChatGPT exhibited inconsistent performance across different time periods, highlighting a flaw in its temporal reasoning.  The authors also noted some performance fluctuations in TempT5, potentially linked to data imbalance.

In conclusion, the work identifies temporal reasoning biases in LLMs, introduces the TempReason benchmark, and proposes a training paradigm to enhance their temporal reasoning capabilities. The research aims to improve LLMs' ability to understand and reason about time, making them more capable of handling complex real-world scenarios.</sample>
    <sample id="125">The provided text does not mention the number of authors.</sample>
    <sample id="126">Yes, the presentation mentions using Google Translate API to translate source to target language as a baseline in the "Translate-Test" setting.</sample>
    <sample id="127">Namgyu Ho from KAIST AI introduces their research paper, "Large Language Models Are Reasoning Teachers." The paper addresses the limitation of chain-of-thought reasoning, which requires massive models like GPT-3 or PaLM, making them impractical for many applications due to high computational costs.

Their solution is to leverage these large models as "reasoning teachers" to transfer their reasoning abilities to smaller models. They introduce a novel technique called "Diverse Reasoning," which generates multiple reasoning samples from the teacher model using stochastic temperature sampling, rather than just one. This allows the smaller models to learn from a wider range of solutions, leading to better performance.

The research demonstrates that fine-tuning smaller models (under 1 billion parameters) with the teacher's reasoning steps significantly improves their ability to solve complex tasks. They tested their method on 12 tasks and found that it outperforms prompt-based baselines and vanilla fine-tuning, especially for text-based tasks. Diverse Reasoning further enhances performance, notably in Multi-Arithmetic tasks.

The study highlights the scalability of their method, showing that performance can be improved by using larger datasets, better teacher models, or larger student models. However, it also acknowledges the trade-offs between development costs (teacher model and dataset) and inference costs (student model size).

The key takeaway is that simple distillation – transferring reasoning abilities from large models to smaller ones – is feasible and effective. This approach has the potential to enable the deployment of reasoning capabilities in resource-constrained environments and may be applicable to other emergent abilities in the future. The researchers provide code and data for their experiments, including access to OpenAI inference costs, encouraging further research in this area.</sample>
    <sample id="128">Akshatha and Martin's work, "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources," explores the challenge of natural language understanding (NLU) models utilizing both pre-training and inference-time knowledge.  The research highlights that while models can leverage knowledge embedded in their parameters (acquired during pre-training), they often lack the ability to access and integrate knowledge relevant to specific, instance-specific situations encountered during inference. This is particularly evident in tasks like coreference resolution, where understanding the referent of a pronoun requires both general background knowledge (e.g., "politicians seek elected seats") and entity-specific knowledge (e.g., "Chichester is a politician").

To address this, the authors introduce the KITMUS dataset, a diagnostic test suite designed to probe for knowledge integration capabilities. The dataset features coreference resolution tasks, where the goal is to identify the correct entity referred to by a pronoun.  The dataset's core innovation lies in its three settings: "Background-Pretrain," "Background-Both," and "Background-Inference."  These settings vary the availability of background knowledge, simulating scenarios where background knowledge is either present during pre-training, available at both pre-training and inference time, or only accessible at inference time. The "Background-Inference" setting is particularly crucial, as it mimics situations where models lack the necessary background knowledge due to evolving information (e.g., new occupations).

The study evaluates models using both human participants and established coreference resolution models.  The results demonstrate that while models trained on generic datasets often rely on surface-level cues, KITMUS reveals limitations in their ability to integrate knowledge from different sources.  Even the best-performing models struggle with reliably integrating knowledge available only at inference time.  

The key takeaway is that many coreference resolution models struggle to reason over knowledge from diverse sources without task-specific training. However, with such training, some models can successfully integrate knowledge from multiple sources.  Despite this progress, the ability to reliably integrate backward knowledge presented only at inference time remains a significant challenge. The authors provide a dataset and code on GitHub for further exploration.</sample>
    <sample id="129">作者给出的“显性群体”(marked group) 的示例包括：

*   亚洲女性 (Asian woman)
*   中东女性 (Middle-Eastern woman)
*   黑人女性 (Black woman)
*   白人男性 (White man)
*   拉丁裔女性 (Latina woman)
*   亚裔女性 (Asian woman)</sample>
    <sample id="130">The paper doesn't explicitly state which model architectures generalize *poorly*. However, it states that **transformer models generally generalize better to new data** than other architectures. This implies that other architectures might have poorer generalization.</sample>
    <sample id="131">The video does not explicitly state the name of the test dataset. It mentions that the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples, but it doesn't name the dataset.</sample>
    <sample id="132">三位作者。</sample>
    <sample id="133">MultiInstruct is a multi-modal instruction tuning dataset.</sample>
    <sample id="135">ABC-Eval is a new dimensional approach to evaluating conversational AI, developed by the Emory NLP Lab and Amazon Alexa AI. It aims to improve upon existing methods like human evaluation and Likert scales by explicitly annotating model responses for specific behaviors, such as irrelevant information, contradictions, and lack of empathy. This reduces subjectivity and provides a more precise assessment of chat quality.

The researchers evaluated four state-of-the-art chat models using ABC-Eval on 100 human-bot conversations, comparing it to three existing methods: turn-level Likert ratings, dialogue-level Likert ratings, and pairwise comparisons.  Their analysis showed that ABC-Eval behavior labels are more reliable and predictive of overall conversation quality than existing methods.  Specifically, metrics like self and partner contradictions explain a significant portion of conversation quality, while Likert consistency scores explain much less.

ABC-Eval metrics also demonstrate distinctiveness, meaning that combining multiple metrics provides a more comprehensive understanding of chat quality than relying on single metrics.  The study quantified common challenges in conversational AI, such as common sense violations, irrelevant responses, and contradictions, revealing that these issues are prevalent in the tested models.

The authors emphasize the importance of reliable and precise evaluation metrics for the rapid advancement of conversational AI. They believe ABC-Eval can serve as a valuable tool for the field, enabling more nuanced comparisons of different models and highlighting areas for improvement.  While acknowledging that error rates may decrease with future model development, the study underscores the need for robust evaluation methods to track progress and identify persistent challenges.</sample>
    <sample id="136">## FERMAT: An Alternative to Accuracy for Numerical Reasoning

Jasivan and Nafise from the University of Sheffield's work, "FERMAT: An Alternative to Accuracy for Numerical Reasoning," addresses the limitations of current numerical reasoning benchmarks and proposes a novel evaluation set to better understand the capabilities and shortcomings of large language models (LLMs).

The motivation stems from the real-world applications of numerical reasoning, such as fact-checking, where accurate mathematical skills are crucial. However, existing benchmarks primarily focus on overall accuracy (e.g., F1 score), which doesn't effectively reveal the strengths and weaknesses of LLMs in mathematical abilities. Larger models tend to perform better, but the underlying reasons for this performance gap remain unclear.

FERMAT is a flexible evaluation set built upon arithmetic types, drawing from Illinois and CommonCore math questions. It introduces variations in number representation (e.g., using 5.0 instead of 5), number types (integers, decimals, large integers), and mathematical operations (single operations vs. combinations). This allows for a more comprehensive assessment of model proficiency across different mathematical domains.

A baseline zero-shot evaluation reveals that most models perform poorly on FERMAT, indicating that current benchmarks might not accurately reflect the necessary mathematical skills for real-world tasks. Fine-tuning models with a large dataset of 200,000 questions generated by math teachers, using templates that replace numbers with placeholders, shows promising performance improvements. This fine-tuning process exposes the importance of diverse number types and operations.

Further analysis reveals that even when models are trained on exact expressions, their accuracy remains below 50%, suggesting they may not fully "memorize" the patterns. The linguistic nuances of words like "increases" versus "another" are also highlighted as potentially important factors.

Finally, the impact of training templates is investigated. Results indicate that incorporating language diversity from datasets like GSM8K and AQUA, alongside mathematical diversity, significantly improves model performance.

In conclusion, FERMAT offers a more informative alternative to traditional accuracy-based benchmarks for evaluating numerical reasoning in LLMs. The work emphasizes the importance of language and mathematical diversity, as well as the need for further investigation into number encoding and tokenization. The paper highlights that current benchmarks are unrepresentative of the true capabilities of these models.</sample>
    <sample id="137">Sicong from the Singapore University of Technology and Design introduces "Tell2Design," a dataset and model for language-guided floor plan generation, published in ACL 2023. The paper addresses the need for design generation that adheres to specific requirements outlined in natural language, contrasting with the current trend of text-conditional generative AI focused on image generation.

The core problem is enabling users to "tell" instructions for floor plan design, a task requiring understanding of semantics, geometry, and topology within a structured layout.  Tell2Design comprises 5,051 human-annotated language instructions and 76,000 artificially generated ones, covering a wide range of floor plan descriptions.

The research tackles three key challenges: strict constraints compared to image generation, understanding complex, unstructured text, and handling ambiguity in user instructions.  Unlike traditional methods, Tell2Design employs a sequence-to-sequence model (encoder-decoder) to reconstruct room bounding boxes into a structured sequence, allowing for handling varying instruction lengths.  Each room is represented by a type label and bounding box coordinates (X, Y, H, W).

The paper evaluates the model's performance on the T2D dataset, demonstrating superior IoU scores (Micro IoU: 54, Macro IoU: 53) compared to text-conditional image generation baselines. This success is attributed to the sequence-to-sequence approach's ability to control the bounding box sequence based on extracted information from the language instructions.  Text-conditional image models struggle because they are optimized for high-level visual concepts, not detailed instruction following.

The study reveals a language distribution gap between artificial and human instructions, but shows that combining artificial and human data during training improves performance.  A case study highlights the failure of image generation models to accurately align with human instructions, emphasizing the need for a more nuanced approach.

Tell2Design provides a foundation for future research in language-guided design generation, paving the way for more intuitive and user-friendly design processes. The paper introduces a sequence-to-sequence model as a strong baseline and compares it with existing text-conditional image generation models.</sample>
    <sample id="138">作者认为 NLU 中研究不足的领域是：

1.  **知识整合：** 自然语言理解模型需要整合预训练时学习到的知识和推理时获取的知识。
2.  **不同知识来源的整合：** 模型需要能够从不同的知识来源（如预训练参数和推理时提供的背景知识）中提取和利用信息。
3.  **推理时获取的知识：** 模型在推理时获取的知识，例如关于特定实体的新信息，往往难以可靠地整合。</sample>
    <sample id="139">Ying and Zhiyang.</sample>
    <sample id="140">Yes, CoScript was validated and tested using crowd-sourced workers who found and revised incorrect samples to ensure quality.</sample>
    <sample id="141">The existing resources for evaluating context-dependent translations have limitations because they only support limited types of context-dependent translations and limited sets of languages, often relying on domain knowledge and human curation.</sample>
    <sample id="142">Javad Hosseini 和 Filip Radlinski、Silvia Pareti 和 Annie Louis 合作研究了“解决间接指代表达式以进行实体选择”问题，并介绍了 AltEntities 数据集。我们的目标是理解用户在选择时使用的语言。例如，“你指的是 ‘Easy on Me’ 还是 ‘I Gotta Feeling’？” 在这里，用户想要选择这两首歌中的一个。最明显的方法是使用直接引用，例如说歌曲名称“Easy on Me”或其位置“第一个”。但有时间接引用更合适，以进行更自然的对话。这可能发生在用户记不住歌曲名称时，或者歌曲的音高太相似难以区分时，或者用户想要指定偏好。以下是一些间接引用的例子，例如“更新的那个”或“不是充满活力的那个”。这是一个在对话系统以及评估大型语言模型实体理解方面都非常重要的问题。我们意识到目前没有大型公共数据集来解决这个问题，因此我们收集了一个通过众包标注的数据集。该数据集涵盖三个不同的领域：音乐、书籍和食谱。我们数据集的收集方法强调非正式性，采用卡通完成设置。卡通包含三个对话气泡。在第一个气泡中，鲍勃说：“请记住我们昨天听的歌？” 这样就设置了对话背景。在第二个气泡中，艾丽斯说：“你指的是 ‘Easy on Me’ 还是 ‘I Gotta Feeling’？” 这是替代问题。在第三个气泡中，鲍勃使用间接引用来选择其中一个实体，例如，“更新的那个”。我们自动生成第一个和第二个对话气泡，但第三个气泡由标注者填写。第一个对话气泡从多个人工提示中选择，这些提示来自不同的领域。第二个对话气泡，即替代问题，通过以下模板生成：“你指的是 A 还是 B？” 其中 A 和 B 是维基百科上的样本。我们使用的不同采样方法如下：数值越高，实体之间的相似度越高，通常越难进行歧义消除。第一个方法是随机均匀采样。第二个方法是在实体标题相似的情况下进行采样，例如两个名为“The Return”的书籍。第三个方法是在维基百科上实体描述相似的情况下进行采样。最后一种方法是在维基百科上实体信息框或属性相似的情况下进行采样，例如同一种流派或同一艺术家。当我们向标注者展示这个替代问题时，他们知道这两个实体的名称，但并不一定了解这些实体本身。因此，我们做的是向标注者展示一些关于这两个实体的背景知识。对于歌曲，我们简单地提供每个歌曲的 Google 搜索链接，然后要求标注者听至少部分歌曲并阅读每个歌曲的介绍。例如，对于歌曲“Easy on Me”的 Google 搜索结果。对于食谱和书籍领域，我们提供维基百科上的背景文本。对于食谱，我们还显示维基百科上的图片，以便标注者了解它们的外观。然后，我们要求标注者选择一个实体，例如，这里是第一个实体，并用三到五个间接指代表达式来描述它。例如，“没有歌词的那一个”、“不是那个有 12 岁男孩的那一个”、“虚构的那个”、“来自阿塞拜疆”等等。AltEntities 数据集包含 6,000 个替代问题，涵盖三个领域，并包含 42,000 个间接指代表达式。使用 T5 XL 模型的结果总结如下。如果语言模型能够获得与标注者完全相同的背景知识，那么准确率将非常高，约为 92% 到 95%。但这在现实中是不可能的。如果语言模型能够获得一些部分重叠的背景知识，那么准确率将介于 82% 到 87% 之间，这更接近现实。例如，如果语言模型检索到实体名称，那么准确率仅为 60%，还有很大的改进空间。我们还证明了模型具有领域泛化能力。这里有一个指向数据集的链接。谢谢。</sample>
    <sample id="143">该方法与 Wait-k 策略、Local Agreement 策略以及专门针对同步预翻译定制的 state-of-the-art 架构进行了比较。</sample>
    <sample id="144">The authors are from Nantes University Hospital.</sample>
    <sample id="145">Jenny.</sample>
    <sample id="146">Yicheng from Fudan University introduces a research paper on dialogue summarization, focusing on the critical issue of omission – the loss of important information in generated summaries. While large language models produce fluent summaries, they often contain factual errors, with omission being a significant problem.  A study reveals that even state-of-the-art models have a high omission rate (around 70%) across various domains.  The research highlights the unstructured nature of dialogues, making key information identification challenging.

To address this, the paper proposes the OLDS dataset, a high-quality dataset of dialogue summaries with omission labels, built upon existing benchmarks.  The dataset includes candidate summaries generated by different models and decoding strategies, with automatic and human validation of omission labels.  The paper explores three model frameworks – pair-wise classification, sequence labeling, and pointer networks – to build omission detection models, evaluating them using Precision, Recall, and F1-score, and a word-level omission recall metric (WR score).  The results show a challenging task with label imbalance, indicating a need for more advanced models.

Furthermore, the research investigates the potential of using detected omissions to refine summaries. A post-editing method concatenates candidate summaries with omission content as input to a sequence-to-sequence model.  Results demonstrate a significant performance boost when omissions are provided, suggesting that omission detection is valuable for improving summary quality. The paper concludes that addressing omission is crucial for enhancing the reliability and usefulness of dialogue summarization.</sample>
    <sample id="147">Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models is co-authored by Esin Durmus, Dan Jurafsky, and Myra.</sample>
    <sample id="148">你好，我是萨拉·帕皮，来自特伦托大学和布鲁诺·科斯克尔基金会，我将简要介绍论文“注意力作为同步语音翻译的指南”，这是一篇马特奥·内格里和马可·图尔基合作完成的工作。同步语音翻译，或SimuST，是指将口语翻译成目标语言的文本，从而实现跨语言交流。而当前SimuST模型的有哪些问题？特定的架构通常会引入额外的模块进行优化。长而复杂的训练过程，例如涉及不同优化目标的训练。以及训练和维护多个模型以达到不同的延迟水平。例如，训练一个平均延迟为一秒的模型，另一个延迟为两秒的模型，以此类推。那么我们的解决方案是什么？首先，使用现有的离线ST模型，而无需重新训练或采用SimuST特定的架构。仅使用一个模型来处理每个延迟水平，并使用特定的参数来处理延迟。利用模型已经获得的知识，通过音频输入和文本输出之间的注意力机制，即交叉注意力机制，你可以看到右边有一个例子。我们的解决方案是提出EDAtt，即编码器-解码器注意力，这是一种策略，根据注意力点在哪里来决定是否发出部分翻译。一个词只发出，如果注意力没有集中，即其总和低于一定的阈值α，相对于最后λ个语音帧，意味着接收到的信息足够稳定。例如，如果我们接收到包含“我将谈论……”的语音片段，我们的模型预测德语翻译，我们会查看交叉注意力权重，我们会看到第一个两个词指向最早接收的语音帧，而最后一个词指向最后接收的语音帧，λ个语音帧。这意味着第一个两个词会被发出，因为由于交叉注意力总和高于一定的阈值α，我们将不会发出最后一个词，并等待另一个语音片段。如果我们继续接收到另一个语音片段，并且我们的模型预测了另外三个词，我们会查看这些交叉注意力权重。我们会看到没有词指向最后λ个语音帧。这意味着这三个词会被发出。如果我们查看EDAtt的主要结果，我们将绘制在图表上的同步语音翻译结果，其中BLEU用于衡量翻译质量，平均滞后是延迟度量，我们还考虑了计算成本相关的平均滞后，这考虑了模型的计算时间来预测输出。因此，我们希望我们的曲线尽可能高，并且朝左移动。我们还与流行的策略进行比较，这些策略也应用于离线模型，即等待k策略和局部一致性。我们还比较了针对同步预翻译特别定制的state-of-the-art架构。所有这些都是同步语音翻译策略在德语上的结果。我们看到，这些策略都优于应用于离线模型的策略，因为曲线朝左移动。我们还看到，如果考虑实际的延迟时间或计算成本相关的延迟时间，这是最快的策略。如果你想了解更多结果，请阅读我们的论文。我们还开源了代码和模型以及同步输出，以方便重现我们的工作。感谢您的关注。</sample>
    <sample id="149">是的，数据集公开。演讲中提到他们收集了来自 Reuters News 的数据，并创建了 CoNLL++ 数据集，并鼓励大家查看他们的论文、数据集以及联系他们提问。</sample>
    <sample id="150">Archiki from Adobe Research and UNC Chapel Hill presented their ACL paper "MEETINGQA," focusing on extractive question-answering on meeting transcripts. The paper highlights the underutilization of the question-answering aspect within the vast amount of meeting data, which are often long, domain-specific documents.  Unlike existing work that primarily focuses on summarization and action item extraction, MEETINGQA introduces a novel dataset containing questions asked during meetings and their corresponding answers.

The dataset comprises 7.7K questions, collected from public meeting transcripts of the AMI corpus, with a high inter-annotator agreement (Krippendorff's alpha of 0.73).  The questions are characterized by being longer, open-ended, and designed to elicit discussions.  The answers exhibit diverse structures, including multiple speakers, discontinuous sentences, and rhetorical questions.  The data is further segmented into Train, Dev, and Test sets.  A significant portion of questions (30%) are unanswerable, and 40% have multi-span answers, while 48% involve multiple speakers.

The paper explores various methods for addressing the challenges of meeting QA. These include context retrieval for short contexts, single-span models focused on extracting relevant sentences, and multi-span models that classify tokens within the answer span.  They also leverage data augmentation using silver annotations from the MediaSum dataset.

Experiments demonstrate a substantial gap between fine-tuned models and human performance (over 25 F1 points). Short-context models like RoBERTa outperformed long-context models like Longformer. Multi-span models showed slightly less performance than single-span models. Zero-shot performance is significantly lower (nearly 50 F1 points) but improves with data augmentation. Larger instruction-tuned models like FLAN-T5 achieve comparable zero-shot results.

Error analysis reveals challenges in identifying rhetorical questions and irrelevant sentences in single-span models.  Furthermore, models struggle to identify which speaker answers a question, particularly in the zero-shot setting.

In conclusion, MEETINGQA presents a valuable dataset for meeting QA research, highlighting the difficulty of this task for existing models, both in fine-tuned and zero-shot scenarios. The paper emphasizes the need for further research to address these challenges.</sample>
    <sample id="151">大家好，我的名字是应和我的同事志阳，我们今天将介绍我们的研究，主题是 MultiInstruct 如何通过指令微调提高多模态零样本学习能力。随着大型语言模型的不断发展，许多研究工作开始探索利用预训练语言模型为不同下游任务进行参数和数据高效复用的新学习范式。最近的许多研究表明，指令微调能够使大型语言模型在没有见过的情况下，通过遵循自然指令执行任务，实现零样本性能。然而，此前关于指令微调的大部分工作都集中在提高语言任务的零样本性能上，而计算机视觉和多模态任务则被忽略了。因此，在我们的工作中，我们希望研究指令微调多模态预训练模型是否能够提高对未见多模态任务的泛化能力。此外，在我们的研究初期，我们发现在 NLP 和多模态任务之间存在相当大的指令数据集可用性差异。目前，有超过 1600 个语言任务指令数据集。然而，尚不存在大型公开的多模态指令任务数据集。因此，这促使我们构建了一个多模态指令微调数据集。我们现在介绍 MultiInstruct，这是第一个多模态指令微调基准数据集，包含 62 种多样化的多模态任务，涵盖 10 个广泛的类别。这些任务是从 21 个现有开源数据集派生的，每个任务都配备了五位专家撰写的指令。为了研究多模态指令微调，我们选择 OFA，一个统一的多模态预训练模型作为我们的基础模型。OFA 使用统一的词汇表来处理语言、图像令牌和边界框的坐标。在这里，我们展示了 MultiInstruct 数据集的一些示例实例，以统一各种输入和输出数据类型的处理。我们遵循 OFA 的方法，将所有任务都格式化为统一的序列到序列格式。在此中，输入文本、图像、指令和边界框都以相同的令牌空间表示。好的，现在我将谈论多模态指令微调。因此，对于训练数据集，我们使用 9 个组中的 53 个任务进行训练，并对每个任务采样 10,000 个实例。对于测试，我们保留了常识推理组的整个常见推理组用于测试，并选择额外的 5 个任务来自 VQ 和 Miscellaneous 组。我们使用测试集中的所有实例对每个任务进行使用。此外，我们随机采样 20 个任务来自自然指令的测试集作为 NLP 的未见任务。因此，我们使用预训练的 OFA 大型模型作为基础模型。在训练过程中，我们将所有任务的实例混合在一起。每个实例都随机组合到一个指令模板中。因此，在测试中，对于每个任务，我们进行总共 5 个实验，通过使用不同的 5 个指令来评估模型。在每个实验中，我们报告性能的最小值和最大值以及性能的标准差，这些值是在所有 5 个实验中计算的。如果任务是多模态分类任务，我们将报告准确率。如果它是多模态生成任务，我们将报告 Rouge-L。对于 NLP 任务，我们将报告 Rouge-L。我们还引入了一个额外的评估指标，称为敏感性。该指标衡量模型在不改变指令措辞的情况下，对同一任务产生相同输出的能力。现在，我们的主要结果是，指令微调可以显著提高 OFA 在已见多模态任务上的性能。此外，从自然指令数据集进行迁移学习可以为指令微调带来好处。正如我们所看到的，随着任务数量的增加，模型能够实现更好的性能，同时降低敏感性。因此，我们还进行了实验。我们使用一个指令与 5 个指令进行比较。正如我们所看到的，使用更多的指令可以提高模型的整体性能并大大降低其敏感性。这表明了不同微调策略对模型敏感性的影响。正如我们所看到的，从自然指令数据集进行迁移学习可以使模型比原始 OFA 模型实现更好的敏感性。我们还看到，从自然指令数据集进行迁移学习可以帮助 OFA 在自然指令数据集上取得更好的性能。总而言之，我们提出了第一个大型多模态指令微调数据集，它显著提高了 OFA 的短能力，并探索了不同的迁移学习技术及其益处。我们设计了一个新的指标，称为敏感性。此外，我们正在收集一个更大的多模态指令微调数据集，其中包含大约 150 个额外的视觉语言任务，我们将发布它们。这是我们数据的 QR 代码和模型。谢谢。</sample>
    <sample id="152">## Exploring Large Language Models for Classical Philology: Key Findings

Frederick Riemenschneider's presentation details a project focused on leveraging Large Language Models (LLMs) for the field of Classical Philology, specifically Ancient Greek and Latin. The research addresses the limitations of existing models, which are primarily monolingual BERT models trained on limited datasets. The project aims to create more robust and versatile models capable of handling both languages and diverse tasks within philology.

The core of the project involves developing new language models, including monolingual models (GreBERTa for Greek and GreTa for Latin) and multilingual models (PhilBERTa and PhilTa) pre-trained on Greek, Latin, and English data.  A key innovation is the creation of a high-quality pre-training corpus for Ancient Greek, utilizing the Internet Archive and a novel method of identifying Greek texts by correcting OCR errors.

The models were rigorously benchmarked on tasks like part-of-speech tagging, dependency parsing, and lemmatization using datasets like the Universal Dependencies treebanks for Greek and the EvaLatina 2022 dataset for Latin.  The results demonstrate significant performance improvements compared to existing state-of-the-art models. Notably, the encoder of the T5 model, used in GreTa, exhibits a different behavior than native encoder-only models, improving performance after further training.

The project also investigated the strengths of encoder-decoder models in lemmatization, achieving a 5 percentage point improvement over existing methods. Furthermore, the models were probed for semantic and world knowledge capabilities, showing strong performance in distinguishing synonyms, antonyms, and identifying relationships between figures in mythology.

Interestingly, the multilingual models did not show a significant performance advantage over the monolingual models in terms of semantic and world knowledge.  The research concludes by highlighting the development of powerful, native-tokenizer language models for Classical Philology, emphasizing the importance of high-quality pre-training data and rigorous benchmarking. The project provides a foundation for future research in applying LLMs to the study of ancient languages and cultures.</sample>
    <sample id="153">Ninareh Mehrabi from Amazon Alexa AI's Responsible AI team presented their work on resolving ambiguities in text-to-image generative models. The core problem addressed is the challenge of generating images that accurately reflect user intent when prompts are ambiguous. The presentation outlined their pipeline for tackling this issue.

Their work centers around a benchmark dataset built upon the LAVA corpus, designed to cover various types of prompt ambiguities.  The framework employs two primary disambiguation methods: 1) using in-context learning with a language model to generate clarifying questions, and 2) generating multiple visual interpretations for the user to choose from.  The user's response to these questions or interpretations is then used to create a disambiguated prompt.

To evaluate the effectiveness of their approach, they propose an automatic evaluation framework. This framework compares the original ambiguous prompt with the disambiguated prompt by feeding both into a text-to-image model.  A Visual Question Answering (VQA) model then assesses whether the generated image aligns with the user's intended meaning, based on a question-answer format.

Key findings include disparities in ambiguity resolution across different prompt types, a positive impact of their disambiguation framework on image fidelity, and strong agreement between their automatic evaluation and human evaluation.  The paper also explores additional findings and discussions.

In conclusion, the work addresses the critical issue of prompt ambiguity in text-to-image models by developing a benchmark dataset and a framework for both resolving and evaluating these ambiguities. The presented methods aim to improve the faithfulness of image generation to user intent. The presentation encourages further exploration of their paper for more detailed information.</sample>
    <sample id="154">The authors are from the University of Trento and Foundazione Bruno Kessler.</sample>
    <sample id="155">Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="157">## SDDS: Dialogue Summarization with Static-Dynamic Structure Fusion Graph

Shen Gao from Shandong University introduces their research on "Dialogue Summarization with Static-Dynamic Structure Fusion Graph" (SDDS), a joint work with several colleagues. The research addresses the challenge of distilling salient information from dialogues into concise summaries, a crucial task for quickly understanding multi-participant conversations.

Existing dialogue summarization methods often rely on pre-computed static graphs based on linguistic tools like discourse parsing. However, these methods suffer from dependence on the accuracy of external tools and a lack of adaptability to the summarization task. SDDS overcomes these limitations by employing a novel approach that combines static and dynamic graph structures.

The SDDS model comprises four main components: an Utterance Encoder for vector representation, a Static-Dynamic Graph module, and a Summary Generator. The Static-Dynamic Graph module constructs static graphs using heuristic methods like Discourse Parsing Graph (based on dependency parsing and key co-occurrence) and Speaker Relationship Modeling (using interaction frequency matrices).  It also incorporates utterance position information as edge features. These static graphs are then fused using a convolutional layer.

The Dynamic Graph module leverages multi-head attention to capture semantic relationships between utterances based on their deep vector representations, without relying on pre-computed structures.  The static and dynamic graphs are integrated using a dual cross-attention mechanism, incorporating graph representation into the summary generation process.

The researchers have released the code and data on GitHub, accessible via a QR code.  SDDS aims to provide a more robust and adaptable solution for dialogue summarization by leveraging both pre-defined dialogue structures and dynamic semantic relationships. The model's ability to learn from the dialogue context allows for more accurate and contextually relevant summaries compared to traditional methods.</sample>
    <sample id="158">Qipeng Guo from AWS introduces their work on "Dual Cache for Long Document Neural Coreference Resolution." The coreference resolution task aims to identify mentions of the same entity within a document, addressing the challenge of computational complexity associated with enumerating all mention pairs.

Traditional cache-based methods reduce complexity to linear time but suffer from high cache misses in long documents due to topic shifts and scattered mentions. The proposed dual cache addresses this by employing both a local cache (LRU eviction) for local entities and a global cache (LFU eviction) for global entities.

The dual cache model processes documents sequentially. New mentions are classified as either new entities or existing ones in the caches.  Entities are added to the global cache based on frequency, while new entities are added to the local cache. Cache eviction policies are applied when either cache is full.

Experiments on LitBank, OntoNotes, and WikiCoref datasets demonstrate that the dual cache outperforms baseline methods, even with unbounded memory, particularly when training data is available.  Performance gaps are more pronounced in book-level documents, highlighting the benefits of the dual cache for longer texts.  The dual cache significantly reduces cache misses compared to single-cache approaches.

The study concludes that the dual cache achieves a high performance/cost ratio, offering a significant improvement over single-cache methods while reducing cache misses.  It effectively separates local and global entities for optimized performance in long document coreference resolution. The dual cache is presented as a cost-effective solution for handling the challenges of coreference resolution in lengthy texts.</sample>
    <sample id="159">Koustav Sinha 先生和 John Gauthier、Aaron Mueller、Kanishka Misra、Karen Fences、Roger Levy 和 Adina Williams 合作，今天我们来谈谈 ACL 2023 论文。语言模型对可接受性的判断并非总是对上下文稳定的。这项工作是基于最小对偶范例。最小对偶范例基本上通过评估语言模型在可接受性判断上的表现，例如 BLiMP、SyntaxGym 或基于样板的判断，来评估语言模型。在最小对偶范例中，通常的做法是展示一个可接受的句子和一个不接受的句子，然后希望模型更倾向于可接受的句子。当前的 MPP 管道通常不允许我们评估模型对较长句子的接受程度。如今，大型语言模型拥有越来越长的上下文窗口。因此，评估模型在整个上下文窗口中的可接受性至关重要，而这就是我们试图做到的。我们试图重新审视 MPP 管道，要求模型在更长序列上评估可接受性。这就是我们的方法。因此，我们通过重新审视数据集本身来模拟这些更长的序列，并选择可接受或不接受的句子。例如，我们选择了 BLiMP 数据集中的 Adjunct Island 案例的典型可接受性对偶。我们提取 Adjunct Island 中的语法句子，并将其作为可接受查询和不接受查询的前缀添加。我们可以对同一匹配的句子进行操作，也可以从不同的子集或不同的数据集选择句子。这就是我们称之为“异构场景”的方法。在这里，句子仍然来自相关的子集，但不是来自评估时使用的相同数据集。我们还可以从完全无关的领域，例如维基百科中选择句子，以了解模型的可接受性判断是否受到任何上下文的影响，即上下文是否来自不同的子集，还是完全与当前正在查看的句子无关。那么，模型表现如何？首先，我们查看了维基百科中的句子，这些句子与当前查询对完全无关，而在那里我们发现 MPP 判断在任意上下文长度下都很大程度上是稳定的。然后，当我们选择来自相同数据集的句子时，即从 BLiMP 或 SyntaxGym 数据集中的可接受和不接受领域选择句子时，MPP 判断会显著增加或减少。然而，当我们匹配结构时，即选择来自同一现象的句子时，我们看到 MPP 判断会大幅增加或大幅减少，这取决于我们选择的前缀是可接受的还是不接受的。这非常大，这种效应随着上下文长度的增加而增加，这可能会影响到具有大型上下文窗口的较新的语言模型。为什么匹配前缀会如此强烈地影响语言模型的判断？因此，我们进行了一系列分析，试图通过对输入句子进行扰动，同时保留相关的结构并向输入句子添加噪声来扰动输入句子。在进行了多次这样的扰动后，我们发现这些噪声实际上并没有让模型改变其在 MPP 判断上的表现方式。也就是说，当我们扰动可接受领域的句子时，我们会看到所有扰动都以相似的方式增加 MPP 判断，当我们扰动不接受领域的句子时，我们会看到 MPP 判断以相似的方式减少。因此，我们的工作的主要结论是，语言模型对共享跨句子的一般性语法和语义特征非常敏感。而我们目前以短句和单个句子为基础的 MPP 评估方法可能无法完全捕捉语言模型在整个上下文窗口中的抽象知识。请阅读我们的论文以获取更多实验细节。感谢您的聆听。</sample>
    <sample id="160">Unordered multiset of tokens.</sample>
    <sample id="161">55,000</sample>
    <sample id="163">MASSalign.</sample>
    <sample id="164">Weak supervision offers a cheaper alternative to manual labeling, using weaker labeling sources like heuristics or knowledge bases. However, it's often noisy, and directly training neural networks on this noisy data can lead to memorization of the noise and poor generalization. 

The paper argues that the performance gains claimed by many weakly supervised learning (WSL) methods are often overestimated because they rely on clean validation sets. The authors' findings suggest that WSL methods require clean validation samples to work properly, and that directly fine-tuning on clean data can often achieve better performance than using the clean data for validation only.</sample>
    <sample id="165">Wenting Zhao from Cornell University presented their paper "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations," introducing a novel unsupervised learning method called LiPoR for abductive reasoning. Abductive reasoning aims to find the most plausible explanation for an observed outcome given a context. The paper addresses the challenge of learning abductive reasoning without relying on manually annotated plausible explanations, a common bottleneck in current supervised approaches.

The core problem is to identify a plausible subset of candidate explanations (Z) that best explain a given context (X) and outcome (Y).  LiPoR tackles this by treating explanations as latent variables and maximizing the marginal likelihood of the outcome given the context, while simultaneously incorporating a regularizer based on mutual exclusivity.  The paper highlights that explanations are mutually exclusive, meaning they cannot both be true simultaneously. This characteristic is leveraged to prefer a subset of explanations over others.

The LiPoR objective function combines two components: maximizing the likelihood of the outcome given the context (likelihood term) and regularizing the probability distribution of explanations to favor those that are mutually exclusive.  The regularizer, denoted by Omega, aims to reduce the entropy of the probability distribution of explanations given the context, effectively preferring a smaller set of plausible explanations.

The authors evaluated LiPoR on the AlphaNLI dataset, a widely used abductive reasoning benchmark.  They compared LiPoR's performance against zero-shot models and the previous best unsupervised approach.  The results demonstrated that LiPoR significantly outperforms all competitors, including a strong zero-shot GPT-3 baseline, by over 4 absolute points in accuracy.

In essence, LiPoR offers a promising approach to unsupervised abductive reasoning by leveraging the inherent mutual exclusivity of explanations to learn plausible reasoning without requiring labeled data. The paper's findings suggest that learning abductive reasoning can be achieved effectively without human annotation, opening up new possibilities for building more robust and adaptable AI systems. The paper is available at tinyurl.com/zhao-lipor.</sample>
    <sample id="166">Yunxin from Harbin Institute of Technology, Shenzhen, introduces their new work, "A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text." The research addresses the challenge of image retrieval from long, complex text descriptions, where typical visual language models struggle. The core idea is to combine the strengths of analogical reasoning (System 1) and logical reasoning (System 2) inspired by Dual-Process Theory, using a Divide-and-Conquer strategy.

The proposed framework, NDCR, consists of three main modules. First, the Proposition Generator decomposes complex propositions into simpler ones, using BART to generate corresponding sentences. Second, the Visual-Linguistic Interactor (System 1) performs visual-proposition interaction, generating matching scores and reasoning states. Third, the Neural-Symbolic Reasoner (System 2) integrates the reasoning states of simple propositions using negation and conjunction operations to arrive at the final solution.

The researchers highlight that NDCR outperforms existing baselines in experimental results, demonstrating the effectiveness of each module through ablation studies.  The system also exhibits interoperability by presenting inference states and results at intermediate steps.

The paper suggests that neural symbolic calculation holds promise for enhancing compositional reasoning and planning in large language models. The Divide-and-Conquer approach, similar to chain-of-thought prompting, breaks down complex problems into smaller, manageable steps. Integrating Dual-Process Theory with Divide-and-Conquer offers a potential path towards more robust and effective reasoning capabilities in AI systems.  The work aims to bridge the gap between analogical and logical reasoning, enabling more sophisticated image retrieval from linguistically complex text.</sample>
    <sample id="167">DEPLAIN-web 包括 750 个文档，其中一部分手动对齐，另一部分使用自动对齐方法进行对齐。</sample>
    <sample id="168">The CoNLL++ Dataset was created by collecting Reuters News data from 2020 and annotating it with the same CoNLL-2003 annotation guidelines.</sample>
    <sample id="169">David Vilar and colleagues from Google Translate presented a study on prompting large language models (LLMs) like PaLM for machine translation. PaLM, a 540 billion parameter model trained on 780 billion tokens, achieved state-of-the-art performance in various NLP tasks. This work is the first systematic investigation into using LLMs for translation, employing best practices from the machine translation community, including using recent test sets and comparing against state-of-the-art systems like WMT.

The study found that prompting significantly impacts LLM translation performance. Even simple one-shot prompting can yield improvements of over one BLEURT point, and this can escalate to 40 BLEURT points in extreme cases.  A 5-shot prompting strategy, where each sentence is labeled with its language, showed minimal difference in performance compared to the prompt's form, highlighting the importance of example quality.  The quality of the examples used in prompting is more crucial than their similarity to the source sentence.  The study emphasizes selecting high-quality translations for prompting examples, particularly leveraging the higher quality of the development data compared to the training data.

While PaLM's translations are close to commercial systems like Google Translate, it still lags behind. Human evaluation using the MQM framework revealed that PaLM's fluency is comparable to state-of-the-art systems, but its accuracy is a key difference.  Common errors include omission errors, where PaLM sometimes drops parts of the source sentence during translation.  Interestingly, PaLM exhibits lower "Style/Awkward" errors compared to other state-of-the-art systems, suggesting it produces fluent output despite accuracy challenges. The paper recommends focusing on prompt selection strategies that prioritize high-quality examples for improved translation results. The full paper provides more detailed insights.</sample>
    <sample id="170">大家好，我是来自宾夕法尼亚州立大学的Yusen Zhang。今天我将介绍我们的工作“XSemPLR：跨语言语义解析在多个自然语言和意义表示中的研究”。语义解析是一个任务，旨在构建用户查询的语义表示，例如 SQL 和 Lambda Calculus。跨语言语义解析的任务是将多个自然语言查询翻译成多个意义表示。正如这张图所示，我们需要使用神经网络模型将多个自然语言查询翻译成 SQL、Lambda 或 FunQL 等等。现有跨语言语义解析模型通常单独提出并评估在有限任务和应用数据集上的表现。例如，在某些自然语言上覆盖广泛，但在中文方面缺乏覆盖，或者在某些意义表示上缺乏覆盖。Lambda Calculus 可能缺失，或者它们只在某些神经网络模型上进行评估。例如，只有一个模型用于评估它们。因此，为此我们提出了 XSemPLR。我们提供了一个统一的数据集 XSemPLR，用于跨语言语义解析在多个自然语言和意义表示中的研究。该数据集包含 9 个不同领域的任务，5 个语义解析任务，8 种意义表示，以及 22 个语言在 15 个语言家族中的 15 个自然语言。为了更好地评估我们的基准，我们考虑了六种训练和评估设置。第一种是 Translate-Test。我们使用 Google Translate API 将源语言翻译成目标语言，然后使用单语模型进行训练和评估。例如，我们使用英语模型在英语查询上进行训练，而在推理过程中，我们将德语查询翻译成英语，然后使用训练好的模型预测 SQL。我们还将测试单语模型。在这个设置中，源语言和目标语言相同，例如德语到德语或英语到英语。我们还将测试单语小样本设置，通过仅使用 10% 的训练数据训练单语模型。我们还将测试多语言模型，例如将德语、英语、中文查询组合在一起训练一个多语言模型。在推理过程中，我们可以使用该模型来翻译德语查询或中文查询等。我们还考虑了跨语言零样本和少样本迁移。我们在一个源语言上进行训练，然后在另一个语言上进行迁移。在训练过程中，我们使用英语查询或英语和德语的少样本查询组合来训练一个多语言模型，以预测 SQL 输出。我们还发现许多有趣的结果。关于单语模型的分析，我们评估了两种模型组：Encoder-PTR（Multilingual Pretrained Encoders with Pointer-based Decoders），例如 XLM-R + PTR 和 mBERT + PTR。我们还评估了 Encoder-Decoder 模型，即 Multilingual Pretrained Encoder-Decoder Models，例如 mBART 和 mT5。我们发现 Encoder-Decoder 在所有九个数据集上的性能最佳。我们在 mT5 和 XLM-R + PTR 上评估了多语言设置。我们发现 Encoder-Decoder 或 Encoder-PTR 可以通过在各种语言的混合中进行训练来改进。这主要是因为大多数主要自然语言都可以获得性能提升，除了英语在七个数据集上的性能下降，而在三个数据集上有所提升，这被称为“多语言性诅咒”。我们还比较了跨语言性能差距。图中的蓝色线表示跨语言少样本迁移，橙色线表示跨语言零样本迁移，绿色线表示单语设置。我们发现，通过比较绿色线和橙色线，我们发现零样本设置的跨语言迁移性能差距很大，而通过比较蓝色线和橙色线，我们发现少样本设置的迁移差距迅速缩小。我们还发现了一些其他有趣的发现。例如，Encoder-Decoder 优于以前的工作或实现了可比的结果。在英语自然语言上进行预训练可以显著提高少样本任务在目标自然语言上的性能。我们还发现，像 Codex 和 BLOOM 这样的多语言语言模型对于跨语言语义解析任务来说仍然不够。总而言之，我们构建了 XSemPLR，这是一个用于跨语言语义解析的统一基准，该解析涉及多个自然语言和意义表示。我们对三种代表性的多语言语言模型进行了全面的基准研究，我们的结果显示了许多有趣的发现。</sample>
    <sample id="171">Existing works can be broadly classified into four categories, but none are applicable to embedding as services or lack transferability.</sample>
    <sample id="172">No, Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="174">Thea, co-author of the "ArgAnalysis35K" paper, introduces this dataset as a significant advancement in argument quality analysis. The core problem with existing datasets is their lack of quality, diversity, depth, and consistent motion association. ArgAnalysis35K addresses these issues with several key features.

Firstly, it's the largest dataset (35K argument-analysis pairs) with high-quality arguments, primarily sourced from high-quality debates and expert/intermediate debaters, with a smaller portion from novice users. This contrasts with crowdsourced datasets.

Secondly, it boasts a diverse range of arguments, moving beyond pre-selected motions to cover 24 themes based on real-world debate scenarios. This provides a more representative sample of arguments encountered in parliamentary debates.

Thirdly, ArgAnalysis35K introduces the concept of "analysis," which goes beyond simple claims and premises to encompass a combination of both. This allows for a more nuanced understanding of argument structure and quality.  The analysis is not just a single element but a coherent explanation of the argument.

Fourthly, the dataset incorporates instance-based annotator reliability. Instead of removing entire annotators due to bias on specific topics, it focuses on removing judgments that are demonstrably biased for individual arguments. This allows for better utilization of annotator expertise.

Finally, ArgAnalysis35K includes a relevance model that assigns scores to arguments based on their relevance to different themes. This captures the broader applicability of arguments beyond a single motion, providing a more comprehensive understanding of argument value.

In essence, ArgAnalysis35K offers a more robust, diverse, and nuanced dataset for argument quality analysis, offering a higher quality of arguments, better relevance scoring, and more reliable annotations compared to existing datasets. The authors encourage further research and feedback on the dataset.</sample>
    <sample id="175">The method addresses the uncertainty in permutation by inducing alignment as part of training. It also approximates the NP-hard permutation search with a GPU-friendly continuous relaxation that allows for backpropagation and learning linguistically plausible permutations.</sample>
    <sample id="176">参考英语内容中，对下游 NLP 模型公平性的定义是：**如果语言模型与不同政治倾向的下游任务在不同群体（例如，不同社会群体）上的表现存在差异，那么就存在公平问题。** 换句话说，公平性体现在模型对不同社会群体表现出不同的偏好或表现，这可能导致某些群体被边缘化或受到歧视。</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustav Sinha.</sample>
    <sample id="179">## Minding Language Models’ (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker - Key Information

Melanie Sclar's presentation introduces SymbolicToM, a novel method to enhance Theory of Mind (ToM) reasoning in Large Language Models (LLMs). ToM is the ability to understand that others have beliefs, desires, and intentions that may differ from one's own.  The presentation highlights the limitations of current LLMs in performing well on ToM tasks, particularly false-belief scenarios, exemplified by the classic Sally-Anne test.

The core problem addressed is the poor performance of LLMs on these tasks, prompting the research question: "How can we improve Theory of Mind reasoning skills in Large Language Models?"  SymbolicToM offers an inference-time solution using explicit graphical representations of mental states.  It leverages off-the-shelf Natural Language Inference (NLI) and OpenIE models to compute belief graphs for all character combinations up to a defined level of complexity. These graphs represent what each character believes to be the current state of the world and what they believe the other character believes.

The method efficiently answers questions by detecting entities, retrieving relevant belief graphs, performing recursive reasoning over the question, and then feeding the resulting sentences and factual question to a language model for the final answer.  Experiments comparing SymbolicToM with supervised baselines (fine-tuned GPT-3 and Textual Time Travel) demonstrate significant performance gains across various LLMs, including GPT-3, Macaw, and Flan-T5-XXL.  The method achieves accuracy improvements ranging from 51 to 67 points.

Furthermore, the research evaluates SymbolicToM's generalization capabilities through new datasets designed to test storage structure and linguistic diversity.  Supervised models struggle with these out-of-domain datasets, but SymbolicToM consistently shows substantial improvements, even surpassing more powerful models like GPT-4.  This indicates that SymbolicToM is robust and avoids overfitting.

In conclusion, SymbolicToM is presented as a plug-and-play method for improving LLM ToM reasoning. It offers interpretable reasoning through explicit graphical representations, avoids overfitting, and demonstrates strong performance across both in-domain and out-of-domain scenarios, making it a promising advancement in the field of artificial intelligence. The paper provides further details and is available for reference.</sample>
    <sample id="180">Myra, Esin Durmus, and Dan Jurafsky.</sample>
    <sample id="181">Fudan University's Siyu Yuan introduces their work on "Distilling Script Knowledge from Large Language Models for Constrained Language Planning." The research addresses the challenge of planning for specific, constrained goals, a gap in current language model planning capabilities, which primarily focus on abstract, stereotypical activities.

The paper defines "constrained language planning" as planning with specific constraints on goals, exemplified by "make a chocolate cake" versus "make a cake."  The core problem is ensuring generated scripts are both reasonable and faithful to these constraints.  The authors tackle this by first evaluating and improving large language models' performance on constrained planning.  They found that existing models struggle with specific goals, exhibiting acceptable semantic completeness but lacking constraint faithfulness.

To address this, they propose an "over-generate-then-filter" method.  This involves using InstructGPT to generate multiple scripts for a given goal, then employing a filter model to select the most constraint-faithful scripts.  The filter leverages embeddings and keyword matching to assess script quality.  This method significantly improves script quality in both semantic completeness and constraint adherence.

Furthermore, the research focuses on creating a high-quality dataset for constrained language planning called "CoScript."  They generate 55,000 specific goals and corresponding scripts using large language models and then employ crowd-sourced workers to revise and correct the dataset.  CoScript exhibits high diversity in goal types.

Finally, the paper demonstrates that smaller, specialized models like T5, fine-tuned on CoScript, can achieve better performance than larger models, highlighting the potential of data-driven approaches to enable constrained language planning with resource-efficient models. The authors conclude that CoScript can serve as a valuable resource for advancing research in language planning.</sample>
    <sample id="182">The term "tropicalism" in the context of this paper refers to a trope that connects to the portrayal of Latina women as "vibrant" and "curvaceous." This trope is linked to a long history of Asian women being hyper-sexualized, seen as very docile and submissive.</sample>
    <sample id="183">The authors created target group personas by prompting the language model to generate descriptions of imagined individuals, specifying their demographic identity (e.g., "Imagine you are an Asian woman. Describe yourself."). They then analyzed the generated personas to identify patterns and stereotypes.</sample>
    <sample id="184">CXMI (Contextualized Word Mover's Distance) is used to measure context usage by machine translation models. It assesses how much information the context provides about the target word given the source sentence. Pointwise CXMI extends this to measure context usage at the sentence or word level.</sample>
    <sample id="185">DrBERT 是一个基于 RoBERTa 的法国医学预训练模型，使用 NACHOS 数据集训练。ChuBERT 是一个基于临床数据的法国医学模型，使用 Nantes University Hospital 数据仓库中的数据训练。</sample>
    <sample id="187">Ying and Zhiyang.</sample>
    <sample id="188">Iterative transfer learning updates the model by training on the latest set of data collected.</sample>
    <sample id="189">The goal of the AltEntities Corpus is to understand how users express choices when they are unsure of the exact entity they want to select, often using indirect references. It aims to address the challenge of resolving indirect referring expressions in entity selection, which is important for conversational systems and benchmarking LLMs.</sample>
    <sample id="190">攻击者通过学习从 embedding 中获取模型信息，并模仿 embedding 服务来提供类似服务，从而窃取模型。</sample>
    <sample id="191">Three.</sample>
    <sample id="192">CAME (Confidence-guided Adaptive Memory Efficient Optimization) addresses the challenge of balancing fast convergence with low memory usage in large language model training, a problem exacerbated by the memory demands of traditional optimizers like Adam and the performance limitations of memory-efficient alternatives like Adafactor.

The core issue is that while NMF (Non-negative Matrix Factorization) offers significant memory reduction, its application in deep learning optimizers like Adafactor introduces errors that slow down convergence.  CAME tackles this by analyzing the instability in the momentum updates, specifically the residual between the predicted and actual updates. This residual is used as a denominator to adaptively adjust the optimization step, mitigating the effects of these errors.  Essentially, CAME leverages a "confidence" factor derived from the instability to guide the optimization process.

Experiments on BookCorpus and English Wikipedia, using BERT, GPT-2, and T5, demonstrate CAME's effectiveness. CAME achieves a 3.4% improvement in validation accuracy compared to Adafactor, using the same training steps.  It also outperforms Adam in pre-training very large models, particularly with increasing batch sizes.  Furthermore, CAME enhances BERT-Large training, achieving comparable performance to Adam and Adafactor while significantly reducing memory consumption.  

The memory usage comparison, even at a batch size of 1, reveals that CAME reduces memory footprint compared to Adam and LAMB, surpassing even existing memory-efficient optimizers like SM3.  The confidence-guided approach allows CAME to effectively handle large batch training, a crucial extension for memory-constrained environments.  In summary, CAME offers a promising solution for efficient and stable training of large language models, addressing the limitations of existing optimizers by intelligently managing memory usage and maintaining fast convergence.</sample>
    <sample id="193">This information is not provided in the text. The text mentions a large-scale annotation, but doesn't specify the number of annotators.</sample>
    <sample id="194">Carnegie Mellon University, University of Washington, and the Allen Institute for AI.</sample>
    <sample id="195">The presentation introduces RoHT (Reasoning over Hierarchical Question Decomposition Tree), a novel framework for Explainable Question Answering (XQA) that addresses limitations of existing methods.  Current XQA approaches, including neuro-symbolic and decompose-based methods, face challenges in knowledge source limitations (neuro-symbolic) and difficulty in handling diverse natural language (decompose-based).  RoHT aims to overcome these limitations by leveraging question decomposition to flexibly select knowledge sources for each sub-question.

The framework operates in a two-stage process. First, it constructs a Hierarchical Question Decomposition Tree (HQDT) to understand the compositional structure of complex questions. The HQDT breaks down the question into atomic questions and intermediate sub-questions.  A question decomposer and question generator are used to create the HQDT, with certainty scores assigned to each node to reflect the likelihood of its generation.

Second, RoHT employs probabilistic reasoning over the HQDT to fuse knowledge from both a knowledge base (KB) and a text corpus at different levels of the reasoning process.  The reasoning proceeds recursively from the root to the leaves, with each node considering appropriate knowledge sources (KB, text, or recursive calls).  An aggregator then combines candidate answers from all sources to produce the final, top-key answer.

The framework is evaluated on two challenging datasets: KQA Pro (KB QA with incomplete knowledge) and Musique (QA comprehension with text and KB).  On KQA Pro, RoHT outperforms existing KB QA methods, especially when combined with Wikipedia. It also surpasses TransferNet, an end-to-end trained model. On Musique, RoHT significantly improves F1 score compared to state-of-the-art methods, and RoHT-mix (combining text and KB) outperforms TransferNet.  The results demonstrate the effectiveness of explicit question decomposition and knowledge fusion for achieving better explainable and accurate question answering.  The framework's ability to leverage heterogeneous knowledge sources and its hierarchical structure offer a promising approach to tackling complex QA tasks.</sample>
    <sample id="196">"I saw Bart and Lisa" is an example of coordination with a governor on the left.</sample>
    <sample id="197">根据内容，最先进的对话系统模型是：

*   **四个 state-of-the-art chat models** (四个最先进的聊天模型)

内容中没有具体说明这些模型是什么，只提到它们被用于 ABC-Eval 的评估。</sample>
    <sample id="198">在整个上下文窗口中评估模型的可接受性是因为现在的语言模型拥有越来越长的上下文窗口，而现有的最小对评估方法（MPP）无法处理更长的序列，因此需要评估模型在不同长度的上下文中的可接受性。</sample>
    <sample id="199">Yes, training on a mixture of various languages can lead to a performance drop in English on seven of the nine datasets, while only gaining performance in three. This is referred to as the "Curse of Multilinguality."</sample>
    <sample id="200">The annotators were shown background knowledge about the entities, but they didn't necessarily know about the entities themselves.</sample>
    <sample id="201">BLEURT scores and expert-based human evaluation results.</sample>
    <sample id="202">是的，泛化中的回归会影响特定的 NER 类型。论文发现，CoNLL-2003 标签器在 2023 年仍然有效，但性能下降的主要原因是时间漂移，这表明不同类型的 NER 任务可能对时间漂移的敏感度不同。</sample>
    <sample id="203">NLPositionality 研究表明，NLP 领域存在立场（positionality），即模型和数据集会反映特定人群的观点和经验，从而导致对不同人群的偏见。这使得 NLP 技术在不同文化和背景下表现出差异，可能加剧社会不平等。因此，理解和解决 NLP 中的立场至关重要，以便构建更公平、包容的 AI 系统。</sample>
    <sample id="204">根据内容，文章中提到 "multilingual language models such as Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks." 这暗示了它们可能没有达到理想的性能，但没有明确说明它们是否采用适配器微调或完整微调。</sample>
    <sample id="205">## Summary of "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models"

Shangbin's presentation details research on political biases in large language models (LLMs) trained on web crawl data, particularly focusing on the influence of political news media coverage. The study investigates how these biases propagate from pretraining to model performance and downstream applications, highlighting potential fairness issues in NLP.

The research begins by evaluating the political leanings of existing LLMs like GPT-4 and the GPT series, finding that they exhibit varying degrees of political bias, with GPT-4 being more liberal and the GPT series generally more socially liberal than BART models.  A controlled experiment further demonstrates that pretraining LLMs on partisan corpora (news and social media categorized by political leaning) shifts their ideological coordinates, with models trained on left-leaning Reddit corpora exhibiting a liberal shift.  Furthermore, the study analyzes the impact of temporal shifts in political polarization, showing that models trained on corpora from after 2017 exhibit a further distancing from the center, indicating the incorporation of societal polarization.

To assess the practical implications, the research evaluates LLMs on hate speech and fake news detection.  Analysis reveals that left-leaning models are better at detecting hate speech targeting minority groups, while right-leaning models perform better at detecting hate speech targeting specific demographics. Similar patterns emerge in fake news detection, with opposite political leanings showing differing detection rates. Qualitative examples further illustrate how LLMs with different political leanings generate varying predictions based on social categories.

The presentation concludes by emphasizing the pressing fairness issue posed by political biases in LLMs.  The researchers acknowledge the dilemma of balancing the propagation of bias with the risk of censorship or exclusion.  They frame the issue as a complex challenge, akin to the "electric trolley problem," where mitigating bias requires careful consideration of what constitutes neutrality in training data.  The study underscores the need to address these biases to prevent potential marginalization and the spread of hate speech and misinformation through LLM-powered applications.</sample>
    <sample id="206">Transfer learning from topic-independent dissonance stance classification (debate) and binary classification of expansion and comparison (CE) classes of PDTB.</sample>
    <sample id="207">WMT evaluation.</sample>
    <sample id="208">The authors ultimately propose three recommendations for model owners.</sample>
    <sample id="209">提议的方法在语义完整性和约束忠实度方面都获得了提升，并且T5在Fine-tuning后能生成比大多数大型语言模型更好的脚本。</sample>
    <sample id="210">Shuheng.</sample>
    <sample id="211">是的，论文中的结果和数据集可以用作基准。DEPLAIN 数据集包含手动对句子进行对齐的句子对，这可以作为自动对齐方法的评估标准。此外，论文还展示了通过对语言模型进行微调来生成简化文本的实验结果，这些结果被认为是自动文本简化问题的基准。</sample>
    <sample id="212">The paper includes experiments with smaller, specialized models like T5 fine-tuned on CoScript. The text states that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models. It doesn't specify the exact number of models tested, but it highlights the success of T5.</sample>
    <sample id="213">OFA (Unified Multi-Modal Pre-trained Model)</sample>
    <sample id="215">Adam Przepiórkowski's talk argues against asymmetric dependency structures of coordination (like those headed by a single conjunct, as in universal dependencies or Mel'čuk's theory) and supports symmetric structures (like those headed by the conjunction, as in the Prague approach or multi-headed approaches). The core argument relies on dependency length minimization, a principle similar to how direct objects prefer proximity to the verb.

The talk uses examples like "Marge read it yesterday" and "Marge read yesterday this absolutely fascinating book about bees" to illustrate this point. It shows that while direct objects ideally are close to the verb, dependency length minimization favors shorter dependencies.  The paper analyzes statistics from the Penn Treebank, revealing that left conjuncts tend to be shorter in coordination, especially when the governor is on the left or absent. This tendency increases with the length difference between the conjuncts.

A key finding is that this length preference only holds true when the governor is on the left or absent. When the governor is on the right, the effect disappears. The talk presents data measured in characters, syllables, and words to demonstrate this correlation.  The paper concludes that this observation provides evidence supporting symmetric coordination structures, as it suggests that the length of the conjuncts is a crucial factor in determining the dependency structure, favoring a more balanced approach. The speaker encourages further discussion at the poster session.</sample>
    <sample id="217">## Seen to Unseen: A Summary of the Research

This work, "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation," addresses the limitations of existing controllable dialogue generation (CDG) methods, which often focus on single attributes or lack the ability to handle continuous attributes effectively. The research highlights that current approaches struggle with compositional generalization, meaning they fail to generate coherent dialogues when attributes are combined in novel ways.

The authors propose **DCG (Disentangled Controllable Generation)**, a novel model designed for multi-attribute CDG. DCG leverages a disentangled representation of attribute concepts learned from seen values, utilizing a disentanglement loss to separate different attribute combinations.  A key innovation is the introduction of a **unified reference-free evaluation framework (MAE)**, which allows for evaluation across different levels of attribute granularity without requiring large labeled datasets.

The model is built upon the DialoGPT framework and employs a **compositional prompt module**.  Two types of prompts are used: **attribute-oriented prompts** that guide the model to focus on specific attribute values, and **task-oriented prompts** that incorporate global dialogue features.  A disentanglement loss further enhances the model's ability to distinguish between different attribute combinations.

Experiments on the DailyDialog-CG dataset demonstrate that DCG outperforms existing baselines in both attribute controllability and text quality.  Attribute-oriented prompts are shown to effectively guide the model, while task-oriented prompts improve text equality.  The disentanglement learning component enhances compositional generalization.  MAE achieves strong correlation with human judgments, even surpassing classic metrics.

The research concludes that DCG successfully tackles the challenge of compositional generalization in multi-attribute CDG by disentangling attribute combinations and learning relationships between attributes.  The proposed prompt-based disentangled model offers a promising approach for generating coherent and controllable dialogues with unseen attribute combinations, demonstrating a significant improvement over previous methods. The ability to generalize from seen to unseen attribute combinations is a key strength of the proposed approach.</sample>
    <sample id="218">Google Translate.</sample>
    <sample id="219">Jia-Huei Ju from Academia Sinica presented their research on a multi-stage pipeline for extracting financial signals from annual reports (Form 10-K). The work addresses the challenge of manually analyzing these reports, which contain repetitive information and yearly-dependent content.

The research focuses on a "highlighting task" – identifying the key words (rationale) that reveal relationships between a company's current report and its previous year's report.  The pipeline consists of three stages: document segmentation, relation recognition, and out-of-domain/in-domain fine-tuning.

Stage 1 classifies report pairs into three types: highly similar (e.g., regulations), similarly structured but semantically different (revised pairs), and dissimilar (new information).

For model tuning, they utilize an external dataset (eSNLI) for out-of-domain fine-tuning, then refine the model using revised pairs and a mix of cross-entropy and KL divergence loss to improve pseudo-label quality.

The evaluation uses both eSNLI and a newly released FINAL dataset, with performance measured by precision and PCC (Pearson correlation coefficient). The research demonstrates that their domain-adaptive highlighting model achieves strong performance on FINAL and maintains generalization ability on eSNLI.  Interestingly, the model also benefits from incorporating mismatched pairs during training.

The authors conclude by highlighting the potential of their method for enhancing information retrieval in financial analysis. They encourage further research and provide links to their paper and GitHub repository for more details.</sample>
    <sample id="220">Vasudha is a Computer Science PhD candidate at Stony Brook University.</sample>
    <sample id="221">The paper evaluated the transition capability of large language models using the best practices of the MT community, which involves using the latest test sets to avoid overlap with the training data. The evaluation included comparison to state-of-the-art systems, so the best performing system, so the WMT evaluation. The examples used in the experiments were translation from German into English.</sample>
    <sample id="222">## "To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering" 总结

本文探讨了在开放域问答（Open-Domain QA）中，如何应对模型在不同领域表现不佳的问题。研究人员以 Wikipedia 为基础的通用领域模型，针对生物医学等特定领域进行测试，发现模型在目标领域表现不佳，主要原因是模型缺乏目标领域的知识，导致检索到的相关 passage 无法准确回答问题。

研究人员提出了三种主要贡献：一是探索了多种数据干预方法，二是识别了目标领域数据带来的数据偏移类型，三是确定了针对不同偏移类型的有效数据干预策略。

**数据干预方法：**

*   **Few-shot learning:** 使用少量目标领域数据，通过提示大型语言模型生成更多目标领域数据，并将其转化为 cloze-style 问题，用于增强 retriever 和 reader 模型的适应性。实验结果表明，few-shot 方法可以显著提升 retriever 和 reader 模型的性能。
*   **Zero-shot learning:**  通过控制问题、答案和上下文之间的交互，来理解不同数据偏移类型对模型学习的影响。研究发现，cloze-style 问题比标准 WH 问题更容易进行数据干预。
*   **答案分布控制:** 通过从目标领域抽取命名实体，并根据实体类型生成 cloze-style 问题，来控制答案的分布，发现均匀分布效果最佳。

**数据偏移类型：**

研究人员利用数据偏移理论，将目标数据集划分为“无偏移”、“概念偏移”、“共变偏移”和“全偏移”四类。通过计算 retriever 和 reader 模型在目标数据集上的兼容性度，可以评估数据偏移类型。实验结果表明，不同类型的偏移需要不同的数据干预策略。

**结论：**

研究表明，few-shot learning 适用于所有目标数据集，而 zero-shot learning 则更适用于概念偏移和共变偏移。针对无偏移数据集，数据干预效果不明显。总而言之，通过合理的干预策略，可以显著提升 open-domain QA 模型在特定领域的性能。</sample>
    <sample id="223">Shangbin.</sample>
    <sample id="224">在实验过程中研究了以下模型：

*   MASSalign (作为自动对齐方法的最佳方法)
*   long-mBART (用于文档级别简化)
*   base mBART (用于句子级别简化)</sample>
    <sample id="225">MultiInstruct 中用于训练和测试目的的 62 个不同任务中，有 53 个用于训练，其余 9 个用于测试。</sample>
    <sample id="226">The provided text does not mention the number of authors. It only mentions Regina Stodden as the presenter.</sample>
    <sample id="227">This research addresses a key challenge in current language models: grounded language understanding.  The core problem is that most language models are pre-trained on text without grounding, leading to a gap between pre-training and real-world application. This makes it difficult for them to translate natural language instructions into executable plans or programs, hindering applications like smart assistants, semantic search, and robotic control.

The research proposes a novel framework called Pangu for grounded language understanding, which shifts the focus from generation to discrimination. Pangu utilizes a symbolic agent to interact with the environment and propose candidate plans, while a language model is used solely to score and rank these candidates. This approach avoids the language model needing to handle the validity and grammar of the target plan, simplifying the task.

Experiments demonstrate Pangu's effectiveness across various language models (BERT, T5, Codex) and training methods (fine-tuning, in-context learning). Pangu achieves strong performance and sample efficiency, even with limited examples.  A key finding is Pangu's robustness to non-independent and identically distributed (non-i.i.d.) data, suggesting it generalizes better than autoregressive models like ArcaneQA, which tend to overfit.

The central takeaway is that for grounded language understanding, discrimination is a more promising strategy than generation.  Pangu separates the symbolic world (agent and environment) from the language model, enabling it to excel at evaluating plans without needing to create them. The framework is generic and applicable to various grounded language understanding tasks, with knowledge-based question answering serving as a representative testbed. The research highlights the potential of leveraging language models for grounded understanding by focusing on their strengths in discrimination rather than generation.</sample>
    <sample id="228">AG News, MIND, SST2, Enron Spam.</sample>
    <sample id="229">Gabriella Skitalinskaya and Henning Wachsmuth's work focuses on improving argumentative writing by detecting improvable claims. The paper addresses the challenge of determining when an argumentative claim is sufficiently phrased and whether further revisions are needed. They introduce two new tasks: Suboptimal Claim Detection (identifying claims needing revision) and Claim Improvement Suggestion (selecting quality issues for revision).

The authors explore the difficulties of learning from revision-based data, which differs across domains due to varying quality notions and revision types. They specifically concentrate on argumentative text, leveraging revision histories from collaborative online debate platforms like Kialo.  The paper highlights four key challenges in working with such data:

1. **Representativity and Reliability:** Ensuring the dataset accurately reflects high-quality arguments and avoiding overlooking truly optimal claims.
2. **Model Complexity and Architecture:** Selecting models sensitive to subtle changes in claim delivery and evaluating the impact of pre-training, fine-tuning, and classification.
3. **Contextual Information:** Determining which contextual elements (e.g., debate structure, parent claim, domain knowledge) are relevant to claim quality assessment.
4. **Topical and User Bias:** Addressing noise in revision histories due to user errors, biases, and the influence of social/cultural context on argument effectiveness.

The paper analyzes strategies to overcome these challenges and compares different approaches for the two tasks.  Their experiments suggest that revision-based data is effective for claim assessment, modeling the distance between claim versions aids in detecting suboptimal claims, and the impact of context depends on the task and the specific quality issues.  The authors conclude that their work provides valuable insights into utilizing revision data for improving argumentative writing. They encourage readers to consult their paper for detailed analysis and findings.</sample>
    <sample id="231">NACHOS is a dataset of medical crawled data from the web.</sample>
    <sample id="232">David Vilar.</sample>
    <sample id="233">The presentation introduces "Attention as a Guide for Simultaneous Speech Translation" paper by Sara Papi, Matteo Negri, and Marco Turchi, focusing on the challenges and solution for Simultaneous Speech Translation (SimuST). SimuST aims to translate spoken language into text in real-time, enabling cross-language communication. Current SimuST models face issues with long training procedures, optimization objectives, and maintaining multiple models for different latency levels (e.g., 1 second and 2 seconds).

The proposed solution, EDAtt (Encoder-Decoder Attention), addresses these problems by leveraging existing offline ST models without retraining or custom architectures. It utilizes a single model for all latency regimes, controlling latency through specific parameters.  The core idea is to use cross-attention to guide partial translation. EDAtt decides whether to emit a word based on the attention weights, specifically checking if the attention is concentrated towards the last lambda speech frames.  A word is emitted if the attention is not concentrated (sum of attention below a threshold alpha), indicating stable information.  Conversely, if attention is spread across the frames, the word is not emitted, and the system waits for more speech.

The presentation highlights experimental results comparing EDAtt with existing strategies like Wait-k and Local Agreement, as well as state-of-the-art architectures for pre-translation.  The results demonstrate that EDAtt outperforms all these strategies in terms of translation quality (BLEU score) and latency (average lagging).  Furthermore, EDAtt achieves the fastest latency, considering both elapsed time and computational-aware time.  The authors emphasize the importance of balancing translation quality and latency.  The code and models are publicly available to promote reproducibility. The paper's findings suggest that EDAtt offers a more efficient and effective approach to SimuST by intelligently utilizing attention mechanisms to guide partial translation.</sample>
    <sample id="234">Prompting has a big influence on the performance of LLMs for translation. A simple experiment showed that the difference in BLEURT scores between one-shot and five-shot prompting was more than one point, and could go up to 40 points in extreme cases. However, for several short promptings, the actual form of the prompting doesn't have a big influence. The examples used in the prompt are more important than the similarity to the source sentence.</sample>
    <sample id="235">This paper was done in collaboration with Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig. The provided text does not explicitly state the authors' institutional affiliations. However, the names suggest they are researchers at a university or research institution.</sample>
    <sample id="236">The presentation does not specify what the five expert-written instructions are. It only states that each task in MultiInstruct is equipped with five expert-written instructions.</sample>
    <sample id="237">作者建议通过设计一个核心指代消解任务，并根据信息来源的可用性（预训练时、同时、仅在推理时）来创建不同的测试设置（KITMUS），来测试模型整合来自多种来源的信息的能力。</sample>
    <sample id="238">MeetingBank is a new benchmark dataset for meeting summarization, created by Yebowen Hu from the University of Central Florida. The dataset comprises 1,366 City Council meetings, including transcripts, reference summaries, and URLs. The creation of MeetingBank addressed challenges in obtaining high-quality summaries and trustworthy public meeting resources.

The data collection process involves using Speechmatics API to transcribe audio, identifying meeting types and IDs, retrieving reference summaries from meeting minutes, and aligning timestamps to create meeting segments.  The dataset includes statistics on meeting duration, token counts, speaker counts, and the year of the meetings.  It also provides summarization instances and average sentence/token counts for source and summary texts.

The dataset is analyzed for abstraction levels using coverage (percentage of summary words in source) and density (degree of extraction). Coverage scores typically range from 0.7 to 0.9, indicating a mix of verbatim and abstract summaries. Density scores show varying levels, with Seattle and Boston having the highest and Denver the lowest.

MeetingBank is used to evaluate various summarization systems, including extractive methods (Oracle, LEAD, LexRank, TextRank) and abstractive methods (BART-Large, Pagasus, Longformer, DialogLM, HMNet).  Extractive systems like Extr-Oracle achieve high ROUGE-2 scores. DialogLM performs best among abstractive models. GPT-3 shows promising results in zero-shot summarization but struggles with automatic metrics.

Human evaluation using metrics like BERTScore, MoverScore, and question/answering methods reveals that GPT-3 achieves the highest overall scores, excelling in fluency and coherence but lacking in informativeness and factuality.  The findings suggest a need to focus on capturing key discussion points and develop better automatic evaluation metrics.

MeetingBank is presented as a valuable resource for researchers and a tool for understanding City Council decision-making processes. The dataset is publicly available for download and use.</sample>
    <sample id="239">大家好，我的名字是戴维·维拉尔，我将简要回顾一下论文“Prompting PaLM for Translation：评估策略与性能”。这是一项与我同事们合作完成的工作，来自谷歌翻译。PaLM 是 2022 年发布的一款 5400 亿参数的大型语言模型。它在 2022 年训练了 7800 亿个 tokens 的大量文本数据集。在发布时，它在数百个 NLP 任务中达到了最先进水平。在这项工作中，我们首次系统地研究了大型语言模型对机器翻译的提示方法。我们使用机器翻译社区的最佳实践来评估这些模型的转变能力。这包括使用最新的测试集，以避免测试数据与语言模型训练数据之间的重叠。我们还将其与最先进系统进行比较，即 WMT 评估中的最佳性能系统。我们使用最先进的神经机器翻译指标，并额外展示了专家基于人类评估的结果。最后，我们提供了一些关于提示选择策略的建议。提示对 LLM 的翻译性能有很大的影响，正如我们简单实验所看到的，我们在每个句子中使用了两种不同的提示，并使用了单次提示。大多数句子（1000 个句子中的 516 个）。观察到的差异超过了 BLEURT 分数的一个点。在极端情况下，这种差异可以高达 40 分数。因此，选择一个好的提示策略非常重要。在我们的实验中，我们采用了五次提示策略，即我们只需标记每个我们提供的句子所使用的语言。例如，在从德语翻译成英语时，德语句子使用德语冒号标记，英语翻译使用英语冒号标记。我们发现，实际的提示形式对几次短提示的影响不大。对于零次和一次提示，这至关重要。当我们采用五次提示时，实际的提示形式几乎没有差异。例子本身携带了大部分权重。我们实验结果的总结是，例子质量比与源句子相似度更重要。因此，重要的是从高质量的翻译中选择例子。特别是，我们比较了从 WMT 评估的开发数据中选择提示，该开发数据比训练数据更经过精心挑选，质量更高，因此更少噪声。他们的结果表明，使用开发数据性能更好。然而，专门的先进系统对 PaLM 的翻译具有显著优势。但是，PaLM 接近商业系统。在我们的案例中，我们选择使用谷歌翻译进行评估。我们通过 MQM 框架进行的专家基于人类评估揭示，PaLM 的流畅度与最先进系统相当，但主要区别在于准确性。特别是，最常见的错误是省略错误。看来 PaLM 会选择产生更好的声音的翻译，有时会丢弃源句中被翻译掉的部分。然而，PaLM 的“风格/不自然”类别低于最先进系统，这进一步表明 PaLM 提供流畅的输出，但仍然存在一些准确性问题。这就是本次简要概述的内容。有关更多详细信息，请访问论文的完整演示。非常感谢。</sample>
    <sample id="240">大家好，我是Dawei，一位来自德国萨尔州立大学的博士生。在这个视频中，我想向大家介绍我们最近的研究成果“更弱于你所想：对弱监督学习的批判性视角”。这项工作是与Xiaoyu Shen、Marius Mosbach、Andreas Stephan和Dietrich Klakow共同完成的。我想从简要介绍弱监督学习和弱监督学习开始。在弱监督学习中，我们无需手动标注数据。相反，我们使用弱标注来源，例如简单的启发式规则、知识库或低质量的众包数据进行标注，如图所示。与人工标注相比，这些弱标注更便宜，但它们也存在噪声，这意味着某些标注是不准确的。如果我们直接在弱标注数据上训练神经网络，神经网络会倾向于记住这些标注中的噪声，而不会泛化。在弱监督学习中，我们提出了一些训练算法，以在如此标注噪声的情况下，Robust地训练神经网络，从而使训练后的模型仍然能够很好地泛化。在弱监督学习（WSL）领域，一个常见的说法是，人们说他们只在弱标注数据上训练模型，并能获得在干净测试集上的高性能。技术上，这个说法并不错误，但有一个问题，那就是人们假设存在额外的干净验证集可用用于模型选择。我们不能停止在这个问题设置，但这暗示了弱监督学习中需要额外的手动标注。但正如一个潜伏在其中的问题一样，这个需求经常被忽视。上述疑虑引发了三个研究问题。首先，弱监督学习是否需要干净的验证数据，或者我们可以使用噪声验证集呢？其次，如果需要干净的数据，或者干净数据是弱监督学习工作正常运行的必要条件，那么我们需要多少干净样本？最后，我们应该只使用干净的样本进行验证，还是有更好的方法可以利用它们？我们在这项工作中就解决了这些研究问题，我们的发现如下。首先，我们发现，最近的弱监督学习方法确实需要干净的验证样本才能正常工作。否则，性能会大幅下降。如图所示，如果没有干净的验证样本，训练后的模型将无法超出原始弱标签，这意味着训练毫无意义。这表明弱监督学习方法实际上需要干净的标注数据才能正常工作，我们应该不忽视获得干净验证样本的标注成本。我们的第二个发现是，增加干净的验证样本的数量将帮助弱监督学习方法获得更好的性能，如图所示。通常我们只需要每类20个样本就能获得高性能。但这还不是全部，如果我们决定使用干净样本，那么直接在它们上进行训练将甚至获得更好的性能。图中的右边显示了直接微调方法与弱监督学习方法（仅使用干净数据进行验证）的性能差异。正如我们所见，如果我们有每类10个样本，直接微调开始优于弱监督学习方法。最后，我们能够实现的前沿弱监督学习方法声称的性能提升很容易通过允许在干净的验证样本上进行持续微调来实现。正如图所示，最初性能较差的Vanilla模型（称为FTw），在更复杂的弱监督学习方法（如COSINE）的帮助下，性能会得到提升。但是，如果我们允许在干净的验证样本上进行持续微调，那么FTw将与其它方法性能相当。因此，在实践中，没有理由选择更复杂的弱监督学习方法，这些方法需要更多的计算时间和磁盘空间。总而言之，我们证明了最近的弱监督学习方法需要干净、手动标注的样本才能正常工作。它们的性能提升和实用性被严重夸大了。我们为未来的工作提出了以下具体建议。首先，报告模型选择标准。例如，报告是否使用干净的验证样本进行模型选择。其次，弱监督学习方法应与少样本学习基线进行比较，因为两者都可以在干净样本上工作。第三，持续微调是一个简单但强大的基线，未来工作应考虑它。最后，我们已开源代码。您可以在此幻灯片上的二维码找到它。请随意查看。谢谢大家，祝大家在会议上愉快。</sample>
    <sample id="241">## Human-in-the-Loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments - Key Information

This paper addresses the shortcomings of current automated misinformation detection systems on social media, which often suffer from unrealistic evaluation methods and a lack of human-centric design. The authors, Ethan, Yang Chen, Wei Xu, and Alan Ritter from Georgia Tech, propose a novel evaluation framework for developing more effective and practical systems.

The core problem is that existing systems are often evaluated using retrospectively constructed datasets, lacking real-time data and vulnerable to leaked counter-evidence. Furthermore, they often fail to adequately incorporate human expertise, treating humans as either final arbiters or completely excluding them from the process.

The proposed framework is end-to-end, integrating human feedback throughout the misinformation detection pipeline. It focuses on building systems that are not authoritative but rather supportive of human moderators. The authors concretely implement and evaluate their workflow using the COVID-19 treatment misinformation case study.

The system comprises two main components: 1) **Misleading Claim Detection:** This component uses keyword filtering and a T5 model for question answering to extract claims about COVID-19 treatments from raw tweets. These claims are ranked by trendiness and then presented to humans for verification. 2) **Policy Violation Verification:** This component leverages a BERT-based stance classification model to determine the author's stance towards unapproved treatments. Supporting stance tweets are flagged for human review.

The evaluation focuses on "early detection," defined as identifying unapproved treatments before they appear in debunking articles. The system achieved a 65% accuracy in flagging tweets violating Twitter's COVID-19 misinformation policies.  The system demonstrated a high efficiency in identifying policy violations, detecting 124.2 violations per human hour worked.

The authors emphasize the importance of evaluating systems in a realistic, human-in-the-loop setting. Their framework provides a valuable, industry-independent perspective on developing and evaluating misinformation detection systems, motivating future research in this area. The paper highlights the critical need for early detection to effectively combat the spread of misinformation and provides a concrete example of how human expertise can be integrated into automated systems.</sample>
    <sample id="242">The common practice for evaluating dialogue quality is using human evaluation, such as asking human judges to select which of two conversations is better or to rate conversations using a Likert scale. However, these methods can be subjective and don't capture all aspects of dialogue quality.</sample>
    <sample id="243">Here's the answer to your question, based on the provided text:

The paper has multiple authors, including Jenny (the presenter), Sebastian Santy, Ronan Le Bras, Katharina Reinecke, and Maarten Sap. The text explicitly mentions these individuals as collaborators.</sample>
    <sample id="244">在 Servin 和 Kea 的示例中，需要以下背景知识：

1.  Servin 是一个法官。
2.  法官在法律法庭审理案件。</sample>
    <sample id="245">Lining Zhang's presentation details a pipeline for identifying high-agreement workers on Amazon Mechanical Turk (MTurk) for summarization tasks. The motivation stems from the limitations of automatic metrics and a lack of established best practices for MTurk recruitment. The pipeline employs a two-step qualification process: a "Qualification Task" and an "Endurance Task."

The Qualification Task assesses annotator ability to evaluate multiple dimensions of summaries, categorizing workers into "gold," "silver," "bronze," and "block" tiers. Only "gold" and "silver" workers pass, resulting in 26 qualified participants (13% of 200). The Endurance Task tests the capacity to handle a heavy workload, with workers evaluating multiple summaries per document. This stage yields 12 qualified workers (6%), with 4 "gold" and 8 "silver." These workers demonstrate high inter-annotator agreement (IAA), exceeding that of experts, as evidenced by a best Krippendorff's Alpha of 0.443.

The pipeline also includes a reference-based task to evaluate general performance. Eight out of twelve pipeline workers completed all HITs, achieving a Krippendorff's Alpha of 0.534.  A comparison with Baseline MTurk workers reveals that the best approach (MACE statistical filter) achieves a Krippendorff's Alpha of 0.380, but with incomplete HIT coverage. CloudResearch MTurk workers, recruited from a dedicated platform, achieve a Krippendorff's Alpha of 0.513, but with a lower task acceptance rate.

Analysis of correctness across annotation sources shows a significant Spearman's correlation between pipeline and CloudResearch workers.  The presentation suggests that the pipeline can achieve high agreement at a lower cost than CloudResearch, while also potentially matching its quality.

In conclusion, the pipeline identifies high-agreement workers, offering a cost-effective solution for large-scale annotation. Future work will focus on improving worker quality in terms of both agreement and correctness, expanding to different tasks, languages, and platforms. Limitations include the English-only focus, the potential for the qualification questions not to be universally effective, and the lack of a guarantee for correctness training. The research was funded by Google.</sample>
    <sample id="246">Yes, the code and dataset are publicly available on GitHub.</sample>
    <sample id="247">## FACTKG: Fact Verification via Reasoning on Knowledge Graphs - Summary

Jiho Kim from KAIST AI presents FACTKG, a novel dataset and task for fact verification leveraging knowledge graphs (KGs). Existing datasets like FEVER and VitaminC primarily use text or tables as evidence, but FACTKG uniquely utilizes KGs to address the limitations of these approaches.

The core idea is that KGs offer a more reliable and practical way to verify claims. Unlike text/table-based evidence, KG evidence is inherently structured and allows for direct reasoning between the claim and the evidence. This facilitates more accurate and consistent fact verification, crucial for applications like dialogue systems.

FACTKG comprises DBpedia as the KG and presents claims in both written and colloquial styles. The dataset includes two labels: SUPPORTED and REFUTED. The task involves retrieving evidence from DBpedia and verifying the claim using reasoning.  Five reasoning types are included: one-hop, conjunction, existence, multi-hop, and negation.

The dataset's structure allows for diverse reasoning scenarios. One-hop claims can be verified by checking direct relationships between entities. Conjunction claims require verifying all individual one-hop claims. Existence claims necessitate checking for the presence of specific relationships. Multi-hop inference is required for claims involving indirect relationships. Negation requires additional inference to determine the validity of the negated claim.

To address the colloquial style, the dataset employs a colloquial style transfer model and presupposition templates.  The dataset statistics are provided, and baseline models are constructed using claim-only verification, the GEAR model (which utilizes graph evidence), and other methods.

The results demonstrate that all baselines outperform the majority class baseline (51%), with the GEAR model, utilizing graph evidence, achieving the best performance.  FACTKG provides a valuable resource for advancing fact verification research and enabling applications that require consistency checks between knowledge and natural language. The dataset is publicly available, and the presenter encourages contact for further inquiries.</sample>
    <sample id="248">```
We find that datasets and models are most aligned to English speaking countries. We also find most additional alignment with people who have a college education. However, when models and datasets are aligned to specific populations, some are inevitably left behind. We find this in the GPT 4 social acceptability task as well as the Dynahate task analysis as well.
```</sample>
    <sample id="249">请在可接受的域中扰乱句子，观察扰动对语言模型可接受性判断的影响。</sample>
    <sample id="250">进行维度评估意味着评估对话模型在多个不同的方面，而不是只关注整体质量。这有助于了解模型的优势和劣势，并更精确地衡量其性能。</sample>
    <sample id="251">University of Science and Technology of China.</sample>
    <sample id="252">U-CREAT is a new approach to Prior Case Retrieval (PCR), a crucial task for legal professionals seeking relevant past precedents. PCR involves retrieving cases cited within a query document, focusing on factual similarity. The research introduces two key contributions: the IL-PCR dataset and the U-CREAT pipeline.

The IL-PCR dataset, the Indian Legal Prior Case Retrieval Dataset, is a benchmark for PCR, containing 7,070 legal cases with a high average number of citations. It offers a more comprehensive test bed compared to existing datasets like COLIEE’21, featuring larger case pools, longer documents, and a richer vocabulary.

The U-CREAT pipeline leverages unsupervised learning and an event-based approach. It extracts events from legal documents using dependency parsing, identifying subject-verb-object triplets. These events are then used to compute an interaction matrix between the query and candidate events, highlighting commonalities. This matrix is used to rank candidate documents.

The research explores various retrieval models, including count-based, transformer-based, and event-based models. Transformer-based models, while showing some promise, underperform compared to baseline methods like BM25, especially with Indian legal text. Event-based models, however, significantly outperform all other methods, achieving higher F1 scores and lower inference times. The Event Filtered Documents model, which filters documents based on matching events, achieves the best performance.

U-CREAT demonstrates superior performance compared to existing approaches, including recent supervised methods. The research concludes that U-CREAT provides a promising framework for advancing PCR, particularly in the Indian legal domain. The paper details the methodology and results for further exploration.</sample>
    <sample id="253">DisorBERT is a research project from Mexico and Spain focused on detecting signs of mental disorders in social media posts. The project aims to leverage the vast amount of online content to identify individuals struggling with conditions like depression, PTSD, and anxiety.

The core idea is to improve the performance of a language model (BERT) in understanding mental health-related language by using a technique called "domain adaptation." This involves taking a general-purpose language model like BERT and fine-tuning it on data specific to social media and mental health.  A key component is "guided masking," which helps the model focus on important words during training, leading to more relevant predictions.

The researchers demonstrate that DisorBERT outperforms BERT in detecting mental disorders, achieving a good balance between precision (correctly identifying those with a disorder) and recall (avoiding false positives).  They use the Beck Depression Inventory (BDI) as a benchmark, showing that DisorBERT is better at identifying words and phrases associated with depression compared to BERT.  

The project utilizes a combination of domain adaptation and guided masking to achieve this.  They analyze the words predicted by both BERT and DisorBERT when masked words are presented, revealing that DisorBERT tends to focus on words related to mental health problems.  Visualizations of attention scores highlight the importance of words like "anxious" and "medication" in identifying depression.

The researchers conclude that their approach is effective and outperforms existing models like MentalBERT. Future work will explore using different lexical resources and incorporating clinical data to further improve accuracy. The ultimate goal is to develop a technology that can provide early warnings about mental health issues and offer supportive evidence.</sample>
    <sample id="254">Sun Qi from Nanjing University of Science and Technology presented research on "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction." The research addresses the challenge of noise in distant supervision (DS) data for document-level relation extraction, which relies on large amounts of human annotations.  DS data often contains false positive labels, leading to incorrect relation extraction.

The proposed framework uses uncertainty estimation to improve label quality. It first trains a pre-denoising DocRE model on both DS and human-annotated data to generate pseudo-labels.  Crucially, it introduces uncertainty estimation to determine the trustworthiness of model predictions, especially for overlapping relations.  The method calculates instance-level uncertainty scores for each relation class, recognizing that the uncertainty distribution varies across classes.  Dynamic class uncertainty thresholds are then used to filter out pseudo-labels with high uncertainty.  Finally, the original DS labels are replaced with pseudo-labels that have lower uncertainty scores.

To further enhance performance, a multi-phase training strategy iteratively re-labels the DS data.  The framework utilizes Monte Carlo dropout to model uncertainty in the pre-denoising DocRE model.  This approach is modified to handle overlapping relations more effectively than previous methods.

Experiments on public datasets demonstrate that the proposed framework outperforms existing baselines.  The key contributions are: a framework for uncertainty-guided label denoising, an instance-level uncertainty estimation method for overlapping relations, a dynamic uncertainty thresholding strategy for long-tail classes, and significant performance gains.  The research aims to mitigate noise in DS data, leading to more accurate document-level relation extraction.</sample>
    <sample id="255">在零次和一次提示的情况下，提示的形式很重要。</sample>
    <sample id="257">The authors evaluated four state-of-the-art chat models.</sample>
    <sample id="258">Chiang Cheng-Han introduces their new work, "Can Large Language Models Be an Alternative to Human Evaluation?". The research explores using large language models (LLMs) to evaluate the quality of text in natural language processing (NLP), aiming to overcome the instability and reproducibility issues of human evaluation.

The core idea is to instruct LLMs with natural language prompts to rate text samples based on attributes like grammar, coherence, likability, and relevance.  The authors highlight that this approach was novel at the time of submission, as prior work primarily relied on human evaluation.

To validate their method, they conducted an experiment evaluating stories generated by GPT-2 and human writers.  English teachers, considered experts in essay scoring, rated the stories based on the same instructions.  The results showed that human raters generally preferred human-written stories, with some larger LLMs (Davinci and ChatGPT) exhibiting a clear preference. Smaller LLMs showed less consistent preference.

The research investigates factors influencing the accuracy and reliability of LLM evaluations, including agreement between LLMs and human raters, variations in instructions, sampling methods, and the benefits/costs compared to human evaluation.  The paper also explores the applicability of LLM evaluation to other NLP tasks.

Chiang Cheng-Han encourages viewers to read the paper or visit their poster at ACL for more details. The work suggests that LLMs can serve as a viable alternative to human evaluation in certain NLP scenarios, offering a potentially more scalable and reproducible method for assessing text quality.</sample>
    <sample id="259">XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations

Yusen Zhang from Penn State University presented their work on XSemPLR, a new unified benchmark for cross-lingual semantic parsing. Semantic parsing aims to translate natural language queries into structured representations like SQL or Lambda Calculus.  The challenge of cross-lingual semantic parsing lies in translating queries across multiple languages into various meaning representations. Existing models often lack coverage for specific languages or meaning representations, creating gaps in the field.

XSemPLR addresses this by providing a comprehensive dataset containing nine datasets across various domains, five semantic parsing tasks, eight meaning representations, and 22 natural languages spanning 15 language families.  The benchmark includes six evaluation settings: Translate-Test (using Google Translate API), Monolingual Model (same language for source and target), Monolingual Few-shot (limited training data), Multilingual Model (trained on all languages), Cross-lingual Zero-shot transfer (trained on one language, tested on another), and Cross-lingual Few-shot transfer (trained on one language, tested on another with limited data).

The study evaluates two model types: Encoder-PTR (like XLM-R + PTR and mBERT + PTR) and Encoder-Decoder (like mBART and mT5).  Encoder-Decoder models consistently achieve the best performance across all datasets.  Training on a mixture of languages improves performance, although English performance can suffer in some cases, highlighting the "Curse of Multilinguality."

The research also investigates the performance gap between zero-shot and few-shot cross-lingual transfer.  Zero-shot transfer shows a significant performance gap compared to few-shot transfer, where the gap is rapidly reduced.  Furthermore, the study finds that pretraining on English improves few-shot performance on target languages, and that large multilingual models like Codex and BLOOM are not yet sufficient for cross-lingual semantic parsing.

In conclusion, XSemPLR provides a valuable resource for researchers to evaluate and advance cross-lingual semantic parsing. The study highlights the importance of multilingual training and the limitations of current multilingual models, paving the way for more robust and effective cross-lingual semantic parsing systems. The paper and code are available for further exploration.</sample>
    <sample id="260">The provided text does not mention the number of authors. It only names one author, Jingwei Yi.</sample>
    <sample id="261">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="262">The provided text does not mention the number of authors. It only identifies Siyu Yuan from Fudan University as the presenter.</sample>
    <sample id="263">This work addresses instability and bias in in-context learning, a popular method for utilizing large language models. The authors argue that existing research hasn't systematically categorized bias types, hindering mitigation efforts. They propose a typology of label biases, identifying "domain-label bias" as a new, significant issue.

The study investigates three types of label biases: vanilla-label bias (uncontextual preference), context-label bias (effects from context), and domain-label bias (task corpus influence). Experiments reveal that random in-domain words can significantly bias model predictions, especially on tasks with high domain-label bias, where performance can drop below chance levels even with calibration.

To tackle these biases, the authors introduce "domain-context calibration." This method uses random in-domain words as content-free text to estimate bias and then calibrates the model's predictions accordingly.  They contrast this with previous calibration methods that rely on single predefined tokens.

The research demonstrates that domain-context calibration significantly improves in-context learning performance across various datasets, particularly on tasks with substantial domain-label bias.  Furthermore, it leads to better decision boundaries in the model's predictions.  Calibration studies reveal that single tokens are insufficient, and using more random words yields further improvements.  Using random in-domain words is crucial for addressing domain-label bias.

The authors conclude that their work provides a systematic approach to understanding and mitigating label biases in in-context learning, offering a more effective calibration method for large language models. They encourage readers to consult their paper for further details.</sample>
    <sample id="264">Lin Wang 介绍了论文“TAVT：可转移的音频-视觉文本生成”的研究。当前，单模态文本生成任务发展迅速，但音频-视觉文本生成数据标注成本高昂，且现有方法在不同领域存在性能下降问题。

TAVT 提出了一种新的任务——可转移的音频-视觉文本生成，主要挑战在于跨模态领域差异，如视觉风格、音频能量等。研究发现，相同事件的视觉内容受风格和角度变化影响大，而音频内容变化影响较小。因此，他们提出利用统一的音频语义空间来连接视觉概念。

TAVT 的框架包含三个模块：音频-视觉元映射网络、音频-视觉编码器和语言模型生成器，以及对抗学习。元映射网络将不同视觉概念映射到统一的音频语义空间，并引入可学习的视觉前缀。编码器和生成器使用Transformer，并引入了alpha来评估不同模态对每个单词的贡献。

为了优化视觉-音频对齐，TAVT 提出了一种双向对抗对比学习（DCLL）方法，通过对抗结果构建精细的监督信号。采用类似MAML的元学习方法进行元训练，并进行元测试。

实验结果表明，TAVT 在MSVD和MSR-VTT两个基准测试中，优于所有对比模型，尤其是在低资源领域表现出色。此外，消融实验表明音频特征对性能有重要影响。

总结来说，TAVT 提出了一种可转移的音频-视觉文本生成方法，通过统一音频语义空间和对抗学习，解决了跨模态领域差异和低资源问题，并在实验中取得了显著成果。</sample>
    <sample id="265">Vasudha.</sample>
    <sample id="266">This text does not mention the author's affiliated institution.</sample>
    <sample id="268">PaLM 最常见的错误是遗漏错误（omission errors）。</sample>
    <sample id="269">大家好，我是詹姆斯·芬奇，我是莎拉·芬奇。今天我们将向大家介绍ABC-Eval，一种新的维度方法来评估对话式人工智能。这项工作由埃莫里NLP实验室的Jinho Choi教授领导，与亚马逊Alexa AI合作完成。那么，假设你已经开发了一个对话模型，想知道它与当前最先进的模型相比如何。常见的做法是使用人工评估，例如让人工评判员选择哪个对话更好，或者根据Likert量表对对话进行评分。这些方法在提供对整体对话质量的全面评估方面效果良好，但对话质量有很多方面。因此，你可能想评估对话质量的多个维度，以便更细致地了解模型的优势和劣势。一种方法是让人工评判员评估对话质量的多个维度，例如模型回复的相关性，使用现有的比较或Likert量表方法。然而，我们认为有一种更精确和可靠的策略来评估维度对话。我们的方法尝试减少人工评估的主观性，通过明确标注模型回复是否表达了某些行为，例如回复不相关信息或自相矛盾。我们称这种方法为标注聊天行为，简称ABC-Eval。我们开发这种方法是为了全面覆盖近年来被认为会影响聊天质量的聊天模型行为。ABC-Eval能够衡量聊天模型在各种主题错误方面的发生率。例如，ABC-Eval衡量聊天模型在对话中忽略对话伙伴、说不相关内容、与自己或对话伙伴自相矛盾、产生虚假事实或违反常识知识以及在展示同理心方面取得成功或失败的次数。为了确定哪种评估方法最有效，我们选择了四个最先进的聊天模型，并使用ABC-Eval对每个模型进行了100个人工与机器人对话的评估。为了比较，我们还使用三种现有的方法对这些对话进行了评估：对每个回合进行Likert评分、对整个对话进行Likert评分以及对话级别的pairwise比较。对于每种现有方法，我们收集了八种最常用的对话质量方面的评估。根据我们对这些评估结果的分析，ABC-Eval行为标签比现有方法收集的标签更可靠，这通过100个双标记对话上的互信度来衡量。此外，ABC-Eval标签对整体对话质量的预测能力也比现有方法指标更高，正如我们所展示的这个简单的线性回归分析所示。例如，衡量回合中自我与对话伙伴矛盾的比例可以解释5%和10%的对话质量，而平均Likert一致性分数仅能解释4%或更少。最后，我们检查了每个评估指标是否捕捉了对话质量的独特方面，通过逐步线性回归。你可以看到，所有ABC-Eval指标的组合可以解释超过25%的对话质量，而当移除一个指标时，大多数指标都会损失相当多的信息。另一方面，所有回合级别Likert指标的组合解释的质量远少于，并且较少的指标携带独特的 معلومات. 这些可靠、信息量大且独特的ABC-Eval指标使我们能够以比以前方法更高的分辨率评估对话式人工智能。正如我们实验结果所示，我们还发现仍然存在一些挑战，并且这些挑战已被精确量化。例如，我们测试的机器人模型在他们的回复中存在常识性错误约20％的次数，产生不相关信息约15％的次数，并且在对话中自相矛盾或与对话伙伴矛盾约10％的次数。随着领域快速进步，未来发布的新模型中的许多错误率可能会下降。然而，这正是我们追求可靠和精确评估指标的原因。我们希望ABC-Eval可以被其他人用于该领域，作为这一方向上的一个有意义的步骤。我们期待着看到对话式人工智能在未来几个月和几年中的发展。谢谢观看。</sample>
    <sample id="270">Emory NLP Lab, led by Professor Jinho Choi at Emory University, in collaboration with Amazon Alexa AI.</sample>
    <sample id="271">FTw</sample>
    <sample id="272">John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams.</sample>
    <sample id="273">Kayo Yin 老师今天将介绍我们的工作，题目是“何时需要上下文进行翻译？一个数据驱动、多语言探索”。这项工作与 Patrick Fernandes、Emmy Liu、André F. T. Martins 和 Graham Neubig 合作完成。因此，很多翻译都依赖于上下文。例如，如果我们说“mole”这个词，在第一个句子是“如果部长们发现事情开始变得危险”，那么“mole”指的是间谍。但如果第一个句子是“医生，这到底有多严重？”那么“mole”指的是胎记。根据上下文，单词的意思会发生变化，因此翻译也会发生变化。然而，评估模型在处理这些情况方面的能力非常困难。首先，只有少量翻译依赖于上下文，这使得基于语料库的指标，如 BLEU，无法捕捉到这些翻译。有些人建议对上下文依赖的翻译进行有针对性的评估，但这些资源仅支持有限类型的上下文依赖翻译，并且仅限于有限的语言对，因为它们通常依赖于领域知识和人工整理。在这项工作中，我们试图回答这两个问题：首先，何时需要上下文进行翻译？其次，模型如何处理这些情况？为了回答第一个问题，我们首先测量单词在翻译过程中依赖上下文的程度。在之前的项目中，我们引入了 CXMI 作为衡量机器翻译模型上下文使用的指标。这通过测量给定源文本 X 和目标文本 Y 时，上下文 C 为目标文本 Y 提供的信息量来衡量。可以认为 CXMI 是模型获得上下文信息的一种方式。在这项工作中，我们扩展了 CXMI 为 Pointwise CXMI，它可以衡量上下文在句子级别或单词级别上的使用情况。我们可以认为那些 P-CXMI 值高的单词，在翻译过程中需要上下文。然后，我们分析 P-CXMI 值高的单词，以寻找这些单词之间的模式。我们对来自 TED 演讲的 14 种不同语言的翻译文本进行分析。我们分别在三个不同的级别上进行分析。首先，我们查看具有高平均 P-CXMI 的词性标签。这可以帮助我们找到，例如，阿拉伯语中具有相对较高 P-CXMI 的双重代词，因为英语没有双重代词，因此需要上下文来确定代词是否为双重，从而进行翻译。类似地，我们发现某些语言在选择适当的动词形式时也需要上下文。然后，我们查看在所有不同出现中平均具有高 P-CXMI 的词汇项。这有助于我们识别出，例如，中文在翻译专有名词时需要上下文，以确保在文档中使用相同的翻译。类似地，我们发现上下文对于翻译正确的礼貌程度也很重要。最后，我们查看不同的单个标记，这些标记具有高 P-CXMI。这允许我们识别无法通过单词本身捕捉到的现象，而是在句子结构中表达的现象，例如省略句的解析。现在，我们使用我们的发现来设计一个用于文档级别翻译的基准。对于我们识别出的五个不同的语料库现象，我们创建了标签器来自动识别涉及这些现象的单词。我们称我们的标签器为 Multilingual Discourse-Aware (MuDA) 标签器。我们还可以注意到，不同语言有不同的语料库现象比例。然后，我们使用 MuDA 标签器，通过在用于评估的平行语料库上应用标签器，并使用我们选择的翻译指标来评估上下文依赖的例子，最后，我们使用我们的基准以及其他指标来评估不同的模型在文档级别机器翻译方面的表现。首先，当我们使用基于语料库的指标时，例如 BLEU，上下文无关的模型表现最好。但是，如果我们使用 COMET，上下文感知模型表现最好。如果我们使用 word f-measure，那么模型和没有上下文的模型在性能上相当。这再次表明，如果我们仅使用基于语料库的指标，很难确定最佳的文档级别翻译系统。现在，我们使用 MuDA 基准来评估模型，我们发现上下文感知模型在处理某些不同的语料库现象方面比不使用上下文的模型更准确，例如礼貌和词汇连贯性。但是，这些模型在处理其他现象方面，如代词、隐喻和动词形式方面，并没有显著优势。这表明我们还需要在文档级别翻译方面取得更多进展。我们还比较了不同的商业系统，我们的基准显示 DeepL 通常比 Google 翻译在文档级别翻译方面更准确。总而言之，我们通过对 14 种不同语言对的数据驱动分析，识别了何时需要上下文进行翻译，然后我们使用我们的发现构建了一个用于文档级别机器翻译的基准，它可以帮助我们识别模型在处理哪些不同的语料库现象方面表现良好或不佳，以及哪些翻译系统在文档级别翻译方面表现出色。谢谢您的聆听。我们下次在多伦多见。</sample>
    <sample id="274">Yusen Zhang.</sample>
    <sample id="276">Ananya and Vignesh's work addresses the understudied area of machine translation metric evaluation for Indian languages, focusing on a dataset called "IndicMT Eval."  They aim to fill the gap in evaluating translation quality beyond English, considering the unique linguistic characteristics of Indian languages.

The study utilizes the Flores dataset, randomly selecting 200 sentences from Tamil, Malayalam, Hindi, Marathi, and Gujarati.  These sentences are translated into English by seven different translation models/APIs, generating 1,400 candidate translations per language, resulting in a total of 7,000 samples.  Human expert annotators evaluate these translations, marking errors with type and severity, and providing overall scores, using the MQM error framework.

The research analyzes the correlation between various machine translation metrics (like BLEU, chrF, BERTscore, and COMET) and human scores.  They find that overlap-based metrics like chrF correlate best across languages, but perform poorly overall. Embedding-based metrics like LabSE and BERTscore show better correlations, especially when using multilingual models. COMET-metric variants demonstrate the highest overall correlations. However, many metrics exhibit skewed score ranges, making interpretation challenging.  Accuracy errors show a stronger correlation with human scores than fluency errors.

To improve metric performance, they fine-tune the COMET metric using their MQM dataset, creating "IndicCOMET" variants.  These variants outperform COMET baselines on three out of five languages and show higher correlations across all languages.  Furthermore, IndicCOMET demonstrates zero-shot translation capabilities on unseen languages and exhibits better robustness compared to COMET on the ACES Translation Accuracy Challenge Sets.

The authors make the IndicMT Eval dataset publicly available, encouraging further research in evaluating machine translation for Indian languages.  The study highlights the need for more nuanced and robust evaluation metrics tailored to the specific characteristics of different languages.</sample>
    <sample id="277">该方法没有名称。</sample>
    <sample id="278">“显性词汇”(marked words) 方法通过分析 LLM 生成的文本，识别与特定群体相关的词语，并比较这些词语与 LLM 对其他群体的描述。该方法基于“标记性”的概念，即某些群体因与默认群体不同而被标记。通过比较不同群体的词语，可以揭示 LLM 中存在的潜在偏见和刻板印象。</sample>
    <sample id="279">University of Washington.</sample>
    <sample id="280">Shi Tao's work, "MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations," addresses the challenges in emotion recognition (ERC) from conversations involving text, audio, and visuals. Existing methods often lack effective multimodal fusion, struggle with minority emotions, and fail to distinguish subtle emotional differences.

The core of MultiEMO is a novel attention-based framework. It comprises four key components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification.  A key innovation is VisExtNet, a visual feature extractor that focuses on facial expressions of speakers, avoiding redundant scene information.  MultiAttn, the multimodal fusion model, uses bidirectional multi-head cross-attention layers to integrate textual, audio, and visual features.  It leverages cross-modal correlations to combine information from different modalities.

To improve performance on challenging cases, MultiEMO introduces a Sample-Weighted Focal Contrastive Loss. This loss prioritizes minority emotion classes and aims to differentiate semantically similar emotions by maximizing inter-class distances.

Experiments on MELD and IEMOCAP demonstrate that MultiEMO achieves state-of-the-art results, particularly in handling minority and subtly expressed emotions.  The framework's ability to effectively integrate information from multiple modalities leads to improved accuracy.

However, the work acknowledges limitations. VisExtNet doesn't differentiate between speakers and background individuals. The Sample-Weighted Focal Contrastive Loss requires a large batch size for MELD.  Furthermore, the performance on minority emotions is still not as strong as on majority classes.  The research aims to overcome these limitations in future work.</sample>
    <sample id="281">Kayo Yin and colleagues investigated when and how machine translation models handle context-dependent translations. Their work, "When Does Translation Require Context? A Data-driven, Multilingual Exploration," addresses the challenge of evaluating translation quality when meaning depends on surrounding text, a problem that traditional metrics like BLEU struggle to capture.

The researchers extended CXMI, a measure of context usage, to Pointwise CXMI, analyzing words with high context dependence across 14 language pairs and 14 TED talks. They identified patterns in part-of-speech tags, vocabulary items, and individual tokens, revealing that context is crucial for translating dual pronouns, verb forms, and proper nouns, as well as resolving ellipses.

To create a more accurate evaluation tool, they developed the Multilingual Discourse-Aware (MuDA) tagger, which automatically identifies discourse phenomena like formality and lexical cohesion. They then used this tagger to evaluate machine translation models on a parallel corpus, finding that context-aware models significantly outperform those without context on phenomena like formality and lexical cohesion. However, performance on other phenomena like ellipsis and pronouns was comparable.

The study also compared commercial translation systems, finding DeepL generally outperforms Google Translate in document-level translation.  The authors conclude that while context-aware models show promise, further progress is needed to improve performance on all discourse phenomena.  The MuDA benchmark provides a valuable tool for identifying strengths and weaknesses of different translation systems in document-level translation.</sample>
    <sample id="282">Xuekai Zhu's ACL 2023 paper introduces StoryTrans, a new model for non-parallel story author-style transfer.  The research addresses the challenge of transferring writing styles at the story level, going beyond sentence-level or token-level approaches.  It highlights the difficulty in imitating author-specific linguistic preferences, particularly discourse structures and topic-dependent styles.

StoryTrans tackles these issues by learning discourse representations from source texts and combining them with learnable style embeddings. A novel training objective is designed to reduce stylistic features from discourse representations, improving content preservation. The generation process is split into two stages: first, masking style-specific content keywords in the source text, and then generating the complete text by incorporating these keywords.

The training framework employs self-reconstruction, disentanglement, sentence order, and style classifier losses in the first stage. The second stage focuses on filling in style-specific content and removing the mask.  Experiments on new datasets in Chinese and English demonstrate StoryTrans's effectiveness in transferring styles while preserving content.  Automatic and manual evaluations show superior style control and content preservation compared to baselines. Style visualization confirms alignment with the golden text's style features.  StoryTrans also demonstrates the ability to enrich storylines with relevant phrases and rewrite sentences while maintaining semantic meaning. The paper includes data and code for reproducibility.</sample>
    <sample id="283">Dependency Structure of Coordination.</sample>
    <sample id="284">Peng Tianshuo from Wuhan University presented "FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction" at ACL Main Conference 4,915. The paper addresses the limitations of current span-based Universal Information Extraction (UIE) models, which heavily rely on precise span boundaries, often ambiguous in annotation. The proposed solution introduces a "fuzzy span" mechanism, representing span boundaries as continuous probability distributions rather than discrete points.

FSUIE tackles the mismatch between transformer feature extraction and information extraction by employing adaptive attention. Instead of static attention, the attention mechanism dynamically adjusts its span length using an optimizable parameter, and the attention distribution decays linearly rather than truncating. This allows the model to capture broader context and model the continuous nature of span boundaries.

The core of FSUIE is the Fuzzy Span Loss (FSL) and Fuzzy Span Attention (FSA). FSL incorporates a fuzzy span boundary into the loss function, while FSA dynamically adjusts the attention span. The combined effect of FSL and FSA leads to significant performance improvements across three information extraction tasks: Named Entity Recognition (NER), Relationship Extraction (RE), and Aspect Sentiment Triplet Extraction (ASTE).

In NER, FSUIE-base achieved significant gains over UIE-base, particularly on small datasets.  FSUIE achieved state-of-the-art results in RE on ACE2004, 2005, and ADE datasets, demonstrating its ability to extract relationships effectively.  Furthermore, FSUIE achieved state-of-the-art results on AST-V2 and 14res datasets in ASTE tasks. Ablation studies confirmed that FSA improves convergence speed and FSL enhances information extraction capability when combined.

Visualizations of the attention distribution revealed that the module focused on semantic information within a limited range of preceding tokens, aligning with the intended behavior.  FSUIE demonstrates strong generalization capabilities and improved performance on domain-specific information.  The paper concludes that FSUIE's fuzzy span mechanism and adaptive attention significantly enhance UIE performance, leading to excellent results across various information extraction tasks.</sample>
    <sample id="285">This presentation by Mingqi Gao from Peking University introduces their work on benchmarking factual error correction for dialogue summarization. The core problem is that summaries generated by models often contain factual errors, leading to the need for solutions like incorporating factuality objectives during training or using a separate Factual Error Correction (FEC) model.

The researchers argue that current evaluation methods for FEC models are flawed. Existing metrics like FactCC and DAE provide overall scores, which are vague and potentially unreliable. Furthermore, they don't adequately distinguish between true error correction and simply generating a different, factually correct summary without addressing the original summary's content.

To address these issues, the authors propose introducing manually annotated reference corrections. This provides more valuable data for training FEC models compared to pseudo-data and enables a more comprehensive evaluation. They also propose a new taxonomy of factual errors, categorizing them as content-based (based on part-of-speech and dependencies) and form-based (based on addition, deletion, and substitution).

Their evaluation framework builds upon ERRANT, consisting of alignment, classification, and comparison steps. Experiments with FEC models in different training modes reveal that training with reference summaries from dialogue datasets yields the best results when using unreliable factuality metrics.

Key findings include the urgent need to change FEC evaluation methods, the improvement achieved by incorporating human-corrected summaries during training, and the promise of combining human-annotated and synthetic data.  The study also highlights that current FEC models struggle with specific error types like addition and attribute/modality/link errors.  The overall message is that a more nuanced and human-centric evaluation approach is crucial for advancing factual error correction in dialogue summarization.</sample>
    <sample id="286">James Finch and Sarah Finch.</sample>
    <sample id="287">This paper has four authors: Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="288">MPP evaluation can be done using datasets like BLiMP and SyntaxGym.</sample>
    <sample id="290">COSINE, FTw, and others.</sample>
    <sample id="291">该模型在命名实体识别、分类、词性标注和问答等公共和私有下游任务上进行了评估。</sample>
    <sample id="294">CamemBERT 最初是在 NACHOS 数据集上训练的。</sample>
    <sample id="295">Adam Przepiórkowski.</sample>
    <sample id="296">Valerio Basile and the University of Turin, in collaboration with Amazon Alexa, explored irony detection using a new dataset called EPIC (English Perspectivist Irony Corpus).  They aimed to move beyond simple binary (ironic/not ironic) classifications to more informative outputs.

EPIC comprises 300 short conversations collected from social media (Reddit, Twitter) over 1.5 years, annotated by 74 annotators across five English varieties.  Annotators, selected from Prolific, rated whether a reply was ironic in relation to the context.  They used a simple interface with a "Ironic" or "Not ironic" option.

The study found that inter-annotator agreement varied significantly depending on annotator characteristics like age, gender, and nationality.  To address this, they developed "perspective-aware models" – models trained on subsets of the data corresponding to different annotator groups.  These models were compared to "gold standard aggregated models."

The key finding was that perspective-aware models exhibited significantly higher confidence in their predictions compared to the aggregated models.  This suggests that individual annotator perspectives contribute to the nuances of irony detection.

Further analysis revealed that generational differences and geographical distribution of annotators were correlated with variations in annotation agreement.  Specifically, annotators from the UK and Ireland showed the most disagreement.

The research highlights the limitations of relying solely on aggregated annotations and the potential benefits of incorporating individual perspectives to improve irony detection models.  The team plans to further investigate the causes of these perspective-related differences.</sample>
    <sample id="297">This research project, "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models," investigates the use of dogwhistles – coded language that conveys a hidden message to an in-group while appearing innocuous to an out-group – in political discourse. The study aims to understand how dogwhistles function, how they are recognized, and how they can evade content moderation.

The project begins with the development of a comprehensive typology and glossary of over 340 dogwhistle terms and symbols, categorized by register (formal/informal) and type (implicature vs. covert persona signaling). This glossary incorporates contextual information and real-world examples, focusing on terms related to racism, transphobia, and anti-Semitism.

A case study of historical U.S. political speeches reveals a correlation between the frequency of racial dogwhistles and the Republican Southern Strategy, highlighting the historical use of coded language to avoid explicit racism. The study also notes a trend towards increased use of dogwhistles associated with conservatism over time.

The research then evaluates the ability of language models, specifically GPT-3, to recognize dogwhistles. Experiments show that GPT-3 can identify many dogwhistles, particularly those in formal registers, but performs poorly with informal social media language and transphobic dogwhistles. Prompting strategies, such as providing definitions and secret cues, significantly improve performance.

Finally, the project examines how dogwhistles can evade content moderation.  The study demonstrates that replacing standard group labels or slurs with dogwhistles can reduce the perceived toxicity of hateful sentences, suggesting a challenge for automated toxicity detection systems.

In conclusion, the project provides a deeper understanding of dogwhistles, their historical context, and their impact on political discourse and online communication. It highlights the difficulty of detecting and mitigating dogwhistles, emphasizing the need for improved NLP techniques and content moderation strategies.</sample>
    <sample id="298">通过实验发现，性能下降的主要原因是**时间漂移**，即训练和测试数据之间的时间差距越来越大。</sample>
    <sample id="299">This presentation introduces a novel training method to improve the robustness of Natural Language Inference (NLI) models, addressing the issue of shortcut reliance. NLI models often achieve high accuracy on standard benchmarks due to learning spurious correlations (shortcuts) between input attributes and labels, which break down when faced with out-of-distribution data.

Existing shortcut mitigation methods often require auxiliary models with domain-specific knowledge or computational overhead, and may not effectively guide the learner to avoid shortcuts. The proposed method tackles these limitations by focusing on "hard" training instances – those that contradict the dominant "easy" examples and are under-represented in the training data.

The core idea is to use a minimax training objective between a learner (the NLI model) and an auxiliary model. The learner aims to minimize the NLI loss, while the auxiliary model tries to maximize it by generating example weights that incentivize the learner to focus on hard examples. This approach encourages the learner to learn from instances that counteract shortcuts, leading to improved out-of-distribution performance.

The method is designed to be dataset-agnostic, relying on the learner's training dynamics to generate example weights. A feed-forward network is used to model the auxiliary.  Experiments on MNLI, FEVER, and QQP, along with adversarial test sets, demonstrate that the minimax training objective consistently improves out-of-distribution accuracy while maintaining high in-distribution accuracy, outperforming standard ERM training and existing shortcut mitigation techniques.

The presentation also explores the effect of pre-training the learner, the optimal size of the auxiliary model, and the characteristics of the learned example weight distribution. The researchers invite questions during the poster session.</sample>
    <sample id="300">Semantic Machines 团队与 Jason Eisner、Adam Pauls 和 Sam Thomson 合作，提出了名为“交互式拼写”的新任务，旨在解决语音输入和编辑文档的自然交互问题。与现有的语音识别系统不同，交互式拼写允许用户在语音输入过程中进行编辑，例如纠正错误或执行命令。

该任务的关键特点是：1. 允许用户灵活地在语音输入和编辑之间切换，无需触发词；2. 使用自然、开放式的语言来指定编辑。

研究人员首先定义了交互式拼写任务的四个步骤：1. 语音识别（ASR）将音频转换为文本；2. 将文本分割为语音输入和命令；3. 提取和标准化命令，并修复 ASR 错误；4. 按照顺序执行语音输入和命令，最终得到文档状态。

为了构建该任务，研究人员设计了一个新的数据收集界面，并创建了一个数据集。他们还构建了一个基线系统，该系统分别训练了四个步骤的模型：分割模型、ASR 修复模型和解释模型。

实验结果表明，GPT-3 模型在准确性上优于 T5 模型，但速度较慢。对于 GPT-3，直接预测最终状态比预测中间程序更准确。T5 模型在效率和准确性之间取得了更好的平衡。

研究人员认为，在交互式拼写任务中仍有很大的改进空间，并公开了代码，欢迎更多研究者参与到该领域的工作中。</sample>
    <sample id="302">The output tokens are not ordered in the first step, so a second model is used to predict a permutation to put them in the right order.</sample>
    <sample id="303">作者建议模型所有者提高偏见缓解方法的透明度，因为：

*   我们不知道偏见是由于模型训练中存在某种过度价值对齐，还是由于其他反刻板印象的方法导致了这些有害模式。
*   缺乏透明度使得我们无法进一步研究这些偏见，也无法做出关于如何解决这些问题的明智决策。</sample>
    <sample id="304">最小对不可接受输入是一种评估语言模型是否能正确判断句子是否可接受的方法。它通过展示一个可接受的句子和一个不可接受的句子，然后观察模型对哪个句子的概率更高。</sample>
    <sample id="305">This video presents the research findings of Dawei and his colleagues on "Weaker Than You Think: A Critical Look at Weakly Supervised Learning." The work challenges the common claim that weakly supervised learning (WSL) methods achieve high performance solely on weakly labeled data, often relying on an additional clean validation set for model selection.

The research investigates whether clean validation data is truly necessary for WSL, how many clean samples are required, and whether using clean samples for validation is the optimal approach. The findings reveal that recent WSL methods *require* clean validation samples to generalize well, otherwise performance drops significantly.  Increasing the number of clean validation samples improves performance, with even direct fine-tuning on clean data outperforming WSL approaches in some cases.

The study demonstrates that the performance gains often attributed to WSL are easily achievable by allowing models to continue fine-tuning on the clean validation samples.  Vanilla models like FTw can perform comparably to more complex WSL methods when fine-tuned on clean data.  

The authors conclude that the practicality and performance benefits of many WSL approaches are overestimated. They recommend that future research should: 1) explicitly report model selection criteria (including the use of clean validation sets), 2) compare WSL with few-shot learning baselines (which also utilize clean samples), and 3) consider continuous fine-tuning as a strong baseline.  The code for the research is open-sourced.</sample>
    <sample id="306">Sebastian Schuster and Najoung Kim's research explores the ability of large language models (LLMs) to track entities and their states in discourse. They argue that this is a crucial skill for understanding longer texts, but current LLMs haven't been systematically evaluated on this task. The research addresses challenges in designing such a task, including the potential for models to rely on common entity associations or simple word-based heuristics learned during pre-training.

Their task involves predicting the contents of boxes after a series of state-changing operations. The models are given an initial description of the boxes and must predict the contents based on the operations performed.  They tested Flan-T5 and GPT-3/3.5 models using 2-shot in-context learning.  The results show that most models simply repeat the initial state, indicating a lack of true entity tracking.  Only text-davinci-003 exhibits non-trivial tracking.

Further analysis reveals that models trained on code demonstrate better entity tracking capabilities than those trained primarily on text.  Fine-tuning smaller models like T5-base can enable entity tracking, but randomly initialized models struggle even with direct supervision. The researchers conclude that pre-training on code is a key factor in developing this ability.  They acknowledge the limited generalizability of their findings and invite further discussion. The paper is available on arXiv.</sample>
    <sample id="307">作者使用了以下评估指标：命名实体识别、分类、词性标注、问答。</sample>
    <sample id="308">Jenny, a PhD student at Carnegie Mellon University, presents "NLPositionality: Characterising Design Biases of Datasets and Models," a collaborative work with the University of Washington and the Allen Institute for AI. The presentation addresses the issue of design biases in NLP, exemplified by the inconsistent performance of toxicity detection APIs across different populations.

The core concept is "positionality," referring to the perspectives shaped by demographics, identity, and life experiences, and how these can influence NLP research. The study investigates whether datasets and models possess positionality by comparing annotations from diverse users with existing datasets and models.

The NLPositionality framework involves re-annotating datasets with a large, diverse group of annotators and then comparing their annotations to models and datasets using Pearson's R correlation. This differs from traditional annotator disagreement studies by focusing on end-user predictions and labels.  The research leverages Lab in the Wild, an online platform for recruiting diverse volunteers.

The study analyzed social acceptability and hate speech detection tasks using datasets like Social Chemistry, Delphi, GPT-4, Dynahate, Perspective API, Rewire API, and Hate Roberta.  Over 16,000 annotations from 1000+ annotators across 87 countries were collected.

Key findings reveal positionality in NLP, with datasets and models showing higher alignment with English-speaking countries and individuals with college education. However, the study also highlights disparities, with models and datasets being less aligned with non-binary individuals.

The presentation concludes with recommendations for mitigating these biases: documenting design choices, adopting a perspectivist approach to research, and building specialized datasets and models for specific communities, referencing the Masakhani initiative. The work emphasizes that inclusive NLP goes beyond simply making technologies accessible to all.</sample>
    <sample id="309">Inter-annotator agreement.</sample>
    <sample id="310">选择来自维基百科的句子。</sample>
    <sample id="311">The provided text does not mention the authors' affiliated institution.</sample>
    <sample id="312">MultiInstruct 是第一个大规模多模态指令微调数据集，包含 62 种不同的多模态任务，涵盖 10 个广泛的类别。它从 21 个现有开源数据集派生而来，每个任务都配备了五种由专家撰写的指令。与以往的指令微调工作不同，MultiInstruct 专注于多模态任务，而之前的研究主要集中在语言任务上。此外，MultiInstruct 解决了 NLP 和多模态任务指令数据集可用性的差距，提供了大量高质量的多模态指令数据。</sample>
    <sample id="313">这篇论文由 Jinho Choi 教授领导的 Emory NLP Lab 和 Amazon Alexa AI 合作完成。虽然论文中提到了 Jinho Choi 教授，但没有明确列出所有作者。</sample>
    <sample id="314">我无法从提供的文本中提取关于“二进制协调”的定义。文本主要讨论的是协调结构中的依赖结构，以及对称与非对称结构之间的关系，并使用依赖长度最小化原则进行论证。</sample>
    <sample id="315">This paper doesn't specify the average length of the prompts used. It focuses on the *type* of prompts – "Imagine you are an Asian woman. Describe yourself." – rather than a specific length.</sample>
    <sample id="316">The findings suggest that smaller, specialized models like T5 can outperform larger language models when trained on a suitable dataset like CoScript. This is because CoScript allows for training on specific, constrained language planning tasks, enabling smaller models to achieve higher quality scripts than larger models alone.</sample>
    <sample id="317">Peng Li from Fudan University presents "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors." The presentation addresses the challenge of mismatched outputs in traditional information extraction models (like T5 and GPT-3) that operate in text-to-text fashion. These models learn to reformat input text into a structured output during inference, but the output itself isn't learned during pre-training, leading to difficulties in generating correct structured information.

CodeIE proposes transforming information extraction into a structure-to-structure code generation task using code-based large language models like Codex. This allows for direct conversion of text to structured output, aligning the input and output formats. The presentation details a prompt design for Named Entity Recognition (NER) and Relation Extraction (RE), leveraging few-shot learning with code.

Experiments on NER and RE datasets using T5, UIE, GPT-3, and Codex demonstrate that CodeIE significantly outperforms traditional baselines, especially with few-shot learning.  The analysis reveals that code-based approaches better align with the information extraction task, leading to lower perplexity and fewer structural errors compared to text-based models.  Codex consistently outperforms GPT-3 in overall performance, particularly in recall.  The presentation highlights that code-based prompts are superior to text-based prompts, especially for recall. The paper and code are publicly available. The goal is to provide inspiration for future research in information extraction.</sample>
    <sample id="318">Yanis Labrak 先生将介绍他们关于“DrBERT：用于生物医学和临床领域的鲁棒预训练模型”的研究成果。本次演示首先讨论了医疗领域的语言建模。然后，他们将介绍论文的主要贡献，即引入了第一个法语生物医学模型 DrBERT，该模型基于 RoBERTa 并使用 NACHOS 数据集进行训练，NACHOS 是一个从网络上收集的医疗数据集合。此外，他们还对比了使用不同预训练设置和数据来源的模型。最后，他们将展示 DrBERT 在 11 个法语生物医学和临床下游任务上的结果，并总结实验结果，同时提供模型访问方式的详细信息。

自 2018 年以来，BERT 已经成为解决自然语言处理任务的最有效方法之一，相比于 Word2vec、fastText 等传统方法，它带来了巨大的性能提升。随后，BERT 被应用于许多其他语言，如 CamemBERT（法语），以及生物医学领域（PubMedBERT、BioBERT）和临床领域（ClinicalBERT），但主要集中在英语上。针对其他语言的专业模型相对稀缺，通常依赖于持续预训练，因为缺乏特定领域的训练数据。然而，法语在生物医学领域缺乏开源模型。因此，他们提出了一个问题：最合适的训练数据来源是什么，而从网络上收集的已用数据可以作为临床数据的合理替代品吗？

为了回答这个问题，他们将 DrBERT 与他们开发的 ChuBERT 模型进行比较，ChuBERT 是基于 Nantes 大学医院数据仓库中匿名数据的临床模型。他们还探讨了训练一个专业模型需要多少数据？是 4GB、8GB 还是更多？为了回答这个问题，他们首先训练并比较了四个从零开始的模型：DrBERT 的第一个版本，使用 7GB 的 NACHOS 数据；第二个版本，使用 4GB 的 NACHOS 数据；ChuBERT 的第一个版本，使用 4GB 的临床笔记数据；以及 ChuBERT 的最后一个版本，使用 4GB 的 NACHOS 数据和 4GB 的临床笔记数据混合。

此外，他们还引入了三个基于持续预训练的模型，以分析预训练策略的影响。第一个基于 CamemBERT 的权重，使用 4GB 的 NACHOS 数据进行训练。第二个也基于 CamemBERT，但使用 4GB 的临床笔记数据进行训练。第三个基于英语生物医学模型 PubMedBERT，使用 4GB 的 NACHOS 数据进行训练。总共有七种模型。为了评估这些模型，他们收集了公共和私有下游任务的数据，如命名实体识别、分类、词性标注和问答。这些模型与六个基线模型进行了比较：CamemBERT OSCAR 138GB、CamemBERT OSCAR 4GB、CamemBERT CCNET 4GB、PubMedBERT、BioBERT 和 ClinicalBERT。评估结果表明，模型在数据类型与训练数据相同的情况下表现最佳。然而，数据来源多样化的模型表现更具通用性。他们还观察到，使用更多数据可以提高性能。总体而言，从零开始的预训练在大多数任务上表现更好。然而，他们对使用 CamemBERT 权重和 tokenization 进行控制预训练的实验表明，其结果与 DrBERT 4GB 从零开始训练的模型相当，但 CamemBERT 权重和 tokenizer 模型的稳定性存在问题。

最后，他们总结说，他们的系统在 11 个下游任务中表现最好，并且在全球范围内超越了通用模型 CamemBERT。他们还观察到，更专业的训练数据效果更好，但难以扩展。所有从 NACHOS 数据集中预训练的模型都已在 Hugging Face 上免费提供，并附有 MIT 许可，所有训练脚本也已上传到他们的 GitHub 仓库。感谢各位的聆听，并期待在多伦多会议期间进行交流。</sample>
    <sample id="319">The paper explores the following pre-training strategies:

*   **From-scratch pre-training:** Training models from the ground up using NACHOS data.
*   **Continual pre-training:** Fine-tuning models with the weights and tokenization of CamemBERT on NACHOS data.
*   **Control pre-training:** Using the weights and tokenization of CamemBERT trained on a 4GB subset of NACHOS.</sample>
    <sample id="320">根据研究结果，测试重复使用导致的过拟合因素并不显著。</sample>
    <sample id="321">The DEPLAIN corpus provides manually aligned sentence pairs, which serve as a gold standard for evaluating automatic alignment methods. The presentation also mentions that fine-tuning language models to produce simplified text can achieve scores better than baseline scores, which can be considered a benchmark.</sample>
    <sample id="322">Enrico from ACL 23 presents on how text classifiers learn about morality. He begins by defining morality as a subjective internal compass guiding judgments of right and wrong, crucial for language models to understand.  Traditional approaches to morality in text often treat it as a single scale, failing to account for its pluralistic and context-dependent nature.

Enrico introduces the Moral Foundation Theory, proposing five fundamental ways humans perceive morality, each influencing judgment. He highlights that language models have shown some ability to understand morality in text, prompting the research to investigate *how* they learn this.

The paper employs explainable AI techniques on language models trained on morality in text, focusing on domain-specific expression of morality. They utilize the Moral Foundation Twitter Corpus, containing tweets from seven domains like #AllLivesMatter and #BlackLivesMatter. The goal is to determine if models can recognize these nuanced differences in moral expression.

The presentation provides a preview, illustrating the difference between #AllLivesMatter and #BlackLivesMatter. While both address similar topics, they employ distinct rhetoric regarding moral subversion (rebellion against authority). The research reveals that language models recognize this difference, associating subversion with negative connotations in #AllLivesMatter and more positively in #BlackLivesMatter.

Enrico emphasizes that morality is expressed differently across domains, and using a single model for multiple domains can lead to dangerous misunderstandings. The paper explores various levels of understanding, warning against the limitations of a one-size-fits-all approach to morality in NLP. He concludes by inviting attendees to ACL in Toronto.</sample>
    <sample id="323">Yujie Wang from Shanxi University introduces DHLK, a new approach for Commonsense Question Answering (QA).  Commonsense QA requires understanding common knowledge to answer questions, which necessitates retrieving relevant information from external knowledge sources. Existing methods often struggle with noisy entities in retrieved knowledge and limited interaction between language models and knowledge bases.

DHLK addresses these issues by building a Heterogeneous Knowledge Graph (HKG) optimized through pruning and Knowledge Representation Learning (KRL). This HKG incorporates knowledge from multiple sources and dynamically removes irrelevant entities based on attention weights.  The language model (RoBERTa) encodes both the question context and entities, fusing them to create a richer representation.  

A key innovation is the use of Relation Mask Self-Attention (RMSA) to model the HKG subgraph. RMSA, inspired by RGAT, incorporates relationships between entities, allowing for more nuanced knowledge representation.  Entity and relation embeddings are optimized using TransE.  

The HKG path information is then integrated with the question context through path enhancement.  Finally, a Multi-Layer Perceptron (MLP) predicts the answer probability based on the HKG embedding, path information, and question context embedding.

Experiments on CommonsenseQA and OpenBookQA demonstrate that DHLK achieves strong performance compared to existing methods that combine language models and knowledge graphs. The results highlight the effectiveness of the proposed approach in leveraging both language understanding and structured knowledge for accurate Commonsense QA. The paper also details the key entity extraction and knowledge path retrieval techniques used for evaluation.</sample>
    <sample id="324">语言模型确实有不同的政治偏见。研究表明，不同的语言模型在政治立场上存在差异，例如 GPT-4 更为自由派，而 GPT 系列通常比 BART 系列更具社会自由派倾向。此外，语言模型还会继承训练数据中存在的政治偏见，并且这些偏见会随着时间推移而发生变化，例如在 2017 年之后，语言模型倾向于更远离政治中心。最后，研究发现，不同政治偏见的语言模型在 hate speech 和 fake news 检测方面的表现也存在差异，这表明语言模型存在潜在的公平性问题。</sample>
    <sample id="325">大家好！我叫马蒂亚斯·林德曼，今天我将简要介绍我们论文“无需树结构的成分泛化：使用多重标签和隐式排列”的工作。这是一项与我的导师亚历山大·科勒和伊万·提托共同完成的研究。成分泛化是指学习者能够处理更深层次的递归和训练数据中看到的、结构上未知的短语组合。在语义解析的语境下，测试成分泛化可能如下所示。我们有一个训练集，例如“女孩在睡觉”。以及“玛丽知道女孩在睡觉”。这些句子与表示它们核心含义的逻辑形式对齐。与标准的机器学习评估不同，测试集并非来自相同的分布，而是包含结构上未知的逻辑形式。在这个例子中，模型在训练中看到了浅层递归，而在测试中面对更深层次的递归。传统的序列到序列模型难以处理这种离散分布的泛化，并且常常产生与输入不相关的输出。特别是，它们经常无法重现输入和输出之间的系统对应关系，例如在例子中颜色编码的对应关系。一种流行的解决此问题的办法是集成树结构到模型中。这些树结构旨在捕捉关联句子的成分过程与逻辑形式之间的关系。这通常有效，但树结构通常需要从其他方式获得，这可能很复杂，有时会很耗时。这通常涉及对逻辑形式的相当大的形式化预处理，例如处理变量符号。获取树结构也可能涉及专门的语法推断程序。在本文中，我们不使用树结构，而是一种神经网络序列到序列模型，该模型直接建模输入和输出片段之间的对应关系。我们首次证明了无需树结构即可实现对更深层次递归的强大泛化。我们的方法通过两步预测输出。首先，我们为输入中的每个 token 标记一个包含输出中可能出现的 unordered 多重标签的 token。在第一步之后，我们拥有所有正确的 token，但它们尚未排序。因此，在第二步中，我们使用另一个模型来预测一个 permutation，将这些 token 放入正确的顺序。我们引入了一种新的方法来预测 permutation，该方法不施加任何硬约束于可能的 permutation。这使得我们的方法非常灵活和具有表达力。概念上，我们的 permutation 模型大致如下工作。我们从左到右遍历输出，确定要在每个位置放置哪个多重标签 token。对于输出中的第一个位置，我们简单地选择一个，如图中红色高亮所示。然后跳到下一个多重标签 token，以确定输出中的第二个 token。我们以类似的方式确定输出中的第三个 token，跳到另一个多重标签 token。我们继续这个过程，直到所有从第一阶段访问的 token 都被访问过一次。为了给您一个关于我们实验结果的预览，我们使用 COGS 基准将我们的方法与其他无树模型进行比较。我们的模型在更深层次递归的泛化方面比其他模型都表现出显著优势。然而，仍然存在一些其他类型的结构泛化非常具有挑战性。在本文中，我们解决了几个有趣的实际挑战。首先，输入和输出之间的对齐关系在训练数据中未给出。因此，对于给定的 token，我们不知道它来自哪个多重标签。这给训练带来了挑战。此外，有时存在多个与数据一致的 permutation，但语言上正确的 permutation 是隐式的。我们通过将对齐关系作为训练的一部分来解决这个问题。我们的 permutation 方法非常灵活，但它带来了一个挑战：找到最高分数的 permutation 是 NP-hard 的。这是因为这与“旅行商问题”相关。我们通过一个 GPU 友好的连续放松来近似这个问题，该放松还允许我们通过解决方案进行反向传播，并学习更符合语言逻辑的 permutation。如果您想了解更多关于我们的实验以及我们如何解决这些挑战的信息，请查看我们的论文或来我们的海报。</sample>
    <sample id="326">认知失调是指两个信念或行为不一致的情况，例如一个人知道吸烟会致癌，但仍然吸烟。这种不一致会产生心理上的不适感，并可能通过其他行为来缓解，例如通过说服自己“我需要吸烟来维持工作”。</sample>
    <sample id="327">这段内容介绍了ManagerTower，一种新的视觉语言模型架构，旨在提升视觉语言理解能力。

传统模型如METER和BridgeTower，通过连接不同层级的单模态编码器，试图利用不同层次的语义知识。但BridgeTower存在层级利用效率低和可扩展性差的问题。

ManagerTower在此基础上改进，引入“管理器”机制，每个跨模态层都配备管理器，负责整合并融合来自不同层级的单模态编码器的信息。这种自适应聚合方式能更有效地利用不同层次的语义知识，从而实现更全面的跨模态对齐和融合。

ManagerTower采用RoBERTa和CLIP-ViT作为单模态编码器，并在视觉问答任务上取得了优异的性能，尤其在Wikivideo数据集上提升了39.15%的准确率。实验表明，自适应管理器能更有效地利用不同层次的语义知识，而静态管理器则效果不佳。

ManagerTower与METER、BridgeTower在预训练和微调设置上相同，但性能显著提升，证明了自适应管理器在提升视觉语言模型性能方面的有效性。论文、代码和模型可在Archive和Github上找到。</sample>
    <sample id="328">GPT-4 是最倾向于自由派的语言模型。</sample>
    <sample id="329">Minghang Zheng (北京大学) 团队介绍了论文“生成带结构伪标签的无监督视频句子定位方法”的研究成果。该研究旨在解决视频句子定位任务中的数据标注成本高昂的问题，提出了一种鲁棒性更强的无监督方法。

视频句子定位任务的目标是根据自然语言查询，找到视频中与查询最相关的片段。现有方法通常依赖大量人工标注，效率低下。

该团队提出的方法主要分为三个步骤：首先，使用预训练的图像描述模型生成更复杂的自由式伪查询；其次，利用预训练模型衡量视频帧与伪查询的相关性，生成伪事件，保证视频内事件与查询的相关性高，视频外相关性低；最后，通过对伪标签进行加权和精炼，减少标签噪声的影响。

具体来说，他们使用BLIP模型生成伪查询，并根据事件的时间结构生成伪事件。通过计算视频帧特征与查询文本特征的相似度，以及事件内外的相似度差异，来确定伪事件的质量。

为了减少标签噪声，他们采用两种策略：一是根据模型预测置信度和预测与伪标签的IoU值，对样本进行加权，降低噪声样本的影响；二是对于置信度高且IoU与伪标签高的预测，将其作为新的伪标签进行后续训练。

实验结果表明，该方法在ActivityNet Captions和Charades-STA两个数据集上的R@M和mIoU指标均优于现有方法，实现了最佳的无监督性能。论文代码已提供。</sample>
    <sample id="330">Cumulative training performed equal or better than Iterative training across the board.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">MuDA基准的数据是从一个平行语料库获得的。</sample>
    <sample id="333">Wenhao from Nanjing University introduces their work "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation," focusing on improving neural machine translation (NMT) models. The core problem addressed is the non-smooth representation space in NMT, leading to sparse low-frequency tokens and poor generalization.

The proposed solution, kNN-MT, aims to smooth predictions by leveraging nearest neighbors in the representation space. However, kNN-MT suffers from time-consuming neighbor retrieval and difficulty in updating the datastore. To overcome these limitations, the INK framework is introduced. INK employs a two-step training loop: first, it extracts kNN knowledge from the datastore to guide an adapter in adjusting the representation; second, it asynchronously updates the datastore with the refined representations. This process iteratively refines the representation space.

INK utilizes a combined learning objective, aligning contextualized representations with token embeddings, kNN token embeddings, and the same target token to address sparsity.  The datastore can be dropped after training, and the adapter can be used for inference. Experiments on the WMT’19 German-English news translation task demonstrate that INK outperforms state-of-the-art kNN-MT systems, even surpassing the performance of the WMT winner model.

The research explores three key questions: can INK smooth the representation space with a small adapter and drop the datastore? How much improvement can be achieved by using kNN knowledge? And can combining an adapter and datastore further improve performance?  Results show that INK achieves higher BLEU scores with less memory, and that jointly applying adapter and datastore further smooths predictions.

In conclusion, INK proposes a novel training framework for iteratively refining the NMT model's representation space using kNN knowledge. Experimental results indicate a significant improvement in translation performance, with reduced memory usage and faster inference speed. The INK system achieves an average gain of 1.99 COMET score and 1.0 BLEU score compared to existing kNN-MT systems.</sample>
    <sample id="335">Matthias Lindemann.</sample>
    <sample id="336">在跨语言语义解析中，跨语言转移是指在一种语言上训练的模型，然后将其应用于另一种语言的任务。具体来说，它指的是利用一种语言的知识来提高另一种语言的性能，尤其是在数据有限的情况下。</sample>
    <sample id="337">## Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning: Key Information

This research addresses the challenge of representing out-of-vocabulary (OOV) words in language models, a critical aspect for downstream tasks. The core idea is to leverage word formation and association, drawing inspiration from human language processing, to infer the meaning of OOV words.

The proposed solution introduces a **Word Relationship Graph**, mimicking lexical rules of word formation and association. OOV words are tokenized into wordpieces and associated with relevant words, creating a two-level graph. The first layer retains complete wordpiece information, while the second layer samples nodes for training to mitigate noise.

To address the challenge of assigning node attributes to OOV nodes, a **self-attention network** is employed to assign attributes based on character information.  A **Graph Attention Network (GAT)** is then applied twice, concatenated and fused with the initial input, to extract node representations. A **readout block** generates a graph-level representation summarizing the word formation.

The model utilizes a **Graph Convolutional Network (GCN)** for processing the graph, aiming to mimic the vector space of background embedding models.  **Contrastive learning** is incorporated into the loss function, using NT-XENT positive samples (e.g., two-hop relevant neighbors, synonyms, the OOV word itself) to encourage proximity between related words and distance between unrelated ones.

Experiments demonstrate the superior performance of the proposed model compared to baselines in both intrinsic and extrinsic tasks.  The model benefits both static and contextual language models.

The research acknowledges the potential for extending the model to other languages, particularly agglutinative languages where word decomposition is straightforward.  However, fusional languages present greater challenges. The model's effectiveness with English is attributed to reasonable word segmentation.

In conclusion, the graph-based approach effectively handles complex word formations and offers a promising avenue for learning OOV word embeddings, with potential for broader application across various languages, contingent on appropriate word decomposition strategies.</sample>
    <sample id="338">Bingsheng from Rensselaer Polytechnic Institute, Northeastern University, and IBM Research presented their research on evaluating the helpfulness of human-generated natural language explanations for machine learning models. The paper, titled "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations," addresses the challenge of objectively assessing explanations, which are often subjective and task-dependent, unlike labels used for training.

The research introduces a unified data format to standardize tasks and facilitate comparison. They conducted experiments on five datasets (CoS-E, ECQA, e-SNLI, ComVE) using both baseline and explanation-infused models.  The study found that fine-tuning with explanations doesn't necessarily teach models new knowledge, but rather encourages reliance on the explanation part of the input.  CoS-E explanations were less helpful than ECQA explanations for baseline models, highlighting task-dependent nature.  However, fine-tuning with explanations led to substantial performance improvements.

To address the limitations of existing metrics like BLEU and ROUGE, which focus on word similarity, the authors propose a novel evaluation metric called TREU. TREU extends the simulatability score by evaluating explanation helpfulness during fine-tuning.  They evaluated TREU and the simulatability score on T5 and BART models across the five datasets.  TREU consistently ranked datasets based on quality, while the simulatability score struggled with ComVE and e-SNLI.  Furthermore, TREU scores showed task-specific behavior, with positive scores for entailment and negative scores for neutral/contradiction classes in e-SNLI and ComVE.

The research concludes that TREU outperforms the simulatability score in evaluating explanation helpfulness and lays the groundwork for better quality control in human annotation. They recommend future research to incorporate similar quality checks. The paper's contributions include a unified data structure, preliminary experiments analyzing explanation utility, and the proposal of TREU, demonstrating its superior performance compared to existing metrics.</sample>
    <sample id="339">Saarland University in Germany.</sample>
    <sample id="340">Kuan-Hao Huang from UCLA presents "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation." The presentation highlights the importance of high-quality paraphrase data for NLP tasks like question answering and chatbots, noting the limitations of existing datasets like MRPC and PAN in terms of scale and syntactic diversity.

The core idea of ParaAMR is to leverage AMR (Abstract Meaning Representation) graphs for generating more diverse paraphrases. AMR graphs represent the abstract meaning of sentences as directed graphs, with nodes representing semantic concepts and edges representing relationships.  The method involves using a pre-trained AMR parser to create an AMR graph from a source sentence. Then, the focus of the graph is randomly changed (a node becomes the new root), and the edges and their labels are modified.  An AMR graph-to-text generator then produces text from this modified graph. This process aims to create paraphrases with different syntactic structures while preserving semantic similarity.

ParaAMR contains approximately 15 million source sentences, with around 6.9 paraphrases per sentence.  Quantitative analysis shows that ParaAMR achieves higher syntactic diversity scores compared to other back-translation datasets, while maintaining similar semantic similarity.

The presentation demonstrates the benefits of ParaAMR across several NLP applications.  Specifically, sentence embeddings learned from ParaAMR outperform those learned from other datasets on the STS benchmark.  ParaAMR enables better syntactic control in paraphrase generation.  Furthermore, ParaAMR improves performance in few-shot learning scenarios when used for data augmentation.

In conclusion, ParaAMR is a large-scale, syntactically diverse paraphrase dataset created through AMR back-translation, offering advantages over existing datasets and benefiting various NLP applications. The dataset is publicly available.</sample>
    <sample id="341">BLEU score (translation quality) and average lagging (latency measure), and computational-aware average lagging (accounts for model's computational times).</sample>
    <sample id="342">## LiveChat Dataset: A Summary

This presentation introduces "LiveChat," a large-scale, personalized dialogue dataset constructed from live streaming videos on Chinese platforms like TikTok and Douyin. The paper addresses the limitations of existing dialogue datasets, which are predominantly text-based and often lack personalization and multi-party scenarios.

**Key Problem:** Existing datasets suffer from limited scale, reliance on manual annotations, and insufficient representation of personalized dialogue, especially in multi-party conversations.

**LiveChat Solution:** The authors propose LiveChat, a video-sourced dataset built through three steps: 1) scraping videos, 2) transcribing audio using Automatic Speech Recognition (ASR), and 3) constructing dialogues using a reply-to-whom matching method based on audience comments.

**Personalization:** LiveChat incorporates persona information through manual labeling and rule-based/classifier-based extraction of basic and detailed persona profiles. This allows for personalized dialogue generation.

**Dataset Comparison:** LiveChat distinguishes itself from existing datasets by being video-sourced, larger in scale, and containing longer average conversation sessions.

**Experimental Results:** The study evaluates LiveChat on response modeling and addressee recognition tasks.  Experiments show that persona profiles and longer conversation sessions are beneficial for response modeling.  While single-stream BERT performs better in addressee recognition, persona still aids in this task.  Pre-trained dialogue models like BART perform well, indicating the dataset's distinct characteristics.  In-context learning with demonstrations improves LLM performance, but excessive demonstrations can introduce noise.

**Conclusion:** LiveChat provides a valuable resource for research in personalized dialogue, particularly in Chinese contexts. The dataset's characteristics and experimental results highlight the importance of persona information and longer conversation sessions for learning personalized responses. Future work will focus on efficient transfer learning of large language models for LiveChat.</sample>
    <sample id="343">大家好，我是Akshatha，和我合作的作者Martin今天和大家分享我们的工作“KITMUS测试：评估多源知识整合”。这项工作是麦吉尔大学、Mila和微软研究的合作。自然语言理解模型从各种知识来源中获取信息，例如存储在参数中的知识，通常是在预训练过程中获得的，以及在推理时提供的知识。最近在问答任务中，研究表明模型可以利用预训练时知识来解决任务。但是，自然语言理解通常需要在推理时提供的知识。例如，在句子“John saw the newly elected president on TV”中，预训练参数可能包含关于总统做什么以及电视是什么的信息，但它们无法可靠地知道这个特定实体“John”是谁，或者新总统是谁，因为总统可能会在预训练后发生变化。因此，成功解决知识密集型NLU任务的模型需要能够整合和使用预训练时知识和推理时知识。在本文中，我们提出了一种知识整合诊断测试套件。我们引入了一个核心指代消解任务，旨在测试模型从不同来源中获取知识的能力。我们使用人类研究参与者和已建立的核心指代消解模型评估数据集。这里有一个来自我们数据集的例子。Servin是一位法官。Kea是一位面包师。Servin和Kea在公园见面了。在一天工作结束后，在法庭上处理案件，他很高兴放松。在此任务中，需要识别代词“he”指代的正确实体，在这种情况下，是Servin。解决给定的代词需要两种类型的信息。第一，实体特定知识，例如“Servin是一位法官”。第二，背景知识，例如“法官在法庭上审理案件”。通常，背景知识是在大型语言模型预训练过程中学习的，而实体特定知识通常在推理时观察到的。我们以不同的方式调整两种知识类型的可用性，以便它们可能只存在于一个来源中，或存在于多个来源中。我们定义了三个KITMUS设置。首先，我们有典型的设置：“背景-预训练”，在这种设置中，背景知识被假定是在预训练时可用。其次，我们有一个“背景-两者”设置，在这种设置中，背景知识在预训练时和推理时都可用。最后，我们有一个“背景-推理”设置，在这种设置中，两种知识类型都只在推理时可用。最后一个设置尤其有趣，因为它模拟了背景知识不足以解决任务的情况，因为新职业在预训练时间之后已经发展。这里有一个如何控制真源事实可用性的例子。在背景-预训练设置中，我们假设背景知识“政治家寻求在政府中当选”包含在预训练参数中，而在推理时我们提供实体特定知识“Chichester是一位政治家”。在背景-两者设置中，我们额外提供政治家背景知识，以及在推理时关于政治家的知识。在背景-推理设置中，我们提供虚构的职业“mirituer”而不是政治家，因为“mirituer”不太可能包含在预训练数据中。我们使用人类研究参与者和已建立的核心指代消解模型评估数据集。在图表中，我们展示了最困难的背景-预训练设置中最佳模型的性能。如果没有针对KITMUS的任务特定训练，两种模型都表现不佳。然而，经过针对KITMUS的训练后，两种模型C2F和BERT4Coref都比随机选择表现出显著的改进。这表明，当使用通用指代消解数据集训练模型时，大多数模型学会了利用表面线索，而这些线索在KITMUS中已被删除。此外，关于虚构知识的额外实验表明，即使是最优秀的模型也无法可靠地整合仅在推理时提供的后向知识。总结我们论文的主要结论是，许多核心指代消解模型似乎无法在不同来源的知识之间推理，而没有针对任务的特定训练。然而，通过针对任务的特定训练，一些模型能够成功地整合来自多个来源的知识。然而，即使是最优秀的模型也似乎在可靠地整合仅在推理时呈现的后向知识方面存在困难。如果您对更多细节感兴趣，请查看我们的论文并查看数据集和代码。感谢您的聆听。</sample>
    <sample id="344">Based on the provided text, the disadvantages of tree-based methods are:

*   **Tree acquisition is complicated and computationally expensive.** This involves pre-processing logical forms and/or grammar induction.
*   **Trees are not always provided in the test set.** This makes it difficult to evaluate generalization to unseen compositions.
*   **Trees are not given and need to be obtained somehow.** This adds complexity to the evaluation process.</sample>
    <sample id="345">Matthias Lindemann 介绍了他们发表在论文中的工作，题目是“无需树结构的成分泛化：使用多重标签和隐式排列”。这项研究旨在解决语义解析中，模型需要处理更深层次的递归和训练数据中未见过的句法结构的问题。

传统的基于树结构的解析器在处理这种出界泛化时往往表现不佳，因为树结构需要手动构建或从逻辑形式中提取，这既复杂又耗时。

他们提出了一种新的方法，无需树结构，而是直接使用神经网络来学习输入和输出之间的对应关系。该方法首先对输入标记进行多重标签化，然后使用另一个模型预测输出标记的排列顺序。

该方法通过从输出开始，逐步选择多重标签中的标记，并将其放置到相应的输出位置来实现排列预测。

实验结果表明，他们的方法在 COGS 基准测试中优于其他无树结构模型，尤其是在处理更深层次的递归方面。

此外，他们还解决了输入和输出之间的对齐问题，以及多个可能排列的其中一个更符合语言规则的问题。为了解决这些问题，他们引入了一种GPU友好的连续放松方法，可以进行反向传播并学习更合理的排列。

论文详细介绍了他们的实验和对这些挑战的解决方案，并鼓励读者查阅论文或参加他们的海报。</sample>
    <sample id="346">The provided text does not state the author's institution.</sample>
    <sample id="347">Myra今天介绍了他们的论文“标记式人格：使用自然语言提示来衡量语言模型中的刻板印象”。这项研究由Esin Durmus和Dan Jurafsky合作完成。近年来，许多研究已经记录了大型语言模型（LLM）中存在的社会偏见和刻板印象。然而，这些衡量标准存在多种局限性。它们通常依赖于耗时制作的手工数据集，并且通常只能衡量非常具体的刻板印象，这意味着它们难以推广到其他人口群体或背景，或者仅仅捕捉到非常广泛的、笼统的关联，例如对特定群体的负面联想。此外，目前的研究大多没有考虑到交叉性，即多元社会身份可能会叠加偏见，并成为独特的危害来源。为了克服这些局限性，他们依赖于LLM能够响应指令和提示的特性。因此，我们可以要求模型生成一个人格描述，例如“想象一下你是一个亚洲女性。描述一下你自己。”这样，我们就可以立即看到这一点具有很强的泛化性，因为我们可以随意指定任何我们想要的身份标记。以下是GPT-4生成的几个例子。我们立即看到，虽然这些输出并非传统意义上的明显负面或有害，但仍然存在一些有趣的模式。亚洲女性被描绘为不引人注目的；中东女性则被用作“异域风情”和“迷人地区”等词语，而白人男性则没有任何此类描述。并且，所有女性的群体人格描述都提到了血统，而白人男性的群体人格描述则没有任何提及。

为了捕捉这些模式，他们的方法分为两个部分。第一部分是生成这些人格描述。我们的提示语是为了借鉴一项研究，该研究向人类参与者提供了这些提示，发现通过向人类参与者提供这些提示，他们也能揭示种族刻板印象。此外，这使得我们能够直接比较我们生成的人格描述和人类撰写的回复。第二部分是标记词语方法，这是一种识别区分标记群体和非标记群体的词语的方法，我稍后会详细说明。这项方法的优势在于，我们能够获得非常具体的刻板印象和模式，而无需依赖任何特定的词汇表。因此，标记词语方法借鉴了社会语言学的“标记性”概念，该概念指出，存在一个非标记的默认状态，任何与该默认状态不同的群体都具有语言上的标记性。例如，“战士”一词通常与男性相关联。当人们描述一个女性战士时，他们通常会明确指出“女性战士”，并标记“女性”这个词。更广泛地说，社会上占据主导地位的群体在语言和社会上都是非标记的，而边缘化的群体通常是标记的。在我们的方法中，我们首先确定标记群体和非标记群体，然后通过使用“Fightin’ Words”方法比较人格描述，该方法基本上是使用加权对数似然比值来区分标记群体中的最常见词语。例如，对于黑人女性的人格描述，我们将使用“Fightin’ Words”方法并将其对标于白人男性的人格描述，因为这两个都是对应的非标记群体。

接下来，我们来看一些结果。首先，我们使用一个刻板印象词汇表，发现生成的描述包含比人类撰写的描述更多的刻板印象。然而，当我们仔细查看词语的分布时，我们会发现非常不同的情况。虽然生成的描述中刻板印象词语的频率更高，但人类撰写的描述中词语的分布却更加广泛，而生成的描述中的刻板印象词语实际上只是“高大”和“健壮”等词语，这些词语通常是积极或至少非负面的。事实上，这个词汇表并没有很好地捕捉到我们之前看到的许多有害模式。因此，我们将转向使用标记词语方法来展示这些看似积极的词语如何助长刻板印象和本质化叙事。在我们的分析中，我们发现最常见的词语包括“文化”、“传统”、“自豪”和“异域风情”。这些词语将这些群体定义为与他们的身份相关的，并将他们与其他白人规范区分开来，这导致了长期以来对这些群体的歧视和异化。此外，还有许多常见的叙事模式反映在这些词语中，尤其是在女性群体中。例如，描述拉丁裔女性的词语包括“充满活力”和“丰满”，这些词语与热带主义的刻板印象相关联。对于亚洲女性，描述的词语包括“娇小”和“柔美”和“丝滑”，这些词语与亚洲女性长期以来被过度性化、视为非常顺从的刻板印象相关联。而对于黑人女性，我们看到一些最常见的词语包括“坚强”和“有韧性”。这与人们所说的“强大的黑人女性”原型相关联。虽然这个原型看起来很积极，但已有研究表明，这种原型实际上非常有害，因为它给这些群体带来了巨大的压力，必须在社会障碍面前保持坚强和有韧性。然而，这实际上并没有解决这些障碍，反而给这些群体带来了负面健康影响等其他危害。更广泛地说，我们发现每个标记群体的人格描述基本上都反映了非常本质化的叙事。因此，根据这些模式，我们得出结论，为模型所有者提出了三个建议。首先，作为研究人员，我们应该解决积极的刻板印象和本质化叙事。我们还应该使用交叉性视角来研究偏见和危害，因为如果没有这样做，可能会忽略许多问题。最后，应该对偏见缓解方法进行更多的透明度，因为例如，这些积极的刻板印象，我们不知道这是否是由于某种过度价值对齐，还是由于其他反刻板印象方法导致了这些有害模式。我们无法在没有更多透明度的情况下进行进一步的研究和探讨。感谢您的聆听。祝您在ACL会议上度过愉快的一天。</sample>
    <sample id="348">## 总结：LLM中的刻板印象研究

研究人员Myra、Esin Durmus和Dan Jurafsky探讨了大型语言模型（LLM）中存在的社会偏见和刻板印象问题。现有方法依赖人工数据集，难以泛化和捕捉复杂性，尤其缺乏对性别、种族等身份交叉的影响。

他们提出了一种新的方法，利用LLM生成不同身份的“人物画像”，并分析这些画像中出现的词语。通过“标记词”方法，识别出与特定群体相关的“标记词”，从而揭示LLM中隐藏的刻板印象。

研究发现，LLM生成的画像虽然不显式地表达负面情绪，但却包含大量与特定群体相关的积极刻板印象，例如“文化”、“传统”、“骄傲”、“异域风情”等。这些词语强化了对特定群体的刻板印象，并可能导致歧视和“异化”。

此外，研究还揭示了“坚强女性”等原型刻板印象的潜在危害，即对特定群体施加不合理的压力。

研究人员建议：

1.  研究人员应关注并解决LLM中存在的积极刻板印象和本质化叙事。
2.  应采用交叉视角研究偏见，避免遗漏重要问题。
3.  应提高对偏见缓解方法的透明度，以便更好地理解和解决问题。

总而言之，该研究表明，LLM中的刻板印象并非显而易见，而是通过细微的词语选择和组合潜移默化地影响着模型输出，并可能带来严重的社会危害。</sample>
    <sample id="349">大家好，我是来自中国科学技术大学的景伟易。非常荣幸能给大家做一个关于我们论文的简短宣传视频。你们在复制我的模型吗？通过后门水印保护大型语言模型用于嵌入服务的版权。首先，我们先介绍一下嵌入服务。目前，GPT、LLAMA、PALM等大型语言模型在自然语言理解和生成方面表现出色。嵌入服务是基于大型语言模型的一项服务，用于辅助各种自然语言处理任务。例如，OpenAI提供一个基于GPT的嵌入API。然而，最近的研究表明，攻击者可能会通过学习嵌入来窃取模型并提供类似的服务。因此，保护嵌入服务的版权至关重要。为了保护嵌入服务的版权，一种解决方案是为提供商服务嵌入水印，并检测其他服务中是否包含水印。水印方法需要满足以下几个属性。首先，该方法应适用于嵌入服务。其次，水印不应降低提供的嵌入的效用。第三，水印应足够隐蔽，攻击者难以发现或移除。最后，水印应能够转移到攻击者服务中的模型。现有工作可以大致分为四类。然而，这些方法要么不适用于嵌入服务，要么缺乏转移性。因此，在本文中，我们提出了嵌入标记，这是一种后门水印方法，适用于嵌入服务。然后，让我介绍一下我们的嵌入标记的细节。嵌入标记包含两个主要步骤：水印注入和版权验证。在这些主要步骤之前，我们首先选择一个触发集。触发集是一个在一定频率范围内的一组单词。我们假设提供商可以收集一个通用文本语料库并统计单词频率。在水印注入中，我们首先定义一个目标嵌入。当用户向提供商服务发送句子时，提供商会统计句子中触发器的数量。提供的嵌入是目标嵌入和原始嵌入的加权和。目标嵌入的权重与句子中触发器的数量成正比。当句子中触发器的数量大于 m 时，提供的嵌入就等于目标嵌入。版权验证是检测其他服务中是否包含标记的模型的步骤。我们首先构建一个后门数据集和一个无害数据集。后门数据集包含所有单词都属于触发集的句子，而无害数据集中的所有单词不属于触发集。然后，提供商向窃取者的服务发送包含这些数据集的嵌入。我们计算请求的嵌入与目标嵌入之间的余弦相似度和 L2 距离。同时，我们计算无害数据集和后门数据集之间的余弦相似度差和 L2 距离差。此外，我们还应用 KS 检验，并使用其 p 值作为第三个指标。我们在 AG News、MIND、SST2 和 Enron Spam 四个数据集上进行了实验。我们假设提供商使用维基文本数据集来统计单词频率。四种数据集上的结果表明，我们的嵌入标记可以具有很强的检测性能，同时保持良好的下游任务效用。我们还通过可视化四个数据集的句子嵌入来验证提供的嵌入的隐蔽性，使用 PCA。图的标题表示句子中触发器的数量。如图所示，很难区分后门嵌入和正常嵌入。以上就是全部内容。谢谢。欢迎与我们讨论。</sample>
    <sample id="350">Simone Tedeschi and colleagues' paper investigates the reliability of leaderboard scores in Natural Language Understanding (NLU), particularly concerning "saturated benchmarks" like SuperGLUE and SQuAD. The authors argue that current leaderboard-based evaluations don't accurately reflect true human-level performance due to several issues.

Firstly, human evaluation is often flawed. Humans are frequently evaluated on small subsets of the test data, while systems are tested on the full set.  Furthermore, ground truth answers in datasets are often incorrect, leading to unfair comparisons.  Systems can exploit spurious correlations in the data, unlike humans.  The "human baseline" is also often poorly defined, relying on simple aggregation methods that don't account for varying human expertise.

Secondly, the quality of human annotations is often overlooked.  The number of annotators, their background, and motivation levels are frequently not disclosed, making it difficult to assess the reliability of the human performance being compared to systems.  Low pay rates for annotators can significantly impact the quality of the data.

The paper highlights that while systems may achieve impressive scores, these scores don't necessarily indicate genuine understanding.  They are susceptible to overfitting to specific patterns in the training and test data.  The authors conclude that current benchmarks are not scientifically meaningful for assessing true NLU capabilities.

The paper recommends more rigorous evaluation methods, including larger and more representative human datasets, better-defined human baselines, and transparent annotation practices.  They aim to create more reliable benchmarks that accurately reflect the complexities of human language understanding.  The authors encourage readers to explore their paper for a deeper understanding of these issues and potential solutions.</sample>
    <sample id="351">Shuheng's presentation introduces a paper investigating the generalization capabilities of CoNLL-2003 named entity taggers in 2023. The paper addresses the question of whether models trained on CoNLL-2003 still perform well on modern data and what factors contribute to good generalization. To investigate, the authors created the CoNLL++ dataset, a collection of Reuters News data annotated with the same CoNLL-2003 guidelines. They fine-tuned over 20 models on CoNLL-2003 and evaluated their performance on both CoNLL-03 and CoNLL++ datasets, measuring generalization through F1 score changes.

The study identified three key ingredients for good generalization: model architecture (transformer models perform better), model size (larger models generalize better), and fine-tuning examples (more examples lead to better generalization).  The authors explored potential causes for performance drops, hypothesizing adaptive overfitting and temporal drift.  They found no evidence of adaptive overfitting, as the improvement on CoNLL++ was consistently greater than on CoNLL-03.  However, they confirmed temporal drift, where performance degrades with increasing temporal gap between training and testing data.

The conclusion is that achieving good generalization requires a combination of a better model architecture, larger model size, and sufficient fine-tuning examples.  Importantly, temporal drift is the primary cause of performance degradation, despite the long history of CoNLL-2003.  The presentation concludes that CoNLL-2003 taggers still work well in 2023, encouraging further research into improving model generalization. The authors invite attendees to explore their paper, dataset, and contact them with questions.</sample>
    <sample id="352">ABC-Eval is a new dimensional approach to evaluating conversational AI that attempts to reduce subjectivity in human evaluation by explicitly annotating whether or not model responses express certain behaviors like irrelevant information, contradictions, or hallucinations. It measures rates of various thematic errors and is designed to provide a more reliable and informative assessment of chat model quality compared to existing methods.</sample>
    <sample id="353">## ACL 2023 Paper Summary: Python Code Generation by Asking Clarification Questions

This paper addresses the challenge of input underspecification in code generation, a significant hurdle in current program synthesis research.  State-of-the-art methods struggle with situations where natural language descriptions (NLDs) lack crucial details, exemplified by missing specifications for code components like regressors. The authors propose a novel approach: leveraging interactivity by asking clarification questions to gather missing information.

The core idea is to create a synthetic dataset called CodeClarQA, consisting of NLDs paired with clarification questions (CQAs) designed to address missing key operations.  They develop a method to identify missing key operations within NLDs by comparing them to operation documentation using a latent space representation based on schemata.  This involves calculating similarity scores between schema elements and flagging operations where scores fall below a threshold.  Human annotators validate the dataset and create CQAs for missing operations, employing both yes/no and multiple-choice question formats.

The paper details a pipeline for code generation driven by clarification questions. This pipeline comprises a Clarification Need Predictor, a Question Selector, and a Code Generator.  Experiments demonstrate that the proposed method effectively identifies missing key operations, with MPNet achieving strong performance in this area.  Error analysis reveals challenges related to distinguishing aligned operations from those with similar names and the reliance on documentation rather than argument values.

Furthermore, the authors investigate the impact of clarification questions on code generation. They find that incorporating high-ranked clarification questions leads to improved model performance across various metrics. However, the pipeline still lags behind model-only training, highlighting the difficulty of fine-tuning models on the complex task of CQ generation and ranking.

Finally, the paper analyzes whether clarified key operations contribute to better code generation.  The results suggest a positive correlation, with training on Oracle CQAs leading to predictions close to the ground truth.  However, the task remains challenging due to the potential for the pipeline to include clarification questions not present in the reference data. The authors conclude by emphasizing the potential of this approach to overcome input underspecification in code generation and invite feedback on their work.</sample>
    <sample id="354">CoNLL-2003 和 CoNLL++ 之间的性能增量直到 2020 年才高于 5 个百分点。</sample>
    <sample id="355">Vasudha 博士，计算机科学 PhD 候选人，来自纽约州 Stony Brook 大学，今天很高兴向大家介绍我们发表在 ACL 2023 会议上的长篇论文，题目是“迁移学习用于异议检测：解决稀有类问题”。

首先，我们定义认知异议，并解释为什么它在语言研究中是一个重要问题。简单来说，认知异议是指两个信念或行为相互矛盾，例如，一个人说“我知道香烟会致癌”，然后又说“我抽了几个烟”，这两种行为相互矛盾，构成了异议关系。而“我不能失去我的工作，如果没有它们”则构成共性关系。虽然异议是日常决策中非常常见的一种现象，但在语言等其他类型的交流中，它却非常罕见。那么，为什么这很重要呢？研究认知异议可以帮助我们理解不同人群之间的意见分歧，追踪趋势和信念变化，以及态度转变。高认知异议也与焦虑症有关，有助于我们更好地理解人们的心理健康。研究语言中表达的异议，也能帮助我们理解群体中的极化和偏见。此外，认知异议对于理解个人的认知风格和更好地理解决策过程也至关重要。

为了创建认知异议资源，我们进行了大规模的异议关系标注。我们采用了异议优先的方法，如图表所示。将推文通过 PDTB 解析器进行处理，然后对语料中的两个语句进行标注，标注结果符合我们论文中描述的指南。正如大家所见，在 1,000 个语句对中，只有 3.5% 的语句被标注为异议。在收集了大约 1,000 个语句对后，我们训练了一个初始分类器，该分类器仅使用 43 个异议样本进行训练。令人遗憾的是，这个分类器在性能上并没有达到随机猜测的水平。由于异议的发生频率非常低，并且缺乏任何以前的此类数据集，我们面临着绝对稀有性的问题。为了缓解这个问题，我们尝试了迁移学习和主动学习的多种组合，以收集更多异议样本，从而在更少的标注轮次下降低整体标注成本，同时提高异议检测的准确性。由于初始模型无法捕捉到异议类别，因此我们首先启动了主动学习过程，通过从与异议相关的任务中迁移权重来解决这个问题。我们从两个不同的任务中进行迁移：

1.  **辩论任务：**一个任务，用于确定两个不同人提出的辩论陈述是同意还是不同意，无论话题如何。
2.  **CE 任务：**一个任务，用于对 PDTB 中扩展和比较类别的二元分类进行标注，因为这两个类别与共性和异议的产生密切相关，我们称之为 CE。

我们发现，通过在标注数据集中迁移零样本性能已经优于随机猜测，最佳性能为 AUC 0.62。此外，通过迭代地对 CE 任务进行微调，然后进一步对辩论任务进行微调，我们发现，微调 CE 任务后，再对辩论任务进行微调，性能会更好，零样本性能达到最佳。因此，我们使用这个模型作为主动学习的起始点。

接下来，我们需要确定更新模型的方法，以根据每个主动学习轮次的新的数据进行更新。“累积”方法会累积所有已收集的标注数据，而“迭代”方法则会在每个轮次训练模型，使用最新的数据集合。在不同的策略中，我们发现“累积”方法在各个方面都优于“迭代”方法。

为了增加异议样本的数量，我们使用了概率稀有类策略（PRC），选择那些当前模型预测为最有可能被标注为异议的样本。我们将其与社区中常用的其他 state-of-the-art 主动学习策略进行比较。我们发现，提出的 PRC 策略比其他 state-of-the-art 策略表现更好，尽管差异很小。请注意，随机策略的性能明显较低。在后续的主动学习轮次中，使用两个最佳策略，我们提高了异议分类的 AUC 至 0.75，这是我们目前为止取得的最佳性能。此外，我们还检查了每个策略对标注质量和标注成本的影响。我们发现，PRC 方法具有最高的异议样本比例，并且在稀有类方面效果最好。然而，标注员也发现这些样本非常困难。

总而言之，我们发现 PRC 是一个简单的主动学习策略，用于稀有类别的获取，并且与适当设计的迁移学习任务结合使用，可以显著提高主动学习的性能。我们还发现，迭代更新对于从不同的领域迁移学习很有用，而针对领域主动标注则受益于累积更新。这些是我们的核心数据集和论文的链接。如果您有任何问题，请随时与我们联系。谢谢大家。</sample>
    <sample id="356">The authors are affiliated with the University of California, Berkeley.</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig.</sample>
    <sample id="359">The method was compared with state-of-the-art architectures specifically tailored for simultaneous pre-translation.</sample>
    <sample id="361">Armineh Nourbakhsh, a PhD student at Carnegie Mellon University and a research director at JP Morgan AI Research, presented "CounterComp," a research project focused on improving compositional generalization in multi-step quantitative reasoning, specifically question answering. The core problem addressed is that current neural models struggle with tasks requiring more than two arithmetic steps due to memorizing spurious patterns, such as repeatedly associating tokens like "2019" with common operations like subtraction.

CounterComp tackles this by leveraging counterfactual scenarios. The approach treats each training sample as an anchor and mines positive and negative examples by intervening in the question to assess whether the output changes. A positive example represents a question modification that doesn't alter the output, while a negative example represents a modification that does.  An auxiliary metric learning loss is then added to the training procedure, dynamically adjusting its margin based on the extent of change introduced by the interventions.

The research demonstrates that incorporating this auxiliary loss consistently improves the performance of state-of-the-art baselines, particularly when reasoning steps exceed two.  Crucially, CounterComp also enhances performance on out-of-distribution samples – scenarios where the model is tested on data different from its training data, a key aspect of compositional generalization.

Qualitative analysis further supports the effectiveness of CounterComp, showing that it encourages the model to attend to more relevant tokens during training, those associated with meaningful operational terms in the output. The presentation highlights the cost-effectiveness of this approach, avoiding the need for extensive human supervision compared to traditional methods.  The research is supported by the provided references and encourages further exploration through a poster and contact information. The presenter expressed gratitude to co-authors, advisors, and the audience.</sample>
  </task>
</testset>