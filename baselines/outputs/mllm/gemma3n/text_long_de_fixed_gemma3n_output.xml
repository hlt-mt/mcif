<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Sprachmodelle sind große Mengen an Webcrawl-Daten, die auch politische Nachrichtenmedien umfassen.</sample>
    <sample id="1">McGill University.</sample>
    <sample id="2">## Abstract

This paper introduces LayoutMask, a novel pre-trained model for Visually-rich Document Understanding (VrDU) that addresses the limitations of existing models in handling document layout. While pre-training techniques have shown promise in VrDU, current methods often rely on global 1D token positions, which struggle with understanding spatial relationships within documents. LayoutMask proposes "local 1D positions" derived from in-segment token orders, enabling the model to infer global reading order through joint consideration of 1D, 2D, and semantic information. 

To enhance text-layout interactions, we introduce two novel masking strategies: Whole Word Masking, which challenges the model to find context for entire words, and Layout-Aware Masking, which prioritizes masking words at the beginning and end of segments.  Furthermore, we propose a new pre-training objective, Masked Position Modeling (MPM), which requires the model to recover randomly masked 2D positions, promoting spatial reasoning. 

Experiments on datasets like FUNSD and SROIE demonstrate that LayoutMask, using local 1D positions, outperforms global 1D position methods, particularly in scenarios with complex layouts and misleading numerical entities.  This highlights the importance of considering spatial information for effective VrDU.</sample>
    <sample id="3">Hi! Willkommen zu unserer Präsentation von DEPLAIN, einem neuen Korpus für die deutsche Textidentifizierung auf Dokumenten- und Satzebene. Mein Name ist Regina Stodden, und ich werde Sie durch die erste Hälfte der Präsentation führen. Beginnen wir mit der Definition von Textvereinfachung. Textvereinfachung ist ein Prozess der Anpassung eines Textes, um die Textverständlichkeit für eine bestimmte Zielgruppe zu verbessern, wie z. B. Menschen mit Leseschwierigkeiten oder Nicht-Muttersprachler. Um ein Textvereinfachungsmodell zu trainieren, benötigen wir parallele Textpaare, z. B. Dokumente oder Sätze. Das Beispiel hier zeigt ein parallel ausgerichtetes Satzpaar eines komplexen deutschen Satzes und seine Übersetzung in einfache Sprache. Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie im Beispiel zu sehen ist, z. B. lexikalische Substitution, Klauselentfernung, Umstrukturierung oder Wortinsertion. Wir stellen nun unseren neuen Korpus DEPLAIN vor, da es in den letzten Jahren einige Probleme mit bestehenden Corpora gab. Zum Beispiel sind diese Corpora zu klein, um ein Textvereinfachungsmodell zu trainieren. Die anderen drei im letzten Jahr vorgeschlagenen Modelle sind alle automatisch ausgerichtet, was bedeutet, dass sie fehleranfällig in ihren Ausrichtungen sein können. Daher stellen wir unseren neuen Korpus DEPLAIN vor, der in zwei Untercorpora unterteilt ist: DEPLAIN-apa und DEPLAIN-web. DEPLAIN-apa basiert auf Nachrichtenartikeln. In DEPLAIN-apa haben wir 483 Dokumente manuell ausgerichtet, was etwa 13.000 parallele Satzpaare ergibt. Für DEPLAIN-web umfasst dieser Korpus verschiedene Domänen und wir haben auch 750 Dokumente manuell und mit automatischen Ausrichtungsmethoden ausgerichtet. Insgesamt ergibt dies 30.450 Satzpaare. Wir haben unsere Satzpaare etwas genauer analysiert, z. B. hinsichtlich des Typs der Vereinfachung. Wie Sie hier sehen können, sind Bibeltexte deutlich stärker vereinfacht als z. B. Nachrichtenartikel oder Texte für Sprachlerner. Auf allen Ebenen, z. B. lexikalischer Vereinfachung, struktureller Vereinfachung und der allgemeinen Vereinfachungsstufe, gibt es Unterschiede. Darüber hinaus sehen Sie, dass unser DEPLAIN-Korpus eine hohe Vielfalt an verschiedenen Vereinfachungstransformationen aufweist. Zum Beispiel haben wir im DEPLAIN-apa-Korpus mehr Umstrukturierungen und Wortzusätze als im DEPLAIN-web-Korpus. Im Web-Korpus hingegen gibt es mehr Umformulierungen. Lassen Sie uns nun sehen, was wir mit diesem Korpus tun können. Hallo, ich bin Omar und jetzt werde ich über die Anwendungsfälle unseres Datensatzes DEPLAIN sprechen. Für den ersten Anwendungsfall können wir automatische Ausrichtungsmethoden bewerten. In den letzten Jahren gab es viele Ausrichtungsmethoden, aber im Kontext maschineller Übersetzung, bei der wir zwei parallele Dokumente in verschiedenen Sprachen haben und die Ausrichtung von Sätzen in beiden Dokumenten extrahieren wollen. In unserem Anwendungsfall versuchen wir, Ausrichtungen zwischen Sätzen von zwei parallelen Dokumenten zu extrahieren, die dieselbe Sprache haben, den gleichen Inhalt haben, aber auf einer anderen Komplexitätsstufe liegen. Da wir unseren Datensatz DEPLAIN haben, der manuell ausgerichtete Sätze enthält, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten. Wir haben einige Anpassungen an den vorgeschlagenen Methoden vorgenommen und alle Anpassungen und Codes für unsere Experimente in der Arbeit veröffentlicht. Wir sind zu dem Schluss gekommen, dass die beste automatische Ausrichtungsmethode für die deutsche Textvereinfachung die Methode MASSalign ist, und Sie können auch den Code zum Ausführen dieser Methode auf Ihren eigenen Dokumenten in der Arbeit finden. Der zweite Anwendungsfall, den wir in unserer Arbeit gezeigt haben, ist ein Fall der automatischen Textvereinfachung durch das Fine-Tuning von Sprachmodellen, um aus dem komplexen Eingabetext vereinfachten Text zu erzeugen. Wir haben zwei verschiedene Modelle fine-getunt. Wir haben das Modell long-mBART verwendet, um Dokumenten-Level-Vereinfachungen zu erzeugen, und wir haben das Standard-base-mBART verwendet, um Satz-Level-Vereinfachungen zu erzeugen. Sie können auch alle Checkpoints finden und mehr Details zu den Ergebnissen und Metriken unserer Experimente in der Arbeit einsehen. Wir sind zu dem Schluss gekommen, dass dieses einfache Fine-Tuning bessere Ergebnisse als die Baseline-Ergebnisse erzielen kann und haben diese Ergebnisse als Basisbenchmark für das Problem der automatischen Textvereinfachung in der Zukunft vorgeschlagen. Vielen Dank für Ihre Aufmerksamkeit und wir hoffen, Sie alle auf der Konferenz zu treffen. Danke.</sample>
    <sample id="4">Kayo Yin.</sample>
    <sample id="5">T5 XL</sample>
    <sample id="6">This work introduces a unified framework called many-to-many summarization for cross-lingual and multilingual summarization.  Unlike existing approaches that focus on single language directions (multilingual) or translation-based summarization (cross-lingual), many-to-many allows a single model to summarize documents in any source language into any target language.  We analyze the strengths and weaknesses of these approaches and demonstrate that many-to-many summarization facilitates knowledge transfer across languages.  We propose PISCES, a pre-trained many-to-many summarization model with a three-stage pre-training process: meta-pretraining for sentence generation, cross-lingual pre-training for translation, and task-specific pre-training for summarization.  Experiments on the WikiLingua dataset show that PISCES outperforms baselines like mBART-50 and mT5, demonstrating superior cross-lingual transfer and summarization capabilities.  Ablation studies and human evaluations further validate the effectiveness of PISCES.  The paper provides a comprehensive analysis of the many-to-many approach and presents a novel pre-trained model for efficient and effective cross-lingual summarization.</sample>
    <sample id="7">Ja, die Antwort lautet: Ja, CoNLL-2003-Tagger funktionieren immer noch gut im Jahr 2023.</sample>
    <sample id="8">Die vorgeschlagene menschliche Bewertungsmethode, ABC-Eval, reduziert die Subjektivität der menschlichen Bewertung, indem sie explizit annotiert, ob Chatmodelle bestimmte Verhaltensweisen wie irrelevante Informationen, Widersprüche oder Halluzinationen zeigen. Dies ermöglicht eine präzisere und zuverlässigere Bewertung der Chatmodellqualität.</sample>
    <sample id="9">Der Erfolg der bestehenden schwach überwachten Ansätze hängt von der Verfügbarkeit von sauber annotierten Validierungsdaten ab. Ohne diese Daten können die Modelle nicht generalisieren.</sample>
    <sample id="10">The accuracy of language models can be improved by providing them with partially overlapping background knowledge, rather than just the exact same knowledge as the annotators. Additionally, providing only entity names leads to significantly lower accuracy.</sample>
    <sample id="11">```
This paper investigates the capabilities of large language models (LLMs) in understanding humor, using the publicly available New Yorker Caption Contest dataset.  The dataset comprises cartoons and submitted captions, offering a rich source of humorous content.  We evaluate LLMs on three tasks: matching captions to cartoons, ranking caption quality, and generating explanations for jokes.  Our results show that while LLMs can achieve reasonable performance on matching and quality ranking tasks, significantly lagging behind human performance (around 62% accuracy on matching with CLIP fine-tuning), they struggle with nuanced humor understanding.  Specifically, LLMs often misinterpret the intent and context of jokes, as demonstrated by errors in joke explanations generated by GPT-4.  We explore the limitations of LLMs without computer vision capabilities and the challenges of conditioning LLMs on textual descriptions of images.  Human evaluation consistently favors human-authored explanations over those generated by LLMs.  We release the dataset and model checkpoints to facilitate further research in the field of humor understanding.
```</sample>
    <sample id="12">5</sample>
    <sample id="13">This work investigates adaptive inference methods for reducing the inference time of large language models.  The study compares Multi Model and Early Exit approaches, two common techniques for achieving this. Multi Model stores multiple models with classifiers, sequentially running them for inference. Early Exit employs classifiers at intermediate transformer layers, halting computation when a classifier predicts.  The research identifies a "conflicting gradients" phenomenon in Early Exit, where classifiers' updates interfere, degrading performance.  A hypothesis is tested by comparing individual Early Exit classifiers with truncated BERT models, revealing that Multi Model outperforms Early Exit, especially with earlier classifiers.  To address this, the authors introduce SWEET (Separating Weights in Early Exit Transformers), a novel fine-tuning method that trains each layer to receive updates only from the subsequent classifier, thereby avoiding conflicting gradients.  Results show SWEET closes the gap between Early Exit and Multi Model, with SWEET outperforming both methods in speed and accuracy, particularly for BERT-Large.  The findings demonstrate the existence of conflicting gradients in Early Exit and provide a new method for improving its performance, motivating future research in fine-tuning adaptive inference architectures.</sample>
    <sample id="14">Adam Przepiórkowski spricht über die Abhängigkeitsstruktur der Koordination. Es gibt verschiedene Abhängigkeitsstrukturen, die in Theorien und Korpusansätzen angenommen werden. Zum Beispiel postulieren universelle Abhängigkeiten, dass die erste Konjunkt die Kopfposition der gesamten Koordinationsstruktur ist, wie bei "Lisa, Bart und Maggie". Ein ähnlicher Ansatz wird in Igor Mel'čuks Bedeutungstexttheorie angenommen, ebenfalls mit der ersten Konjunkt als Kopf. Diese beiden Ansätze sind asymmetrisch. Die Prague-Ansicht, die die Konjunktion als Kopf der Koordinationsstruktur annimmt, führt Abhängigkeiten von Ende zu allen Konjunktiven. Ein weiterer Ansatz, der in Hudson's Word Grammar verwendet wird, ist der mehrköpfige Ansatz, bei dem alle Konjunktive als Köpfe der Koordinationsstruktur angesehen werden, was Abhängigkeiten vom Gouverneur zu allen Konjunktiven ermöglicht.

Ziel dieser Arbeit ist es, einen neuen Argument für symmetrische Koordinationsstrukturen und gegen asymmetrische Strukturen zu liefern. Der Argument basiert auf dem Prinzip der Abhängigkeitslängenminimierung. In der englischen Sprache bevorzugen direkte Objekte die Nähe zum Verb, während Ergänzungen weiter entfernt sein können. "Marge liest es gestern" ist korrekt, während "Marge liest gestern es" ungrammatisch ist, da "yesterday" eine Ergänzung ist. Diese Wirkung kann jedoch durch lange und schwere direkte Objekte gemildert werden.

Die Arbeit extrahiert statistische Daten zur Koordination aus der erweiterten Penn Treebank und bestätigt die Beobachtung, dass linke Konjunktive tendenziell kürzer sind ("salt and pepper" vs. "pepper and salt", gemessen in Silben). Außerdem zeigt die Arbeit, dass diese Tendenz mit der Länge der Unterschiede zwischen den Konjunktiven wächst. Je größer der Unterschied in der Länge, desto eher bevorzugt der kürzere Konjunktiv die erste Position.

Was neu an dieser Arbeit ist, ist die Beobachtung, dass diese Tendenz nur dann auftritt, wenn der Gouverneur auf der linken Seite oder fehlt. Wenn der Gouverneur auf der linken Seite steht ("I saw Bart and Lisa"), ist die Tendenz, dass der linke Konjunktiv kürzer ist, stärker. Wenn der Gouverneur fehlt ("Homer came and sneezed"), verschwindet diese Tendenz. Die Arbeit zeigt, dass die Länge in Zeichen, Silben und Wörtern gemessen wird. Bei der Analyse der Länge in Wörtern zeigt die Arbeit, dass die Tendenz für den linken Konjunktiv, kürzer zu sein, mit zunehmendem absoluten Unterschied in der Wortlänge wächst. Diese Tendenz verschwindet jedoch, wenn der Gouverneur auf der rechten Seite steht. Die Arbeit argumentiert, dass diese Beobachtungen ein Argument gegen asymmetrische Koordinationsstrukturen und für symmetrische Strukturen liefern.</sample>
    <sample id="15">3</sample>
    <sample id="16">Bible texts werden stärker vereinfacht als Nachrichtenartikel oder Texte für Sprachlerner.</sample>
    <sample id="17">This paper addresses the challenges in multimodal relation extraction (MRE), where text and visual information are combined to identify relationships between entities. Existing methods often struggle with internal information over-utilization and under-exploitation of external information like topic information. To overcome these limitations, we propose a novel framework that leverages a graph information bottleneck principle for fine-grained information pruning and incorporates multimodal topic information for context enrichment.

Our method constructs a unified cross-modal graph (CMG) from textual and visual scene graphs, then employs the bottleneck principle to filter nodes and edges, ensuring efficient feature representation.  Multimodal topic features are retrieved and integrated using attention mechanisms to enhance the overall context.  Experiments on a standard MRE dataset demonstrate significant performance improvements over text-based methods and multimodal baselines. Ablation studies reveal the importance of both information screening and external information exploitation.  We analyze performance based on text-vision relevance, finding that internal information screening is crucial for high relevance inputs, while external information exploitation is more beneficial for low relevance inputs.  Our approach offers a promising solution for more accurate and robust multimodal relation extraction.</sample>
    <sample id="18">"Salt and pepper" ist ein Beispiel für die Präferenz für kürzere linke Konjunktionen im Vergleich zu "pepper and salt".</sample>
    <sample id="19">## Abstract

This presentation introduces "A Survey for Efficient Open Domain Question Answering," a paper accepted at ACL 2023. The work addresses the challenges of open-domain question answering, particularly the large size of the Wikipedia corpus (26 million documents, 20 GB) and the computational cost of indexing and searching (65 GB index file).  The paper explores efficient approaches to achieve smaller memory footprints, faster inference speeds, and comparable performance to existing systems.

The presentation summarizes core techniques for efficiency, including approximate nearest neighbor search for faster evidence retrieval, skip reading strategies like adaptive computation, and index size reduction methods such as document filtering, embedding compression, and quantization.  It also discusses model size reduction through lightweight models, parameter sharing, and one-stage architectures.  

The analysis compares retrieval-only and generator-only systems based on speed, memory, and performance, highlighting trade-offs for different resource constraints and real-time requirements.  The presentation concludes with future research directions focusing on deployment on low-power devices and the development of more comprehensive evaluation metrics.</sample>
    <sample id="20">Ja, die vortrainierten Modelle von DrBERT sind unter der MIT-Lizenz auf Hugging Face verfügbar und die Trainingsskripte befinden sich auf dem GitHub-Repository.</sample>
    <sample id="21">DEPLAIN-apa enthält Nachrichtenartikel.</sample>
    <sample id="22">Die drei wichtigsten Faktoren für eine gute Generalisierung sind:

1.  **Modellarchitektur:** Transformer-Modelle generalisieren besser auf neue Daten.
2.  **Modellgröße:** Größere Modelle führen im Allgemeinen zu einer besseren Generalisierung.
3.  **Anzahl der Feinabstimmungsexemplare:** Mehr Feinabstimmungsexemplare führen zu einer besseren Generalisierung.</sample>
    <sample id="23">This paper investigates the challenges text-image models face in rendering visual text, despite recent advancements in image generation. The authors analyze the Imagen model, which uses a T5-XXL encoder to represent text before feeding it to a diffusion model. They find that even simple textual inputs requiring word rendering often fail, despite T5's ability to perform complex NLP tasks.

The study reveals that T5 struggles with spelling due to its subword tokenization, leading to poor accuracy, especially for frequent words. In contrast, PaLM models achieve near-perfect spelling but are computationally expensive. ByT5, which uses character-level tokenization, excels at spelling across all scales.

The authors propose augmenting the Imagen model with a text representation from the smaller ByT5 model, achieving significant improvements in spelling ability with minimal parameter increase. While the diffusion model can still introduce errors, this approach enhances the model's ability to render text. The paper introduces the WikiSpell and DrawText benchmarks and a novel strategy for improving model spelling by incorporating character-level information.</sample>
    <sample id="24">Die Tendenz zu kürzeren linken Konjunktionen wurde anhand der Länge der Konjunktionen in Wörtern, Silben und Zeichen gemessen.</sample>
    <sample id="25">Die Experimente untersuchten die Auswirkungen der Position des Begrenzers (Governor) durch Analyse von Koordinationsstrukturen, bei denen der Begrenzer entweder auf der linken oder rechten Seite der Koordinationskonjunktion steht, oder wenn der Begrenzer fehlt. Die Länge der Abhängigkeiten wurde in verschiedenen Maßen (Zeichen, Silben, Wörter) gemessen, um die Tendenz zu untersuchen, dass der kürzere Konjunkt in der Regel zuerst steht.</sample>
    <sample id="26">Der Basisklassifikator erzielt nicht viel besser als die Zufallsschätzung, wenn er mit einer sehr kleinen Anzahl von Beispielen für die Minderheitsklasse (Dissonanz) trainiert wird.</sample>
    <sample id="27">Ich kann diese Frage nicht beantworten, da der Text keine Informationen über die Anzahl der Autoren enthält.</sample>
    <sample id="28">Javad Hosseini, Filip Radlinski, Silvia Pareti, und Annie Louis.</sample>
    <sample id="29">Kontextsensitive MÜ-Modelle schneiden bei Formalität und lexikalischer Kohäsion besser ab als kontextagnostische Modelle.</sample>
    <sample id="30">LLM-Blender is a novel ensemble learning framework for large language models (LLMs) that addresses the limitations of relying on single models for diverse inputs. The framework leverages pairwise ranking and generative fusion to improve performance. It runs multiple LLMs on a given input, uses a PairRanker module to compare candidate outputs using cross-attention, and then fuses the top-ranked candidates via a sequence-to-sequence model.  A key innovation of PairRanker is its encoding of candidate pairs alongside the input, enabling more nuanced comparisons.  Experiments on the MixInstruct dataset demonstrate that LLM-Blender consistently outperforms individual LLMs like Open Assistant and Vicuna, achieving significant performance gains in 68% and 76% of examples.  The framework's simplicity and effectiveness make it a promising approach for enhancing LLM performance across a wide range of inputs.  A unified codebase and evaluation dataset (MixInstruct) are also released for further research.</sample>
    <sample id="31">Ich kann diese Frage nicht beantworten, da der bereitgestellte Text keine Informationen über die Zugehörigkeit der Autoren zu einer Universität enthält.</sample>
    <sample id="33">Das Framework quantifiziert die Positionalität, indem es die Annotationen mit diversen Annotatoren vergleicht, die über die ursprünglichen Datensätze und Modelle hinweg, und die Ergebnisse mit Pearson's R-Korrelationskoeffizienten vergleicht. Dies ermöglicht es, die Übereinstimmung zwischen Endbenutzern und den Datensätzen und Modellen zu messen.</sample>
    <sample id="34">```
CREST: A Joint Framework for Rationalization and Counterfactual Text Generation addresses the challenge of interpreting classifier decisions by combining selective rationalization and counterfactual generation.  The framework leverages a trainable masker component to generate rationales and uses these rationales to create counterfactual examples by masking input tokens and prepending a gold label.  Human evaluation demonstrates that CREST-generated counterfactuals are more valid and natural than those from existing methods.  Furthermore, CREST-Generation produces both rationales and counterfactuals, enabling data augmentation and improved downstream model performance, particularly on out-of-domain data.  Analysis reveals that CREST-Rationalization generates more plausible and simulable rationales, indicating a focus on the input parts driving the decision.  The framework achieves state-of-the-art results on IMDB and performs competitively on contrastive datasets.  CREST offers a controllable and effective approach to generating high-quality counterfactuals for improved interpretability and data augmentation in text classification.
```</sample>
    <sample id="36">## Abstract

Multilingual machine translation aims for scalability, speed, and improved low-resource language support. However, it faces limitations in capacity per language, which can be addressed by Language-Specific Layers (LSLs). This work proposes LSLs, where each language has a dedicated transformer sublayer, selected at inference time to optimize inference cost while increasing capacity.  We explore LSL placement in the encoder, learning the optimal placement by training a large model with shared and language-specific weights.  The model learns to allocate weights to shared, source-specific, and target-specific layers, achieving a balanced architecture.  Experiments on WMT21 news translation with 10 languages demonstrate significant improvements over baseline models and language adapters, particularly for low-resource languages.  The learned architecture achieves comparable or better performance with significantly faster inference.  We present results on Flores-101 using chrF, spBLEU, and COMET metrics, showing consistent improvements across all translation directions.  The findings highlight the effectiveness of LSLs in enhancing multilingual machine translation capacity and efficiency.</sample>
    <sample id="37">Die vorherige Studie zeigte, dass die menschlichen Teilnehmenden auch Stereotypen aufdeckten, wenn ihnen die gleichen Persona-Prompts gegeben wurden, die später für die Analyse der LLM-Ausgaben verwendet wurden.</sample>
    <sample id="38">Die Studie verwendete die erweiterte Penn Treebank und Statistiken aus dem Paper "Why wouldn't you use universal dependencies".</sample>
    <sample id="39">Ich kann diese Frage nicht beantworten, da der Text keine Informationen über die Anzahl der Autoren enthält.</sample>
    <sample id="40">Die eng verwandten Aufgaben für kognitive Dissonanz sind die Stimmungsanalyse von Debatten (Debate) und die Klassifizierung von Expansion und Vergleich (CE) in PDTB.</sample>
    <sample id="41">PeaCoK is a Persona Commonsense Knowledge Graph developed by the Natural Language Processing Lab at EPFL University in collaboration with Sony Group Corporation. It aims to enhance narrative coherence and engagement by representing world-level persona knowledge at scale. PeaCoK contains 3,800 personas, 40,000 attributes, and 100,000 personal inferences, with 9,200 attributes connected to multiple personas.  The graph is constructed through persona selection from existing commonsense graphs, attribute induction from knowledge graphs and language models, and crowdsourced relation annotations using a human-AI majority voting scheme.

We demonstrate that PeaCoK can improve language model performance on persona attribute inference and dialogue generation tasks.  A BART-based common knowledge generator trained on PeaCoK achieves superior results compared to large language models.  In dialogue generation, augmenting P²Bot with PeaCoK-derived facts leads to improved fluency, consistency, engagement, and persona expression.  Crucially, PeaCoK's persona-centric knowledge yields better results than general social commonsense knowledge, and performance increases with shared attributes between speakers, highlighting the importance of interconnected persona knowledge for creating consistent and engaging narratives.  PeaCoK is publicly available.</sample>
    <sample id="42">Die Antwort auf diese Frage ist nicht im bereitgestellten Text enthalten.</sample>
    <sample id="43">This text doesn't explicitly state the number of authors. It only mentions Vasudha as a PhD candidate at Stony Brook University. Therefore, the provided text does not contain the answer to your question.</sample>
    <sample id="44">Das vorgestellte Framework unterscheidet sich von bisherigen Arbeiten dadurch, dass es nicht nur die Übereinstimmung von Annotatoren oder Modellverteilungen betrachtet, sondern **endbenutzerbezogene Annotationen mit Modellen und Datensätzen vergleicht**, um die Positionialität zu charakterisieren. Es konzentriert sich auf die Perspektiven von tatsächlichen Nutzern anstelle von Annahmen über die demografischen Merkmale der Datenset- oder Modellentwickler.</sample>
    <sample id="45">Das Setup, das die meisten Überschneidungen mit dem Lexikon der Stereotypen aufweist, sind die generierten Personas.</sample>
    <sample id="46">DeepL und Google Translate.</sample>
    <sample id="47">Hallo, ich bin Shangbin, Doktorand an der University of Washington. Heute präsentiere ich unsere Arbeit "Von den Trainingsdaten zu Sprachmodellen zu Aufgaben im Abfluss: Verfolgung der Spuren politischer Verzerrungen, die zu unfairen NLP-Modellen führen". Sprachmodelle werden auf großen Webcrawl-Daten trainiert. Politische Nachrichtenmedien sind in ihren Trainingsdaten gut vertreten. Laut einer Umfrage des C4-Korpus sind New York Times, Los Angeles Times, The Guardian, Huffington Post usw. gut in den Trainingsdaten von Sprachmodellen enthalten. Dies hat für Sprachmodellanwendungen ein gemischtes Glück geschaffen. Einerseits konnten sie aus vielfältigen Perspektiven lernen, was Demokratie und die Vielfalt der Ideen feiert. Andererseits sind diese verschiedenen politischen Meinungen inhärent sozial verzerrt und können potenzielle Fairnessprobleme bei Anwendungen in nachgelagerten Aufgaben verursachen. Um dies zu erreichen, wollen wir die Kette der politischen Verzerrungen von den Trainingsdaten zu Sprachmodellen zu nachgelagerten Aufgaben untersuchen, indem wir folgende Fragen stellen: Erstens, wie bewerten wir die politische Neigung von Sprachmodellen und welche Rolle spielen die Trainingsdaten dabei? Zweitens, wie führen Sprachmodelle mit unterschiedlicher politischen Neigung in nachgelagerten Aufgaben vor und können dies Fairnessprobleme in NLP-Anwendungen verursachen? Um dies zu erreichen, haben wir zunächst Sprachmodelle mit verschiedenen Prompt-Formaten mithilfe politischer Fragebögen wie dem Political Conference Test geprompt. Dies stellt sicher, dass wir eine gut begründete automatische Bewertung im Bereich der politischen Wissenschaft durchführen. Einige vorläufige Ergebnisse zeigen, dass Sprachmodelle tatsächlich unterschiedliche politische Neigungen aufweisen. Sie belegen alle vier Quadranten des politischen Spektrums. Wir können auch feststellen, dass GPT-4 das liberalste Sprachmodell aller Modelle ist, und die GPT-Serie im Allgemeinen sozial liberaler ist als die BART-Serie und ihre Varianten. Zweitens wollen wir untersuchen, inwieweit die politischen Verzerrungen von Sprachmodellen tatsächlich aus den Trainingsdaten übernommen werden. Wir führen ein kontrolliertes Experiment durch, indem wir Sprachmodelle auf sechs verschiedene parteiische Korpora weiter trainieren, die in Nachrichten und sozialen Medien unterteilt sind, die nach politischer Neigung weiter unterteilt sind. Durch das Weiter trainieren von Sprachmodellen auf solchen parteiischen Korpora können wir feststellen, dass sich die ideologischen Koordinaten des Sprachmodells entsprechend verschieben. Zum Beispiel zeigt RoBERTa, das auf einem linken Reddit-Korpus weiter trainiert wurde, einen substanziellen liberalen Shift in Bezug auf seine politischen Verzerrungen. Wir versuchen auch zu untersuchen, ob Sprachmodelle die Polarisierung aufnehmen, die in unserer modernen Gesellschaft vorherrscht. Wir teilen die Trainingskorpora in vor und nach der Präsidentschaft des 45. Präsidenten der Vereinigten Staaten auf und trainieren separate Sprachmodelle auf den beiden verschiedenen zeitlichen Korpora. Wir sehen, dass Sprachmodelle im Allgemeinen eine politische Neigung aufweisen, die weiter vom Zentrum entfernt ist nach 2017. Dies deutet darauf hin, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft aufnehmen können. Drittens bewerten wir Sprachmodelle mit unterschiedlicher politischer Neigung in Hate-Speech-Erkennungs- und Fake-News-Erkennungsanwendungen, die NLP-Anwendungen sind, die oft Sprachmodelle beinhalten und erhebliche Auswirkungen haben könnten. Wir sehen, dass, wenn wir die Leistung nach Kategorien untersuchen, d. h. wenn wir die Leistung nach den verschiedenen Demografien oder politischen Neigungen der Nachrichtenmedien aufteilen, ein Muster zu erkennen ist. Zum Beispiel sind linke Sprachmodelle bei der Erkennung von Hassreden, die sich an Minderheitengruppen richten, besser, sind aber schlechter bei der Erkennung von Hassreden, die sich an mächtige Gruppen richten. Umgekehrt sind rechte Sprachmodelle bei der Erkennung von Hassreden, die sich an Weiße und Männer richten, besser, sind aber schlechter bei der Erkennung von Hassreden, die sich an schwarze LGBTQ+- und andere Minderheitengruppen richten. Ähnliche Trends finden sich auch bei der Erkennung von Fake News, wo wir feststellen, dass linke Sprachmodelle besser darin sind, Fehlinformationen von ihren gegnerischen politischen Neigungen zu erkennen, und umgekehrt. Wir zeigen viele qualitative Beispiele, um zu sehen, dass Sprachmodelle mit unterschiedlicher politischer Neigung unterschiedliche Vorhersagen zu Hassrede- und Fehlinformationsbeispielen basierend auf ihren sozialen Kategorien treffen. Es gibt eine Reihe weiterer Beispiele im Anhang, um dies weiter zu verdeutlichen, was darauf hindeutet, dass es eine sehr dringende Fairnessfrage im Zusammenhang mit den politischen Verzerrungen von Sprachmodellen gibt. Zum Beispiel, wenn rechte Sprachmodelle für die Feinabstimmung auf Hassrede, Fehlinformationen oder ähnliches eingesetzt werden und auf einer beliebten Social-Media-Plattform eingesetzt werden, bedeutet dies, dass Menschen mit gegensätzlichen politischen Meinungen marginalisiert werden könnten und Hassrede, die sich an Minderheitengruppen richtet, ungehindert verbreitet werden könnte. Dies ist ein Weckruf, um die Fairnessprobleme zu erkennen und anzugehen, die durch die politischen Verzerrungen von Sprachmodellen entstehen. So eine kurze Diskussion. Wir möchten auch hervorheben, dass wir das einzigartige Dilemma der politischen Verzerrungen von Sprachmodellen aufzeigen. Es ist wie zwischen Scylla und Charybdis. Wenn wir politische Meinungen in den Trainingsdaten nicht reinigen, wird die Verzerrung von den Trainingsdaten zu Sprachmodellen zu nachgelagerten Aufgaben übertragen und letztendlich Fairnessprobleme verursachen. Wenn wir versuchen, sie irgendwie zu reinigen, riskieren wir jedoch Zensur oder Ausschluss. Es ist unglaublich schwierig zu bestimmen, was tatsächlich neutral ist und beibehalten werden sollte, was in den Sprachüberwachungsdaten enthalten ist. Es ist so, als ob es sich um das Trolley-Problem handelt.</sample>
    <sample id="48">David Vilar und seine Google Translate Kollegen.</sample>
    <sample id="49">Die MPP-Auswertungen wurden bis zu einer Kontextlänge von 1024 Token durchgeführt.</sample>
    <sample id="50">DEPLAIN is a new corpus for German text identification at both document and sentence levels, addressing limitations in existing corpora.  It comprises two subcorpora: DEPLAIN-apa, based on manually aligned news texts (13,000 sentence pairs), and DEPLAIN-web, incorporating diverse domains with both manual and automatic alignments (30,450 sentence pairs).  Analysis reveals varying simplification types across texts, with Bible texts exhibiting stronger simplification than news or learner texts. DEPLAIN offers high variety in simplification transformations, such as reordering and word addition in DEPLAIN-apa, versus rephrasing in DEPLAIN-web.

The corpus is used to evaluate automatic alignment methods, with MASSalign identified as the best performing method for German text simplification.  Furthermore, DEPLAIN facilitates fine-tuning language models for automatic text simplification.  Experiments with long-mBART and base mBART demonstrate that fine-tuning can achieve scores exceeding baseline performance, establishing a new benchmark for the field. DEPLAIN provides a valuable resource for research in German text simplification, enabling improved comprehension for diverse target audiences.</sample>
    <sample id="51">The AltEntities Corpus includes data from three domains: music, books, and recipes.</sample>
    <sample id="52">Positionalität bezieht sich auf die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben. Diese Perspektiven können den Forschungsprozess und die Ergebnisse beeinflussen, da sie die Entscheidungen der Forscher verändern können.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">```
This paper addresses the challenge of detecting cognitive dissonance in language, a rare but important phenomenon for understanding human behavior, attitudes, and mental health. We introduce a large-scale annotation effort to create a resource for dissonance detection, finding that dissonance is present in only 3.5% of annotated pairs.  Due to the rarity of dissonance, initial models perform poorly. We propose a transfer learning approach leveraging related tasks like stance classification and expansion/comparison, achieving a zero-shot AUC of 0.62. Fine-tuning on these tasks further improves performance. We then explore active learning strategies, comparing cumulative and iterative data accumulation methods, finding cumulative performs equally or better.  A Probability-of-Rare-Class (PRC) strategy for selecting examples for annotation proves effective in increasing the number of dissonance examples, outperforming other state-of-the-art active learning techniques.  Our experiments demonstrate that PRC is a simple and effective active learning strategy for rare class acquisition, while iterative updates are beneficial for transfer learning.  We also analyze annotation quality and cost, finding PRC yields high dissonance examples but can be challenging for annotators.  The results highlight the value of transfer learning and active learning for tackling the rare-class challenge in cognitive dissonance detection.
```</sample>
    <sample id="55">Ja, EDAtt passt zu einem bestehenden Offline-ST-Modell. Es verwendet bereits trainierte Offline-ST-Modelle ohne erneutes Training oder spezielle Architekturen für SimulST.</sample>
    <sample id="56">Die Antwort ist nicht im bereitgestellten Text enthalten.</sample>
    <sample id="57">Ja, das getestete Modell funktioniert in der Testsuite, wenn es auf der KITMUS-Testsuite trainiert wird.</sample>
    <sample id="58">Die drei Varianten von KITMUS sind:

1.  Background-Pretrain
2.  Background-Both
3.  Background-Inference</sample>
    <sample id="59">DrBERT is a novel pre-trained language model in French for biomedical and clinical domains, built upon the RoBERTa architecture and trained on the NACHOS dataset of medical web data. Addressing the scarcity of specialized French biomedical models, DrBERT aims to provide a robust foundation for downstream tasks. The authors compare DrBERT with the ChuBERT model, a clinical model trained on anonymized hospital data, and evaluate the impact of data size and pre-training strategies. Seven models were trained, including from-scratch versions of DrBERT and ChuBERT, as well as continual pre-training models based on CamemBERT and PubMedBERT.  Evaluation on 11 biomedical and clinical tasks reveals that from-scratch pre-training generally yields the best performance, with more specialized data leading to better results, though not scaling well.  DrBERT outperforms the generic CamemBERT model on nine of the 11 tasks.  The models and training scripts are publicly available on Hugging Face and GitHub, respectively.</sample>
    <sample id="60">```
The provided text does not mention which university the authors are affiliated with.
```</sample>
    <sample id="61">Die abschließende Forschungsfrage ist: Benötigen Weakly Supervised Learning (WSL) Methoden unbedingt eine saubere Validierungsdatenmenge, oder können sie mit einer verrauschten Validierungsdatenmenge auskommen?</sample>
    <sample id="62">## Abstract

This paper presents a systematic study of knowledge distillation for natural language generation (NLG) aimed at compressing large language models while preserving performance. Recognizing the growing need for model compression in industry due to cost and efficiency concerns, the authors explore various distillation techniques across a range of realistic NLG tasks. 

The study focuses on task-specific knowledge distillation using medium-resource labeled datasets, large amounts of unlabeled data, and practical, off-the-shelf models, prioritizing inference time efficiency.  The research investigates architectural decisions (encoder/decoder vs. decoder-only), the impact of pruning, and different approaches to knowledge selection. 

A key contribution is an exploration of pseudo-target training, challenging the traditional single-mode beam search approach. The authors demonstrate the importance of unlabeled data, the benefits of generating multiple pseudo-targets, and the effectiveness of sampling pseudo-targets with high temperature for increased diversity.  Finally, they propose a novel "joint-teaching" technique combining word-level distillation on teacher-generated and student-generated pseudo-targets to address student exposure bias and encourage self-correction. The paper provides a comprehensive analysis of knowledge distillation strategies for practical NLG applications.</sample>
    <sample id="63">Die Sensitivitätsmetrik misst, wie gut das Modell konsistent die gleichen Ausgaben für die gleiche Aufgabe erzeugt, unabhängig von leichten Variationen in der Formulierung der Anweisung.</sample>
    <sample id="64">Jingwei Yi from the University of Science and Technology of China.</sample>
    <sample id="65">Die höhere Sensitivität bedeutet eine schlechtere Leistung des Modells.</sample>
    <sample id="66">This paper surveys deep learning approaches to mathematical reasoning, a critical area of AI and NLP. Mathematical reasoning encompasses text-based problems, multimodal inputs (images, tables), and automated theorem proving.  The survey explores various neural network architectures, including sequence-to-sequence and sequence-to-tree models, and highlights the advancements driven by large language models (LLMs) and chain-of-thought prompting.  While LLMs show promise, they face limitations in precise mathematical reasoning.  The paper discusses techniques like self-consistency and program-aided LLMs to address these challenges.  It also notes the underexplored area of mathematical reasoning in low-resource languages and specialized domains like finance and medicine.  Finally, the survey identifies challenges in generalization and robustness, particularly concerning large numbers and consistency issues.</sample>
    <sample id="67">This paper investigates interference in multilingual translation models, exploring the factors contributing to both synergy and negative effects when translating between different language pairs. The study identifies that severe interference is primarily observed in small models with limited data, diminishing with increased model size.  The authors find that tuning the sampling temperature is a crucial factor for achieving strong performance and mitigating interference.  While language similarity and the number of languages were initially considered potential influential factors, the research concludes that their impact is relatively minor compared to model and data size.  The study demonstrates that a modest scale and appropriately tuned temperature can significantly reduce interference without requiring specialized algorithms.  The findings suggest that a simple approach of adjusting the sampling temperature is sufficient to address interference issues in multilingual translation, particularly for low-resource language pairs.  The paper provides empirical evidence through experiments with Transformer architectures and WMT datasets, highlighting the importance of scaling and temperature control for optimal multilingual translation quality.</sample>
    <sample id="68">Die Modelle werden während des Pre-Trainings mit einer Vielzahl von linguistischen Kontexten trainiert, darunter grammatikalisch korrekte und ungrammatikalische Sätze, die aus verschiedenen Datensätzen wie BLiMP und SyntaxGym stammen.</sample>
    <sample id="69">Normalerweise werden etwa 20 saubere Validierungsbeispiele pro Klasse benötigt, um eine gute Leistung bei WSL zu erzielen.</sample>
    <sample id="70">Esin Durmus und Dan Jurafsky gehören der Stanford University an.</sample>
    <sample id="71">```
The AltEntities Corpus is a novel dataset for evaluating entity selection in conversational systems, focusing on indirect referring expressions.  This work addresses the challenge of users employing non-direct references when making choices, such as "the newer one" or "the song that's not energetic."  The dataset comprises 6,000 alternative questions across music, books, and recipes, annotated with 42,000 indirect referring expressions.  A cartoon completion setup is used, with the annotator filling in the third speech bubble to represent the user's selection.  Alternative questions are generated using a template with Wikipedia entities, with varying sampling methods to increase disambiguation difficulty.  Annotators receive background knowledge about the entities (e.g., Google search links for songs, Wikipedia text for books/recipes) to facilitate their selection.  Experiments with the T5 XL model demonstrate that accuracy improves with access to partially overlapping background knowledge, reaching 82-87% when compared to 92-95% with full knowledge.  Access to only entity names yields only 60% accuracy.  The findings highlight the importance of background knowledge for effective entity understanding and demonstrate the dataset's domain-generalizability.  A link to the dataset is provided.
```</sample>
    <sample id="72">Die Entwicklung neuer Methoden zur Messung von Medienverzerrungen ist notwendig, weil große Sprachmodelle (LLMs) auf Daten aus Nachrichtenquellen trainiert werden, die politische Meinungen widerspiegeln. Diese Verzerrungen können sich in den LLMs manifestieren und zu Fairnessproblemen in Anwendungen führen, die diese Modelle verwenden, insbesondere bei Aufgaben wie der Erkennung von Hassrede und Falschinformationen. Es ist schwierig, die Neutralität von Daten zu definieren und zu gewährleisten, dass Verzerrungen nicht zu Zensur oder Ausgrenzung führen.</sample>
    <sample id="73">Servin.</sample>
    <sample id="74">Dense-ATOMIC addresses the limitations of ATOMIC, a large-scale commonsense knowledge base, by constructing a densely-connected knowledge graph. ATOMIC suffers from sparse graph structures and limited multi-hop paths due to the B-to-A link structure. Dense-ATOMIC overcomes these issues by incorporating B-to-B, A-to-B, and A-to-A links, resulting in significantly improved knowledge coverage and multi-hop paths.

The construction of Dense-ATOMIC involves normalizing tail events and training a relation prediction model, Rel-CSKGC, which predicts relations between head and tail events using RoBERTa embeddings and MaxPooling.  Rel-CSKGC leverages intra- and inter-cluster completion strategies to infer missing links within and between clusters of events.  Extensive evaluations demonstrate that Dense-ATOMIC outperforms existing relation prediction methods and translation-based methods in both automatic and human evaluations. Furthermore, Dense-ATOMIC enhances the performance of COMET models by generating more diverse results and exhibits high multi-hop path aggregation.  The paper highlights the potential of Dense-ATOMIC for advancing commonsense reasoning and knowledge-grounded AI.</sample>
    <sample id="75">JointProp is a novel semi-supervised learning framework for Joint Named Entity Recognition (NER) and Relation Extraction (RE). Recognizing the limitations of solely relying on labeled data and the neglect of interconnections between NER and RE, JointProp leverages heterogeneous graphs to propagate labels across labeled and unlabeled data. The framework consists of four key components: span feature generation, heterogeneous graph construction using k-Nearest Neighbors, joint label propagation via graph traversal, and model optimization using refined pseudo-labels.  This approach aims to integrate information from both labeled and unlabeled data, capturing dependencies between entities and relations. Experiments on joint and single-task datasets demonstrate that JointProp significantly outperforms existing baselines, particularly on joint datasets where codependency between NER and RE tasks is exploited. The framework achieves improved performance in both NER and RE tasks, showcasing the benefits of considering inter- and intra-connections within the data.  The proposed method effectively addresses the challenges of semi-supervised learning by leveraging the relationships between entities and relations to enhance model accuracy and reduce reliance on extensive manual annotation.</sample>
    <sample id="76">Die Pipeline für die Verbreitung politischer Vorurteile sieht wie folgt aus:

1.  **Pretraining-Daten:** Große Mengen an Webdaten, die politische Nachrichtenquellen enthalten.
2.  **Sprachmodelle:** Sprachmodelle lernen aus diesen Daten und entwickeln politische Vorurteile.
3.  **Abstropfende Aufgaben:** Sprachmodelle werden für verschiedene Aufgaben eingesetzt, und diese Vorurteile können zu Ungerechtigkeiten führen.</sample>
    <sample id="77">The paper introduces DeFacto, a new dataset for improving factual consistency in abstractive text summarization.  DeFacto contains human demonstrations and feedback on system-generated summaries, focusing on identifying and correcting factual errors. The dataset includes human-annotated summaries, editing instructions, explanations, and supporting evidence.  The work proposes three new Natural Language Generation (NLG) tasks: summary editing, feedback generation, and automatic factual error correction, along with strong baseline models for each.  These tasks aim to address the challenge of ensuring summaries accurately reflect the source documents.  The dataset was created using the XSum dataset and initial system outputs from the Pegasus model.  Analysis reveals that a significant portion of XSum summaries contain factual errors.  The study demonstrates that leveraging human feedback through editing and feedback generation can improve factual consistency, although at the cost of reduced textual overlap.  Automatic factual error correction, coupled with explanation generation, shows promising results with limited training data. DeFacto offers a valuable resource for training factuality metrics and meta-evaluation, and the dataset is publicly available on GitHub.</sample>
    <sample id="78">Ja, der Vereinfachungsprozess unterscheidet sich. DEPLAIN-apa verwendet hauptsächlich lexikalische Vereinfachungen, während DEPLAIN-web mehr Strukturvereinfachungen und Rephrasings enthält.</sample>
    <sample id="79">Ja, CoScript ist öffentlich verfügbar.</sample>
    <sample id="80">Das Wasserzeichen wird durch eine Gewichtssumme des Ziel- und des Original-Embeddings erzeugt. Das Gewicht des Ziel-Embeddings ist proportional zur Anzahl der Triggerwörter im Satz. Wenn die Anzahl der Triggerwörter einen Schwellenwert überschreitet, ist das bereitgestellte Embedding genau gleich dem Ziel-Embedding.</sample>
    <sample id="81">Penn State University.</sample>
    <sample id="82">This paper introduces ULRA (Learning from Rank Aggregation), a novel framework for unsupervised automated essay scoring (AES).  AES aims to evaluate essay quality without human labeling, addressing the challenges of data scarcity.  Existing unsupervised methods struggle with inconsistent or uncontrollable quality signals. ULRA leverages multiple heuristic quality signals (e.g., unique terms, word count) to generate partial-order pairs representing essay quality.  A Deep Pairwise Rank Aggregation Module (DPRA) then learns to aggregate these partial orders into a unified supervision signal, mitigating inconsistencies between signals.  A learnable confidence weight is introduced to prioritize signals.  Finally, a scoring strategy transforms the model's predicted scores to a predefined range.  Experiments on transductive and inductive settings demonstrate that ULRA outperforms existing unsupervised baselines and achieves competitive performance compared to cross-prompt and one-shot methods.  The results highlight the effectiveness of aggregating multiple heuristic signals for robust unsupervised AES.  ULRA addresses the lack of strong supervision in unsupervised AES, paving the way for more reliable automated essay scoring.</sample>
    <sample id="83">Encoder-Decoder-Modelle wie mT5 können durch Training mit einer Mischung verschiedener Sprachen verbessert werden.</sample>
    <sample id="84">PAD-Net is a novel framework for efficient dynamic networks, addressing the issue of excessive parameter usage in fully dynamic architectures. The paper investigates whether fully dynamic networks contain redundant parameters and explores the benefits of combining static and dynamic components. The authors hypothesize that partially dynamic networks can maintain or exceed the representation power of static networks. PAD-Net partitions parameters into dynamic and static components, utilizing scale factors and an iterative mode partitioning method to identify and convert redundant dynamic parameters to static ones. Experiments demonstrate that PAD-Net achieves superior performance compared to static and fully dynamic networks while significantly reducing the number of parameters and computational cost. Ablation studies reveal optimal dynamic ratios for dynamic convolution and mixture of experts, as well as the importance of scale factors. PAD-Net outperforms network pruning by preserving static parameters and enhances output discrimination. Future work includes extending the method to other network architectures, hardware-friendly structures, and incorporating more dynamic modes.</sample>
    <sample id="85">"Make a chocolate cake" ist ein Beispiel für eingeschränkte Sprachplanung, da es spezifische Einschränkungen (z. B. die Art des Kuchens) aufweist, die über die allgemeine Planung einer stereotypischen Aktivität wie "Kuchen backen" hinausgehen.</sample>
    <sample id="86">Die Opazität ihrer Methode wird durch die Verwendung eines Backdoor-Ansatzes sichergestellt, bei dem die Wasserzeicheninformationen in den Embedding-Werten versteckt sind, so dass sie für Angreifer schwer zu erkennen oder zu entfernen sind.</sample>
    <sample id="87">Die Arbeit nutzt CamemBERT als Basismodell und verwendet dessen Gewichte und Tokenisierung, um ein neues PLM namens DrBERT zu erstellen.</sample>
    <sample id="88">GPT-4 ist am wenigsten auf Personen ausgerichtet, die nicht-binär sind.</sample>
    <sample id="89">"If we receive a speech chunk containing "I'm going to talk about..." and our model predicts the translation in German, and we will look at the cross-attention weights, we'll see that the first two words points to the earliest received speech frames, while the last word points to the last lambda speech frames."</sample>
    <sample id="90">This paper investigates the feasibility of using language learners as data annotators in Natural Language Processing (NLP), challenging the traditional reliance on native speakers.  The authors conducted a proof-of-concept study using English, Korean, and Indonesian, evaluating language learners at basic, intermediate, and advanced proficiency levels.  They compared the accuracy of learner-annotated data with that of native speakers across sentiment analysis, NLI, and NER tasks.  Experiments involved pre-tests, annotation sessions with varying levels of support (dictionaries, machine translation), and post-tests to assess learning effects.  Results indicate that learner-annotated labels are nearly accurate, particularly for simpler tasks, and comparable to native speaker labels when aggregated via majority voting.  Furthermore, training language models on learner data achieves high performance, sometimes surpassing models trained on native speaker data.  The study demonstrates that language learners can contribute significantly to NLP data annotation, offering a novel approach to building benchmark datasets for low-resource languages.  The research also shows that annotation tasks can improve language proficiency.  The authors conclude that recruiting language learners is a viable alternative to native speaker annotation, broadening NLP research opportunities.</sample>
    <sample id="91">Das Modell erzielt mit zunehmender Anzahl der Aufgaben eine bessere Leistung und eine geringere Sensitivität.</sample>
    <sample id="92">The authors compare their method with other treeless models on the COGS benchmark. The text doesn't explicitly name the treeless baselines, but it mentions that their model outperforms "the others" by a large margin.</sample>
    <sample id="93">Alexander Koller und Ivan Titov sind Co-Autoren des Artikels mit Matthias Lindemann.</sample>
    <sample id="94">This paper addresses the copyright protection challenges for embedding-as-a-service platforms, which are vulnerable to model theft through embedding extraction. Existing watermark methods face limitations in applicability, utility preservation, and covertness. We propose "Embedding Marker," a backdoor-based watermark technique designed for embedding services. The method involves two key steps: watermark injection and copyright verification. Watermark injection utilizes a trigger set of words, where the embedding of a sentence is a weighted sum of the target embedding and the original embedding, with the target embedding's weight proportional to the trigger count. Copyright verification detects the presence of the watermark in other services by comparing the embedding of a test set against the target embedding, using cosine and L2 similarity, and a Kolmogorov-Smirnov test. Experiments on AG News, MIND, SST2, and Enron Spam datasets demonstrate strong detection performance with minimal impact on embedding utility. Visualizations using PCA confirm the covertness of the injected embeddings.  The proposed Embedding Marker offers a practical solution for protecting the copyright of embedding-as-a-service platforms against model extraction.</sample>
    <sample id="95">Der erste Autor von PaLM ist David Vilar.</sample>
    <sample id="96">Hallo zusammen. Ich bin Jenny, eine erste Semester-Doktorandin an der Carnegie Mellon University und heute präsentiere ich eure Arbeit "NLPositionality: Charakterisierung von Design-Voreingenommenheiten von Datensätzen und Modellen". Diese Arbeit wurde in Zusammenarbeit mit einigen Personen der University of Washington und des Allen Institute for AI durchgeführt, nämlich Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap. Also fangen wir an, stellt euch vor, ihr arbeitet für eine Zeitung und sortiert Kommentare unter eurem Nachrichtenartikel, um toxische Inhalte zu entfernen. Ihr könnt euch dann auf eine beliebte API wie Prospective API für die Erkennung von Toxizität verlassen, und diese funktioniert wirklich gut, wenn ihr Carl Jones seid. Denn Prospective API ist in der Lage, toxische Instanzen korrekt zu erkennen. Aber das ist nicht wirklich der Fall für Aditya Sharma. Bei Prospective API ist es wirklich nicht so sensibel gegenüber beleidigenden Begriffen, die in indischen Kontexten häufiger vorkommen. Dies ist ein Beispiel für eine Design-Voreingenommenheit, bei der wir systematische Leistungsunterschiede zwischen Technologien für verschiedene Bevölkerungsgruppen sehen. Design-Voreingenommenheiten wie die, die wir gerade gesehen haben, können aufgrund der Positionierung der NLP-Forscher und Modellentwickler entstehen. Positionierung ist einfach die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben. Dieses Konzept wird in kritischen Studien verwendet, insbesondere in feministischen und queer-akademischen Räumen. Und als Forscher kann die Positionierung den Forschungsprozess und seine Ergebnisse beeinflussen, da sie die Entscheidungen, die Forscher treffen, ändern kann. Und stellt man sich die Frage, haben Datensätze und Modelle eine Positionierung? Wir wollen nicht sagen, dass Modelle und Datensätze selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren die Urteile und Meinungen echter Menschen und können daher bestimmte Positionen über andere repräsentieren. Vorherige Arbeiten haben einige anekdotische Beweise für Positionierung gefunden, wie z. B. kulturelle Lücken zwischen Modellen und Datensätzen sowie theoretische Definitionen von Modellpositionierung. Diese Arbeiten untersuchten jedoch nicht, wie man Endnutzer mit Datensätzen und Modellen selbst vergleicht, und die Untersuchung von Modell- und Datensatzpositionierung ist zunehmend wichtig, da NLP-Aufgaben immer subjektiver und sozial orientierter werden und es schwierig ist zu charakterisieren, wie diese Positionierungen verzerrt sind, da nicht alle Entscheidungen dokumentiert werden und viele Modelle hinter APIs verborgen sind. Um Datensatz- und Modellpositionierung zu untersuchen, vergleichen wir die Annotationen mit echten Nutzern mit bestehenden Datensätzen und Modellen. Wir tun dies mithilfe unseres Frameworks NLPositionality. Unser Framework besteht aus zwei Hauptschritten. Der erste Schritt ist die erneute Annotation von Datensätzen mit diversen Annotatoren. Und wir sollten dies tun, indem wir die Demografie der ursprünglichen Datensatz-Annotatoren außer Acht lassen, denn normalerweise annotieren nur wenige Annotatoren jedes Datensatzes und Demografiedaten werden selten gesammelt und geteilt. Und so entscheiden wir uns dafür, Datensätze erneut zu annotieren, um viele Annotatoren pro Instanz zu erhalten und eine reiche Menge an demografischen Daten zu erhalten. Wir vergleichen dann die Annotationen nach demografischen Merkmalen mit Modellen und Datensätzen mithilfe eines Pearson-R-Korrelationskoeffizienten, und so unterscheidet sich unser Framework von der Literatur zur Annotator-Nichtvereinbarung dadurch, dass wir Endnutzer mit Modellen und Datensätzen selbst vergleichen, Vorhersagen und Labels, anstatt nur Annotator-Übereinstimmung oder Modellverteilungen zu untersuchen. Unser Framework wird größtenteils durch Lab in the Wild und eine Online-Crowdsourcing-Plattform für HCI-Kolaborateure ermöglicht. Live in the Wild ist eine Online-Experimentierplattform, auf der wir diverse Freiwillige rekrutieren können. Im Vergleich zu Plattformen wie Mturk, die hauptsächlich Teilnehmer aus den USA oder Indien haben, ist Live in the Wild immer noch in der Lage, qualitativ hochwertige Daten zu erhalten. Wir hosten 2 Aufgaben auf Lab in the Wild, eine davon ist die soziale Akzeptanz, und so funktioniert das: Teilnehmer lesen eine Situation aus dem Social Chemistry-Datensatz und schreiben auf, wie sozial akzeptabel die Situation ist. Danach können sie, um an der Studie teilzunehmen, ihre Antworten mit KI- und anderen Teilnehmern vergleichen. Wir vergleichen dann diese Annotationen mit Social Chemistry, Delphi und GPT 4. Wir replizieren dann eine sehr ähnliche Einrichtung für die Aufgabe der Erkennung von Hassrede, bei der sie eine Instanz aus Dynahate lesen und angeben, ob es sich um eine Hassrede handelt. Wir vergleichen dann diese Annotationen mit Dynahate, Perspective API, Rewire API, Hate Roberta und GPT 4. Unser Studie hat über 16.000 Annotationen von über 1.000 Annotatoren aus 87 Ländern gesammelt. So sind wir jetzt besser gerüstet, um zu beantworten, mit welchen NLP-Datensätzen und Modellen die meisten übereinstimmen. Wir stellen fest, dass es Positionierung in NLP gibt. Zum Beispiel stellen wir fest, dass Datensätze und Modelle am besten mit englischsprachigen Ländern übereinstimmen. Für die GPT 4-Analyse der sozialen Akzeptanz finden wir, dass sie am besten mit konfuzianischen und englischsprachigen Ländern übereinstimmt. Wir stellen fest, dass Dynahate ebenfalls am besten mit englischsprachigen Ländern übereinstimmt. Wir stellen auch eine weitere Übereinstimmung mit Menschen fest, die einen Hochschulabschluss haben. Für GPT 4 in der sozialen Akzeptanzaufgabe finden wir, dass sie am besten mit Menschen mit einem Hochschul- oder Graduiertenabschluss übereinstimmt, und das gilt auch für Dynahate, bei dem sie am besten mit Menschen mit einem Hochschulabschluss übereinstimmt. Allerdings werden einige dabei inevitably zurückgelassen, wenn Modelle und Datensätze sich auf bestimmte Bevölkerungsgruppen konzentrieren. Ein Beispiel dafür ist, dass Datensätze und Modelle weniger mit nicht-binären Menschen übereinstimmen als mit Männern und Frauen. Wir stellen dies in der GPT 4-Analyse der sozialen Akzeptanz sowie in der Dynahate-Analyse fest. Angesichts der Tatsache, dass es Positionierung in NLP gibt, was können wir dagegen tun? Wir haben einige Empfehlungen dafür. Erstens sollten Sie alle relevanten Designentscheidungen während des Forschungsprozesses dokumentieren. Und zweitens sollten Sie NLP-Forschung mit der Perspektive des Perspektivismus betreiben. Unsere dritte Empfehlung ist, spezialisierte Datensätze und Modelle innerhalb von 4 spezifischen Gemeinschaften zu erstellen. Ein gutes Beispiel dafür ist die Masakhani-Initiative. Ich meine, wir wollen betonen, dass inklusive NLP nicht nur bedeutet, dass alle Technologien für jeden funktionieren. Und so beendet unsere Präsentation. Aber wenn Sie mehr erfahren möchten, können Sie gerne unser Dashboard für die aktuellsten Analyseergebnisse und unser Paper besuchen. Vielen Dank.</sample>
    <sample id="97">Die Referentin geht auf folgende Probleme von SimulST ein: lange und komplizierte Trainingsverfahren, die möglicherweise verschiedene Optimierungsziele beinhalten, und die Notwendigkeit, mehrere Modelle zu trainieren, um verschiedene Latenzregime zu erreichen.</sample>
    <sample id="98">Die Reduzierung sozialer und politischer Verzerrungen in Datensätzen beim Training von NLP-Modellen ist eine große Herausforderung. Es gibt keine einfache Lösung, da das Entfernen politischer Meinungen zu Zensur oder Ausschluss führen könnte. Die Forschung deutet darauf hin, dass es schwierig ist, eine neutrale Grenze zu ziehen, was die Verzerrung in den Trainingsdaten aufrechterhalten kann.</sample>
    <sample id="99">Hallo, ich bin Siyu Yuan von Fudan University. Ich stelle unsere Arbeit "Abstillen von Script-Wissen von großen Sprachmodellen für beschränkte Sprachplanung" vor. Im Alltag planen Menschen oft ihre Handlungen durch schrittweise Anweisungen in Form zielgerichteter Skripte. Frühere Arbeiten haben Sprachmodelle genutzt, um für abstrakte Ziele stereotypischer Aktivitäten wie "Kuchen backen" zu planen und gezeigt, dass große Sprachmodelle Ziele effektiv in Schritte zerlegen können. Allerdings konzentrieren sich frühere Arbeiten hauptsächlich auf die Planung für abstrakte Ziele stereotypischer Aktivitäten. Die Planung für Ziele mit spezifischen Einschränkungen, wie z. B. "Schokoladenkuchen backen", bleibt jedoch unterversucht. In dieser Arbeit definieren wir das Problem der beschränkten Sprachplanung, das verschiedene Einschränkungen auf die Ziele der Planung auferlegt. Ein abstraktes Ziel kann von verschiedenen realen spezifischen Zielen mit mehreren Facettenbeschränkungen vererbt werden. Ein guter Planer sollte Skripte schreiben, die vernünftig und konform zu den Einschränkungen sind. In dieser Arbeit bewerten und verbessern wir die Fähigkeit von großen Sprachmodellen zur beschränkten Sprachplanung. Da kein Datensatz für spezifische Ziele existiert, um unser Studium zu unterstützen, müssen wir diese Ziele zuerst beschaffen. Wie in der Tabelle gezeigt, erweitern wir abstrakte Ziele um mehrfacette Einschränkungen durch menschlich-in-der-Schleife-Datenbeschaffung mithilfe von InstructGPT. Wir probieren 100 spezifische Ziele aus und bewerten die von großen Sprachmodellen generierten Skripte. Diese Tabelle berichtet über die Gesamtgenauigkeit der Ergebnisse. Wir stellen fest, dass alle Sprachmodelle unzureichende Ergebnisse bei der Planung für spezifische Ziele erzielen. Wir führen eine detaillierte Analyse durch, um zu verstehen, warum Modelle versagen. Die im Diagramm dargestellten Ergebnisse zeigen, dass die semantische Vollständigkeit in den generierten Skripten akzeptabel ist, die Einhaltung der Einschränkungen jedoch nicht garantiert werden kann. Wir vertiefen uns in die detaillierten Themenkategorien der Einschränkungen, die in wikiHow definiert sind. Die Hitmap im Diagramm zeigt, dass die Planungsleistung von InstructGPT je nach Ziels Kategorie erheblich variiert. Frühere Studien haben gezeigt, dass die Ausgabequalität von Sprachmodellen eine hohe Varianz aufweist, was zu schlechten Leistungen führt. Daher nehmen wir den Ansatz "übergenerieren-und-filtern" zur Verbesserung der Generierungsqualität an. Zuerst zeigen wir Einschränkungstypen mit Beispielen für InstructGPT und erhalten spezifische Ziele basierend auf den Ausgangsabstrakten Zielen. Dann übergeneriert InstructGPT K Skripte für spezifische Ziele. Als Nächstes wird ein Filtermodell entwickelt, um konforme Skripte auszuwählen. Wir konvertieren Skripte und Ziele in InstructGPT-Embeddings und berechnen die Cosinusähnlichkeit als Ähnlichkeitswerte, um die semantische Ähnlichkeit zu messen. Darüber hinaus belohnen wir Skripte, die Schlüsselwörter der Zielbeschränkung enthalten. Wir behalten nur das Skript, wenn das Ziel die höchste Punktzahl im Zielset hat. Mit unserem Verfahren können InstructGPT-Modelle Skripte von höherer Qualität generieren. Unser Verfahren verbessert die Planungsfähigkeit sowohl in Bezug auf die semantische Vollständigkeit als auch auf die Einhaltung der Einschränkungen. Da große Sprachmodelle teuer einzusetzen sind, ist es wichtig, die Planungsfähigkeit kleiner und spezialisierter Modelle zu ermöglichen. Die Erstellung eines Datensatzes ist ein wesentlicher Schritt hierfür. Frühere Studien ermöglichen jedoch keine Planung für spezifische Ziele und die manuelle Annotation eines Datensatzes ist teuer. Daher folgen wir dem Ansatz der symbolischen Wissensdestillation, um beschränkte Sprachplanungsdatensätze aus großen Sprachmodellen zu destillieren. Wir wenden unser Verfahren an, um einen Datensatz für beschränkte Sprachplanung namens CoScript zu erstellen. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten. Um die Qualität der Validierungs- und Testmengen sicherzustellen, bitten wir Crowd-Sourcing-Worker, fehlerhafte Beispiele zu finden und zu korrigieren. Das Diagramm zeigt die Einschränkungsverteilung von CoScript. Wir stellen fest, dass CoScript eine hohe Pluralität der generierten spezifischen Ziele aufweist. Mit CoScript können wir kleinere, spezialisierte Modelle für beschränkte Sprachplanung ausprobieren. Wir stellen fest, dass T5, das auf CoScript feinabgestimmt wurde, Skripte von höherer Qualität generieren kann als die meisten großen Sprachmodelle, was zeigt, dass kleinere Modelle bei der richtigen Ausbildung auf geeigneten Datensätzen größere Modelle übertreffen können. Zusammenfassend haben wir das Problem der beschränkten Sprachplanung etabliert. Wir bewerten die Fähigkeit von großen Sprachmodellen zur beschränkten Sprachplanung und entwickeln ein Verfahren zum Übergenerieren und Filtern von Skripten für große Sprachmodelle. Wir verwenden große Sprachmodelle zur Generierung eines hochwertigen Skriptdatensatzes, CoScript, für beschränkte Sprachplanung. Wir hoffen, dass der CoScript-Datensatz eine wertvolle Ressource zur Weiterentwicklung der Forschung im Bereich der Sprachplanung sein wird. Vielen Dank für Ihre Zeit. Weitere Details zu CoScript finden Sie in unserer Arbeit.</sample>
    <sample id="100">PromptRank is a data-efficient approach to multi-hop question answering (QA) that leverages language models for few-shot ranking of candidate answer paths.  The method combines unsupervised retrieval (TF-IDF and hyperlink traversal) with a language model-based reranker.  Instead of relying on large labeled datasets, PromptRank achieves strong performance with only 128 training examples.  

The core idea is to construct a chain prompt that incorporates the retrieved documents and an instruction to elicit reasoning from the language model.  The likelihood of the question given the chain prompt serves as the scoring function.  Experiments on HotpotQA demonstrate that PromptRank outperforms fully supervised systems like DrKit and achieves comparable performance to state-of-the-art dense retrievers.  Ablation studies confirm the importance of each component.  Furthermore, PromptRank improves downstream QA performance when used as a retriever, achieving results close to MDR.  The use of language models for few-shot ranking significantly enhances multi-hop QA capabilities, offering a promising alternative to data-intensive methods.</sample>
    <sample id="101">Die Sprachgewandtheit von PaLM ist vergleichbar mit state-of-the-art Systemen.</sample>
    <sample id="102">Die wichtigsten Eigenschaften eines Wasserzeichenverfahrens sind:

1.  Anwendbarkeit auf Embedding-Dienste.
2.  Keine Beeinträchtigung der Embedding-Nützlichkeit.
3.  Ausreichende Tarnung, um die Entfernung zu erschweren.
4.  Übertragbarkeit auf die Dienste des Angreifers.</sample>
    <sample id="103">Die englischen TED Talks wurden in 14 Sprachen übersetzt.</sample>
    <sample id="104">Das Projekt hat über 16.000 Annotationen aus über 1.000 Annotatoren aus 87 Ländern extrahiert.</sample>
    <sample id="105">Cosine Similarity Difference and L2 Similarity Difference.</sample>
    <sample id="106">QUEST is a new retrieval dataset designed to evaluate systems handling information needs with implicit set constraints. The dataset comprises over 3,000 entity-seeking queries, where users express preferences involving intersections and complements of sets (e.g., "red reptile, not more than 12 inches long, found in Costa Rica").  The queries are constructed using Wikipedia categories across four domains (films, books, plants, animals) and refined through human annotation for fluency and relevance.  Annotators identify relevant spans within documents to support different query constraints.  QUEST presents a challenging retrieval problem requiring systems to search large corpora for multi-answer sets, with evidence for relevance potentially distributed across document spans.  Evaluation involves comparing systems to sparse and dense retrievers and a T5-based reranker.  Results demonstrate significant room for improvement in retriever performance and low end-to-end system F1 scores, particularly for queries involving set intersections and differences.  QUEST aims to facilitate research into improved systems for handling selective information needs, exemplified by scenarios like a zoologist seeking a specific reptile or a reader looking for a particular type of book.</sample>
    <sample id="107">XSemPLR evaluierte Encoder-Decoder-Modelle wie mBART und mT5 sowie Encoder-PTR-Modelle wie XLM-R + PTR und mBERT + PTR. Encoder-Decoder-Modelle erzielten in allen neun Datensätzen die beste Leistung.</sample>
    <sample id="108">```
This paper investigates the robustness of language model acceptability judgments to context, addressing a limitation of the current Minimal Pair (MPP) paradigm.  MPP evaluates models by comparing acceptable and ungrammatical sentences, but struggles with longer contexts prevalent in modern LLMs.  The authors revisit MPP by simulating longer sequences using data from existing datasets, creating acceptable and unacceptable prefixes.  Experiments reveal that MPP judgments are relatively stable with increasing context length when using irrelevant data, suggesting models possess robust knowledge of arbitrary contexts. However, when prefixes are chosen from the same dataset (e.g., BLiMP), judgments significantly shift based on whether the prefix is acceptable or unacceptable, and this effect scales with context length.  Analysis indicates that language models are sensitive to shared syntactic and semantic features, rather than solely to the specific words in a sentence.  The current MPP evaluation method, focused on short, single-sentence inputs, may not fully capture this abstract knowledge across longer contexts.  The study highlights the need for a refined evaluation approach that better reflects the capabilities of large language models with extended context windows.
```</sample>
    <sample id="109">The paper introduces Unnatural Instructions, a large dataset of natural language instructions and their corresponding inputs and outputs, created entirely through automated generation by a pre-trained language model (GPT-3).  This approach bypasses the need for human annotation, addressing the limitations of existing instruction tuning data derived from existing NLP benchmarks or user-generated prompts.  The method involves prompting the language model to generate instructions, inputs, and outputs, and then further diversifying the dataset by generating paraphrases of existing instructions.  The resulting dataset comprises 64,000 examples, with approximately 240,000 including paraphrases.  Evaluation reveals high correctness rates (over 50%) and the generation of highly creative and diverse tasks.  Fine-tuning an 11 billion-parameter T5 model on Unnatural Instructions outperforms both T0++ and Tk-instruct on several benchmarks, and amortized training costs demonstrate superior performance compared to training on Super-Natural Instructions.  The study highlights the potential of language models to generate diverse and creative instruction data, offering a faster and cheaper alternative to human annotation while mitigating the predictability of crowd-sourced data.</sample>
    <sample id="111">Die Autoren wählen Wörter mit mittlerer Häufigkeit aus, indem sie einen allgemeinen Textkorpus sammeln und die Häufigkeit jedes Wortes darin zählen.</sample>
    <sample id="112">Hallo zusammen, mein Name ist Shuheng. Heute präsentiere ich unser Paper "Do CoNLL-2003 Named Entity Tagging Modelle noch gut funktionieren im Jahr 2023?". Lasst uns beginnen. Unser Paper untersuchte das Problem der Generalisierung im Bereich der Named Entity Recognition (NER) Aufgabe. Wir beobachten, dass Modelle, die seit fast 20 Jahren für die Entwicklung von NER-Taggern im CoNLL-2003 Datensatz verwendet werden, mehrere Probleme aufwerfen. Erstens: Können diese Modelle auf moderne Daten generalisieren? Und zweitens: Was ist für eine gute Generalisierung bei der Entwicklung neuer Tagger erforderlich? Gleichzeitig: Wenn wir eine schlechte Generalisierung beobachten, was verursacht den Leistungsabfall dieser Modelle? Um diese Probleme zu untersuchen, haben wir den CoNLL++ Datensatz entwickelt. Dies ist ein Datensatz, den wir aus Reuters News von 2020 gesammelt und mit den gleichen CoNLL-2003 Annotationsrichtlinien annotiert haben. Wir haben über 20 Modelle auf dem CoNLL-2003 Datensatz feinabgestimmt. Wir haben sie sowohl auf den CoNLL-03 Testsets als auch auf dem CoNLL++ Datensatz evaluiert und den prozentualen Leistungsverlust der F1-Score berechnet, um die Generalisierung jedes Modells zu bewerten. Was also ist für eine gute Generalisierung erforderlich? Während unserer Experimente haben wir drei Hauptbestandteile identifiziert, die erforderlich sind. Der erste ist die Modellarchitektur. Unsere Experimente haben gezeigt, dass Transformer-Modelle im Allgemeinen besser auf neue Daten generalisieren. Der zweite Bestandteil ist die Modellgröße. Wir haben festgestellt, dass größere Modelle im Allgemeinen zu einer besseren Generalisierung führen. Und schließlich, wie wir alle wissen, beeinflusst die Anzahl der Feinabstimmungsexemplare direkt die Leistung einer nachgelagerten Aufgabe. Hier haben wir auch festgestellt, dass mehr Feinabstimmungsexemplare tatsächlich auch zu einer besseren Generalisierung führen. Zu unserer nächsten Frage: Was verursacht den Leistungsabfall einiger Modelle? Wir hatten zwei Hypothesen. Die erste ist adaptive Überanpassung, bei der der Überanpassungskosten durch wiederholtes Verwenden desselben Testdatensatzes entstehen, was sich normalerweise in sinkenden Renditen auf einem neuen Testdatensatz manifestiert. Die zweite Hypothese ist zeitliche Verzerrung, die Leistungsverschlechterung, die durch die zunehmende zeitliche Lücke zwischen Trainings- und Testdaten verursacht wird. Bei Datenüberanpassung sahen wir, dass die rote beste Passformlinie im Graphen rechts einen größeren Gradienten als eins aufweist. Das bedeutet, dass jede Einheit der Verbesserung auf CoNLL-2003 mehr als eine Einheit der Verbesserung auf CoNLL++ entspricht, was darauf hindeutet, dass es keine diminishing returns gibt. Dies zeigt uns, dass adaptive Überanpassung in diesem Fall nicht beobachtet wird. Was ist nun mit zeitlicher Verzerrung? Um zeitliche Verzerrung zu untersuchen, führten wir ein Experiment durch, bei dem einige Modelle mit mehr aktuellen Daten neu trainiert oder weiter vorab trainiert wurden, und stellten fest, dass die Leistung mit größerer zeitlicher Lücke abnimmt. Dies bestätigt unsere Hypothese, dass die Hauptursache für den Leistungsabfall die zeitliche Verzerrung ist. Unsere Schlussfolgerung ist, dass für eine gute Generalisierung eine bessere Modellarchitektur, eine größere Modellgröße sowie mehr Feinabstimmungsexemplare erforderlich sind. Und diese gehen Hand in Hand; man kann nicht nur einen dieser Bestandteile auslassen. Gleichzeitig haben wir festgestellt, dass der Leistungsabfall durch zeitliche Verzerrung verursacht wird und überraschenderweise nicht durch adaptive Überanpassung, obwohl CoNLL-2003 seit über 20 Jahren verwendet wird. Zurück zur Frage, die wir im Titel unseres Papers aufgeworfen haben: "Do CoNLL-2003 tagger noch gut funktionieren im Jahr 2023?". Und wir haben festgestellt, dass die Antwort ein klares Ja ist. Wir hoffen, dass unser Paper weitere Forschung zur Verbesserung der Generalisierung von Modellen anregt. Und schließlich: Bitte schaut euch unser Paper, unseren Datensatz an und wenn ihr Fragen habt, könnt ihr euch gerne an mich wenden. Vielen Dank.</sample>
    <sample id="114">## Abstract: Finding the Pillars of Strength for Multi-Head Attention

Large language models (LLMs) offer revolutionary capabilities but suffer from limitations including heavy parameter counts, long training times, and large data requirements. This work addresses the heavy parameter problem by proposing a novel grouped head attention (GHT) method for multi-head attention compression. GHT employs a divide-and-conquer strategy, first grouping attention heads and training them with homogenization and diversification objectives to improve similarity within groups and separation between them. Subsequently, a Voting-to-Stay algorithm prunes redundant heads within each group, leaving only one head per group.

Experiments on machine translation, language modeling, and abstractive summarization demonstrate that GHT achieves significant parameter compression (up to 90%) with comparable or improved performance compared to state-of-the-art baselines.  Specifically, GHT-PS achieves 32.1% parameter compression and 3.8-4.4% BLEU improvement in machine translation.  Furthermore, a lightweight LITE model achieves 90% parameter reduction, 62% faster inference, and 80% reduced FLOPs while maintaining performance.  This research leverages the Lottery Ticket Hypothesis to advocate for task-specific automatic pruning, enabling efficient deployment of LLMs by selectively removing redundant parameters based on task relevance.</sample>
    <sample id="115">Lambda speech frames.</sample>
    <sample id="116">Servin ist ein Richter.</sample>
    <sample id="117">Die Qualität der Beispiele ist wichtiger als die Ähnlichkeit mit dem Ausgangssatz.</sample>
    <sample id="118">```
This paper addresses the challenge of code-switching in NLP, where text contains multiple languages within the same sentence. Multilingual pre-trained models often struggle with this phenomenon. We propose SwitchMLM, a novel Masked Language Modeling (MLM) technique specifically designed for code-switched data. SwitchMLM focuses on masking only "switch-points"—tokens that mark language transitions—rather than all tokens, addressing the need for LID tags.  We also introduce FrequencyMLM as a surrogate method when LID tags are unavailable.  Furthermore, we propose architectural modifications, including residual connections to leverage switch-point information in intermediate layers and an auxiliary loss to encourage language information encoding.  Empirical results on sentiment analysis demonstrate that our combined SwitchMLM/FrequencyMLM, ResBERT, and auxiliary loss approach outperforms standard MLM.  Probing experiments using linear and conditional probing classifiers confirm that SwitchMLM effectively increases switch-point information in intermediate and final layers of the model.  We hypothesize and verify that our methods enhance the representation of switch-point information, leading to improved performance.
```</sample>
    <sample id="119">Die Arbeiten konzentrieren sich auf GPT-4, GPT-Modelle und BART-Modelle.</sample>
    <sample id="120">Das Modell verwendet Aufmerksamkeitswerte aus der letzten Lambda-Zeitspanne.</sample>
    <sample id="121">Die Beispiele für direkte Inferenz sind die Nennung des Namens der Song ("Easy on Me" oder "I Gotta Feeling") oder seiner Position ("die erste").</sample>
    <sample id="122">Fudan University.</sample>
    <sample id="123">MultiInstruct addresses the gap in multi-modal instruction tuning by introducing the first large-scale benchmark dataset of 62 diverse multi-modal tasks. Leveraging 21 existing datasets and five expert-written instructions per task, MultiInstruct enables instruction tuning of the unified multi-modal pre-trained model OFA.  The research investigates the effectiveness of instruction tuning on OFA, demonstrating significant performance improvements on seen multi-modal tasks and highlighting the benefits of transfer learning from natural language instruction datasets.  Experiments reveal that using multiple instruction templates during fine-tuning enhances performance and reduces model sensitivity.  A novel "sensitivity" metric is introduced to quantify the model's consistency across varied instruction wording.  The study concludes that MultiInstruct facilitates effective multi-modal instruction tuning, significantly improving OFA's capabilities and offering insights into optimal fine-tuning strategies.  Future work aims to expand MultiInstruct with approximately 150 additional vision-language tasks, making it a valuable resource for the multi-modal instruction tuning community.</sample>
    <sample id="124">```
This work addresses the challenge of improving temporal reasoning capabilities in large language models (LLMs). Temporal reasoning is categorized into three levels: time-to-time, time-to-event, and event-to-event reasoning.  Existing research often focuses on the time-to-event level, neglecting the broader temporal landscape.  We introduce the TempReason dataset, encompassing all three reasoning levels and a wide temporal coverage, constructed from Wikidata and Wikipedia.  We evaluate LLMs like T5, FLAN-T5, and ChatGPT on this dataset, revealing biases towards the 2000-2020 time period and ChatGPT's performance degradation in month prediction.  We propose a novel training strategy, TempT5, incorporating temporal span extraction pre-training and time-sensitive reinforcement learning.  Experiments demonstrate that TempT5 significantly outperforms zero-shot LLMs and substantially improves upon fine-tuned models on TempReason, particularly in Open Book QA and Reasoning QA.  Furthermore, we observe temporal reasoning biases in ChatGPT.  Our findings highlight the importance of comprehensive temporal reasoning benchmarks and training paradigms to enhance LLM performance.
```</sample>
    <sample id="125">Der Text erwähnt keine Anzahl von Autoren.</sample>
    <sample id="126">XSemPLR verwendet Google Translate API, um die Quelle in die Zielsprache zu übersetzen, bevor sie mit einem monolingualen Modell analysiert wird.</sample>
    <sample id="127">## Abstract: Large Language Models Are Reasoning Teachers

This paper introduces "Large Language Models Are Reasoning Teachers," a novel approach to transfer complex reasoning abilities from large language models (LLMs) to smaller, more accessible models.  Chain-of-thought (CoT) prompting, while effective for LLMs, is computationally expensive and limits its deployment to massive models.  We address this by leveraging LLMs as "reasoning teachers" to generate step-by-step solutions for complex tasks, which are then used to fine-tune smaller models.  

Our key innovation is "Diverse Reasoning," which generates multiple reasoning samples from the teacher model using stochastic sampling. This allows the student model to learn from a wider range of solutions, leading to improved performance.  Experiments on 12 tasks demonstrate that our method significantly outperforms prompt-based baselines and vanilla fine-tuning, even with small student models (0.3 billion parameters).  Scalability is also shown through increased dataset size, better teacher models, and larger student models.  The paper highlights the trade-offs between development and inference costs associated with this distillation approach and encourages further research in this area.  Code and data are publicly available.</sample>
    <sample id="128">This paper introduces KITMUS, a novel diagnostic test suite for evaluating knowledge integration in natural language understanding (NLU) models.  The test suite focuses on coreference resolution, probing the ability of models to leverage both pre-training and inference-time knowledge.  We address the challenge of integrating knowledge from diverse sources, where pre-trained parameters contain general knowledge, while inference-time context provides entity-specific information.  KITMUS defines three settings: "Background-Pretrain," "Background-Both," and "Background-Inference," varying the availability of background knowledge.  We evaluate models with human participants and established coreference resolution systems, demonstrating that while task-specific training improves performance, even the best models struggle with reliably integrating knowledge only available at inference time.  Our findings highlight the limitations of current models in reasoning over knowledge from multiple sources and suggest the need for further research in knowledge integration for knowledge-intensive NLU tasks. The KITMUS dataset and code are publicly available.</sample>
    <sample id="129">Die Autoren haben das Beispiel der "Asian woman" gegeben, die in der Prompt-Generierung als "unassuming" beschrieben wurde, im Gegensatz zur "Middle-Eastern woman", die als "exotic" und "mesmerizing" bezeichnet wurde.</sample>
    <sample id="130">Das Paper besagt, dass Transformer-Modelle tendenziell besser generalisieren als andere Modellarchitekturen. Es wird keine spezifische Modellarchitektur genannt, die *nicht* gut generalisiert, aber es wird impliziert, dass Modelle, die nicht auf Transformer-Architekturen basieren, schlechter generalisieren.</sample>
    <sample id="131">Der Text erwähnt keine Testdatensätze. Er konzentriert sich auf Validierungsdatensätze.</sample>
    <sample id="132">Drei.</sample>
    <sample id="133">Multi-modal.</sample>
    <sample id="135">ABC-Eval is a novel dimensional approach to evaluating conversational AI, developed by the Emory NLP Lab and Amazon Alexa AI. It addresses the limitations of existing methods like human ratings by explicitly annotating model behaviors such as irrelevant responses, contradictions, and lack of empathy.  The method measures the frequency of these behaviors, providing a more precise and reliable assessment of dialogue quality.  

A study evaluating four state-of-the-art models using ABC-Eval and three existing methods revealed that ABC-Eval labels demonstrate higher inter-annotator agreement and better predictive power of overall conversation quality.  Specifically, ABC-Eval metrics explain a significantly larger proportion of conversation quality than turn-level Likert ratings.  

The analysis highlights common challenges in conversational AI, including common sense violations, irrelevant information, and contradictions.  ABC-Eval offers a more granular and informative evaluation framework, enabling researchers to identify strengths and weaknesses of models with greater accuracy. The authors advocate for the adoption of ABC-Eval to drive progress in conversational AI evaluation.</sample>
    <sample id="136">## Abstract

This presentation introduces FERMAT, a novel evaluation framework for numerical reasoning designed to address limitations of existing benchmarks.  The work investigates why large language models (LLMs) struggle with numerical reasoning, particularly at the 3-billion parameter scale, despite their overall performance.  FERMAT utilizes a flexible evaluation set constructed from arithmetic-type questions extracted from Illinois and CommonCore, modified to encompass a wider range of number representations (integers, decimals) and mathematical operations.  A zero-shot evaluation reveals poor performance across all aspects, highlighting the inadequacy of standard benchmarks.  Fine-tuning LLMs with a dataset of 200,000 questions generated by math teachers, incorporating diverse number and operation types, demonstrates significant performance gains.  Analysis of training dependencies reveals that models often fail to memorize specific question types, suggesting the importance of linguistic cues.  Finally, the impact of incorporating language and mathematical diversity from datasets like GSM8K and AQUA is explored, showing promising results.  The findings suggest that existing benchmarks are unrepresentative and that a more nuanced evaluation approach, focusing on language and mathematical diversity, is crucial for advancing LLM capabilities in numerical reasoning.</sample>
    <sample id="137">```
This paper introduces Tell2Design, a large-scale dataset for language-guided floor plan generation, addressing the need for design generation that adheres to specific requirements outlined in natural language. Unlike text-conditional image generation, which focuses on high-level visual concepts, this task requires models to understand and satisfy multiple constraints and requirements specified in text. We define floor plan generation as a sequence-to-sequence problem, where natural language instructions are mapped to structured floor plan layouts represented by room type labels and bounding boxes. The dataset comprises 5,051 human-annotated instructions and 76,000 artificially generated instructions.  We evaluate a sequence-to-sequence model based on a transformer architecture, pre-trained with T5, and demonstrate its superior performance compared to existing text-conditional image generation baselines, achieving high IoU scores.  The results highlight the challenges of aligning generated floor plans with complex, potentially ambiguous, and incomplete instructions.  Furthermore, we show that combining artificial and human-written instructions during training improves model performance.  Tell2Design and the proposed sequence-to-sequence model provide a foundation for advancing research in language-guided design generation.
```</sample>
    <sample id="138">Die Autoren sehen ein zu wenig erforschtes Gebiet im Bereich der NLU darin, wie Modelle Wissen aus verschiedenen Quellen integrieren und nutzen können, insbesondere Wissen, das nur während der Inferenz verfügbar ist.</sample>
    <sample id="139">Ying und Zhiyang.</sample>
    <sample id="140">Ja, Coscript wurde durch Crowd-sourced Workers überprüft und korrigiert, um die Qualität der Validierungs- und Testmengen zu gewährleisten.</sample>
    <sample id="141">Die bestehenden Ressourcen für kontextbasierte Übersetzung unterstützen nur begrenzte Arten von Kontextabhängigkeiten und begrenzte Sprachpaare, da sie oft auf Fachwissen und menschlicher Kuratierung basieren.</sample>
    <sample id="142">Javad Hosseini und Filip Radlinski, Silvia Pareti und Annie Louis haben gemeinsam an der Arbeit "Resolving Indirect Referring Expressions for Entity Selection" gearbeitet und den AltEntities Corpus vorgestellt. Ihr Ziel ist es, das Sprachverständnis von Nutzern zu verstehen, wenn sie eine Wahl treffen wollen. Betrachten wir die alternative Frage: "Meinst du 'Easy on Me' oder 'I Gotta Feeling'?" Hier möchte ein Nutzer zwischen diesen beiden Songs wählen. Die offensichtlichste Sache wäre eine direkte Referenz, zum Beispiel durch Nennung des Songnamens "Easy on Me" oder seiner Position, "der erste". Aber manchmal ist eine indirekte Referenz passender für eine natürlichere Unterhaltung. Das kann passieren, wenn der Nutzer den Namen des Songs nicht mehr erinnert. Oder wenn die Aussprachen zu ähnlich sind und eine Unterscheidung schwierig ist. Oder wenn der Nutzer eine Präferenz angeben möchte. Hier sind einige Beispiele für indirekte Referenzen, zum Beispiel "der neuere" oder "der Song, der nicht energiegeladen ist". Dies ist ein wichtiges Problem in konversationellen Systemen und auch für das Benchmarking von LLMs (Large Language Language Models) im Bereich der Entitätserkennung. Wir sind uns nicht einer größeren öffentlichen Datensatzes für diese Aufgabe bewusst, daher haben wir einen eigenen Datensatz mithilfe von Crowd-Annotation erstellt. Unser Datensatz umfasst drei verschiedene Domänen: Musik, Bücher und Rezepte. Unsere Datensatzbeschaffungsmethodik betont Informalität durch eine Cartoon-Vollständigkeits-Aufgabe. Der Cartoon hat drei Sprechblasen. In der ersten Sprechblase sagt Bob: "Erinnerst du dich an den Song, den wir gestern gehört haben?" Und damit setzt Bob den Dialogkontext. In der zweiten Sprechblase sagt Alice: "Meinst du 'Easy on Me' oder 'I Gotta Feeling'?" Dies ist die alternative Frage. Und in der dritten Sprechblase sagt Bob eine indirekte Referenz, um eine der Entitäten auszuwählen, zum Beispiel: "der neuere". Wir stellen die erste und zweite Sprechblase automatisch bereit, aber die dritte wird vom Annotator ausgefüllt. Die erste Sprechblase wird aus einer Reihe manueller Prompts pro Domäne ausgewählt. Die zweite, die alternative Frage, wird wie folgt generiert. Wir verwenden immer ein einfaches Template: "Meinst du A oder B?", wobei A und B Beispiele aus Wikipedia sind. Wir haben verschiedene Sampling-Methoden verwendet. Je weiter wir in der Liste hochgehen, desto ähnlicher werden die Entitäten und desto schwieriger wird die Unterscheidung. Die erste Methode ist zufällig. Die zweite Methode ist, wenn die Entitäten ähnliche Titel haben, zum Beispiel zwei Bücher mit dem Namen "The Return". Die dritte Methode ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben. Und schließlich, wenn sie ähnliche Informationen oder Attribute auf Wikipedia haben, zum Beispiel die gleiche Genre oder der gleiche Künstler für einen Song. Wenn wir diese alternative Frage den Annotatoren zeigen, wissen sie den Namen der Entitäten, aber sie müssen nicht unbedingt über die Entitäten selbst Bescheid wissen. Daher zeigen wir ihnen einige Hintergrundinformationen zu den beiden Entitäten. Für Songs zeigen wir einfach einen Google-Suchlink zu jedem Song und bitten die Annotatoren, mindestens einige der Songs anzuhören und sich über jeden Song zu informieren. Hier ist zum Beispiel der Google-Suchergebnis für den Song "Easy on Me". Für Rezepte und Bücher zeigen wir einige Hintergrundtext aus Wikipedia. Für Rezepte zeigen wir zusätzlich Bilder, ebenfalls von Wikipedia, damit die Annotatoren wissen, wie sie aussehen. Dann baten wir die Annotatoren, eine der Entitäten auszuwählen und sie anhand von drei bis fünf indirekten Referenzierungsäußerungen zu beschreiben. Zum Beispiel die mit der Klaviermusik. Hier sind einige Beispiele aus unserem Datensatz. Zum Beispiel: "der ohne Worte", "nicht der mit dem 12-jährigen Jungen", "der fiktive", "kommt aus Aserbaidschan", usw. Der AltEntities Corpus enthält 6.000 alternative Fragen über drei Domänen und 42.000 indirekte Referenzierungsäußerungen. Die Ergebnisse mit dem T5 XL-Modell sind wie folgt zusammengefasst. Wenn das Sprachmodell Zugriff auf die exakt gleichen Hintergrundinformationen wie die Annotatoren hat, dann ist die Genauigkeit sehr hoch, etwa 92 bis 95 %. Dies ist jedoch nicht realistisch. Wenn das Sprachmodell Zugriff auf teilweise überlappende Hintergrundinformationen hat, dann liegt die Genauigkeit zwischen 82 und 87 %, was realistischer ist. Zum Beispiel, wenn das Sprachmodell die Hintergrundinformationen abruft. Wenn das Sprachmodell nur Zugriff auf Entitätsnamen hat, dann beträgt die Genauigkeit nur 60 %, es gibt also viel Verbesserungspotenzial. Wir haben auch gezeigt, dass die Modelle domänenübergreifend generalisierbar sind. Hier ist ein Link zu unserem Datensatz. Danke.</sample>
    <sample id="143">Der Ansatz wird mit den Strategien Wait-k und Local Agreement sowie mit der state-of-the-art Architektur für die Vorverarbeitung von Simultanspeechübersetzungen verglichen.</sample>
    <sample id="144">The authors are affiliated with Nantes University Hospital.</sample>
    <sample id="145">Jenny.</sample>
    <sample id="146">This paper addresses the critical issue of omission in dialogue summarization, a significant problem despite advancements in using large language models.  A systematic analysis reveals that a high percentage (around 70%) of generated summaries suffer from omissions, indicating a general deficiency in current models' ability to capture key information.  The study introduces the OLDS dataset, a high-quality dataset of dialogue summaries with omission labels, built upon existing benchmarks and validated through human evaluation.  This dataset enables the development of omission detection models, a task previously lacking in the dialogue summarization literature.  The paper explores three model frameworks – pairwise classification, sequence labeling, and pointer networks – and evaluates their performance using metrics like Precision, Recall, and F1-score, as well as the Word-level Omission Recall (WR) score.  The results highlight the challenging nature of the task, with a low F1-score indicating the need for more sophisticated detection models.  Finally, the paper investigates the potential of using detected omissions for summary refinement, demonstrating that incorporating omission information into a sequence-to-sequence model significantly improves summary quality.  The OLDS dataset is publicly available to facilitate further research in this area.</sample>
    <sample id="147">Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models. This work is done in collaboration with Esin Durmus and Dan Jurafsky.</sample>
    <sample id="148">Hi, ich bin Sara Papi vom Universitätszentrum für Grundlagenforschung und Innovation der Universität Triest und der Fondazione Bruno Kessler und ich werde kurz das Paper "Attention as a Guide for Simultaneous Speech Translation" vorstellen, das in Zusammenarbeit mit Matteo Negri und Marco Turchi verfasst wurde. Was ist Simultaneous Speech Translation? Simultaneous Speech Translation, oder SimulST, ist der Prozess der Übersetzung gesprochener Sprache in Text in einer anderen Sprache in Echtzeit, wodurch die Kommunikation zwischen verschiedenen Sprachen ermöglicht wird. Und was sind die Probleme der aktuellen SimulST-Modelle? Spezifische Architekturen werden in der Regel trainiert, die zusätzliche Module erfordern, die optimiert werden müssen. Lange und komplizierte Trainingsverfahren, beispielsweise Trainings, die verschiedene Optimierungsziele beinhalten. Und das Training und die Wartung mehrerer Modelle, um verschiedene Latenzregime zu erreichen. Zum Beispiel das Training eines Modells mit einer durchschnittlichen Latenz von einer Sekunde und eines anderen Modells mit zwei Sekunden Latenz, usw. Was ist unsere Lösung? Erstens, vorhandene Offline-ST-Modelle ohne erneutes Training oder die Einführung spezifischer Architekturen für SimulST verwenden. Verwenden Sie nur ein Modell für jedes Latenzregime und behandeln Sie die Latenz mithilfe spezifischer Parameter. Und nutzen Sie das bereits erlangte Wissen des Modells durch den Attention-Mechanismus zwischen Audio-Eingabe und Text-Ausgabe, den Cross-Attention-Mechanismus, und Sie können ein Beispiel rechts sehen. Unsere Lösung ist die Einführung von EDAtt, oder Encoder-Decoder-Attention, einer Strategie, bei der wir entscheiden, ob wir eine teilweise Übersetzung ausgeben oder nicht, basierend darauf, wo die Aufmerksamkeit liegt. Ein Wort wird ausgegeben, wenn die Aufmerksamkeit nicht konzentriert ist, d. h. seine Summe unter einem bestimmten Schwellenwert alpha gegenüber den letzten lambda Sprachframes liegt, was bedeutet, dass die empfangene Information ausreichend stabil ist. Zum Beispiel, wenn wir einen Sprachabschnitt mit "Ich werde über..." empfangen und unser Modell die Übersetzung ins Deutsche vorhersagt, und wir die Cross-Attention-Gewichte betrachten, werden wir sehen, dass die ersten beiden Wörter auf die frühesten empfangenen Sprachframes zeigen, während das letzte Wort auf die letzten lambda Sprachframes zeigt. Das bedeutet, dass die ersten beiden Wörter ausgegeben werden, während da die Summe der Cross-Attention über einem bestimmten Schwellenwert alpha liegt, wir das letzte Wort nicht ausgeben und auf einen weiteren Sprachabschnitt warten. Wenn wir fortfahren und wir einen weiteren Sprachabschnitt empfangen und unser Modell drei Wörter vorhersagt und wir die Cross-Attention-Gewichte betrachten, werden wir sehen, dass kein Wort auf die letzten lambda Sprachframes zeigt. Das bedeutet, dass diese drei Wörter ausgegeben werden. Wenn wir die Hauptergebnisse von EDAtt betrachten, werden wir die simultane Sprachübersetzungsergebnisse auf Diagrammen plotten, bei denen BLEU auf einer Seite die Qualität der Übersetzung misst und die durchschnittliche Verzögerung die Latenzmaßnahme ist, und wir berücksichtigen auch die rechnerisch bewusste durchschnittliche Verzögerung, die die Rechenzeit des Modells zur Vorhersage der Ausgabe berücksichtigt. Wir wollen unsere Kurven also so hoch wie möglich auf dem Diagramm haben. Aber auch so, dass sie nach links verschoben sind. Und wir vergleichen sie mit beliebten Strategien, die auch auf Offline-Modelle angewendet werden, der Wait-k-Strategie und der Local Agreement. Wir vergleichen auch mit der state-of-the-art-Architektur, die speziell für die simultane Vorübersetzung entwickelt wurde. Dies sind alle die Ergebnisse der simultanen Sprachübersetzungsstrategie auf Deutsch. Und wir sehen, dass sie alle Strategien, die auf Offline-Modelle angewendet werden, übertreffen, da die Kurven nach links verschoben sind. Und wir sehen auch, dass wenn wir die tatsächliche verstrichene Zeit oder die rechnerisch bewusste Zeit betrachten, das die schnellste Strategie ist. Wenn Sie mehr Ergebnisse entdecken möchten, lesen Sie unser Paper. Und wir haben auch den Code und die Modelle und die simultane Ausgabe für die Reproduzierbarkeit unserer Arbeit freigegeben. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="149">Ja, der Datensatz ist öffentlich zugänglich.</sample>
    <sample id="150">MeetingQA is a novel extractive question-answering dataset for meeting transcripts, addressing the gap in NLP research for this domain. Unlike existing datasets focused on summarization and action items, MeetingQA captures the inherent question-answering component of meetings, where participants pose open-ended questions eliciting detailed discussions. The dataset comprises 7.7K questions and answers extracted from public meeting transcripts, annotated with high inter-annotator agreement.  

The dataset features diverse question types, including yes/no, opinion-seeking, rhetorical, and multi-speaker questions, with a significant portion being unanswerable.  We explore various model approaches, including context retrieval, single-span, and multi-span models, and leverage data augmentation techniques.  Fine-tuned models achieve a substantial performance gap compared to human performance, with short-context models outperforming long-context models.  Multi-span models show comparable performance to single-span models.  Zero-shot performance is significantly lower, but data augmentation improves it.  Error analysis reveals challenges in identifying rhetorical questions and speaker attribution, particularly in zero-shot settings. MeetingQA presents a challenging task for current QA models, highlighting the need for further research in meeting-specific NLP.</sample>
    <sample id="151">Hallo zusammen, mein Name ist Ying und mein Kollege Zhiyang und wir präsentieren unsere Forschung zu MultiInstruct, die die Mehrfachmodale Null-Shot-Lernung durch Instruction Tuning verbessert. Angesichts der Fortschritte in großen Sprachmodellen haben viele Arbeiten begonnen, neue Lernparadigmen für die Wiederverwendung vortrainierter Sprachmodelle für verschiedene nachgelagerte Aufgaben in einem parameter- und dateneffizienten Verfahren zu erforschen. Kürzlich haben viele Studien gezeigt, dass Instruction Tuning große Sprachmodelle in einer Null-Shot-Manier auf ungesehenen Aufgaben ausführen können, indem sie natürliche Anweisungen befolgen. Allerdings konzentrierten sich die meisten früheren Arbeiten auf die Verbesserung der Null-Shot-Leistung auf reinen Sprachaufgaben, während Computer-Vision- und mehrmodale Aufgaben vernachlässigt wurden. Daher wollen wir in dieser Arbeit untersuchen, ob das Instruction Tuning eines mehrmodalen vortrainierten Modells die Generalisierung auf ungesehene mehrmodale Aufgaben tatsächlich verbessern kann. Zusätzlich haben wir festgestellt, dass es bei unserer Forschung einen erheblichen Unterschied in der Verfügbarkeit von Instructional-Datensätzen zwischen NLP und mehrmodalen Aufgaben gibt. Es gibt mehr als 1600 Sprach-only-Instruction-Aufgaben. Es gibt jedoch keinen großen, öffentlich zugänglichen mehrmodalen Instruction-Aufgabensatz. Dies motiviert uns, einen mehrmodalen Instruction-Tuning-Datensatz zu erstellen. Wir präsentieren MultiInstruct, den ersten mehrmodalen Instruction-Tuning-Benchmark-Datensatz, der 62 diverse mehrmodale Aufgaben in 10 breiten Kategorien umfasst. Diese Aufgaben stammen aus 21 existierenden Open-Source-Datensätzen und jede Aufgabe ist mit fünf von Experten geschriebenen Anweisungen ausgestattet. Um mehrmodales Instruction Tuning auf unserem vorgeschlagenen Datensatz zu untersuchen, verwenden wir OFA, ein vereinheitlichtes mehrmodales vortrainiertes Modell. OFA verwendet einen vereinheitlichten Vokabular für Sprach-, Bild-Token und die Koordinaten eines Begrenzungsrahmens. Hier zeigen wir einige Beispielinstanzen aus unserem MultiInstruct-Datensatz, um verschiedene Eingabe- und Ausgabe-Datentypen zu vereinheitlichen. Wir folgen der Methode von OFA und formulieren alle Aufgaben in einem vereinheitlichten Sequence-to-Sequence-Format. In dem die Eingabetexte, Bilder, Anweisungen und Begrenzungsrahmen im gleichen Token-Raum dargestellt werden. Okay, jetzt werde ich über mehrmodales Instruction Tuning sprechen. Also für den Trainingsdatensatz verwenden wir 53 Aufgaben aus 9 Gruppen für das Training und wir probieren 10.000 Instanzen pro Aufgabe aus. Für das Testen reservieren wir den gesamten Common Sense Reasoning-Gruppe für das Testen und wählen zusätzlich 5 Aufgaben aus den VQ- und Miscellaneous-Gruppen aus. Wir verwenden alle Instanzen im Testsplit für jede Aufgabe. Darüber hinaus probieren wir zufällig 20 Aufgaben aus dem Testsplit von natürlichen Anweisungen als ungesehene Aufgabe für NLP aus. Wir verwenden das vortrainierte OFA-Large-Modell als Basismodell. Während des Trainings mischen wir alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einer ihrer fünf Anweisungsvorlagen kombiniert. Also während des Tests für jede Aufgabe führen wir insgesamt 5 Experimente durch, indem wir das Modell mit einer der fünf Anweisungen bewerten. In jedem Experiment berichten wir über den Minimum- und Maximumwert der Leistung sowie die Standardabweichung der Leistung über alle 5 Experimente. Wenn die Aufgabe eine mehrmodale Klassifizierungsaufgabe ist, berichten wir über die Genauigkeit. Wenn es sich um eine mehrmodale Generierungsaufgabe handelt, berichten wir über Rouge-L. Für NLP-Aufgaben berichten wir ebenfalls über Rouge-L. Wir führen auch einen zusätzlichen Evaluationsmetrik namens Sensitivity ein. Diese misst die Fähigkeit des Modells, für die gleiche Aufgabe bei geringfügigen Variationen der Formulierung der Anweisung konsistent die gleichen Ausgaben zu erzeugen. Hier ist unser Hauptergebnis. Wie wir sehen können, verbessert das Instruction Tuning die Leistung von OFA auf gesehenen mehrmodalen Aufgaben erheblich. Außerdem kann das Transferlernen von natürlichen Instruction-Datensätzen die Instruction-Tuning verbessern. Hier sehen wir, dass je größer die Anzahl der Aufgaben, desto besser die Leistung des Modells und desto geringer die Sensitivität. Wir haben auch ein Experiment durchgeführt. Wir verwenden eine Anweisung gegen 5 Anweisungen. Wie wir sehen können, kann die Verwendung mehrerer Anweisungen die Gesamtleistung des Modells erheblich verbessern und seine Sensitivität stark reduzieren. Dies zeigt den Effekt verschiedener Fine-Tuning-Strategien auf die Modell-Sensitivität. Wie wir sehen können, kann das Transferlernen von natürlichen Instruction-Datensätzen die Sensitivität des OFA-Modells im Vergleich zum ursprünglichen OFA-Modell deutlich verbessern. Wir sehen auch, dass das Transferlernen von natürlichen Instruction-Datensätzen OFA dabei helfen kann, eine viel bessere Leistung auf dem natürlichen Instruction-Datensatz zu erzielen. Insgesamt schlagen wir einen ersten großen mehrmodalen Instruction-Tuning-Datensatz vor, der die Fähigkeiten von OFA erheblich verbessert, und untersuchen verschiedene Transferlernen-Techniken und zeigen deren Vorteile. Wir entwerfen eine neue Metrik namens Sensitivity. Noch eine Sache, wir sammeln einen viel größeren mehrmodalen Instruction-Tuning-Datensatz mit etwa 150 zusätzlichen Vision-Language-Aufgaben und werden ihn veröffentlichen. Dies ist ein QR-Code für unsere Daten und unser Modell. Vielen Dank.</sample>
    <sample id="152">```
This presentation introduces new large language models (LLMs) specifically designed for classical philology, addressing limitations of existing monolingual and multilingual models for Ancient Greek and Latin. The project developed GreBERTa and GreTa (monolingual RoBERTa and T5 encoder-decoder models for Greek), and PhilBERTa and PhilTa (multilingual models for Greek, Latin, and English).  A key innovation was creating a high-quality pre-training corpus for Ancient Greek by identifying and correcting OCR errors in the Internet Archive.  Models were benchmarked on tasks like part-of-speech tagging, dependency parsing, and lemmatization, achieving state-of-the-art performance for both languages.  Analysis revealed differences in T5 encoder behavior and significant improvements in lemmatization accuracy.  While multilingual models showed performance gains, no significant difference was observed in semantic and world knowledge capabilities compared to monolingual models. The research highlights the potential of LLMs for classical philology, emphasizing the importance of native tokenization and high-quality pre-training data.  A paper detailing the project is available for further information.
```</sample>
    <sample id="153">## Abstract

This work addresses the challenge of ambiguities in text-to-image generative models, aiming to improve the faithfulness of generated images to user intent. We investigate various ambiguities in prompts, such as those related to spatial relationships and object placement. To tackle this, we propose a pipeline consisting of a benchmark dataset based on the LAVA corpus, a prompt disambiguation framework, and an automatic evaluation framework.

Our disambiguation framework employs in-context learning to generate clarifying questions or different visual interpretations, allowing users to refine ambiguous prompts. We evaluate the effectiveness of this framework by generating images from disambiguated prompts using text-to-image models and assessing their faithfulness to user intent using a Visual Question Answering (VQA) model.

Our findings demonstrate that ambiguity resolution varies across different prompt types, but that our framework generally improves image fidelity. Furthermore, our automatic evaluation framework aligns with human evaluation, providing a reliable method for assessing text-to-image model performance. This research contributes to advancing the robustness and user-friendliness of text-to-image generation.</sample>
    <sample id="154">University of Trento and Foundazione Bruno Kessler.</sample>
    <sample id="155">AltEntities Corpus</sample>
    <sample id="157">This paper introduces SDDS, a novel model for dialogue summarization that aims to extract salient information from multi-turn dialogues. Existing methods often rely on pre-computed static graphs, which are susceptible to errors and lack adaptability. SDDS addresses these limitations by employing a Static-Dynamic Structure Fusion Graph. It first constructs static graphs using discourse parsing and speaker interaction analysis, capturing dialogue structure and speaker relationships.  A Dynamic Graph module then leverages deep utterance embeddings and multi-head attention to model semantic relationships between utterances.  These static and dynamic graphs are fused using a graph attention layer, and the fused representation is fed into a pre-trained language model for summary generation. The model incorporates heuristic methods for static graph construction, including discourse parsing, speaker interaction frequency, and relative distance encoding.  The code and data are publicly available on GitHub.  SDDS offers a more robust and adaptable approach to dialogue summarization compared to traditional methods.</sample>
    <sample id="158">The coreference resolution task aims to identify mentions referring to the same entity in a document, a computationally challenging problem due to quadratic complexity. Cache-based methods offer linear complexity but suffer from high cache misses in long documents with topic shifts, particularly for high-frequency entities. This work introduces a dual cache approach combining a local cache (LRU eviction) for local entities and a global cache (LFU eviction) for global entities.  The model scans the document, classifying mentions and adding them to either the local or global cache based on frequency.  Evaluation on benchmark datasets (LitBank, OntoNotes, WikiCoref) demonstrates that the dual cache outperforms baselines, even with unbounded memory, and achieves faster performance without training data.  A book-level evaluation further highlights the performance gap between dual cache and single cache methods.  The dual cache significantly reduces cache misses and offers the best performance-cost ratio among cache-based approaches.  In conclusion, the dual cache effectively addresses the limitations of single cache methods by leveraging both local and global entity information, leading to improved performance and efficiency.</sample>
    <sample id="159">Hallo zusammen. Ich bin Koustav Sinha, und ich freue mich, Sie zu unserem Vortrag über unseren ACL 2023-Paper begrüßen zu dürfen. Sprachmodell-Akzeptanzurteile sind nicht immer robust gegenüber dem Kontext. Dies ist eine gemeinsame Arbeit mit John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy und Adina Williams. In dieser Arbeit revisitieren wir die Minimalpair-Paradigmen. Das Minimalpair-Paradigma bewertet Sprachmodelle anhand von Akzeptanzurteilen, die auch Grammatikalität umfassen können, wie BLiMP, SyntaxGym oder Akzeptanz in Bezug auf Stereotypen wie CrowS-Paired. Im Minimalpair-Paradigma wird typischerweise gezeigt, ob eine akzeptable oder eine ungrammatikalische Satz verwendet wird. Die Hoffnung ist, dass das Modell mehr Wahrscheinlichkeit der akzeptablen Satz zuweist. Die aktuelle MPP-Pipeline ermöglicht es uns nicht, ein Modell auf längere Sätze zu bewerten. Heutzutage kommen große Sprachmodelle mit immer längeren Kontextfenstern. Daher ist es entscheidend, die Akzeptanzmodelle über das gesamte Kontextfenster zu bewerten, und das ist es, was wir hier tun wollen. Wir versuchen, das MPP-Pipeline zu revisitieren, indem wir das Modell auffordern, die Akzeptanz auf längere und längere Sequenzen zu bewerten. Das ist der Ansatz. Wir erstellen also längere Sequenzen, indem wir akzeptable oder unakzeptable Sätze aus diesen Datensätzen auswählen. Zum Beispiel haben wir eine typische Grammatikalitätspaarung aus dem BLiMP-Datensatz aus dem Adjunct Island-Fall gewählt. Wir extrahieren grammatikalisch korrekte Sätze aus Adjunct Island und fügen sie als Präfix sowohl zu den akzeptablen als auch zu den unakzeptablen Anfragen hinzu, damit wir längere Sequenzen erstellen können, die sowohl akzeptabel als auch die gleiche grammatikalische Struktur aufweisen. Wir können dies auch mit unakzeptablen Sätzen aus demselben Datensatz tun oder mit einem anderen Subset oder einem anderen Datensatz. Das nennen wir das Mismatch-Szenario. Hier kommen die Sätze immer aus relevanten Datensätzen, aber nicht aus dem gleichen Datensatz, mit dem Sie die Bewertung durchführen. Wir können dies auch für einen völlig unzusammenhängenden Bereich wie Wikipedia tun. Das wird uns zeigen, ob die Akzeptanzurteile der Modelle tatsächlich vom Kontext beeinflusst werden, d. h. ob der Kontext aus einem anderen Datensatz stammt oder ob er völlig irrelevant für den aktuellen Satz ist, den wir betrachten. Wie funktioniert das? Zuerst betrachten wir Wikipedia-Sätze, die völlig irrelevant für die aktuelle Anfragepaar sind, und dort finden wir heraus, dass die MPP-Urteile im Allgemeinen robust für beliebige Kontextlängen sind. Wir erhöhen die Kontextlänge bis zu 1024 für OPT- und GPT-2-Modelle. Und hier sehen wir in der orangefarbenen gestrichelten Linie, dass die MPP-Urteile relativ stabil sind. Was passiert, wenn wir Sätze aus demselben Datensatz auswählen? Hier wählen wir oder erstellen wir akzeptable und unakzeptable Sätze aus demselben BLiMP- oder SyntaxGym-Datensatz. Und dort sehen wir, dass die MPP-Urteile entweder signifikant ansteigen oder abfallen, wenn wir akzeptable oder unakzeptable Präfixe hinzufügen. Aber wenn wir die Struktur anpassen, d. h. Sätze aus demselben Phänomen in BLiMP oder SyntaxGym auswählen, sehen wir eine massive Erhöhung oder eine massive Abnahme der MPP-Urteile des Modells, je nachdem, ob das gewählte Präfix akzeptabel oder unakzeptabel ist. Und das ist sehr groß, dieser Effekt steigt über die Kontextlänge an, und das wird wahrscheinlich neue Sprachmodelle mit großen Kontextfenstern beeinflussen. Warum beeinflusst das Match-Präfix die Urteile des Sprachmodells so stark? Wir führten eine Reihe von Analysen durch, um zu versuchen, die Eingabesatz durch Hinzufügen von Rauschen zu verändern, wobei die relevante Struktur erhalten bleiben sollte. Nach Durchführung mehrerer dieser Störungen stellen wir fest, dass keine dieser Störungen tatsächlich die Art und Weise beeinflusst, wie das Modell seine MPP-Urteilspresse anzeigt. Wir stellen fest, dass die Modelle auf störenenden Sätzen auf ähnliche Weise reagieren. Wenn wir die Sätze im akzeptablen Bereich stören, sehen wir ähnliche Erhöhungen bei allen Störungen, und wenn wir die Sätze im unakzeptablen Bereich stören, sehen wir ähnliche Abnahmen der MPP-Urteile, je nachdem, ob das gewählte Präfix akzeptabel oder unakzeptabel ist. Die wichtigsten Erkenntnisse unserer Arbeit sind, dass Sprachmodelle empfindlich auf latente syntaktische und semantische Merkmale sind, die über die gesamte Kontextlänge hinweg gemeinsam sind. Die aktuelle MPP-Bewertungsmethode, die mit kurzen und einzelnen Sätzen durchgeführt wird, kann das abstrakte Wissen der Modelle über den Kontextfenster möglicherweise nicht vollständig erfassen. Bitte lesen Sie unseren Paper für weitere Details unserer Experimente. Vielen Dank fürs Zuhören.</sample>
    <sample id="160">Die Input-Token werden im ersten Schritt der Methode mit einem ungeordneten Multiset von Token zugeordnet, die in der Ausgabe erscheinen werden.</sample>
    <sample id="161">55,000</sample>
    <sample id="163">Die beste automatische Ausrichtungsmethode für DEPLAIN ist MASSalign.</sample>
    <sample id="164">Der Vorteil von schwach überwachtem Lernen besteht darin, dass es Daten mit schwachen Labels verwendet, was kostengünstiger ist als die manuelle Annotation, aber dennoch gute Ergebnisse erzielen kann.</sample>
    <sample id="165">This paper introduces LiPoR, an unsupervised learning method for abductive reasoning that aims to identify plausible explanations for a given outcome given a context.  Abductive reasoning, exemplified by the scenario of "Emily was stuck in traffic" leading to "Emily made it to her flight," involves selecting the most likely explanation from a set of possibilities.  Current supervised methods for abductive reasoning rely on annotated plausible explanations, which are often noisy and subjective, as demonstrated by crowd worker disagreement.  LiPoR addresses this by learning abductive reasoning without explicit plausibility annotations.  It treats explanations as latent variables and maximizes the marginal likelihood of the outcome given the context, while incorporating a regularizer based on mutual exclusivity among explanations.  This regularizer, denoted as Omega, encourages the model to favor explanations that are mutually exclusive and reduces the entropy of the probability distribution over explanations.  Experiments on the AlphaNLI dataset show that LiPoR outperforms zero-shot models and previous unsupervised approaches by over 4 absolute points in accuracy.  The paper demonstrates the feasibility of learning abductive reasoning from data alone, without relying on human annotation of plausibility.  The paper is available at tinyurl.com/zhao-lipor.</sample>
    <sample id="166">This paper introduces a novel neural framework, Neural Divide-and-Conquer Reasoning (NDCR), for image retrieval from linguistically complex text.  The problem of retrieving images based on lengthy, nuanced descriptions is challenging due to image similarity and the complexity of the text.  Existing visual-language models struggle with this task, often relying on analogical reasoning (System 1) which is insufficient for complex reasoning.  NDCR addresses this by integrating analogical and logical reasoning (System 1 and System 2).  It employs a Proposition Generator to decompose complex propositions into simpler ones, leveraging BART for sentence generation.  A Visual-Linguistic Interactor (System 1) interacts with visual and proposition information to generate matching scores and reasoning states.  A Neural-Symbolic Reasoner (System 2) then integrates these states using negation and conjunction operations to arrive at a final solution.  Experimental results demonstrate that NDCR outperforms baseline methods, with ablation studies validating the individual module contributions.  The framework allows for the presentation of intermediate inference states and results, enabling interoperable processing.  The authors suggest that neural symbolic calculation and the integration of Dual-Process Theory with Divide-and-Conquer strategies offer promising avenues for enhancing compositional reasoning and planning in large language models.</sample>
    <sample id="167">Die Dokumente in DEPLAIN-web wurden sowohl manuell als auch mit automatischen Alignmentmethoden ausgerichtet.</sample>
    <sample id="168">Der CoNLL++-Datensatz wurde aus Reuters News von 2020 gesammelt und mit den gleichen CoNLL-2003-Annotationsgrundsätzen annotiert.</sample>
    <sample id="169">This paper presents a systematic study of prompting strategies for machine translation using the PaLM language model, a 540 billion parameter model trained on 780 billion tokens.  The authors evaluated the transition capability of PaLM by comparing its performance to state-of-the-art systems on the WMT evaluation using recent test sets.  They found that prompting significantly influences translation performance, with one-shot prompting showing substantial improvements (up to 40 BLEURT points) compared to baseline models.  While prompt form is less critical for multiple examples (5-shot), example quality is paramount.  The study highlights that high-quality translations are more important than prompt similarity.  Human evaluation using the MQM framework revealed that PaLM achieves comparable fluency to state-of-the-art systems but struggles with accuracy, particularly omission errors.  The model tends to prioritize fluent output over complete accuracy.  The findings suggest that careful prompt selection, focusing on high-quality examples, is crucial for leveraging the potential of large language models for machine translation.</sample>
    <sample id="170">Hallo zusammen, mein Name ist Yusen Zhang von der Penn State University. Heute präsentiere ich unsere Arbeit "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations". Semantic Parsing ist eine Aufgabe, um semantische Repräsentationen von Benutzerabfragen zu erstellen, wie z. B. SQL und Lambda Calculus. Cross-Lingual Semantic Parsing ist die Aufgabe, Abfragen in mehreren natürlichen Sprachen in mehrere Bedeutungssimulationen zu übersetzen. Wie auf der Abbildung gezeigt, müssen wir Abfragen in mehreren natürlichen Sprachen mithilfe neuronaler Modelle in SQL, Lambda oder FunQL usw. übersetzen. Bestehende cross-linguale Semantic-Parsing-Modelle werden separat auf Datensätzen mit begrenzten Aufgaben und Anwendungen vorgeschlagen und bewertet. Zum Beispiel gibt es eine große Abdeckung in bestimmten natürlichen Sprachen. Chinesisch fehlt und es gibt eine mangelnde Abdeckung in bestimmten Bedeutungssimulationen. Lambda Calculus fehlt oder sie werden nur auf bestimmte neuronale Modelle bewertet. Zum Beispiel gibt es nur ein einziges Modell, um sie zu bewerten. Um dies zu erreichen, schlagen wir XSemPLR vor. Wir stellen einen einheitlichen Datensatz XSemPLR für cross-linguale Semantic-Parsing in mehreren natürlichen Sprachen und Bedeutungssimulationen bereit. Er enthält 9 Datensätze in verschiedenen Domänen, 5 Semantic-Parsing-Aufgaben, 8 Bedeutungssimulationen und 22 natürliche Sprachen in 15 Sprachfamilien. Um unseren Benchmark besser zu bewerten, berücksichtigen wir sechs Einstellungen für Training und Bewertung. Die erste ist Translate-Test. Wir verwenden die Google Translate API, um die Quelle in die Zielsprache zu übersetzen, und verwenden dann ein monolinguales Modell zum Trainieren und Bewerten. Zum Beispiel trainieren wir das englische Modell auf englischen Abfragen und während der Inferenz übersetzen wir die deutsche Abfrage mithilfe der API ins Englische und verwenden dann das trainierte Modell, um die SQL vorherzusagen. Wir testen auch das Monolinguale Modell. In dieser Einstellung ist die Quellsprache gleich der Zielsprache, z. B. Deutsch zu Deutsch oder Englisch zu Englisch. Wir testen auch die Monolinguale Few-shot-Einstellung, indem wir monolinguale Modelle mit nur 10 % der Trainingsdaten trainieren. Und wir testen das Multilingual-Modell, indem wir ein Multilingual-Modell für alle Sprachen trainieren. Zum Beispiel kombinieren wir deutsche, englische und chinesische Abfragen, um ein Multilingual-Modell zu trainieren. Während der Inferenz können wir dieses Modell verwenden, um deutsche oder chinesische Abfragen zu übersetzen usw. Wir berücksichtigen auch Cross-lingual Zero-shot- und Few-shot-Transfer. Wir trainieren auf einer Quellsprache und übertragen sie auf eine andere Sprache. Während des Trainings trainieren wir es auf englischen Abfragen oder der Kombination von englischen und deutschen Few-shot-Abfragen, um ein Multilingual-Modell zu trainieren, um die SQL-Ausgabe vorherzusagen. Wir stellen auch viele interessante Ergebnisse fest. Bezüglich der Analyse monolingualer Modelle bewerten wir zwei Gruppen von Modellen, darunter Encoder-PTR, das für Multilingual Pretrained Encoders mit Pointer-basierten Decodern steht, wie z. B. XLM-R + PTR und mBERT + PTR. Wir bewerten auch Encoder-Decoder-Modelle, die Multilingual Pretrained Encoder-Decoder-Modelle sind, wie z. B. mBART und mT5. Wir haben festgestellt, dass Encoder-Decoder die beste Leistung auf allen neun Datensätzen erzielen. Wir bewerten mT5 und XLM-R + PTR auf der Multilingual-Einstellung. Wir haben festgestellt, dass Encoder-Decoder oder Encoder-PTR durch Training in einer Mischung aus verschiedenen Sprachen verbessert werden können. Dies liegt daran, dass die meisten wichtigen natürlichen Sprachen Leistungssteigerungen erzielen, außer Englisch, das auf sieben Datensätzen eine Leistungsabnahme und auf drei Datensätzen eine Leistungssteigerung aufweist. Dies wird als der "Fluch der Multilingualität" bezeichnet. Wir vergleichen auch die cross-linguale Leistungsspanne. Die blaue Linie ist Cross-lingual Few-shot-Transfer, die orange Linie ist Cross-lingual Zero-shot-Transfer und die grüne Linie ist die Monolinguale Einstellung. Wir haben festgestellt, dass die Transferleistungspanne im Zero-shot-Setting erheblich ist und dass die Transferspanne im Few-shot-Setting mit der blauen Linie und der orange Linie schnell verringert wird. Wir stellen auch einige andere interessante Ergebnisse fest. Zum Beispiel übertreffen Encoder-Decoder frühere Arbeiten oder erzielen vergleichbare Ergebnisse. Das Vortrainieren auf englischer natürlicher Sprache kann die Leistung von Few-shot auf Zielnatürliche Sprachen erheblich verbessern und wir haben festgestellt, dass Multilingual-Sprachmodelle wie Codex und BLOOM immer noch unzureichend für cross-linguale Semantic-Parsing-Aufgaben sind. Um zusammenzufassen, bauen wir XSemPLR, einen einheitlichen Benchmark für cross-linguale Semantic-Parsing mit mehreren natürlichen Sprachen und Bedeutungssimulationen. Wir führen eine umfassende Benchmark-Studie auf drei repräsentativen Arten von Multilingual-Sprachmodellen durch. Unsere Ergebnisse zeigen viele interessante Ergebnisse. Und usw. Und wir laden Sie ein, unseren Paper und Code zu besuchen. Vielen Dank fürs Zuhören.</sample>
    <sample id="171">Existing works can be broadly classified into four categories, but these methods either are not applicable to embedding as services or lack transferability.</sample>
    <sample id="172">Nein, mehrsprachige LLMs wie Codex und BLOOM sind für CLSP nicht ausreichend.</sample>
    <sample id="174">```
This presentation introduces ArgAnalysis35K, a novel dataset for argument quality analysis, addressing limitations of existing datasets.  The dataset boasts the largest collection of high-quality arguments (35K) sourced from expert and intermediate debaters, offering superior quality compared to crowdsourced data.  It distinguishes itself through a diverse range of arguments, covering 24 themes derived from parliamentary debate sources, rather than relying on pre-selected motions.  Crucially, ArgAnalysis35K incorporates "analysis" – a combination of claims and premises – to provide a more comprehensive understanding of argument strength, a concept not commonly found in NLP datasets.  Furthermore, it employs instance-based annotator reliability, mitigating bias by focusing on individual argument reliability rather than entire annotator groups.  Finally, a relevance model assesses the applicability of arguments across various themes, enhancing the dataset's utility for understanding argument relevance.  ArgAnalysis35K aims to provide a more robust and nuanced resource for research in argument mining and analysis.
```</sample>
    <sample id="175">Die Methode adressiert die Mehrdeutigkeit der Permutationen, indem sie die linguistisch korrekte Permutation als latent betrachtet und diese während des Trainings induziert. Dies geschieht durch eine Approximation des Traveling Salesman Problems, die es ermöglicht, die linguistisch plausibelste Permutation zu lernen.</sample>
    <sample id="176">Das Fairnessproblem wird durch die unterschiedliche Leistung von Sprachmodellen mit unterschiedlichen politischen Neigungen in nachgelagerten NLP-Aufgaben verdeutlicht. Die Ergebnisse zeigen, dass Modelle mit linken Neigungen bei der Erkennung von Hassreden gegen Minderheiten besser sind, aber bei der Erkennung von Hassreden gegen mächtigere Gruppen schlechter sind. Umgekehrt sind Modelle mit rechten Neigungen bei der Erkennung von Hassreden gegen weiße und Männer besser, aber bei der Erkennung von Hassreden gegen schwarze LGBTQ+- und andere Minderheiten schlechter. Diese unterschiedliche Leistung deutet auf Fairnessprobleme hin, da sie zu einer Benachteiligung bestimmter Gruppen führen kann.</sample>
    <sample id="177">Yanis Labrak.</sample>
    <sample id="178">Koustav Sinha.</sample>
    <sample id="179">## Abstract

Large Language Models (LLMs) struggle with Theory of Mind (ToM) reasoning, particularly in understanding false-belief scenarios. This work introduces SymbolicToM, a plug-and-play inference-time method to enhance ToM capabilities in LLMs using explicit graphical representations of mental states. SymbolicToM computes belief graphs for all character combinations up to a defined level, leveraging off-the-shelf Natural Language Inference (NLI) and OpenIE models. These graphs enable efficient question answering by recursively querying the graph and integrating the results with a language model.

Experiments with various LLMs demonstrate significant performance gains (up to 65 accuracy points) compared to out-of-the-box performance on the ToMi dataset.  Furthermore, SymbolicToM exhibits strong generalization across different story structures and linguistic variations, outperforming supervised baselines and even enabling state-of-the-art models like GPT-4 to achieve full performance on novel datasets.  This approach avoids overfitting and provides more interpretable reasoning. SymbolicToM offers a promising solution for improving the robustness and reliability of LLMs in tasks requiring nuanced understanding of human psychology.</sample>
    <sample id="180">Myra, Esin Durmus und Dan Jurafsky.</sample>
    <sample id="181">This paper addresses the challenge of constrained language planning, extending the work on abstract goal planning for stereotypical activities to scenarios with specific constraints.  We define constrained language planning as the problem of generating scripts that satisfy multi-faceted constraints on goals, a gap in current research.  Due to the lack of existing datasets for specific goals, we leverage InstructGPT to generate 100 specific goals with constraints, evaluating the resulting scripts.  Initial results show that large language models struggle with constrained planning, exhibiting acceptable semantic completeness but lacking constraint faithfulness.  We propose an "over-generate-then-filter" method to improve generation quality by iteratively generating scripts, filtering for constraint faithfulness, and incorporating keyword rewards.  This method significantly enhances the quality of generated scripts.  Furthermore, we address the challenge of creating high-quality datasets for constrained language planning by employing symbolic knowledge distillation.  We generate a large dataset of 55,000 specific goals and scripts, named CoScript, using InstructGPT and validating it with human feedback.  We demonstrate that fine-tuning smaller models like T5 on CoScript yields superior performance compared to larger models, highlighting the potential of specialized models for constrained language planning.  CoScript aims to facilitate research in this area by providing a valuable resource for training smaller, more efficient language models.</sample>
    <sample id="182">Der Tropikalismus bezieht sich auf die Verbindung von Begriffen, die Latina-Frauen beschreiben, wie "vibrant" und "curvaceous", zu einer historischen Stereotypisierung von Frauen aus tropischen Regionen als exotisch.</sample>
    <sample id="183">Die Autoren haben die von Menschen verfassten Beschreibungen der Zielgruppen durch das Auffordern von menschlichen Personen, die Beschreibungen zu erstellen, erstellt, ähnlich wie sie es bei der Generierung der Personas mit dem LLM getan haben.</sample>
    <sample id="184">CXMI (Contextualized Machine Translation Information) wurde verwendet, um die Kontextnutzung von Machine-Translation-Modellen zu messen.</sample>
    <sample id="185">DrBERT ist ein neu trainiertes biomedizinisches Sprachmodell in Französisch, das auf dem NACHOS-Datensatz trainiert wurde, während ChuBERT ein klinisches Modell ist, das auf anonymisierten Daten aus dem Nantes University Hospital Data Warehouse trainiert wurde.</sample>
    <sample id="187">Ying und Zhiyang.</sample>
    <sample id="188">Iteratives Transferlernen aktualisiert ein Modell durch Training auf dem neuesten Datensatz, der in jeder Runde der aktiven Annotation gesammelt wird.</sample>
    <sample id="189">Der Datensatz zielt darauf ab, das Verständnis von Benutzersprache zu verstehen, wenn sie eine Wahl treffen müssen, indem er indirekte Referenzen für die Entitätssuche untersucht.</sample>
    <sample id="190">Ein Angreifer kann ein Modell über einen EaaS extrahieren, indem er die Embedding-Werte aus dem Service abruft und diese mit einem Backdoor-Mechanismus manipuliert, der die Modellparameter enthält.</sample>
    <sample id="191">Drei.</sample>
    <sample id="192">CAME (Confidence-guided Adaptive Memory Efficient Optimization) addresses the challenge of balancing fast convergence with low memory usage in large language model training, a problem exacerbated by the memory overhead of traditional optimizers like Adam and the performance limitations of memory-efficient methods like Adafactor.  The work draws inspiration from Non-negative Matrix Factorization (NMF) to reduce memory footprint, but recognizes that NMF-based optimizers can suffer from erroneous updates, leading to slow convergence. CAME mitigates this by leveraging the residual between predicted and actual updates to dynamically adjust the optimization step size. This "instability" is used as a denominator, allowing for more adaptive and stable updates.  Experiments on BookCorpus and English Wikipedia, and comparisons with BERT, GPT-2, and T5, demonstrate that CAME achieves significant improvements over Adam and Adafactor, including increased validation accuracy (3.4% improvement over Adafactor) and better performance with larger batch sizes.  CAME also reduces memory consumption compared to existing memory-efficient optimizers like Adam and LAMB.  The proposed optimizer effectively supports large batch training and offers a promising solution for efficient training of large language models.</sample>
    <sample id="193">Das Paper erwähnt nicht explizit die Anzahl der Annotatoren, die für die Erstellung des ursprünglichen Datensatzes verwendet wurden. Es wird jedoch erwähnt, dass 1.000 Beispiele für Discourse Unit Pairs gesammelt wurden.</sample>
    <sample id="194">Jenny ist eine erste Semester-Doktorandin an der Carnegie Mellon University. Die Autoren arbeiten auch mit der University of Washington und dem Allen Institute for AI zusammen.</sample>
    <sample id="195">This paper introduces RoHT, a novel framework for explainable question answering (XQA) that addresses the limitations of existing approaches. While neuro-symbolic methods struggle with incomplete knowledge bases and decompose-based methods face challenges with natural language diversity, RoHT leverages hierarchical question decomposition to integrate knowledge from heterogeneous sources effectively. The framework builds a Hierarchical Question Decomposition Tree (HQDT) to understand the compositional structure of complex questions, decomposing them into atomic questions.  Probabilistic reasoning is then applied over the HQDT, fusing knowledge from knowledge bases and text corpora at different levels, considering both string generation and answering probabilities.  The reasoning process recursively explores the tree, selecting appropriate knowledge sources (KB, text, or recursive calls) and aggregating candidate answers.  Experiments on challenging datasets KQA Pro and Musique demonstrate RoHT's superior performance compared to existing methods, particularly when integrating knowledge from both sources.  RoHT outperforms KB QA methods, decompose-based methods, and end-to-end models like TransferNet.  The results highlight the benefits of explicit question decomposition and knowledge integration for achieving more accurate and explainable XQA.</sample>
    <sample id="196">"I saw Bart and Lisa"</sample>
    <sample id="197">Der Stand der Technik für Dialogsysteme umfasst die Verwendung von menschlichen Bewertungen, wie z. B. der Auswahl der besseren von zwei Gesprächen oder der Bewertung von Gesprächen anhand einer Likert-Skala. Diese Methoden sind gut geeignet, um eine umfassende Bewertung der Gesamtqualität des Dialogs zu liefern, haben aber viele Aspekte des Dialogs.</sample>
    <sample id="198">Das aktuelle MPP-Pipeline kann nicht die Akzeptanz von Modellen über längere Sätze bewerten, da moderne LLMs längere Kontextfenster haben. Daher ist es wichtig, die Akzeptanz der Modelle über das gesamte Kontextfenster zu bewerten.</sample>
    <sample id="199">Das mehrsprachige Training führte zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell, wobei die Leistung in sieben Datensätzen zurückfiel.</sample>
    <sample id="200">Der Text besagt, dass die Annotatoren die Namen der Entitäten kennen, aber nicht unbedingt die Entitäten selbst.</sample>
    <sample id="201">Neural MT metrics und menschliche Bewertung (MQM).</sample>
    <sample id="202">Das Paper untersucht die Generalisierung von NER-Modellen, aber es gibt keine Hinweise darauf, dass die Regression bei der Generalisierung auf bestimmte NER-Typen auswirkt. Die Studie konzentriert sich auf die allgemeine Generalisierung von Modellen, die auf dem CoNLL-2003-Datensatz trainiert wurden, und findet, dass die Generalisierung durch Modellarchitektur, Modellgröße und die Anzahl der Feinabstimmungsexemplare beeinflusst wird.</sample>
    <sample id="203">Die Positionalität in NLP ist wichtig, weil Datensätze und Modelle die Perspektiven derjenigen widerspiegeln, die sie erstellen und trainieren, was zu systematischen Leistungsschwankungen zwischen verschiedenen Bevölkerungsgruppen führen kann. Dies kann zu ungleichen Ergebnissen und einer Benachteiligung bestimmter Gruppen führen.</sample>
    <sample id="204">Nein, die mehrsprachigen LLMs wie Codex und BLOOM wurden nicht ausreichend für die Aufgabe des mehrsprachigen semantischen Parsings angepasst.</sample>
    <sample id="205">## Abstract

This work investigates the propagation of political biases from pretraining data to language models and their impact on downstream NLP tasks. Leveraging the C4 corpus, we demonstrate that language models exhibit varying political leanings, with GPT-4 leaning more liberal than other models.  We further explore how these biases are amplified through further pretraining on partisan corpora, revealing a shift towards more polarized viewpoints, particularly after 2017.  

Evaluation on hate speech and fake news detection tasks reveals that language models with different political leanings exhibit disparate performance across demographic groups, highlighting potential fairness issues. Left-leaning models show better detection of hate speech targeting minority groups, while right-leaning models perform better on hate speech targeting dominant groups.  Similar patterns emerge in fake news detection.

The study underscores the challenge of mitigating political biases in language models, presenting a dilemma between perpetuating bias and risking censorship.  We conclude that addressing these biases is crucial to ensure fairness in NLP applications, particularly in contexts where language models are deployed for critical tasks like hate speech detection and misinformation control.</sample>
    <sample id="206">Transferlernen wird von zwei verschiedenen Aufgaben verwendet: Stimmungs-unabhängige Stanceklassifizierung (Debate) und die binäre Klassifizierung von Expansion und Vergleich (CE).</sample>
    <sample id="207">Die neuesten Testsets wurden verwendet, um einen Übersetzungsansatz zu vermeiden, der mit den Trainingsdaten des Sprachmodells übereinstimmt.</sample>
    <sample id="208">Die Autoren haben drei Empfehlungen vorgeschlagen.</sample>
    <sample id="209">Die vorgeschlagene Methode verbessert die Qualität der Skripte sowohl in Bezug auf die semantische Vollständigkeit als auch auf die Einhaltung der Einschränkungen im Vergleich zu den meisten großen Sprachmodellen. T5, das auf CoScript feinabgestimmt wurde, generiert Skripte von höherer Qualität als die meisten großen Sprachmodelle.</sample>
    <sample id="210">Shuheng.</sample>
    <sample id="211">Ja, die Ergebnisse und der Datensatz DEPLAIN können als Benchmark für die Textsimplifizierung verwendet werden. Dies wird durch die Verwendung manuell alignierter Daten zur Bewertung von automatischen Alignment-Methoden und durch die Verwendung von feinabgestimmten Sprachmodellen zur Erzeugung von vereinfachtem Text demonstriert.</sample>
    <sample id="212">In der Arbeit wird mit T5 experimentiert, das auf CoScript feinabgestimmt wurde.</sample>
    <sample id="213">OFA</sample>
    <sample id="215">## Abstract

This paper argues for the validity of symmetric dependency structures in coordination, contrasting them with asymmetric approaches. The central argument relies on dependency length minimization, a principle observed in English where direct objects are preferred near the verb, while adjuncts are further away.  The authors demonstrate that this principle is stronger when the governor of the coordination is positioned on the left or absent. 

Analyzing statistics from the Penn Treebank, the study reveals a tendency for the left conjunct to be shorter in coordinated phrases when the governor is on the left or absent. This effect is more pronounced with larger length differences between conjuncts. Conversely, when the governor is on the right, this tendency diminishes.  

The paper provides empirical evidence through length measurements (characters, syllables, words) to support this observation.  By quantifying the dependency length and the governor's position, the authors show that the observed length minimization favors symmetric coordination structures, challenging the validity of asymmetric approaches like universal dependencies and the Prague approach. The findings suggest that the inherent structure of coordination in natural language is more aligned with symmetric dependency patterns.</sample>
    <sample id="217">This paper addresses the challenge of compositional generalization in multi-attribute controllable dialogue generation (CDG). Existing methods often focus on single attributes or rely on limited annotated data, hindering the ability to generate coherent dialogues with unseen attribute combinations. We introduce DCG, a Disentangled Controllable Generation model based on DialoGPT, that learns attribute concepts from seen values using a disentanglement loss.  We propose a unified reference-free evaluation framework, MAE, for various attribute granularities.  Our approach utilizes attribute-oriented and task-oriented prompts to guide the model, and a disentanglement loss to learn compositional representations. Experiments on DailyDialog-CG demonstrate that DCG outperforms baselines in both attribute controllability and text equality, even with minimal performance drop on established metrics like E-ACC and A-ACC.  Furthermore, DCG achieves superior performance compared to CTRL on unseen attribute combinations.  We show that attribute-oriented prompts effectively guide the model to focus on controllable information, while task-oriented prompts improve text equality.  MAE demonstrates strong correlation with human judgments, and its effectiveness is validated across different pre-trained language models.  Our results highlight the potential of prompt-based disentangled models for robust and generalizable multi-attribute CDG.</sample>
    <sample id="218">Google Translate.</sample>
    <sample id="219">This work addresses the challenge of extracting financial signals from annual reports (Form 10-K) by leveraging the inherent similarity between yearly reports.  The authors propose a novel compare-and-contrast multistage pipeline for highlighting relevant information.  The core idea is to identify the rationale (words) that explain the relationship between a target report and its previous year's report.  The pipeline consists of document segmentation, relation recognition, and out-of-domain and in-domain fine-tuning.  The relation recognition stage classifies report pairs into three types: highly similar, revised, and mismatched.  Fine-tuning utilizes the eSNLI dataset for out-of-domain adaptation and revised pairs for in-domain training, employing soft labeling techniques to mitigate issues with low-quality pseudo-labels.  The evaluation is performed on both eSNLI and a newly released FINAL dataset, using precision and PCC as metrics.  The results demonstrate that the proposed domain-adaptive highlighting model achieves state-of-the-art performance on FINAL and maintains generalization capabilities on eSNLI.  Furthermore, the model benefits from incorporating mismatched pairs during simulation.  The authors conclude by highlighting the potential for future research to enhance the model's effectiveness and applicability in information retrieval.</sample>
    <sample id="220">Vasudha ist eine Doktorandin der Informatik an der Stony Brook University.</sample>
    <sample id="221">Deutsch ins Englische.</sample>
    <sample id="222">```
This work addresses the challenge of domain adaptation in open-domain question answering (QA), where models trained on general-purpose corpora like Wikipedia struggle to generalize to specialized domains. We investigate data interventions to enable out-of-domain generalization, identifying the type of dataset shift encountered and determining the effectiveness of different interventions for each shift type.

We evaluate a Wikipedia-based source domain against seven target datasets spanning six domains, employing both zero-shot and few-shot data interventions. Few-shot methods leverage limited target data to prompt large language models for generating additional examples, improving retriever and reader performance. Zero-shot techniques control the interaction of question, answer, and context to understand their impact on model learning.

We categorize dataset shifts into "no shift," "concept shift," "covariate shift," and "full shift," measuring compatibility between source and target models using likelihood calculations. Our findings reveal that few-shot adaptations are effective across all shift types, while zero-shot methods are particularly beneficial for concept and covariate shifts.  We demonstrate that data interventions can improve reader performance by up to 24%, highlighting the importance of tailoring interventions to the specific characteristics of the target domain.
```</sample>
    <sample id="223">Shangbin.</sample>
    <sample id="224">MASSalign und fine-getunte Long-mBART und base mBART Modelle.</sample>
    <sample id="225">Die 62 verschiedenen Aufgaben in MultiInstruct werden für Training und Tests verwendet.</sample>
    <sample id="226">Die Antwort ist nicht im bereitgestellten Text enthalten.</sample>
    <sample id="227">This paper addresses the challenge of grounded language understanding, where natural language expressions are mapped to executable plans or programs in specific environments. While large language models (LLMs) show promise, their pre-training on text corpora without grounding leads to limitations in generating valid and executable plans, hindering applications like smart assistants and robotic control. 

The authors propose a novel framework called Pangu, which shifts the focus from generation to discrimination. Pangu utilizes a symbolic agent to interact with the environment and propose candidate plans, while an LLM scores and ranks these candidates. This approach avoids the LLM's responsibility for plan validity and grammar. 

Experiments on knowledge-based question answering demonstrate Pangu's strong performance across various LLMs (BERT, T5, Codex) and training methods (fine-tuning, in-context learning), exhibiting excellent sample efficiency. Pangu consistently outperforms baselines like ArcaneQA, particularly in in-context learning scenarios.  The framework also shows robustness to non-independent and identically distributed (non-i.i.d.) data, suggesting a strong generalizability. 

The key takeaway is that discrimination, rather than generation, offers a more effective strategy for leveraging LLMs in grounded language understanding.</sample>
    <sample id="228">AG News, MIND, SST2 und Enron Spam.</sample>
    <sample id="229">## Abstract

This paper addresses the challenge of automatically detecting improvable claims in argumentative writing, a crucial step in supporting writing improvement. We introduce two new tasks: Suboptimal Claim Detection (determining if a claim needs revision) and Claim Improvement Suggestion (identifying specific quality issues for revision).  We explore the complexities of leveraging revision-based data, focusing on argumentative text extracted from collaborative online debate platforms like Kialo. 

Our work highlights four key challenges: Representativity and Reliability of the dataset, Model Complexity and Architecture for effective claim assessment, the dependence of argument quality on contextual information, and the presence of Topical and User Bias in revision histories. We investigate these challenges through experiments and propose strategies for addressing them.  

Our findings demonstrate that revision-based data can be effectively utilized for claim assessment, with modeling the distance between claim versions proving beneficial for suboptimal claim detection.  Furthermore, the impact of contextual information varies depending on the task and the specific quality issues being addressed.  We conclude that a nuanced approach to claim assessment is necessary, considering both the textual content and the surrounding context.</sample>
    <sample id="231">NACHOS ist ein Datensatz medizinischer crawled Daten vom Web.</sample>
    <sample id="232">David Vilar.</sample>
    <sample id="233">Simultaneous Speech Translation (SimuST) aims to translate spoken language to text in real-time. Current SimuST models face challenges with long training procedures, optimization objectives, and maintaining multiple models for varying latency. This paper introduces EDAtt (Encoder-Decoder Attention), a strategy for achieving low-latency SimuST by leveraging existing offline ST models. EDAtt utilizes cross-attention to determine when to emit partial translations. It decides to emit a word if its cross-attention weights are below a threshold, indicating stable information. This approach avoids retraining and adapting architectures for different latency regimes, allowing for a single model to handle various latency requirements. Experimental results on German demonstrate that EDAtt outperforms existing strategies like Wait-k and Local Agreement, achieving higher BLEU scores with significantly reduced average lagging.  The computational-aware latency metric further highlights EDAtt's efficiency.  The code and models are publicly available to promote reproducibility.</sample>
    <sample id="234">Die Prompt-Strategie hat einen großen Einfluss auf die Ergebnisse, insbesondere bei Ein- und Zwei-Shot-Prompting, wo die Unterschiede mehr als ein BLEURT-Punkt betragen können und in extremen Fällen bis zu 40 BLEURT-Punkte erreichen. Bei fünf-Shot-Prompting ist die Form der Prompting-Strategie weniger wichtig als die Qualität der Beispiele.</sample>
    <sample id="235">Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig gehören der Universität Toronto an.</sample>
    <sample id="236">```
The presentation does not explicitly state the content of the 5 expert-written instructions. It only mentions that each task in MultiInstruct is equipped with five expert-written instructions.
```</sample>
    <sample id="237">Die Autoren schlagen vor, einen diagnostischen Test-Suite namens KITMUS zu verwenden, der eine Coreference-Resolution-Aufgabe beinhaltet. Diese Aufgabe testet, ob Modelle Informationen aus verschiedenen Quellen (z. B. Pretrain-Zeit- und Inference-Zeit-Wissen) nutzen können, indem sie die Verfügbarkeit von Hintergrundwissen und entity-spezifischem Wissen in verschiedenen Szenarien variieren.</sample>
    <sample id="238">MeetingBank is a new benchmark dataset for meeting summarization, created by the University of Central Florida. Addressing the challenges of high-quality summaries and trustworthy public meeting resources, the dataset comprises 1,366 City Council meetings with transcripts, reference summaries, and URLs. Data collection utilizes the Speechmatics API for transcript conversion and the Boston City Council website for meeting identification and summary retrieval.

The dataset includes statistics on meeting characteristics (duration, tokens, speakers, year) and summarization instance details (average sentence/token count).  Coverage and density scores are used to analyze summary abstraction levels, revealing a preference for verbatim points.  Extractive summarization systems like Oracle and LEAD show promising ROUGE-2 scores, while DialogLM excels in abstractive summarization.  GPT-3 demonstrates strong fluency and coherence but weaker informativeness and factuality according to automatic metrics and human evaluation.

The study highlights the need for improved automatic evaluation metrics aligned with human preferences. MeetingBank offers a valuable resource for researchers to develop advanced meeting summarization technologies and gain insights into City Council decision-making processes. The dataset is publicly available for download and use.</sample>
    <sample id="239">Hallo zusammen, mein Name ist David Vilar, und ich werde eine kurze Übersicht über die Arbeit "Prompting PaLM for Translation: Assessing Strategies and Performance" geben. Dies ist eine gemeinsame Arbeit mit meinen Kollegen von Google Translate. PaLM ist ein 540 Milliarden Parameter großes großes Sprachmodell, das 2022 vorgestellt wurde. Es wurde auf einer großen Sammlung von Texten trainiert, die 780 Milliarden Token umfassen. Zu dem Zeitpunkt der Veröffentlichung erreichte es in Hunderten von NLP-Aufgaben den Stand der Technik. In dieser Arbeit präsentieren wir die erste systematische Studie zur Verwendung von Prompting für große Sprachmodelle bei der maschinellen Übersetzung. Wir haben die Fähigkeit dieser Modelle zur Übergangsleistung anhand der Best Practices der MT-Community bewertet. Dies beinhaltet die Verwendung der neuesten Testdatensätze, um einen Datenüberschneidungsfehler zwischen den Testdaten und den Trainingsdaten des Sprachmodells zu vermeiden. Und wir haben es mit den State-of-the-Art-Systemen verglichen, also dem besten performenden System, dem WMT-Bewertungsverfahren. Wir verwenden State-of-the-Art-neuronale MT-Metriken und zeigen zusätzlich Ergebnisse einer Expertenbewertung durch menschliche Bewerter. Schließlich geben wir einige Empfehlungen für Prompt-Auswahlstrategien. Das Prompting hat einen großen Einfluss auf die Leistung von LLMs bei der Übersetzung, wie wir in einem einfachen Experiment sehen können, bei dem wir eine Ein-Schuss-Prompting-Methode verwendeten und für jede Satzgruppe zwei verschiedene Prompts bereitstellten. Die Mehrheit der Sätze, 516 von 1.000, zeigte eine Differenz von mehr als einer BLEURT-Punktzahl. Dies kann in extremen Fällen bis zu 40 BLEURT-Punkte erreichen. Daher ist es wichtig, eine gute Prompting-Strategie auszuwählen. In unseren Experimenten haben wir uns für eine 5-Schuss-Prompting-Strategie entschieden, bei der wir einfach jeden Satz, den wir dem System zur Verfügung stellen, mit der Sprache markieren, in der er geschrieben ist. In diesem Beispiel hier, bei der Übersetzung von Deutsch nach Englisch, sind die deutschen Sätze, die Quellsätze, mit Deutsch-Doppelpunkt markiert und die englischen Übersetzungen mit Englisch-Doppelpunkt. Wir sahen, dass die tatsächliche Form des Promptings keinen großen Einfluss auf die Leistung bei mehreren kurzen Promptings hat. Es ist entscheidend für Ein-Schuss- und Null-Schuss-Prompting. Und wenn wir, wie in unserem Fall, zu fünf-Schuss-Prompting übergehen, gibt es kaum eine Differenz zur tatsächlichen Form des Promptings. Die Beispiele tragen den größten Teil der Gewichtung. Die Zusammenfassung unserer experimentellen Ergebnisse ist, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit zur Quellsatz. Es ist wichtig, Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die Auswahl von Prompts aus den Trainingsdaten für die WMT-Bewertungen auf dem Entwicklungsdatensatz. Der Entwicklungsdatensatz ist viel besser kuratiert und von höherer Qualität als der Trainingsdatensatz, was bedeutet, dass er lauter ist. Und ihre Ergebnisse zeigen eine bessere Leistung bei der Verwendung des Entwicklungsdatensatzes. Dennoch haben spezialisierte State-of-the-Art-Systeme einen erheblichen Vorteil gegenüber den PaLM-Übersetzungen. Aber PaLM kommt ziemlich nahe an ein kommerzielles System heran. In unserem Fall haben wir mit Google Translate bewertet. Die Erkenntnisse, die wir aus der menschlichen Bewertung gewonnen haben, die wir mit dem MQM-Framework durchgeführt haben, zeigten, dass die Flüssigkeit von PaLM vergleichbar mit State-of-the-Art-Systemen ist, aber die Hauptunterscheidung besteht in der Genauigkeit. Insbesondere sind die häufigsten Fehler Auslassungsfehler. Es scheint, dass PaLM dazu neigt, eine besser klingende Übersetzung zu erzeugen, indem es Teile des Quellsatzes auslässt, die in der Übersetzung entstanden sind. Allerdings ist die Kategorie "Stil/Unangenehm" für PaLM niedriger als bei den State-of-the-Art-Systemen, was ein zusätzliches Signal dafür ist, dass PaLM wirklich flüssige Ausgaben liefert, aber immer noch mit einigen Genauigkeitsproblemen. Und das war es für diese wirklich kurze Übersicht. Für weitere Details kommen Sie bitte zur vollständigen Präsentation der Arbeit. Vielen Dank.</sample>
    <sample id="240">Hallo, ich bin Dawei, ein Doktorand an der Saarland-Universität in Deutschland. In diesem Video möchte ich unsere aktuelle Arbeit "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" vorstellen. Dies ist eine gemeinsame Arbeit mit Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow. Ich möchte mit einer kurzen Einführung in schwach überwachtes Lernen und schwach überwachtes Lernen beginnen. Bei schwach überwachtem Lernen werden die Daten nicht manuell gelabelt. Stattdessen werden die Daten mit schwachen Labeling-Quellen gelabelt, wie z. B. einfachen Heuristiken, Wissensbasen oder niedrigwertiger Crowdsourcing, wie in der Abbildung auf der rechten Seite dargestellt. Im Vergleich zu menschlichen Annotationen sind die schwachen Annotationen viel günstiger, aber auch verrauscht, d. h. eine gewisse Anzahl der Annotationen ist falsch. Wenn wir direkt neuronale Netze auf schwach gelabelten Daten trainieren, neigen die neuronalen Netze dazu, die Label-Störungen zu merken und nicht zu generalisieren. Bei schwach überwachtem Lernen werden Trainingsalgorithmen vorgeschlagen, um neuronale Netze robust unter solchen Label-Störungen zu trainieren, so dass die trainierten Modelle immer noch gut generalisieren. In jüngsten Arbeiten zum WSL (Weakly Supervised Learning) wird oft behauptet, dass man nur auf den schwach gelabelten Daten trainiert und auf sauberen Testdatensätzen hohe Leistungen erzielt. Technisch gesehen ist diese Behauptung nicht falsch, aber es gibt einen Haken, nämlich dass man davon ausgeht, dass ein zusätzlicher sauberer Validierungsdatensatz verfügbar ist. Wir können nicht auf diesem Problemstellung stoppen, aber dies impliziert, dass zusätzliche manuelle Annotationen erforderlich sind, wenn es sich um schwach überwachtes Lernen handelt. Aber wie ein Elefant im Raum wird diese Notwendigkeit oft übersehen. Die oben genannte Frage wird gestellt: Zuerst, ist ein sauberer Validierungsdatensatz für WSL erforderlich oder kann man vielleicht einen verrauschten Validierungsdatensatz verwenden? Zweitens, wenn sauberer Daten erforderlich ist oder wenn sauberer Daten für WSL erforderlich ist, wie viele saubere Stichproben benötigen wir? Und drittens, sollten wir nur die sauberen Stichproben für die Validierung verwenden oder gibt es bessere Möglichkeiten, sie zu nutzen? Wir haben diese Forschungsfragen in unserer Arbeit beantwortet, und unsere Ergebnisse sind wie folgt. Erstens stellen wir fest, dass aktuelle WSL-Methoden tatsächlich saubere Validierungsstichproben benötigen, um ordnungsgemäß zu funktionieren. Andernfalls gibt es einen großen Leistungsabfall. Wie in dieser Abbildung gezeigt, wenn es keine sauberen Validierungsstichproben gibt, können die trainierten Modelle nicht über die ursprünglichen schwachen Labels hinaus generalisieren, was die Trainingsarbeit sinnlos macht. Dies deutet darauf hin, dass WSL-Ansätze tatsächlich saubere Daten benötigen, um ordnungsgemäß zu funktionieren, und die Kosten für die Beschaffung sauberer Validierungsstichproben sollten nicht übersehen werden. Unsere zweite Feststellung ist, dass die Erhöhung der Anzahl der sauberen Validierungsstichproben die WSL-Ansätze dazu verhilft, bessere Leistungen zu erbringen, wie in der linken Abbildung gezeigt. Typischerweise benötigen wir nur 20 Stichproben pro Klasse, um eine hohe Leistung zu erzielen. Aber das ist noch nicht alles, denn wenn wir entweder sauberer Daten Zugang haben, dann das direkte Training auf ihnen wird sogar bessere Leistungen erzielen. Die rechte Abbildung zeigt den Leistungsunterschied zwischen Fine-Tuning-Ansätzen, die direkt auf den sauberen Daten angewendet werden, und WSL-Ansätzen, die die sauberen Daten für die Validierung verwenden. Wie wir sehen können, beginnt die direkte Fine-Tuning-Methode mit 10 Stichproben pro Klasse, die WSL-Ansätze zu übertreffen. Schließlich kann die Leistungsverbesserung, die in früheren WSL-Ansätzen behauptet wird, leicht durch die Erlaubnis, die trainierten Modelle auf den sauberen Validierungsstichproben weiter zu fine-tunen, erreicht werden. Wie aus den Abbildungen hervorgeht, unterliegt der Vanilla-Modell, der FTw genannt wird, initial komplexeren WSL-Methoden wie COSINE, die schlechter abschneiden. Aber wenn wir die Möglichkeit haben, die Modelle auf den sauberen Stichproben weiter zu fine-tunen, dann erzielt FTw die gleiche Leistung wie andere Methoden. Daher gibt es keinen Grund, komplexere WSL-Methoden zu wählen, die mehr Rechenzeit und Speicherplatz erfordern. Zusammenfassend haben wir gezeigt, dass aktuelle WSL-Ansätze saubere, manuell annotierte Stichproben benötigen, um ordnungsgemäß zu funktionieren. Ihre Leistungssteigerung und Praktikabilität sind stark überbewertet. Unsere konkreten Empfehlungen für zukünftige Arbeiten sind wie folgt: Erstens, berichten Sie über die Modellauswahlkriterien. Zum Beispiel berichten Sie, ob die Modellauswahl anhand sauberer Validierungsstichproben erfolgt. Zweitens, WSL-Ansätze sollten mit Few-Shot-Learning-Baselines verglichen werden, da beide auf sauberen Daten arbeiten. Drittens, kontinuierliches Fine-Tuning ist eine einfache, aber starke Baseline, die in zukünftigen WSL-Arbeiten berücksichtigt werden sollte. Schließlich haben wir unseren Code Open Source gemacht. Sie können ihn über den QR-Code auf dieser Folie finden. Bitte fühlen Sie sich frei, ihn zu überprüfen. Vielen Dank und viel Spaß auf der Konferenz.</sample>
    <sample id="241">## Abstract

This paper addresses the limitations of current automated misinformation detection systems, which often suffer from unrealistic evaluations and a lack of human-centric design.  Existing systems frequently rely on retrospectively constructed datasets and fail to account for leaked counter-evidence or the dynamic nature of misinformation.  Furthermore, they often neglect the crucial role of human content moderators. We propose a novel human-in-the-loop evaluation framework for developing more effective and realistic misinformation detection systems. Our framework emphasizes end-to-end workflows, integrating human feedback throughout the process, and prioritizes early detection of misinformation before widespread dissemination.

We demonstrate the framework's efficacy using a case study of COVID-19 treatment misinformation.  Our system combines claim extraction using a T5 model with policy violation verification using a BERT-based stance classification model.  Evaluation reveals that our system achieves 65% accuracy in policy violation detection and can identify 124.2 policy violations per human hour.  We define early detection as identifying unapproved treatments before their appearance in debunking articles.  Our framework provides a more realistic and human-centric approach to misinformation detection, offering a valuable tool for future research and development in this critical area.</sample>
    <sample id="242">Gängige Bewertungsmethoden für Dialogsysteme sind menschliche Bewertungen, wie z. B. die Auswahl der besseren von zwei Gesprächen oder die Bewertung von Gesprächen anhand einer Likert-Skala. Es gibt auch Methoden zur Bewertung von Gesprächen auf der Ebene der einzelnen Gesprächspunkte oder auf der Ebene des gesamten Gesprächs.</sample>
    <sample id="243">Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap.</sample>
    <sample id="244">Es wird Hintergrundwissen benötigt, dass Servin ein Richter ist und dass Richter Fälle in einem Gericht entscheiden.</sample>
    <sample id="245">A novel pipeline for identifying high-agreement Amazon Mechanical Turk (MTurk) workers for summarization is presented. The pipeline employs a two-step qualification process: a qualification task assessing annotator ability to evaluate multiple dimensions, and an endurance task testing workload capacity.  Workers are categorized into gold, silver, bronze, and block tiers, with gold and silver workers demonstrating high inter-annotator agreement (IAA) exceeding that of experts.  The pipeline's performance is evaluated on a reference-based task, yielding a Krippendorff's Alpha of 0.534.  Comparison with baseline MTurk workers (MACE filter) and CloudResearch workers reveals that the pipeline achieves comparable quality at a lower cost and with better task coverage.  Analysis of correctness across annotation sources shows strong correlation between pipeline and CloudResearch workers, while GPT models correlate well with expert judgments.  The study concludes that the pipeline offers a cost-effective and efficient method for large-scale high-agreement annotation, avoiding resource waste. Future work will focus on expanding the pipeline's applicability to diverse tasks, languages, and platforms. Limitations include the English-only focus, the potential for questions not to guarantee correctness, and the lack of a guarantee for training correctness.</sample>
    <sample id="246">Ja, der Code und das Datenset sind auf GitHub verfügbar.</sample>
    <sample id="247">FACTKG is a novel dataset for fact verification leveraging knowledge graphs (KGs) as evidence, addressing the lack of such datasets in the existing literature. Unlike datasets based on text or tables, FACTKG provides intuitive and directly verifiable evidence from KGs, enabling reliable reasoning for fact verification. The dataset comprises DBpedia KGs with claims in both written and colloquial styles, labeled as SUPPORTED or REFUTED.  The task involves retrieving evidence from DBpedia and verifying claims using five reasoning types: one-hop, conjunction, existence, multi-hop, and negation.  The dataset includes claims requiring various reasoning paths, including multi-hop inference for existence claims and negation.  To address the colloquial style, we utilize a colloquial style transfer model and presupposition templates.  We evaluate the dataset and construct baselines, including claim-only baselines and the GEAR model, which utilizes graph evidence.  Results demonstrate that all baselines outperform the majority class baseline (51%), with the GEAR model achieving the best performance by leveraging KG evidence. FACTKG facilitates consistency checks between KGs and natural language, with potential applications in dialogue systems and other consistency-checking tasks.</sample>
    <sample id="248">Die Annotatoren für NLPositionality sind nicht ausgewogen in Bezug auf jede demografische Gruppe. Die Studie zeigt, dass die Annotationen stärker mit englischsprachigen Ländern und Personen mit Hochschulabschluss korrelieren, während bestimmte Gruppen wie nicht-binäre Personen weniger gut repräsentiert sind.</sample>
    <sample id="249">Die Sätze innerhalb der akzeptablen Domain wurden durch Hinzufügen eines akzeptablen oder unakzeptablen Präfixes zu den akzeptablen und unakzeptablen Sätzen aus dem BLiMP-Datensatz erstellt.</sample>
    <sample id="250">Eine dimensionale Bewertung ist ein Ansatz zur Bewertung von Konversations-KI, der die Bewertung mehrerer Aspekte der Chatqualität anstelle einer einzigen Bewertung ermöglicht. Dies hilft dabei, die Stärken und Schwächen eines Modells auf einer feineren Ebene zu verstehen.</sample>
    <sample id="251">University of Science and Technology of China.</sample>
    <sample id="252">```
U-CREAT: Unsupervised Case Retrieval using Events Extraction addresses the challenge of prior case retrieval in legal domains, where identifying relevant precedents is time-consuming. The work introduces the Indian Legal Prior Case Retrieval Dataset (IL-PCR), a benchmark dataset of 7,070 cases with high citation density, offering a comprehensive test bed for PCR algorithms.  U-CREAT leverages unsupervised learning and an event-based approach to retrieve relevant cases.  It extracts events from query and candidate documents using dependency parsing, forming subject-verb-object triplets.  An interaction matrix is then computed to rank candidate documents based on event overlap. Experiments with various models, including count-based, transformer-based, and event-based approaches, demonstrate that event-based models significantly outperform baselines, achieving higher F1 scores and lower inference times.  The Event Filtered Documents model achieves state-of-the-art performance on the COLIEE'21 dataset, surpassing recent supervised approaches. U-CREAT's event-based methodology offers a promising solution for prior case retrieval, particularly in the Indian legal context, and opens avenues for future research.
```</sample>
    <sample id="253">DisorBERT is a novel double domain adaptation model designed for detecting signs of mental disorders in social media. The research addresses the challenge of insufficient annotated data by leveraging knowledge from a related domain (Reddit) and a lexicon to improve performance on a target domain (mental health).  The model integrates information from Reddit and mental health, guided by masking to focus on relevant words.  Experiments on the eRisk dataset demonstrate that DisorBERT achieves a good balance between precision and recall, outperforming baseline models like BERT and MentalBERT.  Analysis of generated words reveals a bias towards terms associated with mental disorders, indicating the model's ability to capture relevant linguistic cues.  Visualization techniques highlight key words and sentences indicative of depression.  The study concludes that the combined approach of double domain adaptation and guided masking is effective for detecting mental disorders in social media. Future work will explore incorporating different lexical resources and clinical data.</sample>
    <sample id="254">This paper presents a novel framework for document-level distant relation extraction (DS-RE) that addresses the noise problem in distant supervision (DS) data. DS-RE aims to extract relationships between entities within a document, but DS data often contains noisy labels generated from distant supervision.  Current methods using pseudo-labeling can be susceptible to false positives, leading to incorrect relation predictions.  

Our approach introduces uncertainty-guided label denoising by leveraging Monte Carlo dropout to estimate model uncertainty during pre-training.  We extend this to instance-level uncertainty estimation for overlapping relations, allowing for more accurate filtering of noisy pseudo-labels.  A dynamic class uncertainty threshold is then used to select pseudo-labels with lower uncertainty, effectively improving label quality.  Finally, we employ a multi-phase training strategy to iteratively refine the DS data.  

Experiments on public datasets demonstrate that our framework significantly outperforms existing baselines, achieving improved performance in DS-RE.  The key contributions are a framework for uncertainty-guided label denoising, an instance-level uncertainty estimation method for overlapping relations, a dynamic uncertainty thresholding strategy, and substantial performance gains.</sample>
    <sample id="255">Die Form des Prompts ist bei Ein- und Null-Shot-Prompting wichtig, hat aber bei fünf-Shot-Prompting kaum Einfluss.</sample>
    <sample id="257">Die Autoren haben vier state-of-the-art Chatmodelle evaluiert.</sample>
    <sample id="258">Chiang Cheng-Han introduces a new work exploring the feasibility of using large language models (LLMs) as an alternative to human evaluation in natural language processing (NLP). The research proposes instructing LLMs to rate text quality based on provided instructions and samples, aiming to overcome the instability and reproducibility issues of human evaluation.  The authors highlight that this approach was novel at the time of submission, as prior work primarily focused on human-based evaluation.

To validate their idea, they conducted an experiment evaluating stories generated by GPT-2 and human writers, using LLMs to rate them on grammar, coherence, likability, and relevance. Human English teachers provided ground-truth ratings.  The results showed that while some smaller LLMs didn't exhibit a clear preference, larger models like Davinci and ChatGPT did, aligning with human preferences.  The paper investigates factors influencing LLM evaluation, such as instruction wording and sampling methods, and discusses the benefits and costs compared to human evaluation.  The research suggests that certain LLMs can serve as viable alternatives to human evaluation, offering a potentially more stable and reproducible method for assessing text quality. The full paper is available for those interested.</sample>
    <sample id="259">XSemPLR introduces a unified benchmark dataset and evaluation framework for cross-lingual semantic parsing, addressing the limitations of existing models which often focus on limited language coverage or specific meaning representations. The dataset comprises nine datasets across various domains, five semantic parsing tasks, eight meaning representations, and 22 natural languages spanning 15 language families.  The framework includes six evaluation settings: Translate-Test, Monolingual, Monolingual Few-shot, Multilingual, Cross-lingual Zero-shot, and Cross-lingual Few-shot transfer.

The study evaluates the performance of multilingual models like XLM-R + PTR, mBERT + PTR, mBART, and mT5. Encoder-Decoder models consistently achieve the best performance across all datasets.  Results reveal the "Curse of Multilinguality," where performance gains are unevenly distributed across languages.  Cross-lingual transfer performance is significantly lower in zero-shot settings but improves rapidly with few-shot learning.  The study highlights that pretraining on English language data can boost few-shot performance on target languages, and that large multilingual models like Codex and BLOOM are not yet optimal for cross-lingual semantic parsing. XSemPLR provides a valuable resource for advancing research in this area.</sample>
    <sample id="260">Die Antwort ist nicht im bereitgestellten Text enthalten.</sample>
    <sample id="261">Ein guter Planer sollte Skripte schreiben, die sowohl sinnvoll als auch konform zu den gegebenen Einschränkungen sind.</sample>
    <sample id="262">Die Arbeit wurde von Siyu Yuan von Fudan University vorgestellt. Es ist nicht angegeben, wie viele Autoren an der Arbeit beteiligt sind.</sample>
    <sample id="263">## Abstract

In-context learning (ICL) with large language models (LLMs) is susceptible to instability due to design choices like example order, leading to biases in predictions. While prior work identifies search instability as a cause, there's a lack of systematic categorization of bias types and mitigation strategies. This work addresses this gap by proposing a typology of label biases in ICL, identifying a novel "domain-label bias" arising from task corpus influence. We demonstrate that random in-domain words can significantly bias model predictions, especially on tasks with strong domain-label bias, often leading to performance degradation even with prior calibration methods.

We introduce "domain-context calibration," a method using random in-domain words as content-free text to estimate and mitigate biases across label types.  Experiments on diverse datasets show that domain-context calibration significantly improves ICL performance, particularly for tasks with higher domain-label bias.  Further analysis reveals that single content-free tokens are suboptimal, and random in-domain words offer superior bias mitigation.  Our findings highlight the importance of considering domain-specific biases in ICL and provide a novel calibration approach to enhance the reliability of LLM predictions.</sample>
    <sample id="264">## Abstract

This paper introduces Transferable Audio-Visual Text Generation (TAVT), a novel approach to address the challenge of multimodal text generation with limited labeled data. While unimodal text generation models have achieved significant success, audio-visual tasks suffer from data annotation difficulties and domain shifts. TAVT tackles these issues by proposing a unified audio semantic space to align visual concepts across domains. The framework consists of an audio-visual meta-mapper network, a transformer-based encoder-generator, and Dual Counterfactual Contrastive Learning (DCLL). The meta-mapper network learns a representation of visual concepts in audio space using a clustering approach and learnable visual prefixes. The transformer encoder utilizes an alpha mechanism to evaluate the contribution of each modality. DCLL optimizes visual-textual alignment by leveraging counterfactual results, eliminating reliance on random negative samples.  TAVT employs a meta-learning strategy similar to MAML, enabling rapid adaptation to new domains.  Experiments on MSVD and MSR-VTT benchmarks demonstrate that TAVT significantly outperforms state-of-the-art methods, especially in low-resource scenarios. Ablation studies confirm the importance of audio features for performance.  This work establishes TAVT as a promising solution for transferable audio-visual text generation.</sample>
    <sample id="265">Vasudha</sample>
    <sample id="266">This text does not mention the authors' affiliation with a university.</sample>
    <sample id="268">Die häufigsten Fehler von PaLM sind Unterlassungsfehler, bei denen Teile der Ausgangssatz im Übersetzungsprozess weggelassen werden.</sample>
    <sample id="269">Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und heute erzählen wir Ihnen alles über ABC-Eval, einen neuen dimensionalen Ansatz zur Bewertung konversationeller KI. Diese Arbeit wurde vom Emory NLP Lab unter der Leitung von Professor Jinho Choi an der Emory University in Zusammenarbeit mit Amazon Alexa AI durchgeführt. Also, sagen wir, Sie haben ein Dialogmodell entwickelt und möchten sehen, wie gut es im Vergleich zum aktuellen Stand der Technik abschneidet. Die übliche Praxis ist, menschliche Bewertungen zu verwenden, z. B. indem man menschliche Gutachter bittet, zu entscheiden, welche von zwei Gesprächen besser ist oder Gespräche anhand einer Likert-Skala zu bewerten. Diese Ansätze funktionieren gut, um umfassende Bewertungen der Gesamtgesprächsqualität zu liefern, aber die Gesprächsqualität hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chatqualität bewerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen. Eine Möglichkeit ist, einfach menschliche Gutachter zu bitten, mehrere Dimensionen der Gesprächsqualität zu bewerten, z. B. die Relevanz der Modellantworten mithilfe bestehender vergleichender oder Likert-Skalen-Methoden. Wir glauben jedoch, dass es einen präziseren und zuverlässigeren Strategie für die dimensionale Bewertung von Dialogen gibt. Unser Ansatz versucht, die Subjektivität menschlicher Bewertungen zu reduzieren, indem wir explizit annotieren, ob jede Modellantwort bestimmte Verhaltensweisen ausdrückt, wie z. B. die Antwort mit irrelevanten Informationen, die sich selbst widerspricht oder mit der Partnerin widerspricht, halluzinierte falsche Fakten oder die Verletzung von allgemeinem Wissen und wann das Modell erfolgreich oder fehlgeschlagen ist, Empathie zu zeigen. Um zu bestimmen, welche Art von Bewertung am effektivsten ist, haben wir vier modernste Chatmodelle ausgewählt und sie anhand von 100 menschlich-Bot-Gesprächen pro Modell mit ABC-Eval bewertet. Zur Vergleichsbewertung haben wir diese Gespräche auch mit drei bestehenden Methoden bewertet: Likert-Ratings auf der Turn-Ebene, Likert-Ratings auf der Dialog-Ebene und Dialog-Ebene-Paarvergleiche. Für jede der drei bestehenden Methoden haben wir Bewertungen auf acht der am häufigsten gemessenen Aspekte des Dialogs gesammelt, da dies die Standardpraxis für die Bewertung von Chatmodellen entlang mehrerer Dimensionen ist. Bei unserer Analyse dieser Bewertungsergebnisse haben wir festgestellt, dass die ABC-Eval-Verhaltensbeschriftungen insgesamt zuverlässiger sind als die von bestehenden Methoden gesammelten Beschriftungen, wie durch die Inter-Annotator-Übereinstimmung bei 100 doppelt beschrifteten Gesprächen gemessen wird. Darüber hinaus sind die ABC-Eval-Beschriftungen im Vergleich zu den von bestehenden Methoden produzierten Metriken aussagekräftiger für die Gesamtgesprächsqualität, wie in dieser einfachen linearen Regression gezeigt wird. Zum Beispiel zeigt, wie der Anteil der Gespräche mit Selbst- und Partner-Kontradiktionen 5 % und 10 % der Gesprächsqualität erklärt, während die durchschnittlichen Likert-Konsistenzbewertungen nur 4 % oder weniger ausmachen. Schließlich haben wir überprüft, ob jede Evaluationsmetrik einen einzigartigen Aspekt der Chatqualität erfasst, mithilfe einer schrittweisen linearen Regression. Sie können sehen, wie die Kombination aller ABC-Eval-Metriken über 25 % der Gesprächsqualität erklärt, und wie die Entfernung der Metriken einzeln zu einem Verlust an Informationen über die Qualität führt. Die Kombination aller Turn-Ebene-Likert-Metriken erklärt jedoch viel weniger der Qualität, und weniger dieser Metriken tragen einzigartige Informationen. Diese zuverlässigen, informativen und unterschiedlichen ABC-Eval-Metriken ermöglichen es uns, konversationelle KI mit einer höheren Auflösung zu bewerten, als es frühere Methoden leisten können. Sie sehen, dass in den Ergebnissen unseres Experiments mehrere Herausforderungen noch bestehen und genau quantifiziert wurden. Zum Beispiel haben die von uns getesteten Bots in etwa 20 % ihrer Antworten allgemeine Sinnverletzungen aufweisen. Sie produzieren in etwa 15 % ihrer Antworten irrelevante Informationen und widersprechen sich oder ihrer Partnerin in etwa 10 % der Zeit. Angesichts des rasanten Fortschritts im Feld könnten viele dieser Fehlerquoten bei neuen Modellen, die seit unserer Bewertung durchgeführt wurden, sinken. Dies ist jedoch gerade der Grund mehr, zuverlässige und präzise Evaluationsmetriken für den Vergleich von Modellen zu entwickeln. Wir hoffen, dass ABC-Eval von anderen im Feld genutzt werden kann als ein bedeutsamer Schritt in diese Richtung. Und wir freuen uns darauf, zu sehen, wie sich konversationelle KI in den kommenden Monaten und Jahren weiterentwickelt. Vielen Dank fürs Zuschauen.</sample>
    <sample id="270">Emory University.</sample>
    <sample id="271">CFT steht für FTw, was für "Fine-tuning" steht.</sample>
    <sample id="272">John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy und Adina Williams.</sample>
    <sample id="273">Hallo, mein Name ist Kayo Yin und ich werde unsere Arbeit mit dem Titel „Wann erfordert Übersetzung Kontext? Eine datengetriebene, mehrsprachige Untersuchung“ vorstellen. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig durchgeführt. Also hängt vieles an der Übersetzung vom Kontext ab. Nehmen wir zum Beispiel die Übersetzung von „mole“ in diesem Satz: Wenn der vorherige Satz „Es könnte gefährlich werden, wenn die Minister die Wahrheit herausfinden“ war, dann bezieht sich „mole“ auf einen Spion. Aber wenn der vorherige Satz „Ist es wirklich etwas Ernstes, Doktor?“ war, dann bezieht sich „mole“ auf eine Birthmark. Je nach Kontext ändert sich die Bedeutung des Wortes, und daher ändert sich auch die Übersetzung. Allerdings ist die Bewertung, wie gut Modelle solche Fälle handhaben, ziemlich schwierig. Erstens liegt nur ein kleiner Teil der Übersetzungen im Kontext, was es für korpusbasierte Metriken wie BLEU unmöglich macht, diese Übersetzungen zu erfassen. Einige haben vorgeschlagen, gezielte Bewertungen auf kontextabhängige Übersetzungen durchzuführen, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Mengen an Sprachen, da sie in der Regel auf Fachwissen und menschlicher Kuratierung beruhen. In dieser Arbeit versuchen wir, diese beiden Fragen zu beantworten. Erstens, wann erfordert Übersetzung Kontext? Und zweitens, wie gut können Modelle diese Fälle bewältigen? Um die erste Frage zu beantworten, haben wir gemessen, wie stark ein Wort während der Übersetzung vom Kontext abhängt. In früheren Arbeiten haben wir CXMI als Maß für die Kontextnutzung von maschinellen Übersetzungsmodellen eingeführt. Dies wird erreicht, indem wir messen, wie viel Informationen der Kontext C dem Modell über das Ziel Y gibt, gegeben das Quell X. Sie können CXMI als die Information betrachten, die durch die Bereitstellung von Kontext für das Modell gewonnen wird. In dieser Arbeit erweitern wir CXMI auf Pointwise CXMI, das den Kontextgebrauch auf Satz- oder Wortebene messen kann. Wir können Wörter mit hohem P-CXMI als solche betrachten, die für die Übersetzung Kontext benötigen. Nun analysieren wir Wörter mit hohem P-CXMI, um Muster zwischen diesen Wörtern zu finden. Wir führen unsere Analyse auf Transkripten von TED Talks durch, die von Englisch in 14 verschiedene Sprachen übersetzt wurden. Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Erstens betrachten wir die Teilwortmarken, die einen hohen durchschnittlichen P-CXMI haben. Dies ermöglicht es uns, beispielsweise duale Pronomen im Arabischen zu finden, die einen relativ hohen P-CXMI haben. Dies kann erklärt werden, weil Englisch keine doppelten Pronomen hat, also ist Kontext erforderlich, um zu bestimmen, ob ein Pronomen doppelt ist, wenn es ins Arabische übersetzt wird. Ähnlich finden wir heraus, dass bestimmte Sprachen Kontext benötigen, um die richtige Verbform auszuwählen. Wir untersuchen dann die Wortschatzartikel mit hohem P-CXMI, gemittelt über alle ihre verschiedenen Vorkommnisse. Dies hilft uns, Fälle wie den hier zu identifizieren, in denen das Chinesische Kontext benötigt, um Substantive zu übersetzen und sicherzustellen, dass wir die gleiche Übersetzung im Dokument verwenden. Ähnlich finden wir heraus, dass Kontext wichtig ist, um in der richtigen Formalität zu übersetzen. Schließlich betrachten wir verschiedene einzelne Token mit hohem P-CXMI. Dies ermöglicht es uns, Phänomene zu identifizieren, die nicht wirklich vom Wort selbst erfasst werden können, sondern eher in der Satzstruktur ausgedrückt werden, wie z. B. Ellipsenauflösung. Nun verwenden wir unsere Ergebnisse aus unserer Analyse, um einen Benchmark für die Dokumentübersetzung zu entwerfen. Für jeden der fünf Diskursphänomene, die wir identifiziert haben, erstellen wir Tagger, um Wörter automatisch zu identifizieren, die sich auf das Phänomen beziehen. Wir haben unseren Tagger Multilingual Discourse-Aware, oder MuDA genannt. Wir können auch feststellen, dass verschiedene Sprachen unterschiedliche Anteile dieser Diskursphänomene haben. Wir verwenden dann den MuDA-Tagger, indem wir den Tagger auf einen Parallelkorpus anwenden, für den wir eine Bewertung durchführen möchten, und unsere gewählten Übersetzungsmetriken auf die kontextabhängigen Beispiele anwenden, die der MuDA-Tagger identifiziert hat. Schließlich verwenden wir unseren Benchmark sowie andere Metriken, um verschiedene Modelle für die Dokumentübersetzung zu bewerten. Erstens, wenn wir korpusbasierte Metriken verwenden: so für BLEU finden wir, dass kontextunabhängige Modelle die beste Leistung erbringen. Aber wenn wir COMET verwenden, erzielen kontextbewusste Modelle die beste Leistung. Und wenn wir die Wort-F-Maßnahme verwenden, dann haben Modelle mit und ohne Kontext vergleichbare Leistungen. Dies zeigt erneut, dass es schwierig ist, das beste System für die Dokumentübersetzung zu bestimmen, wenn wir allein korpusbasierte Metriken verwenden. Wir verwenden unseren MuDA-Benchmark, um Modelle zu bewerten, und wir stellen fest, dass kontextbewusste Modelle für bestimmte Diskursphänomene wie Formalität und lexikalische Kohäsion signifikant genauer sind als Modelle, die keinen Kontext verwenden. Allerdings sind diese Modelle nicht viel besser als Modelle, die keinen Kontext für andere Phänomene wie Ellipsen, Pronomen und Verbform verwenden. Dies deutet darauf hin, wo wir mehr Fortschritte bei der Dokumentübersetzung sehen müssen. Wir haben auch verschiedene kommerzielle Systeme verglichen und unser Benchmark zeigt, dass DeepL in der Regel genauer ist als Google Translate für die Dokumentübersetzung. Zusammenfassend analysieren wir datengestützt über 14 Sprachpaare, um zu identifizieren, wann Übersetzungen Kontext erfordern, und verwenden dann unsere Ergebnisse, um einen Benchmark für die Dokumentübersetzung zu erstellen, der uns helfen kann, zu identifizieren, welche Diskursphänomene Modelle gut oder schlecht handhaben, und welche Übersetzungssysteme gut für die Dokumentübersetzung sind. Vielen Dank für Ihre Aufmerksamkeit. Wir sehen uns in Toronto.</sample>
    <sample id="274">Yusen Zhang von der Penn State University.</sample>
    <sample id="276">This work addresses the understudied area of machine translation metric evaluation for Indian languages, focusing on a dataset of 7,000 translated sentences from the Flores dataset.  We employ bilingual expert annotators to meticulously evaluate translations, capturing error types (from MQM) and severity, alongside overall scores.  A total of seven translation models are used to generate candidate translations for each source sentence across five Indian languages (Tamil, Malayalam, Hindi, Marathi, and Gujarati).  

We analyze the correlation between various metrics (chrF, LabSE embedding, BERTscore, MuRIL, COMET variants) and human scores.  Our findings reveal that overlap-based metrics perform poorly, while embedding-based metrics show better correlations, particularly when using multilingual models. COMET-metric variants demonstrate the highest overall correlations.  Fine-tuning the best-performing metric, COMET, using our MQM dataset yields IndicCOMET MQM, which outperforms COMET baselines on three of five languages and shows higher correlations across all.  Furthermore, IndicCOMET MQM exhibits improved robustness compared to the original COMET metric on the ACES Translation Accuracy Challenge Sets.  We release our dataset for further research.</sample>
    <sample id="277">Multiset Tagging and Latent Permutations.</sample>
    <sample id="278">Die Autoren beschreiben die Methode der „markierten Wörter“ als einen Ansatz, der die linguistische Konzept des „Markiertseins“ nutzt, um zu identifizieren, welche Wörter Gruppen von „markierten“ Gruppen von denen von „unmarkierten“ Gruppen unterscheiden. Sie vergleichen die Personas, die von LLMs generiert wurden, mit denen von Menschen, um die Wörter zu identifizieren, die die Unterschiede zwischen den Gruppen hervorheben.</sample>
    <sample id="279">Shangbin ist PhD-Student an der University of Washington.</sample>
    <sample id="280">MultiEMO is a novel attention-based correlation-aware multimodal fusion framework for emotion recognition in conversations. The goal is to predict the emotion label of each utterance, leveraging textual, audio, and visual modalities. Existing methods struggle with multimodal complementarity, minority emotion classes, and distinguishing semantically similar emotions. MultiEMO addresses these challenges by proposing a novel visual feature extractor, VisExtNet, which focuses on facial expressions without redundant scene information.  A key component is MultiAttn, a multimodal fusion network using bidirectional multi-head cross-attention layers to integrate textual, audio, and visual features.  A Sample-Weighted Focal Contrastive Loss is introduced to improve performance on minority and similar emotion classes.  Experiments on MELD and IEMOCAP demonstrate state-of-the-art results, particularly in challenging scenarios. Limitations include VisExtNet's inability to distinguish speakers, the SWFC loss's batch size requirement on MELD, and the continued performance gap for minority emotions compared to majority classes.</sample>
    <sample id="281">## Abstract

This work investigates the crucial role of context in machine translation, addressing the challenge of evaluating context-dependent translations. We explore when translation requires context by extending the Contextualized Word Mover's Distance (CXMI) measure to a pointwise level, analyzing word and sentence-level context usage across 14 language pairs using TED talks. Our analysis reveals patterns in context dependence based on part-of-speech tags, vocabulary items, and individual tokens, identifying phenomena like pronoun resolution, proper noun translation, and formality. 

To facilitate evaluation, we develop the Multilingual Discourse-Aware (MuDA) tagger, which automatically identifies discourse phenomena relevant to context. We then use this tagger to create a benchmark for document-level machine translation, evaluating models using both corpus-level metrics (BLEU, COMET, word f-measure) and the MuDA benchmark itself. Our findings demonstrate that while context-aware models excel in handling specific discourse phenomena like formality and lexical cohesion, they show limited improvement on others like ellipsis and pronouns.  Furthermore, our benchmark indicates that DeepL often outperforms Google Translate in document-level translation. This research provides valuable insights into context dependence in machine translation and offers a framework for more accurate and nuanced evaluation of translation systems.</sample>
    <sample id="282">StoryTrans addresses the challenge of non-parallel story author-style transfer, a crucial task in natural language generation. Existing methods primarily focus on token or sentence-level style transfer, lacking the ability to imitate author styles at the story level, particularly concerning discourse structures and style-specific content.  The proposed StoryTrans model learns discourse representations from source texts and combines them with learnable style embeddings for text generation. A novel training objective reduces stylistic features from discourse representations, aligning them in the latent space, while a two-stage generation process enhances content preservation. The first stage uses self-reconstruction, disentanglement, sentence order, and style classifier losses. The second stage focuses on filling style-specific content and removing masks.  Experiments on new datasets in Chinese and English demonstrate StoryTrans's superior performance in style control and content preservation compared to strong baselines, as validated by both automatic and manual evaluations. Style visualization confirms alignment with golden text in the style feature space. StoryTrans effectively supplements storylines with relevant phrases and maintains source semantics during rewriting. The model's data and code are publicly available.</sample>
    <sample id="283">Igor Mel'čuk's meaning text theory.</sample>
    <sample id="284">This paper introduces FSUIE, a novel fuzzy span mechanism for enhancing universal information extraction (UIE). Current UIE models rely heavily on precise span boundaries, which are often ambiguous. FSUIE addresses this by proposing a fuzzy span representation, where the span boundary is modeled as a continuous probability distribution.  The model utilizes adaptive attention, allowing for a dynamic and continuous attention span, rather than static truncation.  A fuzzy span attention mask further refines the attention distribution.  The proposed module is integrated into the text encoding process without affecting its core functionality.  Experiments on named entity recognition, relationship extraction, and aspect sentiment triplet extraction demonstrate significant performance improvements, particularly on small-scale datasets. FSUIE achieves state-of-the-art results on ACE2004, 2005, ADE, and AST-V2 datasets. Ablation studies show that the combination of fuzzy span loss (FSL) and fuzzy span attention (FSA) yields the best results, improving convergence speed and information extraction capability. Visualizations confirm that the fuzzy span attention focuses on semantic information within a limited context.  FSUIE offers a more robust and adaptable approach to UIE, achieving excellent performance across various information extraction tasks.</sample>
    <sample id="285">This paper addresses the lack of evaluation methods for Factual Error Correction (FEC) models in dialogue summarization, a relatively unexplored area. While existing factuality metrics like FactCC and DAE provide overall scores, they suffer from vagueness and fail to distinguish between true error correction and simply generating a factually correct summary without modifying the original.  The authors argue that these flaws divert FEC research from its core purpose.

To address this, they propose a new evaluation framework based on manually annotated reference corrections, emphasizing minimal substitution, insertion, and deletion operations for fluency and non-redundancy. This provides more valuable data for training and a more comprehensive assessment of FEC model performance.  A novel taxonomy of factual errors, categorized as content-based and form-based, is introduced.

Experiments with FEC models trained on reference summaries demonstrate that training with human-corrected summaries yields the best results, surpassing unreliable metrics. The paper highlights the need for a shift in evaluation methods and suggests combining human-annotated and synthetic data.  Furthermore, it identifies limitations of current FEC models in correcting specific error types like additions and attribute errors.</sample>
    <sample id="286">James Finch und Sarah Finch.</sample>
    <sample id="287">Filip Radlinski, Silvia Pareti, Annie Louis und Javad Hosseini.</sample>
    <sample id="288">Die Datensätze, die zum Testen syntaktischer Phänomene verwendet werden können, sind BLiMP und SyntaxGym.</sample>
    <sample id="290">Die Abkürzungen der fünf Methoden für die erste Forschungsfrage sind:

1.  **WSL** (Weakly Supervised Learning)
2.  **FTw** (Fine-tuning)
3.  **COSINE**
4.  **Vanilla model**
5.  **Few-shot learning baselines**</sample>
    <sample id="291">Das Modell wird anhand von 11 downstream-Aufgaben wie Named Entity Recognition, Klassifizierung, POS-Tagging und Frage-Antwort-Aufgaben evaluiert.</sample>
    <sample id="294">CamemBERT wurde ursprünglich mit einem 4 GB großen Datensatz aus NACHOS trainiert.</sample>
    <sample id="295">Adam Przepiórkowski.</sample>
    <sample id="296">This work investigates irony detection in natural language processing, moving beyond binary classification to explore more informative outputs. The authors developed the English Perspectivist Irony Corpus (EPIC), a dataset of 300 short conversations collected from social media sources over 1.5 years, annotated by 74 annotators across five English varieties.  They employed crowdsourcing via Prolific, using a simple interface where annotators rated replies as "ironic" or "not ironic."  

The study highlights that inter-annotator agreement varies significantly across annotator groups (gender, age, nationality). To address this, they developed perspective-aware models by fine-tuning a pre-trained language model on datasets split by annotator perspectives.  While raw performance showed no clear trends, perspective-aware models demonstrated significantly higher confidence in their predictions compared to standard aggregated models.  Furthermore, analysis revealed generational and geographical differences in annotation disagreements, particularly between UK/Ireland annotators.  The findings suggest that incorporating annotator perspectives can improve the reliability and confidence of irony detection models.</sample>
    <sample id="297">## Abstract

This work investigates coded rhetoric, specifically dogwhistles, and their implications for natural language processing (NLP) and political discourse. We address the challenge of understanding dogwhistles – terms that convey multiple meanings, often targeting marginalized groups – by developing a typology and glossary of over 340 dogwhistle terms and symbols, enriched with contextual information.  A case study of historical U.S. political speeches reveals a correlation between the use of racial dogwhistles and the Republican Southern Strategy.  We evaluate dogwhistle recognition in language models, particularly GPT-3, finding varying performance across formal vs. informal dogwhistles and different groups.  Prompt engineering significantly impacts model accuracy.  Furthermore, we demonstrate how dogwhistles can evade content moderation by analyzing toxicity detection scores, showing that replacing explicit slurs with dogwhistles can reduce perceived toxicity.  Our research highlights the complexities of meaning in coded language, the challenges of automated detection, and the potential for dogwhistles to influence political discourse and evade online censorship.</sample>
    <sample id="298">Die Experimente zeigten, dass die Performance bei größeren zeitlichen Abständen zwischen Trainings- und Testdaten abnahm, was die Hypothese der zeitlichen Verzerrung bestätigte.</sample>
    <sample id="299">This work addresses the vulnerability of Natural Language Inference (NLI) models to shortcuts, spurious correlations in training data that lead to strong in-distribution performance but poor generalization to out-of-distribution data. Existing shortcut mitigation methods often require domain-specific knowledge or auxiliary models that may not align with the learner's behavior.

We propose a novel training method based on minimax optimization. A learner model is trained to minimize the NLI loss, while an auxiliary model maximizes it. The auxiliary model generates example weights that incentivize the learner to focus on under-represented "hard" training instances – those that contradict dominant "easy" examples and thus counteract shortcuts. This approach avoids assumptions about shortcut types and leverages the learner's own training dynamics.

We evaluate our method on MNLI, FEVER, and QQP, comparing it to ERM and existing shortcut mitigation techniques. Results demonstrate that our minimax training consistently improves out-of-distribution performance while maintaining high in-distribution accuracy.  We also investigate the effect of pre-training, auxiliary model size, and transferability to larger models and synthetic shortcuts.  The paper includes a qualitative evaluation of the learned example weight distribution.</sample>
    <sample id="300">## Abstract

This work introduces interactive dictation, a task enabling users to dictate and edit documents in a natural, conversational manner. Unlike existing speech-to-text systems that primarily support dictation, interactive dictation allows for seamless interleaving of dictation and command utterances, facilitating corrections and modifications through intuitive natural language.  The research formalizes interactive dictation as a four-step process: ASR recognition, utterance segmentation, command normalization, and sequential execution.  A new data collection interface and dataset are designed to support this task.  A baseline system is developed, employing T5 and GPT-3 architectures for ASR repair and state prediction. Experiments demonstrate a trade-off between runtime and accuracy, with GPT-3 achieving higher accuracy but slower performance.  Predicting the final state directly yields better results than predicting intermediate programs.  The research highlights the potential for more intuitive and human-like dictation interfaces and encourages further exploration of this area. Code and paper details are available at [website address].</sample>
    <sample id="302">Die Token für die Ausgabesequenz müssen permutiert werden, weil die Eingabe- und Ausgabesequenzen nicht direkt übereinstimmen und die Reihenfolge der Token in der Ausgabe nicht vorhersehbar ist. Die Permutation ermöglicht es dem Modell, die korrekte Reihenfolge der Token zu lernen, nachdem es die Token in der richtigen Menge vorhergesagt hat.</sample>
    <sample id="303">Die Autoren empfehlen, dass Modellentwickler*innen ihre Bias-Minderungs-Methoden transparenter machen sollten, weil sie nicht wissen, ob positive Stereotypen auf übermäßige Wertorientierung oder anderen, möglicherweise schädlichen, Anti-Stereotypen-Methoden zurückzuführen sind. Ohne Transparenz können keine fundierten Schlussfolgerungen gezogen und die Methoden weiter untersucht werden.</sample>
    <sample id="304">Minimalpaareingaben sind Paare von akzeptablen und inakzeptablen Sätzen, die verwendet werden, um Sprachmodelle auf ihre Fähigkeit zu bewerten, die Akzeptabilität von Sätzen zu beurteilen.</sample>
    <sample id="305">This presentation introduces the research paper "Weaker Than You Think: A Critical Look at Weakly Supervised Learning," co-authored with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. The work challenges the common claim that weakly supervised learning (WSL) methods achieve high performance solely on weakly labeled data, often relying on an additional clean validation set. The authors investigate the necessity of clean validation data, the optimal number of clean samples, and the best utilization of clean data.

The findings reveal that WSL methods require clean validation samples for proper generalization, with performance drops observed without them. Increasing the number of clean samples improves performance, and direct fine-tuning on clean data yields even better results than WSL approaches.  The performance gains often attributed to WSL can be achieved by allowing continued fine-tuning on clean validation samples.  The paper argues that the practicality and performance benefits of many WSL methods are overestimated.

Recommendations for future work include reporting model selection criteria, comparing WSL with few-shot learning, and considering continuous fine-tuning as a baseline. The authors have also open-sourced their code.</sample>
    <sample id="306">Sebastian Schuster and Najoung Kim investigate the extent to which large language models (LLMs) can track entities in discourse. They argue that entity tracking is crucial for understanding longer texts but hasn't been systematically evaluated in LLMs. The research addresses challenges in task design, including preventing models from relying on common entity states or simple word-association heuristics.

The authors designed a task involving boxes and objects, where models predict the contents of boxes after state-changing operations. They evaluated Flan-T5 and GPT-3/3.5 using 2-shot in-context learning.  Results show that most models primarily copy the initial state, with only text-davinci-003 exhibiting non-trivial tracking.  Interestingly, GPT-3.5 models, trained on code, demonstrate better tracking abilities than models with less code in their pre-training.  Fine-tuning can enable smaller models like T5-base to learn tracking, but random initialization fails. The study suggests that pre-training on code is a key factor in enabling entity tracking in LLMs, though the generalizability of these findings remains an open question. The paper is available on arXiv.</sample>
    <sample id="307">Die Autoren haben für die Bewertung ihrer sieben Modelle eine Reihe von Aufgaben aus öffentlichen und privaten Datensätzen verwendet, darunter Named Entity Recognition, Klassifizierung, POS-Tagging und Frage-Antwort.</sample>
    <sample id="308">## Abstract

This work investigates design biases in NLP datasets and models by examining their alignment with diverse user populations.  The study, titled "NLPositionality," addresses the question of whether datasets and models possess positionality – reflecting the perspectives of their creators and the data they are trained on.  Researchers from Carnegie Mellon University, University of Washington, and the Allen Institute for AI developed a framework to compare annotations from diverse annotators with existing datasets and models.  

The framework involves re-annotating datasets with a large, geographically and demographically diverse group of annotators and then correlating these annotations with models like GPT-4, Perspective API, and Dynahate.  The study analyzed over 16,000 annotations from 1000+ annotators across 87 countries.  Results reveal that datasets and models exhibit positionality, aligning most strongly with English-speaking countries and individuals with higher levels of education.  However, this alignment leaves certain populations, such as non-binary individuals, underrepresented.  The research concludes with recommendations for inclusive NLP practices, including documenting design choices, adopting a perspectivist approach, and developing specialized datasets and models for specific communities.</sample>
    <sample id="309">Inter-Annotator Agreement.</sample>
    <sample id="310">Wikipedia.</sample>
    <sample id="311">Der Text enthält keine Informationen darüber, welcher Universität die Autoren angehören.</sample>
    <sample id="312">MultiInstruct ist der erste große Benchmark für die Instruction-Tuning-Methode für multimodale Aufgaben. Im Gegensatz zu anderen Benchmarks konzentriert sich MultiInstruct auf multimodale Aufgaben und bietet eine große Anzahl von Aufgaben (62) mit fünf Experteninstruktionen pro Aufgabe, was eine umfassende Bewertung der Instruction-Tuning-Methoden für multimodale Modelle ermöglicht.</sample>
    <sample id="313">James Finch, Sarah Finch, Jinho Choi und Amazon Alexa AI.</sample>
    <sample id="314">Die binäre Koordination bezieht sich auf die Struktur der Koordination, bei der die erste Konjunkt die Kopfstruktur der gesamten Koordinationsstruktur ist.</sample>
    <sample id="315">Die Studie verwendete Prompts, die von menschlichen Subjekten verwendet wurden, um Stereotypen aufzudecken. Die Dauer der Prompts wurde nicht angegeben.</sample>
    <sample id="316">Die Ergebnisse zeigen, dass das kleinere T5-Modell, das auf dem CoScript-Datensatz feinabgestimmt wurde, bessere Skripte generieren kann als die meisten großen Sprachmodelle.</sample>
    <sample id="317">CodeIE addresses the challenge of mismatched outputs in large language models (LLMs) for information extraction (IE) tasks. Traditional LLMs like T5 and GPT-3 generate text-to-text outputs, while IE requires structured outputs like named entity lists or relation triples. This mismatch often necessitates large structured training datasets and specialized decoding strategies.

CodeIE transforms IE into a structure-to-structure code generation task by leveraging code LLMs like Codex.  The approach involves prompting the model to generate code that extracts and organizes information, effectively converting text into structured data.  Experiments on named entity recognition and relation extraction datasets demonstrate that CodeIE significantly outperforms traditional baselines (T5, UIE, GPT-3) using few-shot learning. 

Analysis reveals that code-based prompting aligns better with the IE task, leading to reduced structural errors and improved performance. Codex consistently outperforms GPT-3 across tasks, particularly in recall.  The study highlights the benefits of using code LLMs for structured information extraction and provides insights into prompt engineering for improved results. The paper and code are publicly available.</sample>
    <sample id="318">Hi, ich bin Yanis Labrak und ich präsentiere Ihnen unsere Arbeit zu "DrBERT: Ein robuster vortrainiertes Modell in Französisch für biomedizinische und klinische Bereiche." In dieser Präsentation sprechen wir zunächst über Sprachmodellierung im Gesundheitswesen. Dann präsentieren wir die Hauptbeiträge unseres Artikels. Wir stellen DrBERT vor, das erste biomedizinische Modell in Französisch, das auf RoBERTa basiert und auf NACHOS trainiert wurde, einem Datensatz medizinischer crawled Daten aus dem Web. Wir haben auch einen Vergleich von Modellen mit verschiedenen Pre-Training-Einstellungen und Datenquellen vorgestellt. Dann präsentieren wir unsere Ergebnisse auf 11 biomedizinischen und klinischen Downstream-Aufgaben in Französisch. Und schließlich schließen wir die Experimente ab und geben Ihnen weitere Details darüber, wie Sie diese Modelle zugänglich machen können. Seit 2018 ist BERT zu einem der effektivsten Ansätze zur Lösung von Natural Language Processing-Aufgaben geworden und bietet im Vergleich zu historischen statischen und kontextbezogenen Methoden wie Word2vec, fastText oder anderen enorme Leistungssteigerungen. Seitdem wurde dieses Modell für viele andere Sprachen, wie z. B. Französisch mit CamemBERT, und auch für Bereiche wie Biomedizin mit PubMedBERT und BioBERT sowie für klinische Bereiche mit ClinicalBERT angepasst, hauptsächlich jedoch auf Englisch. Spezialisierte Modelle für andere Sprachen sind rar und basieren oft auf kontinuierlichem Pre-Training aufgrund des Mangels an in-domain-Daten. Für Französisch gab es bisher kein Open-Source-Modell für die biomedizinische Forschung. Daher stellen wir die Frage, welche Datenquellen am besten geeignet sind für eine breite Nutzung und ob crawled Daten eine gute Alternative zu klinischen Daten darstellen. Um diese Frage zu beantworten, vergleichen wir DrBERT mit unserem ChuBERT-Modell, das auf anonymisierten Daten aus dem Daten-Warteschlangen-Warteschlangen des Nantes University Hospital basiert. Anschließend fragen wir uns, wie viel Daten benötigt werden, um ein spezialisiertes Modell für französische Daten zu trainieren? Ist es 4 Gigabyte, 8 Gigabyte oder mehr? Um diese Frage zu beantworten, trainieren und vergleichen wir vier von Grund auf neu trainierte Modelle: eine erste Version von DrBERT mit 7 GB NACHOS; eine zweite Version mit 4 GB NACHOS; eine erste Version von ChuBERT, ein klinisches Modell mit 4 GB klinischen Notizen; und eine letzte Version von ChuBERT mit einer Mischung aus 4 GB NACHOS und 4 GB klinischen Notizen. Zusätzlich zu diesem Vergleich haben wir drei Modelle mit kontinuierlichem Pre-Training vorgestellt, um den Einfluss der Pre-Training-Strategie zu analysieren. Eines basiert auf der Gewichtung von CamemBERT und wurde mit einem 4 GB NACHOS-Set trainiert. Ein weiteres basiert ebenfalls auf CamemBERT, wurde aber mit 4 GB klinischen Notizen trainiert. Und schließlich eines basiert auf dem englischen biomedizinischen Modell PubMedBERT und wurde mit einem 4 GB NACHOS-Set trainiert. Insgesamt haben wir sieben Modelle. Um unsere sieben Modelle zu bewerten, sammeln wir Daten für öffentliche und private Downstream-Aufgaben wie Named Entity Recognition, Klassifizierung, Part-of-Speech Tagging und Frage-Antwort. Diese Modelle werden mit sechs Basismodellen verglichen: CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT und ClinicalBERT. Die Bewertung zeigt, dass Modelle, die mit Daten der gleichen Art trainiert wurden, in der Regel die besten Ergebnisse erzielen. Wir können jedoch feststellen, dass Daten aus heterogenen Quellen vielseitiger erscheinen. Wir beobachten auch, dass mehr Daten zu besseren Ergebnissen führen. Insgesamt scheint das von Grund auf neu trainierte Pre-Training für die meisten Aufgaben höhere Leistungen zu erbringen. Unser Experiment mit Kontroll-Pre-Training unter Verwendung der Gewichte und Tokenisierung von CamemBERT, das auf dem 4 GB NACHOS-Subset trainiert wurde, zeigte vergleichbare Ergebnisse mit denen von DrBERT 4 GB von Grund auf neu trainiert. Dies gilt jedoch nicht für das Modell, das auf den Gewichten und der Tokenisierung von CamemBERT basiert, das Stabilitätsprobleme aufweist. Schließlich schließen wir, dass unser System in neun von elf Downstream-Aufgaben bessere Leistungen erbracht hat und die Ergebnisse des generischen Modells, CamemBERT, insgesamt übertroffen hat. Wir beobachten auch, dass mehr spezialisierte Daten besser sind, aber nicht skalieren. Alle von NACHOS vorbereiteten Modelle sind frei verfügbar auf Hugging Face und unter der MIT-Lizenz, und alle Trainingsskripte befinden sich auf unserem GitHub-Repository. Vielen Dank für diese Präsentation, und wir freuen uns auf den Austausch auf dem Poster-Session in Toronto.</sample>
    <sample id="319">Die Arbeit untersucht verschiedene Lernstrategien, darunter:

*   **Von Grund auf vorab trainierte Modelle** (DrBERT, ChuBERT)
*   **Kontinuierliches Vorabtraining** (mit CamemBERT-Gewichten und Tokenisierung, mit CamemBERT auf NACHOS-Daten, mit CamemBERT auf klinischen Notizen, mit PubMedBERT auf NACHOS-Daten)
*   **Verschiedene Datensätze** (NACHOS, klinische Notizen)
*   **Verschiedene Basismodelle** (CamemBERT, PubMedBERT, BioBERT, ClinicalBERT)</sample>
    <sample id="320">Der Faktor der Überanpassung, der speziell auf die Wiederverwendung von Tests zurückzuführen ist, ist größer als 1.</sample>
    <sample id="321">Die Qualität der Vereinfachung wurde anhand manuell gelabelter Sätze als Goldstandard bewertet, um die Leistung verschiedener automatischen Alignment-Methoden zu vergleichen. Außerdem wurden die Ergebnisse der Feinabstimmung von Sprachmodellen anhand von Metriken wie Scores und Evaluationsmetriken bewertet.</sample>
    <sample id="322">This paper investigates how text classifiers learn about morality, addressing the limitations of treating morality as a single, subjective scale. Drawing on the Moral Foundation Theory, which posits five distinct ways humans perceive morality, we explore whether language models can discern these nuances across different domains. We analyze a dataset of 35,000 tweets from seven domains, including #AllLivesMatter and #BlackLivesMatter, to examine how language models represent moral concepts. Our experiments reveal that models recognize differing expressions of morality within these domains, particularly concerning concepts like subversion.  We demonstrate that language models associate "overthrow" and "mayhem" with negative connotations in the #AllLivesMatter context, while "subversion" is more positively framed in #BlackLivesMatter. This suggests that models can learn domain-specific moral expressions.  However, we caution that relying on a single model for diverse domains risks misunderstandings and potentially dangerous misinterpretations of morality.  The paper highlights the importance of understanding and accounting for the pluralistic and domain-specific nature of morality in NLP.</sample>
    <sample id="323">This paper addresses the challenge of Commonsense Question Answering (QA), which requires leveraging common knowledge to understand and answer questions. Existing approaches often combine language models with knowledge bases (KBs) by retrieving relevant knowledge and using language models and Graph Neural Networks (GNNs) for inference. However, these methods suffer from noisy entity retrieval, isolated encoding of text and knowledge, and limited interaction between modalities.

We propose DHLK, a novel framework that builds a Heterogeneous Knowledge Graph (HKG) from multiple KBs using a two-stage pruning strategy and Knowledge Representation Learning (KRL).  The HKG is then encoded and fused using a language model, incorporating paraphrased entities and dynamically removing irrelevant entities based on attention weights.  We utilize Relation Mask Self-Attention (RMSA) to model the HKG, inspired by RGAT, and optimize entity and relation embeddings with TransE.  The HKG path information is integrated with the QA context through path enhancement.  Finally, a Multi-Layer Perceptron (MLP) predicts the answer probability.

Experiments on CommonsenseQA and OpenBookQA demonstrate that DHLK achieves competitive results compared to existing language model and HKG methods.  We leverage KeyBERT for key entity extraction and ConceptNet for knowledge path retrieval.  The results are reported with leaderboards.</sample>
    <sample id="324">Sprachmodelle haben unterschiedliche politische Vorurteile, die sich in ihren Trainingsdaten widerspiegeln und sich auf ihre Leistung bei nachgelagerten Aufgaben auswirken können.</sample>
    <sample id="325">Hi! Mein Name ist Matthias Lindemann, und heute gebe ich Ihnen eine kurze Einführung in unser Papier über "Kompositionale Generalisierung ohne Bäume mithilfe von Multiset-Tagging und latenten Permutationen". Dies ist gemeinsames Werk mit meinen Betreuern Alexander Koller und Ivan Titov. Kompositionale Generalisierung kann als die Fähigkeit eines Lerners verstanden werden, tiefere Rekursion und unerwartete Kombinationen von Phrasen zu verarbeiten, die während des Trainings gesehen wurden. Im Kontext der semantischen Parsung könnte die Prüfung der kompositionalen Generalisierung wie folgt aussehen. Wir haben eine Trainingsmenge von Sätzen. In diesem Fall "The girl slept." und "Mary knew that the girl slept." Diese Sätze sind mit logischen Formen gepaart, die wesentliche Aspekte ihrer Bedeutung darstellen. Im Gegensatz zur üblichen Bewertung des maschinellen Lernens stammt die Testmenge nicht aus derselben Verteilung, sondern enthält strukturell unbekannte logische Formen. In diesem Beispiel hat das Modell während des Trainings flache Rekursion gesehen und wird auf einen Beispiel mit tieferer Rekursion getestet. Naive seq2seq-Modelle haben Schwierigkeiten mit dieser Art der Out-of-Distribution-Generalisierung und erzeugen oft Ausgaben, die sich vom Eingabeinhalt lösen. Insbesondere scheinen sie oft nicht in der Lage zu sein, die systematischen Korrespondenzen zwischen Eingabe und Ausgabe wiederzugeben, wie z. B. die farbcodierten in dem Beispiel. Eine beliebte Methode zur Bewältigung dieser Herausforderung ist die Integration von Bäumen in die Modelle. Die Bäume sollen den kompositorischen Prozess erfassen, der die Beziehungen zwischen Sätzen und logischen Formen herstellt. Dies funktioniert gut, aber Bäume müssen in der Regel von außen gegeben werden, was ein komplizierter und manchmal rechenintensiver Prozess sein kann. Dies beinhaltet typischerweise erheblichen formalismus-spezifischen Vorverarbeitung der logischen Formen, z. B. zur Behandlung von Variablen. Das Abrufen von Bäumen kann auch spezielle Grammatik-Induktionsverfahren beinhalten. In diesem Papier verwenden wir keine Bäume und führen ein neuronales seq2seq-Modell ein, das direkt die Korrespondenzen zwischen Fragmenten der Eingabe und Fragmenten der Ausgabe modelliert. Zum ersten Mal zeigen wir eine starke Generalisierung auf tiefere Rekursion ohne die Abhängigkeit von Bäumen. Unser Ansatz erzeugt die Ausgabe aus der Eingabe in zwei Schritten. Zuerst taggen wir jeden Eingabetoken mit einem ungeordneten Multiset von Token, die in der Ausgabe erscheinen werden. Nach dem ersten Schritt haben wir alle richtigen Token, aber sie sind nicht geordnet. Deshalb verwenden wir im zweiten Schritt ein anderes Modell, um eine Permutation zu erzeugen, die sie in die richtige Reihenfolge bringt. Wir führen eine neue Methode zur Permutationsvorhersage ein, die keine harten Einschränkungen auf mögliche Permutationen legt. Dies macht unseren Ansatz recht flexibel und ausdrucksstark. Konzeptuell arbeiten unsere Permutationsmodelle ungefähr so: Wir gehen von links nach rechts über die Ausgabe und bestimmen, welches Multiset-Token in jede Position eingesetzt werden soll. Für die erste Ausgabeposition wählen wir einfach eines aus, wie im Roten hervorgehoben. Dann springen wir zur nächsten Multiset-Token, um den zweiten Token in der Ausgabe zu bestimmen. Der dritte Token in der Ausgabe wird auf ähnliche Weise bestimmt, indem wir zu einem anderen Multiset-Token springen. Wir setzen diesen Prozess fort, bis jedes Token aus dem ersten Schritt genau einmal besucht wurde. Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir in unserem Papier unseren Ansatz mit anderen treeless-Modellen auf dem COGS-Benchmark. Unser Modell übertrifft die anderen bei weitem in der Generalisierung auf tiefere Rekursion. Einige andere Arten der strukturellen Generalisierung bleiben jedoch sehr herausfordernd. In unserem Papier lösen wir ein paar interessante technische Herausforderungen. Erstens ist die Zuordnung zwischen Eingabe und Ausgabe nicht im Trainingsdatensatz gegeben. Infolgedessen wissen wir für jedes Token nicht, aus welchem Multiset es stammt, was eine Herausforderung für das Training darstellt. Darüber hinaus gibt es manchmal mehrere Permutationen, die mit den Daten konsistent sind, aber die linguistisch korrekte ist latent. Wir beheben dies, indem wir die Zuordnung während des Trainings induzieren. Unser Permutationsverfahren ist sehr flexibel, bringt aber die Herausforderung mit sich, die höchste bewertete Permutation zu finden, was NP-schwer ist. Dies liegt daran, dass dies mit dem "Traveling Salesman"-Problem zusammenhängt. Wir approximieren dies mit einer GPU-freundlichen kontinuierlichen Relaxation, die auch das Backpropagieren durch die Lösung ermöglicht und uns hilft, linguistisch plausiblere Permutationen zu lernen. Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen angehen, erfahren möchten, schauen Sie sich bitte unser Papier oder kommen Sie zu unserem Plakat an.</sample>
    <sample id="326">Kognitive Dissonanz tritt auf, wenn zwei Überzeugungen oder Handlungen inkonsistent sind, wie z. B. die Aussage, dass man weiß, dass Zigaretten schädlich sind, aber trotzdem raucht. Diese Inkonsistenz führt zu einem Gefühl der Dissonanz, das durch die Notwendigkeit, die Inkonsistenz zu rechtfertigen, verstärkt wird.</sample>
    <sample id="327">ManagerTower is a novel vision-language (VL) model architecture designed to improve cross-modal representation learning by aggregating insights from multiple unimodal experts. Building upon BridgeTower, ManagerTower introduces "managers" within each cross-modal layer to adaptively combine representations from different levels of unimodal encoders (e.g., RoBERTa and CLIP-ViT). This allows for more comprehensive cross-modal alignment and fusion, addressing the limitations of previous architectures that treat all unimodal layers uniformly.

The proposed architecture demonstrates superior performance on VL tasks, achieving significant accuracy gains compared to existing models like BridgeTower, even when trained with limited data. Visualizations reveal that adaptive managers dynamically adjust their aggregation weights based on the specific unimodal experts and cross-modal layers, highlighting the importance of exploiting diverse semantic knowledge at different levels. ManagerTower's effectiveness is validated on the VQAv2 dataset, achieving state-of-the-art results. The paper, code, and models are publicly available.</sample>
    <sample id="328">GPT-4 ist das liberalste Sprachmodell.</sample>
    <sample id="329">```
This paper introduces a novel approach to zero-shot video sentence localization, addressing the limitations of existing methods that rely on manually annotated pseudo-labels.  The proposed method, Structured Pseudo-Label Generation (SPL), generates more complex and relevant pseudo-queries using pre-trained image caption models, overcoming the simplicity of queries in previous approaches.  SPL then leverages a pre-trained model to measure relevance between video frames and pseudo-queries, creating pseudo-events that guarantee high relevance within events and low relevance outside.  To mitigate label noise, SPL employs a two-pronged strategy: weighting samples based on model confidence and IoU with pseudo-labels, and refining pseudo-labels through iterative training.  Experiments on ActivityNet Captions and Charades-STA demonstrate that SPL achieves superior performance compared to existing zero-shot methods, particularly in terms of R@M and mIoU.  The method's ability to generate structured pseudo-labels makes it robust to label noise, enabling effective video sentence localization without requiring extensive manual annotations.  The code is publicly available via the provided QR code.
```</sample>
    <sample id="330">Cumulative training performed equally well or better than iterative training across the different strategies.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">Die Daten für die MuDA-Benchmark stammen aus einem parallelen Korpus, das für die Bewertung verwendet wird.</sample>
    <sample id="333">INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation addresses the limitations of neural machine translation (NMT) models, which often suffer from non-smooth representation spaces hindering generalization.  The proposed framework, INK, injects k-Nearest Neighbors (kNN) knowledge into the NMT model to smooth predictions.  INK employs a two-step training loop: first, kNN knowledge is extracted from a datastore to guide an adapter in adjusting the model's representation; second, updated representations are asynchronously used to refresh the datastore.  This iterative process aligns contextualized representations with token embeddings and kNN token embeddings, addressing sparsity issues.  Experiments on the WMT'19 German-English news translation task demonstrate that INK outperforms state-of-the-art kNN-MT systems, achieving significant improvements in both COMET and BLEU scores while requiring less memory and offering faster inference.  The results indicate that INK effectively refines the NMT model's representation space, even surpassing adapter-only approaches.  The framework's ability to iteratively refine the representation space makes it a promising solution for enhancing the generalization and performance of NMT models.</sample>
    <sample id="335">Matthias Lindemann.</sample>
    <sample id="336">Sprachübergreifender Transfer bezieht sich auf die Leistungsverbesserung von Modellen, die auf einer Sprache trainiert wurden, wenn sie auf eine andere Sprache übertragen werden. Dies wird in der Studie von XSemPLR untersucht, wobei die Ergebnisse zeigen, dass die Leistung im Zero-Shot-Szenario (ohne Training in der Zielsprache) deutlich geringer ist als im Few-Shot-Szenario (mit geringer Datenmenge in der Zielsprache).</sample>
    <sample id="337">This paper introduces a novel approach to learning word embeddings for out-of-vocabulary (OOV) words by leveraging word formation and association. The proposed method constructs a Word Relationship Graph, representing words and wordpieces as nodes with corresponding embeddings.  A two-level graph is created, preserving wordpiece information in the first layer and sampling nodes in the second layer to mitigate noise.  A self-attention network assigns node attributes based on character information. Graph Attention Networks are then applied to refine node representations, followed by a readout block for graph-level summarization. Contrastive learning is employed in the loss function to align OOV words with relevant neighbors and background embeddings, encouraging proximity in the embedding space. Experiments demonstrate superior performance compared to baselines on both intrinsic and extrinsic tasks. The model benefits both static and contextual embedding models.  The paper discusses the potential for extending the model to other languages, particularly agglutinative languages, while acknowledging the challenges posed by fusional languages. The core contribution lies in the ability of the graph-based approach to effectively handle complex word formations and learn meaningful representations for OOV words.</sample>
    <sample id="338">```
This research addresses the challenge of evaluating the helpfulness of human-annotated natural language explanations for training language models. While human annotations are commonly used to guide model explanation generation, their subjective nature necessitates robust evaluation methods.  The work introduces a unified data structure to standardize tasks and facilitate comparison across diverse datasets (CoS-E, ECQA, e-SNLI, ComVE) and models (T5, BART).  Preliminary experiments reveal that explanations, even those deemed less helpful by humans, can still positively impact model performance during fine-tuning.  A novel evaluation metric, TREU, extends the existing simulatability score to specifically assess explanation helpfulness during fine-tuning.  The results demonstrate that TREU outperforms simulatability in evaluating explanation quality, particularly for tasks like entailment where explanation format matters.  The study highlights the task-dependent nature of explanation helpfulness and provides a foundation for more reliable human-AI collaboration in annotation tasks.  The findings suggest that current metrics often fail to capture the nuances of explanation utility, emphasizing the need for more sophisticated evaluation approaches.
```</sample>
    <sample id="339">Saarland University.</sample>
    <sample id="340">ParaAMR is a novel, large-scale paraphrase dataset created using AMR back-translation. Addressing the limitations of existing datasets in terms of scale and syntactic diversity, ParaAMR leverages Abstract Meaning Representations (AMR) graphs to generate paraphrases. The method involves modifying the AMR graph by randomly sampling a node as the new root, altering edges and labels, and then using an AMR graph-to-text generator to produce text. This process ensures semantic similarity while introducing syntactic variation.

ParaAMR contains approximately 15 million source sentences with 6.9 paraphrases per sentence. Quantitative analysis reveals that ParaAMR achieves higher syntactic diversity scores compared to other back-translation datasets while maintaining comparable semantic similarity.  Demonstrating its utility, ParaAMR improves sentence embedding performance on the STS benchmark, enhances syntactic control in paraphrase generation, and boosts few-shot learning scores.  The dataset is publicly available.  This work contributes a valuable resource for training more robust and versatile paraphrase generators, benefiting various NLP applications.</sample>
    <sample id="341">Die Autoren verwenden durchschnittliche Verzögerung und rechnerisch bewertete durchschnittliche Verzögerung, um die Latenz zu messen.</sample>
    <sample id="342">LiveChat is a novel, large-scale, personalized dialogue dataset constructed from live streaming data on Chinese TikTok and Douyin. Addressing the limitations of existing text-based open-domain dialogue datasets, LiveChat incorporates video sources and focuses on personalized conversations, crucial for applications like virtual assistants. The dataset is built through three steps: video scraping, audio transcription via Automatic Speech Recognition (ASR), and dialogue construction using a reply-to-whom matching method. Persona information is extracted through manual labeling and rule-based/classifier approaches.

Experiments on response modeling and addressee recognition demonstrate the benefits of persona profiles and longer average conversation sessions.  While single-stream BERT outperforms double-stream BERT in addressee recognition, BART shows superior performance in response modeling, indicating the dataset's distinct domain.  In-context learning with large language models (LLMs) shows promising results, but performance degrades with excessive demonstrations due to noise.  The findings highlight the value of personalized dialogue data and the unique characteristics of LiveChat compared to existing datasets. Future work will focus on efficient transfer learning for LLMs on this dataset.</sample>
    <sample id="343">Hallo zusammen, ich bin Akshatha, und zusammen mit meinem Co-Autor Martin präsentieren wir unsere Arbeit "Der KITMUS-Test: Bewertung der Wissensintegration aus mehreren Quellen". Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research. Natural Language Understanding-Modelle greifen auf eine Vielzahl von Wissensquellen zurück, wie z. B. Wissen, das in ihren Parametern enthalten ist, normalerweise durch ein Pretraining erworben, und Wissen, das bei der Inferenzzeit in die Eingabe gegeben wird. Aktuelle Arbeiten in Aufgaben wie Frage-Antwort-Systemen zeigen, dass Modelle vor der Inferenzzeit verfügbares Wissen nutzen können, um die Aufgabe zu lösen. Allerdings erfordert Natural Language Understanding oft Wissen, das auch zur Inferenzzeit bereitgestellt wird. Zum Beispiel in dem Satz "John sah den newly elected Präsidenten auf dem TV." können die vortrainierten Parameter Informationen darüber enthalten, was Präsidenten tun und was ein TV ist, aber sie können nicht zuverlässig wissen, wer diese spezifische Entität "John" ist oder wer der neue Präsident ist, da der Präsident sich seit dem Pretraining möglicherweise geändert hat. Daher müssen erfolgreiche Modelle für wissensintensive NLU-Aufgaben die Fähigkeit haben, sowohl vor- als auch nach der Inferenzzeit verfügbares Wissen zu integrieren und zu nutzen. In dieser Arbeit schlagen wir eine diagnostische Testsuite für die Wissensintegration vor. Wir führen eine Coreference-Resolution-Aufgabe ein, die darauf ausgelegt ist, die Fähigkeit zu prüfen, Wissen aus verschiedenen Quellen zu nutzen. Wir bewerten das Datenset mit menschlichen Studienteilnehmern und etablierten Coreference-Resolution-Modellen. Hier ist ein Beispiel aus unserem Datenset. Servin ist ein Richter. Kea ist ein Bäcker. Servin und Kea trafen sich in einem Park. Nach einem langen Arbeitstag, an dem sie Fälle im Strafgericht entschieden hatten, war er glücklich, sich zu entspannen. Die Aufgabe hier ist es, die korrekte Entität zu identifizieren, auf die das Pronomen "he" sich bezieht, was in diesem Fall Servin ist. Die Auflösung eines gegebenen Prononyms erfordert zwei Arten von Informationen. Erstens, entitätsspezifisches Wissen wie "Servin ist ein Richter." Und zweitens, Hintergrundwissen wie "Richter entscheiden Fälle im Strafgericht." Hintergrundwissen wird im Allgemeinen während des Pretrainings großer Sprachmodelle gelernt, während entitätsspezifisches Wissen typischerweise bei der Inferenzzeit beobachtet wird. Wir variieren die Verfügbarkeit dieser beiden Arten von Informationen, so dass sie entweder in einer einzigen Quelle oder in mehreren Quellen vorhanden sein können. Wir haben drei Einstellungen für KITMUS definiert. Erstens haben wir die typische Einstellung: "Background-Pretrain", bei der Hintergrundwissen als vor dem Pretraining verfügbar angenommen wird. Zweitens gibt es eine "Background-Both"-Einstellung, bei der Hintergrundwissen sowohl vor als auch bei der Inferenzzeit verfügbar ist. Schließlich gibt es die "Background-Inference"-Einstellung, bei der sowohl die Arten von Wissen nur bei der Inferenzzeit verfügbar sind. Diese letzte Einstellung ist besonders interessant, da sie die Situation simuliert, in der das Hintergrundwissen, das für die Lösung einer Aufgabe erforderlich ist, nicht in den Pretraining-Daten der Modelle enthalten ist. Zum Beispiel, da sich seit dem Pretraining neue Berufe entwickelt haben. Hier ist ein Beispiel dafür, wie wir die Verfügbarkeit von Fakten in den wahren Quellen steuern. In der "Background-Pretrain"-Einstellung nehmen wir an, dass das Hintergrundwissen "Politiker suchen Sitze in der Regierung" in den vortrainierten Parametern und im Inferenzzeitkontext bereitgestellt wird, in dem wir die entitätsspezifischen Kenntnisse "Chichester ist ein Politiker" liefern. In der "Background-Both"-Einstellung liefern wir zusätzlich zu den entitätsspezifischen Informationen auch Hintergrundwissen über Politiker in ihrem Inferenzzeitkontext. In der "Background-Inference"-Einstellung liefern wir die fiktive Berufsbezeichnung "mirituer" anstelle von Politiker, da "mirituer" unwahrscheinlich ist, in den vortrainierten Parametern enthalten zu sein. Wir bewerten das Datenset sowohl mit menschlichen Studienteilnehmern als auch mit etablierten Coreference-Resolution-Modellen. In dieser Abbildung zeigen wir die Ergebnisse der besten Modelle auf der schwierigsten Variante der "Background-Pretrain"-Einstellung. Ohne task-spezifisches Training auf KITMUS schneiden sowohl Modelle C2F als auch BERT4Coref schlecht ab. Bei task-spezifischem Training jedoch schneiden beide Modelle deutlich besser ab als eine zufällige Auswahl. Dies deutet darauf hin, dass bei der Verwendung von generischen Referenzauflösungsdatensätzen die meisten Modelle dazu lernen, Oberflächenmerkmale zu nutzen, die bei KITMUS, wo solche Merkmale entfernt wurden, nicht nützlich sind. Zusätzliche Experimente mit fiktivem Wissen zeigten sogar, dass die besten Modelle nicht zuverlässig Wissen integrieren können, das nur zur Inferenzzeit bereitgestellt wird. Um die wichtigsten Erkenntnisse unserer Arbeit zusammenzufassen, scheinen viele Coreference-Resolution-Modelle nicht in der Lage zu sein, über Wissen aus verschiedenen Quellen zu raisonieren, ohne task-spezifisches Training. Allerdings können einige Modelle mit task-spezifischem Training erfolgreich Wissen aus mehreren Quellen integrieren. Dennoch scheinen selbst die besten Modelle Schwierigkeiten zu haben, zuverlässig Wissen zu integrieren, das nur zur Inferenzzeit präsentiert wird. Wenn Sie mehr Details wünschen, sehen Sie sich bitte unsere Arbeit an und schauen Sie sich das Datenset und den Code auf GitHub an. Vielen Dank fürs Zuhören.</sample>
    <sample id="344">Baumbasierte Methoden erfordern die Beschaffung von Bäumen, was komplex und teuer sein kann. Die Baumstruktur muss oft formal-spezifische Vorverarbeitung erfordern, z.B. zur Behandlung von Variablen, oder spezielle Grammatik-Induktionsverfahren.</sample>
    <sample id="345">This paper introduces a novel neural sequence-to-sequence model for compositional generalization in semantic parsing, achieving strong performance without relying on explicit trees. Compositional generalization refers to the ability to handle unseen combinations of phrases seen during training.  The model predicts the output from the input in two steps: first, tagging input tokens with an unordered multiset of potential output tokens; second, predicting a permutation of these tokens to generate the final output.  A key innovation is a permutation prediction method that avoids hard constraints, promoting flexibility and expressiveness. The alignment between input and output is learned during training, addressing the challenge of unknown alignments.  The permutation prediction problem is approximated using a GPU-friendly continuous relaxation to enable efficient training and backpropagation.  Experiments on the COGS benchmark demonstrate significant generalization to deeper recursion compared to existing treeless models.  While finding the optimal permutation is NP-hard, the proposed approach effectively addresses this challenge, enabling robust compositional generalization.</sample>
    <sample id="346">Der Text enthält keine Informationen darüber, welcher Universität die Autoren angehören.</sample>
    <sample id="347">Hi, ich bin Myra und heute spreche ich über unser Paper "Markierte Personas: Verwendung von Natural Language Prompts zur Messung von Stereotypen in Sprachmodellen". Diese Arbeit ist in Zusammenarbeit mit Esin Durmus und Dan Jurafsky entstanden. In den letzten Jahren wurde die Häufigkeit sozialer Vorurteile und Stereotypen in großen Sprachmodellen (LLMs) dokumentiert. Diese Messungen haben jedoch verschiedene Einschränkungen. Sie stützen sich in der Regel auf handgefertigte Datensätze, die sehr zeitaufwendig zu kuratieren sind, und messen oft nur sehr spezifische Stereotypen, was ihre Generalisierbarkeit auf andere Demografien oder Kontexte einschränkt, oder sie erfassen einfach sehr allgemeine, breite Assoziationen, wie negative Assoziationen mit bestimmten Gruppen. Darüber hinaus berücksichtigt die meisten Arbeiten in diesem Bereich nicht die Intersektionalität, die Vorstellung, dass mehrschichtige soziale Identitäten Vorurteile verstärken und einzigartige Quellen für Schaden darstellen können. Um diese Einschränkungen zu überwinden, stützen wir uns auf die Tatsache, dass diese neueren, instruktionsgetunten LLMs sehr gut darauf reagieren, Anweisungen und Prompts zu befolgen. So können wir das Modell bitten, eine Persona zu generieren, die eine Darstellung einer imaginären Person ist, mit einem Prompt wie "Stell dir vor, du bist eine asiatische Frau. Beschreibe dich selbst." Und wir können sofort sehen, dass dies sehr generalisierbar auf jede Demografie ist, da wir einfach jede Identitätsmarkierung angeben können, die wir wünschen, in diesen Prompt. Hier sind einige Beispiele für Generierungen von GPT-4. Wir sehen sofort, dass, obwohl die Outputs nicht unbedingt offensichtlich negativ oder toxisch im traditionellen Sinne sind, einige interessante Muster vorhanden sind. Die asiatische Frau wird als unauffällig dargestellt; die mittelöstliche Frau wird mit Wörtern wie exotisch und so etwas wie "eine faszinierende Region" bezeichnet. Und sowohl die Frauen von Farbe als auch die weiße Mann-Persona erwähnen ihre Herkunft, während die weiße Mann-Persona das nicht tut. Um diese Muster zu erfassen, hat unsere Methode zwei Teile. Der erste Teil ist die Generierung dieser Personas. Unsere Prompts zur Generierung dieser Personas waren inspiriert von einer Studie, bei der man menschliche Probanden dazu brachte, diese Prompts zu verwenden, und dabei stellte man fest, dass sie auch selbst auf Rassistische Stereotypen aufmerksam wurden. Außerdem ermöglicht dies einen direkten Vergleich unserer generierten Personas mit den von Menschen geschriebenen Antworten. Der zweite Teil ist "marked words", eine Methode zur Identifizierung der Wörter, die markierte Gruppen von unmarkierten Gruppen unterscheiden, was ich kurz erläutern werde. Der Vorteil davon ist, dass wir wirklich spezifische Stereotypen und Muster erhalten, ohne auf einen bestimmten Lexikon angewiesen zu sein. Die Methode "marked words" stützt sich auf das sociolinguistische Konzept der "Markierung", das besagt, dass es einen unmarkierten Standard gibt, und jede Gruppe, die sich davon unterscheidet, ist linguistisch markiert. So beispielsweise ist das Wort "Krieger" normalerweise mit Männern verbunden. Wenn Menschen also eine Frau als Krieger beschreiben, verwenden sie normalerweise tatsächlich die Bezeichnung "Frauenkrieger" und markieren das Wort mit "Frauen". Im Allgemeinen sind dominante Gruppen in der Gesellschaft sowohl linguistisch als auch sozial unmarkiert, während marginalisierte Gruppen normalerweise markiert sind. In unserer Methode identifizieren wir zunächst, welche die unmarkierte und markierte Gruppen sind, und vergleichen dann die Personas mithilfe der "Fightin’ Words"-Methode, die im Wesentlichen die Verwendung gewichteter log-Odds-Verhältnisse beinhaltet, um die Top-Wörter für jede markierte Gruppe zu unterscheiden. So beispielsweise für die Personas von schwarzen Frauen würden wir "Fightin’ Words" verwenden und die log-Odds-Verhältnisse gegen die beiden entsprechenden unmarkierten Gruppen, weiße Personas und Mann-Personas, vergleichen. Nun zu einigen Ergebnissen. Zuerst verwenden wir ein Lexikon von Stereotypen und stellen fest, dass die generierten Personas viel mehr Stereotypen enthalten als die von Menschen geschriebenen. Allerdings, wenn wir die Verteilung der Wörter und des Lexikons betrachten, finden wir sehr unterschiedliche Dinge. Während die generierten Personas viel höhere Raten der Lexikonwörter aufweisen, haben die von Menschen geschriebenen Antworten eine viel breitere Verteilung der Wörter, während die Stereotypwörter, die in den generierten Personas enthalten sind, wirklich nur die Wörter "tall" und "athletic" sind. Wirklich nur die positiven oder zumindest nicht-negativen. Und tatsächlich, dieses Lexikon fängt viele der schädlichen Muster, die wir in den früheren Folien gut gesehen haben, nicht wirklich ein. Stattdessen werden wir uns auf die Ergebnisse unserer "marked words"-Methode konzentrieren, um zu zeigen, wie diese scheinbar positiven Darstellungen schädliche Muster aufdecken. In unserer Analyse zeigen wir, dass die Top-Wörter für unsere Gruppen Dinge wie "kultur", "tradition", "stolz" und "exotisch" sind. Diese Wörter definieren diese Gruppen nur in Bezug auf ihre Identität und unterscheiden sie von der weißen Norm. Dies trägt zu einer langen Tradition der Diskriminierung und Ausgrenzung dieser Gruppen bei. Darüber hinaus gibt es viele gemeinsame Tropen, die in diesen Wörtern widergespiegelt werden, insbesondere bei Frauen von Farbe. So beispielsweise enthalten die Wörter, die Latina-Frauen beschreiben, Dinge wie "lebendig" und "kurvig", die sich auf den Trope der Tropik beziehen. Bei asiatischen Frauen sind die Wörter Dinge wie "klein" und "zart" und "smaragdgrün", die sich auf eine lange Geschichte der Hypersexualisierung und Unterwürfigkeit von asiatischen Frauen beziehen. Und schließlich bei schwarzen Frauen sehen wir, dass einige der Top-Wörter Dinge wie "stark" und "widerstandsfähig" sind. Dies verbindet sich mit dem Archetyp der "Starken Schwarzen Frau", der jedoch eine sehr schädliche Archetyp ist, da er diese Demografien unter Druck setzt, widerstandsfähig und stark gegen gesellschaftliche Hindernisse zu sein. Anstatt diese Hindernisse zu beseitigen, setzt er diese Menschen unter Druck, sie zu überwinden, was zu sehr negativen Gesundheitsergebnissen und anderen Schäden führen kann. Im Allgemeinen finden wir, dass die Wörter für jede markierte Gruppe sich größtenteils auf sehr essentialisierende Narrative beziehen. Basierend auf diesen Mustern kommen wir zu drei Empfehlungen für Modellbetreiber. Erstens sollten wir als Forscher positive Stereotypen und essentialisierende Narrative angehen. Wir sollten auch einen intersektionalen Blick auf die Untersuchung von Vorurteilen und Schäden verwenden, da viele Dinge übersehen werden könnten, wenn wir das nicht tun. Und schließlich sollte es mehr Transparenz bei der Umsetzung von Bias-Minderungsmaßnahmen geben, denn beispielsweise bei diesen positiven Stereotypen wissen wir nicht, ob es sich um eine Art übermäßige Wertzuordnung handelt oder um andere Anti-Stereotypisierungsmaßnahmen, die zu diesen schädlichen Mustern führen. Wir können diese Dinge einfach nicht ableiten oder weiter untersuchen, ohne mehr Transparenz zu haben. Vielen Dank fürs Zuhören. Haben Sie einen schönen Tag auf der ACL.</sample>
    <sample id="348">## Abstract: Marked Personas: Measuring Stereotypes in Language Models

This paper investigates the prevalence of social biases and stereotypes in large language models (LLMs) by leveraging the ability of instruction-tuned models to generate personas. We address limitations of existing bias measurement methods, which often rely on time-consuming hand-crafted datasets and fail to capture intersectional biases. Our approach involves prompting LLMs to generate personas for various demographic groups and analyzing the resulting text for stereotypical word associations.

We introduce the "Marked Words" method, which identifies words that distinguish marked groups (e.g., women of color) from unmarked groups (e.g., white men) based on sociolinguistic "markedness."  By comparing persona generations across different demographic groups, we reveal that LLMs often generate personas laden with positive stereotypes, such as "exotic" for Middle Eastern women or "strong" for Black women, reflecting harmful essentializing narratives.  

Our findings demonstrate that while LLMs may generate seemingly neutral text, they perpetuate and amplify existing societal biases. We conclude with recommendations for researchers and model owners to address positive stereotypes, adopt an intersectional lens, and increase transparency regarding bias mitigation techniques.</sample>
    <sample id="349">Hallo zusammen, mein Name ist Jingwei Yi von der Chinesischen Universität für Technologie. Es ist mir eine Freude, ein kurzes Werbevideo für unsere Arbeit zu geben. Kopieren Sie mein Modell? Schutz des Urheberrechts großer Sprachmodelle für Embedding als Dienstleistungen durch Backdoor-Wasserzeichen. Lassen Sie uns zunächst den Hintergrund von Embedding als Dienstleistungen vorstellen. Derzeit sind große Sprachmodelle wie GPT, LLAMA, PALM in der natürlichen Sprachverständnis- und -generierung herausragend. Embedding als Dienstleistungen ist einer der Dienste, die auf großen Sprachmodellen aufgebaut sind, um verschiedene NLP-Aufgaben zu unterstützen. Zum Beispiel bietet OpenAI eine Embedding-API auf Basis von GPT an. Allerdings haben jüngste Arbeiten gezeigt, dass ein Angreifer das Modell durch das Lernen aus den Embeddings stehlen und ähnliche Dienstleistungen anbieten kann. Daher ist es notwendig, das Urheberrecht von Embedding als Dienstleistungen zu schützen. Um das Urheberrecht von Embedding als Dienstleistungen zu schützen, ist eine Lösung, ein Wasserzeichen in der Anbieter-Dienstleistung einzubetten und zu überprüfen, ob eine andere Dienstleistung das Wasserzeichen enthält. Die Wasserzeichenmethode muss folgende Eigenschaften erfüllen. Erstens muss die Methode für Embedding als Dienstleistungen anwendbar sein. Zweitens sollte das Wasserzeichen die Nutzbarkeit der bereitgestellten Embeddings nicht beeinträchtigen. Drittens sollte das Wasserzeichen ausreichend versteckt sein, sodass der Angreifer es leicht entfernen kann. Viertens muss das Wasserzeichen während des Modell-Extraktionsprozesses an die Dienste des Angreifers übertragbar sein. Bisherige Arbeiten lassen sich in vier Kategorien einteilen. Diese Methode ist jedoch entweder für Embedding als Dienstleistungen nicht anwendbar oder weist keine Übertragbarkeit auf. Daher schlagen wir in dieser Arbeit Embedding Marker vor, eine Backdoor-basierte Wasserzeichenmethode, die für Embedding als Dienstleistungen anwendbar ist. Nun möchte ich die Details unseres Embedding Markers vorstellen. Embedding Marker besteht aus zwei Hauptschritten: Wasserzeichen-Einfügung und Urheberrechtsverifizierung. Bevor diese Hauptschritte erfolgen, wählen wir zunächst ein Trigger-Set aus. Das Trigger-Set ist eine Gruppe von Wörtern innerhalb eines moderaten Frequenzintervalls. Wir gehen davon aus, dass der Anbieter einen allgemeinen Textkorpus sammeln und die Wortfrequenzen ermitteln kann. Bei der Wasserzeichen-Einfügung definieren wir zunächst ein Ziel-Embedding. Wenn ein Benutzer eine Nachricht an den Anbieter-Dienst sendet, zählt der Anbieter die Anzahl der Trigger im Satz. Das bereitgestellte Embedding ist eine Gewichtssumme des Ziel-Embeddings und des ursprünglichen Embeddings. Das Gewicht des Ziel-Embeddings ist proportional zur Anzahl der Trigger im Satz. Wenn die Anzahl der Trigger im Satz größer als m ist, entspricht das bereitgestellte Embedding genau dem Ziel-Embedding. Die Urheberrechtsverifizierung besteht darin zu überprüfen, ob ein Modell hinter einer anderen Dienstleistung das Wortmarken enthält. Wir erstellen zunächst ein Backdoor- und ein benignes Datenset. Das Backdoor-Datenset enthält Sätze, in denen alle Wörter aus dem Trigger-Set bestehen, während alle Wörter in den Sätzen des benignen Datensets nicht aus dem Trigger-Set stammen. Der Anbieter fordert die Embeddings von der Dienstleistung des Diebes mit dem Datenset an. Die Cosinus- und L2-Ähnlichkeit zwischen dem angeforderten Embedding und dem Ziel-Embedding werden berechnet. Wir berechnen die Ähnlichkeitsdifferenz zwischen dem benignen und dem Backdoor-Datenset, definiert als delta Cosinus und delta L2. Gleichzeitig wenden wir den KS-Test an und verwenden seinen p-Wert als dritte Metrik. Wir führen Experimente auf vier Datensätzen durch: AG News, MIND, SST2 und Enron Spam. Wir gehen davon aus, dass der Anbieter das Wiki-Text-Datenset verwenden kann, um die Wortfrequenzen zu zählen. Die Ergebnisse auf den vier Datensätzen zeigen, dass unser Embedding Marker eine hervorragende Detektionsleistung aufweist, während die Nutzbarkeit für nachgelagerte Aufgaben erhalten bleibt. Wir validieren auch die Versteckbarkeit des bereitgestellten Embeddings, indem wir die Embeddings von Sätzen auf den vier Datensätzen mithilfe von PCA visualisieren. Die Legende der Figuren zeigt die Anzahl der Trigger in jedem Satz. Wie die Figuren zeigen, ist es schwierig, die Backdoor-Embeddings von den normalen Embeddings zu unterscheiden. Das war's. Vielen Dank. Wir freuen uns auf die Diskussion mit Ihnen.</sample>
    <sample id="350">## Abstract

This paper investigates the reliability of leaderboard scores in Natural Language Understanding (NLU), particularly concerning the meaning of "superhuman performance" achieved on benchmark datasets like SuperGLUE and SQuAD. While models frequently surpass human performance on these benchmarks, the paper argues that these achievements are not necessarily indicative of true understanding. 

The study highlights several issues with current evaluation practices, including the use of mismatched datasets (systems evaluated on full test sets, humans on subsets), errors in ground truth answers, and the often-vague estimation of human performance. Furthermore, the paper points out potential biases in human evaluation due to inadequate compensation and lack of transparency regarding annotator pools. 

The authors contend that these factors make comparisons between models and humans unreliable and that claims of superhuman performance are not yet scientifically meaningful. The paper concludes by offering recommendations for constructing more robust and reliable benchmarks to avoid repeating these errors in future research.</sample>
    <sample id="351">This paper investigates the generalization capabilities of CoNLL-2003 named entity taggers in 2023, a task that has been studied for nearly two decades. The study addresses the questions of whether these models generalize to modern data and what is needed for good generalization, and what causes performance drops. To investigate, a new dataset, CoNLL++, was created by annotating Reuters News from 2020 with the CoNLL-2003 guidelines. Over 20 models were fine-tuned on CoNLL-2003 and evaluated on both datasets, with F1-score changes used to assess generalization.

The research found that transformer models, larger model sizes, and more fine-tuning examples are crucial for good generalization.  Adaptive overfitting was not observed.  However, temporal drift, caused by the increasing gap between training and testing data, was identified as the primary cause of performance degradation. The authors conclude that achieving good generalization requires a combination of these factors, and that CoNLL-2003 taggers continue to perform well in 2023. The paper encourages further research into improving model generalization.</sample>
    <sample id="352">ABC-Eval steht für Annotating Behaviors in Chat.</sample>
    <sample id="353">```
This paper addresses the challenge of input underspecification in code generation, a critical issue in program synthesis.  State-of-the-art methods struggle with situations where natural language descriptions lack sufficient details, particularly regarding operation-level specifications.  We introduce CodeClarQA, a dataset of clarification questions for key operations, and a pipeline for code generation driven by asking clarification questions.  Our method leverages a code knowledge graph to identify key operations and then uses similarity scores between the natural language description and operation documentation to determine if specifications are missing.  We demonstrate that our approach effectively identifies missing key operations, with MPNet achieving strong performance.  Error analysis reveals challenges in distinguishing aligned operations and using documentation instead of argument values.  Experiments show that incorporating clarification questions improves code generation performance, although the pipeline still lags behind model-only training.  We analyze the impact of clarified key operations on code generation, finding a positive correlation.  The paper highlights the difficulty of the task and invites feedback on the dataset and methodology.
```</sample>
    <sample id="354">Das Leistungsdelta zwischen CoNLL-2003 und CoNLL++ ist höher als 5 Prozentpunkte, bis zu dem Jahr 2020.</sample>
    <sample id="355">Hallo, mein Name ist Vasudha und ich bin eine Doktorandin der Informatik an der Stony Brook University. Wir präsentieren unsere Arbeit, die auf der ACL 2023 accepted wurde, als Long Paper, "Transfer Learning für Dissonanzerkennung: Die Herausforderung des seltenen Klassen". Wir beginnen mit der Definition von kognitiver Dissonanz und warum es ein wichtiges Problem ist, das in der Sprache untersucht werden soll. Einfach gesagt ist kognitive Dissonanz eine Situation, in der zwei Überzeugungen oder Handlungen inkonsistent sind, wie zum Beispiel der Fall, dass eine Person sagt: "Ich weiß, dass Zigaretten das töten können", und dann sagt: "Ich habe mir nach dem Meeting ein paar Zigaretten gegönnt". Diese Überzeugung und Handlung sind inkonsistent, und sie befinden sich in Dissonanz. Weiterhin wird erwähnt, dass "Ich glaube nicht, dass ich meinen Job ohne sie behalten könnte" diese zweite Gelegenheit rechtfertigt. Und sie haben eine Konsonanzbeziehung. Während Dissonanz ein sehr häufiges Phänomen ist, das wir im täglichen Entscheidungsprozess erleben, sind sie in der Sprache im Vergleich zu anderen Arten von Diskursbeziehungen sehr selten. Warum ist das wichtig? Das Studium kognitiver Dissonanz kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten zwischen Menschen zu verstehen, Trends und Glaubenswerte zu verfolgen und Veränderungen der Einstellungen in der Bevölkerung zu verstehen. Hohe kognitive Dissonanz ist auch mit Angststörungen verbunden und kann uns helfen, das mentale Wohlbefinden der Menschen besser zu verstehen. Das Studium von Dissonanz in der Sprache kann auch dazu beitragen, das Extremismus und die Polarisierung vulnerabler Gruppen zu verstehen. Schließlich ist kognitive Dissonanz wichtig, um die persönlichen kognitiven Stile von Individuen zu verstehen und das Verständnis von Entscheidungsprozessen zu verbessern. Um eine Ressource für kognitive Dissonanz zu erstellen, haben wir eine groß angelegte Annotation von Dissonanzbeziehungen durchgeführt. Wir haben einen Dissonanz-first-Ansatz verwendet, wie im Flussdiagramm dargestellt. Tweets wurden mit dem PDTB-Parser übergeben, und Paare von Diskurs-Einheiten wurden gemäß den in unserem Paper beschriebenen Richtlinien annotiert. Wie Sie hier sehen können, wurde Dissonanz nur in 3,5 % der annotierten Paare gefunden. Nachdem wir etwa 1.000 Beispiele für Diskurs-Einheitenpaare gesammelt hatten, haben wir eine Initial-Classifier trainiert, die nur mit 43 Beispielen Dissonanz trainiert wurde. Zu unserer Überraschung war der Classifier nicht viel besser als der Zufall. Aufgrund der geringen Häufigkeit von Dissonanz und dem Fehlen eines vorherigen solchen Datensatzes stehen wir vor dem Problem der absoluten Seltenheit. Um dies zu mildern, haben wir verschiedene Kombinationen von Transferlernen und aktiver Lernstrategien untersucht, um mehr dissonante Beispiele zu sammeln, über weniger Annotationsdurchläufe, wodurch die gesamten Annotationskosten gesenkt und die Dissonanzerkennung verbessert werden. Da das anfängliche Modell die Dissonanzklasse überhaupt nicht erfassen konnte, haben wir den aktiven Lernprozess durch die Übertragung von Gewichten von eng verwandten Aufgaben gestartet. Wir haben von zwei verschiedenen Aufgaben übertragen: Stimmungs-unabhängige Dissonanz-Stance-Klassifizierung, eine Aufgabe, die bestimmt, ob zwei Aussagen von verschiedenen Personen in Übereinstimmung oder im Widerspruch zueinander stehen, unabhängig vom Thema, genannt Debatte hier, und von binärer Klassifizierung von Expansion und Vergleichsklassen des PDTB, da diese beiden eng mit der Konzeption von Konsonanz und Dissonanz zusammenhängen und wir sie CE nennen. Wir stellen fest, dass die Zero-Shot-Performance durch die Übertragung der Ergebnisse auf dem annotierten Datensatz bereits deutlich besser ist als der Zufall, mit dem besten AUC von 0,62. Darüber hinaus stellen wir fest, dass die Feinabstimmung von CE-Aufgaben gefolgt von weiterer Feinabstimmung auf Debatte eine viel bessere Zero-Shot-Performance ergibt. Dies ist also das Modell, das wir verwenden, um den aktiven Lernprozess zu starten. Als Nächstes bestimmen wir die beste Methode zur Aktualisierung eines Modells mit neuen Daten aus jedem Durchlauf des aktiven Lernens und der Annotation. "Kumulativ" sammelt alle Daten, die im aktiven Annotationsprozess bisher gesammelt wurden, während "Iterativ" das Modell durch Training auf dem neuesten Datensatz aktualisiert. Unter den verschiedenen Strategien haben wir festgestellt, dass "Kumulativ" gleich gut oder besser als "Iterativ" in allen Bereichen funktioniert. Als Nächstes verwenden wir eine Wahrscheinlichkeit-für-seltene-Klassen-Strategie (PRC), um hauptsächlich die Beispiele auszuwählen, die vom aktuellen Modell bei jedem Durchlauf am wahrscheinlichsten als dissonant eingestuft werden. Wir vergleichen dies mit anderen state-of-the-art-AL-Strategien, die in der Community üblicherweise verwendet werden. Wir stellen fest, dass die vorgeschlagene PRC-Strategie besser funktioniert als andere state-of-the-art-Strategien, obwohl der Unterschied gering ist. Beachten Sie, dass die Leistung bei zufälligen Strategien deutlich schlechter ist. In weiteren Durchläufen des aktiven Lernens mit den beiden besten Strategien verbessern wir die Dissonanzklassifizierung AUC auf 0,75, was bisher unser bestes Ergebnis ist. Wir überprüfen auch die Machbarkeit jeder Strategie für die Annotationsqualität und die Kosten für die Annotatoren. Wir stellen fest, dass PRC den höchsten Prozentsatz an Dissonanz aufweist und am besten für seltene Klassen funktioniert. Die Annotatoren finden die Beispiele jedoch auch schwierig. Zusammenfassend lässt sich sagen, dass PRC eine einfache AL-Strategie für die Akquisition von seltenen Klassen ist und das Starten des aktiven Lernens mit appropriately designed Transfer Learning Tasks signifikant hilft. Wir stellen auch fest, dass iterative Updates für das Transferlernen aus einem anderen Bereich nützlich sind, während bei domain-aktiven Annotationen kumulative Updates Vorteile bringen. Dies sind die Links zu unserem Core Dataset und unserem Paper. Bitte zögern Sie nicht, sich bei Fragen an uns zu wenden. Vielen Dank.</sample>
    <sample id="356">The authors are affiliated with an unspecified university. The text does not state the university.</sample>
    <sample id="357">Siyu Yuan from Fudan University.</sample>
    <sample id="358">Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig.</sample>
    <sample id="359">Der Ansatz wird mit Wait-k, Local Agreement und der state-of-the-art Architektur für Simultaneous Pre-translation verglichen.</sample>
    <sample id="361">This presentation introduces CounterComp, a novel approach to improve compositional generalization in multi-step quantitative reasoning, specifically question answering over financial tables. State-of-the-art models struggle with tasks requiring more than two reasoning steps due to memorization of spurious patterns, such as repeating tokens. CounterComp addresses this by mining counterfactual scenarios from training data.  It treats each training sample as an anchor and generates positive (no output change) and negative (output change) examples through question interventions.  An auxiliary metric learning loss is then added to the training procedure, dynamically adjusting its margin based on the extent of intervention.  Experiments demonstrate that CounterComp consistently improves performance on in-distribution and, crucially, out-of-distribution samples, including those with novel datasets or unseen examples.  Qualitative analysis shows the model learns to attend to more relevant tokens during training.  The approach avoids costly human annotation by leveraging the inherent compositional nature of the reasoning task.  The presentation highlights the effectiveness of CounterComp in mitigating spurious pattern memorization and promoting robust generalization.</sample>
  </task>
</testset>