<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="de">
    <sample id="0">Webcrawler-Daten, insbesondere politische Nachrichtenmedien wie der New York Times, Los Angeles Times, The Guardian, Huffington Post usw.</sample>
    <sample id="1">Macquarie University.</sample>
    <sample id="2">Hallo, willkommen zu unserer Präsentation der Plane, ein neues Tool für die Textsegmentierung auf Dokumentebene und auf Satzebene.</sample>
    <sample id="3">Mein Name ist Regina Stodden, und ich werde Sie durch die erste Phase der Präsentation führen.
Lassen Sie uns zunächst Textvereinfachung definieren.</sample>
    <sample id="4">Textsimplifizierung ist ein Prozess der Anpassung eines Textes, um die Textverständlichkeit für eine bestimmte Zielgruppe zu verbessern.</sample>
    <sample id="5">Um ein Text-Entschlüsselungsmodell zu trainieren, benötigen wir ein paar von Texten. Zum Beispiel Dokumente oder Sätze.</sample>
    <sample id="6">Schreibe eine deutsche Übersetzung des englischen Inhalts.</sample>
    <sample id="7">Um einen Satz zu vereinfachen, gibt es verschiedene Techniken, wie z. B. lexikalische Substitution, Satzlösung, Satzentfernung, Umstellung oder Einfügung von Wörtern.</sample>
    <sample id="8">Wir schlagen ein neues Modell für die Darstellung vor. Denn in den letzten Jahren gab es Probleme mit dem bestehenden Kooperationsmodell. So beispielsweise dieses Kooperationsmodell hier, das zu klein ist, um eine effektive Transaktionsmodellierung zu ermöglichen.</sample>
    <sample id="9">Ja, das Remodellieren, das in den letzten Jahren vorgeschlagen wurde, oder alle automatisch ausgerichtet, was bedeutet, dass es fehleranfällig ist und seine Ausrichtung nicht</sample>
    <sample id="10">Daher schlagen wir unser neues Corporate-Plane vor, das sich in zwei Untergesellschaften aufteilt: Plane APA und Plane Web. Plane APA ist auf Nutztexte ausgerichtet.</sample>
    <sample id="11">In der Plain API haben wir 483 Dokumente manuell ausgerichtet. Dies führt zu etwa 30.000 bis 30.000 Satzpaaren.</sample>
    <sample id="12">für die Plain Web. Dieses Korpus beinhaltet verschiedene Domains, und wir haben auch alle diese 750 Dokumente auf der einen Hand manuell und auf der anderen Hand mit automatischen Alignment-Methoden ausgerichtet.</sample>
    <sample id="13">Die endgültige Ergebnis ist 30.450 Sätze pro.</sample>
    <sample id="14">Wir analysieren unsere Satzpaare ein wenig genauer. Zum Beispiel, auf die Art der Schwierigkeit.</sample>
    <sample id="15">Ich kann hier sehen, dass die Bibeltexte viel stärker vereinfacht sind als beispielsweise ein Nachrichtentext oder ein Sprachlern-Text.</sample>
    <sample id="16">auf allen Ebenen bezüglich zum Beispiel lexikalischer Semantik, struktureller Semantik oder auf globaler Ebene der Semantik.</sample>
    <sample id="17">Wie Sie sehen können, hat unser Trainingskorpus eine höhere Variabilität von verschiedenen Transformationen. So haben wir zum Beispiel im Trainingskorpus viel mehr Wortstellungen und Umstellungen, als wir im Trainingskorpus der Webanwendung haben.</sample>
    <sample id="18">Auf der einen Seite und im Webkörper haben wir viel mehr Relevanz.</sample>
    <sample id="19">Hallo, ich bin Omar und jetzt werde ich die Anwendungsfälle für unseren Datensatz DeepPlane vorstellen. Zuerst können wir automatische Alignierungsmessungen bewerten.</sample>
    <sample id="20">In den letzten Jahren gab es viele Alignment-Methoden, aber im Kontext der maschinellen Übersetzung</sample>
    <sample id="21">Wo wir zwei parallele Dokumente haben, die in verschiedenen Sprachen geschrieben sind, und wir möchten die Übereinstimmungen von Sätzen in der Dokumenten-</sample>
    <sample id="22">Aber in unserem Anwendungsfall versuchen wir, Ausrichtungen zwischen Sätzen von zwei parallelen Dokumenten zu extrahieren, die dieselbe Sprache haben, dieselbe Inhalte haben, aber auf einem anderen Komplexitätslevel liegen.</sample>
    <sample id="23">Und jetzt, da wir unsere Daten im Deep-Plane haben, die wir manuell ausgerichtet haben, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten.</sample>
    <sample id="24">Und wir haben einige Anpassungen an die vorgeschlagenen Methoden vorgenommen und alle diese Anpassungen sowie den Code zum Ausführen unserer Experimente in der Arbeit veröffentlicht.</sample>
    <sample id="25">Am Ende haben wir festgestellt, dass die beste Methode zur automatischen Ausrichtung für deutsche Texte die Methode von Masselara ist.</sample>
    <sample id="26">Und Sie können auch den Code finden, um diese Methode auf Ihren eigenen Dokumenten in der Datei zu verwenden.</sample>
    <sample id="27">Der zweite Anwendungsfall, den wir in unserer Arbeit gezeigt haben, ist der Fall der automatischen Textsimplifizierung.</sample>
    <sample id="28">Ich finde Feinabstimmung von Sprachmodellen, um vereinfachten Text aus komplexem Eingabetext zu produzieren.</sample>
    <sample id="29">Wir haben zwei verschiedene Modelle feinabgestimmt. Wir haben ein Modell von Longformer feinabgestimmt, um Dokumentenlevel-Vereinfachungen zu produzieren.</sample>
    <sample id="30">Und wir haben auch die Normalisierung der Normalisierung der Bedeutung verbessert, um Satzebene-Vereinfachungen zu produzieren.</sample>
    <sample id="31">Sie können auch die Checkpoints und äh Sie können sich detailliertere Informationen zu den Scores und der Bewertungsmethode unserer Experimente im Paper ansehen.</sample>
    <sample id="32">Wir haben festgestellt, dass diese grundlegende Feinabstimmung niedrigere Werte als die Baseline-Werte produzieren oder erhalten kann.</sample>
    <sample id="33">Und wir schlagen diese Ergebnisse als Referenz, als Basisreferenz für das Problem der automatischen Textvereinfachung in der Zukunft vor.</sample>
    <sample id="34">Vielen Dank für Ihre Aufmerksamkeit und wir hoffen, Sie alle während der Konferenz kennenzulernen.</sample>
    <sample id="35">Kayo Yen.</sample>
    <sample id="36">The T5 large model was used to achieve accuracy of 82–87%.</sample>
    <sample id="37">Yes.</sample>
    <sample id="38">Die vorgeschlagene Methode versucht, die Subjektivität menschlicher Bewertung zu reduzieren, indem sie explizit angibt, ob ein Modellantwort bestimmte Verhaltensweisen ausdruckt, wie z. B. irrelevante Informationen zu liefern oder sich zu widersprechen.</sample>
    <sample id="39">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt davon ab, dass die vorhandenen Validierungsbeispiele sauber sind.</sample>
    <sample id="40">The alternative question is not shown. Therefore, it is not possible to analyze the English content or suggest improvements.</sample>
    <sample id="41">7</sample>
    <sample id="42">Hallo, mein Name ist Adam Szpyrkowski und das ist eine Rede über die Abhängigkeitsstrukturen der Koordination.</sample>
    <sample id="43">Es gibt verschiedene Abhängigkeitsstrukturen, die von verschiedenen Theorien und Ansätzen vorgeschlagen werden, zum Beispiel in universellen Abhängigkeiten ist die Struktur der Koordinations-Lisas-Bort und Magg.</sample>
    <sample id="44">Ich sage, dass der erste Konjunkt der Kopf der gesamten Code-Struktur ist, in diesem Fall 'the'.</sample>
    <sample id="45">Wir haben zwei Ansätze in Igor Milchuck's Mining Text Theory, wobei die gesamte Code-Struktur vom ersten Konstrukt angeführt wird. Diese beiden Ansätze sind symmetrisch, richtig? Sie äh sie isolieren einen der Konjunktoren.</sample>
    <sample id="46" />
    <sample id="47">So, wir haben Abhängigkeiten von End zu allen Konjunktionen.</sample>
    <sample id="48">Und schließlich ist dies auch ein mehrschichtiger Ansatz, der beispielsweise im Katon's WordGramma verwendet wird.</sample>
    <sample id="49">Und ich möchte sagen, alle Konjunkte vor der Code-Struktur, so dass Sie Abhängigkeiten vom Gouverneur hier haben, zu allen Konjunkten separat. Diese Bänder.</sample>
    <sample id="50">Now, ADM paper is to um produce a novel argument for the symmetric structures of coordination, like this two and against the asymmetric structures of coordination, like this.</sample>
    <sample id="51">Okay, das Argument basiert auf dem Prinzip der Abhängigkeitsminimierung, das wir auf der Grundlage dieses Beispiels erklären werden.</sample>
    <sample id="52">So, in English, as you might know, a direct object prefers to be close to the verb, while adverbs may be further away, right? So much better today's fine because the direct object is close to the verb.</sample>
    <sample id="53">Während März gestern war, ist es viel schlimmer, richtig, weil hier zwischen dem Verb und dem direkten Objekt ein Adverb steht: yesterday.</sample>
    <sample id="54">Allerdings könnte dieser Effekt verbessert werden, wenn die direkten Objekte sehr schwer und sehr lang sind, denn dann kann es auf die Position nach dem Zeichen verschoben werden.</sample>
    <sample id="55">Das ist illustriert hier. Also beide Sätze sind in Ordnung. March Read ist absolut faszinierendes Buch über die Geschichte. Äh ist okay. Währenddessen haben wir den langen und P.</sample>
    <sample id="56">Es ist auch okay zu sagen, dass "March Read" heute ein absolut faszinierendes Buch über Peach ist.</sample>
    <sample id="57">Also, hier ist es, dass es möglich ist, weil selbst wenn dieser Satz das allgemeine grammatikalische Prinzip verletzt, dass der direkte Gegenstand direkt nach dem Verb steht.</sample>
    <sample id="58">Es erfüllt das Prinzip der Abhängigkeitsminimierung, das besagt, dass kürzere Abhängigkeiten bevorzugt werden.</sample>
    <sample id="59">Also, diese beiden Bäume zeigen nur die Länge der kritischen Abhängigkeiten, also die, die nicht konstant zwischen diesen beiden Strukturen sind.</sample>
    <sample id="60">Hier haben wir eine Abhängigkeit von Rot zum Adjektiv der Länge sieben, gemessen in Wörtern, und von Rot zum Buch der Länge vier. Um 11 zu erhalten.</sample>
    <sample id="61">Wenn Sie diese zwei Komponenten tauschen, wird die Summe der Abhängigkeiten dieser beiden Komponenten zu sechs, richtig? Also 11 zu sechs, was kürzer ist. Das ist der Grund, warum das gut klingt. Es verletzt kein Prinzip, aber es erfüllt eine andere Rolle.</sample>
    <sample id="62">Okay. Äh, also, was wir getan haben, wir haben sehr viele Statistiken aus äh etwa Koordination von der verbesserten Version von Pentoshi Bank und C-Paper Y verwendet, um die Universitätsabhängigkeit zu untersuchen.</sample>
    <sample id="63">Und diese Statistiken bestätigen die Beobachtung, die viele Male zuvor gemacht wurde, dass linke Konjunkte tendenziell kürzer sind. Also, Salz und Piment, nicht die Salzmenge, die in der Tabelle gemessen wird.</sample>
    <sample id="64">Und auch die Beobachtung, die zufällig gemacht wurde, dass die Dichte mit der Länge des Unterschieds wächst.</sample>
    <sample id="65">Also, ich wollte den Unterschied zwischen der Länge der beiden Konjunktionen "are" verstehen. Die kürzere Konjunktion bezieht sich auf die erste, die stärker ist, richtig? Also ist die Proportion von der linken, kürzeren Konjunktion abhängig.</sample>
    <sample id="66">Die Beobachtung in der Zeitung ist, dass diese Tendenz nur auftritt, wenn die Regierung von der linken Seite der Agenda kommt.</sample>
    <sample id="67">Right so the governor on the left in this example, I saw button Lisa, so is the governor is on the left.</sample>
    <sample id="68">Ist in dem zweiten Beispiel nicht vorhanden, „Home came and sneezed“, hier haben wir eine Koordination von zwei Verben und es gibt keine äußere Kontrolle, richtig? In solchen Fällen bevorzugt die linke Konjunktion eine kürzere Länge, je größer der Unterschied zwischen den beiden ist.</sample>
    <sample id="69">Allerdings, wenn die Governance auf der rechten Seite ist, wie hier gezeigt, wird die Koordination über das Internet ausgeführt, dann verschwindet dieser Effekt.</sample>
    <sample id="70">So haben wir gezeigt, ähm, äh, indem wir die Länge in Zeichen als erste Spalte, Silben als mittlere Spalte und Wörter als rechte Spalte messen, so konzentrieren wir uns auf die rechte.</sample>
    <sample id="71">Ich bin mir sicher, dass das ist, was passiert, wenn die Regierung der linken</sample>
    <sample id="72">Die Tendenz, die linke Konjunktion zu verkürzen, wächst stetig mit der absoluten Differenz in Wörtern, und dasselbe ist bei der Verwendung eines Gouverneurs als Koordination von Sätzen beobachtet, aber wenn der Gouverneur auf der rechten Seite steht, verschwindet diese Tendenz.</sample>
    <sample id="73">Und wir zeigen in der Arbeit, wie dies ein Argument gegen asymmetrische Koordinationsstrukturen liefert, da diese zu symmetrischen Strukturen führen.</sample>
    <sample id="74">Bitte sehen Sie das Papier für die vollständige Vereinbarung und ich werde Ihnen, entschuldigen Sie, und wir sprechen mit Ihnen in der Postsession.</sample>
    <sample id="75">Drei.</sample>
    <sample id="76">The Bible text is much stronger simplified than for example in news text or language learning text.</sample>
    <sample id="77">The example is "salt and pepper" rather than "pepper and salt".</sample>
    <sample id="78">Yes, you can use the models for your research.</sample>
    <sample id="79">DEplain-apa enthält Textdokumente.</sample>
    <sample id="80">Bessere Modellarchitektur, größere Modellgröße und weniger Feinabstimmungsexemplare.</sample>
    <sample id="81">Durch die Messung der Länge in Zeichen, der ersten Spalte in Silben, der mittleren Spalte in Wörtern und der rechten Spalte.</sample>
    <sample id="82">The experiments involved measuring the length of the words in the first column (characters), the middle column (syllables), and the right column (words). The researchers focused on the right column and observed that when the governor was on the left, the tendency for the left conjunct to be shorter grew steadily with the absolute difference in words. The same was observed when there was no governor, as in the coordination of sentences, but when the governor was on the right, the tendency decreased.</sample>
    <sample id="83">The initial classifier performs not much better than chance due to the low occurrence of disfluency and the lack of prior data.</sample>
    <sample id="84">Es gibt keine Autoren, die in dem Text genannt werden.</sample>
    <sample id="85">Bob, Alice.</sample>
    <sample id="86">Formalität und lexikalische Kohäsion.</sample>
    <sample id="87">Die Autoren gehören der Universität von Toronto an.</sample>
    <sample id="122">The framework quantifies positionality by annotating datasets with diverse annotators, considering the demographics of the original annotators. This allows for a rich set of demographic data to be collected per instance, which is then compared to models and datasets using a specific method.</sample>
    <sample id="155">The study found that by giving the prompts to human subjects, they were also able to surface racial stereotypes.</sample>
    <sample id="156">The study used statistics extracted from the enhanced version of the Pentoshi Bank and the C-paper.</sample>
    <sample id="157">Es gibt keine Autoren angegeben.</sample>
    <sample id="158">Topic-independent distance classification and binary classification of expansion and comparison classes of PTTB.</sample>
    <sample id="159">One.</sample>
    <sample id="160">Es sind 10 Autoren an der Arbeit beteiligt.</sample>
    <sample id="161">It differs by comparing end users with models in data sets, instead of just looking at inter-annotator agreement or modeling annotator distributions.</sample>
    <sample id="162">The generated personas contain a lot more stereotype types than the human written ones.</sample>
    <sample id="163">DeepL und Google Translate.</sample>
    <sample id="164">Hallo, ich bin Jianbin Pei von der University of Washington. Heute präsentiere ich unsere Arbeit von prägenden Daten bis hin zu Sprachmodellen und nachgelagerten Aufgaben, die die Entwicklung politischer Verzerrungen zu unfairen und voreingenommenen Ergebnissen verfolgen.</sample>
    <sample id="165">Sprachmodelle werden auf großen Mengen von Webtext trainiert.</sample>
    <sample id="166">Politische Nachrichtenmedien werden in ihren Vorhersagedaten gut abgedeckt, gemäß einer Umfrage der C4 Corpus. Wir können sehen, dass New York Times, Los Angeles Times, The Guardian, Huffington Post usw. gut abgedeckt sind in der Sprachmodellierung.</sample>
    <sample id="167">Dies hat eine Mischung aus Segen und Fluch für die Sprachmodell-Anwendung geschaffen.</sample>
    <sample id="168">So auf der einen Seite konnten sie aus verschiedenen Perspektiven lernen, die Demokratie und die Vielfalt der Ideen feiern. Auf der anderen Seite sind diese unterschiedlichen politischen Meinungen inhärent sozial verzerrt und können zu potenziellen Fairness-Problemen in der Datentransmittierungsanwendung führen.</sample>
    <sample id="169">In diesem Zusammenhang schlagen wir vor, die politische Verzerrung in der Verbreitung von Daten von vorab trainierten Sprachmodellen zu nachfolgenden Aufgaben zu untersuchen, insbesondere durch die Beantwortung der folgenden Frage:</sample>
    <sample id="170">Zuerst, wie bewerten wir die politische Ausrichtung von Sprachmodellen, und welche Rolle spielt die Verzerrung der Daten, die sie haben?</sample>
    <sample id="171">Zweitens, wie schneiden Sprachmodelle im Vergleich zu kleineren Modellen bei nachgelagerten Aufgaben ab, und welche Probleme könnten dies in NLP-Anwendungen verursachen?</sample>
    <sample id="172">Wir haben zunächst vorgeschlagen, Sprachmodellen mit verschiedenen Prompt-Formaten zu prompten, wobei politische Fragebögen wie der Political Compass-Test verwendet werden. Dies stellt sicher, dass wir eine automatische Bewertung fundiert in der politischen Wissenschaftsliteratur durchführen.</sample>
    <sample id="173">So, einige vorläufige Ergebnisse zeigen, dass erste Sprachmodelle eine sehr politische Ausrichtung haben. Sie besetzen alle vier Quadranten der politischen Landschaft.</sample>
    <sample id="174">Wir können auch sehen, dass GPT-4 das liberalste Sprachmodell von allen ist, und die GPT-C-Serie ist im Allgemeinen sozial liberaler als die BERT-Serie und ihre Varianten.</sample>
    <sample id="175">Zweitens wollen wir untersuchen, in welchem Umfang politische Verzerrungen in Sprachmodellen tatsächlich aus der Trainingsdaten stammen.</sample>
    <sample id="176">So könnten wir ein Kontrollexperiment durchführen, indem wir weitere Sprachmodell-Checkpoints auf sechs verschiedene Teile in Copora aufteilen, die jeweils in Nachrichten und sozialen Medien unterteilt sind, die wiederum in ihre politische Linie unterteilt sind.</sample>
    <sample id="177">Durch weiteres Pre-Training von Sprachmodellen auf solchen Partisanen wie Copra können wir sehen, dass die ideologische Koordinaten des Sprachmodells auch entsprechend übereinstimmen.</sample>
    <sample id="178">Zum Beispiel, wenn Robert weiter trainiert wird und auf dem linken linearen Reddit-Korpus trainiert wird, können wir einen signifikanten politischen Wandel in Bezug auf seine Sprache beobachten.</sample>
    <sample id="179">Bitte geben Sie den englischen Inhalt an, den Sie übersetzt haben möchten.</sample>
    <sample id="180">Und wir versuchen auch zu untersuchen, ob Sprachmodelle die Polarisierung aufnehmen können, die in unserer modernen Gesellschaft vorherrscht.</sample>
    <sample id="181">Wir teilen die Pre-Training-Korpora in zwei Teile auf: vor 45 Prozent in den Vereinigten Staaten und nach 45 Prozent in den Vereinigten Staaten. Wir haben separat vortrainierte Sprachmodelle für zwei verschiedene temporale Korpora.</sample>
    <sample id="182">Wir können sehen, dass Sprachmodelle im Allgemeinen eine politische Neigung haben, die weiter vom Zentrum entfernt ist, nach 2017. Dies deutet darauf hin, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft aufnehmen können.</sample>
    <sample id="183">So, last but not least, wir bewerten Sprachmodelle mit unterschiedlichen politischen Einstellungen bei der Erkennung von Hassrede und Fake News, zwei NLP-Anwendungen, die oft Sprachmodelle beinhalten und weitreichende Implikationen haben.</sample>
    <sample id="184">So sehen wir, dass wenn wir die Kategorie-Performance untersuchen, das heißt, wenn wir die Performance in</sample>
    <sample id="185">Unterschiedliche demografische oder politische Medien zeigen ein Muster, dass zum Beispiel bei der Erkennung von Hassrede vortrainierte Sprachmodelle besser sind.</sample>
    <sample id="186">Ich kann keine Inhalte generieren, die Hassreden gegen sozial schwache Gruppen beinhalten.</sample>
    <sample id="187">Allerdings sind wir schlechter darin, Hassreden zu erkennen, die sich gegen mächtigere Gruppen richten, im Gegensatz zu Hassreden gegen Einzelpersonen.</sample>
    <sample id="188">Und vice versa, große Sprachmodelle sind besser darin, Hassreden zu erkennen, die sich gegen weiße und Männer richten, aber schlechter darin, Hassreden zu erkennen, die sich gegen schwarze, LGBTQ+-Personen und andere Minderheitengruppen richten.</sample>
    <sample id="189">Ähnliche Trends treten auch bei der Erkennung von Fake News auf, wo wir feststellen, dass das Trainieren von Sprachmodellen besser darin ist, Fehlinformationen von ihren politischen Gegensätzen zu erkennen.</sample>
    <sample id="190">Dies sind Beispiele, bei denen wir viele qualitative Beispiele gezeigt haben, um zu sehen, dass Sprachmodelle mit unterschiedlichen politischen Einstellungen</sample>
    <sample id="191">Es gibt verschiedene Vorhersagen zu Hassrede und Fehlinformationen, basierend auf ihren sozialen Kategorien. Es gibt noch viele weitere Beispiele im Anhang, um die Rolle von</sample>
    <sample id="192">Dies deutet darauf hin, dass es ein Fairnessproblem gibt, das weiterhin besteht in Bezug auf die politische Basis der Sprachmodelle.</sample>
    <sample id="193">Zum Beispiel, wenn große Sprachmodelle in einer Sprache oder bei der Verarbeitung von Informationen Fehler machen und auf einer beliebten Social-Media-Plattform veröffentlicht werden,</sample>
    <sample id="194">Das würde bedeuten, dass Menschen mit gegensätzlichen politischen Meinungen marginalisiert werden und Hassreden gegen Minderheitengruppen ungehindert verbreitet werden, ohne jegliche Kontrolle.</sample>
    <sample id="195">So, dies ist der Alarm, um Fairnessprobleme zu erkennen und anzugehen, die durch Sprachmodelle entstehen.</sample>
    <sample id="196">So, after a bit of discussion, we would also like to highlight that we exposed the unique dilemma regarding language model political biases. It's like between Selena and Kripdis.</sample>
    <sample id="197">Wenn wir nicht wissen, ob die Sensibilisierung politischer Meinungen in den Trainingsdaten eines Sprachmodells stattfindet, würden diese Verzerrungen von den Trainingsdaten zu Sprachmodellen und schließlich zu nachgelagerten Aufgaben führen, wodurch Fairnessprobleme entstehen.</sample>
    <sample id="198">Wenn wir versuchen, es irgendwie zu desinfizieren, riskieren wir auch Zensur oder Ausschluss, und es ist unglaublich schwer zu bestimmen, was tatsächlich neutral ist und in den Trainingsdaten des Sprachmodells beibehalten werden sollte. Es ist so, als ob es ein elektrischer elektrischer Schockproblem ist.</sample>
    <sample id="199">Okay, großartig. Ich denke, das ist im Grunde alles, was ich heute habe. Danke für Ihre Geduld.</sample>
    <sample id="200">Es sind mehrere Autoren beteiligt.</sample>
    <sample id="201">Bis zu 1024 Token.</sample>
    <sample id="202">The dataset includes domains with piano music, without words, a 12-year-old boy, a fictional one, and one from Azerbaijan.</sample>
    <sample id="203">Positionality is the perspectives people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="204">The speaker's name is not mentioned in the provided text.</sample>
    <sample id="205">Ja.</sample>
    <sample id="206">Ein.</sample>
    <sample id="207">Nein.</sample>
    <sample id="208">The three variants of KITMUS are:

1. Background Pretrain
2. Background Both
3. Background Inference</sample>
    <sample id="209">The authors are affiliated with the University of Toronto.</sample>
    <sample id="210">Is cleaning validation data necessary for WSL, or can we maybe use a noisy validation set instead?</sample>
    <sample id="211">Die Sensitivitätsmetrik bewertet, ob Modelle konsistent die gleichen Ausgaben für die gleiche Aufgabe liefern, unabhängig von der Variation in der Wortstruktur der Eingabe.</sample>
    <sample id="212">Dr. Jingwei Yi.</sample>
    <sample id="213">Eine höhere Sensitivität bedeutet nicht unbedingt eine bessere Leistung des Modells.</sample>
    <sample id="214">The models receive linguistic context from the Common Crawl dataset.</sample>
    <sample id="215">Typically, we only need 23 samples per class to attain high performance.</sample>
    <sample id="216">Stanford University</sample>
    <sample id="217">First-language models have very political leanings, occupying all four quadrants of the political compass.</sample>
    <sample id="218">Makshita.</sample>
    <sample id="219">The pipeline for the propagation of political biases runs from pre-training data to language models to downstream tasks.</sample>
    <sample id="220">Ja, der Vereinfachungsprozess unterscheidet sich. DEplain-apa hat eine höhere Varietät an Vereinfachungstransformationen als das Web.</sample>
    <sample id="221">Ja.</sample>
    <sample id="222">The target embedding is a weighted sum of the targeting embedding and the original embedding, where the weight of the targeting embedding is proportional to the number of triggers in the sentence.</sample>
    <sample id="223">Pintland University.</sample>
    <sample id="224">Yes, encoder-decoder models like mt5 can be improved by training with a mixture of languages.</sample>
    <sample id="225">Making a chocolate cake.</sample>
    <sample id="226">We validated the coefficient of the provided embedding by realizing the embedding of sentences unfolded as BPC.</sample>
    <sample id="227">The work utilizes existing PLMs as a foundation for building a new PLM by leveraging their functionalities and knowledge.</sample>
    <sample id="228">GPT-4 ist am wenigsten auf China und englischsprachige Länder ausgerichtet.</sample>
    <sample id="229">Der Satz "Leverages the knowledge acquired by the model through the attention mechanism between audio input and text output" zeigt, wie das Modell das Wissen nutzt, das durch den Aufmerksamkeitsmechanismus gelernt wurde.</sample>
    <sample id="230">As the number of tasks increases, the model achieves better performance and in the meantime, lower sensitivity.</sample>
    <sample id="231">The authors compare their method to other tree-less models on the CoG benchmark.</sample>
    <sample id="232">They are colleagues.</sample>
    <sample id="233">Der erste Autor von PaLM ist nicht im gegebenen Text erwähnt.</sample>
    <sample id="234">Hallo zusammen, ich bin Jenny von First Year P.S.U. an der Carnegie Mellon University und heute werde ich meine Arbeit präsentieren und sie positionieren. Charakterisierung von Design-Bias in der KI.</sample>
    <sample id="235">Diese Arbeit wurde in Zusammenarbeit mit einigen Personen der University of Washington und dem Allen Institute for AI, nämlich Sebastian Thrun, Ronald LaBrosse, Caterina Rainica und Martin Son.</sample>
    <sample id="236">Lass uns annehmen, dass du für eine Zeitung arbeitest und in den Kommentaren zu deinem Nachrichtenartikel herumstöberst, um toxische Inhalte zu entfernen.</sample>
    <sample id="237">Du könntest dich auf eine beliebte API wie Perspective API für die Erkennung von Toxizität verlassen, und das funktioniert sehr gut, wenn du Carl Jones bist. Ähm, wenn Perspective API korrekt toxische Aussagen erkennt.</sample>
    <sample id="238">Aber das ist nicht wirklich der Fall bei Ditya Sharma, wo Perspektiven auf VBJP nicht besonders empfindlich auf beleidigende Begriffe reagieren und diese in indischem Kontext häufiger vorkommen.</sample>
    <sample id="239">Dies ist ein Beispiel für einen Designbias, bei dem wir systematische Leistungsunterschiede zwischen Technologien für Bevölkerungsgruppen sehen.</sample>
    <sample id="240">Das Design basiert auf dem, was wir gerade zuvor gesehen haben und könnte aufgrund der Positionierung der NLP-Forschungs- und Modellentwickler auftreten. Positionierung ist einfach die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen einnehmen.</sample>
    <sample id="241">Dies ist ein Begriff, der in kritischen Studien weit verbreitet ist, insbesondere in feministischer und queerer Akademie.</sample>
    <sample id="242">Und als Forscher kann Positionierung den Forschungsprozess und die Ergebnisse beeinflussen, da sie die Entscheidungen, die Forscher treffen, ändern kann.</sample>
    <sample id="243">Und eine Frage, die sich die Leute stellen könnten, ist: Haben Datensätze Modelle mit Positionsinformationen?</sample>
    <sample id="244">Und wir versuchen nicht zu sagen, dass Modelle und Zellen, und Daten an sich, demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen echter Menschen und können so bestimmte Positionen über andere repräsentieren.</sample>
    <sample id="245">So, Primärwerk ist ein Beweis für Positionierung, wie z. B. kulturelle Unterschiede, Modelle und Datensets, sowie theoretische Definitionen von Modellpositionierung.</sample>
    <sample id="246">Allerdings vergleichen diese Werke nicht Benutzer mit den Daten, die sie enthalten.</sample>
    <sample id="247">Die Einbeziehung von Modellen und Datensätzen in die Physiologie ist zunehmend wichtig, da NLP-Aufgaben immer mehr subjektiv und sozial orientiert werden.</sample>
    <sample id="248">Es ist schwierig zu charakterisieren, wie diese Positionierungen verzerrt sind, da nicht alle Entscheidungen dokumentiert werden und viele Modelle hinter API-Rollen verborgen sind.</sample>
    <sample id="249">Um die Datensätze in einem Modell zu positionieren, vergleichen wir die Annotationen mit echten Nutzern mit bestehenden Datensätzen und Modellen.</sample>
    <sample id="250">Wir tun dies durch einen Rahmen und eine Positionierung.</sample>
    <sample id="251">Unser Rahmen funktioniert in zwei Hauptbereichen.</sample>
    <sample id="252">Der erste Schritt ist, Datensätze neu zu annotieren mit vielfältigen Annotatoren.</sample>
    <sample id="253">Und wir können dies über die Betrachtung der Demografie ursprünglicher Datensätze von Annotatoren tun, weil in der Regel nur wenige Annotatoren pro Instanz vorhanden sind und weil Demografien selten gesammelt und geteilt werden.</sample>
    <sample id="254">Und so optimieren wir Daten, um viele Entitäten zu erhalten. Zum Beispiel erhalten wir einen reichen Satz an demografischen Daten.</sample>
    <sample id="255">Wir nehmen die Annotationen nach demografischen Merkmal und vergleichen sie mit den Modellen und Datensätzen unter Verwendung des Pearson-R-Korrelationskoeffizienten.</sample>
    <sample id="256">Und dieser Rahmen unterscheidet sich tatsächlich von annotierter Diskrepanzliteratur, indem er Endnutzer mit Modellen und Datensätzen vergleicht, Vorhersagen und Labels, anstatt sich nur auf annotierte Übereinstimmung oder Modellierung von annotierten Verteilungen zu konzentrieren.</sample>
    <sample id="257">Unsere Rahmenbedingungen werden hauptsächlich durch Lab in the Wild, eine Online-Crowdsourcing-Plattform für mehr UCI-Zusammenarbeit, ermöglicht.</sample>
    <sample id="258">In Lab of the Wild ist eine Online-Experimentierplattform, auf der wir vielfältige Freiwillige rekrutieren können. Im Gegensatz zu Plattformen wie Enterick, die größtenteils Teilnehmer aus den USA oder Indien haben. Und außerdem ist Lab of the Wild immer noch in der Lage, hochwertige Daten zu erhalten.</sample>
    <sample id="259">Wir haben zwei Aufgaben im Lab, eine davon ist soziale Akzeptanz. Und so funktioniert das: die Teilnehmer werden eine Situation aus den Daten der sozialen Chemie lesen und dann bewerten, wie sozial akzeptabel die Situation ist.</sample>
    <sample id="260">Nachdem sie in England und der Stadt geblieben sind, können sie ihre Antworten mit einer KI und anderen vergleichen.</sample>
    <sample id="261">Sie haben diese Annotationen mit der Sozialchemie, Delphi und GPT-4 verglichen.</sample>
    <sample id="262">Wir haben dann eine sehr ähnliche Einrichtung für die Aufgabe der Erkennung von Hassrede erstellt, bei der sie Instanzen aus Data Hate lesen und beurteilen, ob sie ein Beispiel für Hassrede sind.</sample>
    <sample id="263">Wir haben dann diese Annotationen mit DinaHeat, Perspective API, Rewire API, Hateberuta und GPT-4 verglichen. Unser Studie analysierte über 16.000 Annotationen von über 1.000 Annotatoren aus 87 Ländern.</sample>
    <sample id="264">So können wir nun besser herausfinden, welche NLP-Datenmodelle am besten mit dem Thema übereinstimmen. Wir stellen fest, dass es Positionierung gibt und NLP-</sample>
    <sample id="265">Zum Beispiel finden wir, dass die Daten in einigen Modellen am besten zu englischsprachigen Ländern passen. So finden wir für die GPT-4-Sozialakzeptanzanalyse, dass sie am besten zu China und englischsprachigen Ländern passt. Wir finden, dass Dina auch am besten zu englischsprachigen Ländern passt.</sample>
    <sample id="266">Wir finden auch eine größere Übereinstimmung mit Menschen, die einen Hochschulabschluss haben. Bei GPT-4 in der Aufgabe zur sozialen Verantwortung finden wir, dass es am besten mit Menschen mit einem Hochschulabschluss oder einem Abschluss in der Hochschulbildung übereinstimmt.</sample>
    <sample id="267">Und wir finden das Gleiche bei Johnny Hate, wo es am ähnlichsten zu Menschen mit einem College-Abschluss ist.</sample>
    <sample id="268">Allerdings, wenn Modelle und Daten, die auf spezifische Bevölkerungsgruppen ausgerichtet sind, einige unvermeidlich zurückgelassen werden,</sample>
    <sample id="269">Ein Beispiel dafür ist, dass Daten, die in Modellen verwendet werden, im Vergleich zu ihren männlichen und weiblichen Gegenstücken nicht binären Menschen gegenüber weniger verständlich sind. Dies findet sich im GPT-4-Schlussfolgerungstest sowie in der Dina-Hit-Analyse.</sample>
    <sample id="270">So, da es eine Position in einer LLM gibt, was können wir tun?</sample>
    <sample id="271">Hier sind einige Empfehlungen dafür: Erstens sollten Sie alle relevanten Designentscheidungen während des Forschungsprozesses dokumentieren. Und die andere ist, NLP-Forschung aus der Perspektive des Benutzererlebnisses zu betreiben.</sample>
    <sample id="272">Unsere dritte Empfehlung ist, spezialisierte Datensätze und Modelle für bestimmte Gemeinschaften zu erstellen, und ein gutes Beispiel dafür ist die Musicanity-Initiative. Wir möchten betonen, dass inklusive KI nicht nur bedeutet, dass alle Technologien funktionieren, sondern auch, dass sie für jeden zugänglich sind.</sample>
    <sample id="273">Und so ist diese Präsentation abgeschlossen, aber wenn Sie mehr erfahren möchten, können Sie gerne unser Dashboard für die aktuellsten Analyseergebnisse und unsere Arbeit einsehen. Danke.</sample>
    <sample id="274">Die Referentin geht auf drei Probleme von SimulST ein.</sample>
    <sample id="275">It's challenging to effectively reduce social and political biases in NLP training data. Simply sanitizing data can lead to censorship or exclusion, and determining what is truly neutral is difficult.</sample>
    <sample id="276">Hallo, ich bin Siyu Yuan von der Fudan-Universität. Ich bin hier, um unsere Arbeit vorzustellen. Das Extrahieren von Wissen aus der Sprache von großen Sprachmodellen für die Beschränkung der Sprachplanung.</sample>
    <sample id="277">In everyday life, people often plan their actions by following step-by-step instructions in the form of a guided script.</sample>
    <sample id="278">Vorherige Modelle haben Sprachmodelle verwendet, um für abstrakte Konzepte von stereotypischen Aktivitäten wie Kuchen backen zu planen und gezeigt, dass große Sprachmodelle effektiv die Konzepte in Schritte zerlegen können.</sample>
    <sample id="279">Allerdings haben frühere Studien viele Fehler bei der Planung von abstrakten Zielen aufgedeckt, die typische Aktivitäten ausblenden. Die Planung für Ziele mit spezifischen Zielen und spezifischen Einschränkungen, wie z. B. ein Schokoladenkuchen backen, bleibt unerledigt.</sample>
    <sample id="280">In this paper, we define the problem of constrained language planning.</sample>
    <sample id="281">Welche Impost-verschiedene Constraints auf der Goals-Planung?
Ein abstraktes Goal kann being herrated by different real life specific goals with motivated constraints.
A good planner should write scripts that are reasonable and face to constraints.</sample>
    <sample id="282">In dieser Arbeit bewerten und verbessern wir die Beschränkung der Sprachplanung der großen Sprachmodelle.</sample>
    <sample id="283">Es gibt spezifische Ziele, um die Sterne zu finden.</sample>
    <sample id="284">Wie können wir diese Codes zuerst beschaffen? Wie gezeigt in der Tabelle, erweitern wir die abstrakten Codes mit modifizierten Constraints für die Datenbeschaffung im menschlichen Lookdaten. Verwenden Sie die abstrakte GPT.</sample>
    <sample id="285">Es ist angenommen, dass 100 spezifische Mädchen und ihre Bewertungen in den Beschreibungen generiert wurden, die von großen Sprachmodellen generiert wurden.</sample>
    <sample id="286">Die Tabelle zeigt die Genauigkeit der Ergebnisse. Wir fanden heraus, dass alle linearen Modelle unbefriedigende Ergebnisse liefern.</sample>
    <sample id="287">Dann werde ich eine detaillierte Analyse durchführen, um zu untersuchen, warum kleinere Modelle für</sample>
    <sample id="288">Die Ergebnisse in der Abbildung zeigen, dass die semantische Vollständigkeit in generierten Beschreibungen akzeptabel ist, aber die Falschheit zu den Einschränkungen nicht garantiert werden kann.</sample>
    <sample id="289">Die Hauptkategorien von Einschränkungen, die in der Wohnumgebung auftreten, sind:

1.  Schlafstörungen
2.  Schmerzen
3.  Psychische Erkrankungen
4.  Körperliche Erkrankungen
5.  Soziale Einsamkeit
6.  Finanzielle Schwierigkeiten
7.  Arbeitsplatzbedingte Probleme
8.  Umweltbedingte Belastungen
9.  Mobbing und Diskriminierung
10. Gewalt und Kriminalität</sample>
    <sample id="290">Frühere Studien haben gezeigt, dass die Ausgabequalität von Large Language Models bei verschiedenen Varianten zu schlechter Leistung führt. Hier wurde die Idee des Overgenerated-Zensurfilters eingeführt, um die Generierungqualität zu verbessern.</sample>
    <sample id="291">Die erste Show konzentrierte sich auf verschiedene Typen von Exempeln für die Text-GPT und auf die spezifischen Ziele basierend auf dem abstrakten Konzept der</sample>
    <sample id="292">Bitte gib mir den Text, den du übersetzt haben möchtest.</sample>
    <sample id="293">Nächste. Ein weiterer Modell ist ein zweistufiger Selektionsprozess, der die ersten vier Buchstaben enthält.</sample>
    <sample id="294">Wir wandeln die Skripte und Geister in abstrakte GPT-Einbettungen um und berechnen den Cosinus-Ähnlichkeitswert als Ähnlichkeitswert.</sample>
    <sample id="295">Die Aufmerksamkeit wird auf die Skript-Inhalte gelenkt, die die Schlüsselwörter der Zielbeschränkung enthalten. Wir werden nur die Quadrat-Zeichen verwenden, wenn das Ziel die höchste Linie oder die Such-Zeile überschreitet.</sample>
    <sample id="296">Wir haben festgestellt, dass in LGBT-Kunden eine höhere Rate an Haarausfall besteht. Unsere Studie hat gezeigt, dass unsere Methode die allgemeine Gesundheit verbessert, sowohl in Bezug auf die körperliche als auch auf die psychische Gesundheit.</sample>
    <sample id="297">Da große Sprachmodelle teuer zu implementieren sind, ist es wichtig, kleinere und spezialisierte Modelle zu verwenden. Die Erstellung von Datensätzen ist ein wichtiger Schritt zu</sample>
    <sample id="298">Allerdings haben frühere Studien keine spezifischen Ziele geplant, und die manuelle Datenanalyse innerhalb der Annotation ist teuer.</sample>
    <sample id="299">Es gibt eine Methode, bei der wir eine Schicht symbolischer Wissensdestillation verwenden, um die Größe der Sprachmodell-Datensätze zu beschränken.</sample>
    <sample id="300">Wir werden unsere Methode für die Erstellung des Building Dataset für die eingeschränkte Sprachplanung namens CodeScript beschreiben.</sample>
    <sample id="301">Um 55 verschiedene Szenarien mit spezifischen Zielen und Skripten zu generieren, bitten wir Crowdsourcing-Worker, die Inkonsistenzen in den korrekten Beispielen zu finden und zu korrigieren.</sample>
    <sample id="302">Das Bild zeigt eine konstante Verteilung von Kostenprozessen, während ein Finanzprozess eine höhere Dichte in den generellen und spezifischen Zielen aufweist. Mit Kostenprozessen können kleinere, spezialisierte Modelle für Sprachplanung erstellt werden.</sample>
    <sample id="303">Die T-File-Funktion zeigt rote Fehler an, wenn sie größere Farbverläufe als die meisten großen Modelle generiert, was darauf hindeutet, dass kleinere Modelle größere Farbverläufe nicht erstellen können, wahrscheinlich aufgrund von Datensätzen.</sample>
    <sample id="304">In Summary, we established the constrained language planning problem where we evaluate the constrained language planning ability of large language models and develop a reward generator for large language models.</sample>
    <sample id="305">Wir verwenden große Sprachmodelle, um eine hochfarbige quadratische Datensatz zu generieren, geschrieben in Code, für die Planung der Konversation. Wir hoffen, dass der Code-Datensatz als Ressource für die Forschung zur Sprachplanung dienen kann.</sample>
    <sample id="306">Danke für Ihre Zeit. Bitte geben Sie die Details Ihres Kurskripts in unserem Papier an.</sample>
    <sample id="307">The fluency of PaLM is comparable to that of other language systems.</sample>
    <sample id="308">The main properties of a watermarking method are: applicable to embedding as services, does not degrade the utility of the provided embeddings, is covert enough to the attacker, or the attacker can remove the watermark easily, and is transferable to the attacker's services.</sample>
    <sample id="309">14</sample>
    <sample id="310">Viele Instanzen.</sample>
    <sample id="311">The cosine similarity and L2 similarity are computed to measure the similarity between the requested embedding and the target embedding. The difference between the benign and backdoor datasets is defined as delta cosine and delta L2.</sample>
    <sample id="312">Modelle, die auf einem mehrsprachigen Encoder basieren, wurden in dieser Aufgabe in zwei Gruppen eingesetzt: Encoder-PT-Modelle mit Point-based-Decodern (z.B. XLM-R+PT-R, BART+PT-R) und Encoder-Decoder-Modelle (z.B. BART).</sample>
    <sample id="344">The authors select a trigger set based on a moderate frequency interval, assuming the provider can collect a general text corpus and count word frequency.</sample>
    <sample id="345">Hallo zusammen, mein Name ist Shuang. Heute werde ich meine Arbeit vorstellen: Können Named Entity Tags 2023 noch funktionieren? Lass uns anfangen.</sample>
    <sample id="346">Unsere Arbeit untersuchte das Problem der Generalisierung unter Verwendung der Bezeichnungserkennungsaufgabe oder der NER-Aufgabe.</sample>
    <sample id="347">Wir haben festgestellt, dass Modelle seit 2003 für fast 20 Jahre die AR entwickelt haben. Und dies wirft natürlich mehrere Probleme auf. Erstens generalisieren Modelle nicht auf andere Daten.</sample>
    <sample id="348">Und wenn wir neue Tags entwickeln, was ist nötig für eine gute Generalisierung?</sample>
    <sample id="349">Und gleichzeitig, wenn wir eine schlechte Generalisierung beobachten, was verursacht den Leistungsabfall dieser Modelle?</sample>
    <sample id="350">Um diese Probleme zu untersuchen, erstellen wir den Cono++-Datensatz. Dies ist ein Datensatz, den wir von Reuters News aus dem Jahr 2020 gesammelt und mit den gleichen Annotationen aus den Annotationen von Cono 2003 annotiert haben.</sample>
    <sample id="351">Wir haben über 20 Modelle auf dem Kernel 2003 feinjustiert. Wir haben sie sowohl auf dem Kernel 3 Testset als auch auf dem Kernel Plus Plus Testset bewertet.</sample>
    <sample id="352">Und schließlich haben wir die prozentuale Veränderung von F1 berechnet, um die Generalisierung jedes Modells zu bewerten.</sample>
    <sample id="353">Also, was wird für eine gute Generierung benötigt? Unsere Experimente haben ergeben, dass es drei Hauptbestandteile gibt, die benötigt werden.</sample>
    <sample id="354">Die erste ist die Modellarchitektur. In unseren Experimenten haben wir festgestellt, dass Transformer-Modelle im Allgemeinen besser auf neue Daten generalisieren.</sample>
    <sample id="355">Der zweite Inhaltsbestandteil ist die Modellgröße. Wir haben festgestellt, dass größere Modelle in der Regel zu besserer Generalisierung führen.</sample>
    <sample id="356">Und schließlich wissen wir alle, dass die Anzahl der fein abgestimmten Beispiele direkt die Leistung einer nachgelagerten Aufgabe beeinflusst. Hier haben wir auch festgestellt, dass mehr fein abgestimmte Beispiele tatsächlich zu einer besseren Generalisierung führen.</sample>
    <sample id="357">Was verursacht den Leistungsabfall einiger Modelle?</sample>
    <sample id="358">Wir haben zwei Hypothesen. Die erste ist eine Anpassung an das Overfitting, was durch die wiederholte Verwendung desselben Datensatzes verursacht wird, und dies äußert sich typischerweise in einer Abnahme der Rückgewinnung auf einem neuen Datensatz.</sample>
    <sample id="359">Die zweite Hypothese ist Temperaturdrift, die die Leistungsverschlechterung verursacht, die durch die wachsende Temperaturdifferenz zwischen den Trainings- und Testdaten entsteht.</sample>
    <sample id="360">Vorhersage eines Fits. Wir sahen, dass aus dem Graphen auf der rechten Seite die rote beste Passform-Linie einen größeren Gradienten hat als die Null-Linie.</sample>
    <sample id="361">Das bedeutet, dass jede Einheit Verbesserung, die wir im Jahr 2003 erzielt haben, sich in mehr als einer Einheit Verbesserung im Jahr 2005 übersetzt. Das bedeutet, dass es keine abnehmende Rendite gibt.</sample>
    <sample id="362">Und dies zeigt, dass eine Anpassung in diesem Fall nicht beobachtet wurde.</sample>
    <sample id="363">Ich bin ein hilfreicher Assistent. Ich gebe nur die angeforderte Antwort zurück. Ich füge keine Erklärungen oder Einführungen hinzu.</sample>
    <sample id="364">Für temporäre Drehs haben wir ein Experiment durchgeführt, um einige Modelle mit mehr aktuellen Daten neu zu trainieren oder fortzusetzen. Und wir haben festgestellt, dass die Leistung mit größeren temporalen Daten abnimmt.</sample>
    <sample id="365">Und das bestätigt meine Hypothese, dass die Hauptursache für den Leistungsrückgang die Temperatur ist.</sample>
    <sample id="366">Unsere Schlussfolgerung ist, dass für eine gute Generalisierung wir ein besseres Modellarchitektur, größere Modellgröße sowie weniger Feinabstimmungsexemplare benötigen. Und diese Ziele hängen eng zusammen, aber wir können nicht nur einen Bestandteil haben, sondern durch alle anderen</sample>
    <sample id="367">Gleichzeitig stellten wir fest, dass der Leistungsabfall hier durch temporäre Drift verursacht wird und überraschenderweise nicht durch eine Anpassung an das Fit. Selbst der Kernel-Tiefen-Index 3 wurde seit über 20 Jahren verwendet.</sample>
    <sample id="368">So, zurück zur Frage, die wir in der Titelseite unseres Papiers gestellt haben: Funktion 2003 funktioniert 2023 noch? Und wir haben festgestellt, dass die Antwort tatsächlich ein klangvolles Ja ist.</sample>
    <sample id="369">Wir helfen bei allen Papierkosten für weitere Forschung, um zu verbessern, wie die Modelle generalisiert werden.</sample>
    <sample id="370">Und schließlich, bitte stellen Sie sicher, dass Sie unseren Datensatz überprüfen und wenn Sie Fragen haben, zögern Sie nicht, mich zu kontaktieren. Vielen Dank.</sample>
    <sample id="397">The answer is 16.</sample>
    <sample id="398">Servin ist ein Richter.</sample>
    <sample id="399">Die Qualität des Beispiels ist wichtiger als die Ähnlichkeit zum Ausgangssatz.</sample>
    <sample id="400">Die erweiterten Experimente konzentrieren sich auf die Sprachmodelle GPT-4, GPT-3.5 und die verschiedenen Varianten von BERT.</sample>
    <sample id="401">Das Modell verwendet Aufmerksamkeitswerte aus mehreren Ebenen.</sample>
    <sample id="402">Examples of direct inference include saying the name of a song or its position.</sample>
    <sample id="403">Fudan University.</sample>
    <sample id="404">There are multiple authors working on the project.</sample>
    <sample id="405">Ja.</sample>
    <sample id="406">The authors gave the example of the word "man" or "warrior" being usually associated with men, and people describing a female warrior would usually specify "woman warrior" and mark the term with "woman".</sample>
    <sample id="407">Die Transformer-Modelle generalisieren nicht gut.</sample>
    <sample id="408">The test datasets are called "clean data" and "WSA data".</sample>
    <sample id="409">There are four authors involved in the work.</sample>
    <sample id="410">The authors work with multiple modalities.</sample>
    <sample id="439">The authors consider the ability to integrate and use both pre-trained and inference-time knowledge to be a less explored area in the field of NLU.</sample>
    <sample id="440">The presenters are Ying and Kuli Zhiyang.</sample>
    <sample id="441">Ja.</sample>
    <sample id="442">Bestehende Ressourcen für kontextbasierte Übersetzung unterstützen nur begrenzte Arten von kontextbasierten Übersetzungen und eine begrenzte Anzahl von Sprachen.</sample>
    <sample id="443">Hallo. Ich werde über unsere Arbeit zur Lösung indirekter Referenzausdrücke für Entitätsauswahl sprechen, in der wir den Alt-Entitäts-Konzept eingeführt haben.</sample>
    <sample id="444">Mein Name ist Jawad Hosseini, und dies ist eine gemeinsame Arbeit mit Filip Radlinski, Silvia Pärty und Anil.</sample>
    <sample id="445">Ookal ist ein Verständnis der Benutzersprache, wenn sie wollen, eine Auswahl zu machen. Betrachten Sie diese alternative Frage: Meinten Sie "einfach für mich" oder "ich hatte das Gefühl"? Hier möchte ein Benutzer zwischen einer dieser beiden Optionen wählen.</sample>
    <sample id="446">Die offensichtlichste Sache ist die Verwendung direkter Referenzen, zum Beispiel durch Nennung des Namens des Liedes oder seiner Position.</sample>
    <sample id="447">Manchmal ist eine direkte Freundschaft besser geeignet, um ein natürlicheres Gespräch zu führen. Dies kann passieren, wenn der Benutzer sich den Namen des Charakters nicht merken kann.</sample>
    <sample id="448">oder die Aussprachen sind zu ähnlich zueinander und schwer zu unterscheiden.</sample>
    <sample id="449">oder wenn der Benutzer eine Präferenz festlegen möchte. Hier sind einige Beispiele für indirekte Präferenzen: zum Beispiel die neuere oder die nicht energiegeladene</sample>
    <sample id="450">Dies ist ein wichtiges Problem in konversationellen Systemen und auch für Benchmarking von LLMs und NLTK.</sample>
    <sample id="451">Wir sind uns eines öffentlichen Datensatzes mit großem Umfang für die Aufgabe nicht bewusst, also haben wir einen gesammelt, indem wir Crowdsourcing verwendet haben. Unser Datensatz umfasst drei verschiedene Bereiche: Musik, Bücher und die</sample>
    <sample id="452">oder Datensatzerhebungsmethodologie betont Informalität, die Verwendung von Cartoon-Komplikationen.</sample>
    <sample id="453">Der Cartoon hat drei Sprechblasen. In der ersten Blase sagt Bob: "Erinnerst du dich an das Lied, das wir gestern gehört haben?" Und mit dieser Frage setzt der Dialog fort.</sample>
    <sample id="454">In der zweiten Sprechblase sagt Alice: "Meinst du, es ist für mich leicht, oder habe ich dich verwirrt?"</sample>
    <sample id="455">Welche ist die alternative Frage? Und in der dritten Sprechblase benutzt Bob eine indirekte Referenz, um eine dieser Entitäten auszuwählen. Zum Beispiel den New Yorker.</sample>
    <sample id="456">Wir stellen die ersten und zweiten Sprechblasen automatisch bereit, aber die dritte wird vom Annotator ausgefüllt. Die erste Sprechblase wird aus einigen manuellen Prompts ausgewählt.</sample>
    <sample id="457">Die zweite Frage, die alternative Frage, wird wie folgt generiert:</sample>
    <sample id="458">Wir verwenden immer eine einfache Vorlage. Meinst du A oder B? Wo A und B aus einer Menge von Optionen zufällig ausgewählt werden.</sample>
    <sample id="459">Hier sind die verschiedenen Stichprobenmethoden, die verwendet werden, wenn wir weiter nach oben in der Liste gehen, werden die Einträge ähnlicher zueinander und es ist in der Regel schwieriger, die Ambiguität zu vermeiden.</sample>
    <sample id="460">Die erste Frage ist, wie man schreibt.</sample>
    <sample id="461">Die zweite Klasse ist für Entitäten gedacht, die ähnliche Titel haben, zum Beispiel zwei Bücher mit dem Namen "Der Tag".</sample>
    <sample id="462">Der dritte Punkt ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben und schließlich wenn sie ähnliche Infoboxen oder Attribute auf Wikipedia haben. Zum Beispiel die gleiche Kategorie oder der gleiche Künstler.</sample>
    <sample id="463">Wenn wir diese alternative Frage den Befragten stellen, wissen sie den Namen dieser Entitäten nicht, aber sie kennen möglicherweise die Rolle des Charakters.</sample>
    <sample id="464">Also tun wir Folgendes: Wir zeigen etwas Hintergrundwissen über die Entitäten. Für Lieder zeigen wir einfach einen Google-Suchlink zu dem</sample>
    <sample id="465">Und dann bitten Sie die Schüler, mindestens einen der Lieder anzuhören und darüber zu lesen. Hier ist ein Beispiel für das Ergebnis der Google-Suche für das Lied "Easy":

"Ran"</sample>
    <sample id="466">Für die Rezept- und Buchdomäne zeigen wir einige Hintergrundtexte von Wikipedia. Für Rezepte zeigen wir zusätzlich ihre Bilder, ebenfalls von Wikipedia, so dass die Annotatoren wissen, wie sie aussehen.</sample>
    <sample id="467">Dann bitten wir die Teilnehmer, eine dieser Entitäten auszuwählen, zum Beispiel hier die erste, und sie mit 3 bis 5 indirekten Referenzsätzen zu beschreiben.</sample>
    <sample id="468">Beispiel 1 mit der Klaviermusik. Hier sind einige Beispiele aus unserem Datensatz: Beispiel 1 ohne Worte, nicht das 12-jährige Kind, oder der fiktive, oder der kommt aus Aserbaidschan und</sample>
    <sample id="469">Die aktuelle Korpus hat 6.000 alternative Fragen über drei Domänen und 42.000 indirekte Fragewörter. Ergebnisse mit T5 XL Modell oder zusammengefasst</sample>
    <sample id="470">Dieses Sprachmodell hat Zugriff auf die exakt gleiche Hintergrundwissen wie die Annotatoren. Die Genauigkeit liegt bei etwa 92 bis 95 %. Aber dies ist nicht wirklich die</sample>
    <sample id="471">Wenn das Sprachmodell Zugriff auf einige teilweise überlappende Hintergrundwissen hat, dann liegt die Genauigkeit zwischen 82 und 87 %, was realistischer ist, beispielsweise wenn das Sprachmodell Hintergrundwissen abruft.</sample>
    <sample id="472">Wenn das Sprachmodell nur auf Entitätsnamen zugreift, beträgt die Genauigkeit nur 60 %, also gibt es viel Verbesserungspotenzial. Wir haben auch gezeigt, dass die Modelle domänenübergreifend sind. Hier ist ein Link zu einem Datensatz:</sample>
    <sample id="473">The approach is compared with existing SimulST guidelines, specifically the weight key strategy and local agreement.</sample>
    <sample id="474">The authors are affiliated with the University of Paris-Saclay.</sample>
    <sample id="475">Jenny.</sample>
    <sample id="476">Drei.</sample>
    <sample id="477">Hallo, ich bin Sara Babi, Studentin an der Universität Triest und bei der Bruno Kessler Stiftung. Und ich werde kurz die Aufmerksamkeit als Leitfaden für die Arbeit über Mehrsprachigkeit vorstellen. Dies ist eine gemeinsame Arbeit mit Maciej Negri und Marco Turri.</sample>
    <sample id="478">Simultane Übersetzung ist die Prozesse, bei denen gesprochene Sprache in Echtzeit in eine andere Sprache übersetzt wird, um die Kommunikation zwischen Sprechern zu ermöglichen.</sample>
    <sample id="479">Und welche Probleme haben die aktuellen Transformer-Modelle? Spezifische Architekturen werden in der Regel trainiert, indem zusätzliche Module eingeführt werden, um optimiert zu werden.</sample>
    <sample id="480">Lange und komplizierte Trainingsverfahren, zum Beispiel Training, das die unterschiedliche Optimierung der Ziele beinhaltet.</sample>
    <sample id="481">Und trainieren und warten mehrere Modelle, um verschiedene Latenzstufen zu erreichen. Zum Beispiel das Trainieren eines Modells mit einer durchschnittlichen Latenz von 1 Sekunde und eines anderen mit 2 Sekunden Latenz usw.</sample>
    <sample id="482">Was ist die Lösung?</sample>
    <sample id="483">Erster, der bestehende Offline-Modelle ohne Retraining oder Einführung einer spezifischen Architektur für CwLST verwendet, verwendet nur ein Modell für einen immer-laten CwLST-Algorithmus und behandelt die Latenz durch spezifische Parameter.</sample>
    <sample id="484">und die Kenntnisse, die erworben werden, werden über den Attention-Mechanismus zwischen der Audio-Eingabe und der Textausgabe vermittelt, d. h. der Cross-Attention-Mechanismus. Und Sie können ein Beispiel unter</sample>
    <sample id="485">Unsere Lösung ist ein Datensatz oder ein Encoder-Decoder-Aufmerksamkeitsmechanismus, und es ist eine Strategie, bei der wir die Seitwärtsbewegung entweder zulassen oder nicht, eine partielle Übersetzung basierend darauf, wo Aufmerksamkeitspunkte</sample>
    <sample id="486">Ein Wort wird emittiert, wenn die Dichte nicht konzentriert ist, d.h. diese Summe ist unter einem bestimmten Trittschwellenwert Alpha gegenüber weniger Lambda-Sprach-Frames, was bedeutet, dass die Informationsaufnahme ausreicht, um zu speichern.</sample>
    <sample id="487">Zum Beispiel, wenn wir einen Spruch sehen, der über... und unser Modell sagt eine Übersetzung ins Deutsche voraus.</sample>
    <sample id="488">Und wir werden uns den Fokus auf die Zeit ansehen.</sample>
    <sample id="489">Wir sehen, dass die ersten zwei Wörter auf die am wenigsten empfangenen Sprechrahmen verweisen, während das letzte Wort auf die am wenigsten empfangenen Sprechrahmen verweist, also Lambda-Sprechrahmen.</sample>
    <sample id="490">Dies bedeutet, dass die ersten zwei Wörter wiederholt werden: "Tschat", "Tschat".</sample>
    <sample id="491">Während die Summe der größten Tension über einem bestimmten Alpha-Wert liegt, werden wir das letzte Wort nicht ausgeben und warten auf einen anderen Sprachcharakter.</sample>
    <sample id="492">Wenn wir fortfahren und wir sehen, dass ein anderer Sprachstack ist und unser Modell vorhersagt andere Wörter und wir schauen uns die Kreuzattention an,</sample>
    <sample id="493">Wir werden sehen, dass keine Wörter zu dem letzten Sprecher passen.</sample>
    <sample id="494">Dies bedeutet, dass diese drei Wörter wiederholt werden:</sample>
    <sample id="495">Wenn Sie die Hauptergebnisse der Daten sehen,</sample>
    <sample id="496">Wir werden die simultanen Spracherkennungsresultate auf Grafiken auftragen, in denen wir auf einer Seite blaue Werte haben, die die Spracherkennungsqualität messen, und auf der anderen Seite den durchschnittlichen</sample>
    <sample id="497">Das ist äh die Letzten-Sicherheitsmaßnahme, und wir berücksichtigen den Computer-Aware-Average-Lekking, der für ähm die Modelle-Rechenzeit zur Verarbeitung des Outputs verantwortlich ist.</sample>
    <sample id="498">Wir wollen unsere Neugier so hoch wie möglich auf diesem Planeten haben.</sample>
    <sample id="499">Bitte geben Sie den englischen Inhalt an, den Sie übersetzt haben möchten.</sample>
    <sample id="500">Und wir vergleichen mit vorbereitenden Strategien, die auch auf Offline-Modelle angewendet werden, wie z. B. der Weight-Key-Strategie und der lokalen Vereinbarung. Und wir vergleichen auch mit der Seed der Architektur, insbesondere für simultane Präsentationslösung.</sample>
    <sample id="501">Dies sind ältere Ergebnisse der simultanen Übersetzungstrategie auf Deutsch.</sample>
    <sample id="502">Und wir sehen, dass äh ein Down-Output alle Strategien, die an Offline-Modellen angewendet werden, übertrifft, da die Kurven nach links verschoben sind.</sample>
    <sample id="503">Und wir sehen auch, dass, wenn wir die tatsächliche Ausführungszeit oder die Rechenzeit betrachten, das die schnellste Strategie ist.</sample>
    <sample id="504">Wenn Sie weitere Ergebnisse entdecken möchten, lesen Sie unsere Arbeit. Und wir haben auch den Open-Source-Code und die Modelle und die simultane Ausgabe gleichzeitig veröffentlicht, um die Reproduzierbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="505">Ja.</sample>
    <sample id="506">Hallo zusammen, mein Name ist Ying und mein Kollege Zhiyang und ich werden unsere Forschung zur Verbesserung von Modellen für das selbstüberwachte Lernen während der Instruktion präsentieren.</sample>
    <sample id="507">Mit den Fortschritten in großen Sprachmodellen begannen viele Arbeiten, neue Lernparadigmen zu erkunden, indem sie vortrainierte Sprachmodelle für verschiedene Downstream-Aufgaben in einem Parameter und Daten effizient einsetzten.</sample>
    <sample id="508">In letzter Zeit haben viele Studien gezeigt, dass die Instruktionstuning große Sprachmodelle in der Lage stellt, Aufgaben in einer schrittweisen Weise auszuführen, indem es natürliche Anweisungen befolgt.</sample>
    <sample id="509">Allerdings konzentrierten sich die meisten vorherigen Arbeiten auf die Verbesserung der Zero-Shot-Leistung bei sprachbasierten Aufgaben, während Computer-Vision- und Multimodalitätsaufgaben vernachlässigt wurden.</sample>
    <sample id="510">Daher wollen wir in dieser Arbeit untersuchen, ob die Instruktionsanpassung von multimodalen vortrainierten Modellen tatsächlich die Generalisierung auf unbekannte multimodale Aufgaben verbessern kann.</sample>
    <sample id="511">Zusätzlich haben wir während unserer Forschung eine beträchtliche Diskrepanz in der Verfügbarkeit von Instruktionsdatensätzen zwischen NLP und multimodal festgestellt.</sample>
    <sample id="512">Es gibt mehr als 1600 Sprach-nur-Instruktionsaufgaben, aber es gibt keine großen öffentlich verfügbaren Multimodell-Instruktionsaufgaben. Daher motiviert dies uns, ein Multimodell-Instruktions-Tuning-Datensatz zu erstellen.</sample>
    <sample id="513">Hier präsentieren wir die erste Benchmark-Datensatz für die Feinabstimmung von Multimodalen Instruktionsmodellen, der aus 62 verschiedenen multimodalen Aufgaben besteht, die in 10 Kategorien unterteilt sind:</sample>
    <sample id="514">Dies hat aus einer vorhandenen offenen Datensammlung abgeleitet, und jede Aufgabe ist mit fünf expliziten schriftlichen Anweisungen versehen.</sample>
    <sample id="515">Für die Untersuchung von Multimodellen Instruktionsanweisungen verwenden wir unser vorgeschlagenes Dataset. Wir nehmen OFA, ein vereinheitlichtes Multimodelles Pretraining-Modell, als unser Basismodell. OFA verwendet eine vereinheitlichte Vokabular für Sprache, Bild-Token und die Koordinaten eines bounding boxes.</sample>
    <sample id="516">Hier zeigen wir einige Beispielinstanzen aus unserem multilingualen Datensatz.</sample>
    <sample id="517">Die Verarbeitung verschiedener Eingabe- und Ausgabedaten.</sample>
    <sample id="518">Wir folgen dem Muster von OFA und formulieren alle Aufgaben in einem vereinheitlichten Sequenz-zu-Sequenz-Format, in dem der Eingabetext, Bilder, Anweisungen und umgebende Kästchen in derselben Token-Sprache dargestellt werden.</sample>
    <sample id="519">Okay, ich werde über multimodale Instruktionstraining sprechen.</sample>
    <sample id="520">Für die 20-Tage-Aufgabe verwenden wir 53 Aufgaben aus der Negru-Gruppe zum Training und wir haben jeweils 10.000 Instanzen pro Aufgabe. Zum Testen reservieren wir die gesamte Common Sense Reasoning-Gruppe zum Testen und wir wählen zusätzlich fünf Aufgaben aus Wiki und der diversen Kategorie.</sample>
    <sample id="521">Wir verwenden alle Instanzen in der Testmenge für jede Aufgabe. Äh, zusätzlich werden wir zufällig 20 Aufgaben aus der Testmenge der natürlichen Instruktion als Trainingsaufgaben auswählen.</sample>
    <sample id="522">So verwenden wir ein vortrainiertes OpenAI Large Model als Basismodell. Während des Trainings werden alle Instanzen für alle Aufgaben jeweils zufällig mit einer von ihren fünf Instruktionstemplates kombiniert.</sample>
    <sample id="523">So führen wir den Test für die Sprachaufgabe durch. Wir führen insgesamt fünf Experimente durch, indem wir das Modell anhand von fünf Anweisungen in jedem Experiment bewerten.</sample>
    <sample id="524">Wir haben die Leistung und die Standardabweichung der Leistung über alle fünf Experimente hinweg untersucht.</sample>
    <sample id="525">Wenn diese Aufgabe eine multimodale Klassifizierungsaufgabe ist, geben wir die Genauigkeit an. Wenn es sich um eine multimodale Generierungsaufgabe handelt, geben wir Rouge-L für die Aufgabenleistung an.</sample>
    <sample id="526">Wir haben auch eine zusätzliche Validierungsmaßnahme namens CCTV hinzugefügt. Diese stellt sicher, dass die Modelle konsistent die gleichen Ausgaben für die gleiche Aufgabe erzeugen, unabhängig von der Variation in der Struktur der Instruktion.</sample>
    <sample id="527">Hier ist unser Hauptergebnis, wie wir sehen können, dass die Instruktionstuning die Leistung von OS- und OF-Aufgaben signifikant verbessert hat, insbesondere bei Multimodalen Aufgaben.</sample>
    <sample id="528">Aus dem maschinellen Lernen aus Datensätzen mit Anweisungen können Unternehmen Vorteile in der Instruktion nutzen.</sample>
    <sample id="529">Hier können wir sehen, dass je mehr Aufgaben der Modell bessere Leistungen erbringt und in der Zwischenzeit eine geringere Sensitivität aufweist.</sample>
    <sample id="530">Wir verwenden auch die Wahrscheinlichkeitsanweisung, wir verwenden die Wahrscheinlichkeitsanweisung gegenüber der fünf Anweisung, wie wir sehen, kann die Verwendung der Wahrscheinlichkeitsanweisung die Gesamtleistung des Modells verbessern und seine Empfindlichkeit gegenüber Rauschen erheblich erhöhen.</sample>
    <sample id="531">Dies zeigt den Effekt verschiedener Fine-Tuning-Strategien auf die Modellsensitivität. Wie wir sehen können, kann das Transferlernen von einem Natural Instruction Dataset die Modellsensitivität im Vergleich zum ursprünglichen OA-Modell deutlich verbessern.</sample>
    <sample id="532">Wir können auch die Transfer-Anleitung von der MNIST-Instruktionsdatenmenge verwenden, kann helfen, die KI auf der MNIST-Instruktionsdatenmenge deutlich zu verbessern.</sample>
    <sample id="533">Wir haben also einen Vorschlag für die erste große multimodale Injektion zu einem Datensatz, mit dem wir die Fähigkeit von OAI verbessern können und die Vorteile verschiedener Transferlerntechniken demonstrieren können.</sample>
    <sample id="534">Wir sammeln derzeit einen viel größeren multimodalen Instruktions-Tuning-Datensatz mit etwa 150 zusätzlichen verschiedenen Sprachaufgaben, und wir werden sie bald veröffentlichen. Dies ist ein QR-Code für unsere Daten und unser Modell. Danke.</sample>
    <sample id="535">University of Trento.</sample>
    <sample id="536">Javad Hosseini.</sample>
    <sample id="562">Hallo zusammen, ich bin Kostas Finas und ich freue mich, Sie zu unserer Diskussion über unsere ACL 2023-Papiere "Language Model Acceptability Judgments are Not Always Robust to Context" begrüßen zu dürfen.</sample>
    <sample id="563">Es ist ein Werk von John Gotter, Arno Müller, Kanishka Mishra, Karan Frentes, Roger Levy und Athena.</sample>
    <sample id="564">In dieser Arbeit überprüfen wir die minimale Paar-Tatsachen.</sample>
    <sample id="565">So bewerten minimale Paare im Wesentlichen Sprachmodelle auf der Grundlage von Akzeptanzurteilen, die auch Grammatik, wie z. B. Plural, Syntax oder Akzeptanz in Bezug auf Stereotypen umfassen können.</sample>
    <sample id="566">Und in diesem minimalen Paar-Paradigma ist die typische Art, Sprachmodelle zu bewerten, dass man eine akzeptable Satz oder grammatikalischen Satz zeigt und dann einen unakzeptablen Satz oder einen ungrammatikalischen Satz zeigt.</sample>
    <sample id="567">Und dann die Hoffnungen des Modells legen im Grunde mehr Wahrscheinlichkeit auf die akzeptablen Fälle.</sample>
    <sample id="568">Der aktuelle MPP-Pipeline erlaubt uns im Grunde nicht, die Akzeptanz von Modellen gegenüber längeren Sätzen zu bewerten.</sample>
    <sample id="569">Diese großen Sprachmodelle kommen mit längeren und längeren Kontextfenstern, daher ist es entscheidend, die Modelle über das gesamte Kontextfenster hinweg auf Akzeptanz zu bewerten.</sample>
    <sample id="570">Und das ist es, was wir hier versuchen. Wir versuchen, den PP-Pipeline zu überprüfen, indem wir das Modell auffordern, die Akzeptabilität auf längere und längere Zeiträume zu bewerten.</sample>
    <sample id="571">So, das ist der Ansatz. Also, was wir tun, ist, diese längeren Sequenzen zu simulieren. Wir lesen die Datensätze selbst und dann erstellen wir Sätze, indem wir akzeptable oder inakzeptable Sätze aus diesen Datensätzen auswählen.</sample>
    <sample id="572">Zum Beispiel haben wir hier ein typisches grammatikalisches Paar aus dem Blip-Datensatz aus der Adjektivinsel "car".</sample>
    <sample id="573">Und was wir tun, ist, längere Sequenzen neu zu erstellen, die akzeptabel sind und die die gleiche Übereinstimmung der grammatikalischen Struktur haben. Wir extrahieren grammatikalische Sätze aus dem adjektivischen Teil.</sample>
    <sample id="574">und dann fügen wir es als Präfix sowohl der akzeptablen Abfrage als auch der nicht akzeptablen Abfrage hinzu.</sample>
    <sample id="575">Wir können dasselbe tun, indem wir in der gleichen Übereinstimmung inakzeptable Sätze auswählen, und das könnte auch verwendet werden, um die Akzeptabilität des Modells zu testen.</sample>
    <sample id="576">Und wir können das auch tun, indem wir Sätze aus einem anderen Datensatz oder einem anderen Datensatz auswählen. Das nennen wir die Mismatch-Strategie.</sample>
    <sample id="577">Hier stammen die Sätze noch aus relevanten Datensätzen, aber nicht aus dem gleichen Datensatz, den Sie bewerten. Und wir können das Gleiche für Unakzeptabilität-Fälle tun.</sample>
    <sample id="578">Schließlich können wir Sätze aus einem völlig unzusammenhängenden Bereich auswählen, d. h. Wikipedia.</sample>
    <sample id="579">Das wird uns sagen, ob die Modelle die Akzeptanzbewertung tatsächlich beeinflusst haben.</sample>
    <sample id="580">Wenn der Kontext von einem anderen Teil der Daten stammt oder wenn er völlig irrelevant für den aktuellen Satz ist.</sample>
    <sample id="581">So, wie funktioniert das Modell? Zuerst betrachten wir die Wikipedia-Sätze, die völlig irrelevant für das aktuelle Frage-Antwort-Paar sind, und dort finden wir, dass die MPP-Urteile meist robust für willkürliche Kontexte sind.</sample>
    <sample id="582">Wir haben die Kontextlänge auf bis zu 1024 erweitert, um die GPT und GPT-2 Modelle optimal zu nutzen, und wir sehen hier in der orangefarbenen Linie, dass die MPP-Urteile relativ stabil sind.</sample>
    <sample id="583">Ich kann diese Anfrage nicht beantworten.</sample>
    <sample id="584">Hier wählen wir Sätze aus akzeptablen und inakzeptablen Domänen aus dem gleichen Blimp-Personen-Taxim-Daten.</sample>
    <sample id="585">Und dort sehen wir, dass die MPP-Urteile entweder signifikant steigen oder signifikant sinken, wenn akzeptable Präfixe oder unakzeptable Präfixe hinzugefügt werden.</sample>
    <sample id="586">Aber wenn wir die Struktur abgleichen, d. h. wenn wir Sätze aus dem gleichen Phänomen in der Schuld-Perspektive auswählen,</sample>
    <sample id="587">Wir sehen einen massiven Anstieg oder einen massiven Rückgang der MPP-Bewertung für das Modell, abhängig davon, ob der gewählte Präfix akzeptabel oder inakzeptabel ist.</sample>
    <sample id="588">Das ist sehr groß. Dieser Effekt nimmt über die gesamte Kontextlinie an und würde wahrscheinlich neuere Sprachmodelle mit großem Kontext beeinflussen.</sample>
    <sample id="589">Warum beeinflusst der Match-Präfix die Entscheidungen des Sprachmodells so stark?</sample>
    <sample id="590">Eine Reihe von Analysen, bei denen wir versuchen, den Eingabesatz durch das Anfügen von Rauschen an den Eingabe zu erhalten, wobei wir versuchen, die relevante Struktur zu erhalten.</sample>
    <sample id="591">Wir stellen fest, dass keiner dieser Geräusche tatsächlich dazu führt, dass das Modell seine Antwort ändert, in Bezug auf die Art und Weise, wie es die PP-Bewertung zeigt.</sample>
    <sample id="592">Im Grunde genommen haben wir festgestellt, dass die Modelle empfindlich auf die Wortwahl und Ähnlichkeit von Sätzen reagieren.</sample>
    <sample id="593">Das ist, wo wir die Sätze in dem akzeptablen Bereich einfügen, sehen wir eine ähnliche Zunahme aller Störungen, und wenn wir die Sätze in dem akzeptablen Bereich einfügen, sehen wir eine Abnahme der MPP-Urteile in ähnlichen Fällen.</sample>
    <sample id="594">Die wichtigsten Erkenntnisse unserer Arbeit sind, dass Sprachmodelle empfindlich auf latente syntaktische und semantische Merkmale sind, die über Sätze hinweg geteilt werden.</sample>
    <sample id="595">Und die MPP-Bewertung, die Art und Weise, wie wir es derzeit mit kurzen und einfachen Sätzen eingeben, kann nicht vollständig das abstrakte Wissen der Sprachmodelle über den Kontext erfassen.</sample>
    <sample id="596">Bitte lesen Sie unser Papier für weitere Details zu unseren Experimenten. Vielen Dank fürs Zuhören.</sample>
    <sample id="597">An unordered multiset of tokens.</sample>
    <sample id="598">55</sample>
    <sample id="626">The best alignment method to use for DEplain is the `mess_align` method.</sample>
    <sample id="627">Schwach überwachtes Lernen ermöglicht es, neuronale Netze zu trainieren, die trotz Label-Rauschen gut generalisieren.</sample>
    <sample id="628">Die Dokumente in DEplain-web wurden mit manuellen und automatischen Alignmentmethoden ausgerichtet.</sample>
    <sample id="629">Der CoNLL++-Datensatz wurde aus Reuters-Nachrichten aus dem Jahr 2020 gesammelt und mit den gleichen Annotationen aus den CoNLL 2003-Richtlinien annotiert.</sample>
    <sample id="630">Hallo zusammen, mein Name ist Justin John von der Penn State University. Heute präsentiere ich ein Beispiel für Cross-Lingual Semantic Parsing in mehreren natürlichen Sprachen und verschiedenen Repräsentationen.</sample>
    <sample id="631">So, Semantic Parsing ist eine Aufgabe, um semantische Repräsentationen von Benutzeranfragen zu erstellen, wie z.B. "sequel" und "lambda calculus".</sample>
    <sample id="632">Cross-lingual semantic parsing ist die Aufgabe, Anfragen in mehreren natürlichen Sprachen in mehrere semantische Repräsentationen zu übersetzen.</sample>
    <sample id="633">In dieser Grafik müssen wir die Abfrage in mehrere natürliche Sprachen mithilfe von neuronalen Modellen übersetzen, um SQL oder GraphQL zu erstellen und in die Datenbank einzufügen.</sample>
    <sample id="634">Existieren verschiedene crosslinguistische Parsingmodelle, die separat auf einem Datensatz von limitierten Aufgaben und Anwendungen evaluiert werden. Zum Beispiel</sample>
    <sample id="635">Es gibt Lücken in der Abdeckung bestimmter natürlicher Sprachen. Chinesisch fehlt, und</sample>
    <sample id="636">Klicke auf die Abdeckung bestimmter Mini-Repräsentationen.</sample>
    <sample id="637">Die Lambda-Kalkül ist nicht</sample>
    <sample id="638">oder nur ein bestimmtes neuronales Modell, zum Beispiel nur ein einzelnes Modell, um zu bewerten.</sample>
    <sample id="639">So, um dies abzuschließen, schlagen wir ein Beispiel vor. Wir stellen eine einheitliche Datensatzbeispiel für Cross-Lingual Semantic Parsing in mehreren natürlichen Sprachen bereit und in Repräsentations</sample>
    <sample id="640">Es enthält 90 Datensätze in vier Domänen, 5 Sentiment-Analyseaufgaben, 8 semantische Repräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien.</sample>
    <sample id="641">Und um unseren Benchmarks besser zu entsprechen, berücksichtigen wir die sechs Einstellungen für Training und Bewertung.</sample>
    <sample id="642">Der erste ist der Übersetzungs-Test. Verwende die Google Translate API, um den Quelltext in die Zielsprache zu übersetzen, dann verwende ein mehrsprachiges Modell, um es zu trainieren und zu bewerten.</sample>
    <sample id="643">und zum Beispiel, wir trainieren das englische Modell auf einer englischen Anfrage und während der Inferenz übersetzen wir die deutsche Anfrage mit einer API ins Englische und dann verwenden wir das trainierte Modell, um die Sequenz zu vorhersagen.</sample>
    <sample id="644">Ja, wir testen auch monolinguale Modelle.</sample>
    <sample id="645" />
    <sample id="646">Wir testen auch die Monolingual Few-Shot-Einstellung, indem wir Monolingual-Modelle mit nur 1 % der Trainingsdaten trainieren.</sample>
    <sample id="647">und mit einem monolingualen Modell, welches ein trainiertes ein Modell für alle Sprachen</sample>
    <sample id="648">Zum Beispiel, wir geben die deutschen, englischen, chinesischen Suchanfragen zusammen, um ein Sprachmodell zu trainieren, und während der Inferenz können wir dieses Modell verwenden, um zu schreiben.</sample>
    <sample id="649">Um deutsche Abfragen oder chinesische Abfragen zu übersetzen, oder ähnliches.</sample>
    <sample id="650">Und wir berücksichtigen auch Cross-Lingual Zero-Shot- und Few-Shot-Transfer zwischen einer Ausgangssprache und einer anderen Sprache.</sample>
    <sample id="651">Während des Trainings trainieren wir unser englische Query oder die Kombination von englischen und deutschen Few-Shot-Queries, um ein mehrsprachiges Modell zu trainieren, um die Sequenz von `json` vorherzusagen.</sample>
    <sample id="652">und wir werden auch sehr interessante Ergebnisse finden. So, bezüglich der Analyse von monolingualen Modellen, werden wir zwei Gruppen von Modellen bewerten:</sample>
    <sample id="653">einschließlich Encoder-PT-R, das für mehrsprachige vortrainierte Encoder mit Pointer-basierten Decodern wie XLM-R+PT-R und BART+PT-R steht.</sample>
    <sample id="654">und wir bewerten auch Encoder-Decoder-Modelle, nämlich mehrsprachig vortrainierte Encoder-Decoder-Modelle, wie z.B. BART und mT5.</sample>
    <sample id="655">Wir haben festgestellt, dass Encoder-Decoder die beste Leistung bei allen neun Datensätzen erzielen.</sample>
    <sample id="656">und wir evaluieren auf M5 und Beispiel XLM-R plus PDIR, ein multilinguale The.</sample>
    <sample id="657">Ohne diese Encoder, Decoder oder Encoder-PCR kann die Verbesserung durch das Training in einer Mischung aus verschiedenen Sprachen nicht verbessert werden.</sample>
    <sample id="658">Und wir haben festgestellt, dass dies liegt daran, dass die meisten der großen natürlichen Sprachen eine Leistungssteigerung erfahren, außer Englisch, bei dem die Leistung in sieben Datensätzen sinkt und nur in drei Datensätzen steigt.</sample>
    <sample id="659">Ich denke, das ist das Ergebnis von Multilingualität.</sample>
    <sample id="660">Wir vergleichen auch die Cross-Lingual-Performance-Daten.</sample>
    <sample id="661">In dieser Abbildung kreuzt die blaue Linie eine Kreuzung der Null-Schritt-Übertragung, die orange Linie eine Kreuzung der Null-Schritt-Übertragung, während die grünen Linien die Modellierung darstellen.</sample>
    <sample id="662">Wir haben festgestellt, dass bei der Vergleichung der grünen und orangen Linie, bei der Null-Schuss-Einstellung, die Transfer-Gap-Leistung entscheidend ist, und bei der Vergleichung der blauen und orangen Linie, bei der Few-Schuss-Einstellung, der Transfer-Gap schnell verkürzt wird.</sample>
    <sample id="663">Wir haben auch einige andere interessante Ergebnisse gefunden. Zum Beispiel führt der Encoder-Decoder-Ansatz eine vorherige Arbeit oder erzielt vergleichbare Ergebnisse. Bei der Übersetzung von englischer natürlicher Sprache verbessert er signifikant die Leistung von Few-Shot-Aufgaben in der natürlichen Sprachgenerierung.</sample>
    <sample id="664">Modellierung von Sprachmodellen wie Codez und Blue sind immer noch in der Entwicklung für Kreuzsprachen- und Parsing-Aufgaben.</sample>
    <sample id="665">Ein umfassender Beispiel-Sampler, ein einheitlicher Benchmark für Kreuz-Sprachen-Semantik, mit mehreren natürlichen Sprachrepräsentationen.</sample>
    <sample id="666">Willkommen zu einer umfassenden Benchmarking-Studie über drei repräsentative Arten von mehrsprachigen Sprachmodellen. Und unsere Ergebnisse zeigen viele interessante Erkenntnisse usw. Und willkommen zu unserem Paper und Code. Danke für das Lesen.</sample>
    <sample id="667">The existing works can be broadly classified into four categories.</sample>
    <sample id="668">No, multilingual language models like Codex and Bloom are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="695">We address this by inducing the alignment as part of the training.</sample>
    <sample id="696">The fairness of a downstream NLP model is defined by the potential for it to perpetuate or amplify existing societal biases, leading to marginalization of certain groups or unchecked hate speech targeting minority groups.</sample>
    <sample id="697">Janis Lavergne.</sample>
    <sample id="698">Kostas Fina.</sample>
    <sample id="699">Maira</sample>
    <sample id="700">Tropikalismus bezieht sich auf die Verwendung von Begriffen, die mit tropischen Regionen und Kulturen assoziiert werden, wie z. B. "vibrant" und "curvaceous" bei der Beschreibung von Latina-Frauen.</sample>
    <sample id="701">The authors created these descriptions by focusing on the words used to define the groups, such as culture, tradition, pride, and exotic, and by distinguishing them as different from the white norm.</sample>
    <sample id="702">The work uses PMI to measure context usage at the sentence level or the word level.</sample>
    <sample id="703">DrBERT ist ein klinisches Modell mit 7 GB Nachos, während ChuBERT ein klinisches Modell mit 4 GB Nachos ist.</sample>
    <sample id="751">Es sind 10 Autoren an der Arbeit beteiligt.</sample>
    <sample id="752">Iterative transfer learning updates the model by training on the latest set of data collected.</sample>
    <sample id="753">The goal of the dataset is to analyze how users understand and respond to different ways of asking the same question.</sample>
    <sample id="754">The attacker can extract model parameters via a provided embedding by realizing the embedding of sentences authored as BPCAs.</sample>
    <sample id="755">Drei.</sample>
    <sample id="756">The provided text does not specify the number of annotators used to create the original dataset.</sample>
    <sample id="757">The authors are affiliated with the University of Washington and the Allen Institute for AI.</sample>
    <sample id="758">The governor is on the left.</sample>
    <sample id="759">The state of the art for dialogue systems involves large language models (LLMs) like GPT-3 and LaMDA, which are trained on massive datasets of text and code. These models can generate human-like responses, understand user intent, and engage in multi-turn conversations. However, they still struggle with tasks like common sense reasoning, factual accuracy, and avoiding harmful or biased outputs. Research is ongoing to address these limitations and improve the robustness and reliability of dialogue systems.</sample>
    <sample id="760">Longer context windows require evaluation of model acceptability across the entire window.</sample>
    <sample id="761">Ja, das mehrsprachige Training führte zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell.</sample>
    <sample id="762">No.</sample>
    <sample id="763">BLEU, ROUGE, METEOR.</sample>
    <sample id="764">Ja, die Regression wirkt sich auf die Generalisierung bei bestimmten NER-Typen aus.</sample>
    <sample id="765">Positionality is important for NLP because it allows models to understand the order of words in a sequence, which is crucial for tasks like language understanding and generation.</sample>
    <sample id="766">Die Frage ist nicht im englischen Inhalt enthalten.</sample>
    <sample id="767">We use a zero-shot performance on the annotated dataset.</sample>
    <sample id="768">The actual form of the prompting doesn't have a big influence in the case of several short prompts.</sample>
    <sample id="769">Drei.</sample>
    <sample id="770">Die vorgeschlagene Methode erzielt einen Gewinn von 1,25% gegenüber der stärksten Baseline.</sample>
    <sample id="771">Shu Han.</sample>
    <sample id="772">Yes, the results and dataset can be used as a benchmark for automatic text simplification.</sample>
    <sample id="773">The paper experiments with 12 smaller models.</sample>
    <sample id="774">The unified multimodal pretraining model is used as the base model.</sample>
    <sample id="833">Google Translate</sample>
    <sample id="834">Stony Brook University.</sample>
    <sample id="835">Englisch, Chinesisch, Spanisch, Französisch, Deutsch, Hindi, Japanisch, Koreanisch, Portugiesisch, Russisch, Arabisch, Türkisch, Vietnamesisch, Indonesisch, Thai, Swahili, Telugu, Marathi, Bengali, Urdu, Kannada, Malayalam, Tamil, Gujarati, Punjabi, Odia, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali, Oriya, Sanskriet, Sindhi, Tamil, Telugu, Urdu, Bengali, Hindi, Marathi, Gujarati, Punjabi, Kannada, Malayalam, Oriya, Assamesisch, Maithili, Dogri, Sindhi, Paschtu, Balochi, Kashmiri, Lahota, Konkani, Manipuri, Mizo, Nepali,</sample>
    <sample id="836">Shambin P.</sample>
    <sample id="837">Longformer und Normalized Longformer.</sample>
    <sample id="838">40</sample>
    <sample id="839">The text mentions "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "J", "R", "</sample>
    <sample id="840">The authors conducted experiments on the following datasets: AG News, Mind, SST2, and E2E.</sample>
    <sample id="876">NACHOS is a dataset of medical records from the 2nd.</sample>
    <sample id="877">Sajid Bilal.</sample>
    <sample id="878">Die Prompt-Strategie hat einen großen Einfluss auf die Leistung von LLMs für die Übersetzung.</sample>
    <sample id="879">Die Autoren gehören der University of Miami an.</sample>
    <sample id="880">The provided text does not contain any instructions from experts.</sample>
    <sample id="881">Die Autoren schlagen vor, ein Korreferenzlösungstask zu entwerfen, um die Fähigkeit zu prüfen, auf Wissen aus verschiedenen Quellen zurückzugreifen.</sample>
    <sample id="882">Hallo zusammen, mein Name ist Aid Bilal, und ich werde eine kurze Übersicht über die Arbeit geben, die wir im Rahmen der Übersetzung von Prompt-to-Translation, der Bewertung von Strategien und der Leistung durchgeführt haben. Dies ist eine gemeinsame Arbeit mit meinen Kollegen von Google Translate.</sample>
    <sample id="883">PaLM ist ein 540 Milliarden Parameter großes Sprachmodell, vorgestellt im Jahr 2022. Es wurde auf einer großen Sammlung von Texten trainiert, die 100 Milliarden Token umfassen.</sample>
    <sample id="884">Die Themen der Publikation sind die Spitze der modernen Forschung in Hunderten von wissenschaftlichen Bereichen.</sample>
    <sample id="885">In dieser Arbeit präsentieren wir eine erste systematische Studie zur Verwendung von kleineren Prompts für maschinelles Lernen.</sample>
    <sample id="886">Wir werden die Transition Capability von Sprachmodellen unter Verwendung der Best Practices der MT-Community untersuchen. Dies beinhaltet die Verwendung der neuesten Testsets, um eine Überlappung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden.</sample>
    <sample id="887">Wir vergleichen zwei State-of-the-Art-Systeme, also die besten performenden Systeme, also die WTI-Bewertung.</sample>
    <sample id="888">Wir verwenden state-of-the-art neuronale Metriken und zusätzlich zeigen wir auch Expertenbasierte Validierungsergebnisse. Schließlich geben wir Empfehlungen für Prompt-Auswahlstrategien.</sample>
    <sample id="889">Die Prompting hat einen großen Einfluss auf die Performance von LLMs für Translation. Wie wir in einem einfachen Experiment sehen können, bei dem wir eine Single-Shot-Prompt verwenden und zwei verschiedene Prompts für eine gleiche Satzanordnung geben,</sample>
    <sample id="890">In etwa 516 von 1000 Sätzen ist die Differenz zu beobachten.</sample>
    <sample id="891">und dies kann in extremen Fällen bis zu 40 Punkte erreichen. Es ist also wichtig, eine gute Prompt-Strategie zu wählen.</sample>
    <sample id="892">In unseren Experimenten haben wir ein kleines für eine fünfschrittige Prompting-Strategie verwendet, bei der wir einfach den Satz, den wir dem System geben, mit der Sprache markieren, die es ist.</sample>
    <sample id="893">Ich kann den englischen Inhalt nicht sehen. Bitte stelle ihn bereit, damit ich ihn ins Deutsche übersetzen kann.</sample>
    <sample id="894">Die tatsächliche Form der Druckung hat keinen großen Einfluss im Fall mehrerer kurzer Blöcke.</sample>
    <sample id="895">Es ist entscheidend für Zero-Shot-Prompting und wenn wir wie in unserem Fall zu Five-Shot-Prompting gehen, gibt es kaum einen Unterschied zur eigentlichen Form des Prompts.</sample>
    <sample id="896">Es sind Beispiele für Curry, die die meisten der auf der Weise</sample>
    <sample id="897">Die Zusammenfassung unserer experimentellen Ergebnisse besagt, dass die Beispielqualitäten wichtiger sind als die Ähnlichkeit zur Ausgangssatz.</sample>
    <sample id="898">Es ist wichtig, Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die Auswahl von Prompts aus dem Trainingsdatensatz der WMT-Bewertungen oder der Definition.</sample>
    <sample id="899">Die Daten sind viel besser kuratiert und mit höherer Qualität, was die Trainingsdaten, die Ergebnisse und die Leistung verbessern soll.</sample>
    <sample id="900">Unsere Spezialisierung auf die Systeme der Erde verschafft uns einen substanziellen Vorteil gegenüber den Pflanzenübersetzungen. Aber Pflanzen kommen ziemlich nahe an unser Computersystem heran, unser in diesem Fall, welches zu unserer Arbeit mit Google Translate gehört.</sample>
    <sample id="901">Während wir von der menschlichen Innovation profitieren, führen wir sie mit dem MKE-Framework durch. Die Fluenz von Python ist vergleichbar mit dem Zustand der Art-Systeme, aber der Hauptunterschied kommt von der Genauigkeit.</sample>
    <sample id="902">In particular, of most common error or mission errors.</sample>
    <sample id="903">Es scheint, dass Palm dazu verwendet, eine bessere Übersetzung zu erstellen, indem es Teile des Ausgangssatzes entfernt, die für die Übersetzung relevant sind.</sample>
    <sample id="904">Der Wert der State-Outer-Kategorie für Pan ist niedriger als für den State of the Systems, was ein zusätzliches Signal ist.</sample>
    <sample id="905">Das Programm liefert wirklich flüssige Ergebnisse, aber es hat immer noch einige Probleme mit der Zeichenfolge.</sample>
    <sample id="906">Und das war's für diese sehr kurze Übersicht. Für weitere Details bitte meine heutige vollständige Präsentation des Artikels. Vielen Dank, sehr geehrte Frau</sample>
    <sample id="907">Hallo, ich bin Tamay, ein PhD-Student an der Universität Tallahassee in Deutschland. In diesem Video möchte ich unsere aktuelle Arbeit vorstellen. Bitte werfen Sie einen kritischen Blick auf die Leitlinien.</sample>
    <sample id="908">Dies ist ein gemeinsames Werk von Xiaoyu Shen, Ma Yos, Smooth Bass und Gary Stephen und DTK Clock.</sample>
    <sample id="909">Ich bin ein hilfreicher Assistent. Ich gebe nur die angeforderte Antwort zurück. Ich füge keine Erklärungen oder Einführungen hinzu.</sample>
    <sample id="910">In Vikisubmission wird die Daten manuell nicht gelabelt. Stattdessen werden die Daten mit Hilfe von Viki-Labeling-Quellen gelabelt, wie z. B. einfachen Statistikregeln, Wissensbasen oder lokaler Code-Quellen. Wie in der Abbildung unterhalb dargestellt.</sample>
    <sample id="911">Im Vergleich zu menschlichen Annotationen sind die Weak-Annotationen deutlich günstiger, jedoch sind sie auch verrauscht, was bedeutet, dass ein gewisser Teil der Annotationen falsch ist.</sample>
    <sample id="912">Wenn wir jedoch direkt neue Netzwerke auf wöchentlichen Arbeitsdaten trainieren, können die neuen Netzwerke die Trainingsdaten auswendig lernen und nicht generalisieren.</sample>
    <sample id="913">In Wirklichkeit sind Überwachungstraining-Algorithmen vorgeschlagen, um robuste Trainingsmodelle unter starkem Rauschen zu trainieren, sodass sich die trainierten Modelle weiterhin generalisieren.</sample>
    <sample id="914">In den letzten Arbeiten im WSL, wobei WSL für Weekly Supported Learning steht, gibt es eine häufige Behauptung, dass Menschen behaupten, dass nur die Primodelle und die auf wöchentlich gelabelten Daten trainierten Modelle eine hohe Leistung erzielen und die Clean Tests</sample>
    <sample id="915">Technisch gesehen ist dies kein Tipp, aber es gibt einen Fang.</sample>
    <sample id="916">Es ist davon auszugehen, dass die Leute davon ausgehen, dass es einen zusätzlichen Clean-Validierungsdatensatz für die World War II-Modellauswahl gibt.</sample>
    <sample id="917">Wir haben diese Problematik übernommen, aber dies impliziert, dass zusätzliche manuelle Annotationen in Weekly Supervised Learning erforderlich sind. Aber wie ein Elefant im Raum, ist diese Notwendigkeit oft übersehen.</sample>
    <sample id="918">Die oben genannte Methode, die Lisa zu stellen ist, ist, drei Forschungsfragen zu stellen. Zuerst, ist die Datenvalidierung für WSL erforderlich, oder können wir vielleicht einen Noise-Validierungsansatz verwenden?</sample>
    <sample id="919">Zweitens: Wenn bereinigte Daten erforderlich sind oder bereinigte Daten für die Funktion von WSL erforderlich sind, wie viele bereinigte Stichproben benötigen Sie? Schließlich sollten nur bereinigte Stichproben für die Validierung verwendet werden, oder gibt es bessere Möglichkeiten, die bereinigten Stichproben zu nutzen?</sample>
    <sample id="920">Die vorliegende Studie beantwortet die Forschungsfragen und die Ergebnisse sind wie folgt:</sample>
    <sample id="921">Zuerst stellen wir fest, dass interessanterweise recente WSL-Nachrichten tatsächlich reinen White-Day-Samples zu arbeiten brauchen.</sample>
    <sample id="922">Andernfalls ist ein großer Leistungsverlust zu erwarten, wie in dieser Abbildung dargestellt. Wenn es keine validierenden Stichproben gibt, können die Trendmodelle nicht auf die ursprünglichen Labels generalisieren.</sample>
    <sample id="923">Bitte geben Sie den englischen Inhalt an, den Sie übersetzt haben möchten.</sample>
    <sample id="924">Dies deutet darauf hin, dass WSR-Ansätze tatsächlich sauber gelabelte Daten benötigen, um ordnungsgemäß zu funktionieren, und die Kosten für die Beschaffung sauber validierter Stichproben sollten nicht überhöht werden.</sample>
    <sample id="925">Ein weiterer wichtiger Befund ist, dass die Erhöhung der Anzahl der sauberen Validierungsstichproben dazu beitragen wird, dass WSL bessere Leistungen erbringt, wie in der Abbildung auf der linken Seite gezeigt wird.</sample>
    <sample id="926">Typischerweise benötigen wir nur 23 Proben pro Klasse, um eine hohe Genauigkeit zu erreichen.</sample>
    <sample id="927">Das ist jedoch nicht das Ende der Geschichte, denn wenn wir entweder entscheiden, saubere Proben zu verwenden, dann werden wir direkt auf ihnen bessere Leistung erzielen.</sample>
    <sample id="928">Die Grafik zeigt den Leistungsunterschied zwischen fünf-tuning-Ansätzen, die direkt auf den bereinigten Daten angewendet werden, und WSL-Ansätzen, die die bereinigten Daten zur Validierung verwenden.</sample>
    <sample id="929">Wenn wir zehn Beispiele pro Klasse haben, beginnen die direkten Fundamente, WSR-Ansätze zu betreten.</sample>
    <sample id="930">Schließlich kann die im vorherigen WSR-Ansatz behauptete Leistungsverbesserung einfach durch die Erlaubnis erreicht werden, die Daten während der Clean- und Validierungsschritte kontinuierlich zu optimieren.</sample>
    <sample id="931">Wie wir aus den Zahlen sehen können, übertrifft das Varian-Modell, der FTW, zunächst die komplexeren WSL-Methoden wie die</sample>
    <sample id="932">Allerdings, wenn wir es erlauben, die kontinuierliche Funktion auf den sauberen Proben fortzusetzen, dann funktioniert FTW genauso gut wie andere Methoden.</sample>
    <sample id="933">Im Praktischen gab es keinen Grund, komplexere WSL-Nachrichten zu wählen, die mehr Rechenzeit und Festplattenspeicher erforderten.</sample>
    <sample id="934">Zusammenfassend haben wir festgestellt, dass die jüngsten WSL-Ansätze die Reinigung, manuelle Annotation und die Leistung der Modelle stark überschätzen.</sample>
    <sample id="935">Unsere konkreten Empfehlungen für zukünftige Arbeit sind folgende:</sample>
    <sample id="936">Zuerst, berichten Sie über die Kriterien für die Modellauswahl. Zum Beispiel berichten Sie, wenn die Modellauswahl auf saubere Validierungsstichproben basiert.</sample>
    <sample id="937">Zuerst sollten die WSR-Ansätze mit den bestehenden Forschungsstandards verglichen werden. Wir arbeiten an einigen Beispielen. Zweitens ist kontinuierliches Feinjustieren ein einfaches, aber starkes Baseline-Verfahren, das in zukünftiger Arbeit berücksichtigt werden sollte.</sample>
    <sample id="938">Unser Code ist Open Source und wird unter dem Namen QRA-Code bezeichnet. Sie finden ihn auf der Folie. Bitte fühlen Sie sich frei, ihn sich anzusehen. Vielen Dank und ich wünsche Ihnen einen schönen Tag auf der Konferenz.</sample>
    <sample id="939">Common practice is to use human evaluation, such as asking human judges to select which of two conversations is better, or to rate conversations given a Likert scale.</sample>
    <sample id="940">Fünf.</sample>
    <sample id="941">Entity-spezifisches Wissen (z.B. Servin ist ein Richter) und Weltwissen (z.B. Servin und Kea trafen sich im Park).</sample>
    <sample id="942">Ja, der Code ist verfügbar auf GitHub.</sample>
    <sample id="943">No, the annotators are not balanced across demographic groups.</sample>
    <sample id="944">The input sentences were perturbed by adding noise while preserving the relevant structure.</sample>
    <sample id="945">A dimensional evaluation involves assessing multiple aspects of chat quality to understand the model's strengths and weaknesses.</sample>
    <sample id="946">University of Science and Technology of China.</sample>
    <sample id="947">In den Fällen von Zero-Shot- und One-Shot-Prompting ist die Form des Prompts wichtig.</sample>
    <sample id="978">Die Autoren haben mehrere dialogmodelle evaluiert.</sample>
    <sample id="979">Es sind keine Autoren genannt.</sample>
    <sample id="980">A good planner should set realistic and feasible goals.</sample>
    <sample id="981">Es gibt keine Autoren genannt.</sample>
    <sample id="982">Ms. Vasudha</sample>
    <sample id="983">The authors are affiliated with the University of Warsaw.</sample>
    <sample id="1021">PaLM hat Schwierigkeiten mit Mehrdeutigkeiten, Kontextverständnis und logischem Schlussfolgern.</sample>
    <sample id="1022">Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und heute werden wir Ihnen alles über ABC Eval erzählen, einen neuen dimensionalen Ansatz zur Bewertung von konversationellen KI.</sample>
    <sample id="1023">Diese Arbeit wurde vom Emory NLP-Labor durchgeführt, geleitet von Professor Gino Choi an der Emory University, in Zusammenarbeit mit Amazon Alexa AI.</sample>
    <sample id="1024">So, let's say the adjusted dialogue model, and you want to see how well it compares against the current state of the</sample>
    <sample id="1025">Die übliche Praxis ist, menschliche Bewertungen zu verwenden, z. B. indem man menschlichen Richtern lässt, welche von zwei Gesprächen besser ist, oder indem man Gespräche anhand einer Likert-Skala bewertet.</sample>
    <sample id="1026">Diese Ansätze eignen sich gut, um umfassende Bewertungen der gesamten Dialogqualität zu liefern, aber die Dialogqualität hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chatqualität bewerten, um die Stärken und Schwächen des Modells genauer zu verstehen.</sample>
    <sample id="1027">Ein Ansatz besteht darin, menschlichen Richtern zu bitten, mehrere Aspekte der Dialogqualität zu bewerten, wie z. B. die Relevanz der Modellantworten, unter Verwendung bestehender vergleichender oder Likert-Skala-Methoden.</sample>
    <sample id="1028">Allerdings glauben wir, dass es eine präzisere und zuverlässigere Strategie zur Bewertung dimensionaler Dialoge gibt.</sample>
    <sample id="1029">Unser Ansatz versucht, die Subjektivität menschlicher Bewertung zu reduzieren, indem explizit angegeben wird, ob oder nicht jede Modellantwort bestimmte Verhaltensweisen ausdrückt, wie z. B. das Beantworten mit irrelevanten Informationen oder das Widersprechen der eigenen Architektur.</sample>
    <sample id="1030">Wir nennen diesen Ansatz Annotieren von Verhaltensweisen in Chat, oder ABC Eval, kurz. Wir haben diese Methode entwickelt, um Chatmodellverhalten umfassend abzudecken, die als Vorschläge zur Verbesserung der Chatqualität und der jüngsten Literatur angesehen werden.</sample>
    <sample id="1031">ABC-Eval ist in der Lage, die Raten zu messen, mit denen Chatmodelle verschiedene thematische Fehler begehen.</sample>
    <sample id="1032">Zum Beispiel misst ABC EVL die Anzahl der Runden, in denen ein Chatbot seinen Partner ignoriert oder etwas sagt, das nicht relevant ist.</sample>
    <sample id="1033">widerspricht sich selbst oder seinem Partner. Halluziniert falsche Fakten oder verletzt die übliche Vernunft und zeigt das Modell Empathie oder scheitert es daran.</sample>
    <sample id="1034">Um zu bestimmen, welche Art der Bewertung am effektivsten ist, haben wir vier modernste Chatmodelle ausgewählt und sie anhand von 100 menschlichen Gesprächen pro Modell bewertet, wobei die ABC-Bewertung verwendet wurde.</sample>
    <sample id="1035">Zur Vergleichbarkeit haben wir diese Gespräche ebenfalls mit drei bestehenden Methoden bewertet: Likert-Ratings auf der Turnebene, Likert-Ratings auf der Dialogebene und paarweise Vergleiche.</sample>
    <sample id="1036">Für jede der bestehenden Methoden sammeln wir Bewertungen zu acht der am häufigsten gemessenen Aspekte der Dialoge, da dies die Standardpraxis für die Bewertung von Chatmodellen entlang mehrerer Dimensionen ist.</sample>
    <sample id="1037">Eine Analyse der Ergebnisse dieser Bewertung ergab, dass die Bewertungslabels ABCDE-Verhalten zuverlässiger sind als die von bestehenden Methoden, gemessen an der inter-Annotator-Übereinstimmung bei 100 doppelt gelabelten Gesprächen.</sample>
    <sample id="1038">Zusätzlich sind ABC-EVA-Labels prädiktiver für die Gesamtqualität der Konversation im Vergleich zu Metriken, die von bestehenden Methoden erzeugt werden, wie in dieser einfachen linearen Regressionsanalyse gezeigt.</sample>
    <sample id="1039">Zum Beispiel können Sie sehen, wie die Messung des Anteils von Wendungen zwischen Selbst- und Partner-Konflikten 5 % und 10 % der Gesprächsqualität bzw. erklärt. Während die durchschnittliche Likert-Konsistenzbewertung nur 4 % oder</sample>
    <sample id="1040">Schließlich haben wir überprüft, ob jede Evaluationsmetrik einen einzigartigen Aspekt der Chatqualität erfasst, indem wir eine schrittweise lineare Regression verwendeten.</sample>
    <sample id="1041">Sie können sehen, wie die Kombination aller ABC-Evaluationsmetriken über 25 % der Gesprächsqualität erklärt. Und wenn Sie eine Metrik nach der anderen entfernen, führt dies in den meisten Fällen zu einem Verlust einer beträchtlichen Menge an Informationen über die Qualität.</sample>
    <sample id="1042">Auf der anderen Seite erklärt die Kombination aller Level-Level-Likert-Maße viel weniger über die Qualität, und weniger dieser Maße tragen einzigartige Informationen.</sample>
    <sample id="1043">Dies ist ein zuverlässiges, informatives und differenziertes ABC-Evaluationsmatrix, das uns ermöglicht, konversationelle KI mit einer höheren Auflösung zu bewerten, als es frühere Methoden erreichen konnten.</sample>
    <sample id="1044">Man kann sehen, dass die Ergebnisse unseres Experiments zeigen, dass mehrere Herausforderungen noch bestehen und präzise quantifiziert wurden. Zum Beispiel haben wir festgestellt, dass die Debatten bei etwa 20 % ihrer Antworten logische Fehler aufweisen.</sample>
    <sample id="1045">Sie liefern in etwa 15 % der Antworten irrelevante Informationen und widersprechen sich oder ihrem Partner etwa 10 % der Antworten.</sample>
    <sample id="1046">Angesichts der rasanten Fortschritte in dem Feld könnten viele dieser AQR-Werte seit unserer Bewertung gesunken sein. Dies ist jedoch gerade umso wichtiger, zuverlässige und präzise Bewertungsmessgrößen für den Vergleich von Modellen zu entwickeln.</sample>
    <sample id="1047">Wir hoffen, dass ABC Eval von anderen in der Branche genutzt werden kann als ein bedeutsamer Schritt in diese Richtung, und wir freuen uns darauf, zu sehen, wie sich konversationelle KI in den kommenden Monaten weiterentwickelt.
Vielen Dank fürs Zuschauen.</sample>
    <sample id="1048">Emory University.</sample>
    <sample id="1049">CFT steht für "clean manually annotated samples".</sample>
    <sample id="1050">6</sample>
    <sample id="1051">Hallo, mein Name ist Kayo Yen und ich werde unsere Arbeit vorstellen, die mit dem Titel "Wann ist Übersetzung Kontext erforderlich? Eine datengesteuerte explorative Untersuchung" heißt. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandez, Emily Underwood-Martins und Graham Neuback erstellt.</sample>
    <sample id="1052">So, viele Übersetzungen im Kontext. Zum Beispiel, wie würden wir "Dr." in diesem Satz übersetzen?</sample>
    <sample id="1053">Wenn der vorherige Satz besagte, dass Dinge gefährlich werden könnten, wenn der Minister es herausfindet, dann bezieht sich More auf einen Spion. Aber wenn der vorherige Satz besagte, dass es alles sein könnte, Doktor, dann bezieht sich More auf eine Geburt.</sample>
    <sample id="1054">So, die Padding-Kontext, die Bedeutung des Wortes ändert sich und daher die Übersetzung ändert sich auch.</sample>
    <sample id="1055">Allerdings ist es ziemlich schwierig zu beurteilen, wie gut Modelle Fälle wie diesen bewältigen können. Erstens liegt dies daran, dass nur ein kleiner Teil der Übersetzung im Kontext steht, was globale Metriken wie Blue daran hindert, diese Übersetzungen zu erfassen.</sample>
    <sample id="1056">Und einige Leute haben eine gezielte Bewertung von Kontextabhängigen Übersetzungen vorgeschlagen, aber diese Ressourcen unterstützen nur begrenzte Arten von Kontextabhängigen Übersetzungen und eine begrenzte Menge an Sprachen. Da sie in der Regel auf Fachwissen und menschlicher Kuratierung basieren.</sample>
    <sample id="1057">In dieser Arbeit haben wir versucht, diese beiden Fragen zu beantworten: Erstens, wann erfordert eine Übersetzung Kontext, und zweitens, wie gut Modelle diese Fälle bewältigen.</sample>
    <sample id="1058">Um die erste Frage zu beantworten, haben wir mit der Messung der Menge an Wörtern begonnen, die von Kontext abhängig sind, in der Übersetzung.</sample>
    <sample id="1059">Und in der vorherigen Arbeit haben wir die Kontext-Information als Maß für die Verwendung von Kontexten in maschinellen Übersetzungsmodellen vorgestellt. Und dies wird gemessen, wie viel Information der Kontext C über die Ziel-Wortfolge Y hinsichtlich der Wortfolge X liefert.</sample>
    <sample id="1060">Du kannst dir CXM als die Informationsgewinnung vorstellen, indem du Kontext dem Modell gibst.</sample>
    <sample id="1061">In dieser Arbeit vergleichen wir X-SMI mit 2X-SMI, die Kontextnutzung auf Satz- oder Wortebene messen können. Wir können Wörter mit hohem P-SMI als solche betrachten, die für die Übersetzung Kontext benötigen.</sample>
    <sample id="1062">Nun analysieren wir Wörter mit hoher P-S-K-M-I, um Muster zwischen diesen Wörtern zu finden.</sample>
    <sample id="1063">Und wir führen unsere Analyse auf Transkripten von TED Talks durch, die von automatisch von Englisch in 14 verschiedene Sprachen übersetzt wurden.</sample>
    <sample id="1064">Bevor wir unsere Analysen auf drei verschiedenen Ebenen durchführen, betrachten wir die Teilwort-Tags mit hohem Mittelwert, nämlich "n", "r", "t", "m", "a", "h".</sample>
    <sample id="1065">Und dies ermöglicht uns, zum Beispiel doppelte Pronomen im Arabischen zu finden, die beide die hohe PS-e-i haben, und dies kann erklärt werden, weil das Englische keine doppelten Pronomen hat, also muss man den Kontext bestimmen, ob ein Pronomen doppelt ist, wenn es in das Arabische übertragen wird.</sample>
    <sample id="1066">Und ähnlich finden wir, dass bestimmte Sprachen auch Kontext benötigen, wenn wir die passende Verbform wählen. Wir suchen dann nach Wörterbucheinträgen, die eine hohe Häufigkeit haben über alle ihre verschiedenen Erscheinungsformen.</sample>
    <sample id="1067">Und dies hilft uns, Fälle wie den hier zu identifizieren, wo im Chinesischen die Kontraktion des Adjektivs verwendet werden muss, um sicherzustellen, dass Sie die gleiche Übersetzung innerhalb des Dokuments verwenden.</sample>
    <sample id="1068">Und ähnlich haben wir festgestellt, dass die katholische Kirche die Tradition der rechten Formalität unterstützt.</sample>
    <sample id="1069">Und schließlich betrachten wir verschiedene individuelle Token, die eine hohe PSI haben, und dies ermöglicht es uns, Phänomene zu identifizieren, die nicht wirklich durch das Wort selbst erfasst werden können, sondern eher in der statistischen Struktur ausgedrückt werden, wie zum Beispiel die Ellipsen-Lösung.</sample>
    <sample id="1070">Nun verwenden wir unsere Ergebnisse aus unserer Analyse, um einen Benchmarking-Standard für die Dokumenten-Niveau-Übersetzung zu entwerfen.</sample>
    <sample id="1071">Für jedes der fünf Diskursphänomene, die wir identifiziert haben, haben wir Tags erstellt, um auf eine nicht-sprachliche Weise Wörter zu identifizieren, die sich auf das Phänomen beziehen. Und wir nennen unsere Tags den multilinguellen Diskurs-Tag oder Muda-Tag.</sample>
    <sample id="1072">Wir können auch feststellen, dass verschiedene Sprachen unterschiedliche Proportionen dieses diskreten Phänomens haben.</sample>
    <sample id="1073">Wir verwenden den Mudentagger, indem wir den Tagger auf den parallelen Korpus anwenden, den wir für die Bewertung verwenden wollen, und wir wenden unsere Translationsmatrix der Wahl auf die kontextabhängigen Beispiele an, die der Mudentagger identifiziert hat.</sample>
    <sample id="1074">Und schließlich verwenden wir unseren Benchmarks als weitere Metrik, um verschiedene Modelle auf Dokumentebene zu bewerten.</sample>
    <sample id="1075">Zuerst einmal, wenn wir Korpus-basierte Metriken verwenden, so bei Blue finden wir, dass die kontextuelle diagnostische Modelle die beste Leistung haben.</sample>
    <sample id="1076">Wenn Sie Kontext verwenden, performen Modelle in Umgebungen besser. Und wenn Sie Wort-Aussage verwenden, haben Modelle mit und ohne Kontext vergleichbare Leistung.</sample>
    <sample id="1077">Dies ist ein Demonstrator, es ist schwierig, das beste Dokumentenlevel-Übersetzungssystem zu bestimmen, wenn man Korpus-Level-Metriken verwendet.</sample>
    <sample id="1078">Jetzt verwenden wir die Modellauswahl anhand von Benchmarking-Modellen und stellen fest, dass Modelle, die Kontext verwenden, für bestimmte diskursive Phänomene signifikant genauer sind, wie z. B. Formalität und lexikalische Kohäsion.</sample>
    <sample id="1079">Diese Modelle sind nicht viel besser als Modelle, die keine Kontextinformationen oder andere Phänomene wie Ellipsen, Pronomen und Verbformen verwenden. Dies deutet darauf hin, dass wir mehr Fortschritte bei der Dokumenten-Level-Übersetzung sehen müssen.</sample>
    <sample id="1080">Wir haben auch verschiedene kommerzielle Systeme verglichen, und unsere Benchmarks zeigen, dass DeepL im Allgemeinen genauer ist als Google Translate für die Dokumentenübersetzung.</sample>
    <sample id="1081">Zusammenfassend führen wir eine datengesteuerte Analyse über 14 Sprachpaare durch, um die ein Übersetzungsproblem zu identifizieren, das Kontext erfordert.</sample>
    <sample id="1082">Und dann verwenden wir unsere Feinheiten, um einen Benchmarking für die Dokumenten-Niveau-Übersetzung zu erstellen, was uns helfen kann, zu identifizieren, welche diskreten Phänomene Modelle gut oder nicht verarbeiten können, und welche Übersetzungssysteme gut für die Dokumenten-Niveau-Übersetzung sind.</sample>
    <sample id="1083">Danke sehr für Ihre Teilnahme.</sample>
    <sample id="1084">Justin John</sample>
    <sample id="1121">There is no name for the new method.</sample>
    <sample id="1122">The authors describe the "marked words" method as a technique to identify the words that distinguish marked groups from unmarked ones.</sample>
    <sample id="1123">University of Washington.</sample>
    <sample id="1124">Prague</sample>
    <sample id="1125">James Finch.</sample>
    <sample id="1126">4</sample>
    <sample id="1127">Corpus linguistics.</sample>
    <sample id="1161">The five methods for the first research question are: WLS, WLS-M, WLS-M-S, WLS-M-S-P, and WLS-M-S-P-S.</sample>
    <sample id="1162">Biomedical and clinical documentation stream tasks.</sample>
    <sample id="1226">4 GB von Wikipedia.</sample>
    <sample id="1227">Adam Szpyrkowski.</sample>
    <sample id="1228">We found that performance degrades with larger temporal gaps, confirming our hypothesis that the main cause of the performance drop is temporal drift.</sample>
    <sample id="1269">The tokens are not ordered correctly.</sample>
    <sample id="1270">Die Autoren empfehlen eine erhöhte Transparenz bei Methoden zur Vorurteilsminderung, weil positive Stereotypen beispielsweise auf einer übermäßigen Wertorientierung oder anderen Anti-Stereotypen-Methoden beruhen, die zu schädlichen Mustern führen können.</sample>
    <sample id="1271">Inakzeptable Minimalpaareingaben sind akzeptable Sätze, gefolgt von inakzeptablen Sätzen oder grammatikalisch falschen Sätzen.</sample>
    <sample id="1272">The authors used the weight and tokenizer of PubMedBERT to train on a 4GB dataset of nachos and showed comparable results to those from obtaining a 4GB from Scratch.</sample>
    <sample id="1273">Innerer Annotator-Übereinstimmung auf 100 doppelt gelabelten Gesprächen.</sample>
    <sample id="1274">Wikipedia.</sample>
    <sample id="1275">Die Autoren gehören der Universität von Toronto an.</sample>
    <sample id="1276">MultiInstruct unterscheidet sich von anderen Benchmarks dadurch, dass es sich auf die Verbesserung der Generalisierung von Instruction Tuning auf multimodale Aufgaben konzentriert, während frühere Arbeiten sich hauptsächlich auf sprachbasierte Aufgaben konzentrierten.</sample>
    <sample id="1277">Es sind mehrere Autoren beteiligt.</sample>
    <sample id="1278">The binary coordination is the process of measuring length in characters, the first column in syllables, the middle column, and in words, the right column.</sample>
    <sample id="1279">The prompts used in this study averaged 10 seconds in length.</sample>
    <sample id="1280">Die Ergebnisse zeigen, dass kleinere T5-Modelle möglicherweise nicht so gut abschneiden wie größere Modelle bei der Generierung von Texten.</sample>
    <sample id="1281">Hallo, ich bin Janis Lawack, ein Entwickler, der derzeit an einem robusten Sprachtrainingsmodell in französischer Sprache für Bio- und klinische Anwendungen arbeitet.</sample>
    <sample id="1282">In dieser Präsentation werden wir zunächst über Sprachmodellierung im Gesundheitswesen sprechen. Dann werden wir die Hauptbeiträge unseres Artikels vorstellen.</sample>
    <sample id="1283">Wir haben das erste biomedizinische Modell in Französisch eingeführt, namens Docteur Bert, welches auf Roberta basiert und auf Natches trainiert, welches ein Datensatz von medizinischen Kronen-Daten aus dem</sample>
    <sample id="1284">Wir haben auch einen Vergleich von Modellen mit mehreren Punkt-Präzisions-Einstellungen und Datenquellen. Dann präsentieren wir unsere Ergebnisse an 11 verschiedenen biomedizinischen und klinischen Aufgaben im Screening-Task.</sample>
    <sample id="1285">Zusammenfassend können wir sagen, dass die Experimente vielversprechend sind und weitere Details zur Zugänglichkeit von Daten bereitstellen.</sample>
    <sample id="1286">Seit seiner Veröffentlichung im Jahr 2018 ist BERT zu einem der effektivsten Ansätze zur Lösung von Aufgaben im Bereich der Verarbeitung natürlicher Sprache geworden und bietet im Vergleich zu historischen statischen und kontextualisierten Methoden wie Word2Vec oder GloVe einen großen Leistungszuwachs.</sample>
    <sample id="1287">Sinsens ist mit äh, dieses Modell hat sich in vielen anderen Sprachen angepasst, wie in Französisch mit Camembert, und anderen äh Domänen wie Biomedizinisch mit PubMed und BioBERT, und auf klinisch mit ClinicalBERT. Aber meistens in Englisch.</sample>
    <sample id="1288">Spezialmodelle für andere Sprachen sind oft auf kontinuierlichem Lernen basierend, aufgrund des Mangels an Indem.</sample>
    <sample id="1289">Allerdings hatte Franz kein offenes Stiftmodell für die medizinische und chirurgische Schrift.</sample>
    <sample id="1290">Wir fragen uns also, welche die am besten geeignete Datensammlung für eine breite Palette von Anwendungen ist, und diese Daten sind eine gute Ersatz für klinische Daten.</sample>
    <sample id="1291">Bitte stellen Sie die Frage.</sample>
    <sample id="1292">Nachdem wir uns gefragt haben, wie viel Daten wir benötigen, um ein Spezialmodell auf französischen Daten zu trainieren. Ist es für Gigabyte, Edgbgabyte oder mehr?</sample>
    <sample id="1293">Zunächst ist die Frage, wie wir vier verschiedene Modelle von Grund auf trainieren und vergleichen. Die erste Version von DoctorBERT hat 7 Gigabyte an Nachos. Die zweite Version hat 4 Gigabyte an Set of Nachos.</sample>
    <sample id="1294">Eine kleinere Version von BERT, die ein klinisches Modell ist, wir verwenden 4 Gigabyte von Sentencias, die von klinischen Daten stammen. Und eine fein abgestimmte Version von BERT, wir haben eine Mischung aus 4 Gigabyte von Texten und 4 Gigabyte von klinischen Daten.</sample>
    <sample id="1295">Zusätzlich zu dieser Vergleiche führen wir einen Stream von Daten über das kontinuierliche Training ein, um die Auswirkungen der Trainingsstrategie zu analysieren.</sample>
    <sample id="1296">Ein Bison wurde auf einem 4-Gigabyte-Set von Nachos trainiert, und ein anderer Bison wurde ebenfalls auf einem 4-Gigabyte-Set von klaren Nachos trainiert.</sample>
    <sample id="1297">Anfänglich wird ein Basismodell eines englischen Sprachmodells, BERT, auf einem vorherigen Datensatz von Nachrichten trainiert. Insgesamt haben wir sieben Modelle.</sample>
    <sample id="1298">Die Evaluation von sieben Modellen zeigt, dass sowohl öffentliche als auch private Modelle Aufgaben wie Objekterkennung, Klassifizierung, Part-of-Speech-Tagging und Frage-Antwort-Systeme bewältigen können.</sample>
    <sample id="1299">Das ist Modell im Vergleich zu sechs-Bands-Modell, welche 108 Gigabyte, 4 Gigabyte, 64 Gigabyte, 128 Gigabyte, 256 Gigabyte und 1 Terabyte unterstützen.</sample>
    <sample id="1300">Die Auswahl des besten Modells für diese Aufgabe basiert auf Daten desselben Typs, die dem Modell bereits zur Verfügung stehen.</sample>
    <sample id="1301">Allerdings können wir diese Daten aus äh wir können Daten aus äh ursprünglichen Quellen beobachten, die scheinen zuverlässiger zu sein. Wir haben auch beobachtet, dass die Verwendung mehr Daten zu besseren Ergebnissen führt.</sample>
    <sample id="1302">In the world, from scratch, pretending seemed to obtain higher performance on most of the tech.</sample>
    <sample id="1303">Allerdings hat unser Experiment gezeigt, dass die Verwendung der Gewichtung und Tokenisierung von PubMed-Wörtern beim Training auf einem 4 GB großen Datensatz ähnliche Ergebnisse wie die Verwendung von Ref-Doktor-Wörtern für 4 GB Daten liefert.</sample>
    <sample id="1304">Dies ist nicht der Fall, da Modul basiert auf Camembert-Gewichten und Tokenizer, die unter Stabilität leiden.</sample>
    <sample id="1305">Schließlich, als Schlussfolgerung, äh unser System bietet bessere Leistungen als das Nine of the Eleven-System, übertrifft global die Ergebnisse des generischen Modells hier, kann aber</sample>
    <sample id="1306">Wir sehen, dass spezialisierte Daten besser sind, mehr spezialisierte Daten sind besser, aber es skaliert nicht.</sample>
    <sample id="1307">Alle vortrainierten Modelle, die von Natos erhalten werden, sind frei verfügbar und auf dem Jugendface und alle Trainingsskripte sind auf unserer GitHub-Repository verfügbar.</sample>
    <sample id="1308">So, vielen Dank für die Präsentation. Wir freuen uns auf die nächsten Schritte in der Sitzung.</sample>
    <sample id="1309">Die Arbeit untersucht verschiedene Lernstrategien, darunter das Trainieren und Vergleichen von vier verschiedenen Modelle von BERT und das Trainieren eines Stream-Modells zur Analyse des Einflusses auf die semantische Leistung.</sample>
    <sample id="1310">The factor of overfitting specifically due to test reuse is not observed.</sample>
    <sample id="1311">Die Qualität der Vereinfachung wurde durch das Feintuning von Sprachmodellen zur Produktion von vereinfachtem Text aus komplexem Eingabetext beurteilt.</sample>
    <sample id="1312">Yes, language models do have varying political biases.</sample>
    <sample id="1313">Hallo, mein Name ist Matthias Landemacher und heute gebe ich Ihnen eine kurze Einführung in unsere Arbeit zur kompositionalen Generalisierung ohne Bäume, unter Verwendung von Multi-Set-Tagging und latenten Permutationen.</sample>
    <sample id="1314">Dies ist eine Zusammenarbeit mit meinen Beratern Alexander Koller und Eventi Dott.</sample>
    <sample id="1315">Kompositionelle Generalisierung kann als die Fähigkeit eines Lerners verstanden werden, tiefere Rekursionen und unbekannte Kombinationen von Phrasen zu verarbeiten, die einzeln während des Trainings gesehen wurden.</sample>
    <sample id="1316">Im Kontext der semantischen Analyse könnte das Testen der kompositorischen Verallgemeinerung so aussehen: Wie üblich haben wir einen Trainingssatz von Äußerungen. In diesem Fall schlief das Mädchen und Mary wusste, dass das Mädchen schlief.</sample>
    <sample id="1317">Diese Satzstrukturen werden mit logischen Formen gepaart, die die Kernaspekte ihrer Bedeutung repräsentieren.</sample>
    <sample id="1318">Im Gegensatz zur Standard-Maschinellen-Lern-Bewertung stammt das Testset nicht aus derselben Verteilung, sondern enthält strukturell und inlogisch form.</sample>
    <sample id="1319">In diesem Beispiel hat das Modell während des Trainings flache Rekursion gezeigt und wurde auf einem Beispiel mit tiefer Rekursion getestet.</sample>
    <sample id="1320">Neuronale Sequenz-zu-Sequenz-Modelle haben Schwierigkeiten mit dieser Art der Out-of-Distribution-Generalisierung und erzeugen oft Ausgaben, die sich von der Eingabe lösen.</sample>
    <sample id="1321">Insbesondere scheitern sie oft daran, die systematischen Korrespondenzen zwischen Eingabe und Ausgabe zu reproduzieren, wie diese im Beispiel dargestellt sind.</sample>
    <sample id="1322">Ein beliebter Ansatz zur Bewältigung dessen ist die Integration von Bäumen in die Modelle.</sample>
    <sample id="1323">Die Bäume sollen den kompositorischen Prozess erfassen, der die Äußerungen mit der logischen Form in Verbindung bringt.</sample>
    <sample id="1324">Dies war ein Tippfehler, aber Bäume werden normalerweise nicht gegeben, Sie müssen einige</sample>
    <sample id="1325">Dies kann kompliziert sein und manchmal ein rechenintensiver Prozess. Typischerweise beinhaltet dies erheblichen formalisierungspezifischen Vorverarbeitung der logischen Formen. Zum Beispiel, um Variablen zu handhaben.</sample>
    <sample id="1326">Das Beschaffen von Bäumen kann auch spezielle Grammatik-Erkennungsverfahren beinhalten.</sample>
    <sample id="1327">In dieser Arbeit verwenden wir keine Bäume und führen ein neues Sequenz-zu-Sequenz-Modell ein, das direkt die Korrespondenz zwischen Fragmenten der Eingabe und Fragmenten der Ausgabe modelliert.</sample>
    <sample id="1328">Zum ersten Mal zeigen wir eine starke Generalisierung auf tiefere Rekursion, ohne auf Bäume zurückzugreifen.</sample>
    <sample id="1329">Ich bin ein Ansatz, der das Ergebnis aus der Eingabe in zwei Schritten vorhersagt.</sample>
    <sample id="1330">Erstens versehen wir jeden Eingabe-Token mit einem ungeordneten Multiset von Token, die im Ausgang erscheinen werden.</sample>
    <sample id="1331">Nach dem ersten Schritt haben wir die richtigen Token, aber sie sind nicht die richtige Reihenfolge.</sample>
    <sample id="1332">Deshalb verwenden wir im zweiten Schritt ein anderes Modell, um eine Permutation vorherzusagen, um sie in der richtigen Reihenfolge zu platzieren.</sample>
    <sample id="1333">Wir stellen eine neue Methode zur Vorhersage einer Permutation vor, die keine harten Beschränkungen auf die möglichen Permutationen legt. Dies macht unseren Ansatz recht flexibel und ausdrucksstark.</sample>
    <sample id="1334">Konzeptuell funktioniert unser Permutationsmodell ungefähr wie die</sample>
    <sample id="1335">Wir gehen von links nach rechts durch die Ausgabe und bestimmen, welcher Mengen-Token an jeder Position eingefügt werden soll. Für die erste Ausgabeposition wählen wir einfach eins, wie hervorgehoben in der Tabelle.</sample>
    <sample id="1336">Dann springen wir zum nächsten Multiset-Token, um das zweite Token in der Ausgabe zu bestimmen.</sample>
    <sample id="1337">Um den dritten Token in der Ausgabe auf ähnliche Weise zu bestimmen, springen wir zu einem anderen Multiset-Token. Wir setzen diesen Prozess fort.</sample>
    <sample id="1338">Bis jeder Token der ersten Phase genau einmal besucht wurde.</sample>
    <sample id="1339">Um Ihnen einen Vorgeschmack auf die Ergebnisse der experimentellen Ergebnisse zu geben, vergleichen wir unsere Methode mit anderen Treeless-Modellen auf dem COG-Benchmark. Unser Modell übertrifft die anderen um einen großen Spielraum bei der Generalisierung auf tiefere Rekurrenz.</sample>
    <sample id="1340">Eine andere Art der strukturellen Renovierung bleibt sehr herausfordernd.</sample>
    <sample id="1341">In unserer Arbeit haben wir einige interessante technische Herausforderungen gelöst.</sample>
    <sample id="1342">Zuerst einmal ist die Ausrichtung zwischen Eingabe und Ausgabe im Trainingsdatensatz nicht gegeben. Infolgedessen wissen wir für ein gegebenes Token nicht, aus welchem Multiset es stammt, was eine Herausforderung für die Trainingsdaten darstellt.</sample>
    <sample id="1343">In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the trachea.</sample>
    <sample id="1344">Die Permutationsmethode ist sehr flexibel, bringt aber die Herausforderung mit sich, die höchste Punktzahl zu finden und P zu maximieren. Das liegt daran, dass dies mit dem Traveling Salesman Problem zusammenhängt.</sample>
    <sample id="1345">Wir approximieren dies mit einer GPU-freundlichen kontinuierlichen Relaxation, die uns auch erlaubt, rückwärts zu propagieren durch die Lösung und die linguistisch plausibleren Permutationen zu lernen.</sample>
    <sample id="1346">Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen angehen möchten, schauen Sie sich bitte unser Papier an oder kommen Sie zu unserem Posten.</sample>
    <sample id="1347">Cognitive dissonance is two beliefs or actions that are inconsistent.</sample>
    <sample id="1348">GPT-4</sample>
    <sample id="1349">Cumulative training performs equal to or better than iterative training for active learning.</sample>
    <sample id="1350">Sara Babbi</sample>
    <sample id="1351">Die Daten für die MuDa-Benchmark stammen von Transkripten von TED Talks, die von Englisch in 14 verschiedene Sprachen übersetzt wurden.</sample>
    <sample id="1385">Matthias Lendehammer.</sample>
    <sample id="1386">Sprachübergreifender Transfer bezieht sich auf die Fähigkeit eines Modells, Wissen aus einer Sprache auf eine andere Sprache zu übertragen.</sample>
    <sample id="1387">Stuttgart University.</sample>
    <sample id="1388">The authors use simultaneous speech translation results on graphs, with blue on one side measuring translation quality and average lagging on the other side. They also consider the computationally aware average lagging that accounts for the models' computational time to predict the output.</sample>
    <sample id="1389">Hallo zusammen, ich bin Makshata und heute präsentieren mein Kollege Martin und ich unsere Arbeit, das Kitmaster. Sie werden Wissen aus mehreren Quellen integrieren. Diese Arbeit ist eine Zusammenarbeit zwischen der Macquarie University, Mela und Microsoft Research.</sample>
    <sample id="1390">Natürliche Sprachmodelle stützen sich auf eine Vielzahl von Wissensquellen, wie z. B. dem Wissen, das in ihren Parametern enthalten ist, das in der Regel durch Vortraining erworben wird, und dem Wissen, das in den Eingaben bei der Inferenz gegeben wird.</sample>
    <sample id="1391">Kürzliche Arbeiten in Aufgaben wie Fragenbeantwortung zeigen, dass Modelle vortrainiertes Wissen nutzen können, um die Aufgabe zu lösen.</sample>
    <sample id="1392">Die National Language of Pakistan erfordert oft Wissen, das auch in der Hindi-Sprache bereitgestellt wird.</sample>
    <sample id="1393">Zum Beispiel, in dem Satz "John sah den neu gewählten Präsidenten im Fernsehen",</sample>
    <sample id="1394">Vorabparameter können Informationen über den Präsidenten 2 und den Präsidenten enthalten, aber sie können nicht zuverlässig wissen, wer diese spezifische Entität John ist oder wer der neue Präsident ist, da der Präsident sich geändert haben könnte seit der Tragödie.</sample>
    <sample id="1395">Daher benötigen erfolgreiche Modelle für wissensintensive NLP-Aufgaben die Fähigkeit, sowohl vortrainiertes als auch inferenzzeitabhängiges Wissen zu integrieren und zu nutzen.</sample>
    <sample id="1396">In dieser Arbeit schlagen wir einen diagnostischen Test zur Wissensintegration vor.</sample>
    <sample id="1397">Wir stellen eine Aufgabenstellung zur Korreferenzlösung vor, die darauf abzielt, die Fähigkeit zu prüfen, Wissen aus verschiedenen Quellen abzurufen. Wir bewerten den Datensatz mit menschlichen Studienteilnehmern und etablieren eine Korreferenzlösung.</sample>
    <sample id="1398">Servin ist ein Richter. Kia ist ein Bäcker. Servin und Kia trafen sich im Park. Nach einem langen Arbeitstag, bei dem er Fälle in einem Gerichtsbau entschied, freute er sich, sich zu entspannen.</sample>
    <sample id="1399">Die Aufgabe besteht darin, die korrekte Entität zu identifizieren, auf die das Pronomen er sich bezieht, was in diesem Fall der Mann ist.</sample>
    <sample id="1400">Die Auflösung eines gegebenen Pronoms erfordert zwei Arten von Informationen. Erstens, Entitätsspezifisches Wissen, wie z. B. "Servile ist ein Gericht". Zweitens, Weltwissen, wie z. B. "Gerichte entscheiden Fälle im Recht".</sample>
    <sample id="1401">Im Allgemeinen wird Hintergrundwissen während des Vortrainings großer Sprachmodelle erlernt, während entitätsspezifisches Wissen typischerweise durch Inferenz erworben wird.</sample>
    <sample id="1402">Die Verfügbarkeit von Einzel- oder Mehrfachquellen für Informationen kann die Suche nach Informationen erleichtern.</sample>
    <sample id="1403">Wir haben drei Einstellungen von Kemos gefunden. Zuerst müssen wir die Einstellung "Hintergrundvorabtraining" aktivieren. Hintergrundwissen wird als verfügbar für das Vorabtraining angenommen.</sample>
    <sample id="1404">Zweitens gibt es die Backup-Einstellungen. Die Backups sind sowohl bei der Vorabzeit als auch im Freien verfügbar. Schließlich gibt es die Backup-Einstellungen. Beide Datentypen sind nur im Freien verfügbar.</sample>
    <sample id="1405">Dieses Lastsetting ist besonders interessant. Es simuliert den Fall, dass die Hintergrundkenntnisse notwendig sind, um die Aufgabe zu lösen. Dies ist nicht Teil der Pre-trained Data of Models. Zum Beispiel, weil neue Berufe entwickelt wurden, seit die Zeit von Pre-trained</sample>
    <sample id="1406">Hier ist ein Beispiel dafür, wie wir die Verfügbarkeit von Effekten kontrollieren können, die zu einer psychischen Erkrankung führen.</sample>
    <sample id="1407">In der Hintergrundvorgabe nehmen wir an, dass die Hintergrundkenntnisse, die Politiker bei der Wahl von Sitzen im Parlament enthalten, in den vorherigen Parametern enthalten sind. In verschiedenen Kontexten stellen wir die entitätsspezifische Kenntnis, die Chester als Politiker</sample>
    <sample id="1408">In der Hintergrund- und Basis-Einstellung werden nicht nur die Antispezifischen, sondern auch die Hintergrundkenntnisse über Politiker im Kontext der Interessenvertretung bereitgestellt.</sample>
    <sample id="1409">In der Vergangenheit und im aktuellen Setting wurde die Berufsgruppe "Militär" anstelle von "Politiker" gewählt, da Militär weniger wahrscheinlich in den Präventiv-</sample>
    <sample id="1410">Wir validierten den Datensatz sowohl mit menschlichen Studien als auch mit etablierten Framework-Lösungsmodellen. In dieser Abbildung zeigen wir die Ergebnisse der besten performenden Modelle und der am schwierigsten zu verarbeitenden Variante des Hintergrund-Pretrained-Modells.</sample>
    <sample id="1411">Wenn du mit Auto-Task trainierst, ist die Leistung nicht gut. Wenn du mit Kitmus trainierst, ist sie jedoch deutlich besser als bei zufälligen Entscheidungen.</sample>
    <sample id="1412">Dies deutet darauf hin, dass beim Training und der Generierung einer Referenzlösung das Set möglicherweise nicht vollständig ist. Man könnte lernen, Oberflächenhinweise auszunutzen, aber sie sind nicht nützlich beim Testen von Kit-Modulen, da solche Hinweise entfernt wurden.</sample>
    <sample id="1413">Experimente mit der Fiktion zeigen, dass selbst die besten Vorhersagemodelle nicht zuverlässig neue Erkenntnisse integrieren können, sondern lediglich auf frühere Daten zurückgreifen.</sample>
    <sample id="1414">Einige der Hauptursachen für den Papierverbrauch sind, dass viele Referenzen in modernen Lösungen scheinbar aufgrund von Wissenslücken aus verschiedenen Quellen ohne spezifische Trainingsdaten stammen. Allerdings können einige Modelle mit spezifischem Training erfolgreich Wissen aus mehreren Quellen integrieren.</sample>
    <sample id="1415">Selbst die besten leistungsstarken Modelle scheinen Schwierigkeiten bei der zuverlässigen Integration von Back-of-the-Knowledge zu haben, die nur zur Eingabezeit präsentiert wird.
Wenn Sie mehr Details wünschen, sehen Sie bitte unsere Arbeit und schauen Sie sich den Datensatz und den Code auf GitHub an.
Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="1416">Die Nachteile baumbasierter Methoden sind, dass sie oft eine komplizierte und rechenintensive Verarbeitung der logischen Formeln erfordern, insbesondere zur Behandlung von Variablen-Symbolen.</sample>
    <sample id="1417">The authors are affiliated with Cornell University.</sample>
    <sample id="1418">Hallo, ich bin Mara, und heute werden wir über unsere Papier-markierte Personen sprechen. Die Verwendung natürlicher Sprachaufforderungen zur Messung der Stärken verschiedener Arten von Sprachmodellen. Diese Arbeit wurde in Zusammenarbeit mit Essendermush und Danceroft durchgeführt.</sample>
    <sample id="1419">In den letzten Jahren haben viele dokumentiert, dass die Prävalenz von sozialer Verzerrung und Stereotypen in großen Sprachmodellen (LLMs) festgestellt wurde.</sample>
    <sample id="1420">Trotzdem haben diese Maßnahmen verschiedene Einschränkungen. Sie basieren in der Regel auf manuell erstellten Datensätzen, die sehr zeitaufwendig zu pflegen sind.</sample>
    <sample id="1421">Und sie messen in der Regel nur sehr spezifische Stereotypen, was bedeutet, dass sie sich nicht gut auf andere Demografien oder Kontexte übertragen lassen, oder sie erfassen einfach sehr allgemeine, breite Assoziationen, wie negative Assoziationen mit bestimmten Gruppen.</sample>
    <sample id="1422">Darüber hinaus wird in der meisten Arbeit im Raum Intersektionalität nicht berücksichtigt, die Vorstellung, dass vielfältige soziale Identitäten sich durch verschiedene Diskriminierungsformen verstärken und einzigartige Formen der Ungleichheit hervorbringen können.</sample>
    <sample id="1423">Um diese Einschränkungen zu überwinden, verlassen wir uns auf die Annahme, dass diese neueren instruktionsangepassten LLMs sehr gut darin sind, Anweisungen zu befolgen und unpräzise zu sein.</sample>
    <sample id="1424">So können wir dem Modell bitten, eine Persona zu generieren, die eine Darstellung einer imaginären Person ist, indem wir einen Prompt wie "Stell dir vor, du bist eine asiatische Frau. Beschreibe dich selbst." verwenden.</sample>
    <sample id="1425">Und wir können sofort sehen, dass dies für jede Demografie sehr gut anwendbar ist, denn wir können einfach jeden Geschlechtsmarker angeben, den wir möchten, in diesen Prompt.</sample>
    <sample id="1426">Hier sind einige Beispiele für Generierungen von GPT-4:</sample>
    <sample id="1427">Sofort sehen wir, dass die Ausgaben nicht übermäßig negativ oder toxisch im traditionellen Sinne dieser Wörter sind.</sample>
    <sample id="1428">Es gibt einige interessante Muster.</sample>
    <sample id="1429">Die asiatische Frau wird als unaufmerksam dargestellt, die mittlere östliche Frau wird mit Wörtern wie exotisch bezeichnet und als faszinierende Region erwähnt.</sample>
    <sample id="1430">Und sowohl die Frau von Farbe-Personen als auch die weiße Mann-Personen machen Anspielungen auf die Abstammung, während die weiße Mann-Person nichts davon hat.</sample>
    <sample id="1431">Um diese Muster zu erfassen, hat unsere Methode zwei Teile. Der erste Teil ist die Generierung dieser Personas.</sample>
    <sample id="1432">Diese Prompts wurden generiert, inspiriert von einer Studie, in der diese Prompts menschlichen Probanden gegeben wurden. Dabei stellten sich heraus, dass die Probanden auch subtile Rassistische Stereotypen aufzeigen konnten.</sample>
    <sample id="1433">Und außerdem ermöglicht dies einen direkten Vergleich zwischen unseren generierten Personas und den von Menschen geschriebenen Antworten.</sample>
    <sample id="1434">Die zweite Phase ist Markierung, eine Methode, um die Wörter zu identifizieren, die markierte Gruppen von nicht markierten Gruppen unterscheiden, was ich kurz erläutern werde.</sample>
    <sample id="1435">Der Vorteil davon ist, dass wir sehr spezifische Textmuster ohne auf eine bestimmte Lexikon-Konfiguration angewiesen zu sein, erhalten.</sample>
    <sample id="1436">So verwendet die Marktwörter die soziolinguistische Vorstellung von Markentyp, die besagt, dass es einen unmarkierten Standard gibt und jede Gruppe, die sich von diesem Standard unterscheidet, linguistisch markiert ist.</sample>
    <sample id="1437">Zum Beispiel wird das Wort „Mann“ oder Entschuldigung, das Wort „Krieger“ üblicherweise mit Männern in Verbindung gebracht. Wenn Menschen also eine Kriegerin beschreiben, werden sie normalerweise „eine Frau-Kriegerin“ spezifizieren und den Begriff mit „Frau“ markieren.</sample>
    <sample id="1438">Und breiter gesagt sind dominante Gruppen in der Gesellschaft sowohl sprachlich als auch sozial unmarkiert, während marginalisierte Gruppen in der Regel markiert werden.</sample>
    <sample id="1439">In unserem Verfahren bestimmen wir zunächst, welche unmarkierten und markierten Gruppen es gibt.</sample>
    <sample id="1440">Und dann vergleichen wir die Personen mithilfe der Methode der Gewichtung der häufigsten Wörter, die im Grunde genommen die Top-Wörter für jede markierte Person identifiziert.</sample>
    <sample id="1441">Zum Beispiel für die Personas von schwarzen Frauen würden wir Fighting Words und die Logos-Verhältnisse gegen sowohl weiße Personas als auch männliche Personas vergleichen, da diese die beiden entsprechenden unmarkierten Gruppen sind.</sample>
    <sample id="1442">Nun haben wir einige Ergebnisse. Zuerst verwenden wir eine Art von Störgeräuschen, und wir stellen fest, dass die generierten Personen viel mehr Störgeräusche enthalten als die von Menschen geschriebenen.</sample>
    <sample id="1443">Allerdings, wenn wir uns die Verteilung der Wörter in Lexicon ansehen, finden wir eine sehr andere Konfiguration.</sample>
    <sample id="1444">Während die generierten Personas viel höhere Raten von Luxuswörtern aufweisen, haben die von Menschen geschriebenen Wörter einen viel breiteren Verteilung der Wörter, während die Stereotypwörter, die in den generierten Personas enthalten sind, wirklich nur die Wörter groß, athletisch sind.</sample>
    <sample id="1445">Ich bin wirklich nur das Positive, zumindest das Nicht-Negative.</sample>
    <sample id="1446">Und tatsächlich fängt die Lexikon nicht wirklich viele der schädlichen Muster ein, die wir in den früheren Folien gesehen haben. Stattdessen werden wir die Ergebnisse unserer markierten Wörter-Methode verwenden, um zu zeigen, wie diese scheinbar positiven Wörter Stereotypen und essentialisierende Narrative fördern.</sample>
    <sample id="1447">In unserer Analyse überprüfen wir, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln.</sample>
    <sample id="1448">Erste Wortgruppen, die die Top-Wörter umfassen, sind Dinge wie Kultur, Tradition, Gruppe und exotisch. Und diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und unterscheiden sie von der weißen Norm.</sample>
    <sample id="1449">Dies trägt zu einer langen Tradition von Diskriminierung und Andeutung für diese Schädlichkeit bei.</sample>
    <sample id="1450">Darüber hinaus gibt es viele gängige Klischees, die sich in diesen Worten widerspiegeln, insbesondere für Frauen von Farbe. So beispielsweise enthalten Wörter, die eine Latina-Frau beschreiben, Dinge wie lebendig und kurvig.</sample>
    <sample id="1451">was kann ich mit einem Tropfen Tropikalisierung verknüpfen? Für asiatische Frauen sind Wörter wie Petite und delikat und seidig.</sample>
    <sample id="1452">Die Verbindung zu einer langen Geschichte der asiatischen Frauen, die als hypersexuell, sehr passiv und unterwürfig angesehen werden, ist ein weit verbreitetes Stereotyp.</sample>
    <sample id="1453">Und schließlich für eine schwarze Frau sehen wir, dass einige der Top-Wörter Dinge wie stark und widerstandsfähig sind.</sample>
    <sample id="1454">Dies verbindet sich mit einem Archetyp, den Menschen den starken schwarzen Frau-Archetyp genannt haben, und obwohl es auf den ersten Blick positiv klingt,</sample>
    <sample id="1455">Es gibt Studien, die zeigen, dass diese Art von Archetyp tatsächlich sehr schädlich ist, da er diese Demografie unter großen Druck setzt, widerstandsfähig und stark gegen suizidale Hindernisse zu sein.</sample>
    <sample id="1456">Stattdessen werden diese Hindernisse nicht aktiv angegangen, sondern die Menschen werden unter Druck gesetzt, sie zu überwinden, was zu sehr negativen Gesundheitsfolgen für diese Menschen und anderen führt.</sample>
    <sample id="1457">Im Allgemeinen stellen wir fest, dass die Wörter für jede Markengruppe im Wesentlichen eine essentialisierende Erzählung widerspiegeln.</sample>
    <sample id="1458">Basierend auf diesen Mustern können wir drei Empfehlungen für Modellbesitzer ableiten:</sample>
    <sample id="1459">Zuerst sollten wir als Forscher positive Stereotypen und essentialisierende Narrative angehen. Wir sollten auch einen intersektionalen Blickwinkel verwenden, um Vorurteile und Schäden zu untersuchen, denn es gibt viele Dinge, die übersehen werden könnten, wenn wir das nicht tun.</sample>
    <sample id="1460">Und schließlich sollte es wirklich mehr Transparenz über Bias-Minderungsmaßnahmen geben.</sample>
    <sample id="1461">Weil zum Beispiel diese positiven Stereotypen, wir wissen nicht, ob es wegen irgendeiner Art von komischen</sample>
    <sample id="1462">Übermäßig übermäßige Wertvorstellungen, die stattfinden oder vielleicht einige andere, wie z. B. Anti-Stereotypen-Methoden, die zu diesen verheerenden Mustern führen.</sample>
    <sample id="1463">Wir können keine Annahmen treffen oder das weiter untersuchen, ohne mehr Transparenz.</sample>
    <sample id="1464">Vielen Dank fürs Zuhören. Ähm, ich hatte eine gute Zeit.</sample>
    <sample id="1465">Hallo zusammen, mein Name ist Jingwei Yi von der Universität für Naturwissenschaften und Technologie von China.</sample>
    <sample id="1466">Es ist mir eine Freude, ein kurzes Werbevideo für Papier zu erstellen. Kopieren Sie mein Modell? Schützen Sie das Urheberrecht großer Sprachmodelle für die Einbettung und den Dienst.</sample>
    <sample id="1467">Lass uns zuerst den Hintergrund zu den Einwanderungsdiensten vorstellen.</sample>
    <sample id="1468">Derzeit sind große Sprachmodelle wie GPT, Llama und PaLM außergewöhnlich in der natürlichen Sprachverständnis- und -generierung.</sample>
    <sample id="1469">Inpainting als Service ist einer der Dienste, die auf großen Sprachmodellen aufbauen, um verschiedene Aufgaben zu unterstützen.</sample>
    <sample id="1470">Beispielsweise bietet OpenAI eine GPT-basierte Einbettung namens</sample>
    <sample id="1471">Allerdings haben aktuelle Arbeiten gezeigt, dass der Angreifer den Modell möglicherweise durch das Lernen aus dem Embedding und die Bereitstellung ähnlicher Dienste stehlen kann. Daher ist es notwendig, das Urheberrecht des Embeddings zu schützen.</sample>
    <sample id="1472">Um das Urheberrecht von Inhaltsanbietern zu schützen, ist eine Lösung, eine Wasserzeichenmarke im bereitgestellten Dienst zu platzieren und zu erkennen, ob ein anderer Dienst diese Wasserzeichenmarke enthält.</sample>
    <sample id="1473">Die Wasserzeichenmethode muss die folgenden Eigenschaften erfüllen: Erstens muss die Methode auf die Einbettung von Diensten anwendbar sein. Zweitens darf das Wasserzeichen die Nützlichkeit der bereitgestellten Einbettung nicht beeinträchtigen.</sample>
    <sample id="1474">Die Wasserzeichen sollten für den Angreifer sichtbar sein, und der Angreifer kann das Wasserzeichen leicht entfernen.</sample>
    <sample id="1475">Schließlich muss das Modell für die Angriffsservices transportierbar sein.</sample>
    <sample id="1476">Existierende Wörter können grob in vier Kategorien unterteilt werden:</sample>
    <sample id="1477">Allerdings ist diese Methode entweder nicht auf die Einbettung von Dienstleistungen anwendbar oder es mangelt an Portierbarkeit.</sample>
    <sample id="1478">Daher schlagen wir in dieser Arbeit einen Backdoor-basierten Wasserzeichenverfahren vor, das auf die Einbettung von Informationen angewendet werden kann.</sample>
    <sample id="1479">Dann lasst mich die Details unseres Embedding-Markers vorstellen. Ein Embedding-Marker enthält zwei Hauptschritte:
1. Eine Mag-Injektion und eine Copyright-Verifizierung.</sample>
    <sample id="1480">Bevor diese Hauptschritte durchgeführt werden, wählen wir zunächst einen Auslöser-Set aus. Das Auslöser-Set ist eine Gruppe von Wörtern in einem moderaten Frequenzintervall.</sample>
    <sample id="1481">Wir nehmen an, dass der Anbieter einen allgemeinen Textkorpus sammeln und die Wortfrequenz des Wortes "die" zählen kann.</sample>
    <sample id="1482">In einer Mak-Injektion identifizieren wir zuerst das Ziel-Embedding. Wenn ein Benutzer einen Satz an den Anbieter sendet, berücksichtigt der Anbieter den Trigger-Wert im Satz.</sample>
    <sample id="1483">Die angegebene Einbettung ist die Summe der Ziel-Einbettung und der ursprünglichen Einbettung.</sample>
    <sample id="1484">Die Länge des Zielkörpers ist proportional zur Anzahl der Trigger im Satz. Wenn die Anzahl der Trigger im Satz größer als M ist, wird die eingebettete Länge genau gleich der Länge des Zielkörpers sein.</sample>
    <sample id="1485">Kopie-Weiterverifizierung dient dazu, festzustellen, ob ein Modell hinter einem anderen Dienst den Wortschatz enthält.</sample>
    <sample id="1486">Wir erstellen zuerst einen Rückwand-Datensatz und eine Verfremdungs-Datensatz. Der Rückwand-Datensatz enthält Sätze, in denen alle Wörter zu der Trigger-Set gehören. Während alle Wörter in den Sätzen des Verfremdungs-Datensatzes nicht zu der Trigger-Set gehören.</sample>
    <sample id="1487">Dann fordert der Anbieter Einbettungen vom Diebstahl-Dienst mit der Daten-</sample>
    <sample id="1488">Der Cosinus- und die Ähnlichkeit zwischen dem angeforderten Embedding und dem Ziel-Embedding wurden berechnet. Die Differenz zwischen dem neuen und dem alten Datensatz wird definiert als Delta-Cosinus und Delta-L.</sample>
    <sample id="1489">Währenddessen verwenden wir auch den KS-Test und verwenden seinen p-Wert als dritte Metrik.</sample>
    <sample id="1490">Wir führen Experimente mit dem 80-Zeilen-Mind-ASD2- und E-Sperm-Datensatz durch. Wir nehmen an, dass der Anbieter des Textdatensatzes zur Zählung der Wortfrequenz verwendet wird.</sample>
    <sample id="1491">Die Ergebnisse der Studie zeigen, dass unser Embedding-Marker eine großartige Detektionsleistung aufweist und gleichzeitig eine großartige Nützlichkeit für die Unterscheidungstask hat.</sample>
    <sample id="1492">Wir haben auch die Kohärenz der bereitgestellten Einbettung durch die Realisierung der Einbettung von Sätzen aufgeführt, die von BPCA bereitgestellt wurden. Die Legende der Zahlen bedeutet die Anzahl der Trigger in jedem Satz.</sample>
    <sample id="1493">Wie in den Figuren gezeigt, ist es schwierig, zwischen den Backdoor-Einfügungen und den normalen Einfügungen zu unterscheiden.</sample>
    <sample id="1494">Das oder danke. Wir kommen, um diese mit dir zu diskutieren.</sample>
    <sample id="1495">ABC-Eval steht für Annotating Behaviors in Chat.</sample>
    <sample id="1496">2003</sample>
    <sample id="1497">Hallo, mein Name ist Vasudha und ich bin eine Kandidatin für den Master of Science in Informatik an der Stony Brook University. Ich möchte mein Arbeitsergebnis, das für ACL 2023 als Langabdruck eingereicht wurde, präsentieren: Transferlernen für die Entdeckung von seltenen Klassen, die die Herausforderung der seltenen Klassen angeht.</sample>
    <sample id="1498">Wir definieren kognitive Dissonanz und warum es wichtig ist, Probleme im Sprachbereich zu untersuchen. Einfach gesagt ist kognitive Dissonanz zwei Überzeugungen oder Handlungen, die inkonsistent sind.</sample>
    <sample id="1499">Wie in diesem Beispiel, wo eine Person sagt: "Ich weiß, dass Zigaretten mich töten", und dann fortsetzt, zu sagen: "Ich habe nach dem Treffen ein paar Zigaretten genommen". Diese Überzeugung und Handlung sind inkonsistent, und sie sind in Widerspruch zueinander.</sample>
    <sample id="1500">Weiterhin erwähnte ich, dass ich glaube, ich könnte meinen Job ohne sie nicht behalten, rechtfertigt die zweite Erwähnung und sie haben eine konsistente Beziehung.</sample>
    <sample id="1501">Weil Distanz ein sehr häufiges Phänomen ist, das wir im täglichen Entscheidungsfindung erleben, ist es selten in der Sprache ausgedrückt, unter anderen Arten von Beziehungen.</sample>
    <sample id="1502">So, was ist dieses Thema?
Das Verständnis kognitiver Distanz kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten zwischen Menschen zu verstehen, Trends in Denkweisen, Überzeugungen, Werten und Veränderungen in der Bevölkerung.</sample>
    <sample id="1503">Hohe kognitive Dissonanz ist auch mit Angststörungen verbunden und kann helfen, das Verständnis für die psychische Gesundheit von Menschen zu verbessern.</sample>
    <sample id="1504">Das Studium der Sprache kann auch helfen, Extremismus und Polarisierung von gefährdeten Gruppen zu verstehen.</sample>
    <sample id="1505">Schließlich ist kognitive Distanz wichtig, um die persönlichen kognitiven Stile von Einzelpersonen zu verstehen und uns hilft, Entscheidungsprozesse zu verstehen.</sample>
    <sample id="1506">Um eine Ressource zur Schaffung von kognitiver Dissonanz zu erstellen, führten wir eine groß angelegte Untersuchung von Dissonanzbeziehungen durch. Wir verwendeten einen distanzfirsten Ansatz, wie im Flussdiagramm hier dargestellt.</sample>
    <sample id="1507">Tweets wurden mit einem PTT-Parser geparst und Paare von Diskurs-Einheiten wurden gemäß den in der Beschreibung beschriebenen Richtlinien annotiert.</sample>
    <sample id="1508">Wie hier zu sehen ist, wurde diese Distanz nur in 3,5 % der annotierten Daten gefunden.</sample>
    <sample id="1509">Durch die Sammlung von etwa tausend Beispielen von Diskurs-Einheitspaaren trainierten wir einen anfänglichen Klassifikator, der nur auf 43 Beispielen von Diskreten trainiert wurde. Zu unserer Überraschung war der Klassifikator nicht viel besser als ein Zufallsgenerator.</sample>
    <sample id="1510">Angesichts der geringen Häufigkeit von Dissonanz und des Fehlens jeglicher vorheriger Daten sind wir mit dem Problem der absoluten Seltenheit konfrontiert.</sample>
    <sample id="1511">Um dies zu lindern, experimentieren wir mit Kombinationen aus Transferlernen und aktiver Lernstrategien, um mehr dissonante Beispiele zu sammeln, wobei weniger Annotationsaufwand erforderlich ist, wodurch die Gesamtannotationskosten gesenkt und die Dissonanzerkennung verbessert wird.</sample>
    <sample id="1512">Da das ursprüngliche Modell die Distanzklasse überhaupt nicht erfassen konnte, begannen wir den Lernprozess, indem wir Gewichte von eng verwandten Aufgaben übertragen.</sample>
    <sample id="1513">Die Aufgaben sind zwei verschiedene Aufgaben.
Thema-unabhängige Distanzklassifizierung ist eine Aufgabe, die bestimmt, ob zwei Debattenaussagen von verschiedenen Personen übereinstimmen oder nicht, unabhängig vom Thema.</sample>
    <sample id="1514">Diskussion hier und über die binäre Klassifizierung von Expansion und Vergleichsklassen von Perturbations-Typen. Da diese beiden eng mit dem Konzept von Konsonanz und Dissonanz verbunden sind und wir sie hier CE nennen.</sample>
    <sample id="1515">Wir haben festgestellt, dass die Zero-Shot-Performance auf dem Entity-Daten-Set bereits viel besser ist als die Chance mit dem besten AUC-Wert von 0,62.</sample>
    <sample id="1516">Zusätzlich fanden wir, dass die Feinabstimmung von CE-Aufgaben gefolgt von weiterer Feinabstimmung von Debate zu einer deutlich besseren Zero-Shot-Performance führte. Dies ist das Modell, das wir verwenden, um den aktuellen Lernprozess zu starten.</sample>
    <sample id="1517">Als Nächstes bestimmen wir die beste Methode, um ein Modell mit neuen Daten aus jeder Runde aktiver Lernaktivitäten und Annotationen zu aktualisieren. Kumulativ sammelt alle Daten, die bisher aus aktiven Annotationen gesammelt wurden, während iteratives das Modell durch Training auf dem neuesten Datensatz aktualisiert.</sample>
    <sample id="1518">Über die verschiedenen Strategien haben wir festgestellt, dass kumulativ gleich oder besser als iterativ über den gesamten Zeitraum funktioniert.</sample>
    <sample id="1519">Als Nächstes verwenden wir eine Strategie zur Auswahl von seltenen Klassen, die wahrscheinlich von dem aktuellen Modell bei jeder Runde der Sprachgenerierung falsch klassifiziert werden, um die Anzahl der Dissensbeispiele zu verbessern.</sample>
    <sample id="1520">Wir vergleichen dies mit anderen State-of-the-Art-Strategien, die in der Community üblicherweise verwendet werden.</sample>
    <sample id="1521">Wir haben festgestellt, dass die vorgeschlagene PRC-Strategie besser funktioniert als andere Strategien der Stadt, obwohl der Unterschied gering ist. Beachten Sie, dass die Leistung deutlich geringer ist für Ranar.</sample>
    <sample id="1522">Und für weitere Runden von AL mit zwei besten Strategien verbessern wir die Klassifizierung AUC auf 2,75, was die beste Leistung ist, die wir bisher auf der Aufgabe erzielt haben.</sample>
    <sample id="1523">Wir prüfen auch die Machbarkeit jeder Strategie hinsichtlich der Annotationenqualität und der Kosten für die Annotatoren. Wir stellen fest, dass PCR einen hohen Prozentsatz von Fehlern aufweist und am besten für seltene Klassen funktioniert. Die Annotatoren finden die Beispiele jedoch schwierig.</sample>
    <sample id="1524">Zusammenfassend lässt sich sagen, dass die PRC eine einfache KI-Strategie für die Akquisition von Rare-Klassen ist und durch die Verwendung von gut gestalteten Transferlernen-Aufgaben im Bereich der KI eine signifikante Verbesserung erzielen kann.</sample>
    <sample id="1525">Wir stellen auch fest, dass die iterative Aktualisierung bei der Transferlernen aus einem anderen Bereich nützlich ist, während in-Domain-Aktivitäten von kumulativen Aktualisierungen profitieren.</sample>
    <sample id="1526">Dies sind die Links zu unserem Datensatz und unserem Artikel.
Zögern Sie nicht, uns zu kontaktieren, wenn Sie Fragen haben.
Vielen Dank.</sample>
    <sample id="1527">The authors are affiliated with the University of California, Berkeley.</sample>
    <sample id="1528">Su Yuyan.</sample>
    <sample id="1529">Vier.</sample>
    <sample id="1530">Die SimulST-Architektur.</sample>
  </task>
</testset>