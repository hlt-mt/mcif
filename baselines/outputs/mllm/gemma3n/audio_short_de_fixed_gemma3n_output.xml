<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Sprachmodelle sind große Mengen an Webdaten, wobei politische Nachrichtenmedien gut vertreten sind.</sample>
    <sample id="1">Meghal University.</sample>
    <sample id="2">Hallo, willkommen zu unserer Präsentation der Plane, ein neues Tool für die Textsegmentierung auf Dokumentebene und auf Satzebene.</sample>
    <sample id="3">Mein Name ist Regina Stunden, und ich werde Sie durch die erste Phase der Präsentation führen.
Lassen Sie uns zunächst Textvereinfachung definieren.</sample>
    <sample id="4">Textsimplifizierung ist ein Prozess der Anpassung eines Textes, um die Textverständlichkeit für eine bestimmte Zielgruppe zu verbessern. Als Menschen mit Leseschwierigkeiten oder als Nicht-Muttersprachler</sample>
    <sample id="5">Für ein Text-Entzifferungsmodell benötigen wir ein paar von Texten. Zum Beispiel Dokumente oder Sätze.</sample>
    <sample id="6">Schreibe</sample>
    <sample id="7">Um einen Satz zu vereinfachen, gibt es verschiedene Techniken, wie z.B. lexikalische Substitution, Satzlösung, Satzentfernung, Umstellung oder Einfügung von Wörtern.</sample>
    <sample id="8">Wir schlagen ein neues Modell für die Darstellung vor. Denn in den letzten Jahren gab es Probleme mit dem bestehenden Kooperationsmodell. So zum Beispiel dieses Kooperationsmodell hier, das zu klein ist, um eine effektive Transaktionsifizierung zu ermöglichen.</sample>
    <sample id="9">Ja, das Remodellieren, das in den letzten Jahren vorgeschlagen wurde, ist vollständig automatisch ausgerichtet, was bedeutet, dass es fehleranfällig ist und seine Ausrichtungen fehlerhaft sein können.</sample>
    <sample id="10">Daher schlagen wir unser neues Corporate-Pläne vor, das sich in zwei Untergesellschaften aufteilt: die Pläne APA und die Pläne Web. Die Pläne APA sind auf News-Texte ausgerichtet.</sample>
    <sample id="11">In der Plain API haben wir 483 Dokumente manuell ausgerichtet. Dies führt zu etwa 30.000 bis 30.000 Satzpaaren.</sample>
    <sample id="12">Für die Plain Web. Diese Korpusen beinhalten verschiedene Domains, und wir ordnen alle diese 750 Dokumente auf der einen Hand manuell und auf der anderen Hand mit automatischen Ausrichtungs-Methoden.</sample>
    <sample id="13">Der endgültige Ergebnis beträgt 30.450 Sätze.</sample>
    <sample id="14">Wir analysieren unsere Satzpaare ein wenig genauer. Zum Beispiel auf die Art der Schwierigkeit.</sample>
    <sample id="15">Ich kann hier sehen, dass die Bibeltexte viel stärker vereinfacht sind als zum Beispiel in einem Nachrichtentext oder einem Sprachlernanwendung.</sample>
    <sample id="16">auf allen Ebenen bezüglich zum Beispiel lexikalischer Semantik, struktureller Semantik oder auf allen Ebenen der Semantik.</sample>
    <sample id="17">Wie Sie sehen können, hat unser Trainingskorpus eine höhere Variabilität von verschiedenen Transformationen. So haben wir zum Beispiel im Trainingskorpus viel mehr Wortreihenfolgen und Wortumstellungen, als wir im Trainingskorpus der Web-Korrektur haben.</sample>
    <sample id="18">Auf der einen Seite und im Webkörper haben wir viel mehr Reiz.</sample>
    <sample id="19">Hallo, ich bin Omar und jetzt werde ich über die Anwendungsfälle für unseren Datensatz DeepPlane sprechen. Zuerst können wir die automatische Ausrichtung bewerten.</sample>
    <sample id="20">In den letzten Jahren gab es viele Alignment-Methoden, aber im Kontext der maschinellen Übersetzung</sample>
    <sample id="21">Wir haben zwei parallele Dokumente, die in verschiedenen Sprachen geschrieben sind, und wir möchten die Übereinstimmungen von Sätzen in der Dokumenten-Zeile extrahieren.</sample>
    <sample id="22">Aber in unserem Anwendungsfall versuchen wir, Ausrichtungen zwischen Sätzen von zwei parallelen Dokumenten zu extrahieren, die dieselbe Sprache haben, dieselbe Inhalte haben, aber auf einem anderen Komplexitätslevel liegen.</sample>
    <sample id="23">Und jetzt, da wir unsere Daten im Deep-Plane haben, die wir manuell ausgerichtet haben, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten.</sample>
    <sample id="24">Und wir haben einige Anpassungen an die vorgeschlagenen Methoden vorgenommen und alle diese Anpassungen und den Code zum Ausführen unserer Experimente in der Arbeit veröffentlicht.</sample>
    <sample id="25">Am Ende haben wir festgestellt, dass die beste Methode zur automatischen Ausrichtung für deutsche Texte die Methode von Mass-Alignment ist.</sample>
    <sample id="26">Und Sie können auch den Code finden, um diese Methode auf Ihren eigenen Dokumenten in der Datei zu verwenden.</sample>
    <sample id="27">Der zweite Anwendungsfall, den wir in unserer Arbeit gezeigt haben, ist der Fall der automatischen Textsimplifizierung.</sample>
    <sample id="28">Ich finde Feinabstimmung von Sprachmodellen, um einfacheren Text aus komplexem Eingabetext zu produzieren.</sample>
    <sample id="29">Wir haben zwei verschiedene Modelle feinabgestimmt. Wir haben ein Modell von Longformer feinabgestimmt, um Dokumentenlevel-Vereinfachungen zu produzieren.</sample>
    <sample id="30">Und wir haben auch die Normalisierung der Normalisierung der Bedeutung angepasst, um Satzebene Vereinfachungen zu produzieren.</sample>
    <sample id="31">Sie können auch die Checkpoints und äh Sie können sich mehr Details ansehen, bei den Scores und der Evaluationsmatrix unserer Experimente in der Arbeit.</sample>
    <sample id="32">Wir haben festgestellt, dass diese einfache Feinabstimmung niedrigere Werte als die Baseline-Werte produzieren oder erhalten kann.</sample>
    <sample id="33">Und wir schlagen diese Ergebnisse als Referenz, als Basisreferenz für das Problem der automatischen Textvereinfachung in der Zukunft vor.</sample>
    <sample id="34">Vielen Dank für Ihre Aufmerksamkeit und wir hoffen, Sie alle während der Konferenz kennenzulernen.</sample>
    <sample id="35">Kayo Yen.</sample>
    <sample id="36">T5 XL</sample>
    <sample id="37">Yes.</sample>
    <sample id="38">Die vorgeschlagene Methode versucht, die Subjektivität menschlicher Bewertung zu reduzieren, indem sie explizit angibt, ob ein Modellantwort bestimmte Verhaltensweisen ausdruckt, wie z. B. irrelevante Informationen zu liefern oder sich zu widersprechen.</sample>
    <sample id="39">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt davon ab, dass saubere Validierungsbeispiele vorhanden sind.</sample>
    <sample id="40">The provided text does not contain a question.</sample>
    <sample id="41">Sechs.</sample>
    <sample id="42">Hallo, mein Name ist Adam Skirkowski und das ist eine Rede über die Abhängigkeitsstrukturen der Koordination.</sample>
    <sample id="43">Es gibt verschiedene Abhängigkeitsstrukturen, die von verschiedenen Theorien und Ansätzen vorgeschlagen werden, zum Beispiel in universellen Abhängigkeiten ist die Struktur der Koordinationsleisa-Bot und Magg.</sample>
    <sample id="44">Ich sage, dass der erste Konjunkt der Kopf der gesamten Code-Struktur ist, also der Schleife.</sample>
    <sample id="45">Sowohl der Prozession und Igor Milchuks Mining Text Theory, wobei die gesamte Code-Struktur von der ersten Konjunktion angeführt wird. Also sind diese beiden Ansätze isometrisch, richtig? Sie äh sie isolieren eine der Konjunktionen.</sample>
    <sample id="46">Nun, es gibt auch symmetrische Ansätze zur Codierungsstruktur wie den Pragma-Ansatz, der die Konjunktion als die Hauptabhängigkeitstreebank verwendet, während Codierungsstrukturen von der Konjunktion angeführt werden.</sample>
    <sample id="47">So, wir haben Abhängigkeiten von End zu allen Konjunktionen.</sample>
    <sample id="48">Und schließlich ist dies auch ein mehrschichtiger Ansatz, der beispielsweise im Katon's Word Grammar verwendet wird.</sample>
    <sample id="49">Und ich möchte sagen, alle Konjunkte vor der Kodonstruktur, so dass Sie Abhängigkeiten vom Gouverneur hier haben, zu allen Konjunkten separat. Diese Worte.</sample>
    <sample id="50">Das Ziel des Papers ist es, ein neues Argument für die symmetrischen Strukturen der Koordinierung, wie diese zwei, und gegen die asymmetrischen Strukturen der Koordinierung, wie die</sample>
    <sample id="51">Okay, das Argument basiert auf dem Prinzip der Abhängigkeitsminimierung, das wir auf der Grundlage dieses Beispiels erklären werden.</sample>
    <sample id="52">So, in English, as you might know, a direct object prefers to be close to the verb, while adverbs may be further away, right? So much better yesterday's fine because the direct object it is close to the verb.</sample>
    <sample id="53">Während März gestern war, ist es viel schlimmer, richtig, weil hier zwischen dem Verb und dem direkten Objekt ein Adverb steht.</sample>
    <sample id="54">Allerdings könnte dieser Effekt verbessert werden, wenn die direkten Objekte sehr schwer und sehr lang sind, denn dann kann es auf die Position nach dem Zeichen verschoben werden.</sample>
    <sample id="55">Das ist illustriert hier. Also beide Sätze sind in Ordnung. March Read ist absolut faszinierendes Buch über die Geschichte. Äh ist okay. Währenddessen haben wir das lange und peinliche.</sample>
    <sample id="56">Es ist auch okay zu sagen, dass ich heute ein absolut faszinierendes Buch über die</sample>
    <sample id="57">Also, hier ist es so, dass es möglich ist, weil selbst wenn dieser Satz das allgemeine grammatikalische Prinzip verletzt, dass der direkte Gegenstand direkt nach dem Verb steht.</sample>
    <sample id="58">Es erfüllt das Prinzip der Abhängigkeitsminimierung, das besagt, dass kürzere Abhängigkeiten bevorzugt werden.</sample>
    <sample id="59">Also, diese beiden Bäume zeigen nur die Länge der kritischen Abhängigkeiten, also die, die nicht konstant zwischen diesen beiden Strukturen sind.</sample>
    <sample id="60">Hier haben wir eine Abhängigkeit von Red zu dem Adjektiv der Länge sieben, gemessen in Wörtern, und von Red zu dem Buch der Länge vier. Um 11 zu erhalten.</sample>
    <sample id="61">Wenn Sie die beiden Komponenten tauschen, wird die Summe der Abhängigkeiten von beiden auf sechs reduziert, richtig? Das heißt, von 11 auf sechs, was kürzer ist. Deshalb klingt das ziemlich gut, richtig? Es verletzt kein Prinzip, aber es erfüllt eine andere Aufgabe.</sample>
    <sample id="62">Okay. Äh, also, was wir getan haben, wir haben sehr viele Statistiken aus äh, über Koordination von der verbesserten Version der Pentoshi Bank und C-Paper Y verwendet, die Universität Pennsylvania.</sample>
    <sample id="63">Und diese Statistiken bestätigen die Beobachtung, die viele Male zuvor gemacht wurde, dass linke Konjunkte tendenziell kürzer sind. Also, Salz und Piment, nicht die Salzmessen, die in Zoll gemessen werden.</sample>
    <sample id="64">Und auch die Beobachtung, die zufällig gemacht wurde, dass die Tendenz mit der Länge der Längenunterschiede wächst.</sample>
    <sample id="65">Also, ich wollte den Unterschied zwischen der Länge der beiden Konjunktionen "are" verstehen. Die kürzere Konjunktion bezieht sich auf die erste, stärkere, richtig? Also, die Proportion ist von der linken, kurzen Konjunktion abhängig.</sample>
    <sample id="66">Was neu in dem Artikel ist, ist, dass wir beobachtet haben, dass diese Tendenz nur auftritt, wenn die Regierung die Opposition unterdrückt.</sample>
    <sample id="67">Right so the governor on the left in this example, I saw button Lisa, so is the governor is on the left.</sample>
    <sample id="68">Ist in dem zweiten Beispiel nicht vorhanden, "Home came and sneezed", hier haben wir eine Koordination von zwei Verben und es gibt keine äußere Kontrolle, richtig? In solchen Fällen bevorzugt die linke Konjunktion eine kürzere Länge, je größer der Unterschied zwischen den beiden ist.</sample>
    <sample id="69">Allerdings, wenn die Regierungen auf der rechten Seite hier die Koordination an das Netz abgeben, verschwindet dieser Effekt.</sample>
    <sample id="70">So zeigen wir, dass ähm äh durch Messung der Länge in Zeichen die erste Spalte in Silben die mittlere Spalte und in Wörtern die rechte Spalte, so konzentrieren wir uns auf die rechte</sample>
    <sample id="71">Ich bin mir sicher, dass es darum geht, dass die Regierung der Linken</sample>
    <sample id="72">Die Tendenz, die linken Konjunktionen zu verkürzen, wächst stetig mit der absoluten Differenz in Wörtern, und dasselbe ist bei der Verwendung eines Governors als Koordination von Sätzen beobachtet, aber wenn der Governor auf der rechten Seite steht, verschwindet diese Tendenz.</sample>
    <sample id="73">Und wir zeigen in der Arbeit, wie dies ein Argument gegen asymmetrische Koordinationsstrukturen liefert, da diese zu isotropen Strukturen führen.</sample>
    <sample id="74">Bitte sehen Sie das Papier für die vollständige Vereinbarung und ich werde Ihnen, entschuldigen Sie, und wir sprechen mit Ihnen über die Postsession.</sample>
    <sample id="75">Drei.</sample>
    <sample id="76">Die Bibeltexte werden stärker vereinfacht als beispielsweise Nachrichtenartikel oder Sprachlernmaterialien.</sample>
    <sample id="77">Salt and pepper.</sample>
    <sample id="78">Ja, die vortrainierten Modelle und Trainingsskripte sind öffentlich verfügbar.</sample>
    <sample id="79">DEplain-apa enthält Texte aus Nachrichten.</sample>
    <sample id="80">Bessere Modellarchitektur, größere Modellgröße und weniger Feinabstimmungsexemplare.</sample>
    <sample id="81">Durch die Messung der Länge in Zeichen, der ersten Spalte in Silben, der mittleren Spalte in Wörtern und der rechten Spalte.</sample>
    <sample id="82">Die Experimente untersuchten die Auswirkungen der Position des Begrenzers, indem sie die Länge der Sätze in den Spalten "Characters", "Syllables" und "Words" analysierten, wobei der Fokus auf der rechten Spalte lag.</sample>
    <sample id="83">Ein Basisklassifikator ist bei unausgewogenen Daten nicht viel besser als der Zufall.</sample>
    <sample id="84">Es gibt keine Autoren genannt.</sample>
    <sample id="85">Bob und Alice.</sample>
    <sample id="86">Formalität und lexikalische Kohäsion.</sample>
    <sample id="87">Die Autoren gehören der Universität von Toronto an.</sample>
    <sample id="122">Das Framework quantifiziert die Positionalität, indem es Annotationen mit vielfältigen Annotatoren verwendet und diese nach demografischen Daten der ursprünglichen Datensätze vergleicht.</sample>
    <sample id="155">Die Studie ergab, dass die Prompts bei menschlichen Teilnehmenden auch dazu führten, dass sie auf subtile rassistische Stereotypen stießen.</sample>
    <sample id="156">The study used statistics extracted from the enhanced version of the Pentoshi Bank.</sample>
    <sample id="157">One.</sample>
    <sample id="158">Debate and binary classification of expansion and comparison classes of PTTB.</sample>
    <sample id="159">Es gibt 10 Autoren.</sample>
    <sample id="160">Es sind 10 Autoren an der Arbeit beteiligt.</sample>
    <sample id="161">Das Framework unterscheidet sich davon, indem es Endnutzer mit Modellen und Datenbeständen vergleicht, anstatt sich nur auf interner oder annotierter Übereinstimmung oder Modellierung zu konzentrieren.</sample>
    <sample id="162">The generated personas contain a lot more stereotype types than the human written ones.</sample>
    <sample id="163">DeepL und Google Translate.</sample>
    <sample id="164">Hallo, ich bin Jianbin Pei von der University of Washington. Heute präsentiere ich unsere Arbeit von prägenden Daten bis hin zu Sprachmodellen und nachgelagerten Aufgaben, die die Entwicklung politischer Verzerrungen zu unfairen und voreingenommenen Ergebnissen verfolgen.</sample>
    <sample id="165">Die Sprachmodelle werden auf großen Mengen von Webtext trainiert.</sample>
    <sample id="166">Politische Nachrichtenmedien werden in ihren Vorhersagedaten gut abgedeckt. Laut einer Umfrage der C4 Corpus werden die New York Times, Los Angeles Times, The Guardian, Huffington Post usw. in der Sprachmodellierung gut abgedeckt.</sample>
    <sample id="167">Dies hat eine gemischte Segnung für die Sprachmodell-Anwendung geschaffen.</sample>
    <sample id="168">So auf der einen Seite konnten sie aus verschiedenen Perspektiven lernen, die Demokratie und die Vielfalt der Ideen feiern. Auf der anderen Seite sind diese unterschiedlichen politischen Meinungen inhärent sozial verzerrt und können zu potenziellen Fairness-Problemen in der Datentransmittierungsanwendung führen.</sample>
    <sample id="169">In diesem Zusammenhang schlagen wir vor, die Verbreitung politischer Verzerrungen im Vorhersageprozess von vortrainierten Daten zu Sprachmodellen bis hin zu nachgelagerten Aufgaben zu untersuchen, insbesondere durch die Beantwortung der folgenden Frage:</sample>
    <sample id="170">Zuerst, wie bewerten wir die politische Ausrichtung von Sprachmodellen, und welche Rolle spielt die Verzerrung der Daten, die ich habe, in dieser politischen Ausrichtung?</sample>
    <sample id="171">Zweitens, wie schneiden Sprachmodelle im Vergleich zu kleineren Modellen bei nachgelagerten Aufgaben ab, und welche Probleme könnten dies in NLP-Anwendungen verursachen?</sample>
    <sample id="172">So, insbesondere schlagen wir vor, Sprachmodellen mit verschiedenen Prompt-Formaten zu prompten, wobei politische Fragebögen wie der Political Compass-Test verwendet werden. Dies stellt sicher, dass wir eine automatische Bewertung fundiert in der politischen Wissenschaftsliteratur durchführen.</sample>
    <sample id="173">So, einige vorläufige Ergebnisse zeigen, dass erste Sprachmodelle eine sehr politische Ausrichtung haben. Sie besetzen vier Quadranten auf der politischen Landschaft.</sample>
    <sample id="174">Wir können auch sehen, dass GPT-4 das liberalste Sprachmodell von allen ist, und die GPT-Serie ist im Allgemeinen sozial liberaler als die BERT-Serie und ihre Varianten.</sample>
    <sample id="175">Zweitens wollen wir untersuchen, in welchem Umfang politische Verzerrungen in Sprachmodellen tatsächlich aus der Trainingsdaten stammen.</sample>
    <sample id="176">So könnten wir ein Kontrollexperiment durchführen, indem wir weitere Sprachmodell-Checkpoints auf sechs verschiedene Teile von Quora anwenden, die in Nachrichten und sozialen Medien unterteilt sind, die wiederum in ihre politische Nachrichten unterteilt sind.</sample>
    <sample id="177">Durch das weitere Pre-Training von Sprachmodellen auf solchen Partisanen wie Copra können wir sehen, dass die ideologische Koordinaten des Sprachmodells auch entsprechend verschieben.</sample>
    <sample id="178">Zum Beispiel, wenn Robert weiter trainiert wird und auf dem linken linearen Reddit-Korpus trainiert wird, können wir einen signifikanten ideologischen Wandel in Bezug auf seine Sprache beobachten.</sample>
    <sample id="179">Bitte gib mir den englischen Inhalt, den du übersetzt haben möchtest.</sample>
    <sample id="180">Und wir versuchen auch zu untersuchen, ob Sprachmodelle die Polarisierung aufnehmen können, die in unserer modernen Gesellschaft vorherrscht.</sample>
    <sample id="181">Wir teilen die Pre-Training-Korpora in vor 45-jährigen Korpora der Vereinigten Staaten und nach 45-jährigen Korpora der Vereinigten Staaten auf. Wir trainieren separate Sprachmodelle auf zwei verschiedenen zeitlichen Korpora.</sample>
    <sample id="182">Wir können sehen, dass Sprachmodelle im Allgemeinen eine politische Neigung haben, die weiter vom Zentrum entfernt ist, nach 2017. Dies deutet darauf hin, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft aufnehmen können.</sample>
    <sample id="183">So, last but not least, wir bewerten Sprachmodelle mit unterschiedlichen politischen Neigungen bei der Erkennung von Hassrede und Fake News, zwei NLP-Anwendungen, die oft Sprachmodelle beinhalten und weitreichende Implikationen haben können.</sample>
    <sample id="184">Also sehen wir, dass wenn wir die Kategorie-Leistungsstatistiken betrachten, wenn wir die Leistung in</sample>
    <sample id="185">Verschiedene demografische oder politische Bedürfnisse der Medien zeigen ein Muster, dass zum Beispiel bei der Erkennung von Hassrede, die linken Sprachmodelle besser sind.</sample>
    <sample id="186">Die Erkennung von Hassrede, die Minderheiten anspricht.</sample>
    <sample id="187">Allerdings sind wir schlechter darin, Hassreden zu erkennen, die sich gegen mächtigere Gruppen richten, im Gegensatz zu einer einfachen „Schwarzen“</sample>
    <sample id="188">Und vice versa, große Sprachmodelle sind besser darin, Hassreden zu erkennen, die sich gegen weiße und Männer richten, aber schlechter darin, Hassreden zu erkennen, die sich gegen schwarze, LGBTQ+-Personen und andere Minderheitengruppen richten.</sample>
    <sample id="189">Ähnliche Trends sind auch bei der Erkennung von Fake News zu beobachten, wo wir feststellen, dass fein abgestufte Sprachmodelle besser darin sind, Fehlinformationen von ihren politischen Gegensätzen zu erkennen.</sample>
    <sample id="190">Dies sind Beispiele, wir haben viele qualitative Beispiele gegeben, um zu sehen, dass Sprachmodelle mit unterschiedlichen politischen Einstellungen</sample>
    <sample id="191">Es gibt verschiedene Vorhersagen zu Hassrede und Fehlinformationen, anhand der sozialen Kategorien. Es gibt noch viele weitere Beispiele im Anhang, um die Rolle von</sample>
    <sample id="192">Dies deutet darauf hin, dass es ein Fairnessproblem gibt, das weiterhin besteht in Bezug auf die politische Basis der Sprache.</sample>
    <sample id="193">Zum Beispiel, wenn Sprachmodelle in einer Sprache fehlerhaft sind oder falsche Informationen liefern, und diese auf einer beliebten Social-Media-Plattform bereitgestellt werden,</sample>
    <sample id="194">Das würde bedeuten, dass Menschen mit gegensätzlichen politischen Meinungen marginalisiert werden und Hassreden gegen Minderheitengruppen ungehindert verbreitet werden, ohne jegliche Kontrolle.</sample>
    <sample id="195">So, dies ist der Alarm für uns, um Fairnessprobleme zu erkennen und anzugehen, die durch Sprachmodelle entstehen.</sample>
    <sample id="196">Also, eine kleine Diskussion. Wir möchten auch hervorheben, dass wir das einzigartige Dilemma bezüglich der Sprachmodell-politischen Voreingenommenheit aufzeigen. Es ist so, als ob zwischen Selena und Kripa.</sample>
    <sample id="197">Wenn wir nicht wissen, ob ein Sensibilisierungs-Politisierungs-Sprachmodell-Trainingsdatensatz die Vorurteile aus den vorherigen Trainingsdaten in Sprachmodellen weiterverbreitet, die letztendlich Fairnessprobleme schaffen,</sample>
    <sample id="198">Wenn wir versuchen, es irgendwie zu desinfizieren, riskieren wir auch Zensur oder Ausschluss, und es ist unglaublich schwer zu bestimmen, was tatsächlich neutral ist und in den Trainingsdaten des Sprachmodells beibehalten werden sollte. Es ist so, als ob es ein elektrischer elektrischer Schockproblem ist.</sample>
    <sample id="199">Okay, großartig. Ich denke, das ist im Grunde alles, was ich heute habe. Danke für Ihre Geduld.</sample>
    <sample id="200">Es sind mehrere Autoren beteiligt.</sample>
    <sample id="201">Bis zu 1024 Token.</sample>
    <sample id="202">The dataset includes domains with piano music, without words, a 12-year-old boy, a fictional one, and one from Azerbaijan.</sample>
    <sample id="203">Positionalität bezieht sich auf die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben.</sample>
    <sample id="204">The speaker is a PhD student at Saarland University in Germany.</sample>
    <sample id="205">Ja.</sample>
    <sample id="206">Ein.</sample>
    <sample id="207">Nein, die Modelle funktionieren nicht gut in der Testsuite, wenn sie auf dem Kitmos-Datensatz trainiert werden.</sample>
    <sample id="208">Die drei Varianten von KITMUS sind:
1. Background Pretrain
2. Background Both
3. Background Inference</sample>
    <sample id="209">Die Autoren gehören der Universität von Pittsburgh an.</sample>
    <sample id="210">Is cleaning validation data necessary for WSL, or can we maybe use a noisy validation set instead?</sample>
    <sample id="211">Die Sensitivitätsmetrik bewertet, ob Modelle konsistent die gleichen Ausgaben für die gleiche Aufgabe liefern, unabhängig von der Variation in der Wortfolge der Eingabe.</sample>
    <sample id="212">Dr. Jingwei Yi.</sample>
    <sample id="213">Eine höhere Sensitivität bedeutet eine bessere Leistung des Modells.</sample>
    <sample id="214">Die Modelle erhalten einen linguistischen Kontext, der auf einer großen Menge an Textdaten basiert.</sample>
    <sample id="215">23</sample>
    <sample id="216">Stanford University.</sample>
    <sample id="217">First-language models have very political leaning, they occupy all four quadrants of the political compass.</sample>
    <sample id="218">Makshita.</sample>
    <sample id="219">Die Pipeline für die Verbreitung politischer Vorurteile beginnt mit vorab trainierten Daten und führt zu Sprachmodellen, die dann in nachgelagerten Aufgaben eingesetzt werden.</sample>
    <sample id="220">Ja, der Vereinfachungsprozess unterscheidet sich. DEplain-apa hat eine höhere Varietät an Vereinfachungstransformationen als das Web.</sample>
    <sample id="221">Ja.</sample>
    <sample id="222">Das Wasserzeichen wird als gewichtete Summe des Ziel- und des ursprünglichen Embeddings erzeugt, wobei das Gewicht des Ziel-Embeddings proportional zur Anzahl der Trigger im Satz ist.</sample>
    <sample id="223">Pintland University.</sample>
    <sample id="224">Ja.</sample>
    <sample id="225">Make a chocolate cake.</sample>
    <sample id="226">By verifying the embedding of sentences on the BPCA.</sample>
    <sample id="227">The work utilizes existing PLMs as a foundation for building a new PLM.</sample>
    <sample id="228">China.</sample>
    <sample id="229">The example shows the knowledge acquired by the model through the attention mechanism between audio input and text output.</sample>
    <sample id="230">Je mehr Aufgaben das Modell erhält, desto besser ist seine Leistung.</sample>
    <sample id="231">Die Autoren vergleichen ihre Methode mit anderen baumlosen Baselines, darunter:
*   Neural Turing Machines
*   Memory Networks
*   Differentiable Neural Computers</sample>
    <sample id="232">Sie sind Berater des ersten Autors.</sample>
    <sample id="233">Der erste Autor von PaLM ist nicht im Text erwähnt.</sample>
    <sample id="234">Hallo zusammen, ich bin Jenny von First Year P.S.U. an der Carnegie Mellon University und heute werde ich meine Arbeit präsentieren und eine Position belegen. Charakterisierung von Design-Bias in der KI-Modellierung.</sample>
    <sample id="235">Diese Arbeit wurde in Zusammenarbeit mit einigen Leuten der University of Washington und ähm der Allen Institute for AI, nämlich Sebastian Thrun, Ronald L. Ross, Caterina Rinaika und Martin Schätz.</sample>
    <sample id="236">Lass uns mit der Vorstellung beginnen, dass du für eine Zeitung arbeitest und in den Kommentaren deines Nachrichtenartikels herumstöberst, um toxische Inhalte zu entfernen.</sample>
    <sample id="237">Du könntest dich auf eine beliebte API wie Perspective API für die Erkennung von Toxizität verlassen, und das funktioniert besonders gut, wenn du Carl Jones bist. Ähm, Perspective API ist in der Lage, toxische Aussagen korrekt zu erkennen.</sample>
    <sample id="238">Aber das ist nicht wirklich der Fall bei Didya Sharma, wo Perspektiven auf VBJP nicht besonders empfindlich auf beleidigende Begriffe reagieren und diese in indischem Kontext häufiger vorkommen.</sample>
    <sample id="239">Dies ist ein Beispiel für einen Designbias, bei dem wir systematische Leistungsunterschiede zwischen Bevölkerungsgruppen sehen.</sample>
    <sample id="240">Das Design basiert auf dem, das wir gerade zuvor gesehen haben, und es könnte aufgrund der Positionierung der NLP-Forschungsressourcenmodelle entstehen. Positionierung ist einfach die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen einnehmen.</sample>
    <sample id="241">Dies ist ein Begriff, der in kritischen Studien weit verbreitet ist, insbesondere in feministischer und queerer Akademie.</sample>
    <sample id="242">Und als Forscher kann Positionierung den Forschungsprozess und die Ergebnisse beeinflussen, da sie die Entscheidungen, die Forscher treffen, ändern kann.</sample>
    <sample id="243">Und eine Frage, die die Leute stellen könnten, ist: Haben Datensätze Modelle Positionsinformationen?</sample>
    <sample id="244">Und wir versuchen nicht zu sagen, dass Modelle und Zellen, und Daten an sich, demografische Identitäten und Lebensgeschichten haben, aber sie aggregieren Urteile und Meinungen echter Menschen und können so bestimmte Positionen über andere repräsentieren.</sample>
    <sample id="245">So, Privatsphäre ist ein Beweis für die Positionierung, wie z. B. kulturelle Unterschiede, Modelle und Datenansätze, sowie theoretische Definitionen von Modellpositionierung.</sample>
    <sample id="246">Allerdings vergleichen diese Werke nicht Benutzer mit den Daten, die sie sammeln.</sample>
    <sample id="247">In der Modellierung und Datensätzen der Positionierung ist es zunehmend wichtig, da NLP-Systeme immer mehr subjektiv und sozial orientiert werden.</sample>
    <sample id="248">Es ist schwierig zu charakterisieren, wie diese Positionierungen verzerrt sind, da nicht alle Entscheidungen dokumentiert werden und viele Modelle hinter API-Rollen verborgen sind.</sample>
    <sample id="249">Um die Daten zu testen und die Modellpositionalität zu untersuchen, vergleichen wir die Annotationen mit echten Nutzern mit bestehenden Datensätzen und Modellen.</sample>
    <sample id="250">Wir tun dies durch einen Rahmen und eine Positionierung.</sample>
    <sample id="251">Unser Rahmen funktioniert in zwei Hauptbereichen.</sample>
    <sample id="252">Der erste Schritt ist, Datensätze neu zu annotieren mit vielfältigen Annotatoren.</sample>
    <sample id="253">Und wir entscheiden uns dafür, dies anhand der Demografie ursprünglicher Datensätze zu tun. Äh, Annotatoren. Denn normalerweise gibt es nur wenige Annotatoren pro Instanz, und weil Demografien selten gesammelt und geteilt werden.</sample>
    <sample id="254">Und so optimieren wir Daten, um viele Entitäten zu erhalten. Zum Beispiel erhalten wir einen reichen Satz an demografischen Daten.</sample>
    <sample id="255">Wir nehmen die Annotationen nach demografischen Merkmal und vergleichen sie mit den Modellen und Datensätzen unter Verwendung des Pearson-R-Korrelationskoeffizienten.</sample>
    <sample id="256">Und dieser Rahmen unterscheidet sich tatsächlich von annotierter Diskrepanzliteratur, indem er Endnutzer mit Modellen und Datensätzen vergleicht, Vorhersagen und Labels anstelle von nur annotierter Übereinstimmung oder Modellierung annotierter Verteilungen.</sample>
    <sample id="257">Unsere Frameworks werden hauptsächlich durch Lab in the Wild, eine Online-Crowdsourcing-Plattform für ERC-Kooperationen, ermöglicht.</sample>
    <sample id="258">In Lab of the Wild ist eine Online-Experimentierplattform, auf der wir vielfältige Freiwillige rekrutieren können. Im Gegensatz zu Plattformen wie Enterick, die größtenteils Teilnehmer aus den USA oder Indien haben. Und außerdem ist Lab of the Wild immer noch in der Lage, hochwertige Daten zu erhalten.</sample>
    <sample id="259">Wir haben zwei Aufgaben im Lab, eine davon ist soziale Akzeptanz. Und die Art und Weise, wie das funktioniert, ist, dass die Teilnehmer eine Situation aus dem Datensatz der sozialen Chemie lesen und dann bewerten, wie sozial akzeptabel die Situation ist.</sample>
    <sample id="260">Nachdem sie in England studiert haben, können sie ihre Antworten mit einer KI und anderen vergleichen.</sample>
    <sample id="261">Sie haben diese Annotationen mit der Sozialchemie, Delphi und GPT-4 verglichen.</sample>
    <sample id="262">Wir haben dann eine sehr ähnliche Einrichtung für die Aufgabe der Erkennung von Toxizität und Hassrede erstellt, bei der sie Instanzen aus "Danny Hate" lesen und beurteilen, ob es sich um einen Vorfall von Hassrede handelt.</sample>
    <sample id="263">Wir haben dann diese Annotationen mit DinaHeat, Perspective API, Rewire API, Hateberuta und GPT-4 verglichen. Unser Studie analysierte über 16.000 Annotationen von über 1.000 Annotatoren aus 87 Ländern.</sample>
    <sample id="264">So können wir nun besser herausfinden, welche NLP-Datenmodelle am besten mit dem Thema übereinstimmen. Wir finden, dass es Positionierung gibt und NLP-</sample>
    <sample id="265">Zum Beispiel finden wir, dass die Datensets einiger Modelle am besten zu englischsprachigen Ländern passen. So finden wir für die GPT-4-Sozialakzeptanzanalyse, dass sie am besten zu China und englischsprachigen Ländern passt. Wir finden, dass Dialekt auch am besten zu englischsprachigen Ländern passt.</sample>
    <sample id="266">Wir finden auch eine größere Übereinstimmung mit Menschen, die einen Hochschulabschluss haben. Bei GPT-4 in der Aufgabenstellung zur sozialen Gerechtigkeit finden wir, dass es am besten mit Menschen mit einem Hochschulabschluss oder einem Abschluss der Universität übereinstimmt.</sample>
    <sample id="267">Und wir finden das Gleiche bei Johnny Hate, wo es am ähnlichsten zu Menschen mit einem College-Abschluss ist.</sample>
    <sample id="268">Allerdings, wenn Modelle und Daten, die auf spezifische Bevölkerungsgruppen ausgerichtet sind, einige unvermeidlich zurückgelassen werden,</sample>
    <sample id="269">Ein Beispiel dafür ist, dass Daten, die in Modellen verwendet werden, sich von den männlichen und weiblichen Gegenstücken unterscheiden. Dies findet sich im GPT-4-Sprachverständnis-Aufgabentest sowie in der Dina-Heiß-Analyse.</sample>
    <sample id="270">So, da es eine Position und eine ALD-LP gibt, was können wir tun?</sample>
    <sample id="271">Wir haben einige Empfehlungen für das. Die erste ist, alle relevanten Designentscheidungen während des Rechercheprozesses aufzuzeichnen. Und die andere ist, NLP-Recherche aus der Perspektive des Nutzers durchzuführen.</sample>
    <sample id="272">Unsere dritte Empfehlung ist, spezialisierte Datensätze für bestimmte Gemeinschaften zu erstellen, und ein gutes Beispiel dafür ist die Musicanity-Initiative. Wir möchten betonen, dass inklusive KI nicht nur bedeutet, dass alle Technologien für jeden funktionieren, sondern auch, dass sie für jeden zugänglich sind.</sample>
    <sample id="273">Und so ist diese Präsentation abgeschlossen, aber wenn Sie mehr erfahren möchten, fühlen Sie sich frei, unser Dashboard für die aktuellsten Analyseergebnisse und unsere Arbeit zu überprüfen. Danke.</sample>
    <sample id="274">Die Referentin geht auf drei Probleme von SimulST ein.</sample>
    <sample id="275">Es ist schwierig, soziale und politische Verzerrungen effektiv in Datensätzen beim Training von NLP-Modellen zu reduzieren, da die Verzerrungen aus den Trainingsdaten selbst stammen und es schwierig ist, eine neutrale Definition zu finden.</sample>
    <sample id="276">Hallo, ich bin Siyu Yuan von der Fudan University. Ich bin hier, um unsere Arbeit vorzustellen. Das Extrahieren von Wissen aus der Sprache von linearen Sprachmodellen für die Beschränkung der Sprachplanung.</sample>
    <sample id="277">In all aspects of life, people often plan their actions by following step-by-step instructions in the form of a guided script.</sample>
    <sample id="278">Vorherige Modelle haben Sprachmodelle verwendet, um für abstrakte Konzepte von stereotypischen Aktivitäten wie Kuchen backen zu planen und gezeigt, dass große Sprachmodelle effektiv die Konzepte in die einzelnen Teile zerlegen können.</sample>
    <sample id="279">Allerdings haben frühere Studien, die sich mit Planungszielen für abstrakte Ziele befassten, typischerweise die üblichen Aktivitäten vernachlässigt. Die Planung für Ziele mit spezifischen Zielen und spezifischen Einschränkungen, wie z. B. das Backen eines Schokoladenkuchens, bleibt unerledigt.</sample>
    <sample id="280">In dieser Arbeit definieren wir das Problem der beschränkten Sprachplanung.</sample>
    <sample id="281">Welche Impost-verschiedene Constraints auf der Goalsplanung? Ein abstraktes Goal kann being herren day by different real life specific goals with motivated constraints. A good planner should write scripts that are reasonable and face to constraints.</sample>
    <sample id="282">In dieser Arbeit bewerten und verbessern wir die Konvergenz der Sprachplanung der großen Sprachmodelle.</sample>
    <sample id="283">Es gibt eine Vielzahl von spezifischen Rollen, die dazu dienen, unsere Daten zu überwachen.</sample>
    <sample id="284">Wie können wir diese Codes zuerst erhalten? Wie gezeigt in der Tabelle, erweitern wir die abstrakten Codes mit modifizierten Constraints für die menschliche Lookup-Datenakquisition, die in Abstract GPT</sample>
    <sample id="285">Es sind 3000 verschiedene Mädchen, die in den Beschreibungen bewertet werden, generiert von einem großen Sprachmodell.</sample>
    <sample id="286">Die Tabelle zeigt die Genauigkeit der Ergebnisse. Wir fanden heraus, dass alle linearen Modelle unbefriedigende Ergebnisse liefern.</sample>
    <sample id="287">Bitte gib mir den englischen Inhalt, den du übersetzt haben möchtest.</sample>
    <sample id="288">Die Ergebnisse in der Abbildung zeigen, dass die semantische Vollständigkeit in generierten Skripten akzeptabel ist, aber die Falschheit zu den Einschränkungen nicht garantiert werden kann.</sample>
    <sample id="289">Die Hauptkategorien von Einschränkungen, die in Wayfair vorkommen, sind:

1.  Schlaf
2.  Küche
3.  Badezimmer
4.  Wohnzimmer
5.  Esszimmer
6.  Garten
7.  Büro
8.  Spielzimmer
9.  Garage
10. Außenbereich</sample>
    <sample id="290">Frühe Studien haben gezeigt, dass die Ausgabequalität von Large Language Models bei niedrigen Varianten zu schlechter Leistung führt. Hier wurde die Idee des Overgenerated-Zensurfilters eingeführt, um die Generierungqualität zu verbessern.</sample>
    <sample id="291">Die erste Show konzentrierte sich auf Constraint-Typen mit Beispielen für Intrigue-GPT und auf den auf den spezifischen Gelegenheiten basierenden abstrakten Kontext.</sample>
    <sample id="292">Ja, extrahiere GPT-4-generierte Schlüsselwörter für das spezifische Konzept „Schreien“.</sample>
    <sample id="293">Nächste, ein paar weitere Modelle sind verfügbar: zwei Schritte, wähle die erste Sprache, ja.</sample>
    <sample id="294">Wir konvertieren die Skripte und Geister in abstrakte GPT-Einbettungen und berechnen den Cosinus-Ähnlichkeitswert als Ähnlichkeitswert.</sample>
    <sample id="295">Die Aufmerksamkeit wird auf die Skript-Dateien gelegt, die die Inhalte der Schlüsselwörter der Zielbeschränkung enthalten. Wir werden nur die Klammern verwenden, wenn das Ziel die höchste in der Such-</sample>
    <sample id="296">Weiß zum Messed, in Strati-BT kann jeder Rate schwere Haarschäden. Unsere Methode greift direkt in die Pflanzenbelag ein, sowohl in semantisches Komplettness als auch in Felsones zu der Constriction.</sample>
    <sample id="297">Da große Sprachmodelle teuer zu implementieren sind, ist es wichtig, die Sprachplanung auf eine Reihe kleinerer und spezialisierter Modelle zu konzentrieren. Die Erstellung von Datensätzen ist ein zweistufiger Prozess.</sample>
    <sample id="298">Allerdings haben frühere Studien keine spezifischen Ergebnisse für die geplante Forschung. Und die manuelle Datenanalyse innerhalb der Annotation ist teuer.</sample>
    <sample id="299">Es gibt eine Methode, die auf der Destillation symbolischer Kenntnisse basiert, um die Beschränkung der Größe der Sprachmodell-Datensätze zu überwinden.</sample>
    <sample id="300">Wir werden unsere Methode für die Erstellung des Building Dataset für die eingeschränkte Sprachplanung namens CodeScript beschreiben.</sample>
    <sample id="301">Um 55 Stunden lang spezifische Codes mit Skripten zu generieren, müssen wir die Qualität der Validierung und die Testfälle sicherstellen. Wir bitten Crowd-Sourcer, die Inkonsistenzen im korrekten Beispiel zu finden und zu korrigieren.</sample>
    <sample id="302">Das Bild zeigt eine konstante Verteilung von Kostenprozessen, während ein Finanzprozess eine höhere Dichte in den generellen Retail-Spezifikationen aufweist. Mit Kostenprozessen können kleinere, aber spezialisierte Modelle für Sprachplanung erstellt werden.</sample>
    <sample id="303">Die TF-IDF-Funktion zeigt rote Ergebnisse, was darauf hindeutet, dass kleinere Modelle weniger relevante Skripte von Haarfarben als größere Modelle erkennen, was wahrscheinlich auf eine ungeeignete Datensätze zurückzuführen ist.</sample>
    <sample id="304">In Summary, we established the constrained language planning problem where we develop a constrained language planning ability of large language models and develop a reward generative future measure for large language models.</sample>
    <sample id="305">Wir verwenden Large Language Models, um eine hoch kolorierte Testdatenmenge zu generieren, die für die Konstitutionssprachplanung verwendet werden kann. Wir hoffen, dass die Testdatenmenge, die von der Konstitutionssprachplanung generiert wurde, als Ressource für die Forschung zu Sprachplanung genutzt werden kann.</sample>
    <sample id="306">Danke für Ihre Zeit. Bitte geben Sie detaillierte Informationen zu Ihrem Papier.</sample>
    <sample id="307">Die Sprachgewandtheit von PaLM ist vergleichbar mit dem Stand der künstlichen Intelligenz.</sample>
    <sample id="308">Die wichtigsten Eigenschaften eines Wasserzeichenverfahrens sind: Anwendbarkeit auf eingebettete Dienste, keine Beeinträchtigung der Nutzbarkeit der eingebetteten Dienste, ausreichend für den Angreifer zu erkennen, oder der Angreifer kann das Wasserzeichen leicht entfernen, und Portabilität des Wasserzeichens zu den Diensten des Angreifers.</sample>
    <sample id="309">Die englischen TED Talks wurden in 14 Sprachen übersetzt.</sample>
    <sample id="310">Viele Instanzen.</sample>
    <sample id="311">Delta Cosine und Delta L2.</sample>
    <sample id="312">Modelle, die auf einem mehrsprachigen Encoder basieren, wurden in dieser Aufgabe in zwei Gruppen eingesetzt: Encoder-PT-Modelle mit Point-based-Decodern wie XLM-R+PT-R und BART+PT-R, sowie Encoder-Decoder-Modelle wie BART.</sample>
    <sample id="344">Die Autoren gehen davon aus, dass der Anbieter einen allgemeinen Textkorpus sammeln und die Häufigkeit der Wörter zählen kann.</sample>
    <sample id="345">Hallo zusammen, mein Name ist Shuang. Heute werde ich meine Arbeit vorstellen: Können Named Entity Tags 2023 noch funktionieren? Lass uns anfangen.</sample>
    <sample id="346">Unser Papier untersuchte das Problem der Generalisierung unter Verwendung der sogenannten Named Entity Recognition Aufgabe oder der NER Aufgabe.</sample>
    <sample id="347">Wir haben festgestellt, dass Modelle seit 2003 für fast 20 Jahre die KI entwickelt haben. Und dies wirft natürlich mehrere Probleme auf. Erstens generalisieren Modelle nicht auf mehr als Daten.</sample>
    <sample id="348">Und wenn wir neue Tags entwickeln, was ist nötig für eine gute Generalisierung?</sample>
    <sample id="349">Und gleichzeitig, wenn wir eine schlechte Generalisierung beobachten, was verursacht den Leistungsabfall dieser Modelle?</sample>
    <sample id="350">Um diese Probleme zu untersuchen, erstellen wir den Cono++ Datensatz. Dies ist ein Datensatz, den wir von Reuters News aus dem Jahr 2020 gesammelt und mit den gleichen Annotationen aus den Annotationen von Cono 2003 annotiert haben.</sample>
    <sample id="351">Wir haben über 20 Modelle auf Kernel 2003 feinjustiert. Wir bewerteten sie sowohl auf dem Kernel 3 Testset als auch auf dem Kernel Plus Plus Testset.</sample>
    <sample id="352">Und schließlich haben wir die prozentuale Veränderung von F1 für jeden Modellausgang berechnet, um die Generalisierung jedes Modells zu bewerten.</sample>
    <sample id="353">Also, was wird für eine gute Generierung benötigt?
In unseren Experimenten haben wir festgestellt, dass es drei Hauptbestandteile gibt, die benötigt werden.</sample>
    <sample id="354">Die erste ist die Modellarchitektur. In unseren Experimenten haben wir festgestellt, dass Transformer-Modelle im Allgemeinen besser auf neue Daten generalisieren.</sample>
    <sample id="355">Der zweite Inhaltsbestandteil ist die Modellgröße. Wir haben festgestellt, dass größere Modelle in der Regel zu besserer Generalisierung führen.</sample>
    <sample id="356">Und schließlich wissen wir alle, dass die Anzahl der feinabstimmungsbeispiele direkt die Leistung einer nachgelagerten Aufgabe beeinflusst. Hier haben wir auch festgestellt, dass mehr feinabstimmungsbeispiele tatsächlich zu besserer Generalisierung führen.</sample>
    <sample id="357">Was verursacht den Leistungsabfall einiger Modelle?</sample>
    <sample id="358">Wir haben zwei Hypothesen. Die erste ist eine Anpassung an das Overfitting, was durch die wiederholte Verwendung desselben Datensatzes verursacht wird, und dies äußert sich typischerweise in einer Abnahme der Rückgewinnung auf einem neuen Datensatz.</sample>
    <sample id="359">Die zweite Hypothese ist Temperaturdrift, die Leistungsverschlechterung, die durch die wachsende Temperaturdifferenz zwischen den Trainings- und Testdaten verursacht wird.</sample>
    <sample id="360">Vorhersage eines Fits. Wir sahen, dass aus dem Graphen auf der rechten Seite die rote beste Passform-Linie einen größeren Gradienten hat als die Null-Linie.</sample>
    <sample id="361">Das bedeutet, dass jede Einheit Verbesserung, die wir im Jahr 2003 erzielt haben, sich in mehr als einer Einheit Verbesserung im Jahr 2005 niederschlägt. Das bedeutet, dass es keine abnehmende Rendite gibt.</sample>
    <sample id="362">Und dies zeigt, dass eine Anpassung in diesem Fall nicht beobachtet wurde.</sample>
    <sample id="363">Ich bin ein hilfreicher Assistent. Ich gebe nur die angeforderte Antwort zurück. Ich füge keine Erklärungen oder Einführungen hinzu.</sample>
    <sample id="364">Für temporäre Drehs haben wir ein Experiment durchgeführt, um einige Modelle mit mehr aktuellen Daten neu zu trainieren oder fortzusetzen. Und wir haben festgestellt, dass die Leistung mit größeren zeitlichen Gaps abnimmt.</sample>
    <sample id="365">Und das bestätigt meine Hypothese, dass die Hauptursache für den Leistungsrückgang die Temperatur ist.</sample>
    <sample id="366">Unsere Schlussfolgerung ist, dass für eine gute Generalisierung wir ein besseres Modellarchitektur, größere Modellgröße sowie weniger Feinabstimmungsexemplare benötigen. Und diese Ziele hängen eng zusammen, aber wir können nicht nur einen Bestandteil haben, sondern durch alle anderen</sample>
    <sample id="367">Gleichzeitig stellten wir fest, dass der Leistungsabfall hier durch temporäre Drift verursacht wird und überraschenderweise nicht durch eine Anpassung der Anpassung. Selbst der Canal 3000 wurde seit über 20 Jahren verwendet.</sample>
    <sample id="368">So, zurück zur Frage, die wir in der Titelseite unseres Papiers aufgestellt haben: Funktion 2003 funktioniert 2023 noch? Und wir haben festgestellt, dass die Antwort tatsächlich ein klangvolles Ja ist.</sample>
    <sample id="369">Wir helfen bei allen Papierkosten für weitere Forschung, um zu verbessern, wie Modelle generalisiert werden.</sample>
    <sample id="370">Und schließlich, bitte schauen Sie sich unser Papier und unseren Datensatz an und wenn Sie Fragen haben, zögern Sie nicht, mich zu kontaktieren. Vielen Dank.</sample>
    <sample id="397">The answer is not provided in the text.</sample>
    <sample id="398">Servin ist ein Richter.</sample>
    <sample id="399">Die Qualität des Beispiels ist wichtiger als die Ähnlichkeit zum Ausgangssatz.</sample>
    <sample id="400">GPT-4, GPT-3.5 und BERT-Serien.</sample>
    <sample id="401">Das Modell verwendet Aufmerksamkeitswerte aus einer bestimmten Ebene.</sample>
    <sample id="402">Beispiele für direkte Inferenz sind das Nennen des Namens eines Liedes oder seiner Position.</sample>
    <sample id="403">Fudan University.</sample>
    <sample id="404">Es sind mehrere Autoren beteiligt.</sample>
    <sample id="405">Ja.</sample>
    <sample id="406">Die Autoren haben das Beispiel "Mann" oder "Krieger" gegeben, das typischerweise mit Männern assoziiert wird.</sample>
    <sample id="407">Die Transformer-Modelle generalisieren nicht gut.</sample>
    <sample id="408">Clean data and WSL data.</sample>
    <sample id="409">Vier.</sample>
    <sample id="410">Die Autoren arbeiten mit mehreren Modalitäten.</sample>
    <sample id="439">The authors believe that the ability to integrate and use both pre-training and inference-time knowledge is a less explored area in the field of NLU.</sample>
    <sample id="440">Eing und ihre Kollegin Zhiyang.</sample>
    <sample id="441">Ja.</sample>
    <sample id="442">Die bestehenden Ressourcen für kontextbasierte Übersetzung unterstützen nur begrenzte Arten von kontextbasierten Übersetzungen und eine begrenzte Anzahl von Sprachen.</sample>
    <sample id="443">Hallo. Ich werde über unsere Arbeit zur Lösung indirekter Funktionsausdrücke für Entitätsauswahl sprechen, in der wir den Alternativentitäts-Konzept vorgestellt haben.</sample>
    <sample id="444">Mein Name ist Jawad Hosseini, und dies ist eine Zusammenarbeit mit Filip Radlinski, Silvia Pärty und Anil.</sample>
    <sample id="445">Ookal ist ein Verständnis der Benutzersprache, wenn sie eine Wahl treffen wollen. Betrachten Sie diese alternative Frage: Meinten Sie "einfach für mich" oder "ich hatte das Gefühl"? Hier möchte ein Benutzer zwischen einer dieser beiden Optionen wählen.</sample>
    <sample id="446">Die offensichtlichste Sache ist die Verwendung einer direkten Referenz, zum Beispiel durch Nennung des Namens des Liedes oder seiner Position.</sample>
    <sample id="447">Manchmal ist es besser, einen direkten Freund zu haben, um ein natürlicheres Gespräch zu führen. Dies kann passieren, wenn der Benutzer sich den Namen des Charakters nicht merken kann.</sample>
    <sample id="448">oder die Aussprachen sind zu ähnlich zueinander und schwer zu unterscheiden.</sample>
    <sample id="449">oder wenn der Benutzer eine Präferenz festlegen möchte. Hier sind einige Beispiele für indirekte Präferenzen: zum Beispiel die neuere oder die Melodie, die nicht energisch ist.</sample>
    <sample id="450">Dies ist ein wichtiges Problem in konversationellen Systemen und auch für Benchmarking von LLMs und Entity Recognition.</sample>
    <sample id="451">Wir sind uns eines öffentlichen Datensatzes, eines großen öffentlichen Datensatzes für die Aufgabe, nicht bewusst, also sammeln wir einen mithilfe von Crowdsourcing. Unser Datensatz umfasst drei verschiedene Domänen: Musik, Bücher und die</sample>
    <sample id="452">Die Datensatzerfassungsmethodologie betont Informalität, die Verwendung von Cartoon-Komplikationen.</sample>
    <sample id="453">Der Cartoon hat drei Sprechblasen. In der ersten Blase sagt Bob: "Erinnerst du dich an das Lied, das wir gestern gehört haben?" Und mit dieser Aussage setzt der Dialog fort.</sample>
    <sample id="454">In der zweiten Sprechblase sagt Alice: "Meinst du, es ist für mich oder habe ich dich verärgert?"</sample>
    <sample id="455">Welche ist die alternative Frage? Und in der dritten Sprechblase benutzt Bob eine indirekte Referenz, um eine dieser Entitäten auszuwählen. Zum Beispiel die neue</sample>
    <sample id="456">Wir stellen die ersten und zweiten Sprechblasen automatisch bereit, aber die dritte wird vom Annotator ausgefüllt. Die erste Sprechblase wird aus einigen manuellen Prompts ausgewählt.</sample>
    <sample id="457">Die zweite Frage, die alternative Frage, wird wie folgt generiert:</sample>
    <sample id="458">Wir verwenden immer eine einfache Vorlage. Meinst du A oder B? Wo A und B aus einer Menge von Optionen zufällig ausgewählt werden.</sample>
    <sample id="459">Hier sind die verschiedenen Stichprobenmethoden, die verwendet werden, wenn wir weiter nach oben in der Liste gehen, werden die Einträge ähnlicher zueinander und es ist in der Regel schwieriger, die Ambiguität zu vermeiden.</sample>
    <sample id="460">Die erste Frage ist, wie man schreibt.</sample>
    <sample id="461">Die zweite Klasse ist für Entitäten gedacht, die ähnliche Titel haben, zum Beispiel zwei Bücher mit dem Namen "Der Tag".</sample>
    <sample id="462">Der dritte Punkt ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben und schließlich ähnliche Infoboxen oder Attribute auf Wikipedia haben. Zum Beispiel die gleiche Kategorie oder der gleiche Künstler.</sample>
    <sample id="463">Wenn wir diese alternative Frage den Befragten zeigen, wissen sie den Namen dieser Entitäten nicht, aber sie kennen möglicherweise die Rolle des Charakters.</sample>
    <sample id="464">Also tun wir Folgendes: Wir zeigen einige Hintergrundkenntnisse über die Entitäten. Für Lieder zeigen wir einfach einen Google-Suchlink zu dem</sample>
    <sample id="465">Und dann bitten Sie die Schüler, mindestens einen der Lieder anzuhören und darüber zu lesen. Hier ist ein Beispiel für das Google-Suchergebnis für das Lied "I Can".</sample>
    <sample id="466">Für die Rezept- und Buchdomäne zeigen wir Hintergrundtext von Wikipedia. Für Rezepte zeigen wir zusätzlich ihre Bilder, ebenfalls von Wikipedia, sodass die Annotatoren wissen, wie sie aussehen.</sample>
    <sample id="467">Dann bitten wir die Teilnehmer, eine dieser Entitäten auszuwählen, zum Beispiel hier die erste, und sie mit 3 bis 5 indirekten Referenzsätzen zu beschreiben.</sample>
    <sample id="468">Beispiel 1 mit Klaviermusik. Hier sind einige Beispiele aus unserem Datensatz: Beispiel 1 ohne Worte, nicht das 12-jährige Kind, oder der fiktive, oder kommt aus Aserbaidschan und</sample>
    <sample id="469">Die altentitis-Korpus hat 6.000 alternative Fragen über drei Domänen und hat 42.000 indirekt gefundene Ausdrücke. Ergebnisse mit T5 XL-Modell oder zusammengefasst</sample>
    <sample id="470">Dieses Sprachmodell hat Zugriff auf die exakt gleiche Hintergrundwissen wie die Annotatoren. Die Genauigkeit liegt bei etwa 92 bis 95 %. Aber dies ist nicht wirklich der</sample>
    <sample id="471">Wenn das Sprachmodell Zugriff auf einige teilweise überlappende Hintergrundwissen hat, dann liegt die Genauigkeit zwischen 82 und 87 %, was realistischer ist, beispielsweise wenn das Sprachmodell Hintergrundwissen abruft.</sample>
    <sample id="472">Wenn das Sprachmodell nur Entitätsnamen hat, beträgt die Genauigkeit nur 60 %, also gibt es viel Verbesserungspotenzial. Wir haben auch gezeigt, dass die Modelle domänenübergreifend sind. Hier ist ein Link zu einem Datensatz:</sample>
    <sample id="473">The approach is compared with existing SimulST policies such as the weight key strategy and local agreement.</sample>
    <sample id="474">Université de Paris-Saclay.</sample>
    <sample id="475">Jenny.</sample>
    <sample id="476">Drei.</sample>
    <sample id="477">Hallo, ich bin Sara Babi von der Universität Triest und der Bruno Kessler Stiftung. Und ich werde kurz die Aufmerksamkeit als Leitfaden für die simultane Sprachübersetzung vorstellen, eine gemeinsame Arbeit mit Maciej Negri und Marco Turri.</sample>
    <sample id="478">Simultane Übersetzung ist die Prozesse, bei denen gesprochene Sprache in Text in einer anderen Sprache in Echtzeit übersetzt wird, um die Kommunikation zwischen Sprechern zu ermöglichen.</sample>
    <sample id="479">Und welche Probleme haben die aktuellen Transformer-Modelle? Spezifische Architekturen werden in der Regel trainiert, indem zusätzliche Module eingeführt werden, um optimiert zu werden.</sample>
    <sample id="480">Lange und komplizierte Trainingsverfahren, zum Beispiel Training, das die unterschiedliche Optimierung der Ziele beinhaltet.</sample>
    <sample id="481">Und trainieren und warten mehrere Modelle, um verschiedene Latenzstufen zu erreichen. Zum Beispiel das Trainieren eines Modells mit einer durchschnittlichen Latenz von 1 Sekunde und eines anderen mit 2 Sekunden Latenz usw.</sample>
    <sample id="482">Was ist Ihre Lösung?</sample>
    <sample id="483">Erster, der eine bereits existierende Offline-Modell verwendet, ohne diese neu zu trainieren oder eine spezifische Architektur für CVAE zu adoptieren. Verwende nur ein Modell für jeden latenten CVAE-Algorithmus und behandle die Latenz durch spezifische Parameter.</sample>
    <sample id="484">und die Kenntnisse, die durch den Attention-Mechanismus zwischen der Audioeingabe und der Textausgabe erworben werden, das ist der Cross-Attention-Mechanismus. Und Sie können ein Beispiel unter</sample>
    <sample id="485">Unsere Lösung ist Propose A Dot oder Encoder-Decoder-Attention, und es ist eine Strategie, bei der wir die Seite entweder treffen oder nicht, eine partielle Übersetzung basierend darauf, wo die Aufmerksamkeit Punkte</sample>
    <sample id="486">Ein Wort wird emittiert, wenn die Dichte nicht konzentriert ist, d.h. diese Summe ist unter einem bestimmten Trittschwellenwert Alpha gegenüber weniger Lambda-Sprach-Frames, was bedeutet, dass die Informationsaufnahme ausreicht, um zu speichern.</sample>
    <sample id="487">Zum Beispiel, wenn wir einen Satz betrachten, der über... und unser Modell sagt eine Übersetzung ins Deutsche voraus.</sample>
    <sample id="488">Und wir werden uns den Fokus auf die Frage anschauen, welche</sample>
    <sample id="489">Wir sehen, dass die ersten zwei Wörter auf die am wenigsten empfangenen Sprechrahmen verweisen, während das letzte Wort auf die am wenigsten empfangenen Sprechrahmen verweist, also Lambda-Sprechrahmen.</sample>
    <sample id="490">Dies bedeutet, dass die ersten zwei Wörter wiederholt werden: "Dr."</sample>
    <sample id="491">Während die Summe der größten Tension über einem bestimmten Alpha-Schwellenwert liegt, werden wir das letzte Wort nicht aussprechen und warten auf einen anderen Sprachabschnitt.</sample>
    <sample id="492">Wenn wir fortfahren und wir sehen, dass ein anderer Sprachstack ist und unser Modell vorhersagt andere Wörter und wir schauen uns die Cross-Attention an, die</sample>
    <sample id="493">Wir werden sehen, dass keine Worte zu dem letzten Sprecher passen.</sample>
    <sample id="494">Dies bedeutet, dass diese drei Wörter ein Meme sind.</sample>
    <sample id="495">Wenn Sie das Ergebnis der Datenauswertung betrachten,</sample>
    <sample id="496">Wir werden die simultanen Übersetzungsresultate auf Grafiken auftragen, in denen wir auf einer Seite blaue Werte haben, die die Übersetzungqualität messen, und auf der anderen Seite den durchschnittlichen</sample>
    <sample id="497">Das ist äh die Letzten-Sicherheitsmaßnahme, und wir berücksichtigen auch den Computer-Aware-Average-Lekking, der für ähm die Modelle-Rechenzeit zur Verarbeitung des Outputs verantwortlich ist.</sample>
    <sample id="498">Wir wollen unsere Neugier so hoch wie möglich auf diesem Planeten haben.</sample>
    <sample id="499">Bitte geben Sie den englischen Inhalt an, den Sie übersetzt haben möchten.</sample>
    <sample id="500">Und wir vergleichen mit vorbereitenden Strategien, die auch auf Offline-Modelle angewendet werden, also der Weight-Key-Strategie und der lokalen Vereinbarung. Und wir vergleichen auch mit der Seed of the Architectur, speziell für simultane Präsentationslösung.</sample>
    <sample id="501">Dies sind ältere Ergebnisse der simultanen Übersetzungstrategie auf Deutsch.</sample>
    <sample id="502">Und wir sehen, dass äh ein Data outperforms alle Strategien, die an Offline-Modellen angewendet werden, da die Kurven nach links verschoben sind.</sample>
    <sample id="503">Und wir sehen auch, dass, wenn wir die tatsächliche Ausführungszeit oder die Rechenzeit betrachten, das die schnellste Strategie ist.</sample>
    <sample id="504">Wenn Sie weitere Ergebnisse entdecken möchten, lesen Sie unsere Arbeit. Wir haben auch den Open-Source-Code und die Modelle und die simultane Ausgabe gleichzeitig veröffentlicht, um die Reproduzierbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="505">Ja.</sample>
    <sample id="506">Hallo zusammen, mein Name ist Ying und mein Kollege Zhiyang und ich werden unsere Forschung zur Verbesserung von Modellauswirkungen durch maschinelles Lernen während der Instruktion präsentieren.</sample>
    <sample id="507">Mit den Fortschritten in großen Sprachmodellen begannen viele Forscher, neue Lernparadigmen zu erkunden, indem sie vortrainierte Sprachmodelle für verschiedene Downstream-Aufgaben in einem Parameter und Daten effizient einsetzten.</sample>
    <sample id="508">In letzter Zeit haben viele Studien gezeigt, dass die Instruktionstuning große Sprachmodelle in der Lage stellt, Aufgaben in einer Weise auszuführen, die auf natürliche Anweisungen basiert.</sample>
    <sample id="509">Allerdings konzentrierten die meisten vorherigen Arbeiten auf die Verbesserung der Zero-Shot-Leistung bei sprachbasierten Aufgaben, während Computer-Vision- und Multimodal-Aufgaben vernachlässigt wurden.</sample>
    <sample id="510">Daher wollen wir in dieser Arbeit untersuchen, ob die Instruktionsanpassung von multimodalen Sprachmodellen die Generalisierung auf ungesehene multimodale Aufgaben verbessern kann.</sample>
    <sample id="511">Zusätzlich haben wir während unserer Forschung eine beträchtliche Diskrepanz in der Verfügbarkeit von Instruktionsdatensätzen zwischen NLP und multimodal festgestellt.</sample>
    <sample id="512">Es gibt über 1.600 Sprachmodell-Instruktionsaufgaben, aber es gibt keine großen, öffentlich zugänglichen Multimodell-Instruktionsaufgaben. Dies motiviert uns, ein Multimodell-Instruktions-Tuning-Dataset zu erstellen.</sample>
    <sample id="513">Hier präsentieren wir die erste Benchmark-Datensatz für die Feinabstimmung von Multimodalen Instruktionsmodellen, der aus 62 verschiedenen multimodalen Aufgaben besteht, die in 10 Kategorien unterteilt sind.</sample>
    <sample id="514">Dies hat aus einer vorhandenen offenen Datensammlung abgeleitet, und jede Aufgabe ist mit fünf Experten-Schreiben-Anweisungen ausgestattet.</sample>
    <sample id="515">Für die Untersuchung von multimodalen Instruktionsanweisungen verwenden wir unsere vorgeschlagenen Daten. Wir nehmen OFA, ein vereinheitlichtes multimodales vortrainiertes Modell, als unser Basismodell. OFA verwendet eine vereinheitlichte Vokabular für Sprache, Bild-Token und die Koordinaten eines bounding box.</sample>
    <sample id="516">Hier zeigen wir einige Beispielinstanzen aus unserem medizinischen Wörterbuch.</sample>
    <sample id="517">Die Verarbeitung verschiedener Eingabe- und Ausgabedaten.</sample>
    <sample id="518">Wir folgen dem Muster von OFA und formulieren alle Aufgaben in einem vereinheitlichten Sequenz-zu-Sequenz-Format, in dem der Eingabetext, Bilder, Anweisungen und umgebende Kästchen in derselben Token-Sprache dargestellt werden.</sample>
    <sample id="519">Okay, jetzt werde ich über multimodale Instruktionstraining sprechen.</sample>
    <sample id="520">Für die 20-Tage-Aufgabe verwenden wir 53 Aufgaben aus der Negru-Gruppe zum Training und wir haben jeweils 10.000 Instanzen pro Aufgabe. Zum Testen reservieren wir die gesamte CommonSense-Gruppe zum Testen und wir wählen zusätzlich fünf Aufgaben aus Wiki und der verschiedenen Kategorie.</sample>
    <sample id="521">Wir verwenden alle Instanzen in der Testmenge für jede Aufgabe. Äh, zusätzlich werden wir zufällig 20 Aufgaben aus der Testmenge der natürlichen Instruktion als Trainingsaufgaben auswählen.</sample>
    <sample id="522">So verwenden wir ein vortrainiertes OpenAI Large Model als Basismodell. Während des Trainings werden für alle Aufgaben Instanzen erstellt. Jede Instanz wird zufällig mit einer von ihren fünf Instruktionstemplaten kombiniert.</sample>
    <sample id="523">So, für den Test der Fischaufgabe führen wir insgesamt fünf Experimente durch, indem wir das Modell anhand der fünf Anweisungen in jedem Experiment bewerten.</sample>
    <sample id="524">Wir haben die Leistung und die Standardabweichung der Leistung über alle fünf Experimente analysiert.</sample>
    <sample id="525">Wenn diese Aufgabe eine multimodale Klassifikationsaufgabe ist, geben wir die Genauigkeit an. Wenn es sich um eine multimodale Generierungsaufgabe handelt, geben wir Rouge-L an. Für eine Aufgabengenerierungsaufgabe geben wir Rouge-L an.</sample>
    <sample id="526">Wir haben auch ein zusätzliches Validierungsverfahren namens CSTA hinzugefügt. Dieses Verfahren stellt sicher, dass die Modelle bei der Ausführung derselben Aufgabe unabhängig von der Variation der Wortfolge konsistent das gleiche Ergebnis liefern.</sample>
    <sample id="527">Hier ist unser Hauptergebnis, wie wir sehen können, dass die Instruktionstuning die Leistung von OS und OFS signifikant verbessert hat bei der Durchführung von Multimodallaufaufgaben.</sample>
    <sample id="528">Aus dem Transferlernen aus mehreren Instruktionsdatensätzen ergeben sich Unternehmensvorteile.</sample>
    <sample id="529">Hier können wir sehen, dass je mehr Aufgaben der Modell bessere Leistungen erbringt und in der Zwischenzeit eine geringere Sensitivität aufweist.</sample>
    <sample id="530">Wir verwenden auch die Wahrscheinlichkeitsanweisung, wir verwenden die Wahrscheinlichkeitsanweisung gegenüber der fünf Anweisung, wie wir sehen, kann die Verwendung der Wahrscheinlichkeitsanweisung die Gesamtleistung des Modells verbessern und die Empfindlichkeit des Radios erheblich erhöhen.</sample>
    <sample id="531">Dies zeigt den Effekt verschiedener Fine-Tuning-Strategien auf die Modellsensitivität. Wie wir sehen können, erzielt das Fine-Tuning von einem Transferlernen auf dem Natural Instruction Dataset eine viel bessere Sensitivität im Vergleich zum ursprünglichen OA-Modell.</sample>
    <sample id="532">Wir können auch die Datenübertragung von der MNIST-Instruktionsdatensatz helfen, die KI eine viel bessere Leistung auf dem MNIST-Instruktionsdatensatz zu erzielen.</sample>
    <sample id="533">Wir haben einen Vorschlag für die erste große multimodale Injektion zu einem Datensatz, mit dem wir die Fähigkeit von OAI verbessern, die Wahrscheinlichkeit verschiedener Transferlerntechniken zu zeigen und die Vorteile mit einem neuen Metrik-Konzept zu illustrieren.</sample>
    <sample id="534">Wir sammeln derzeit ein viel größeres multimodales Instruktions-Tuning-Datenset mit etwa 150 zusätzlichen verschiedenen Sprachaufgaben und wir werden sie bald veröffentlichen. Dies ist ein QR-Code für unser Datenset und Modell. Danke.</sample>
    <sample id="535">University of Trento.</sample>
    <sample id="536">Javad Hosseini.</sample>
    <sample id="562">Hallo zusammen, ich bin Kostas Finas und ich freue mich, Sie zu unserer Diskussion über unsere ACL 2023-Papiere "Language Model Acceptability Judgments are Not Always Robust to Context" begrüßen zu dürfen.</sample>
    <sample id="563">Es ist ein Werk von John Gotter, Arno Müller, Kanishka Mishra, Karan Frentles, Roger Levy und Athena.</sample>
    <sample id="564">In dieser Arbeit revisitieren wir den Minimalpair-Begriff.</sample>
    <sample id="565">So bewerten minimale Paare im Wesentlichen Sprachmodelle auf der Grundlage von Akzeptanzurteilen, die auch Grammatik, wie z. B. Plural, Syntax oder Akzeptanz in Bezug auf Stereotypen, wie z. B. Hautfarben, beinhalten.</sample>
    <sample id="566">Und in diesem minimalen Paar-Paradigma ist die typische Art, Sprachmodelle zu bewerten, dass man eine akzeptable Satz oder grammatikalischer Satz zeigt und dann einen inakzeptablen Satz oder einen ungrammatikalischen Satz zeigt.</sample>
    <sample id="567">Und dann die Hoffnungen des Modells legen im Grunde mehr Wahrscheinlichkeit auf die akzeptablen Fälle.</sample>
    <sample id="568">Der aktuelle MPP-Pipeline erlaubt uns im Grunde nicht, die Akzeptanz von Modellen gegenüber längeren Sätzen zu bewerten.</sample>
    <sample id="569">Diese großen Sprachmodelle kommen mit längeren und längeren Kontextfenstern, daher ist es entscheidend, die Modelle über das gesamte Kontextfenster hinweg auf Akzeptabilität zu bewerten.</sample>
    <sample id="570">Und das ist es, was wir hier versuchen. Wir versuchen, den PP-Pipeline zu überprüfen, indem wir das Modell bitten, die Akzeptabilität über einen längeren Zeitraum zu bewerten.</sample>
    <sample id="571">So, das ist der Ansatz. Also, was wir tun, ist, diese längeren Sequenzen zu simulieren. Wir lesen die Datensätze selbst ein und dann erstellen wir Sätze, indem wir akzeptable oder inakzeptable Sätze aus diesen Datensätzen auswählen.</sample>
    <sample id="572">Zum Beispiel haben wir hier ein typisches grammatikalisches Paar aus dem Blip-Datensatz aus dem Adjektiv-Insel-Kontext ausgewählt.</sample>
    <sample id="573">Und was wir tun, ist, dass wir längere Sequenzen neu erstellen, die akzeptabel sind und die die gleiche Übereinstimmung der grammatikalischen Struktur haben. Wir extrahieren grammatikalische Sätze aus dem adjektivischen Teil.</sample>
    <sample id="574">und dann fügen wir es als Präfix sowohl der akzeptablen Abfrage als auch der unakzeptablen Abfrage hinzu.</sample>
    <sample id="575">Wir können dasselbe tun, indem wir in der gleichen Übereinstimmung inakzeptable Sätze auswählen, und das könnte auch verwendet werden, um die akzeptable Qualität des Modells zu testen.</sample>
    <sample id="576">Und wir können das auch tun, indem wir Sätze aus einem anderen Teil der Daten oder einem anderen Datensatz auswählen. Das nennen wir die Mismatch-Strategie.</sample>
    <sample id="577">Hier stammen die Sätze noch aus relevanten Datensätzen, aber nicht aus dem gleichen Datensatz, den Sie bewerten. Und wir können das Gleiche für Unakzeptabilität tun.</sample>
    <sample id="578">Schließlich können wir Sätze aus einem völlig unzusammenhängenden Bereich auswählen, d. h. Wikipedia.</sample>
    <sample id="579">Das wird uns sagen, ob die Modelle die Akzeptanzbewertung tatsächlich von irgendeinem Kontakt beeinflusst haben.</sample>
    <sample id="580">Entweder kommt der Kontext von einem anderen Teil des Datensatzes oder er ist völlig irrelevant für den aktuellen Satz, den wir gerade lesen.</sample>
    <sample id="581">So, wie funktioniert das Modell? Zuerst betrachten wir die Wikipedia-Sätze, die völlig irrelevant für das aktuelle Frage-Paar sind, und dort finden wir, dass die MPP-Urteile meist robust für willkürliche Kontexte sind.</sample>
    <sample id="582">Wir haben die Kontextlänge auf bis zu 1024 erweitert, um die GPT und GPT-2 Modelle optimal zu nutzen, und wir sehen hier in der orangefarbenen Linie, dass die MPP-Urteile relativ stabil sind.</sample>
    <sample id="583">Ich kann den englischen Inhalt nicht sehen. Bitte stelle ihn bereit, damit ich ihn übersetzen kann.</sample>
    <sample id="584">Hier wählen wir Sätze aus akzeptablen und inakzeptablen Domänen aus dem gleichen Blimp-Person-Taxim-Datensatz aus.</sample>
    <sample id="585">Und dort sehen wir, dass die MPP-Urteile entweder signifikant steigen oder signifikant sinken, wenn akzeptable Präfixe oder unakzeptable Präfixe akzeptiert werden.</sample>
    <sample id="586">Aber wenn wir die Struktur abgleichen, d. h. wenn wir Sätze aus dem gleichen Phänomen in der Schuld-Perspektive auswählen,</sample>
    <sample id="587">Wir sehen einen massiven Anstieg oder einen massiven Rückgang der MPP-Bewertung für das Modell, abhängig davon, ob der gewählte Präfix akzeptabel oder inakzeptabel ist.</sample>
    <sample id="588">Das ist sehr groß. Dieser Effekt nimmt über die gesamte Kontextlinie an und würde wahrscheinlich neuere Sprachmodelle mit großem Kontext beeinflussen.</sample>
    <sample id="589">Warum beeinflusst der Match-Präfix die Entscheidungen des Sprachmodells so stark?</sample>
    <sample id="590">Eine Reihe von Analysen, bei denen wir versuchen, den Eingabesatz durch das Anfügen von Rauschen an den Eingabe zu erhalten, wobei wir versuchen, die relevante Struktur zu erhalten. Nach Durchführung mehrerer dieser Störungen</sample>
    <sample id="591">Wir stellen fest, dass keiner dieser Geräusche tatsächlich die Modellierung in Bezug auf seine Darstellung der PP-Urteil verändert.</sample>
    <sample id="592">Im Grunde genommen haben wir festgestellt, dass die Modelle empfindlich auf die Wortwahl und Ähnlichkeiten reagieren.</sample>
    <sample id="593">Das ist, wo wir die Sätze in dem akzeptablen Bereich einfügen, sehen wir eine ähnliche Zunahme aller Störungen, und wenn wir die Sätze in dem akzeptablen Bereich stören, sehen wir einen Rückgang der MPP-Urteile in ähnlichen Fällen.</sample>
    <sample id="594">Die wichtigsten Erkenntnisse unserer Arbeit sind, dass Sprachmodelle empfindlich auf latente syntaktische und semantische Merkmale sind, die sich über Sätze hinweg teilen.</sample>
    <sample id="595">Und die MPP-Bewertung, die Art und Weise, wie wir es derzeit mit kurzen und einfachen Sätzen eingeben, kann nicht vollständig das abstrakte Wissen der Sprachmodelle über den Kontext erfassen.</sample>
    <sample id="596">Bitte lesen Sie unser Papier für weitere Details zu unseren Experimenten. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="597">Ein ungeordnetes Multiset von Token.</sample>
    <sample id="598">55</sample>
    <sample id="626">The best alignment method to use for DEplain is the `mess_align` method.</sample>
    <sample id="627">Schwach überwachtes Lernen ermöglicht es, neuronale Netze robust aufzutrainieren, wenn die Trainingsdaten Label-Rauschen enthalten, sodass die trainierten Modelle weiterhin generalisieren können.</sample>
    <sample id="628">Die Dokumente in DEplain-web wurden mit manuellen und automatischen Alignmentmethoden ausgerichtet.</sample>
    <sample id="629">Der CoNLL++-Datensatz wurde aus Reuters-Nachrichten aus dem Jahr 2020 gesammelt und mit den gleichen Annotationen aus den CoNLL 2003-Richtlinien versehen.</sample>
    <sample id="630">Hallo zusammen, mein Name ist Justin John von der Penn State University. Heute präsentiere ich einen Arbeitsprojekt: Cross-lingual Semantic Parsing in mehreren natürlichen Sprachen und verschiedene Repräsentationen.</sample>
    <sample id="631">So, Semantic Parsing ist eine Aufgabe, um semantische Repräsentationen von Benutzeranfragen zu erstellen, wie z.B. "sequel" und "lambda calculus".</sample>
    <sample id="632">CrossLingual Semantic Parsing ist die Aufgabe, Abfragen in mehreren natürlichen Sprachen in mehrere semantische Repräsentationen zu übersetzen.</sample>
    <sample id="633">Wir müssen die Abfrage in mehreren natürlichen Sprachen mit Hilfe von neuronalen Modellen in SQL oder GraphQL übersetzen und dabei die</sample>
    <sample id="634">Es gibt verschiedene mehrsprachige Parsing-Modelle, die separat auf Datensätzen von limitierten Aufgaben und Anwendungen evaluiert werden. Zum Beispiel</sample>
    <sample id="635">Es gibt Lücken in der Abdeckung bestimmter natürlicher Sprachen. Chinesisch fehlt, und</sample>
    <sample id="636">Klicke auf die Abdeckung bestimmter Mini-Repräsentationen.</sample>
    <sample id="637">Der Lambda-Kalkulus ist eine</sample>
    <sample id="638">oder nur ein bestimmtes neuronales Modell, zum Beispiel nur ein einzelnes Modell, um zu bewerten</sample>
    <sample id="639">Um dies zu erreichen, schlagen wir ein Beispiel vor und stellen eine einheitliche Datensatzbeispiel für Cross-Linguale Semantik in mehreren natürlichen Sprachen bereit und in Repräsentations</sample>
    <sample id="640">Es enthält 90 Datensätze in 5 verschiedenen Domänen, 5 Sentiment-Analyseaufgaben, 8 semantische Repräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien.</sample>
    <sample id="641">Und um unseren Benchmark besser zu bewerten, betrachten wir die sechs Einstellungen für Training und Evaluierung.</sample>
    <sample id="642">Der erste ist ein Test. Verwende die Google Translate API, um den Quelltext in die Zielsprache zu übersetzen, dann verwende ein mehrsprachiges Modell, um zu trainieren und zu bewerten.</sample>
    <sample id="643">und zum Beispiel, wir trainieren das englische Modell auf einer englischen Anfrage und dann inferieren wir, wir übersetzen die deutsche Anfrage mit einer API ins Englische und dann verwenden wir das trainierte Modell, um die Antwort zu vorhersagen.</sample>
    <sample id="644">Ich werde auch monolinguale Modelle testen.</sample>
    <sample id="645" />
    <sample id="646">Wir testen auch die Monolingual Few-Shot-Einstellung, indem wir Modelle mit nur 1 % der Trainingsdaten trainieren.</sample>
    <sample id="647">und hat ein monolinguale Modell, welches wir trainieren, ein monolinguale Modell für alle Sprachen</sample>
    <sample id="648">Zum Beispiel, wir geben die deutschen, englischen und chinesischen Suchanfragen zusammen ein, um ein Sprachmodell zu trainieren, und während der Inferenz können wir dieses Modell verwenden, um zu</sample>
    <sample id="649">Übersetzen Sie den englischen Inhalt nach Deutsch.</sample>
    <sample id="650">Und wir berücksichtigen auch Cross-Lingual Zero-Shot und Few-Shot-Transfer zwischen einer Ausgangssprache und einer anderen Sprache.</sample>
    <sample id="651">Während des Trainings trainieren wir unser englische Query oder die Kombination von englischen und deutschen Few-Shot-Queries, um ein mehrsprachiges Modell zu trainieren, um die Sequenz der Wörter vorherzusagen.</sample>
    <sample id="652">Und wir werden auch viele interessante Ergebnisse finden. So, bezüglich der Analyse von monolingualen Modellen, werden wir sie anhand zweier Datensätze bewerten.</sample>
    <sample id="653">einschließlich Encoder-PT-R, das für mehrsprachige vortrainierte Encoder mit Pointer-basierten Decodern wie XLM-R+PT-R und BART+PT-R steht.</sample>
    <sample id="654">Und wir bewerten auch Encoder-Decoder-Modelle, nämlich mehrsprachig vortrainierte Encoder-Decoder-Modelle, wie z.B. mpart und MT-F.</sample>
    <sample id="655">Wir haben festgestellt, dass Encoder-Decoder die beste Leistung auf allen neun Datensätzen erzielen.</sample>
    <sample id="656">Und wir bewerten auf M5 und Beispiel XLM-R plus PDR-Multilingual Search.</sample>
    <sample id="657">Ohne diese Encoder, Decoder oder Encoder-PCR kann die Verbesserung durch das Training in einer Mischung aus verschiedenen Sprach-</sample>
    <sample id="658">Und wir haben festgestellt, dass dies liegt daran, dass die meisten der großen natürlichen Sprachen eine Leistungssteigerung erzielen, außer Englisch, bei dem die Leistung in sieben Datensätzen sinkt und nur in drei Datensätzen steigt.</sample>
    <sample id="659">Ich denke, das ist das Ergebnis von Multilingualität.</sample>
    <sample id="660">Wir vergleichen auch die Cross-Lingua-Leistung der</sample>
    <sample id="661">In dieser Abbildung kreuzt die blaue Linie die Winkel-Null-Schubtransferenz, die orange Linie die Winkel-Null-Schubtransferenz, während die grünen Linien die Modellierung der Schärfe darstellen.</sample>
    <sample id="662">Wir haben festgestellt, dass bei der Vergleichung der grünen und orangen Linie, bei der Null-Schuss-Einstellung, die Transfer-Gap-Leistung signifikant ist, und bei der Vergleichung der blauen und orangen Linie, bei der Few-Schuss-Einstellung, der Transfer-Gap schnell verkürzt wird.</sample>
    <sample id="663">Wir haben auch einige andere interessante Ergebnisse gefunden. Zum Beispiel führt der Encoder-Decoder-Ansatz eine vorherige Arbeit oder erreicht vergleichbare Ergebnisse. Bei der Verarbeitung von englischer natürlicher Sprache verbessert er signifikant die Leistung von Few-Shot-Aufgaben in der natürlichen Sprachverarbeitung.</sample>
    <sample id="664">Modellierung von Sprachmodellen wie Codez und Bloom sind immer noch in der Entwicklung für Kreuzsprachen- und Personenaufgaben.</sample>
    <sample id="665">Ein umfassender Beispiel-Pool, ein einheitlicher Benchmark für Cross-Angle Sentiment-Parsing mit mehreren natürlichen Sprachrepräsentationen.</sample>
    <sample id="666">Willkommen zu einer umfassenden Benchmarking-Studie über drei repräsentative Arten von mehrsprachigen Sprachmodellen. Und unsere Ergebnisse zeigen viele interessante Erkenntnisse usw. Und willkommen zu unserem Paper und Code. Danke für das Lesen.</sample>
    <sample id="667">The provided text lists four categories: 

1.  
2.  
3.  
4.</sample>
    <sample id="668">Nein, mehrsprachige LLMs wie Codex und Bloom sind für CLSP noch nicht ausreichend.</sample>
    <sample id="695">Die Methode geht mit der Mehrdeutigkeit der Permutationen um, indem sie die Ausrichtung als Teil des Trainings verwendet.</sample>
    <sample id="696">Die Fairness eines nachgeschalteten NLP-Modells wird definiert durch die Möglichkeit, dass Menschen mit gegensätzlichen politischen Meinungen marginalisiert werden und Hassreden gegen Minderheitengruppen ungehindert verbreitet werden können.</sample>
    <sample id="697">Janis Lavergne.</sample>
    <sample id="698">Kostas Finas.</sample>
    <sample id="699">Maira.</sample>
    <sample id="700">Tropikalismus bezieht sich auf die Verwendung von Begriffen, die mit tropischen Regionen und Kulturen assoziiert werden, wie z. B. "vibrant" und "curvaceous" bei der Beschreibung von Latina-Frauen.</sample>
    <sample id="701">Die Autoren haben die Beschreibungen der Zielgruppen erstellt, indem sie sich auf die Beziehungen der Wörter zu ihrer Identität und die Unterscheidung von der weißen Norm konzentrierten.</sample>
    <sample id="702">The work used the PMI score to measure context usage.</sample>
    <sample id="703">DrBERT ist eine Version von BERT mit 7 GB Natches, während ChuBERT eine Version von BERT mit 4 GB Natches ist.</sample>
    <sample id="751">Sieben.</sample>
    <sample id="752">Iteratives Transferlernen aktualisiert das Modell durch Training auf dem neuesten Datensatz, der in jeder Runde der aktiven Lern- und Annotationen gesammelt wurde.</sample>
    <sample id="753">The goal of the dataset is to understand users' language when they want to make a choice.</sample>
    <sample id="754">Ein Angreifer kann Modellparameter über einen EaaS extrahieren, indem er die Einbettung von Sätzen auf der Basis von BPCA verwendet.</sample>
    <sample id="755">Drei.</sample>
    <sample id="756">The provided text does not specify the number of annotators used to create the original dataset.</sample>
    <sample id="757">University of Washington.</sample>
    <sample id="758">Beantworte die folgende Frage kurz und bündig basierend auf dem englischen Inhalt: Wie lautet das Beispiel mit dem Begrenzer auf der linken Seite?

The governor is on the left.</sample>
    <sample id="759">The state of the art for dialogue systems is capable of measuring the rates at which chat models will commit various thematic errors.</sample>
    <sample id="760">Die Modelle werden mit längeren Kontextfenstern entwickelt, daher ist es wichtig, ihre Akzeptanz über das gesamte Fenster zu bewerten.</sample>
    <sample id="761">Ja, das mehrsprachige Training führte zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell in einigen Datensätzen.</sample>
    <sample id="762">Nein.</sample>
    <sample id="763">The provided text does not contain any information about MT metrics used for evaluation.</sample>
    <sample id="764">Ja, die Regression wirkt sich auf die Generalisierung bei bestimmten NER-Typen aus.</sample>
    <sample id="765">Der Text erwähnt nicht, dass Positionalität für NLP wichtig ist.</sample>
    <sample id="766">The text states that multilingual LLMs like BLOOM were adapted using adapters.</sample>
    <sample id="767">The model used for transfer learning is a zero-shot performance on the annotated dataset.</sample>
    <sample id="768">The actual form of the prompting doesn't have a big influence in the case of several short prompts.</sample>
    <sample id="769">Drei.</sample>
    <sample id="770">The proposed method shows a higher plot size in the general retail specific goals.</sample>
    <sample id="771">Shu Han.</sample>
    <sample id="772">Ja.</sample>
    <sample id="773">Es werden 14 kleineren Modellen experimentiert.</sample>
    <sample id="774">The unified multimodal pre-training model is used as the base model.</sample>
    <sample id="833">Google Research.</sample>
    <sample id="834">Stony Brook University.</sample>
    <sample id="835">Englisch und Chinesisch.</sample>
    <sample id="836">Shambin P.</sample>
    <sample id="837">Longformer und Normalized Longformer.</sample>
    <sample id="838">53 Aufgaben für Training und 62 Aufgaben für Tests.</sample>
    <sample id="839">Es sind 24 Autoren an der Arbeit beteiligt.</sample>
    <sample id="840">The authors conducted experiments on the AG News, Mind, SST2, and E2000 datasets.</sample>
    <sample id="876">NACHOS ist ein Datensatz mit medizinischen Bilddaten.</sample>
    <sample id="877">Sajid Bilal.</sample>
    <sample id="878">Die Prompt-Strategie hat einen großen Einfluss auf die Leistung von LLMs für die Übersetzung.</sample>
    <sample id="879">University of Puerto Rico at Mayagüez.</sample>
    <sample id="880">Die 5 Anweisungen der Expert*innen sind nicht im Text enthalten.</sample>
    <sample id="881">Die Autoren schlagen vor, ein Korreferenzlösungstask zu verwenden, um die Fähigkeit zu prüfen, auf Wissen aus verschiedenen Quellen zurückzugreifen.</sample>
    <sample id="882">Hallo zusammen, mein Name ist Said Bilal, und ich werde Ihnen eine kurze Übersicht über die Arbeit geben, die wir im Rahmen der Übersetzung von Prompt-Prompt-Translation, der Bewertung von Strategien und der Leistung durchgeführt haben. Dies ist eine gemeinsame Arbeit mit meinen Kollegen von Google Translate.</sample>
    <sample id="883">PaLM ist ein 540 Milliarden Parameter großes Sprachmodell, vorgestellt im Jahr 2022. Es wurde auf einer großen Sammlung von Texten trainiert, die 100 Milliarden Token umfassen.</sample>
    <sample id="884">Der Thema Publikation ist der State of the Art in hundertfach der Art.</sample>
    <sample id="885">In dieser Arbeit präsentieren wir eine erste systematische Studie zur Verwendung von kleineren Prompts für maschinelles Lernen.</sample>
    <sample id="886">Wir werden die Übertragbarkeit von Sprachmodellen mithilfe der Best Practices der MT-Community untersuchen. Dies beinhaltet die Verwendung der neuesten Testsets, um eine Überlappung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden.</sample>
    <sample id="887">Wir vergleichen den aktuellen Stand der Systeme, also die besten performenden Systeme, also die WTI-Öl-Rohöl-Qualität.</sample>
    <sample id="888">Wir verwenden state-of-the-art Neuronale Netzwerke und zusätzlich zeigen wir Expert-basierte Validierungsergebnisse. Schließlich geben wir Empfehlungen für Prompt-Auswahlstrategien.</sample>
    <sample id="889">Die Prompting hat einen großen Einfluss auf die Performance der LLMs für Translation. Wie wir in einem einfachen Experiment sehen können, bei dem wir eine Single-Shot-Prompt verwenden und zwei verschiedene Prompts für eine gleiche Satz verwenden,</sample>
    <sample id="890">In etwa 516 von 1000 Sätzen ist die Differenz größer als ein Blurry Point.</sample>
    <sample id="891">und dies kann in extremen Fällen bis zu 40 Punkte erreichen. Es ist also wichtig, eine gute Prompting-Strategie zu wählen.</sample>
    <sample id="892">In unseren Experimenten haben wir eine kleine für eine fünfschritt-Prompting-Strategie verwendet, bei der wir einfach den Satz markieren, den wir dem System geben, mit der Sprache, die es</sample>
    <sample id="893">Ich kann den englischen Inhalt nicht sehen. Bitte stelle ihn bereit, damit ich ihn übersetzen kann.</sample>
    <sample id="894">Die tatsächliche Form der Druckung hat keinen großen Einfluss im Fall mehrerer kurzer Blöcke.</sample>
    <sample id="895">Es ist entscheidend für Zero-Shot-Prompting und wenn wir gehen, wie in unserem Fall zu Few-Shot-Prompting, gibt es kaum einen Unterschied zur eigentlichen Form des Prompts.</sample>
    <sample id="896">Es sind Beispiele für Curry, die die meisten der auf der Weise</sample>
    <sample id="897">Die Zusammenfassung unserer experimentellen Ergebnisse besagt, dass die Beispielqualität wichtiger ist als die Ähnlichkeit zur Ausgangssatz.</sample>
    <sample id="898">Es ist wichtig, die Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die Auswahl von Prompts aus dem Trainingsdatensatz der WMT-Bewertungen oder der Deft-Daten.</sample>
    <sample id="899">Die Daten sind viel besser kuratiert und mit höherer Qualität, sodass die Trainingsdaten informativer sind und die Ergebnisse besser sind. Daher eine bessere Leistung bei der Verwendung des Deep Learning.</sample>
    <sample id="900">Nichtsdestotrotz spezialisiert sich der Artsystem auf den Zustand der anderen Systeme, hat aber eine substanzielle Vorteile gegenüber den anderen Übersetzungen. Aber die Form kommt ziemlich nahe an unserem Übersetzungsystem, unserem neuen Fall, welches zu unserem mit Google Translate.</sample>
    <sample id="901">Während wir von der menschlichen Innovation profitieren, führen wir sie mit dem MQA-Framework durch. Die Fluenz von Palm ist vergleichbar mit dem Zustand der AR-Systeme, aber der Hauptunterschied kommt von der Genauigkeit.</sample>
    <sample id="902">In particular, of most common errors or misnomers.</sample>
    <sample id="903">Es scheint, dass Palm dazu verwendet, eine bessere Übersetzung zu erstellen, indem es Teile des Ausgangssatzes entfernt, die in der Übersetzung enthalten sind.</sample>
    <sample id="904">Die Kategorie der Staaten außerhalb des Staates der Vereinigten Staaten ist niedriger als die des Staates der Vereinigten Staaten, was ein zusätzliches Signal ist.</sample>
    <sample id="905">Das Programm liefert wirklich flüssige Ergebnisse, aber es hat immer noch einige Probleme mit der Zeichenkodierung.</sample>
    <sample id="906">Und das war's für diese sehr kurze Übersicht. Für mehr Details bitte meine heutige vollständige Präsentation des Artikels. Vielen Dank.</sample>
    <sample id="907">Hallo, ich bin Tamay, ein PhD-Student an der Talant University in Deutschland. In diesem Video möchte ich unsere Arbeit vorstellen. Bitte werfen Sie einen kritischen Blick auf unsere Forschung.</sample>
    <sample id="908">Dies ist ein gemeinsames Werk, das sich auf die Seele konzentriert. Es hat glatte Pfade und die Asche von Stephen und die britische Klara.</sample>
    <sample id="909">Ich bin ein hilfreicher Assistent. Ich gebe nur die angeforderte Antwort zurück. Ich füge keine Erklärungen oder Einführungen hinzu.</sample>
    <sample id="910">In Vikisubmission wird die Daten nicht manuell gelabelt. Stattdessen werden die Daten mit vordefinierten Labeling-Quellen gelabelt, wie z. B. einfachen Feature-Regeln, Wissensbasen oder lokaler Code-Sourcing. Wie in der Abbildung unterhalb illustriert ist.</sample>
    <sample id="911">Im Vergleich zu menschlichen Annotationen sind die Weak-Annotationen deutlich günstiger, jedoch sind sie auch verrauscht, was bedeutet, dass ein gewisser Teil der Annotationen falsch ist.</sample>
    <sample id="912">Wenn wir versuchen, neue Netzwerke auf wöchentlichen Arbeitsdaten zu trainieren, können die neuen Netzwerke die Trainingsdaten auswendig lernen und nicht generalisieren.</sample>
    <sample id="913">In der Regel werden Überwachungstraining-Algorithmen vorgeschlagen, um neue Daten unter solchen Lärmbedingungen robust zu trainieren, sodass die trainierten Modelle weiterhin generalisieren.</sample>
    <sample id="914">In den letzten Arbeiten im WSL, wobei WSL für Weekly Supported Learning steht, gibt es eine häufige Behauptung, dass Menschen behaupten, dass nur trainierte Modelle und die auf wöchentlich gelabelten Daten erzielten hohe Leistungen unklare Tests sind.</sample>
    <sample id="915">Technisch gesehen ist dies keine Rolle, aber es gibt einen Haken.</sample>
    <sample id="916">Manche Leute nehmen an, dass es einen zusätzlichen Clean-Validierungsdatensatz für World War II gibt.</sample>
    <sample id="917">Wie kann man dieses Problem lösen, aber dies impliziert, dass zusätzliche manuelle Annotationen in Weekly Supervised Learning erforderlich sind. Aber wie ein Elefant im Raum, ist diese Notwendigkeit oft übersehen.</sample>
    <sample id="918">Die oben genannte Adopt-Lisa-Methode fordert drei Forschungsfragen. Zuerst ist die Datenvalidierung für WSL erforderlich oder können wir vielleicht einen Noise-Validierungsansatz verwenden?</sample>
    <sample id="919">Zweitens: Wenn bereinigte Daten erforderlich sind oder bereinigte Daten für die Funktion von WSL erforderlich sind, wie viele bereinigte Stichproben benötigen Sie? Schließlich sollten nur bereinigte Stichproben für die Validierung verwendet werden, oder gibt es bessere Möglichkeiten, die bereinigten Stichproben zu nutzen?</sample>
    <sample id="920">Die vorliegende Studie beantwortet Forschungsfragen im Rahmen der Arbeit und unsere Ergebnisse sind als fortlaufend zu betrachten.</sample>
    <sample id="921">Zuerst stellen wir fest, dass interessanterweise recente WSL-Nachrichten tatsächlich reinen White-Day-Samples zu arbeiten brauchen.</sample>
    <sample id="922">Andernfalls ist ein großer Leistungsverlust zu erwarten, wie in dieser Abbildung dargestellt. Wenn es keine Clean-Validationspunkte gibt, können die Trendmodelle nicht auf die ursprünglichen Objektlabels generalisieren.</sample>
    <sample id="923">Bitte gib mir den englischen Inhalt, den du übersetzt haben möchtest.</sample>
    <sample id="924">Dies deutet darauf hin, dass WSR-Ansätze tatsächlich sauber gelabelte Daten verarbeiten müssen, um ordnungsgemäß zu funktionieren, und die Kosten für die Beschaffung sauberer Validierungsbeispiele sollten nicht übersehen werden.</sample>
    <sample id="925">Ein weiterer wichtiger Befund ist, dass die Erhöhung der Anzahl der sauberen Validierungsstichproben dazu beitragen wird, dass WSL bessere Leistungen erbringt, wie in der Abbildung auf der linken Seite gezeigt wird.</sample>
    <sample id="926">Typischerweise benötigen wir nur 23 Beispiele pro Klasse, um eine hohe Genauigkeit zu erreichen.</sample>
    <sample id="927">Aber das ist noch nicht das Ende der Geschichte, denn wenn wir entweder entscheiden, saubere Proben zu verwenden, dann werden wir direkt auf ihnen bessere Leistung erzielen.</sample>
    <sample id="928">Die Grafik zeigt den Leistungsunterschied zwischen fünf-tuning-Ansätzen, die direkt auf den bereinigten Daten angewendet werden, und WSL-Ansätzen, die die bereinigten Daten zur Validierung verwenden.</sample>
    <sample id="929">Wenn wir zehn Beispiele pro Klasse haben, beginnen die direkten Fundamente, WSR-Ansätze zu betreten.</sample>
    <sample id="930">Schließlich kann die im vorherigen WSR-Ansatz behauptete Leistungsverbesserung leicht durch die Erlaubnis erreicht werden, die Daten während der Clean-Validierungsphase kontinuierlich zu optimieren.</sample>
    <sample id="931">Wie wir aus den Zahlen sehen können, übertrifft das Varian-Modell, der FTW, zunächst die komplexeren WSL-Methoden wie die</sample>
    <sample id="932">Allerdings, wenn wir die kontinuierliche Funktion auf den sauberen Proben fortsetzen, dann funktioniert FTW genauso gut wie andere Methoden.</sample>
    <sample id="933">In der Praxis gibt es keinen Grund, komplexere WSL-Nachrichten zu wählen, die mehr Rechenzeit und Festplattenspeicher benötigen.</sample>
    <sample id="934">Wir haben festgestellt, dass die jüngsten WSL-Ansätze die Reinigung, manuelle Annotation und die Überprüfung von Daten für sie zu kompliziert machen. Ihre Leistung und Praktikabilität werden stark überschätzt.</sample>
    <sample id="935">Unsere konkreten Empfehlungen für zukünftige Arbeit sind folgende:</sample>
    <sample id="936">Zuerst, berichten Sie über die Kriterien für die Modellauswahl. Zum Beispiel berichten Sie, wenn die Modellauswahl auf saubere Validierungsstichproben basiert.</sample>
    <sample id="937">Dritte, kontinuierliche Feinabstimmung ist eine einfache, aber starke Baseline, die in zukünftiger Arbeit mit W&amp;A berücksichtigt werden sollte.</sample>
    <sample id="938">Finaly, we have open source software called Qark code. You can find it where the QR code on this slide. Please feel free to check it out. Thank you and enjoy the conference.</sample>
    <sample id="939">Die gängige Praxis ist die Verwendung menschlicher Bewertung, z. B. durch das Beurteilen, welche von zwei Gesprächen besser ist, oder durch die Bewertung von Gesprächen anhand einer Likert-Skala.</sample>
    <sample id="940">Fünf.</sample>
    <sample id="941">Entity-spezifisches Wissen (z. B. Servin ist ein Richter) und Weltwissen (z. B. Servin und Kea trafen sich im Park).</sample>
    <sample id="942">Ja, der Code ist auf GitHub verfügbar.</sample>
    <sample id="943">Nein, die Annotatoren sind nicht ausgewogen in Bezug auf demografische Gruppen.</sample>
    <sample id="944">Verschiedene Störungen wurden hinzugefügt, um die relevante Struktur zu bewahren.</sample>
    <sample id="945">Eine dimensionale Bewertung bedeutet, die verschiedenen Aspekte der Chatqualität zu untersuchen, um die Stärken und Schwächen des Modells besser zu verstehen.</sample>
    <sample id="946">University of Science and Technology of China.</sample>
    <sample id="947">In den Fällen von Zero-Shot- und One-Shot-Prompting ist die Form des Prompts wichtig.</sample>
    <sample id="978">Die Autoren haben mehrere dialogmodelle evaluiert.</sample>
    <sample id="979">Es sind 16 Autoren an der Arbeit beteiligt.</sample>
    <sample id="980">A good planner should set reasonable and feasible goals.</sample>
    <sample id="981">Es sind keine Autoren genannt.</sample>
    <sample id="982">Vasudha.</sample>
    <sample id="983">The authors are affiliated with the University of Warsaw.</sample>
    <sample id="1021">PaLM hat Schwierigkeiten mit der Interpretation von Kontext, der Generierung von kohärentem Text und der Beantwortung von Fragen, die auf Schlussfolgerungen basieren.</sample>
    <sample id="1022">Hallo, ich bin James Finch und ich bin Sarah Finch. Und heute werden wir Ihnen alles über ABC Eval erzählen, einen neuen dimensionalen Ansatz zur Bewertung von konversationeller KI.</sample>
    <sample id="1023">Diese Arbeit wurde vom Emory NLP-Labor durchgeführt, geleitet von Professor Gino Choi an der Emory University, in Zusammenarbeit mit Amazon Alexa AI.</sample>
    <sample id="1024">Was sagt der neueste Dialogmodell und du möchtest sehen, wie gut es sich mit dem aktuellen Stand des</sample>
    <sample id="1025">Die übliche Praxis ist, menschliche Bewertungen zu verwenden, z. B. indem man menschlichen Richtern lässt, welche von zwei Gesprächen besser ist, oder indem man Gespräche anhand einer Likert-Skala bewertet.</sample>
    <sample id="1026">Diese Ansätze eignen sich gut, um umfassende Bewertungen der gesamten Dialogqualität zu liefern, aber die Dialogqualität hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chatqualität bewerten, um die Stärken und Schwächen des Modells genauer zu verstehen.</sample>
    <sample id="1027">Ein Ansatz besteht darin, menschlichen Richtern zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, z. B. die Relevanz der Modellantworten, unter Verwendung bestehender vergleichender oder Likert-Skala-Methoden.</sample>
    <sample id="1028">Allerdings glauben wir, dass es eine präzisere und zuverlässigere Strategie zur Bewertung dimensionaler Dialoge gibt.</sample>
    <sample id="1029">Unser Ansatz versucht, die Subjektivität menschlicher Bewertung zu reduzieren, indem explizit angegeben wird, ob oder nicht jede Modellantwort bestimmte Verhaltensweisen ausdrückt, wie z. B. das Beantworten mit irrelevanter Information oder das Widersprechen der eigenen Architektur.</sample>
    <sample id="1030">Wir nennen diesen Ansatz das Annotieren von Verhaltensweisen in Chat, oder ABCEval, kurz. Wir haben diese Methode entwickelt, um Chatmodellverhalten umfassend abzudecken, die als Vorschläge zur Verbesserung der Chatqualität und der jüngsten Literatur angesehen werden.</sample>
    <sample id="1031">ABC-Eval ist in der Lage, die Raten zu messen, mit denen Chatmodelle verschiedene thematische Fehler begehen.</sample>
    <sample id="1032">Zum Beispiel misst ABC EVL die Anzahl der Runden, in denen ein Chatbot seinen Partner ignoriert oder etwas sagt, das nicht relevant ist.</sample>
    <sample id="1033">widerspricht sich selbst oder seinem Partner, halluziniert falsche Fakten oder verletzt die allgemeine Vernunft und zeigt bei Erfolg oder Misserfolg der Empathie.</sample>
    <sample id="1034">Um zu bestimmen, welche Art der Bewertung am effektivsten ist, haben wir vier modernste Chatmodelle ausgewählt und sie anhand von 100 menschlichen Gesprächsbeispielen pro Modell mit der ABC-Methode bewertet.</sample>
    <sample id="1035">Zur Vergleichbarkeit haben wir diese Gespräche auch mit drei bestehenden Methoden bewertet: Likert-Ratings auf der Turnebene, Likert-Ratings auf der Dialogebene und paarweise Vergleiche.</sample>
    <sample id="1036">Für jede der bestehenden Methoden haben wir Bewertungen zu acht der am häufigsten gemessenen Aspekte der Dialoge gesammelt, da dies die Standardpraxis für die Bewertung von Chatmodellen entlang mehrerer Dimensionen ist.</sample>
    <sample id="1037">Die Analyse der Ergebnisse dieser Bewertung ergab, dass die Bewertung von ABC-Eval-Verhaltenslabels insgesamt zuverlässiger ist als die von bestehenden Methoden, gemessen an der inter-Annotator-Übereinstimmung bei 100 doppelt gelabelten Gesprächen.</sample>
    <sample id="1038">Zusätzlich sind ABC-EV-Labels prädiktiver für die Gesamtqualität der Konversation im Vergleich zu Metriken, die von bestehenden Methoden erzeugt werden, wie in dieser einfachen linearen Regressionsanalyse gezeigt.</sample>
    <sample id="1039">Zum Beispiel können Sie sehen, wie die Messung des Anteils von Wendungen zwischen Selbst- und Partner-Konflikten 5 % und 10 % der Gesprächsqualität bzw. erklärt. Während die durchschnittliche Likert-Konsistenzbewertung nur 4 % oder</sample>
    <sample id="1040">Schließlich haben wir überprüft, ob jede Evaluationsmetrik einen einzigartigen Aspekt der Chatqualität erfasst, indem wir eine schrittweise lineare Regression verwendeten.</sample>
    <sample id="1041">Sie können sehen, wie die Kombination aller ABC-V-Metriken über 25 % der Gesprächsqualität erklärt. Und wenn Sie eine Metrik nach der anderen entfernen, führt dies in den meisten Fällen zum Verlust einer beträchtlichen Menge an Informationen über die Qualität.</sample>
    <sample id="1042">Auf der anderen Seite erklärt die Kombination aller Level-Likert-Maße viel weniger über die Qualität, und weniger dieser Maße tragen einzigartige Informationen.</sample>
    <sample id="1043">Dies ist ein zuverlässiges, informatives und distinktes ABC-Evaluationsmatrix, das uns ermöglicht, konversationelle KI mit einer höheren Auflösung zu bewerten, als es frühere Methoden erreichen konnten.</sample>
    <sample id="1044">Man kann sehen, dass die Ergebnisse unseres Experiments zeigen, dass mehrere Herausforderungen bestehen bleiben und präzise quantifiziert wurden. Zum Beispiel haben wir festgestellt, dass die Debatten bei etwa 20 % ihrer Antworten logische Fehler aufweisen.</sample>
    <sample id="1045">Sie liefern irrelevante Informationen in etwa 15 % der Antworten und widersprechen sich oder ihrem Partner etwa 10 % der Antworten.</sample>
    <sample id="1046">Angesichts der rasanten Fortschritte in dem Feld könnten viele dieser AQR-Werte bei neuen Modellen, die seit unserer Bewertung durchgeführt wurden, sinken. Dies ist jedoch gerade umso wichtiger, zuverlässige und präzise Bewertungsmesswerte für den Vergleich von Modellen zu entwickeln.</sample>
    <sample id="1047">Wir hoffen, dass ABC Eval von anderen in der Branche genutzt werden kann als ein bedeutsamer Schritt in diese Richtung, und wir freuen uns darauf, zu sehen, wie sich konversationelle KI in den kommenden Monaten und Jahren weiterentwickelt. Vielen Dank fürs Zuschauen.</sample>
    <sample id="1048">Emory University.</sample>
    <sample id="1049">CFT steht für "recent WSL approaches".</sample>
    <sample id="1050">6</sample>
    <sample id="1051">Hallo, mein Name ist Kayo Yen und ich werde unsere Arbeit vorstellen, die mit dem Titel "Wann ist Übersetzung Kontext erforderlich? Eine datengesteuerte multilinguale Exploration" lautet. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandez, Emily Underwood, Andrea Martinez und Graham Neubauer erstellt.</sample>
    <sample id="1052">So, a lot of translations depend on context. For example, how would we translate "moland" in the sentence?</sample>
    <sample id="1053">Während der vorherige Satz besagte, dass Dinge gefährlich werden könnten, wenn der Minister es herausfindet, dann bezieht sich More auf einen Spion. Aber wenn der vorherige Satz besagte, dass es alles sein könnte, Doktor, dann bezieht sich More auf eine Geburt.</sample>
    <sample id="1054">So, die Padding-Kontext, die Bedeutung des Wortes ändert sich und daher die Übersetzung ändert sich zu "war".</sample>
    <sample id="1055">Allerdings ist es ziemlich schwierig zu beurteilen, wie gut Modelle Fälle wie diesen bewältigen können. Erstens, weil nur ein kleiner Teil der Übersetzung im Kontext steht, was globale Metriken wie Blue nicht in die Lage versetzt, diese Übersetzungen zu erfassen.</sample>
    <sample id="1056">Und einige Leute haben vorgeschlagen, eine gezielte Bewertung von Kontextabhängigen Übersetzungen durchzuführen, aber diese Ressourcen unterstützen nur begrenzte Arten von Kontextabhängigen Übersetzungen und eine begrenzte Menge an Sprachen. Da sie in der Regel auf Fachwissen und menschlicher Kuratierung basieren.</sample>
    <sample id="1057">In dieser Arbeit versuchen wir, diese beiden Fragen zu beantworten: Erstens, wann erfordert eine Übersetzung Kontext, und zweitens, wie gut Modelle diese Fälle bewältigen.</sample>
    <sample id="1058">Die Antwort auf die erste Frage beginnt damit, wie viel die Wortwahl in der Übersetzung von Kontext abhängt.</sample>
    <sample id="1059">Und in der vorherigen Arbeit haben wir die Kontext-Information als Maß für die Verwendung von Kontexten in maschinellen Übersetzungsmodellen vorgestellt. Und dies wird durch die Messung der Menge an Informationen, die der Kontext über die Ziel-Wortfolge liefert, gegeben die Ausgangs-Wortfolge, erreicht.</sample>
    <sample id="1060">Du kannst dir CXM als die Informationsgewinnung vorstellen, indem du Kontext dem Modell gibst.</sample>
    <sample id="1061">In dieser Arbeit vergleichen wir X-SMI mit 2X-SMI, welches Kontextnutzung auf Satzebene oder auf Wortebene messen kann. Wir können Wörter mit hohem P-SMI als solche betrachten, die Kontext für die Übersetzung benötigen.</sample>
    <sample id="1062">Nun analysieren wir Wörter mit hoher P-S-E-M-I, um Muster zwischen diesen Wörtern zu finden.</sample>
    <sample id="1063">Und wir führen unsere Analyse auf Transkripten von TED Talks durch, die von transkribiert wurden aus Englisch in 14 verschiedene Sprachen.</sample>
    <sample id="1064">Bevor man meine Analysen auf drei verschiedenen Ebenen betrachtet, schauen wir uns zunächst die Part of Speech Tags an, die einen hohen Mittelwert von P, S, X, M, R haben.</sample>
    <sample id="1065">Und dies ermöglicht uns, ein Beispiel für Dualpronomen im Arabischen zu finden, die beide die hohe PS-e-i haben, und dies kann erklärt werden, weil Englisch keine Dualpronomen hat, also muss man den Kontext bestimmen, ob ein Pronomen dual ist, wenn es in das Arabische übertragen wird.</sample>
    <sample id="1066">Und ähnlich finden wir, dass bestimmte Sprachen auch Kontext benötigen, wenn wir die passende Verbform wählen. Wir dann nach Vokabeln suchen, die hohe Häufigkeit haben über all ihre verschiedenen Erscheinungen.</sample>
    <sample id="1067">Und dies hilft bei der Identifizierung von Fällen wie dem hier, wo im Chinesischen die Kontraktion der Präpositionen ausgesprochen werden, um sicherzustellen, dass Sie die gleiche Übersetzung innerhalb des Dokuments verwenden.</sample>
    <sample id="1068">Und ähnlich haben wir festgestellt, dass die katholische Kirche die Transzendenz ihrer Rechte formalisiert hat.</sample>
    <sample id="1069">Und schließlich betrachten wir verschiedene individuelle Token, die hohe PSI haben, und dies ermöglicht es uns, Phänomene zu identifizieren, die nicht wirklich durch das Wort selbst erfasst werden können, sondern eher in der statistischen Struktur ausgedrückt werden, wie zum Beispiel die Ellipsen-Lösung.</sample>
    <sample id="1070">So verwenden wir unsere Ergebnisse aus unserer Analyse, um einen Benchmarking für die Dokumenten-Niveau-Übersetzung zu entwerfen.</sample>
    <sample id="1071">Für jedes der fünf Diskursphänomene, die wir identifiziert haben, haben wir Tags erstellt, um auf unumstößliche Weise Wörter zu identifizieren, die sich auf das Phänomen beziehen. Und wir nennen unsere Tags den mehrsprachigen Diskursbewusstsein oder Muda-Tag.</sample>
    <sample id="1072">Bicanen ähm haben auch festgestellt, dass verschiedene Sprachen unterschiedliche Proportionen dieses diskreten Phänomens haben.</sample>
    <sample id="1073">Wir verwenden den Mudentagger, indem wir den Tagger auf die parallelen Korpora anwenden, die wir für die Bewertung verwenden wollen, und wir wenden unsere Translationsmatrix der Wahl auf die kontextabhängigen Beispiele an, die der Mudentagger identifiziert hat.</sample>
    <sample id="1074">Und schließlich verwenden wir unseren Benchmarks als weitere Metrik, um verschiedene Modelle auf Dokumentebene zu bewerten.</sample>
    <sample id="1075">Zunächst einmal, wenn wir Korpus-basierte Metriken verwenden, so bei Blue finden wir, dass die kontextuelle diagnostische Modelle die beste Leistung haben.</sample>
    <sample id="1076">Aber wenn Sie Kontext verwenden, sind die Modelle dort am besten. Und wenn Sie Wort-Aussage messen, haben die Modelle mit und ohne Kontext eine vergleichbare Leistung.</sample>
    <sample id="1077">Dies ist ein Demonstrator, es ist schwierig, das beste Dokumentenlevel-Übersetzungssystem zu bestimmen, wenn man Korpuslevelmetriken verwendet.</sample>
    <sample id="1078">Nun verwenden wir die Muta-Benchmark-Validierungsmodelle und stellen fest, dass Modelle, die Kontext verwenden, für bestimmte diskrete Phänomene signifikant genauer sind, wie z. B. Formalität und lexikalische Kohäsion.</sample>
    <sample id="1079">Diese Modelle sind nicht viel besser als Modelle, die keine Kontextinformationen oder andere Phänomene wie Ellipsen, Pronomen und Verbformen verwenden. Dies deutet darauf hin, dass wir mehr Fortschritte bei der Dokumenten-Level-Übersetzung sehen müssen.</sample>
    <sample id="1080">Wir haben auch verschiedene kommerzielle Systeme verglichen, und unsere Benchmarks zeigen, dass DeepL im Allgemeinen genauer ist als Google Translate für die Dokumentenübersetzung.</sample>
    <sample id="1081">Zusammenfassend führen wir eine datengesteuerte Analyse über 14 Sprachpaare durch, um die ein Übersetzungsproblem zu identifizieren, das Kontext erfordert.</sample>
    <sample id="1082">Und dann verwenden wir unsere Feinheiten, um einen Benchmarking für die Dokumentenlevel-Maschinentranslation zu erstellen, was uns helfen kann, zu identifizieren, welche diskreten Phänomene Modelle gut oder nicht verarbeiten können und welche Übersetzungssysteme gut für die Dokumentenlevel-Übersetzung sind.</sample>
    <sample id="1083">Danke sehr für Ihre Teilnahme.</sample>
    <sample id="1084">Justin John</sample>
    <sample id="1121">The new method has no name.</sample>
    <sample id="1122">Die Autoren beschreiben die Methode der „markierten Wörter“ als eine Methode zur Identifizierung der Wörter, die markierte Gruppen von markierten Gruppen unterscheiden.</sample>
    <sample id="1123">University of Washington.</sample>
    <sample id="1124">Prague</sample>
    <sample id="1125">James Finch und Sarah Finch.</sample>
    <sample id="1126">4</sample>
    <sample id="1127">Grammaticality, like blame, syntax, etc.</sample>
    <sample id="1161">Die Abkürzungen der fünf Methoden für die erste Forschungsfrage sind WLS, OLS, GLS, SLS und GLS.</sample>
    <sample id="1162">Biomedical and clinical documentation stream tasks.</sample>
    <sample id="1226">4 GB von Wikipedia.</sample>
    <sample id="1227">Adam Szpyrkowski.</sample>
    <sample id="1228">Die Leistung der Modelle verschlechterte sich mit größerer zeitlicher Verzögerung, was die Hypothese bestätigte, dass die zeitliche Verzögerung die Hauptursache für den Leistungsverlust ist.</sample>
    <sample id="1269">Die Token sind korrekt, aber nicht in der richtigen Reihenfolge.</sample>
    <sample id="1270">Die Autoren empfehlen eine erhöhte Transparenz bei Methoden zur Vorurteilsminderung, weil die positiven Stereotypen möglicherweise auf einer übermäßigen Wertorientierung oder anderen Anti-Stereotypen-Methoden beruhen, die zu schädlichen Mustern führen.</sample>
    <sample id="1271">Inakzeptable Minimalpaareingaben sind ungrammatische Sätze.</sample>
    <sample id="1272">Die Autoren haben die Leistung von Dr. BERT und dem Gewicht und Tokenizer von PubMedBERT verglichen.</sample>
    <sample id="1273">Inner Annotator Agreement.</sample>
    <sample id="1274">Wikipedia.</sample>
    <sample id="1275">Die Autoren gehören der Universität von Toronto an.</sample>
    <sample id="1276">MultiInstruct unterscheidet sich von anderen Benchmarks dadurch, dass es sich auf die Verbesserung der Generalisierung von Instruction Tuning auf multimodale Aufgaben konzentriert, während frühere Arbeiten sich hauptsächlich auf Sprachaufgaben konzentrierten.</sample>
    <sample id="1277">Es sind mehrere Autoren beteiligt.</sample>
    <sample id="1278">The binary coordination is defined by measuring length in characters, the first column in syllables, the middle column, and in words, the right column.</sample>
    <sample id="1279">The prompts used in this study were on average 100 words long.</sample>
    <sample id="1280">The results indicate that smaller models can perform as well as larger models when properly trained on suitable datasets.</sample>
    <sample id="1281">Hallo, ich bin Yannis Lavrakis, ich arbeite derzeit an einem robusten Sprachtrainingsmodell im Französischen für die Biowissenschaften und klinische Bereiche.</sample>
    <sample id="1282">In dieser Präsentation werden wir zunächst über Sprachmodellierung im Gesundheitswesen sprechen. Dann werden wir die Hauptbeiträge unseres Artikels vorstellen.</sample>
    <sample id="1283">Wir haben das erste biomedizinische Modell in Französisch eingeführt, namens Docteur Bert, welches auf Roberta basiert und auf Natjos trainiert wurde, welches ein Datensatz medizinischer Krankenakten-Daten aus dem</sample>
    <sample id="1284">Wir haben auch einen Vergleich von Modellen mit mehreren Punkt-Präzisions-Einstellungen und Datenquellen vorgestellt. Dann präsentieren wir unsere Ergebnisse an 11 verschiedenen biomedizinischen und klinischen Datensätzen.</sample>
    <sample id="1285">Zusammenfassend können wir sagen, dass die Experimente vielversprechend sind und weitere Details zu den Zugriffsbedingungen für die Daten bereitgestellt werden.</sample>
    <sample id="1286">Seit seiner Veröffentlichung im Jahr 2018 ist es zu einem der effektivsten Ansätze zur Lösung von Aufgaben der Verarbeitung natürlicher Sprache geworden und bietet im Vergleich zu historischen statischen und kontextualisierten Methoden wie Word2Vec, FastText oder GloVe einen großen Leistungsgewinn.</sample>
    <sample id="1287" />
    <sample id="1288">Spezialmodelle für andere Sprachen wie Care sind oft auf kontinuierlichem Training basierend, aufgrund des Mangels an Domänen-Daten.</sample>
    <sample id="1289">Allerdings hatte Franz kein neues Pinselmodell für die medizinische Zeichnung und die Schrift.</sample>
    <sample id="1290">Wir fragen uns also die Frage, welche die am besten geeignete Datensammlung für eine breite Palette von Anwendungen ist, und diese Rohdaten sind eine gute Ersatz für klinische Daten.</sample>
    <sample id="1291">Bitte gib mir den englischen Inhalt, den du übersetzt haben möchtest.</sample>
    <sample id="1292">Nachdem wir uns gefragt haben, wie wir ein Spezialmodell auf französischen Daten trainieren müssen, ist es für ein Gigabyte, ein Gigabyte oder ein</sample>
    <sample id="1293">Zunächst ist die Frage, wir trainieren und vergleichen vier Modelle von Grund auf. Die erste Version von DoctorBERT hat 7 GB an Nachos. Die zweite Version hat 4 GB an Set of Nachos.</sample>
    <sample id="1294">Eine kleinere Version von BERT, die ein klinisches Modell ist, wir verwenden 4 Gigabyte von Token-Sätzen, die aus klinischen Daten stammen. Und eine fein abgestimmte Version von BERT, wir haben eine Mischung aus 4 Gigabyte von Texten und 4 Gigabyte von klinischen Daten.</sample>
    <sample id="1295">Zusätzlich zu dieser Vergleiche führen wir einen Stream von Daten über das kontinuierliche Training ein, um die Auswirkungen der Trainingsstrategie zu analysieren.</sample>
    <sample id="1296">Ein Bison wurde auf einem 4-Gigabyte-Set von Nachos trainiert, und ein anderer Bison wurde ebenfalls auf einem 4-Gigabyte-Set von klarem Wasser trainiert.</sample>
    <sample id="1297">Eine finale One-Base-von-einem-englischen-Sprachmodell, bei dem BERT trainiert wurde, auf einer vorherigen Menge von Texten. Insgesamt haben wir sieben Modelle.</sample>
    <sample id="1298">Die Evaluierung von sieben Modellen zeigt, dass sowohl öffentliche als auch private Modelle Aufgaben wie Objekterkennung, Klassifizierung, Part-of-Speech-Tagging und Frage-Antwort-Suchen bewältigen können.</sample>
    <sample id="1299">Das ist ein Modell im Vergleich zu einem 6-Bit-Modell, welches 108 Gigabyte, 4 Gigabyte, 64 Gigabyte, 128 Gigabyte und 16 Gigabyte beträgt.</sample>
    <sample id="1300">Die Auswahl eines Hervorhebungsstils, der bei der Aufgabe am besten funktioniert, basiert auf den Daten desselben Typs, auf denen der Hervorhebungsstil verwendet wurde.</sample>
    <sample id="1301">Allerdings können wir diese Daten aus äh wir können Daten aus äh wir beobachten, dass Daten aus ursprünglichen Quellen scheinen zuverlässiger zu sein. Wir beobachten auch, dass die Verwendung mehr Daten zu besserer Leistung führt.</sample>
    <sample id="1302">In the world, from scratch, pretraining seems to obtain higher performance on most of the tasks.</sample>
    <sample id="1303">Allerdings hat unser Experiment gezeigt, dass die Verwendung der Gewichtung und Tokenisierung von PubMed-Wörtern beim Training auf einem 4 GB großen Datensatz von Nachrichten ähnliche Ergebnisse wie die Verwendung von Ref-Doktor-Wörtern für 4 GB Daten aus Scratch liefert.</sample>
    <sample id="1304">Dies ist nicht der Fall, da das Modell nicht auf Camembert basiert und Tokenizer leiden unter Stabilitätsproblemen.</sample>
    <sample id="1305">Schließlich, als Schlussfolgerung, äh unser System hat eine bessere Leistung bei der 9/11-Aufgabe, übertrifft global das Ergebnis des generischen Modells hier, kann aber</sample>
    <sample id="1306">Wir sehen, dass spezialisierte Daten besser sind, mehr spezialisierte Daten sind besser, aber es skaliert nicht.</sample>
    <sample id="1307">Alle vortrainierten Modelle, die von Natos erhalten wurden, sind frei verfügbar und auf dem Jugendface und alle Trainingsskripte sind auf unserer GitHub-Repository verfügbar.</sample>
    <sample id="1308">So, vielen Dank für die Präsentation. Wir freuen uns auf die nächsten Schritte in dieser Sitzung.</sample>
    <sample id="1309">Stream-Modelltraining und kontinuierliche Vorhersage.</sample>
    <sample id="1310">Der Faktor der Überanpassung, der speziell auf die Wiederverwendung von Tests zurückzuführen ist, beträgt größer als 1.</sample>
    <sample id="1311">Die Qualität der Vereinfachung wurde durch das Feintuning von Sprachmodellen zur Erzeugung vereinfachter Texte aus komplexen Texten beurteilt.</sample>
    <sample id="1312">Ja, Sprachmodelle haben unterschiedliche politische Vorurteile.</sample>
    <sample id="1313">Hallo, mein Name ist Matthias Landemann und heute gebe ich Ihnen eine kurze Einführung zu unserer Arbeit über kompositorische Generalisierung ohne Bäume, unter Verwendung von Multi-Set-Tagging und latenten Permutationen.</sample>
    <sample id="1314">Dies ist eine Zusammenarbeit mit meinen Beratern Alexander Koller und Eva Dittmann.</sample>
    <sample id="1315">Kompositionelle Generalisierung kann als die Fähigkeit eines Lerners verstanden werden, tiefere Rekursionen und unbekannte Kombinationen von Phrasen zu verarbeiten, die während des Trainings einzeln gesehen wurden.</sample>
    <sample id="1316">Im Kontext der semantischen Analyse könnte das Testen der kompositorischen Verallgemeinerung so aussehen: Wie üblich haben wir einen Trainingssatz von Äußerungen. In diesem Fall: Das Mädchen schlief und Mary wusste, dass das Mädchen schlief.</sample>
    <sample id="1317">Diese Satzstrukturen werden mit logischen Formen gepaart, die die Kernaspekte ihrer Bedeutung repräsentieren.</sample>
    <sample id="1318">Im Gegensatz zur Standard-Maschinellen-Lern-Evaluierung stammt das Testset nicht aus derselben Verteilung, sondern enthält strukturell und inlogisch form.</sample>
    <sample id="1319">In diesem Beispiel hat das Modell während des Trainings flache Rekursion gezeigt und wurde auf einem Beispiel mit tiefer Rekursion getestet.</sample>
    <sample id="1320">Neuronale Sequenz-zu-Sequenz-Modelle haben Schwierigkeiten mit dieser Art der Out-of-Distribution-Generalisierung und erzeugen oft Ausgaben, die sich von der Eingabe lösen.</sample>
    <sample id="1321">Insbesondere scheitern sie oft daran, die systematischen Korrespondenzen zwischen Eingabe und Ausgabe zu reproduzieren, wie diese im Beispiel dargestellt sind.</sample>
    <sample id="1322">Ein beliebter Ansatz zur Bewältigung dessen ist die Integration von Bäumen in die</sample>
    <sample id="1323">Die Bäume sollen den kompositorischen Prozess erfassen, der die Äußerungen mit der logischen Form in Verbindung bringt.</sample>
    <sample id="1324">Das ist gut geschrieben, aber Bäume werden normalerweise nicht gegeben, man muss sie beschaffen.</sample>
    <sample id="1325">Dies kann kompliziert sein und manchmal ein rechenintensiver Prozess. Typischerweise beinhaltet dies erheblichen formalisierungspezifischen Vorverarbeitung der logischen Formen. Zum Beispiel, um Variablen zu handhaben.</sample>
    <sample id="1326">Das Beschaffen von Bäumen kann auch spezialisierte Grammatik-Erkennungsverfahren beinhalten.</sample>
    <sample id="1327">In dieser Arbeit verwenden wir keine Bäume und führen ein neues Sequenz-zu-Sequenz-Modell ein, das direkt die Korrespondenz zwischen Fragmenten der Eingabe und Fragmenten der Ausgabe modelliert.</sample>
    <sample id="1328">Zum ersten Mal zeigen wir eine starke Generalisierung auf tiefere Rekursion, ohne auf Bäume zurückzugreifen.</sample>
    <sample id="1329">Ich möchte einen Ansatz, der die Ausgabe aus der Eingabe in zwei Schritten vorhersagt.</sample>
    <sample id="1330">Zuerst versehen wir jeden Eingabetoken mit einem ungeordneten Multiset von Token, die im Ausgabeprozess erscheinen werden.</sample>
    <sample id="1331">Nach dem ersten Schritt haben wir die richtigen Token, aber sie sind nicht die richtige Reihenfolge.</sample>
    <sample id="1332">Deshalb verwenden wir im zweiten Schritt ein anderes Modell, um eine Permutation vorherzusagen, um sie in der richtigen Reihenfolge zu bringen.</sample>
    <sample id="1333">Wir stellen eine neue Methode zur Vorhersage einer Permutation vor, die keine harten Beschränkungen auf die möglichen Permutationen legt. Dies macht unseren Ansatz recht flexibel und ausdrucksstark.</sample>
    <sample id="1334">Konzeptuell funktioniert unser Permutationsmodell ungefähr wie die</sample>
    <sample id="1335">Wir gehen von links nach rechts durch die Ausgabe und bestimmen, welcher Mengen-Token an jeder Position eingefügt werden soll. Für die erste Ausgabeposition wählen wir einfach eins, wie hervorgehoben in der Tabelle.</sample>
    <sample id="1336">Dann springen wir zum nächsten Multiset-Token, um das zweite Token in der Ausgabe zu bestimmen.</sample>
    <sample id="1337">Um den dritten Token in der Ausgabe auf ähnliche Weise zu bestimmen, springen wir zu einem anderen Multiset-Token. Wir setzen diesen Prozess fort.</sample>
    <sample id="1338">Bis jeder Token der ersten Phase genau einmal besucht wurde.</sample>
    <sample id="1339">Um Ihnen einen Vorgeschmack auf die Ergebnisse der experimentellen Ergebnisse zu geben, vergleichen wir unsere Methode mit anderen Tree-less-Modellen auf dem COG-Benchmark. Unser Modell übertrifft die anderen um einen großen Spielraum bei der Generalisierung auf tiefere Rekurrenz.</sample>
    <sample id="1340">Eine andere Art der strukturellen Renovierung bleibt sehr herausfordernd.</sample>
    <sample id="1341">In unserer Arbeit lösen wir einige interessante technische Herausforderungen.</sample>
    <sample id="1342">Zuerst einmal ist die Ausrichtung zwischen Eingabe und Ausgabe im Trainingsdatensatz nicht gegeben. Infolgedessen wissen wir für ein gegebenes Token nicht, aus welchem Multiset es stammt, was eine Herausforderung für die Trainingsdaten darstellt.</sample>
    <sample id="1343">In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We addressed this by inducing the alignment as part of the trachea.</sample>
    <sample id="1344">Die Permutationsmethode ist sehr flexibel, bringt aber die Herausforderung mit sich, die höchste Punktzahl zu finden und P zu berechnen. Das liegt daran, dass dies mit dem Traveling Salesman Problem zusammenhängt.</sample>
    <sample id="1345">Wir approximieren dies mit einer GPU-freundlichen kontinuierlichen Relaxation, die uns auch erlaubt, rückwärts zu propagieren durch die Lösung und die linguistisch plausibleren Permutationen zu lernen.</sample>
    <sample id="1346">Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen angehen möchten, schauen Sie sich bitte unseren Artikel oder kommen Sie zu unserem Posten.</sample>
    <sample id="1347">Kognitive Dissonanz ist, wenn zwei Überzeugungen oder Handlungen inkonsistent sind.</sample>
    <sample id="1348">GPT-4</sample>
    <sample id="1349">Ja.</sample>
    <sample id="1350">Sara Babbi</sample>
    <sample id="1351">Die Daten für die MuDa-Benchmark stammen von Transkripten von TED Talks.</sample>
    <sample id="1385">Matthias Lendehammer.</sample>
    <sample id="1386">Sprachübergreifender Transfer bezieht sich auf die Übertragung von Wissen aus einer Sprache in eine andere.</sample>
    <sample id="1387">Stalland University.</sample>
    <sample id="1388">Die Autoren verwenden die simultane Sprachsynthese-Ergebnisse auf Grafiken, bei denen blaue Werte die Übersetzungqualität messen und durchschnittliche Latenzwerte die durchschnittliche Latenz messen.</sample>
    <sample id="1389">Hallo zusammen, ich bin Makshata und heute präsentieren mein Kollege Martin und ich unsere Arbeit, das Kitmaster. Sie werden Wissensintegration aus mehreren Quellen bewerten. Diese Arbeit ist eine Zusammenarbeit zwischen der Macquarie University, Mela und Microsoft Research.</sample>
    <sample id="1390">Natürliche Sprachmodelle stützen sich auf eine Vielzahl von Wissensquellen, wie z. B. dem Wissen, das in ihren Parametern enthalten ist, das in der Regel durch Vortraining erworben wird, und dem Wissen, das in den Eingaben bei der Inferenz gegeben wird.</sample>
    <sample id="1391">Kürzliche Arbeiten in Aufgaben wie Fragenbeantwortung zeigen, dass Modelle vortrainiertes Wissen nutzen können, um die Aufgabe zu lösen.</sample>
    <sample id="1392">Die National Language of Pakistan erfordert oft Wissen, das auch in der Hindi-Sprache bereitgestellt wird.</sample>
    <sample id="1393">Zum Beispiel, in dem Satz "John saw the newly elected president on TV",</sample>
    <sample id="1394">Präzise Parameter können Informationen über den Präsidenten 2 und den Präsidenten enthalten, aber sie können nicht zuverlässig wissen, wer diese spezifische Entität John ist oder wer der neue Präsident ist, da der Präsident sich geändert haben könnte seit der Tragödie.</sample>
    <sample id="1395">Daher benötigen erfolgreiche Modelle für wissensintensive NLP-Aufgaben die Fähigkeit, sowohl während des Vorabtrainings als auch während der Inferenzzeit Wissen zu integrieren und zu nutzen.</sample>
    <sample id="1396">In dieser Arbeit schlagen wir einen diagnostischen Test zur Wissensintegration vor.</sample>
    <sample id="1397">Wir stellen eine Aufgabenstellung zur Korreferenzlösung vor, die darauf abzielt, die Fähigkeit zu prüfen, Wissen aus verschiedenen Quellen abzurufen. Wir bewerten den Datensatz mit menschlichen Studienteilnehmern und etablieren eine Korreferenzlösung.</sample>
    <sample id="1398">Servin ist ein Richter. Kia ist ein Bäcker. Servin und Kia trafen sich im Park. Nach einem langen Arbeitstag, bei dem er Fälle in einem Gerichtsbau entschied, freute er sich, sich zu entspannen.</sample>
    <sample id="1399">Die Aufgabe ist es, die korrekte Entität zu identifizieren, auf die das Pronomen er sich bezieht, was in diesem Fall der Mann ist.</sample>
    <sample id="1400">Die Auflösung eines gegebenen Pronoms erfordert zwei Arten von Informationen. Erstens, Entitätsspezifisches Wissen, wie z.B. "Servin ist ein Richter". Und zweitens, Weltwissen, wie z.B. "Richter entscheiden Fälle in Gerichten".</sample>
    <sample id="1401">Im Allgemeinen wird Hintergrundwissen während des Vortrainings großer Sprachmodelle erlernt, während entitätsspezifisches Wissen typischerweise durch Inferenz erworben wird.</sample>
    <sample id="1402">Die Verfügbarkeit von Einzel- oder Mehrfachquellen für Informationen kann die Suche nach Informationen erleichtern.</sample>
    <sample id="1403">Wir haben drei Einstellungen von Keras gefunden. Zuerst müssen wir die Einstellung "Background Pretrain" aktivieren. Hintergrundwissen wird als verfügbar für das Pretraining angenommen.</sample>
    <sample id="1404">Zweitens ist die Backbone-Hintergrundeinstellung verfügbar sowohl beim Vortrainieren als auch im Feintuning. Schließlich ist die Backbone-Feintuning-Einstellung verfügbar nur im Feintuning.</sample>
    <sample id="1405">Dieses Lastsetting ist besonders interessant. Es simuliert den Fall, bei dem das Hintergrundwissen notwendig ist, um eine Aufgabe zu lösen, es ist nicht Teil der Pretrain Data of Models. Zum Beispiel, weil neue Berufe entwickelt wurden, seit die Zeit von Pre-</sample>
    <sample id="1406">Hier ist ein Beispiel dafür, wie wir die Verfügbarkeit von Effekten in der zweiten Phase kontrollieren können.</sample>
    <sample id="1407">In der Hintergrundvoraussetzung nehmen wir an, dass die Hintergrundkenntnisse, die Politiker bei der Wahl von Sitzen im Parlament enthalten, in den Prä-Trainingsparametern enthalten sind. In verschiedenen Kontexten stellen wir die entitätsspezifische Kenntnis, ChatGPT als Politiker, bereit.</sample>
    <sample id="1408">In der Hintergrund- und Basis-Einstellung bieten wir nicht nur entitätsspezifische, sondern auch Hintergrundwissen über Politiker im Interessenkontext.</sample>
    <sample id="1409">In der Hintergrund- und Film-Einstellung wird die fiktive Berufung "Miretura" anstelle von "Politiker" verwendet, weil Miretura unwahrscheinlich ist, in der Präsentations-</sample>
    <sample id="1410">Wir validierten den Datensatz sowohl mit menschlichen Experten als auch mit etablierten grafischen Lösungsmodellen. In dieser Abbildung zeigen wir die Ergebnisse der besten performenden Modelle auf der anspruchsvollsten Variante des Hintergrund-Pretrainings.</sample>
    <sample id="1411">Wenn du auf der Trainingsmenge von Kitemus trainierst, performst du nicht gut. Wenn du auf der Trainingsmenge von Kitemus jedoch trainierst, performt sowohl TF als auch BERT deutlich besser als zufällige Modelle.</sample>
    <sample id="1412">Dies deutet darauf hin, dass bei der Training und Generierung einer allgemeinen Anfrage für eine Lösung Daten gesetzt werden. Man könnte lernen, Oberflächenhinweise zu nutzen, aber sie sind nicht nützlich beim Testen von Kit-Muster, da solche Hinweise entfernt wurden.</sample>
    <sample id="1413">Erweiterungsversuche für fiktives Wissen zeigten, dass selbst die besten leistungsstarken Modelle nicht zuverlässig neue Informationen integrieren können.</sample>
    <sample id="1414">Einige der Hauptursachen für den Papierverbrauch sind, dass viele Referenzen in modernen Lösungen scheinbar aufgrund von Wissen aus verschiedenen Quellen ohne aufgabenspezifische Schulung nicht verfügbar sind. Allerdings können einige Modelle mit aufgabenspezifischer Schulung erfolgreich Wissen aus mehreren Quellen integrieren.</sample>
    <sample id="1415">Selbst die leistungsstärksten Modelle scheinen Schwierigkeiten bei der zuverlässigen Integration von Back-of-the-Knowledge, präsentiert nur im Inferenztag zu haben.
Wenn Sie mehr Details wünschen, sehen Sie bitte unseren Artikel und schauen Sie sich den Datensatz und den Code auf GitHub an.
Danke für Ihre Aufmerksamkeit.</sample>
    <sample id="1416">Baumbasierte Methoden können komplex und rechenintensiv sein, insbesondere bei der Verarbeitung logischer Formeln und der Handhabung von Variablen.</sample>
    <sample id="1417">Die Autoren gehören der Cornell University an.</sample>
    <sample id="1418">Hallo, ich bin Mara, und heute werden wir über unsere Papier-Markierungspersonen sprechen. Die Verwendung natürlicher Sprachaufforderungen zur Messung der Stärken von Sprachmodellen. Diese Arbeit wurde in Zusammenarbeit mit Essendermush und Dancaro durchgeführt.</sample>
    <sample id="1419">In den letzten Jahren haben viele dokumentiert die Prävalenz von sozialer Verzerrung und Stereotypen in großen Sprachmodellen oder LLMs.</sample>
    <sample id="1420">Diese Maßnahmen haben verschiedene Einschränkungen. Sie basieren in der Regel auf manuell erstellten Datensätzen, die sehr zeitaufwendig zu erstellen sind.</sample>
    <sample id="1421">Und sie messen in der Regel nur sehr spezifische Stereotypen, was bedeutet, dass sie sich nicht gut auf andere Demografien oder Kontexte übertragen lassen, oder sie erfassen einfach sehr allgemeine, breite Assoziationen, wie negative Assoziationen mit bestimmten Gruppen.</sample>
    <sample id="1422">Darüber hinaus berücksichtigt die meisten Arbeit im Weltraum nicht die Intersektionalität, die Vorstellung, dass vielfältige soziale Identitäten sich überschneiden und verstärken können und einzigartige Formen von Diskriminierung hervorbringen.</sample>
    <sample id="1423">Um diese Einschränkungen zu überwinden, verlassen wir uns auf die Annahme, dass diese neueren instruktionsangepassten LLMs sehr gut darin sind, Anweisungen zu befolgen und unpräzise zu sein.</sample>
    <sample id="1424">So können wir dem Modell bitten, eine Persona zu generieren, die eine Darstellung einer imaginären Person ist, indem wir einen Prompt wie "Stell dir vor, du bist eine asiatische Frau. Beschreibe dich selbst." verwenden.</sample>
    <sample id="1425">Und wir können sofort sehen, dass dies für jede Demografie sehr gut generalisierbar ist, denn wir können einfach jeden Geschlechtsmarker angeben, den wir möchten, in diesen Prompt.</sample>
    <sample id="1426">Hier sind einige Beispiele für Generierungen von GPT-4:</sample>
    <sample id="1427">Sofort sehen wir, dass die Ausgaben nicht übermäßig negativ oder toxisch im traditionellen Sinne dieser Wörter sind.</sample>
    <sample id="1428">Es gibt einige interessante Muster.</sample>
    <sample id="1429">Die asiatische Frau wird als unaufmerksam dargestellt, die mittlere östliche Frau wird mit Wörtern wie exotisch bezeichnet und als faszinierende Region erwähnt.</sample>
    <sample id="1430">Und sowohl die Frau von Farbe-Personen als auch der weiße Mann-Personen machen Anspielungen auf die Abstammung, während die weiße Mann-Person nichts davon hat.</sample>
    <sample id="1431">Um diese Muster zu erfassen, hat unsere Methode zwei Teile. Der erste Teil ist die Generierung dieser Personas.</sample>
    <sample id="1432">Wir haben diese Prompts generiert, inspiriert von einer Studie, in der sie diese Prompts an menschliche Probanden gaben und dabei feststellten, dass sie auch bei menschlichen Probanden subtile rassistische Stereotypen hervorrufen konnten.</sample>
    <sample id="1433">Und außerdem ermöglicht dies einen direkten Vergleich zwischen unseren generierten Personas und den menschlichen schriftlichen Antworten.</sample>
    <sample id="1434">Die zweite Phase ist Markierung, eine Methode, um die Wörter zu identifizieren, die markierte Gruppen von nicht markierten Gruppen unterscheiden, was ich kurz erläutern werde.</sample>
    <sample id="1435">Der Vorteil davon ist, dass wir sehr spezifische Textmuster ohne auf bestimmte Lexikon-</sample>
    <sample id="1436">So verwendet die Marktwörter die soziolinguistische Vorstellung von Markentyp, die besagt, dass es einen unmarkierten Standard gibt und jede Gruppe, die sich von diesem Standard unterscheidet, linguistisch markiert ist.</sample>
    <sample id="1437">Zum Beispiel wird das Wort „Mann“ oder Entschuldigung, das Wort „Krieger“ üblicherweise mit Männern in Verbindung gebracht. Wenn Menschen also eine Kriegerin beschreiben, werden sie normalerweise „eine Frau-Kriegerin“ spezifisch erwähnen und den Begriff mit „Frau“ kennzeichnen.</sample>
    <sample id="1438">Und breiter gesagt sind dominante Gruppen in der Gesellschaft sowohl sprachlich als auch sozial markiert, während marginalisierte Gruppen in der Regel stigmatisiert werden.</sample>
    <sample id="1439">In unserem Ansatz bestimmen wir zunächst, welche unmarkierten und markierten Gruppen es gibt.</sample>
    <sample id="1440">Und dann vergleichen wir die Personen anhand der Methode der Gewichtung der häufigsten Wörter, die im Grunde genommen verwendet wird, um die Top-Wörter für jede markierte Kategorie zu unterscheiden.</sample>
    <sample id="1441">Zum Beispiel für die Personas der schwarzen Frau würden wir Fighting Words und die Logos-Verhältnisse gegen sowohl weiße Personas als auch männliche Personas vergleichen, da dies die beiden entsprechenden unmarkierten Gruppen sind.</sample>
    <sample id="1442">Nun haben wir schon einige Ergebnisse. Zuerst haben wir verschiedene Arten von Stereotypen verwendet, und wir haben festgestellt, dass die generierten Personas viel mehr Stereotypen enthalten als die von Menschen geschriebenen.</sample>
    <sample id="1443">Allerdings, wenn wir uns die Verteilung der Wörter in Lexicon ansehen, finden wir sehr unterschiedliche Ergebnisse.</sample>
    <sample id="1444">Während die generierten Personas viel höhere Raten von Luxuswörtern aufweisen, haben die von Menschen geschriebenen Wörter einen viel breiteren Verteilung der Wörter, während die Stereotypwörter, die in den generierten Personas enthalten sind, wirklich nur die Wörter groß und athletisch sind.</sample>
    <sample id="1445">Ich bin wirklich nur das Positive, zumindest nichts Negatives.</sample>
    <sample id="1446">Und tatsächlich fängt der Lexicon nicht wirklich viele der schädlichen Muster ein, die wir in den früheren Folien gesehen haben. Stattdessen werden wir die Ergebnisse unserer markierten Wörter-Methode verwenden, um zu zeigen, wie diese scheinbar positiven Wörter Stereotypen und essentialisierende Narrative fördern.</sample>
    <sample id="1447">In unserer Analyse überprüfen wir, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln.</sample>
    <sample id="1448">Erste Wortgruppen, die die Top-Wörter umfassen, sind Dinge wie Kultur, Tradition, Stolz und Exotisch. Und diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und unterscheiden sie von der weißen Norm.</sample>
    <sample id="1449">Dies trägt zu einer langen Tradition von Diskriminierung und Andeutung für diese Schädlichkeit bei.</sample>
    <sample id="1450">Darüber hinaus gibt es viele gängige Klischees, die in diesen Worten widergespiegelt werden, insbesondere für Frauen von Farbe. So beinhalten Wörter, die eine Latina-Frau beschreiben, Dinge wie lebendig und kurvig.</sample>
    <sample id="1451">was kann ich mit einem Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen-Tropen</sample>
    <sample id="1452">Die Verbindung zu einer langen Geschichte der asiatischen Frauen, die als hypersexuell, sehr unterwürfig und so unterwürfig angesehen werden.</sample>
    <sample id="1453">Und schließlich für eine schwarze Frau sehen wir, dass einige der Top-Wörter Dinge wie stark und widerstandsfähig sind.</sample>
    <sample id="1454">Dies verbindet sich mit einem Archetyp, den Menschen den starken schwarzen Frau-Archetyp genannt haben, und obwohl es auf den ersten Blick positiv klingt,</sample>
    <sample id="1455">Es gibt Studien, die zeigen, dass diese Art von Archetyp tatsächlich sehr schädlich ist, da er diese Demografie unter großen Druck setzt, widerstandsfähig und stark gegen sukzessuale Hindernisse zu sein.</sample>
    <sample id="1456">Stattdessen werden diese Hindernisse beseitigt und die Menschen unter Druck gesetzt, sie zu überwinden, was zu sehr negativen Gesundheitsfolgen für diese Menschen und anderen führt.</sample>
    <sample id="1457">Im Allgemeinen stellen wir fest, dass die Wörter für jede Marktgruppe im Wesentlichen eine verallgemeinernde Erzählung widerspiegeln.</sample>
    <sample id="1458">Basierend auf diesen Mustern können wir drei Empfehlungen für Modellbesitzer ableiten:</sample>
    <sample id="1459">Zuerst sollten wir als Forscher positive Stereotypen und essentialisierende Narrative angehen. Wir sollten auch einen intersektionalen Blickwinkel verwenden, um Vorurteile und Schäden zu untersuchen, denn es gibt viele Dinge, die übersehen werden könnten, wenn wir das nicht tun.</sample>
    <sample id="1460">Und schließlich sollte es wirklich mehr Transparenz über Bias-Minderungsmaßnahmen geben.</sample>
    <sample id="1461">Weil zum Beispiel diese positiven Stereotypen, wir wissen nicht, ob es wegen irgendeiner Art von komischen</sample>
    <sample id="1462">Übermäßig übermäßige Wertvorstellungen, die stattfinden oder vielleicht einige andere, wie z. B. Anti-Stereotypen-Methoden, die zu diesen verheerenden Mustern führen.</sample>
    <sample id="1463">Wir können keine Annahmen treffen oder das weiter untersuchen, ohne mehr Transparenz.</sample>
    <sample id="1464">Vielen Dank fürs Zuhören. Ähm, ich hatte eine gute Zeit.</sample>
    <sample id="1465">Hallo zusammen, mein Name ist Jingwei E von der Universität der Wissenschaft und Technologie von China.</sample>
    <sample id="1466">Es ist mir eine Freude, ein kurzes Werbevideo für Papier zu geben. Kopieren Sie mein Modell? Schützen Sie das Urheberrecht großer Sprachmodelle für die Einbettung und den Dienst.</sample>
    <sample id="1467">Lass uns zuerst den Hintergrund zu den Einwanderungsdiensten vorstellen.</sample>
    <sample id="1468">Derzeit sind große Sprachmodelle wie GPT, Llama, PaLM in der natürlichen Sprachverständnis- und -generierung außergewöhnlich.</sample>
    <sample id="1469">In-bed-Services sind eine der Dienstleistungen, die auf großen Sprachmodellen aufbauen, um verschiedene Aufgaben zu unterstützen.</sample>
    <sample id="1470">Beispielsweise bietet OpenAI eine GPT-basierte Einbettung von Texten.</sample>
    <sample id="1471">Allerdings haben aktuelle Arbeiten gezeigt, dass der Angreifer den Modell möglicherweise durch das Lernen aus dem Embedding und die Bereitstellung ähnlicher Dienste stehlen kann. Daher ist es notwendig, das Urheberrecht des Embeddings zu schützen.</sample>
    <sample id="1472">Um das Urheberrecht von Inhaltsanbietern zu schützen, ist eine Lösung, eine Wasserzeichenmarke im bereitgestellten Dienst zu platzieren und zu erkennen, ob ein anderer Dienst diese Wasserzeichenmarke enthält.</sample>
    <sample id="1473">Die Wasserzeichenmethode muss folgende Eigenschaften aufweisen: Erstens muss die Methode auf die Einbettung von Diensten anwendbar sein. Zweitens darf das Wasserzeichen die Nützlichkeit der bereitgestellten Einbettung nicht beeinträchtigen.</sample>
    <sample id="1474">Dritter: Der Wasserzeichen sollte dem Angreifer zugänglich sein, oder der Angreifer kann das Wasserzeichen leicht entfernen.</sample>
    <sample id="1475">Schließlich muss die Wärme für die Angriffsservices während der Modellextraktion zugänglich sein.</sample>
    <sample id="1476">Existierende Wörter können grob in vier Kategorien unterteilt werden:</sample>
    <sample id="1477">Allerdings ist diese Methode entweder nicht auf die Einbettung von Diensten anwendbar oder es mangelt an Portierbarkeit.</sample>
    <sample id="1478">Daher schlagen wir in dieser Arbeit einen Backdoor-basierten Wasserzeichenverfahren vor, das auf die Einbettung von Zeichen basiert.</sample>
    <sample id="1479">Dann lasst mich die Details unseres Embedding-Markers vorstellen. Ein Embedding-Marker enthält zwei Hauptschritte:
1. Die Mag-Injektion und die Copyright-Verifizierung.</sample>
    <sample id="1480">Bevor diese Hauptschritte, wählen wir zuerst einen Auslöser-Set aus. Das Auslöser-Set ist eine Gruppe von Wörtern in einem moderaten Frequenzintervall.</sample>
    <sample id="1481">Wir nehmen an, dass der Anbieter einen allgemeinen Textkorpus sammeln und die Wortfrequenz des Wortes "the" zählen kann.</sample>
    <sample id="1482">In einer Mak-Injektion identifizieren wir zuerst das Ziel-Embedding. Wenn ein Benutzer einen Satz an den Anbieter sendet, berücksichtigt der Anbieter die Trigger-Nummer im Satz.</sample>
    <sample id="1483">Die bereitgestellte Einbettung ist die Summe der Ziel-Einbettung und der ursprünglichen Einbettung.</sample>
    <sample id="1484">Die Länge des Zielkörpers ist proportional zur Anzahl der Trigger im Satz. Wenn die Anzahl der Trigger im Satz größer als M ist, wird die eingebettete Länge genau gleich der Länge des Zielkörpers.</sample>
    <sample id="1485">Copywrite-Verifizierung dient dazu, festzustellen, ob ein Modell hinter einem anderen Dienst den Wortschatz enthält.</sample>
    <sample id="1486">Wir erstellen den Konstruktor eines Backdoor-Datensatzes. Ein Backdoor-Datensatz enthält Sätze, bei denen alle Wörter zu der Trigger-Set gehören. Während alle Wörter in den Sätzen des Backdoor-Datensatzes nicht zu der Trigger-Set gehören.</sample>
    <sample id="1487">Dann fordert der Anbieter Einbettungen vom Diebstahl-Dienst mit der Daten-</sample>
    <sample id="1488">Der Cosinus und die Ähnlichkeit zwischen dem angeforderten Embedding und dem Ziel-Embedding werden berechnet. Wir berechnen die Differenz zwischen dem neuen und dem alten Datensatz, die definiert ist als Delta-Cosinus und Delta-L.</sample>
    <sample id="1489">Währenddessen wenden wir auch den KS-Test an und verwenden seinen p-Wert als dritte Metrik.</sample>
    <sample id="1490">Wir führen Experimente mit dem 80-Neuesten-Datensatz, Mind, SST2 und ERS-Daten durch. Wir nehmen an, dass der Anbieter des Textdatensatzes zur Zählung der Wortfrequenzen</sample>
    <sample id="1491">Die Ergebnisse der Studie zeigen, dass unser Embedding-Marker eine großartige Detektionsleistung aufweist und gleichzeitig eine großartige Nützlichkeit für die Unterscheidungsaufgabe hat.</sample>
    <sample id="1492">Wir haben auch die Kohärenz der bereitgestellten Einbettung durch die Realisierung der Einbettung von Sätzen aufgeführt, die bei BPCA verwendet werden. Die Legende der Zahlen bedeutet die Anzahl der Trigger in jedem Satz.</sample>
    <sample id="1493">Wie in den Figuren gezeigt, ist es schwierig, zwischen den Backdoor-Einfügungen und den normalen Einfügungen zu unterscheiden.</sample>
    <sample id="1494">Das oder danke. Wir kommen, um dies mit Ihnen zu besprechen, Doktor.</sample>
    <sample id="1495">ABC-Eval steht für Annotating Behaviors in Chat.</sample>
    <sample id="1496">2003</sample>
    <sample id="1497">Hallo, mein Name ist Vasudha und ich bin eine Kandidatin für den Master of Science in Informatik an der Stony Brook University. Ich möchte mein Arbeitsergebnis, das für ACL 2023 als Langabdruck eingereicht wurde, präsentieren: Transferlernen für die Entdeckung von seltenen Klassen, die die Herausforderung der seltenen Klassen angeht.</sample>
    <sample id="1498">Wir beginnen mit der Definition von kognitiver Dissonanz und warum es wichtig ist, Probleme im Sprachbereich zu untersuchen. Einfach gesagt ist kognitive Dissonanz, wenn zwei Überzeugungen oder Handlungen inkonsistent sind.</sample>
    <sample id="1499">Wie in diesem Beispiel, wo eine Person sagt: „Ich weiß, dass Zigaretten mich töten“, und dann fortsetzt, zu sagen: „Ich habe nach dem Treffen ein paar Zigaretten genommen.“ Diese Überzeugung und Handlung sind inkonsistent, und sie sind in Widerspruch zueinander.</sample>
    <sample id="1500">Darüber hinaus erwähnte ich, dass ich ohne sie meinen Job nicht behalten könnte, rechtfertigt die zweite Erwähnung und sie haben eine konsensualen Beziehung.</sample>
    <sample id="1501">Weil Distanz ein sehr häufiges Phänomen ist, das wir im täglichen Entscheidungsfindung erleben, sind sie sehr selten in der Sprache ausgedrückt, unter anderen Arten von Beziehungen.</sample>
    <sample id="1502">So, was ist dieses Thema?
Studien zur kognitiven Distanz können helfen, die Auswirkungen von Dissens in der Bevölkerung zu verstehen, Trends in Überzeugungen, Werten und Einstellungen sowie Veränderungen in der Bevölkerung zu verfolgen.</sample>
    <sample id="1503">Hohe kognitive Dissonanz ist auch mit Angststörungen verbunden und kann helfen, das Verständnis für die psychische Gesundheit von Menschen zu verbessern.</sample>
    <sample id="1504">Das Studium der Sprache kann auch dazu beitragen, Extremismus und Polarisierung von gefährdeten Gruppen zu verstehen.</sample>
    <sample id="1505">Schließlich ist kognitive Distanz wichtig, um die persönlichen kognitiven Stile von Individuen zu verstehen und hilft uns, Entscheidungsprozesse zu verstehen.</sample>
    <sample id="1506">Um ein Ressourcenmaterial zur Schaffung von kognitiver Dissonanz zu erstellen, führten wir eine groß angelegte Untersuchung von Dissonanzbeziehungen durch. Wir verwendeten einen distanzfirsten Ansatz, wie im Flussdiagramm hier dargestellt.</sample>
    <sample id="1507">Tweets wurden mit einem Purity TV Parser geparst und Paare von Discourse-Einheiten wurden gemäß den Richtlinien annotiert, die in der Beschreibung beschrieben sind.</sample>
    <sample id="1508">Wie hier zu sehen ist, wurde diese Distanz nur in 3,5 % der annotierten Daten gefunden.</sample>
    <sample id="1509">Durch die Sammlung von etwa tausend Beispielen von Diskurs-Einheitspaaren trainierten wir einen anfänglichen Klassifikator, der nur auf 43 Beispielen von Diskurstypen trainiert wurde. Zu unserer Überraschung war der Klassifikator nicht viel besser als ein Zufallsgenerator.</sample>
    <sample id="1510">Angesichts der geringen Häufigkeit von Dissonanz und dem Fehlen jeglicher vorheriger Daten, stehen wir vor dem Problem der absoluten Seltenheit.</sample>
    <sample id="1511">Um dies zu lindern, experimentieren wir mit Kombinationen aus Transferlernen und aktiver Lernstrategie, um mehr dissonante Beispiele zu sammeln, wobei weniger Annotationsaufwand erforderlich ist, wodurch die Gesamtannotationskosten gesenkt und die Dissonanzerkennung verbessert wird.</sample>
    <sample id="1512">Da das ursprüngliche Modell die Distanzklasse überhaupt nicht erfassen konnte, begannen wir den aktiven Lernprozess, indem wir Gewichte von eng verwandten Aufgaben übertragen.</sample>
    <sample id="1513">Die Transformation in zwei verschiedene Aufgaben.
Thema-unabhängige Distanzklassifizierung ist eine Aufgabe, die bestimmt, ob zwei Debattenerklärungen von verschiedenen Personen übereinstimmen oder nicht, unabhängig vom Thema.</sample>
    <sample id="1514">Diskussion hier und über die binäre Klassifizierung von Expansion und Vergleichsklassen von Perturbations-Biologie, da diese beiden eng mit dem Konzept von Konsonanz und Dissonanz verbunden sind und wir sie hier CE nennen.</sample>
    <sample id="1515">Wir finden, dass die Zero-Shot-Performance auf dem Entity-Daten-Set bereits viel besser ist als die Chance mit dem besten AUC-Wert 0,6 ist.</sample>
    <sample id="1516">Weiterhin, durch iteratives Feintuning beider Aufgaben, stellen wir fest, dass das Feintuning der CE-Aufgabe gefolgt vom weiteren Feintuning der Debatte zu einer deutlich besseren Zero-Shot-Performance führt. Dies ist das Modell, das wir verwenden, um den aktuellen Lernprozess zu starten.</sample>
    <sample id="1517">Als Nächstes bestimmen wir die beste Methode, um ein Modell mit neuen Daten aus jeder Runde aktiver Lernaktivitäten und Annotationen zu aktualisieren. Kumulativ sammelt alle Daten, die bisher aus aktiven Annotationen gesammelt wurden, während iteratives Aktualisieren das Modell durch Training auf dem neuesten Datensatz aktualisiert.</sample>
    <sample id="1518">Über die verschiedenen Strategien haben wir festgestellt, dass kumulativ gleich oder besser als iterativ über den gesamten Zeitraum funktioniert.</sample>
    <sample id="1519">Als Nächstes verwenden wir die Strategie der Wahrscheinlichkeit seltener Klassen (PCR), um hauptsächlich Beispiele auszuwählen, die vom aktuellen Modell in jeder Runde wahrscheinlich falsch klassifiziert werden.</sample>
    <sample id="1520">Wir vergleichen dies mit dem anderen Stand der Technik, den Strategien, die in der Community üblicherweise verwendet werden.</sample>
    <sample id="1521">Wir fanden heraus, dass die vorgeschlagene PRC-Strategie besser funktioniert als andere State-of-the-art-Strategien, obwohl die Unterschiede gering sind. Beachten Sie, dass die Leistung für Ran-</sample>
    <sample id="1522">Und für weitere Runden von AL mit zwei besten Strategien verbessern wir die Klassifizierung AUC zu 2,75, was die beste Leistung ist, die wir bisher auf der Aufgabe erzielt haben.</sample>
    <sample id="1523">Wir prüfen auch die Machbarkeit jeder Strategie hinsichtlich der Annotationenqualität und der Kosten für die Annotatoren. Wir stellen fest, dass PRC einen hohen Prozentsatz von Diskrepanzen aufweist und am besten für seltene Klassen funktioniert. Die Annotatoren finden die Beispiele jedoch schwierig.</sample>
    <sample id="1524">Zusammenfassend finden wir, dass der PRC eine einfache KI-Strategie für die Akquisition von Rare-Klassen ist und ein mit angemessen gestalteten Transferlernen-Aufgaben verbundener KI-Start mit erheblichen Vorteilen helfen kann.</sample>
    <sample id="1525">Wir stellen auch fest, dass die iterative Aktualisierung nützlich für Transferlernen aus einem anderen Bereich ist, während in-Domain-Aktivitäten von kumulativen Vorteilen profitieren.</sample>
    <sample id="1526">Dies sind die Links zu unserem Datensatz und unserem Artikel.
Fühlen Sie sich frei, uns zu kontaktieren, wenn Sie Fragen haben.
Danke.</sample>
    <sample id="1527">Die Autoren gehören der Universität Zürich an.</sample>
    <sample id="1528">Su Yuyan.</sample>
    <sample id="1529">Vier.</sample>
    <sample id="1530">The Seat of the Arza architecture.</sample>
  </task>
</testset>