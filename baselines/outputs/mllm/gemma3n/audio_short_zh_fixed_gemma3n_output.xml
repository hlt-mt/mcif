<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="zh">
    <sample id="0">语言模型主要训练在大型网页数据上，其中政治新闻媒体的数据被很好地覆盖。</sample>
    <sample id="1">麦基尔大学、米拉和微软研究院。</sample>
    <sample id="2">嗨，欢迎来到我们关于“Plane”，一种用于文档级别和句子级别的文本识别的新算法的演示。</sample>
    <sample id="3">我的名字是丽吉娜·斯托恩，我将引导您完成本次演示文稿的第一部分。

首先，我们来定义文本简化。</sample>
    <sample id="4">文本简化是指将文本调整以提高其针对特定目标受众的理解能力。对于阅读障碍人士或非母语人士而言，</sample>
    <sample id="5">请提供英文内容。</sample>
    <sample id="6">请提供英文内容。</sample>
    <sample id="7">简化句子可以使用多种技术，例如词语替换、句子删除、句子插入、句子重新排序或插入词语。</sample>
    <sample id="8">我们建议创建一个新的合作平台。因为近年来，存在着许多与现有合作相关的难题。例如，这个合作平台，由于规模太小，难以开展技术创新模式。</sample>
    <sample id="9">是的，最近几年提出的重新建模方法，会自动对齐，这意味着它们可能存在错误，并且对齐可能不准确。</sample>
    <sample id="10">因此，我们建议将我们的公司拆分为两个子公司：Deepen API 和 Deepen Web。Deepen API 专注于文本内容。</sample>
    <sample id="11">在平民API中，我们手动整理了483份文档，结果大约是3万3千个句子对。</sample>
    <sample id="12">对于平面网页，这些内容包括不同的域名，并且我们还手动对这些750个文档进行对齐，另一方面则使用自动对齐方法。</sample>
    <sample id="13">总共结果为 38,450 个句子。</sample>
    <sample id="14">我们再分析一下句子对，例如，在类型方面。</sample>
    <sample id="15">我在这里可以看到，圣经文本比例如新闻文本或语言学习材料更简洁。</sample>
    <sample id="16">在所有层面，例如词法简化、结构简化，或者在所有层面上的简化。</sample>
    <sample id="17">正如你所看到的，我们的平面 корпуса具有更高的差异化转换形式多样性。例如，在平面 API Корпус中，我们有更多的重排和旋转，而没有在平面 Web Корпус中。</sample>
    <sample id="18">在另一个方面，在网页库中，我们有许多重复的“n”。</sample>
    <sample id="19">好的，让我们看看我们可以用这个数据集做什么。

你好，我是奥马尔，现在我将介绍我们数据集的用例。

所以，第一个用例是我们能够评估自动对齐消息。</sample>
    <sample id="20">最近几年，有很多对齐方法，但在机器翻译的背景下，</sample>
    <sample id="21">我们有两个并行文档，分别用不同的语言书写，我们想要提取句子之间的对齐关系。</sample>
    <sample id="22">但在我们的用例中，我们试图提取两个平行的文档之间的对齐关系，这两个文档使用相同的语言，包含相同的文本，但复杂度不同。</sample>
    <sample id="23">现在我们有了我们手动对齐的深度平原数据，我们可以将这些句子作为标准对齐方案来评估一些拟议的对齐方法。</sample>
    <sample id="24">我们对拟议的方法进行了调整，并已发表了所有这些调整以及运行我们实验的代码。</sample>
    <sample id="25">在最后，我们得出结论，对于德语文本简化，最佳的自动对齐方法是使用“mess align”方法。</sample>
    <sample id="26">你可以找到一个代码来运行此方法在你自己的文档中。</sample>
    <sample id="27">我们论文中展示的第二个用例是自动文本简化。</sample>
    <sample id="28">我发现微调语言模型，以生成从复杂输入文本中简化文本。</sample>
    <sample id="29">我们对两个不同的模型进行了微调。我们对长文本模型进行了微调，以产生文档级别的简化。</sample>
    <sample id="30">我们还调整了基于正态分布的长期、基于正态分布的导入，以产生句子级别的简化。</sample>
    <sample id="31">你可以查看所有检查点，并可以查看实验的得分和评估指标的更多细节。</sample>
    <sample id="32">我们得出结论，这种基本的微调可以产生或获得低于基线得分的得分。</sample>
    <sample id="33">我们建议将这些结果作为自动文本简化问题的基准，作为未来工作的基准。</sample>
    <sample id="34">非常感谢您的关注，我们希望在会议期间见到大家。</sample>
    <sample id="35">Kayo Yen.</sample>
    <sample id="36">T5 XL</sample>
    <sample id="37">Yes.</sample>
    <sample id="38">该方法通过明确标注模型回复是否表现出某些行为，例如提供无关信息或相互矛盾，从而减少了人类评估的主观性。</sample>
    <sample id="39">现有弱监督方法的成功在很大程度上依赖于干净的验证样本。</sample>
    <sample id="40">The provided text does not contain any information about how to improve scores.</sample>
    <sample id="41">七位。</sample>
    <sample id="42">嗨，我的名字是萨达姆·斯皮尔科夫斯基，这关于协调的依赖结构。</sample>
    <sample id="43">例如，不同的理论和方法会产生不同的依赖结构，例如，在统一依赖中，坐标协调的结构是关于和麦基。</sample>
    <sample id="44" />
    <sample id="45" />
    <sample id="46">现在，还有一些对称的坐标结构，例如Pragma Approach、Conjunction Header Approach，假设Pragma Dependency Treebanks，而坐标结构则由连接符引导。</sample>
    <sample id="47">所以，我们从 `end` 到所有连词的依赖关系。</sample>
    <sample id="48">最后，这还是一种多层次的方法，例如在卡森斯词法中。</sample>
    <sample id="49">所有从句都位于主句前面，因此从句的依赖关系从主句的引导词到所有从句分别。</sample>
    <sample id="50">现在，在纸上，我们旨在提出一个关于配位结构的对称结构，例如这两种，以及非对称结构，例如</sample>
    <sample id="51">好的，这个论点基于依赖性最小化原则，我们将基于这个例子进行解释。</sample>
    <sample id="52">所以，正如你可能知道的，直接宾语通常会靠近动词，而形容词则可能更远一些，对吧？所以，今天读了“The”还挺好的，因为直接宾语它靠近了“the”。</sample>
    <sample id="53">虽然昨天读了，但现在情况更糟，因为这里动词和直接宾语之间有一个副词“yesterday”。</sample>
    <sample id="54">然而，这种效果可能通过当直接对象非常重且非常长时得到改善，因为这样它可以移动到句子的末尾。</sample>
    <sample id="55">这在这里说明了。所以，这两个句子都很好。马丁·谢尔德的《昨天》是一本非常迷人的书。这没问题。我们用它代替了。我们有这本长而复杂的书。</sample>
    <sample id="56">我也可以说玛丽今天读了一本非常迷人的书，关于海。</sample>
    <sample id="57">所以这里是，这之所以是可能的，是因为即使这句话违反了一般语法原则，即直接宾语应该紧跟动词。</sample>
    <sample id="58">它满足了依赖长度最小化的原则，该原则指出，更短的依赖更受欢迎。</sample>
    <sample id="59">所以，这两个树只显示了关键依赖的长度，即那些在两个结构中不恒定的那些。</sample>
    <sample id="60">所以这里我们有从“red”到“adjunct”的长度为七的依赖关系，以及从“red”到“book”的长度为四的依赖关系。为了得到“eleven”，我们需要</sample>
    <sample id="61">当你移动或交换这两个成分时，这两个依赖的总和变为六，对吧？所以11减去6，它听起来很不错，对吧？它违反了一个原则，但它满足了另一个。</sample>
    <sample id="62">好的。呃，所以我们做了什么，我们从增强版本的潘德银行的关于协调的统计数据中提取了数据，并查看了为什么您使用大学依赖。</sample>
    <sample id="63">这些统计数据证实，观察结果多次出现，左侧共轭通常较短。因此，盐酸和盐酸的测量值是</sample>
    <sample id="64">并且还观察到，随着长度的增加，这种倾向会增长。</sample>
    <sample id="65">所以，我想知道两个连词长度之间的区别。较短的连词通常指的是第一个更强，对吧？所以比例是左侧的。</sample>
    <sample id="66">关于这篇论文的一个新发现是，我们观察到这种倾向只发生在政府在左手手掌上。</sample>
    <sample id="67">右边政府在例子中，我看到按钮“丽莎”，所以政府在左边。</sample>
    <sample id="68">在第二个例子中，“home came”中没有出现。在“sneezed”中，我们有动词的协调，并且没有外部的支配，对吧？在这些情况下，左边的连词倾向于更短，而右边的连词则更长。差距越大，两者之间的差异就越大。</sample>
    <sample id="69">然而，当右边的治理，例如这里，将协调权交给网络时，这种效果消失了。</sample>
    <sample id="70">我们显示，通过测量长度为字符的第一列，为音节的第二列，为单词的第三列，我们将集中在第三列。</sample>
    <sample id="71">我必须说的是，当政府在左翼时，</sample>
    <sample id="72">左侧的倾向随着词语绝对差异的增加而稳步增长，情况与当有主语时观察到的情况相同，但在右侧主语时，这种倾向消失了。</sample>
    <sample id="73">并且我们在论文中展示了如何通过提供一个反对不对称配位结构论证，证明对称结构是更优的。</sample>
    <sample id="74">请参阅完整协议的纸张，我将解释，很抱歉，并在后续会议中与您讨论。</sample>
    <sample id="75">四位。</sample>
    <sample id="76">圣经文本。</sample>
    <sample id="77">左并列词。</sample>
    <sample id="78">是的，你可以使用这些模型。</sample>
    <sample id="79">DEplain-apa 包含新闻文本。</sample>
    <sample id="80">更好的模型架构、更大的模型大小以及更少的微调示例。</sample>
    <sample id="81">通过测量左侧词的长度，即第一列（字符数）、中间列（音节数）和右侧列（单词数）。</sample>
    <sample id="82">实验设计应包括以下步骤：

1. **定义变量：**
    * **自变量：** 支配词的位置（左侧、右侧）。
    * **因变量：** 支配词的长度（以字符数衡量）。
    * **控制变量：** 句子长度、词汇复杂程度、句子结构等。

2. **设计句子：** 创建一系列包含不同支配词位置的句子。

3. **测量支配词长度：** 对于每个句子，测量支配词的字符数。

4. **分析数据：** 使用统计方法（如回归分析）分析支配词长度与支配词位置之间的关系。

5. **重复实验：** 为了验证结果，可以重复实验多次，并使用不同的句子和词汇。</sample>
    <sample id="83">基线分类器在不平衡数据上的训练效果不佳，性能与随机猜测相当。</sample>
    <sample id="84">这篇论文有两位作者。</sample>
    <sample id="85">Bob, Alice</sample>
    <sample id="86">在正式性和词汇连贯性等话语现象上，语境感知 MT 模型比语境无关模型更有优势。</sample>
    <sample id="87">The authors are affiliated with the University of California, Berkeley.</sample>
    <sample id="122">框架通过重新标注数据集，并使用来自不同背景的标注者，来量化立场。</sample>
    <sample id="155">研究结果是，当人类受试者被给予这些提示时，研究者也能够识别出他们潜在的种族刻板印象。</sample>
    <sample id="156">The study used statistics extracted from the enhanced version of the Pentoshi Bank.</sample>
    <sample id="157">1</sample>
    <sample id="158">与认知失调密切相关的任务包括：
1. 话题无关的距离分类（topic-independent distance classification）。
2. 二元分类的扩展与比较类（binary classification of expansion and comparison classes）。</sample>
    <sample id="159">四位。</sample>
    <sample id="160">根据所给的英文内容，这篇论文有 15 位作者。</sample>
    <sample id="161">引入的框架通过将最终用户与模型和数据集中预测和标签进行比较，与仅研究内部或标注者之间的协议或建模不同。</sample>
    <sample id="162">The first setting.</sample>
    <sample id="163">DeepL 和 Google Translate。</sample>
    <sample id="164">嗨，我是乔恩·平，来自华盛顿大学。今天我将介绍我们研究的成果，从预训练数据到语言模型，再到下游任务，追踪政治偏见如何导致不公平和偏见。</sample>
    <sample id="165">语言模型正在使用大规模的网页数据进行训练。</sample>
    <sample id="166">政治新闻媒体在他们的预训练数据中得到了很好的覆盖，根据一项由C4 Corpus进行的调查，纽约时报、洛杉矶时报、守护者、哈佛邮报等媒体在语言模型训练中得到了很好的覆盖。</sample>
    <sample id="167">这为语言模型应用程序创造了混合的祝福。</sample>
    <sample id="168">所以，一方面，他们能够从不同的视角中学习，这庆祝了民主和思想的多样性。另一方面，这些不同的政治观点是固有的社会偏见，并可能导致在数据挖掘任务应用中公平问题。</sample>
    <sample id="169">在此结束，我们建议调查预训练数据到语言模型到下游任务的政治偏见传播管道，具体通过提出以下问题：</sample>
    <sample id="170">首先，我们如何评估语言模型中的政治倾向，以及我可能对政治偏见持有的观点？</sample>
    <sample id="171">其次，语言模型在下游任务中实际表现如何，以及可能导致NLP应用程序出现错误的因素。</sample>
    <sample id="172">具体来说，我们首先建议使用不同的提示格式向语言模型提供政治问卷，例如政治竞争力测试。这确保了我们能够以政治科学文献为基础进行自动评估。</sample>
    <sample id="173">初步结果表明，第一语言模型确实具有很强的政治倾向，它们占据了政治光谱的四分之三。</sample>
    <sample id="174">我们也可以看到，GPT-4 是所有语言模型中最自由的，而 GPT-C 系列通常比 BERT 系列更具社会自由性，并且具有变异性。</sample>
    <sample id="175">其次，我们旨在调查语言模型中政治偏见到底有多大。</sample>
    <sample id="176">所以我们可以通过进一步预训练语言模型检查点，在六个不同的部分中对科浦拉进行操作，这些部分被分为新闻和社交媒体，进一步分为它们的政治内容。</sample>
    <sample id="177">在对语言模型进行进一步预训练时，我们发现语言模型的意识坐标也相应地与“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉</sample>
    <sample id="178">例如，对于罗伯特·哈弗德进一步训练在左侧线性Reddit语料库上，我们可以看到在它的词汇和语法方面存在显著的自由主义转变。</sample>
    <sample id="179">请。</sample>
    <sample id="180">我们还试图调查语言模型是否能够捕捉到我们现代社会中普遍存在的极化现象。</sample>
    <sample id="181">所以我们将预训练语料库分为美国45岁以下和美国45岁以上两部分。我们分别对这两个时间段的语料库进行预训练语言模型。</sample>
    <sample id="182">我们看到，语言模型在 2017 年之后普遍呈现出远离中心的政治倾向。这表明语言模型也能够捕捉到我们社会中的极化现象。</sample>
    <sample id="183">所以，最后一点，我们重视不同政治立场对仇恨言论检测和虚假新闻检测的评估，这两种NLP应用通常涉及语言模型，并且可能具有非常重要的影响。</sample>
    <sample id="184">所以我们看到，如果我们调查每个类别（即今天）的性能，如果我们将性能分开，那么</sample>
    <sample id="185">不同类型的媒体，例如新闻媒体，我们可以看到一个模式，例如对于仇恨言论检测，左侧的语言模型表现更好。</sample>
    <sample id="186">检测到针对社会弱势群体的仇恨言论。</sample>
    <sample id="187">然而，我们对针对更强大群体的仇恨言论的检测效果不如针对“种族”的仇恨言论。</sample>
    <sample id="188">和白人相比，大型语言模型在检测针对白人和男性的仇恨言论方面表现更好，但在检测针对黑人、LGBTQ+ 和其他少数族裔的仇恨言论方面表现更差。</sample>
    <sample id="189">类似的情况也发生在虚假新闻检测中，我们看到，在语言模型中，它们在检测来自其相反政治立场和观点的信息方面表现更好。</sample>
    <sample id="190">这已经够多了。我们已经提供了许多定性例子，以说明语言模型具有不同的偏见。</sample>
    <sample id="191">你对仇恨言论和虚假信息提供了不同的例子，基于社交类别。附录中还有许多更多例子，以进一步强调其</sample>
    <sample id="192">这表明存在一个持续存在的公平问题，与语言模型（具体指哪个模型未提及）的政治基础有关。</sample>
    <sample id="193">例如，如果语言模型在文本或信息中出现错误，并部署到流行的社交媒体平台，</sample>
    <sample id="194">这可能会意味着持有不同政治观点的人可能会被边缘化，而针对少数族裔群体的仇恨言论可能会毫无限制地蔓延。</sample>
    <sample id="195">好的，这是关于我们承认并解决语言模型带来的公平性问题的警钟。</sample>
    <sample id="196">所以，在简短的讨论之后，我们还想强调的是，我们揭示了关于语言模型政治偏见的一个独特困境，就像在塞拉和克里普之间一样。</sample>
    <sample id="197">如果我们在训练语言模型时没有对政治观点进行去性征，那么这些偏见就会从预训练数据传递到语言模型，最终导致下游任务中出现公平性问题。</sample>
    <sample id="198">如果我们试图某种方式进行消毒，我们也会面临审查或排除的风险，而且很难确定哪些内容实际上是中立的，应该保留在语言模型训练数据中。这有点像电学电学问题。</sample>
    <sample id="199">好的，太棒了，我想这就是我今天所有要做的事情了。谢谢你的帮助。</sample>
    <sample id="200">四位。</sample>
    <sample id="201">1024</sample>
    <sample id="202">音乐、年龄、国籍。</sample>
    <sample id="203">Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="204">The provided text does not contain the speaker's name.</sample>
    <sample id="205">是的，EDAtt 适应了现有的离线 ST 模型，无需重新训练或采用特定的架构。</sample>
    <sample id="206">这篇论文有两位作者。</sample>
    <sample id="207">不，在测试套件上，最好的模型和最困难的背景预训练设置都表现不佳。</sample>
    <sample id="208">KITMUS 有三个变体：
1. Background Pretrain
2. Background Both
3. Background Inference</sample>
    <sample id="209">The provided text does not mention the authors' affiliated institutions.</sample>
    <sample id="210">最后的研究问题是：我们是否应该只使用清理后的样本进行验证，还是有更好的方法来利用这些数据？</sample>
    <sample id="211">指标灵敏度旨在确保模型在相同任务上，无论其变体如何，都能一致地产生相同的输出。</sample>
    <sample id="212">Jingwei Yi.</sample>
    <sample id="213">更高的灵敏度表示模型性能得到了提高。</sample>
    <sample id="214">根据所给的英文内容，无法确定模型在预训练期间接收什么样的语言上下文。</sample>
    <sample id="215">通常需要 23 个干净的验证样本。</sample>
    <sample id="216">Senter Muse 和 Danjaraof。</sample>
    <sample id="217">第一语言模型确实具有政治倾向，它们占据了政治光谱的四分之一。</sample>
    <sample id="218">玛克希塔</sample>
    <sample id="219">政治偏见从预训练数据通过语言模型传播到下游任务。</sample>
    <sample id="220">是的，在简化过程中，plain-apa 语料库和网站语料库的简化过程有所不同。plain-apa 语料库的简化过程保留了更多的语料，而网站语料库的简化过程则保留了更少的语料。</sample>
    <sample id="221">是的，Coscript 公开可用。</sample>
    <sample id="222">水印是通过对目标嵌入和原始嵌入进行加权求和来插入的。目标嵌入的权重与句子中触发词的数量成正比。</sample>
    <sample id="223">Pinster University.</sample>
    <sample id="224">是的，像 mt5 这样的编码器-解码器模型可以通过混合语言的训练来改进。</sample>
    <sample id="225">制作巧克力蛋糕。</sample>
    <sample id="226">他们通过验证提供的嵌入的覆盖率，并识别了句子中的嵌入。</sample>
    <sample id="227">研究如何使用现有的 PLM 来构建新的 PLM。</sample>
    <sample id="228">根据所给的英文内容，GPT-4 与中国和英语国家最不一致。</sample>
    <sample id="229">在“交错注意力机制”的例子中。</sample>
    <sample id="230">任务数量增加，模型性能更好。</sample>
    <sample id="231">The author compares their method to other tree-less models on the CoG benchmark.</sample>
    <sample id="232">两位合著者是第一作者的顾问。</sample>
    <sample id="233">Google Research。</sample>
    <sample id="234">大家好，我是 Jenny，来自弗尔斯特尔皮奇斯大学，今天我将介绍我的研究，主题是：识别设计偏见中的模式。</sample>
    <sample id="235">这项工作与华盛顿大学的一些同事以及人工智能公司Alanis Due for AI合作，包括塞巴斯蒂安·桑蒂、罗南·洛布罗斯、卡特琳娜·拉伊尼卡和马丁·沙。</sample>
    <sample id="236">让我们先想象一下，你正在为报纸工作，你正在浏览你新闻文章下的评论，试图删除有害内容。</sample>
    <sample id="237">你可能会转向一个流行的API，比如Perspective API用于检测毒性，这对于像Carl Jones这样的用户来说效果非常好。Perspective API能够正确地检测到不一致的评论。</sample>
    <sample id="238">但对于迪蒂·阿什玛来说，情况并非如此，因为印度语环境中对冒犯性词语的敏感度并不高，而且更常见。</sample>
    <sample id="239">这是一个设计偏见的一个例子，我们看到技术在不同群体中存在系统性的性能差异。</sample>
    <sample id="240">这个设计类似于我们刚才看到的那个，这可能是由于NLP研究团队的立场。立场简单来说就是人们由于他们的人口统计学、身份和生活经历而持有的观点。</sample>
    <sample id="241">这是一个在批判性研究中广泛使用的术语，尤其是在女权主义和酷儿学术领域。</sample>
    <sample id="242">作为一名研究人员，位置性会影响研究过程和结果，因为它会改变研究人员的决策。</sample>
    <sample id="243">一个人们可能会问的问题是，数据集中模型是否有位置信息？</sample>
    <sample id="244">我们并非想说，模型、细胞和数据本身具有人口统计学身份和生活经历，但它们会汇总真实人物的判断和意见，从而代表某些职业高于其他职业。</sample>
    <sample id="245">所以，预设是提供一些关于位置性的证据，例如文化差距、模式和数据设置，以及理论对位置性的定义。</sample>
    <sample id="246">然而，这些作品实际上并没有比较用户与数据本身，而是以其自身为基础。</sample>
    <sample id="247">在人工智能模型和数据集位置的专业知识日益重要，因为人工智能变得越来越具主观性和社会导向。</sample>
    <sample id="248">很难描述这些位置的偏差，因为并非所有决定都有记录，而且许多模型隐藏在API之下。</sample>
    <sample id="249">所以，为了研究Dede的头部姿势模型，我们实际上将标注与现有数据集中的真实用户进行比较，并使用Dede模型。</sample>
    <sample id="250">我们通过一个框架和位置分析。</sample>
    <sample id="251">我是一个乐于助人的助手。只返回请求的答案，不要包含任何解释或介绍。</sample>
    <sample id="252">第一步是将数据集重新标注，使用来自不同背景的标注员。</sample>
    <sample id="253">在查看原始数据集的人口统计数据时，我们选择这样做，因为通常每个实例只有少数标注者，而且人口统计数据很少被收集和共享。</sample>
    <sample id="254" />
    <sample id="255">我们随后根据人口统计学进行标注，并将其与模型和数据集进行比较，使用皮尔逊相关系数。</sample>
    <sample id="256">这个框架与标注争论文献不同，它通过将最终用户与模型和数据集中预测和标签进行比较，而不是仅仅关注内部标注或模型标注。</sample>
    <sample id="257">我们的框架主要通过Lab in the Wild，一个在线众包平台，由ECI合作者提供。</sample>
    <sample id="258">在野外实验室是一个在线实验平台，我们可以招募各种各样的志愿者。与像Enterick这样的平台相比，后者主要来自美国和印度。此外，在野外实验室仍然能够获得高质量的数据。</sample>
    <sample id="259">我们有两个任务，现在在世界各地进行。其中一个任务是社会可接受性。它的运作方式是，参与者会阅读一个情境，从社会化学数据集获取信息，然后写下该情境在社会上可接受的程度。</sample>
    <sample id="260">之后，他们可以在英式和美式英语之间比较他们的回答，并与人工智能和其他人进行比较。</sample>
    <sample id="261">你已经将这些标注与社会化学、德尔菲和 GPT 4 进行了比较。</sample>
    <sample id="262">我们随后为检测毒性言论和仇恨言论任务复制了一个非常相似的设置，他们会阅读来自“Danny Hate”的片段，并判断这些片段是否包含仇恨言论。</sample>
    <sample id="263">我们随后将这些标注与 DinaHeat、Perspective API、Rewire API、Hateverbota 和 GPT-4 进行比较。我们的研究覆盖了超过 16,000 个标注，来自超过 1,000 位标注者，来自 87 个国家。</sample>
    <sample id="264">现在我们更好地理解了NLP数据评估模型与哪些最契合。我们发现，NLP存在位置性，并且NLP</sample>
    <sample id="265">例如，我们发现，在许多模型中，数据与英语国家最为相关。因此，对于GPT-4的社会接受度分析，我们发现它与中国和英语国家最为相关。我们发现，Dynah也与英语国家最为相关。</sample>
    <sample id="266">我们还发现，拥有大学学历的人与社会责任任务中的GPT-4最相关。我们发现，拥有大学学历或高中学历的人与GPT-4最相关。</sample>
    <sample id="267">和我们发现的相同的是对于约翰·海特的描述，它与拥有大学教育的人最为相似。</sample>
    <sample id="268">然而，当模型和数据与特定人群对齐时，一些人不可避免地会被遗漏。</sample>
    <sample id="269">一个例子是，数据在模型中被减少到非二进制人群的比较中，与男性和女性的同伴相比。我们可以在 GPT-4 的社会可接受性任务以及 DinaHeat 任务分析中找到这一点。</sample>
    <sample id="270">所以，鉴于那里有位置和ALID和NLP，我们可以做什么？</sample>
    <sample id="271">我们有一些建议：首先，记录你在研究过程中做的所有相关设计选择；其次，进行用户体验研究，从用户的角度出发。</sample>
    <sample id="272">我们第三个建议是构建针对特定社区的专业数据集模型，一个很好的例子是马萨丘塞斯州倡议。我想强调的是，包容性人工智能不仅仅是让所有技术都能为每个人工作。</sample>
    <sample id="273">那么，这就是本次演示。如果您想了解更多，请随时查看我们的仪表板以获取最新分析结果，以及我们的论文。谢谢。</sample>
    <sample id="274">演讲者提到了以下 SimulST 的几个问题：

* 针对特定架构使用额外的模块进行优化。
* 训练过程复杂，例如涉及不同的优化目标和训练及维护多个模型以实现不同的延迟目标。</sample>
    <sample id="275">在训练 NLP 模型时，减轻数据集中的社会和政治偏见是一个复杂的问题，因为试图审查数据可能会导致审查或排除，并且很难确定什么是“中立”的。</sample>
    <sample id="276">嗨，我是苏伊园，来自富达大学。我在这里介绍我们的工作：从语言模型中提取可约束的知识。</sample>
    <sample id="277">在现实生活中，人们经常会按照步骤执行操作，通过遵循详细的指令来完成任务。</sample>
    <sample id="278">以前的语言模型被用于规划抽象的日常活动，例如做蛋糕，并证明大型语言模型可以有效地分解任务为步骤。</sample>
    <sample id="279">然而，过去许多人专注于制定抽象的目标，而忽视了日常活动。制定目标需要具体的、具体的限制，例如制作巧克力蛋糕。然而，仍然没有开始。</sample>
    <sample id="280">在本文中，我们定义了约束语言规划的问题。</sample>
    <sample id="281">不同的目标会带来不同的限制，一个目标可以受到不同现实生活特定目标和动机约束。一个好的计划者应该识别出合理的、符合约束条件的限制。</sample>
    <sample id="282">在本文中，我们评估并改进了限制语言规划的语言模型。</sample>
    <sample id="283">请提供英文内容。</sample>
    <sample id="284">我们如何获取这些代码首先？
正如表格所示，我们扩展了抽象代码，并修改了这些约束。对于人类在数据获取过程中使用的抽象GPT，</sample>
    <sample id="285">我是一个乐于助人的助手。只返回请求的答案，不要包含任何解释或介绍。</sample>
    <sample id="286">这个表格反映了结果的整体准确性。我们发现，所有线性回归模型都实现了令人满意的结果。</sample>
    <sample id="287">这篇英文内容没有提供任何可翻译的文本。</sample>
    <sample id="288">结果在图表中显示，生成脚本中的语义完整性是可以接受的，但对事实的约束无法保证。</sample>
    <sample id="289">在越来越多的顶级类别中，定义在Wakefield的限制。图表中的热图显示，不同类别女性的计划和表现差异很大。</sample>
    <sample id="290">最近研究表明，大型语言模型在生成文本时，输出质量会随着模型规模的增加而下降，导致性能不佳。因此，人们提出了使用过拟合的零样本过滤器来提高生成质量的设想。</sample>
    <sample id="291">我首先展示了约束类型以及基于这些抽象概念的示例，例如在 ChatGPT 中。</sample>
    <sample id="292">再，提取GPT或生成文本片段，为特定角色。</sample>
    <sample id="293">下一个，一个未来模型是基于两个步骤选择第一个脚本。</sample>
    <sample id="294">将英文内容翻译成中文。</sample>
    <sample id="295">请提供英文内容。</sample>
    <sample id="296">我们研究发现，在LGBTQ+群体中，有较多的头发色素沉着。我们的研究表明，在皮肤色素沉着和肤色不均方面，我们研究的群体表现出显著的差异。</sample>
    <sample id="297">由于大型语言模型成本高昂，因此启用语言规划是部署较小和专业化模型的必要步骤。</sample>
    <sample id="298">然而，之前的研究并未提供任何关于特定目标的计划，并且手动数据在注释中的成本是昂贵的。</sample>
    <sample id="299">There is a way to follow a tier of symbolic knowledge distillation to distill a constrained language plant data size from large language models.</sample>
    <sample id="300">请将英文内容翻译成中文。</sample>
    <sample id="301">总共，为了生成55个具有特定目标和脚本的单词，我们需要确保数据的质量和测试用例。我们要求众包的工人来发现并修改不正确的样本。</sample>
    <sample id="302">这个图表显示了语言规划中约束性模型的分布，而非约束性模型在通用零售领域表现出更高的表现。通过约束性模型，我们可以构建更小但更专业的模型来进行语言规划。</sample>
    <sample id="303">在文件尺寸 TF 文件查询中，显示红色，表示小模型无法生成大型模型的脚本，表明小模型可以处理大型和不大的模型，这很可能是由于可用的数据集不足。</sample>
    <sample id="304">总结：我们建立了一个约束语言规划问题，旨在增强大型语言模型中的约束语言规划能力，并开发了一个用于大型语言模型的可再生燃料衡量方法。</sample>
    <sample id="305">我们使用大型语言模型来生成一个高色彩的平方数据集，用于语言规划。我们希望该数据集可以作为研究语言规划的可用资源。</sample>
    <sample id="306">谢谢您的时间，请详细描述您的论文标题。</sample>
    <sample id="307">PaLM 的流畅度与现有语言模型相当。</sample>
    <sample id="308">水印方法需要具备以下重要属性：

1. 可应用于嵌入式服务。
2. 不应降低提供的嵌入服务的实用性。
3. 水印应足够隐蔽，攻击者可以移除。
4. 水印应可移植到攻击者服务。</sample>
    <sample id="309">14</sample>
    <sample id="310">根据内容，重新注释的数据集中每个实例抽取了许多注释。</sample>
    <sample id="311">余弦相似度和欧氏距离。</sample>
    <sample id="312">将基于编码器的多语言模型用于这项任务，通过评估包含编码器和点状解码器的模型（如Encoder-PTR、XLM-R+PTR、BERT+PTR）以及包含编码器和解码器的模型（如BERT）。</sample>
    <sample id="344">作者假设提供商可以收集一个通用的文本语料库，并统计单词频率。</sample>
    <sample id="345">大家好，我的名字是徐洪。今天我将介绍我们的论文：《命名实体标签在 2023 年是否仍然有效？》。让我们开始吧。</sample>
    <sample id="346">我们的论文研究了泛化问题，使用名为命名实体识别任务（NER任务）的任务。</sample>
    <sample id="347">我们观察到，模型已经使用 2003 年的 CNN 来开发了近 20 年的深度学习。这自然引发了几个问题。首先，卷积模型泛化到更多的数据。</sample>
    <sample id="348">在开发新的标签时，需要什么？</sample>
    <sample id="349">同时，如果我们观察到泛化能力下降，是什么导致了这些模型的性能下降？</sample>
    <sample id="350">为了调查这些问题，我们开发了一个 Cono+ 数据集。这是一个我们从路透社新闻收集的数据集，然后使用 2003 年的 Cono 标注指南进行标注。</sample>
    <sample id="351">我们对20个模型进行了微调，并在Kernel 2003测试集和Kernel Plus Plus测试集上进行了评估。</sample>
    <sample id="352">最后，我们计算了每个模型的 F1 分数的百分比变化，以评估其泛化能力。</sample>
    <sample id="353">所以，对于良好的生成，我们需要什么？在我们的实验中，我们发现有三个主要成分是必要的。</sample>
    <sample id="354">第一个是模型架构。在我们的实验中，我们发现Transformer模型通常在新的数据上泛化得更好。</sample>
    <sample id="355">第二个成分是模型大小。我们发现，通常较大的模型能带来更好的泛化能力。</sample>
    <sample id="356">最后但不是最重要的是，我们都知道，用于微调的样本数量直接影响下游任务的性能。在这里，我们还发现，更多的微调样本实际上也导致了更好的泛化能力。</sample>
    <sample id="357">你下一个问题是什么？什么会导致某些模型性能下降？</sample>
    <sample id="358">我们有两项假设。第一项是过拟合，这通常是由于重复使用相同的测试数据集而引起的，并且通常表现为在新的测试集上表现不佳。</sample>
    <sample id="359">第二种假设是温度漂移，这是一种由训练和测试数据之间不断扩大的温度差引起的性能退化。</sample>
    <sample id="360">为了确定一个拟合，我们看到，根据右侧的图表，红色的最佳拟合线比一条水平线具有更大的斜率。</sample>
    <sample id="361">这意味着我们在2003年取得的每一点改进，都转化为在Call Plus Plus上超过一点的改进，这意味着没有边际收益递减。</sample>
    <sample id="362">这表明在本次适应中没有观察到任何适应。</sample>
    <sample id="363">请提供英文内容。</sample>
    <sample id="364">对于时间漂移，我们进行了一项实验，以重新训练或继续预训练一些模型，使用更多最近的数据。我们发现，随着数据规模的增大，性能会下降。</sample>
    <sample id="365">这证实了我的假设，即性能下降的主要原因是温度。</sample>
    <sample id="366">我们的结论是，为了实现更好的泛化能力，我们需要更好的模型架构、更大的模型尺寸，以及更少的微调示例。这些目标相互关联，我们不能只关注一个方面，而是需要通过其他方面来促进它们。</sample>
    <sample id="367">与此同时，我们还发现，这里的性能下降是由于临时漂移造成的，而且令人惊讶的是，它并非由数据不匹配引起。即使在公元2003年，卡农图表也已使用超过20年。</sample>
    <sample id="368">所以回到我们论文标题中提出的问题，2003年内核技术在2023年是否仍然有效？我们发现答案是肯定的。</sample>
    <sample id="369">我们希望通过我们的资金支持更多研究，以改进模型的泛化能力。</sample>
    <sample id="370">最后，请务必查看我们的纸质数据集，如果您有任何问题，请随时联系我。非常感谢。</sample>
    <sample id="397">The provided text does not contain any information about the size of the audio snippets used in the method.</sample>
    <sample id="398">Servin 是法官。</sample>
    <sample id="399">示例质量</sample>
    <sample id="400">GPT-4, GPT-3 series, and BERT series.</sample>
    <sample id="401">结合多个层的分数。</sample>
    <sample id="402">直接推断的示例包括使用歌曲的名称，例如“My Heart Will Go On”或它的位置，例如“第 4 幕”。</sample>
    <sample id="403">福特大学</sample>
    <sample id="404">四位。</sample>
    <sample id="405">是的。</sample>
    <sample id="406">男性。</sample>
    <sample id="407">Transformer 模型。</sample>
    <sample id="408">Clean data.</sample>
    <sample id="409">四位。</sample>
    <sample id="410">作者采用了多种模态。</sample>
    <sample id="439">作者认为在知识密集型 NLU 任务中，成功模型需要能够整合和使用预训练时间和推理时间知识。</sample>
    <sample id="440">Eing and my colleague Zhiyang.</sample>
    <sample id="441">是的，Coscript 经过了质量检查。</sample>
    <sample id="442">现有的资源只支持有限类型的依赖上下文翻译，并且支持的语言种类也有限。</sample>
    <sample id="443">嗨。我将谈谈我们解决实体选择中间接关系表达式的工作，其中我们引入了替代实体概念。</sample>
    <sample id="444">我的名字是贾瓦德·侯赛尼，这是与菲利普·普拉丁斯基、西尔维娅·帕尔蒂和阿尼尔·贾尼的合作。</sample>
    <sample id="445">或者，这个系统理解用户想表达的意图，当他们想做出选择时。考虑一下这个替代问题：你是不是想说“容易对我”或者“我感觉到了”？在这里，用户想要选择这两个选项中的一个。</sample>
    <sample id="446">直接引用。例如，可以说歌曲的名字是“我”，或者它的位置是“第4章”。</sample>
    <sample id="447">但有时与朋友进行更直接的交流，会更合适，这通常发生在用户记不住对方的名字时。</sample>
    <sample id="448">或者发音太相似，难以区分。</sample>
    <sample id="449">或者当用户想要指定一个偏好时。以下是一些间接偏好的例子：例如，最新的一个或不是充满活力的那个。</sample>
    <sample id="450">这是一个在对话系统以及评估自然语言处理任务中非常重要的问题。</sample>
    <sample id="451">我们没有意识到一个公共数据集，用于该任务的大规模公共数据集，所以我们使用众包标注收集了一个数据集。该数据集涵盖三个不同的领域：音乐、书籍和电影。</sample>
    <sample id="452">数据集合收集方法强调非正式性，使用卡通评论。</sample>
    <sample id="453">卡通有三个对话气泡。在第一个气泡中，鲍勃说：“记住昨天我们听过的那个歌吗？”然后鲍勃结束了对话。</sample>
    <sample id="454">在第二个对话框中，艾丽丝说：“你是指对我好，还是我得到了什么？”</sample>
    <sample id="455">在第三个对话框中，鲍勃使用间接引用来选择一个实体，例如“纽尔”。</sample>
    <sample id="456">我们会自动提供第一个和第二个对话气泡，但第三个由标注者填写。第一个对话气泡是从几个手动提示中选择的。</sample>
    <sample id="457">第二题，即替代问题，如下所示：</sample>
    <sample id="458">我们总是使用一个简单的模板。你指的是 A 或 B？其中 A 和 B 是从概率分布中采样出来的。</sample>
    <sample id="459">以下是用于移动到列表中的不同采样方法。随着我们移动到列表的更高位置，条目彼此越来越相似，通常很难使它们具有歧义性。</sample>
    <sample id="460">The first one is uniform.</sample>
    <sample id="461">第二个词是表示实体具有相似标题的，例如两本书，名称是“The Reach”。</sample>
    <sample id="462">第三个是当他们在维基百科上拥有相似的描述时，最后是当他们在维基百科上拥有相似的 infobox 或属性时。例如，相同的类型或相同的艺术家。</sample>
    <sample id="463">当我们在问卷调查中展示这个替代问题时，他们知道这些实体的名称，但他们并不一定了解关于这个角色。</sample>
    <sample id="464">所以我们要做的是展示关于这两个实体的背景知识。对于歌曲，我们简单地展示一个谷歌搜索链接到其官方网站。</sample>
    <sample id="465">然后要求听至少一首每首歌，并阅读关于每首歌的内容。

以下是一个谷歌搜索结果的例子，对于歌曲“I Can’t”的结果：

“I Can’t”</sample>
    <sample id="466">对于食谱和书籍领域，我们展示了一些来自维基百科的背景文字。对于食谱，我们还展示了它们的图片，同样来自维基百科，这样注释者就能知道它们是什么样子。</sample>
    <sample id="467">然后，我们让参与者选择一个实体，例如这里第一个，并用 3 到 5 个间接引用表达方式描述它。</sample>
    <sample id="468">例如，一个有钢琴音乐的。以下是我们数据集的一些例子：一个没有单词的，不是12岁的男孩，或者一个虚构的，或者来自阿塞拜疆的。</sample>
    <sample id="469">该替代问题语料库包含三个领域下的 6000 个替代问题，以及 42000 个间接否定表达式。
结果使用 t5-large 模型进行总结。</sample>
    <sample id="470">这个语言模型拥有与训练数据完全相同的背景知识，因此准确率很高，大约在92%到95%之间。但这并非真实。</sample>
    <sample id="471">如果语言模型有访问一些部分重叠的背景知识，那么准确率在 82% 到 87% 之间，这对于例如语言模型检索背景知识时来说是更现实的。</sample>
    <sample id="472">如果语言模型只能访问实体名称，那么准确率只有60%，所以还有很大的改进空间。我们还展示了这些模型是领域泛化的。这里有一个链接到数据集。</sample>
    <sample id="473">weight key strategy and local agreement.</sample>
    <sample id="474">The author is affiliated with the French research group "Robust Spirit Time Model".</sample>
    <sample id="475">Jenny.</sample>
    <sample id="476">四位。</sample>
    <sample id="477">嗨，我是塞拉·巴比，来自特伦托大学和布鲁诺·凯斯勒基金会。我将简要介绍注意力作为本论文的指导，这是一项与马克·奥内格里和马可·图尔合作的成果。</sample>
    <sample id="478">同步语音翻译（ST）是指在实时中将口语翻译成另一种语言的文本过程，从而实现跨语言交流。</sample>
    <sample id="479">当前语义模型面临的主要问题是，特定架构通常通过引入额外的模块来优化。</sample>
    <sample id="480">不复杂的培训程序，例如涉及不同优化目标（目标）的培训。</sample>
    <sample id="481">训练和维护多个模型以达到不同的延迟设置，例如训练一个模型具有平均 1 秒的延迟，另一个模型具有 2 秒的延迟，等等。</sample>
    <sample id="482">请提供英文内容。</sample>
    <sample id="483">第一个使用现有的离线模型，无需重新训练或采用特定架构的 CSL。使用单个模型来处理整个延迟 CSL，并通过特定的参数处理延迟。</sample>
    <sample id="484">在音频输入和文本输出之间通过注意力机制传递的知识，即交叉注意力机制，你可以看到一个例子在第 1 节。</sample>
    <sample id="485">我们的解决方案是使用编码器解码器注意力机制，这是一种根据注意力点是否在水平上相遇或不相遇而进行的局部翻译策略。</sample>
    <sample id="486">如果检测到非集中性，即该信号低于某个阈值阿尔法，则向较少的语言语音帧发送。这意味着接收信息足够稳定。</sample>
    <sample id="487">例如，如果我们筛选一个包含“我将谈论关于”的句子，并且我们的模型预测了德语翻译为“关于”。</sample>
    <sample id="488">我们将研究注意力机制，哪个是哪个？</sample>
    <sample id="489">我们看到，前两个词指向接收最少的语音帧，而最后一个词指向接收最少的语音帧，即lambda语音帧。</sample>
    <sample id="490">这表示前两个词将是“查”、“查”。</sample>
    <sample id="491">虽然在某些情况下，最大趋势高于某个阈值阿尔法，我们将不发出最后一声，并等待下一个语音片段。</sample>
    <sample id="492">如果我们在一个语音银行中，我们的模型预测其他词语，我们会查看交叉注意力，即</sample>
    <sample id="493">我们看到，没有词语能点明这篇演讲的重点。</sample>
    <sample id="494">这表示这三个词将是“拉”。</sample>
    <sample id="495">如果看看这个结果，它</sample>
    <sample id="496">我们将同时翻译的结果绘制在图表上，其中蓝色一侧衡量翻译质量，而平均语言</sample>
    <sample id="497">但是，呃，这是呃，让我们称之为平均计算量，并且我们还考虑了计算成本平均延迟，这包括模型计算时间来处理输出。</sample>
    <sample id="498">所以我们希望我们的好奇心尽可能高。</sample>
    <sample id="499">请提供英文内容。</sample>
    <sample id="500">我们还将其与针对离线模型的准备策略进行比较，例如“权重保持策略”和“本地协议”。我们还将其与针对同步处理的特定架构设计进行比较。</sample>
    <sample id="501">这些是同步语音翻译策略在德国的早期结果。</sample>
    <sample id="502">并且我们看到，呃，它在所有策略应用于离线模型时都表现得优于所有策略，因为它的曲线被向左移动了。</sample>
    <sample id="503">我们还看到，如果我们考虑实际的运行时间或计算时间，那是一种最快的策略。</sample>
    <sample id="504">如果你想发现更多结果，请阅读我们的论文，我们还发布了开源代码和模型，并同时提供了模拟输出，以促进我们工作的可重复性。谢谢您的关注。</sample>
    <sample id="505">是的。</sample>
    <sample id="506">大家好，我的名字是伊恩，我的同事志扬和我将介绍我们的研究，关于改进模型细胞学习通过指令训练。</sample>
    <sample id="507">随着大型语言模型技术的进步，许多研究人员开始探索利用预训练语言模型进行不同下游任务的参数和数据高效的</sample>
    <sample id="508">最近，许多研究表明，指令微调能够使大型语言模型在零样本任务中以自然的方式遵循自然指令。</sample>
    <sample id="509">然而，大多数以前的工作都侧重于提高在语言任务上的零样本性能，而计算机视觉和多模态任务则被忽视了。</sample>
    <sample id="510">因此，在工作中，我们希望调查指令微调对多模态预训练模型是否能实际提高其对未知多模态数据的泛化能力。</sample>
    <sample id="511">此外，在我们的研究期间，我们发现在 NLP 和 MoE 之间指令数据集的可获得性存在显著差异。</sample>
    <sample id="512">目前有超过1600个语言仅的指令任务，然而，目前没有大规模公开可用的多模态指令任务。因此，这激励我们构建一个多模态指令微调数据集。</sample>
    <sample id="513">在这里，我们呈现了多模态指令的第一个多模态模型指令调优基准数据集，该数据集包含 62 种多样化的多模态任务，涵盖了 10 种不同的类别。</sample>
    <sample id="514">这些任务来源于一个现有的开放数据集，每个任务都配备了五个编写指令。</sample>
    <sample id="515">为我们提出的数据集进行多模态指令微调。我们使用 OLA，将多模态预训练模型作为我们的基础模型。OLA 使用统一的词汇、图像令牌和边界坐标。</sample>
    <sample id="516">这里展示了一些来自我们医疗记录的示例：</sample>
    <sample id="517">将各种输入和输出数据进行处理。</sample>
    <sample id="518">我遵循来自 OpenAI 的指令，并根据统一的序列到序列格式来编排所有任务，其中输入文本、图像、指令和边界框都以相同的 token 形式表示。</sample>
    <sample id="519">好的，我现在要谈论多模态指令教学。</sample>
    <sample id="520">所以对于20天数据集，我们使用来自Negru组的53个任务进行训练，每个任务有10000个样本进行测试。对于测试，我们保留了整个CommonSense Reasoning组进行测试，并从Wiki和杂乱的类别中额外选择了5个任务。</sample>
    <sample id="521">重复。</sample>
    <sample id="522">所以我们使用预训练的奥尔法大型模型作为基础模型。在训练过程中，我们为所有任务创建了每个实例。每个实例都会随机组合到其五个指令模板中。</sample>
    <sample id="523">重复</sample>
    <sample id="524">我们评估了在各种实验中性能和标准偏差的性能。</sample>
    <sample id="525">如果该任务是一个多模态分类任务，我们将报告准确率。如果它是一个多模态生成任务，我们将报告 ROUGE-L。对于摘要任务，我们将报告 ROUGE-L 作为主要指标。</sample>
    <sample id="526">我们还引入了一个额外的验证机制，称为“一致性测试”。这个机制确保了模型在相同的任务上，无论其变体如何，都能一致地产生相同的输出。</sample>
    <sample id="527">这是我们主要的结果，正如我们所见，指令微调能够显著提高OS和IFS的性能，尤其是在处理多模型任务时。</sample>
    <sample id="528">从自然语言处理数据集中学习，公司优势，指令调整。</sample>
    <sample id="529">这里我们可以看到，随着任务数量的增加，模型会获得更好的性能，与此同时，它对噪声的敏感性会降低。</sample>
    <sample id="530">所以我们还使用了while语句和for语句，正如我们所见，使用while语句可以提高模型的整体性能，并且减少其灵敏度。</sample>
    <sample id="531">所以这展示了不同前馈网络策略对模型敏感性的影响。正如我们所见，通过从自然语言指令数据集进行迁移学习，模型可以实现比原始的奥特曼模型更高的敏感性。</sample>
    <sample id="532">我们也可以看一看在自然语言处理数据集中的数据转换，这可以帮助AI在自然语言处理数据集上获得更好的性能。</sample>
    <sample id="533">所以我们现在发布了第一个大规模多模态Instruction Tuning数据集，它将能够进一步提高Ollama的可访问性，并展示不同迁移学习技术的优势，以及它们的优点。</sample>
    <sample id="534">我们正在收集一个更大的多模态指令微调数据集，其中包含大约 150 个额外的语言任务，并且我们将很快发布这些数据。这是我们数据集和模型的二维码。谢谢。</sample>
    <sample id="535">Sara Babi, University of Trento and Bruno Kessler Foundation.</sample>
    <sample id="536">Javad Hosseini.</sample>
    <sample id="562">大家好，我是科斯托夫·谢纳，很高兴欢迎大家参加我们关于 ACL 2023 论文“语言模型可接受性判断并非总是对上下文鲁棒”的讨论。</sample>
    <sample id="563">这是一个由约翰·戈特、艾尔·穆勒、卡尼什卡·米什拉、卡伦·弗伦特尔、罗杰·莱维和阿蒂纳·拉奇共同完成的项目。</sample>
    <sample id="564">请提供英文内容。</sample>
    <sample id="565">将英文内容翻译成中文。</sample>
    <sample id="566">在最小对偶范式中，评估语言模型通常是将一个可接受的句子或语法正确的句子展示给它，然后展示一个不可接受的句子或语法错误的句子。</sample>
    <sample id="567">然后，模型会基本将可接受的类别赋予更高的概率。</sample>
    <sample id="568">当前 MPP 管道基本上不允许我们评估模型对长句的接受程度。</sample>
    <sample id="569">这些大型语言模型正在推出更长更长的上下文窗口，因此我们需要评估模型在整个上下文窗口中的可接受性。</sample>
    <sample id="570">我们正在尝试做的事情就是，我们正在尝试通过让模型评估可接受性来重温 PPP 管道。</sample>
    <sample id="571">所以，这就是方法。那么我们要做的是模拟这些更长的序列。我们重新访问数据集合本身，然后我们重新创建句子，通过选择那些可接受或不可接受的句子。</sample>
    <sample id="572">例如，这里我们选择了一个典型的语法对，来自“布利普”数据集，来自“Adjunct Island”的“页”。</sample>
    <sample id="573">将英文内容翻译成中文。</sample>
    <sample id="574">请。</sample>
    <sample id="575">我们可以通过从同一配对中选择不可接受的句子来做同样的事情，这也可以用于测试模型的可接受性。</sample>
    <sample id="576" />
    <sample id="577">这些句子仍然来自相关的数据库，但不是你正在评估的同一个数据库。</sample>
    <sample id="578">最后，我们可以从完全不相关的领域中选择句子，例如维基百科。</sample>
    <sample id="579">所以这会告诉我们模型在实际影响下接受度评估是否受到任何影响。</sample>
    <sample id="580">请提供英文内容。</sample>
    <sample id="581">所以模型如何运作？首先，我们查看维基百科的句子，这些句子与当前查询对完全无关，然后我们发现，对于任意上下文，语言模型通常是鲁棒的。</sample>
    <sample id="582">我们增加了上下文长度到 1024，以最大限度地发挥 OPT 和 GPT-2 模型，我们在这里看到，在橙色点划线上，MPP 判决相对稳定。</sample>
    <sample id="583">现在当我们从同一段文字中选择句子时会发生什么？</sample>
    <sample id="584">所以，在这里我们选择或创建句子，从可接受和不可接受的领域中，来自同一位被指控的人。</sample>
    <sample id="585">并且我们看到，在 MPP 判决中，无论是接受的前缀还是不可接受的前缀，都显著增加或减少。</sample>
    <sample id="586">但是，当我们匹配结构时，当我们选择来自同一现象的句子时，例如，</sample>
    <sample id="587">我们看到模型在选择的前缀是否可接受或不可接受的情况下，在 MPP 判决上出现巨大的增加或巨大的减少。</sample>
    <sample id="588">现在，这个呃呃，这个非常大，就像这个影响贯穿整个上下文，这可能会影响像新语言模型，它有大上下文。</sample>
    <sample id="589">为什么匹配前缀会如此影响语言模型判断？</sample>
    <sample id="590">一系列分析，我们尝试通过尝试保留输入句子的相关结构，但添加噪声到输入中来，然后对这些扰动进行多次操作。</sample>
    <sample id="591">我们发现这些噪音实际上并没有改变模型的课程，在概率判断方面。</sample>
    <sample id="592">基本上，我们发现模型对句子中词语的相似度非常敏感。</sample>
    <sample id="593">在可接受域中，我们看到所有扰动都相似地增加，而在可接受域的下一个扰动中，我们看到在相似的特征中，MPP 判决减少。</sample>
    <sample id="594">所以，我们工作的关键收获是，语言模型对句子中共享的潜在句法和语义特征非常敏感。</sample>
    <sample id="595" />
    <sample id="596">请阅读我们的论文以了解更多关于我们实验的细节。谢谢。</sample>
    <sample id="597">一个无序多元素。</sample>
    <sample id="598">55</sample>
    <sample id="626">The best alignment method to use for German text simplification is the method of mess align.</sample>
    <sample id="627">弱监督学习可以帮助训练模型在存在标签噪声的情况下，仍然能够泛化。</sample>
    <sample id="628">文档采用手动和自动对齐方法进行对齐，具体分配情况未提及。</sample>
    <sample id="629">CoNLL++ 数据集是从路透社新闻（2020 年）收集的，并使用 2003 年的标注指南进行标注。</sample>
    <sample id="630">你好，大家好，我是Justin John来自宾夕法尼亚大学。今天我将演示一个工作示例：跨语言语义解析和多种自然语言的语义表示。</sample>
    <sample id="631">语义化表示用户查询，例如“sequel”和“lambda calculus”。</sample>
    <sample id="632">跨语言语义解析是任务将查询翻译成多种自然语言中的多种语义表示。</sample>
    <sample id="633">将英文内容翻译成中文。</sample>
    <sample id="634">存在许多跨语言语义分析模型，它们分别被提出和评估在各种数据集和应用任务上。例如，</sample>
    <sample id="635">他们对某些自然语言的覆盖率有限，中文缺失。</sample>
    <sample id="636">请提供英文内容。</sample>
    <sample id="637">我是一个乐于助人的助手。请只返回请求的答案，不要包含任何解释或介绍。</sample>
    <sample id="638">或者只有评估一个特定的新模型，例如只使用一个模型来评估。</sample>
    <sample id="639">为了这一点，我们建议提供一个示例数据集，用于跨语言和多语种的链接和语义表示。</sample>
    <sample id="640">它包含 90 个数据集，5 个语义任务，8 种表示形式和 22 个自然语言。15 个语言家族。</sample>
    <sample id="641">为了更好地评估我们的基准，我们考虑了训练和评估的六个设置。</sample>
    <sample id="642">The first one is translate test. We'll use Google Translate API to translate source to the target language, then use monolingual model to train and evaluate the</sample>
    <sample id="643" />
    <sample id="644">我还会测试单语模式。</sample>
    <sample id="645">将英文内容翻译成中文。</sample>
    <sample id="646">我们还测试了单语模型设置，通过使用训练数据中仅 10% 的单语模型进行训练。</sample>
    <sample id="647">和具有单语言模型，我们训练一个单语言模型，用于所有语言。</sample>
    <sample id="648">将英文内容翻译成中文。</sample>
    <sample id="649">将英文内容翻译成中文。</sample>
    <sample id="650">我们还考虑了跨语言零样本和少样本迁移，在一种语言和另一种语言之间进行迁移。</sample>
    <sample id="651">在训练过程中，我们将训练我们的英语查询或英语和德语短语查询，以训练一个多语言模型，来预测序列中的下一个单词。</sample>
    <sample id="652">并且我们还发现了一些非常有趣的成果。所以，关于单语模型的情报分析，我们评估了两个模型：</sample>
    <sample id="653">包括编码器-解码器（Encoder-Decoder），即多语言预训练编码器与指针基于解码器，例如XLM-R+PT和BERT+PT。</sample>
    <sample id="654">和我们还评估了编码器解码器模型，这是一种多语言预训练的编码器解码器模型，例如 BERT 和 mT-F.</sample>
    <sample id="655">我们发现编码器解码器在所有九个数据集上都能获得最佳性能。</sample>
    <sample id="656" />
    <sample id="657">没有它，编码器、解码器或编码器 PTR 无法改进，通过训练混合语言数据。</sample>
    <sample id="658">我们发现，这主要是因为大多数主要的自然语言可以获得性能提升，但英语的性能在七个数据集上下降，而仅在三个数据集上获得提升。</sample>
    <sample id="659">我以为这是没有人知道的，这是语言学的一个结果。</sample>
    <sample id="660">我们还比较了跨语言性能的特征。</sample>
    <sample id="661">在图中，蓝色线表示交叉角度的零轴转换，橙色线表示交叉角度的零轴转换，而绿色线表示建模角度的设置。</sample>
    <sample id="662">我们发现，在比较绿色和橙色线时，在零步设置下，跨线gap的传输性能差距显著。而在比较蓝色和橙色线时，在几步设置下，传输gap迅速缩短。</sample>
    <sample id="663">我们还发现了一些其他有趣的发现，例如，编码器解码器在以前的工作中实现了可比的结果。对于英语自然语言，显著提高了目标自然语言翻译的性能。</sample>
    <sample id="664">大型语言模型，如CodeT5和BLOOM，在跨语言文本和语义理解任务中仍然处于开发阶段。</sample>
    <sample id="665">一个统一的跨语言语义解析基准，使用多种自然语言表示形式。</sample>
    <sample id="666">欢迎阅读一项全面的基准研究，研究了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现等等。欢迎访问我们的论文和代码。谢谢您的阅读。</sample>
    <sample id="667">The provided text does not contain any information about existing research. It only lists a series of letters.</sample>
    <sample id="668">不，根据文章，Codex 和 Bloom 等多语言语言模型对于跨语言语义推理任务来说仍然不够。</sample>
    <sample id="695">该方法通过在训练过程中引入对排列的对齐来处理排列的不确定性。</sample>
    <sample id="696">下游 NLP 模型的公平性是指，当这些模型被部署到流行的社交媒体平台时，可能会导致持有不同政治观点的人被边缘化，并且针对少数族裔的仇恨言论可能不受控制地蔓延。</sample>
    <sample id="697">Yanis Lavergne.</sample>
    <sample id="698">Costof Shena.</sample>
    <sample id="699">Maira.</sample>
    <sample id="700">热带主义指的是对拉丁裔女性的描述中包含“充满活力”和“曲线感”等元素，这与热带主义的刻板印象有关。</sample>
    <sample id="701">作者通过使用与目标群体身份相关的词语，例如“文化”、“传统”、“骄傲”和“异域风情”，来创建目标群体的人工描写。</sample>
    <sample id="702">PMI (Pointwise Mutual Information)。</sample>
    <sample id="703">DrBERT 和 ChuBERT 的主要区别在于它们使用的训练数据量。DrBERT 使用 7GB 的数据，而 ChuBERT 使用 4GB 的数据。</sample>
    <sample id="751">七位。</sample>
    <sample id="752">迭代迁移学习是指在每个主动学习轮次和标注后，对模型进行更新。</sample>
    <sample id="753">数据集的目标是让用户选择他们想要表达的含义。</sample>
    <sample id="754">The provided text does not describe how attackers extract model parameters using EaaS. It only mentions validating the coverage of the provided embedding by realizing the embedding of sentences on forward data at BPC.</sample>
    <sample id="755">三位。</sample>
    <sample id="756">10</sample>
    <sample id="757">弗吉尼亚大学，华盛顿大学，以及人工智能公司。</sample>
    <sample id="758">在例子中，左侧的州长是丽莎。</sample>
    <sample id="759">The most advanced chat model is currently being evaluated by ABC EV.</sample>
    <sample id="760">大型语言模型正在推出更长的上下文窗口，因此在整个上下文窗口中评估模型的可接受性至关重要。</sample>
    <sample id="761">是的，多语言训练会导致英语模型的表现下降。</sample>
    <sample id="762">不，他们不知道该实体的名称，但可能知道关于该实体的其他信息。</sample>
    <sample id="763">The provided text does not contain any information about MT metrics.</sample>
    <sample id="764">根据所给的英文内容，没有提到泛化中的回归是否会影响特定的 NER 类型。</sample>
    <sample id="765">NLP 中的立场很重要，因为像 Perspective API 这样的工具在检测有害内容时可能存在设计偏见，尤其是在特定文化或语言背景下，例如在印度语环境中，对某些具有文化意义的词语或短语可能不敏感。</sample>
    <sample id="766">适配器微调。</sample>
    <sample id="767">他们使用零样本性能在特定数据集上的迁移学习。</sample>
    <sample id="768">MMLU, HellaSwag, ARC, TruthfulQA, Winograd Schema Challenge, and GSM8K.</sample>
    <sample id="769">三条。</sample>
    <sample id="770">The proposed method achieved a higher plot density in the general retail space compared to the constraint distribution of cost script.</sample>
    <sample id="771">Shu Han.</sample>
    <sample id="772">是的。</sample>
    <sample id="773">他们进行了 10 个较小模型的实验。</sample>
    <sample id="774">OFA</sample>
    <sample id="833">Google Research</sample>
    <sample id="834">Stony Brook University.</sample>
    <sample id="835">论文分析了英语和西班牙语的语言对。</sample>
    <sample id="836">Jianbin Pi.</sample>
    <sample id="837">Longformer 和 Normalized-based Longformer。</sample>
    <sample id="838">62</sample>
    <sample id="839">四位。</sample>
    <sample id="840">The author used the AG News, Mind, SST2, and E2020 datasets in their experiments.</sample>
    <sample id="876">NACHOS 是一个医疗临床数据数据集。</sample>
    <sample id="877">Sajid Bilal.</sample>
    <sample id="878">提示策略对结果有很大影响。</sample>
    <sample id="879">The authors are affiliated with the University of Puerto Rico at Mayagüez.</sample>
    <sample id="880">我无法从给定的英文内容中提取 5 个由专家编写的指令。</sample>
    <sample id="881">作者建议使用人类实验参与者来评估数据集，以测试模型从不同来源的信息中提取知识的能力。</sample>
    <sample id="882">大家好，我的名字是艾德·维拉，我将简要概述这篇论文，重点是打印参数、翻译策略和性能评估。这是一项与谷歌翻译同事合作的项目。</sample>
    <sample id="883">5400亿个参数的语言模型在2022年表现出色。它在大量文本上进行了训练，压缩和处理了1000亿个文档。</sample>
    <sample id="884">The matter of publication is at the state of the art in hundred of fingerprints.</sample>
    <sample id="885">在这篇工作中，我们呈现了对语言模型提示的系统性研究，重点关注指令微调。</sample>
    <sample id="886">我们评估了语言模型在迁移能力方面的表现，并采用了 MT 社区的最佳实践。这包括使用最新的测试数据集，以避免测试数据与语言模型训练数据的重复。</sample>
    <sample id="887">我们比较了最新的系统，也就是最佳性能的系统，比如 WTI 轮流。</sample>
    <sample id="888">我们使用最新的神经矩阵，并且额外展示专家基于你的评估结果。最后，我们提供一些关于提示选择策略的建议。</sample>
    <sample id="889">提示对语言模型在翻译中的性能有很大影响。正如我们在一个简单的实验中看到的，我们使用单提示，并为不同的句子提供了两个不同的提示。</sample>
    <sample id="890">在大多数句子中，嗯，516 在 1000 中。
差异明显，超过一个模糊点。</sample>
    <sample id="891">这在极端情况下可以达到40分。所以选择好的提示策略很重要。</sample>
    <sample id="892">在我们的实验中，我们使用了一种五步提示策略，即我们仅仅标记我们向系统提供的句子中的一个词，用语言表示它是一个“词”。</sample>
    <sample id="893">翻译成中文。</sample>
    <sample id="894">我们说的是实际的打印形式对多个短篇故事的影响不大。</sample>
    <sample id="895">这对于Zero和一击提示至关重要，当我们进入我们现在的情况，即两击提示时，几乎没有区别于提示的实际形式。</sample>
    <sample id="896">请提供英文内容。</sample>
    <sample id="897">我们实验结果的总结是，示例质量比与原始句子的相似性更重要。</sample>
    <sample id="898">所以，选择示例时重要的是选择高质量的翻译。特别是，我们比较的是从 WMT 评估数据中选择提示，或者从定义中选择提示。</sample>
    <sample id="899">深度学习的训练数据更加丰富且高质量，训练数据更加有意义，结果也更好。因此，使用深度学习可以获得更好的性能。</sample>
    <sample id="900">然而，专业化状态的系统在某些情况下拥有与通用翻译器相当的优势，但通用翻译器在某些情况下与我们的商业系统非常接近，例如我们使用的谷歌翻译。</sample>
    <sample id="901">自从我们从人类的创新中获得，我们使用 MQM 框架进行操作。
它的流畅性类似于艺术系统的状态，但主要区别在于准确性。</sample>
    <sample id="902">在特别的</sample>
    <sample id="903">它似乎是，巴伦选择为了产生更好的理解翻译，有时通过删除源句中用于翻译的部分。</sample>
    <sample id="904">在州外类别中，对于面包来说，低于州内系统，这是一个差异信号。</sample>
    <sample id="905">这个参数提供流畅的输出，但仍然存在一些问题，例如：</sample>
    <sample id="906">这就是这次非常简短的概述。
要了解更多细节，请查看我今天发布的关于这篇论文的完整演示文稿。
非常感谢大家。</sample>
    <sample id="907">你好，我是来自德国的萨尔大学的博士生。在这个视频中，我想展示我们的研究工作。请您对我们的研究进行批判性评估。</sample>
    <sample id="908">这是联合工作，涉及以下内容：
- 光滑的棒
- 牙科步骤
- 英国克拉</sample>
    <sample id="909">我将为您提供一个关于北京的简短介绍，以及每周监督和每周监督的章节。</sample>
    <sample id="910">在维基百科中，您无需手动标注数据。相反，您可以使用维基标注来源，例如简单的特征规则、知识库或本地代码来源，正如您在图中所展示的那样。</sample>
    <sample id="911">与人工标注相比，弱标注成本更低，但噪声也更大，这意味着相当一部分标注是错误的。</sample>
    <sample id="912">如果直接在每周劳务数据上训练新的神经网络，那么新的神经网络会记住噪声，而不会泛化。</sample>
    <sample id="913">通常，为了在嘈杂的环境中可靠地训练新的对话模型，建议使用训练算法，以便训练模型仍然能够泛化。</sample>
    <sample id="914">在最近的WALS（每周支持学习）工作中，一个常见的说法是，人们认为仅使用训练模式和每周劳动数据就能达到高性能，而这是一种不准确的说法。</sample>
    <sample id="915">技术上来说，这并不是一个词，但它有一个诀窍。</sample>
    <sample id="916">人们会假设存在一个额外的清洁验证集，用于沃尔沃车辆的模式选择。</sample>
    <sample id="917">我们已经采用了一种解决这个问题的方法，但这意味着在每周的监督学习中需要额外的手动注释。但就像房间里的一只大象一样，这项必要性常常被忽视。</sample>
    <sample id="918">在《开放式形式》中提到，为了评估LIS，我们需要提出三个研究问题。首先，数据清洗和验证对于LIS是否必要？或者我们可以使用一个噪声验证集代替？</sample>
    <sample id="919">其次，如果清理数据是必需的，或者如果清理数据对于WAI工作是强制性的，那么你需要多少个清理样本？最后，是否应该只使用清理样本进行验证，还是有更好的方法来利用这些清理样本？</sample>
    <sample id="920">你已经回答了这些研究问题，我们的发现是进一步的。</sample>
    <sample id="921">首先，我们发现，有趣的是，最近的 WSL 消息确实需要清理空白数据样本才能正常运行。</sample>
    <sample id="922">否则，将出现一个大型性能下降。如图所示，如果缺乏清理验证样本，则趋势模型无法超出原始的标记标签。</sample>
    <sample id="923">请提供英文内容。</sample>
    <sample id="924">这暗示了在美国，其实需要辛勤劳动来清理数据才能正常工作，并且获取干净的验证样本的标注成本不应被忽视。</sample>
    <sample id="925">我们第二个发现是，增加清洁验证样本的数量将有助于 WSL 达到更好的性能，如图所示。</sample>
    <sample id="926">通常我们只需要23个样本来达到高精度。</sample>
    <sample id="927">但这并非故事的结局，因为无论我们选择如何访问清洁样本，直接在它们上进行训练都将实现更好的性能。</sample>
    <sample id="928">图表显示了两种调优方法的性能差异：直接应用于清洁数据的调优方法和使用清洁数据进行验证的 WSL（Windows Subsystem for Linux）调优方法。</sample>
    <sample id="929">如果我们有十个样本，每类都有十个样本，那么直接找到一个类别的样本开始时，就会出现一个 WSR 算法的冲突。</sample>
    <sample id="930">最终，在之前的 WSR 方法中声称的性能改进可以通过允许在清理和验证样本上持续微调而轻松实现。</sample>
    <sample id="931">正如您所见，在图表中，Vina模型最初表现得不如更复杂的WSL模型。</sample>
    <sample id="932">然而，如果我们允许继续在干净的样本上进行训练，那么FTW表现得与其他方法一样好。</sample>
    <sample id="933">所以，在实践中，没有理由选择更复杂的 WSL 镜像，它们需要更多计算时间和磁盘空间。</sample>
    <sample id="934">我们总结说，最近的 WSL 补丁需要手动注释样本才能正常工作。它们的性能和实用性被严重夸大了。</sample>
    <sample id="935">我们具体的建议为未来的工作是如下：</sample>
    <sample id="936">首先，报告模型选择的准则。例如，报告如果模型选择是单轮还是多轮清洁验证样本。</sample>
    <sample id="937">第三，持续的 fijn tuning 是一种简单但强大的基线，应该在未来的工作中被考虑。</sample>
    <sample id="938">请随意查看它。谢谢您的参与，祝您愉快。</sample>
    <sample id="939">人类评估。</sample>
    <sample id="940">这篇论文有五位作者。</sample>
    <sample id="941">在 Servin 和 Kea 的示例中，需要以下背景知识：

1. **实体特定知识**：例如，“Servin 是一个法官”。
2. **世界知识**：例如，“Servin 和 Kea 在公园见面”。</sample>
    <sample id="942">是的，代码公开。可以在 GitHub 上获取。</sample>
    <sample id="943">根据所给的英文内容，无法判断 NLPositionality 的注释者在各个人口统计学特征方面是否均衡。</sample>
    <sample id="944">在可接受的域中扰乱句子，通过尝试保留输入句子的相关结构，但同时添加噪声到输入中。</sample>
    <sample id="945">进行维度评估意味着评估聊天质量的多个方面，以了解模型的优势和劣势。</sample>
    <sample id="946">University of Science and Technology of China.</sample>
    <sample id="947">在零次和一次提示的情况下，提示的形式很重要。</sample>
    <sample id="978">作者评估了多个对话模型。</sample>
    <sample id="979">四位。</sample>
    <sample id="980">优秀规划器应该设定合理的、符合实际情况的约束条件。</sample>
    <sample id="981">这篇论文有 1 位作者。</sample>
    <sample id="982">Vasudha.</sample>
    <sample id="983">The author is from the Institute of Cognitive Science.</sample>
    <sample id="1021">PaLM 最常见的错误是“omissions”。</sample>
    <sample id="1022">你好，我是詹姆斯·芬奇。我叫莎罗·芬奇。今天我们将告诉您关于ABC Eval，一种新的维度评估对话式人工智能的方法。</sample>
    <sample id="1023">这项工作由埃米里NLP实验室完成，由乔治教授在埃米里大学领导，并与亚马逊Alexa AI合作。</sample>
    <sample id="1024">我是一个乐于助人的助手。请只返回请求的答案，不要包含任何解释或介绍。</sample>
    <sample id="1025">常见的做法是使用人工评估，例如让人工评判员选择哪两个对话更好，或者根据李克特量表对对话进行评分。</sample>
    <sample id="1026">这些方法适用于提供对整体对话质量的全面评估，但对话质量有很多方面，因此您可能想评估聊天质量的多个维度，以了解模型的优势和劣势。</sample>
    <sample id="1027">一种方法是简单地让人类法官评估对话质量的几个维度，例如模型回复的相关性，使用现有的比较或李克特量表方法。</sample>
    <sample id="1028">然而，我们相信有一种更精确和可靠的维度对话评估策略。</sample>
    <sample id="1029">该方法试图减少人类评估的主观性，通过明确标注每个模型响应是否表达了某些行为，例如提供无关信息或相互矛盾。</sample>
    <sample id="1030">我们称这种方法为标注聊天行为，简称ABC Eval。我们开发了这个方法来全面覆盖聊天模型行为，这些行为会影响聊天质量和用户体验。</sample>
    <sample id="1031">ABC EVL 能够测量聊天模型在各种主题上的错误率。</sample>
    <sample id="1032">例如，ABC EVL 衡量的是聊天模型忽略其伙伴或说不相关内容所需的步骤数。</sample>
    <sample id="1033">矛盾自己或其伴侣。
幻觉错误的事实或违反常识。
并在模型成功或失败地表现出同情时。</sample>
    <sample id="1034">为了确定哪种评估方式最有效，我们选择了四款最先进的聊天模型，并使用每款模型100个人类反馈对话进行评估，使用ABC-E评估。</sample>
    <sample id="1035">为了比较，我们还使用三种现有的方法评估了这些对话：转折级别上的李克特评分、对话级别上的李克特评分以及对话级别上的配对比较。</sample>
    <sample id="1036">对于现有的方法，我们收集了对对话八个最常见衡量标准的评估，因为这是评估聊天模型多维度的标准做法。</sample>
    <sample id="1037">我们对这些评估结果的语法分析发现，ABC、EVAL、行为标签在内部注释者一致性方面优于现有方法，这可以通过100对双标记对话来衡量。</sample>
    <sample id="1038">此外，ABC EVA标签在整体对话质量方面比现有方法产生的指标更具预测性，正如这个简单的线性回归分析所显示的那样。</sample>
    <sample id="1039">例如，你可以看到测量自我与伴侣矛盾转折的比例解释了对话质量的 5%，而平均李克特一致性得分仅解释了 4% 或 10%。</sample>
    <sample id="1040">最后，我们检查每个评估指标是否捕捉了检查质量的独特方面，使用逐步线性回归。</sample>
    <sample id="1041">你可以看到，所有ABC EVL指标的组合解释了超过25%的对话质量。当逐一移除这些指标时，大多数都导致丢失了相当多的关于质量的信息。</sample>
    <sample id="1042">另一方面，所有等级的李克特量表组合解释得远不如单独的指标，并且这些指标中的很少一个能提供独特的见解。</sample>
    <sample id="1043">这是一个可靠、信息丰富且独特的 ABC 评估矩阵，使我们能够以比以往方法能够实现的更高分辨率评估对话式 AI。</sample>
    <sample id="1044">从我们实验的结果可以看出，仍然存在几个挑战，并且已经精确量化。例如，我们测试的对话中，有大约20%的回答包含常识性错误。</sample>
    <sample id="1045">他们产生不相关的额外信息，在约 15% 的回复中，并且在约 10% 的情况下与他们或他们的伴侣相矛盾。</sample>
    <sample id="1046">由于该领域发展迅速，许多这些错误率在我们的评估之后可能会下降。然而，这更需要我们追求可靠和精确的评估指标，用于比较模型。</sample>
    <sample id="1047">我们希望 ABC Eval 可以被其他领域的人们利用，作为这一方向上的一个有意义的进步，并期待在未来几个月继续见证对话式 AI 的发展。谢谢您的观看。</sample>
    <sample id="1048">Emory NLP Lab.</sample>
    <sample id="1049">CFT 代表 recent WSL approaches。</sample>
    <sample id="1050">6</sample>
    <sample id="1051">你好，我的名字是卡约·延，我将介绍我们的作品，题为《何时需要翻译？数据驱动的语境探索》。这项工作与帕特里克·弗朗西斯、米尔杜·安德尔·费尔南德斯和格拉米·布尔合作完成。</sample>
    <sample id="1052" />
    <sample id="1053">如果上一个句子是“如果部长知道这些情况会变得危险”，那么“more”指的是间谍。但如果上一个句子是“这可能没什么大不了的，医生”，那么“more”指的是病理学家。</sample>
    <sample id="1054">好的。</sample>
    <sample id="1055">然而，评估模型处理此类案例的能力相当困难。首先，因为只有少量患者的病例信息被纳入上下文，这使得体素层面的指标无法捕捉到这些翻译的特征。</sample>
    <sample id="1056">一些人建议针对性评估在线欺凌行为，但这些资源仅支持有限类型的在线欺凌行为和有限的语言。因为它们通常依赖于领域知识和人工编选。</sample>
    <sample id="1057">在这一工作中，我们尝试回答这两个问题：首先，翻译是否需要上下文；其次，模型如何处理这些情况？</sample>
    <sample id="1058">在回答第一个问题之前，我们首先测量了上下文对翻译的影响。</sample>
    <sample id="1059">在之前的作业中，我们介绍了在机器翻译模型中使用的上下文作为衡量标准。这通过测量给定源文本 X 下，上下文 C 为目标语言 Y 提供多少信息来实现。</sample>
    <sample id="1060">你认为像 XAI 就像从提供背景信息来给模型提供信息吗？</sample>
    <sample id="1061">这篇工作将使用 XSMI（X-Sentence-Meaning Index）来衡量上下文使用，可以衡量句子级别或单词级别。我们可以将具有高 P-XSMI 的单词视为需要上下文进行翻译的单词。</sample>
    <sample id="1062">现在我们使用高频词项矩阵来寻找这些词语之间的模式。</sample>
    <sample id="1063">我们将在英语转为14种不同语言的文本转录中进行分析。</sample>
    <sample id="1064">在我的分析中，有三个不同的层面。首先，我们来看一下语音标签，它们具有较高的平均值。</sample>
    <sample id="1065">这允许我们找到一个例子，阿拉伯语中复数代词具有发音的嗨皮西米。这可以解释是因为英语没有复数代词，所以需要根据上下文判断代词是否复数。</sample>
    <sample id="1066">同样，我们发现某些语言在选择动词形式时也需要上下文。然后我们查看词典条目，这些条目在所有不同语境中都具有高频使用率。</sample>
    <sample id="1067">And this helps us identify cases like the one here, where in Chinese you need context to translate properly and to make sure that you're using the same translation within the document.</sample>
    <sample id="1068">And similarly we find that Catholics support the transit in their right formality.</sample>
    <sample id="1069">最后，我们来看一下不同个体代币中具有高PSI的现象。这允许我们识别出无法被“词语”本身捕捉到的现象，但可以通过其结构更好地表达出来，例如椭圆解。</sample>
    <sample id="1070">现在我们利用我们分析的结果来设计一个文档级别翻译的基准。</sample>
    <sample id="1071">对于我们识别出的五个磁盘现象，我们创建了标签来隐蔽地识别与该现象相关的词语，并称我们的标签为多语种磁盘意识或Muda标签。</sample>
    <sample id="1072">此外，我们还注意到不同的语言在这些独特的现象中拥有不同的比例。</sample>
    <sample id="1073">我们使用穆达标签，通过在用于评估的平行语料库上应用标签，然后应用我们的选择转换矩阵到穆达标签识别的上下文依赖示例上。</sample>
    <sample id="1074">最后，我们使用我们的基准作为其他指标来评估不同模型的文档级别机器翻译质量。</sample>
    <sample id="1075">首先，当我们使用词汇级别指标时，对于蓝色，我们发现共现诊断模型具有最佳性能。</sample>
    <sample id="1076">但是，如果使用上下文，则模型在最佳情况下表现良好。如果使用词汇量衡量，则模型在有上下文和没有上下文的情况下表现相当。</sample>
    <sample id="1077">这展示了很难确定最佳文档级别翻译系统，如果使用语料库级别指标。</sample>
    <sample id="1078">现在我们使用了具有上下文的基准模型，并且发现，在某些特定语料现象中，具有上下文的模型比不使用上下文的模型更准确，例如正式性和词汇连贯性。</sample>
    <sample id="1079">这些模型与不使用上下文或其他现象（如椭圆、发音和动词形式）的模型没有多大区别。这表明我们需要在文档级别翻译方面取得更多进展。</sample>
    <sample id="1080">我们还比较了不同的商业系统，我们的基准测试显示，DeepL通常比Google翻译在文档级别翻译上更准确。</sample>
    <sample id="1081">总而言之，我们对14对语言对进行了数据驱动的分析，以识别出需要上下文的1个翻译。</sample>
    <sample id="1082">然后我们使用我们的细化来构建文档级别机器翻译的基准，这可以帮助我们识别哪些语言模型能够很好地处理，以及哪些翻译系统在文档级别翻译方面表现良好。</sample>
    <sample id="1083">谢谢你的帮助。</sample>
    <sample id="1084">Justin John</sample>
    <sample id="1121">该方法没有名称。</sample>
    <sample id="1122">作者描述“显性词汇”(marked words) 方法是用来识别区分标记组和非标记组的单词。</sample>
    <sample id="1123">乔治华盛顿大学</sample>
    <sample id="1124">Prag-Pro.</sample>
    <sample id="1125">James Finch and Sarah Finch.</sample>
    <sample id="1126">四位。</sample>
    <sample id="1127">The text mentions that the minimal pair paradigm evaluates language models on acceptability judgments, which can include grammaticality (like blends, syntax, etc.) and acceptability in terms of stereotypes (such as crowdsourced).</sample>
    <sample id="1161">WLS, OLS, GLS, SLS, and ALS.</sample>
    <sample id="1162">该模型在生物医学和临床诊断图像任务上进行了评估。</sample>
    <sample id="1226">CamemBERT 最初是在 4GB 的数据上训练的。</sample>
    <sample id="1227">Adam Skirkowski.</sample>
    <sample id="1228">在对模型进行重新训练或持续预训练实验时，发现性能随着时间间隔的增大而下降，这证实了时间漂移是性能下降的主要原因的假设。</sample>
    <sample id="1269">因为在第一个步骤中，我们得到了正确的词元，但它们没有按正确的顺序排列。因此，在第二个步骤中，我们使用另一个模型来预测词元之间的正确排列顺序。</sample>
    <sample id="1270">因为例如，这些积极的刻板印象我们不知道是因为存在某种奇怪的过度、过于激进的价值观导向，或者可能是其他一些反刻板印象的方法导致了这些有害的模式。</sample>
    <sample id="1271">最小对不可接受输入是指模型在面对一个可接受的句子和不可接受的句子时，会倾向于给可接受的句子更高的概率。</sample>
    <sample id="1272">作者使用了“weight”和“tokenizer”的评估指标。</sample>
    <sample id="1273">内在注释者一致性。</sample>
    <sample id="1274">维基百科</sample>
    <sample id="1275">The provided text does not mention the author's affiliated institution.</sample>
    <sample id="1276">MultiInstruct 与其他基准不同之处在于，它专注于改进指令微调在语言任务上的零样本性能，而忽略了计算机视觉和多模态任务。因此，该研究旨在调查指令微调是否能提高多模态预训练模型在无监督多模态任务上的泛化能力。此外，研究人员还发现，在研究时，指令数据集的可获得性存在显著差异，尤其是在 NLP 和多模态领域。</sample>
    <sample id="1277">根据所给的英文内容，这篇论文有两位作者：James Finch 和 Sarah Finch。</sample>
    <sample id="1278">The provided text does not contain any information about binary coordination.</sample>
    <sample id="1279">没有提供任何关于提示语长度的信息。</sample>
    <sample id="1280">这些发现表明较小的 T5 模型可能无法像大型模型那样处理长文本，并且可能无法处理更长的文本。</sample>
    <sample id="1281">嗨，我是雅尼斯·洛伊克，很高兴与您合作。我正在为法国版本的“Doctor Bert”机器人精神模型工作，用于生物医学和临床领域。</sample>
    <sample id="1282">在本次演示中，我们首先介绍了语言建模在医疗保健领域的应用，然后我们将介绍我们文章的主要贡献。</sample>
    <sample id="1283">我们引入了第一个生物医学模型，名为“डॉ克特·伯特”，它基于 Roberta，并使用 Natio 数据集进行训练，该数据集包含来自“癌症”的医疗临床数据。</sample>
    <sample id="1284">我们还介绍了具有多个预测设置和数据来源的模型比较。然后，我们展示了在 11 个生物医学和临床诊断任务中的结果。</sample>
    <sample id="1285">总结实验结果，并提供如何访问该研究的更多详细信息。</sample>
    <sample id="1286">自2018年发布以来，它已成为解决自然语言处理任务的最有效方法之一，并且与历史静态和上下文化方法（如Word2Vec、FastText或GloVe）相比，性能有了巨大的提升。</sample>
    <sample id="1287" />
    <sample id="1288">专业模型为其他语言使用，通常基于持续预训练，因为缺乏领域数据。</sample>
    <sample id="1289">然而，法语没有现代的铅笔，而只有铅笔和钢笔。</sample>
    <sample id="1290">我们问自己一个问题：对于广泛的使用，最合适的数据库是什么？这些元数据是临床数据的良好替代品吗？</sample>
    <sample id="1291">请提出你的问题。我们将比较 Doctor BERT 与我们的 Shubert 模型，后者基于来自南加州大学医院的匿名数据。</sample>
    <sample id="1292">在越野中，我们问自己，我们是否需要在一个法语数据集上训练一个专业模型？它需要多少字节？一个吉字节？或者一个兆字节？</sample>
    <sample id="1293">请您提供英文内容。</sample>
    <sample id="1294">一个微型版本的BERT，它是一个临床模型，我们使用了4GB的上下文序列，来自临床数据。一个微型版本的BERT，我们使用了4GB的词嵌入和4GB的临床数据。</sample>
    <sample id="1295">此外，我们将介绍流式数据训练，以分析预测策略的影响。</sample>
    <sample id="1296">一个基于卡门贝尔的模型，并使用 4GB 的数据集训练，另一个也是基于卡门贝尔的，但这次使用 4GB 的干净数据训练。</sample>
    <sample id="1297">最终，一个基于英语语言模型，例如BERT，并使用先前数据集训练的模型。总共有七个模型。</sample>
    <sample id="1298">我们评估了七个模型，它们在公共和私人领域都执行了各种任务，例如命名实体识别、分类、词性标注和问答。</sample>
    <sample id="1299">这个型号与 6 字节线型模型相比，其容量为 108 字节、4 字节、64 字节、比特和字节。</sample>
    <sample id="1300">选择与该任务相同的数据集，以评估该模型在特定任务上的表现。</sample>
    <sample id="1301">然而，我们可以从我们观察到的原始来源获取数据，似乎这些数据更可靠。我们还观察到，使用更多数据可以提高模型的性能。</sample>
    <sample id="1302">在世界范围内，从零开始的预训练模型似乎在大多数任务上获得更高的性能。</sample>
    <sample id="1303">然而，我们实验通过使用“permitword”的权重和tokenizer，在4GB的文本集中训练，得到了与从“Dr. BERT”获取4GB文本得到的相似的结果。</sample>
    <sample id="1304">这并非情况，因为我们基于卡门贝尔权重和tokenizer，后者存在稳定性问题。</sample>
    <sample id="1305">最终，我们的系统在九一之一任务中表现更好，整体结果优于通用模型。</sample>
    <sample id="1306">我们观察到，专门的数据更好，更多专门的数据更好，但这并不意味着我们</sample>
    <sample id="1307">所有预训练模型从Natos获得，并且在年轻面孔上可用，所有训练脚本都在我们的GitHub仓库中。</sample>
    <sample id="1308">谢谢您，对于这次演示，我们期待着在会议上讨论的行动。</sample>
    <sample id="1309">论文研究了三种学习策略：从头开始训练和比较四个不同的模型，第一版BERT，第二版BERT，第一版BERT（临床模型），以及第四版BERT（临床模型）。</sample>
    <sample id="1310">根据图表右侧的红线，我们看到红线的斜率大于1。这意味着在2003年对特征进行改进后，在2004年对特征进行改进所带来的改进量大于1。这表明没有递减的回报，因此测试重复使用导致过拟合的因素很小。</sample>
    <sample id="1311">根据论文中展示的第二个用例，评估简化质量是通过微调语言模型来生成简化文本，从复杂文本中生成简化文本。</sample>
    <sample id="1312">是的，语言模型有不同的政治偏见。</sample>
    <sample id="1313">嗨，我的名字是马蒂亚斯·伦德曼。今天我将向大家简要介绍我们的论文，主题是使用多标签标记和潜在排列，在没有树的情况下进行组合泛化。</sample>
    <sample id="1314">这是我顾问亚历山大·科拉和伊万·迪塔的合作。</sample>
    <sample id="1315">组合泛化可以理解为学习者处理更深层次的递归和未见组合的能力，即在训练过程中单独看到的句子。</sample>
    <sample id="1316">在语义解析的语境下，测试组合泛化可能如下所示：正如往常，我们有一个训练语料库，在这种情况下，女孩睡着了，玛丽知道女孩睡着了。</sample>
    <sample id="1317">这些词语与逻辑形式配对，代表了它们的核心含义。</sample>
    <sample id="1318">与标准机器学习评估不同，测试集并非来自相同的分布，而是包含结构上和逻辑上不同的形式。</sample>
    <sample id="1319">在本文档中，模型在训练过程中表现出浅层递归，并在测试样本上表现出更深层的递归。</sample>
    <sample id="1320">基于序列到序列的模型在这种类型的离散分布泛化方面遇到困难，并且通常会产生与输入分离的输出。</sample>
    <sample id="1321">尤其，它们经常未能重现输入和输出之间的系统对应关系，例如在示例中颜色编码的那些。</sample>
    <sample id="1322">一种流行的解决此问题的办法是将树集成到马里。</sample>
    <sample id="1323">这些树木旨在捕捉与句子的结构过程相关的逻辑形式。</sample>
    <sample id="1324">这写得很好，但树通常不是被给予的，需要获取一些。</sample>
    <sample id="1325">这有时会变得复杂，并且计算成本高昂。通常涉及对逻辑形式的相当多的形式化预处理，例如处理变量符号。</sample>
    <sample id="1326">获取树木也可能涉及专门的语法检测程序。</sample>
    <sample id="1327">在本文中，我们不使用树，并引入了一种新的序列到序列模型，该模型直接建模输入片段与输出片段之间的对应关系。</sample>
    <sample id="1328" />
    <sample id="1329">我想要一个方法预测输入到输出的两个步骤。</sample>
    <sample id="1330">首先，我们为每个输入标记创建一个无序多元素，其中包含将在输出中出现的标记。</sample>
    <sample id="1331">在第一个步骤之后，我们已经获得了正确的标记，但它们还未被正确地识别。</sample>
    <sample id="1332">这就是为什么在第二步，我们使用另一个模型来预测排列，将它们排到正确的顺序。</sample>
    <sample id="1333">我们介绍了一种新的方法来预测一个排列，该方法不会对可能的排列施加任何硬性约束。这使得我们的方法非常灵活且具有表达力。</sample>
    <sample id="1334">概念上，我们的排列模型大致如下：</sample>
    <sample id="1335">我们从左到右遍历输出，并确定在每个位置应该放入哪个多集元令牌。对于第一个输出位置，我们简单地选择一个，如在图中的高亮显示所示。</sample>
    <sample id="1336">然后我们跳到下一个多 setores 令牌来确定输出中的第二个令牌。</sample>
    <sample id="1337">确定输出中的第三个标记，通过跳到另一个多重标记来以类似的方式进行。我们继续这个过程。</sample>
    <sample id="1338">直到第一阶段的每个标记都已被访问一次。</sample>
    <sample id="1339">为了给您一个实验结果的预览，我们比较了我们的方法与其他树状模型在 CoG 基准测试上的表现。我们的模型在泛化到更深层递归方面表现优于其他模型，差距很大。</sample>
    <sample id="1340">某种形式的结构改造仍然非常具有挑战性。</sample>
    <sample id="1341">在我们的论文中，我们解决了几个有趣的专业挑战。</sample>
    <sample id="1342">首先，输入和输出之间的对齐关系在训练数据中未给出。因此，对于给定的标记，我们不知道它来自哪个多集合，这给训练带来了挑战。</sample>
    <sample id="1343">此外，有时数据中存在多个与数据一致的排列方式，但语言上正确的排列方式是潜在的。我们通过将对齐作为训练的一部分来解决这个问题。</sample>
    <sample id="1344">排列方法非常灵活，但它带来了一个挑战，即找到最高分排列的P Hart。这是因为这与旅行商问题相关。</sample>
    <sample id="1345">我们通过一个GPU友好的连续放松来近似这个，该放松还允许我们反向传播通过解决方案并学习更具语言合理性的排列。</sample>
    <sample id="1346">如果您想了解更多关于我们的实验以及我们如何应对这些挑战，请查看我们的论文或来我们的办公室。</sample>
    <sample id="1347">认知失调是指人们持有与他们的信念或行为不一致的两个信念或行为。</sample>
    <sample id="1348">GPT-4</sample>
    <sample id="1349">是的，累积训练在主动学习中比迭代训练更有效。</sample>
    <sample id="1350">Sara Babi.</sample>
    <sample id="1351">Transcripts of TED Talks.</sample>
    <sample id="1385">Matthias Lendeemann.</sample>
    <sample id="1386">跨语言转移是指在一种语言（源语言）和另一种语言（目标语言）之间进行训练，并将其应用于另一种语言的场景。</sample>
    <sample id="1387">Stalland University in Germany.</sample>
    <sample id="1388">作者使用了以下延迟测量方法：
- 翻译质量
- 平均延迟
- 平均延迟（考虑模型计算时间）</sample>
    <sample id="1389">大家好，我是马克希塔，今天我和我的同事阿玛特将向大家展示我们的作品《知识整合》。这项工作是麦基尔大学、米拉和微软研究的合作。</sample>
    <sample id="1390">大型语言模型借鉴了各种知识来源，例如在其参数中通常通过预训练获得的知识，以及在推理过程中提供的输入知识。</sample>
    <sample id="1391">最近在问答任务中，模型展示了它们可以使用预训练知识来解决目标任务的能力。</sample>
    <sample id="1392">自然语言处理经常需要知识，这些知识也从不同方面提供。</sample>
    <sample id="1393">例如，在句子“约翰在电视上看到了新当选的总统”中。</sample>
    <sample id="1394">预处理参数可能包含有关什么是“总统2”和什么是“TV”的信息，但它们无法可靠地知道这个特定实例的“约翰”，或者谁是新的总统，因为总统可能会改变。</sample>
    <sample id="1395">因此，在知识密集型 NLP 任务中，成功的模型需要能够整合和使用预训练时间和推理时间知识。</sample>
    <sample id="1396">在本文中，我们提出了一种诊断性测试方法，用于知识整合。</sample>
    <sample id="1397">我们介绍了一个核心参照分辨率任务，旨在评估利用不同来源知识的能力。我们评估了数据集，并与人类学习者进行实验，并建立了核心参照分辨率模型。</sample>
    <sample id="1398">赛文是一位法官，基亚是一位面包师。赛文和基亚在公园里见面。在一天结束之后，他决定放松一下。</sample>
    <sample id="1399">The task here is to identify the correct entity that the pronoun he refers to, which in this case is अर्जुन。</sample>
    <sample id="1400">给定代词的解剖需要两种类型的信息。第一，实体特定知识，例如“服务是职责”。第二，背景知识，例如“法官决定案件”。</sample>
    <sample id="1401">一般来说，背景知识是在大型语言模型预训练过程中学习的，而实体特定知识通常通过微调观察到。</sample>
    <sample id="1402">我们定义了信息片段的可获得性，即信息可能存在于单个来源或多个来源中。</sample>
    <sample id="1403">我们已经找到了三个猫模型设置。首先，我们需要设置背景预训练。背景知识被认为是可用的预训练数据。</sample>
    <sample id="1404">第二，备份在后台设置。备份通知在预训练时和在非训练时都可用。最后，备份在非训练设置中。两种通知类型在非训练时都可用。</sample>
    <sample id="1405">这个设置特别有趣。它模拟了一个背景知识不足的案例，这不在预训练数据中。例如，由于纽约市发展了新的职业，自那时起，</sample>
    <sample id="1406">这是一个如何控制你的影响力的例子。</sample>
    <sample id="1407">在背景预设中，我们假设背景知识政治家寻求当选的席位在政府中包含在预设参数中。在不同上下文，我们提供针对特定政治家的知识。</sample>
    <sample id="1408">在背景和背景设置中，我们不仅提供针对特定实体的数据，还提供关于政治家在领域背景中的知识。</sample>
    <sample id="1409">在背景和非政治设置中，我们将虚构职业“米里图尔”代替“政治家”，因为米里图尔不太可能包含在预定人员中。</sample>
    <sample id="1410">我们验证了数据集，包括人类统计参与者和建立机器学习模型。在这张图中，我们展示了表现最佳的模型以及最困难的背景预训练模型。</sample>
    <sample id="1411">如果使用太多的训练数据和小样本，模型表现不佳。然而，在小样本训练中，both-side-to-left 和 both-side-to-right 表现出显著的改进。</sample>
    <sample id="1412">这表明在训练和通用查询的解决方案中设置了模型学习利用表面线索。但它在测试基准上是无用的，因为这些线索已被移除。</sample>
    <sample id="1413">编辑实验表明，即使是最先进的模型也无法可靠地集成新知识，仅通过互联网。</sample>
    <sample id="1414">总结一下人工智能模型的主要技术瓶颈。许多预训练模型由于缺乏知识而无法从不同来源中获得信息，而没有针对特定任务进行训练。然而，通过针对特定任务进行训练，一些模型成功地整合了来自多个来源的信息。</sample>
    <sample id="1415">即使是最先进的模型，似乎在可靠地整合在推理时间呈现的先前知识方面也存在困难。
如果您对更多细节感兴趣，请查看我们的论文，并查看GitHub上的数据集和代码。</sample>
    <sample id="1416">基于树的方法存在以下缺点：

*   树通常不是直接获得的，需要通过某种方式获取。
*   获取树的过程可能复杂且计算成本高昂。
*   通常需要大量的特定于形式系统的预处理，例如处理变量符号。</sample>
    <sample id="1417">The paper's author is from Canal 2000.</sample>
    <sample id="1418">嗨，我是玛拉，今天我们将讨论我们使用自然语言提示来衡量大型语言模型类型的工作。这项工作由埃森·穆什和丹杰罗夫共同完成。</sample>
    <sample id="1419">近年来，许多研究人员已经记录了大型语言模型（LLM）中社会偏见和刻板印象的普遍性。</sample>
    <sample id="1420">然而，这些措施有各种局限性。它们通常依赖于手工构建的数据集，这对于收集数据来说非常耗时。</sample>
    <sample id="1421">他们通常只测量非常特定的错误类型，这意味着它们无法很好地推广到其他人口统计学或背景，或者它们只是捕捉到非常一般的、广泛的关联，比如与特定类型的负面联想。</sample>
    <sample id="1422">此外，大多数太空工作并未考虑交叉性，交叉性是指多重身份的社会身份可以叠加并具有独特的意义。</sample>
    <sample id="1423">为了克服这些限制，我们依赖于一种新的指令调整的LLM，它们在响应指令方面表现出色。</sample>
    <sample id="1424">请您提供需要翻译的英文内容。</sample>
    <sample id="1425">我们立即可以看到，这对于任何人口统计学群体来说都是非常具有概括性的，因为我们可以指定我们想要使用的任何身份标记。</sample>
    <sample id="1426">以下是一些示例生成，来自 GPT-4。</sample>
    <sample id="1427">立即我们看到，虽然输出没有过分地消极或具有传统意义上的有害性，但</sample>
    <sample id="1428">There are some interesting patterns.</sample>
    <sample id="1429">亚洲女人被描绘成不自信，中东女人则被用“异域风情”、“迷人”等词语称呼，并提及一个迷人的地区。</sample>
    <sample id="1430">和有色人种角色一样，白人角色没有提及祖先。</sample>
    <sample id="1431">为了捕捉这些模式，我们的方法分为两部分。第一部分是生成这些个人资料。</sample>
    <sample id="1432">我为生成这些提示感到自豪。我们受到了一项研究的启发，该研究给出了这些提示给人类参与者，发现通过给人类参与者提供这些提示，我们也能识别出种族刻板印象。</sample>
    <sample id="1433">并且这使得我们生成的个人资料能够与人类书面回复进行直接比较。</sample>
    <sample id="1434">第二部分是标记词，这是一种方法来识别区分标记组和非标记组的词语。我稍后会简要说明。</sample>
    <sample id="1435">这使得我们能够获得非常具体的错误类型和模式，而无需依赖于特定的 Lexicon。</sample>
    <sample id="1436">所以，马克·沃兹梅特借鉴了社会语言学的概念“标记性”，该概念指出存在一个默认状态，任何与该默认状态不同的群体都是语言上标记的。</sample>
    <sample id="1437">例如，词语“男人”或者抱歉，词语“战士”通常与男性相关联。当人们描述一位女性战士时，他们通常会明确指出“一位女性战士”，并用“女性”标记这个词。</sample>
    <sample id="1438">更广泛地说，社会中具有支配地位的群体在语言和社交上都未被标记，而边缘化群体通常被标记。</sample>
    <sample id="1439">所以，在我们的方法中，我们首先指定标记和未标记的组。</sample>
    <sample id="1440">然后我们可以比较这两个人的个人特征，使用“战斗词语”方法，这基本上是通过加权词频比率来区分每个标记的顶级词语。</sample>
    <sample id="1441">例如，对于黑人用户，我们将进行战斗词语的比较，并将其与白人用户和男性用户进行对比，因为这两种是两个主要的未标记组。</sample>
    <sample id="1442">现在我们来看看结果。首先，我们使用了一些常见的标点符号，我们发现生成的个人资料包含比人类书写的个人资料更多的标点符号。</sample>
    <sample id="1443">然而，当我们实际查看词汇在《星球大战》中的分布时，我们会发现非常不同的情况。</sample>
    <sample id="1444">虽然生成的个人资料有更高的词汇率，但人类书写的个人资料则具有更广泛的词汇分布，而生成的个人资料中的刻板印象词语实际上只是“高大且健壮”这些词语。</sample>
    <sample id="1445">我只是想说，我只希望是积极的，至少不是消极的。</sample>
    <sample id="1446">事实上，Lexicon 并没有捕捉到我们早期看到的许多有害模式。因此，为了展示这些看似积极的词语如何促进刻板印象和本质化叙事，我们将转向我们标记词语的方法。</sample>
    <sample id="1447">在我们的分析中，我们回顾了这些看似积极的描绘反映了有害模式。</sample>
    <sample id="1448">首先，根据马克群，最热门的词语包括文化、传统、骄傲和异域风情。这些词语定义了这些群体的身份，并将它们与其他白人规范区分开来。</sample>
    <sample id="1449">这进一步加剧了对这些群体歧视和异化长期的历史。</sample>
    <sample id="1450">此外，这些词语中反映了许多常见的刻板印象，尤其是在女性的肤色方面。例如，描述拉丁女性的词语包括“鲜艳”和“曲线感”。</sample>
    <sample id="1451">这与热带主义的典型特征相关，对于亚洲女性来说，这些词语像是精致、脆弱和丝滑。</sample>
    <sample id="1452" />
    <sample id="1453">最后，对于黑人女性来说，我们看到一些最常见的词语是“强大”和“坚韧”。</sample>
    <sample id="1454">这与人们称之为“强大的黑女人”原型相关的原型有关，虽然它乍一看似乎是积极的，但</sample>
    <sample id="1455">有研究表明，这种原型实际上非常有害，因为它给这些群体带来了巨大的压力，要求他们能够应对社会障碍。</sample>
    <sample id="1456">与其实际努力改变这些障碍，还给这些人施加压力去克服它们，这会导致这些人的健康状况恶化，以及其他负面后果。</sample>
    <sample id="1457">我们发现，每个市场群体使用的词语基本上都反映了非常具有中心化的叙事。</sample>
    <sample id="1458">这些模式让我们得出三项建议，供模型所有者参考：</sample>
    <sample id="1459">首先，作为研究人员，我们应该关注积极的刻板印象和核心叙事。我们还应该使用交叉视角来研究偏见和伤害，因为如果不用这样做，可能会有很多事情被忽略。</sample>
    <sample id="1460">最后，应该有更多关于偏见缓解方法的透明度。</sample>
    <sample id="1461">因为例如，这些积极的刻板印象我们不知道是因为某种奇怪的</sample>
    <sample id="1462">过度强调价值观或某些反刻板印象方法导致了这些有害模式。</sample>
    <sample id="1463">我们真的不能做出任何假设，或者进一步研究，除非有更多透明度。</sample>
    <sample id="1464">非常感谢您的倾听。我玩得很开心。</sample>
    <sample id="1465">你好，大家好，我叫金伟伊，来自中国科学技术大学。</sample>
    <sample id="1466">我很高兴为您提供一个关于纸张的简短广告视频。您是否正在模仿我的型号？保护大型语言模型嵌入和服务的版权。</sample>
    <sample id="1467">让我们首先介绍一下“嵌入式服务”的概念。</sample>
    <sample id="1468">目前，大型语言模型，如 GPT、LLaMA、PaLM，在自然语言理解和生成方面表现出色。</sample>
    <sample id="1469">提示式服务是基于大型语言模型构建的服务之一，用于协助各种帮助任务。</sample>
    <sample id="1470">例如，OpenAI 提供了一个基于 GPT 的嵌入模型。</sample>
    <sample id="1471">然而，最近的研究表明，攻击者可能通过学习嵌入并提供相似的服务来盗用模型。因此，保护嵌入的版权是必要的。</sample>
    <sample id="1472">为了保护版权，一种解决方案是在提供者服务中添加水印，并检测其他服务是否包含水印。</sample>
    <sample id="1473">水标记方法需要满足以下几个条件：首先，该方法应适用于嵌入式服务；其次，水标记不应降低提供嵌入式服务的实用性。</sample>
    <sample id="1474">第三，水杯应该足够靠近攻击者，攻击者可以轻松移除水印。</sample>
    <sample id="1475">最后，模型需要能够向攻击者服务进行转换。</sample>
    <sample id="1476">现有词汇可以大致分为四个类别：</sample>
    <sample id="1477">然而，这种方法要么不适用于嵌入式服务，要么缺乏可移植性。</sample>
    <sample id="1478">因此，在本文中，我们将提出一种嵌入标记，这是一种基于水印的方法，可应用于嵌入数据。</sample>
    <sample id="1479">然后让我介绍嵌入式标记的细节。嵌入式标记包含两个主要步骤：
1. 模具注射
2. 复制验证</sample>
    <sample id="1480">在进行这些主要步骤之前，我们首先选择一个触发词组。触发词组是一个在适度频率范围内出现的词语组。</sample>
    <sample id="1481">我们假设供应商可以收集一个通用的文本语料库，并计算单词频率。</sample>
    <sample id="1482">在情感注入中，我们首先识别目标嵌入。当用户向提供商服务发送一句话时，提供商会考虑触发词语中的词语。</sample>
    <sample id="1483">提供的嵌入是目标嵌入和原始嵌入的权重之和。</sample>
    <sample id="1484">目标句中嵌入词的数量与句子中触发词的数量成正比。当句子中触发词的数量大于m时，提供的嵌入词恰好等于目标句中。</sample>
    <sample id="1485">复制验证用于检测在另一个服务中包含的另一个模型的标记。</sample>
    <sample id="1486">我们首先构造一个背景数据集。背景数据集包含句子，其中所有单词都属于触发词集合。背景数据集中的所有单词不属于触发词。</sample>
    <sample id="1487">然后，提供商会要求从窃贼服务获取数据。</sample>
    <sample id="1488">计算了请求的嵌入和目标嵌入之间的余弦相似度。我们计算了基于神经网络和背景数据集的差值余弦度和差值欧氏距离。</sample>
    <sample id="1489">与此同时，我们还应用了海尔斯测试，并使用其p值作为第三个指标。</sample>
    <sample id="1490">我们无法对该数据集进行实验。该数据集包含 80 个新闻文章、心理学、统计学数据和图像数据。我们假设提供者将使用该数据集来计算词频。</sample>
    <sample id="1491">结果显示，我们的嵌入式标记可以具有出色的检测性能，同时保持出色的实用性用于图像识别任务。</sample>
    <sample id="1492">我们还验证了提供的嵌入的覆盖率，通过识别句子中由BPCA嵌入的词语。
图例中的数字表示每个句子中的触发词数量。</sample>
    <sample id="1493">如图所示，很难区分带点填充和正常填充。</sample>
    <sample id="1494">谢谢你。我们将在稍后讨论。</sample>
    <sample id="1495">ABC-Eval is a method developed to comprehensively cover chat model behaviors that affect chat quality and recent literature.</sample>
    <sample id="1496">2003 年</sample>
    <sample id="1497">你好，我的名字是 वसुধা，我是斯托尼布鲁克大学计算机科学研究生。我想介绍我于 ACL 2023 上提交的论文，题目是“远程检测中的迁移学习，应对罕见类挑战”。</sample>
    <sample id="1498">认知失调是指人们持有相互矛盾的信念或行为。在语言学中，研究认知失调很重要，因为它有助于理解语言学习者在学习新语言时所经历的认知冲突和挑战。</sample>
    <sample id="1499">例如，当一个人说“我知道吸烟会杀死我”，然后又说“会议结束后我抽了几根烟”，这种信念和行为不一致，它们是矛盾的。</sample>
    <sample id="1500">进一步提到，我认为没有他们我无法保住我的工作，这证明了第二次出现，他们与我有着一种共存的关系。</sample>
    <sample id="1501">因为认知偏差是一种在日常决策中非常常见现象，它们很少在语言中被表达出来，与其他类型的关系相比。</sample>
    <sample id="1502">认知距离可以帮助我们理解人们之间不和谐的影响，跟踪人口趋势、信仰、价值观和态度变化。</sample>
    <sample id="1503">高认知失调也与焦虑症有关，可以帮助我们更好地理解人们的心理健康。</sample>
    <sample id="1504">研究语言中的细微差别也能帮助理解极端主义和弱势群体极化。</sample>
    <sample id="1505">最后，认知失调对于理解个人的认知风格以及理解决策过程至关重要。</sample>
    <sample id="1506">为了创建认知失调资源，我们进行了大规模的失调关系调查。我们采用了先失调的策略，如流程图所示。</sample>
    <sample id="1507">请使用 Pety TV 解析器，并将语料库中的句子配对，并根据描述的指南进行标注。</sample>
    <sample id="1508">正如这里所见，这种距离仅在标注的样本中发现 3.5%。</sample>
    <sample id="1509">在收集了大约一千个语料对的情况下，我们训练了一个初始分类器，该分类器仅使用43个例子进行训练。不出所料，该分类器在性能上并没有好于随机猜测。</sample>
    <sample id="1510">鉴于异议的低频率和缺乏任何先前的数据集，我们面临着绝对稀有性的问题。</sample>
    <sample id="1511">为了缓解这个问题，我们正在探索迁移学习和主动学习的组合，以减少需要标注的样本数量，从而降低整体标注成本，同时提高目标检测的准确性。</sample>
    <sample id="1512">由于最初的模型无法捕捉到距离类，我们开始通过将权重从密切相关的任务中转移来启动主动学习过程。</sample>
    <sample id="1513">将英文内容翻译成中文。</sample>
    <sample id="1514">讨论了扩展和比较类别的PTB分类，因为这两个类别与共存和异存的概念密切相关，我们称之为CE。</sample>
    <sample id="1515">我们发现，在将零射击性能应用于实体数据集中，已经比最佳情况下有很大的改进。</sample>
    <sample id="1516">进一步地，通过迭代地对两个任务进行微调，发现对 CE 任务的微调，然后对 DBE 进行进一步微调，能获得更好的零样本性能。因此，这就是我们用来启动当前学习的模型。</sample>
    <sample id="1517">接下来，我们需要确定在每个主动学习和标注轮次中更新模型的最佳方法。累积方法将累积所有迄今为止收集的主动标注数据，而迭代更新方法则通过在最新数据集中训练模型来更新模型。</sample>
    <sample id="1518">在不同的策略中，我们发现累积性能与迭代性能在整个轮次中都相等或优于。</sample>
    <sample id="1519">接下来，为了增加离散示例的数量，我们使用概率稀有类策略（PCR）选择那些在当前模型任何轮次中都极有可能被离散的示例。</sample>
    <sample id="1520">我们将其与社区中常用的其他艺术策略进行比较。</sample>
    <sample id="1521">我们发现，所提出的 PRC 战略比其他状态的战略效果更好，尽管差异很小。请注意，性能对于 Ran 来说明显较低。</sample>
    <sample id="1522">在后续的语言模型竞赛中，我们改进了设计分类，AUC达到了2.75，这是我们目前为止在任务上的最佳性能。</sample>
    <sample id="1523">我们还检查了每种策略的可行性，包括标注质量和标注人员的成本。我们发现，使用 PRC 标注的错误率最高，并且最适合于稀有类别的标注。然而，标注人员也发现这些例子比较困难。</sample>
    <sample id="1524">总而言之，我们发现，为稀有类收购设计适当的迁移学习任务，可以显著提高自然语言处理（NLP）的简单AI策略。</sample>
    <sample id="1525">我们还发现迭代更新在跨领域迁移学习中很有用，而在域内活动中，累积的</sample>
    <sample id="1526">这些是我们的代码数据集和论文的链接。
请随时与我们联系，如果您有任何问题。
谢谢。</sample>
    <sample id="1527">The authors are associated with the University of California, Berkeley.</sample>
    <sample id="1528">Ciyuan.</sample>
    <sample id="1529">四位。</sample>
    <sample id="1530">该方法与专门为 simultaneous processing 设计的 simulST 架构进行了比较。</sample>
  </task>
</testset>