<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="zh">
    <sample id="0">语言模型主要训练在大型网页数据上，其中政治新闻媒体的数据被广泛覆盖。</sample>
    <sample id="1">麦基尔大学、米拉和微软研究中心。</sample>
    <sample id="2">收到。</sample>
    <sample id="3">我的名字是丽吉娜·斯托恩，我将引导您完成本次演示的第一个部分。

首先，我们来定义文本简化。</sample>
    <sample id="4">文本简化是指将文本调整以提高其针对特定目标受众的理解能力。对于有阅读障碍或非母语人士而言，这尤其重要。</sample>
    <sample id="5">收到英文内容后，用中文表述其意思。</sample>
    <sample id="6">请提供英文内容。</sample>
    <sample id="7">收到英文内容后，用中文表述其意思。</sample>
    <sample id="8">我们建议创建一个新的合作平台。因为近年来，存在着许多与现有合作相关的难题。例如，这个合作平台，由于规模太小，难以开展技术创新模式。</sample>
    <sample id="9">这是一种近年来提出的重新建模方法，它会自动对齐，这意味着可能会出现错误且对齐不准确。</sample>
    <sample id="10">因此，我们建议将我们的公司拆分为两个子公司：Deploy APA 和 Deploy Web。Deploy APA 专注于新闻文本。</sample>
    <sample id="11">在平面API中，我们手动整理了483份文档，结果大约有30,000到30,000个句子对。</sample>
    <sample id="12">对于平面网页，这些内容包括不同的域名，并且我们还手动对这些750个文档进行对齐，另一方面则使用自动对齐方法。</sample>
    <sample id="13">收到。</sample>
    <sample id="14">我分析了我们句子对，例如，关于某种难以理解的词语。</sample>
    <sample id="15">我无法理解您提供的英文内容。请您提供清晰的英文文本，我将尽力用中文表述其意思。</sample>
    <sample id="16">收到。</sample>
    <sample id="17">目前，你可以看到，我们的平面 корпуса具有更高的差异化转换信息多样性。例如，在平面 API Корпус中，我们有更多的排序和旋转，而没有在平面 Web Корпус中。</sample>
    <sample id="18">我无法理解您提供的英文内容。请您提供清晰、完整的英文文本，我将尽力为您翻译成中文。</sample>
    <sample id="19">好的，我们来看看我们可以用这个数据集做什么。

你好，我是奥马尔，现在我将介绍我们数据集的用例。

所以，第一个用例是我们能够评估自动对齐消息。</sample>
    <sample id="20">最近有许多对齐方法，但在机器翻译的背景下，</sample>
    <sample id="21">收到英文内容后，用中文表述其意思。</sample>
    <sample id="22">在我们的用例中，我们试图提取两个平行的文档之间的对齐关系。这两个文档使用相同的语言，包含相同的文本内容，但复杂度不同。</sample>
    <sample id="23">现在我们有了我们手动对齐的深度平行的句子，我们可以将这些句子作为标准对齐方案来评估一些提出的对齐方法。</sample>
    <sample id="24">我们对提出的方法进行了调整，并发布了所有这些调整以及运行我们实验的代码，这些内容在论文中。</sample>
    <sample id="25">最终，我们得出结论，对于德语文本简化，最佳的自动对齐方法是使用“mas-l”方法。</sample>
    <sample id="26">我理解您希望我用中文来表达您提供的英文内容。请您提供英文内容，我将尽力用中文准确地表达其意思。</sample>
    <sample id="27">我们论文中展示的第二个用例是自动文本简化。</sample>
    <sample id="28">我发现微调语言模型，以生成从复杂输入文本中简化文本。</sample>
    <sample id="29">我们对两个不同的模型进行了微调。我们对长文本模型进行了微调，以产生文档级别的简化。</sample>
    <sample id="30">好的，请提供英文内容。</sample>
    <sample id="31">你可以查看所有检查点，并查看实验的得分和评估指标的更多细节。</sample>
    <sample id="32">我们得出结论，这种基本的微调可以产生或获得低于基线得分的得分。</sample>
    <sample id="33">我们建议将这些结果作为自动文本简化问题的基准，作为未来的基准。</sample>
    <sample id="34">非常感谢您的关注，我们希望在会议期间见到大家。</sample>
    <sample id="35">卡约·伊恩</sample>
    <sample id="36">T5 XL</sample>
    <sample id="37">Yes.</sample>
    <sample id="38">该方法通过明确标注模型响应中是否表现出某些行为，例如提供无关信息或相互矛盾，来减少人类评估的主观性。</sample>
    <sample id="39">现有弱监督方法的成功在很大程度上依赖于干净的验证样本。</sample>
    <sample id="40">The provided text does not contain any information about how to improve scores.</sample>
    <sample id="41">这篇论文有四位作者。</sample>
    <sample id="42">嗨，我的名字是萨达姆·斯皮尔科夫斯基，这关于协调的依赖结构。</sample>
    <sample id="43">例如，不同的理论和方法会产生不同的依赖结构，例如，在统一依赖中，坐标协调的结构是关于和麦基。</sample>
    <sample id="44">拉</sample>
    <sample id="45">我们使用一种处理系统，在 Igor Milchuk 的 Mining Text Theory 中，同样，整个代码结构由第一个子结构主导。所以这两个方法是等价的，对吧？它们分别独立出一种子结构。</sample>
    <sample id="46">现在，还有一些对称的坐标结构，例如Pragapro、Conjunction Header Approach，假设Prag Dependency Treebanks，而坐标结构由连接符引导。</sample>
    <sample id="47">所以，我们从一个到所有连接词。</sample>
    <sample id="48">最后，这还是一种多层次的方法，例如在卡森斯词法中。</sample>
    <sample id="49">所有从句都位于主句前面，因此从句的依赖关系从主句的引导词到所有从句分别。</sample>
    <sample id="50">现在，在纸上，目的是为了产生一种新的论点，关于配位的对称结构，比如这两个，以及反对配位的非对称结构，比如</sample>
    <sample id="51">好的，该论点基于依赖性最小化原则，我们将基于这个例子进行解释。</sample>
    <sample id="52">所以，正如你可能知道的，直接宾语通常会靠近动词，而状语则可能更远一些，对吧？所以，今天读了这篇文章很棒，因为它直接宾语位于动词旁边。</sample>
    <sample id="53">虽然昨天读了，但现在情况更糟，因为这里动词和直接宾语之间有一个副词“yesterday”。</sample>
    <sample id="54">然而，这种效果可能通过当直接对象非常重且非常长时得到改善，因为这样它可以移动到句子的末尾。</sample>
    <sample id="55">这已经说明在这里了。所以，这两个句子都很好。玛丽亚·特雷蒂亚斯绝对是一本迷人的关于昨天历史的书。这没问题。我们用它代替的是一本长而冗长的书。</sample>
    <sample id="56">我也可以说玛丽亚今天读了一本非常迷人的书，关于海。</sample>
    <sample id="57">所以这里是，这之所以是可能的，是因为即使这句话违反了一般语法原则，即直接宾语应该紧跟动词。</sample>
    <sample id="58">它满足了依赖长度最小化的原则，该原则指出，更短的依赖关系更受欢迎。</sample>
    <sample id="59">所以，这两个树只显示了关键依赖的长度，即那些在两个结构中都不恒定的那些。</sample>
    <sample id="60">所以在这里我们有从“red”到“adjunct”的长度为七的依赖关系，以及从“red”到“book”的长度为四的依赖关系。为了得到“eleven”，我们需要</sample>
    <sample id="61">当你移动或交换这两个成分时，这两个依赖的总和变为六，对吧？所以11减去6，它变得更短，这就是为什么它听起来还不错，它违反了原则，但它满足了某种要求。</sample>
    <sample id="62">好的。呃，所以我们做了什么，我们从增强版本的潘德潘德银行的关于协调的统计数据中提取了数据，并查看了为什么您使用大学依赖。</sample>
    <sample id="63">这些统计数据证实，观察结果多次出现，左侧共轭通常较短。因此，盐和酸的相对比例，盐酸的测量值，</sample>
    <sample id="64">并且也观察到，在通过时做出的观察，是随着长度差异的增加，倾向增长。</sample>
    <sample id="65">所以，我想知道两个连词的长度之间的区别。较短的连词通常指的是第一个更强，对吧？所以比例是左侧的。</sample>
    <sample id="66">关于我们最近在报纸上看到的一件事是，我们观察到这种倾向只发生在政府在左手边的时候。</sample>
    <sample id="67">右边，左边，例子中，我看到一个按钮，丽莎，所以州长在左边。</sample>
    <sample id="68">在第二个例子中，“home came”中没有出现缺失的词语，这里有动词的协调，并且没有外部的支配，对吧？在这样的情况下，左侧连词倾向于更短，而右侧则更长，而两者之间的差异越大，就越容易出现这种现象。</sample>
    <sample id="69">然而，当右边的治理，例如这里，将治理协调转交给网络时，这种效果消失了。</sample>
    <sample id="70">我们显示，通过测量长度为字符的第一列，为音节的第二列，为单词的第三列，我们将集中在第三列。</sample>
    <sample id="71">请提供英文内容。</sample>
    <sample id="72">左侧的倾向随着词语绝对差异的增加而稳步增长，情况与当有主语时观察到的情况相同，但在当主语位于右侧时，这种倾向消失了。</sample>
    <sample id="73">并且我们在论文中展示了如何通过这个来提供反对不对称配位结构的一个论点，因为这些是等同的结构。</sample>
    <sample id="74">请参考论文以获取完整协议，我将稍后与您讨论。</sample>
    <sample id="75">这篇论文有三位作者。</sample>
    <sample id="76">Bible text.</sample>
    <sample id="77">左并列词。</sample>
    <sample id="78">是的，你可以使用这些模型。</sample>
    <sample id="79">DEplain-apa 包含新闻文本。</sample>
    <sample id="80">For good generalization, we would need a better model architecture, larger model size, as well as more fine-tuning examples. These goals hand-in-hand, we can't just have one ingredient, but throughout the others. And at the same time, we also found that the performance drop here is caused by temperature drifts, and kind of surprisingly, it is not caused by adaptive overfitting. Even though convolutional neural networks has been used for over 20 years.</sample>
    <sample id="81">By measuring the length in characters of the first column (syllables), the middle column (in words), and the right column (as the focus).</sample>
    <sample id="82">设计实验，将支配词放置在左侧、中间或右侧，并测量其长度。</sample>
    <sample id="83">The baseline classifier performed no much better than chance.</sample>
    <sample id="84">3</sample>
    <sample id="85">Bob, Alice</sample>
    <sample id="86">语境感知 MT 模型在正式性和词汇连贯性等话语现象上比语境无关模型更有优势。</sample>
    <sample id="87">The authors are affiliated with the University of California, Berkeley.</sample>
    <sample id="122">The framework quantifies stance by annotating datasets with diverse annotators, considering the demographics of the original datasets. This allows for a large number of annotations per instance and the collection of rich demographic data, which is then compared to models and datasets.</sample>
    <sample id="155">研究发现，人类受试者也能够产生种族刻板印象。</sample>
    <sample id="156">The study used statistics extracted from the enhanced version of the Pentoshi Bank.</sample>
    <sample id="157">One.</sample>
    <sample id="158">The tasks closely related to the concept of cognitive dissonance are:
1. Topic-independent dissonance classification: Determines if two debate statements from different people are in agreement or disagreement, irrespective of the topic.
2. Binary classification of expansion and comparison classes of PTTB: This task classifies into two categories: expansion and comparison.</sample>
    <sample id="159">四位。</sample>
    <sample id="160">根据英语内容，这篇论文有 10 位作者。</sample>
    <sample id="161">引入的框架通过将最终用户与模型和数据集中预测和标签进行比较，与仅关注内部标注或建模的以前研究不同。</sample>
    <sample id="162">第一种设置。</sample>
    <sample id="163">DeepL and Google Translate.</sample>
    <sample id="164">大家好，我是乔恩·平，来自华盛顿大学。今天我将介绍我们研究的成果，从预训练数据到语言模型，再到下游任务，追踪政治偏见如何导致不公平和偏见。</sample>
    <sample id="165">收到。</sample>
    <sample id="166">根据C4 Corpus的一项调查，纽约时报、洛杉矶时报、守护者、哈佛邮报等媒体在语言模型训练中得到了很好的覆盖。</sample>
    <sample id="167">这创造了语言模型应用程序的混合祝福。</sample>
    <sample id="168">一方面，他们能够从不同的视角学习，这庆祝了民主和思想的多样性。另一方面，这些不同的政治观点是固有的社会偏见，并可能导致在数据挖掘任务应用中出现公平问题。</sample>
    <sample id="169">我们建议调查预训练数据到语言模型再到下游任务的政治偏见传播管道，具体通过提出以下问题：</sample>
    <sample id="170">如何评估语言模型中的政治偏见，以及我可能对这些偏见持有的看法？</sample>
    <sample id="171">其次，语言模型在下游任务中实际表现如何，以及可能导致NLP应用程序出现错误的场景。</sample>
    <sample id="172">具体来说，我们首先建议使用不同的提示格式向语言模型提出问题，例如政治问卷调查，如政治竞争力测试。这确保了我们能够以政治科学文献为基础进行自动评估。</sample>
    <sample id="173">初步结果表明，第一语言模型确实具有很强的政治倾向，它们占据了政治光谱的四分之三。</sample>
    <sample id="174">GPT-4是所有语言模型中最自由的，而GPT-C系列通常比Bird系列更具社会自由性，并且具有变异性。</sample>
    <sample id="175">其次，我们旨在调查语言模型中政治偏见到底有多大。</sample>
    <sample id="176">好的，请提供英文内容。</sample>
    <sample id="177">我进一步预训练语言模型在SuchPartisan和Coppola，我们可以看到语言模型的意识坐标也相应地与“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“</sample>
    <sample id="178">例如，对于罗伯特·哈佛，进一步训练在左侧线性Reddit语料库上，我们可以看到在词语的词性方面存在显著的自由变化。</sample>
    <sample id="179">收到。</sample>
    <sample id="180">我们还试图调查语言模型是否能够捕捉到我们现代社会中普遍存在的极化现象。</sample>
    <sample id="181">所以我们将预训练语料库分为美国45岁以下和美国45岁以上两部分，然后分别在两个不同的时间段上预训练语言模型。</sample>
    <sample id="182">我们看到，语言模型在 2017 年之后，普遍呈现出与中心偏离的政治倾向。这表明语言模型也能够捕捉到我们社会中的极化现象。</sample>
    <sample id="183">所以，最后一点，我们重视不同政治立场对仇恨言论检测和虚假新闻检测的评估，这两种NLP应用通常涉及语言模型，并且可能具有非常重要的影响。</sample>
    <sample id="184">所以我们看到，如果我们调查每个类别（即今天）的性能，如果我们将性能分开，那么</sample>
    <sample id="185">不同类型的媒体，例如新闻媒体，我们看到一个模式，例如对于仇恨言论检测，左侧的语言模型表现更好。</sample>
    <sample id="186">我无法理解您提供的英文内容。请您提供清晰的英文文本，我将尽力用中文表述其意思。</sample>
    <sample id="187">我无法理解您提供的英文内容。请您提供清晰、完整的英文文本，我将尽力用中文表述其意思。</sample>
    <sample id="188">人工智能模型在检测针对白人和男性的仇恨言论方面表现更好，但在检测针对黑人、LGBTQ+ 和其他少数族裔的仇恨言论方面表现更差。</sample>
    <sample id="189">相似的趋势也发生在虚假新闻检测领域，我们看到，在语言模型中，它们在检测来自其相反政治立场和观点的信息方面表现更好。</sample>
    <sample id="190">这已经够多了。我们进一步展示了许多定性例子，以说明语言模型具有不同的偏见。</sample>
    <sample id="191">你给出了针对仇恨言论和虚假信息的不同例子，基于社交媒体类别。附录中还有许多更多例子，以进一步强调其特点。</sample>
    <sample id="192">这表明存在一个持续存在的公平问题，与语言模型（具体指哪个模型未提及）的政治基础有关。</sample>
    <sample id="193">请提供英文内容。</sample>
    <sample id="194">这可能会导致持有不同政治观点的人被边缘化，并且针对少数族裔的仇恨言论不受约束地蔓延。</sample>
    <sample id="195">好的，请提供英文内容。</sample>
    <sample id="196">所以，在进行了一些讨论之后，我们还想强调一下，我们揭示了关于语言模型政治偏见的一个独特困境，就像在塞尔维亚和克罗地亚之间一样。</sample>
    <sample id="197">如果我们在训练语言模型时，没有对政治观点进行去性征，那么这些偏见就会从训练数据传递到语言模型，最终导致下游任务中出现不公平问题。</sample>
    <sample id="198">如果尝试某种方式进行消毒，我们也会面临审查或排除的风险，而且很难确定哪些内容实际上是中立的，应该保留在语言模型训练数据中。这有点像电学电击问题。</sample>
    <sample id="199">收到。</sample>
    <sample id="200">四位。</sample>
    <sample id="201">1024</sample>
    <sample id="202">他们的数据集中包含音乐、没有文字、12岁男孩、虚构人物以及来自阿塞拜疆的人。</sample>
    <sample id="203">Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="204">演讲者的名字是“我”。</sample>
    <sample id="205">Yes, EDAtt is designed to be compatible with existing offline ST models without requiring retraining or adopting a specific architecture for CSLT.</sample>
    <sample id="206">这篇论文有两位作者。</sample>
    <sample id="207">不，被测模型在测试套件上表现不佳。</sample>
    <sample id="208">KITMUS has three settings: Background Pretrain, Background Both, and Background Inference.</sample>
    <sample id="209">The provided text does not mention the authors' affiliations.</sample>
    <sample id="210">最后，我们是否应该只使用清理后的样本进行验证，还是有更好的方法来利用这些数据？</sample>
    <sample id="211">指标灵敏度确保模型在相同任务上，无论其变体如何，都能产生一致的输出。</sample>
    <sample id="212">Jingwei Yi.</sample>
    <sample id="213">更高的灵敏度通常表示模型性能得到了提高。</sample>
    <sample id="214">模型会接收来自文本的语言上下文。</sample>
    <sample id="215">通常需要 23 个干净的验证样本。</sample>
    <sample id="216">Senter Muse 和 Danjaraof。</sample>
    <sample id="217">现有方法无法准确衡量媒体偏见，因此需要开发新的方法。</sample>
    <sample id="218">玛克希塔</sample>
    <sample id="219">The political bias propagation pipeline from pre-training data to downstream tasks is investigated by asking the following questions:</sample>
    <sample id="220">是的，在简化过程中，plain-apa 语料库和网站语料库的简化过程有所不同。plain-apa 语料库的简化过程包含更多的排序和反转，而网站语料库的简化过程包含更多的删除。</sample>
    <sample id="221">Yes, Coscript is publicly available.</sample>
    <sample id="222">水印是通过对文本进行修改，在文本中插入水印来实现的。</sample>
    <sample id="223">Pinster University.</sample>
    <sample id="224">Yes, encoder-decoder models like mt5 can be improved through training on a mixture of languages.</sample>
    <sample id="225">制作巧克力蛋糕。</sample>
    <sample id="226">They validated the coverage of the provided embedding by realizing the embedding of sentences on four different datasets.</sample>
    <sample id="227">研究如何使用现有的 PLM 来构建新的 PLM。</sample>
    <sample id="228">GPT-4 与中国和英语国家最不一致。</sample>
    <sample id="229">The example sentence where the speaker demonstrates how the model utilizes knowledge learned through the attention mechanism is: "Leverages the knowledge acquired by the model through the attention mechanism between audio input and text output."</sample>
    <sample id="230">As the number of tasks increases, the model achieves better performance and exhibits lower sensitivity.</sample>
    <sample id="231">The author compares their method to three other tree-less models on the CoG benchmark.</sample>
    <sample id="232">The co-authors are advisors to the first author.</sample>
    <sample id="233">The first author of PaLM is Rohin Shah.</sample>
    <sample id="234">大家好，我是 Jenny，来自弗尔斯特大学，今天我将介绍我的研究，主题是：刻画设计偏见中的性别差异。</sample>
    <sample id="235">这项工作与华盛顿大学的一些同事以及人工智能公司Alan Turing for AI合作，包括塞巴斯蒂安·桑蒂、罗南·洛布罗斯、卡塔琳娜·莱尼卡和马丁·沙。</sample>
    <sample id="236">让我们先想象一下，你正在为报纸工作，你正在浏览你新闻文章下的评论，试图删除有害内容。</sample>
    <sample id="237">你可能会转向一个流行的API，比如Perspective API用于检测毒性，这对于像卡尔·琼斯这样的用户来说效果非常好。Perspective API能够正确地检测到不一致的言语。</sample>
    <sample id="238">但对于迪蒂·阿什玛来说，这种情况下，对冒犯性词语的敏感度并不高，而且在印度语境中更为常见。</sample>
    <sample id="239">这是一个设计偏见的一个例子，我们看到技术在不同群体中存在系统性的性能差异。</sample>
    <sample id="240">设计方式类似于我们刚才看到的那个，这可能与NLP研究团队的立场有关。立场简单来说就是人们由于其人口统计学、身份和生活经历而持有的视角。</sample>
    <sample id="241">这是一个在批判性研究中广泛使用的术语，尤其是在女权主义和酷儿学术领域。</sample>
    <sample id="242">作为一名研究人员，位置性会影响研究过程和结果，因为它会改变研究人员的决策。</sample>
    <sample id="243">一个人们可能会问的问题是，数据集中模型是否有位置信息？</sample>
    <sample id="244">我们并非想说，模型、细胞和数据本身具有人口统计学身份和生活经历，但它们会汇总真实人物的判断和意见，从而代表某些职业高于其他职业。</sample>
    <sample id="245">所以，预设是提供一些关于位置性的证据，例如文化差距、模式和数据设置，以及理论对位置性的定义。</sample>
    <sample id="246">然而，这些作品实际上并没有比较用户与数据本身，而是</sample>
    <sample id="247">在人工智能模型和数据集位置的专业知识日益重要，因为人工智能变得越来越具主观性和社会导向。</sample>
    <sample id="248">在描述这些位置的偏差方面，很难确定，因为并非所有决定都有记录，而且许多模型都隐藏在API之下。</sample>
    <sample id="249">所以，为了研究Dede的头部姿势模型，我们实际上将标注与真实用户与现有数据集和模型进行比较。</sample>
    <sample id="250">我们通过一个框架和位置分析。</sample>
    <sample id="251">我框架在两个主要领域运作：</sample>
    <sample id="252">第一步是将数据集重新标注，使用不同类型的标注者。</sample>
    <sample id="253">我们选择通过查看原始数据集的人口统计学信息来完成这项任务，因为通常每个实例只有少数标注者，而且人口统计学信息很少被收集和共享。</sample>
    <sample id="254" />
    <sample id="255">我们随后根据人口统计学进行标注，并将其与模型和数据集进行比较，使用皮尔逊相关系数。</sample>
    <sample id="256">该框架与标注争论文献不同，它通过将最终用户与模型和数据集中预测和标签进行比较，而不是仅仅关注内部标注或模型标注，而是关注标注的分布。</sample>
    <sample id="257">我们的框架主要通过Lab in the Wild，一个在线众包平台，由ECI合作者提供。</sample>
    <sample id="258">在野外实验室是一个在线实验平台，我们可以招募各种各样的志愿者。与像Enterick这样的平台相比，后者大多来自美国或印度。此外，在野外实验室仍然能够获取高质量的数据。</sample>
    <sample id="259">我们举办了两项任务，现在在《荒野》中。其中一项是社会可接受性。这项任务的工作方式是，参与者会阅读一个情境，从社会化学数据集入手，然后写出该情境在社会上是否可接受。</sample>
    <sample id="260">之后，他们可以在英式和美式英语之间比较他们的回答与人工智能和其他人的。</sample>
    <sample id="261">你已经将这些标注与社会化学、德尔菲和 GPT 结合起来。</sample>
    <sample id="262">我们随后为检测毒性言论和仇恨言论任务复制了一个非常相似的设置，他们会阅读来自“Danny Hate”的片段，并判断这些片段是否包含仇恨言论。</sample>
    <sample id="263">我们随后将这些标注与 DinaHeat、Perspective API、Rewire API、HateBERT 和 GPT-4 进行比较。我们的研究覆盖了超过 16,000 个标注，来自超过 1,000 位标注者，来自 87 个国家。</sample>
    <sample id="264">所以现在我们更好地理解了NLP数据评估模型与哪些最契合。我们发现，NLP存在位置性，并且NLP</sample>
    <sample id="265">例如，我们发现，在许多模型中，数据与英语国家最为相关。因此，对于GPT-4的社会接受度分析，我们发现它与中国和英语国家最为相关。我们发现，Dynah也与英语国家最为相关。</sample>
    <sample id="266">我们还发现，拥有大学学历的人与社会责任任务中的最相关。对于GDP4，我们发现它与拥有大学学历或高中学历的人最相关。</sample>
    <sample id="267">并且我们发现这对于约翰·海特来说也是如此，它与拥有大学教育的人最为相似。</sample>
    <sample id="268">然而，当模型和数据是针对特定人群的，那么一些人不可避免地会被遗漏。</sample>
    <sample id="269">一个例子是，数据在模型中对非二进制人士的比较与男性和女性的同类比较。我们可以在 GPT-4 的社会能力任务以及 Dina Heat 任务分析中找到这一点。</sample>
    <sample id="270">所以，鉴于那里有位置和ALID和NLP，我们可以做什么？</sample>
    <sample id="271">所以我们有一些建议。第一个是记录所有相关的设计选择，贯穿整个研究过程。第二个是进行用户体验研究，从用户的角度出发。</sample>
    <sample id="272">我们第三个建议是构建针对特定社区的专业数据集模型，一个很好的例子是马萨丘塞斯州倡议。我想强调的是，包容性人工智能不仅仅是让所有技术都能为每个人工作。</sample>
    <sample id="273">所以，这就是本次演示。如果您想了解更多，请随时查看我们的仪表盘以获取最新分析结果，以及我们的论文。谢谢。</sample>
    <sample id="274">The main problems of current SimulST models are specific architectures requiring additional modules for optimization, long and complicated training procedures, and the need to train and maintain multiple models to achieve different latency regimes.</sample>
    <sample id="275">It's challenging to effectively mitigate societal and political biases in NLP model training data. Simply sanitizing the data can lead to censorship or exclusion, and defining "neutral" is difficult.</sample>
    <sample id="276">嗨，我是徐宇源，来自复旦大学。我在这里介绍我们的工作：从语言模型中提取可约束的知识。</sample>
    <sample id="277">在现实生活中，人们经常会按照步骤执行操作，通过遵循明确的指令来完成任务。</sample>
    <sample id="278">以前，大型语言模型被用于规划抽象的日常活动，例如做蛋糕，并证明大型语言模型可以有效地分解任务为步骤。</sample>
    <sample id="279">然而，以往许多人专注于规划广泛的抽象目标，而忽视了典型的活动。规划目标需要具体的、具体的限制，例如制作巧克力蛋糕。仍然处于第一阶段。</sample>
    <sample id="280">在本文中，我们定义了约束语言规划的问题。</sample>
    <sample id="281">计划的约束条件会因不同的现实生活目标而异，并且受到各种动机性约束。一个好的计划者应该识别出合理的、符合约束条件的约束。</sample>
    <sample id="282">在本文中，我们评估并改进了限制语言规划的能力的大型语言模型。</sample>
    <sample id="283">请将英文内容转换为汉语。</sample>
    <sample id="284">我们如何获取这些代码首先？
正如在表格中所显示的那样，我们将抽象代码与修改后的约束相结合，用于人类在数据获取过程中使用的抽象 GPT。</sample>
    <sample id="285">我是一个乐于助人的助手。只返回请求的答案，不要包含任何解释或介绍。</sample>
    <sample id="286">这个表格反映了结果的整体准确性。我们发现，所有线性回归模型都实现了令人满意的结果。</sample>
    <sample id="287">真，我无法提供详细的分析来调查为什么语言模型会产生这些内容。</sample>
    <sample id="288">结果在图表中显示，生成脚本中的语义完整性是可以接受的，但对事实的约束则无法保证。</sample>
    <sample id="289">在越来越多的顶级类别中，定义在Wakefield的限制。
图表中的热图显示，不同类别女性的计划和表现差异显著。</sample>
    <sample id="290">我们已经看到，在大型语言模型中，输出质量在不同版本中有所不同，导致性能下降。因此，人们采用了过度生成的Z-过滤器来提高生成质量。</sample>
    <sample id="291">我首先展示了约束类型以及基于这些抽象概念的示例，例如在 ChatGPT 中。</sample>
    <sample id="292">赞，提取GPT或生成文本片段，为特定角色。</sample>
    <sample id="293">下一个，一个未来模型是开发一个两步选择第一个脚本。</sample>
    <sample id="294">我们将脚本和鬼魂转换为提取的 GPT 嵌入，并计算相似度作为相似度分数，从而衡量文本相似性。</sample>
    <sample id="295">请将英文内容转换为汉语。</sample>
    <sample id="296">我们的研究表明，在LGBTQ+群体中，有较多的头发色素沉着。我们的研究表明，在男性中，有明显的皮肤色素沉着，以及50%的女性有皮肤色素沉着。</sample>
    <sample id="297">鉴于大型语言模型成本高昂，因此启用语言规划是部署较小和专业化模型的必要步骤。创建数据集是第二步。</sample>
    <sample id="298">然而，之前的研究并未提供任何关于特定目标的计划，并且手动数据在注释中的成本是昂贵的。</sample>
    <sample id="299">我们遵循一个象征性知识蒸馏的阶段，以约束语言模型的数据大小。从轻量级模型开始。</sample>
    <sample id="300">请将英文内容转换为汉语。</sample>
    <sample id="301">总共，为了生成55个具有特定目标和脚本的单词，我们需要确保颜色、验证和测试套件。我们要求众包的工人来查找并修改不正确的样本。</sample>
    <sample id="302">这个图表显示了语言规划中约束性模型的分布，而非约束性模型在通用零售领域表现出更高的表现。通过约束性模型，我们可以构建更小但更专业的模型来进行语言规划。</sample>
    <sample id="303">在非发色T型文件中，颜色是红色，可以生成更长的头发颜色，比大多数大型模型更长，表明较小的模型可以处理更长的头发，而较大的模型可能需要更多的数据。</sample>
    <sample id="304">总结：我们建立了一个约束语言规划问题，旨在增强大型语言模型对约束语言规划能力的理解，并开发了一个用于大型语言模型后生成评估的指标。</sample>
    <sample id="305">我们使用大型语言模型来生成一个高色彩的平方数据集，用于语言规划。我们希望该数据集可以作为研究语言规划的可用资源。</sample>
    <sample id="306">谢谢您的时间，请详细描述您的论文标题。</sample>
    <sample id="307">PaLM 的流畅度与人类语言相似，但主要区别在于其准确性。</sample>
    <sample id="308">水印方法需要具备以下属性：

1. 可应用于嵌入服务。
2. 不应降低提供的嵌入的实用性。
3. 水印应足够隐蔽，攻击者可以移除。
4. 水印应可移植到攻击者服务。</sample>
    <sample id="309">14</sample>
    <sample id="310">我们选择重新注释数据，以获得每个实例的多个注释，并获取丰富的细分人口统计学数据。</sample>
    <sample id="311">The cosine similarity and L2 distance are used to measure the difference between the requested embedding and the target embedding.</sample>
    <sample id="312">将基于编码器的多语言模型用于这项任务，我们评估了包含编码器和点状解码器的模型，例如编码器-PT-解码器（如XLM-R+PT和BERT+PT），以及包含编码器和解码器的模型，例如BERT。</sample>
    <sample id="344">The author assumes the provider can collect a general text corpus and count the word frequency within a moderate frequency interval.</sample>
    <sample id="345">大家好，我叫徐洪。今天我将介绍我们的论文：《命名实体标签在 2023 年是否仍然有效？》。让我们开始吧。</sample>
    <sample id="346">我们的论文研究了泛化问题，使用名为命名实体识别任务（NER任务）的任务。</sample>
    <sample id="347">我们观察到，模型已经使用公元2003年以来开发了人工智能近20年，这自然引发了许多问题。首先，这些模型泛化到更多数据。</sample>
    <sample id="348">在开发新的标签时，需要什么？</sample>
    <sample id="349">与此同时，如果我们观察到泛化能力下降，是什么导致了这些模型的性能下降？</sample>
    <sample id="350">为了调查这些问题，我们开发了一个 Cono+ 数据集。这是一个我们从路透社新闻收集的数据集，然后使用 2003 年的 Cono 标注指南进行标注。</sample>
    <sample id="351">我们对20个模型进行了微调，并在Kernel 2003测试集和Kernel Plus Plus测试集上进行了评估。</sample>
    <sample id="352">最后，我们计算了每个模型的 F1 分数的百分比变化，以评估其泛化能力。</sample>
    <sample id="353">所以，对于良好的生成，我们需要什么？在我们的实验中，我们发现有三个主要成分是必要的。</sample>
    <sample id="354">第一个是模型架构。在我们的实验中，我们发现Transformer模型通常在新的数据上泛化得更好。</sample>
    <sample id="355">第二个成分是模型大小。我们发现，通常较大的模型能带来更好的泛化能力。</sample>
    <sample id="356">最后，最重要的是，我们都知道，用于微调的样本数量直接影响下游任务的性能。在这里，我们还发现，更多的微调样本实际上也导致了更好的泛化能力。</sample>
    <sample id="357">你下一个问题是什么？什么导致了某些模型性能下降？</sample>
    <sample id="358">我们有两项假设。第一项是过拟合，这通常是由于重复使用相同的测试数据集而导致的，并且通常表现为在新的测试集上返回值减少。</sample>
    <sample id="359">第二种假设是温度漂移，这是一种由训练和测试数据之间不断扩大的温度差引起的性能退化。</sample>
    <sample id="360">拟合的函数。我们看到，从右边的图上，红色的最佳拟合线斜率大于y轴。</sample>
    <sample id="361">这意味着我们在2003年取得的每一点改进，都转化为在Call Plus Plus上超过一点的改进，这意味着没有边际收益递减。</sample>
    <sample id="362">这表明在当前情况下，适应性没有观察到。</sample>
    <sample id="363">那么关于临时特赦呢？</sample>
    <sample id="364">对于时间漂移，我们进行了一项实验，以重新训练或继续预训练一些模型，使用更多最近的数据。我们发现，随着数据规模的增大，性能会下降。</sample>
    <sample id="365">这证实了我的假设，即性能下降的主要原因是温度。</sample>
    <sample id="366">我们的结论是，为了更好的泛化，我们需要更好的模型架构、更大的模型尺寸，以及更少的微调示例。这些目标相互关联，我们不能只关注一个方面，而是需要通过其他方面来实现。</sample>
    <sample id="367">与此同时，我们还发现，这里的性能下降是由于临时漂移造成的，而且令人惊讶的是，它并非由数据不匹配引起。即使在公元2003年，卡农图表也已经使用了超过20年。</sample>
    <sample id="368">所以回到我们论文标题中提出的问题，2003年内核技术在2023年是否仍然有效？我们发现答案是肯定的。</sample>
    <sample id="369">我们希望通过我们的论文来推动更多关于如何改进模型泛化能力的研发。</sample>
    <sample id="370">最后，请务必查看我们的纸质数据集，如果您有任何问题，请随时联系我。非常感谢。</sample>
    <sample id="397">The size of the voice fragments used in this method is not specified in the provided text.</sample>
    <sample id="398">Servin 是法官。</sample>
    <sample id="399">示例质量</sample>
    <sample id="400">GPT-4, GPT-3 series, and BERT series.</sample>
    <sample id="401">该模型结合了多个层的注意力分数。</sample>
    <sample id="402">直接推断的示例包括直接说出歌曲名称，或者直接说出歌曲的位置。</sample>
    <sample id="403">福特大学</sample>
    <sample id="404">3</sample>
    <sample id="405">是的，在语义解析之前，使用 Google Translate API 将源语言翻译成目标语言，然后使用单语模型进行训练和评估。</sample>
    <sample id="406">男性。</sample>
    <sample id="407">The provided text does not mention any model architectures with poor generalization ability.</sample>
    <sample id="408">Clean data.</sample>
    <sample id="409">四位。</sample>
    <sample id="410">The author investigated whether instruction tuning of multimodal pre-trained models can actually improve generalization to unseen multimodal tasks.</sample>
    <sample id="439">作者认为在知识密集型 NLU 任务中，成功模型需要能够整合和使用预训练时间和推理时间知识。</sample>
    <sample id="440">The speaker's name is Ying and her colleague Zhiyang.</sample>
    <sample id="441">是的，Coscript 经过了质量检查。</sample>
    <sample id="442">现有的资源仅支持有限类型的依赖上下文翻译，并且仅支持有限的语言。</sample>
    <sample id="443">嗨。我将谈谈我们解决实体选择中间接关系表达式的工作，其中我们引入了替代实体概念。</sample>
    <sample id="444">我的名字是贾瓦德·侯赛尼，这是与菲利普·拉德林斯基、西尔维娅·帕尔蒂和阿尼尔·贾尼的合作。</sample>
    <sample id="445">或者，这个系统理解用户语言，当他们想做选择时。考虑这个替代问题：你是不是想说“容易对我”或者“我感觉到了”？在这里，用户想选择这两个选项中的一个。</sample>
    <sample id="446">直接引用。例如，通过说歌曲的名称，或者它的位置。</sample>
    <sample id="447">但有时与亲密朋友交谈更合适，以便进行更自然的对话。这通常发生在用户记不住对方的名字时。</sample>
    <sample id="448">或者发音太相似，难以区分。</sample>
    <sample id="449">或者当用户想要指定一个偏好时。这里有一些间接偏好的例子：例如，最新的一个或不是充满活力的那个。</sample>
    <sample id="450">这是一个在对话系统以及评估自然语言处理任务中非常重要的问题。</sample>
    <sample id="451">我们没有意识到一个公共数据集，一个大型规模的公共数据集用于该任务，所以我们收集了一个通过众包标注的数据集。该数据集涵盖三个不同的领域：音乐、书籍和电影。</sample>
    <sample id="452">或者数据集收集方法强调非正式性，使用卡通评论。</sample>
    <sample id="453">卡通有三个对话气泡。在第一个气泡中，鲍勃说：“记住昨天我们听过的那个歌吗？”然后鲍勃结束了对话。</sample>
    <sample id="454">在第二个对话框中，艾丽丝说：“你是指对我吗，还是我错了？”</sample>
    <sample id="455">Which is the alternative question. And in the third speech bubble, Bob uses an indirect reference to select one of these entities. For example, the new chair.</sample>
    <sample id="456">请将所给出的英文翻译成中文。</sample>
    <sample id="457">请将所给出的英文翻译成中文。</sample>
    <sample id="458">请将所给出的英文翻译成中文。</sample>
    <sample id="459">以下是用于移动到列表更高位置时使用的不同采样方法。随着我们移动到列表的更高位置，实体变得越来越相似，通常很难使其具有歧义性。</sample>
    <sample id="460">请将所给出的英文翻译成中文。</sample>
    <sample id="461">The second one is when the entities have similar titles, for example, two books with the name "The Reach".</sample>
    <sample id="462">第三个是当他们在维基百科上拥有相似的描述时，最后是当他们在维基百科上拥有相似的 infobox 或属性时。例如，相同的类型或相同的艺术家。</sample>
    <sample id="463">当我们在问卷调查中展示这个替代问题时，他们知道这些实体的名称，但他们并不一定了解关于这个角色。</sample>
    <sample id="464">我们要做的是展示关于这两个实体的背景知识。对于歌曲，我们简单地展示一个谷歌搜索链接到其标题。</sample>
    <sample id="465">然后请让听众听至少一首歌曲，并阅读关于每首歌曲的内容。

以下是一个谷歌搜索结果的例子，对于歌曲“I Can’t”的结果：

“I Can’t”</sample>
    <sample id="466">对于食谱和书籍领域，我们展示了一些来自维基百科的背景文本。对于食谱，我们还展示了它们的图片，同样来自维基百科，这样注释者就能知道它们是什么样子。</sample>
    <sample id="467">然后，我们让参与者选择一个实体，例如这里第一个，并用 3 到 5 个间接引用表达方式描述它。</sample>
    <sample id="468">请将所给出的英文翻译成中文。</sample>
    <sample id="469">The AltText corpus has 6,000 alternative questions across three domains, and it has 42,000 indirect phrasing expressions. Results with t5 x large model are summarized.</sample>
    <sample id="470">这个语言模型拥有与训练数据完全相同的背景知识，因此准确率很高，大约在92%到95%之间。但这并非真实。</sample>
    <sample id="471">如果语言模型有访问一些部分重叠的背景知识，那么在82%到87%的准确率之间，这对于例如当语言模型检索背景知识时来说是更现实的。</sample>
    <sample id="472">如果语言模型只能访问实体名称，那么准确率只有60%，所以还有很大的改进空间。我们还展示了这些模型是领域泛化的。这里有一个链接到数据集。</sample>
    <sample id="473">weight key strategy and local agreement.</sample>
    <sample id="474">The author is affiliated with the University of Paris-Saclay.</sample>
    <sample id="475">Jenny.</sample>
    <sample id="476">Four.</sample>
    <sample id="477">嗨，我是塞拉·巴比，来自特伦托大学和布鲁诺·凯斯勒基金会。我将简要介绍注意力作为本论文的指导，这是一项与马克·奥内格里和马可·图尔合作的成果。</sample>
    <sample id="478">同步语音翻译（Simultaneous Speech Translation，简称SST）是指在实时环境下，将口语翻译成另一种语言的文字。</sample>
    <sample id="479">在当前语义模型中，主要的缺点是特定架构通常使用额外的模块来优化。</sample>
    <sample id="480">冗长且复杂的培训程序，例如涉及不同优化目标（目标）的培训。</sample>
    <sample id="481">训练和维护多个模型以达到不同的延迟设置，例如训练一个模型具有平均 1 秒的延迟，另一个模型具有 2 秒的延迟，等等。</sample>
    <sample id="482">请将所给出的英文翻译成中文。</sample>
    <sample id="483">第一个使用现有的离线模型，无需重新训练或采用特定架构的CVLST。使用单个模型来处理整个延迟视频流，并通过特定的参数处理延迟。</sample>
    <sample id="484">和劳尔的知识是通过注意力机制在音频输入和文本输出之间习得的，即交叉注意力机制。您可以在以下示例中看到一个：</sample>
    <sample id="485">我们的解决方案是预备点，或者编码器解码器注意力，而这是一种策略，用于在侧面决定是否会相遇，或者不，部分翻译，基于注意力点的。</sample>
    <sample id="486">如果检测到没有集中性，也就是说，该信号低于某个阈值阿尔法，则向较少的语言语音帧发送。这意味着接收信息足够稳定。</sample>
    <sample id="487">例如，如果我们筛选一个包含“我将谈论关于”的句子，并且我们的模型预测了翻译为德语的“这”。</sample>
    <sample id="488">和我们将会研究的，是注意力，嗯，哪个？</sample>
    <sample id="489">我们会看到第一个词指向最少接收到的语音帧，而最后一个词指向最少接收到的语音帧，也就是lambda语音帧。</sample>
    <sample id="490">这表示前两个词将是“查”、“查”。</sample>
    <sample id="491">虽然在某些情况下，最大趋势高于某个阈值阿尔法，我们将不发出最后一声，并等待下一个语音片段。</sample>
    <sample id="492">如果我们在一个语音银行中，我们的模型预测其他词语，我们会查看交叉注意力，即</sample>
    <sample id="493">我们将会看到，没有词语能点明这篇演讲的重点。</sample>
    <sample id="494">这表示这三个词将是“拉”。</sample>
    <sample id="495">如果看看那个主要结果，那就是</sample>
    <sample id="496">我们将同时进行翻译结果绘制在图表上，其中蓝色一侧衡量翻译质量，而平均语言</sample>
    <sample id="497">但是，呃，这个是，呃，让我们来衡量，而且我们还考虑了计算成本平均延迟，这包括了模型计算时间来处理输出。</sample>
    <sample id="498">所以我们希望我们的好奇心尽可能地高。</sample>
    <sample id="499">请将所给出的英文翻译成中文。</sample>
    <sample id="500">和我们比较的是针对我们非线性模型也适用的准备策略，例如权重策略和局部协议。我们还比较了基于架构的策略，特别是专门为同步处理设计的。</sample>
    <sample id="501">这些是同步语音翻译策略在德语上的早期结果。</sample>
    <sample id="502">并且我们看到，呃，它在所有策略应用于离线模型时都优于所有策略，因为它的曲线被向左移动了。</sample>
    <sample id="503">和我们还看到，如果我们考虑实际的运行时间或计算时间，那是一种最快的策略。</sample>
    <sample id="504">如果你想发现更多结果，请阅读我们的论文，我们还发布了开源代码和模型，并同时提供了模拟输出，以促进我们工作的可重复性。谢谢您的关注。</sample>
    <sample id="505">是的。</sample>
    <sample id="506">大家好，我的名字是伊恩，我的同事志扬和我将介绍我们的研究，关于改进模型驱动的序列学习，并使用图神经网络。</sample>
    <sample id="507">随着大型语言模型技术的进步，许多研究人员开始探索新的学习范式，即利用预训练语言模型进行不同下游任务的参数和数据高效的检索。</sample>
    <sample id="508">最近，许多研究表明，指令微调能够使大型语言模型在零样本任务中以自然的方式遵循自然指令。</sample>
    <sample id="509">然而，大多数以前的工作都侧重于提高在语言任务上的零样本性能，而计算机视觉和多模态任务则被搁置一边。</sample>
    <sample id="510">因此，在工作中，我们想要调查是否通过指令微调多模态预训练模型可以实际提高其对未知多模态数据的泛化能力。</sample>
    <sample id="511">此外，在我们的研究期间，我们发现在NLP和MoE模型之间指令数据集的可获得性存在显著差异。</sample>
    <sample id="512">目前有超过1600个语言仅指令任务，然而，目前没有大规模公开可用的多模态指令任务。因此，这激励我们构建一个多模态指令微调数据集。</sample>
    <sample id="513">在这里，我们呈现了多模态指令第一个多模态模型指令调优基准数据集，该数据集包含62个多样化的多模态任务，涵盖了10个不同的类别。</sample>
    <sample id="514">这任务来源于一个现有的开放数据集，每个任务都配备了五个编写指令。</sample>
    <sample id="515">为我们提出的数据集进行多模态指令微调。我们使用OFA，将多模态预训练模型作为我们的基础模型。OFA使用统一的词汇、图像token和边界坐标。</sample>
    <sample id="516">这里展示了一些我们多项式数据中的示例实例。</sample>
    <sample id="517">将这段英语内容表达为中文。</sample>
    <sample id="518">将这段英语内容表达为中文。</sample>
    <sample id="519">好的，我现在要谈论多模态指令教学。</sample>
    <sample id="520">所以对于20天数据集，我们使用53个任务从Negru组进行训练，每个任务有10000个样本进行测试，我们保留整个CommonSense Reasoning组进行测试，并且我们从Wiki和杂乱的类别中额外选择5个任务。</sample>
    <sample id="521">重复出现。</sample>
    <sample id="522">所以我们使用预训练的奥尔法大型模型作为基础模型。在训练过程中，我们会为所有任务创建不同的实例。每个实例都会随机组合到其五个指令模板中。</sample>
    <sample id="523">重复。</sample>
    <sample id="524">我们评估了在各种实验中性能和标准偏差的性能。</sample>
    <sample id="525">这任务是一个多模态分类任务，我们会报告准确率。如果它是一个多模态生成任务，我们会报告ROUGE-L。对于开放式任务，我们会报告ROUGE-L。</sample>
    <sample id="526">我们还引入了一个额外的验证机制，称为“一致性测试”。这个机制确保了模型在相同的任务上，无论其变体如何，都能一致地产生相同的输出。</sample>
    <sample id="527">这是我们的主要结果，正如我们所见，指令调整可以显著提高OS和iOS的性能，在处理多模型任务时。</sample>
    <sample id="528">从自然语言处理中学习来自大规模文本数据集的商业优势，指令调优。</sample>
    <sample id="529">这里我们可以看到，随着任务数量的增加，模型会获得更好的性能，与此同时，它对噪声的敏感性会降低。</sample>
    <sample id="530">所以我们还使用了while语句和for语句，正如我们所见，使用while语句可以提高模型的整体性能，并且减少其灵敏度。</sample>
    <sample id="531">所以这展示了不同前馈网络策略对模型敏感性的影响。正如我们所见，通过从自然语言指令数据集进行迁移学习，模型可以实现比原始的 OpenAI 模型更高的敏感性。</sample>
    <sample id="532">我们也可以看一看在自然语言处理中从自然语言指令数据集获取的转换数据，这可以帮助AI在自然语言指令数据集上获得更好的性能。</sample>
    <sample id="533">所以我们现在要提出第一个大规模多模态Instruction Tuning数据集，我们将能够进一步提高Ollama的可访问性，并展示不同迁移学习技术及其益处，并设计一个神经结构可变性。</sample>
    <sample id="534">我们正在收集一个更大的多模态指令微调数据集，其中包含大约150个额外的语言任务，并且我们将很快发布这些数据。这是我们数据集和模型的二维码。谢谢。</sample>
    <sample id="535">Sara Babi from the University of Trento and the Bruno Kessler Foundation.</sample>
    <sample id="536">Javad Hosseini.</sample>
    <sample id="562">大家好，我是科斯托夫·谢纳，很高兴欢迎大家参加我们关于 ACL 2023 论文“语言模型可接受性判断并非总是对上下文鲁棒”的讨论。</sample>
    <sample id="563">这是一个由约翰·戈特、艾尔·穆勒、卡尼什卡·米什拉、卡伦·弗伦特尔斯、罗杰·莱维和阿蒂纳·拉奇共同完成的作品。</sample>
    <sample id="564">所以，在本文中，我们将回顾最小配对。</sample>
    <sample id="565">最小对偶时间基本上评估语言模型在可接受性判断之上，这些判断也可能包括语法性、如拼写、语法等，或可接受性方面，如刻板印象，如种族偏见。</sample>
    <sample id="566">在最小对偶范式中，评估语言模型通常是将一个可接受的句子或语法正确的句子展示给它，然后展示一个不可接受的句子或语法错误的句子。</sample>
    <sample id="567">然后，模型会基本将可接受的类别赋予更高的概率。</sample>
    <sample id="568">当前模型管道基本上不允许我们评估模型对长句的接受程度。</sample>
    <sample id="569">这些大型语言模型正在推出更长更长的上下文窗口，因此我们需要评估模型在整个上下文窗口中的可接受性。</sample>
    <sample id="570">我们正在尝试做的事情就是，我们正在尝试通过让模型评估可接受性来重温 PPP 管道。</sample>
    <sample id="571">所以，这就是方法。那么我们要做的是模拟这些更长的序列。我们重新访问数据集合本身，然后我们重新创建句子，通过选择那些可接受或不可接受的句子。</sample>
    <sample id="572">例如，这里我们选择了一个典型的语法对，来自“blip”数据集，来自“adjunct island”的“页”。</sample>
    <sample id="573" />
    <sample id="574">请。</sample>
    <sample id="575">所以我们可以通过从同一对匹配中选择不可接受的句子来做同样的事情，这也可以用来测试模型的可接受性。</sample>
    <sample id="576">我们也可以通过从不同的子集或不同的数据集选择句子来做到这一点。这就是我们称之为“不匹配策略”的方法。</sample>
    <sample id="577">这里句子仍然来自相关的数据库，但不是来自你正在评估的同一个数据库。</sample>
    <sample id="578">最后，我们可以从完全不相关的领域中选择句子，例如维基百科。</sample>
    <sample id="579">所以这会告诉我们模型在实际影响下接受度评估是否受到任何影响。</sample>
    <sample id="580">你是否来自数据集中不同的子集，还是完全与当前句子无关？</sample>
    <sample id="581">所以模型如何运作？首先，我们查看维基百科的句子，这些句子与当前查询对完全无关，然后我们发现，对于任意上下文，语言模型通常是鲁棒的。</sample>
    <sample id="582">我们增加了上下文长度到 1024，以最大限度地发挥 OPT 和 GPT-2 模型，我们在这里看到，在橙色点划线上，MPP 判决相对稳定。</sample>
    <sample id="583">现在当我们从同一段文字中选择句子时会发生什么？</sample>
    <sample id="584">所以在这里，我们选择或创建句子，从可接受和不可接受的领域中，来自同一位被指控的人。</sample>
    <sample id="585">并且我们看到，在 MPP 判决中，无论是接受的前缀还是不可接受的前缀，都会显著增加或减少。</sample>
    <sample id="586">但是，当我们匹配结构时，当我们选择来自同一现象的句子时，例如，</sample>
    <sample id="587">我们看到模型在选择的前缀是否可接受或不可接受的情况下，在 MPP 判决上出现巨大的增加或巨大的减少。</sample>
    <sample id="588">现在，这个呃呃，这个非常大，就像这个影响贯穿整个上下文线，这可能会影响像新语言模型，它有大上下文。</sample>
    <sample id="589">为什么匹配前缀会如此严重地影响语言模型判断？</sample>
    <sample id="590">一系列分析，我们尝试通过尝试保留输入句子的相关结构，但添加噪声到输入中来，然后对这些扰动进行多次操作。</sample>
    <sample id="591">我们发现这些噪音实际上并没有改变模型的课程，在概率判断方面。</sample>
    <sample id="592">基本上，我们发现模型对句子中词语的相似度非常敏感。</sample>
    <sample id="593">当我们在可接受域中对句子进行扰动时，我们看到所有扰动都出现相似的增加，而在我们扰动句子在可接受概率域中时，我们看到在相似特征上的概率判断减少。</sample>
    <sample id="594">所以，我们工作的关键收获是语言模型对句子中共享的潜在句法和语义特征非常敏感。</sample>
    <sample id="595">并且，我们目前采用的基于短句和单个句子的评估方法，可能无法完全捕捉语言模型在整个上下文中的抽象知识。</sample>
    <sample id="596">请阅读我们的论文以了解更多关于我们实验的细节。谢谢。</sample>
    <sample id="597">一个无序多元素。</sample>
    <sample id="598">55</sample>
    <sample id="626">The best alignment method to use for German text simplification is the `mess_align` method.</sample>
    <sample id="627">Weakly supervised learning allows training new networks on weakly labeled data, enabling them to generalize despite label noise.</sample>
    <sample id="628">文档采用手动和自动对齐方法进行对齐，具体分配情况未提及。</sample>
    <sample id="629">CoNLL++ 数据集是从路透社新闻（2020 年）收集的，并使用 2003 年的标注指南进行标注。</sample>
    <sample id="630">你好，大家好，我是Justin John来自宾夕法尼亚大学。今天我将演示一个工作示例：跨语言语义解析和多种自然语言的语义表示。</sample>
    <sample id="631">语义化表示用户查询，例如“sequel”和“lambda calculus”。</sample>
    <sample id="632">跨语言语义解析是任务将查询翻译成多种自然语言中的多种语义表示。</sample>
    <sample id="633">在语音中，我们需要使用神经模型将查询翻译成多种自然语言，并插入一个 lambda 函数或 SQL。</sample>
    <sample id="634">存在许多跨语言语义解析模型，它们分别被提出和评估在各种数据集和应用任务上。例如，</sample>
    <sample id="635">他们对某些自然语言的覆盖率是有限的，中文缺失，</sample>
    <sample id="636">请提供英文内容。</sample>
    <sample id="637">The lambda calculus is missing.</sample>
    <sample id="638">或者只有评估一个特定的新模型，例如只使用一个模型来评估。</sample>
    <sample id="639">为了这一点，我们建议提供一个示例数据集，提供跨语言和多语种的统一数据集，用于跨语言和多语种的表示。</sample>
    <sample id="640">它包含 90 个数据集，570 个语法任务，8 种表示形式和 22 个自然语言。</sample>
    <sample id="641">为了更好地评估我们的基准，我们考虑了训练和评估的六个设置。</sample>
    <sample id="642">The first one is translate test. We'll use Google Translate API to translate source to the target language, then use monolingual model to train and evaluate the</sample>
    <sample id="643" />
    <sample id="644">我还会测试单语模式。</sample>
    <sample id="645">请提供英文内容。</sample>
    <sample id="646">我们还测试了单语语言的Few-shot设置，通过使用少量数据训练单语语言模型。</sample>
    <sample id="647">和具有单语言模型，我们训练一个单语言模型，用于所有语言。</sample>
    <sample id="648">例如，我们把德语语言的中文查询结合起来，来训练一个语言模型，并在推理时，我们可以使用这个模型来总结英文内容。</sample>
    <sample id="649">请提供您想要翻译的英文内容。</sample>
    <sample id="650">我们还考虑了跨语言零样本和少样本迁移，在一种语言和另一种语言之间进行迁移。</sample>
    <sample id="651">在训练过程中，我们将训练我们的英语查询或英语和德语短语查询，以训练一个多语言模型，来预测序列中的下一个单词。</sample>
    <sample id="652">并且我们还发现了一些非常有趣的成果。所以，关于单语模型，我们评估了两个模型：</sample>
    <sample id="653">包括编码器-解码器（Encoder-Decoder），即多语言预训练编码器与指针基于解码器，例如XLM-R+PT和BERT+PT。</sample>
    <sample id="654" />
    <sample id="655">我们发现编码器解码器在所有九个数据集上都能获得最佳性能。</sample>
    <sample id="656">和我们评估了 M5 和示例 XLR + PDR 的多语言工具。</sample>
    <sample id="657">没有它，编码器、解码器或编码器 PTR 无法改进，通过训练混合语言数据。</sample>
    <sample id="658">我们发现，这主要是因为大多数主要的自然语言可以获得性能提升，但英语的性能在七个数据集上下降，而仅在三个数据集上获得提升。</sample>
    <sample id="659">我以为这是没有人知道的，这是语言学的一个结果。</sample>
    <sample id="660">我们还比较了跨语言性能的特征。</sample>
    <sample id="661">在图中，蓝色线表示交叉角度的零轴转换，橙色线表示交叉角度的零轴转换，而绿色线表示建模角度的设置。</sample>
    <sample id="662">我们发现，在比较绿色和橙色线时，在零步设置下，跨线gap的传输性能差距显著，而在比较蓝色和橙色线时，在几步设置下，传输gap迅速缩短。</sample>
    <sample id="663">我们还发现了一些其他有趣的发现，例如，编码器解码器在以前的工作中实现了可比的结果。对于英语自然语言，显著提高了目标自然语言翻译的性能。</sample>
    <sample id="664">大型语言模型，如CodeT5和BLOOM，在跨语言文本和语义理解任务中仍然处于开发阶段。</sample>
    <sample id="665">一个统一的跨语言语义解析基准，使用多种自然语言表示形式。</sample>
    <sample id="666">欢迎阅读一项全面的基准研究，研究了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现等等。欢迎访问我们的论文和代码。谢谢您的阅读。</sample>
    <sample id="667">The provided text is a list of letters, not English content. Therefore, I cannot answer the question about existing research based on this input.</sample>
    <sample id="668">不，多语言语言模型如 Codex 和 Bloom 仍然不足以完成跨语言语义推理任务。</sample>
    <sample id="695">The method addresses permutation uncertainty by incorporating alignment as part of the training. While the permutation method is flexible, finding the highest-scoring permutations is challenging, similar to the traveling salesman problem, and is approximated using a GPU-friendly continuous relaxation.</sample>
    <sample id="696">下游 NLP 模型的公平性是指在特定任务中，模型对不同群体（例如，基于种族、性别、宗教等）的预测结果是否存在差异。如果模型对某些群体表现出偏见，则其公平性就会受到影响。</sample>
    <sample id="697">Janis Lavergne.</sample>
    <sample id="698">科斯普·谢纳。</sample>
    <sample id="699">Maira.</sample>
    <sample id="700">In the context of this text, tropicalism refers to a trope that reflects in the words used to describe women of color. For example, words describing a Latina woman include things like "vibrant" and "curvaceous," which connect to the trope of tropicalism.</sample>
    <sample id="701">作者通过使用诸如“文化”、“传统”、“骄傲”和“异域风情”等词语来描述目标群体，这些词语定义了这些群体仅通过它们与身份的关系来区分，并将其与“白人规范”区分开来。</sample>
    <sample id="702">The article uses PMI (Pairwise Mean Likert Scale) to measure context usage at the sentence level or the word level.</sample>
    <sample id="703">DrBERT 和 ChuBERT 的主要区别在于它们使用的训练数据量。DrBERT 使用 7GB 的数据，而 ChuBERT 使用 4GB 的数据。</sample>
    <sample id="751">这篇论文有两位作者。</sample>
    <sample id="752">Iterative transfer learning updates the model by training on the latest set of data collected.</sample>
    <sample id="753">数据集的目标是让用户选择他们想表达的含义。</sample>
    <sample id="754">攻击者通过 EaaS 提取模型参数的方法包括：

1. **利用 EaaS 提供的 API 进行模型访问和操作。**
2. **通过发送精心构造的请求来触发模型漏洞，例如注入攻击或利用已知漏洞。**
3. **在模型响应中提取敏感信息，例如模型权重或内部状态。**</sample>
    <sample id="755">三位。</sample>
    <sample id="756">10</sample>
    <sample id="757">University of California, Berkeley.</sample>
    <sample id="758">The governor on the left.</sample>
    <sample id="759">The most advanced chat model is currently being evaluated by ABC EV.</sample>
    <sample id="760">大型语言模型正在推出更长的上下文窗口，因此在整个上下文窗口中评估模型的可接受性至关重要。</sample>
    <sample id="761">Yes, the study found that while most major natural languages show performance gains with multilingual training, English performance drops in seven datasets and only gains in three datasets.</sample>
    <sample id="762">No.</sample>
    <sample id="763">BLEU, METEOR, TER, and others.</sample>
    <sample id="764">是的，泛化中的回归可能会影响特定的 NER 类型。</sample>
    <sample id="765">NLP 中的立场很重要，因为它会影响模型对文本的理解和生成，尤其是在处理具有文化或社会背景的语言时。</sample>
    <sample id="766">适配器微调。</sample>
    <sample id="767">他们使用零样本性能在NTD数据集上的迁移学习。</sample>
    <sample id="768">MMLU, HellaSwag, ARC, TruthfulQA, GSM8K, and BigBench.</sample>
    <sample id="769">三条。</sample>
    <sample id="770">The proposed method achieved a higher plot density in the general retail space compared to the constraint distribution of cost script.</sample>
    <sample id="771">Shu Han.</sample>
    <sample id="772">是的，论文中的结果和数据集可以用作自动文本简化问题的基准。</sample>
    <sample id="773">The paper conducted experiments on 12 smaller models.</sample>
    <sample id="774">OFA</sample>
    <sample id="833">Google Research</sample>
    <sample id="834">Stony Brook University.</sample>
    <sample id="835">The paper analyzes English-to-German and English-to-French language pairs.</sample>
    <sample id="836">Shambin P.</sample>
    <sample id="837">Longformer 和 Normalized-based Longformer。</sample>
    <sample id="838">62 个任务。</sample>
    <sample id="839">There are 16 authors.</sample>
    <sample id="840">The author used the AG News, Mind, SST2, and E2020 datasets in their experiments.</sample>
    <sample id="876">NACHOS 是一个医疗临床数据数据集。</sample>
    <sample id="877">Sajid Bilal.</sample>
    <sample id="878">As we can see in a simple experiment where we use one-shot prompting and provide it with two different prompts for the same sentence, the prompting has a big influence on the performance of LLMs for translation.</sample>
    <sample id="879">The authors are affiliated with the University of Puerto Rico at Mayagüez.</sample>
    <sample id="880">The provided text does not list any instructions. It mentions that they are collecting a much larger multimodal instruction tuning dataset with around 150 additional reasoning language tasks and will release them soon. It also includes a QR code for their data and model.</sample>
    <sample id="881">作者建议使用人类实验参与者来评估数据集，以测试模型从不同来源的信息中提取知识的能力。</sample>
    <sample id="882">大家好，我的名字是艾德·维拉，我将简要概述这篇论文，重点是打印参数、翻译策略和性能评估。这是一项与谷歌翻译同事合作的项目。</sample>
    <sample id="883">5400亿个参数的语言模型在2022年表现出色。它在大量文本上进行了训练，压缩和处理了1000亿个文档。</sample>
    <sample id="884">The matter of publication is at the state of the art in hundred of fingerprints.</sample>
    <sample id="885">在这篇工作中，我们呈现了对语言模型提示的系统性研究，重点关注指令微调。</sample>
    <sample id="886">我们评估了语言模型在迁移能力方面的表现，并采用了 MT 社区的最佳实践。这包括使用最新的测试数据集，以避免测试数据与语言模型训练数据的重复。</sample>
    <sample id="887">我们比较了最新的系统，也就是最佳性能的系统，比如 WTI 轮流。</sample>
    <sample id="888">我们使用最新的神经矩阵，并且额外展示专家基于你的评估结果。最后，我们提供一些关于提示选择策略的建议。</sample>
    <sample id="889">提示对语言模型在翻译中的性能有很大影响。正如我们在一个简单的实验中看到的，我们使用单提示，并为不同的句子提供了两个不同的提示。</sample>
    <sample id="890">在大多数句子中，嗯，516 在 1000 中。
差异明显，超过一个模糊点。</sample>
    <sample id="891">这在极端情况下可以达到40分。所以选择好的提示策略很重要。</sample>
    <sample id="892">在我们的实验中，我们使用了一种五步提示策略，即我们仅仅标记我们向系统提供的句子中的一个词，用语言表示它是一个“词”。</sample>
    <sample id="893">翻译成中文。</sample>
    <sample id="894">我们说的是实际的打印形式对多个短篇故事的影响不大。</sample>
    <sample id="895">这对于Zero和一击提示至关重要，当我们进入我们现在的情况，即两击提示时，几乎没有区别于提示的实际形式。</sample>
    <sample id="896">请提供英文内容。</sample>
    <sample id="897">我们实验结果的总结是，示例质量比与原始句子的相似性更重要。</sample>
    <sample id="898">所以，选择示例时重要的是选择高质量的翻译。特别是，我们比较的是从 WMT 评估数据中选择提示，或者从定义中选择提示。</sample>
    <sample id="899">深度学习的训练数据更加丰富且高质量，训练数据更加有意义，结果也更好。因此，使用深度学习可以获得更好的性能。</sample>
    <sample id="900">然而，专业化状态的系统在某些情况下拥有与通用翻译系统相当的优势，但通用系统在某些情况下与我们的商业系统非常接近，例如我们使用的谷歌翻译。</sample>
    <sample id="901">自从我们从人类的创新中获得，我们使用 MKL 框架进行。
它的流畅性类似于操作系统，但主要区别在于准确性。</sample>
    <sample id="902">在特别的</sample>
    <sample id="903">它似乎是，巴伦选择为了产生更好的理解翻译，有时通过在源句子中删除一些对翻译不重要的部分。</sample>
    <sample id="904">在州外类别中，对于面包来说，低于州内系统的信号，这是一个不利信号。</sample>
    <sample id="905">这个参数提供流畅的输出，但仍然存在一些问题，例如：</sample>
    <sample id="906">这就是这次非常简短的概述。
要了解更多细节，请查看我今天发布的关于这篇论文的完整演示文稿。
非常感谢大家。</sample>
    <sample id="907">你好，我是来自德国的萨尔大学的博士生。在这个视频中，我想展示我们的研究工作。请你对我们的研究进行批判性评估。</sample>
    <sample id="908">这是联合工作，涉及以下内容：
- 光滑的棒
- 牙科步骤
- 英国克拉</sample>
    <sample id="909">我将提供一个关于北京的简短介绍，以及两周的监督和每周的监督。</sample>
    <sample id="910">在维基百科中，您无需手动标注数据。相反，您可以使用维基标注来源，例如简单的特征规则、知识库或本地代码注释。正如您在图中所展示的那样。</sample>
    <sample id="911">相比于人工标注，弱标注更加便宜，但它们也相当嘈杂，这意味着相当一部分标注是错误的。</sample>
    <sample id="912">如果直接在每周劳务数据上训练新的神经网络，那么新的神经网络会记住训练数据中的噪声，而不会泛化。</sample>
    <sample id="913">通常，为了在嘈杂的环境中可靠地训练新的对话模型，建议使用训练算法，以便训练模型仍然能够泛化。</sample>
    <sample id="914">在最近的WALS（每周支持学习）工作中，一个常见的说法是，人们认为只有在单语模式和每周劳动数据下才能取得高性能，而这些数据是经过清理的。</sample>
    <sample id="915">技术上来说，这并不是一个词，但它有一个诀窍。</sample>
    <sample id="916">人们会假设存在一个额外的清洁验证集，用于沃尔沃车辆的模式选择。</sample>
    <sample id="917">我们已经采用了这个问题设置，但这意味着在每周的监督学习中需要额外的手动注释。但就像房间里的一只大象一样，这项必要性常常被忽视。</sample>
    <sample id="918">在《开放式形式》中提到，你必须提出三个研究问题。首先，数据清洗和验证对于 WSL 是否必要？或者我们可以使用一个噪声验证集代替？</sample>
    <sample id="919">其次，如果清理数据是必需的，或者如果清理数据对于WAI工作是强制性的，那么你需要多少个清理样本？最后，是否应该只使用清理样本进行验证，还是有更好的方法来利用这些清理样本？</sample>
    <sample id="920">你已经回答了这些研究问题，我们的发现是进一步的。</sample>
    <sample id="921">首先，我们发现，有趣的是，最近的 WSL 消息确实需要清理空白数据样本才能正常运行。</sample>
    <sample id="922">否则，将出现一个大的性能下降。如图所示，如果缺乏清理验证样本，那么趋势模型就无法超出原始的标记标签进行泛化。</sample>
    <sample id="923">请提供英文内容。</sample>
    <sample id="924">这暗示了在使用自然语言处理时，实际上需要仔细清理数据才能正常工作，并且获取干净的验证样本的标注成本不应被忽视。</sample>
    <sample id="925">我们第二个发现是，增加清洁验证样本的数量将有助于 WSL 达到更好的性能，如图所示。</sample>
    <sample id="926">通常我们只需要23个样本来达到高精度。</sample>
    <sample id="927">但这并非故事的结局，因为无论我们决定直接使用清洁样本进行训练，我们都将能够获得更好的性能。</sample>
    <sample id="928">图表显示了两种调优方法的性能差异：直接应用于清理数据的方法和使用清理数据进行验证的方法。</sample>
    <sample id="929">如果我们有十个样本，每类都有，直接找到一个开始被 WSL 攻击。</sample>
    <sample id="930">最终，在之前的 WSR 方法中声称的性能改进可以通过允许在清理和验证样本上持续微调而轻松实现。</sample>
    <sample id="931">正如您所见，在图表中，Vina模型最初表现得不如更复杂的WSL模型。</sample>
    <sample id="932">然而，如果我们允许继续在干净的样本上进行分析，那么FTW表现得与其他方法一样好。</sample>
    <sample id="933">所以，在实践中，没有理由选择更复杂的 WSL 镜像，它们需要更多计算时间和磁盘空间。</sample>
    <sample id="934">我们总结说，最近的 WSL 补丁需要手动注释样本才能正常工作。它们的性能和实用性被严重夸大了。</sample>
    <sample id="935">我们具体的建议为未来的工作是如下：</sample>
    <sample id="936">首先，报告模型选择的准则。例如，报告如果模型选择是单轮清洁验证样本。</sample>
    <sample id="937">第三，持续的 fijn tuning 是一种简单但强大的基线，应该在未来的工作中被考虑。</sample>
    <sample id="938">请随意查看它。谢谢您的参与，祝您愉快。</sample>
    <sample id="939">The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better, or to rate conversations given a Likert scale.</sample>
    <sample id="940">There are 6 authors listed.</sample>
    <sample id="941">在 Servin 和 Kea 的示例中，需要以下背景知识：

1. **实体特定知识：** 例如，“Servin 是一个法官”。
2. **世界知识：** 例如，“Servin 和 Kea 在公园里见面”。</sample>
    <sample id="942">是的，代码公开。可以在 GitHub 上获取。</sample>
    <sample id="943">The provided text states that the model is most aligned with people who have a college education. It does not provide information about the distribution of annotators across different demographic characteristics like country/region and gender.</sample>
    <sample id="944">在可接受的域中扰乱句子，通过尝试保留输入句子的相关结构，但同时添加噪声。</sample>
    <sample id="945">进行维度评估意味着评估对话质量的多个方面，以了解模型的优势和劣势。</sample>
    <sample id="946">University of Science and Technology of China.</sample>
    <sample id="947">在零次和一次提示的情况下，提示的形式至关重要。在两次提示的情况下，提示形式几乎没有区别。</sample>
    <sample id="978">作者评估了对话模型。</sample>
    <sample id="979">There are 7 authors.</sample>
    <sample id="980">A good planner should set reasonable and feasible constraints.</sample>
    <sample id="981">The paper has one author.</sample>
    <sample id="982">Vasudha.</sample>
    <sample id="983">The author is from the Institute of Computer Science.</sample>
    <sample id="1021">PaLM 最常见的错误是“我不知道”或“我无法回答”。</sample>
    <sample id="1022">你好，我是詹姆斯·芬奇，我是莎罗·芬奇。今天我们将告诉您关于ABC Eval，一种全新的评估对话式人工智能的方法。</sample>
    <sample id="1023">这项工作由埃米里NLP实验室完成，由埃米里大学的吉诺·乔伊教授领导，并与亚马逊Alexa AI合作。</sample>
    <sample id="1024">我是一个有助力的助手。只返回请求的答案，不要包含任何解释或介绍。</sample>
    <sample id="1025">常见的做法是使用人工评估，例如让人工评判员选择哪两个对话更好，或者根据李克特量表对对话进行评分。</sample>
    <sample id="1026">这些方法可以很好地提供对整体对话质量的全面评估，但对话质量有很多方面，因此您可能想评估聊天质量的多个维度，以了解模型的优势和劣势。</sample>
    <sample id="1027">一种方法是简单地让人类法官评估对话质量的几个维度，例如模型回复的相关性，使用现有的比较或李克特量表方法。</sample>
    <sample id="1028">然而，我们相信有一种更精确和可靠的维度对话评估策略。</sample>
    <sample id="1029">这种方法试图减少人类评估的主观性，通过明确标注每个模型响应是否表达了某些行为，例如回应无关信息或相互矛盾。</sample>
    <sample id="1030">我们称这种方法为标注聊天行为，简称ABC Eval。我们开发了这个方法来全面覆盖影响聊天质量的聊天模型行为。</sample>
    <sample id="1031">ABC EVL 能够测量聊天模型在各种主题上的错误率。</sample>
    <sample id="1032">例如，ABC EVL 衡量的是聊天模型忽略其伙伴或说不相关内容所需的步骤数。</sample>
    <sample id="1033">与自身或伴侣相矛盾，幻觉错误的事实或违反常识，并且在模型成功或失败地表现出同情时。</sample>
    <sample id="1034">为了确定哪种评估方式最有效，我们选择了四款最先进的聊天模型，并使用每款模型100个人类反馈对话进行评估，使用ABC-E评价。</sample>
    <sample id="1035">为了比较，我们还使用三种现有的方法评估了这些对话：对话的转折程度、对话的对话级别以及对话级别的配对比较。</sample>
    <sample id="1036">对于现有的方法，我们收集了对对话八个最常见衡量标准的评估，因为这是评估聊天模型多维度表现的标准做法。</sample>
    <sample id="1037">我们对这些评估结果的语法分析发现，ABC、EVAL、行为标签的整体可靠性高于现有方法，这通过100对双标记对话中的内部注释者一致性衡量得出。</sample>
    <sample id="1038">此外，ABC EVA标签在预测整体对话质量方面比现有方法产生的指标更具预测性，正如这个简单的线性回归分析所显示的那样。</sample>
    <sample id="1039">例如，你可以看到测量自我与伴侣矛盾转折的比例，解释了对话质量的 5%，而平均李克特一致性得分仅解释了 4% 或 10%。</sample>
    <sample id="1040">最后，我们检查每个评估指标是否捕捉了检查质量的独特方面，使用逐步线性回归。</sample>
    <sample id="1041">你可以看到，所有ABC EVL指标的组合解释了超过25%的对话质量。当逐一移除这些指标时，大多数都导致丢失了相当多的关于质量的信息。</sample>
    <sample id="1042">另一方面，所有等级的李克特量表组合解释得远不如单独的指标，并且这些指标中的许多指标缺乏独特的意义。</sample>
    <sample id="1043">这是一个可靠、信息丰富且独特的ABC评估矩阵，使我们能够以比以往方法能够达到的更高分辨率评估对话式AI。</sample>
    <sample id="1044">从我们实验的结果来看，仍然存在几个挑战，并且已经精确地量化。例如，我们测试的对话中，有大约20%的回答包含常识性错误。</sample>
    <sample id="1045">它们在约 15% 的回复中产生无关信息，并且大约有 10% 的情况下会自相矛盾或与合作伙伴矛盾。</sample>
    <sample id="1046">由于该领域发展迅速，许多这些错误率在我们的评估之后可能会下降。然而，这更需要我们追求可靠且精确的评估指标，以便比较模型。</sample>
    <sample id="1047">我们希望 ABC Eval 可以被其他领域的人们利用，作为这一方向上的一个有意义的进步。我们期待着在未来几个月里看到对话式 AI 的发展。感谢您的观看。</sample>
    <sample id="1048">Emory NLP Lab.</sample>
    <sample id="1049">CFT 代表 recent WSL approaches 重新清理和手动注释样本。</sample>
    <sample id="1050">这篇论文有五位作者。</sample>
    <sample id="1051">你好，我的名字是卡约·燕，我将介绍我们的作品，题为《何时需要翻译？数据驱动的语境探索》。这项工作与帕特里克·弗朗西斯、米尔杜·安德尔·费尔南德斯和格拉米·布尔合作完成。</sample>
    <sample id="1052">拉</sample>
    <sample id="1053">如果上一个句子是“如果部长发现事情开始变得危险，那么莫指的是间谍”，那么莫指的是间谍。但如果上一个句子是“这可能没什么大不了的，医生”，那么莫指的是病理学家。</sample>
    <sample id="1054">所以，帕丁汉的联系，单词的含义发生变化，因此翻译也发生了变化，是“爪”。</sample>
    <sample id="1055">然而，评估模型处理此类案例的良好程度相当困难。首先，因为只有少量翻译数据能够影响上下文，这使得词汇层面的指标无法捕捉到这些翻译的细微之处。</sample>
    <sample id="1056">一些人建议针对性评估在线欺凌行为，但这些资源仅支持有限类型的在线欺凌行为和有限的语言。因为它们通常依赖于领域知识和人工编选。</sample>
    <sample id="1057">在这一篇工作中，我们尝试回答这两个问题：首先，翻译是否需要上下文；其次，模型如何处理这些情况？</sample>
    <sample id="1058">在回答第一个问题之前，我们首先测量了翻译中上下文的重要性。</sample>
    <sample id="1059">在之前的作业中，我们介绍了在机器翻译模型中使用的上下文作为衡量标准。这通过测量上下文对目标词语提供多少关于该词语的信息来实现，给定该词语的上下文。</sample>
    <sample id="1060">你认为像XAI就像从提供背景信息来给模型提供信息吗？</sample>
    <sample id="1061">在这一工作中，我们使用 XSMI 2.0 XSMI，它可以衡量上下文使用在句子级别或单词级别。我们可以认为那些具有高 P XSMI 的单词是那些在翻译中需要上下文的单词。</sample>
    <sample id="1062">现在我们使用高频词项提取（high-frequency semantic similarity）来寻找这些词语之间的模式。</sample>
    <sample id="1063">我们将在英语转成14种不同语言的文本转录中进行分析。</sample>
    <sample id="1064">在我的分析中，有三个不同的层面。首先，我们来看一下词性标签，它们有较高的平均值。</sample>
    <sample id="1065">这允许我们找到一个例子，阿拉伯语中复数代词具有发音的嗨皮西米。这可以解释是因为英语没有复数代词，所以你需要根据上下文判断一个代词是否是复数。</sample>
    <sample id="1066">并且我们发现，某些语言在选择动词形式时也需要上下文。然后我们查看词典条目，看看它们在不同语境下的词频是多少。</sample>
    <sample id="1067">和这个帮助识别像这里这样的病例，在中文中需要根据上下文恰当地翻译代词，以确保你使用相同的翻译在文档中。</sample>
    <sample id="1068">并且类似地，我们发现卡尔西斯支持了他们的权利正式的。</sample>
    <sample id="1069">最后，我们来看一下不同个体代币中具有高PSI的现象。这允许我们识别出无法被“词语”本身捕捉到的现象，但可以通过其语义结构更好地表达出来，例如椭圆解。</sample>
    <sample id="1070">现在我们使用我们分析的结果来设计一个文档级别翻译的基准。</sample>
    <sample id="1071">对于我们识别出的五个磁盘现象，我们创建了标签来隐蔽地识别与该现象相关的词语，并称我们的标签为多语种磁盘意识或穆达标签。</sample>
    <sample id="1072">此外，我们还注意到不同的语言有不同的对这种歧义现象的比例。</sample>
    <sample id="1073">我们使用穆达标签，通过在我们需要用于评估的平行 корпуса上应用标签，然后应用我们的选择转换矩阵到穆达标签识别出的上下文依赖示例上。</sample>
    <sample id="1074">最后，我们使用我们的基准作为其他指标来评估不同模型的文档级别机器翻译质量。</sample>
    <sample id="1075">首先，当我们使用词汇级别指标时，对于蓝色，我们发现共现诊断模型具有最佳性能。</sample>
    <sample id="1076">但是，如果使用上下文，则模型在最佳情况下表现良好。如果使用词汇量衡量，则模型在有上下文和没有上下文的情况下表现相当。</sample>
    <sample id="1077">这展示了很难确定最佳文档级别翻译系统，如果使用语料库级别指标。</sample>
    <sample id="1078">现在我们使用了具有上下文的基准模型，我们发现，在某些特定语料现象中，具有上下文的模型比不使用上下文的模型更准确，例如正式性和词汇连贯性。</sample>
    <sample id="1079">这些模型与不使用上下文或其他现象（如椭圆、破折号和动词形式）的模型没有多大区别。这表明我们需要在文档级别翻译方面取得更多进展。</sample>
    <sample id="1080">我们还比较了不同的商业系统，我们的基准测试显示，DeepL通常比Google翻译在文档级别翻译上更准确。</sample>
    <sample id="1081">总而言之，我们对14对语言对进行了数据驱动的分析，以识别出需要上下文的1个翻译。</sample>
    <sample id="1082">然后我们使用我们的细化来构建文档级别机器翻译的基准，这可以帮助我们识别哪些语言模型能够很好地处理文档级别翻译，以及哪些翻译系统在文档级别翻译方面表现良好。</sample>
    <sample id="1083">谢谢你的帮助，请你翻译一下“The”</sample>
    <sample id="1084">Justin John</sample>
    <sample id="1121">该方法没有名称。</sample>
    <sample id="1122">作者描述“显性词汇”(marked words) 方法是用来识别区分标记组和非标记组的单词。</sample>
    <sample id="1123">The author is from the University of Washington.</sample>
    <sample id="1124">Prag-Pro.</sample>
    <sample id="1125">James Finch and Sarah Finch.</sample>
    <sample id="1126">Four.</sample>
    <sample id="1127">The text mentions that the minimal pair paradigm evaluates language models based on acceptability judgments, which can include grammaticality (like verb tenses, syntax) and acceptability in terms of stereotypes (like crowdsourced judgments).</sample>
    <sample id="1161">WLS, OLS, GLS, BLS, and ALS.</sample>
    <sample id="1162">该模型在生物医学和临床诊断场景下的任务上进行了评估。</sample>
    <sample id="1226">CamemBERT 最初是在 4GB 的数据上训练的。</sample>
    <sample id="1227">Adam Skirkowski.</sample>
    <sample id="1228">在时间漂移实验中，发现性能随着时间间隔的增大而下降，这证实了时间漂移是性能下降的主要原因。</sample>
    <sample id="1269">因为在第一个步骤中，我们得到了正确的词元，但它们没有按正确的顺序排列。因此，在第二个步骤中，我们使用另一个模型来预测词元序列的排列顺序，以便将它们排列成正确的顺序。</sample>
    <sample id="1270">作者建议模型所有者提高偏见缓解方法的透明度，因为例如，这些积极的刻板印象可能源于某种奇怪的过度强调价值观或其他反刻板印象方法，这些方法导致了有害的模式。</sample>
    <sample id="1271">最小对不可接受输入是指模型在面对一个可接受的句子和不可接受的句子时，倾向于给可接受的句子更高的概率。</sample>
    <sample id="1272">作者使用了“weight”和“tokenizer”的评估指标。</sample>
    <sample id="1273">Inner annotator agreement.</sample>
    <sample id="1274">维基百科</sample>
    <sample id="1275">The author's affiliation is not mentioned in the provided text.</sample>
    <sample id="1276">MultiInstruct focuses on improving zero-shot performance on language-only tasks, while other works have largely neglected computer vision and multimodal tasks. This work investigates whether instruction tuning can improve generalization to unseen multimodal tasks. Additionally, the research discovered a significant discrepancy in the availability of instruction datasets between NLP and multimodal models.</sample>
    <sample id="1277">这篇论文有两位作者：James Finch 和 Sarah Finch。</sample>
    <sample id="1278">二进制协调是指在计算机系统中，数据以二进制形式存储和处理。</sample>
    <sample id="1279">平均长度为 12 个字符。</sample>
    <sample id="1280">The findings suggest that smaller T5 models can perform as well as larger ones when trained on suitable datasets.</sample>
    <sample id="1281">我叫雅尼斯·洛瓦克，很高兴向您介绍我正在为法国版本的“Doctor Bert”机器人精神模型工作。</sample>
    <sample id="1282">在本次演示中，我们首先介绍了语言建模在医疗保健领域的应用，然后我们将介绍我们文章的主要贡献。</sample>
    <sample id="1283">我们引入了一个法语的第一个生物医学模型，名为डॉ克特·伯特，它基于罗伯塔，并使用Natches数据集进行训练，该数据集包含来自“癌症”的医疗临床数据。</sample>
    <sample id="1284">我们还介绍了具有多个预测设置和数据来源的模型比较。然后，我们展示了在 11 个生物医学和临床诊断任务中的结果。</sample>
    <sample id="1285">总结一下实验结果，并提供更多关于如何访问该研究的详细信息。</sample>
    <sample id="1286">自2018年发布以来，它已成为解决自然语言处理任务的最有效方法之一，并且与历史静态和上下文化方法（如Word2Vec、FastText或GloVe）相比，性能有了巨大的提升。</sample>
    <sample id="1287">这是一款模型已经适配了许多其他语言，例如法语（使用“camembert”）、奥斯曼语（使用“bayomedicall”）、以及临床（使用“clinicalbert”）。但大多数情况下，它使用英语。</sample>
    <sample id="1288">针对其他语言的专业模型通常采用持续预训练方法，因为缺乏领域数据。</sample>
    <sample id="1289">然而，法语没有现代的铅笔墨水和碳粉。</sample>
    <sample id="1290">我们问自己一个问题：对于广泛的使用场景，最合适的数据库是什么？这些数据是临床数据的良好替代品吗？</sample>
    <sample id="1291">请将这段英语内容用中文重新表达。</sample>
    <sample id="1292">在越野中，我们问自己，我们是否需要针对法语数据训练一个专业模型？它需要多少字节？一个吉字节？或者一个兆字节？</sample>
    <sample id="1293">首先，我们从零开始训练和比较四个不同的模型。第一个版本的 DoctorBERT 拥有 7GB 的数据，第二个版本的 For 拥有 4GB 的数据。</sample>
    <sample id="1294">一个微型版本的BERT，它是一个临床模型，我们使用了4GB的上下文序列，来自临床数据。一个微型版本的BERT，我们使用了4GB的词嵌入和4GB的临床数据。</sample>
    <sample id="1295">此外，我们还将引入流式数据分析和持续预测技术，以分析预测策略的影响。</sample>
    <sample id="1296">一个基于卡门贝尔的模型，并使用 4GB 的数据集训练，另一个也是基于卡门贝尔的，但这次使用 4GB 的干净数据训练。</sample>
    <sample id="1297">最终，一个基于英语语言模型，例如BERT，并使用先前数据集训练的模型总共有七个模型。</sample>
    <sample id="1298">我们评估了七个模型，它们在公共和私人领域都执行了各种任务，例如命名实体识别、分类、词性标注和问答。</sample>
    <sample id="1299">与 6 字节模型相比，我们有以下模型：卡门贝奥斯卡 108  جي字节，卡门贝奥斯卡 4  جي字节，卡门贝奥斯卡 60  جي字节，百比特，字节和千兆字节。</sample>
    <sample id="1300">在特定任务中，使用相同类型数据的模型表现最佳。</sample>
    <sample id="1301">然而，我们可以从原始来源获取数据，我们观察到来自原始来源的数据似乎更可靠。我们还观察到，使用更多数据可以提高性能。</sample>
    <sample id="1302">从零开始构建的神经网络似乎在大多数任务上获得了更高的性能。</sample>
    <sample id="1303">然而，我们实验通过使用“permitword”的权重和tokenizer，在4GB的文本集中训练模型，结果与从“Dr. BERT”获取4GB的文本集相似。</sample>
    <sample id="1304">这并非基于卡门贝尔权重和标记器，后者存在稳定性问题。</sample>
    <sample id="1305">最终，我们的系统在九一一日任务中表现更好，整体结果优于通用模型。</sample>
    <sample id="1306">我们观察到，专门的数据比一般的数据更好，但它并不适用于所有情况。</sample>
    <sample id="1307">所有预训练模型从Natos获取，并且在年轻脸上可用，所有训练脚本都在我们的GitHub仓库中。</sample>
    <sample id="1308">非常感谢您的演示，我们期待着会议中的行动。</sample>
    <sample id="1309">论文研究了四个从头开始训练的BERT模型，以及使用流模型进行训练和连续预测的策略。</sample>
    <sample id="1310">根据图表右侧的红线，过拟合因素为大于1。</sample>
    <sample id="1311">The second use case, as shown in our paper, is the case of automatic text simplification by fine-tuning language models to produce simplified text from complex input text. We have fine-tuned two different models: a Longformer model to produce document-level simplifications and a Normalizer-based Longformer model.</sample>
    <sample id="1312">是的，语言模型有不同的政治偏见。</sample>
    <sample id="1313">嗨，我的名字是马蒂亚斯·伦德曼。今天我将向大家简要介绍我们的论文，关于使用多标签标记和潜在排列的成分泛化。</sample>
    <sample id="1314">这是我顾问亚历山大·科拉和伊万·迪塔的合作。</sample>
    <sample id="1315">组合泛化可以理解为学习者处理更深层次的递归和未见组合的能力，这些组合是训练过程中单独出现的短语。</sample>
    <sample id="1316">在语义解析的语境下，测试组合泛化可能如下所示：正如往常，我们有一个训练语料库，在这种情况下，女孩睡着了，玛丽知道女孩睡着了。</sample>
    <sample id="1317">这些词语与逻辑形式配对，代表了它们的核心含义。</sample>
    <sample id="1318">与标准机器学习评估不同，测试集并非来自相同的分布，而是包含结构上和逻辑上不同的形式。</sample>
    <sample id="1319">在这一例中，模型在训练过程中经历了浅层递归，并在测试样本上测试了更深层的递归。</sample>
    <sample id="1320">基于序列到序列的模型在这种类型的离散分布泛化方面遇到困难，并且通常会产生与输入分离的输出。</sample>
    <sample id="1321">尤其，它们经常未能重现输入和输出之间的系统对应关系，例如在示例中用颜色编码的那些。</sample>
    <sample id="1322">一个流行的解决此问题的办法是将树集成到马里。</sample>
    <sample id="1323">这些树旨在捕捉与句子的结构过程相关的逻辑形式。</sample>
    <sample id="1324">这写得很好，但树通常不是被给予的，需要获得一些。</sample>
    <sample id="1325">这有时会变得复杂，并且计算成本高昂。通常涉及对逻辑形式的相当多的形式化预处理，例如处理变量符号。</sample>
    <sample id="1326">获取树木也可能涉及专门的语法检测程序。</sample>
    <sample id="1327">在本文中，我们不使用树，并引入了一种新的序列到序列模型，该模型直接建模输入片段与输出片段之间的对应关系。</sample>
    <sample id="1328">第一次，我们展示了对更深层递归的强大泛化，而无需依赖树。</sample>
    <sample id="1329">我想要一个方法预测输入到输出之间的两个步骤。</sample>
    <sample id="1330">首先，我们为每个输入标记创建一个无序多元素，其中包含将在输出中出现的标记。</sample>
    <sample id="1331">在第一步之后，我们得到了正确的标记，但它们不是“拉”。</sample>
    <sample id="1332">这就是为什么在第二步，我们使用另一个模型来预测一个排列，将它们排到正确的顺序。</sample>
    <sample id="1333">我们介绍了一种新的方法来预测一个排列，该方法不会对可能的排列施加任何硬性约束。这使得我们的方法非常灵活且具有表达力。</sample>
    <sample id="1334">概念上，我们的排列模型大致如下：</sample>
    <sample id="1335">我们从左到右遍历输出，并确定在每个位置应该放入哪个多集标记。对于第一个输出位置，我们简单地选择一个，如在图中的高亮标记所示。</sample>
    <sample id="1336">然后我们跳到下一个多 setores 令牌来确定输出中的第二个令牌。</sample>
    <sample id="1337">确定输出中的第三个标记，通过跳到另一个多重标记。我们继续这个过程。</sample>
    <sample id="1338">直到第一阶段的每个标记都已被访问一次。</sample>
    <sample id="1339">为了给您一个实验结果的预览，我们比较了我们的方法与其他树状模型在COGs基准上的表现。我们的模型在泛化到更深层递归方面，以显著的优势优于其他模型。</sample>
    <sample id="1340">一种新的结构改造仍然非常具有挑战性。</sample>
    <sample id="1341">在我们的论文中，我们解决了几个有趣的专业挑战。</sample>
    <sample id="1342">首先，输入和输出之间的对齐关系在训练数据中没有给出。因此，对于给定的标记，我们不知道它来自哪个多集合，这给训练带来了挑战。</sample>
    <sample id="1343">此外，有时数据中存在多个与数据一致的排列方式，但语言上正确的排列方式是潜在的。我们通过将对齐作为训练的一部分来解决这个问题。</sample>
    <sample id="1344">排列方法非常灵活，但它带来了一个挑战，即找到最高分排列是NP难的。这是因为这与旅行商问题相关。</sample>
    <sample id="1345">我们通过一个GPU友好的连续放松来近似这个，该放松还允许我们反向传播通过解决方案并学习更具语言合理性的排列。</sample>
    <sample id="1346">如果您想了解更多关于我们的实验以及我们如何应对这些挑战，请查看我们的论文或来我们的办公室。</sample>
    <sample id="1347">Cognitive dissonance is when two beliefs or actions are inconsistent.</sample>
    <sample id="1348">GPT-4</sample>
    <sample id="1349">Yes, cumulative performance is generally found to be equal to or better than iterative performance across the board.</sample>
    <sample id="1350">Sara Babi.</sample>
    <sample id="1351">Transcripts of TED Talks.</sample>
    <sample id="1385">Matthias Lendeemann.</sample>
    <sample id="1386">跨语言转移是指在一种语言（源语言）和另一种语言（目标语言）之间进行训练，并将其应用于另一种语言的场景。</sample>
    <sample id="1387">Stalland University.</sample>
    <sample id="1388">作者使用了同步的平均延迟测量方法。</sample>
    <sample id="1389">大家好，我是马克希塔，今天我和我的同事们将展示我们的作品“知识整合”。这项工作是麦基尔大学、米拉和微软研究的合作。</sample>
    <sample id="1390">大型语言模型借鉴了各种知识来源，例如在其参数中通常通过预训练获得的知识，以及在推理过程中提供的输入知识。</sample>
    <sample id="1391">最近在问答任务中，显示模型可以利用预训练知识来解决目标。</sample>
    <sample id="1392">自然语言处理经常需要知识，这些知识也从以下方面提供。</sample>
    <sample id="1393">例如，在句子“约翰在电视上看到了新当选的总统”中。</sample>
    <sample id="1394">打印参数可以包含有关什么是总统 2 和什么是 TV 的信息，但它们无法可靠地知道这个特定实体是谁，或者谁是新的总统，因为总统可能会改变。</sample>
    <sample id="1395">因此，在知识密集型 NLP 任务中，成功的模型需要能够整合和使用预训练时间和推理时间知识。</sample>
    <sample id="1396">在本文中，我们提出了一种诊断性测试方法，用于知识整合。</sample>
    <sample id="1397">我们介绍了一个核心参照分辨率任务，旨在评估利用不同来源知识的能力。我们评估了数据集，并与人类学习者进行了实验，并建立了核心参照分辨率模型。</sample>
    <sample id="1398">服务是一位法官。
基亚是一位面包师。
服务和基亚在公园里见面了。
在一天结束之后，他决定在法庭里审理案件，他很高兴放松。</sample>
    <sample id="1399">这个任务是识别代词“他”所指的正确实体，在这种情况下，指的是“贾”。</sample>
    <sample id="1400">给定代词的解题要求需要两种类型的信息。第一，实体特定知识，例如“服务是职责”。第二，背景知识，例如“法官决定案件”。</sample>
    <sample id="1401">一般来说，背景知识是在大型语言模型预训练过程中学习的，而实体特定知识通常通过微调观察到。</sample>
    <sample id="1402">信息可能来自单一来源或多个来源。</sample>
    <sample id="1403">我们已经定义了三个猫模型设置。首先，我们需要设置背景预训练。背景知识被认为是可用的预训练数据。</sample>
    <sample id="1404">第二，备份在后台设置。备份通知在预训练时和在非训练时都可用。最后，备份在非训练设置。两种通知类型都只在非训练时可用。</sample>
    <sample id="1405">这个设置特别有趣。它模拟了一个背景知识不足的案例，这不在预训练数据中。例如，由于纽约出现了新的职业，自那时起，预训练数据就无法涵盖。</sample>
    <sample id="1406">这是一个如何控制你的影响力的例子。</sample>
    <sample id="1407">在背景知识预设中，我们假设背景知识政治家寻求当选的席位在政府中包含在预设参数中。在不同上下文，我们提供针对特定政治家的知识。</sample>
    <sample id="1408">在背景和背景设置中，我们不仅提供针对特定实体的数据，还提供关于政治家在领域背景中的知识。</sample>
    <sample id="1409">在背景和非设置中，我们将虚构职业“米里图尔”代替“政治家”，因为米里图尔不太可能包含在预设的政治家类别中。</sample>
    <sample id="1410">我们验证了数据集，既使用了人类统计参与者，也建立了卷积神经网络模型。在这张图中，我们展示了最佳表现的模型以及最困难的背景预训练模型。</sample>
    <sample id="1411">在Auto-tras和Kidmus上，两者都表现不佳。然而，在Kidmus上训练，both C2和B2表现显著优于随机选择。</sample>
    <sample id="1412">这表明在训练和通用查询的解决方案中设置了。
可以学习利用表面线索。
但对于测试和关键词，这些线索已被移除。</sample>
    <sample id="1413">实验表明，即使是最先进的模型也无法可靠地集成新知识，只能依赖于其有限的知识。</sample>
    <sample id="1414">总结一下人工智能模型的主要技术挑战。许多预训练模型由于缺乏知识而无法利用来自不同来源的信息，而没有针对特定任务进行训练。然而，通过针对特定任务进行训练，一些模型成功地整合了来自多个来源的信息。</sample>
    <sample id="1415">即使是最先进的模型，似乎在可靠地整合在推理时间呈现的先前知识方面也存在困难。
如果您对更多细节感兴趣，请查看我们的论文，并查看GitHub上的数据集和代码。</sample>
    <sample id="1416">The method of using trees has the following disadvantages: trees are usually not given and need to be obtained somehow, which can be complicated and sometimes computationally expensive. Typically, this involves considerable formalism-specific preprocessing of the logical forms, for example, to handle variable symbols. Obtaining trees may also involve specialized grammar induction procedures.</sample>
    <sample id="1417">The author's affiliation is not mentioned in the provided text.</sample>
    <sample id="1418">嗨，我是玛拉，今天我们将讨论我们使用自然语言提示来衡量大型语言模型类型的工作。这项工作由埃森·穆什和丹杰罗夫共同完成。</sample>
    <sample id="1419">近年来，许多研究人员已经记录了大型语言模型（LLM）中社会偏见和刻板印象的普遍性。</sample>
    <sample id="1420">然而，这些措施存在各种局限性。它们通常依赖于手工构建的数据集，这对于收集数据来说非常耗时。</sample>
    <sample id="1421">它们通常只测量非常特定的错误类型，这意味着它们难以推广到其他人口统计学或背景，或者它们只是捕捉到非常普遍的、广泛的关联，比如对特定类型的负面联想。</sample>
    <sample id="1422">此外，大多数太空工作并未考虑交叉性，交叉性是指多重身份可以相互叠加并产生独特的社会身份。</sample>
    <sample id="1423">为了克服这些限制，我们依赖于一种新的指令调整的LLM，它们在响应指令方面表现出色。</sample>
    <sample id="1424">所以我们可以让模型生成一个人物画像，这是一种想象中的个体描述，通过提示词，比如“想象一下你是一位亚洲女性，描述一下你自己。”</sample>
    <sample id="1425">我们可以立即看到，这对于任何人口统计学群体来说都是非常具有概括性的，因为我们可以指定我们想要使用的任何身份标记。</sample>
    <sample id="1426">以下是一些示例生成，来自 GPT-4。</sample>
    <sample id="1427">我们立即看到，虽然输出内容并非过度负面或具有传统意义上的有害性，但</sample>
    <sample id="1428">以下是一些有趣的模式。</sample>
    <sample id="1429">亚洲女性被描绘成不自信，中东女性则被用“异域风情”、“迷人”等词语称呼，并提及一个迷人的地区。</sample>
    <sample id="1430">并且，有色人种角色经常提及祖先，而白人角色则没有任何关于祖先的提及。</sample>
    <sample id="1431">为了捕捉这些模式，我们的方法分为两部分。第一部分是生成这些个人资料。</sample>
    <sample id="1432">这些提示是在一个研究中生成的，该研究给人类参与者提供了这些提示，结果发现，通过向人类参与者提供这些提示，他们也能揭示种族刻板印象。</sample>
    <sample id="1433">并且这使得我们生成的个人资料能够与人类书面回复进行直接比较。</sample>
    <sample id="1434">第二部分是标记词，是一种方法来识别区分标记组和非标记组的词语。我稍后会简要说明。</sample>
    <sample id="1435">这使得我们能够获得非常具体的错误类型和模式，而无需依赖于特定的 Lexicon。</sample>
    <sample id="1436">所以，马克·沃兹的论文借鉴了社会语言学的概念“标记性”，指出存在一个默认状态，任何与该默认状态不同的群体，都具有语言上的标记性。</sample>
    <sample id="1437">例如，词语“男人”或者抱歉，词语“战士”通常与男性相关联。当人们描述一位女性战士时，他们通常会明确指出“一位女性战士”，并用“女性”标记这个词。</sample>
    <sample id="1438">更广泛地说，社会中具有支配地位的群体在语言和社交上都未被标记，而边缘化群体通常被标记。</sample>
    <sample id="1439">所以，在我们的方法中，我们首先指定标记和未标记的组。</sample>
    <sample id="1440">然后我们可以比较这两个人物，使用“战斗词语法”，这基本上是通过加权词频比率来区分每个标记的顶级词语。</sample>
    <sample id="1441">例如，对于黑人用户，我们会进行战斗词的比较，并将其与白人用户和男性用户进行对比，因为这两种是主要的未标记群体。</sample>
    <sample id="1442">现在我们来看看结果。首先，我们使用了看起来像词汇的类型，我们发现生成的Persona包含比人类书写的词汇类型要多得多。</sample>
    <sample id="1443">然而，当我们实际查看词汇在《星球大战》中的分布时，我们会发现非常不同的情况。</sample>
    <sample id="1444">虽然生成的个人资料有更高的词汇率，但人类书写的个人资料词汇分布更广。而生成的个人资料中出现的刻板印象词语，实际上只是“高大且健壮”这些词语。</sample>
    <sample id="1445">我只是想表达积极的，或者至少不是消极的。</sample>
    <sample id="1446">事实上，这个词汇分析并没有捕捉到我们在早期幻灯片中看到的许多有害模式。因此，为了展示这些看似积极的词语如何促进刻板印象和本质化叙事，我们将转向我们标记词语的方法。</sample>
    <sample id="1447">在我们的分析中，我们回顾了这些看似积极的描绘反映了有害模式。</sample>
    <sample id="1448">根据马克群，最常见的词语包括文化、传统、骄傲和异域风情。这些词语定义了这些群体的身份，并将其与其他白人规范区分开来。</sample>
    <sample id="1449">这进一步延续了对这些群体歧视和异化的长久历史。</sample>
    <sample id="1450">此外，这些词语中反映了许多常见的刻板印象，尤其是在女性群体中。例如，描述拉丁裔女性的词语会包含“鲜艳”和“曲线感”等。</sample>
    <sample id="1451">这与一种热带主义的刻板印象相关，对于亚洲女性来说，这些词语像是娇弱、精致、丝滑。</sample>
    <sample id="1452">这段内容与亚洲女性长期以来被过度性化、被视为非常温顺和顺从的历史相关，并带有贬义色彩。</sample>
    <sample id="1453">最后，对于黑人女性来说，我们看到一些最常见的词语是“强大”和“坚韧”。</sample>
    <sample id="1454">这与人们称之为“强大黑女性原型”的原型相关联，虽然听起来在第一眼看来是积极的，但</sample>
    <sample id="1455">有研究表明，这种类型的原型实际上非常有害，因为它给这些群体带来了巨大的压力，要求他们要具备韧性和力量，以应对社会障碍。</sample>
    <sample id="1456">与其实际努力改变这些障碍，就给那些人施加压力让他们克服它们，这会导致这些人的健康状况恶化，以及其他负面后果。</sample>
    <sample id="1457">我们发现，每个市场群体所使用的词语基本上都反映了非常具有中心化的叙事。</sample>
    <sample id="1458">根据这些模式，我们可以得出三条建议，供模型所有者参考：</sample>
    <sample id="1459">首先，作为研究人员，我们应该关注积极的刻板印象和核心叙事。我们还应该使用交叉视角来研究偏见和伤害，因为如果不用这样做，可能会有很多事情被忽略。</sample>
    <sample id="1460">最后，应该有更多关于偏见缓解方法的透明度。</sample>
    <sample id="1461">因为例如，这些积极的刻板印象我们不知道是因为某种奇怪的</sample>
    <sample id="1462">过度夸大价值判断，或者其他一些带有刻板印象的方法，导致了这些有害的模式。</sample>
    <sample id="1463">我们真的不能做出任何假设，或者进一步研究，除非有更多透明度。</sample>
    <sample id="1464">非常感谢您的倾听。我玩得很开心。</sample>
    <sample id="1465">你好，大家好，我叫金伟伊，来自中国科学技术大学。</sample>
    <sample id="1466">我很高兴为您提供一个关于纸张的简短广告视频。您是否正在模仿我的型号？保护大型语言模型嵌入和服务的版权。</sample>
    <sample id="1467">请将所给出的英文翻译成中文。</sample>
    <sample id="1468">目前，大型语言模型，如 GPT、LLaMA、PaLM，在自然语言理解和生成方面表现出色。</sample>
    <sample id="1469">填充式服务是基于大型语言模型构建的服务之一，用于协助各种帮助任务。</sample>
    <sample id="1470">请将所给出的英文翻译成中文。</sample>
    <sample id="1471">然而，最近的研究表明，攻击者可能通过学习嵌入并提供相似的服务来盗用模型。因此，保护嵌入的版权是必要的。</sample>
    <sample id="1472">为了保护版权，一种解决方案是在提供者服务中添加水印，并检测其他服务是否包含水印。</sample>
    <sample id="1473">水痕方法需要满足以下几个条件：首先，该方法应适用于嵌入式服务；其次，水痕不应降低提供嵌入式服务的实用性。</sample>
    <sample id="1474">第三，水杯应该足够靠近攻击者，攻击者可以轻松移除水印。</sample>
    <sample id="1475">最后，模型需要能够转移到攻击者服务上，在模型提取期间。</sample>
    <sample id="1476">已有的词汇可以大致分为四个类别。</sample>
    <sample id="1477">然而，这种方法要么不适用于嵌入式服务，要么缺乏可移植性。</sample>
    <sample id="1478">因此，在本文中，我们将提出一种嵌入标记，这是一种基于水印的方法，可应用于嵌入数据。</sample>
    <sample id="1479">嵌入式标记包含两个主要步骤：
1. 镁注入
2. 版权验证</sample>
    <sample id="1480">在进行这些主要步骤之前，我们首先选择一个触发器集。触发器集是一个在适度频率范围内出现的词语组。</sample>
    <sample id="1481">我们假设供应商可以收集一个通用文本语料库，并计算单词频率。</sample>
    <sample id="1482">在情感注入中，我们首先识别目标嵌入。当用户向提供商服务发送一句话时，提供商会考虑触发词并将其嵌入到句子中。</sample>
    <sample id="1483">The provided embedding is a weighted sum of the targeted embedding and the original embedding.</sample>
    <sample id="1484">The weight of the target embedding is proportional to the number of triggers in the sentence. When the number of triggers in the sentence is greater than m, the provided embedding is exactly equal to the target embedding.</sample>
    <sample id="1485">复制验证是为了检测在另一个服务中包含的单词。</sample>
    <sample id="1486">The first constructed object of a backend dataset. Backend dataset contains sentences of which all words belong to the trigger set. While all words in the sentences of backend dataset do not belong to the trigger. The</sample>
    <sample id="1487">然后提供者要求从窃贼服务获取数据。</sample>
    <sample id="1488">计算了请求的嵌入和目标嵌入之间的余弦相似度。我们计算了基于当前和背景数据集的差值余弦度和差值欧氏距离。</sample>
    <sample id="1489">meanwhile, we also apply KS test and use its p value as the third metric.</sample>
    <sample id="1490">我们无法对该数据集进行实验。
AG News、Mind、AST2 和 Iris 数据集。
我们假设提供者应用维基文本数据集来计算词频。</sample>
    <sample id="1491">结果显示，我们的嵌入式标记可以具有极高的检测性能，同时保持在图像识别任务中的极高实用性。</sample>
    <sample id="1492">“呢”</sample>
    <sample id="1493">如图所示，很难区分带点填充和正常填充。</sample>
    <sample id="1494">谢谢你。我们将在稍后讨论。</sample>
    <sample id="1495">ABC-Eval is a method developed to comprehensively cover chat model behaviors that affect chat quality and recent literature.</sample>
    <sample id="1496">2003年。</sample>
    <sample id="1497">你好，我的名字是 वसुধা，我是斯托尼布鲁克大学计算机科学研究生。我想介绍我于 ACL 2023 上发表的论文，题目是“远程检测中的迁移学习，应对罕见类挑战”。</sample>
    <sample id="1498">认知失调是指人们持有与他们行为不一致的信念或行为。在语言学中研究认知失调很重要，因为它有助于理解语言学习者在学习新语言时所经历的认知冲突和挑战。</sample>
    <sample id="1499">例如，当一个人说“我知道吸烟会杀死我”，然后又说“会议结束后我抽了几根烟”，这种信念和行为不一致，它们是矛盾的。</sample>
    <sample id="1500">进一步提到，我认为没有他们我无法保住我的工作，这证明了第二次出现，他们与我之间存在着一种共识关系。</sample>
    <sample id="1501">因为认知偏差是一种在日常决策中非常常见现象，它们很少在语言中被表达，与其他类型的关系相比。</sample>
    <sample id="1502">认知距离可以帮助我们理解人们之间不和谐的影响，跟踪人口趋势、信仰、价值观和态度变化。</sample>
    <sample id="1503">高认知失调也与焦虑症有关，可以帮助我们更好地理解人们的心理健康。</sample>
    <sample id="1504">研究语言中的细微差别也能帮助理解极端主义和弱势群体极化。</sample>
    <sample id="1505">最后，认知失调对于理解个人的认知风格以及理解决策过程至关重要。</sample>
    <sample id="1506">为了创建认知失调资源，我们进行了大规模的失调关系调查。我们采用了先失调的策略，如图表所示。</sample>
    <sample id="1507">请将英文内容转换为汉语。</sample>
    <sample id="1508">正如这里所见，这种距离仅在标注的样本中发现3.5%。</sample>
    <sample id="1509">通过收集大约一千个语料对，我们训练了一个初始分类器，该分类器仅使用四十三个例子进行训练。不出所料，分类器性能与随机猜测没有多大区别。</sample>
    <sample id="1510">鉴于异议的低频率和缺乏任何先前的数据集，我们面临着绝对稀有性的问题。</sample>
    <sample id="1511">为了缓解这个问题，我们正在探索迁移学习和主动学习的组合，以减少需要标注的样本数量，从而降低整体标注成本，同时提高目标检测的准确性。</sample>
    <sample id="1512">由于最初的模型无法捕捉到距离类，我们开始通过将权重从密切相关的子任务中转移来启动主动学习过程。</sample>
    <sample id="1513">跨领域距离分类任务是指一个任务确定两个来自不同人的辩论陈述是同意还是不同意，而不考虑主题。</sample>
    <sample id="1514">讨论会和关于PTB扩展和比较类别的二元分类，因为这两个类别与共存和异存的概念密切相关，我们称之为CE。</sample>
    <sample id="1515">我们发现，在将零射击性能应用于实体数据集中，已经比最佳情况下有很大的改进。</sample>
    <sample id="1516">进一步地，通过迭代地对两个任务进行微调，发现对 CE 任务的微调，然后对 DBE 进行进一步微调，能获得更好的零样本性能。因此，这就是我们用来启动当前学习的模型。</sample>
    <sample id="1517">接下来，我们需要确定在每个主动学习和标注轮次中更新模型的最佳方法。累积式累积所有已收集的主动标注数据，而迭代式更新则通过在最新数据集中训练模型来更新模型。</sample>
    <sample id="1518">在不同的策略中，我们发现累积性能与迭代性能在整个轮次中都相等或优于。</sample>
    <sample id="1519">接下来，为了增加离散示例的数量，我们使用概率稀有类策略（PCR）选择那些在当前模型任何轮次中都极有可能被离散的示例。</sample>
    <sample id="1520">我们将其与社区中常用的其他艺术策略进行比较。</sample>
    <sample id="1521">我们发现，所提出的 PRC 战略比其他状态的战略效果更好，尽管差异很小。请注意，性能对于 Ran 而言明显较低。</sample>
    <sample id="1522">在后续的语言模型竞赛中，我们改进了设计分类，AUC达到了2.75，这是我们目前为止在任务上的最佳性能。</sample>
    <sample id="1523">我们还检查了每种策略的可行性，包括标注质量和标注人员的成本。我们发现，使用 PRC 具有较高的准确率，并且最适合稀有类别的标注。然而，标注人员也发现这些例子比较困难。</sample>
    <sample id="1524">总而言之，我们发现，为稀有类收购设计适当的迁移学习任务，可以显著提高自然语言处理（NLP）的简单AI策略。</sample>
    <sample id="1525">我们还发现迭代更新在跨领域迁移学习中很有用，而在同一领域中的主动学习则受益于累积的知识。</sample>
    <sample id="1526">这些是我们的代码数据集和我们的论文的链接。
请随时与我们联系，如果您有任何问题。
谢谢。</sample>
    <sample id="1527">The authors are affiliated with the University of California, Berkeley.</sample>
    <sample id="1528">Ciyuan.</sample>
    <sample id="1529">四位。</sample>
    <sample id="1530">The method was compared with the SE of the Arda architecture, specifically tailored for simultaneous processing and translation.</sample>
  </task>
</testset>