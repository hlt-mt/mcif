<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">From the C4 Corpus, including news from sources like the New York Times, Los Angeles Times, The Guardian, and Huffington Post.</sample>
    <sample id="1">McGill University</sample>
    <sample id="2">Tu Yi from Ant Group presented their team's paper on Visually-rich Document Understanding (VrDU), focusing on addressing the limitations of existing pre-trained models that struggle with reading order in documents like forms and receipts.  The paper introduces LayoutMask, a novel pre-trained model designed to enhance text-layout interactions.

LayoutMask differentiates itself from previous methods by using "local 1D positions" (token orders within segments) instead of global 1D positions, allowing for the inference of global reading order through joint consideration of local, 2D, and semantic information.  It employs two novel masking strategies: Whole Word Masking (challenging, promotes context understanding) and Layout-Aware Masking (prioritizes masking at the beginning and end of segments, encouraging cross-segment order learning).  Furthermore, LayoutMask incorporates a new pre-training objective, Masked Position Modeling (MPM), which involves recovering randomly masked 2D positions, fostering semantic and spatial inference.

Experiments on datasets like FUNSD and SROIE demonstrate that Local-1D outperforms Global-1D, particularly in cases with entities like "Total" that are difficult to recognize based on a simple global reading order.  The results highlight the importance of considering both text and layout information for effective VrDU. The paper emphasizes the benefits of joint learning of semantic and spatial information for improved layout representations and better performance.  The team encourages further inquiries via email.</sample>
    <sample id="3">Hi! Willkommen zu unserer Präsentation von DEPLAIN, einem neuen Korpus für die deutsche Textidentifizierung auf Dokumenten- und Satzebene. Mein Name ist Regina Stodden, und ich werde Sie durch die erste Hälfte der Präsentation führen. Beginnen wir mit der Definition von Textvereinfachung. Textvereinfachung ist ein Prozess der Anpassung eines Textes, um die Textverständlichkeit für eine bestimmte Zielgruppe zu verbessern, wie z. B. Menschen mit Leseschwierigkeiten oder Nicht-Muttersprachler. Um ein Textvereinfachungsmodell zu trainieren, benötigen wir parallele Textpaare, beispielsweise von Dokumenten oder Sätzen. Das Beispiel hier zeigt ein parallel ausgerichtetes Satzpaar eines komplexen deutschen Satzes und seine Übersetzung in einfache Sprache. Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie im Beispiel zu sehen ist, z. B. lexikalische Substitution, Klauselentfernung, Umstrukturierung oder Wortinsertion. Wir stellen nun unseren neuen Korpus DEPLAIN vor, da es in den letzten Jahren einige Probleme mit bestehenden Korpora gab. Zum Beispiel sind diese Korpora zu klein, um ein Textvereinfachungsmodell zu trainieren. Die anderen drei im letzten Jahr vorgeschlagenen Modelle sind alle automatisch ausgerichtet, was bedeutet, dass sie fehleranfällig in ihren Ausrichtungen sein können. Daher stellen wir unseren neuen Korpus DEPLAIN vor, der in zwei Unterkorpora unterteilt ist: DEPLAIN-apa und DEPLAIN-web. DEPLAIN-apa basiert auf Nachrichtenartikeln. In DEPLAIN-apa haben wir 483 Dokumente manuell ausgerichtet, was etwa 13.000 parallele Satzpaare ergibt. Für DEPLAIN-web umfasst dieser Korpus verschiedene Domänen und wir haben auch 750 Dokumente manuell und mit automatischen Ausrichtungsmethoden ausgerichtet. Insgesamt ergibt dies 30.450 Satzpaare. Wir haben unsere Satzpaare etwas genauer analysiert, z. B. hinsichtlich des Typs der Vereinfachung. Wie Sie hier sehen können, sind Bibeltexte deutlich stärker vereinfacht als beispielsweise Nachrichtenartikel oder Texte für Sprachlerner. Auf allen Ebenen, z. B. bei lexikalischer Vereinfachung, Strukturschließung und der allgemeinen Vereinfachung, gibt es Unterschiede. Darüber hinaus sehen Sie, dass unser DEPLAIN-Korpus eine hohe Vielfalt an verschiedenen Vereinfachungstransformationen aufweist. Zum Beispiel haben wir im DEPLAIN-apa-Korpus mehr Umstrukturierungen und Wortzusätze als im DEPLAIN-web-Korpus. Im Web-Korpus hingegen gibt es mehr Umformulierungen. Lassen Sie uns nun sehen, was wir mit diesem Korpus tun können. Hallo, ich bin Omar und jetzt werde ich über die Anwendungsfälle unseres Datensatzes DEPLAIN sprechen. Für den ersten Anwendungsfall können wir automatische Ausrichtungsmethoden bewerten. In den letzten Jahren gab es viele Ausrichtungsmethoden, aber im Kontext maschineller Übersetzung, bei der wir zwei parallele Dokumente in verschiedenen Sprachen haben und die Ausrichtung von Sätzen in beiden Dokumenten extrahieren wollen. In unserem Anwendungsfall versuchen wir, Ausrichtungen zwischen Sätzen von zwei parallelen Dokumenten zu extrahieren, die dieselbe Sprache haben, den gleichen Inhalt haben, aber auf einer anderen Komplexitätsstufe liegen. Da wir nun unseren Datensatz DEPLAIN haben, der manuell ausgerichtete Sätze enthält, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten. Wir haben einige Anpassungen an den vorgeschlagenen Methoden vorgenommen und alle Anpassungen und Codes für unsere Experimente in der Arbeit veröffentlicht. Wir sind zu dem Schluss gekommen, dass die beste automatische Ausrichtungsmethode für die deutsche Textvereinfachung die Methode MASSalign ist, und Sie können auch den Code zum Ausführen dieser Methode auf Ihren eigenen Dokumenten in der Arbeit finden. Der zweite Anwendungsfall, den wir in unserer Arbeit gezeigt haben, ist ein Fall der automatischen Textvereinfachung durch das Fine-Tuning von Sprachmodellen, um aus dem komplexen Eingabetext vereinfachten Text zu erzeugen. Wir haben zwei verschiedene Modelle fine-getunt. Wir haben das Modell long-mBART verwendet, um Dokumenten-Level-Vereinfachungen zu erzeugen, und wir haben das Standard-base-mBART verwendet, um Satz-Level-Vereinfachungen zu erzeugen. Sie können auch alle Checkpoints finden und mehr Details zu den Ergebnissen und Metriken unserer Experimente in der Arbeit einsehen. Wir sind zu dem Schluss gekommen, dass dieses einfache Fine-Tuning bessere Ergebnisse als die Baseline-Ergebnisse erzielen kann und haben diese Ergebnisse als Basisbenchmark für das Problem der automatischen Textvereinfachung in der Zukunft vorgeschlagen. Vielen Dank für Ihre Aufmerksamkeit und wir hoffen, Sie alle auf der Konferenz zu treffen. Danke.</sample>
    <sample id="4">Kayo Yin.</sample>
    <sample id="5">The T5 XL model was used to achieve accuracy between 82 and 87%.</sample>
    <sample id="6">Jiaan and colleagues present "Towards Unifying Multi-Lingual and Cross-Lingual Summarization," introducing a new framework called many-to-many summarization. This approach aims to build a single model capable of summarizing documents in any source language into any target language, unifying previous multilingual and cross-lingual summarization methods.

The paper analyzes the differences between these approaches, highlighting that many-to-many summarization facilitates knowledge transfer across languages.  They propose PISCES, a pre-trained many-to-many summarization model with a three-stage pre-training process: meta-pretraining for sentence generation, cross-lingual pre-training for target language generation from parallel data, and task-specific pre-training using pseudo-summarization samples.

Preliminary experiments on the WikiLingua dataset demonstrate that the many-to-many model outperforms multilingual and cross-lingual models in knowledge transfer.  Furthermore, PISCES achieves state-of-the-art results compared to baselines like mBART-50 and mT5, validated through ablation studies and human evaluations. The authors encourage readers to consult their paper for further details.</sample>
    <sample id="7">Yes, CoNLL-2003 taggers still work well in 2023. The paper found that models generalize to modern data, and the performance drop is primarily due to temporal drift, not adaptive overfitting.</sample>
    <sample id="8">Die vorgeschlagene menschliche Bewertungsmethode, ABC-Eval, reduziert die Subjektivität der menschlichen Bewertung, indem sie explizit annotiert, ob Chatmodelle bestimmte Verhaltensweisen zeigen, wie z. B. irrelevante Informationen liefern, sich selbst oder ihren Gesprächspartner widersprechen oder Fakten halluzinieren.</sample>
    <sample id="9">The success of existing weakly supervised learning approaches hinges on the availability of clean validation samples. Without them, performance drops significantly, and training is pointless. Increasing the number of clean validation samples improves performance, and directly fine-tuning on clean samples yields even better results.</sample>
    <sample id="10">```
The accuracy of language models can be improved by providing them with more relevant and comprehensive background knowledge, beyond just entity names.  Specifically, incorporating information about entity attributes, relationships, and context can lead to better disambiguation.
```</sample>
    <sample id="11">Jack Hessel von AI2 präsentiert seine Forschung über Humorverständnis anhand der New Yorker Caption Contest Daten. Die Arbeit, in Zusammenarbeit mit verschiedenen Universitäten und OpenAI, untersucht, wie gut Large Language Models (LLMs) Humor verstehen können.

Die Studie nutzt die New Yorker Caption Contest Daten, die jährlich eine große Anzahl von Cartoons und dazugehörigen Bildunterschriften generieren. Die Daten wurden in drei Aufgaben unterteilt: Matching (Identifizierung der korrekten Bildunterschrift), Quality Ranking (Bewertung der Qualität von Bildunterschriften) und Explanation Generation (Erklärung des Humors).

Die Ergebnisse zeigen, dass LLMs in den Aufgaben Matching und Quality Ranking deutlich hinter menschlichen Bewertern zurückbleiben, selbst wenn sie mit zusätzlichen Bildbeschreibungen gefüttert werden. Die Explanation Generation zeigt ebenfalls Defizite, da LLMs oft fehlerhafte oder unpassende Erklärungen liefern.

Die Forscher haben eine neue Annotationen-Datenbank erstellt, um die LLMs besser zu trainieren. Sie stellen die Daten und die Modelle auf einer Online-Leaderboard zur Verfügung. Die Arbeit unterstreicht die Herausforderungen beim Humorverständnis durch LLMs und bietet einen wertvollen Datensatz für weitere Forschung in diesem Bereich.</sample>
    <sample id="12">5</sample>
    <sample id="13">Daniel Rotem's presentation details research on adaptive inference for large language models, aiming to reduce inference time and costs. The study compares two common methods: Multi Model, which uses multiple models with classifiers, and Early Exit, which uses classifiers at intermediate layers.

The research identifies a key issue with Early Exit: conflicting gradients.  Each classifier tries to optimize its own loss, potentially interfering with each other and degrading overall performance.  A test comparing Early Exit and Multi Model classifiers, truncated versions of BERT, showed that Multi Model outperformed Early Exit by an average of 2.3%, especially with earlier classifiers.

To address this, Rotem introduces SWEET (Separating Weights in Early Exit Transformers). SWEET trains Early Exit architectures where each layer only receives updates from the *next* classifier, eliminating conflicting gradients.  Results indicate that SWEET largely closes the gap between Early Exit and Multi Model, with SWEET outperforming both methods in speed and accuracy, particularly for BERT-Large.

The key takeaways are the existence of conflicting gradients in Early Exit training and the introduction of SWEET as a novel fine-tuning method. The research motivates further work on tailored fine-tuning algorithms for Early Exit architectures. The presentation concludes with a call to visit the paper on Archive for more information.</sample>
    <sample id="14">Adam Przepiórkowski spricht über die Abhängigkeitsstruktur der Koordination. Es gibt verschiedene Abhängigkeitsstrukturen, die in Theorien und Korpusansätzen angenommen werden. Zum Beispiel postulieren universelle Abhängigkeiten, dass die erste Konjunktion die Kopfposition der gesamten Koordinationsstruktur ist, wie bei "Lisa, Bart und Maggie". Ein ähnlicher Ansatz wird in Igor Mel'čuks Bedeutungstexttheorie angenommen, ebenfalls mit der ersten Konjunktion als Kopf. Diese beiden Ansätze sind asymmetrisch. Die Prague-Ansicht, die die Konjunktion als Kopf der Koordinationsstruktur annimmt, führt Abhängigkeiten von End zu allen Konjunktionen. Ein weiterer Ansatz, der in Hudson's Word Grammar verwendet wird, ist der mehrköpfige Ansatz, bei dem alle Konjunktionen als Köpfe der Koordinationsstruktur angesehen werden, was Abhängigkeiten vom Gouverneur zu allen Konjunktionen separat erzeugt.

Ziel dieser Arbeit ist es, einen neuen Argument für symmetrische Koordinationsstrukturen und gegen asymmetrische Strukturen zu liefern. Der Argument basiert auf dem Prinzip der Abhängigkeitslängenminimierung. In der englischen Sprache bevorzugen direkte Objekte die Nähe zum Verb, während Ergänzungen weiter entfernt sein können. "Marge liest es gestern" ist korrekt, während "Marge liest gestern es" fehlerhaft ist, da "yesterday" eine Ergänzung ist. Diese Wirkung kann jedoch durch lange und schwere direkte Objekte gemildert werden.

Die Arbeit extrahiert verschiedene Statistiken zur Koordination aus der erweiterten Penn Treebank und bestätigt die Beobachtung, dass linke Konjunktionen tendenziell kürzer sind ("salt and pepper" vs. "pepper and salt", gemessen in Silben). Außerdem zeigt die Arbeit, dass diese Tendenz mit der Länge der Unterschiede zwischen den Konjunktionen wächst. Je größer der Unterschied, desto eher ist die kürzere Konjunktion die erste.

Was neu an dieser Arbeit ist, ist die Beobachtung, dass diese Tendenz nur dann auftritt, wenn der Gouverneur auf der linken Seite oder fehlt. Wenn der Gouverneur auf der linken Seite steht ("I saw Bart and Lisa"), ist die Tendenz, dass die linke Konjunktion kürzer ist, stärker. Wenn der Gouverneur fehlt ("Homer came and sneezed"), verschwindet dieser Effekt. Die Arbeit zeigt, dass die Länge der Abhängigkeiten in verschiedenen Maßen (Zeichen, Silben, Wörter) gemessen wird. Bei der Analyse der Abhängigkeitslänge zeigt sich, dass die Tendenz für die linke Konjunktion, kürzer zu sein, mit zunehmender absoluten Differenz in Wörtern wächst. Dieser Effekt ist bei Koordinationen von Sätzen ebenfalls beobachtet.

Die Arbeit argumentiert, dass diese Beobachtungen ein Argument gegen asymmetrische Koordinationsstrukturen und für symmetrische Strukturen liefern. Die Ergebnisse werden in der Arbeit ausführlich dargelegt.</sample>
    <sample id="15">Three.</sample>
    <sample id="16">Bible texts are more strongly simplified than news texts or language learner texts.</sample>
    <sample id="17">This paper addresses the challenges in multimodal relation extraction (MRE), where text and visual information are combined to identify relationships between entities. Existing methods often struggle with internal information over-utilization and under-exploitation of external information like topic information. To overcome these limitations, we propose a novel framework that leverages a graph information bottleneck principle for fine-grained information pruning and incorporates multimodal topic information for context enrichment.

Our method constructs a unified cross-modal graph (CMG) from textual and visual scene graphs, then employs the bottleneck principle to filter nodes and edges, ensuring efficient feature representation.  Multimodal topic features are retrieved and integrated using attention mechanisms to enhance the overall context.  Experiments on a standard MRE dataset demonstrate significant performance improvements over text-based methods and multimodal baselines. Ablation studies reveal the importance of both information screening and external information exploitation.  We analyze performance based on text-vision relevance, finding that internal information screening is crucial for high relevance inputs, while external information exploitation is more beneficial for low relevance inputs.  Our approach offers a promising solution for more accurate and robust multimodal relation extraction.</sample>
    <sample id="18">"salt and pepper" ist bevorzugt gegenüber "pepper and salt".</sample>
    <sample id="19">Zhang Qin from Shenzhen University presented their work, "A Survey for Efficient Open Domain Question Answering," accepted at ACL 2023. The presentation focused on the challenges of open-domain question answering, particularly the large size of the Wikipedia corpus (26 million documents, 20 GB) and the bottleneck of index searching (65 GB).  The motivation is to create efficient systems with smaller memory footprints, faster inference, and comparable performance.

The presentation outlined core techniques for achieving efficiency, including approximate nearest neighbor search for faster evidence retrieval and skip reading strategies (like adaptive computation) for faster reading.  They also discussed methods to reduce index size (document filtering, embedding compression) and model size (lightweight models, parameter sharing, knowledge distillation, one-stage models).

The presentation compared existing retrieval-only and generator-only systems, highlighting the trade-offs between speed, memory, and performance. Retrieval-only systems are fast but require large indexes, while generator-only systems are fast but use large models and often have lower performance.

The conclusion suggests that resource constraints favor generator-only systems or embedding compression, while real-time feedback benefits retrieval-only systems. Retrieval and reader systems offer a balance.  Future work will explore deployment on low-power devices and the development of more comprehensive evaluation metrics.</sample>
    <sample id="20">Yes, the pre-trained models from NACHOS are freely available on Hugging Face under the MIT license, and the training scripts are on their GitHub repository.</sample>
    <sample id="21">DEPLAIN-apa enthält Nachrichtenartikel.</sample>
    <sample id="22">*   Bessere Modellarchitektur (Transformer-Modelle)
*   Größere Modellgröße
*   Mehr Feinabstimmungsexemplare</sample>
    <sample id="23">Dan Garrette discusses challenges in text-image models, particularly the difficulty in accurately rendering text, exemplified by the Imagen model. The core issue lies in how text encoders, like T5, process text. T5 uses subword tokenization, forcing the model to decompose words into individual letter representations, which it struggles to master, especially at smaller scales.  While larger T5 models improve, they still lag behind models like PaLM.

ByT5, a model using byte-level tokenization, excels at spelling, as it directly accesses character information.  The research highlights that frequent words are problematic for T5 because they are represented by fewer, larger subword units, requiring more decomposition. ByT5, however, handles this well.

To address this, the team augmented Imagen with a text representation from ByT5-small, achieving significant improvements in text rendering. This augmentation, requiring only a small increase in parameters, allows the model to better "know" the spelling of words.  

The research introduces the WikiSpell and DrawText benchmarks for evaluating text-only and text-to-image models, respectively.  A key takeaway is an efficient strategy: concatenating a character-aware text representation to improve model spelling ability.  While the diffusion model can still introduce errors, the improved text encoder significantly enhances the overall image generation quality and text rendering capabilities.</sample>
    <sample id="24">Die Tendenz zu kürzeren linken Konjunktionen wurde anhand der Länge der Konjunktionen in Wörtern, Silben und Zeichen gemessen.</sample>
    <sample id="25">The experiments involved extracting statistics about coordination from the enhanced version of the Penn Treebank. The length of the conjuncts (in words, syllables, and characters) was measured, and the position of the governor (left or right, or absent) was considered. The results showed that when the governor is on the left or absent, the tendency for the left conjunct to be shorter increases with the absolute difference in lengths. This effect disappears when the governor is on the right.</sample>
    <sample id="26">The initial classifier trained on only 43 examples of dissonance performed not much better than chance. This highlights the problem of absolute rarity when dealing with imbalanced data.</sample>
    <sample id="27">Ich kann diese Frage nicht beantworten, da der Text keine Informationen über die Anzahl der Autoren enthält.</sample>
    <sample id="28">Javad Hosseini, Filip Radlinski, Silvia Pareti, und Annie Louis.</sample>
    <sample id="29">Kontextsensitive MÜ-Modelle schneiden bei Formalität und lexikalischer Kohäsion besser ab als kontextagnostische Modelle.</sample>
    <sample id="30">LLM-Blender ist ein einfaches, aber effektives Ensemble-Lernframework für große Sprachmodelle (LLMs), das auf Pairwise-Ranking und generativer Fusion basiert. Die Forscher von AI2 und USC stellen diese Arbeit vor, die die Notwendigkeit hervorhebt, bei der Auswahl des besten LLMs für eine bestimmte Eingabe zu berücksichtigen, da die Leistung von Modell zu Modell stark variieren kann.

Das Framework besteht aus zwei Hauptkomponenten: PairRanker und GenFuser. PairRanker vergleicht die Ausgaben mehrerer LLMs für eine Eingabe mithilfe von Cross-Attention und lernt, welche Kandidaten besser sind. Dies führt zu einem Ranking der Modelle. GenFuser wählt die Top-K-Kandidaten (hier drei) aus und verwendet sie als Eingabe für ein sequenziellem Modell, um die endgültige Ausgabe zu generieren.

Im Gegensatz zu vorherigen Methoden verwendet PairRanker eine Encoding-Stufe, die die subtilen Unterschiede zwischen den Kandidaten besser analysiert. Die Ergebnisse zeigen, dass PairRanker die Leistung der LLMs deutlich verbessert, insbesondere im Vergleich zu etablierten Modellen wie Open Assistant und Vicuna.  Die Forscher haben einen neuen Datensatz namens MixInstruct erstellt, um die Leistung von LLMs zu bewerten, und haben eine offene Codebasis für die Evaluierung und zukünftige Forschung bereitgestellt. LLM-Blender ist ein vielversprechender Ansatz für Ensemble-Lernen, der die Leistung von LLMs erheblich steigert.</sample>
    <sample id="31">This information is not provided in the text.</sample>
    <sample id="33">The NLPositionality framework quantifies positional alignment by comparing re-annotated data with diverse annotators to existing datasets and models using Pearson's R correlation score. This allows for a direct comparison of end-user predictions and labels with model and dataset outputs, rather than relying on annotator agreement or distributions.</sample>
    <sample id="34">Marcos Treviso presents CREST, a framework combining rationalization and counterfactual text generation for improved decision explanations. CREST addresses the challenge of interpreting classifier decisions by generating both faithful rationalizations and human-like counterfactuals. The framework utilizes a masked language model to create counterfactual examples by masking input tokens and prepending a gold label.  Human evaluations show CREST's counterfactuals are more valid and natural than those from other methods.

CREST-Generation further enhances the framework by generating both rationalizations and counterfactuals, enabling a shared rationalizer and predictor.  A regularization term encourages rationales to align with both factual and counterfactual reasoning. Experiments on IMDB demonstrate that CREST-Rationalization achieves top performance, matching human-generated counterfactuals on contrastive datasets and outperforming other methods on out-of-domain data.

Furthermore, CREST-Rationalization produces more plausible and highly simulable rationalizations, meaning they effectively guide decision changes when contrasted with the original input.  The framework leverages counterfactuals for data augmentation and improves downstream model performance.  CREST offers a controllable and effective approach to generating high-quality explanations by focusing on the contrasting aspects of the input.</sample>
    <sample id="36">Telmo Pessoa Pires from ACL introduces "Learning Language-Specific Layers for Multilingual Machine Translation," a work by Pessoa, Schmidt, Liao, and Peitz. The paper addresses the benefits of multilingual translation – scalability, speed, and improved low-resource language support – while acknowledging limitations in capacity per language. Their solution, Language-Specific Layers (LSLs), aims to increase capacity where needed without increasing inference costs.

LSLs involve having a regular transformer layer per language, with the model selecting the appropriate sublayer at inference time.  The paper focuses on LSL placement within the encoder, proposing a method to learn the optimal placement by training a large model with shared and language-specific weights.  This allows the model to determine where to insert the LSLs, leading to a more efficient architecture.

Experiments on WMT21 news translation with 10 languages demonstrate significant improvements over baseline models and language adapters, particularly for low-resource languages. The learned architecture achieves better performance while maintaining faster inference speeds.  The paper provides detailed results across 90 translation directions and statistically significant improvements for 84 of them.  The authors invite further questions and encourage readers to consult the full paper or attend their poster session.</sample>
    <sample id="37">The previous study with human participants found that the prompts generated stereotypes.</sample>
    <sample id="38">The study used statistics extracted from the enhanced version of the Penn Treebank.</sample>
    <sample id="39">This text is a presentation about a research paper. The speaker mentions the authors of the paper but doesn't explicitly state the number. Therefore, I cannot answer the question.</sample>
    <sample id="40">The closely related tasks for cognitive dissonance are:

*   **Topic-independent stance classification (debate):** Determining if two debate statements from different people are in agreement or disagreement.
*   **Binary classification of expansion and comparison classes (CE):** These are closely related to the concept of consonance and dissonance.</sample>
    <sample id="41">PeaCoK is a new knowledge graph developed by the Natural Language Processing Lab at EPFL University in collaboration with Sony, designed to enhance the consistency and engagement of narratives. It represents world-level persona knowledge, containing 3,800 personas, 40,000 attributes, and 100,000 inferences, with 9,200 attributes connected to multiple personas.

The project addresses the challenge of representing rich, interconnected persona knowledge, crucial for understanding how speakers' backgrounds influence their behavior and communication. PeaCoK is built through three steps: selecting personas from existing knowledge graphs, inducing attributes using commonsense and language models, and crowdsourcing relation annotations with AI assistance.

Experiments demonstrate that PeaCoK improves the performance of language models like BART, enabling them to generate more accurate and contextually relevant persona-based attributes. Furthermore, PeaCoK enhances dialogue generation in persona-grounded conversations, leading to improved fluency, consistency, engagement, and persona expression.  The results show that PeaCoK's persona-centric knowledge is more effective than general social knowledge.  The more shared attributes between speakers, the better the dialogue quality.  PeaCoK offers a valuable resource for building more coherent and engaging narratives by leveraging detailed persona knowledge. The project's paper and GitHub repository are publicly available.</sample>
    <sample id="42">The provided text does not mention the number of authors involved in the paper.</sample>
    <sample id="43">This information is not provided in the text.</sample>
    <sample id="44">The NLPositionality framework differs from previous work by comparing end users with datasets and models, rather than just annotator agreement or modeling annotator distributions. It also uses a framework enabled by Lab in the Wild and online crowdsourcing platforms to recruit diverse volunteers, unlike platforms like Mturk.</sample>
    <sample id="45">The generated personas contain more stereotypes than human-written ones, but the distribution of words in the generated personas differs significantly from the stereotype lexicon. The generated personas tend to use only positive or at least non-negative words like "tall" and "athletic," while the stereotype lexicon has a wider distribution of words. The Marked Words method reveals that the top words in the generated personas often reflect harmful patterns and essentializing narratives, such as "culture," "tradition," "proud," and "exotic," which define groups only by their relationship to their identity and distinguish them from the white norm.</sample>
    <sample id="46">DeepL und Google Translate.</sample>
    <sample id="47">Hi, ich bin Shangbin, Doktorand an der University of Washington. Heute präsentiere ich unsere Arbeit "Von den Trainingsdaten zu Sprachmodellen zu Aufgaben im Abfluss: Verfolgung der Spuren politischer Verzerrungen, die zu unfairen NLP-Modellen führen". Sprachmodelle werden auf großen Webcrawl-Daten trainiert. Politische Nachrichtenmedien sind in ihren Trainingsdaten gut vertreten. Laut einer Umfrage des C4-Korpus sind New York Times, Los Angeles Times, The Guardian, Huffington Post usw. gut in den Trainingsdaten von Sprachmodellen enthalten. Dies hat für Sprachmodellanwendungen sowohl einen Segen als auch einen Fluch. Einerseits konnten sie von vielfältigen Perspektiven lernen, die Demokratie und die Vielfalt der Ideen fördern. Andererseits sind diese verschiedenen politischen Meinungen inhärent sozial verzerrt und können potenzielle Fairnessprobleme bei Anwendungen im Abfluss verursachen. In diesem Zusammenhang schlagen wir vor, die Kette der politischen Verzerrungen von den Trainingsdaten zu den Sprachmodellen und zu den Aufgaben im Abfluss zu untersuchen, indem wir folgende Fragen stellen: Erstens, wie bewerten wir die politische Neigung von Sprachmodellen und welche Rolle spielen die Trainingsdaten dabei? Zweitens, wie führen Sprachmodelle mit unterschiedlicher politischen Neigung bei Aufgaben im Abfluss tatsächlich zu Leistungsproblemen und können dies Fairnessprobleme in NLP-Anwendungen verursachen? Um dies zu erreichen, haben wir zunächst Sprachmodelle mit verschiedenen Prompt-Formaten mithilfe politischer Fragebögen wie dem Political Conference Test geprompt. Dies stellt sicher, dass wir eine gut begründete automatische Bewertung im Bereich der politischen Wissenschaft erhalten. Einige vorläufige Ergebnisse zeigen, dass Sprachmodelle tatsächlich unterschiedliche politische Neigungen aufweisen. Sie belegen alle vier Quadranten des politischen Spektrums. Wir können auch feststellen, dass GPT-4 das liberalste Sprachmodell aller Modelle ist, und die GPT-Serie im Allgemeinen sozial liberaler ist als die BART-Serie und ihre Varianten. Zweitens wollen wir untersuchen, inwieweit die politischen Verzerrungen von Sprachmodellen tatsächlich aus den Trainingsdaten übernommen werden. Wir führen ein kontrolliertes Experiment durch, indem wir Sprachmodelle auf sechs verschiedene parteiische Korpora weiter trainieren, die in Nachrichten und sozialen Medien unterteilt sind, die nach politischer Neigung weiter unterteilt sind. Durch das Weitertrainieren von Sprachmodellen auf solchen parteiischen Korpora können wir feststellen, dass die ideologischen Koordinaten der Sprachmodelle entsprechend verschieben. Zum Beispiel zeigt die für RoBERTa durch das Training auf einem linken Reddit-Korpus erzielte substantielle liberale Verschiebung in Bezug auf die politischen Verzerrungen. Wir versuchen auch zu untersuchen, ob Sprachmodelle die Polarisierung aufnehmen, die in unserer modernen Gesellschaft vorherrscht. Wir unterteilen die Trainingskorpora in vor und nach der Präsidentschaft von Präsident Trump. Wir trainieren separate Sprachmodelle auf den beiden verschiedenen zeitlichen Korpora. Wir sehen, dass Sprachmodelle im Allgemeinen eine politische Neigung aufweisen, die weiter vom Zentrum entfernt ist nach 2017. Dies deutet darauf hin, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft aufnehmen können. Drittens bewerten wir Sprachmodelle mit unterschiedlicher politischer Neigung bei Aufgaben im Abfluss, die oft Sprachmodelle beinhalten und erhebliche Auswirkungen haben können. Wir sehen, dass, wenn wir die Leistung nach Kategorien untersuchen, d. h. wenn wir die Leistung nach den Demografien oder der politischen Neigung der Nachrichtenmedien aufteilen, ein Muster zu erkennen ist. Zum Beispiel sind linke Sprachmodelle bei der Erkennung von Hassreden, die sich an Minderheitengruppen richten, besser, sind aber schlechter bei der Erkennung von Hassreden, die sich an mächtige Gruppen richten. Umgekehrt sind rechte Sprachmodelle bei der Erkennung von Hassreden, die sich an Weiße und Männer richten, besser, sind aber schlechter bei der Erkennung von Hassreden, die sich an schwarze LGBTQ+- und andere Minderheitengruppen richten. Ähnliche Trends finden sich auch bei der Erkennung von Fake News, wo wir feststellen, dass linke Sprachmodelle besser darin sind, Fehlinformationen von ihren politischen Gegensätzen zu erkennen, und umgekehrt. Wir zeigen zahlreiche qualitative Beispiele, um zu zeigen, dass Sprachmodelle mit unterschiedlicher politischer Neigung unterschiedliche Vorhersagen zu Hassrede- und Fehlinformationsbeispielen basierend auf ihren sozialen Kategorien treffen. Es gibt eine Reihe weiterer Beispiele im Anhang, um dies weiter zu verdeutlichen, was darauf hindeutet, dass es ein sehr dringendes Fairnessproblem im Zusammenhang mit den politischen Verzerrungen von Sprachmodellen gibt. Zum Beispiel, wenn rechte Sprachmodelle für die Feinabstimmung auf Hassrede, Fehlinformationen oder ähnliches verwendet würden und auf einer beliebten Social-Media-Plattform eingesetzt würden, würde dies bedeuten, dass Menschen mit gegensätzlichen politischen Meinungen marginalisiert werden könnten und Hassrede, die sich an Minderheitengruppen richtet, ungehindert verbreitet werden könnte. Dies ist ein Weckruf, um die Fairnessprobleme zu erkennen und anzugehen, die durch die politischen Verzerrungen von Sprachmodellen entstehen. So eine kurze Diskussion. Wir möchten auch hervorheben, dass wir das einzigartige Dilemma der politischen Verzerrungen von Sprachmodellen aufzeigen. Es ist wie zwischen Scylla und Charybdis. Wenn wir politische Meinungen in den Trainingsdaten nicht reinigen, würde sich die Verzerrung von den Trainingsdaten zu den Sprachmodellen und zu den Aufgaben im Abfluss propagieren und letztendlich Fairnessprobleme verursachen. Wenn wir versuchen, sie irgendwie zu reinigen, würden wir auch das Risiko der Zensur oder der Ausgrenzung eingehen. Es ist unglaublich schwierig zu bestimmen, was tatsächlich neutral ist und beibehalten werden sollte, was in den Sprachüberwachungsdaten enthalten ist. Es ist so, als ob wir uns im Trolley-Problem befinden. Okay, großartig. Ich denke, das ist im Grunde alles, was ich heute zu berichten habe. Vielen Dank für Ihre Zeit.</sample>
    <sample id="48">David Vilar und seine Google Translate Kollegen.</sample>
    <sample id="49">Die MPP-Auswertungen wurden bis zu einer Kontextlänge von 1024 Token durchgeführt.</sample>
    <sample id="50">DEPLAIN is a new corpus for German text identification at both document and sentence levels, addressing limitations in existing corpora. It comprises two subcorpora: DEPLAIN-apa, based on manually aligned news texts (approximately 13,000 sentence pairs), and DEPLAIN-web, incorporating diverse domains with both manual and automatic alignments (30,450 sentence pairs). Analysis reveals varying simplification types across texts, with Bible texts exhibiting stronger simplification than news or learner texts. DEPLAIN offers a high variety of simplification transformations, with more reorderings and additions in DEPLAIN-apa and more rephrasings in DEPLAIN-web.

The corpus is valuable for evaluating automatic alignment methods, with MASSalign identified as the best performing method for German text simplification.  Furthermore, DEPLAIN facilitates automatic text simplification by fine-tuning language models.  Experiments with long-mBART and base mBART demonstrated that fine-tuning can achieve scores exceeding baseline performance, establishing a new benchmark for automatic text simplification. The authors provide code and checkpoints for reproducibility. DEPLAIN aims to improve the quality and reliability of text simplification models for German.</sample>
    <sample id="51">The AltEntities Corpus includes data from three domains: music, books, and recipes.</sample>
    <sample id="52">Positionalität bezieht sich auf die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben. Diese Perspektiven können den Forschungsprozess und die Ergebnisse beeinflussen, da sie die Entscheidungen der Forscher verändern können.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">```
This paper addresses the challenge of detecting cognitive dissonance in natural language, a rare but important phenomenon for understanding human behavior, attitudes, and mental health. We introduce a large-scale annotation effort to create a dataset of dissonance relations, revealing that dissonance is present in only 3.5% of annotated pairs.  Due to the rarity of dissonance, initial models perform poorly. We propose a transfer learning and active learning approach to overcome this challenge.  We leverage pre-trained models on related tasks (stance classification and expansion/comparison) to initialize a dissonance detection model, achieving a zero-shot AUC of 0.62.  Iterative fine-tuning on these tasks further improves performance.  We evaluate different active learning strategies, finding that the Probability-of-Rare-Class (PRC) strategy outperforms state-of-the-art methods in selecting high-priority examples for annotation, although annotators find these examples difficult.  We also determine that cumulative data accumulation is more effective than iterative updates for transfer learning.  Our results demonstrate that PRC and transfer learning are effective for acquiring data for rare-class detection, while iterative updates are beneficial for domain adaptation.  We conclude that a combination of transfer learning and PRC active learning significantly improves dissonance detection performance.
```</sample>
    <sample id="55">Ja, EDAtt passt zu einem bestehenden Offline-ST-Modell. Es verwendet bereits trainierte Offline-ST-Modelle ohne erneutes Training oder spezielle Architekturen für SimulST.</sample>
    <sample id="56">The provided text is a presentation of a research paper. It mentions Yusen Zhang from Penn State University as the presenter. The text does not explicitly state the number of authors.

The answer is: The text does not specify the number of authors.</sample>
    <sample id="57">The presented model performs significantly better on the KITMUS test suite when trained on it compared to a random choice. However, even the best-performing models struggle to reliably integrate knowledge presented only at inference time.</sample>
    <sample id="58">The three variants of KITMUS are:

1.  **Background-Pretrain:** Background knowledge is available at pretrain time.
2.  **Background-Both:** Background knowledge is available both at pretrain time and inference time.
3.  **Background-Inference:** Background knowledge is available only at inference time.</sample>
    <sample id="59">Yanis Labrak's presentation introduces DrBERT, a robust pre-trained model in French for biomedical and clinical domains. The talk begins by discussing language modeling in healthcare, highlighting the advancements BERT has brought to NLP, particularly in English.  A gap in specialized French models for biomedical data is identified, prompting the development of DrBERT.

DrBERT is built upon RoBERTa and trained on the NACHOS dataset, a collection of medical data crawled from the web. The presentation compares DrBERT with the ChuBERT model, a clinical model trained on anonymized hospital data.  Experiments explore the impact of data size, comparing from-scratch pre-training with continual pre-training using CamemBERT and PubMedBERT.  Seven models were trained and evaluated on various downstream tasks like named entity recognition and question answering, against baseline models including CamemBERT, PubMedBERT, and BioBERT.

The results show that models trained on data similar to the training data perform best, but heterogeneous data is more versatile.  More data generally leads to better performance. From-scratch pre-training yields higher performance overall.  Control pre-training using CamemBERT weights showed comparable results to DrBERT 4GB from-scratch, while models based on CamemBERT weights suffered from stability issues.  DrBERT outperformed the generic CamemBERT on nine of eleven downstream tasks.  The presentation concludes by offering access to the pre-trained models and training scripts on Hugging Face and GitHub, respectively.</sample>
    <sample id="60">This information is not provided in the text.</sample>
    <sample id="61">The final research questions are:

1. Is clean validation data necessary for WSL, or can a noisy validation set be used?
2. If clean data is required, how many clean samples are needed?
3. Should we only use clean samples for validation, or are there better ways to utilize them?</sample>
    <sample id="62">Nitay Calderon introduces their ACL paper, "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training," a collaborative effort with Amir and Subhabrata from Microsoft and PhD advisor Roi. The paper addresses the growing need to compress large language models (LLMs) in natural language generation (NLG) without sacrificing performance, a challenge driven by increasing model size and computational costs.

The study explores knowledge distillation, a technique where a smaller "student" model learns from a larger "teacher" model.  They focus on task-specific distillation in NLG, contrasting it with previous work primarily focused on classification or pre-training.  Their research considers realistic, industry-driven setups characterized by medium-resource labeled data, large unlabeled data, and a focus on inference time efficiency.  They investigate four NLG tasks: summarization, question generation, common sense reasoning, and simplification/style transfer.

The paper systematically examines architectural decisions, pruning techniques, and different approaches to knowledge selection.  A key contribution is an exploration of pseudo-targets, where the teacher model generates output text used to augment the student's training data.  The study challenges traditional sequence-level distillation by demonstrating the benefits of generating multiple, diverse pseudo-targets through sampling and temperature scaling.  Furthermore, they propose "joint-teaching," a novel technique combining word-level distillation on pseudo-targets generated by both teacher and student, to address student exposure bias and encourage self-correction. The paper provides a comprehensive analysis of these techniques and offers a "recipe" for effective knowledge distillation in NLG.</sample>
    <sample id="63">Die Sensitivitätsmetrik misst, wie gut das Modell konsistente Ausgaben für dieselbe Aufgabe erzeugt, unabhängig von leichten Variationen in der Formulierung der Anweisung.</sample>
    <sample id="64">Jingwei Yi from the University of Science and Technology of China.</sample>
    <sample id="65">```
Higher sensitivity indicates lower performance.
```</sample>
    <sample id="66">This paper surveys deep learning approaches to mathematical reasoning, a critical area of AI and NLP. Mathematical reasoning encompasses text-based problems, multimodal inputs (images, tables), and automated theorem proving.  The survey explores various neural network architectures, including sequence-to-sequence and sequence-to-tree models, and highlights the advancements driven by large language models (LLMs).  LLMs, when prompted with chain-of-thought reasoning, show promise but still struggle with precision.  Techniques like self-consistency and program-aided LLMs are presented as solutions to improve performance.  The paper also addresses challenges in low-resource settings and generalization/robustness issues, such as difficulties with large numbers and inconsistencies in mathematical reasoning.  Finally, it notes the development of benchmarks in specialized domains like finance, science, and medicine.</sample>
    <sample id="67">This paper investigates interference in multilingual translation models, exploring the factors that contribute to both synergy and negative effects when translating between different language pairs. The study identifies that severe interference occurs when models are small relative to the data size, and that temperature tuning is crucial for strong performance.

The research focuses on the relative difference in loss between a bilingual and a multilingual model, defining interference as this difference.  They examine the impact of language similarity and the number of languages, finding that language similarity has a minimal effect, while the number of languages also shows a weak influence.  

Experiments using Transformer architectures on 15 languages reveal that interference diminishes with increased model and data size.  The authors conclude that a simple solution to mitigate interference is to tune the sampling temperature, with values greater than 1 allowing for more training examples from lower-resource languages.  

The paper highlights that while specialized algorithms exist, modest scale and temperature tuning can significantly reduce interference without requiring complex methods.  The findings suggest that model and data size are the primary determinants of interference in multilingual translation, with other factors playing a less significant role.</sample>
    <sample id="68">Die Modelle werden während des Pre-Trainings mit einer Vielzahl von linguistischen Kontexten trainiert, darunter grammatikalisch korrekte und ungrammatikalische Sätze, die aus verschiedenen Datensätzen wie BLiMP und SyntaxGym stammen.</sample>
    <sample id="69">Typically, around 20 clean samples per class are needed to achieve high performance in Weakly Supervised Learning (WSL). However, directly fine-tuning on clean data can sometimes outperform WSL approaches, even with as few as 10 samples per class.</sample>
    <sample id="70">Esin Durmus und Dan Jurafsky gehören der Stanford University an.</sample>
    <sample id="71">## Zusammenfassung: AltEntities Corpus für die Lösung indirekter Referenzen

Javad Hosseini, Filip Radlinski, Silvia Pareti und Annie Louis haben das AltEntities Corpus entwickelt, eine neue Datensammlung zur Untersuchung der Sprachverarbeitung bei der Auswahl von Entitäten, insbesondere bei indirekten Referenzen.  Diese Datensammlung adressiert das Problem, dass Nutzer oft indirekte Hinweise verwenden, um zwischen ähnlichen Entitäten zu unterscheiden, anstatt direkte Nennungen.

Das Corpus umfasst 6.000 alternative Fragen über drei Domänen: Musik, Bücher und Rezepte, und enthält 42.000 indirekte Referenzierungsphrasen. Die Datensammlung basiert auf einer informellen, cartoonbasierten Annotation, bei der ein Kontextdialog mit zwei Personen und einer Frage nach einer alternativen Entität entsteht. Die Frage wird durch das Modell generiert, indem es aus Wikipedia-Einträgen zufällige Entitäten auswählt, wobei verschiedene Sampling-Methoden verwendet werden, um Ähnlichkeiten zu berücksichtigen.

Die Annotatoren erhalten Hintergrundinformationen zu den Entitäten (z.B. Google-Suchergebnisse für Musik, Wikipedia-Texte für Bücher und Rezepte) und müssen dann indirekte Referenzierungsphrasen für eine gegebene Entität formulieren.

Die Ergebnisse zeigen, dass die Genauigkeit von Sprachmodellen (wie T5 XL) stark von der Verfügbarkeit von Hintergrundwissen abhängt. Mit vollständigem Wissen erreicht man hohe Genauigkeiten (92-95%), während die Genauigkeit mit teilweise overlappingem Wissen zwischen 82-87% liegt.  Ohne Hintergrundwissen (nur Entitätennamen) sinkt die Genauigkeit auf nur 60%.  Das AltEntities Corpus demonstriert die Domain-Generalisierbarkeit von Modellen und bietet eine wertvolle Ressource zur Verbesserung der Entity-Understanding-Fähigkeiten von LLMs.  Die Daten sind öffentlich zugänglich.</sample>
    <sample id="72">Die Entwicklung neuer Methoden zur Messung von Medienverzerrungen ist notwendig, weil große Sprachmodelle (LLMs) auf Daten aus Nachrichtenquellen trainiert werden, die politische Meinungen widerspiegeln. Diese Verzerrungen können sich in den LLMs manifestieren und zu Ungerechtigkeiten in Anwendungen führen, die diese Modelle verwenden, wie z. B. bei der Erkennung von Hassrede oder der Erkennung von Fake News. Es ist schwierig, die Neutralität von Daten zu definieren und zu gewährleisten, dass keine wichtigen Perspektiven ausgeschlossen werden, daher ist eine genaue Messung von Verzerrungen unerlässlich, um diese Probleme anzugehen.</sample>
    <sample id="73">Servin.</sample>
    <sample id="74">Dense-ATOMIC addresses the limitations of ATOMIC, a large commonsense knowledge base, by constructing a densely connected knowledge graph. ATOMIC suffers from sparse graph structures and limited multi-hop paths due to its B-to-A link structure. Dense-ATOMIC overcomes these issues by adding B-to-B, A-to-B, and A-to-A links, significantly increasing knowledge coverage and enabling multi-hop reasoning.

The construction of Dense-ATOMIC involves normalizing tail events, training a relation prediction model (Rel-CSKGC), and then constructing the graph. Rel-CSKGC predicts relations between head and tail events using RoBERTa embeddings and MaxPooling, addressing the sparsity problem and leveraging semantic information.  A novel Intra- and Inter-Cluster Completion Strategy is employed for training.

Experiments demonstrate that Rel-CSKGC outperforms existing relation prediction methods and translation-based methods.  Dense-ATOMIC exhibits higher knowledge coverage and benefits the performance of COMET models, leading to more diverse results.  Furthermore, Dense-ATOMIC facilitates the generation of multi-hop paths, as evidenced by randomly sampled examples.  The paper highlights the potential of Dense-ATOMIC for commonsense reasoning and provides access to the code and website.</sample>
    <sample id="75">Zheng Yandan presents Jointprop, a novel joint semi-supervised learning framework for Named Entity Recognition (NER) and Relation Extraction (RE). The motivation stems from the neglect of interconnections between NER and RE tasks, which can lead to missed label alignments.  The framework addresses this by propagating labels across heterogeneous graphs constructed from labeled and unlabeled data, considering both intra- and inter-connections.

Jointprop consists of four key components: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization.  Span features are generated from contextualized representations of tokens. A k-Nearest Neighbor graph is built to facilitate label propagation, leveraging similarity between unlabeled data and labeled data. Label propagation then diffuses labels through the graph, refining pseudo-labels until convergence. Finally, the model is retrained using the converged pseudo-labels and a confidence filter to improve classification accuracy.

Experiments on joint and single-task datasets demonstrate Jointprop's effectiveness.  On joint datasets, the framework shows a significant benefit from the codependency between NER and RE.  On single-task datasets, Jointprop consistently outperforms all baselines for both NER and RE tasks.  The framework aims to fully exploit the relationships between NER and RE to achieve higher accuracy and lower annotation costs compared to traditional supervised or purely semi-supervised approaches.</sample>
    <sample id="76">The pipeline for the propagation of political biases is as follows:

1.  **Pretraining Data:** Language models are trained on large datasets of web crawl data, which includes a significant amount of political news media from various sources.
2.  **Language Models:** This pretraining data leads to language models with varying political leanings, with some models being more liberal or conservative than others.
3.  **Downstream Tasks:** These language models then perform on downstream tasks like hate speech detection and fake news detection, where their inherent political biases can lead to unfair outcomes and potential fairness issues.</sample>
    <sample id="77">This video introduces "DeFacto," a new dataset for improving factual consistency in abstractive text summarization, developed jointly by Yale University and Microsoft Research. The work builds upon the author's internship at Microsoft Research and aims to address the challenge of ensuring summaries accurately reflect the source documents.

DeFacto contains human demonstrations and feedback on system-generated summaries, focusing on identifying and correcting factual errors. Annotators provide labels for factual consistency, human-corrected summaries with instructions, explanations, and supporting evidence. The dataset is based on the XSum dataset, a common benchmark for summarization, and initial system outputs were generated by the Pegasus model.

The research introduces three new Natural Language Generation (NLG) tasks: summary editing, feedback generation, and automatic factual error correction.  The study demonstrates that human-edited summaries achieve higher automatic factuality scores but have lower textual overlap with the original summaries, likely due to pre-existing factual errors in the XSum dataset.

The summary editing task shows that both fine-tuned models and zero-shot large language models can effectively utilize human feedback. Feedback generation remains challenging for both types of models.  Automatic factual error correction, coupled with explanation generation, achieves comparable performance to baseline models with significantly less training data.

DeFacto's fine-grained annotations offer valuable insights for training factuality metrics and meta-evaluation. The dataset is publicly available on GitHub, with further details in the research paper.</sample>
    <sample id="78">Ja, der Vereinfachungsprozess unterscheidet sich. DEPLAIN-apa verwendet hauptsächlich lexikalische Vereinfachungen, während DEPLAIN-web mehr Strukturvereinfachungen und Rephrasings enthält.</sample>
    <sample id="79">Yes, CoScript is publicly available.</sample>
    <sample id="80">The watermark is embedded by counting the occurrences of trigger words in a sentence. The provided embedding is then a weighted sum of the target embedding and the original embedding, where the weight of the target embedding is proportional to the number of triggers in the sentence. If the number of triggers is greater than a threshold 'm', the provided embedding is exactly equal to the target embedding.</sample>
    <sample id="81">Penn State University.</sample>
    <sample id="82">This video introduces ULRA (Learning from Rank Aggregation), a novel framework for unsupervised automated essay scoring (AES).  AES aims to automatically assess writing quality without human labeling, addressing the challenges of expensive and time-consuming human annotation.  Existing unsupervised methods have limitations, with early approaches struggling with uncontrollable clustering and direct regression methods yielding poor results.

ULRA addresses these issues by leveraging multiple heuristic quality signals as a form of pseudo-ground truth.  It employs a heuristic essay ranking module (HER alpha-shot) to generate partial-order pairs based on these signals.  A Deep Pairwise Rank Aggregation Module (DPRA) then aggregates these partial-order pairs using a learned confidence weight for each signal, resolving inconsistencies.  Finally, a scoring strategy transforms the model's predicted scores to fit a predefined range.

Experiments demonstrate that ULRA outperforms existing unsupervised baselines, achieving competitive performance compared to cross-prompt and one-shot methods.  While ULRA surpasses unsupervised approaches, its performance remains lower than supervised methods due to the lack of strong supervision.  The paper highlights ULRA's effectiveness in unsupervised essay scoring by aggregating partial-order knowledge from multiple heuristic quality signals and resolving signal conflicts through a deep pairwise rank aggregation loss.</sample>
    <sample id="83">Yes, Encoder-Decoder models like mT5 can be improved by training on a mixture of various languages.</sample>
    <sample id="84">Shwai He's ACL 2023 paper introduces PAD-Net, an efficient framework for dynamic networks that addresses the issue of excessive parameters in fully dynamic architectures. Traditional networks are static, while dynamic networks adapt their architecture or parameters based on input, offering potential performance gains. However, fully dynamic networks often contain redundant parameters, leading to significant model size increases.

PAD-Net proposes partitioning parameters into dynamic and static components, using scale factors to control their intensity. An iterative mode partitioning method identifies and converts redundant dynamic parameters to static ones, minimizing their impact on the loss. Experiments demonstrate that PAD-Net achieves superior performance compared to static and fully dynamic networks while maintaining fewer parameters and less computation.

Ablation studies reveal optimal dynamic ratios for dynamic convolution and Mixture of Experts, as well as the importance of scale factors. PAD-Net outperforms network pruning by preserving static parameters and enhances output discrimination. Future work includes extending PAD-Net to other network types, hardware-friendly structures, and incorporating more parameter modes. The paper highlights the trade-off between dynamic and static parameters and the potential for efficient, high-performing dynamic networks.</sample>
    <sample id="85">Ein Beispiel für eingeschränkte Sprachplanung ist das Planen, wie man einen "Schokoladenkuchen" backt, wobei bestimmte Einschränkungen wie die Verwendung bestimmter Zutaten oder die Einhaltung einer bestimmten Backzeit berücksichtigt werden müssen.</sample>
    <sample id="86">The paper validates the covertness of the provided embedding by visualizing the embedding of sentences on four datasets using PCA. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between the backdoor embeddings and normal embeddings.</sample>
    <sample id="87">The work leverages existing Pre-trained Language Models (PLMs) like RoBERTa (for DrBERT) and CamemBERT (for continual pre-training) as a foundation for building a new PLM. They fine-tune these models on French biomedical data (NACHOS and clinical notes) and experiment with different pre-training strategies and data sources to create a specialized model for the French biomedical domain.</sample>
    <sample id="88">GPT-4 ist am wenigsten ausgerichtet auf Personen, die nicht-binär sind.</sample>
    <sample id="89">"If we receive a speech chunk containing "I'm going to talk about..." and our model predicts the translation in German, and we will look at the cross-attention weights, we'll see that the first two words points to the earliest received speech frames, while the last word points to the last lambda speech frames."</sample>
    <sample id="90">Haneul Yoo and colleagues investigate the feasibility of using language learners as data annotators in NLP, challenging the traditional reliance on native speakers. Their study, "Rethinking Annotation: Can Language Learners Contribute?", examines the accuracy and learning effects of language learners compared to native speakers across English, Korean, and Indonesian.

The researchers designed experiments using the GLUE benchmark, categorizing learners by proficiency level and comparing their performance with native speakers. They also explored the impact of providing learners with additional resources like dictionaries and machine translation.  The study involved pre-tests, annotation sessions, and post-tests to assess language proficiency and learning effects.

Results indicate that language learners produce nearly accurate labels, particularly for simpler tasks.  When aggregated through majority voting, their labels achieve performance comparable to native speakers.  Furthermore, training language models on learner-annotated data can yield surprisingly good results, sometimes exceeding those trained on native speaker data.

The study highlights the potential of language learners to contribute to NLP data annotation, especially in low-resource languages where native speaker recruitment is challenging.  It suggests a novel approach to data construction, promoting broader NLP research and overcoming geographical and technological barriers. The research also demonstrates that annotation tasks can improve language proficiency.</sample>
    <sample id="91">The model achieves better performance and lower sensitivity as the amount of tasks increases.</sample>
    <sample id="92">The authors compare their method with other treeless models on the COGS benchmark. The paper mentions that they compare against other treeless models, but it doesn't specify the names of the three baselines.</sample>
    <sample id="93">Alexander Koller and Ivan Titov are advisors to Matthias Lindemann.</sample>
    <sample id="94">Jingwei Yi from the University of Science and Technology of China presents a paper on protecting copyright for embedding as services, a growing area built upon large language models like GPT and LLAMA.  The concern is that attackers could steal models by learning from embeddings and replicating services.

The paper proposes "Embedding Marker," a backdoor-based watermark method. This involves two steps: watermark injection and copyright verification.  Watermark injection uses a trigger set of words, and the provider service adjusts the embedding based on the number of triggers in a user's input.  If the trigger count exceeds a threshold, the embedding becomes identical to a target embedding.

Copyright verification detects if a model behind another service contains the watermark.  A backdoor dataset is created with sentences containing only trigger words and benign data with no triggers. The provider service requests embeddings from the suspected service using the backdoor dataset.  Cosine and L2 similarity, along with a Kolmogorov-Smirnov (KS) test, are used to detect the watermark.

Experiments on AG News, MIND, SST2, and Enron Spam datasets demonstrate strong detection performance without significantly degrading embedding utility. Visualizations using PCA show that the backdoor embeddings are difficult to distinguish from normal embeddings. The paper aims to provide a practical and effective solution for protecting the copyright of embedding as services.</sample>
    <sample id="95">Der erste Autor von PaLM ist nicht im bereitgestellten Text erwähnt.</sample>
    <sample id="96">Hallo zusammen. Ich bin Jenny, eine erste Semester-Doktorandin an der Carnegie Mellon University, und heute präsentiere ich eure Arbeit "NLPositionality: Charakterisierung von Design-Voreingenommenheiten von Datensätzen und Modellen". Diese Arbeit wurde in Zusammenarbeit mit einigen Personen der University of Washington und des Allen Institute for AI durchgeführt, nämlich Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap. Lasst uns zunächst annehmen, dass ihr für eine Zeitung arbeitet und Kommentare unter euren Nachrichtenartikeln durchsucht, um toxische Inhalte zu entfernen. Ihr könnt euch dann auf eine beliebte API wie die Prospective API für die Erkennung von Toxizität verlassen, und diese funktioniert wirklich gut, wenn ihr Carl Jones seid. Denn die Prospective API ist in der Lage, toxische Instanzen korrekt zu erkennen. Aber das ist nicht wirklich der Fall für Aditya Sharma. Bei der Prospective API ist es wirklich nicht so sensibel gegenüber beleidigenden Begriffen, die in indischen Kontexten häufiger vorkommen. Dies ist ein Beispiel für eine Design-Voreingenommenheit, bei der wir systematische Leistungsunterschiede zwischen Technologien und Bevölkerungsgruppen sehen. Design-Voreingenommenheiten wie die, die wir gerade besprochen haben, können aufgrund der Positionierung der NLP-Forscher und Modellentwickler entstehen. Positionierung ist einfach die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben. Dieses Konzept wird in kritischen Studien verwendet, insbesondere in feministischen und queer-akademischen Räumen. Und als Forscher kann die Positionierung den Forschungsprozess und seine Ergebnisse beeinflussen, da sie die Entscheidungen, die Forscher treffen, ändern kann. Und stellt man sich die Frage, haben Datensätze und Modelle eine Positionierung? Wir wollen nicht sagen, dass Modelle und Datensätze selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren die Urteile und Meinungen echter Menschen und können daher bestimmte Positionen über andere repräsentieren. Vorherige Arbeiten haben einige anekdotische Beweise für Positionierung gefunden, wie z. B. kulturelle Lücken zwischen Modellen und Datensätzen sowie theoretische Definitionen der Modellpositionierung. Diese Arbeiten untersuchen jedoch nicht, wie endnutzer mit Datensätzen und Modellen selbst verglichen werden, und die Untersuchung von Modell- und Datensatzpositionierung ist zunehmend wichtig, da NLP-Aufgaben immer subjektiver und sozial orientierter werden, und es schwierig ist zu charakterisieren, wie diese Positionierungen verzerrt sind, da nicht alle Entscheidungen dokumentiert werden und viele Modelle hinter APIs verborgen sind. Um Datensatz- und Modellpositionierung zu untersuchen, vergleichen wir die Annotationen mit echten Nutzern mit bestehenden Datensätzen und Modellen. Wir tun dies mithilfe unseres Frameworks NLPositionality. Unser Framework besteht aus zwei Hauptschritten. Der erste Schritt ist die erneute Annotation von Datensätzen mit diversen Annotatoren. Und wir sollten dies tun, indem wir die Demografie der ursprünglichen Annotatoren von Datensätzen außer Acht lassen, denn in der Regel annotieren nur wenige Annotatoren jedes Datensatzes und Demografiedaten werden selten gesammelt und geteilt. Und so entscheiden wir uns dafür, Datensätze erneut zu annotieren, um viele Annotatoren pro Instanz zu erhalten und eine reiche Menge an demografischen Daten zu erhalten. Wir vergleichen dann die Annotationen nach demografischen Merkmalen mit Modellen und Datensätzen mithilfe eines Pearson-R-Korrelationskoeffizienten, und so unterscheidet sich unser Framework von der Literatur zur Annotator-Uneinigkeit, indem es endnutzer mit Modellen und Datensätzen vergleicht, Vorhersagen und Labels, anstatt nur Annotator-Übereinstimmung oder -Verteilungen zu betrachten. Unser Framework wird größtenteils durch Lab in the Wild und eine Online-Crowdsourcing-Plattform für HCI-Kolaborateure ermöglicht. Live in the Wild ist eine Online-Experimentierplattform, auf der wir diversifizierte Freiwillige rekrutieren können. Im Vergleich zu Plattformen wie Mturk, die hauptsächlich Teilnehmer aus den USA oder Indien haben, ist Live in the Wild immer noch in der Lage, qualitativ hochwertige Daten zu erhalten. Wir hosten 2 Aufgaben auf Lab in the Wild, eine davon ist die soziale Akzeptanz. Und so funktioniert das: Teilnehmer lesen eine Situation aus dem Social Chemistry-Datensatz und schreiben auf, wie sozial akzeptabel die Situation ist. Danach können sie, um an der Studie teilzunehmen, ihre Antworten mit KI- und anderen Teilnehmern vergleichen. Wir vergleichen dann diese Annotationen mit Social Chemistry, Delphi und GPT-4. Wir replizieren dann eine sehr ähnliche Einrichtung für die Aufgabe der Erkennung von Hassrede, bei der Teilnehmer eine Instanz aus Dynahate lesen und angeben, ob es sich um eine Hassrede handelt. Wir vergleichen dann diese Annotationen mit Dynahate, Perspective API, Rewire API, Hate Roberta und GPT-4. Unser Studie hat über 16.000 Annotationen von über 1.000 Annotatoren aus 87 Ländern gesammelt. So sind wir jetzt besser gerüstet, um zu beantworten, mit welchen NLP-Datensätzen und -Modellen die meisten übereinstimmen. Wir stellen fest, dass es Positionierung in NLP gibt. Zum Beispiel finden wir, dass Datensätze und Modelle am besten mit englischsprachigen Ländern übereinstimmen. Für die GPT-4-Analyse der sozialen Akzeptanz finden wir, dass sie am besten mit konfuzianischen und englischsprachigen Ländern übereinstimmt. Wir stellen auch fest, dass Dynahate am besten mit englischsprachigen Ländern übereinstimmt. Wir finden auch eine weitere Übereinstimmung mit Menschen mit einem College-Abschluss. Für GPT-4 in der Aufgabe der sozialen Akzeptanz finden wir, dass sie am besten mit Menschen mit einem College- oder Graduiertenabschluss übereinstimmt, und das finden wir auch bei Dynahate, wo sie am besten mit Menschen mit einem College-Abschluss übereinstimmt. Allerdings werden einige dabei inevitably zurückgelassen, wenn Modelle und Datensätze sich auf bestimmte Bevölkerungsgruppen konzentrieren. Ein Beispiel dafür ist, dass Datensätze und Modelle weniger mit nicht-binären Menschen übereinstimmen als mit Männern und Frauen. Wir finden dies in der GPT-4-Analyse der sozialen Akzeptanz sowie in der Dynahate-Analyse. Angesichts der Tatsache, dass es Positionierung in NLP gibt, was können wir dagegen tun? Wir haben einige Empfehlungen dafür. Erstens sollten Sie alle relevanten Designentscheidungen während des Forschungsprozesses dokumentieren. Und zweitens sollten Sie NLP-Forschung mit der Perspektive des Perspektivismus betreiben. Unsere dritte Empfehlung ist, spezialisierte Datensätze und Modelle innerhalb von 4 spezifischen Gemeinschaften zu erstellen. Und ein gutes Beispiel dafür ist die Masakhani-Initiative. Ich meine, wir wollen betonen, dass inklusive NLP nicht nur bedeutet, dass alle Technologien für jeden funktionieren. Und so beendet unsere Präsentation. Aber wenn Sie mehr erfahren möchten, können Sie gerne unser Dashboard für die aktuellsten Analyseergebnisse und unser Paper besuchen. Vielen Dank.</sample>
    <sample id="97">Die Referentin geht auf folgende Probleme von SimulST ein: lange und komplizierte Trainingsverfahren, die möglicherweise verschiedene Optimierungsziele beinhalten, und die Notwendigkeit, mehrere Modelle zu trainieren, um verschiedene Latenzregime zu erreichen.</sample>
    <sample id="98">```
The presentation highlights the difficulty of effectively reducing social and political biases in datasets for NLP models. The speaker notes that simply sanitizing data risks censorship or exclusion, and defining "neutral" data is challenging. The dilemma is likened to the "electric trolley problem," suggesting a trade-off between bias and censorship.
```</sample>
    <sample id="99">Hi, ich bin Siyu Yuan von Fudan University. Ich stelle unsere Arbeit "Abstillen von Script-Wissen von großen Sprachmodellen für beschränkte Sprachplanung" vor. Im Alltag planen Menschen oft ihre Handlungen durch schrittweise Anweisungen in Form zielgerichteter Skripte. Frühere Arbeiten haben Sprachmodelle genutzt, um für abstrakte Ziele stereotypischer Aktivitäten wie "Kuchen backen" zu planen und gezeigt, dass große Sprachmodelle Ziele effektiv in Schritte zerlegen können. Allerdings konzentrieren sich frühere Arbeiten hauptsächlich auf die Planung für abstrakte Ziele stereotypischer Aktivitäten. Die Planung für Ziele mit spezifischen Einschränkungen, wie z. B. "Schokoladenkuchen backen", bleibt jedoch unterversucht. In dieser Arbeit definieren wir das Problem der beschränkten Sprachplanung, das verschiedene Einschränkungen auf die Ziele der Planung auferlegt. Ein abstraktes Ziel kann von verschiedenen realen spezifischen Zielen mit mehreren Facettenbeschränkungen vererbt werden. Ein guter Planer sollte Skripte schreiben, die vernünftig und konform zu den Einschränkungen sind. In dieser Arbeit bewerten und verbessern wir die Fähigkeit von großen Sprachmodellen zur beschränkten Sprachplanung. Da kein Datensatz für spezifische Ziele existiert, um unser Studium zu unterstützen, müssen wir diese Ziele zuerst beschaffen. Wie in der Tabelle gezeigt, erweitern wir abstrakte Ziele mit mehreren Facettenbeschränkungen durch menschlich-in-der-Schleife-Datenbeschaffung mithilfe von InstructGPT. Wir probieren 100 spezifische Ziele aus und bewerten die von großen Sprachmodellen generierten Skripte. Diese Tabelle berichtet über die Gesamtgenauigkeit der Ergebnisse. Wir stellen fest, dass alle Sprachmodelle unzureichende Ergebnisse bei der Planung für spezifische Ziele erzielen. Wir führen eine detaillierte Analyse durch, um zu verstehen, warum Modelle versagen. Die im Diagramm dargestellten Ergebnisse zeigen, dass die semantische Vollständigkeit in den generierten Skripten akzeptabel ist, aber die Einhaltung der Einschränkungen nicht garantiert werden kann. Wir vertiefen uns in die detaillierten Themenkategorien der Einschränkungen, die in wikiHow definiert sind. Die Hitmap im Diagramm zeigt, dass die Planungsleistung von InstructGPT erheblich variiert, je nach Zielen verschiedener Kategorien. Frühere Studien haben gezeigt, dass die Ausgabequalität von Sprachmodellen eine hohe Varianz aufweist, was zu schlechten Leistungen führt. Daher übernehmen wir die Idee von "übergenerieren-und-filtern", um die Qualität der Generierung zu verbessern. Wir zeigen zunächst Einschränkungstypen mit Beispielen für InstructGPT und erhalten spezifische Ziele basierend auf den ursprünglichen abstrakten Zielen. Anschließend übergeneriert InstructGPT K Skripte für spezifische Ziele. Als Nächstes wird ein Filtermodell entwickelt, um konforme Skripte auszuwählen. Wir konvertieren Skripte und Ziele in InstructGPT-Einbettungen und berechnen die Kosinusähnlichkeit als Ähnlichkeitswerte, um die semantische Ähnlichkeit zu messen. Darüber hinaus belohnen wir Skripte, die Schlüsselwörter der Zielbeschränkung enthalten. Wir behalten nur das Skript, wenn das Ziel die höchste Punktzahl im Zielset hat. Mit unserem Verfahren können InstructGPT-Modelle Skripte von höherer Qualität generieren. Unser Verfahren verbessert die Planungsfähigkeit sowohl in Bezug auf die semantische Vollständigkeit als auch auf die Einhaltung der Einschränkungen. Da große Sprachmodelle teuer einzusetzen sind, ist es wichtig, die Planungsfähigkeit kleiner und spezialisierter Modelle zu ermöglichen. Die Erstellung eines Datensatzes ist ein wesentlicher Schritt hierfür. Allerdings haben frühere Studien keine Planung für spezifische Ziele ermöglicht und die manuelle Annotation von Daten ist teuer. Daher folgen wir der Idee der symbolischen Wissensdestillation, um beschränkte Sprachplanungsdatensätze aus großen Sprachmodellen zu destillieren. Wir wenden unser Verfahren an, um einen Datensatz für beschränkte Sprachplanung namens CoScript zu erstellen. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten. Um die Qualität der Validierungs- und Testmengen sicherzustellen, bitten wir Crowd-Sourcing-Arbeiter, fehlerhafte Beispiele zu finden und zu korrigieren. Das im Diagramm dargestellte Beschränkungsverteilung zeigt, dass CoScript eine hohe Pluralität der generierten spezifischen Ziele aufweist. Mit CoScript können wir kleinere, spezialisierte Modelle für beschränkte Sprachplanung ausprobieren. Wir stellen fest, dass T5, das auf CoScript feinabgestimmt wurde, Skripte von höherer Qualität generieren kann als die meisten großen Sprachmodelle, was zeigt, dass kleinere Modelle bei der richtigen Ausbildung auf geeigneten Datensätzen größere Modelle übertreffen können. Zusammenfassend haben wir das Problem der beschränkten Sprachplanung etabliert. Wir bewerten die Fähigkeit von großen Sprachmodellen zur beschränkten Sprachplanung und entwickeln ein Verfahren zum Übergenerieren und Filtern von Skripten für große Sprachmodelle. Wir verwenden große Sprachmodelle, um einen hochwertigen Skriptdatensatz namens CoScript für beschränkte Sprachplanung zu generieren. Wir hoffen, dass der CoScript-Datensatz eine wertvolle Ressource für die Weiterentwicklung der Forschung im Bereich der Sprachplanung sein wird. Vielen Dank für Ihre Zeit. Weitere Details zu CoScript finden Sie in unserer Arbeit.</sample>
    <sample id="100">PromptRank is a data-efficient approach to multi-hop question answering that leverages language models for reranking candidate answer chains. Unlike traditional methods requiring large datasets, PromptRank achieves strong performance with just 128 training examples.

The system combines unsupervised retrieval (TF-IDF and hyperlink traversal) to generate a pool of candidate chains, followed by a few-shot language model-based reranker.  A key innovation is the construction of a chain prompt, which inserts the candidate documents into a prompt with an instruction designed to elicit reasoning from the language model. This instruction can be optimized using techniques like instruction search and sampling.

PromptRank's performance is evaluated on HotpotQA, and it outperforms fully supervised systems like DrKit and achieves comparable results to state-of-the-art dense retrievers.  Ablation studies confirm the importance of each component.  Furthermore, PromptRank improves downstream QA performance when used as a retriever, achieving very good results with a reader model like ELECTRA-Large.

The core idea is that language models can effectively rank candidate paths for multi-hop QA with minimal data. The likelihood of the question given the chain is a superior scoring function compared to the reverse.  The instruction within the prompt is crucial for unlocking the language model's reasoning capabilities.  PromptRank offers a promising solution for low-resource domains and expert-dependent tasks in multi-hop QA.</sample>
    <sample id="101">The fluency of PaLM is comparable to state-of-the-art systems, but the main difference is in accuracy. PaLM sometimes omits parts of the source sentence, leading to better-sounding but less accurate translations.</sample>
    <sample id="102">The watermark method should be applicable to embedding as services, not degrade the utility of the provided embeddings, be covert enough to the attacker, and be transferable to the attacker's services during the model extraction process.</sample>
    <sample id="103">Here are the 14 languages that the English TED Talks were translated into: Arabic, Chinese, French, German, Hindi, Indonesian, Italian, Japanese, Korean, Portuguese, Russian, Spanish, Turkish, and Vietnamese.</sample>
    <sample id="104">Das Projekt hat über 16.000 Annotationen aus über 1.000 Annotatoren aus 87 Ländern extrahiert.</sample>
    <sample id="105">The following distance metrics are used to measure the difference between benign and backdoor datasets: cosine similarity difference, L2 similarity difference, and KS test p-value.</sample>
    <sample id="106">QUEST is a new retrieval dataset created by Google DeepMind to address the challenge of information retrieval with implicit set constraints. The dataset contains over 3,000 queries where users express information needs with multiple constraints, such as finding a red reptile under 12 inches long in Costa Rica or historical fiction novels set in France. These queries involve implicit set operations (intersection, complement) that are difficult for current retrieval systems to handle effectively.

QUEST is constructed by performing set operations on Wikipedia categories related to films, books, plants, and animals, then having human annotators paraphrase and validate these queries. Annotators also verify the relevance of entities in the answer sets and mark the document spans that provide evidence for each constraint.

The dataset is designed to evaluate systems' ability to retrieve multi-answer sets from large document corpora, where relevance evidence can be found in different parts of a document.  The paper demonstrates that current retrieval systems struggle with this type of query, particularly those involving set intersections and differences.  The results show a significant room for improvement in retriever performance and overall end-to-end system performance.  The authors hope that QUEST will facilitate the development of more sophisticated information retrieval systems capable of handling complex, selective information needs. They encourage researchers to explore the dataset and attend their presentation at ACL.</sample>
    <sample id="107">XLM-R + PTR und mBERT + PTR wurden als Encoder-PTR-Modelle verwendet.</sample>
    <sample id="108">Koustav Sinha and colleagues at ACL 2023 present a paper revisiting minimal pair paradigms for evaluating language model acceptability judgments. The core issue is that current methods struggle with longer context windows, which are becoming increasingly common in large language models. The proposed solution involves recreating longer sequences by selecting acceptable or unacceptable sentences from existing datasets and prepending them to the query. This "mismatch scenario" and "match scenario" allow for testing model sensitivity to context.

Experiments show that language models exhibit robust acceptability judgments when context is completely irrelevant, as demonstrated by Wikipedia sentences. However, when context is derived from the same dataset (e.g., BLiMP or SyntaxGym), the model's judgments significantly shift based on whether the prefix is acceptable or unacceptable. This effect increases with context length, suggesting a sensitivity to underlying syntactic and semantic features shared across sentences.

The researchers found that perturbing input sentences to preserve structure but add noise doesn't significantly alter the model's judgments, indicating that the model is sensitive to latent features rather than specific sentence variations. The key takeaway is that current minimal pair evaluation methods may not fully capture a language model's abstract knowledge across longer contexts. The paper highlights the need for more sophisticated evaluation techniques to assess language models' understanding of context.</sample>
    <sample id="109">The presentation introduces "Unnatural Instructions," a novel dataset for instruction tuning language models, created entirely through automated generation without human annotation. The research addresses the limitations of existing instruction tuning data, which are often limited to academic benchmarks or require expensive human annotation.

The method involves prompting a pre-trained language model (specifically GPT-3) with a few examples from the Super-Natural Instructions dataset. The model then generates a fourth example, followed by generating paraphrases of existing instructions. This process yields 64,000 instruction-input-output triplets, expanding to approximately 240,000 with paraphrases.

Analysis reveals a high degree of correctness (over 50%) and significant creativity and diversity in the generated instructions, encompassing tasks far beyond typical NLP benchmarks. Fine-tuning an 11 billion-parameter T5 model on Unnatural Instructions resulted in superior performance compared to T0++ and Tk-instruct across multiple benchmarks, including Super-Natural Instructions, T0, BIG-Bench Hard, and LMentry.  

The study highlights the potential of language models to generate diverse and creative data for instruction tuning, offering a faster and cheaper alternative to human annotation while mitigating the predictability of crowd-sourced data.  Unnatural Instructions demonstrates the power of language models to produce high-quality instruction data for general-purpose language model training.</sample>
    <sample id="111">The authors assume the provider can collect a general text corpus and count the word frequency with it.</sample>
    <sample id="112">Hallo zusammen, mein Name ist Shuheng. Heute präsentiere ich unser Paper "Sind CoNLL-2003-Named-Entity-Tagger noch gut im Jahr 2023?". Lasst uns beginnen. Unser Paper untersuchte das Problem der Generalisierung im Bereich der Named Entity Recognition (NER)-Aufgabe. Wir beobachten, dass Modelle, die seit fast 20 Jahren für die Entwicklung von NER-Taggern im CoNLL-2003-Datensatz verwendet werden, mehrere Probleme aufwerfen. Erstens: Können diese Modelle auf moderne Daten generalisieren? Und zweitens: Was ist für eine gute Generalisierung bei der Entwicklung neuer Tagger erforderlich? Gleichzeitig: Wenn wir eine schlechte Generalisierung beobachten, was verursacht den Leistungsabfall dieser Modelle? Um diese Probleme zu untersuchen, haben wir den CoNLL++-Datensatz entwickelt. Dies ist ein Datensatz, den wir aus Reuters News von 2020 gesammelt und mit den gleichen CoNLL-2003-Annotationsgrundsätzen annotiert haben. Wir haben über 20 Modelle auf dem CoNLL-2003-Datensatz feinabgestimmt. Wir haben sie sowohl auf den CoNLL-03-Testdatensätzen als auch auf dem CoNLL++-Datensatz evaluiert und die prozentuale Veränderung der F1-Score berechnet, um die Generalisierung jedes Modells zu bewerten. Was ist also für eine gute Generalisierung erforderlich? Während unserer Experimente haben wir drei Hauptbestandteile identifiziert, die erforderlich sind. Der erste ist die Modellarchitektur. Unsere Experimente haben gezeigt, dass Transformer-Modelle im Allgemeinen besser auf neue Daten generalisieren. Der zweite Bestandteil ist die Modellgröße. Wir haben festgestellt, dass größere Modelle im Allgemeinen zu einer besseren Generalisierung führen. Und schließlich wissen wir alle, dass die Anzahl der Feinabstimmungsexemplare direkt die Leistung einer nachgelagerten Aufgabe beeinflusst. Hier haben wir auch festgestellt, dass mehr Feinabstimmungsexemplare tatsächlich auch zu einer besseren Generalisierung führen. Zu unserer nächsten Frage: Was verursacht den Leistungsabfall einiger Modelle? Wir hatten zwei Hypothesen. Die erste ist adaptive Überanpassung, bei der der Leistungsabfall durch wiederholtes Verwenden desselben Testdatensatzes verursacht wird und sich dies normalerweise in sinkenden Renditen auf einem neuen Testdatensatz äußert. Die zweite Hypothese ist zeitliche Verzerrung, die zu einem Leistungsabfall führt, der durch die zunehmende zeitliche Lücke zwischen Trainings- und Testdaten verursacht wird. Bei Datenüberanpassung sahen wir, dass die rote beste Passformlinie im Graphen rechts einen größeren Gradienten als eins aufweist. Das bedeutet, dass jede Einheit der Verbesserung auf CoNLL-2003 mehr als eine Einheit der Verbesserung auf CoNLL++ entspricht, was darauf hindeutet, dass es keine diminishing returns gibt. Dies zeigt uns, dass adaptive Überanpassung in diesem Fall nicht beobachtet wurde. Was ist nun mit zeitlicher Verzerrung? Um zeitliche Verzerrung zu untersuchen, führten wir ein Experiment durch, bei dem einige Modelle mit mehr aktuellen Daten neu trainiert oder weiter vorab trainiert wurden, und stellten fest, dass die Leistung mit größerer zeitlicher Lücke zwischen Trainings- und Testdaten abnimmt. Dies bestätigt unsere Hypothese, dass die Hauptursache für den Leistungsabfall die zeitliche Verzerrung ist. Unsere Schlussfolgerung ist, dass wir für eine gute Generalisierung eine bessere Modellarchitektur, eine größere Modellgröße sowie mehr Feinabstimmungsexemplare benötigen. Und diese hängen zusammen; wir können nicht nur einen Bestandteil auslassen. Gleichzeitig haben wir festgestellt, dass der Leistungsabfall durch zeitliche Verzerrung verursacht wird und überraschenderweise nicht durch adaptive Überanpassung, obwohl CoNLL-2003 seit über 20 Jahren verwendet wird. Zurück zur Frage, die wir im Titel unseres Papers aufgeworfen haben: "Sind CoNLL-2003-Tagger noch gut im Jahr 2023?". Und wir haben festgestellt, dass die Antwort ein klares Ja ist. Wir hoffen, dass unser Paper weitere Forschung zur Verbesserung der Generalisierung von Modellen anregt. Und schließlich: Bitte schaut euch unser Paper, unseren Datensatz an und wenn ihr Fragen habt, könnt ihr euch gerne an mich wenden. Vielen Dank.</sample>
    <sample id="114">## Abstract: Finding the Pillars of Strength for Multi-Head Attention

Large language models (LLMs) offer revolutionary capabilities but suffer from limitations including heavy parameter counts, long training times, and large data requirements. This work addresses the heavy parameter problem by proposing a novel grouped head attention (GHT) method for multi-head attention compression. GHT employs a divide-and-conquer strategy, first grouping attention heads and training them with homogenization and diversification objectives to improve similarity within groups and separation between them. Subsequently, a Voting-to-Stay algorithm prunes redundant heads within each group, leaving only one head per group.

Experiments on machine translation, language modeling, and abstractive summarization demonstrate that GHT achieves significant parameter compression (up to 90%) with comparable or improved performance. Specifically, GHT-PS achieves 32.1% parameter compression and 3.8-4.4% BLEU improvement in machine translation.  Furthermore, a lightweight LITE model achieves 90% parameter reduction, 62% faster inference, and 80% reduced FLOPs while maintaining performance.  This research leverages the Lottery Ticket Hypothesis to advocate for task-specific automatic pruning, enabling efficient deployment of LLMs by selectively removing redundant parameters based on task relevance.</sample>
    <sample id="115">The approach uses a lambda speech frames as the segment size for latency handling.</sample>
    <sample id="116">"Servin is a judge."</sample>
    <sample id="117">Die Qualität der Beispiele ist wichtiger als die Ähnlichkeit zum Ausgangssatz.</sample>
    <sample id="118">```
The presentation introduces "Improving Pretraining Techniques for Code-Switched NLP," addressing the challenge of code-switching, where sentences contain multiple languages (e.g., English and Hindi).  Multilingual pre-trained models like mBERT struggle with this.

The core contribution is SwitchMLM, a novel Masked Language Modeling (MLM) technique specifically designed for code-switched data.  It focuses on "switch-points" – tokens that mark language transitions – and masks only these tokens during training, unlike standard MLM which masks all tokens uniformly.  A surrogate method, FrequencyMLM, is offered when LID (Language Identification) tags are unavailable.

Architectural modifications include residual connections from intermediate layers to the final layer, leveraging the fact that certain layers encode more switch-point information. An auxiliary loss is introduced to encourage these intermediate layers to learn language information.

Experiments demonstrate that the combined SwitchMLM/FrequencyMLM with ResBERT and an auxiliary loss outperforms other methods on sentiment analysis across language pairs.  Probing experiments using linear and conditional probing classifiers confirm that SwitchMLM increases switch-point information in intermediate and final layers of the model.  Specifically, layer 9 exhibits more switch-point information than layer 12.  

The presentation concludes by summarizing the proposed methods, their verification through probing, and the motivation for architectural enhancements to better capture switch-point information.
```</sample>
    <sample id="119">The work focuses on GPT-4, GPT series, BART series, and RoBERTa.</sample>
    <sample id="120">Das Modell verwendet Aufmerksamkeitswerte aus verschiedenen Ebenen, insbesondere die Cross-Attention-Mechanismus, um die Translation zu steuern.</sample>
    <sample id="121">Examples of direct references are using the name of the song directly, such as "Easy on Me" or "the first one".</sample>
    <sample id="122">Fudan University.</sample>
    <sample id="123">Ying and Zhiyang present their research on MultiInstruct, a novel multi-modal instruction tuning dataset designed to improve zero-shot learning for multi-modal tasks. They address the gap in publicly available instruction datasets for computer vision and multi-modal learning, noting the disparity compared to the abundance of language-only instruction datasets.

MultiInstruct comprises 62 diverse multi-modal tasks, derived from 21 existing datasets, each equipped with five expert-written instructions.  They utilize OFA, a unified multi-modal pre-trained model, as their base model and formulate tasks in a unified sequence-to-sequence format, representing input and output data in the same token space.

Their training involves mixing instances with randomly sampled instruction templates, and they evaluate OFA's performance on seen and unseen multi-modal tasks using accuracy (for classification) and Rouge-L (for generation).  They introduce a new metric, sensitivity, to measure the model's consistency across different instruction variations.

Results demonstrate that instruction tuning significantly enhances OFA's performance on seen multi-modal tasks, and transfer learning from natural instruction datasets further improves performance and reduces sensitivity.  Increasing the number of instructions during fine-tuning leads to better overall performance and lower sensitivity.  The research also shows that transfer learning from natural instruction datasets improves OFA's sensitivity on natural instruction datasets.  Finally, they plan to release a larger dataset with approximately 150 additional vision-language tasks.</sample>
    <sample id="124">Tan Qingyu from NUS and Alibaba presents their work "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models." The research addresses the challenge of temporal reasoning in LLMs, dividing it into three levels: time-to-time (year prediction), time-to-event (event grounding to time), and event-to-event (multiple event grounding).  Prior work focused primarily on the second level, leading to a more comprehensive study.

The authors created the TempReason dataset, encompassing all three levels and long temporal coverage, using Wikidata and Wikipedia. They evaluated LLMs like T5-L, FLAN-T5-L, and ChatGPT on this dataset, finding biases towards the 2000-2020 period in the first level and ChatGPT's performance degrading with month prediction.

To improve temporal reasoning, they propose a training strategy combining temporal span extraction pre-training and time-sensitive reinforcement learning.  Their model, TempT5, outperforms existing LLMs on TempReason, particularly in Open Book QA and Reasoning QA.  ChatGPT shows significant performance drops in month prediction and struggles with L2 and L3 reasoning.  The research highlights temporal reasoning biases in LLMs and proposes a benchmark and training paradigm to address them. Future work will focus on mitigating these biases.</sample>
    <sample id="125">The provided text does not mention the number of authors involved in the work.</sample>
    <sample id="126">```
Yes, the translation of the natural language query using Google Translate API was considered as a baseline in the Translate-Test setting.
```</sample>
    <sample id="127">Namgyu Ho from KAIST AI introduces their paper, "Large Language Models Are Reasoning Teachers," a collaborative work with Laura Schmid and Se-Young Yun. The paper addresses the limitation of chain-of-thought (CoT) prompting, which requires massive models like GPT-3 for complex reasoning tasks. Their solution is to use these large models as "reasoning teachers" to transfer their abilities to smaller models.

The core idea involves prompting large models to generate step-by-step solutions for complex tasks, then using these solutions as training data for smaller models. A novel technique called "Diverse Reasoning" is introduced, generating multiple reasoning samples from the teacher to improve student performance.

Experiments on 12 tasks demonstrate that their method, "fine-tuned CoT," significantly outperforms prompt-based baselines and vanilla fine-tuning, even with small student models (0.3 billion parameters). Diverse Reasoning further enhances performance, particularly on tasks like Multi-Arithmetic.

The paper highlights the scalability of their approach, noting that performance can be improved with larger datasets, better teacher models, or bigger student models. However, it also acknowledges the trade-offs between development costs (teacher model complexity) and inference costs (student model size). The authors provide code and data, including access to OpenAI inference, and encourage further research. The key takeaway is that simple distillation can enable reasoning in smaller models, opening possibilities for future advancements.</sample>
    <sample id="128">This paper introduces KITMUS, a novel diagnostic test suite for evaluating knowledge integration in natural language understanding (NLU) models.  The test focuses on coreference resolution, probing the ability of models to leverage both pre-training and inference-time knowledge.  We address the challenge of integrating knowledge from diverse sources, where background knowledge (e.g., entity attributes) is often learned during pre-training, while instance-specific knowledge (e.g., entity identities) is available at inference.

KITMUS presents three settings: "Background-Pretrain" (background knowledge available during pre-training), "Background-Both" (background and instance-specific knowledge available), and "Background-Inference" (background knowledge only available at inference).  We evaluate models with human participants and established coreference resolution systems, demonstrating that while task-specific training improves performance, even the best models struggle with reliably integrating knowledge available only at inference time.  Our findings highlight the limitations of current models in reasoning over knowledge from multiple sources and suggest the need for task-specific training to effectively integrate knowledge.  The KITMUS dataset and code are publicly available.</sample>
    <sample id="129">The authors used the example of "Asian woman" as a marked group. They observed that the persona generated for this group was depicted as unassuming, while the persona for a Middle-Eastern woman was described using words like "exotic" and "mesmerizing."</sample>
    <sample id="130">Der Text erwähnt keine Modellarchitekturen, die nicht gut generalisieren. Es wird lediglich festgestellt, dass Transformer-Modelle im Allgemeinen besser generalisieren als andere Architekturen.</sample>
    <sample id="131">The video does not mention the name of the test datasets. It focuses on the validation datasets.</sample>
    <sample id="132">Three.</sample>
    <sample id="133">Multi-modal.</sample>
    <sample id="135">James Finch and Sarah Finch introduce ABC-Eval, a new dimensional approach to evaluating conversational AI developed by the Emory NLP Lab.  They contrast ABC-Eval with traditional human evaluation methods like Likert scales, which offer a holistic view but lack granularity. ABC-Eval focuses on annotating specific behaviors in chat models, such as irrelevant responses, contradictions, and hallucinations.

The researchers evaluated four state-of-the-art models using ABC-Eval on 100 human-bot conversations, comparing it to existing methods (Likert ratings at turn and dialogue levels, and pairwise comparisons).  Their analysis revealed that ABC-Eval labels are more reliable and predictive of overall conversation quality than existing methods, as measured by inter-annotator agreement and linear regression.  Specifically, metrics like self-contradiction explain a significant portion of conversation quality, while Likert scores are less informative.

ABC-Eval metrics also demonstrate distinctiveness, with the combination of multiple metrics explaining more of the quality than any single metric.  The study quantified common challenges in conversational AI, such as common sense violations and irrelevant responses.  The authors emphasize the importance of reliable and precise evaluation metrics for advancing the field and hope ABC-Eval will be a valuable tool for others.</sample>
    <sample id="136">Jasivan and Nafise from the University of Sheffield presented "FERMAT: An Alternative to Accuracy for Numerical Reasoning," addressing the limitations of current numerical reasoning benchmarks.  The motivation stems from the need for more informative evaluations beyond simple accuracy scores, especially for large language models (LLMs).  Current benchmarks often fail to reveal the strengths and weaknesses of these models in mathematical ability.

FERMAT is a novel evaluation set built on arithmetic types, drawing from Illinois and CommonCore math questions.  It introduces variations in number representation (e.g., 5.0 instead of 5), number types (integers, decimals), and mathematical operations (single vs. combined).  A zero-shot evaluation reveals poor performance across all aspects.  Fine-tuning with human-written templates (200,000 examples) significantly improves performance, demonstrating the importance of diverse number and operation representations.

The research further investigates training dependency, finding that models often fail to memorize specific question-answer pairs, suggesting linguistic nuances (e.g., "increases" vs. "another") are crucial.  Finally, the impact of training templates from GSM8K and AQUA is explored, showing that incorporating diverse language and mathematical concepts leads to substantial performance gains.  The study concludes that existing benchmarks are insufficient and that FERMAT offers a more comprehensive and informative alternative, highlighting the importance of language and mathematical diversity, as well as number encoding and tokenization, for improving numerical reasoning capabilities.</sample>
    <sample id="137">Sicong from the Singapore University of Technology and Design presents "Tell2Design: A Dataset for Language-Guided Floor Plan Generation," published in ACL 2023. The paper addresses the need for generating designs from natural language instructions, a crucial step in bridging the gap between user needs and expert design. Unlike text-to-image models focused on artistic generation, Tell2Design tackles the more constrained task of floor plan generation, where instructions specify semantics, geometry, and topology.

The research introduces a novel sequence-to-sequence model, leveraging a transformer-based encoder-decoder architecture and pre-trained language models like T5, to generate floor plan layouts from language instructions.  A large-scale dataset of 5,051 human-annotated instructions and 76,000 artificially generated instructions is created to train and evaluate the model.  The model achieves high IoU scores (Micro IoU 54, Macro IoU 53) compared to text-to-image baselines, demonstrating its ability to follow complex instructions.

The study highlights the challenges of this task, including strict constraints, understanding unstructured text, and handling ambiguity in human instructions.  The paper also explores the language distribution gap between artificial and human instructions, showing that a combination of both during training improves performance.  The research establishes Tell2Design as a valuable resource and proposes a sequence-to-sequence model as a strong baseline for future advancements in language-guided design generation.</sample>
    <sample id="138">The authors believe that integrating knowledge from different sources (pretrain-time and inference-time) is a relatively underexplored area in Natural Language Understanding (NLU). They highlight that while models can leverage pre-trained knowledge, they often struggle to effectively utilize knowledge available only at inference time, such as entity-specific information or background knowledge that has evolved since pre-training.</sample>
    <sample id="139">Ying and Zhiyang.</sample>
    <sample id="140">Yes, CoScript underwent quality control through crowd-sourced workers who found and revised incorrect samples.</sample>
    <sample id="141">The existing resources for context-dependent translation are limited in scope and often rely on domain knowledge and human curation. They typically support only a limited range of context-dependent translations and a restricted set of languages.</sample>
    <sample id="142">Javad Hosseini und Filip Radlinski, Silvia Pareti und Annie Louis haben gemeinsam an der Arbeit "Resolving Indirect Referring Expressions for Entity Selection" gearbeitet und den AltEntities Corpus vorgestellt. Ihr Ziel ist es, das Sprachverständnis von Nutzern zu verstehen, wenn sie eine Wahl treffen wollen. Betrachten wir die alternative Frage: "Meinst du 'Easy on Me' oder 'I Gotta Feeling'?" Hier möchte ein Nutzer zwischen diesen beiden Songs wählen. Die offensichtlichste Sache wäre eine direkte Referenz, zum Beispiel durch Nennung des Songnamens "Easy on Me" oder seiner Position, "der erste". Aber manchmal ist eine indirekte Referenz passender für eine natürlichere Unterhaltung. Dies kann passieren, wenn der Nutzer den Namen des Songs nicht mehr erinnert. Oder wenn die Aussprachen zu ähnlich sind und eine Unterscheidung schwierig ist. Oder wenn der Nutzer eine Präferenz angeben möchte. Hier sind einige Beispiele für indirekte Referenzen, zum Beispiel "der neuere" oder "der Song, der nicht energiegeladen ist". Dies ist ein wichtiges Problem in konversationellen Systemen und auch für das Benchmarking von LLMs im Bereich der Entitätserkennung. Wir sind uns nicht bewusst über einen größeren öffentlichen Datensatz für diese Aufgabe, daher haben wir einen eigenen Datensatz mithilfe von Crowd-Annotation erstellt. Unser Datensatz umfasst drei verschiedene Domänen: Musik, Bücher und Rezepte. Unsere Datensatzbeschaffungsmethodik betont Informalität durch eine Cartoon-Vollständigkeits-Aufgabe. Der Cartoon hat drei Sprechblasen. In der ersten Sprechblase sagt Bob: "Erinnerst du dich an den Song, den wir gestern gehört haben?" Und damit setzt Bob den Dialogkontext. In der zweiten Sprechblase sagt Alice: "Meinst du 'Easy on Me' oder 'I Gotta Feeling'?" Dies ist die alternative Frage. Und in der dritten Sprechblase sagt Bob eine indirekte Referenz, um eine der Entitäten auszuwählen, zum Beispiel: "der neuere". Wir stellen die erste und zweite Sprechblase automatisch bereit, aber die dritte wird vom Annotator ausgefüllt. Die erste Sprechblase wird aus einigen manuellen Prompts pro Domäne ausgewählt. Die zweite, die alternative Frage, wird wie folgt generiert. Wir verwenden immer ein einfaches Template: "Meinst du A oder B?", wobei A und B Beispiele aus Wikipedia sind. Wir haben verschiedene Sampling-Methoden verwendet. Je weiter wir in der Liste hochgehen, desto ähnlicher werden die Entitäten und desto schwieriger wird die Unterscheidung. Die erste Methode ist zufällig. Die zweite Methode ist, wenn die Entitäten ähnliche Titel haben, zum Beispiel zwei Bücher mit dem Namen "Der Rückkehr". Die dritte Methode ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben. Und schließlich, wenn sie ähnliche Informationen oder Attribute auf Wikipedia haben, zum Beispiel die gleiche Genre oder der gleiche Künstler für einen Song. Wenn wir diese alternative Frage den Annotatoren zeigen, wissen sie den Namen der Entitäten, aber sie müssen nicht unbedingt über die Entitäten selbst Bescheid wissen. Daher zeigen wir ihnen einige Hintergrundinformationen zu den beiden Entitäten. Für Songs zeigen wir einfach einen Google-Suchlink zu jedem Song und bitten die Annotatoren, mindestens einige der Songs anzuhören und sich über jeden Song zu informieren. Hier ist zum Beispiel der Google-Suchergebnis für den Song "Easy on Me". Für Rezepte und Bücher zeigen wir einige Hintergrundtext aus Wikipedia. Für Rezepte zeigen wir zusätzlich ihre Bilder, ebenfalls von Wikipedia, damit die Annotatoren wissen, wie sie aussehen. Dann baten wir die Annotatoren, eine der Entitäten auszuwählen und sie mit drei bis fünf indirekten Referenzierungsäußerungen zu beschreiben. Zum Beispiel die mit der Klaviermusik. Hier sind einige Beispiele aus unserem Datensatz. Zum Beispiel: "der ohne Worte", "nicht der mit dem 12-jährigen Jungen", oder "der fiktive", oder "kommt aus Aserbaidschan", usw. Der AltEntities Corpus enthält 6.000 alternative Fragen über drei Domänen und 42.000 indirekte Referenzierungsäußerungen. Die Ergebnisse mit dem T5 XL-Modell sind wie folgt zusammengefasst. Wenn das Sprachmodell Zugriff auf die exakt gleichen Hintergrundinformationen wie die Annotatoren hat, dann ist die Genauigkeit sehr hoch, etwa 92 bis 95 %. Dies ist jedoch nicht realistisch. Wenn das Sprachmodell Zugriff auf teilweise überlappende Hintergrundinformationen hat, dann liegt die Genauigkeit zwischen 82 und 87 %, was realistischer ist. Zum Beispiel, wenn das Sprachmodell die Hintergrundinformationen abruft. Wenn das Sprachmodell nur Zugriff auf Entitätsnamen hat, dann liegt die Genauigkeit nur bei 60 %, es gibt also viel Verbesserungspotenzial. Wir haben auch gezeigt, dass die Modelle domänenübergreifend generalisierbar sind. Hier ist ein Link zu unserem Datensatz. Danke.</sample>
    <sample id="143">The approach is compared with the Wait-k strategy and Local Agreement, as well as state-of-the-art architectures specifically tailored for simultaneous pre-translation.</sample>
    <sample id="144">The authors are affiliated with Nantes University Hospital.</sample>
    <sample id="145">Jenny.</sample>
    <sample id="146">Yicheng from Fudan University presents a research paper on dialogue summarization, focusing on the significant problem of omission – where critical information is missing from generated summaries. Despite advancements using large language models, these summaries often suffer from factual errors, including omissions, which severely impact quality.

The paper analyzes the prevalence of omission, finding that even state-of-the-art models have a high omission rate (around 70%).  It investigates the distribution of omitted information, revealing a random pattern across dialogues and positions, highlighting the challenge of identifying key information.

To address this, the researchers created the OLDS dataset, a high-quality dataset of dialogue summaries with omission labels, built upon five existing benchmarks. They developed an automatic method for generating these labels and ensured quality through human evaluation.  They then explored three model frameworks – pairwise classification, sequence labeling, and pointer networks – to build omission detection models.

The results show that omission detection is a challenging task, with a low F1-score, indicating a need for more advanced models.  Furthermore, the paper explores using detected omissions to refine summaries, demonstrating that incorporating omission information into a sequence-to-sequence model significantly improves summary quality.  The research concludes that omission detection is a valuable area for improving dialogue summarization.</sample>
    <sample id="147">Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models is a collaborative work by Esin Durmus, Dan Jurafsky, and Myra.</sample>
    <sample id="148">Hi, ich bin Sara Papi vom Universitätszentrum für Grundlagenforschung und Innovation der Universität Triest und der Fondazione Bruno Kessler und ich werde kurz das Paper "Attention as a Guide for Simultaneous Speech Translation" vorstellen, das in Zusammenarbeit mit Matteo Negri und Marco Turchi verfasst wurde. Was ist Simultaneous Speech Translation? Simultaneous Speech Translation, oder SimulST, ist der Prozess der Übersetzung gesprochener Sprache in Text in einer anderen Sprache in Echtzeit, wodurch die Kommunikation zwischen verschiedenen Sprachen ermöglicht wird. Und was sind die Probleme der aktuellen SimulST-Modelle? Spezifische Architekturen werden in der Regel trainiert, die zusätzliche Module erfordern, die optimiert werden müssen. Lange und komplizierte Trainingsverfahren, beispielsweise Trainings, die verschiedene Optimierungsziele beinhalten. Und das Training und die Wartung mehrerer Modelle, um verschiedene Latenzregime zu erreichen. Zum Beispiel das Training eines Modells mit einer durchschnittlichen Latenz von einer Sekunde und eines anderen Modells mit zwei Sekunden Latenz, usw. Was ist unsere Lösung? Erstens, vorhandene Offline-ST-Modelle verwenden, ohne neu trainieren oder spezifische Architekturen für SimulST anpassen zu müssen. Verwenden Sie nur ein Modell für jedes Latenzregime und behandeln Sie die Latenz mithilfe spezifischer Parameter. Und nutzen Sie das bereits erlangte Wissen des Modells durch den Attention-Mechanismus zwischen Audio-Eingabe und Text-Ausgabe, den Cross-Attention-Mechanismus, und Sie können ein Beispiel rechts sehen. Unsere Lösung ist die Einführung von EDAtt, oder Encoder-Decoder-Attention, einer Strategie, bei der wir entscheiden, ob wir eine teilweise Übersetzung ausgeben oder nicht, basierend darauf, wo die Aufmerksamkeit liegt. Ein Wort wird ausgegeben, wenn die Aufmerksamkeit nicht konzentriert ist, d. h. seine Summe unter einem bestimmten Schwellenwert alpha gegenüber den letzten lambda Sprachframes liegt, was bedeutet, dass die empfangene Information ausreichend stabil ist. Zum Beispiel, wenn wir einen Sprachabschnitt mit "I'm going to talk about..." empfangen und unser Modell die Übersetzung ins Deutsche vorhersagt, und wir die Cross-Attention-Gewichte betrachten, werden wir sehen, dass die ersten beiden Wörter auf die frühesten empfangenen Sprachframes und das letzte Wort auf die letzten lambda Sprachframes zeigen. Das bedeutet, dass die ersten beiden Wörter ausgegeben werden, während da die Summe der Cross-Attention über einem bestimmten Schwellenwert alpha liegt, wir das letzte Wort nicht ausgeben und auf einen weiteren Sprachabschnitt warten. Wenn wir fortfahren und einen weiteren Sprachabschnitt empfangen, und unser Modell drei Wörter vorhersagt, betrachten wir die Cross-Attention-Gewichte und sehen, dass kein Wort auf die letzten lambda Sprachframes zeigt. Das bedeutet, dass diese drei Wörter ausgegeben werden. Wenn wir die Hauptergebnisse von EDAtt betrachten, werden wir die simultane Sprachübersetzungsergebnisse auf Diagrammen plotten, bei denen BLEU die Qualität der Übersetzung misst und die durchschnittliche Verzögerung die Latenzmaßnahme ist, und wir berücksichtigen auch die rechnerisch bewertete durchschnittliche Verzögerung, die die Rechenzeit des Modells zur Vorhersage der Ausgabe berücksichtigt. Wir wollen, dass unsere Kurven so hoch wie möglich auf dem Diagramm sind. Aber auch, dass sie nach links verschoben sind. Und wir vergleichen sie mit beliebten Strategien, die auch auf Offline-Modelle angewendet werden, der Wait-k-Strategie und der Local Agreement. Wir vergleichen auch mit der state-of-the-art-Architektur, die speziell für die simultane Vorübersetzung entwickelt wurde. Dies sind alle die Ergebnisse der simultanen Sprachübersetzungsstrategie auf Deutsch. Und wir sehen, dass sie alle Strategien, die auf Offline-Modelle angewendet werden, übertreffen, da die Kurven nach links verschoben sind. Und wir sehen auch, dass wenn wir die tatsächliche oder die rechnerisch bewertete Zeit berücksichtigen, d. h. die schnellste Strategie, wir die besten Ergebnisse erzielen. Wenn Sie mehr Ergebnisse entdecken möchten, lesen Sie unser Paper. Und wir haben den Code und die Modelle und die simultane Ausgabe für die Reproduzierbarkeit unserer Arbeit geöffnet. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="149">Ja, der Datensatz ist öffentlich zugänglich.</sample>
    <sample id="150">Archiki from Adobe Research and UNC Chapel Hill presented their ACL paper "MEETINGQA," focusing on extractive question-answering in meeting transcripts. They highlight the unique challenge of meeting data – long, domain-specific, and information-rich – which is currently underutilized for QA.  MEETINGQA is a new dataset containing 7.7K questions and answers extracted from public meeting transcripts (AMI corpus), with high inter-annotator agreement.

The dataset features open-ended questions, multi-speaker answers, rhetorical questions, and unanswerable questions.  The paper explores various methods, including context retrieval, single-span and multi-span models, and data augmentation using MediaSum.  

Results show a significant gap between fine-tuned models and human performance (over 25 F1 points). Short-context models outperformed long-context models, while multi-span models showed comparable performance to single-span models. Data augmentation significantly improved zero-shot performance.  Error analysis reveals challenges with rhetorical question identification and speaker attribution, particularly in zero-shot settings.

MEETINGQA presents a challenging task for QA models, requiring them to understand the nuances of real-world meeting discussions. The paper concludes that the dataset is far from being fully solved, highlighting the potential for future research in this domain.</sample>
    <sample id="151">Hallo zusammen, mein Name ist Ying und mein Kollege Zhiyang und wir präsentieren unsere Forschung zu MultiInstruct, die die Mehrfachmodale Null-Shot-Lernung durch Instruction Tuning verbessert. Angesichts der Fortschritte in großen Sprachmodellen haben viele Arbeiten begonnen, neue Lernparadigmen für die Wiederverwendung vortrainierter Sprachmodelle für verschiedene nachgelagerte Aufgaben in einem parameter- und dateneffizienten Verfahren zu erforschen. Kürzlich haben viele Studien gezeigt, dass Instruction Tuning große Sprachmodelle in einer Null-Shot-Art und Weise auf ungesehenen Aufgaben ausführen können, indem sie natürliche Anweisungen befolgen. Allerdings konzentrierten sich die meisten vorherigen Arbeiten auf die Verbesserung der Null-Shot-Leistung auf Sprach-nur-Aufgaben, während Computer-Vision- und mehrmodale Aufgaben vernachlässigt wurden. Daher wollen wir in dieser Arbeit untersuchen, ob das Instruction Tuning eines mehrmodalen vortrainierten Modells die Generalisierung auf ungesehene mehrmodale Aufgaben tatsächlich verbessern kann. Darüber hinaus haben wir während unserer Forschung eine erhebliche Diskrepanz in der Verfügbarkeit von Instructional-Datensätzen zwischen NLP und mehrmodalen Aufgaben festgestellt. Es gibt mehr als 1600 Sprach-nur-Instruction-Aufgaben. Es gibt jedoch keinen großen, öffentlich zugänglichen mehrmodalen Instruction-Aufgaben-Datensatz. Dies motiviert uns, einen mehrmodalen Instruction-Tuning-Datensatz zu erstellen. Wir präsentieren MultiInstruct, den ersten mehrmodalen Instruction-Tuning-Benchmark-Datensatz, der 62 diverse mehrmodale Aufgaben umfasst, die aus 21 existierenden Open-Source-Datensätzen abgeleitet wurden und jede Aufgabe mit fünf von Experten geschriebenen Anweisungen ausgestattet ist. Um mehrmodales Instruction Tuning auf unserem vorgeschlagenen Datensatz zu untersuchen, verwenden wir OFA, ein vereinheitlichtes mehrmodales vortrainiertes Modell. OFA verwendet einen vereinheitlichten Vokabular für Sprach-, Bild-Token und die Koordinaten eines Begrenzungsrahmens. Hier zeigen wir einige Beispielinstanzen aus unserem MultiInstruct-Datensatz, um verschiedene Eingabe- und Ausgabe-Datentypen zu vereinheitlichen. Wir folgen der Methode von OFA und formulieren alle Aufgaben in einem vereinheitlichten Sequence-to-Sequence-Format. In dem die Eingabetexte, Bilder, Anweisungen und Begrenzungsrahmen im gleichen Token-Raum dargestellt werden. Okay, jetzt werde ich über mehrmodales Instruction Tuning sprechen. Also für den Trainingsdatensatz verwenden wir 53 Aufgaben aus 9 Gruppen für das Training und wir probieren 10.000 Instanzen pro Aufgabe aus. Für das Testen reservieren wir den gesamten Common Sense Reasoning-Gruppe für das Testen und wählen zusätzlich 5 Aufgaben aus den VQ- und Miscellaneous-Gruppen aus. Wir verwenden alle Instanzen im Testsplit für jede Aufgabe. Darüber hinaus probieren wir zufällig 20 Aufgaben aus dem Testsplit von natürlichen Anweisungen als ungesehene Aufgabe für NLP aus. Wir verwenden das vortrainierte OFA-große Modell als Basismodell. Während des Trainings mischen wir alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einer ihrer fünf Anweisungsvorlagen kombiniert. Also während des Tests für jede Aufgabe führen wir insgesamt 5 Experimente durch, indem wir das Modell mit einer der fünf Anweisungen bewerten. In jedem Experiment berichten wir über den Minimum- und Maximumwert der Leistung sowie die Standardabweichung der Leistung über alle 5 Experimente. Wenn die Aufgabe eine mehrmodale Klassifizierungsaufgabe ist, berichten wir über die Genauigkeit. Wenn es sich um eine mehrmodale Generierungsaufgabe handelt, berichten wir über Rouge-L. Für NLP-Aufgaben berichten wir ebenfalls über Rouge-L. Wir führen auch einen zusätzlichen Evaluationsmetrik namens Sensitivity ein. Diese misst die Fähigkeit des Modells, für die gleiche Aufgabe bei geringfügigen Variationen der Formulierung der Anweisung konsistent die gleichen Ausgaben zu erzeugen. Hier ist unser Hauptergebnis. Wie wir sehen können, verbessert das Instruction Tuning die Leistung von OFA auf gesehenen mehrmodalen Aufgaben erheblich. Außerdem kann das Transferlernen von natürlichen Anweisungsdatensätzen die Instruction-Tuning-Verfahren verbessern. Hier sehen wir, dass je größer die Anzahl der Aufgaben, desto besser die Leistung des Modells und desto geringer die Sensitivität. Wir haben auch ein Experiment durchgeführt. Wir verwenden eine Anweisung gegen 5 Anweisungen. Wie wir sehen können, verbessert die Verwendung mehrerer Anweisungen die Gesamtleistung des Modells erheblich und reduziert seine Sensitivität stark. Dies zeigt den Effekt verschiedener Fine-Tuning-Strategien auf die Modell-Sensitivität. Wie wir sehen können, kann das Transferlernen von natürlichen Anweisungsdatensätzen die Sensitivität des OFA-Modells deutlich verbessern im Vergleich zum ursprünglichen OFA-Modell. Wir sehen auch, dass das Transferlernen von natürlichen Anweisungsdatensätzen OFA helfen kann, eine viel bessere Leistung auf dem natürlichen Instruction-Datensatz zu erzielen. Insgesamt schlagen wir einen ersten großen mehrmodalen Instruction-Tuning-Datensatz vor, der die Fähigkeiten von OFA erheblich verbessert, und untersuchen verschiedene Transferlernen-Techniken und zeigen deren Vorteile. Wir entwerfen eine neue Metrik namens Sensitivity. Noch eine Sache, wir sammeln einen viel größeren mehrmodalen Instruction-Tuning-Datensatz mit etwa 150 zusätzlichen Vision-Language-Aufgaben und werden ihn veröffentlichen. Dies ist ein QR-Code für unsere Daten und unser Modell. Vielen Dank.</sample>
    <sample id="152">## Summary of "Exploring Large Language Models for Classical Philology"

Frederick Riemenschneider's presentation details work on leveraging Large Language Models (LLMs) for Classical Philology, specifically Ancient Greek and Latin.  The research addresses the limitations of existing monolingual BERT models, which are not optimized for these languages and lack multilingual capabilities.

The project developed several new models: GreBERTa (monolingual RoBERTa for Greek), GreTa (monolingual T5 encoder-decoder for Greek), PhilBERTa (multilingual model for Greek, Latin, and English), and PhilTa (multilingual model for Greek, Latin, and English).  A key innovation was creating a high-quality pre-training corpus for Greek by identifying and correcting OCR errors in the Internet Archive.

The models were benchmarked on tasks like part-of-speech tagging, dependency parsing, and lemmatization, achieving state-of-the-art performance for both languages.  Analysis revealed that T5 encoders behave differently than native encoder-only models, and encoder-decoder models excel at lemmatization.  The multilingual models showed comparable performance to monolingual models in semantic and world knowledge tasks.

The research emphasizes the importance of native tokenization and rigorous data curation for effective LLMs in Classical Philology.  The findings suggest that multilingual models offer a promising avenue for processing and understanding ancient texts across multiple languages.  The presentation concludes with a call to explore the full potential of these models, with a paper available for further details.</sample>
    <sample id="153">Ninareh Mehrabi from Amazon Alexa AI's Responsible AI team presented their work on resolving ambiguities in text-to-image generative models. The research addresses the challenge of ambiguous prompts, which can lead to inaccurate image generation. The study focuses on identifying and mitigating these ambiguities and evaluating the faithfulness of generated images to user intent.

The project utilizes a benchmark dataset based on LAVA, encompassing various ambiguity types. A key component is a prompt disambiguation framework. This framework employs a language model to generate clarifying questions or different visual interpretations, allowing users to provide answers that align with their intended meaning. These answers are then used to disambiguate the original prompt.

To evaluate the faithfulness of generated images, the research proposes an automatic evaluation framework using a Visual Question Answering (VQA) model. The VQA model analyzes images and human intentions to determine if the image accurately reflects the user's intent. The findings demonstrate that ambiguity resolution varies across different types of ambiguities, that the proposed framework improves faithful image generation, and that the automatic evaluation framework aligns with human evaluation. The paper also includes additional findings and discussions. The work aims to improve the reliability and accuracy of text-to-image models by addressing prompt ambiguities.</sample>
    <sample id="154">The authors are affiliated with the University of Trento and Foundazione Bruno Kessler.</sample>
    <sample id="155">Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="157">Shen Gao from Shandong University introduces their work, "Dialogue Summarization with Static-Dynamic Structure Fusion Graph," a joint project with several researchers. The research addresses the challenge of creating concise summaries from dialogues, aiming to capture key information efficiently. Existing methods rely on pre-computed static graphs, which are vulnerable to errors from external linguistic tools and don't adapt to the summarization task.

Their SDDS model employs a four-component architecture. First, an Utterance Encoder converts dialogue utterances into vector representations. Then, a Static-Dynamic Graph module constructs static graphs based on discourse parsing, key co-occurrence, and speaker interaction frequency, capturing the dialogue structure.  A Dynamic Graph module uses multi-head attention to model semantic relationships between utterances based on their vector representations. Finally, a pre-trained language model generates the summary by fusing the static and dynamic graph information.

The model utilizes heuristic methods for static graph construction, including Discourse Parsing Graph, Key Co-occurrence, and Speaker Interaction Frequency Matrix, to represent dialogue structure and speaker relationships.  A Dynamic Graph module leverages multi-head attention to capture semantic connections.  The static and dynamic graphs are fused using a unified graph, and a graph attention layer is incorporated into the self-attention mechanism to integrate graph representation into the summary generation process. The code and data are available on GitHub.</sample>
    <sample id="158">Qipeng Guo from AWS introduces their work on "Dual Cache for Long Document Neural Coreference Resolution." Coreference resolution identifies mentions of the same entity within a document, a task hindered by the quadratic complexity of traditional methods. Cache-based methods offer linear complexity but suffer from high cache misses in long documents due to topic shifts and the LRU eviction policy.

The proposed dual cache utilizes a local cache for local entities (LRU eviction) and a global cache for global entities (LFU eviction). This approach addresses the issue of high misses by prioritizing frequently mentioned entities globally. The model scans the document, classifying mentions as new or existing, and adds them to the appropriate cache based on frequency.

Experiments on LitBank, OntoNotes, and WikiCoref demonstrate that the dual cache outperforms baselines, even with unbounded memory, and achieves faster performance.  A book-level evaluation further highlights the significant performance gap between the dual cache and single cache methods.  The dual cache also significantly reduces cache misses compared to single cache approaches.

The research concludes that the dual cache offers the best performance-to-cost ratio among cache-based methods, effectively addressing the challenges of long documents and improving coreference resolution accuracy while maintaining efficiency.  The dual cache's separation of local and global entities allows for optimized memory management and improved performance.</sample>
    <sample id="159">Hallo zusammen. Ich bin Koustav Sinha, und ich freue mich, Sie zu unserem Vortrag über unseren ACL 2023-Paper begrüßen zu dürfen. Sprachmodell-Akzeptanzurteile sind nicht immer robust gegenüber dem Kontext. Dies ist eine gemeinsame Arbeit mit John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy und Adina Williams. In dieser Arbeit revisitieren wir die Minimalpair-Paradigmen. Das Minimalpair-Paradigma bewertet Sprachmodelle anhand von Akzeptanzurteilen, die auch Grammatikalität umfassen können, wie BLiMP, SyntaxGym oder Akzeptanz in Bezug auf Stereotypen wie CrowS-Paired. Im Minimalpair-Paradigma wird typischerweise gezeigt, ob eine akzeptable oder eine ungrammatikalische Satz verwendet wird. Die Hoffnung ist, dass das Modell mehr Wahrscheinlichkeit der akzeptablen Satz zuweist. Die aktuelle MPP-Pipeline ermöglicht es uns nicht, ein Modell hinsichtlich der Akzeptanz längerer Sätze zu bewerten. Heutzutage kommen große Sprachmodelle mit immer längeren Kontextfenstern. Daher ist es entscheidend, die Akzeptanzmodelle über das gesamte Kontextfenster zu bewerten, und das ist es, was wir hier tun wollen. Wir versuchen, das MPP-Pipeline zu revisitieren, indem wir das Modell auffordern, die Akzeptanz auf längeren und längeren Sequenzen zu bewerten. Das ist der Ansatz. Wir erstellen also längere Sequenzen, indem wir akzeptable oder unakzeptable Sätze aus diesen Datensätzen auswählen und neu erstellen. Zum Beispiel haben wir eine typische Grammatikalitätspaarung aus dem BLiMP-Datensatz im Bereich der Adjunct Island-Klausel gewählt. Wir extrahieren grammatikalisch korrekte Sätze aus Adjunct Island und fügen sie als Präfix sowohl der akzeptablen als auch der unakzeptablen Anfrage hinzu, damit wir die gleiche grammatikalische Struktur haben. Dies können wir auch mit unakzeptablen Sätzen aus dem gleichen Matching tun, was auch zur Prüfung der Akzeptanz des Modells verwendet werden könnte. Wir können dies auch mit Sätzen aus einem anderen Subset oder einem anderen Datensatz tun. Das nennen wir das Mismatch-Szenario. Hier kommen die Sätze immer aus relevanten Datensätzen, aber nicht aus dem gleichen Datensatz, mit dem Sie bewerten. Wir können dies auch für das Unakzeptabilität-Szenario tun, indem wir Sätze aus einem völlig unzusammenhängenden Bereich wie Wikipedia auswählen. Das wird uns zeigen, ob die Akzeptanzurteile des Modells tatsächlich vom Kontext beeinflusst werden, d. h. ob der Kontext aus einem anderen Datensatz stammt oder ob er völlig irrelevant für den aktuellen Satz ist, den wir betrachten. Wie funktioniert das Modell? Zuerst betrachten wir die Wikipedia-Sätze, die völlig irrelevant für die aktuelle Anfragepaarung sind, und dort finden wir heraus, dass die MPP-Urteile im Allgemeinen robust für beliebige Kontextlängen sind. Wir erhöhen die Kontextlänge bis zu 1024 für OPT- und GPT-2-Modelle. Und hier sehen wir in der orangefarbenen gestrichelten Linie, dass die MPP-Urteile relativ stabil sind. Was passiert, wenn wir Sätze aus dem gleichen Datensatz auswählen? Hier wählen wir oder erstellen wir Sätze aus akzeptablen und unakzeptablen Bereichen aus dem gleichen BLiMP- oder SyntaxGym-Datensatz. Und dort sehen wir, dass die MPP-Urteile entweder signifikant ansteigen oder abfallen, wenn wir akzeptable oder unakzeptable Präfixe hinzufügen. Aber wenn wir die Struktur anpassen, d. h. Sätze aus dem gleichen Phänomen in BLiMP oder SyntaxGym auswählen, sehen wir eine massive Erhöhung oder eine massive Abnahme der MPP-Urteile des Modells, je nachdem, ob der ausgewählte Präfix akzeptabel oder unakzeptabel ist. Und das ist sehr groß, dieser Effekt steigt über die Kontextlänge an, und das wird wahrscheinlich neue Sprachmodelle mit großen Kontextfenstern beeinflussen. Warum beeinflusst das Match-Präfix die Urteile des Sprachmodells so stark? Wir führten eine Reihe von Analysen durch, um zu versuchen, die Eingabesatz durch Hinzufügen von Rauschen zu verändern, wobei die relevante Struktur erhalten bleiben sollte. Nach Durchführung mehrerer dieser Störungen stellen wir fest, dass keiner dieser Rauscheffekte die Art und Weise, wie das Modell seine MPP-Urteilspresse anzeigt, tatsächlich verändert. Wir stellen fest, dass die Modelle auf perturbed Sätze auf ähnliche Weise reagieren. Wenn wir Sätze aus dem akzeptablen Bereich stören, sehen wir ähnliche Erhöhungen bei allen Störungen, und wenn wir Sätze aus dem unakzeptablen Bereich stören, sehen wir ähnliche Abnahmen der MPP-Urteile, je nachdem, ob der ausgewählte Präfix akzeptabel oder unakzeptabel ist. Die wichtigsten Erkenntnisse unserer Arbeit sind, dass Sprachmodelle empfindlich auf latente syntaktische und semantische Merkmale sind, die über die gesamte Kontextlänge gemeinsam sind. Die aktuelle MPP-Bewertungsmethode, die wir mit kurzen und einzelnen Sätzen durchführen, kann das gesamte Wissen des Sprachmodells nicht vollständig erfassen, das über das Kontextfenster verteilt ist. Bitte lesen Sie unseren Paper für weitere Details unserer Experimente. Vielen Dank fürs Zuhören.</sample>
    <sample id="160">Unordered multisets of tokens that will appear in the output.</sample>
    <sample id="161">55,000</sample>
    <sample id="163">According to the presentation, the best automatic alignment method for German text simplification is MASSalign.</sample>
    <sample id="164">Schwach überwachtes Lernen ist kostengünstiger als die manuelle Annotation von Daten, da es schwache Label-Quellen wie Heuristiken, Wissensbasen oder Low-Quality-Crowdsourcing verwendet.</sample>
    <sample id="165">Wenting Zhao from Cornell University presented their paper "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations," focusing on unsupervised abductive reasoning. Abductive reasoning aims to find the most plausible explanation for an observed outcome given a context. The paper addresses the challenge of learning abductive reasoning without labeled plausibility, a problem exacerbated by the subjectivity of human annotation.

The proposed method, LiPoR (Likelihood Learning with Posterior Regularization), treats explanations as latent variables and maximizes the marginal likelihood of the outcome given the context.  A key innovation is a regularizer based on mutual exclusivity among explanations.  The regularizer, denoted as Omega, balances maximizing the likelihood of the outcome with preferring a subset of explanations by minimizing the entropy of the probability distribution over explanations given the context.  This encourages the model to favor explanations that are mutually exclusive and thus more likely to be the correct answer.

The LiPoR method was evaluated on the AlphaNLI dataset and demonstrated superior performance compared to zero-shot models and previous unsupervised approaches, including a strong GPT-3 baseline, achieving over 4 absolute points in accuracy. The paper highlights the potential of unsupervised learning for abductive reasoning, enabling models to learn from data without relying on human-annotated plausibility. The paper is available at tinyurl.com/zhao-lipor.</sample>
    <sample id="166">This paper introduces a novel neural framework, Neural Divide-and-Conquer Reasoning (NDCR), for image retrieval from linguistically complex text.  The problem of retrieving images based on lengthy, nuanced descriptions is challenging due to image similarity and the complexity of the text.  Existing visual-language models struggle with this task, often relying on analogical reasoning (System 1) and failing when confronted with complex reasoning.  NDCR addresses this by integrating analogical and logical reasoning (System 1 and System 2) using a Divide-and-Conquer strategy inspired by Dual-Process Theory.

The framework comprises a Proposition Generator to decompose complex text into simple propositions, a Visual-Linguistic Interactor (System 1) to interact with visual and proposition information, and a Neural-Symbolic Reasoner (System 2) to integrate reasoning states and produce the final solution.  The Neural-Symbolic Reasoner utilizes negation and conjunction operations.  Experimental results demonstrate that NDCR outperforms baseline methods, with ablation studies validating the individual module contributions.  The system's ability to present inference states and results mid-process highlights its interoperability.  The authors suggest that neural symbolic calculation can enhance compositional reasoning in large language models, aligning with the effectiveness of Divide-and-Conquer and Dual-Process Theory in complex problem-solving.</sample>
    <sample id="167">Die Dokumente in DEPLAIN-web wurden sowohl manuell als auch mit automatischen Alignmentmethoden ausgerichtet.</sample>
    <sample id="168">Der CoNLL++-Datensatz wurde aus Reuters News von 2020 gesammelt und mit den gleichen CoNLL-2003-Annotationsgrundsätzen annotiert.</sample>
    <sample id="169">David Vilar from Google Translate presents a review of the paper "Prompting PaLM for Translation: Assessing Strategies and Performance." The paper investigates the effectiveness of prompting large language models (LLMs) like PaLM, a 540 billion parameter model trained on 780 billion tokens, for machine translation. PaLM achieved state-of-the-art results in various NLP tasks.

The study provides the first systematic evaluation of LLM prompting in MT, using best practices and comparing PaLM to state-of-the-art systems like WMT.  They employed neural MT metrics and human evaluation using the MQM framework.  The research highlights that prompting significantly impacts translation performance, with even small changes in prompt design leading to substantial differences (BLEURT scores of 1+ to 40+ points).

The study found that the quality of the example translations in the prompt is more crucial than the similarity to the source sentence, especially for zero-shot and one-shot prompting.  Five-shot prompting showed minimal difference in performance compared to example quality.  The paper also explores the impact of using training data versus curated development data for prompt selection, with development data yielding better results.

While PaLM's translation quality is close to commercial systems like Google Translate, it struggles with accuracy, particularly omission errors.  PaLM produces fluent translations but sometimes sacrifices accuracy for stylistic reasons.  The study concludes that prompt selection strategies need to prioritize high-quality examples to achieve optimal translation performance.  The full paper provides more detailed insights.</sample>
    <sample id="170">Hallo zusammen, mein Name ist Yusen Zhang von der Penn State University. Heute präsentiere ich unsere Arbeit "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations". Semantisches Parsen ist eine Aufgabe, um semantische Repräsentationen von Benutzerabfragen zu erstellen, wie z. B. SQL und Lambda Calculus. Und Cross-Lingual Semantic Parsing ist die Aufgabe, Abfragen in mehreren natürlichen Sprachen in mehrere Bedeutungssimulationen zu übersetzen. Wie auf der Abbildung gezeigt, müssen wir Abfragen in mehreren natürlichen Sprachen mithilfe neuronaler Modelle in SQL, Lambda oder FunQL usw. übersetzen. Bestehende cross-linguale semantische Parsenmodelle werden separat auf Datensätzen mit begrenzten Aufgaben und Anwendungen vorgeschlagen und bewertet. Zum Beispiel gibt es eine große Abdeckung in bestimmten natürlichen Sprachen. Aber Chinesisch fehlt und es gibt eine mangelnde Abdeckung in bestimmten Bedeutungssimulationen. Der Lambda Calculus fehlt, oder sie werden nur auf bestimmte neuronale Modelle bewertet. Zum Beispiel gibt es nur ein einziges Modell, um sie zu bewerten. Um dies zu erreichen, schlagen wir XSemPLR vor. Wir stellen einen einheitlichen Datensatz XSemPLR für cross-linguale semantische Parsen in mehreren natürlichen Sprachen und Bedeutungssimulationen bereit. Er enthält 9 Datensätze in verschiedenen Domänen, 5 semantische Parsenaufgaben, 8 Bedeutungssimulationen und 22 natürliche Sprachen in 15 Sprachfamilien. Und um unseren Benchmark besser zu bewerten, berücksichtigen wir sechs Einstellungen für Training und Bewertung. Die erste ist Translate-Test. Wir verwenden die Google Translate API, um die Quelle in die Zielsprache zu übersetzen, und dann verwenden wir ein monolinguales Modell zum Trainieren und Bewerten. Zum Beispiel trainieren wir das englische Modell auf englischen Abfragen und während der Inferenz übersetzen wir die deutsche Abfrage mithilfe der API ins Englische und verwenden dann das trainierte Modell, um die SQL vorherzusagen. Wir werden auch das Monolinguale Modell testen. In dieser Einstellung ist die Quell- und Zielsprache gleich, z. B. Deutsch zu Deutsch oder Englisch zu Englisch. Wir testen auch die Monolinguale Few-shot-Einstellung, indem wir monolinguale Modelle mit nur 10 % der Trainingsdaten trainieren. Und wir testen das Multilingual-Modell, indem wir ein Multilingual-Modell für alle Sprachen trainieren. Zum Beispiel fügen wir deutsche, englische und chinesische Abfragen zusammen, um ein Multilingual-Modell zu trainieren. Während der Inferenz können wir dieses Modell verwenden, um deutsche oder chinesische Abfragen zu übersetzen usw. Wir berücksichtigen auch Cross-lingual Zero-shot- und Few-shot-Transfer. Wir trainieren auf einer Quellsprache und übertragen sie auf eine andere Sprache. Während des Trainings trainieren wir es auf englischen Abfragen oder der Kombination von englischen und deutschen Few-shot-Abfragen, um ein Multilingual-Modell zu trainieren, um die SQL-Ausgabe vorherzusagen. Wir stellen auch viele interessante Ergebnisse fest. Bezüglich der Analyse monolingualer Modelle bewerten wir zwei Gruppen von Modellen, darunter Encoder-PTR, das für Multilingual Pretrained Encoders mit Pointer-basierten Decodern steht, wie z. B. XLM-R + PTR und mBERT + PTR. Wir bewerten auch Encoder-Decoder-Modelle, die Multilingual Pretrained Encoder-Decoder-Modelle sind, wie z. B. mBART und mT5. Wir haben festgestellt, dass Encoder-Decoder die beste Leistung auf allen neun Datensätzen erzielen. Wir bewerten mT5 und XLM-R + PTR auf der Multilingual-Einstellung. Wir haben festgestellt, dass Encoder-Decoder oder Encoder-PTR durch Training in einer Mischung aus verschiedenen Sprachen verbessert werden können. Dies liegt daran, dass die meisten wichtigen natürlichen Sprachen Leistungssteigerungen erzielen, außer Englisch, das auf sieben Datensätzen eine Leistungsabnahme und auf drei Datensätzen eine Leistungssteigerung aufweist. Dies wird als der "Fluch der Multilingualität" bezeichnet. Wir vergleichen auch die cross-linguale Leistungsspanne. Die blaue Linie ist Cross-lingual Few-shot-Transfer, die orange Linie ist Cross-lingual Zero-shot-Transfer und die grüne Linie ist die Monolinguale Einstellung. Wir haben festgestellt, dass die Leistungsspanne bei Zero-shot-Transfer cross-lingual signifikant ist, und dass die Transferspanne bei Few-shot-Transfer mit der blauen Linie und der orange Linie schnell verringert wird. Wir stellen auch einige andere interessante Ergebnisse fest. Zum Beispiel übertreffen Encoder-Decoder frühere Arbeiten oder erzielen vergleichbare Ergebnisse. Das Vortrainen auf englischer natürlicher Sprache kann die Leistung von Few-shot auf Zielnatürliche Sprachen erheblich verbessern, und wir haben festgestellt, dass Multilingual-Sprachmodelle wie Codex und BLOOM immer noch unzureichend für cross-linguale semantische Parsenaufgaben sind. Um zusammenzufassen, bauen wir XSemPLR, einen einheitlichen Benchmark für cross-linguale semantisches Parsen mit mehreren natürlichen Sprachen und Bedeutungssimulationen. Wir führen eine umfassende Benchmarkstudie auf drei repräsentativen Arten von Multilingual-Sprachmodellen durch. Und unsere Ergebnisse zeigen viele interessante Ergebnisse. Und usw. Und wir laden Sie ein, unseren Paper und Code zu besuchen. Vielen Dank fürs Zuhören.</sample>
    <sample id="171">Existing works can be broadly classified into four categories, but they either are not applicable to embedding as services or lack transferability.</sample>
    <sample id="172">No, multilingual LLMs like Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="174">Thea from ArgAnalysis35K explains what makes their dataset unique for argument quality analysis. Current datasets often suffer from quality issues due to crowdsourcing, lack of diversity, and insufficient depth in argument explanation. ArgAnalysis35K addresses these problems by being the largest dataset (35K argument-analysis pairs) with high-quality arguments sourced from expert and intermediate debaters, rather than just novice users.

The dataset boasts diverse arguments based on 24 themes, covering a wider range of parliamentary debate topics than typical datasets.  Crucially, ArgAnalysis35K introduces the concept of "analysis," which goes beyond simple claims and premises to encompass a combination of both, providing a more comprehensive understanding of the argument's strength.

Furthermore, the dataset incorporates instance-based annotator reliability, allowing for the utilization of annotator judgments even when they exhibit biases towards certain topics. Finally, a relevance model assigns scores to arguments' relevance to specific themes, capturing the broader applicability of arguments beyond a single motion.

In essence, ArgAnalysis35K offers a more diverse, higher-quality, and reliably scored dataset for argument analysis, providing a richer understanding of argument quality in a parliamentary debate context. Thea encourages viewers to explore their paper and provide feedback.</sample>
    <sample id="175">The method addresses the ambiguity of permutations by inducing alignment as part of the training process. It also approximates the NP-hard permutation search with a GPU-friendly continuous relaxation that allows for learning linguistically more plausible permutations.</sample>
    <sample id="176">The fairness of a downstream NLP model is defined by its performance on different demographics or political leanings of news media, with patterns emerging in hate speech detection and fake news detection. Specifically, left-leaning models show better performance on hate speech targeting minority groups, while right-leaning models perform better on hate speech targeting more powerful groups. Similar trends exist for fake news detection.</sample>
    <sample id="177">Yanis Labrak.</sample>
    <sample id="178">Okay, here's the analysis and answer:

**Analysis:**

The speaker, Koustav Sinha, is presenting research on evaluating language models using minimal pair paradigms. The research focuses on the limitations of current methods in handling longer context windows and proposes a new approach to recreate longer sequences for evaluation. The study finds that language models exhibit sensitivity to shared syntactic and semantic features across sentences, suggesting that current evaluation methods may not fully capture their abstract knowledge. The work uses datasets like BLiMP and SyntaxGym, and explores different scenarios including irrelevant context, matching context, and mismatch scenarios.

**Answer:**

Koustav Sinha.</sample>
    <sample id="179">## Abstract

Large Language Models (LLMs) struggle with Theory of Mind (ToM) reasoning, particularly in understanding false-belief scenarios. This work introduces SymbolicToM, a plug-and-play inference-time method to enhance ToM capabilities in LLMs using explicit graphical representations of mental states. SymbolicToM computes belief graphs for all character combinations up to a defined level, leveraging off-the-shelf Natural Language Inference (NLI) and OpenIE models. These graphs enable efficient question answering by recursively querying the graph and integrating the results with a language model.

Experiments with various LLMs demonstrate significant performance gains (up to 65 accuracy points) compared to out-of-the-box performance on the ToMi dataset.  Furthermore, SymbolicToM exhibits strong generalization across different story structures and linguistic variations, outperforming supervised baselines on out-of-domain datasets (D1, D2, D3) and the ParaphrasedToMi dataset.  This approach avoids overfitting and provides more interpretable reasoning.  SymbolicToM offers a promising solution for improving the robustness and reliability of LLMs in tasks requiring nuanced understanding of mental states.</sample>
    <sample id="180">Myra, Esin Durmus, and Dan Jurafsky.</sample>
    <sample id="181">This paper addresses the challenge of constrained language planning, extending the work on abstract goal planning for stereotypical activities to scenarios with specific constraints.  While large language models (LLMs) demonstrate promise in decomposing goals, their performance on constrained goals remains limited. We define constrained language planning and address the lack of existing datasets for this task.  We leverage InstructGPT to generate a large dataset of 55,000 specific goals and corresponding scripts, termed CoScript.  Our analysis reveals that LLMs struggle to guarantee constraint faithfulness despite acceptable semantic completeness.  We introduce an "over-generate-then-filter" method to improve script quality by generating multiple scripts and filtering for those that satisfy constraints and maintain semantic coherence.  Furthermore, we employ symbolic knowledge distillation to create CoScript, enabling training of smaller, specialized models.  Experiments show that fine-tuning T5 on CoScript yields higher-quality scripts than most LLMs, highlighting the potential of smaller models when trained on appropriate datasets.  CoScript aims to facilitate research in constrained language planning by providing a valuable resource for training more efficient and effective language planning systems.</sample>
    <sample id="182">The term "tropikalismus" refers to a trope of tropicalism, which connects to the words describing Latina women in the generated personas. These words, like "vibrant" and "curvaceous," reinforce a stereotype of Latina women as being associated with a tropical setting, which is a harmful and often exoticizing trope.</sample>
    <sample id="183">The authors generated personas by prompting the language model to describe imagined individuals based on specific demographic identities (e.g., "Imagine you are an Asian woman. Describe yourself."). They also used a study where prompts were given to human subjects to surface racial stereotypes, enabling direct comparison with the generated personas.</sample>
    <sample id="184">CXMI (Contextualized Cross-lingual Meaning Information) wurde verwendet, um die Kontextnutzung von Wörtern während der Übersetzung zu messen. CXMI wird erweitert zu Pointwise CXMI, um die Kontextnutzung auf Satz- oder Wortebene zu messen.</sample>
    <sample id="185">DrBERT ist ein speziell für französische biomedizinische und klinische Texte vortrainiertes Modell, das auf dem NACHOS-Datensatz trainiert wurde. ChuBERT ist ein klinisches Modell, das auf anonymisierten Daten aus dem Nantes University Hospital Data Warehouse trainiert wurde.</sample>
    <sample id="187">Ying und Zhiyang.</sample>
    <sample id="188">```
Iterative transfer learning updates the model by training on the latest set of data collected.
```</sample>
    <sample id="189">The goal of the dataset is to understand how users refer to entities in conversations, particularly when using indirect references, and to benchmark the understanding of entity selection in large language models (LLMs).</sample>
    <sample id="190">An attacker can extract model parameters by learning from the embeddings provided by an Embedding as a Service (EaaS). The paper proposes a backdoor-based watermark method called "Embedding marker" to protect the copyright of embedding as services. This method involves injecting a watermark into the embeddings and then verifying the presence of the watermark in the embeddings provided by other services. If the watermark is detected, it indicates that the model behind the service has been extracted.</sample>
    <sample id="191">Drei.</sample>
    <sample id="192">Yang Luo's presentation introduces CAME (Confidence-guided Adaptive Memory Efficient Optimization), a new optimizer designed to address the memory limitations of existing optimizers like Adam and the performance penalties of memory-efficient alternatives like Adafactor. The core challenge is to achieve both fast convergence and low memory usage.

The presentation highlights the memory reduction potential of Non-negative Matrix Factorization (NMF) and how Adafactor attempts to leverage this, but suffers from slow convergence due to inherent errors in its update process. CAME tackles this by introducing a confidence-guided approach. It calculates the residual between the predicted and actual updates, using this "instability" as a measure of confidence. This instability is then used to adjust the denominator of the momentum term, making the optimizer more adaptive and less prone to errors.

Experiments on BookCorpus and English Wikipedia, using BERT, GPT-2, and T5, demonstrate CAME's superior performance compared to Adam and Adafactor. CAME achieves a 3.4% improvement in validation accuracy with the same training steps as Adafactor and better performance than Adam in pre-training large models, especially with increasing batch sizes.  Furthermore, CAME reduces memory footprint compared to existing memory-efficient optimizers like SM3.  The results show that CAME effectively balances convergence speed and memory efficiency, making it suitable for large-scale language model training, including large batch sizes.</sample>
    <sample id="193">The paper states that around 1,000 examples of discourse unit pairs were collected for the initial annotation. It does not specify the number of annotators used.</sample>
    <sample id="194">Jenny, a first-year PhD student at Carnegie Mellon University, is one of the authors. The work was done in collaboration with researchers from the University of Washington and the Allen Institute for AI.</sample>
    <sample id="195">The paper introduces RoHT (Reasoning over Hierarchical Question Decomposition Tree), a novel framework for explainable question answering (XQA) that addresses limitations of existing methods.  Current XQA approaches, like neuro-symbolic and decompose-based methods, face challenges with knowledge source limitations (incomplete KBs for neuro-symbolic, reliance on free-text for decompose-based) and difficulty in handling complex questions due to granularity and uncertainty.

RoHT tackles these issues by building a Hierarchical Question Decomposition Tree (HQDT) to understand the compositional structure of complex questions. This tree breaks down the question into atomic questions, allowing flexible knowledge source selection at each level.  The framework uses probabilistic reasoning over the HQDT, combining knowledge from KBs and text corpora.  Each node in the tree receives answers with associated probabilities from various sources, and an aggregator combines these to produce the final answer.

The paper demonstrates RoHT's effectiveness on challenging datasets like KQA Pro (with incomplete KBs and Wikipedia) and Musique (with text and Wikidata).  RoHT outperforms existing KB QA methods, TransferNet (an end-to-end model), and state-of-the-art text QA methods, especially when integrating knowledge from both KBs and text.  The results highlight the benefits of hierarchical decomposition and probabilistic reasoning for achieving more accurate and explainable question answering, particularly for complex queries.  The framework's ability to leverage heterogeneous knowledge sources is a key advantage.</sample>
    <sample id="196">"I saw Bart and Lisa"</sample>
    <sample id="197">The current standard practice for evaluating conversational AI involves human judges selecting better conversations or rating them on Likert scales. However, this approach is subjective and doesn't capture the many aspects of dialogue quality. ABC-Eval, developed by the Emory NLP Lab, offers a more precise and reliable strategy by explicitly annotating model behaviors like irrelevant responses, contradictions, and hallucinations.  ABC-Eval is more reliable and predictive of overall conversation quality than existing methods like Likert ratings and pairwise comparisons.  It also reveals specific error rates in models, such as common sense violations and irrelevant information.</sample>
    <sample id="198">Das aktuelle MPP-Pipeline kann nicht die Akzeptanz von längeren Sätzen bewerten, da moderne LLMs längere Kontextfenster verwenden. Daher ist es wichtig, die Akzeptanz der Modelle über das gesamte Kontextfenster zu bewerten.</sample>
    <sample id="199">Yes, the presentation mentions the "Curse of Multilinguality," where training on a mixture of languages can lead to a performance drop in English on some datasets, while improving performance in others.</sample>
    <sample id="200">The annotators know the names of the entities but not necessarily about them.</sample>
    <sample id="201">Neural MT metrics and expert-based human evaluation results.</sample>
    <sample id="202">Das Paper untersucht die Generalisierung von NER-Modellen, aber es gibt keine Hinweise darauf, dass die Regression bei der Generalisierung auf bestimmte NER-Typen auswirkt. Die Studie konzentriert sich auf die allgemeine Generalisierung von Modellen, die auf dem CoNLL-2003-Datensatz trainiert wurden, und findet, dass die Generalisierung durch Modellarchitektur, Modellgröße und die Anzahl der Feinabstimmungsexemplare beeinflusst wird.</sample>
    <sample id="203">## Analyse des englischen Inhalts:

Der Vortrag von Jenny, einer PhD-Studentin an der Carnegie Mellon University, präsentiert die Arbeit "NLPositionality: Characterising design biases of datasets and Models". Die Arbeit untersucht, ob NLP-Datensätze und Modelle eine Positionierung haben – d.h. ob sie systematische Verzerrungen gegenüber bestimmten Bevölkerungsgruppen aufweisen.

**Kernpunkte:**

*   **Definition von Positionierung:** Positionierung bezieht sich auf die Perspektiven, die durch Demografie, Identität und Lebenserfahrungen geprägt sind.
*   **Problemstellung:** NLP-Technologien können aufgrund der Positionierung ihrer Entwickler und der Daten, auf denen sie trainiert werden, systematische Verzerrungen aufweisen. Dies führt zu unterschiedlichen Leistungen für verschiedene Bevölkerungsgruppen.
*   **Methodik:** Die Studie verwendet das Framework NLPositionality, das die erneute Annotation von Datensätzen mit diversen Annotatoren und den Vergleich der Annotationen mit den Modellen und Datensätzen selbst beinhaltet.
*   **Ergebnisse:** Die Studie zeigt, dass NLP-Datensätze und Modelle tendenziell stärker mit englischsprachigen Ländern und Personen mit Hochschulabschluss korrelieren. Es gibt auch eine Disparität in der Alignment gegenüber nicht-binären Personen.
*   **Empfehlungen:** Die Arbeit schlägt vor, Designentscheidungen während des Forschungsprozesses zu dokumentieren, NLP-Forschung aus einer Perspektivischen Sichtweise zu betreiben und spezialisierte Datensätze und Modelle für bestimmte Gemeinschaften zu entwickeln.

## Kurze Antwort auf die Frage:

Positionalität ist für NLP wichtig, weil Datensätze und Modelle systematische Verzerrungen gegenüber bestimmten Bevölkerungsgruppen aufweisen können, was zu ungleichen Leistungen und potenziell diskriminierenden Ergebnissen führt. Die Arbeit zeigt, dass diese Verzerrungen existieren und dass es wichtig ist, sie zu erkennen und zu adressieren, um inclusive NLP zu fördern.</sample>
    <sample id="204">No, the presentation states that multilingual language models like Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks. It doesn't specify whether they were adapted with adapters or full fine-tuning.</sample>
    <sample id="205">## Zusammenfassung: Politische Verzerrungen in Sprachmodellen

Shangbin von der University of Washington präsentiert die Arbeit "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models". Die Studie untersucht, wie politische Verzerrungen in großen Sprachmodellen (LLMs) entstehen und sich in verschiedenen Anwendungen manifestieren.

LLMs werden auf Webdaten trainiert, die politische Nachrichtenquellen enthalten. Dies führt zu einer Mischung aus vielfältigen Perspektiven und potenziellen Fairnessproblemen. Die Forscher evaluieren die politischen Neigungen von LLMs mithilfe von politischen Fragebögen und zeigen, dass Modelle wie GPT-4 tendenziell liberaler sind als andere.

Durch gezieltes Vortraining auf partisanen Korpora (Nachrichten und Social Media, nach politischen Neigungen aufgeteilt) zeigen sie, dass sich die politischen Neigungen der Modelle entsprechend verschieben.  Die Analyse der Entwicklung nach dem Amtsantritt des 45. Präsidenten der USA deutet darauf hin, dass LLMs eine zunehmend polarisierte Sichtweise entwickeln.

Die Studie untersucht die Auswirkungen dieser politischen Neigungen auf Anwendungen wie Hate Speech- und Fake-News-Erkennung. Die Ergebnisse zeigen, dass Modelle mit unterschiedlichen politischen Neigungen unterschiedlich effektiv bei der Erkennung von Hassreden und Falschinformationen sind, insbesondere in Bezug auf bestimmte Bevölkerungsgruppen.

Die Forscher betonen die Herausforderungen bei der Bekämpfung politischer Verzerrungen in LLMs, da eine vollständige Entfernung von politischen Meinungen zu Censorship führen könnte. Sie argumentieren, dass die Bewältigung dieser Probleme ein komplexes Dilemma darstellt, das die Entwicklung fairer und unvoreingenommener NLP-Modelle erfordert.</sample>
    <sample id="206">Transfer Learning from debate and expansion/comparison (CE) tasks.</sample>
    <sample id="207">Die neuesten Testsets wurden verwendet, um einen Übersetzungsansatz zu vermeiden, der mit den Trainingsdaten des Sprachmodells übereinstimmt.</sample>
    <sample id="208">Die Autoren haben drei Empfehlungen vorgeschlagen.</sample>
    <sample id="209">The proposed method using CoScript and fine-tuning T5 achieves higher quality scripts than most large language models, including InstructGPT. Specifically, T5 fine-tuned on CoScript generates scripts of higher quality than the majority of large language models. The paper doesn't provide a precise numerical gain, but it highlights that smaller models can outperform larger ones when trained on suitable datasets.</sample>
    <sample id="210">Shuheng.</sample>
    <sample id="211">Yes, the results and dataset of the study can be used as a benchmark. The presentation explicitly states that the manually aligned sentences in the DEPLAIN corpus are used as a gold standard for evaluating automatic alignment methods, and the fine-tuning of language models provides a base benchmark for automatic text simplification.</sample>
    <sample id="212">The paper experiments with T5 fine-tuned on CoScript.</sample>
    <sample id="213">OFA (Unified Multi-Modal Pre-trained Model)</sample>
    <sample id="215">Adam Przepiórkowski's talk argues against asymmetric dependency structures of coordination, such as those proposed by universal dependencies and Igor Mel'čuk's meaning text theory, and supports symmetric structures. The core argument relies on dependency length minimization, a principle similar to how direct objects prefer proximity to the verb. The speaker illustrates this with examples like "Marge read it yesterday" and "Marge read yesterday this absolutely fascinating book about bees," showing that shorter dependencies are preferred.

The presentation uses statistics from the Penn Treebank, confirming a tendency for left conjuncts to be shorter, particularly when the governor is on the left or absent. This tendency increases with the length difference between conjuncts.  A key finding is that this preference for shorter left conjuncts only occurs when the governor is on the left or absent; it disappears when the governor is on the right.  

The speaker demonstrates this with data measured in characters, syllables, and words, showing a steady growth in the tendency for the left conjunct to be shorter as the absolute difference in word length increases when the governor is on the left or absent.  This observation provides evidence for symmetric coordination structures, contrasting with the asymmetric approaches. The paper details these findings and provides a comprehensive argument for symmetric structures.</sample>
    <sample id="217">"Seen to Unseen" addresses the challenge of controllable dialogue generation with multiple attributes, a gap in existing methods that primarily focus on single-attribute control. The work introduces DCG, a Disentangled Controllable Generation model, which learns attribute concepts from seen values using a disentanglement loss.  A key contribution is the development of MAE, a unified, reference-free evaluation framework for various attribute granularities.

The model leverages compositional prompts – attribute-oriented and task-oriented – to guide the generation process. Attribute-oriented prompts focus on specific attribute values, while task-oriented prompts guide the model towards global features. A disentanglement loss further enhances the model's ability to distinguish between different attribute combinations.

Experiments on DailyDialog-CG demonstrate that DCG outperforms baselines in both attribute controllability and text equality, even with minimal performance drop on established metrics like E-ACC and A-ACC.  The method successfully generalizes from seen attribute combinations to unseen ones.  MAE achieves better correlation with human judgments than traditional metrics.  Visualizations show that DCG effectively disentangles attribute combinations and learns relationships between attributes.  The study concludes that prompt-based disentangled control is a promising approach for multi-attribute controllable dialogue generation.</sample>
    <sample id="218">Google Translate.</sample>
    <sample id="219">Jia-Huei Ju from Academia Sinica presented their research on a compare-and-contrast multistage pipeline for uncovering financial signals in financial reports, specifically using the Form 10-K. The work addresses the challenge of extracting valuable information from annual reports, which often require significant human effort. The researchers observed high text similarity between yearly reports and introduced a highlighting task to identify the rationale behind relationships between reports.

The proposed pipeline consists of three stages: document segmentation, relation recognition, and out-of-domain/in-domain fine-tuning. Stage 1 classifies report pairs into three types: highly similar (e.g., regulations), revised (syntactically similar, semantically different), and mismatched (new information).  Fine-tuning utilizes the eSNLI dataset for out-of-domain learning and revised pairs for intermediate tuning, employing soft labeling techniques to improve pseudo-label quality.

The evaluation uses both eSNLI and a newly released FINAL dataset, with performance measured by precision and PCC. The research demonstrates that their domain-adaptive highlighting model achieves strong performance on FINAL and maintains generalization capabilities on eSNLI.  Furthermore, the model benefits from incorporating mismatched pairs during training. The researchers conclude by highlighting the potential for future work in enhancing the model's effectiveness and applicability in information retrieval.  The paper and GitHub repository are available for further details.</sample>
    <sample id="220">Vasudha ist eine Doktorandin der Informatik an der Stony Brook University.</sample>
    <sample id="221">German to English.</sample>
    <sample id="222">The paper "To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering" addresses the problem of adapting open-domain question answering models to new domains, particularly when the source model is trained on a general-purpose corpus like Wikipedia. The authors investigate how to enable out-of-domain generalization by exploring data interventions and identifying the type of dataset shift encountered.

They present a setup where a Wikipedia-based retriever and reader model are tested on seven target datasets spanning six domains.  The study explores two data intervention methods: few-shot learning using manually crafted fact-based questions and zero-shot learning by controlling the interaction of question, answer, and context variables.  

The authors define and measure different types of dataset shifts – no shift, concept shift, covariate shift, and full shift – based on the compatibility of the retriever and reader models with the target domain. They find that few-shot learning is effective across all shift types, while zero-shot learning is more beneficial for concept and covariate shifts.  

The experiments demonstrate that data interventions can improve reader performance by up to 24%. The paper concludes by highlighting the importance of understanding the nature of dataset shift to select the most appropriate data intervention strategy for achieving effective domain adaptation in open-domain QA.</sample>
    <sample id="223">Shangbin, PhD student in the University of Washington.</sample>
    <sample id="224">MASSalign und fine-getunte Long-mBART und base mBART Modelle.</sample>
    <sample id="225">MultiInstruct verwendet 53 Aufgaben für das Training und 5 Aufgaben für das Testen.</sample>
    <sample id="226">The provided text does not mention the number of authors involved in the work. It only names Regina Stodden as the presenter.</sample>
    <sample id="227">The research addresses a key challenge in current language models: grounded language understanding. This involves translating natural language into executable plans or programs within a specific environment, crucial for applications like smart assistants, semantic search, and robotic control.  The core problem lies in the lack of grounding during pre-training, leading to models that struggle with the gap between pre-training data and real-world application scenarios.

Existing approaches often rely on language models to directly generate plans, but these plans can be invalid or grammatically incorrect.  The proposed solution, named Pangu, shifts the focus from generation to discrimination. Pangu employs a symbolic agent to interact with the environment and propose candidate plans, while the language model acts as a scorer, ranking these candidates. This approach avoids the language model's responsibility for plan validity and grammar.

Experiments with BERT, T5, and Codex demonstrate Pangu's effectiveness across fine-tuning and in-context learning, achieving strong sample efficiency, particularly with Codex. Pangu consistently outperforms baseline models like ArcaneQA in sample efficiency.  A notable finding is Pangu's robustness to non-independent and identically distributed (non-i.i.d.) training data, suggesting it generalizes better than autoregressive models.

The central takeaway is that for grounded language understanding, discrimination offers a more promising strategy than generation, leveraging the strengths of language models in evaluating and ranking options.  Pangu represents a novel framework for bridging the gap between language models and real-world environments.</sample>
    <sample id="228">AG News, MIND, SST2 und Enron Spam.</sample>
    <sample id="229">Gabriella Skitalinskaya and Henning Wachsmuth's work focuses on improving argumentative writing by detecting improvable claims. They address the challenge of determining when a claim is well-phrased and whether further revisions are needed. Their research introduces two new tasks: Suboptimal Claim Detection (identifying claims needing revision) and Claim Improvement Suggestion (selecting quality issues for revision).

The paper explores the difficulties of learning from revision-based data, which differs across domains. They specifically focus on argumentative text, leveraging revision histories from collaborative online debate platforms like Kialo.  They identify four key challenges: Representativity and Reliability (ensuring the dataset accurately reflects high-quality claims), Model Complexity and Architecture (selecting models sensitive to small changes in claims), Contextual Information (determining relevant contextual factors for quality assessment), and Topical and User Bias (addressing noise and biases in revision histories).

The authors investigate these challenges through experiments, comparing different models and approaches. Their findings suggest that revision-based data is effective for these tasks, and modeling the distance between claim versions is helpful for identifying suboptimal claims. The impact of contextual information varies depending on the task and the specific quality issues.  The paper concludes that these challenges can be addressed, and revision-based data offers a promising approach to supporting argumentative writing.</sample>
    <sample id="231">NACHOS is a dataset of medical crawled data from the web.</sample>
    <sample id="232">David Vilar.</sample>
    <sample id="233">Simultaneous Speech Translation (SimuST) aims to translate spoken language to text in real-time. Current SimuST models face challenges with long training procedures, optimization objectives, and maintaining multiple models for varying latency. This paper introduces EDAtt (Encoder-Decoder Attention), a strategy for achieving low-latency SimuST by leveraging existing offline ST models. EDAtt utilizes cross-attention to determine when to emit partial translations. It decides to emit a word if its cross-attention weights are below a threshold, indicating stable information. This approach avoids retraining and adapting architectures for different latency regimes, allowing for a single model to handle various latency requirements. Experimental results on German demonstrate that EDAtt outperforms existing strategies like Wait-k and Local Agreement, achieving higher BLEU scores with significantly reduced average lagging.  The computational-aware latency metric further highlights EDAtt's efficiency.  The code and models are publicly available to promote reproducibility.</sample>
    <sample id="234">The prompt strategy has a significant influence on the performance of LLMs for translation, with a difference of over one BLEURT point observed in simple one-shot prompting. While the form of the prompt isn't crucial for several short promptings, the quality of the examples (the prompt examples) is more important than their similarity to the source sentence, especially for five-shot prompting.</sample>
    <sample id="235">```
The authors are affiliated with the University of Toronto.
```</sample>
    <sample id="236">```
The presentation does not explicitly state the content of the 5 expert-written instructions. It only mentions that each task in MultiInstruct is equipped with five expert-written instructions.
```</sample>
    <sample id="237">Die Autoren schlagen vor, ein diagnostisches Test-Suite namens KITMUS zu verwenden, die auf eine Coreference-Resolution-Aufgabe basiert. Sie variieren die Verfügbarkeit von Hintergrundwissen (das während des Pretrainings gelernt wird) und entitätsspezifischem Wissen (das bei der Inferenz verfügbar ist), um verschiedene Szenarien zu simulieren und die Fähigkeit der Modelle zu testen, Informationen aus verschiedenen Quellen zu integrieren.</sample>
    <sample id="238">MeetingBank is a new benchmark dataset for meeting summarization, created by Yebowen Hu from the University of Central Florida. The dataset comprises 1,366 City Council meetings, including transcripts, reference summaries, and URLs. The collection process involved using Speechmatics API for audio transcription and identifying meeting details from the Boston City Council website to retrieve corresponding summaries and segments.

The dataset includes statistics on meeting duration, speaker count, and the year of the meeting.  Analysis reveals that summaries often include verbatim points (high coverage) but can lack abstraction (low density). Extractive summarization systems like Oracle and LEAD show promising results, while abstractive models like DialogLM perform well.  GPT-3, despite strong automatic metrics, performs poorly in human evaluation regarding informativeness and factuality, excelling in fluency and coherence.

The research highlights the need for improved automatic evaluation metrics that better align with human preferences. MeetingBank is intended as a valuable resource for researchers developing advanced meeting summarization technologies and offers insights into City Council decision-making. The dataset is publicly available for download and use.</sample>
    <sample id="239">Hallo zusammen, mein Name ist David Vilar, und ich werde eine kurze Zusammenfassung des Artikels "Prompting PaLM für Übersetzung: Strategien und Leistung bewerten" geben. Dies ist eine gemeinsame Arbeit mit meinen Kollegen von Google Translate. PaLM ist ein 540 Milliarden Parameter großes großes Sprachmodell, das 2022 vorgestellt wurde. Es wurde auf einer großen Sammlung von Texten trainiert, die 780 Milliarden Token umfassen. Zu dem Zeitpunkt der Veröffentlichung erreichte es in Hunderten von NLP-Aufgaben den Stand der Technik. In dieser Arbeit präsentieren wir die erste systematische Studie zur Verwendung von Prompting für große Sprachmodelle bei der maschinellen Übersetzung. Wir haben die Fähigkeit dieser Modelle zur Übergangsleistung anhand der Best Practices der MT-Community bewertet. Dies beinhaltet die Verwendung der neuesten Testdatensätze, um eine Überschneidung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden. Und wir haben es mit den State-of-the-Art-Systemen verglichen, also dem besten performenden System, dem WMT-Bewertungssystem. Wir verwenden State-of-the-Art-neuronale MT-Metriken und zeigen zusätzlich Ergebnisse einer Expertenbewertung durch menschliche Bewerter. Schließlich geben wir einige Empfehlungen für Prompt-Auswahlstrategien. Das Prompting hat einen großen Einfluss auf die Leistung von LLMs bei der Übersetzung, wie wir in einem einfachen Experiment sehen können, bei dem wir eine Ein-Schuss-Prompting-Strategie verwendeten und für jede Satzgruppe zwei verschiedene Prompts bereitstellten. Die Mehrheit der Sätze, 516 von 1.000, die beobachteten Unterschiede betrugen mehr als eine BLEURT-Punktzahl. Dies kann in extremen Fällen bis zu 40 BLEURT-Punkte erreichen. Daher ist es wichtig, eine gute Prompting-Strategie auszuwählen. In unseren Experimenten haben wir uns für eine 5-Schuss-Prompting-Strategie entschieden, bei der wir einfach jeden Satz, den wir dem System zur Verfügung stellen, mit der Sprache markieren, in der er geschrieben ist. In diesem Beispiel hier, bei der wir von Deutsch nach Englisch übersetzen, sind die deutschen Sätze, die Quellsätze, mit Deutsch-Doppelpunkt markiert und die englischen Übersetzungen mit Englisch-Doppelpunkt. Wir haben festgestellt, dass die tatsächliche Form des Promptings keinen großen Einfluss auf die Leistung bei mehreren kurzen Promptings hat. Es ist entscheidend für Ein-Schuss- und Null-Schuss-Prompting. Und wenn wir, wie in unserem Fall, zu fünf-Schuss-Prompting übergehen, gibt es kaum einen Unterschied zur tatsächlichen Form des Promptings. Die Beispiele tragen den größten Teil der Gewichtung. Die Zusammenfassung unserer experimentellen Ergebnisse ist, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit zur Quellsatz. Es ist wichtig, Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die Auswahl von Prompts aus den Trainingsdaten für die WMT-Bewertungen auf dem Entwicklungsdatensatz. Der Entwicklungsdatensatz ist viel besser kuratiert und von höherer Qualität als der Trainingsdatensatz, was bedeutet, dass er lauter ist. Und ihre Ergebnisse sind daher besser, wenn sie den Entwicklungsdatensatz verwenden. Dennoch haben spezialisierte State-of-the-Art-Systeme einen erheblichen Vorteil gegenüber den PaLM-Übersetzungen. Aber PaLM kommt ziemlich nahe an ein kommerzielles System heran. In unserem Fall haben wir mit Google Translate bewertet. Die Erkenntnisse, die wir aus der menschlichen Bewertung gewonnen haben, die wir mit dem MQM-Framework durchgeführt haben, zeigten, dass die Flüssigkeit von PaLM vergleichbar mit State-of-the-Art-Systemen ist, aber die Hauptunterscheidung besteht in der Genauigkeit. Insbesondere sind die häufigsten Fehler Auslassungsfehler. Es scheint, dass PaLM dazu neigt, eine besser klingende Übersetzung zu erzeugen, indem es Teile des Quellsatzes entfernt, die in der Übersetzung entstanden sind. Allerdings ist die Kategorie "Stil/Unangenehm" für PaLM niedriger als bei den State-of-the-Art-Systemen, was ein zusätzliches Signal dafür ist, dass PaLM wirklich flüssige Ausgaben liefert, aber immer noch mit einigen Genauigkeitsproblemen. Und das war's für diese wirklich kurze Übersicht. Für weitere Details kommen Sie bitte zur vollständigen Präsentation des Artikels. Vielen Dank.</sample>
    <sample id="240">Hallo, ich bin Dawei, Doktorand an der Universität Saarland in Deutschland. In diesem Video möchte ich unsere aktuelle Arbeit "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" vorstellen. Dies ist eine gemeinsame Arbeit mit Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow. Ich möchte mit einer kurzen Einführung in schwach überwachtes Lernen und schwach überwachtes Lernen beginnen. Bei schwach überwachtem Lernen werden die Daten nicht manuell gelabelt. Stattdessen werden die Daten mit schwachen Labeling-Quellen gelabelt, wie z. B. einfachen Heuristiken, Wissensbasen oder niedrigwertiger Crowdsourcing, wie in der Abbildung auf der rechten Seite dargestellt. Im Vergleich zu menschlichen Annotationen sind die schwachen Annotationen viel günstiger, aber auch verrauscht, d. h. eine gewisse Anzahl der Annotationen ist falsch. Wenn wir direkt neuronale Netze auf schwach gelabelten Daten trainieren, neigen die neuronalen Netze dazu, die Label-Störungen zu merken und nicht zu generalisieren. Bei schwach überwachtem Lernen werden Trainingsalgorithmen vorgeschlagen, um neuronale Netze robust unter solchen Label-Störungen zu trainieren, so dass die trainierten Modelle immer noch gut generalisieren. In jüngsten Arbeiten zum WSL (Weakly Supervised Learning) wird oft behauptet, dass man nur auf den schwach gelabelten Daten trainiert und auf sauberen Testdatensätzen hohe Leistungen erzielt. Technisch gesehen ist diese Behauptung nicht falsch, aber es gibt einen Haken: man geht davon aus, dass es einen zusätzlichen sauberen Validierungsdatensatz gibt, der zur Modellauswahl verwendet werden kann. Wir können nicht auf diesem Problemfall verharren, aber dies impliziert, dass zusätzliche manuelle Annotationen erforderlich sind, um schwach überwachtes Lernen zu ermöglichen. Aber wie ein Elefant im Raum wird diese Notwendigkeit oft übersehen. Die oben genannte Frage wird gestellt: Erstens, ist ein sauberer Validierungsdatensatz für WSL erforderlich oder kann man vielleicht einen verrauschten Validierungsdatensatz verwenden? Zweitens, wenn saubere Daten erforderlich sind oder wenn saubere Daten für WSL obligatorisch sind, wie viele saubere Stichproben benötigen wir? Und drittens, sollten wir nur die sauberen Stichproben für die Validierung verwenden oder gibt es bessere Möglichkeiten, sie zu nutzen? Wir haben diese Forschungsfragen in unserer Arbeit beantwortet, und unsere Ergebnisse sind wie folgt. Erstens stellen wir fest, dass aktuelle WSL-Methoden tatsächlich saubere Validierungsstichproben benötigen, um ordnungsgemäß zu funktionieren. Andernfalls gibt es einen großen Leistungsabfall. Wie in dieser Abbildung gezeigt, wenn es keine sauberen Validierungsstichproben gibt, können die trainierten Modelle nicht über die ursprünglichen schwachen Labels hinaus generalisieren, was die Trainingsarbeit sinnlos macht. Dies deutet darauf hin, dass WSL-Ansätze tatsächlich saubere Daten benötigen, um ordnungsgemäß zu funktionieren, und die Kosten für die Beschaffung sauberer Validierungsstichproben sollten nicht übersehen werden. Unsere zweite Feststellung ist, dass die Erhöhung der Anzahl der sauberen Validierungsstichproben die WSL-Ansätze dazu verhilft, bessere Leistungen zu erbringen, wie in der linken Abbildung gezeigt. Typischerweise benötigen wir nur 20 Stichproben pro Klasse, um eine hohe Leistung zu erzielen. Aber das ist noch nicht alles, denn wenn wir entweder auf saubere Stichproben zugreifen oder sie für WSL obligatorisch machen, dann führt das direkte Training auf ihnen sogar zu einer besseren Leistung. Die rechte Abbildung zeigt den Leistungsunterschied zwischen Fine-Tuning-Ansätzen, die direkt auf den sauberen Daten angewendet werden, und WSL-Ansätzen, die die sauberen Daten für die Validierung verwenden. Wie wir sehen können, beginnt die direkte Fine-Tuning-Methode mit 10 Stichproben pro Klasse, die WSL-Ansätze zu übertreffen. Schließlich kann die Leistungsverbesserung, die in früheren WSL-Ansätzen behauptet wird, leicht durch die Möglichkeit, die trainierten Modelle auf den sauberen Validierungsstichproben weiter zu verfeinern, erreicht werden. Wie aus den Abbildungen hervorgeht, unterliegt der Vanilla-Modell (FTw) initial den komplexeren WSL-Methoden wie COSINE, die schlechter abschneiden. Aber wenn wir die Möglichkeit haben, die trainierten Modelle auf den sauberen Stichproben weiter zu verfeinern, dann erzielt FTw die gleiche Leistung wie andere Methoden. In der Praxis gibt es also keinen Grund, komplexere WSL-Methoden zu wählen, die mehr Rechenzeit und Speicherplatz erfordern. Zusammenfassend haben wir gezeigt, dass aktuelle WSL-Ansätze saubere, manuell annotierte Stichproben benötigen, um ordnungsgemäß zu funktionieren. Ihre Leistungssteigerung und Praktikabilität sind stark überbewertet. Unsere konkreten Empfehlungen für zukünftige Arbeiten sind wie folgt: Erstens, berichten Sie über die Modellauswahlkriterien. Zum Beispiel berichten Sie, ob die Modellauswahl anhand von sauberen Validierungsstichproben erfolgt. Zweitens, WSL-Ansätze sollten mit Few-Shot-Learning-Baselines verglichen werden, da beide auf sauberen Daten arbeiten. Drittens ist kontinuierliches Fine-Tuning ein einfacher, aber starker Baseline-Ansatz, der in zukünftigen WSL-Arbeiten berücksichtigt werden sollte. Schließlich haben wir unseren Code Open Source gestellt. Sie können ihn über den QR-Code auf diesem Slide finden. Bitte fühlen Sie sich frei, ihn zu überprüfen. Vielen Dank und viel Spaß auf der Konferenz.</sample>
    <sample id="241">Ethan, zusammen mit Yang Chen, Wei Xu und Alan Ritter von Georgia Tech, stellt in ihrer Arbeit "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments" einen neuen Ansatz zur Bewertung von Systemen zur Erkennung von Fehlinformationen vor. Sie kritisieren bestehende Methoden, die unrealistische Evaluierungsdaten und eine fehlende Berücksichtigung menschlicher Faktoren aufweisen.

Die Autoren schlagen einen end-to-end-Evaluierungsrahmen vor, der menschliches Feedback in allen Phasen des Prozesses integriert. Ihr System für COVID-19-Behandlungsinformationen umfasst die Erkennung von potenziell falschen Behauptungen durch Keyword-Filterung und eine T5-basierte Frage-Antwort-Modellierung, gefolgt von einer Trendanalyse und anschließender menschlicher Überprüfung. Ein zweiter Teil des Systems konzentriert sich auf die Überprüfung von Verstößen gegen die Richtlinien von Twitter, wobei ein BERT-basiertes Stance-Classification-Modell verwendet wird.

Die Evaluation zeigt, dass das System eine hohe Genauigkeit bei der Erkennung von Policy-Verstößen erreicht (65%) und eine hohe Anzahl von potenziellen Verstößen pro menschliche Arbeitsstunde ermöglicht (124,2).  Die Autoren betonen die Bedeutung der frühen Erkennung von Fehlinformationen und die Notwendigkeit, menschliche Expertise in den Entwicklungsprozess einzubeziehen. Ihr Framework bietet eine realistischere Bewertung von Systemen und motiviert die Entwicklung zukünftiger human-in-the-loop-Ansätze.</sample>
    <sample id="242">Gängige Bewertungsmethoden für Dialogsysteme sind menschliche Bewertungen, wie z. B. die Auswahl der besseren von zwei Gesprächen oder die Bewertung von Gesprächen anhand einer Likert-Skala. Es gibt auch Methoden zur Bewertung von Gesprächen auf der Ebene der einzelnen Gesprächspunkte oder auf der Ebene des gesamten Gesprächs.</sample>
    <sample id="243">```
Sebastian Santy, Ronan Le Bras, Katharina Reinecke, and Maarten Sap.
```</sample>
    <sample id="244">Servin ist ein Richter und Richter entscheiden Fälle in einem Gericht.</sample>
    <sample id="245">Lining Zhang's presentation details a pipeline for identifying high-agreement workers on Amazon Mechanical Turk (MTurk) for summarization tasks. The motivation stems from the limitations of automatic metrics and a lack of established best practices for MTurk recruitment. The pipeline employs a two-step qualification process: a qualification task assessing annotator ability to evaluate summaries based on six dimensions, and an endurance task testing workload capacity.

The qualification task categorizes workers into "gold," "silver," "bronze," and "block" tiers, with only "gold" and "silver" workers passing. The endurance task further filters, resulting in 12 workers, with 4 "gold" and 8 "silver." These workers demonstrate high inter-annotator agreement (IAA), exceeding that of experts.

A reference-based task further evaluates performance, with pipeline workers achieving a Krippendorff's Alpha of 0.534.  The presentation compares the pipeline to Baseline MTurk workers (using MACE as the best approach, achieving a 0.380 Alpha) and CloudResearch MTurk workers (achieving a 0.513 Alpha but with lower task acceptance).  Analysis of correctness across annotation sources reveals significant Spearman's correlation between pipeline and CloudResearch workers, while GPT models correlated well with expert judgments.

The pipeline offers a cost-effective solution for high-agreement annotations, potentially rivaling CloudResearch's quality.  Future work will focus on improving worker quality in terms of both agreement and correctness, and expanding the pipeline to various tasks, languages, and platforms. Limitations include the English-only focus, the potential for the qualification questions not to be universally effective, and the lack of a guarantee of correctness.</sample>
    <sample id="246">Yes, the code and dataset are available on GitHub.</sample>
    <sample id="247">## FACTKG: Fact Verification via Reasoning on Knowledge Graphs - Summary

Jiho Kim from KAIST AI presents FACTKG, a novel dataset and task for fact verification leveraging knowledge graphs (KGs). Existing datasets like FEVER and VitaminC primarily use text or tables as evidence, but FACTKG introduces a KG-based approach, addressing the need for reliable fact verification in a more practical and scalable manner.

The core idea is that KGs offer intuitive and directly connectable evidence, enabling robust reasoning beyond simple text matching. This is particularly relevant for applications like dialogue systems that interact with KGs to ensure consistency. FACTKG utilizes DBpedia as the KG and presents claims in both written and colloquial styles, categorized as SUPPORTED or REFUTED.

The task involves retrieving evidence from DBpedia and verifying the claim through reasoning, encompassing five types: one-hop, conjunction, existence, multi-hop, and negation.  The dataset includes examples for each reasoning type, requiring varying levels of inference.  

To address the colloquial style, the researchers employed a colloquial style transfer model and presupposition templates.  They also constructed baseline models, comparing claim-only verification with evidence-based verification using the GEAR model.  The results demonstrate that GEAR, utilizing KG evidence, significantly outperforms all other baselines, achieving a verification accuracy exceeding 50%.  FACTKG provides a valuable resource for advancing fact verification research and practical applications involving knowledge graphs.</sample>
    <sample id="248">```json
{
  "answer": "The study did not find that annotators were balanced across all demographic groups. For example, datasets and models were less aligned with non-binary people compared to men and women. The study amassed over 16,000 annotations from over 1000 annotators from 87 countries, but the distribution of annotators across different demographics was not uniform."
}
```</sample>
    <sample id="249">The sentences within the acceptable domain were perturbed by adding noise while preserving the relevant structure.</sample>
    <sample id="250">Eine dimensionale Bewertung von konversationeller KI bedeutet, die Qualität des Gesprächs anhand mehrerer Aspekte zu messen, anstatt nur einen einzigen Aspekt zu bewerten. Dies ermöglicht ein detaillierteres Verständnis der Stärken und Schwächen eines Modells.</sample>
    <sample id="251">University of Science and Technology of China.</sample>
    <sample id="252">```
U-CREAT: Unsupervised Case Retrieval using Events Extraction, presented by Sai Kiran Tanikella from IIT Kanpur, addresses the challenge of prior case retrieval in legal domains.  The task involves identifying relevant past precedents cited within a query document.  The presentation highlights two key contributions: the IL-PCR dataset and the U-CREAT pipeline.

The IL-PCR dataset, a benchmark for prior case retrieval, comprises 7,070 Indian legal cases with a high average citation count, offering a more comprehensive test bed than existing datasets like COLIEE'21.  U-CREAT leverages unsupervised learning and an event-based approach to retrieve relevant cases.  It extracts events from documents using dependency parsing, identifying subject-verb-object triplets.  These events are then used to compute an interaction matrix between the query and candidate events, enabling ranking.

Experiments with various models, including count-based, transformer-based, and event-based approaches, demonstrate U-CREAT's effectiveness. While transformer models showed limited performance, event-based models significantly outperformed baselines, achieving higher F1 scores and lower inference times.  The Event Filtered Documents model achieved the best results, surpassing existing supervised approaches like MTFT-BERT on the COLIEE'21 dataset.  U-CREAT offers a promising solution for prior case retrieval, particularly in the Indian legal context, and opens opportunities for future research.
```</sample>
    <sample id="253">DisorBERT is a new model developed by a team of researchers from Mexico and Spain to detect signs of mental disorders in social media posts. The project addresses the challenge of insufficient labeled data by leveraging domain adaptation, a technique that transfers knowledge from a related domain (like general text data used to train BERT) to a more specific domain (social media related to mental health).

The model utilizes a double domain adaptation approach, integrating information from Reddit and mental health resources, and employs guided masking to focus on important words during training. This allows DisorBERT to better understand the nuances of language associated with mental disorders.

Experiments on the eRisk dataset show that DisorBERT achieves a good balance between precision and recall, outperforming baseline models like BERT and MentalBERT.  Analysis of the model's predictions reveals a tendency to generate words related to psychological distress, such as "focus," "talk," and "sleep," compared to BERT's more general vocabulary.  Visualization techniques highlight key words and sentences indicative of depression, like "anxious" and "medication."

The researchers conclude that DisorBERT effectively captures signs of mental disorders in social media, demonstrating a solid balance between detection accuracy and correct labeling. Future work will focus on exploring different lexical resources and incorporating clinical data to further improve the model's performance.</sample>
    <sample id="254">Sun Qi from Nanjing University of Science and Technology presents research on "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction." The work addresses the challenge of noise in distant supervision (DS) data for document-level relation extraction, which relies on large human annotations.  Current methods using pseudo-labels are susceptible to false positives, leading to incorrect relation predictions.

The proposed framework employs uncertainty-guided label denoising to improve label quality. It first trains a pre-denoising DocRE model on DS and human-annotated data to generate pseudo-labels.  Crucially, it introduces instance-level uncertainty estimation to assess the trustworthiness of model predictions, especially for overlapping relations.  A dynamic class uncertainty threshold is then used to filter out low-confidence pseudo-labels.  Finally, a multi-phase training strategy iteratively refines the DS data.

The research leverages Monte Carlo dropout for uncertainty modeling in the DocRE task, addressing limitations of previous methods that struggled with overlapping relations.  The study demonstrates improved performance compared to strong baselines on public datasets.  Key contributions include the uncertainty-guided denoising framework, instance-level uncertainty estimation for overlapping relations, dynamic class uncertainty thresholds, and significant performance gains. The work aims to mitigate noise in DS data, leading to more accurate document-level relation extraction.</sample>
    <sample id="255">Die Form des Prompts ist bei Ein- und Null-Shot-Prompting wichtig, hat aber bei fünf-Shot-Prompting kaum Einfluss.</sample>
    <sample id="257">Die Autoren haben vier state-of-the-art Chatmodelle evaluiert.</sample>
    <sample id="258">Chiang Cheng-Han introduces their new work, "Can Large Language Models Be an Alternative to Human Evaluation?" The research explores using large language models (LLMs) to evaluate the quality of text in natural language processing, offering a potential alternative to the unstable and difficult-to-reproduce human evaluation process.

The motivation stems from the limitations of human evaluation, which can be subjective and inconsistent. The researchers propose leveraging LLMs' ability to follow natural language instructions to assess text quality based on attributes like grammar, coherence, likability, and relevance. They conduct an experiment using GPT-2 and human-written stories, evaluating them with four different LLMs (T0, InstructGPT, and ChatGPT).

The results show that human raters generally prefer human-written stories, but some smaller LLMs don't show a clear preference. However, larger models like Davinci and ChatGPT exhibit a clear preference for human-written text, aligning with human judgment. This suggests that certain LLMs can indeed serve as an alternative to human evaluation.

The video highlights that the paper addresses further questions regarding the agreement between LLMs and human evaluators, the impact of instruction wording and sampling methods, and the benefits and costs of using LLM evaluation compared to human evaluation.  The researchers invite viewers to read the paper or visit their poster at ACL for more details.</sample>
    <sample id="259">XSemPLR is a new benchmark dataset and evaluation framework for cross-lingual semantic parsing, addressing the limitations of existing models which often focus on limited language coverage or specific meaning representations. The dataset includes 9 datasets across various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages spanning 15 language families.

The study explores six evaluation settings: Translate-Test (using Google Translate), Monolingual, Monolingual Few-shot, Multilingual, Cross-lingual Zero-shot, and Cross-lingual Few-shot transfer.  The research compares the performance of different multilingual language models, including Encoder-PTR (like XLM-R + PTR) and Encoder-Decoder models (like mBART and mT5).

Findings reveal that Encoder-Decoder models achieve the best performance across all datasets. Training on a mixture of languages improves performance, although English performance can suffer due to the "Curse of Multilinguality."  Cross-lingual transfer performance gaps are significant in zero-shot settings but narrow with few-shot learning.  The study also highlights that pretraining on English can boost few-shot performance on other languages, and that large multilingual models like Codex and BLOOM are not yet optimal for this task.  XSemPLR provides a valuable resource for advancing research in cross-lingual semantic parsing.</sample>
    <sample id="260">The provided text is an advertisement for a paper. It mentions Jingwei Yi from the University of Science and Technology of China as the presenter. The text does not explicitly state the number of authors involved in the paper.

The answer is: The text does not specify the number of authors.</sample>
    <sample id="261">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="262">Der Text erwähnt nur Siyu Yuan von Fudan University. Es wird nicht erwähnt, wie viele Autoren an der Arbeit beteiligt sind.</sample>
    <sample id="263">This work addresses instability in in-context learning, a popular method for utilizing large language models. The instability stems from design choices like example order, introducing biases in model predictions.  The authors propose a systematic categorization of label biases, identifying "domain-label bias" as a new, significant issue.

They demonstrate that the task corpus can bias model predictions, even with random English words, and that this bias is amplified in tasks with strong domain-label bias.  Existing calibration methods, using single content-free tokens, are insufficient for mitigating this domain-label bias.

The authors introduce "domain-context calibration," which uses random in-domain words as content-free text to estimate and then calibrate model predictions.  Experiments on various datasets show that domain-context calibration significantly improves in-context learning performance, especially in tasks with high domain-label bias.  Furthermore, it leads to better decision boundaries.

The study reveals that pre-defined content-free tokens can also be biased, and using more random words yields further improvements.  By using random in-domain words, the method effectively addresses domain-label bias.  The authors conclude that domain-context calibration offers a holistic approach to mitigating all types of label biases in in-context learning, leading to more robust and accurate results.</sample>
    <sample id="264">Lin Wang from Zhejiang University presents "TAVT: Towards Transferable Audio-Visual Text Generation," addressing the challenge of data scarcity and domain shifts in multimodal text generation tasks like audio-visual text generation. Existing models struggle with varying conditions across domains, particularly visual style and audio characteristics.

The proposed TAVT framework tackles this by introducing a "unified audio semantic space" to align visual concepts across domains.  It consists of three key components: an audio-visual meta-mapper network, an audio-visual encoder and language model generator, and counterfactual contrastive learning. The meta-mapper network uses audio clustering and learnable visual prefixes to map visual concepts to the audio space. The encoder and generator utilize a transformer architecture with an alpha parameter to assess the contribution of each modality.

A crucial innovation is Dual Counterfactual Contrastive Learning (DCLL), which leverages counterfactual results to directly optimize visual-textual alignment, bypassing the need for high-quality negative samples.  The framework employs a meta-learning approach similar to MAML, using support and query sets for fast adaptation to new domains.

Experiments on MSVD and MSR-VTT benchmarks demonstrate that TAVT significantly outperforms state-of-the-art RNN and transformer-based models, especially in low-resource domains. Ablation studies confirm the importance of audio features.  The research highlights the potential of transferable audio-visual text generation for applications with limited labeled data.</sample>
    <sample id="265">Vasudha</sample>
    <sample id="266">This text does not mention the authors' affiliation with a university.</sample>
    <sample id="268">Die häufigsten Fehler von PaLM sind Unterlassungsfehler.</sample>
    <sample id="269">Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und heute erzählen wir Ihnen alles über ABC-Eval, einen neuen dimensionalen Ansatz zur Bewertung konversationeller KI. Diese Arbeit wurde vom Emory NLP Lab unter der Leitung von Professor Jinho Choi an der Emory University in Zusammenarbeit mit Amazon Alexa AI durchgeführt. Also, sagen wir, Sie haben ein Dialogmodell entwickelt und möchten sehen, wie gut es im Vergleich zum aktuellen Stand der Technik abschneidet. Die übliche Praxis ist, menschliche Bewertungen zu verwenden, z. B. indem man menschliche Gutachter bittet, zu entscheiden, welche von zwei Gesprächen besser ist oder Gespräche anhand einer Likert-Skala zu bewerten. Diese Ansätze funktionieren gut, um umfassende Bewertungen der Gesamtgesprächsqualität zu liefern, aber die Gesprächsqualität hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chatqualität bewerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen. Eine Möglichkeit ist, einfach menschliche Gutachter zu bitten, mehrere Dimensionen der Gesprächsqualität zu bewerten, z. B. die Relevanz der Modellantworten mithilfe bestehender vergleichender oder Likert-Skalen-Methoden. Wir glauben jedoch, dass es einen präziseren und zuverlässigeren Strategie für die dimensionale Bewertung von Dialogen gibt. Unser Ansatz versucht, die Subjektivität menschlicher Bewertungen zu reduzieren, indem wir explizit annotieren, ob jede Modellantwort bestimmte Verhaltensweisen ausdrückt, wie z. B. die Antwort mit irrelevanten Informationen, die sich selbst widerspricht oder mit der Partnerin widerspricht, halluzinierte falsche Fakten oder die Verletzung von allgemeinem Wissen und wann das Modell erfolgreich oder fehlgeschlagen ist, die Empathie zu zeigen. Um zu bestimmen, welche Art von Bewertung am effektivsten ist, haben wir vier modernste Chatmodelle ausgewählt und sie anhand von 100 menschlich-roboter-Gesprächen pro Modell mit ABC-Eval bewertet. Zur Vergleichsbewertung haben wir diese Gespräche auch mit drei bestehenden Methoden bewertet: Likert-Bewertungen auf der Turn-Ebene, Likert-Bewertungen auf der Dialog-Ebene und Dialog-Ebene-Paarvergleiche. Für jede dieser bestehenden Methoden haben wir Bewertungen auf acht der am häufigsten gemessenen Aspekte des Dialogs gesammelt, da dies die Standardpraxis für die Bewertung von Chatmodellen entlang mehrerer Dimensionen ist. Bei unserer Analyse dieser Bewertungsergebnisse haben wir festgestellt, dass die ABC-Eval-Verhaltensbeschriftungen insgesamt zuverlässiger sind als die von bestehenden Methoden gesammelten Beschriftungen, wie durch die Inter-Annotator-Übereinstimmung bei 100 doppelt beschrifteten Gesprächen gemessen. Darüber hinaus sind die ABC-Eval-Beschriftungen im Vergleich zu den von bestehenden Methoden produzierten Metriken aussagekräftiger für die Gesamtgesprächsqualität, wie in dieser einfachen linearen Regression gezeigt. Zum Beispiel zeigt, wie der Anteil der Gespräche mit Selbst- und Partner-Kontradiktionen 5 % und 10 % der Gesprächsqualität erklärt, während die durchschnittlichen Likert-Konsistenzbewertungen nur 4 % oder weniger ausmachen. Schließlich haben wir überprüft, ob jede Evaluationsmetrik einen einzigartigen Aspekt der Chatqualität erfasst, mithilfe einer schrittweisen linearen Regression. Sie können sehen, wie die Kombination aller ABC-Eval-Metriken über 25 % der Gesprächsqualität erklärt, und wie die Entfernung der Metriken einzeln zu einem Verlust an Informationen über die Qualität führt. Die Kombination aller Turn-Ebene-Likert-Metriken erklärt jedoch viel weniger der Qualität, und weniger dieser Metriken tragen einzigartige Informationen. Diese zuverlässigen, informativen und unterschiedlichen ABC-Eval-Metriken ermöglichen es uns, konversationelle KI mit einer höheren Auflösung zu bewerten, als es frühere Methoden leisten können. Sie können sehen, dass in den Ergebnissen unseres Experiments mehrere Herausforderungen noch bestehen und genau quantifiziert wurden. Zum Beispiel haben die von uns getesteten Roboter in etwa 20 % ihrer Antworten allgemeine Sinnverletzungen aufweisen. Sie produzieren in etwa 15 % ihrer Antworten irrelevante Informationen und widersprechen sich oder ihrer Partnerin in etwa 10 % der Zeit. Angesichts des rasanten Fortschritts im Feld könnten viele dieser Fehlerquoten in neuen Modellen, die seit unserer Bewertung durchgeführt wurden, sinken. Dies ist jedoch gerade der Grund mehr, zuverlässige und präzise Evaluationsmetriken für den Vergleich von Modellen zu entwickeln. Wir hoffen, dass ABC-Eval von anderen im Feld genutzt werden kann als ein bedeutsamer Schritt in diese Richtung. Und wir freuen uns darauf, zu sehen, wie sich konversationelle KI in den kommenden Monaten und Jahren weiterentwickelt. Vielen Dank fürs Zuschauen.</sample>
    <sample id="270">Emory University.</sample>
    <sample id="271">CFT steht für Fine-tuning.</sample>
    <sample id="272">John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams.</sample>
    <sample id="273">Kayo Yin und ihre Kollegen, Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig, präsentieren ihre Arbeit "When Does Translation Require Context? A Data-driven, Multilingual Exploration". Die Arbeit zeigt, dass viele Übersetzungen kontextabhängig sind. Ein Beispiel ist die Übersetzung des Wortes "mole". In einem Satz könnte es sich auf einen Spion beziehen, in einem anderen auf eine Narbe. Die Schwierigkeit bei der Bewertung von Modellen liegt darin, dass nur ein kleiner Teil der Übersetzungen kontextabhängig ist, was traditionelle Metriken wie BLEU nicht erfassen können. Es gibt jedoch Versuche, sich auf kontextabhängige Übersetzungen zu konzentrieren, aber diese Ressourcen sind oft auf bestimmte Domänen und Sprachen beschränkt.

Die Arbeit untersucht daher zwei Fragen: Wann ist Kontext für die Übersetzung erforderlich, und wie gut können Modelle diese Fälle bewältigen? Um die erste Frage zu beantworten, messen sie, wie stark ein Wort während der Übersetzung vom Kontext abhängt. Sie erweitern die Maßnahme CXMI, die bereits verwendet wurde, um die Kontextnutzung von maschinellen Übersetzungsmodellen zu messen, auf Pointwise CXMI, um die Kontextnutzung auf Satz- oder Wortebene zu messen. Wörter mit hohem P-CXMI werden als solche identifiziert, die für die Übersetzung Kontext benötigen.

Die Forscher analysieren Wörter mit hohem P-CXMI anhand von Transkripten von TED Talks, die von Englisch in 14 verschiedene Sprachen übersetzt wurden. Sie untersuchen dabei drei Ebenen: Part-of-Speech-Tags mit hohem P-CXMI (z.B. Dualpronomen in Arabisch), Wortvokabeln mit hohem P-CXMI (z.B. die Notwendigkeit, in Chinesisch die gleiche Übersetzung für Eigennamen beizubehalten) und einzelne Tokens mit hohem P-CXMI (z.B. Ellipsenauflösung).

Auf der Grundlage ihrer Ergebnisse entwickeln sie einen Benchmark für die Dokumentübersetzung namens MuDA (Multilingual Discourse-Aware Tagger). Sie erstellen Tagger, die automatisch Wörter identifizieren, die bestimmte Diskursphänomene aufweisen. Sie verwenden dann diesen Tagger, um einen Parallelkorpus für die Bewertung zu analysieren und die Leistung verschiedener Modelle anhand von Kontext-abhängigen Beispielen zu messen.

Die Ergebnisse zeigen, dass während corpus-level Metriken wie BLEU kontextunabhängige Modelle die beste Leistung erbringen, kontextbewusste Modelle bei Metriken wie COMET besser abschneiden und word f-measure bei Modellen mit und ohne Kontext vergleichbare Ergebnisse liefert. Die Verwendung des MuDA-Benchmarks zeigt, dass kontextbewusste Modelle bei bestimmten Diskursphänomenen wie Formalität und lexikalischer Kohäsion deutlich genauer sind, aber bei anderen wie Ellipsen, Pronomen und Verbform nicht viel besser sind.

Verglichen mit kommerziellen Systemen zeigt der MuDA-Benchmark, dass DeepL in der Regel genauer ist als Google Translate bei der Dokumentübersetzung.

Zusammenfassend identifizieren die Forscher anhand einer Datenanalyse in 14 Sprachen, wann Übersetzungen Kontext benötigen, und entwickeln einen Benchmark, der hilft zu verstehen, welche Diskursphänomene Modelle gut oder schlecht bewältigen und welche Übersetzungsmodelle für die Dokumentübersetzung geeignet sind.</sample>
    <sample id="274">Yusen Zhang.</sample>
    <sample id="276">Ananya and Vignesh's work addresses the understudied area of evaluating machine translation metrics for Indian languages, focusing on a dataset called "IndicMT Eval." They collected 7,000 translation samples across five Indian languages (Tamil, Malayalam, Hindi, Marathi, Gujarati) using seven translation models and obtained human annotations detailing errors and overall scores, following the MQM framework.

The study analyzed the correlation between various metrics (chrF, LabSE embedding, BERTscore, MuRIL, COMET variants) and human judgments.  While overlap-based metrics like chrF showed the highest overall correlation, they performed poorly. Embedding-based metrics like LabSE and BERTscore showed better correlations, especially when using multilingual models. COMET-metric variants exhibited the strongest correlations overall, but with a skewed score distribution.  Accuracy errors showed a stronger correlation with human scores than fluency errors.

To improve metric performance, they fine-tuned the COMET metric using their MQM dataset, creating IndicCOMET variants.  These variants outperformed COMET baselines on three of the five languages and showed higher correlations across all languages.  Furthermore, IndicCOMET demonstrated zero-shot translation accuracy on an unseen language and exhibited greater robustness compared to the original COMET metric on the ACES Translation Accuracy Challenge Sets. The dataset is publicly available for use.</sample>
    <sample id="277">The new method is called "Multiset Tagging and Latent Permutations".</sample>
    <sample id="278">The authors describe the "marked words" method as a way to identify words that distinguish marked groups from unmarked ones. It leverages the sociolinguistic concept of "markedness," where dominant groups are linguistically and socially unmarked, while marginalized groups are usually marked. The method compares personas generated for different groups using weighted log-odds ratios to identify the top words associated with each marked group, revealing patterns of stereotypes and essentializing narratives.</sample>
    <sample id="279">Shangbin ist PhD-Student an der University of Washington.</sample>
    <sample id="280">Shi Tao's presentation introduces MultiEMO, a novel attention-based framework for emotion recognition in conversations (ERC). The goal is to predict the emotion of each utterance, considering textual, audio, and visual modalities. Existing methods struggle with multimodal complementarity, minority emotion classes, and distinguishing semantically similar emotions.

MultiEMO addresses these challenges with four key components: VisExtNet for visual feature extraction (integrating facial expressions without redundant scene information), MultiAttn for multimodal fusion (using bidirectional multi-head cross-attention layers to integrate text, audio, and visual cues), a Sample-Weighted Focal Contrastive Loss to handle minority and similar emotions, and finally, emotion classification.

VisExtNet integrates facial expressions from multiple frames, avoiding redundant scene data. MultiAttn fuses modalities through stacked cross-attention layers, allowing for complementary information. The Sample-Weighted Focal Contrastive Loss prioritizes difficult cases and separates similar emotions.

Experiments on MELD and IEMOCAP demonstrate MultiEMO's state-of-the-art performance, particularly in challenging scenarios and for minority emotions. Limitations include VisExtNet's inability to distinguish speakers, the SWFC loss's batch size requirement on MELD, and still-worse performance on minority emotions compared to majority classes.</sample>
    <sample id="281">This presentation details research on when and how context impacts machine translation. The study, conducted by a team including Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig, investigates the challenges of evaluating translation accuracy when context is crucial.

The research addresses two key questions: when does translation require context, and how well do current models handle these cases?  They extend the CXMI measure to P-CXMI, quantifying context usage at the word or sentence level. Analyzing transcripts of English-to-14-language TED talks, they identify patterns in word usage related to context, such as dual pronouns in Arabic, verb form choices, and formality.

To create a more accurate evaluation benchmark, they developed the Multilingual Discourse-Aware (MuDA) tagger, which automatically identifies discourse phenomena like formality and lexical cohesion.  They then used this benchmark to evaluate various machine translation models, finding that context-aware models significantly outperform context-agnostic models on phenomena like formality and lexical cohesion. However, performance on phenomena like ellipsis and pronouns is similar.

The study also compares commercial translation systems, finding DeepL often outperforms Google Translate in document-level translation.  The overall conclusion is that while context is vital for accurate translation, current models struggle with it, highlighting areas for future improvement in document-level machine translation.</sample>
    <sample id="282">StoryTrans addresses the challenge of non-parallel story author-style transfer, a crucial task in natural language generation. Existing methods primarily focus on token or sentence-level style transfer, lacking the ability to transfer complex author linguistic preferences at the story level. This work introduces StoryTrans, a model that learns discourse representations from source texts and combines them with learnable style embeddings for style transfer. A novel training objective reduces stylistic features from discourse representations, aligning representations across texts, and enhances content preservation through a two-stage generation process. The first stage uses self-reconstruction, disentanglement, sentence order, and style classifier losses. The second stage focuses on filling style-specific content and removing masks. Extensive experiments on new datasets in Chinese and English demonstrate StoryTrans's superior performance in style control and content preservation compared to strong baselines. Style visualization confirms alignment with golden text in the style feature space. StoryTrans effectively enriches storylines with relevant phrases and maintains source semantics during rewriting. The model's data and code are publicly available.</sample>
    <sample id="283">Dependency Structure of Coordination.</sample>
    <sample id="284">Peng Tianshuo from Wuhan University presented "FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction" at ACL Main Conference 4,915. The paper addresses the limitations of current span-based Universal Information Extraction (UIE) models, which heavily rely on precise span boundaries, often ambiguous in annotation. The proposed solution introduces a "fuzzy span" mechanism, representing span boundaries as continuous probability distributions rather than discrete points.

FSUIE addresses the mismatch between transformer feature extraction and information extraction by employing adaptive attention. Instead of static attention, the attention mechanism dynamically adjusts its span length using an optimizable parameter, and the attention distribution decays linearly. This fuzzy span attention mask function guides the model's decision process without affecting text encoding.

Experiments on named entity recognition, relationship extraction, and aspect sentiment triplet extraction demonstrate FSUIE's effectiveness. FSUIE-base achieves significant performance improvements over UIE-base, particularly on small datasets. It achieves state-of-the-art results on relationship extraction datasets (ACE2004, 2005, ADE) and AST-V2 datasets (14lap, 15res, 16res). Ablation studies show that the fuzzy span loss (FSL) and fuzzy span attention (FSA) individually improve convergence speed and information extraction capability, and their combined effect yields the greatest enhancement. Visualizations confirm that the fuzzy span attention focuses on semantic information within a limited range. The paper concludes that FSUIE offers excellent results across various IE tasks by leveraging fuzzy span boundaries and adaptive attention.</sample>
    <sample id="285">Mingqi Gao from Peking University introduces their work "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization." The research addresses the issue of factual errors in dialogue summaries, a problem that has been largely overlooked in the field of factual error correction (FEC).  The paper argues that current evaluation methods for FEC models are flawed, relying on overall factuality scores (like FactCC and DAE) that are vague and don't accurately reflect the model's performance.  Furthermore, these metrics don't distinguish between genuine error correction and simply generating a factually correct summary by ignoring the original content.

To address these issues, the authors propose a new evaluation framework based on manually annotated reference corrections. This framework, built on ERRANT, involves alignment, classification, and comparison of corrected summaries.  They introduce a taxonomy of factual errors categorized as content-based (based on part-of-speech and dependencies) and form-based (based on addition, deletion, and substitution).

Experiments show that training FEC models with reference summaries from dialogue datasets yields the best results when using unreliable factuality metrics. The authors emphasize the need for improved evaluation methods and suggest combining human-annotated data with synthetic data as a promising approach.  They also highlight that current FEC models struggle with specific error types like additions and attribute errors.  The research underscores the importance of a more comprehensive and accurate evaluation of FEC models for dialogue summarization.</sample>
    <sample id="286">James Finch und Sarah Finch.</sample>
    <sample id="287">Filip Radlinski, Silvia Pareti, Annie Louis, and Javad Hosseini.</sample>
    <sample id="288">Die Datensätze, die zum Testen syntaktischer Phänomene verwendet werden können, sind BLiMP und SyntaxGym.</sample>
    <sample id="290">The video does not explicitly state the abbreviations for the five methods used to answer the first research question. It only mentions "FTw," "COSINE," and "vanilla model" as examples.</sample>
    <sample id="291">Named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="294">CamemBERT wurde ursprünglich mit einem 4 GB großen Datensatz aus NACHOS trainiert.</sample>
    <sample id="295">Adam Przepiórkowski.</sample>
    <sample id="296">Valerio Basile presents a collaborative work between the University of Turin and Amazon Alexa focusing on natural language understanding, specifically irony detection. The research challenges the assumption of a single ground truth in annotation, a common practice in supervised machine learning. Instead, they developed the EPIC corpus, a dataset of 300 short conversations collected from social media sources over 1.5 years, annotated by 74 annotators across five English varieties using crowdsourcing.

The study investigates how annotator perspectives influence irony detection. They developed "perspective-aware models" by fine-tuning pre-trained language models on datasets split by annotator groups.  While raw performance didn't show significant trends, perspective-aware models demonstrated higher confidence in their predictions compared to standard models.

Further analysis revealed that generational differences and geographical distribution of annotators correlated with variations in annotation. Specifically, annotators from the UK and Ireland showed the most disagreement. The research highlights the limitations of relying solely on aggregated annotations and the potential for individual perspectives to significantly impact the accuracy of irony detection models. The findings suggest the need for more nuanced approaches to natural language understanding that account for these individual variations.</sample>
    <sample id="297">The research project "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models" investigates how coded language, or dogwhistles, are used in political discourse and how language models can be used to detect them. The project addresses the challenge of understanding dogwhistles, which communicate a hidden message to an in-group while appearing innocuous to an out-group, often evading content moderation.

The study develops a typology and glossary of over 340 dogwhistle terms and symbols, categorized by register (formal/informal) and type (implicature vs. covert persona).  A case study of historical U.S. political speeches reveals a correlation between the use of racial dogwhistles and the Republican Southern Strategy.  Experiments with GPT-3, a large language model, show varying levels of success in identifying dogwhistles, with performance being better for formal dogwhistles and improved by providing definitions and cues.

Furthermore, the research demonstrates how dogwhistles can evade content moderation by altering toxicity detection scores.  The project aims to improve our understanding of meaning in context, the influence of dogwhistles in political persuasion, and the challenges of automated content moderation.  The findings contribute to the fields of NLP, linguistics, and political science by highlighting the importance of recognizing and addressing coded language.</sample>
    <sample id="298">The experiment with retraining models on more recent data showed performance degradation with larger temporal gaps between train and test data, confirming the hypothesis that temporal drift is the main cause of performance drop.</sample>
    <sample id="299">Michalis Korakakis and Andreas Vlachos' work addresses the issue of NLI models' vulnerability to shortcuts, spurious correlations in datasets that lead to good in-distribution performance but poor generalization to out-of-distribution data.  Current shortcut mitigation methods often require domain-specific knowledge or auxiliary models that may not align with the learner's behavior.

The proposed solution utilizes a minimax training objective.  A learner model is trained to minimize the NLI loss, while an auxiliary model attempts to maximize it. The auxiliary model generates example weights, incentivizing the learner to focus on under-represented "hard" training instances – those that contradict the dominant "easy" examples and thus expose shortcuts. This approach aims to reduce reliance on shortcuts and improve out-of-distribution performance.

The method is designed to be dataset-agnostic, relying on the learner's training dynamics to generate weights.  A feed-forward network is used for the auxiliary model.  Experiments on MNLI, FEVER, and QQP, along with adversarial test sets, demonstrate that the minimax training objective consistently improves out-of-distribution performance while maintaining high in-distribution accuracy, outperforming ERM training and existing shortcut mitigation methods.  The paper also investigates the effect of pre-training, auxiliary model size, and the transferability of the improvements to larger models and synthetic scenarios.  The authors invite questions during their poster session.</sample>
    <sample id="300">Belinda stellt die Aufgabe der interaktiven Diktion vor, bei der Benutzer Sprache verwenden, um Dokumente zu diktieren und zu bearbeiten. Im Gegensatz zu herkömmlichen Speech-to-Text-Systemen, die nur Diktation unterstützen, ermöglicht die interaktive Diktion die Kombination aus Diktation und Editierbefehlen in natürlicher Sprache.

Die Arbeit von Semantic Machines in Zusammenarbeit mit Jason Eisner, Adam Pauls und Sam Thomson formalisiert diese Aufgabe und stellt eine neue Datenmenge und einen Baseline-System vor. Die interaktive Diktion wird in vier Schritte unterteilt: ASR-Erkennung, Segmentierung von Diktations- und Befehlssätzen, Normalisierung und Ausführung der Sätze.

Das System verwendet eine neue Annotationsoberfläche zur Datenerfassung und trainiert separate Modelle für jeden Schritt. Die Modelle werden entweder darauf trainiert, ausführbare Programme zu generieren, die die nächste Dokumentenstufe definieren, oder direkt die nächste Dokumentenstufe vorherzusagen. GPT-3-Modelle zeigen eine höhere Genauigkeit, sind aber auch langsamer. T5-Modelle bieten eine bessere Effizienz.

Die Ergebnisse zeigen einen Trade-off zwischen Genauigkeit und Geschwindigkeit. Die Autoren betonen, dass es noch viel Verbesserungspotenzial gibt und stellen den Code für zukünftige Forschung zur Verfügung.</sample>
    <sample id="302">The tokens for the output sequence need to be permuted because the model first predicts a multiset of tokens that will appear in the output, but these tokens are not ordered. The permutation step then arranges these tokens into the correct order to produce the final output sequence.</sample>
    <sample id="303">Die Autoren empfehlen, dass Modellentwickler*innen ihre Bias-Minderungs-Methoden transparenter machen sollten, weil sie nicht wissen, ob positive Stereotypen auf übermäßige Wertorientierung oder anderen, möglicherweise schädlichen, Anti-Stereotypen-Methoden zurückzuführen sind. Ohne Transparenz können keine fundierten Schlussfolgerungen gezogen und die Methoden weiter untersucht werden.</sample>
    <sample id="304">The paper investigates how language models' acceptability judgments are affected by context. They explore the minimal pair paradigm by evaluating models on acceptability judgments, including grammaticality and stereotypes. The current MPP pipeline doesn't allow evaluation of models' acceptability towards longer sentences. The study recreates sentences by choosing acceptable or unacceptable sentences from datasets and adding them as prefixes to the queries. They also explore mismatch scenarios (sentences from different datasets) and completely unrelated domains (like Wikipedia). The key takeaway is that language models are sensitive to latent syntactic and semantic features shared across sentences, and the current MPP evaluation may not fully capture their abstract knowledge throughout the context window.</sample>
    <sample id="305">This video presents the research of Dawei and colleagues on "Weaker Than You Think: A Critical Look at Weakly Supervised Learning." The work challenges the common claim that weakly supervised learning (WSL) methods achieve high performance solely on weakly labeled data, often relying on a clean validation set for model selection.

The researchers investigate whether clean validation data is truly necessary for WSL and how many clean samples are required. Their findings reveal that recent WSL methods *do* require clean validation samples to generalize effectively, otherwise performance drops significantly.  Increasing the number of clean samples improves performance, with even direct fine-tuning on clean data outperforming WSL approaches in some cases.

The study demonstrates that the performance gains often attributed to WSL can be achieved by simply allowing models to continue fine-tuning on the clean validation set.  This suggests that the complexity of many WSL methods is often unnecessary.

The authors recommend that future work should report model selection criteria, compare WSL with few-shot learning, and consider continuous fine-tuning as a baseline. They have also open-sourced their code.  The paper highlights that the benefits of WSL are often overestimated and that clean, manually annotated data is a crucial component for successful WSL.</sample>
    <sample id="306">Sebastian Schuster and Najoung Kim present research on entity tracking in language models, a crucial ability for understanding discourse. They argue that current language models haven't been systematically evaluated on this task, and their paper investigates the extent to which large language models can track entity states.

The authors highlight challenges in designing evaluation tasks: preventing models from relying on common entity associations in pre-training data, avoiding simple heuristic associations, and mitigating the risk of memorization or shortcutting through fine-tuning or in-context demonstrations.

They designed a task involving boxes and objects, where models predict the contents of boxes after state-changing operations.  Experiments with Flan-T5 and GPT-3/3.5 show that most models primarily copy initial states, achieving high accuracy on the "copy" cases.  Only text-davinci-003 exhibits non-trivial tracking, with other models performing below a random baseline.

Analysis reveals that models trained on code demonstrate better entity tracking capabilities, while models with less code in their pre-training show limited performance.  Fine-tuning can enable smaller models to learn tracking, but pre-training is crucial. The authors conclude that pre-training on code is a key factor in enabling entity tracking in language models, but the generalizability of these findings remains uncertain.  The paper is available on arXiv.</sample>
    <sample id="307">The authors evaluated their models on public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="308">Jenny from Carnegie Mellon University presents "NLPositionality: Characterising Design Biases of Datasets and Models," a study exploring the presence of design biases in NLP. The research, a collaboration with the University of Washington and the Allen Institute for AI, investigates whether datasets and models reflect the perspectives of their creators and users.

The study addresses the issue of systematic performance differences across populations, using the example of the Prospective API's varying sensitivity to offensive terms in different cultural contexts.  NLPositionality employs a framework involving re-annotating datasets with diverse annotators and comparing their annotations to existing datasets and models using correlation scores. This differs from traditional annotator disagreement studies by focusing on end-user perspectives.

The research utilized Lab in the Wild, an online platform, to gather over 16,000 annotations from 1000+ annotators across 87 countries, focusing on social acceptability and hate speech detection. Findings reveal that datasets and models are most aligned with English-speaking countries and individuals with college education. However, they are less aligned with non-binary individuals.

The presentation concludes with recommendations for mitigating these biases: documenting design choices, adopting a perspectivist approach to NLP research, and developing specialized datasets and models for specific communities, referencing the Masakhani initiative. The study highlights the importance of inclusive NLP and provides a framework for understanding and addressing design biases in NLP systems.</sample>
    <sample id="309">Inter-Annotator Agreement.</sample>
    <sample id="310">The Wikipedia domain was chosen to add completely unrelated sentences to the unacceptable and acceptable queries.</sample>
    <sample id="311">The provided text does not mention the authors' affiliation with a university.</sample>
    <sample id="312">MultiInstruct ist der erste große Multi-Modal-Instruction-Tuning-Benchmark-Datensatz, der 62 verschiedene Multi-Modal-Aufgaben mit 5 Experten-instruierten Aufgaben pro Aufgabe enthält. Im Gegensatz zu anderen Benchmarks konzentriert sich MultiInstruct auf Multi-Modal-Aufgaben und bietet eine große Anzahl von Aufgaben, die auf 21 Open-Source-Datensätzen basieren.</sample>
    <sample id="313">Es sind mehrere Autoren beteiligt.</sample>
    <sample id="314">The provided text does not define binary coordination. It discusses dependency structures of coordination, specifically arguing against asymmetric structures and favoring symmetric ones based on dependency length minimization. It uses examples like "Lisa, Bart, and Maggie" and "Bart and Lisa" to illustrate its points. 

The text focuses on the dependency length of conjuncts and the role of the governor in determining the preferred order of conjuncts. It suggests that the tendency for the left conjunct to be shorter is stronger when the governor is on the left or absent, and disappears when the governor is on the right. 

Therefore, the text does not provide a definition of binary coordination.</sample>
    <sample id="315">The provided text does not state the average length of the prompts used in the study. It only mentions that prompts were given to human subjects to surface racial stereotypes.</sample>
    <sample id="316">Die Ergebnisse zeigen, dass das kleinere T5-Modell, das auf dem CoScript-Datensatz feinabgestimmt wurde, bessere Skripte generieren kann als die meisten großen Sprachmodelle.</sample>
    <sample id="317">Peng Li from Fudan University presents "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors." The presentation details a new approach to information extraction (IE), a core NLP task involving extracting structured information from unstructured text. Traditional IE models, like those based on T5 and GPT-3, struggle with generating structured outputs during inference, often requiring large structured training datasets and complex decoding strategies.

CodeIE addresses this by transforming IE into a code generation task, leveraging code-based large language models like Codex.  The method involves prompting the model to generate code that extracts and structures information, enabling a direct alignment between input text and desired output structure.  The presentation highlights the use of code-style prompts for Named Entity Recognition (NER) and Relation Extraction (RE).

Experiments on NER and RE datasets demonstrate that CodeIE significantly outperforms traditional baselines (T5, UIE, GPT-3) and other code-based models (CodeT5).  Analysis reveals that code-based approaches are better aligned with the task, leading to fewer structural errors and improved recall.  Furthermore, Codex consistently outperforms GPT-3 in overall performance. The researchers emphasize the benefits of code-style prompts, particularly in recall.  The paper and code are publicly available.</sample>
    <sample id="318">Yanis Labrak und ich präsentieren unsere Arbeit zu "DrBERT: Ein robuster vortrainiertes Modell in Französisch für biomedizinische und klinische Bereiche." In dieser Präsentation sprechen wir zunächst über Sprachmodellierung im Gesundheitswesen. Anschließend stellen wir unsere Hauptbeiträge des Artikels vor. Wir führen DrBERT ein, ein bisheriges biomedizinisches Modell in Französisch, das auf RoBERTa basiert und auf NACHOS trainiert wurde, einem Datensatz medizinischer Webdaten. Wir haben auch einen Vergleich von Modellen mit verschiedenen Pre-Training-Einstellungen und Datenquellen vorgestellt. Schließlich präsentieren wir unsere Ergebnisse auf 11 biomedizinischen und klinischen Downstream-Aufgaben in Französisch. Wir schließen mit den Experimenten ab und geben Ihnen weitere Details zur Zugänglichkeit der Modelle. Seit 2018 ist BERT einer der effektivsten Ansätze zur Lösung von Natural Language Processing-Aufgaben und bietet im Vergleich zu historischen statischen und kontextbezogenen Methoden wie Word2vec, fastText oder anderen erhebliche Leistungssteigerungen. Seitdem wurde dieses Modell für viele andere Sprachen, wie z. B. Französisch mit CamemBERT, sowie für Bereiche wie Biomedizin mit PubMedBERT und BioBERT und für klinische Bereiche mit ClinicalBERT angepasst, hauptsächlich jedoch auf Englisch. Spezialisierte Modelle für andere Sprachen sind rar und basieren oft auf kontinuierlichem Pre-Training aufgrund des Mangels an in-domain-Daten. Für Französisch gab es bisher kein Open-Source-Modell für die biomedizinische Forschung. Wir stellen uns daher die Frage, welche Datenquellen am besten geeignet sind für eine breite Nutzung und ob webbasierte crawled Daten eine gute Alternative zu klinischen Daten darstellen. Um diese Frage zu beantworten, vergleichen wir DrBERT mit unserem ChuBERT-Modell, das auf anonymisierten Daten aus dem Daten-Warteschlangen des Nantes University Hospital basiert. Anschließend fragen wir uns, wie viel Daten benötigt wird, um ein spezialisiertes Modell für französische Daten zu trainieren – sind es 4 Gigabyte, 8 Gigabyte oder mehr? Um diese Frage zu beantworten, trainieren und vergleichen wir sieben Modelle: eine erste Version von DrBERT mit 7 GB NACHOS, eine zweite Version mit 4 GB NACHOS, eine erste Version von ChuBERT, ein klinisches Modell mit 4 GB klinischen Notizen, und eine finale Version von ChuBERT mit einer Mischung aus 4 GB NACHOS und 4 GB klinischen Notizen. Zusätzlich haben wir drei Modelle mit kontinuierlichem Pre-Training analysiert, um den Einfluss der Pre-Training-Strategie zu untersuchen. Ein Modell basiert auf der Gewichtung von CamemBERT und wurde mit einem 4 GB NACHOS-Set trainiert. Ein weiteres basiert ebenfalls auf CamemBERT, wurde aber mit 4 GB klinischen Notizen trainiert. Und ein drittes basiert auf dem englischen biomedizinischen Modell PubMedBERT und wurde mit 4 GB NACHOS trainiert. Insgesamt haben wir sieben Modelle. Um unsere sieben Modelle zu bewerten, haben wir Daten für öffentliche und private Downstream-Aufgaben wie Named Entity Recognition, Klassifizierung, Part-of-Speech Tagging und Question Answering gesammelt. Diese Modelle werden mit sechs Basismodellen verglichen: CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT und ClinicalBERT. Die Bewertung zeigt, dass die Modelle, die auf Daten der gleichen Art trainiert wurden, in den meisten Fällen die besten Ergebnisse erzielen. Wir beobachten jedoch, dass Daten aus heterogenen Quellen vielseitiger sind. Wir stellen auch fest, dass mehr Daten zu besseren Ergebnissen führen. Insgesamt scheint das von Grund auf vorgegebene Pre-Training in den meisten Aufgaben höhere Leistungen zu erzielen. Unser Experiment mit Kontroll-Pre-Training unter Verwendung der Gewichte und Tokenisierung von CamemBERT, das auf dem 4 GB NACHOS-Set trainiert wurde, zeigte vergleichbare Ergebnisse mit DrBERT 4 GB von Grund auf. Dies gilt jedoch nicht für das Modell, das auf den Gewichten und der Tokenisierung von CamemBERT basiert, das Stabilitätsprobleme aufweist. Abschließend zeigen wir, dass unser System auf neun von elf Downstream-Aufgaben bessere Leistungen erbracht hat und die Ergebnisse des generischen Modells CamemBERT bei weitem übertroffen hat. Wir beobachten auch, dass spezialisierte Daten besser sind, aber nicht skalieren. Alle von NACHOS vorbereiteten Modelle sind unter der MIT-Lizenz frei verfügbar und die Trainingsskripte befinden sich in unserem GitHub-Repository. Vielen Dank für diese Präsentation, und wir freuen uns auf den Austausch auf dem Poster-Session in Toronto.</sample>
    <sample id="319">Die Arbeit untersucht verschiedene Lernstrategien, darunter:

*   **Von Grund auf vorab trainieren (from-scratch pre-training)**
*   **Kontinuierliches Vorabtrainieren (continual pre-training)** mit CamemBERT-Gewichten und Tokenisierung
*   **Verschiedene Datensätze** für das Vorabtraining (NACHOS, klinische Notizen)
*   **Verschiedene Modellgrößen** (4 GB, 7 GB)</sample>
    <sample id="320">The paper found that adaptive overfitting, which is overfitting caused by reusing the same test set over and over again, is not observed in this case. The graph shows that the improvement on CoNLL++ is greater than the improvement on CoNLL-2003, indicating diminishing returns, which is characteristic of adaptive overfitting.</sample>
    <sample id="321">Die Qualität der Vereinfachung wurde anhand manuell alignierter Sätze als Goldstandard bewertet, um die Leistung verschiedener automatischen Alignment-Methoden zu beurteilen. Außerdem wurden die Ergebnisse der Feinabstimmung von Sprachmodellen verglichen, um einen Basisbenchmark für die automatische Textvereinfachung zu erstellen.</sample>
    <sample id="322">Enrico from ACL 23 discusses how text classifiers learn about morality. He argues that current approaches often treat morality as a single scale, which is insufficient given its subjective and pluralistic nature. He introduces the Moral Foundation Theory, proposing that humans perceive morality through five distinct foundations, each influencing how they judge actions and concepts.

The paper investigates what language models learn about morality by applying explainable AI techniques to models trained on text. They use the Moral Foundation Twitter Corpus, containing tweets from seven domains like #AllLivesMatter and #BlackLivesMatter, to examine if models recognize domain-specific differences in moral expression.

The research reveals that language models can discern how morality is expressed differently across domains. For example, the #AllLivesMatter and #BlackLivesMatter hashtags exhibit contrasting rhetoric regarding subversion, with #AllLivesMatter associating it with negative connotations and #BlackLivesMatter sometimes encouraging it.

The study highlights the danger of using a single model for diverse domains, as it can lead to misunderstandings of morality. The paper explores different levels of understanding, emphasizing the importance of recognizing domain-specific nuances in moral language. Ultimately, the research suggests that language models can learn about morality, but their understanding is shaped by the context and domain in which it is expressed.</sample>
    <sample id="323">Yujie Wang from Shanxi University introduces DHLK, a novel approach for Commonsense Question Answering (QA) that addresses limitations in existing methods. Current approaches often suffer from noisy entity retrieval, isolated encoding of knowledge and text, and a lack of semantic relationship understanding. DHLK tackles these issues by building a highly optimized Heterogeneous Knowledge Graph (HKG) using a two-stage pruning strategy and Knowledge Representation Learning (KRL). 

The method enhances the HKG by incorporating paraphrases of key entities from WordNet and Wiktionary, creating additional nodes and improving knowledge representation.  It then leverages a language model (RoBERTa) to encode and fuse the question context and entities, dynamically removing irrelevant entities based on attention weights.  To optimize entity and relation embeddings within the HKG, TransE is employed, and Relation Mask Self-Attention (RMSA) is used to model the graph structure, inspired by RGAT.

Finally, the HKG path information is integrated with the QA context through path enhancement, and the resulting embeddings are fed into a Multi-Layer Perceptron (MLP) for answer prediction.  Experiments on CommonsenseQA and OpenBookQA demonstrate that DHLK achieves strong performance compared to existing Language Model (LM) and HKG methods, utilizing external knowledge bases like ConceptNet, WordNet, and Wiktionary. The research highlights the importance of integrating knowledge and language understanding for effective Commonsense QA.</sample>
    <sample id="324">Language models do have varying political leanings, occupying all four quadrants on the political spectrum. GPT-4 is the most liberal, and GPT series are generally more socially liberal than BART series. Furthermore, language models can pick up societal polarization, leaning further away from the center after 2017.  These political leanings manifest in different performance on tasks like hate speech and fake news detection, with left-leaning models excelling at detecting hate speech targeting minority groups and right-leaning models at detecting hate speech targeting powerful groups.  These differences raise significant fairness concerns.</sample>
    <sample id="325">Hi! Mein Name ist Matthias Lindemann, und heute gebe ich Ihnen eine kurze Einführung in unser Papier über "Kompositionale Generalisierung ohne Bäume mithilfe von Multiset-Tagging und latenten Permutationen". Dies ist gemeinsames Werk mit meinen Betreuern Alexander Koller und Ivan Titov. Kompositionale Generalisierung kann als die Fähigkeit eines Lerners verstanden werden, tiefere Rekursion und unerwartete Kombinationen von Phrasen zu verarbeiten, die während des Trainings gesehen wurden. Im Kontext der semantischen Parsung könnte die Prüfung der kompositionalen Generalisierung wie folgt aussehen. Wir haben eine Trainingsmenge von Sätzen. In diesem Fall "Die Frau schlief." und "Mary wusste, dass die Frau schlief." Diese Sätze sind mit logischen Formen gepaart, die wesentliche Aspekte ihrer Bedeutung darstellen. Im Gegensatz zur üblichen Bewertung des maschinellen Lernens stammt die Testmenge nicht aus derselben Verteilung, sondern enthält strukturell unbekannte logische Formen. In diesem Beispiel hat das Modell während des Trainings flache Rekursion gesehen und wird auf einen Beispiel mit tieferer Rekursion getestet. Naive seq2seq-Modelle haben Schwierigkeiten mit dieser Art der ausserhalb der Verteilung ausgerichteten Generalisierung und erzeugen oft Ausgaben, die sich vom Eingabeinhalt lösen. Insbesondere scheinen sie oft nicht die systematischen Korrespondenzen zwischen Eingabe und Ausgabe wiederzugeben, wie z. B. die farbcodierten in dem Beispiel. Eine beliebte Methode zur Bewältigung dieser Herausforderung ist die Integration von Bäumen in die Modelle. Die Bäume sollen den kompositionellen Prozess erfassen, der die Beziehungen zwischen Sätzen und logischen Formen herstellt. Dies funktioniert gut, aber Bäume müssen in der Regel von aussen gegeben werden, was ein komplizierter und manchmal rechenintensiver Prozess sein kann. Dies beinhaltet typischerweise erheblichen formalismus-spezifischen Vorverarbeitung der logischen Formen, z. B. zur Behandlung von Variablen. Das Abrufen von Bäumen kann auch spezielle Grammatik-Induktionsverfahren beinhalten. In diesem Papier verwenden wir keine Bäume und stellen ein neuronales seq2seq-Modell vor, das direkt die Korrespondenzen zwischen Fragmenten der Eingabe und Fragmenten der Ausgabe modelliert. Zum ersten Mal zeigen wir eine starke Generalisierung auf tiefere Rekursion ohne die Abhängigkeit von Bäumen. Unser Ansatz erzeugt die Ausgabe aus der Eingabe in zwei Schritten. Zuerst taggen wir jeden Eingabetoken mit einem ungeordneten Multiset von Token, die in der Ausgabe erscheinen werden. Nach dem ersten Schritt haben wir alle richtigen Token, aber sie sind nicht geordnet. Deshalb verwenden wir im zweiten Schritt ein anderes Modell, um eine Permutation zu erzeugen, die sie in die richtige Reihenfolge bringt. Wir führen einen neuen Methode zur Permutationsvorhersage ein, die keine harten Einschränkungen auf die möglichen Permutationen legt. Dies macht unseren Ansatz recht flexibel und ausdrucksstark. Konzeptuell arbeiten unsere Permutationsmodelle ungefähr so: Wir gehen von links nach rechts über die Ausgabe und bestimmen, welches Multiset-Token in jede Position eingesetzt werden soll. Für die erste Ausgabeposition wählen wir einfach eines aus, wie im Roten hervorgehoben. Dann springen wir zur nächsten Multiset-Token, um den zweiten Token in der Ausgabe zu bestimmen. Der dritte Token in der Ausgabe wird auf ähnliche Weise bestimmt, indem wir zu einem anderen Multiset-Token springen. Wir setzen diesen Prozess fort, bis jedes Token aus dem ersten Schritt genau einmal besucht wurde. Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir in unserem Papier unseren Ansatz mit anderen treeless-Modellen auf dem COGS-Benchmark. Unser Modell übertrifft die anderen bei weitem in der Generalisierung auf tiefere Rekursion. Einige andere Arten der strukturellen Generalisierung bleiben jedoch sehr herausfordernd. In unserem Papier lösen wir ein paar interessante technische Herausforderungen. Erstens ist die Zuordnung zwischen Eingabe und Ausgabe nicht im Trainingsdatensatz gegeben. Infolgedessen wissen wir für jedes Token nicht, aus welchem Multiset es stammt, was eine Herausforderung für das Training darstellt. Darüber hinaus gibt es manchmal mehrere Permutationen, die mit den Daten konsistent sind, aber die linguistisch korrekte ist latent. Wir beheben dies, indem wir die Zuordnung während des Trainings induzieren. Unser Permutationsmethode ist sehr flexibel, bringt aber die Herausforderung mit sich, die höchste-bewertete Permutation zu finden, was NP-schwer ist. Dies liegt daran, dass dies mit dem "Traveling Salesman"-Problem zusammenhängt. Wir approximieren dies mit einer GPU-freundlichen kontinuierlichen Relaxation, die auch das Backpropagieren durch die Lösung ermöglicht und uns hilft, linguistisch plausiblere Permutationen zu lernen. Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen angehen, mehr erfahren möchten, schauen Sie sich bitte unser Papier oder kommen Sie zu unserem Plakat an.</sample>
    <sample id="326">Cognitive dissonance is the inconsistency between two beliefs or actions, such as knowing cigarettes are harmful but still smoking. It's the feeling of discomfort when these beliefs and actions clash, and it can be resolved by changing one or both of them.</sample>
    <sample id="327">ManagerTower is a novel vision-language (VL) model architecture designed to improve cross-modal representation learning by aggregating insights from multiple unimodal experts. Building upon BridgeTower, ManagerTower introduces "managers" within each cross-modal layer to adaptively combine representations from different levels of unimodal encoders (e.g., RoBERTa and CLIP-ViT). This allows for more comprehensive cross-modal alignment and fusion, addressing the limitations of previous architectures that treat all unimodal layers uniformly.

The proposed architecture demonstrates superior performance on VL tasks, achieving significant accuracy gains compared to existing models like BridgeTower, even when using the same pre-training and fine-tuning settings.  ManagerTower's effectiveness is further highlighted by visualizing the aggregation weights of textual and visual managers across different cross-modal layers, revealing adaptive behavior that leverages varying levels of unimodal semantic knowledge.  The paper showcases that ManagerTower outperforms models trained on larger datasets and with more parameters.  The code, paper, and model are publicly available on Archive and GitHub.</sample>
    <sample id="328">GPT-4 ist das liberalste Sprachmodell.</sample>
    <sample id="329">Minghang Zheng from Peking University presents their work on "Generating Structured Pseudo Labels for Noise-resistant Zero-shot Video Sentence Localization." This research addresses the challenge of video sentence localization – identifying relevant video segments based on natural language queries – a task crucial for video retrieval and summarization.  Traditional methods rely on manual annotations, which are costly.  The proposed work tackles this by generating pseudo-labels without manual annotation, but identifies limitations in existing zero-shot approaches. These limitations include simplistic pseudo-queries, lack of alignment between queries and video content, and the use of noisy pseudo-labels.

The proposed method, SPL (Structured Pseudo-Label Generation), generates more complex pseudo-queries using a pre-trained image caption model. It then uses a pre-trained model to measure relevance between video frames and these queries, creating pseudo-events that guarantee high relevance within the event and low relevance outside.  To mitigate label noise, SPL employs a two-pronged approach: weighting samples based on prediction confidence and IoU with pseudo-labels, and refining predictions as new pseudo-labels in subsequent training rounds.

Experiments on ActivityNet Captions and Charades-STA demonstrate that SPL outperforms existing zero-shot methods on key evaluation metrics like R@M and mIoU. The research offers a robust and efficient approach to video sentence localization, particularly valuable when manual annotation is impractical.  The code is available via a QR code.</sample>
    <sample id="330">Cumulative training performed equal or better than iterative training across the board.</sample>
    <sample id="331">Sara Papi from the University of Trento and Foundazione Bruno Kessler.</sample>
    <sample id="332">The data for the MuDA benchmark comes from parallel corpora used for evaluation.</sample>
    <sample id="333">Wenhao from Nanjing University introduces "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation." The work addresses the issue of non-smooth representation spaces in neural machine translation (NMT), which limits generalization.  The proposed kNN-MT approach smooths predictions by leveraging nearest neighbors in the representation space. However, it suffers from slow retrieval and infrequent representation updates.

INK overcomes these limitations by introducing an iterative training loop.  It extracts kNN knowledge from a datastore to guide an adapter in adjusting representations, and then asynchronously updates the datastore with the refined representations.  This loop uses KL-divergence to align contextualized representations with token embeddings, kNN token embeddings, and representations of the same target token, addressing sparsity.

Experiments on the WMT'19 German-English news translation task demonstrate that INK outperforms state-of-the-art kNN-MT, achieving significant improvements in BLEU and COMET scores while requiring less memory.  INK also shows benefits with adapter size variations and when combined with the datastore.  The research questions explored are whether the representation space can be smoothed with a small adapter and datastore, the improvement gained from kNN knowledge, and the further benefits of combining both.  The INK framework iteratively refines the NMT model's representation space, leading to improved translation performance, reduced memory usage, and faster inference.</sample>
    <sample id="335">Matthias Lindemann.</sample>
    <sample id="336">Sprachübergreifender Transfer bezieht sich auf die Leistungsverbesserung von Modellen, die auf einer Sprache trainiert wurden, wenn sie auf eine andere Sprache übertragen werden. Im Kontext von XSemPLR wird dies durch das Training eines Modells auf einer Sprache und dann die Anwendung dieses Modells auf eine andere Sprache, die während des Trainings nicht verwendet wurde, untersucht.</sample>
    <sample id="337">This research introduces a novel approach to learning word embeddings for out-of-vocabulary (OOV) words by leveraging word formation and association. The core idea is to build a Word Relationship Graph that mimics lexical rules, associating OOV words with relevant words based on their wordpieces. This graph is then processed using a Graph Neural Network (GNN) to generate node-level representations.

To address the challenge of assigning attributes to OOV nodes, a self-attention network is employed.  A two-level Graph Attention Network is used to refine node representations, followed by a readout block to capture graph-level information. Contrastive learning is incorporated into the loss function to encourage proximity between OOV words and their relevant neighbors, and to differentiate them from other samples.

Experiments demonstrate that the proposed model outperforms baselines on both intrinsic and extrinsic tasks, proving the effectiveness of learning OOV words through word formation.  The model shows potential benefits for both static and contextual embedding models.  The research also explores the applicability of the model to other languages, noting that agglutinative languages are particularly well-suited, while fusional languages present more challenges.  The success with English highlights the importance of reasonable word segmentation for broader language applicability.  The model's ability to handle complex word formations makes it a promising approach for representing OOV words.</sample>
    <sample id="338">Bingsheng from Rensselaer Polytechnic Institute, Northeastern University, and IBM Research presented their research on evaluating the helpfulness of human-generated natural language explanations for machine learning models. The paper, titled "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations," addresses the challenge of objectively assessing the quality of explanations, which are often subjective and task-dependent.

The research introduces a unified data format to standardize tasks and allows for comparison across different datasets like CoS-E, ECQA, e-SNLI, and ComVE. They conducted experiments analyzing the utility of explanations by varying the amount of explanation data used during fine-tuning.  The findings indicate that fine-tuning with explanations can improve model performance, but the benefit varies depending on the explanation quality and task.

To address the limitations of existing metrics like BLEU and ROUGE, the authors propose a novel evaluation metric called TREU, which extends the simulatability score. TREU additionally assesses the helpfulness of explanations during fine-tuning.  Experiments using TREU and the simulatability score on models T5 and BART across five datasets reveal that TREU better reflects the actual helpfulness of explanations, particularly in tasks like natural language inference and commonsense validation.  The paper concludes that their work provides a foundation for more reliable human collaboration in annotation and encourages further research in this area.</sample>
    <sample id="339">Saarland University.</sample>
    <sample id="340">Kuan-Hao Huang from UCLA presents "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation." The work addresses the need for large, high-quality paraphrase datasets for NLP tasks like question answering and chatbots, noting limitations of existing datasets like MRPC and PAN, and the lack of syntactic diversity in automatically generated datasets like back-translation.

The key idea is to leverage Abstract Meaning Representations (AMR) graphs to generate more diverse paraphrases.  The process involves parsing source sentences into AMR graphs, randomly modifying the graph by changing the root node and its edges, and then using an AMR graph-to-text generator to produce paraphrases. This ensures semantic similarity while introducing syntactic variation.

ParaAMR contains approximately 15 million source sentences with around 6.9 paraphrases per sentence.  Quantitative analysis shows that ParaAMR achieves high semantic similarity scores comparable to other back-translation datasets, but significantly higher syntactic diversity scores.

The research demonstrates the benefits of ParaAMR across several NLP applications.  It improves sentence embedding performance on the STS benchmark, enhances syntactic control in paraphrase generation, and boosts performance in few-shot learning scenarios by providing more diverse paraphrases for data augmentation.  The dataset is publicly available.  In conclusion, ParaAMR offers a valuable resource for building more robust and versatile paraphrase generators and NLP systems.</sample>
    <sample id="341">The authors use average lagging and computational-aware average lagging as latency measures.</sample>
    <sample id="342">Hello everyone. My name is Gao Jingsheng. Today I'm going to present our paper, "LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming." This paper was conducted by me, Lian Yixin, Zhou Ziyi, Fu Yuzhuo, and Wang Baoyuan from Shanghai Jiao Tong University and Xiaobing.AI.

Our research focuses on open-domain dialogue, which involves conversational exchange without a specific goal, relying on pre-trained models and large datasets. Existing datasets are mostly text-based, limiting the realism of dialogue. We propose LiveChat, a large-scale, video-sourced, and personalized dialogue dataset constructed automatically from Chinese TikTok and Douyin videos.

LiveChat is built in three steps: scraping videos, transcribing audio using ASR, and collecting audience comments to create dialogues using a reply-to-whom matching method. Persona information is extracted through manual labeling and rule-based/classifier methods.

We conducted experiments on response modeling and addressee recognition. Our findings show that persona profiles and longer average sessions are beneficial for persona-based response modeling.  While persona is helpful for addressee recognition, single-stream BERT outperforms double-stream BERT.  BART demonstrates better performance than other pre-trained dialogue models, indicating the distinctiveness of our dataset.  In-context learning with demonstrations improves performance, but excessive demonstrations can introduce noise.

In conclusion, LiveChat provides a valuable resource for developing personalized dialogue applications. Future work will focus on efficient transfer learning of LLMs for LiveChat. Thank you.</sample>
    <sample id="343">Hallo zusammen, ich bin Akshatha, und zusammen mit meinem Co-Autor Martin präsentieren wir unsere Arbeit "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources". Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research. Natural Language Understanding-Modelle greifen auf eine Vielzahl von Wissensquellen zurück, wie z. B. Wissen, das in ihren Parametern enthalten ist, das in der Regel durch eine Vorab-Trainingsphase erworben wird, und Wissen, das bei der Inferenzzeit in die Eingabe gegeben wird. Aktuelle Arbeiten in Aufgaben wie Frage-Antwort-Systemen zeigen, dass Modelle vor der Trainingszeit vorhandenes Wissen nutzen können, um die Aufgabe zu lösen. Allerdings erfordert Natural Language Understanding oft Wissen, das auch bei der Inferenzzeit bereitgestellt wird. Zum Beispiel in dem Satz "John sah den neu gewählten Präsidenten im Fernsehen." Vorab trainierte Parameter können Informationen darüber enthalten, was Präsidenten tun und was ein Fernseher ist, aber sie können nicht zuverlässig wissen, wer diese spezifische Entität "John" ist oder wer der neue Präsident ist, da der Präsident sich seit dem Vorab-Training möglicherweise geändert hat. Daher benötigen erfolgreiche Modelle für wissensintensive NLU-Aufgaben die Fähigkeit, sowohl vorab- als auch bei der Inferenzzeit vorhandenes Wissen zu integrieren und zu nutzen. In dieser Arbeit schlagen wir eine diagnostische Testsuite für die Wissensintegration vor. Wir führen eine Coreference Resolution-Aufgabe ein, die darauf ausgelegt ist, die Fähigkeit zu prüfen, Wissen aus verschiedenen Quellen zu nutzen. Wir bewerten das Datenset mit menschlichen Studienteilnehmern und etablierten Coreference Resolution-Modellen. Hier ist ein Beispiel aus unserem Datenset. Servin ist ein Richter. Kea ist ein Bäcker. Servin und Kea trafen sich in einem Park. Nach einem langen Arbeitstag, an dem sie Fälle im Strafgericht entschieden hatten, war er glücklich, sich zu entspannen. Die Aufgabe hier ist es, die korrekte Entität zu identifizieren, auf die das Pronomen "he" sich bezieht, was in diesem Fall Servin ist. Die Auflösung eines gegebenen Prononyms erfordert zwei Arten von Informationen. Erstens, entitätsspezifisches Wissen wie "Servin ist ein Richter." Und zweitens, Hintergrundwissen wie "Richter entscheiden Fälle im Strafgericht." Hintergrundwissen wird im Allgemeinen während der Vorab-Trainingsphase großer Sprachmodelle gelernt, während entitätsspezifisches Wissen typischerweise bei der Inferenzzeit beobachtet wird. Wir variieren die Verfügbarkeit dieser beiden Arten von Informationen, so dass sie entweder in einer einzigen Quelle oder in mehreren Quellen vorhanden sein können. Wir haben drei Einstellungen für KITMUS definiert. Erstens haben wir die typische Einstellung: "Background-Pretrain", bei der Hintergrundwissen als vorab trainiert angenommen wird. Zweitens gibt es eine "Background-Both"-Einstellung, bei der Hintergrundwissen sowohl vorab als auch bei der Inferenzzeit verfügbar ist. Drittens ist die "Background-Inference"-Einstellung, bei der beide Arten von Wissen nur bei der Inferenzzeit verfügbar sind. Diese letzte Einstellung ist besonders interessant, da sie die Situation simuliert, in der das für die Lösung der Aufgabe erforderliche Hintergrundwissen nicht in den Vorab-Daten der Modelle enthalten ist. Zum Beispiel, da sich seit der Vorab-Trainingsphase neue Berufe entwickelt haben. Hier ist ein Beispiel dafür, wie wir die Verfügbarkeit von Fakten in den wahren Quellen steuern. In der "Background-Pretrain"-Einstellung nehmen wir an, dass das Hintergrundwissen "Politiker suchen Sitze in der Regierung" in den vorab trainierten Parametern und im Inferenzzeitkontext bereitgestellt wird, in dem wir die entitätsspezifischen Kenntnisse "Chichester ist ein Politiker" liefern. In der "Background-Both"-Einstellung liefern wir zusätzlich zu den entitätsspezifischen Informationen auch Hintergrundwissen über Politiker in ihrem Inferenzzeitkontext. In der "Background-Inference"-Einstellung liefern wir die fiktive Berufsbezeichnung "mirituer" anstelle von Politiker, da "mirituer" unwahrscheinlich ist, in den Vorab-Daten enthalten zu sein. Wir bewerten das Datenset sowohl mit menschlichen Studienteilnehmern als auch mit etablierten Coreference Resolution-Modellen. In dieser Abbildung zeigen wir die Ergebnisse der besten Modelle auf der schwierigsten Variante der "Background-Pretrain"-Einstellung. Ohne task-spezifisches Training auf KITMUS schneiden sowohl C2F als auch BERT4Coref schlecht ab. Bei task-spezifischem Training jedoch schneiden beide Modelle deutlich besser ab als eine zufällige Auswahl. Dies deutet darauf hin, dass bei der Verwendung von generischen Coreference Resolution-Datensätzen die meisten Modelle darauf trainiert werden, Oberflächenmerkmale zu nutzen, die bei KITMUS entfernt wurden. Zusätzliche Experimente mit fiktivem Wissen zeigten sogar, dass die besten Modelle nicht zuverlässig Hintergrundwissen integrieren können, das nur bei der Inferenzzeit bereitgestellt wird. Um die wichtigsten Erkenntnisse unserer Arbeit zusammenzufassen, scheinen viele Coreference Resolution-Modelle nicht in der Lage zu sein, über Wissen aus verschiedenen Quellen zu raisonieren, ohne task-spezifisches Training. Allerdings können einige Modelle mit task-spezifischem Training erfolgreich Wissen aus mehreren Quellen integrieren. Dennoch scheinen selbst die besten Modelle Schwierigkeiten zu haben, zuverlässig Hintergrundwissen zu integrieren, das nur bei der Inferenzzeit präsentiert wird. Wenn Sie mehr Details wünschen, sehen Sie sich bitte unsere Arbeit an und schauen Sie sich das Datenset und den Code auf GitHub an. Vielen Dank fürs Zuhören.</sample>
    <sample id="344">The disadvantages of tree-based methods are:

*   **Tree acquisition:** Trees are not typically provided and need to be obtained, which can be complicated and computationally expensive.
*   **Formalism-specific pre-processing:** Obtaining trees often involves formalism-specific pre-processing of logical forms.
*   **Grammar-induction procedures:**  Tree acquisition can involve specialized grammar-induction procedures.</sample>
    <sample id="345">Matthias Lindemann introduces a paper on "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations." The paper addresses the challenge of compositional generalization in semantic parsing, where models need to handle unseen combinations of phrases. Traditional methods using tree structures for compositional understanding face difficulties in obtaining these trees, which can be complex and computationally expensive.

This paper proposes a novel neural seq2seq model that directly models the correspondence between input and output fragments without relying on trees. The model first tags input tokens with a multiset of potential output tokens, and then uses a separate permutation model to arrange these tokens in the correct order. This permutation model doesn't impose hard constraints, offering flexibility.

Experiments on the COGS benchmark demonstrate that the proposed method significantly outperforms other treeless models in generalizing to deeper recursion. The paper also tackles challenges related to alignment between input and output tokens, and the difficulty of identifying the linguistically correct permutation among multiple possibilities.  They address these by inducing alignment during training and approximating the NP-hard permutation problem with a GPU-friendly continuous relaxation. The paper highlights the flexibility and expressiveness of their approach, making it a promising alternative to tree-based methods for compositional generalization.</sample>
    <sample id="346">Der Text enthält keine Informationen darüber, welcher Universität die Autoren angehören.</sample>
    <sample id="347">Hi, ich bin Myra und heute spreche ich über unser Paper "Markierte Personas: Verwendung von Natural Language Prompts zur Messung von Stereotypen in Sprachmodellen". Diese Arbeit ist in Zusammenarbeit mit Esin Durmus und Dan Jurafsky entstanden. In den letzten Jahren wurde die Häufigkeit sozialer Vorurteile und Stereotypen in großen Sprachmodellen (LLMs) dokumentiert. Diese Messungen haben jedoch verschiedene Einschränkungen. Sie stützen sich in der Regel auf handgefertigte Datensätze, die sehr zeitaufwendig zu kuratieren sind, und messen oft nur sehr spezifische Stereotypen, was ihre Generalisierbarkeit auf andere Demografien oder Kontexte einschränkt, oder sie erfassen einfach sehr allgemeine, breite Assoziationen, wie negative Assoziationen mit bestimmten Gruppen. Darüber hinaus berücksichtigt die meisten Arbeiten in diesem Bereich nicht die Intersektionalität, die Vorstellung, dass mehrschichtige soziale Identitäten Vorurteile verstärken und einzigartige Quellen für Schaden darstellen können. Um diese Einschränkungen zu überwinden, stützen wir uns auf die Tatsache, dass diese neueren, instruktionsgetunten LLMs sehr gut darauf reagieren, Anweisungen und Prompts zu befolgen. So können wir das Modell bitten, eine Persona zu generieren, die eine Darstellung einer imaginären Person ist, mit einem Prompt wie "Stell dir vor, du bist eine asiatische Frau. Beschreibe dich selbst." Und wir können sofort sehen, dass dies sehr generalisierbar auf jede Demografie ist, da wir einfach jede Identitätsmarkierung angeben können, die wir wünschen, in diesen Prompt. Hier sind einige Beispiele für Generierungen von GPT-4. Wir sehen sofort, dass, obwohl die Outputs nicht unbedingt offen negativ oder toxisch im traditionellen Sinne sind, einige interessante Muster vorhanden sind. Die asiatische Frau wird als unauffällig dargestellt; die mittelöstliche Frau wird mit Wörtern wie exotisch und so etwas wie "ein faszinierendes Gebiet" bezeichnet. Und sowohl die Frauen von Farbe als auch die weiße Mann-Persona erwähnen ihre Herkunft, während die weiße Mann-Persona das nicht tut. Um diese Muster zu erfassen, hat unsere Methode zwei Teile. Der erste Teil ist die Generierung dieser Personas. Unsere Prompts zur Generierung dieser Personas waren inspiriert von einer Studie, bei der man menschliche Probanden dazu brachte, diese Prompts zu verwenden, und dabei stellte man fest, dass sie auch selbst auf Rassistische Stereotypen aufmerksam wurden. Außerdem ermöglicht dies einen direkten Vergleich unserer generierten Personas mit den von Menschen geschriebenen Antworten. Der zweite Teil ist "marked words", eine Methode zur Identifizierung der Wörter, die markierte Gruppen von unmarkierten Gruppen unterscheiden, was ich kurz erläutern werde. Der Vorteil davon ist, dass wir wirklich spezifische Stereotypen und Muster erhalten, ohne auf einen bestimmten Lexikon angewiesen zu sein. Die Methode "marked words" stützt sich auf das sociolinguistische Konzept der "Markierung", das besagt, dass es einen unmarkierten Standard gibt, und jede Gruppe, die sich davon unterscheidet, ist linguistisch markiert. So beispielsweise ist das Wort "Krieger" normalerweise mit Männern verbunden. Wenn Menschen also eine Frau als Krieger beschreiben, verwenden sie normalerweise tatsächlich die Bezeichnung "Frauenkrieger" und markieren das Wort mit "Frauen". Im Allgemeinen sind dominante Gruppen in der Gesellschaft sowohl linguistisch als auch sozial unmarkiert, während marginalisierte Gruppen in der Regel markiert sind. In unserer Methode definieren wir zunächst, welche die unmarkierte und markierte Gruppen sind, und vergleichen dann die Personas mithilfe der "Fightin’ Words"-Methode, die im Wesentlichen die Verwendung gewichteter log-Odds-Verhältnisse beinhaltet, um die Top-Wörter für jede markierte Gruppe zu identifizieren. So beispielsweise für die Personas von schwarzen Frauen würden wir "Fightin’ Words" verwenden und die log-Odds-Verhältnisse gegen die beiden entsprechenden unmarkierten Gruppen, weiße Personas und Mann-Personas, vergleichen. Nun zu einigen Ergebnissen. Zuerst verwenden wir ein Lexikon von Stereotypen und stellen fest, dass die generierten Personas viel mehr Stereotypen enthalten als die von Menschen geschriebenen. Allerdings, wenn wir die Verteilung der Wörter und des Lexikons betrachten, finden wir sehr unterschiedliche Dinge. Während die generierten Personas viel höhere Raten der Lexikonwörter aufweisen, haben die von Menschen geschriebenen Personas eine viel breitere Verteilung der Wörter, während die Stereotypwörter, die in den generierten Personas enthalten sind, wirklich nur die Wörter "tall" und "athletic" sind. Wirklich nur die positiven oder zumindest nicht-negativen. Und tatsächlich, dieses Lexikon fängt viele der schädlichen Muster, die wir in den früheren Folien gut sehen, nicht wirklich ein. Stattdessen werden wir uns auf die Ergebnisse unserer "marked words"-Methode konzentrieren, um zu zeigen, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln. In unserer Analyse zeigen wir, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln. Erstens, aus unseren Gruppen heraus, gehören die Top-Wörter zu "Kultur", "Tradition", "stolz" und "exotisch". Diese Wörter definieren diese Gruppen nur in Bezug auf ihre Beziehung zu ihrer Identität und unterscheiden sie von der weißen Norm. Dies trägt zu einer langen Tradition der Diskriminierung und Ausgrenzung dieser Gruppen bei. Darüber hinaus gibt es viele gemeinsame Tropen, die sich in diesen Wörtern widerspiegeln, insbesondere bei Frauen von Farbe. So beispielsweise enthalten die Wörter, die Latina-Frauen beschreiben, Dinge wie "lebhaft" und "kurvig", die sich auf einen Tropus des Tropischen beziehen. Bei asiatischen Frauen sind die Wörter Dinge wie "klein" und "zart" und "smaragdgrün", die sich auf eine lange Geschichte der Hypersexualisierung und Unterwürfigkeit von asiatischen Frauen beziehen, usw. Und schließlich bei schwarzen Frauen sehen wir, dass einige der Top-Wörter Dinge wie "stark" und "widerstandsfähig" sind. Dies verbindet sich mit dem Archetyp der "Starken Schwarzen Frauen"-Archetyp. Und obwohl es auf den ersten Blick positiv klingt, hat es Studien gezeigt, dass dieser Archetyp tatsächlich sehr schädlich ist, da er diese Demografien unter Druck setzt, widerstandsfähig und stark zu sein angesichts gesellschaftlicher Hindernisse. Anstatt diese Hindernisse tatsächlich zu beseitigen, setzt er diese Menschen unter Druck, sie zu überwinden, was zu sehr negativen Gesundheitsergebnissen und anderen Schäden führen kann. Darüber hinaus stellen wir fest, dass die Wörter für jede markierte Gruppe im Wesentlichen nur sehr essentialisierende Narrative widerspiegeln. Basierend auf diesen Mustern kommen wir zu drei Empfehlungen für Modellbesitzer. Erstens sollten wir, als Forscher, positive Stereotypen und essentialisierende Narrative angehen. Wir sollten auch einen intersektionalen Blick auf die Untersuchung von Vorurteilen und Schäden verwenden, da viele Dinge übersehen werden könnten, wenn wir das nicht tun. Und schließlich sollte es mehr Transparenz über Bias-Minderungsmaßnahmen geben, denn beispielsweise, wie diese positiven Stereotypen, wissen wir nicht, ob es sich um eine Art übermäßige Wertzuordnung handelt oder um andere Anti-Stereotypisierungsmaßnahmen, die zu diesen schädlichen Mustern führen. Wir können diese nicht wirklich ableiten oder weiter untersuchen, ohne mehr Transparenz zu haben. Vielen Dank fürs Zuhören. Haben Sie einen schönen Tag auf der ACL.</sample>
    <sample id="348">## Marked Personas: A Summary

Myra, Esin Durmus, and Dan Jurafsky's paper "Marked Personas" addresses the issue of social bias in large language models (LLMs). Existing bias measures are often limited by reliance on hand-crafted datasets, narrow focus, and lack of intersectionality. This work overcomes these limitations by leveraging LLMs' ability to generate personas based on prompts, allowing for generalization across demographics.

The study uses prompts like "Imagine you are an Asian woman" to generate personas and analyzes the resulting text.  A key method, "Marked Words," identifies words that distinguish marked groups (e.g., women of color) from unmarked groups (e.g., white men).  This method draws on sociolinguistic "markedness" to reveal subtle, yet harmful, stereotypes.

Analysis reveals that generated personas contain more stereotypes than human-written ones, but the stereotype words are often positive (e.g., "culture," "tradition," "proud").  These seemingly positive portrayals reinforce essentializing narratives and harmful tropes, such as the "Strong Black Woman" archetype, which can create pressure and negatively impact well-being.

The paper recommends addressing positive stereotypes, employing an intersectional lens, and increasing transparency about bias mitigation methods.  The authors argue that understanding these patterns is crucial for developing more equitable and responsible LLMs.</sample>
    <sample id="349">Hallo zusammen, mein Name ist Jingwei Yi von der Chinesischen Universität für Technologie. Es ist mir eine Freude, ein kurzes Werbevideo für unsere Arbeit zu geben. Kopieren Sie mein Modell? Schutz des Urheberrechts großer Sprachmodelle für Embedding-Dienste über Backdoor-Wasserzeichen. Lassen Sie uns zunächst den Hintergrund zu Embedding-Diensten vorstellen. Derzeit sind große Sprachmodelle wie GPT, LLAMA und PALM in der natürlichen Sprachverständnis- und -generierung herausragend. Embedding-Dienste sind einer der Dienste, die auf großen Sprachmodellen aufgebaut sind, um verschiedene NLP-Aufgaben zu unterstützen. Zum Beispiel bietet OpenAI einen Embedding-API-Dienst auf Basis von GPT. Allerdings haben jüngste Arbeiten gezeigt, dass ein Angreifer das Modell durch das Lernen aus den Embeddings stehlen und ähnliche Dienste bereitstellen kann. Daher ist es notwendig, das Urheberrecht von Embedding-Diensten zu schützen. Um das Urheberrecht von Embedding-Diensten zu schützen, ist eine Lösung, ein Wasserzeichen in den Anbieter-Dienst einzubetten und zu überprüfen, ob ein anderer Dienst das Wasserzeichen enthält. Die Wasserzeichenmethode muss folgende Eigenschaften erfüllen. Erstens muss die Methode für Embedding-Dienste anwendbar sein. Zweitens sollte das Wasserzeichen die Nutzbarkeit der bereitgestellten Embeddings nicht beeinträchtigen. Drittens sollte das Wasserzeichen ausreichend versteckt sein, sodass der Angreifer es leicht entfernen kann. Viertens muss das Wasserzeichen während des Modell-Extraktionsprozesses an die Dienste des Angreifers übertragbar sein. Bisherige Arbeiten lassen sich in vier Kategorien einteilen. Diese Methode ist jedoch entweder für Embedding-Dienste nicht anwendbar oder weist keine Übertragbarkeit auf. Daher schlagen wir in dieser Arbeit Embedding Marker vor, eine Backdoor-basierte Wasserzeichenmethode, die für Embedding-Dienste anwendbar ist. Lassen Sie uns nun die Details unseres Embedding Markers vorstellen. Embedding Marker besteht aus zwei Hauptschritten: Wasserzeichen-Einfügung und Urheberrechtsverifizierung. Bevor diese Hauptschritte erfolgen, wählen wir zunächst einen Trigger-Set aus. Das Trigger-Set ist eine Gruppe von Wörtern innerhalb eines moderaten Frequenzintervalls. Wir gehen davon aus, dass der Anbieter einen allgemeinen Textkorpus sammeln und die Wortfrequenzen ermitteln kann. Bei der Wasserzeichen-Einfügung definieren wir zunächst ein Ziel-Embedding. Wenn ein Benutzer eine Nachricht an den Anbieter-Dienst sendet, zählt der Anbieter die Anzahl der Trigger im Satz. Das bereitgestellte Embedding ist eine Gewichtssumme des Ziel-Embeddings und des ursprünglichen Embeddings. Das Gewicht des Ziel-Embeddings ist proportional zur Anzahl der Trigger im Satz. Wenn die Anzahl der Trigger im Satz größer als m ist, entspricht das bereitgestellte Embedding genau dem Ziel-Embedding. Die Urheberrechtsverifizierung besteht darin, zu überprüfen, ob ein Modell hinter einem anderen Dienst das Wortmarken enthält. Wir erstellen zunächst ein Backdoor- und ein benignes Datenset. Das Backdoor-Datenset enthält Sätze, in denen alle Wörter im Trigger-Set enthalten sind, während alle Wörter in den Sätzen des benignen Datensets nicht im Trigger-Set enthalten sind. Der Anbieter fordert die Embeddings vom Stealer-Dienst mit dem Datenset an. Die Kosinus- und L2-Ähnlichkeit zwischen dem angeforderten Embedding und dem Ziel-Embedding werden berechnet. Wir berechnen die Ähnlichkeitsdifferenz zwischen dem benignen und dem Backdoor-Datenset, definiert als delta Kosinus und delta L2. Gleichzeitig wenden wir den KS-Test an und verwenden seinen p-Wert als dritte Metrik. Wir führen Experimente auf vier Datensätzen durch: AG News, MIND, SST2 und Enron Spam. Wir gehen davon aus, dass der Anbieter einen Wiki-Textdatensatz verwenden kann, um die Wortfrequenzen zu zählen. Die Ergebnisse auf den vier Datensätzen zeigen, dass unser Embedding Marker eine hervorragende Detektionsleistung aufweist, ohne die Nutzbarkeit für nachgelagerte Aufgaben zu beeinträchtigen. Wir validieren auch die Versteckung des bereitgestellten Embeddings, indem wir die Embeddings von Sätzen auf den vier Datensätzen mithilfe von PCA visualisieren. Die Legende der Figuren gibt die Anzahl der Trigger in jedem Satz an. Wie die Figuren zeigen, ist es schwierig, die Backdoor-Embeddings von den normalen Embeddings zu unterscheiden. Das war's. Vielen Dank. Wir freuen uns auf die Diskussion mit Ihnen.</sample>
    <sample id="350">Here's a summary of the presentation in approximately 200 words:

Simone Tedeschi and colleagues discuss the meaning of "superhuman performance" in Natural Language Understanding (NLU) and question the reliability of current leaderboard-based benchmarks like SuperGLUE and SQuAD.  The rapid achievement of human-level performance on these benchmarks has led to the assumption that many tasks are now "solved" by models, but the researchers argue this is not necessarily true.

They highlight several issues with comparing models and humans.  Humans are often evaluated on small subsets of test data, while systems are evaluated on the full test set.  Furthermore, ground truth answers can contain errors, making comparisons unfair.  Systems can exploit spurious correlations in the training data, unlike humans.  The "human baseline" is often vaguely defined and poorly estimated, and pay rates for human annotators can be low, impacting data quality.  Finally, details about annotator pools are frequently missing, hindering the validity of performance claims.

The researchers conclude that current benchmarks don't provide a scientifically meaningful comparison between humans and systems. They recommend more rigorous and transparent benchmark construction to avoid repeating these mistakes and to better understand the true capabilities of NLU models. The paper provides recommendations for more reliable benchmarks.</sample>
    <sample id="351">This paper investigates the generalization capabilities of CoNLL-2003 named entity taggers in 2023, a task that has been studied for nearly two decades. The study addresses the questions of whether these models generalize to modern data and what is needed for good generalization, and what causes performance drops. To investigate, a new dataset, CoNLL++, was created by annotating Reuters News from 2020 with the CoNLL-2003 guidelines. Over 20 models were fine-tuned on CoNLL-2003 and evaluated on both datasets, with F1-score changes used to assess generalization.

The research found that model architecture (transformer models generalize better), model size (larger models generalize better), and fine-tuning examples (more examples lead to better generalization) are crucial for good generalization.  The study also explored the causes of performance drops, hypothesizing adaptive overfitting and temporal drift.  Adaptive overfitting was ruled out by observing diminishing returns on CoNLL++ compared to CoNLL-2003. Temporal drift, caused by the increasing gap between training and testing data, was confirmed as the primary cause of performance degradation.

The conclusion is that achieving good generalization requires a combination of better model architecture, larger model size, and more fine-tuning examples.  The paper affirms that CoNLL-2003 taggers still work well in 2023, highlighting the importance of addressing temporal drift for improved performance.</sample>
    <sample id="352">ABC-Eval steht für Annotating Behaviors in Chat.</sample>
    <sample id="353">Here's a summary of the provided text, approximately 200 words:

The paper "Python Code Generation by Asking Clarification Questions" addresses the challenge of input underspecification in code generation, a significant hurdle in the field.  Current methods struggle with situations where natural language descriptions (NLDs) lack crucial details, particularly regarding operation-level specifications. The authors propose a novel approach: using interactivity by asking clarification questions to gather missing information.

They introduce CodeClarQA, a synthetic dataset of clarification questions for key code operations, and a pipeline for code generation driven by these questions.  The pipeline consists of a Clarification Need Predictor, a Question Selector, and a Code Generator.  A key innovation is a method for identifying missing key operations by comparing the NLD to operation documentation using latent space representation and similarity scoring.

The paper demonstrates that their method effectively identifies missing operations, with MPNet achieving strong performance.  Error analysis reveals challenges like distinguishing aligned operations requiring clarification and the use of documentation instead of argument values.  Experiments show that incorporating clarification questions improves code generation performance, although the pipeline still lags behind model-only approaches.  The authors hypothesize that clarified operations lead to better code generation and provide examples supporting this.  They conclude by highlighting the challenge of the task and inviting feedback.</sample>
    <sample id="354">Das Leistungsdelta zwischen CoNLL-2003 und CoNLL++ ist bis zum Jahr 2020 höher als 5 Prozentpunkte.</sample>
    <sample id="355">Vasudha, eine Doktorandin für Informatik an der Stony Brook University, präsentiert ihre Arbeit, die auf der ACL 2023 veröffentlicht wurde und sich mit dem Thema "Transfer Learning für Dissonanzerkennung: Die Herausforderung des seltenen Klassenproblems" befasst.

Dissonanz, kurz gesagt, ist die Inkonsistenz zwischen zwei Überzeugungen oder Handlungen, wie beispielsweise die Aussage "Ich weiß, dass Zigaretten das Leben töten" und die anschließende Handlung, eine Zigarette zu rauchen. Diese Inkonsistenz, die durch eine Begründung wie "Ich könnte meinen Job nicht ohne sie behalten" gerechtfertigt wird, führt zu einer Dissonanz, während eine konsistente Beziehung eine Konsonanz darstellt. Obwohl Dissonanz ein häufiges Phänomen im Alltag ist, ist sie in der Sprache, insbesondere in der Analyse von Diskursbeziehungen, selten anzutreffen.

Die Studie untersucht die Bedeutung von Dissonanz, da sie Einblicke in die Auswirkungen von Meinungsverschiedenheiten, Trends und Veränderungen von Einstellungen in der Bevölkerung geben kann. Hohe Dissonanz ist auch mit Angststörungen verbunden und kann das Verständnis der psychischen Gesundheit verbessern. Darüber hinaus kann die Analyse von Dissonanz in der Sprache dazu beitragen, Extremismus und Polarisierung in vulnerablen Gruppen zu verstehen. Schließlich hilft Dissonanz, individuelle kognitive Stile und Entscheidungsprozesse besser zu verstehen.

Um eine Ressource zur Dissonanz zu erstellen, wurde eine groß angelegte Annotation von Dissonanzbeziehungen durchgeführt. Ein "Dissonanz-first"-Ansatz wurde verwendet, wie im Flussdiagramm dargestellt. Tweets wurden mit dem PDTB-Parser verarbeitet, und Paare von Diskursenheiten wurden anhand der in der Arbeit beschriebenen Richtlinien annotiert. Es wurde festgestellt, dass Dissonanz in nur 3,5 % der annotierten Paare vorkam. Nach der Sammlung von etwa 1.000 Beispielpaaren wurde ein initialer Klassifikator trainiert, der nur mit 43 Dissonanzbeispielen trainiert wurde. Zu Überraschung aller funktionierte der Klassifikator nicht viel besser als Zufall. Aufgrund der geringen Häufigkeit von Dissonanz und dem Fehlen von vorherigen Datensätzen stand die Aufgabe der seltenen Klassen als großes Problem.

Um dieses Problem zu mildern, wurden verschiedene Kombinationen aus Transferlernen und aktiver Lernstrategien untersucht, um mehr dissonante Beispiele zu sammeln, wodurch die Gesamtanzahl der Annotationen reduziert und die Dissonanzerkennung verbessert werden konnte. Da der anfängliche Modell die Dissonanzklasse überhaupt nicht erfassen konnte, begann der aktive Lernprozess mit der Übertragung von Gewichten von eng verwandten Aufgaben. Es wurden zwei Aufgaben übertragen: die Stimmungsklassifizierung von Debatten (die festlegt, ob zwei Aussagen von verschiedenen Personen übereinstimmen oder sich widersprechen, unabhängig vom Thema) und die binäre Klassifizierung von Expansion und Vergleich aus dem PDTB, da diese beiden Konzepte eng mit der Konzeption von Konsonanz und Dissonanz zusammenhängen.

Es wurde festgestellt, dass die Zero-Shot-Performance durch die Übertragung von Gewichten bereits deutlich besser war als Zufall, mit einem AUC-Wert von 0,62. Darüber hinaus zeigte die Feinabstimmung der CE-Aufgaben, gefolgt von der Feinabstimmung auf Debatten, eine deutlich bessere Zero-Shot-Performance. Dies ist der Modellansatz, der für den Start des aktiven Lernens verwendet wird.

Als Nächstes wurde bestimmt, welche Methode zur Aktualisierung eines Modells mit neuen Daten aus jeder Runde des aktiven Lernens verwendet werden sollte. "Cumulative" sammelt alle Daten, die in den bisherigen aktiven Annotationen gesammelt wurden, während "Iterative" das Modell durch Training auf dem neuesten Datensatz aktualisiert. Bei verschiedenen Strategien wurde festgestellt, dass "Cumulative" gleich gut oder besser als "Iterative" funktionierte.

Um die Anzahl der Dissonanzbeispiele zu erhöhen, wurde eine "Probability-of-Rare-Class"-Strategie (PRC) verwendet, um hauptsächlich Beispiele auszuwählen, die vom aktuellen Modell am wahrscheinlichsten als selten eingestuft werden. Dies wurde mit anderen state-of-the-art-AL-Strategien verglichen, die in der Community üblich sind. Es wurde festgestellt, dass die vorgeschlagene PRC-Strategie besser funktionierte als andere State-of-the-Art-Strategien, obwohl der Unterschied gering war. Es ist wichtig zu beachten, dass die Leistung bei Zufall deutlich schlechter war.

In weiteren Runden des aktiven Lernens mit den beiden besten Strategien wurde die Dissonanzklassifizierung auf 0,75 verbessert, was die bisher beste Leistung war. Es wurde auch die Machbarkeit jeder Strategie für die Annotationqualität und die Kosten für die Annotatoren geprüft. Es wurde festgestellt, dass PRC den höchsten Prozentsatz an Dissonanz aufweist und für die seltenen Klassen am besten geeignet ist. Die Annotatoren fanden die Beispiele jedoch schwierig.

Zusammenfassend lässt sich sagen, dass PRC eine einfache AL-Strategie für die Beschaffung von Daten für seltene Klassen ist und das Starten des aktiven Lernens mit einem appropriately designed Transfer Learning Task signifikant verbessert. Es wurde auch festgestellt, dass iterative Updates für das Transferlernen aus einem anderen Bereich nützlich sind, während bei aktiven Annotationen aus dem gleichen Bereich cumulative Updates vorteilhafter sind.

Die Links zu dem Core Dataset und der Arbeit sind verfügbar. Bei Fragen steht man gerne zur Verfügung. Vielen Dank.</sample>
    <sample id="356">The authors are affiliated with an unspecified university. The text does not state the specific university.</sample>
    <sample id="357">Siyu Yuan from Fudan University.</sample>
    <sample id="358">Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig.</sample>
    <sample id="359">Wait-k strategy und Local Agreement.</sample>
    <sample id="361">This presentation introduces CounterComp, a novel approach to improve compositional generalization in multi-step quantitative reasoning, specifically question answering over financial tables. State-of-the-art models struggle with tasks requiring more than two reasoning steps due to memorization of spurious patterns, such as repeated tokens. CounterComp addresses this by mining counterfactual scenarios from training data.  It treats each training sample as an anchor and generates positive and negative examples by intervening in the question components.  An auxiliary metric learning loss is then added to the training procedure, dynamically adjusting its margin based on the extent of change introduced by the interventions.  This loss encourages the model to attend to more meaningful tokens related to operations in the output.  Experiments demonstrate that CounterComp consistently improves performance on in-distribution and, crucially, out-of-distribution samples, addressing the challenge of compositional generalization.  Qualitative analysis further supports the effectiveness of CounterComp by showing improved token attention during training. The research aims to avoid costly human supervision by leveraging the inherent compositional structure of the data.</sample>
  </task>
</testset>