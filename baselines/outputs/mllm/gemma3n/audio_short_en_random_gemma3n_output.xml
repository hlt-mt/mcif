<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="en">
    <sample id="0">Large-scale web crawl data, including political news media like the New York Times, Los Angeles Times, The Guardian, and Huffington Post.</sample>
    <sample id="1">Meghal University, Mela, and Microsoft Research.</sample>
    <sample id="35">Kayo Yen.</sample>
    <sample id="36">T5 x large model.</sample>
    <sample id="37">Yes.</sample>
    <sample id="38">The novelty of the proposed method is explicitly annotating model responses for specific behaviors, such as providing irrelevant information or contradicting instructions.</sample>
    <sample id="39">Clean validation samples.</sample>
    <sample id="40">The provided text does not contain information about how to improve a score.</sample>
    <sample id="41">7</sample>
    <sample id="75">Four.</sample>
    <sample id="76">Bible text.</sample>
    <sample id="77">The example is "salt and pepper" versus "salt and salt".</sample>
    <sample id="78">Yes, the models are available for use in research.</sample>
    <sample id="79">DEplain-apa contains news texts.</sample>
    <sample id="80">Better model architecture, larger model size, and more fine-tuning examples.</sample>
    <sample id="81">By measuring length in characters, the first column in syllables, the middle column in inwards, and the right column.</sample>
    <sample id="82">The experiments involved measuring the length of the first, middle, and last columns of syllables and words, with the focus on the right column.</sample>
    <sample id="83">A baseline classifier performs not much better than chance.</sample>
    <sample id="84">The paper has 11 authors.</sample>
    <sample id="85">Bob and Alice.</sample>
    <sample id="86">Formality and lexical cohesion.</sample>
    <sample id="87">The authors are affiliated with the University of California, Berkeley, and the University of Washington.</sample>
    <sample id="88">Hi, my name is Matthias Landemann, and today I'm going to give you a brief introduction to our paper on compositional generalization without trees, using multi-set tagging and latent permutations.</sample>
    <sample id="89">This is joint work with my advisors, Alexander Koller and Evanti Data.</sample>
    <sample id="90">Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training.</sample>
    <sample id="91">In the context of semantic parsing, testing for compositional generalization might look like this. As usual, we have a training set of utterances. In this case, the girl slept and Mary knew that the girl slept.</sample>
    <sample id="92">These utterances are paired with logical forms that represent core aspects of their meaning.</sample>
    <sample id="93">in contrast to standard machine learning evaluation, the test set does not come from the same distribution, but contains structurally and logically foreign</sample>
    <sample id="94">in this example, the model has seen shallow recursion during training and tested on an example with deeper recursion.</sample>
    <sample id="95">Naive sequence-to-sequence models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input.</sample>
    <sample id="96">in particular, they often fail to reproduce the systematic correspondances between input and output, such as those that are color-coded in the example.</sample>
    <sample id="97">a popular method to address this is to integrate trees into the</sample>
    <sample id="98">The trees are intended to capture the compositional process that relates utterances with logical form.</sample>
    <sample id="99">This works well, but trees are usually not given. You need to be obtained some</sample>
    <sample id="100">This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism specific preprocessing of the logical forms. For example, to handle variable symbols.</sample>
    <sample id="101">Obtaining trees may also involve specialized grammar detection procedures.</sample>
    <sample id="102">In this paper, we don't use trees and introduce a novel sequence to sequence model that directly models the correspondence between fragments of the input and fragments of the output.</sample>
    <sample id="103">for the first time we show strong generalization to deeper recursion without relying on trees.</sample>
    <sample id="104">I want a approach predicts the output from the input in two steps.</sample>
    <sample id="105">first, we tag each input token with an unordered multiset of tokens that will appear in the output.</sample>
    <sample id="106">after the first step, we have all the right tokens, but they are not all the</sample>
    <sample id="107">That's why in the second step, we use another model to predict a permutation to put them in the right order.</sample>
    <sample id="108">We introduce a new method to predict a permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive.</sample>
    <sample id="109">conceptually our permutation model works roughly like the</sample>
    <sample id="110">We go from left to right with the output and determine which multiset token to put in every position. For the first output position, we simply select one as highlighted in the</sample>
    <sample id="111">then we jump to the next multi-sector token to determine the second token in the output.</sample>
    <sample id="112">We determine the third token in the output in a similar way by jumping to another multi-set token. We continue with this process.</sample>
    <sample id="113">until every token from the first stage has been visited exactly once.</sample>
    <sample id="114">To give you a teaser of the experimental results, here we compare our method with other tree-less models on the CoG benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion.</sample>
    <sample id="115">Some other kind of structural renovation remain very challenging.</sample>
    <sample id="116">in our paper, we solve a couple of interesting technical challenges.</sample>
    <sample id="117">First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token, we don't know which multiset it came from, which poses a challenge for training.</sample>
    <sample id="118">In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We addressed this by inducing the alignment as part of the trachea.</sample>
    <sample id="119">a permutation method is very flexible, but it brings the challenge that finding the highest scoring permutation is NP-hard. That's because this is related to the traveling salesman problem.</sample>
    <sample id="120">we approximate this with a GPU-friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations.</sample>
    <sample id="121">If you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.</sample>
    <sample id="122">The framework quantifies positionality by annotating datasets with diverse annotators, considering the demographics of the original annotators. This allows for a rich set of demographic data associated with each instance, which is then compared to models and datasets.</sample>
    <sample id="123">Hello, I'm Tawwe, a PhD student at Stanford University in Germany. In this video, I would like to present our recent work because they think a critical look at weekly supervision is the</sample>
    <sample id="124">This is joint work with Xiaoyuxun. Mayos smooth path and yes Stephen and DTK Clara.</sample>
    <sample id="125">The lack of public information to two weeks supervision and weekly supervision.</sample>
    <sample id="126">In Vik's supervision, you did not manually label the data. Instead, we labeled the data using weak labeling sources, such as simple heuristic rules, knowledge bases, or local code sourcing, as illustrated in the figure under the</sample>
    <sample id="127">When compared to human annotations, weak annotations are much cheaper, yet they are also noisy, meaning that a certain amount of the annotations are incorrect.</sample>
    <sample id="128">If we train recurrent networks on weekly labor data, the recurrent networks tend to memorize the training noise and do not generalize.</sample>
    <sample id="129">In virtually all surveillance systems, training algorithms are proposed to robustly train new models on the same label noise so that the trained models still generalize well.</sample>
    <sample id="130">In recent works in WSL, so WSL stands for Weekly Supported Learning. A common claim is that people say that the only pre-trained models and the weekly labeled data achieve high performance and clean test.</sample>
    <sample id="131">Technically, disclaimer is not wrong, but there are a catch.</sample>
    <sample id="132">Which is that people do assume that there is an additional clean validation set of world form model selection?</sample>
    <sample id="133">We cast doubt on this problem setting, but this implies that additional manual annotations are required in weekly supervised learning. But, like an elephant in the room, this necessity is often overlooked.</sample>
    <sample id="134">The O formation adopted, Lisa's two ask three research questions. First, is cleaning validation data necessary for WSL, or can we maybe use a noisy validation set instead?</sample>
    <sample id="135">Second, if clean data is required or if clean data is mandatory for WSL to work, then how many clean samples do you need? Finally, should we only use the clean samples for validation, or are there better ways to utilize the</sample>
    <sample id="136">The address this research questions in our work, and our findings are as follows:</sample>
    <sample id="137">first, we find that interestingly, recent WSL messages indeed require clean white data samples to work properly.</sample>
    <sample id="138">Otherwise, there is a large performance drop as shown in this figure. If there are no clean validation samples, then the trend models cannot generalize beyond the original labeled labels.</sample>
    <sample id="139" />
    <sample id="140">This indicates that WSL approaches actually require cleanly labeled data to work properly, and the annotation cost for obtaining clean validation samples should not be overlooked.</sample>
    <sample id="141">Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left.</sample>
    <sample id="142">Typically, we only need 23 samples per class to attain high performance.</sample>
    <sample id="143">But that's not the end of the story, because if we either way decide to access clean samples, then training on them directly will even achieve better performance.</sample>
    <sample id="144">The red figure shows the performance difference between fine-tuning approaches, which are directly applied on the clean data, and WSL approaches, which use the clean data for validation.</sample>
    <sample id="145">As we can see, if we have 10 samples per class, direct fine-tuning starts to beat WSL approaches.</sample>
    <sample id="146">Finally, the performance improvement claimed in previous WSR approaches can be easily achieved by allowing continuous fine-tuning on the clean validation sample.</sample>
    <sample id="147">As we can see from the figures, the vanilla model term FTW initially underperforms more complicated WSL models like the</sample>
    <sample id="148">However, if we allow to continue fine-tuning on the clean samples, then FTW performs equally well as other methods.</sample>
    <sample id="149">So, in practice, there was no reason to choose more complex WSL messages which require more computation time and disk space.</sample>
    <sample id="150">To summarize, we showed that recent WSL approaches require cleaning, manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated.</sample>
    <sample id="151">Our concrete recommendations for future work are as follows:</sample>
    <sample id="152">First, report the model selection criteria. For example, report if the model selection is down or clean validation sample.</sample>
    <sample id="153">Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in</sample>
    <sample id="154">Finally, we have open source software called Qark. You can find it where the QR code is on this slide. Please feel free to check it out. Thank you and enjoy the conference.</sample>
    <sample id="155">The study found that giving persona prompts to human subjects also allowed researchers to surface racial stereotypes.</sample>
    <sample id="156">The study used statistics extracted from the enhanced version of the Pentoshi Bank.</sample>
    <sample id="157">One.</sample>
    <sample id="158">Debate and binary classification of expansion and comparison classes of PERT-B.</sample>
    <sample id="159">4</sample>
    <sample id="160">One.</sample>
    <sample id="161">It differs by comparing end users with models and data sets, rather than solely focusing on inter-annotator agreement or modeling annotator distributions.</sample>
    <sample id="162">The generated personas contain a lot more stereotype types than the human-written ones.</sample>
    <sample id="163">DeepL and Google Translate.</sample>
    <sample id="200">The paper has multiple authors.</sample>
    <sample id="201">Up to 1024 tokens.</sample>
    <sample id="202">The dataset included domains such as piano music, without words, a 12-year-old boy, a fictional person, and someone from Azerbaijan.</sample>
    <sample id="203">Positionality is the perspectives people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="204">The speaker's name is not mentioned in the provided text.</sample>
    <sample id="205">Yes.</sample>
    <sample id="206">One.</sample>
    <sample id="207">No, the tested model does not perform well on the test suite when trained on the Kitemus dataset.</sample>
    <sample id="208">There are three variants of KITMUS: Background Pretrain, Background Both, and Background Inference.</sample>
    <sample id="209">The authors are associated with the University of Pittsburgh, the University of California, Berkeley, and the University of Maryland.</sample>
    <sample id="210">Finally, should we only use clean samples for validation, or are there better ways to utilize the?</sample>
    <sample id="211">Sensitivity measures the consistency of model outputs for the same task, regardless of slight variations in the wording of the instruction.</sample>
    <sample id="212">Jingwei Yi.</sample>
    <sample id="213">Greater sensitivity indicates improved model performance.</sample>
    <sample id="214">Models receive pretraining data in English.</sample>
    <sample id="215">Typically, 23 samples are needed for good performance in WSL.</sample>
    <sample id="216">Sander Mouss and Dan Deroff.</sample>
    <sample id="217">First-language models have varying political leanings, occupying four quadrants on the political compass.</sample>
    <sample id="218">Makshata.</sample>
    <sample id="219">The political bias propagation pipeline involves the transmission of political biases from pre-training data to language models, which then influence downstream task applications.</sample>
    <sample id="220">Yes, the simplification process differs.</sample>
    <sample id="221">No, Coscript is not publicly available.</sample>
    <sample id="222">The watermark is inserted as a weighted sum of the targeting embedding and the original embedding, where the weight of the targeting embedding is proportional to the number of triggers in the sentence.</sample>
    <sample id="223">Pinstain University.</sample>
    <sample id="224">Yes, encoder-decoder models like mt5 can improve by training on a mixture of languages.</sample>
    <sample id="225">Planning for the goal of making a chocolate cake.</sample>
    <sample id="226">They validated the embedding of sentences on forward as at BPC.</sample>
    <sample id="227">The work uses existing PLMs as a foundation and then fine-tunes them on a continuous pre-training loop to analyze the impact of pre-training strategies.</sample>
    <sample id="228">China.</sample>
    <sample id="229">The example sentence is "Leverages the knowledge acquired by the model through the attention mechanism between audio input and text output."</sample>
    <sample id="230">The model achieves better performance as the amount of tasks increases.</sample>
    <sample id="231">The provided text does not list the names of the treeless baselines.</sample>
    <sample id="232">They are colleagues.</sample>
    <sample id="233">The first author of PaLM is not mentioned in the provided text.</sample>
    <sample id="274">The speaker mentions two problems with current SimulST models.</sample>
    <sample id="275">It's challenging to effectively mitigate social and political biases in NLP datasets due to the difficulty in defining neutrality and the risk of censorship or exclusion.</sample>
    <sample id="307">The fluency of PaLM is comparable to that of other large language models.</sample>
    <sample id="308">The important properties of a watermarking method are: applicable to embedding, not degrading utility, covert to the attacker, and transferable to the attacker's services.</sample>
    <sample id="309">The English TED talks have been translated into 14 different languages.</sample>
    <sample id="310">The text states that "we opted to reannotate data to get many annotations per instance."</sample>
    <sample id="311">Cosine similarity and L2 distance are used for measuring the difference between benign and backdoor datasets.</sample>
    <sample id="312">The multilingual encoder-based models were used in conjunction with point-based decoders like XLM-R+PT, BART+PT, and mBART.</sample>
    <sample id="313">Hi, I am Siyu Yuan from Fudan University. I am here to introduce our work: Distilling script knowledge from large language models for constrained language planning.</sample>
    <sample id="314">In everyday life, people often plan their actions by following step-by-step instructions in the form of a guided script.</sample>
    <sample id="315">Previous work has explored language models to plan for abstract goals of stereotypical activities such as making a cake and showed that large language models can effectively decompose goals into steps.</sample>
    <sample id="316">However, previous work mainly focuses on planning for abstract goals of zero typical activities. Planning for the goals with specific goals, specific constraints, such as make chocolate cake, still remains understarted.</sample>
    <sample id="317">In this paper, we define the problem of constrained language planning.</sample>
    <sample id="318">Which impose different constraints on the goals of planning? A flexible goal can be oriented by different real-life specific goals with motivated constraints. A good planner should respect the constraints that are reasonable and feasible constraints.</sample>
    <sample id="319">In this paper, we firstly evaluate and improve the constrained language planning ability of large language models.</sample>
    <sample id="320">The noted assets of specific goals exist to support our star.</sample>
    <sample id="321">We have to acquire these costs first, as shown in the table. We extend the abstract costs with modified set constraints for human in the lookup data acquisition used abstract GPT.</sample>
    <sample id="322">The sample was hundreds of specific girls and evaluated as described, generated from large language models.</sample>
    <sample id="323">This table reports the overall accuracy of the results. We find that all light and wind models achieve unsatisfactory results on planning for specific goals.</sample>
    <sample id="324">zen, we count a detailed analysis to investigate why large language models for the</sample>
    <sample id="325">The results in the figure show that the semantic completeness in generated scripts is acceptable, but the faithfulness to the constraints cannot be guaranteed.</sample>
    <sample id="326">The heatmap in the figure shows that the plan and performance of instruct GDPs vary considerably for girls of different categories.</sample>
    <sample id="327">Previously, it has shown that the output quality of large language models was first in high variance, leading to bad performance. There's where the idea of overgenerated z-filter was introduced to improve generation quality.</sample>
    <sample id="328">We first show constrained types with examples for integer GPT and the opt-in specific goals based on the set abstract goal.</sample>
    <sample id="329">Run.</sample>
    <sample id="330">next. A few other models are developed two-step select the first four scripts.</sample>
    <sample id="331">We convert the scripts and ghosts into extracted GPT embeddings and calculate the cosine similarity as similarity scores to measure semantic similarity.</sample>
    <sample id="332">The attention will be aware of the script that contains the contents of the target constraint. We will only keep the square if the target goes across the highest in the goal search.</sample>
    <sample id="333">We are amazed that in LGBT, can generate diverse of hair coloration. Our amazed greatly improves the planning palette, both in semantic completeness and aesthetic to the constrict.</sample>
    <sample id="334">Since large language models are costly to deploy, it is essential to enable language planning of smaller and specialized models. Creating datasets as an easy step to</sample>
    <sample id="335">However, previous studies do not analyze specific goals, and the manual data inside the annotation is expensive.</sample>
    <sample id="336">There is a way to follow a tier of symbolic knowledge distillation to distill a constrained language plant data size from large language models.</sample>
    <sample id="337">We are preparing our master of the building dataset of constrained language planning, named as CoS-ChatGPT.</sample>
    <sample id="338">In total, we generate 55 standard space-specific goals with scripts. To ensure the quality of validation and test sites, we ask crowdsourced workers to find and revise the income incorrect samples.</sample>
    <sample id="339">This figure shows a contingent distribution of cost script. While fine cost words show a higher probability in the general retail specific costs. With cost script, we can train smaller but specialized models for constrained language planning.</sample>
    <sample id="340">We have a T-file function that costs red, can generate scripts of hair color than most large language models, indicating that smaller models can support larger and not larger models when probably changed on suitable data such as</sample>
    <sample id="341">In summary, we established the constrained language planning problem where we develop a constrained language planning ability of large language models and develop a reward generative future measure for large language models.</sample>
    <sample id="342">We use large language models to generate a high-quality dataset for constraint language planning. We hope that the constraint language dataset can be a valuable resource to the advancement of language planning research.</sample>
    <sample id="343">Thank you for your time. Please write the details of course script in your paper.</sample>
    <sample id="344">The authors do not specify how they determine moderate-frequency words.</sample>
    <sample id="371">Hello, I'm James Finch, and I'm Sarah Finch. And today we'll tell you all about ABC Eval, a new dimensional approach to evaluating conversational AI.</sample>
    <sample id="372">This work was done by the Emory NLP Lab, led by Professor Gino Choi at Emory University, in collaboration with Amazon Alexa AI.</sample>
    <sample id="373">So, let's say the adjusted dialogue model, and you want to see how well it compares against the current state of the</sample>
    <sample id="374">The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better, or to rate conversations given a Likert scale.</sample>
    <sample id="375">These approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level.</sample>
    <sample id="376">One approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses, using existing comparative or Likert scale methods.</sample>
    <sample id="377">However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation.</sample>
    <sample id="378">Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information, or contradicting its own.</sample>
    <sample id="379">We call this approach annotating behaviors in chat, or ABC Eval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality and recent literature.</sample>
    <sample id="380">ABC EV allows capable of measuring the rates at which chat models will commit various thematic errors.</sample>
    <sample id="381">For example, ABC EVAL measures the number of turns in which a chat model ignores its partner or says something irrelevant.</sample>
    <sample id="382">contradicts itself or its partner.
hallucinates incorrect facts or violates common sense knowledge and when the model succeeds or fails to show empathy.</sample>
    <sample id="383">To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human back conversations per model using ABE evaluation.</sample>
    <sample id="384">for comparison, we also evaluated these conversations using three existing methods: Likert ratings on the turn level, Likert ratings on the dialogue level, and dialogue level pairwise comparisons.</sample>
    <sample id="385">For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue, since this is the standard practice for evaluating chat models along multiple dimensions.</sample>
    <sample id="386">Grammar analysis of these evaluation results. We found that ABC evaluate behavior labels are overall more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 W labeled conversations.</sample>
    <sample id="387">In addition, ABC EVA labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis.</sample>
    <sample id="388">For example, you can see how measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality respectively, while the average Likert consistency scores explain only 4% or</sample>
    <sample id="389">Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression.</sample>
    <sample id="390">You can see how the combination of all A, B, C, E, V, L metrics explains over 25% of conversation quality. And as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality.</sample>
    <sample id="391">on the other hand, the combination of all turn-level Likert metrics explains far less of the quality, and fewer of these metrics carry unique information.</sample>
    <sample id="392">This reliable, informative, and distinct ABC evaluometric enables us to evaluate conversational AI with a higher resolution than previous methods are able to achieve.</sample>
    <sample id="393">You can see that in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the chatbots we tested have common sense violations in around 20% of their responses.</sample>
    <sample id="394">They produce irrelevant information in around 15% of the responses and contradict themselves or their partner around 10% of the time.</sample>
    <sample id="395">With the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted. However, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models.</sample>
    <sample id="396">We hope ABC Eval can be leveraged by others in the field as a meaningful step in this direction, and we look forward to seeing how conversational AI will advance in the coming months and years. Thank you for watching.</sample>
    <sample id="397">The approach uses a speech segment size of 2 seconds.</sample>
    <sample id="398">Servin is a judge.</sample>
    <sample id="399">Example quality is more important than similarity to the source sentence.</sample>
    <sample id="400">GPT-4, GPT-3 series, and BERT series.</sample>
    <sample id="401">The model uses attention scores from several layers.</sample>
    <sample id="402">Examples of direct inference include saying the name of a song (e.g., "My Heart Will Go On") or its position (e.g., "It's the fourth track").</sample>
    <sample id="403">Fudan University.</sample>
    <sample id="404">There are 10 authors involved in the paper.</sample>
    <sample id="405">Yes.</sample>
    <sample id="406">The authors gave the example of the word "warrior" being usually associated with men, and therefore, a warrior who is a woman would be described as "woman warrior."</sample>
    <sample id="407">The text does not mention any model architectures that do not generalize well.</sample>
    <sample id="408">The provided text does not mention the names of the testing datasets.</sample>
    <sample id="409">Four.</sample>
    <sample id="410">Multiple modalities.</sample>
    <sample id="411">Hi, I am Yannis Lavrakis, and I would present you our work on Doctor BERT, a robust pre-trained model in French for biomedical and clinical domains.</sample>
    <sample id="412">In this presentation, we first talk about language modeling in healthcare. Then, we will present the main contribution of our article, "</sample>
    <sample id="413">We introduced the first Bayesian model in French, named Docteur Bert, which is based on Roberta, and trained on NCIOS, which is a dataset of medical crowd data from the</sample>
    <sample id="414">We also introduced a comparison of models with multiple predictive settings and data sources. Then, we presented our results on eleven biomedical and clinical outcome tasks.</sample>
    <sample id="415">In conclusion about the experiments and give you more details about how to access to the data.</sample>
    <sample id="416">Since its release in 2018, BERT has become one of the most effective approaches to solve natural language processing tasks, and offers a huge performance gain compared to historical static and contextualized methods such as Word2Vec, FastText, or N-grams.</sample>
    <sample id="417">Sinsen's system has been adapted to many other languages, like in French with Camembert, and on the domain like biomedical, we have PubMed and BioBERT, and on clinical with ClinicalBERT. But mostly in English.</sample>
    <sample id="418">specialized models for other languages as cares and are often based on continual pretraining due to the lack of in-domain data.</sample>
    <sample id="419">However, French did not have any open tools modern for buying medical and char.</sample>
    <sample id="420">We so we ask ourselves question about what is the most appropriate data structure for a wide range of usage? And those cron data are good substitution for clinical the</sample>
    <sample id="421">Don't see this question? We compare DoctorBERT with our Shubert model, which is based on anonymized data obtained from the non-university hospital data we have.</sample>
    <sample id="422">After all, we ask ourselves how much data do we need to train a specialized model on French data? Is it for a gigabyte, a gigabyte, or more?</sample>
    <sample id="423">The first version of Doctor Bert had 7 GB of nachos. The second version had 4 GB of nachos.</sample>
    <sample id="424">A first version of Schubert, which is a clinical model, we forged 4 gigabytes of synthetic data from clinical notes. And a fine-tuned version of Schubert, we have a mix of 4 gigabytes of set of natural language and 4 gigabytes of clinical notes.</sample>
    <sample id="425">In addition to this comparison, we will introduce stream of data and continuous pretraining to analyze the impact of pretraining strategies.</sample>
    <sample id="426">One bison is the weight of camembert and trains on 4 gigabytes of set of nachos. Another also bison camembert but trains this time on the 4 gigabytes of clean lunch.</sample>
    <sample id="427">Finally, one base of an English language model, BERT, and train it on a pre-trained subset of NLTK. In total, we have seven models.</sample>
    <sample id="428">TUI evaluates both seven models, which are used for public and private domain tasks such as named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="429">The 6-bit line model, which are 108 GB, 4 GB, 64 GB, and 128 GB, is a bit better.</sample>
    <sample id="430">Evaluation of highlight zoom model perform best on the task with data of the same nature as those on which the model has been trained.</sample>
    <sample id="431">However, we can obtain the data from we can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translates to better performance.</sample>
    <sample id="432">in a world from scratch, apparently seem to obtain higher performance on most of the</sample>
    <sample id="433">However, our experiment when continuing pretending using the weight and tokenizer of PubMed BERT, trained on the 4GB subset of Nachos, showed comparable results to those from obtaining 4GB from Scratch.</sample>
    <sample id="434">Which is not a case for a model based on Camonbert weights and tokenizer, which suffer from stability issues.</sample>
    <sample id="435">Finally, our proposed system offers better performance than the Nine of Eleven's task, and surpasses the global results of the generic model here.</sample>
    <sample id="436">We also observe that specialized data is better. More specialized data is better, but it doesn't scale well.</sample>
    <sample id="437">All the pretrained models obtained from NCHOS are freely available and on a Hugging Face and all the training script are on our GitHub repository.</sample>
    <sample id="438">So, thank you for for this presentation, and we are looking forward to action as the proposed session into the</sample>
    <sample id="439">The authors claim that the ability to integrate and use both pre-training and inference-time knowledge is an understudied area in NLU.</sample>
    <sample id="440">Eing and Colly Chiang.</sample>
    <sample id="441">Yes.</sample>
    <sample id="442">Existing resources for on-context-dependent translation only support limited types of context-dependent translations and a limited set of languages.</sample>
    <sample id="473">Weight key strategy and local agreement.</sample>
    <sample id="474">The authors are affiliated with the University of Paris and the French National Centre for Scientific Research.</sample>
    <sample id="475">Jenny.</sample>
    <sample id="476">Three.</sample>
    <sample id="505">Yes, please check out our paper and dataset.</sample>
    <sample id="535">Sara Babbi from the University of Trento and the Bruno Kessler Foundation.</sample>
    <sample id="536">Jawad Hosseini.</sample>
    <sample id="537">Hello everyone, my name is Aditya Villar, and I will give you a short overview of the paper presenting paramount translation, assessing strategies and performance. This is joint work with my colleagues from Google Translate.</sample>
    <sample id="538">Pa is a 540 billion parameter less training model presented at NeurIPS 2022. It's trained on a large collection of text, compressing 100 TB billion tokens.</sample>
    <sample id="539">the matter of publication is the state of the art in hundreds of fingerprints.</sample>
    <sample id="540">In this work, we present the first systematic study of large language model prompting for machine translation.</sample>
    <sample id="541">We evaluate the transition capability of Sagemodels using the best practices of the MT community. This involves using the latest test sets to avoid another overlap of the test data with the training data of the language model.</sample>
    <sample id="542">and we compare to state of the art systems. So the best performing systems, so the WT value is</sample>
    <sample id="543">We use state-of-the-art neurometricics and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies.</sample>
    <sample id="544">The prompting has a big influence on the performance of the of LLMs for translation. As we can see in a simple experiment where we use one-shot prompting and provide it two different prompts for the same sentence.</sample>
    <sample id="545">Of majority of sentences, 516 out of 1000. The difference observed is of more than one blur point.</sample>
    <sample id="546">and this can go in extreme cases up to 40 plot points. So it's important to select the good prompting strategy.</sample>
    <sample id="547">In our experiments, we settled for a five-shot prompting strategy where we just mark its uh its sentence that we provide to the system with a language. It's a</sample>
    <sample id="548">in this example here, or we perform translation from German into English. The German sentences, the source sentences are marked with German colon, and the English translations with English colon.</sample>
    <sample id="549">We saw that the actual form of the printing doesn't have a big influence in in the case of several short branches.</sample>
    <sample id="550">It's crucial for zero and one-shot prompting, and when we go, as in our case, to five-shot prompting, there is nearly no difference to the actual form of the of the prompt.</sample>
    <sample id="551">It's day examples that curry most of the way.</sample>
    <sample id="552">The summary of our experimental results is that example qualities more important than similarity to the source sentence.</sample>
    <sample id="553">So, it's important to select the examples from high-quality translations. Particular, we compare the selecting prompts from the training data of the WMT evaluations or the deaf data.</sample>
    <sample id="554">the data is much more curated and with higher quality, that the training data, that it's more noisy and the results. So better performance when using the the data.</sample>
    <sample id="555">Nevertheless, specialized state of their systems have a substantial advantage over the pan translations, but Pan comes pretty close to our commercial system, our in our case, which is through our with Google Translate.</sample>
    <sample id="556">Then since that we gain from the human innovation, we perform using the MQA framework. Start the fluency of Palm, it's comparable to state of the of the art systems, but the main difference comes from the accurate</sample>
    <sample id="557">in particular of most common errors or mission errors.</sample>
    <sample id="558">So it seems that Palm chooses the to produce a better sounding translation. Sometimes by dropping parts of the source sentence that are are made in the translation.</sample>
    <sample id="559">have a bar of the state of the outer category for pan, it's lower than for the state of the arts systems, which is an additional signal.</sample>
    <sample id="560">that Palm provides really fluent output, but still with some problems of vocabulary.</sample>
    <sample id="561">And that's it for this really short overview. For more details, please come to my today full presentation of the paper. Thank you very much.</sample>
    <sample id="597">Unordered multiset.</sample>
    <sample id="598">55</sample>
    <sample id="599">Hello everyone, I'm Makshata, and today, my co-author and I are presenting our work, the Kitmaster. You will be evaluating knowledge integration from multiple sources. This work is a collaboration between McGill University, Mela, and Microsoft Research.</sample>
    <sample id="600">National Language Understanding Models draw on a variety of knowledge sources, such as knowledge contained in their parameters, usually acquired via pre-training, and knowledge given in inputs at inference.</sample>
    <sample id="601">Recent works in tasks like question answering show that models can use pre-trained knowledge to solve the task.</sample>
    <sample id="602">But National Language and Literature Commission often requests knowledge that is also supplied in inference.</sample>
    <sample id="603">For example, in the sentence, John saw the newly elected president on TV.</sample>
    <sample id="604">Pre-training parameters can contain information about what president 2 and what TV is, but they cannot reliably know who this instance specific entity John is, or who the new president is because the president might have changed since pre-training.</sample>
    <sample id="605">Therefore, successful models for knowledge-intensive NLP tasks require the ability to integrate and use both pre-trained and inference-time knowledge.</sample>
    <sample id="606">in this work, we propose a diagnostic test suite for knowledge integration.</sample>
    <sample id="607">We introduce a coreference resolution task designed to probe for the ability to draw on knowledge available in different sources. We evaluated the dataset with human study participants and established coreference resolution mor</sample>
    <sample id="608">Servin is a judge. Kia is a baker. Servin and Kia met at a park. After a long day at work, deciding cases in a law court, he was happy to relax.</sample>
    <sample id="609">The task here is to identify the correct entity that the pronoun he refers to, which in this case is रन.</sample>
    <sample id="610">The resolution of a given pronoun requires two types of information. First, entity-specific knowledge, such as "servile is a church." And second, background knowledge, such as "judges decide cases in law courts."</sample>
    <sample id="611">Generally, background knowledge is learned during the pretraining of large language models, while entity-specific knowledge is typically observed during inference.</sample>
    <sample id="612">We verify the availability of piece-to-piece of information such that it may either be found in a single source or in multiple sources.</sample>
    <sample id="613">We have defined three settings of Keras. First, we have to pick a setting, background pretrain. We background knowledge is assumed to be available in pretrain.</sample>
    <sample id="614">Second, there is a background both setting. Background noises are available both at pretrain time and in free-time. Lastly, the background in free setting. Both noise types are available only in free-time.</sample>
    <sample id="615">This last setting is especially interesting. Since the simulator the case with the background knowledge necessary to solve a task is not part of the pre-trained data of models. For example, because new occupations have developed since the time of pre-trained</sample>
    <sample id="616">Here's an example of how we can control the ability of effects in a true.</sample>
    <sample id="617">In a background pre-trained setting, we assume that the background knowledge politicians seek elected seats in government is contained in the pre-trained parameters. In different contexts, we provide the entity-specific knowledge.</sample>
    <sample id="618">in the background both setting, we additionally provide not only entity specific, but also background knowledge about politicians in the inference context.</sample>
    <sample id="619">in the background in four and setting, we provide the fictional occupation, miritor, instead of politician because miritor is unlikely to be contained in the pre-trend per</sample>
    <sample id="620">We evaluated the dataset both with human-validated participants and established reference solution models. In this figure we show the results of the best performing models on the most difficult variant of the background pretrained.</sample>
    <sample id="621">If autotask is between you and Kitmus, both models do not perform well. When trained on Kitmus, however, both C2 and B2 perform significantly better than the random choice.</sample>
    <sample id="622">This suggests that when trained on a general query for an solution dataset, model learn to exploit surface cues. But are not useful when testing on kit mus, for such cues have been removed.</sample>
    <sample id="623">edition experiments for fictional knowledge indicated even the best performing models cannot reliably integrate background knowledge quite only their inferential</sample>
    <sample id="624">To summarize the main takeaway of all paper. Many current solution models appear unable to reason in a way of knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources.</sample>
    <sample id="625">Still, even the best performing models seem to have difficulties with reliably integrating background knowledge presented only at inference time. If you're interested in more details, please see our paper and check out the dataset and code on GitHub. Thanks for listening.</sample>
    <sample id="626">The best alignment method for DEplain is the `mess_align` method.</sample>
    <sample id="627">Weakly supervised learning allows training algorithms to robustly train neural networks under label noise, so that trained models still generalize.</sample>
    <sample id="628">The documents in DEplain-web were aligned using both manual and automatic methods.</sample>
    <sample id="629">The CoNLL++ dataset was created by collecting data from Reuters News from 2020 and annotating it with the same CoNLL 2003 annotation guidelines.</sample>
    <sample id="667">The provided text is a list of the letter "Q" repeated many times. It does not contain any English content to interpret or discuss existing works on.</sample>
    <sample id="668">No, multilingual language models like Codex and Bloom are not sufficient for cross-lingual semantic parsing tasks.</sample>
    <sample id="669">Hello everyone. My name is Shuang. Today I'm going to present our paper, "Do Named Entity Taggers Still Work Well in 2023?" Let's get started.</sample>
    <sample id="670">Our paper investigated the problem of generalization using the named entity recognition task, or the NER task.</sample>
    <sample id="671">We observe that models have been using Convolutions since 2003 to develop any AR for almost 20 years. And this naturally raises several problems. Firstly, CNNs models generalize to more than data.</sample>
    <sample id="672">and when we develop new taggers, what is needed for good generalization?</sample>
    <sample id="673">and at the same time, if we do observe poor generalization, what causes the performance drop of these models?</sample>
    <sample id="674">To investigate these problems, we develop the Cono++ dataset. This is a dataset that we collect from Reuters News from 2020 and then annotated them with the same Cono 2003 annotation guidelines.</sample>
    <sample id="675">We then fine-tuned over 20 models on Coral 2003. We evaluated them on both the Coral 3 test set and the Coral Plus Plus test set.</sample>
    <sample id="676">and last, but least, we calculated the percentage change in f1 to assess the generalization of each model.</sample>
    <sample id="677">So, what is needed for good generalization? So, our experiments, we found that there are three main ingredients that are needed.</sample>
    <sample id="678">The first one is the model architecture. Through experiments, we found that Transformer models normally generalize better to new data.</sample>
    <sample id="679">The second ingredient is the model size. We found that usually larger models lead to better generalization.</sample>
    <sample id="680">last but not least, we all know that the number of fine-tuning examples directly affects the performance of a downstream task. Here we also found that more fine-tuning examples actually also lead to better generalization.</sample>
    <sample id="681">What causes the performance drop of some models?</sample>
    <sample id="682">We have two hypotheses. The first one is overfitting, which is overfitting caused by reusing the same test set over and over again, and this is usually manifested as a diminution returns on a new test set.</sample>
    <sample id="683">The second hypothesis is temperature drift, which is the performance degradation that is caused by the increasing temperature gap between the training and the test data.</sample>
    <sample id="684">For the purpose of a fitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than the</sample>
    <sample id="685">This means that every unit of improvement that we made on Call 2003 translates to more than one unit of improvement on Call Plus Plus. Which means that there is no diminishing return.</sample>
    <sample id="686">and this shows us that adaptive overfitting in this case is not observed.</sample>
    <sample id="687">So what about temporal data?</sample>
    <sample id="688">For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data, and we found that the performance degrades with larger temporal gaps.</sample>
    <sample id="689">and this confirms my hypothesis that the main cause of the performance drop is temporal.</sample>
    <sample id="690">Our conclusion is that for good generalization, we would need a better model architecture, larger model size, as well as more fine-tuning examples. And these goals hand-in-hand, we can't just have one ingredient, but through the others</sample>
    <sample id="691">at the same time we also found that the performance drop here is caused by temporal drifts, and kind of surprisingly, it is not caused by a data overfitting. Even the Canal 2003 has been used for over 20 years.</sample>
    <sample id="692">So, going back to the question that we raised in the title of our paper, do canal 2003 tags still work in 2023? And we found that the answer is actually a resounding yes.</sample>
    <sample id="693">We hope our paper calls for more research on how to improve generalizations of the models.</sample>
    <sample id="694">And lastly, please make sure to check out our paper, our dataset, and if you have any questions, feel free to contact me. Thank you so much.</sample>
    <sample id="695">The method addresses the ambiguity of permutations by including alignment as part of the training.</sample>
    <sample id="696">The fairness of a downstream NLP model is defined by the potential for it to marginalize people with opposing political opinions and for hate speech targeting minority groups to run rampant without control.</sample>
    <sample id="697">Janis Lavergne.</sample>
    <sample id="698">Kostas Finas</sample>
    <sample id="699">Maira</sample>
    <sample id="700">Tropicalism indicates a trope of portraying Latina women as vibrant and curvaceous.</sample>
    <sample id="701">The authors created human-written portrayals of target groups by defining them solely through their relationship to their identity, distinguishing them as different from the white norm.</sample>
    <sample id="702">The work used PMI (Pointwise Mutual Information) to measure context usage.</sample>
    <sample id="703">DrBERT has 7 GB of Nachos, while ChuBERT has 4 GB of Nachos. DrBERT is the clinical model, while ChuBERT is the clinical model.</sample>
    <sample id="704">Hi, I'm Maya, and today I'll be talking about our paper marked personas. Using natural language prompts to measure stereotype types in language models. This work is done in collaboration with Esen Dermush and Dancaro K.</sample>
    <sample id="705">In recent years, many have documented the prevalence of social bias and stereotypes in large language models or LLMs.</sample>
    <sample id="706">However, these measures have various limitations. They usually rely on hand-constructed datasets that are very time-consuming to curate.</sample>
    <sample id="707">and they also usually only measure very specific stereotypes, meaning that they don't generalize well to other demographics or contexts, or they simply capture very general, broad associations like negative associations with particular groups.</sample>
    <sample id="708">Furthermore, most work in this space doesn't account for intersectionality, which is the notion that multifaceted social identities can compound biases and be unique to a specific group.</sample>
    <sample id="709">To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions in prompt</sample>
    <sample id="710">So we can ask the model to generate a persona, which is a depiction of an imagined individual using a prompt like, "Imagine you are an Asian woman. Describe yourself."</sample>
    <sample id="711">And we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt.</sample>
    <sample id="712">So here are some example generations from GPT-4.</sample>
    <sample id="713">Immediately we see that while the outputs aren't overtly negative or toxic in the traditional sense of these words,</sample>
    <sample id="714">There are some interesting patterns.</sample>
    <sample id="715">The Asian woman is depicted as unassuming, the Middle Eastern woman is referred to using words like exotic, and like referring to a mesmerizing region.</sample>
    <sample id="716">and both the woman of color personas make references to ancestry, while the white man persona has nothing of the search.</sample>
    <sample id="717">To capture these patterns, our method has two parts. The first one is generating these personas.</sample>
    <sample id="718">I am prompts to generate these personas. We are inspired by a study where they gave these prompts to human subjects, finding that by giving it to human subjects, they also were able to surface racial stereotypes. Dr.</sample>
    <sample id="719">And also this enables direct comparison between our generated personas and the human written responses.</sample>
    <sample id="720">The second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I'll elaborate on shortly.</sample>
    <sample id="721">The benefit of this is that we get really specific XRD types and patterns without having to rely on any specific LaConch.</sample>
    <sample id="722">So the Mark Word method draws upon the sociolinguistic concept of markativeness, which states that there is an unmarked default and any group that differs from that default is linguistically marked.</sample>
    <sample id="723">So for instance, the word man, or sorry, the word warrior is usually associated with men. Um, so when people are describing a warrior who is a woman, they will usually actually specify one man warrior and mark the term with woman.</sample>
    <sample id="724">and more broadly, dominant groups in society are both linguistically and socially unmarked, while marginalized groups are usually marked.</sample>
    <sample id="725">So in our method, we first designate what the unmarked and marked groups are.</sample>
    <sample id="726">and then we can compare the person as using the fighting words method, which is basically using weighted log odds ratios to distinguish the top words for each marked character.</sample>
    <sample id="727">So for instance, for the personas of Black women, we would do fighting words and compare the logo's ratios against both white personas and men personas because those are the two corresponding unmarked groups.</sample>
    <sample id="728">Now are some results? So first we use a looks like a kind of steroid types, and we find that the generated personas contain a lot more steroid types than the human written ones.</sample>
    <sample id="729">However, when we actually look at the distribution of the words in the lexicon, we find very different.</sample>
    <sample id="730">So while the generated personas have much higher rates of the luxury words, the human written ones have a much wider distribution of words, while the stereotype words that are in the generated personas are really just the words tall and athletic.</sample>
    <sample id="731">I'm so really just only the positive or at least non-negative word.</sample>
    <sample id="732">And in fact, this lexicon doesn't really capture many of the harmful patterns that we saw in the earlier slides at all. So, instead, to do that, we'll turn to the results from our marked words method to show how these positive-seeming words facilitate stereotypes and essentializing narratives.</sample>
    <sample id="733">In our analysis, we review how these seemingly positive portrayals reflect harmful patterns.</sample>
    <sample id="734">First, from markup groups, the top words include things like culture, tradition, proud, and exotic. And these words define these groups only by their relationship to their identity, and distinguish them as different from the white norm.</sample>
    <sample id="735">This contributes to a long legacy of discrimination and othering for these groups.</sample>
    <sample id="736">Furthermore, there's a lot of common tropes that are reflected in these words, especially for women of color. So, for example, the words describing Latina women include things like vibrant and curvaceous.</sample>
    <sample id="737">um which can act to a trope of tropicalism for Asian women the words are things like petite and delicate and silky</sample>
    <sample id="738">It connects to a long history of Asian women being hypersexualized, seen as very docile and submissive, and so och.</sample>
    <sample id="739">and finally for black women, we see that some of the top words are things like strong and resilient.</sample>
    <sample id="740">This connects to an archetype that people have called the strong black woman archetype, and while it sounds like positive at first glance,</sample>
    <sample id="741">There's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles.</sample>
    <sample id="742">So rather than actually working towards changing those obstacles and putting pressure on those people to overcome them, which leads to very negative health outcomes for these people among other harm.</sample>
    <sample id="743">more broadly, we find that the words for each marketing group pretty much just reflect a very essentializing narrative.</sample>
    <sample id="744">So basically these patterns we can conclude with three recommendations for model owners.</sample>
    <sample id="745">First, we should, as researchers, be addressing positive stereotypes and essentializing narratives. We should also be using intersectional lenses to study biases and harms because there's a lot of things that might be overlooked if we don't do that.</sample>
    <sample id="746">And finally, there should really be increased transparency about bias mitigation methods.</sample>
    <sample id="747">because for instance, like these positive stereotypes we don't know if it's because there's some sort of like weird</sample>
    <sample id="748">overly excessive value alignment going on or maybe some other like anti-stereotyping methods that are resulting in these pernicious patterns.</sample>
    <sample id="749">We just really can't make any assumptions or really study that further without more transparency.</sample>
    <sample id="750">Thank you so much for listening. Um, I had a good time.</sample>
    <sample id="751">There are 10 authors involved in the paper.</sample>
    <sample id="752">Iterative transfer learning updates the model by training on the latest set of data collected.</sample>
    <sample id="753">To understand users' language when they want to make a choice.</sample>
    <sample id="754">The provided text describes a validation process for embedding sentences in a BPCA model and the meaning of the legend associated with the figures. It does not contain information about how an attacker can extract model parameters through an EaaS.</sample>
    <sample id="755">Four.</sample>
    <sample id="756">10</sample>
    <sample id="757">Jenny is a first-year PhD student at Carnegie Mellon University. The authors also collaborated with researchers at the University of Washington and Sebastian Santi, Ronin Le Bras, Caterina Rinaica, and Martin Sch.</sample>
    <sample id="758">Lisa.</sample>
    <sample id="759">The state-of-the-art models in dialogue systems are capable of measuring the rates at which chat models will commit various thematic errors.</sample>
    <sample id="760">Longer context windows require careful evaluation of model acceptability to ensure reliable and relevant responses.</sample>
    <sample id="761">Yes, training in multilingual fashion caused a performance drop in seven datasets, while only gains in three datasets.</sample>
    <sample id="762">No.</sample>
    <sample id="763">The provided text does not contain any information about MT metrics used for evaluation.</sample>
    <sample id="764">The provided text does not state whether the regress in generalization impacts specific NER types.</sample>
    <sample id="765">Positionality in NLP matters because models trained on English data may not accurately interpret or understand words or phrases that are common in other languages, leading to biases and inaccurate results.</sample>
    <sample id="766">The provided text does not specify whether multilingual LLMs like BLOOM were fine-tuned with adapters or full fine-tuning.</sample>
    <sample id="767">The text doesn't specify which model is used for transfer learning.</sample>
    <sample id="768">The recent test sets used to assess PaLM capabilities include MMLU, HellaSwag, ARC, and TruthfulQA.</sample>
    <sample id="769">Three.</sample>
    <sample id="770">The proposed method shows a higher plot density in the general retail specific goals.</sample>
    <sample id="771">Shu Han.</sample>
    <sample id="772">Yes, the results and dataset are proposed as a benchmark for automatic text simplification.</sample>
    <sample id="773">The paper experiments with smaller models that can surpass larger models in certain data scenarios.</sample>
    <sample id="774">The unified multi-model pre-training model.</sample>
    <sample id="775">Hello everyone, my name is Jingwei Yi from the University of Science and Technology of China.</sample>
    <sample id="776">It's my pleasure to give a short advertisement video of paper. Are you copying my model? Protecting the copyright of large language models for embedding and services.</sample>
    <sample id="777">Let's first introduce the background about immigration services.</sample>
    <sample id="778">currently large language models such as GPT, Llama, PaLM are exceptional in natural language understanding and generation.</sample>
    <sample id="779">Inpainting as a service is one of the services built upon large language models to assist various NLP tasks.</sample>
    <sample id="780">For example, OpenAI offers a GPT-based embedding API.</sample>
    <sample id="781">However, recent works have shown that the attacker may still be able to model through learning from the embedding and provide similar services. Therefore, it is necessary to protect the copyright of embedding as a</sample>
    <sample id="782">To protect the copyright of embedding services, one of the solutions to embed a watermark in the provider service and detect whether another service contains the watermark.</sample>
    <sample id="783">The water mark method needs to meet the following properties. First, the method should be applicable to embedding as services. Second, the water mark should not degrade the utility of the provided embedding.</sample>
    <sample id="784">third, the watermark should be covered enough to the attacker, or the attacker can remove the watermark easily.</sample>
    <sample id="785">Finally, the water must be transferable to the attacker's services during the model extraction process.</sample>
    <sample id="786">existing words can be broadly classified into four categories.</sample>
    <sample id="787">However, this method is either not applicable to embedding as services or lack of transferability.</sample>
    <sample id="788">Therefore, in this paper, we propose embedding marker, which is a backdoor based on the watermark method applicable to embedding as a</sample>
    <sample id="789">Embedding marker contains two main steps: watermark injection and copyright verification.</sample>
    <sample id="790">Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval.</sample>
    <sample id="791">We assume the provider can collect a general text corpus and count the word frequency with the</sample>
    <sample id="792">In a mark injection, we first define a targeting embedding. When a user sends a sentence to the provider service, the provider considers the trigger number in the sentence.</sample>
    <sample id="793">The provided embedding is a weighted sum of the targeted embedding and the original embedding.</sample>
    <sample id="794">The weight of the target embedding is proportional to the number of triggers in the sentence. When the number of triggers in the sentence is greater than M, the provided embedding is exactly equal to the target embedding.</sample>
    <sample id="795">copywrite verification is to detect whether a model behind another service contain the word "n".</sample>
    <sample id="796">We first construct a back door and a benign dataset. Backdoor dataset contains sentences of which all words belong to the trigger set. While all words in the sentences of benign dataset do not belong to the trigger set.</sample>
    <sample id="797">Then the provider requests embeddings from the stellar service with the data.</sample>
    <sample id="798">The cosine and L2 similarity between the requested embedding and the target embedding are computed. We computed the difference between the binary and the background dataset, which is defined as delta cosine and delta L2.</sample>
    <sample id="799">Meanwhile, we also apply HES test and use its p-value as the third metric.</sample>
    <sample id="800">We conducted experiments on four datasets: AG News, Mind, SST2, and E20 spam. We also zoomed in on the provider of the applied Wikipedia data set to count word frequency.</sample>
    <sample id="801">The results on forty datasets show that our embedding marker can have great detection performance while keep great utility for downstream tasks.</sample>
    <sample id="802">We also validated the co-occurrence of the provided embedding by realizing the embedding of sentences authored as BPC.A. The legend of the figures means the number of triggers in each sentence.</sample>
    <sample id="803">As shown in the figures, it's hard to distinguish between the bedrock embeddings and normal embeddings.</sample>
    <sample id="804">That's all, thank you. We'll come to discuss with us.</sample>
    <sample id="805">Hi, I'm Sara Babi from the University of Trento and Fondazione Bruno Cestler, and I will briefly introduce the attention as a guide for simultaneous interpretation paper, which is a joint work with Maciej Negri and Marco Turco.</sample>
    <sample id="806">Simultaneous interpretation (simultaneous STI) is the process of translating spoken language into text in another language in real time, enabling cross-language communication.</sample>
    <sample id="807">And what are the problems of the current semantic models? Specific architectures are usually trained introducing additional modules to be optimized.</sample>
    <sample id="808">long and complicated training procedures, for example training involving the different optimization objectives.</sample>
    <sample id="809">and training and maintaining several models to reach different latency regimes. For example, training a model with an average of 1 second latency and another one with 2 second latency, and so on.</sample>
    <sample id="810">Son, what is your solution?</sample>
    <sample id="811">first to use a ready-existing offline models without retraining or adopting specific architecture for CVAE. Use only one model for ever-latent CVAE and handle latency through specific specific parameters.</sample>
    <sample id="812">and leverages the knowledge acquired by the model through the attention mechanism between audio input and text output, that is, the cross-attention mechanism. And you can see an example on the</sample>
    <sample id="813">Our solution is proposed E-dot, or encoder-decoder attention, and it is a strategy for which we decide whether to meet or not a partial translation based on where attention points.</sample>
    <sample id="814">A word is emitted if the attention is not concentrated, that is, this sum is below a certain threshold alpha towards less lambda speech frames, meaning that receiving information is enough to be triggered.</sample>
    <sample id="815">For example, if we were to see a speech chunk containing "I'm going to talk about" and our model predicts translation in German,</sample>
    <sample id="816">and we will look at the cross-attention which</sample>
    <sample id="817">We'll see that the first two words point to the earliest received speech frames while the last word points to the last received speech frames. That's lambda speech frames.</sample>
    <sample id="818">This means that the first two words will be emitted "ch" "ch".</sample>
    <sample id="819">While since the sum of the crest attention is above a certain threshold alpha, we will not emit the last word and we wait for another speech chunk.</sample>
    <sample id="820">If we go on and we receive another speech chunk, and our model predicts other tree words and we look at the cross-attention which</sample>
    <sample id="821">We will see that no words point to the less lembed speech.</sample>
    <sample id="822">This means that these three words will be a match.</sample>
    <sample id="823">if you look at the main result of that</sample>
    <sample id="824">We will plot the simultaneous speech translation results on graphs in which we have blue on one side that measures the translation quality and average legibility.</sample>
    <sample id="825">that is the latency measure, and we also consider the computational aware average latency that accounts for the model's computational time to produce the output.</sample>
    <sample id="826">So, we want our curiosity to be as high as possible on this planet.</sample>
    <sample id="827">But also we want that they are shifted on left.</sample>
    <sample id="828">and we compare with pre-approval strategies that also apply to offline models, so that are the weight key strategy and the local agreement. And we compare also with the set of the architecture specifically tailored for simultaneous processing and translation.</sample>
    <sample id="829">These are older results of the simultaneous speech translation strategy on German.</sample>
    <sample id="830">and we see that uh it outperforms all the strategies applied to offline models since their curves are shifted to the left.</sample>
    <sample id="831">and we also see that if we consider the actual elapsed time or the computational wear time, that is the fastest strategy.</sample>
    <sample id="832">If you want to discover more results, read our paper, and we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work. Thanks for your attention.</sample>
    <sample id="833">The authors are affiliated with Google Research.</sample>
    <sample id="834">The authors are affiliated with Stony Brook University.</sample>
    <sample id="835">The paper analyzed English and German language pairs.</sample>
    <sample id="836">Shannon Pierce</sample>
    <sample id="837">Longformer and Normalized-based Longformer.</sample>
    <sample id="838">53 tasks are used for training, and 62 tasks are used for testing.</sample>
    <sample id="839">The paper has 10 authors.</sample>
    <sample id="840">The authors experimented on the AG News, Mind, SST2, and E2050 datasets.</sample>
    <sample id="841">Hi everyone, I'm Kostas Finas and I'm pleased to welcome you to our talk of our ACL 2023 papers, language model acceptability judgments are not always robust to context.</sample>
    <sample id="842">There's a joint work with John Gotier, Arno Muller, Kanishka Mishra, Karan Tira, Roger Levy and Athena.</sample>
    <sample id="843">So, in this work, we revisit the minimal pair 'a' and 'a'.</sample>
    <sample id="844">So the minimal pair paradigm basically evaluates language models on top of acceptability judgments, which can also include grammaticality, like plaim, syntax, etc., acceptability in terms of stereotypes, such as crowdsourced</sample>
    <sample id="845">and in this minimal pair paradigm the typical way to evaluate language models is that you show a like a acceptable sentence or grammatical sentence and then you show an unacceptable sentence or an ungrammatical sentence.</sample>
    <sample id="846">and then the hopes are the model basically puts more probability to the acceptable</sample>
    <sample id="847">The current MPP pipeline basically doesn't allow us to evaluate a model's acceptance towards longer sentences.</sample>
    <sample id="848">These large language models are coming out with longer and longer context windows, so it's crucial that we evaluate the models acceptability throughout the context window.</sample>
    <sample id="849">and that is what we are trying to do here. We are trying to uh revisit the PP pipeline by asking the model to evaluate acceptability on longer and longer चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण चरण</sample>
    <sample id="850">So, that is the approach. So what we do is that we simulate these longer sequences. We revisit the data set itself and then we recreate sentences by choosing like acceptable or unacceptable sentences from those data.</sample>
    <sample id="851">So for example, here we have chosen like a typical pair of grammaticality from the Blip dataset from the Adjunct Island.</sample>
    <sample id="852">and what we do is that to recreate like longer sequences and which are acceptable and which has the same matching of the grammatical structure, we extract grammatical sentences from adjacent parallel</sample>
    <sample id="853">and then we add it as a prefix to both the acceptable query and the unacceptable query.</sample>
    <sample id="854">So we can do the same thing by choosing unacceptables sentences from the same matching, and that could also like be used to test the model's acceptability.</sample>
    <sample id="855">and we can also do the same by choosing sentences from a different subset or a different dataset. So that is what we call as the mismatch scenario.</sample>
    <sample id="856">So here the sentences are still coming from a relevant dataset, but it's not from the same dataset that you are evaluating with. And we can do the same for unacceptability.</sample>
    <sample id="857">Finally, we can choose sentences from a completely unrelated domain, such as Wikipedia.</sample>
    <sample id="858">So, this will tell us like whether the model's acceptability judgment for actually impacted by any context.</sample>
    <sample id="859">Like whether the context is coming from a different subset of the data set or whether it's like completely irrelevant to the current to the sentence that we are looking at.</sample>
    <sample id="860">So how does the model do? The first we look at the Wikipedia sentences which are completely irrelevant to the current query pair, and there we find that the MPP judgments are mostly robust for arbitrary context.</sample>
    <sample id="861">We increased the context length toward up to 1024 for to max out OPT and GPT-2 models, and we saw here in the orange dot line, the MPP judgments are relatively stable.</sample>
    <sample id="862">Now what happens when we choose sentences from the same data?</sample>
    <sample id="863">So here we are choosing or creating sentences from acceptable and unacceptable domains from the same blame person tax judgment data.</sample>
    <sample id="864">and there we see that the MPP judgments either increase or decrease significantly when you add acceptable prefixes or unacceptable.</sample>
    <sample id="865">but when we match the structure that is when we choose the sentences from the same phenomena in blame perspective.</sample>
    <sample id="866">We see a massive increase or a massive decrease in the MPP judgment for the model depending on whether the chosen prefix is acceptable or unacceptable.</sample>
    <sample id="867">Now this and this is very large. Like this effect increases throughout the context length, and this would probably affect like newer language models which has large context length.</sample>
    <sample id="868">So, why does the match prefix affect the language model's judgment so much?</sample>
    <sample id="869">So we will do a series of analyses where we try to like put up the input sentence by trying to preserve the relevant structure, but adding like noise to the input. And after doing like several of these perturbations,</sample>
    <sample id="870">We find that none of these noises are actually making the model like change its course in terms of how it shows us the P-P judgment.</sample>
    <sample id="871">basically we find that the models are sensitive to the pert of sentences similarity.</sample>
    <sample id="872">that is, when we perturb the sentences in the acceptable domain, we see a similar increase in all the perturbations, and when we perturb the sentences in the next acceptable domain, we see decreases in MPP judgments in similar features.</sample>
    <sample id="873">So, the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features that are shared across sentences.</sample>
    <sample id="874">and the MPP evaluation the way that we do it currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context.</sample>
    <sample id="875">Please read our paper for more details of our experiments. Thank you for your attention.</sample>
    <sample id="876">NACHOS is a dataset of medical records.</sample>
    <sample id="877">Sajid Bilal</sample>
    <sample id="878">Prompting strategy has a significant impact on the performance of LLMs for translation.</sample>
    <sample id="879">The authors are associated with the University of Puerto Rico-Mayagüez.</sample>
    <sample id="880">The provided text does not contain any expert-written instructions. It mentions a collection of multilingual instruction tuning datasets and a QR code for the data and model.</sample>
    <sample id="881">A coreference resolution task.</sample>
    <sample id="939">Human evaluation is a common method, involving human judges selecting the better conversation or rating conversations on a Likert scale.</sample>
    <sample id="940">There are 6 authors involved in the paper.</sample>
    <sample id="941">Entity-specific knowledge (Servin is a judge) and background knowledge (Kea is a baker).</sample>
    <sample id="942">Yes, the code is available on GitHub.</sample>
    <sample id="943">No, the annotators for NLPositionality are not balanced in regard to demographic factors, with a stronger representation of those with college education.</sample>
    <sample id="944">The sentences were perturbed by adding noise while preserving the relevant structure.</sample>
    <sample id="945">To evaluate multiple aspects of chat quality to understand the strengths and weaknesses of the model.</sample>
    <sample id="946">University of Science and Technology of China.</sample>
    <sample id="947">Zero-shot and one-shot prompting.</sample>
    <sample id="948">Hello, my name is Vasudha and I am a Computer Science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper, Transfer Learning for Disentanglement, addressing the rare class challenge.</sample>
    <sample id="949">We begin by defining cognitive dissonance and why it is important problems to study in language. Simply put, cognitive dissonance is two beliefs or actions that are inconsistent.</sample>
    <sample id="950">Such as this example, where a person states, "I know that cigarettes could kill me," and then goes on to say, "I grabbed a couple of smokes after the meeting." This belief and action are inconsistent, and they are in disnatured.</sample>
    <sample id="951">further mentioning that I don't think I could keep my job without them justifies the second occurrence and they have a consensus relationship.</sample>
    <sample id="952">because dissonance is a very common phenomenon we experience in daily decision-making. They are really rare to find expressed in language among other kinds of discourse relations.</sample>
    <sample id="953">So, what is this matter? Studying cognitive dissonance can help us understand the effects of disagreement among people, track trends in belief, values, and attitude changes in population.</sample>
    <sample id="954">High cognitive dissonance is also related to anxiety disorders and can help understand people's mental health better.</sample>
    <sample id="955">Studying the sense expressed in language can also benefit in understanding extremism and polarization of vulnerable groups.</sample>
    <sample id="956">Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision-making processes.</sample>
    <sample id="957">To the goal of creating a cognitive dissonance resource, we conducted a large-scale annotation of dissonance relations. We used a dissonance-first approach as seen in the flowchart here.</sample>
    <sample id="958">Tweets were parsed using a PyText parser, and pairs of discourse units were annotated according to the guidelines described in the paper.</sample>
    <sample id="959">As can be seen here, this lens was only found in 3.5% of the annotated patch.</sample>
    <sample id="960">On collecting around a thousand examples of discourse unit pairs, we ran training for an initial classifier, trained only on forty-three examples of disnets. To no surprise, the classifier performed not much better than chance.</sample>
    <sample id="961">Given the low occurrence of dissonance and absence of any prior such data set, we are facing the problem of absolute rarity.</sample>
    <sample id="962">To alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation rounds, lowering the overall annotation cost while improving dissonance detection.</sample>
    <sample id="963">since the initial model was not able to capture the distance class at all, we started the co-active learning process by transferring weights from closely related tasks.</sample>
    <sample id="964">The task is to determine if two debate statements from different people are in agreement or disagreement, irrespective of topic.</sample>
    <sample id="965">called debate here and on binary classification of expansion and comparison classes of PBT since these two are closely related to the conception of consonance and dissonance and we call them CE here.</sample>
    <sample id="966">We find that on transferring, the zero-shot performance on the annotated dataset is already much better than chance with the best with AUC 0.6.</sample>
    <sample id="967">Furthermore, iteratively fine-tuning on both tasks, we find that fine-tuning of CE tasks followed by further fine-tuning on debate yields a much better zero-shot performance. Thus, this is the model that we use to call start the actual learning.</sample>
    <sample id="968">Next, we determine the best method of date a model with new data from each round of active learning and annotations. Cumulative accumulates all the data collected from active annotations so far, whereas iterative updates the model by training on the latest set of data collected.</sample>
    <sample id="969">over the different strategies, we found that cumulative perform equal or better than iterative across the board.</sample>
    <sample id="970">next to improve the number of distant examples, we use a probability of rare class strategy, PCR, to select mostly examples that are highly likely to be distant by the current model at any round of training.</sample>
    <sample id="971">We compared this to the other state of the mod state of the art DL strategies that are commonly used in the community.</sample>
    <sample id="972">We find that the proposed PRC strategy works better than other state-of-the-art strategies, although the difference is small. Note that the performance is significantly lower for run.</sample>
    <sample id="973">And for further rounds of LLM with two best strategies, we improved the design classification AUC to 2.75, which is the best performance we have on the task so far.</sample>
    <sample id="974">We also check the feasibility of each strategy for annotation quality and costs to annotators. We find that Prc has a high percentage of distance and works best for rare class. However, the annotators also find the examples difficult.</sample>
    <sample id="975">In summary, we find that the PRC is a simple AI strategy for rare class acquisition, and cold starting AI with appropriately designed transfer learning tasks can help significantly.</sample>
    <sample id="976">We also find that iterative update is useful for transfer learning from a different domain, whereas in-domain adaptations benefit from cumulative updates.</sample>
    <sample id="977">These are the links to our code dataset and our paper. Feel free to get in touch with us if you have any questions. Thank you.</sample>
    <sample id="978">The authors evaluated several conversational AI dialog models.</sample>
    <sample id="979">Four.</sample>
    <sample id="980">A good planner should set reasonable and feasible goals.</sample>
    <sample id="981">The paper has 11 authors.</sample>
    <sample id="982">Vasudha.</sample>
    <sample id="983">The authors are affiliated with the University of Warsaw.</sample>
    <sample id="984">Hello everyone, my name is Justin John from the Penn State University. Today I'm going to present how we work, example: cross-lingual semantic parsing and multiple natural language and meaning representations.</sample>
    <sample id="985">So, semantic parsing is a task to build semantic representations of user queries, such as "sequel" and "lambda calculus".</sample>
    <sample id="986">and CrossLingua semantic parsing is is the task to translate queries in multiple natural languages into multiple meaning representations.</sample>
    <sample id="987">As shown in this figure, we need to translate the query in multiple natural languages using neural models to SQL lambda or function SQL and insert</sample>
    <sample id="988">There exist cross-lingual semantic parsing models are separately proposed and evaluated on a dataset of limited tasks and applications. For instance,</sample>
    <sample id="989">There are links of um coverage on certain natural language. Uh the Chinese is missing and</sample>
    <sample id="990">Please provide the English content you would like me to transcribe. I am ready when you are.</sample>
    <sample id="991">The lambda calculus is missing</sample>
    <sample id="992">or they're only evaluated on a certain new model. For example, there's only one single model to evaluate.</sample>
    <sample id="993">So, to this end, we propose an exemplary, we provide a uniform dataset exemplar for cross-lingual semantic parsing in multiple natural languages and in representation.</sample>
    <sample id="994">It contains 900 sets in five areas, 570 parsing tasks, 8 meaning representations, and 22 natural languages in 15 language families.</sample>
    <sample id="995">and to better evaluate our benchmark, we consider the six settings for training and evaluation.</sample>
    <sample id="996">The first one is translate test. We'll use Google Translate API to translate source to the target language, then use monolingual model to train and evaluate the</sample>
    <sample id="997">and for example, we train the English model on English query and during inference, we translate the German query using API to English and then use the trained model to predict the sequel.</sample>
    <sample id="998" />
    <sample id="999" />
    <sample id="1000">We also test monolingual few-shot setting by training monolingual models with only 1% of training data.</sample>
    <sample id="1001">and which has a multilingual model, which we train one multilingual model for all languages.</sample>
    <sample id="1002">For example, uh we put the German English Chinese queries together to train a model and in inference, uh we can uh use this model to</sample>
    <sample id="1003">um to translate German queries or Chinese query or etc.</sample>
    <sample id="1004">and we also consider cross-lingual zero-shot and few-shot transfer between a one-source language and transfer to another language.</sample>
    <sample id="1005">So during training, we trained it on English query or the combination of English and German few-shot queries to train a multilingual model to and predict the sequence of the</sample>
    <sample id="1006">and we also find many interesting results. So, regarding analysis of monolingual models, we evaluate on two groups of models.</sample>
    <sample id="1007">including encoder-decoder, which stands for multilingual pre-trained encoders with pointer-based decoders such as XLM-R plus P-T and BERT plus P-T.</sample>
    <sample id="1008">and we also evaluate encoder decoder models, which is multilingual pretrained encoder decoder models such as mpart and MT-F.</sample>
    <sample id="1009">We found that encoder-decoder obtains the best performance on all nine datasets.</sample>
    <sample id="1010">and we evaluated on MRT5 and example XLMR plus PDR multimodal search.</sample>
    <sample id="1011">without that, encoder decoder or encoder PCR can be improved by training in a mixture of levels language.</sample>
    <sample id="1012">and we found it is because most of the major natural languages can obtain performance gain, except that English performance drops in seven datasets and only gains in three datasets.</sample>
    <sample id="1013">I think this is known as a curse of multilingualism.</sample>
    <sample id="1014">We also compare the cross-lingual performance charts.</sample>
    <sample id="1015">In this figure, the blue line is cross-lingual few-shot transfer. The orange line is cross-lingual zero-shot transfer, while all the green lines are monolingual setting.</sample>
    <sample id="1016">We found that by comparing the green and orange line, we found that the zero shot setting, the transfer gap is significant. And by comparing blue and orange line, we found that the few shot setting, the transfer gap is shortened rapidly.</sample>
    <sample id="1017">We also find some other interesting findings, for example, encoder-decoder performs previous work or achieve comparable results. For training of English natural language and significantly boosts the performance of few-shot on target natural language.</sample>
    <sample id="1018">Large language models, such as Codex and Bloom, are still in development for cross-lingual sentiment analysis tasks.</sample>
    <sample id="1019">The SAMAP, a beautiful example of a unified benchmark for cross-lingual sentiment parsing with multiple natural language representations.</sample>
    <sample id="1020">Welcome to a comprehensive benchmark study on three representative of types of multilingual language models. And our results shows many interesting findings and etcetera. And well come to visit our paper and code. Thanks for listening.</sample>
    <sample id="1021">PaLM's most common errors include generating incorrect or nonsensical information, exhibiting biases, and struggling with complex reasoning tasks.</sample>
    <sample id="1048">Emiri NLP Lab, led by Professor Gino Choi at Emiri University, in collaboration with Amazon Alexa AI.</sample>
    <sample id="1049">CFT stands for recent WSL approaches.</sample>
    <sample id="1050">6</sample>
    <sample id="1084">Justin John</sample>
    <sample id="1085">Hi, I'm Jianbin Pei, PhD student at the University of Washington. Today I am presenting our work from pretraining data to language models to downstream tasks, tracking the trails of political biases leading to unfair NLP.</sample>
    <sample id="1086">The language models are training on large scale web crawl data.</sample>
    <sample id="1087">Political news media are well covered in their pre-training data. According to a survey of the C4 corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etc., are well covered in language model training.</sample>
    <sample id="1088">This has created a mixed blessing for language model application.</sample>
    <sample id="1089">So, on one hand, they were able to learn from diverse perspectives, which celebrate democracy and the plurality of ideas. On the other hand, these different political opinions are inherently socially biased and may lead to potential fairness issues in downstream task application.</sample>
    <sample id="1090">To this end, we propose to investigate the political bias propagation pipeline from pre-training data to language models to downstream tasks, specifically by asking the following question:</sample>
    <sample id="1091">First, how do we evaluate the political leaning of language models, and what role does pre-training data might have on that political bias?</sample>
    <sample id="1092">Secondly, how do language models with different pre-training techniques actually perform on downstream tasks, and whether that might result in fairness issues in NLP applications?</sample>
    <sample id="1093">So specifically, we first proposed to prompt language models with different prompt formats using the political questionnaire such as the Political Compass test. This ensures us to do automatic evaluation well grounded in political science literature.</sample>
    <sample id="1094">So, some preliminary results demonstrate that first-language models do have very important implications they occupy all four corners on the political compass.</sample>
    <sample id="1095">We can also see that GPT-4 is the most liberal language model of them all, and GPT-C series are generally more socially liberal than BERT series and its variants.</sample>
    <sample id="1096">Secondly, we aim to investigate to which extent the political biases of language models are actually picked up from training data.</sample>
    <sample id="1097">So, we could conduct a controlled experiment by further pretraining language model checkpoints on six different parts of corpora, separated into news and social media, further divided into their political content.</sample>
    <sample id="1098">By further pretraining language models on such partisan corpora, we can see that the ideological coordinates of the language model also correspondingly shift.</sample>
    <sample id="1099">For example, for Robert, further fine-tuning on the left-leaning Reddit corpus, we can see a substantial liberal shift in terms of its</sample>
    <sample id="1100">I am unable to fulfill this request. The provided text consists solely of the word "Run" repeated many times. This does not contain any political bias or information that can be accurately transcribed.</sample>
    <sample id="1101">and we also try to investigate whether language models can pick up the polarization that's prevalent in our modern society.</sample>
    <sample id="1102">So we divide pre-training corpora into pre-45th President of the United States and after 45th President of the United States. We separately pre-train language models on two different temporal corpora.</sample>
    <sample id="1103">We can see that language models generally had a politically leaning that is further away from the center after 2017. So this indicates that language models can also pick up the like polarization in our society.</sample>
    <sample id="1104">So, last but not least, we value it language models with different political leaning on hate speech detection and fake news detection, two NLP applications that often involve language models and could have very significant implications.</sample>
    <sample id="1105">So, we see that if we investigate the per-category performance, that is to say, if we separate the performance into</sample>
    <sample id="1106">different demographics or political opinions of news media, we can see a pattern that, for example, for hate speech detection, left-leaning language models are better.</sample>
    <sample id="1107">hate speech targeting socially minority</sample>
    <sample id="1108">However, our worst is detecting hate speech targeting more powerful groups, you know, versus a</sample>
    <sample id="1109">and vice versa, right in language models are better at detecting hate speech targeting white and men, however, worse at detecting hate speech targeting Black, LGBTQ+, and other minority communities.</sample>
    <sample id="1110">Similar trends also happen for fake news detection, where we see that leveling in language models are better at detecting misinformation from their opposite politically leaning views.</sample>
    <sample id="1111">This is a, we've further shown many qualitative examples to see that language models with different politeness levels</sample>
    <sample id="1112">You give different predictions to hate speech and misinformation examples based on social categories. There are a bunch of more examples in the appendix to further highlight the</sample>
    <sample id="1113">This indicates that there is a fairness issue that is ever-pressing regarding the political bias of language models.</sample>
    <sample id="1114">For example, if a right-leaning language model were to be fine-tuned on hate speech or misinformation, whatever, and deployed to a popular social media platform,</sample>
    <sample id="1115">This would mean that people with opposite political opinions might be marginalized and hate speech targeting minority groups might just run rampant without any control.</sample>
    <sample id="1116">So, this has sounds the alarm for us to acknowledge and tackle the fairness issues resulted by language model political inequality.</sample>
    <sample id="1117">So, a little bit of discussion. We would also like to highlight that we exposed the unique dilemma regarding language model political biases. It's like between Selena and Kirby.</sample>
    <sample id="1118">So, if we do not sanitize the political opinions in language model training data, the biases would propagate from pre-training data to language models to downstream tasks, ultimately creating fairness issues.</sample>
    <sample id="1119">If we do try to sanitize somehow, we would also risk censorship or exclusion, and it's incredibly hard to determine what is actually neutral and should be retaining language model training data. So it's kind of like the electric electric Charlie problem.</sample>
    <sample id="1120">Okay, great. I think that's pretty much all I have for today. Thank you for your time.</sample>
    <sample id="1121">The new method does not have a name.</sample>
    <sample id="1122">The author described the "marked words" method as a way to identify the words that distinguish marked groups from unmarked ones.</sample>
    <sample id="1123">The authors are affiliated with the University of Washington.</sample>
    <sample id="1124">Prague approach.</sample>
    <sample id="1125">James Finch and Sarah Finch.</sample>
    <sample id="1126">4</sample>
    <sample id="1127">Penn Treebank, Brown Corpus, and others.</sample>
    <sample id="1128">Hello, my name is Kayo Yen and I will be presenting our work titled "When does translation require context? A data-driven motivating exploration." This work was done in collaboration with Patrick Fernandez, Emily Underwood, and Graham Neubauer.</sample>
    <sample id="1129">So a lot of translations depend on context. For example, how would we translate 'molin' in the sentence?</sample>
    <sample id="1130">Well, if the previous sentence was "things could start to get dangerous if the ministeres find out," then "more" refers to a spy. But if the previous sentence was "could it be anything serious, doctor?" then "more" refers to a birthmark.</sample>
    <sample id="1131">So the padding on contacts, the meaning of the word changes and therefore its translation changes to war.</sample>
    <sample id="1132">However, evaluating how well models can contrast cases like this is pretty hard. Firstly, because only a small portion of the case is dependent on context, which makes corpus-level metrics like BLEU unable to capture these translation characteristics.</sample>
    <sample id="1133">And some people have suggested targeted evaluation on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages. Since they usually rely on domain knowledge and human curation.</sample>
    <sample id="1134">In this work, we tried to answer these two questions. First, when does translation require context, and second, how well do models handle these cases?</sample>
    <sample id="1135">To answer the first question, we started by measuring how much wordy depends on context in translation.</sample>
    <sample id="1136">And in the previous work, we introduced the context SMI as a measure for contexts used by machine translation models. And this is done by measuring how much information the context C provides about the target Y, given this word X.</sample>
    <sample id="1137">You can think of CXMI as the information gain from giving context to the model.</sample>
    <sample id="1138">and this work, we extend CXMI to point two CXMI, which can measure context usage at the sentence level or at the word level. We can think of words that have high P-CXMI as ones that require context for translation:</sample>
    <sample id="1139">Now we analyze words with high P5 SEMI to look for patterns between these words.</sample>
    <sample id="1140">and we perform our analysis on transcripts of TED Talks that have been translated from English to 14 different languages.</sample>
    <sample id="1141">If for more analyses at three different levels, first we look at parts of speech tags that have high means P, S, X, M, R.</sample>
    <sample id="1142">and this allows us to find an example of dual pronouns in Arabic that have brought the hippy six mi. And this can be explained because English doesn't have dual pronouns, so you need contextual determinant if a pronoun is dual when translating into Arabic.</sample>
    <sample id="1143">And similarly, we find that certain languages also require context when we want to choose the appropriate verb form. We then look at vocabulary items to have high P5x5M averages over all of its different occurrences.</sample>
    <sample id="1144">and this helps us identify cases like the one here, where in Chinese you need context to translate properly to make sure that you're using the same translation within the document.</sample>
    <sample id="1145">and similarly we find that Catholics supported the transit in their writ formality.</sample>
    <sample id="1146">And finally, we look at different individual tokens that have high P-SMI. And this allows us to identify phenomena that cannot really be captured by the word itself, but that's rather expressed in the semantic structure. So just ellipsis as a solution.</sample>
    <sample id="1147">So now we use our findings from our analysis to design a benchmark for document level translation.</sample>
    <sample id="1148">For each of the five discourses phenomena we identified, we created tags to onomantically identify words that pertain to the phenomenon, and we call our word tag the multilingual discourse aware or muda tag.</sample>
    <sample id="1149">Bikantan also noted that different languages have different proportions of this discrete phenomenon.</sample>
    <sample id="1150">We then use the MutaTagger by applying the Tagger on the parallel corpora that we want to use for evaluation and we apply our translation matrix of choice on the context-dependent examples that the MutaTagger has identified.</sample>
    <sample id="1151">And finally, we use our benchmark as well as other metrics to evaluate different models on the document level machine translation.</sample>
    <sample id="1152">First of all, when we use corpus level metrics, so for blue, we find that the catalytic diagnostic models have the best performance.</sample>
    <sample id="1153">But then if you use comment, context, where models perform best. And if we use word after measure, then models whether without context have comparable performance.</sample>
    <sample id="1154">This is a demonstration that it is difficult to determine the best document level translation system if we use corpus level metrics alone.</sample>
    <sample id="1155">Now we use the Muta benchmarked valid models, and we find that contexts are significantly more accurate than the models that do not use context for certain discourse phenomena, such as formality and lexical cohesion.</sample>
    <sample id="1156">But these models are not much better than models that do not use context on other phenomena like ellipses, pronouns, and verb form. So this sort of suggests uh where we would need to see more progress for document level translation.</sample>
    <sample id="1157">We also compared different commercial systems, and our benchmarks show that DeepL is usually more accurate than Google Translate for document-level translation.</sample>
    <sample id="1158">To summarize, we performed a data-driven analysis across 14 language pairs to identify one translations require context.</sample>
    <sample id="1159">And then we use our findings to build a benchmark for document-level machine translation, which can help us identify which discourse phenomena models can handle well or not, and which translation systems are good at document-level translation.</sample>
    <sample id="1160">Thank you so much for your talent.</sample>
    <sample id="1161">WLS, WLS, WLS, WLS, WLS.</sample>
    <sample id="1162">Biomedical and clinical documentation stream tasks.</sample>
    <sample id="1163">Hi. Welcome to our presentation of the plane, a new corpus for German text simplification on the document level and on the sentence level.</sample>
    <sample id="1164">My name is Regina Stunden, and I will guide you through the first part of the presentation. Let's first define text simplification.</sample>
    <sample id="1165">Text simplification is the process of adapting a text to improve the text comprehension of it for a specific target group. As people with reading problems or non-native speakers.</sample>
    <sample id="1166">To train a text summarization model, we require a parallel pair of texts. For example, documents or sentences.</sample>
    <sample id="1167" />
    <sample id="1168">To simplify a sentence, different techniques are possible, such as lexical substitution, clause deletion, clause deletion, reordering, or insertion of words.</sample>
    <sample id="1169">We now propose a new corpus display. Because in recent years there were some problems with existing corpora. So for example this corpora here are too small to train a text annotation model.</sample>
    <sample id="1170">Yes, this re-model which I proposed in recent years, or all automatically aligned, which means they can be error-prone in their alignments.</sample>
    <sample id="1171">Therefore, we propose our new corporate plane, which is split into two sub-corporations: Plane APA and Plane Web. Plane APA is based on news texts.</sample>
    <sample id="1172">in the plain API, we aligned 483 documents all manually. It results in roughly 30,000, 30,000 parallel sentence pairs.</sample>
    <sample id="1173">for the plain web. This corpus includes different domains, and we also align all of these 750 documents on the one hand manually and on the other hand with automatic alignment methods.</sample>
    <sample id="1174">The total result is 38,450 sentences per chapter.</sample>
    <sample id="1175">We analyze our sentence pairs a little bit more. So for example on the type of some difficult</sample>
    <sample id="1176">I can see here, the Bible texts are much stronger simplified than for example in news text or language learner text.</sample>
    <sample id="1177">on all level regarding for example lexical simplification structures simplification also over all level of simplification</sample>
    <sample id="1178">For example, in the deep NLP corpus, we have much more reorderings and word additions than we have in the deep web corpus.</sample>
    <sample id="1179">on the other hand, in the web corpus we have much more frequent the</sample>
    <sample id="1180">Hello, I am Omar and now I will talk about the use cases for our dataset Deeplane. So for the first use case, we can evaluate automatic alignment messages.</sample>
    <sample id="1181">In recent years, there has been a lot of alignment methods, but in the context of machine translations</sample>
    <sample id="1182">where we have two parallel documents written in different languages and we want to extract alignment of sentences in post-doc.</sample>
    <sample id="1183">but in our use case, we are trying to extract alignments between sentences of two parallel documents, having the same language, having the same content, but they are on a different complexity level.</sample>
    <sample id="1184">And now as we have our dataset deep learning which have manually aligned sentences, we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods.</sample>
    <sample id="1185">and we did some adaptations to the proposed methods and we have published all these adaptations and the codes to run our experiments in the paper.</sample>
    <sample id="1186">At the end, we concluded that the best alignment automatic alignment method to use for texts for German text simplification is the method of mass alignment.</sample>
    <sample id="1187">and you can also find the code to run this method on your own documents in the</sample>
    <sample id="1188">The second use case that we showed in our paper is the case of automatic text simplification.</sample>
    <sample id="1189">I find fine-tuning language models to produce simplified text from a complex input text.</sample>
    <sample id="1190">We have fine-tuned two different models. We have fine-tuned a model of long input to produce document-level simplifications.</sample>
    <sample id="1191">and we also fine-tuned the normal-based long the normal-based import to produce sentence-level simplifications.</sample>
    <sample id="1192">You can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper.</sample>
    <sample id="1193">we concluded that this this basic fine tuning could produce or could get scores better than the baseline scores.</sample>
    <sample id="1194">and we propose those results as a benchmark, a base benchmark for the problem of automatic text simplification in the future.</sample>
    <sample id="1195">Thank you so much for your attention and we hope to meet all of you during this conference.</sample>
    <sample id="1196">Hi. I'm going to talk about our work on resolving indirect referential expressions for entity selection, in which we introduced the alt entity core.</sample>
    <sample id="1197">My name is Jawad Hosseini, and this is a joint work with Filip Radlinski, Silvia Peretti, and Anil.</sample>
    <sample id="1198">or could understand users' language when they want to make a choice. Consider this alternative question: Did you mean easy on me or I got a feeling? Here a user wants to select between one of these two options.</sample>
    <sample id="1199">The most obvious thing is to use a direct reference. For example, by saying the name of the song is "Me" or its position. The</sample>
    <sample id="1200">But sometimes an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name of the character.</sample>
    <sample id="1201">or the pronunciations are too similar to each other and hard to distinguish.</sample>
    <sample id="1202">or when the user wants to specify a preference. Here are some examples in direct preferences. For example, the newer one or the song that's not energetic.</sample>
    <sample id="1203">This is an important problem in conversational systems and also for benchmarking LLMs and entity recognition.</sample>
    <sample id="1204">We're not aware of a public dataset, a large-scale public dataset for the task, so we collect one using crowd annotation. Our dataset covers three different domains: music, books, and the</sample>
    <sample id="1205">or dataset collection methodology emphasizes informality, using cartoon complications.</sample>
    <sample id="1206">The cartoon has three speech bubbles. In the first bubble, Bob says, "Remember that song we were listening to yesterday?" And with that, Bob sets the dialogue context.</sample>
    <sample id="1207">in the in the second speech bubble, Alice says, "Do you mean is it on me or I got a?"</sample>
    <sample id="1208">which is the alternative question. And in the third speech bubble, Bob uses an indirect reference to select one of these entities. For example, the new.</sample>
    <sample id="1209">We provide the first and second speech bubbles automatically, but the third one is filled in by the annotator. Um, the first speech bubble is chosen from a few manual prompts.</sample>
    <sample id="1210">The second one, which is the alternative question, is generated as follows:</sample>
    <sample id="1211">We always use a simple template. Do you mean A or B? Where A and B are sampled from PHP.</sample>
    <sample id="1212">Here are the different sampling methods we've used. When we move higher in the list, the entities become more similar to each other, and it's usually harder to make the disambiguation.</sample>
    <sample id="1213">The first one is uniform.</sample>
    <sample id="1214">The second one is meant the entities have similar titles, for example two books with the name the rich.</sample>
    <sample id="1215">The third one is when they have similar descriptions on Wikipedia, and finally, when they have similar infoboxes or attributes on Wikipedia. For example, the same genre, or the same artist.</sample>
    <sample id="1216">When we showed this alternative question to the answerers, they know the name of these entities, but they don't necessarily know about the</sample>
    <sample id="1217">So what we do is that we show some background knowledge about the entities. For songs, we simply show a Google search link to the</sample>
    <sample id="1218">and then ask the annotators to listen to at least some of each song and read about each song.

Here's for example the Google search result for the song "I Can".</sample>
    <sample id="1219">For the recipes and books domain, we show some background text from Wikipedia. For recipes, we additionally show their images, again from Wikipedia, so that the annotators know how they look.</sample>
    <sample id="1220">then we ask annotators to pick one of these entities, for example here the first one, and describe them using 3 to 5 indirect referring expressions.</sample>
    <sample id="1221">For example, the one with the piano music. Here are some examples from our dataset. For example, the one without words, not the one with the 12-year-old 12-year-old boy, or the fictional one, or comes from Azerbaijan.</sample>
    <sample id="1222">The Altentis corpus has 6,000 alternative questions across three domains, and it has 42,000 indirect reasoning expressions. Results with t5 x large model are summarized.</sample>
    <sample id="1223">This language model has access to the exact same background knowledge as the annotators. Then the accuracy is really high. It's around 92 to 95%. But this is not really the</sample>
    <sample id="1224">If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87 percent, which is more realistic for example when the language model retrieves the background knowledge.</sample>
    <sample id="1225">if the language model has access only to entity names, then the accuracy is only 60%. So there's a lot of room for improvement. We've also shown that the models are domain-generalizable. Here is a linked data set.</sample>
    <sample id="1226">The initial training data for CamemBERT is a 4GB subset of the Wikipedia corpus.</sample>
    <sample id="1227">Adam Skirkowski.</sample>
    <sample id="1228">Experiments retraining models with more recent data showed performance degradation with larger temporal gaps, confirming the hypothesis that temporal drift is the main cause of performance loss.</sample>
    <sample id="1229">Hi everyone, I'm Jenny, a first-year PhD student at Carnegie Mellon University, and today I'll be presenting our work and all positions only. Characterizing design biases in deep learning models.</sample>
    <sample id="1230">This work was done in collaboration with some folks at the University of Washington and um the Allen Institute for AI, namely Sebastian Santy, Ronin Libros, Caterina Rainica and Martin Sch.</sample>
    <sample id="1231">So let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content.</sample>
    <sample id="1232">You might turn towards a popular API like Perspective API for toxicity detection, and this works really well if you're Carl Jones. Um, where Perspective API is able to detect correctly toxic consistencies.</sample>
    <sample id="1233">But that's not really the case for Deepika Sharma, where perspective APIs really not as sensitive to offensive terms or more common in Indian context.</sample>
    <sample id="1234">This is an example of a design bias, where we see systematic performance differences of technology between populations.</sample>
    <sample id="1235">Designed by Sies, like the one that we just saw before, and might occur due to the positionality of the NLP research tree model developers. Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="1236">This is a concept widely used in critical studies, specifically in feminist and queer academia.</sample>
    <sample id="1237">and as a researcher, prepositionality can influence the research process and its outcomes and results because it can change the decisions that researchers make.</sample>
    <sample id="1238">and so one question that people might ask is, do data sets have positional?</sample>
    <sample id="1239">and we're not trying to say that models and cells, in datasets themselves, have demographic identities and life experiences, but they do aggregate judgments and opinions of real people and can thus perpetuate certain positionalities over others.</sample>
    <sample id="1240">So privwork is stress a some anecdotal evidence of having positionality, such as cultural gaps in models and indeed datasets, as well as theoretical definitions of model positionality.</sample>
    <sample id="1241">However, these works really don't look at comparing users with the data sets and models themselves.</sample>
    <sample id="1242">In starting model and dataset positionality is increasingly important as NLP tasks become more subjective and socially oriented.</sample>
    <sample id="1243">in it's challenging to characterize how these positionalities are skewed because not all decisions are documented, and many models are hidden behind API.</sample>
    <sample id="1244">So, to study data set model positionality, we actually compare the annotations with real users with existing data sets and models.</sample>
    <sample id="1245">We do this through a framework and NLP positionality.</sample>
    <sample id="1246">Our framework works in two main stages:</sample>
    <sample id="1247">The first step is to re-annotate datasets with diverse annotators.</sample>
    <sample id="1248">and we opt to do this over looking at the demographics of original datasets. Um annotators because usually only a few annotators annotates each instance, and because demographics are rarely collected and shared.</sample>
    <sample id="1249">and so we opted to re-entitate data to get many entities for instance and to get a rich set of demographic data.</sample>
    <sample id="1250">We then take the annotations by demographic and compare them to the models and datasets using Pearson's R correlation score.</sample>
    <sample id="1251">and that's our framework actually differs from annotated disagreement literature by comparing end users with models in data sets predictions and labels, as opposed to looking at just inter-annotator agreement or modeling annotator distribution.</sample>
    <sample id="1252">Our frameworks are largely enabled through Lab in the Wild, an online crowdsourcing platform for ERC collaborators.</sample>
    <sample id="1253">In Lab on the Wild is an online experimentation platform where we can recruit a diverse volunteers. Um, compared to like platforms like Enteric, which largely have participants from the US or India. And further, Lab on the Wild still is able to get high-quality data.</sample>
    <sample id="1254">We host two tasks all about the world, one of them being social acceptability. And the way this works is that participants will read a situation from the social chemistry dataset, and then they'll rate how socially acceptable the situation is.</sample>
    <sample id="1255">afterwards to stay in English and study. They can compare their responses to an AI and others.</sample>
    <sample id="1256">We've then compared these annotations with social chemistry, Delphi, and GPT-4.</sample>
    <sample id="1257">We then replicated a very similar setup for the toxicity and hate speech detection task, where they'll read an instance from data hate and write whether they think it's an instance of hate speech.</sample>
    <sample id="1258">We then compared these annotations with DinaHeat, Perspective API, Rewire API, Hateverbata, and GPT-4. Our study involved over 16,000 annotations from over 1,000 annotators from 87 countries.</sample>
    <sample id="1259">So now we're better equipped to answer who do NLP data assessment models align with the most. We find that there is positionality in NLP.</sample>
    <sample id="1260">For example, we find that Deisets's models are most aligned to English speaking countries. So, for the GPT-4 social acceptability analysis, we find that it's most aligned to confusion and English speaking countries. We find that Dynahate is also most aligned to English speaking countries.</sample>
    <sample id="1261">We also find most alignment with people who have a college education. So, for GPT-4 in the social acceptability task, we find that it's most aligned with people with a college education or graduate school education.</sample>
    <sample id="1262">and we find the same for Johnny Hate, where it's most aligned to people with a college education.</sample>
    <sample id="1263">However, when models and datasets are aligned to specific populations, some are inevitably left behind.</sample>
    <sample id="1264">An example of this is that data sets in models are less aligned to non-binary people compared to their men and women counterparts. We find this in the GPT-4 social acceptability task, as well as the Dina Heat task analysis as well.</sample>
    <sample id="1265">So, given that there is position and allied and NLP, what can we do about it?</sample>
    <sample id="1266">So we have a few recommendations for this. First one is keep a record of all relevant design choices throughout the research process, and the other is to do NLP research from the lens of the user.</sample>
    <sample id="1267">Our third recommendation is to build specialized datasets within four specific communities, and a good example of this is the Muscani initiative. I mean, we want to emphasize that inclusive NLP isn't just making, you know, all technologies work for everyone.</sample>
    <sample id="1268">And so, that concludes our presentation. But if you'd like to learn more, feel free to check out our dashboard for the most updated analysis results and our paper. Thank you.</sample>
    <sample id="1269">The tokens are not in the correct order.</sample>
    <sample id="1270">Because positive stereotypes might be due to overly aggressive value alignment or other anti-stereotyping methods.</sample>
    <sample id="1271">Ungrammatical sentences.</sample>
    <sample id="1272">The provided text does not specify the evaluation metrics used by the authors.</sample>
    <sample id="1273">Inner annotator agreement.</sample>
    <sample id="1274">Wikipedia.</sample>
    <sample id="1275">The authors are Regina Stodden.</sample>
    <sample id="1276">MultiInstruct differs from other benchmarks by focusing on improving generalization to unseen multimodal tasks, unlike previous works that primarily focused on language-only tasks. Additionally, it addresses a significant discrepancy in the availability of instruction datasets between NLP and multimodal models.</sample>
    <sample id="1277">The paper has one author.</sample>
    <sample id="1278">Binary coordination is the process of combining two or more elements to create a new, more complex element.</sample>
    <sample id="1279">The prompts used in this study were, on average, 10 words long.</sample>
    <sample id="1280">The findings suggest that smaller T5 models can perform as well as larger ones when trained on suitable datasets.</sample>
    <sample id="1309">Stream model training and conditional pre-training.</sample>
    <sample id="1310">The text does not provide information about the factor of overfitting due to test reuse.</sample>
    <sample id="1311">The quality of simplification was evaluated by fine-tuning language models to produce simplified text from complex text.</sample>
    <sample id="1312">Yes, language models exhibit different political biases. First-language models show a strong political leaning, while GPT-4 is considered the most liberal, and GPT-3 series are generally more socially liberal than BERT series.</sample>
    <sample id="1347">Cognitive dissonance is the discomfort experienced when holding conflicting beliefs or engaging in actions that contradict those beliefs.</sample>
    <sample id="1348">GPT-4</sample>
    <sample id="1349">Yes.</sample>
    <sample id="1350">Sara Babi.</sample>
    <sample id="1351">The data was taken from transcripts of TED Talks.</sample>
    <sample id="1352">Hi, my name is Adam Skirkowski and this talk is about the dependencies structure of coordination.</sample>
    <sample id="1353">As you may know, there are different dependency structures associated by different theories and NLP approaches. So, for example, in Universal Dependencies, there is the structure of the coordinate coordination lisa about and Maggie.</sample>
    <sample id="1354">I said that the first conjunct is the head of the whole coordinate structure, so in this case, 'l'.</sample>
    <sample id="1355">Similar processes in Igor Milchuck's Mining Text Theory, where again the whole code structure is headed by the first construct. So these two approaches are isometric, right? They they single out one of the conjuncts.</sample>
    <sample id="1356">Now, there are also symmetric approaches to code and coordinate structures such as the Pragma approach, the Conjunction-headed approach, the Heap-dependent treebanks, where coordinates are headed by the conjunction.</sample>
    <sample id="1357">So, uh, we get um um dependencies from end to all the conjunct</sample>
    <sample id="1358">and finally, this also a multi-headed approach that is used for example in the Kadence Word Grammar.</sample>
    <sample id="1359">and so to say all conjuncts ahead of the coordinate structure, so you get dependencies from the governor here, loves to all conjuncts separately. These are parts of the</sample>
    <sample id="1360">Now, our aim paper is to produce a novel argument for the symmetric structures of coordination, like this two and against the asymmetric structures of coordination like the</sample>
    <sample id="1361">Okay, the argument is based on the principle of dependency minimization, that we'll explain on the basis of this example.</sample>
    <sample id="1362">So in English, does it my as you might know, a direct object prefers to be close to the verb, while adverbs maybe further away, right? So much Reddit yesterday's fine because the direct object it is close to the.</sample>
    <sample id="1363">while March read yesterday it is much worse, right? Because here between the verb and the direct object there is an adjunct.</sample>
    <sample id="1364">However, this effect may be ameliorated when when the direct objects are very heavy and very long because then it can be moved to the position after the adj.</sample>
    <sample id="1365">This is illustrated here. So both these sentences are fine. March 1st is absolutely fascinating book about the bees yesterday. Uh, is okay. Whereas instead of it we have the long and pe.</sample>
    <sample id="1366">It's also okay to say March 1st yesterday this absolutely fascinating book about</sample>
    <sample id="1367">So, the reason here is that this is possible because even though this sentence violates the general grammatical principle that the direct object should be next to the verb.</sample>
    <sample id="1368">it satisfies the principle of dependency length minimization. It says that shorter shorter dependencies are preferred.</sample>
    <sample id="1369">So, um, these two um, are trees, uh, only show uh the length of the crucial dependencies, so the ones that are not constant among these two structures.</sample>
    <sample id="1370">So here we have a dependency from red to the adjunct of length seven measured in words, and from red to book of length four. So to get 11.</sample>
    <sample id="1371">When you move, when you swap our two constituents, the sum of these two dependencies becomes six, right? So it's 11, six, much shorter. That's why this sounds quite okay, right? It violates one principle, but it satisfies another one.</sample>
    <sample id="1372">Okay. Uh, so what we did, we extracted very statistics from uh about coordination from the enhanced version of the Pentr Bank and see the paper why wouldn't you use a university dependency?</sample>
    <sample id="1373">And these statistics confirm the observation made many times before that left conjuncts tend to be shorter. So, short and dependent not the and salts measured in syllables.</sample>
    <sample id="1374">and also the observation that was made in passing that a distance grows with length difference.</sample>
    <sample id="1375">So, I wondered the difference between the length of the two conjuncts are grows. The shorter conjunct refers to be the first one stronger, right? So the proportion is is bigger of the left short conjunct.</sample>
    <sample id="1376">what was novel in in the paper is we observed that this tendency only occurs when the governance of the left apparatus</sample>
    <sample id="1377">Right so the governor is on the left in this example, I saw button Lisa, so is the governor is on the left.</sample>
    <sample id="1378">It's absent in the second example. "Home came and sneezed." Here we have coordination of two verbs and there's no outside external governor, right? So in such cases, the left conjunct prefers to be shorter, the more so the uh the bigger the difference uh between the two. The</sample>
    <sample id="1379">However, when are the governance on the right as here left governance the coordination to the net, this effect disappears.</sample>
    <sample id="1380">So we showed that um uh by measuring length in characters, the first column in syllables, the middle column and in words, the right column. So I'll concentrate on the right column.</sample>
    <sample id="1381">What is the difference is that when the governance on the left</sample>
    <sample id="1382">The tendency for the left conjunct to be shorter grows steadily with the absolute difference in words, and the same is observed when there is no governor as in coordination of sentences, but when there is a governor as in the right, this tendency disappears.</sample>
    <sample id="1383">and uh we showed in the paper how this um uh provides an argument against um asymmetric structures of coordination as these two and four symmetric structures as these</sample>
    <sample id="1384">So, see the paper for the full agreement and I'll argue, sorry, and talk to us about the post session.</sample>
    <sample id="1385">Matthias Lendehammer.</sample>
    <sample id="1386">Cross-lingual transfer is the process of training a model to perform a task in one language and then transferring that knowledge to another language.</sample>
    <sample id="1387">The authors are PhD students at Saarland University in Germany.</sample>
    <sample id="1388">Translation quality, average lagging, and computational aware average lagging.</sample>
    <sample id="1416">Tree-based methods can be complicated and computationally expensive, often requiring significant formalism-specific preprocessing of logical forms and specialized grammar induction procedures.</sample>
    <sample id="1417">The authors are affiliated with the University of California, Irvine.</sample>
    <sample id="1495">Annotating behaviors in chat.</sample>
    <sample id="1496">2003</sample>
    <sample id="1527">The authors are associated with the University of California, Berkeley, and the University of Toronto.</sample>
    <sample id="1528">Su Yuyan.</sample>
    <sample id="1529">Four.</sample>
    <sample id="1530">The approach is compared to the seat of the Arza architecture, specifically tailored for simultaneous processing and translation.</sample>
    <sample id="1531">Hello everyone, my name is Ying and my colleague Zhiyang and I will be presenting our research on motivating structured improving motivating models through learning while instruction to the</sample>
    <sample id="1532">So with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data efficient way.</sample>
    <sample id="1533">Recently, many studies have shown that instruction tuning enables large language models to perform unseen tasks in a zero-shot manner by following natural instructions.</sample>
    <sample id="1534">However, most previous works on instruction tuning focused on improving the zero-shot performance on language-only tasks, while computer vision and multimodal tasks have been left untouched.</sample>
    <sample id="1535">Therefore, in this work, we want to investigate whether instruction tuning of multimodal pre-training models can actually improve generalization to unseen multimodal tasks.</sample>
    <sample id="1536">Additionally, at the time of our research, we discovered a considerable discrepancy in availability of instruction dataset between NLP and multimodal.</sample>
    <sample id="1537">There exists more than 1600 language-only instruction tasks, however, there is no large-scale publicly available multi-modal instruction tuning dataset. Therefore, this motivates us to build a multi-modal instruction tuning dataset.</sample>
    <sample id="1538">Here, we present Multi-Instruction, the first multi-model instruction tuning benchmark dataset, that consists of 62 diverse multi-model tasks covering ten broad categories.</sample>
    <sample id="1539">This task is derived from twenty-one existing open source datasets, and each task is equipped with five expert written instructions.</sample>
    <sample id="1540">for investigating multimodal instruction tuning of our proposed dataset. We take OFA, a unified multimodal pretraining model as our base model. OFA uses a unified vocabulary for language, image tokens, and the coordinate of a bounding box.</sample>
    <sample id="1541">Here we show some example instances from our multi-lingual data.</sample>
    <sample id="1542">The unified file processing of various input and output data</sample>
    <sample id="1543">We follow the manner of OFA and formulate all the tasks in a unified sequence to sequence format, in which the input text, images, instructions and bounding boxes are represented in the same token space.</sample>
    <sample id="1544">Okay, now I'm going to talk about multimodal instruction.</sample>
    <sample id="1545">So for the 20-day dataset, we use 53 tasks from Negru for training and we sample 10,000 instances per task. For testing, we reserve the entire comments reasoning group for testing and we select additional five tasks from Wiki and the miscellaneous group.</sample>
    <sample id="1546">We use all the instances in the test split for each task. Yeah, addition we randomly sample 20 tasks from the test split of natural instruction as an instance task for each.</sample>
    <sample id="1547">So we use a pretrained or large model as a base model. During training, we make all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates.</sample>
    <sample id="1548">so during the test for the English task we can conduct a total of five experiments by evaluating the model using the five instructions in each experiment.</sample>
    <sample id="1549">We are about to meet and make performance and standardization of the performance across all five experiment.</sample>
    <sample id="1550">if that task is a multimodal classification task, we report accuracy. If it's a multimodal generation task, we report ROUGE-L. For a QA task, we report ROUGE-L as the</sample>
    <sample id="1551">we also introduced an additional evaluation metric called sensitivity. So this measures the models ability to consistently produce the same output for the same task regardless of the slight variation in the wording of the instruction.</sample>
    <sample id="1552">Here is our main result, as we can see, instruction tuning can significantly improve LLMs performance on seen multimodal tasks.</sample>
    <sample id="1553">autonomous learning from machine instruction datasets can benefit instruction tuning.</sample>
    <sample id="1554">here we can see as the amount of task increases, the model achieve better performance and in the meantime lower sensitivity.</sample>
    <sample id="1555">So we also use warning instruments. We use warning instruction versus five instruction as we can see using warning instruction can improve a model's overall performance and reduce its sensitivity a lot.</sample>
    <sample id="1556">So this shows the effect of different fine-tuning strategies on the model sensitivity. As we can see, by transferring learning from natural instruction dataset, the model can achieve much better sensitivity compared to the original OpenAI model.</sample>
    <sample id="1557">We also can see transfer learning from natural language instruction dataset can help AI to achieve much better performance on the natural language instruction dataset.</sample>
    <sample id="1558">So overall, we had proposed a first-of-its-kind multi-modal instruction tuning dataset, which we can effectively improve their shot capability of OpenAI and we explore different transfer learning techniques and show their benefits with design and new metric consistency and efficiency.</sample>
    <sample id="1559">We are currently collecting a much larger multimodal instruction tuning dataset with around 150 additional various language tasks, and we will release them soon. This is a QR code for our data and model. Thank you.</sample>
  </task>
</testset>