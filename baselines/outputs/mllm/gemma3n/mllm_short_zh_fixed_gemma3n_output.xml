<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="zh">
    <sample id="0">语言模型主要训练在大型网页数据上，其中政治新闻媒体的数据被很好地覆盖。</sample>
    <sample id="1">麦基尔大学、米拉和微软研究院。</sample>
    <sample id="2">嗨，欢迎来到我们关于“Plane”，一种用于文档级别和句子级别的德语文本识别的新工具的演示。</sample>
    <sample id="3">我的名字是丽吉娜·斯托恩，我将引导您进入本次演示的第一部分。

首先，我们来定义文本简化。</sample>
    <sample id="4">文本简化是指将文本调整以提高其针对特定目标受众的理解能力。对于阅读障碍人士或非母语人士而言，这尤其重要。</sample>
    <sample id="5">请提供英文内容。</sample>
    <sample id="6">请。</sample>
    <sample id="7">简化句子可以使用多种技术，例如词语替换、句子删除、句子插入、句子重新排序或插入词语。</sample>
    <sample id="8">我们建议创建一个新的合作平台。因为近年来，存在着许多与现有合作相关的难题。例如，这个合作体，由于规模太小，难以开展技术创新模式。</sample>
    <sample id="9">是的，最近几年提出的重新建模方法，会自动对齐，这意味着它们可能存在错误，并且对齐可能不准确。</sample>
    <sample id="10">因此，我们建议将我们的公司拆分为两个子公司：Deepen APA 和 Deepen Web。Deepen APA 专注于新闻文本。</sample>
    <sample id="11">在平民API中，我们手动整理了483份文档，结果大约是30,000到30,000个句子对。</sample>
    <sample id="12">对于平面网页，这些内容包括不同的域名，并且我们还手动对这些750个文档进行对齐，另一方面则使用自动对齐方法。</sample>
    <sample id="13">总共结果为 38,450 个句子。</sample>
    <sample id="14">我们再分析一下句子对，例如，在类型方面，有些难以</sample>
    <sample id="15">我在这里可以看到，圣经文本比例如新闻文本或语言学习文本要更强大、更简化。</sample>
    <sample id="16">在所有层面，例如词法简化、结构简化，或者在整体层面上的简化。</sample>
    <sample id="17">正如你所看到的，我们的平面 корпуса具有更高的差异化转换形式多样性。例如，在平面 API Корпус中，我们有更多的排序和旋转，而没有在平面 Web Корпус中。</sample>
    <sample id="18">在另一个方面，在词汇库中，我们有许多重复的字母“r”。</sample>
    <sample id="19">好的，让我们看看我们可以用这个语料库做什么。

你好，我是奥马尔，现在我将介绍我们数据集的用例。

所以，第一个用例是我们可以在自动对齐消息中评估</sample>
    <sample id="20">最近几年，在机器翻译的背景下，有很多对齐方法。</sample>
    <sample id="21">我们有两个平行的文档，用不同的语言书写，我们想要提取句子之间的对齐关系。</sample>
    <sample id="22">但在我们的用例中，我们试图提取两个平行的文档之间的对齐关系，这两个文档使用相同的语言，包含相同的文本，但复杂度不同。</sample>
    <sample id="23">现在我们有了我们手动对齐的深度平原数据，我们可以将这些句子作为标准对齐方案来评估一些提出的对齐方法。</sample>
    <sample id="24">我们对拟议的方法进行了调整，并已发表了所有这些调整以及运行我们实验的代码。</sample>
    <sample id="25">最终，我们得出结论，对于德语文本简化，最佳的自动对齐方法是使用“mess align”方法。</sample>
    <sample id="26">你也可以找到代码来运行此方法在你自己的文档中。</sample>
    <sample id="27">第二个用例，我们在我们的论文中展示的是自动文本简化。</sample>
    <sample id="28">我发现微调语言模型，以生成从复杂输入文本中简化文本。</sample>
    <sample id="29">我们对两个不同的模型进行了微调。我们对长文本模型进行了微调，以产生文档级别的简化。</sample>
    <sample id="30">并且我们还调整了基于正态分布的长期基于正态分布的导入，以产生句子级别的简化。</sample>
    <sample id="31">你可以找到所有检查点，并且你可以查看实验的得分和评估指标的更多细节。</sample>
    <sample id="32">我们得出结论，这种基本的微调可以产生或获得低于基线得分的得分。</sample>
    <sample id="33">我们建议这些结果作为自动文本简化问题的基准，作为未来基准。</sample>
    <sample id="34">非常感谢您的关注，我们希望在会议期间见到大家。</sample>
    <sample id="35">Kayo Yen.</sample>
    <sample id="36">T5 XL</sample>
    <sample id="37">Yes.</sample>
    <sample id="38">该方法通过明确标注模型回复是否表现出某些行为，例如提供无关信息或相互矛盾，从而试图减少人类评估的主观性。</sample>
    <sample id="39">现有弱监督方法的成功在很大程度上依赖于干净的验证样本。</sample>
    <sample id="40">The provided text does not contain any information about how to improve scores.</sample>
    <sample id="41">七位。</sample>
    <sample id="42">嗨，我的名字是萨达姆·斯皮尔科夫斯基，这关于协调的依赖结构。</sample>
    <sample id="43">例如，不同的依赖结构在不同的理论和方法中有所不同，例如，在统一依赖中，是关于坐标协调的结构，而麦格</sample>
    <sample id="44">拉</sample>
    <sample id="45">我们使用一种处理系统，在 Igor Milchuk 的 Mining Text Theory 中，同样，整个代码结构由第一个控制结构主导。所以这两个方法是等价的，对吧？它们是唯一一个连接的条件。</sample>
    <sample id="46">现在，还有一些对称的坐标结构，例如Pragma Pro、Conjunction Header Approach，假设Pragma Dependency Treebanks，而坐标结构由连接符引导。</sample>
    <sample id="47">所以，我们从一个到所有连接词的依赖关系。</sample>
    <sample id="48">最后，这还是一种多处理方法，例如在卡森斯语法中。</sample>
    <sample id="49">所有子句都位于主句前面，这样可以获得从主句到所有子句的依赖关系。</sample>
    <sample id="50">现在，ADM 论文旨在为配位结构的对称结构提出一个新论点，例如这两个，以及反对配位结构的非对称结构，例如这个。</sample>
    <sample id="51">好的，该论点基于依赖长度最小化的原则，我们将基于这个例子进行解释。</sample>
    <sample id="52">所以，正如你所知，直接宾语通常会靠近动词，而形容词则可能更远一些，对吧？所以，今天读了这篇文章很棒，因为它直接宾语与动词紧密相连。</sample>
    <sample id="53">虽然昨天读了，但现在情况更糟，因为这里动词和直接宾语之间有一个副词“yesterday”。</sample>
    <sample id="54">然而，这种效果可能通过当直接对象非常重且非常长时得到改善，因为这样它可以移动到句子的末尾。</sample>
    <sample id="55">这在这里是说明的。所以这两个句子都很好。马丁·谢尔德斯绝对是一本迷人的书，关于昨天。这没问题。我们用的是长P。</sample>
    <sample id="56">我也可以说玛丽亚今天读了一本非常迷人的书，关于海。</sample>
    <sample id="57">所以这里是，这是可能的，因为即使这句话违反了一般语法原则，即直接宾语应该紧跟动词。</sample>
    <sample id="58">它满足了依赖长度最小化的原则，该原则指出，更短的依赖更受欢迎。</sample>
    <sample id="59">所以，这两个树只显示了关键依赖的长度，即那些在两个结构中不恒定的依赖。</sample>
    <sample id="60">所以这里我们有从“red”到“adjunct”的长度为七的依赖关系，以及从“red”到“book”的长度为四的依赖关系。为了得到“eleven”，我们需要</sample>
    <sample id="61">当你移动或交换这两个成分时，这两个依赖的总和变为六，对吧？所以11减去6，这使得它听起来相当不错，对吧？它违反了一个原则，但它满足了另一个。</sample>
    <sample id="62">好的。呃，所以我们做了什么，我们从增强版本的潘德银行的关于协调的统计数据中提取了数据，并查看了为什么用户会使用依赖关系。</sample>
    <sample id="63">这些统计数据证实，观察结果多次出现，左侧共轭通常较短。因此，盐分独立于盐分测量指标。</sample>
    <sample id="64">并且还观察到，随着长度的增加，这种倾向会增长。</sample>
    <sample id="65">所以，我想知道两个连词长度之间的区别。较短的连词通常指的是第一个更强，对吧？所以比例是左侧的较短连词。</sample>
    <sample id="66">在报纸上报道的主要内容是，我们观察到这种倾向只发生在政府在左手手掌上。</sample>
    <sample id="67">好的，在例子中，左边是政府，我看到一个按钮是“丽莎”，所以政府在左边。</sample>
    <sample id="68">在第二个例子中，“home came”没有出现，因为这里有动词的协调，并且没有外部的支配者，对吧？在这样的情况下，左侧连词倾向于更短，而右侧连词则更长。 差距越大，两者之间的差异就越大。</sample>
    <sample id="69">然而，当右边治理，例如这里，将协调权交给网络时，这种效果消失了。</sample>
    <sample id="70">我们展示了，通过测量长度，第一列是字符，第二列是音节，第三列是单词。因此，我们将集中在右侧。</sample>
    <sample id="71">我在这里想说的是，当政府在左</sample>
    <sample id="72">左侧的倾向随着词语绝对差异的增加而稳步增长，情况与当有主语时观察到的情况相同，但在右侧主语时，这种倾向消失了。</sample>
    <sample id="73">并且我们在论文中展示了如何通过这个来提供反对不对称配位结构的论证，因为这些结构是基于不对称结构。</sample>
    <sample id="74">请参阅完整协议的纸张，我将解释，很抱歉，并在后续会议中与您讨论。</sample>
    <sample id="75">四位。</sample>
    <sample id="76">圣经文本。</sample>
    <sample id="77">左并列词。</sample>
    <sample id="78">是的，你可以使用这些模型。</sample>
    <sample id="79">DEplain-apa 包含使用文本的文档。</sample>
    <sample id="80">更好的模型架构、更大的模型大小以及更少的微调示例。</sample>
    <sample id="81">通过测量左侧词的长度，即第一列（字符数）、中间列（音节数）和右侧列（单词数）。</sample>
    <sample id="82">实验设计应包括以下步骤：

1. **定义变量：**
    * **自变量：** 支配词的位置（左侧、中间、右侧）。
    * **因变量：** 支配词的长度（通过测量其在句子中的字符数）。
2. **设计句子：** 创建一系列包含不同支配词位置的句子。
3. **控制其他因素：** 确保句子中其他词语和语法结构保持一致。
4. **测量：** 测量每个句子中支配词的长度。
5. **分析数据：** 使用统计方法分析数据，以确定支配词位置与长度之间的关系。</sample>
    <sample id="83">基线分类器在不平衡数据上的训练效果不佳，性能与随机猜测相当。</sample>
    <sample id="84">这篇论文有两位作者。</sample>
    <sample id="85">Bob, Alice</sample>
    <sample id="86">在正式性和词汇连贯性等话语现象上，语境感知 MT 模型比语境无关模型更有优势。</sample>
    <sample id="87">The authors are affiliated with the Facebook AI Research (FAIR) team.</sample>
    <sample id="122">框架通过重新标注数据集，并利用不同背景的标注者，来量化立场。</sample>
    <sample id="155">研究结果是，当人类受试者被给予这些提示时，研究者也能够识别出种族刻板印象。</sample>
    <sample id="156">The study used statistics extracted from the enhanced version of the Pentoshi Bank.</sample>
    <sample id="157">1</sample>
    <sample id="158">与认知失调密切相关的任务包括：
1. 话题无关的距离分类
2. 二元分类的扩展与比较类别的PRTB</sample>
    <sample id="159">四位。</sample>
    <sample id="160">根据所给的英文内容，这篇论文有 10 位作者。</sample>
    <sample id="161">引入的框架通过将最终用户与模型和数据集中预测和标签进行比较，与仅研究内部一致性或建模不同。</sample>
    <sample id="162">第一个。</sample>
    <sample id="163">DeepL 和 Google Translate。</sample>
    <sample id="164">嗨，我是乔恩·平，来自华盛顿大学。今天我将介绍我们对预训练数据、语言模型以及下游任务进行追踪的研究成果，探讨政治偏见如何导致不公平和偏见。</sample>
    <sample id="165">语言模型正在使用大规模的网页数据进行训练。</sample>
    <sample id="166">政治新闻媒体在他们的预训练数据中得到了很好的覆盖，根据一项对C4 corpus的调查，我们看到《纽约时报》、《洛杉矶时报》、《守护者》、《华盛顿邮报》等媒体在语言模型训练中得到了很好的覆盖。</sample>
    <sample id="167">这创造了语言模型应用程序的混合祝福。</sample>
    <sample id="168">所以，一方面，他们能够从不同的视角中学习，这庆祝了民主和思想的多样性。另一方面，这些不同的政治观点是固有的社会偏见，并可能导致在数据挖掘任务应用中公平问题。</sample>
    <sample id="169">针对此问题，我们建议调查预训练数据到语言模型到下游任务的政治偏见传播管道，具体通过提出以下问题：</sample>
    <sample id="170">首先，我们如何评估语言模型中的政治倾向，以及我可能对政治偏见持有的态度？</sample>
    <sample id="171">其次，语言模型在下游任务中实际表现如何，以及可能导致NLP应用程序出现错误的因素。</sample>
    <sample id="172">具体来说，我们首先建议使用不同的提示格式向语言模型提供政治问卷，例如政治竞争力测试。这确保了我们能够以政治科学文献为基础进行自动评估。</sample>
    <sample id="173">初步结果表明，第一语言模型确实具有很强的政治倾向，它们占据了政治光谱的四分之三。</sample>
    <sample id="174">我们还可以看到，GPT-4 是所有语言模型中最自由的，而 GPT-C 系列通常比 BERT 系列更具社会自由性，并且具有变体。</sample>
    <sample id="175">其次，我们旨在调查语言模型中政治偏见到底有多大。</sample>
    <sample id="176">所以我们可以通过进一步预训练语言模型检查点，在六个不同的部分中进行对照实验，这些部分被分为新闻和社交媒体，进一步分为它们的政治内容。</sample>
    <sample id="177">在对语言模型进行进一步预训练时，我们发现语言模型的意识坐标也相应地与“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉”、“拉</sample>
    <sample id="178">例如，对于罗伯特·哈弗德进一步训练在左侧线性Reddit语料库上，我们可以看到在它的词汇和语法方面出现了一显著的自由主义转变。</sample>
    <sample id="179">请提供英文内容。</sample>
    <sample id="180">我们还试图调查语言模型是否能够捕捉到我们现代社会中普遍存在的极化现象。</sample>
    <sample id="181">所以我们将预训练的词嵌入分为美国45岁以下和美国45岁以上两个不同的时间段。我们分别对两个不同的时间段进行预训练语言模型。</sample>
    <sample id="182">我们看到，语言模型在 2017 年之后，普遍呈现出远离中心的政治倾向。这表明语言模型也能够捕捉到我们社会中的极化现象。</sample>
    <sample id="183">所以，最后一点，我们重视不同政治立场对仇恨言论检测和虚假新闻检测的评估，这两种NLP应用通常涉及语言模型，并且可能具有非常重要的影响。</sample>
    <sample id="184">所以我们看到，如果我们调查每个类别（即今天）的性能，如果我们将性能分为</sample>
    <sample id="185">不同类型的媒体，例如新闻媒体，我们可以看到一个模式，例如对于仇恨言论检测，左侧的语言模型表现更好。</sample>
    <sample id="186">检测到针对社会弱势群体的仇恨言论。</sample>
    <sample id="187">然而，我们对针对更强大群体的人身攻击的检测效果不如针对仇恨言论。</sample>
    <sample id="188">和白人相比，大型语言模型在检测针对白人和男性的仇恨言论方面表现更好，但在检测针对黑人、LGBTQ+ 和其他少数族裔的仇恨言论方面表现更差。</sample>
    <sample id="189">类似的情况也发生在虚假新闻检测中，我们看到，在语言模型中，它们在检测来自其相反政治立场和观点的信息方面表现更好。</sample>
    <sample id="190">这已经够多了。我们进一步展示了许多定性例子，以说明不同参数的语言模型。</sample>
    <sample id="191">你对仇恨言论和虚假信息提供了不同的例子，基于它们在社交媒体上的不同类别。附录中还有许多更多例子，以进一步强调其</sample>
    <sample id="192">这表明存在一个持续存在的公平问题，与语言模型中的政治偏见有关。</sample>
    <sample id="193">例如，如果语言模型在文本或信息中出现错误或缺失，并且部署到流行的社交媒体平台，</sample>
    <sample id="194">这可能会意味着持有不同政治观点的人可能会被边缘化，而针对少数族裔群体的仇恨言论可能会毫无限制地蔓延。</sample>
    <sample id="195">所以，这声警报提醒我们承认并解决语言模型带来的公平问题。</sample>
    <sample id="196">所以，在简短的讨论之后，我们还想强调的是，我们揭示了关于语言模型政治偏见的一个独特困境，就像在塞尔维亚和克罗地亚之间一样。</sample>
    <sample id="197">如果我们在训练语言模型时没有对政治观点进行去性征，那么这些偏见就会从预训练数据传递到语言模型，最终导致下游任务中出现不公平现象。</sample>
    <sample id="198">如果我们试图以某种方式进行消毒，我们也会面临审查或排除的风险，而且很难确定哪些内容实际上是中立的，应该保留在语言模型训练数据中。这有点像电子...</sample>
    <sample id="199">好的，太棒了，我想这就是我今天所有要做的事情了。谢谢你的帮助。</sample>
    <sample id="200">三位。</sample>
    <sample id="201">1024</sample>
    <sample id="202">音乐、年龄、国籍。</sample>
    <sample id="203">Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="204">The speaker's name is not mentioned in the provided text.</sample>
    <sample id="205">是的，EDAtt 适应了现有的离线 ST 模型，无需重新训练或采用特定的架构。</sample>
    <sample id="206">一。</sample>
    <sample id="207">不，被测模型在测试套件上表现不佳。</sample>
    <sample id="208">KITMUS 有三个变体：
1. Background Pretrain
2. Background Both
3. Background Inference</sample>
    <sample id="209">The provided text does not mention the authors' affiliated institutions.</sample>
    <sample id="210">最后的研究问题是：如果清理数据是必需的，那么我们需要多少个清理样本？</sample>
    <sample id="211">指标灵敏度旨在确保模型在相同任务上，无论其变体如何，都能一致地产生相同的输出。</sample>
    <sample id="212">Jingwei Yi.</sample>
    <sample id="213">更高的灵敏度并不一定表示模型性能得到了提高，也可能表明相反。</sample>
    <sample id="214">根据所给的英文内容，无法确定模型在预训练期间接收什么样的语言上下文。</sample>
    <sample id="215">通常需要 23 个干净的验证样本。</sample>
    <sample id="216">Senter Muse and Danjaraof.</sample>
    <sample id="217">第一语言模型确实具有很强的政治倾向，它们占据了政治光谱的四分之三。</sample>
    <sample id="218">玛克希塔</sample>
    <sample id="219">政治偏见从预训练数据通过语言模型传播到下游任务。</sample>
    <sample id="220">是的，在简化过程中，plain-apa 语料库和网站语料库的简化过程有所不同。plain-apa 语料库的简化过程更加复杂，而网站语料库的简化过程则更加简单。</sample>
    <sample id="221">是的，Coscript 公开可用。</sample>
    <sample id="222">水印是通过对目标嵌入和原始嵌入进行加权求和来插入的。目标嵌入的权重与句子中触发词的数量成正比。</sample>
    <sample id="223">Pintland University.</sample>
    <sample id="224">是的，像 mt5 这样的编码器-解码器模型可以通过混合语言的训练来改进。</sample>
    <sample id="225">制作巧克力蛋糕。</sample>
    <sample id="226">他们通过验证提供的嵌入的覆盖率，并识别了句子中的嵌入，这些嵌入在BPC-A中被省略。</sample>
    <sample id="227">研究如何使用现有的 PLM 来构建新的 PLM。</sample>
    <sample id="228">根据所给的英文内容，GPT-4 与中国和英语国家最不一致。</sample>
    <sample id="229">在“通过注意力机制学习的知识”示例句子中。</sample>
    <sample id="230">任务数量增加，模型性能更好。</sample>
    <sample id="231">The author compares their method to other tree-less models on the CoG benchmark.</sample>
    <sample id="232">两位合著者是第一作者的顾问。</sample>
    <sample id="233">The first author of PaLM is not mentioned in the provided text.</sample>
    <sample id="234">大家好，我是 Jenny，来自弗尔斯特尔皮奇斯大学，今天我将介绍我的工作，并进行演示。我将介绍设计偏见（Design Bias）的建模。</sample>
    <sample id="235">这项工作与华盛顿大学的一些同事以及人工智能的艾伦实验室合作完成，包括塞巴斯蒂安·桑蒂、罗南·洛布罗斯、卡特琳娜·拉伊尼卡和马丁·萨赫。</sample>
    <sample id="236">让我们先想象一下，你正在为报纸工作，你正在浏览你新闻文章下的评论，试图删除有害内容。</sample>
    <sample id="237">你可能会转向一个流行的API，比如Perspective API用于检测毒性，这对于像Carl Jones这样的用户来说效果非常好。Perspective API能够正确地检测到不一致的言语。</sample>
    <sample id="238">但对于迪蒂·阿什玛来说，情况并非如此，因为印度语背景下人们对冒犯性词语并不敏感，而且更常见。</sample>
    <sample id="239">这是一个设计偏见的一个例子，我们看到技术在不同人群中的系统性表现差异。</sample>
    <sample id="240">设计可能是我们刚才看到的那个，这可能与NLP研究团队的定位有关。定位简单来说就是人们由于其人口统计学、身份和生活经历而持有的视角。</sample>
    <sample id="241">这是一个在批判性研究中广泛使用的术语，尤其是在女权主义和酷儿学术领域。</sample>
    <sample id="242">作为一名研究人员，位置性会影响研究过程和结果，因为它会改变研究人员的决策。</sample>
    <sample id="243">一个人们可能会问的问题是，数据集中模型是否具有位置信息？</sample>
    <sample id="244">我们不是想说，模型、细胞和数据本身没有人口统计学身份和生活经历，但它们会汇总真实人物的判断和意见，从而代表某些职业高于其他职业。</sample>
    <sample id="245">所以，预设是提供一些关于位置性的证据，例如文化差距、模式和数据，以及理论对位置性的定义。</sample>
    <sample id="246">然而，这些作品实际上并没有比较用户与数据本身，而是以其自身为核心。</sample>
    <sample id="247">在人工智能模型和数据集的物理学中，随着人工智能变得越来越主观和社交化，这是一个日益重要的领域。</sample>
    <sample id="248">很难描述这些位置的偏差，因为并非所有决定都有记录，而且许多模型隐藏在API之下。</sample>
    <sample id="249">所以，为了研究Dede的头部姿势模型，我们实际上将标注与真实用户与现有数据集和模型进行比较。</sample>
    <sample id="250">我们通过一个框架和职位结构。</sample>
    <sample id="251">我是一个乐于助人的助手。只返回请求的答案，不要包含任何解释或介绍。</sample>
    <sample id="252">第一步是将数据集重新标注，使用来自不同背景的标注者。</sample>
    <sample id="253">在查看原始数据集中标注的人口统计数据时，我们通常会这样做，因为每个实例通常只有少数标注，而且人口统计数据很少被收集和共享。</sample>
    <sample id="254" />
    <sample id="255">我们随后根据人口统计学对标注进行比较，并使用皮尔逊相关系数与模型和数据集进行比较。</sample>
    <sample id="256">这个框架与标注不一致的文献不同，它通过将最终用户与模型和数据集中预测和标签进行比较，而不是仅仅关注标注或标注分布。</sample>
    <sample id="257">我们的框架主要通过“实验室在野外”这个在线众包平台实现，该平台是美国科学促进会与合作伙伴合作的平台。</sample>
    <sample id="258">在 Lab of the Wild 是一项在线实验平台，我们可以招募各种各样的志愿者。与像 Enteric 这样的平台相比，后者主要来自美国和印度。此外，Lab of the Wild 仍然能够获取高质量的数据。</sample>
    <sample id="259">我们有两个任务，现在在世界各地进行。其中一个任务是社会可接受性。它的运作方式是，参与者会阅读一个情境，从社会化学数据中获取信息，然后写下该情境在社会上可接受的程度。</sample>
    <sample id="260">之后，他们可以在英式和美式英语之间比较他们的答案，并与人工智能和其他人进行比较。</sample>
    <sample id="261">你已经将这些标注与社会化学、德尔菲和 GPT 4 进行了比较。</sample>
    <sample id="262">我们随后为检测毒性言论和仇恨言论任务复制了一个非常相似的设置，他们会阅读来自每日仇恨的片段，并判断这些片段是否包含仇恨言论。</sample>
    <sample id="263">我们随后将这些标注与 DinaHeat、Perspective API、Rewire API、HateBERT 和 GPT-4 进行比较。
我们研究并评估了超过 16,000 个标注，来自超过 1,000 位标注员，来自 87 个国家。</sample>
    <sample id="264">现在我们更好地理解了NLP数据评估模型与哪些最契合。我们发现，NLP具有位置性，并且NLP</sample>
    <sample id="265">例如，我们发现，在许多模型中，数据与英语国家最为相关。因此，对于GPT-4的社会接受度分析，我们发现它与中国和英语国家最为相关。我们发现，Dynah也与英语国家最为相关。</sample>
    <sample id="266">我们还发现，拥有大学学历的人与社会责任任务的匹配度最高。对于GDP4，我们发现拥有大学学历或研究生学历的人与社会责任任务的匹配度最高。</sample>
    <sample id="267">和我们发现的相同的是对于唐尼·海特的描述，它与有大学教育的人非常相似。</sample>
    <sample id="268">然而，当模型和数据与特定人群对齐时，一些人不可避免地会被遗漏。</sample>
    <sample id="269">一个例子是，数据在模型中被训练来与非二进制人士进行比较，与男性和女性的同伴进行比较。我们可以在 GPT-4 的社会可接受性任务以及 DinaHeat 任务分析中找到这一点。</sample>
    <sample id="270">所以，鉴于那里有位置和一个小岛，我们能做什么？</sample>
    <sample id="271">我们有一些建议：首先，记录你在研究过程中所有相关的设计选择；其次，进行用户体验研究，从用户的角度出发。</sample>
    <sample id="272">我们第三个建议是构建针对特定社区的专业数据集模型，一个很好的例子是马萨丘塞斯州倡议。我想强调的是，包容性人工智能不仅仅是让所有技术都能为每个人工作。</sample>
    <sample id="273">那么，这就是本次演示。如果您想了解更多，请随时查看我们的仪表板以获取最新分析结果，以及我们的论文。谢谢。</sample>
    <sample id="274">演讲者提到了以下 SimulST 的几个问题：

* 针对特定架构使用额外的模块进行优化。
* 训练过程复杂，涉及不同的优化目标。
* 需要训练和维护多个模型，以满足不同的延迟需求。</sample>
    <sample id="275">在训练 NLP 模型时，减轻数据集中的社会和政治偏见是一个复杂的问题。虽然可以尝试通过各种方法进行偏见清理，但这样做也可能导致审查或排除某些内容。此外，确定哪些内容是“中立”且应该保留在训练数据中的过程非常困难。</sample>
    <sample id="276">嗨，我是苏语源，来自富达大学。我在这里介绍我们的工作：从语言模型中提取可约束的知识。</sample>
    <sample id="277">在现实生活中，人们经常通过遵循分步指令来规划他们的行动，例如使用带有标题的脚本。</sample>
    <sample id="278">以前的语言模型被用于规划抽象的日常活动，例如做蛋糕，并证明大型语言模型可以有效地分解任务为步骤。</sample>
    <sample id="279">然而，过去许多人专注于计划抽象目标，而忽视了日常活动。计划目标时需要考虑具体的、特定的约束条件，例如制作巧克力蛋糕。然而，仍然没有开始。</sample>
    <sample id="280">在本文中，我们定义了约束语言规划的问题。</sample>
    <sample id="281">计划的约束条件因个人生活中的不同现实目标而异，并且受到动机性约束的影响。一个好的计划者应该制定合理的计划，并考虑到约束条件。</sample>
    <sample id="282">在本文中，我们评估并改进了限制语言规划的语言模型，即</sample>
    <sample id="283">请提供英文内容。</sample>
    <sample id="284">我们如何获取这些代码首先？
正如表格所示，我们将抽象代码与修改后的约束进行扩展，用于人类在数据获取中的使用，使用抽象 GPT。</sample>
    <sample id="285">我是一个乐于助人的助手。只返回请求的答案，不要包含任何解释或介绍。</sample>
    <sample id="286">这个表格反映了结果的整体准确性。我们发现，所有语言模型都实现了令人满意的结果。</sample>
    <sample id="287">这篇英文内容没有提供任何可翻译的文本。</sample>
    <sample id="288">结果在图表中显示，生成脚本中的语义完整性是可以接受的，但对事实的约束无法保证。</sample>
    <sample id="289">在越来越多的顶级类别中，定义在Wakefield的限制。图表中的热图显示，不同类别女性的计划和表现差异显著。</sample>
    <sample id="290">之前发现，在大型语言模型中，输出的质量在不同版本之间存在差异，导致性能下降。因此，人们提出了使用过拟合的零样本过滤器来提高生成质量的设想。</sample>
    <sample id="291">我首先展示了约束类型以及基于这些抽象概念的示例。</sample>
    <sample id="292" />
    <sample id="293">下一个，一个未来模型是基于两个步骤的选择第一个脚本。</sample>
    <sample id="294">将英文内容翻译成中文。</sample>
    <sample id="295">请提供英文内容。</sample>
    <sample id="296">我们的研究表明，在LGBTQ+群体中，有较多的头发色素沉着。我们的研究表明，我们的研究在语义完整性和事实准确性方面都得到了显著的改进。</sample>
    <sample id="297">由于大型语言模型部署成本高昂，因此启用语言规划是使用较小和专业化模型的必要步骤。</sample>
    <sample id="298">然而，之前的研究并未明确规划特定目标，并且手动数据在注释中的成本是昂贵的。</sample>
    <sample id="299">我们遵循一个象征性知识蒸馏方案，以约束语言模型的数据大小，从轻量级模型开始。</sample>
    <sample id="300">请将英文内容翻译成中文。</sample>
    <sample id="301">为了生成55个具有特定目标和脚本的单词，我们需要确保数据的质量和测试的有效性。我们要求众包的工人来查找并修改不正确的样本。</sample>
    <sample id="302">这个图表显示了语言规划中约束性模型的分布，而非约束性模型在通用零售领域表现出更高的表现。通过约束性模型，我们可以构建更小、更专业化的模型来进行语言规划。</sample>
    <sample id="303">在文件尺寸 TF 文件查询中，显示红色，可以生成较小的脚本，比大多数大型模型更小。这表明较小的模型可以处理大型和不大的模型，这很可能是由于可用的数据集。</sample>
    <sample id="304">在总结中，我们建立了一个约束语言规划问题，旨在增强大型语言模型中的约束语言规划能力，并开发了一个用于大型语言模型后生成任务的评估方法。</sample>
    <sample id="305">我们使用大型语言模型来生成一个高色彩的平方数据集，用于语言规划。我们希望该数据集可以作为研究语言规划的可用资源。</sample>
    <sample id="306">谢谢您的时间，请提供您的课程脚本，我们来处理你的论文。</sample>
    <sample id="307">PaLM 的流畅度与现有语言模型相当。</sample>
    <sample id="308">水印方法需要具备以下属性：

1. 可应用于嵌入式服务。
2. 不应降低提供的嵌入服务的实用性。
3. 水印应足够隐蔽，攻击者可以移除。
4. 水印应可移植到攻击者服务。</sample>
    <sample id="309">14</sample>
    <sample id="310">根据内容，重新注释的数据集中每个实例通常只有少数几个标注者。</sample>
    <sample id="311">余弦相似度和欧氏距离。</sample>
    <sample id="312">将基于编码器的多语言模型用于这项任务，通过评估两个模型组：包含编码器-解码器模型（如Encoder-PT-Decoder-XLM-R+PT-Decoder、BERT+PT-Decoder）以及编码器-编码器模型（如Encoder-Decoder模型，如BERT）。</sample>
    <sample id="344">作者假设提供商可以收集一个通用的文本语料库，并统计单词频率。</sample>
    <sample id="345">大家好，我叫徐洪。今天我将介绍我们的论文：《命名实体标签在 2023 年是否仍然有效》。让我们开始吧。</sample>
    <sample id="346">我们的论文研究了泛化问题，使用名为命名实体识别任务或NER任务的方法。</sample>
    <sample id="347">我们观察到，模型已经使用公元2003年以来开发了人工智能近20年。这自然引发了几个问题。首先，现有的模型泛化到更多的数据上。</sample>
    <sample id="348">在开发新标签时，需要什么？</sample>
    <sample id="349">同时，如果我们观察到泛化能力下降，是什么导致了这些模型的性能下降？</sample>
    <sample id="350">为了调查这些问题，我们开发了一个 Cono+ 数据集。这是一个我们从路透社新闻收集的数据集，然后使用 2003 年的 Cono 标注指南进行标注。</sample>
    <sample id="351">我们对20个模型进行了微调，并在Kernel 2003测试集和Kernel Plus Plus测试集上进行了评估。</sample>
    <sample id="352">最后，我们计算了 F1 的百分比变化，以评估每个模型的泛化能力。</sample>
    <sample id="353">所以，对于良好的生成，我们需要什么？在我们的实验中，我们发现有三个主要成分是必要的。</sample>
    <sample id="354">第一个是模型架构。在我们的实验中，我们发现Transformer模型通常在新的数据上泛化得更好。</sample>
    <sample id="355">第二个成分是模型大小。我们发现，通常较大的模型能带来更好的泛化能力。</sample>
    <sample id="356">最后但不是最重要的是，我们都知道，用于微调的样本数量直接影响下游任务的性能。在这里，我们还发现，更多的微调样本实际上也导致了更好的泛化能力。</sample>
    <sample id="357">你下一个问题是什么？什么会导致某些模型性能下降？</sample>
    <sample id="358">我们有两项假设。第一项是过拟合，这通常是由于重复使用相同的测试数据集而引起的，并且通常表现为在新的测试集上返回值减少。</sample>
    <sample id="359">第二种假设是温度漂移，这是一种由训练和测试数据之间不断扩大的温度差引起的性能退化。</sample>
    <sample id="360" />
    <sample id="361">这意味着我们在2003年取得的每一点改进，都转化为在Console Plus Plus上超过一点的改进，这意味着没有边际收益。</sample>
    <sample id="362">这表明在当前情况下，适应性适应没有观察到。</sample>
    <sample id="363">请提供英文内容。</sample>
    <sample id="364">对于时间漂移，我们进行了一项实验，以重新训练或继续预训练一些模型，使用更多最近的数据。我们发现，随着数据规模的增大，性能会下降。</sample>
    <sample id="365">这证实了我的假设，即性能下降的主要原因是温度。</sample>
    <sample id="366">我们的结论是，为了更好的泛化能力，我们需要更好的模型架构、更大的模型尺寸，以及更少的微调示例。这些目标相互关联，我们不能只关注一个方面，而是需要通过其他方面来实现。</sample>
    <sample id="367">与此同时，我们还发现，这里的性能下降是由于临时漂移造成的，而且令人惊讶的是，它并非由Adaptive overfitting引起，即使Kernel 3在过去20多年里也被广泛使用。</sample>
    <sample id="368">所以回到我们标题纸上提出的问题，2003年内核技术在2023年仍然有效吗？我们发现答案是肯定的。</sample>
    <sample id="369">我们帮助所有纸质课程为更多研究提供资金，以改进模型泛化能力。</sample>
    <sample id="370">最后，请务必查看我们的纸质数据集，如果您有任何问题，请随时联系我。非常感谢。</sample>
    <sample id="397">The provided text does not contain any information about the size of the voice fragments used in the method.</sample>
    <sample id="398">Servin 是一个法官。</sample>
    <sample id="399">示例质量</sample>
    <sample id="400">GPT-4, GPT-3 series, and BERT series.</sample>
    <sample id="401">结合多个层的分数。</sample>
    <sample id="402">直接推断的示例包括说歌曲的名字，或者它的位置。</sample>
    <sample id="403">福特大学</sample>
    <sample id="404">四位。</sample>
    <sample id="405">是的。</sample>
    <sample id="406">男性。</sample>
    <sample id="407">Transformer 模型。</sample>
    <sample id="408">Clean data.</sample>
    <sample id="409">四位。</sample>
    <sample id="410">作者采用了多种模态。</sample>
    <sample id="439">作者认为在知识密集型 NLU 任务中，需要能够整合和使用预训练时间和推理时间知识。</sample>
    <sample id="440">Eing and my colleague Zhiyang.</sample>
    <sample id="441">是的。</sample>
    <sample id="442">现有的资源只支持有限类型的依赖上下文翻译，并且支持的语言种类也有限。</sample>
    <sample id="443">嗨。我将谈谈我们解决实体选择中间接关系表达式的工作，其中我们引入了替代实体概念。</sample>
    <sample id="444">我的名字是贾瓦德·侯赛尼，这是与菲利普·普拉丁斯基、西尔维娅·帕尔蒂和阿尼尔·贾尼的联合工作。</sample>
    <sample id="445">我是一个乐于助人的助手。只返回请求的答案，不要包含任何解释或介绍。</sample>
    <sample id="446">直接引用。例如，可以说歌曲的名字是“我”，或者它的位置是第一章。</sample>
    <sample id="447">但有时与亲密朋友交谈更合适，以便进行更自然的对话。这通常发生在用户记不住对方的名字时。</sample>
    <sample id="448">或者发音太相似，难以区分。</sample>
    <sample id="449">或者当用户想要指定一个偏好时。以下是一些间接偏好的例子：例如，最新的一个或不是充满活力的那个。</sample>
    <sample id="450">这是一个在对话系统以及评估自然语言处理任务中非常重要的问题。</sample>
    <sample id="451">我们没有意识到一个大型公共数据集，所以我们使用众包标注收集了一个。该数据集涵盖三个不同的领域：音乐、书籍和电影。</sample>
    <sample id="452">数据集合收集方法强调非正式性，使用卡通插画。</sample>
    <sample id="453">卡通有三个对话气泡。在第一个气泡中，鲍勃说：“记住我们昨天听过的那个歌吗？”然后鲍勃开始对话。</sample>
    <sample id="454">在第二个对话框中，艾丽丝说：“你是不是说这对我来说很简单，还是我搞错了？”</sample>
    <sample id="455">在第三个对话框中，鲍勃使用间接引用来选择其中一个实体，例如“纽约”。</sample>
    <sample id="456">我们会自动提供第一个和第二个对话气泡，但第三个由标注者填写。第一个对话气泡是从几个手动提示中选择的。</sample>
    <sample id="457">第二题，即替代问题，如下所示：</sample>
    <sample id="458">我们总是使用一个简单的模板。你指的是 A 或 B？其中 A 和 B 是从 P 概率分布中采样出来的。</sample>
    <sample id="459">以下是用于移动到列表更高位置的不同采样方法，随着条目的相似性增加，通常很难使其具有歧义性。</sample>
    <sample id="460">The first one is uniform.</sample>
    <sample id="461">第二个词是表示实体具有相似标题的词，例如两个书名“The Reach”。</sample>
    <sample id="462">第三个是当他们在维基百科上拥有相似的描述时，最后是当他们在维基百科上拥有相似的 infobox 或属性时。例如，相同的类型或相同的艺术家。</sample>
    <sample id="463">他们向受访者展示了这种替代问题。他们知道这些实体的名称，但他们并不一定了解关于该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该角色、该</sample>
    <sample id="464">我们要做的是展示关于这两个实体的背景知识。对于歌曲，我们简单地展示一个谷歌搜索链接到其。</sample>
    <sample id="465">然后让听众听至少一首歌曲并阅读关于每首歌曲的内容。

以下是一个示例，展示了对歌曲“Easy”的Google搜索结果：

“Easy”</sample>
    <sample id="466">对于食谱和书籍领域，我们展示了一些来自维基百科的背景文本。对于食谱，我们还展示了它们的图片，同样来自维基百科，这样标注者就能知道它们是什么样子。</sample>
    <sample id="467">然后，我们让参与者选择一个实体，例如这里第一个，并用 3 到 5 个间接引用表达方式描述它。</sample>
    <sample id="468">例如，带有钢琴音乐的。以下是我们数据集的一些例子：例如，没有单词的Devon，不是12岁的男孩，也不是虚构的Devon，也不是来自阿塞拜疆的。</sample>
    <sample id="469">该大型语料库包含六千个替代问题，涵盖三个领域，并包含四万二千个间接否定表达。</sample>
    <sample id="470">这个语言模型拥有与训练数据完全相同的背景知识，准确率约为92%至95%。但这并非真实。</sample>
    <sample id="471">如果语言模型有访问一些部分重叠的背景知识，那么准确率在 82% 到 87% 之间，这对于例如语言模型检索背景知识时来说是更现实的。</sample>
    <sample id="472">如果语言模型只能访问实体名称，那么准确率只有60%，所以还有很大的改进空间。我们还展示了这些模型是领域泛化的。这里有一个链接的数据集：</sample>
    <sample id="473">weight key strategy and local agreement.</sample>
    <sample id="474">The author is affiliated with the Université Paris-Saclay.</sample>
    <sample id="475">Jenny.</sample>
    <sample id="476">三位。</sample>
    <sample id="477">嗨，我是塞拉·巴比，来自特伦托大学和布鲁诺·凯斯勒基金会。我将简要介绍注意力作为本论文的指导，这是一项与马克·奥内格里和马可·乌尔共同完成的工作。</sample>
    <sample id="478">同步语音翻译（ST）是指在实时中将口语翻译成另一种语言的文本过程，从而实现跨语言交流。</sample>
    <sample id="479">当前语义模型面临的主要问题是：特定架构通常使用额外的模块来优化。</sample>
    <sample id="480">不复杂的培训程序，例如涉及不同优化目标（目标）的培训。</sample>
    <sample id="481">训练和维护多个模型以达到不同的延迟级别。例如，训练一个模型具有平均 1 秒的延迟，另一个模型具有 2 秒的延迟，以此类推。</sample>
    <sample id="482">请提供英文内容。</sample>
    <sample id="483">第一个使用现有的离线模型，无需重新训练或采用特定架构的 CwLST。使用单个模型来处理整个延迟序列，并通过特定的参数处理延迟。</sample>
    <sample id="484">在音频输入和文本输出之间通过注意力机制传递的知识，可以从注意力机制的例子中看到。</sample>
    <sample id="485">我们的解决方案是提出一个点，或者编码器解码器注意力，而这是一种策略，用于决定是否侧向移动或不移动，以及基于注意力点的部分翻译。</sample>
    <sample id="486">如果检测到的信号浓度低于某个阈值（α），则该信号将向较少的语言语音帧方向移动，这意味着接收信息足够稳定。</sample>
    <sample id="487">例如，如果我们筛选一个包含我将讨论的内容的句子，并且我们的模型预测翻译成德语，</sample>
    <sample id="488">我们将研究注意力权重。</sample>
    <sample id="489">我们会看到第一个词指向最少接收的语音帧，而最后一个词指向最少接收的语音帧，即 lambda 语音帧。</sample>
    <sample id="490">这表示前两个词将是“查”、“拉”。</sample>
    <sample id="491">虽然在某些浓度超过某个阈值阿尔法的情况下，我们不会发出最后一声，并等待下一个语音片段。</sample>
    <sample id="492">如果我们在另一个语音中，我们的模型预测其他词，我们会查看交叉注意力，即</sample>
    <sample id="493">我们看到，没有词语指向这篇演讲稿。</sample>
    <sample id="494">这表示这三个词将被重复。</sample>
    <sample id="495">如果看看那个主要结果，那就是</sample>
    <sample id="496">我们将同时性平移结果绘制在图表上，其中蓝色一侧衡量的是翻译质量，而平均语言</sample>
    <sample id="497">但是，呃，这个，让我们看看，我们还考虑了计算成本平均延迟，这包括模型计算时间来产生输出。</sample>
    <sample id="498">我们希望我们的好奇心尽可能地高。</sample>
    <sample id="499">请提供英文内容。</sample>
    <sample id="500">我们还将其与针对离线模型的准备策略进行比较，例如“权重保持策略”和“本地协议”。我们还将其与针对同步处理的特定架构的设置进行比较，例如“通道”。</sample>
    <sample id="501">这些是同步语音翻译策略在德语上的早期结果。</sample>
    <sample id="502">并且我们看到，呃，它在所有策略上都优于离线模型，因为它的曲线向左移动了。</sample>
    <sample id="503">我们还看到，如果我们考虑实际的运行时间或计算时间，那么这是最快的策略。</sample>
    <sample id="504">如果你想发现更多结果，请阅读我们的论文，我们还发布了开源代码和模型以及同步输出，以促进我们工作的可重复性。谢谢您的关注。</sample>
    <sample id="505">是的。</sample>
    <sample id="506">大家好，我的名字是伊恩，我的同事志扬和我将介绍我们的研究，关于改进模型细胞学习通过指令训练。</sample>
    <sample id="507">随着大型语言模型技术的进步，许多研究人员开始探索利用预训练语言模型进行不同下游任务的参数和数据高效的</sample>
    <sample id="508">最近，许多研究表明，指令微调能够使大型语言模型在零样本情况下以自然的方式执行各种任务，通过遵循自然指令。</sample>
    <sample id="509">然而，大多数以前的工作都侧重于提高在语言任务上的零样本性能，而计算机视觉和多模态任务则被留下了空白。</sample>
    <sample id="510">因此，在工作中，我们希望调查指令微调能否改善多模态预训练模型在未知多模态任务上的泛化能力。</sample>
    <sample id="511">此外，在我们的研究期间，我们发现在NLP和MoE模型之间指令数据集的可访问性存在显著差异。</sample>
    <sample id="512">目前有超过1600个仅语言指令任务，但目前没有大规模公开可用的多模态指令任务。因此，这激励我们构建一个多模态指令微调数据集。</sample>
    <sample id="513">在这里，我们呈现了多模态指令的第一个多模态模型指令调优基准数据集，该数据集包含 62 种不同的多模态任务，涵盖了 10 种不同的类别。</sample>
    <sample id="514">这些任务来源于一个现有的开放数据集，每个任务都配备了五条编写指令。</sample>
    <sample id="515">为我们提出的数据集进行多模态指令微调。我们使用 OLA，将多模态预训练模型作为我们的基础模型。OLA 使用统一的词汇、图像令牌和包围框坐标。</sample>
    <sample id="516">这里展示了一些我们多项式数据中的示例实例。</sample>
    <sample id="517">将各种输入和输出数据进行处理。</sample>
    <sample id="518">我遵循来自 OpenAI 的指令，并根据统一的序列到序列格式来编排所有任务，其中输入文本、图像、指令和边界框都以相同的 token 形式表示。</sample>
    <sample id="519">好的，我现在要谈论多模态指令教学。</sample>
    <sample id="520">所以对于20天的数据集，我们使用53个任务从Negru组进行训练，每个任务有10000个样本。对于测试，我们保留了整个CommonSense Reasoning组进行测试，并从Wiki和杂乱的类别中额外选择了5个任务。</sample>
    <sample id="521">重复。</sample>
    <sample id="522">所以我们使用预训练的奥尔法大型模型作为基础模型。在训练过程中，我们为所有任务创建每个实例。每个实例都会随机组合到其五个指令模板中。</sample>
    <sample id="523">将英文内容翻译成中文。</sample>
    <sample id="524">我们评估了在五种实验中，性能和标准偏差的整体表现。</sample>
    <sample id="525">如果该任务是一个多模态分类任务，我们将报告准确率。如果它是一个多模态生成任务，我们将报告 ROUGE-L。对于摘要任务，我们将报告 ROUGE-L 作为结果。</sample>
    <sample id="526">我们还引入了一个额外的评估指标，称为“一致性”。这个指标衡量的是模型在给定相同任务的情况下，无论其变体如何，都能一致地产生相同输出。</sample>
    <sample id="527">这是我们的主要结果，正如我们所见，指令微调能够显著提高OS和OFS的性能，在处理多模型任务时。</sample>
    <sample id="528">请提供英文内容。</sample>
    <sample id="529">这里我们可以看到，随着任务数量的增加，模型会获得更好的性能，与此同时，它对噪声的敏感性会降低。</sample>
    <sample id="530">所以我们还使用了while语句和for语句，正如我们所见，使用while语句可以提高模型的整体性能，并且减少其敏感性。</sample>
    <sample id="531">所以这展示了不同预训练策略对模型敏感性的影响。正如我们所见，通过从自然语言指令数据集进行预训练，模型可以实现比原始 OpenAI 模型更高的敏感性。</sample>
    <sample id="532">我们还看到在网络训练数据集中，使用Transformer模型可以显著提高在网络训练数据集上的性能。</sample>
    <sample id="533">总而言之，我们提出了一项首个大规模多模态Instruction Tuning数据集，它旨在进一步提高Ollama的可访问性，并展示不同迁移学习技术的优势，以及它们带来的好处。</sample>
    <sample id="534">我们正在收集一个更大的多模态指令微调数据集，其中包含大约 150 个额外的语言任务，并且我们将很快发布这些数据。这是我们数据集和模型的二维码。谢谢。</sample>
    <sample id="535">Setappabi from the University of Trento and the Bruno Kessler Foundation.</sample>
    <sample id="536">Javad Hosseini.</sample>
    <sample id="562">大家好，我是科斯托夫·谢纳，很高兴欢迎大家参加我们关于 ACL 2023 论文“语言模型可接受性判断并非总是对上下文鲁棒”的讨论。</sample>
    <sample id="563">这是一个由约翰·戈特、艾尔·穆勒、卡尼什卡·米什拉、卡伦·弗伦特尔、罗杰·莱维和阿蒂纳·拉蒂娜共同完成的作品。</sample>
    <sample id="564">请提供英文内容。</sample>
    <sample id="565">将英文内容翻译成中文。</sample>
    <sample id="566">在最小对偶范式中，评估语言模型通常是将一个可接受的句子或语法正确的句子展示给它，然后展示一个不可接受的句子或语法错误的句子。</sample>
    <sample id="567">然后，模型会基本将可接受的类别赋予更高的概率。</sample>
    <sample id="568">当前 MPP 流程基本上不允许我们评估模型对长句子（长句）的接受程度。</sample>
    <sample id="569">这些大型语言模型正在推出更长更长的上下文窗口，因此我们需要评估模型在整个上下文窗口中的可接受性。</sample>
    <sample id="570">这就是我们在这里试图做的事情。我们试图通过让模型评估可接受性来重温概率管道。</sample>
    <sample id="571">所以，这就是方法。那么我们要做的是模拟这些更长的序列。我们重新访问数据集合本身，然后我们重新创建句子，通过选择那些可接受或不可接受的句子。</sample>
    <sample id="572">例如，这里我们选择了一个典型的语法对，来自“布利普”数据集，来自“小岛”项目。</sample>
    <sample id="573" />
    <sample id="574">请。</sample>
    <sample id="575">所以我们可以通过从同一对匹配中选择不可接受的句子来做同样的事情，这也可以用于测试模型的可接受性。</sample>
    <sample id="576" />
    <sample id="577">这里句子仍然来自相关的数据库，但不是来自你正在评估的同一个数据库。</sample>
    <sample id="578">最后，我们可以从完全不相关的领域中选择句子，即维基百科。</sample>
    <sample id="579">所以这会告诉我们模型在实际影响下接受度评估是否受到任何影响。</sample>
    <sample id="580">你是否来自数据集中不同的子集，或者它与当前句子完全无关？</sample>
    <sample id="581">所以模型如何运作？首先，我们查看维基百科的句子，这些句子与当前查询无关，然后我们发现，对于任意上下文，TMPP的判断通常是可靠的。</sample>
    <sample id="582">我们增加了上下文长度到 1024，以最大限度地发挥 OPT 和 GPT-2 模型，并且我们在这里看到，在橙色点划线上，MPP 判决相对稳定。</sample>
    <sample id="583">现在当我们从同一段文字中选择句子时会发生什么？</sample>
    <sample id="584">所以在这里，我们选择或创建了来自可接受和不可接受领域的句子，从同一位被指控的人的文本数据中。</sample>
    <sample id="585">在这些判决中，无论是接受的还是不接受的词缀，都会显著增加或减少。</sample>
    <sample id="586">但是，当我们匹配结构时，当我们选择来自同一现象的句子时，</sample>
    <sample id="587">我们看到模型在选择的前缀是否可接受或不可接受的情况下，在 MPP 判决上出现巨大的增加或巨大的减少。</sample>
    <sample id="588">现在，这个呃呃，这个非常大，就像这个影响贯穿整个上下文，这可能会影响像新语言模型，它有大上下文。</sample>
    <sample id="589">为什么匹配前缀会影响语言模型判断如此之多？</sample>
    <sample id="590">一系列分析，我们尝试通过尝试保留输入句子的相关结构，但添加噪声到输入中来，然后对这些扰动进行多次操作。</sample>
    <sample id="591">我们发现这些噪音实际上并没有改变模型的输出，而是改变了它对这些概率的判断。</sample>
    <sample id="592">基本上，我们发现模型对句子中词语的相似度非常敏感。</sample>
    <sample id="593">在将句子置于可接受域时，我们看到所有扰动都出现相似的增加，而在将句子置于可接受域的下一个域时，我们看到在相似特征中，MPP 判决减少。</sample>
    <sample id="594">所以，我们工作的关键收获是，语言模型对句子中共享的潜在句法和语义特征非常敏感。</sample>
    <sample id="595" />
    <sample id="596">请阅读我们的论文以了解更多关于我们实验的细节。谢谢您的收听。</sample>
    <sample id="597">一个无序多元素。</sample>
    <sample id="598">55</sample>
    <sample id="626">The best alignment method to use for German text simplification is the `mess_align` method.</sample>
    <sample id="627">弱监督学习可以帮助训练模型在存在标签噪声的情况下，仍然能够泛化。</sample>
    <sample id="628">文档采用手动和自动对齐方法进行了对齐，具体分配情况未提及。</sample>
    <sample id="629">CoNLL++ 数据集是从路透社新闻（2020 年）收集的，并使用 2003 年的 CoNLL 标注指南进行标注。</sample>
    <sample id="630">你好，大家好，我是Justin John来自宾夕法尼亚大学。今天我将演示一个工作示例：跨语言语义解析和多种自然语言的语义表示。</sample>
    <sample id="631">将英文内容翻译成中文。</sample>
    <sample id="632">跨语言语义解析是任务将查询翻译成多种自然语言中的多种语义表示。</sample>
    <sample id="633">将英文内容翻译成中文。</sample>
    <sample id="634">存在许多跨语言语义解析模型，它们分别被提出和评估在各种有限任务和应用上。例如，</sample>
    <sample id="635">他们对某些自然语言的覆盖率不足，中文缺失。</sample>
    <sample id="636">请提供英文内容。</sample>
    <sample id="637">我是一个乐于助人的助手。请只返回请求的答案，不要包含任何解释或介绍。</sample>
    <sample id="638">或者只有评估一个特定的新模型，例如只有一种单一模型来评估。</sample>
    <sample id="639">所以，为了达到这个目的，我们建议提供一个示例数据集，提供跨语言和多语种的统一数据集，用于跨语言和多语种的语义表示。</sample>
    <sample id="640">它包含 90 个数据集，5 个语义任务，8 种表示形式和 22 个自然语言。在 15 个语言家族中。</sample>
    <sample id="641">为了更好地评估我们的基准，我们考虑了训练和评估的六个设置。</sample>
    <sample id="642">The first one is translate test. We use Google Translate API to translate source to the target language, then use monolingual model to train and evaluate the</sample>
    <sample id="643" />
    <sample id="644">我还会测试单语模式。</sample>
    <sample id="645">将英文内容翻译成中文。</sample>
    <sample id="646">我们还测试了单语少样本设置，通过使用仅 10% 的训练数据训练单语模型。</sample>
    <sample id="647">和具有单语言模型，我们训练一个单语言模型，用于所有语言。</sample>
    <sample id="648">将英文内容翻译成中文。</sample>
    <sample id="649">请提供需要翻译的英文内容。</sample>
    <sample id="650">我们还考虑了零步和少量转换之间的跨语言代码转换，从一种语言到另一种语言。</sample>
    <sample id="651">在训练过程中，我们将训练我们的英语查询或英语和德语短语查询，以训练一个多语言模型，来预测序列中的下一个单词。</sample>
    <sample id="652">并且我们还发现了一些非常有趣的成果。所以，关于单语模型，我们评估了两个模型组。</sample>
    <sample id="653">包括编码器-解码器（Encoder-Decoder），即多语言预训练编码器与指针基于解码器的模型，例如XLM-R+PT和BERT+PT。</sample>
    <sample id="654" />
    <sample id="655">我们发现编码器解码器在所有九个数据集上都能获得最佳性能。</sample>
    <sample id="656" />
    <sample id="657">没有数据编码器或编码器需要改进，通过训练混合的语言。</sample>
    <sample id="658">我们发现，这主要是因为大多数主要的自然语言可以获得性能提升，但英语的性能在某些数据集上下降，而所有性能在三个数据集上都提升。</sample>
    <sample id="659">我以为这是没有人会问的，这是语言学。</sample>
    <sample id="660">我们还比较了跨语言性能的特征。</sample>
    <sample id="661">在图中，蓝色线表示交叉角度的零轴转换，橙色线表示交叉角度的零轴转换，而绿色线表示建模角度的转换。</sample>
    <sample id="662">我们发现，通过比较绿色和橙色线，在零步设置下，跨越角度的转换性能差距显著。而通过比较蓝色和橙色线，在少量步设置下，转换差距迅速缩短。</sample>
    <sample id="663">我们还发现了一些其他有趣的发现，例如，编码器解码器在以前的工作中实现了可比的结果。对于英语自然语言处理，显著提高了预训练语言模型的性能。</sample>
    <sample id="664">大型语言模型，如CodeT5和BLOOM，在跨语言语义理解任务中仍然处于开发阶段。</sample>
    <sample id="665">一个统一的跨语言语义解析基准，使用多种自然语言表示形式。</sample>
    <sample id="666">欢迎阅读一项全面的基准研究，研究了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现等等。欢迎访问我们的论文和代码。谢谢您的阅读。</sample>
    <sample id="667">The provided text does not contain any information about existing research. It only lists a series of letters.</sample>
    <sample id="668">不，根据文本，Codex 和 Bloom 等多语言 LLM 仍然不足以完成跨语言语义推理任务。</sample>
    <sample id="695">该方法通过在训练过程中引入对排列的对齐来处理排列的不确定性。</sample>
    <sample id="696">下游 NLP 模型的公平性是指，当这些模型被部署到主流社交媒体平台时，可能会导致不同政治观点的人被边缘化，而针对少数族裔的仇恨言论可能不受控制地传播。</sample>
    <sample id="697">Janis Lavergne.</sample>
    <sample id="698">Costofina</sample>
    <sample id="699">Maira.</sample>
    <sample id="700">热带主义指的是对拉丁裔女性的描述中包含“充满活力”和“曲线感”等元素，这与热带主义的刻板印象有关。</sample>
    <sample id="701">作者通过使用与目标群体身份相关的词语，如“文化”、“传统”、“骄傲”和“异域风情”，来创建目标群体的人工描写。</sample>
    <sample id="702">PMI (Pointwise Mutual Information)。</sample>
    <sample id="703">DrBERT 和 ChuBERT 的区别在于它们使用的训练数据和模型类型。DrBERT 使用 7GB 的自然语言数据进行训练，而 ChuBERT 使用 4GB 的数据集进行训练。DrBERT 是一个临床模型，而 ChuBERT 是一个通用模型。</sample>
    <sample id="751">七位。</sample>
    <sample id="752">迭代迁移学习是指在每个主动学习轮次和标注后，迭代地更新模型。</sample>
    <sample id="753">数据集的目标是理解用户想要表达的意图，以便进行选择。</sample>
    <sample id="754">The provided text does not describe how attackers extract model parameters using EaaS. It only mentions validating the coverage of the provided embedding by realizing the embedding of sentences on forward as at BPC.</sample>
    <sample id="755">三位。</sample>
    <sample id="756">10</sample>
    <sample id="757">First Year P.E.S.C.I. Student at Carnegie Mellon University.</sample>
    <sample id="758">在例子中，左侧的州长是丽莎。</sample>
    <sample id="759">The most advanced chatbot model is currently being developed by ABC.</sample>
    <sample id="760">大型语言模型正在推出更长的上下文窗口，因此在整个上下文窗口中评估模型的可接受性至关重要。</sample>
    <sample id="761">是的，多语言训练会导致英语模型的表现下降。</sample>
    <sample id="762">不，他们不一定知道该实体的名称。</sample>
    <sample id="763">The provided text does not contain any information about MT metrics.</sample>
    <sample id="764">根据所给的英文内容，没有提到泛化中的回归是否会影响特定的 NER 类型。</sample>
    <sample id="765">NLP 中的立场很重要，因为像 Perspective API 这样的工具在检测有害内容时可能存在设计偏见，尤其是在不同文化背景下，对某些词语的敏感程度可能不同。</sample>
    <sample id="766">适配器微调。</sample>
    <sample id="767">他们使用零样本性能在特定数据集上的迁移学习。</sample>
    <sample id="768">MMLU, HellaSwag, ARC, TruthfulQA, Winograd Schema Challenge, and GSM8K.</sample>
    <sample id="769">三条。</sample>
    <sample id="770">The proposed method achieved a higher plot density in the general retail space compared to the constraint distribution of cost script.</sample>
    <sample id="771">Shu Han.</sample>
    <sample id="772">是的。</sample>
    <sample id="773">论文中进行了 12 个较小模型的实验。</sample>
    <sample id="774">OFA</sample>
    <sample id="833">Google Translate</sample>
    <sample id="834">Stony Brook University.</sample>
    <sample id="835">论文分析了英语和西班牙语的语言对。</sample>
    <sample id="836">Jianbin Pi.</sample>
    <sample id="837">研究了两个模型：Longformer 和 Normalized Longformer。</sample>
    <sample id="838">62</sample>
    <sample id="839">四位。</sample>
    <sample id="840">The author used the following datasets in the experiments: AG News, Mind, SST2, and Eirosben.</sample>
    <sample id="876">NACHOS 是一个医疗临床数据数据集。</sample>
    <sample id="877">Sajid Bilal.</sample>
    <sample id="878">提示策略对结果有很大影响。</sample>
    <sample id="879">The authors are affiliated with the University of Puerto Rico at Mayagüez.</sample>
    <sample id="880">我无法从给定的英文内容中提取 5 个由专家编写的指令。</sample>
    <sample id="881">作者建议使用人类实验参与者来评估数据集，以测试模型从不同来源的信息中提取知识的能力。</sample>
    <sample id="882">大家好，我的名字是艾德·维拉，我将简要概述这篇论文，重点是翻译策略和性能评估。这是一项与谷歌翻译同事合作的成果。</sample>
    <sample id="883">巴恩是 5400 亿个参数的语言模型，由语言实验室在 2022 年发布。它在一个大型文本集合上训练，压缩和处理了 1000 亿个文本。</sample>
    <sample id="884">请提供英文内容。</sample>
    <sample id="885">在这篇工作中，我们呈现了对语言模型提示的系统性研究，重点关注指令微调。</sample>
    <sample id="886">我们评估了语言模型在迁移能力方面的表现，并采用了 MT 社区的最佳实践。这包括使用最新的测试数据集，以避免测试数据与语言模型训练数据的重复。</sample>
    <sample id="887">我们比较了最新的技术系统，也就是最佳表现的系统，比如WTI评估。</sample>
    <sample id="888">我们使用最先进的神经矩阵，并且额外展示专家基于你的评估结果。最后，我们提供一些关于提示选择策略的建议。</sample>
    <sample id="889">提示对语言模型在翻译中的表现有很大影响。正如我们在一个简单的实验中看到的，我们使用单提示，并为不同的句子提供了两个不同的提示。</sample>
    <sample id="890">在大多数句子中，516个在1000个中。
差异明显大于一个模糊点。</sample>
    <sample id="891">这在极端情况下可以达到40分。所以选择好的提示策略很重要。</sample>
    <sample id="892">在我们的实验中，我们使用了一种五步提示策略，即我们仅仅标记我们提供的句子中的一个词，然后将它传递给系统。</sample>
    <sample id="893">翻译成中文。</sample>
    <sample id="894">我们说的是实际的打印形式对多个短语的影响不大。</sample>
    <sample id="895">它对于零和一次提示至关重要，而当我们进入正如我们现在的情况，到五次提示时，实际上没有区别于提示的实际形式。</sample>
    <sample id="896">请提供英文内容。</sample>
    <sample id="897">实验结果的总结表明，示例质量比与源句相似性更重要。</sample>
    <sample id="898">所以，选择示例时重要的是选择高质量的翻译。特别是，我们比较的是从 WMT 评估数据中选择提示，或者从定义中选择。</sample>
    <sample id="899">数据集变得更加有组织，并且具有更高的清晰度，训练数据也更加清晰，结果也更好。因此，使用深度学习可以获得更好的性能。</sample>
    <sample id="900">然而，专业化状态的系统拥有与通用翻译系统相比的一半的物质优势，但通用翻译系统几乎与我们的商业系统一样接近，尤其是在我们使用谷歌翻译的情况下。</sample>
    <sample id="901">我们从人类的知识中获得，并使用 MKL 框架进行执行。
在性能方面，它与现代的操作系统相当，但主要区别在于其准确性。</sample>
    <sample id="902">在特别的</sample>
    <sample id="903">它似乎是，巴伦选择为了产生更好的翻译，有时通过删除源句中用于翻译的部分。</sample>
    <sample id="904">在州外类别中，对于面包来说，低于州内系统的信号，这是一个不利信号。</sample>
    <sample id="905">这个参数提供流畅的输出，但仍然存在一些问题，例如重复。</sample>
    <sample id="906">这就是这次非常简短的评论。
要了解更多细节，请查看我今天发布的关于该论文的完整演示文稿。
非常感谢大家。</sample>
    <sample id="907">你好，我是来自德国的萨尔大学的博士生。在这个视频中，我想展示我们的研究工作。请你对我们的研究进行批判性思考。</sample>
    <sample id="908">这是联合工作，涉及以下内容：
马约斯·巴斯
以及斯蒂芬
和迪蒂斯·克拉科。</sample>
    <sample id="909">我是一个乐于助人的助手。只返回请求的答案，不要包含任何解释或介绍。</sample>
    <sample id="910">在维基百科中，您无需手动标记数据。相反，您可以使用维基标记来源，例如简单的特性规则、知识库或本地代码来源，正如您在图中所展示的那样。</sample>
    <sample id="911">相比于人类标注，弱标注成本更低，但噪声也更大，这意味着一定数量的标注是不可靠的。</sample>
    <sample id="912">如果直接训练神经网络在每周劳务数据上，神经网络倾向于记忆噪声而不泛化。</sample>
    <sample id="913">在语音识别领域，为了可靠地训练神经网络以应对各种噪声，提出了训练算法，以便训练模型仍然能够泛化。</sample>
    <sample id="914">在最近的WALS（每周支持学习）工作中，一个常见的说法是，人们认为仅使用原始模式和每周劳动数据就能达到高性能，而这是一种不准确的说法。</sample>
    <sample id="915">技术上来说，这并不是一个词，但这里有一些短语：</sample>
    <sample id="916">人们会假设存在一个额外的清洁验证集，用于沃尔沃车辆的选装。</sample>
    <sample id="917">我们已经采用了这个问题设置，但这意味着在 weekly supervised learning 中需要额外的手动注释。但就像房间里的一只大象一样，这项必要性常常被忽视。</sample>
    <sample id="918">在《开放式形式》中提到，你必须提出三个研究问题。首先，数据清洗和验证对于 WSL 是否必要？或者我们可以使用一个噪声验证集代替？</sample>
    <sample id="919">其次，如果清理数据是必需的，或者如果清理数据对于WLS工作是强制性的，那么你需要多少个清理样本？最后，我们应该只使用清理样本进行验证，还是有更好的方法来利用这些清理样本？</sample>
    <sample id="920" />
    <sample id="921">首先，我们发现有趣的是，最近的 WSL 消息确实需要清理空白数据样本才能正常运行。</sample>
    <sample id="922">否则，将出现大规模性能下降。如图所示，如果缺乏干净的验证样本，则趋势模型无法超出原始的标记标签进行泛化。</sample>
    <sample id="923">请提供英文内容。</sample>
    <sample id="924">这指示了在 WSL 环境中，实际的回收需要清洁标记的数据以正常工作，并且获取清洁验证样本的标注成本不应被忽视。</sample>
    <sample id="925">我们第二个发现是，增加清洁验证样本的数量将有助于 WSL 达到更好的性能，如图所示。</sample>
    <sample id="926">通常我们只需要23个样本来达到高精度。</sample>
    <sample id="927">但这并非故事的结局，因为无论我们决定直接使用清洁样本进行训练，我们都将能够获得更好的性能。</sample>
    <sample id="928">图表显示了两种调优方法的性能差异：直接应用于清理数据的方法和使用清理数据进行验证的方法。</sample>
    <sample id="929">如果我们有十个样本，每类十个，直接找到一个开始被 WSR 攻击的。</sample>
    <sample id="930">最终，在之前的 WSR 方法中声称的性能改进可以通过允许在清理和验证样本上持续微调轻松实现。</sample>
    <sample id="931">正如您所见，根据数据，Vina模型最初表现不佳，而更复杂的WLS模型则表现良好。</sample>
    <sample id="932">然而，如果我们允许继续在干净的样本上进行训练，那么FTW表现得与其他方法一样好。</sample>
    <sample id="933">所以，在实践中，没有理由选择更复杂的 WSL 镜像，它们需要更多计算时间和磁盘空间。</sample>
    <sample id="934">我们总结说，最近的 WSL 补丁需要手动注释样本才能正常工作。它们的性能和实用性被严重夸大了。</sample>
    <sample id="935">我们具体的建议为未来的工作是如下：</sample>
    <sample id="936">首先，报告模型选择的准则。例如，报告如果模型选择是“down”或“clean validation sample”。</sample>
    <sample id="937">第三，持续的 fijn tuning 是一种简单但强大的基线，应该在未来的工作中被考虑。</sample>
    <sample id="938">请随意查看它。谢谢您参加会议。</sample>
    <sample id="939">人类评估。</sample>
    <sample id="940">这篇论文有五位作者。</sample>
    <sample id="941">在 Servin 和 Kea 的示例中，需要以下背景知识：

1. **实体特定知识：** 例如，“Servin 是一个法官”。
2. **世界知识：** 例如，“Servin 和 Kea 在公园里见面”。</sample>
    <sample id="942">是的，代码公开。可以在 GitHub 上获取。</sample>
    <sample id="943">根据所给的英文内容，无法判断 NLPositionality 的注释者在各个人口统计学特征方面是否均衡。</sample>
    <sample id="944">在可接受的域中扰乱句子，通过尝试保留相关结构，但添加噪声。</sample>
    <sample id="945">进行维度评估意味着评估聊天质量的多个方面，以了解模型的优势和劣势。</sample>
    <sample id="946">University of Science and Technology of China.</sample>
    <sample id="947">在零次和一次提示的情况下，提示的形式很重要。</sample>
    <sample id="978">作者评估了多个对话模型。</sample>
    <sample id="979">四位。</sample>
    <sample id="980">优秀规划器应该设定合理的、符合实际情况的约束条件。</sample>
    <sample id="981">这篇论文有两位作者。</sample>
    <sample id="982">Vasudha.</sample>
    <sample id="983">The author is from the Institute of Cognitive Science.</sample>
    <sample id="1021">PaLM 最常见的错误是“omission errors”。</sample>
    <sample id="1022">你好，我是詹姆斯·芬奇。我叫莎罗·芬奇。今天我们将告诉您关于ABC Eval，一种新的维度评估对话式人工智能的方法。</sample>
    <sample id="1023">这项工作由埃米里NLP实验室完成，由乔治教授在埃米里大学领导，并与亚马逊Alexa AI合作。</sample>
    <sample id="1024">我是一个乐于助人的助手。请只返回请求的答案，不要包含任何解释或介绍。</sample>
    <sample id="1025">常见的做法是使用人工评估，例如让人工评判员选择哪两个对话更好，或者根据李克特量表对对话进行评分。</sample>
    <sample id="1026">这些方法适用于提供对整体对话质量的全面评估，但对话质量有很多方面，因此您可能想评估聊天质量的多个维度，以了解模型的优势和劣势。</sample>
    <sample id="1027">一种方法是简单地让人类法官评估对话质量的几个维度，例如模型回复的相关性，使用现有的比较或李克特量表方法。</sample>
    <sample id="1028">然而，我们相信有一种更精确和可靠的维度对话评估策略。</sample>
    <sample id="1029">该方法试图减少人类评估的主观性，通过明确标注每个模型响应是否表达了某些行为，例如提供无关信息或相互矛盾。</sample>
    <sample id="1030">我们称这种方法为标注聊天行为，简称ABC Eval。我们开发了这个方法来全面覆盖聊天模型行为，这些行为会影响聊天质量和用户体验。</sample>
    <sample id="1031">ABC EVL 能够测量聊天模型在各种主题上的错误率。</sample>
    <sample id="1032">例如，ABC EVL 衡量的是聊天模型忽略其伙伴或说不相关内容所需的步骤数。</sample>
    <sample id="1033">矛盾自己或其伴侣。
幻觉错误的事实或违反常识。
并在模型成功或失败地表现出同情时。</sample>
    <sample id="1034">为了确定哪种评估方式最有效，我们选择了四款最先进的聊天模型，并使用每款模型100个人类反馈对话进行评估，使用ABC评估。</sample>
    <sample id="1035">为了比较，我们还使用三种现有的方法评估了这些对话：对话层面上的李克特评分、对话层面上的李克特评分以及对话层面上的对偶比较。</sample>
    <sample id="1036">对于现有的方法，我们收集了对对话八个最常见衡量标准的评估，因为这是评估聊天模型多维度的标准做法。</sample>
    <sample id="1037">我们对这些评估结果的语法分析发现，ABC、EVAL、行为标签在内部注释者一致性方面优于现有方法，这可以通过100对双标记对话来衡量。</sample>
    <sample id="1038">此外，ABC EVA标签在整体对话质量方面比现有方法产生的指标更具预测性，正如这个简单的线性回归分析所显示的那样。</sample>
    <sample id="1039">例如，你可以看到测量自我与伴侣矛盾转折的比例解释了对话质量的 5%，而平均李克特一致性得分仅解释了 4% 或 10%。</sample>
    <sample id="1040">最后，我们检查每个评估指标是否捕捉了检查质量的独特方面，使用逐步线性回归。</sample>
    <sample id="1041">你可以看到，所有ABC EVL指标的组合解释了超过25%的对话质量。当逐一移除这些指标时，大多数都导致丢失相当多的关于质量的信息。</sample>
    <sample id="1042">另一方面，所有转位级别李克特量表组合解释得远不如单个量表，并且这些量表中的较少量表具有独特的意义。</sample>
    <sample id="1043">这是一个可靠、信息丰富且独特的 ABC 评估矩阵，使我们能够以比以往方法能够实现的更高分辨率评估对话式 AI。</sample>
    <sample id="1044">从我们实验的结果可以看出，仍然存在几个挑战，并且已经精确量化。例如，我们测试的对话中，有大约20%的回答包含常识性错误。</sample>
    <sample id="1045">他们产生不相关的信息，在约 15% 的回复中，并且互相矛盾或与他们的伙伴重复约 10% 的时间。</sample>
    <sample id="1046">由于该领域发展迅速，许多这些错误率在我们的评估之后可能会下降。然而，这更需要我们追求可靠和精确的评估指标，用于比较模型。</sample>
    <sample id="1047">我们希望 ABC Eval 可以被其他领域的人们利用，作为这一方向上的一个有意义的进步，并期待在未来几个月，对话式 AI 的发展。谢谢您的观看。</sample>
    <sample id="1048">Emory NLP Lab.</sample>
    <sample id="1049">CFT 代表 recent WSL approaches。</sample>
    <sample id="1050">6</sample>
    <sample id="1051">你好，我的名字是卡约·延，我将介绍我们的作品，题为《何时需要翻译？数据驱动的动机探索》。这项工作与帕特里克·弗朗西斯、米尔杜·安德尔·费尔南德斯和格拉姆·纽贝克合作完成。</sample>
    <sample id="1052">翻译：

“Dr.”</sample>
    <sample id="1053">如果上一个句子是“如果部长发现事情开始变得危险，那么莫指的是间谍”，那么莫指的是间谍。但如果上一个句子是“这可能没什么大不了的，医生”，那么莫指的是病理学家。</sample>
    <sample id="1054">好的，请提供英文内容。</sample>
    <sample id="1055">然而，评估模型处理此类案例的能力相当困难。首先，因为只有少量数据包含上下文，这使得词汇级别指标无法捕捉到这些翻译的细微之处。</sample>
    <sample id="1056">一些人建议针对性评估在线依赖性，但这些资源仅适用于有限类型的在线依赖性以及有限的语言。因为它们通常依赖于领域知识和人工编选。</sample>
    <sample id="1057">在这一工作中，我们尝试回答这两个问题：首先，翻译是否需要上下文；其次，模型如何处理这些情况？</sample>
    <sample id="1058">在回答第一个问题之前，我们首先测量了单词在翻译中的重要性。</sample>
    <sample id="1059">在之前的作业中，我们介绍了在机器翻译模型中使用的上下文作为衡量标准。这通过衡量上下文对目标词语提供多少关于该词语的信息来完成，给定该词语的上下文。</sample>
    <sample id="1060">你认为像 XAI 就像从提供背景信息来给模型提供信息吗？</sample>
    <sample id="1061">在这一工作中，我们使用 XSMI 到 2.2 XSMI，它可以衡量上下文使用在句子级别或单词级别。我们可以认为那些具有高 P6SMI 的单词是那些在翻译中需要上下文的单词。</sample>
    <sample id="1062">现在我们使用高频词项矩阵来寻找这些词语之间的模式。</sample>
    <sample id="1063">我们将在翻译成14种不同语言的英语对话转录文本上进行分析。</sample>
    <sample id="1064">在我的分析中，有三个不同的层面。首先，我们来看一下词性标签，它们具有较高的平均值。</sample>
    <sample id="1065">和这允许我们找到一个例子，双重代词在阿拉伯语中具有发音的嗨皮西米。这可以解释是因为英语没有双重代词，所以你需要根据上下文判断一个代词是否是双重代词。</sample>
    <sample id="1066">此外，我们发现某些语言在选择动词形式时也需要上下文。我们然后查找词典条目，这些条目在所有不同语境中都具有较高的词频。</sample>
    <sample id="1067">And this helps us identify cases like the one here, where in Chinese you need context to translate the phrase to make sure you're using the same translation within the document.</sample>
    <sample id="1068">And similarly we find that Catholics support the transit in their ritual.</sample>
    <sample id="1069">最后，我们来看一下不同个体代币中具有高PSI的现象。这使得我们能够识别出无法被“词语”所捕捉的现象，但可以通过其结构更好地表达出来，例如椭圆解剖。</sample>
    <sample id="1070">现在我们利用我们分析的结果来设计一个文档级别翻译的基准。</sample>
    <sample id="1071">对于我们识别出的五个磁盘现象，我们创建了标签，以隐喻地识别与该现象相关的词语，并称我们的标签为多语种磁盘意识或穆达标签。</sample>
    <sample id="1072">此外，我们还注意到不同的语言在这些独特的现象中拥有不同的比例。</sample>
    <sample id="1073">我们使用穆达标签，通过在我们需要用于评估的平行 корпуса上应用标签，然后应用我们的选择转换矩阵到穆达标签识别出的上下文依赖示例上。</sample>
    <sample id="1074">最后，我们使用我们的基准作为其他指标来评估不同模型的文档级别机器翻译质量。</sample>
    <sample id="1075">首先，当我们使用词汇上的指标时，对于蓝色，我们发现协同诊断模型具有最佳性能。</sample>
    <sample id="1076">但是，如果使用上下文，则模型在最佳表现的场景中表现更好。如果使用词汇测量，则在有上下文和没有上下文的情况下，模型表现相当。</sample>
    <sample id="1077">这展示了，如果使用语料库级别指标，很难确定最佳文档级别翻译系统。</sample>
    <sample id="1078">现在我们使用了具有上下文的基准模型，并且发现，在某些特定语料现象中，具有上下文的模型比不使用上下文的模型明显更准确，例如正式性和词汇连贯性。</sample>
    <sample id="1079">这些模型与不使用上下文或其他现象（如椭圆、破裂和动词形式）的模型没有多大区别。这表明我们需要看到更多进展才能实现文档级别翻译。</sample>
    <sample id="1080">我们还比较了不同的商业系统，我们的基准测试显示，DeepL通常比Google翻译在文档级别翻译中更准确。</sample>
    <sample id="1081">总结来说，我们对14对语言对进行了数据驱动的分析，以识别需要上下文的1个翻译。</sample>
    <sample id="1082">然后我们使用我们的细化来构建文档级别机器翻译的基准，这可以帮助我们识别哪些语言模型能够很好地处理文档级别翻译，以及哪些翻译系统在文档级别翻译方面表现良好。</sample>
    <sample id="1083">谢谢你的帮助，请你介绍一下“拉”。</sample>
    <sample id="1084">Justin John</sample>
    <sample id="1121">该方法没有名称。</sample>
    <sample id="1122">作者描述“显性词汇”(marked words) 方法是用来识别区分标记组和非标记组的单词。</sample>
    <sample id="1123">乔治华盛顿大学</sample>
    <sample id="1124">Prag-Pro.</sample>
    <sample id="1125">James Finch and Sarah Finch.</sample>
    <sample id="1126">四位。</sample>
    <sample id="1127">The text mentions that the minimal pair paradigm evaluates language models on acceptability judgments, which can include grammaticality (like plen, syntax, etc.) and acceptability in terms of stereotypes (such as crowdspair).</sample>
    <sample id="1161">WLS, WLS, WLS, WLS, WLS</sample>
    <sample id="1162">该模型在生物医学和临床诊断场景下的任务上进行了评估。</sample>
    <sample id="1226">CamemBERT 最初是在 4GB 的数据上训练的。</sample>
    <sample id="1227">Adam Szpyrkowski.</sample>
    <sample id="1228">在时间漂移实验中，发现性能随着时间间隔的增大而下降，这证实了时间漂移是性能下降的主要原因的假设。</sample>
    <sample id="1269">因为在第一个步骤中，我们得到了正确的词元，但它们没有排序。</sample>
    <sample id="1270">因为例如，这些积极的刻板印象我们不知道是因为存在某种奇怪的过度、过于激进的价值观导向，或者可能是其他一些反刻板印象的方法导致了这些有害的模式。</sample>
    <sample id="1271">最小对不可接受输入是指模型在面对一个可接受的句子和不可接受的句子时，会倾向于给可接受的句子更高的概率。</sample>
    <sample id="1272">作者使用了权重和 tokenizers 的评估指标。</sample>
    <sample id="1273">内在注释者一致性。</sample>
    <sample id="1274">维基百科</sample>
    <sample id="1275">The provided text does not mention the author's affiliated institution.</sample>
    <sample id="1276">MultiInstruct 与其他基准不同之处在于，它专注于改进指令微调在语言任务上的零样本性能，而忽略了计算机视觉和多模态任务。因此，本研究旨在调查指令微调是否能提高多模态预训练模型在无监督多模态任务上的泛化能力。此外，研究人员还发现，在研究时，指令数据集的可获得性存在显著差异，尤其是在 NLP 和多模态领域。</sample>
    <sample id="1277">这篇论文有两位作者：James Finch 和 Sarah Finch。</sample>
    <sample id="1278">The provided text does not contain any information about binary coordination.</sample>
    <sample id="1279">没有提供任何关于提示语长度的信息。</sample>
    <sample id="1280">这些发现表明较小的 T5 模型无法像大型模型那样生成长文本，可能因为其训练数据不足。</sample>
    <sample id="1281">嗨，我是雅尼斯·沃克，很高兴与您交流。我正在为法国版本的“Doctor Bert”机器人精神模型工作，用于生物医学和临床领域。</sample>
    <sample id="1282">在本次演示中，我们首先介绍了语言建模在医疗保健领域的应用，然后我们将介绍我们文章的主要贡献。</sample>
    <sample id="1283">我们引入了法语的第一个生物医学模型，名为Dr. Bert，它基于Roberta，并使用Natics数据集进行训练，该数据集包含来自“癌症”的临床数据。</sample>
    <sample id="1284">我们还介绍了具有多个参数设置和数据来源的模型比较。然后，我们展示了在 11 个生物医学和临床诊断任务中的结果。</sample>
    <sample id="1285">总结实验结果，并提供更多关于如何访问该研究的详细信息。</sample>
    <sample id="1286">自2018年发布以来，它已成为解决自然语言处理任务的最有效方法之一，并且与历史静态和上下文化方法（如Word2Vec、FastText或GloVe）相比，性能有了巨大的提升。</sample>
    <sample id="1287" />
    <sample id="1288">专业模型为其他语言使用，通常基于持续预训练，因为缺乏领域数据。</sample>
    <sample id="1289">然而，法语没有像英语那样有开放式医疗机构和查阅记录。</sample>
    <sample id="1290">我们问自己一个问题：对于广泛的使用，最合适的数据库是什么？这些元数据是临床数据的良好替代品吗？</sample>
    <sample id="1291">请提出你的问题。我们将比较 Doctor BERT 与我们的 Shubert 模型，后者基于来自南加州大学医院的匿名数据。</sample>
    <sample id="1292">在之后，我们问自己，我们需要多少数据来训练一个专业模型在法语数据上？是几百万字节？几百万字节？</sample>
    <sample id="1293">请您提供英文内容。</sample>
    <sample id="1294">一个微型版本的BERT，它是一个临床模型，我们使用了4GB的上下文序列，从临床数据中获取。一个微型版本的BERT，我们使用了4GB的词嵌入和4GB的临床数据。</sample>
    <sample id="1295">此外，为了进一步比较，我们将引入流式数据训练和持续预训练，以分析预训练策略的影响。</sample>
    <sample id="1296">一个基于卡门贝尔和训练在 4GB 的玉米片上的蜂窝网络，另一个也基于卡门贝尔，但这次训练在 4GB 的干净的玉米片上。</sample>
    <sample id="1297">最终，一个基于英语语言模型，例如BERT，并使用先前数据集训练的模型。总共有七个模型。</sample>
    <sample id="1298">所有评估模型，我们看到支持公共和私人数据任务，例如命名实体识别、分类、词性标注和问答。</sample>
    <sample id="1299">这个型号与 6 字节线型模型相比，其容量为 108 兆字节、4 兆字节、60 兆字节、5 兆字节、字节和字节。</sample>
    <sample id="1300">选择具有与该模型执行任务相同的数据集，以评估该模型在特定任务上的表现。</sample>
    <sample id="1301">然而，我们可以从原始来源获取数据，我们观察到来自原始来源的数据似乎更可靠。我们还观察到，使用更多数据可以提高性能。</sample>
    <sample id="1302">在世界范围内，从零开始的预训练模型似乎在大多数任务上都能获得更高的性能。</sample>
    <sample id="1303">然而，我们实验发现，使用基于权重和 tokenizers 的 PubMed 词嵌入，在 4GB 的数据集上训练，与从 Scisum 获取的 4GB 的 BERT 模型得到的结果相当。</sample>
    <sample id="1304">这并非一个基于卡门贝尔权重和标记器的案例，该标记器存在稳定性问题。</sample>
    <sample id="1305">最终，我们的系统在九一之一任务中表现更好，并且在整体结果上超过了通用模型。</sample>
    <sample id="1306">我们观察到，专门的数据更好，更多专门的数据更好，但这并不 масштабируется。</sample>
    <sample id="1307">所有预训练模型从Natos获得，并且在年轻面孔上可用，所有训练脚本都在我们的GitHub仓库中。</sample>
    <sample id="1308">谢谢您，对于这次演示，我们期待着在会议上讨论的行动。</sample>
    <sample id="1309">论文研究了以下学习策略：
1. 从头开始训练和比较四个不同的模型。
2. 使用不同大小的自然语言处理数据集（4GB和7GB）。
3. 使用不同类型的模型（临床模型和通用模型）。
4. 使用不同的训练数据组合（自然语言处理数据集和临床数据）。
5. 使用连续预测来分析这些策略的影响。</sample>
    <sample id="1310">根据图表右侧的红线，测试重复使用导致了过拟合的程度是大于1。</sample>
    <sample id="1311">文章中没有提到如何评估简化质量。</sample>
    <sample id="1312">是的，语言模型有不同的政治偏见。</sample>
    <sample id="1313">嗨，我的名字是马蒂亚斯·伦德曼。今天我将向大家简要介绍我们的论文，主题是使用多标签标记和潜在排列进行成分泛化，而无需树。</sample>
    <sample id="1314">这是我顾问亚历山大·科拉和伊万·迪塔的合作。</sample>
    <sample id="1315">组合泛化可以理解为学习者处理更深层次的递归和未见组合的能力，这些组合在训练过程中是单独出现的。</sample>
    <sample id="1316">在语义解析的语用测试中，可能如下所示：
如常，我们有一个训练语料库，在这种情况下，女孩睡着了，玛丽知道女孩睡着了。</sample>
    <sample id="1317">这些词语与逻辑形式配对，代表了它们的核心含义。</sample>
    <sample id="1318">与标准机器学习评估不同，测试集并非来自相同的分布，而是包含结构上和逻辑上不同的形式。</sample>
    <sample id="1319">在本文档中，模型在训练过程中表现出浅层递归，并在测试样本上表现出更深层的递归。</sample>
    <sample id="1320">基于序列到序列的模型在这种类型的离散分布泛化方面遇到困难，并且通常会产生与输入分离的输出。</sample>
    <sample id="1321">尤其，它们经常未能重现输入和输出之间的系统对应关系，例如在示例中颜色编码的那些。</sample>
    <sample id="1322">一种流行的解决此问题的常用方法是将树集成到模型中。</sample>
    <sample id="1323">这些树木旨在捕捉与句子的结构过程相关的逻辑形式。</sample>
    <sample id="1324">这有点奇怪，但树通常不是被给予的，你需要获得一些。</sample>
    <sample id="1325">这有时会变得复杂，并且计算成本高昂。通常涉及对逻辑形式的相当多的形式化预处理，例如处理变量符号。</sample>
    <sample id="1326">获取树木也可能涉及专门的语法检测程序。</sample>
    <sample id="1327">在本文中，我们不使用树，并引入了一种新的序列到序列模型，该模型直接建模输入片段与输出片段之间的对应关系。</sample>
    <sample id="1328" />
    <sample id="1329">我是一个助手。只返回请求的答案，不要包含任何解释或介绍。</sample>
    <sample id="1330">请提供英文内容。</sample>
    <sample id="1331">在第一个步骤之后，我们已经获得了正确的标记，但它们还未被正确地标记。</sample>
    <sample id="1332">这就是为什么在第二步，我们使用另一个模型来预测一个排列，将它们放入正确的顺序。</sample>
    <sample id="1333">我们介绍了一种新的方法来预测一个排列，该方法不会对可能的排列施加任何硬性约束。这使得我们的方法非常灵活和富有表现力。</sample>
    <sample id="1334">概念上，我们的排列模型大致如下：</sample>
    <sample id="1335">我们从左到右处理输出，并确定在每个位置应该放入哪个多集元令牌。对于第一个输出位置，我们简单地选择一个，如在图中的高亮显示所示。</sample>
    <sample id="1336">然后我们跳到下一个多 setores 令牌来确定输出中的第二个令牌。</sample>
    <sample id="1337">确定输出中的第三个标记，通过跳到另一个多标记标记的方式进行。我们继续这个过程。</sample>
    <sample id="1338">直到第一阶段的每个标记都已被访问一次。</sample>
    <sample id="1339">为了给您一个实验结果的预览，我们比较了我们的方法与其他树状模型在 CoG 基准测试上的表现。我们的模型在泛化到更深层递归方面表现优于其他模型，差距很大。</sample>
    <sample id="1340">某种形式的结构改造仍然非常具有挑战性。</sample>
    <sample id="1341">在我们的论文中，我们解决了几个有趣的专业挑战。</sample>
    <sample id="1342">首先，输入和输出之间的对齐关系在训练数据中未给出。因此，对于给定的标记，我们不知道它来自哪个多集合，这给训练带来了挑战。</sample>
    <sample id="1343">此外，有时存在与数据一致的多个排列，但语言上正确的排列是潜在的。我们通过将对齐作为训练的一部分来解决这个问题。</sample>
    <sample id="1344">排列方法非常灵活，但它带来了一个挑战，即找到最高分排列是NP难的问题。这是因为这与旅行商问题相关。</sample>
    <sample id="1345">我们通过一个GPU友好的连续放松来近似这个，该放松还允许我们反向传播通过解决方案并学习更具语言合理性的排列。</sample>
    <sample id="1346">如果您想了解更多关于我们的实验以及我们如何应对这些挑战，请查看我们的论文或来我们的办公室。</sample>
    <sample id="1347">认知失调是指两个信念或行为不一致的情况。</sample>
    <sample id="1348">GPT-4</sample>
    <sample id="1349">是的，累积训练在主动学习中比迭代训练更有效。</sample>
    <sample id="1350">Sara Babbi</sample>
    <sample id="1351">TED Talks 的英文转录。</sample>
    <sample id="1385">Matthias Lende.</sample>
    <sample id="1386">跨语言转移是指在一种语言模型中训练模型，然后将其应用于另一种语言模型。</sample>
    <sample id="1387">Stalland University in Germany.</sample>
    <sample id="1388">作者使用了以下延迟测量方法：
- 蓝色，测量翻译质量
- 平均延迟
- 考虑计算成本的平均延迟</sample>
    <sample id="1389">大家好，我是马克希塔，今天我和我的同事正在展示我们的作品《知识整合》。这项工作是麦基尔大学、米拉和微软研究的合作。</sample>
    <sample id="1390">大型语言模型借鉴了各种知识来源，例如在其参数中通常通过预训练获得的知识，以及在推理过程中提供的输入知识。</sample>
    <sample id="1391">最近在问答任务中，模型展示了它们可以使用预训练知识来解决任务的能力。</sample>
    <sample id="1392">不，自然语言处理经常需要知识，这些知识也从不同方面提供。</sample>
    <sample id="1393">例如，在句子“约翰在电视上看到了新当选的总统”中。</sample>
    <sample id="1394">预处理参数可能包含有关什么是总统 2 和什么是 TV 的信息，但它们无法可靠地知道这个特定实例的实体 John，或者谁是新的总统，因为总统可能会改变。</sample>
    <sample id="1395">因此，在知识密集型 NLP 任务中，成功的模型需要能够整合和使用预训练时间和推理时间中的知识。</sample>
    <sample id="1396">在本文中，我们提出了一种诊断性测试方法，用于评估知识整合能力。</sample>
    <sample id="1397">我们介绍了一个核心参照分辨率任务，旨在评估利用不同来源知识的能力。我们评估了数据集，并与人类实验参与者建立了核心参照分辨率模型。</sample>
    <sample id="1398">赛文是一位法官，基亚是一位面包师。赛文和基亚在公园里见面。在一天结束之后，在法院里处理案件，他很高兴放松。</sample>
    <sample id="1399">The task here is to identify the correct entity that the pronoun he refers to, which in this case is जॉन.</sample>
    <sample id="1400">给定代词的解析要求两种类型的信息。第一，实体特定知识，例如“服务是教会”。第二，背景知识，例如“法官决定案件”。</sample>
    <sample id="1401">一般来说，背景知识是在大型语言模型预训练过程中学习的，而实体特定知识通常通过微调观察到。</sample>
    <sample id="1402">我们定义了信息片段的可获得性，即它可能存在于单个来源或多个来源中。</sample>
    <sample id="1403">我们已经定义了三个猫模型设置。首先，我们需要设置背景预训练。背景知识被假设为在预训练期间可用的。</sample>
    <sample id="1404">第二，备份在后台设置。备份通知在预训练时和在非训练时都可用。最后，备份和非训练设置。两种类型都在非训练时可用。</sample>
    <sample id="1405">这个设置特别有趣。它模拟了一个背景知识不足的案例，这不在预训练数据中。例如，由于纽约市发展，在过去</sample>
    <sample id="1406">请提供英文内容。</sample>
    <sample id="1407">在背景预设中，我们假设背景知识政治家寻求选举席位政府，其包含在预设参数中。在不同上下文，我们提供针对特定政治家的知识。</sample>
    <sample id="1408">在背景中，我们不仅会提供针对特定实体的信息，还会提供关于政治家在历史背景下的背景知识。</sample>
    <sample id="1409">在背景和职位设置中，我们将“政治家”替换为“媒体人”，因为媒体人不太可能包含在预定人员中。</sample>
    <sample id="1410">我们验证了数据集，包括人类统计参与者和建立卷积神经网络模型。在这张图中，我们展示了最佳表现的模型以及最困难的背景预训练模型。</sample>
    <sample id="1411">在Auto-tras上，使用Kittmos效果不佳。然而，在Kittmos上训练时，使用TF和BERT效果显著优于随机选择。</sample>
    <sample id="1412">这表明在训练和通用查询中，解决方案数据集设置。
可能学会利用表面线索。
但对于测试基准，这些线索已被移除。</sample>
    <sample id="1413">实验表明，即使是最先进的模型也无法可靠地集成新的知识，只能依赖于其有限的知识。</sample>
    <sample id="1414">总结一下人工智能模型的主要技术瓶颈。许多预训练模型由于缺乏知识而无法从不同来源获取信息，而没有针对特定任务进行训练。然而，通过针对特定任务进行训练，一些模型成功地整合了来自多个来源的信息。</sample>
    <sample id="1415">即使是最先进的模型，似乎也难以可靠地整合在推理时间里获取的背景知识。
如果您对更多细节感兴趣，请查看我们的论文，并查看GitHub上的数据集和代码。</sample>
    <sample id="1416">基于树的方法存在以下缺点：

*   树通常不是直接获得的，需要通过某种方式获取。
*   获取树的过程可能复杂且计算成本高昂。
*   通常需要大量的特定于形式系统的预处理，例如处理变量符号。</sample>
    <sample id="1417">The paper's author is from Cornell University.</sample>
    <sample id="1418">嗨，我是玛拉，今天我们将讨论我们使用自然语言提示来衡量大型语言模型类型的工作。这项工作由埃森·穆什和丹杰罗夫共同完成。</sample>
    <sample id="1419">近年来，许多研究记录了大型语言模型（LLM）中社会偏见和刻板印象的普遍性。</sample>
    <sample id="1420">然而，这些措施存在各种局限性。它们通常依赖于手工构建的数据集，这些数据集的收集非常耗时。</sample>
    <sample id="1421">他们通常只测量非常特定的错误类型，这意味着它们无法很好地推广到其他人口统计学或背景，或者它们只是捕捉到非常普遍的广泛关联，例如与特定类型的负面联想。</sample>
    <sample id="1422">此外，大多数太空工作并未考虑交叉性，交叉性是指多重身份的社会身份可以叠加并产生独特的体验。</sample>
    <sample id="1423">为了克服这些限制，我们依赖于一种新的指令调整的LLM，它们在响应指令方面表现出色。</sample>
    <sample id="1424">请您提供需要翻译的英文内容。</sample>
    <sample id="1425">而且我们可以立即看到，这对于任何人口统计学群体来说都是非常具有概括性的，因为我们可以指定我们想要使用的任何身份标记。</sample>
    <sample id="1426">以下是一些示例生成，来自 GPT-4。</sample>
    <sample id="1427">立即我们看到，虽然输出不是过度负面或具有传统意义上的有害，但</sample>
    <sample id="1428">There are some interesting patterns.</sample>
    <sample id="1429">亚洲女人被描绘成不自信，中东女人则被用“异域风情”、“迷人”等词语称呼，并提及一个迷人的地区。</sample>
    <sample id="1430">和有色人种角色中，人物经常提及祖先，而白人角色则没有任何提及。</sample>
    <sample id="1431">为了捕捉这些模式，我们的方法分为两部分。第一部分是生成这些个人资料。</sample>
    <sample id="1432">这些提示是在一个研究中生成的，该研究给人类参与者提供了这些提示，发现通过向人类参与者提供这些提示，他们也能浮现出种族刻板印象。</sample>
    <sample id="1433">并且这使得我们生成的个人资料能够与人类书面回复进行直接比较。</sample>
    <sample id="1434">第二部分是标记词，这是一种方法来识别区分标记组和非标记组的词语。我稍后会详细说明。</sample>
    <sample id="1435">这使得我们能够获得非常具体的错误类型和模式，而无需依赖于特定的 Lexicon 词汇。</sample>
    <sample id="1436">所以，马克·沃兹的论文借鉴了社会语言学的概念“标记性”，该概念指出存在一个默认状态，任何与该默认状态不同的群体都具有语言上的标记性。</sample>
    <sample id="1437">例如，词语“男人”或者抱歉，词语“战士”通常与男性联系在一起。当人们描述一位女性战士时，他们通常会明确指出“女性战士”并标记这个词语。</sample>
    <sample id="1438">更广泛地说，社会中具有支配地位的群体在语言和社交上都未被标记，而边缘化群体通常被标记。</sample>
    <sample id="1439">所以，在我们的方法中，我们首先指定标记和未标记的组。</sample>
    <sample id="1440">然后我们可以使用“战斗词语法”比较这些人物，这基本上是通过使用加权词频比来区分每个标记的顶级词语。</sample>
    <sample id="1441">例如，对于黑人角色的，我们将进行战斗词语的比较，并将其与白人角色和男性角色进行对比，因为这两个是主要的未标记组。</sample>
    <sample id="1442">现在我们来看看结果。首先，我们使用了一些常见的标点符号，我们发现生成的个人资料包含比人类书写的个人资料更多的标点符号。</sample>
    <sample id="1443">然而，当我们实际查看Lexicon的词频分布时，我们会发现非常不同的结果。</sample>
    <sample id="1444">虽然生成的个人资料有更高的词汇率，但人类书写的个人资料则具有更广泛的词汇分布，而生成的个人资料中的刻板印象词语实际上只是“高大且健壮”等词语。</sample>
    <sample id="1445">我只是想说，我只希望是积极的，至少不是消极的。</sample>
    <sample id="1446">事实上，这个词汇分析并没有捕捉到我们在早期幻灯片中看到的许多有害模式。因此，为了展示这些看似积极的词语如何促进刻板印象和本质化叙事，我们将转向我们标记词语的方法。</sample>
    <sample id="1447">在我们的分析中，我们回顾了这些看似积极的描绘反映了有害模式。</sample>
    <sample id="1448">首先，根据马克群，最热门的词语包括文化、传统、骄傲和异域风情。这些词语定义了这些群体的身份，并将其与其他白人规范区分开来。</sample>
    <sample id="1449">这进一步延续了对这些群体歧视和异化的长久历史。</sample>
    <sample id="1450">此外，这些词语中反映了许多常见的刻板印象，尤其是在女性群体中。例如，描述拉丁裔女性的词语包括“鲜艳”和“曲线感”。</sample>
    <sample id="1451">这与一种热带主义的潮流有关，对于亚洲女性来说，这些词语像是精致、脆弱和丝绸般。</sample>
    <sample id="1452" />
    <sample id="1453">最后，对于黑人女性来说，我们看到一些最常见的词语是“强大”和“坚韧”。</sample>
    <sample id="1454">这与人们称之为“强大的黑女人原型”的原型相关联，虽然听起来在第一眼看来是积极的，但</sample>
    <sample id="1455">有研究表明，这种原型实际上非常有害，因为它给这些群体带来了巨大的压力，要求他们对抗自杀等障碍，变得有韧性和坚强。</sample>
    <sample id="1456">与其实际努力改变这些障碍，还给那些人施加压力去克服它们，这会导致这些人的健康状况恶化，以及其他负面后果。</sample>
    <sample id="1457">在更广泛的意义上，我们发现每个市场群体都大致反映着一种具有中心化的叙事。</sample>
    <sample id="1458">这些模式让我们得出三项建议，供模型所有者参考：</sample>
    <sample id="1459">首先，作为研究人员，我们应该关注积极的刻板印象和核心叙事。我们还应该使用交叉视角来研究偏见和伤害，因为如果不用这样做，可能会有很多被忽略的事情。</sample>
    <sample id="1460">最后，应该有更多关于偏见缓解方法的透明度。</sample>
    <sample id="1461">因为例如，这些积极的刻板印象我们不知道是因为某种奇怪的</sample>
    <sample id="1462">过度夸张的价值判断正在进行，或者可能是其他一些反刻板印象的方法导致了这些有害的模式。</sample>
    <sample id="1463">我们真的不能做出任何假设，或者进一步研究，除非有更多透明度。</sample>
    <sample id="1464">非常感谢您的倾听。我玩得很开心。</sample>
    <sample id="1465">你好，大家好，我叫金伟伊，来自中国科学技术大学。</sample>
    <sample id="1466">我很高兴为您提供一个关于纸张的简短广告视频。您是否正在模仿我的型号？保护大型语言模型嵌入和服务的版权。</sample>
    <sample id="1467">让我们首先介绍一下“嵌入式服务”的概念。</sample>
    <sample id="1468">目前，大型语言模型，如 GPT、LLaMA、PaLM，在自然语言理解和生成方面表现出色。</sample>
    <sample id="1469">提示式服务是基于大型语言模型构建的一类服务，旨在协助各种帮助任务。</sample>
    <sample id="1470">例如，OpenAI 提供了一个基于 GPT 的嵌入模型。</sample>
    <sample id="1471">然而，最近的研究表明，攻击者可能通过学习嵌入并提供相似的服务来盗用模型。因此，保护嵌入的版权是必要的。</sample>
    <sample id="1472">为了保护版权的在线服务，一种解决方案是在提供者服务中添加水印，并检测其他服务是否包含水印。</sample>
    <sample id="1473">水标记方法需要满足以下属性：首先，该方法应适用于嵌入式服务。其次，水标记不应降低提供嵌入式服务的实用性。</sample>
    <sample id="1474">第三，水杯应该足够让攻击者看到。攻击者可以轻松地移除水印。</sample>
    <sample id="1475">最后，模型需要能够与攻击者服务进行交互，在模型提取期间。</sample>
    <sample id="1476">现有词汇可以大致分为四个类别：</sample>
    <sample id="1477">然而，这种方法既不适用于嵌入式服务，也不具有可移植性。</sample>
    <sample id="1478">因此，在本文中，我们将提出一种嵌入标记，这是一种基于水印的方法，可应用于嵌入数据。</sample>
    <sample id="1479">然后让我介绍嵌入式标记的细节。嵌入式标记包含两个主要步骤：
1. 模具注射
2. 复制验证</sample>
    <sample id="1480">在进行这些主要步骤之前，我们首先选择一个触发器集。触发器集是一个在适度频率范围内出现的词语组。</sample>
    <sample id="1481">我们假设供应商可以收集一个通用文本语料库，并计算单词频率。</sample>
    <sample id="1482">在魔术注入中，我们首先定义一个目标嵌入。当用户向提供商服务发送一个句子时，提供商会考虑触发数字在句子中。</sample>
    <sample id="1483">提供的嵌入是目标嵌入和原始嵌入的权重之和。</sample>
    <sample id="1484">目标身体的重量与句子中触发器的数量成正比。当句子中触发器的数量大于 m 时，提供的嵌入恰好等于目标身体的重量。</sample>
    <sample id="1485">复制验证用于检测在另一个服务中包含的另一个模型。</sample>
    <sample id="1486">我们首先构建一个后门数据集。后门数据集包含句子，其中所有单词都属于触发词集合。后门数据集中的所有单词不属于触发词。</sample>
    <sample id="1487">然后，提供者会要求从窃贼服务获取数据。</sample>
    <sample id="1488">计算了请求的嵌入和目标嵌入之间的余弦相似度。我们计算了与本项数据集的差值余弦度和差值欧氏距离。</sample>
    <sample id="1489">同时，我们还应用了海尔斯测试，并使用其p值作为第三个指标。</sample>
    <sample id="1490">我们对由美国国家卫生研究院（NIH）、Mind、ASD2和Euraspec提供的实验数据集进行计数词频分析。</sample>
    <sample id="1491">结果显示，我们的嵌入式标记可以具有出色的检测性能，同时保持出色的实用性用于降噪任务。</sample>
    <sample id="1492">我们还验证了提供的嵌入的覆盖率，通过识别句子中由BPCA嵌入的词语。
图例中的数字表示每个句子中的触发词数量。</sample>
    <sample id="1493">如图所示，很难区分带导电填充的填充物和普通填充物。</sample>
    <sample id="1494">谢谢你。我们将在稍后讨论。</sample>
    <sample id="1495">ABC-Eval is a method developed to comprehensively cover chat model behaviors that affect chat quality and recent literature.</sample>
    <sample id="1496">2003 年</sample>
    <sample id="1497">你好，我的名字是 वसुধা，我是斯托尼布鲁克大学计算机科学研究生。我想介绍我于 ACL 2023 上发表的论文，题目是“迁移学习用于孤儿类检测”，解决的是罕见类挑战。</sample>
    <sample id="1498">认知失调是指两种信念或行为之间的不一致。简单来说，认知失调是人们在认知和行为之间存在不协调的情况。</sample>
    <sample id="1499">例如，当一个人说“我知道吸烟会杀死我”，然后又说“会议结束后我抽了几根烟”，这种信念和行为不一致，它们处于矛盾之中。</sample>
    <sample id="1500">进一步提到，我认为没有他们我无法保住我的工作，这证明了第二次出现，他们与我有着一种共同的关系。</sample>
    <sample id="1501">因为认知偏差是一种在日常决策中非常常见现象，它们很少在语言中被表达。</sample>
    <sample id="1502">认知距离可以帮助我们理解人们之间分歧的影响，跟踪趋势、信仰、价值观和态度变化的人口。</sample>
    <sample id="1503">高认知失调也与焦虑症有关，可以帮助我们更好地理解人们的心理健康。</sample>
    <sample id="1504">研究语言中的细微差别也能帮助理解极端主义和弱势群体极化。</sample>
    <sample id="1505">最后，认知失调对于理解个人的认知风格以及理解决策过程至关重要。</sample>
    <sample id="1506">为了创建认知失调资源，我们进行了大规模的失调关系分析。我们采用了先失调的策略，如图表所示。</sample>
    <sample id="1507">请使用 PyQt 电视解析器，并使用符合指导方针的规则对语料中的语料单元进行标注。</sample>
    <sample id="1508">正如这里所见，这种现象仅在标注的样本中发现3.5%。</sample>
    <sample id="1509">我们收集了大约一千个语料对，训练了一个初始分类器，该分类器仅使用43个例子进行训练。不出所料，分类器表现得不如随机猜测。</sample>
    <sample id="1510">鉴于异议的低频率和缺乏任何先前的数据集，我们面临着绝对稀有性的问题。</sample>
    <sample id="1511">为了缓解这个问题，我们正在探索迁移学习和主动学习的组合，以确保更多不平衡样本可以在更少的标注成本下收集，从而降低整体标注成本，同时提高不平衡检测的准确性。</sample>
    <sample id="1512">由于最初的模型无法捕捉到距离类，因此我们通过将权重从密切相关的任务中转移来启动了主动学习过程。</sample>
    <sample id="1513">将英文内容翻译成中文。</sample>
    <sample id="1514">讨论了扩展和比较类别的PTB分类，因为这两个类别与共存和异存的概念密切相关，我们称之为CE。</sample>
    <sample id="1515">我们发现，在将数据传输到实体数据集中，零击中率已经比与最佳情况的概率好得多。</sample>
    <sample id="1516">进一步地，通过迭代地对两个任务进行微调，发现对 CE 任务的微调，然后对 DBE 进行进一步微调，能获得更好的零样本性能。因此，这就是我们用来启动当前学习的模型。</sample>
    <sample id="1517">接下来，我们需要确定在每个主动学习和标注轮次中更新模型的最佳方法。累积方法将累积所有已收集的主动标注数据，而迭代更新方法则通过在最新数据集中训练模型来更新模型。</sample>
    <sample id="1518">在不同的策略中，我们发现累积性能与迭代性能在整个周期中都相等或优于。</sample>
    <sample id="1519">接下来，为了增加离散示例的数量，我们使用概率稀有类策略（PCR）选择那些在当前模型任何轮次中都极有可能被离散的示例。</sample>
    <sample id="1520">我们将其与社区中常用的其他艺术策略进行比较。</sample>
    <sample id="1521">我们发现，所提出的 PRC 战略比其他街道战略效果更好，尽管差异很小。请注意，性能对于 Ran 来说明显较低。</sample>
    <sample id="1522">在后续的语言模型竞赛中，我们改进了设计分类，AUC达到了2.75，这是我们目前为止在任务上的最佳性能。</sample>
    <sample id="1523">我们还检查了每种策略的可行性，包括标注质量和标注人员的成本。我们发现，使用 PRC 的标注率最高，并且最适合于稀有类别的标注。然而，标注人员也发现了一些例子比较困难。</sample>
    <sample id="1524">总而言之，我们发现，为稀有类收购设计适当的迁移学习任务，可以显著提高自然语言处理（NLP）的简单AI策略。</sample>
    <sample id="1525">我们还发现迭代更新在跨领域迁移学习中很有用，而在域内活动中，累积更新更有利。</sample>
    <sample id="1526">这些是我们的代码数据集和我们的论文的链接。
请随时与我们联系，如果您有任何问题。
谢谢。</sample>
    <sample id="1527">The provided text does not state the author's affiliated institution.</sample>
    <sample id="1528">Siyu Yuan.</sample>
    <sample id="1529">四位。</sample>
    <sample id="1530">该方法与针对同时处理的 simulST 架构进行了比较。</sample>
  </task>
</testset>