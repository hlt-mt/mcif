<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="en">
    <sample id="0">The main data sources for language models are large-scale web crawl data, including news media from sources like the New York Times, Los Angeles Times, The Guardian, and Huffington Post.</sample>
    <sample id="1">McGill University, Mila, and Microsoft Research.</sample>
    <sample id="2">Tu Yi from Ant Group presented their team's paper on Visually-rich Document Understanding (VrDU), focusing on addressing the reading order issues in pre-trained multi-modal models. Existing models often use global 1D positions, which can be problematic for documents with complex layouts.

The proposed solution, LayoutMask, utilizes "local 1D positions" derived from in-segment token orders, aiming to enhance text-layout interactions.  LayoutMask differs from previous work in its choice of 1D position, masking strategy, and pre-training objectives.

To promote text-layout interactions, LayoutMask employs two novel masking strategies within Masked Language Modeling: Whole Word Masking (challenging, promotes context reliance) and Layout-Aware Masking (prioritizes masking at the beginning and end of segments).  A new pre-training objective, Masked Position Modeling (MPM), involves recovering randomly masked 2D positions, encouraging spatial reasoning and further interaction between text and layout.

Experiments on datasets like FUNSD and SROIE show that Local-1D outperforms Global-1D, particularly in cases with entities like "Total" that are difficult to identify based on a simple reading order.  The results suggest that local 1D positions are more adaptive to complex document layouts. The paper details these findings and further information are available in their paper and posters.</sample>
    <sample id="4">Kayo Yin.</sample>
    <sample id="5">T5 XL model.</sample>
    <sample id="6">Jiaan presents their work, "Towards Unifying Multi-Lingual and Cross-Lingual Summarization," a joint effort with several researchers. The core contribution is the introduction of "many-to-many summarization," a unified framework for generating summaries in any source language to any target language. This contrasts with existing approaches like multilingual and cross-lingual summarization, which restrict input and output languages to be the same or different, respectively.

The research analyzes the strengths and weaknesses of these approaches, finding that many-to-many summarization facilitates better knowledge transfer across languages.  They propose PISCES, a pre-trained many-to-many summarization model built through a three-stage pre-training process: meta-pretraining, cross-lingual pre-training, and task-specific pre-training.

Preliminary experiments on the WikiLingua dataset demonstrate that the many-to-many model outperforms multilingual and cross-lingual models in knowledge transfer.  PISCES also surpasses baselines like mBART-50 and mT5.  Further analysis and human studies validate the effectiveness of PISCES. The authors encourage readers to consult their paper for detailed information.</sample>
    <sample id="7">Yes, CoNLL-2003 taggers still work well in 2023.</sample>
    <sample id="8">ABC-Eval is a new dimensional approach to evaluating conversational AI that reduces subjectivity in human evaluation by explicitly annotating model behaviors like irrelevant responses, contradictions, and hallucinations. It measures the rates of these behaviors, offering a more reliable and informative way to assess conversation quality compared to existing methods like Likert scales and pairwise comparisons.</sample>
    <sample id="9">Clean validation samples.</sample>
    <sample id="10">The accuracy of language models can be improved by providing them with partially overlapping background knowledge, rather than just entity names. Domain-generalization is also possible.</sample>
    <sample id="11">Jack Hessel from AI2 presents research on humor understanding using The New Yorker Caption Contest data. The study explores how large language models (LLMs) perform on tasks related to humor, including matching captions to cartoons, ranking caption quality, and generating explanations for jokes.

The research utilizes a dataset of over 700 cartoons and corresponding captions, annotated with descriptions, entity links, and joke explanations.  The study found that while LLMs like CLIP achieve around 62% accuracy on caption matching, significantly lower than human performance (94%), even models like GPT-4 struggle with nuanced humor.  GPT-4's joke explanations often contain factual errors and fail to capture the intended humor.

The study highlights a substantial gap between LLM performance and human understanding of humor, even when provided with additional context.  The researchers emphasize the potential of the dataset and the leaderboard for further research in this area. They conclude that while LLMs can generate and even attempt to explain jokes, they don't yet truly "understand" humor in the way humans do. The research underscores the challenges of evaluating and replicating human-like humor comprehension in AI.</sample>
    <sample id="12">5</sample>
    <sample id="13">Daniel Rotem's presentation details research on adaptive inference in large language models, focusing on methods like Multi Model and Early Exit to reduce inference time and cost. Adaptive inference leverages data complexity to use smaller models for faster sampling.

The presentation highlights the trade-offs of each method. Multi Model offers versatility but is expensive to store and suffers from overhead due to sequential model execution. Early Exit provides faster inference and memory efficiency but faces the "conflicting gradients" problem, where classifiers' updates interfere with each other, degrading performance.

To address this, Rotem's team developed SWEET (Separating Weights in Early Exit Transformers), a novel fine-tuning method that trains each layer of an Early Exit model only with updates from the subsequent classifier. This avoids conflicting gradients.

Experiments comparing SWEET with standard Early Exit and Multi Model showed that SWEET significantly outperformed both, particularly for BERT-large, and closed the gap between Early Exit and Multi Model. While SWEET sometimes negatively impacts later classifiers, it consistently outperforms both methods in speed/accuracy trade-offs.

The key takeaways are the existence of conflicting gradients in Early Exit training and the introduction of SWEET, which motivates further research into tailored fine-tuning algorithms for Early Exit architectures. The research is available in the paper "Finding the SWEET spot" on Archive.</sample>
    <sample id="15">Three.</sample>
    <sample id="16">Bible texts are more strongly simplified than news texts or language learner texts.</sample>
    <sample id="17">Shengqiong Wu from NUS introduces their work on multimodal relation extraction, addressing challenges in extracting relationships between entities using both text and visual information. The research highlights issues of over-utilization of text-based information and under-exploitation of external information like topic data.

Their proposed method, called CMG (Cross-Modal Graph), tackles these problems by first constructing unified scene graphs from text and images.  A graph information bottleneck principle guides the refinement of these graphs, filtering nodes and edges to focus on relevant information.  Multimodal topic information is then integrated to enrich the context through attention mechanisms.

Experiments on a standard MRE dataset demonstrate significant performance improvements over text-based methods and multimodal baselines. Ablation studies show that both information screening and external information exploitation are crucial for optimal performance, with the relative importance varying based on the relevance of text and visual information.  

The research concludes that a combination of internal information pruning and external information enrichment is key to effective multimodal relation extraction.  A QR code is provided for further details.</sample>
    <sample id="18">"salt and pepper" is preferred over "pepper and salt" because the left conjunct ("salt") is shorter.</sample>
    <sample id="19">Zhang Qin from Shenzhen University presented their work, "A Survey for Efficient Open Domain Question Answering," accepted at ACL 2023. The presentation focused on the challenges of open-domain QA, particularly the large size of the Wikipedia corpus (26 million documents, 20GB) and the bottleneck of index searching (65GB).  The motivation is to create efficient systems with smaller memory, faster inference, and comparable performance.

The presentation outlined core techniques, including retrieval-only and generator-only frameworks, and explored methods for faster evidence retrieval (approximate nearest neighbor search), faster reading (skip reading), and reducing index size (document filtering, embedding compression) and model size (lightweight models, parameter sharing, knowledge distillation).

The speaker analyzed existing models, noting that retrieval-only systems are fast but require large indexes, while generator-only systems are large models with low performance.  The conclusion suggests that resource constraints favor generator-only systems or embedding compression, while real-time feedback benefits from retrieval-only systems.  Future work will explore deployment on low-power devices and the development of more comprehensive evaluation metrics.</sample>
    <sample id="20">Yes, the pre-trained models are freely available on Hugging Face under the MIT license, and the training scripts are on their GitHub repository.</sample>
    <sample id="21">DEPLAIN-apa is based on news texts.</sample>
    <sample id="22">Good generalization is achieved through:

*   Better model architecture (transformer models generally generalize better)
*   Larger model size
*   More fine-tuning examples 

These factors work together, and temporal drift, not adaptive overfitting, is the primary cause of performance drop.</sample>
    <sample id="23">Dan Garrette discusses challenges in text-image models rendering visual text, highlighting the limitations of current approaches like Imagen. While these models generate high-quality images, they often struggle with accurate text representation, even for simple words. This stems from the text encoder's reliance on subword tokenization (like SentencePiece), which requires the model to decompose words into individual letters.

Experiments show that even large T5 models exhibit poor spelling accuracy, particularly for frequent words, due to the way subwords are represented. PaLM models perform better but are computationally expensive. ByT5, which uses character-level tokenization, excels at spelling across all scales.

Garrette's team addressed this by augmenting Imagen with a text representation from ByT5-small, significantly improving spelling accuracy without drastically increasing model size.  While the diffusion model can still introduce errors, the improved text encoder allows for better text rendering.  The research introduces the WikiSpell and DrawText benchmarks and a new strategy: concatenating character-aware text representations to enhance model spelling abilities.  The key takeaway is leveraging character-level information to improve text rendering in text-image models.</sample>
    <sample id="24">The tendency for left conjuncts to be shorter was measured in terms of length in characters, syllables, and words.</sample>
    <sample id="25">The experiments measured the length of dependencies (in characters, syllables, and words) between conjuncts, with the governor on the left or right, to observe the tendency for the left conjunct to be shorter.</sample>
    <sample id="26">The baseline classifier performed not much better than chance when trained on only 43 examples of dissonance, highlighting the problem of absolute rarity in the data.</sample>
    <sample id="27">This information is not provided in the text. The text only mentions "our work" and "we propose".</sample>
    <sample id="28">Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="29">Context-aware MT models significantly improve over context-agnostic ones for formality and lexical cohesion. They show less improvement on phenomena like ellipsis, pronouns, and verb form.</sample>
    <sample id="30">LLM-Blender is a new ensemble learning framework for large language models (LLMs) proposed by AI2 and USC. The core idea is to leverage pairwise ranking and generative fusion to improve performance beyond relying on a single top-ranked model. The paper highlights that optimal model selection varies for each input, and a single model isn't always the best choice.

LLM-Blender consists of two stages: first, it runs multiple LLMs (n models) on an input and uses a "PairRanker" module to compare their outputs pairwise, learning to distinguish which candidate is better for a given input. This pairwise comparison generates a ranking matrix.  The PairRanker differs from prior methods by encoding pairs of candidates alongside the input, enabling more nuanced comparisons.

The second stage selects the top K candidates (e.g., top three) and feeds them into a sequence-to-sequence model for "generated fusion," producing the final output.  Experiments on the MixInstruct dataset demonstrate that LLM-Blender consistently outperforms individual models like Open Assistant and Vicuna, achieving significant improvements in metrics like BERTScore, BLUERT, and BARTScore. The authors release a unified codebase for evaluation and future research.  The key takeaway is that LLM-Blender offers a simple yet effective way to enhance LLM performance by considering multiple models for each input.</sample>
    <sample id="31">John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams.</sample>
    <sample id="33">The NLPositionality framework quantifies positionality by comparing annotations from diverse end-users with the performance of existing datasets and models using Pearson's R correlation score. This score measures the degree of alignment between the user annotations and the model/dataset predictions/labels.</sample>
    <sample id="34">CREST is a new framework for generating rationalizations and counterfactuals to improve text classification. It combines selective rationalization, which highlights faithful tokens, with counterfactual generation, which edits input to explore alternative scenarios. CREST generates counterfactuals by masking input tokens and prepending the gold label, then using a masked language model to fill in the masked parts. Human evaluation shows CREST's counterfactuals are more valid and natural than those from other methods.

CREST-Generation further enhances this by producing both rationalizations and counterfactuals, allowing for a shared rationalizer and predictor. This approach encourages the model to focus on the parts of the input relevant to factual and counterfactual reasoning. Experiments on IMDB demonstrate that CREST-Rationalization achieves top results, performs comparably to human-generated counterfactuals on contrastive datasets, and outperforms other methods on out-of-domain data.

Furthermore, CREST-Rationalization produces more plausible and simulable rationalizations, meaning they can effectively change the classifier's decision when presented with counterfactual edits.  The framework leverages counterfactuals during training to generate interpretable explanations that highlight contrasting input elements.  CREST aims to produce controllable, valid, and diverse counterfactuals for improved text understanding and classification.</sample>
    <sample id="36">ACL's "Learning Language-Specific Layers for Multilingual Machine Translation" addresses the challenges and opportunities of multilingual translation. The paper highlights the advantages of multilingual models – scalability, speed, and improved low-resource language support – while acknowledging limitations in capacity per language.

The authors propose Language-Specific Layers (LSLs) to increase capacity where needed without increasing inference costs. LSLs are regular transformer layers per language, selected at inference time based on the source or target language.  A key innovation is learning the optimal placement of these LSLs within the encoder, using a three-weight approach (shared, source, target) and training the model to determine the best component for each encoder layer.

Experiments on WMT21 news translation demonstrate significant improvements over baseline models and language adapters, particularly for low-resource languages. The learned architecture achieves substantial gains in metrics like chrF, spBLEU, and COMET, while maintaining faster inference speeds.  The paper also explores shared and separate decoder models and provides statistically significant results across 84 out of 90 translation directions. The authors encourage further exploration of the full paper and poster session for more details.</sample>
    <sample id="37">The previous study found that human subjects also surfaced racial stereotypes when given the same persona prompts as the language model.</sample>
    <sample id="38">The study used the enhanced version of the Penn Treebank and statistics from the paper "Why wouldn't you use universal dependencies".</sample>
    <sample id="39">This text doesn't explicitly state the number of authors. It only mentions "this paper" and refers to a paper in general.</sample>
    <sample id="40">The closely related tasks for cognitive dissonance are topic-independent stance classification (debate) and binary classification of expansion and comparison classes (CE).</sample>
    <sample id="41">PeaCoK is a new knowledge graph developed by the Natural Language Processing Lab at EPFL, in collaboration with Sony, designed to represent persona-grounded commonsense knowledge at scale. It contains 3,800 personas with 40,000 attributes, forming 100,000 personal inferences, and 9,200 connections between attributes and personas. This allows for rich, interconnected persona representations.

The project addresses the challenge of narrative systems needing to understand how personas influence storytelling. PeaCoK was built through three steps: selecting personas from existing knowledge graphs, inducing attributes using commonsense and language models, and crowdsourcing relation annotations with AI assistance.

Experiments demonstrate that PeaCoK improves language model performance in persona attribute inference, achieving better results than large pre-trained models like GPT-3.5.  Furthermore, PeaCoK enhances dialogue generation in PersonaChat, leading to more fluent, consistent, engaging, and persona-expressive conversations.  The results show that leveraging persona-centric knowledge is more effective than general social commonsense knowledge.  The study highlights the importance of interconnected persona knowledge for creating coherent and engaging narratives.  The PeaCoK knowledge graph and associated code are publicly available.</sample>
    <sample id="42">The provided text does not mention the number of authors involved in the paper. It only identifies Shuheng as the presenter.</sample>
    <sample id="43">This information is not provided in the text.</sample>
    <sample id="44">The NLPositionality framework differs from annotator disagreement literature by comparing end users (real people) with models and datasets, rather than just looking at annotator agreement or modeling annotator distributions. It also uses a framework enabled by Lab in the Wild and online crowdsourcing platforms to gather diverse annotations.</sample>
    <sample id="45">"Tall" and "athletic" words.</sample>
    <sample id="46">DeepL and Google Translate.</sample>
    <sample id="48">Two.</sample>
    <sample id="49">The MPP evaluations were performed up to a context length of 1024 tokens.</sample>
    <sample id="50">DEPLAIN is a new corpus for German text identification at both document and sentence levels, addressing limitations in existing corpora. It comprises two subcorpora: DEPLAIN-apa, based on manually aligned news texts (13,000 sentence pairs), and DEPLAIN-web, incorporating diverse domains with both manual and automatic alignments (30,450 sentence pairs). Analysis reveals varying simplification types across texts, with Bible texts being significantly more simplified than news or language learner texts. DEPLAIN exhibits a high variety of simplification transformations, such as more reorderings and additions in DEPLAIN-apa compared to rephrasings in DEPLAIN-web.

The corpus is valuable for evaluating automatic alignment methods, with MASSalign identified as the best performing method for German text simplification.  Furthermore, DEPLAIN facilitates fine-tuning language models for automatic text simplification.  Experiments with long-mBART and base mBART demonstrated that fine-tuning can achieve scores exceeding baseline performance, establishing a new benchmark for the field. The authors provide code and checkpoints for reproducibility. DEPLAIN offers a comprehensive resource for research in German text simplification, enabling both evaluation and development of advanced simplification techniques.</sample>
    <sample id="51">AltEntities Corpus includes data from music, books, and recipes domains.</sample>
    <sample id="52">Positionality refers to the perspectives people hold as a result of their demographics, identity, and life experiences. It can influence research decisions and outcomes because datasets and models aggregate judgments and opinions of real people, potentially representing certain positionalities over others.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">Vasudha from Stony Brook University presents their ACL 2023 paper on transfer learning for dissonance detection in language. Dissonance, the inconsistency between beliefs and actions, is a rare but important phenomenon in language, impacting understanding of disagreement, belief trends, mental health, and cognitive styles.  The research addresses the challenge of low data availability by employing a dissonance-first annotation approach.

Initial attempts with a small dataset yielded poor performance. To overcome this, they explored transfer learning from related tasks like stance classification (debate) and expansion/comparison (CE) from the PDTB dataset.  Transferring weights from these tasks significantly improved zero-shot performance.  They then investigated active learning strategies, comparing "Cumulative" and "Iterative" data update methods, finding "Cumulative" performed equally or better.

A Probability-of-Rare-Class (PRC) strategy was used to select examples likely to reveal dissonance, outperforming other state-of-the-art active learning approaches.  Through iterative active learning, they achieved a dissonance classification AUC of 0.75.  The study also assessed annotation quality and cost, finding PRC effective for rare class acquisition, though challenging for annotators.  The research concludes that PRC and transfer learning are valuable for tackling the rare-class challenge in dissonance detection.</sample>
    <sample id="55">Yes, EDAtt adapts an existing offline ST model by leveraging the knowledge already acquired by the model through the attention mechanism. It doesn't require retraining or adopting a specific architecture for SimulST.</sample>
    <sample id="56">The provided text does not mention the number of authors. It only names Yusen Zhang from Penn State University.</sample>
    <sample id="57">Yes, the tested models perform significantly better on the KITMUS test suite when trained on it compared to a random choice, suggesting they can learn to integrate knowledge from multiple sources. However, even the best models struggle with reliably integrating knowledge presented only at inference time.</sample>
    <sample id="58">The three variants of KITMUS are:

1.  Background-Pretrain
2.  Background-Both
3.  Background-Inference</sample>
    <sample id="59">Yanis Labrak's presentation introduces DrBERT, a robust pre-trained language model in French for biomedical and clinical domains. Building upon the success of BERT and its adaptations in other languages and domains, DrBERT is the first open-source biomedical model in French, trained on the NACHOS dataset of medical crawled data.

The research addresses the scarcity of specialized French language models and explores optimal data sources for training.  A comparative study of seven models – DrBERT, ChuBERT, and continual pre-trained models – was conducted using varying data sizes (4GB, 7GB) and pre-training strategies (CamemBERT, PubMedBERT).  These models were evaluated on 11 downstream tasks, including named entity recognition, classification, and question answering, against six baseline models.

The results indicate that models trained on data similar to the training data perform best, but heterogeneous data shows greater versatility.  More data generally leads to better performance.  From-scratch pre-training yields superior results, though continual pre-training with CamemBERT weights achieves comparable performance to DrBERT 4GB.  DrBERT outperformed the generic model, CamemBERT, on nine of the eleven tasks.  The research highlights the importance of specialized data, but its effectiveness diminishes with increasing data size.  The pre-trained models and training scripts are publicly available on Hugging Face and GitHub, respectively.</sample>
    <sample id="60">Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="61">Should we only use the clean samples for validation, or are there better ways to utilize them?</sample>
    <sample id="62">Nitay Calderon introduces their ACL paper, "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training," a collaborative effort with Amir and Subhabrata from Microsoft and advisor Roi. The paper addresses the growing need to compress large language models (LLMs) in NLG to reduce costs without sacrificing performance.

The study explores knowledge distillation, a technique where a smaller "student" model learns from a larger "teacher" model.  They focus on task-specific distillation in NLG, contrasting it with previous work on classification or pre-training.  Their research utilizes realistic, industry-driven setups with medium-resource labeled data, large unlabeled data, and a focus on inference time efficiency.  Four NLG tasks – summarization, question generation, common sense reasoning, and simplification/style transfer – are investigated.

The paper systematically examines architectural decisions, the impact of pruning, and different approaches to knowledge selection.  A key contribution is an exploration of pseudo-targets, challenging the standard sequence-level distillation by proposing methods for generating multiple, diverse pseudo-targets through sampling and temperature adjustments.  Furthermore, they introduce "joint-teaching," a novel technique combining word-level distillation on pseudo-targets generated by both teacher and student, to address student exposure bias and encourage self-correction. The paper provides a detailed methodology and encourages readers to consult the paper for further information.</sample>
    <sample id="63">The sensitivity metric measures the model's ability to consistently produce the same outputs for the same task, regardless of slight variations in the wording of the instruction.</sample>
    <sample id="64">Jingwei Yi.</sample>
    <sample id="65">The presentation suggests that greater sensitivity indicates lower performance. The researchers found that a model with lower sensitivity is better at consistently producing the same outputs for the same task, which is a desirable characteristic of a good model.</sample>
    <sample id="66">The paper "Deep Learning for Mathematical Reasoning" explores the field of AI and NLP focused on enabling machines to solve math problems and prove theorems. Mathematical reasoning encompasses text-based problems with arithmetic operations, multimodal problems involving images and tables, and automated theorem proving.

The paper discusses various approaches, including sequence-to-sequence models and sequence-to-tree models, which map input problems to output solutions. Large Language Models (LLMs) have shown promise, particularly when guided by chain-of-thought prompting. However, LLMs still struggle with precision and consistency.

To address these limitations, researchers are exploring self-consistency methods and augmenting LLMs with external tools like program-aided LLMs (e.g., Chameleon) to compose different tools for complex tasks.  

The paper also highlights the underexplored area of mathematical reasoning in low-resource languages and the development of benchmarks in specialized domains like finance, science, and medicine.  Despite progress, models often face generalization and robustness issues, particularly with large numbers and inconsistencies in mathematical reasoning.</sample>
    <sample id="67">This work investigates interference in multilingual translation models, exploring how synergy between language pairs can be disrupted. The study identifies that severe interference occurs when models are small relative to data size, and that temperature tuning is crucial for strong performance. 

The research focuses on the relative difference in loss between a bilingual and a multilingual model, defining interference as this difference. They find that language similarity and the number of languages have a limited impact on interference levels. While language similarity can initially show a difference, it largely diminishes with sufficient data.

Experiments reveal that severe interference is primarily observed in small models, resolving with increased model size.  The study highlights that a simple solution is temperature sampling, where higher temperatures (T &gt; 1) allow for sampling from lower-resource languages.  Uncalibrated temperature values, even in larger models, can also negatively impact performance.

The authors conclude that model and data size are the primary factors influencing interference, with modest scale and tuned temperature offering a significant mitigation strategy without requiring specialized algorithms.  The findings suggest that a baseline approach of scaling up model size and calibrating temperature is sufficient to address interference in multilingual translation.</sample>
    <sample id="68">The provided text doesn't explicitly detail the linguistic context models receive during pretraining. However, it implies that models are sensitive to latent syntactic and semantic features shared across sentences, suggesting they are pretrained on a large corpus of text containing diverse linguistic structures and information. The paper focuses on how these features influence acceptability judgments, implying the models have learned a broad range of linguistic patterns during pretraining.</sample>
    <sample id="69">Typically, around 20 clean samples per class are needed to attain high performance in WSL. However, directly fine-tuning on the clean data can achieve even better performance.</sample>
    <sample id="70">Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models is done in collaboration with Esin Durmus and Dan Jurafsky.</sample>
    <sample id="71">Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis's work introduces the AltEntities Corpus, a large-scale dataset for evaluating entity selection in conversational systems. The corpus focuses on indirect referring expressions, which are common in natural language but often overlooked in current models.  The dataset comprises 6,000 alternative questions across music, books, and recipes, containing 42,000 indirect referring expressions.

The data is collected using a cartoon completion setup, where annotators choose an entity from an alternative question given a dialogue context.  The alternative questions are generated using a template with Wikipedia entities, with varying sampling methods to increase difficulty. Annotators are provided with background knowledge about the entities, such as Google search links for songs or Wikipedia text for books and recipes.

Experiments with the T5 XL model show that accuracy improves with access to more relevant background knowledge, ranging from 60% to 95% depending on the knowledge's overlap with the annotators' knowledge. The study demonstrates the potential for domain-generalizable models and highlights the importance of considering indirect referring expressions for better conversational AI. The dataset is publicly available.</sample>
    <sample id="72">The need for new methods to measure media biases arises because language models are trained on large datasets of news, which inherently contain political opinions and biases. These biases can then be propagated into the language models, leading to unfair outcomes in downstream applications like hate speech detection and fake news detection, potentially marginalizing certain groups or allowing harmful content to spread.</sample>
    <sample id="73">Akshatha.</sample>
    <sample id="74">Dense-ATOMIC addresses the limitations of ATOMIC, a large-scale commonsense knowledge base, by constructing a densely-connected knowledge graph. ATOMIC suffers from sparse graph structures and limited multi-hop paths due to its B-to-A link structure. Dense-ATOMIC overcomes these issues by adding B-to-B, A-to-B, and A-to-A links, significantly increasing knowledge coverage and enabling multi-hop reasoning.

The construction of Dense-ATOMIC involves normalizing tail events and training a relation prediction model, Rel-CSKGC. Rel-CSKGC predicts relations between head and tail events using RoBERTa embeddings and MaxPooling, addressing sparsity and leveraging semantic information.  A novel Intra- and Inter-Cluster Completion Strategy is employed for training.

Evaluations demonstrate that Rel-CSKGC outperforms existing relation prediction methods and translation-based methods on both automatic and human evaluations.  Dense-ATOMIC itself exhibits higher knowledge coverage and benefits the performance of COMET models, leading to more diversified results.  Furthermore, Dense-ATOMIC facilitates the discovery of multi-hop paths, enabling more complex commonsense reasoning. The paper concludes that Dense-ATOMIC enhances commonsense reasoning capabilities.</sample>
    <sample id="75">Zheng Yandan and colleagues present Jointprop, a novel joint semi-supervised learning framework for Named Entity Recognition (NER) and Relation Extraction (RE). The work addresses the limitations of existing approaches that often neglect the interconnections between NER and RE tasks, such as syntactic similarities and dependencies. Jointprop aims to leverage these connections by propagating labels across heterogeneous graphs constructed from labeled and unlabeled data.

The framework consists of four key components: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization.  Span features are generated from contextualized representations of tokens. A k-Nearest Neighbor graph is built to capture similarity between unlabeled data and labeled data. Label propagation then distributes labels through the graph, refining pseudo-labels until convergence. Finally, the model is retrained using the converged pseudo-labels and a confidence filter.

Experiments on joint and single-task datasets demonstrate that Jointprop significantly outperforms baseline models, particularly on joint datasets where the codependency between NER and RE tasks is exploited.  The framework shows consistent improvements across both NER and RE tasks in single-task settings.  The authors highlight the potential of Jointprop to fully integrate information and infer correct labels by considering all inter- and intra-connections within the data.</sample>
    <sample id="76">The political bias propagation pipeline involves:

1.  **Pretraining Data:** Large-scale web crawl data, including news from various sources (e.g., New York Times, The Guardian), inherently contains political biases.
2.  **Language Models:** Pretraining on this data results in language models exhibiting varying political leanings (e.g., GPT-4 is more liberal than BART).
3.  **Downstream Tasks:** These language models, with their inherent biases, are applied to tasks like hate speech detection and fake news detection, leading to unfair outcomes based on demographic and political leaning.

The pipeline essentially transfers political biases from the training data to the language models and then to the applications.</sample>
    <sample id="77">The research paper "On Improving Summarization Factual Consistency from Natural Language Feedback" from Yale University and Microsoft Research introduces the DeFacto dataset, designed to improve factual consistency in abstractive text summarization. The dataset contains human demonstrations and feedback on system-generated summaries, focusing on identifying and correcting factual errors.  The study proposes three new Natural Language Generation (NLG) tasks: summary editing, feedback generation, and automatic factual error correction, using the XSum dataset as a foundation.

The research highlights that a significant portion of XSum summaries already contain factual errors, leading to a trade-off between improved factuality scores and reduced textual overlap with reference summaries when using human-edited versions.  The study demonstrates that fine-tuned models and zero-shot large language models can effectively utilize human feedback for summary editing.  Feedback generation remains a challenging task.  Furthermore, the research shows that an editor model can achieve comparable performance to baseline models with fewer training data, and generating explanations alongside corrections improves performance.  The DeFacto dataset offers valuable fine-grained annotations for training factuality metrics and meta-evaluation. The dataset is publicly available on GitHub.</sample>
    <sample id="78">Yes, the simplification process differs between DEPLAIN-apa and DEPLAIN-web. DEPLAIN-apa, based on news texts, shows stronger simplification in general, particularly for Bible texts. DEPLAIN-web, including different domains, has more rephrasings, while DEPLAIN-apa has more reorderings and word additions.</sample>
    <sample id="79">Yes, the CoScript dataset is publicly available. The paper mentions that the dataset is generated and the data quality is ensured through crowd-sourced revision.</sample>
    <sample id="80">The watermark is inserted by counting the frequency of trigger words in a sentence. If the number of trigger words exceeds a threshold (m), the provided embedding is equal to a target embedding. This is done by weighting the original embedding with the number of triggers in the sentence.</sample>
    <sample id="81">Penn State University.</sample>
    <sample id="82">This paper introduces ULRA (Learning from Rank Aggregation), a novel framework for unsupervised automated essay scoring (AES).  AES aims to evaluate essay quality without human labeling, addressing the challenges of data scarcity and cost associated with supervised approaches.  Existing unsupervised methods struggle with inconsistent or uncontrollable quality signals. ULRA overcomes these limitations by aggregating multiple heuristic quality signals – such as unique word count – to create a unified supervision for training a neural AES model.

The framework comprises a heuristic essay ranking module (HER alpha-shot) that generates partial-order pairs based on signal values, and a Deep Pairwise Rank Aggregation module (DPRA) that learns to aggregate these pairs.  DPRA utilizes a learnable confidence weight for each signal to address inconsistencies and improve the model's ability to judge essay quality relationships.  A scoring strategy is implemented to map predicted scores to a predefined range.

Experiments on both transductive and inductive settings demonstrate that ULRA significantly outperforms existing unsupervised baselines and achieves competitive performance compared to cross-prompt and one-shot methods.  While ULRA surpasses unsupervised methods, its performance remains lower than supervised approaches due to the lack of strong supervision.  The paper highlights ULRA's effectiveness in leveraging multiple heuristic signals to achieve robust and accurate unsupervised essay scoring.</sample>
    <sample id="83">Yes, training encoder-decoder models like mT5 on a mixture of languages can improve their performance, especially in cross-lingual semantic parsing. The results show that this training leads to performance gains across most languages, except for English, which experiences a performance drop.</sample>
    <sample id="84">Shwai He's ACL 2023 paper introduces PAD-Net, an efficient framework for dynamic networks that addresses the issue of excessive parameter usage in fully dynamic architectures.  Traditional dynamic networks, where all parameters are dynamic, often lead to significantly larger model sizes, hindering practical applications. PAD-Net hypothesizes that fully dynamic networks contain redundant dynamic parameters that can be replaced with static parameters without sacrificing representation power. 

The framework partitions parameters into dynamic and static components, utilizing scale factors to control their intensity and a constraint-based Iterative Mode Partitioning method to identify and convert redundant dynamic parameters to static ones.  Experiments demonstrate that PAD-Net achieves superior performance compared to static and fully dynamic networks while maintaining fewer parameters and lower computation. Ablation studies reveal optimal dynamic ratios for dynamic convolution and Mixture of Experts, as well as the importance of scale factors.  PAD-Net outperforms network pruning by preserving static parameters and enhances output discrimination. Future work includes extending PAD-Net to other network architectures, hardware-friendly structures, and incorporating more parameter modes.  The paper highlights the potential of PAD-Net to improve the efficiency and practicality of dynamic networks.</sample>
    <sample id="85">"Make a chocolate cake" is an example of constrained language planning. It's a specific goal with constraints (chocolate flavor).</sample>
    <sample id="86">They visualize the embedding of sentences on four datasets using PCA to show that the backdoor embeddings are hard to distinguish from normal embeddings.</sample>
    <sample id="87">DrBERT builds upon the RoBERTa model, a pre-trained language model, and fine-tunes it on a French biomedical dataset called NACHOS. They also explore continual pre-training using CamemBERT weights and tokenization.</sample>
    <sample id="88">GPT-4 is least aligned with non-binary people compared to men and women.</sample>
    <sample id="89">"I'm going to talk about..."</sample>
    <sample id="90">Haneul Yoo and colleagues argue that language learners can be valuable contributors to NLP data annotation, challenging the traditional reliance on native speakers. Their paper, "Rethinking Annotation: Can Language Learners Contribute?", investigates the feasibility of using language learners, particularly in low-resource languages where native speaker recruitment is difficult.

The authors conducted a proof-of-concept study using English, Korean, and Indonesian, employing the GLUE benchmark and CFR criteria to categorize learners into basic, intermediate, and advanced levels. They compared the accuracy of language learners with native speakers, controlling for various variables.  Experiments involved pre-tests, annotation sessions with varying levels of support, and post-tests to assess learning effects.

Results showed that language learners produced nearly accurate labels, especially for simpler tasks.  Aggregated labels from learners performed comparably to native speaker labels.  Furthermore, training language models on learner-annotated data achieved high performance, sometimes exceeding that of models trained on native speaker data.  The study also demonstrated that annotation tasks improved language proficiency and vocabulary/grammar.  The authors conclude that language learners can significantly contribute to NLP data annotation, opening up opportunities for broader research in low-resource languages.</sample>
    <sample id="91">The more tasks used for instruction tuning, the better the model's performance, and the lower its sensitivity.</sample>
    <sample id="92">The provided text does not explicitly name three treeless baselines that the authors compare their method with. It only mentions that their method outperforms "other treeless models" on the COGS benchmark.</sample>
    <sample id="93">Alexander Koller and Ivan Titov are advisors to the first author, Matthias Lindemann.</sample>
    <sample id="94">Jingwei Yi from the University of Science and Technology of China presents a paper on protecting copyright for embedding as services, a growing area built upon large language models like GPT and LLAMA.  The core problem is that attackers can potentially steal models by learning from embeddings and replicating the service.  The paper proposes "Embedding Marker," a backdoor-based watermark method to address this.

Embedding Marker involves two key steps: watermark injection and copyright verification.  Watermark injection uses a trigger set of words, and the provider service adjusts the embedding based on the number of triggers in a user's input.  Copyright verification uses a backdoor dataset – sentences containing only trigger words – to detect if a service is using the embedded model.  The system compares the embedding of a service's output to the target embedding, using cosine and L2 similarity, along with a Kolmogorov-Smirnov test, to identify potential copyright infringement.

Experiments on datasets like AG News, MIND, SST2, and Enron Spam demonstrate strong detection performance without significantly impacting embedding utility.  Visualizations using PCA show that the backdoor embeddings are difficult to distinguish from normal embeddings, highlighting the covertness of the method. The paper aims to provide a practical solution for protecting the intellectual property of embedding as services.</sample>
    <sample id="95">The provided text does not mention the first author of PaLM. It only mentions David Vilar as the presenter of the review.</sample>
    <sample id="97">The speaker mentions two main problems with current SimulST models: long and complicated training procedures, and the need to train and maintain several models to reach different latency regimes.</sample>
    <sample id="98">```
The presentation highlights the difficulty of mitigating political biases in NLP models. The core challenge is balancing the need to preserve diverse perspectives with the risk of censorship or exclusion. There's no simple solution, and the ideal approach is still under investigation.
```</sample>
    <sample id="100">PromptRank is a data-efficient approach to multi-hop question answering, addressing the need for large training datasets. It combines unsupervised retrieval (TF-IDF and hyperlink traversal) with a few-shot language model reranker. The core idea is to use the likelihood of the question given the chain of documents as a scoring function, prompting a language model to assess this probability.

The system constructs a chain prompt by inserting the retrieved documents, an indicator token, and an instruction (e.g., "Read the previous documents and ask a question").  This instruction guides the language model to reason over the chain.  Techniques like instruction search and sampling are used to optimize prompt performance.

PromptRank achieves strong performance on HotpotQA, outperforming fully supervised systems like DrKit and comparable to state-of-the-art dense retrievers.  Ablation studies confirm the importance of each component.  When used as a retriever, PromptRank significantly improves downstream QA performance, achieving results close to MDR.  The likelihood of the question given the chain is a superior scoring function compared to the reverse.  The instruction plays a crucial role in eliciting reasoning from the language model.  PromptRank demonstrates that language models can effectively rank candidate paths for multi-hop QA with minimal training data.</sample>
    <sample id="101">The fluency of PaLM is comparable to state-of-the-art systems, but it often comes at the cost of accuracy, with common errors being omission errors.</sample>
    <sample id="102">The watermarking method needs to be applicable to embedding as services, not degrade the utility of the provided embeddings, be covert enough to the attacker, and be transferable to the attacker's services during the model extraction process.</sample>
    <sample id="103">"14 different languages" are translated in the TED talks used for the study. The specific languages are not listed in the provided text.</sample>
    <sample id="104">This information is not explicitly stated in the provided text. The text mentions re-annotating datasets with "many annotates for instance," but it doesn't specify the exact number of instances sampled from a single dataset.</sample>
    <sample id="105">Cosine similarity, L2 similarity, and Kolmogorov-Smirnov (KS) test p-value.</sample>
    <sample id="106">The QUEST dataset, developed in collaboration with Google DeepMind, addresses the challenge of information retrieval with implicit set constraints, exemplified by queries like finding a specific reptile species or historical fiction novels set in France.  The dataset comprises over 3,000 entity-seeking queries containing these constraints, with verified answers and attributable spans within documents. 

QUEST was constructed by performing set operations on Wikipedia categories related to films, books, plants, and animals, then having human annotators paraphrase and validate these queries. Annotators also verified the relevance of entities in the answer sets and marked evidence within documents. 

The dataset presents a significant challenge for retrieval systems, requiring them to search large corpora for multi-answer sets where evidence for relevance can be found in multiple document sections.  Initial evaluations reveal substantial room for improvement in retriever performance, with end-to-end systems achieving relatively low F1 scores.  Queries involving set intersections and differences prove particularly difficult.  The research highlights the need for systems capable of handling complex, selective information needs, and the QUEST dataset provides a valuable resource for advancing this area. The authors encourage readers to explore their paper and attend their presentation at ACL.</sample>
    <sample id="107">The multilingual encoder-based models (Encoder-Decoder models like mBART and mT5) were used for cross-lingual semantic parsing by training them on a mixture of various languages and then using them to predict SQL outputs for queries in different languages. The models were evaluated on multiple settings, including zero-shot and few-shot transfer, to assess their performance in cross-lingual scenarios.</sample>
    <sample id="108">Koustav Sinha introduces a paper at ACL 2023 that addresses the issue of language model acceptability judgments being context-dependent. The study revisits the minimal pair paradigm, evaluating models on acceptability judgments using longer sequences, a crucial aspect given the increasing context windows of large language models.

The researchers recreate longer sequences by selecting acceptable or unacceptable sentences from existing datasets like BLiMP and SyntaxGym, adding them as prefixes to the queries. They explore three scenarios: using completely unrelated Wikipedia sentences (robustness), sentences from the same dataset (sensitivity to context), and sentences from different datasets (mismatch).

Results show that MPP judgments are relatively stable when using irrelevant Wikipedia sentences, but significantly change when sentences are chosen from the same dataset, particularly when matching grammatical structure. This sensitivity is attributed to latent syntactic and semantic features shared across sentences. Perturbations of the input sentences don't significantly alter the model's judgments, suggesting the models are sensitive to underlying features rather than specific sentence variations.

The key takeaway is that current MPP evaluation methods, focused on short, single-sentence inputs, may not fully capture language models' abstract knowledge across longer contexts. The paper highlights the need for more robust evaluation methods that account for context sensitivity.</sample>
    <sample id="109">This paper introduces "Unnatural Instructions," a large dataset of natural language instructions and their corresponding inputs and outputs, created entirely through automated generation by a pre-trained language model (GPT-3).  The method involves prompting the model to generate instructions, inputs, and outputs, and then further diversifying the dataset by generating paraphrases of existing instructions.  The resulting dataset comprises 64,000 examples, with approximately 240,000 including instruction paraphrases.  Evaluation reveals high correctness rates (over 50%) and significant creativity and diversity in the generated tasks, including novel and unconventional applications of language models.  Fine-tuning an 11 billion-parameter T5 model on Unnatural Instructions outperforms both T0++ and Tk-instruct on several benchmarks, including Super-Natural Instructions, T0, BIG-Bench Hard, and LMentry.  The study demonstrates the potential of language models to generate diverse and creative instruction data, offering a cost-effective and scalable alternative to human annotation.  This approach overcomes limitations of crowd-sourced data, which often suffers from predictable heuristics and annotation artifacts.  Unnatural Instructions provides a valuable resource for instruction tuning, enabling language models to generalize to a wider range of tasks with minimal human effort.</sample>
    <sample id="111">The authors assume the provider can collect a general text corpus and count the word frequency with it. They then select a group of words in a moderate frequency interval from this corpus as the trigger set.</sample>
    <sample id="114">The presentation introduces "Finding the Pillars of Strength for Multi-Head Attention," a work from Nanyang Technological University on optimizing large language models (LLMs). LLMs, while powerful, suffer from limitations like heavy parameter counts, long training times, and large data requirements. The research focuses on addressing the heavy parameter problem through a novel grouped head attention (GHT) method.

GHT employs a divide-and-conquer strategy to compress multi-head attention. It involves group-constrained training to group heads into similar and separate clusters, followed by a voting-to-stay algorithm to prune redundant heads within each group. This results in significant parameter compression, with potential reductions of up to 90%.

The GHT method has been evaluated on machine translation, language modeling, and abstractive summarization tasks, achieving performance improvements and compression rates comparable to existing state-of-the-art models.  Furthermore, the research demonstrates a "LITE" model achieving 90% parameter reduction, 62% faster inference, and 80% reduced FLOPs while maintaining performance. The authors believe task-specific automatic pruning, guided by the Lottery Ticket Hypothesis, holds promise for further reducing the size and computational cost of LLMs, making them more deployable and efficient for real-world applications. They invite attendees to their poster session for more details.</sample>
    <sample id="115">The approach uses a lambda speech frames as the segment size for latency handling.</sample>
    <sample id="116">"Servin is a judge."</sample>
    <sample id="117">Example quality is more important than similarity to the source sentence.</sample>
    <sample id="118">```
The presentation introduces "Improving Pretraining Techniques for Code-Switched NLP," addressing the challenges of code-switching (mixing languages) in NLP.  Multilingual models like mBERT struggle with this task, so the work proposes novel methods to improve performance.

The core contribution is **SwitchMLM**, a masked language modeling (MLM) technique specifically designed for code-switching. It focuses on masking only "switch-points" – tokens transitioning between languages – rather than all tokens.  A surrogate method, **FrequencyMLM**, is offered when LID (Language Identification) tags are unavailable.

Architectural modifications include **residual connections** leveraging switch-point information in intermediate layers, and an **auxiliary LID-based loss** to encourage language information encoding.  The combined method of SwitchMLM (or FrequencyMLM) with ResBERT and an auxiliary loss achieves the best results on sentiment analysis across language pairs.

Probing experiments using linear and conditional probing demonstrate that the proposed methods increase switch-point information in intermediate and final layers of the model.  This confirms the hypothesis that the model learns to better represent code-switched data.  The findings suggest that incorporating switch-point information is crucial for effective code-switching NLP models.
```</sample>
    <sample id="119">The paper focuses on GPT-4, GPT series, and BART series language models in the extended experiments.</sample>
    <sample id="120">The model uses attention scores from the cross-attention mechanism between audio input and textual output.</sample>
    <sample id="121">```
The examples of direct inference are when the user explicitly states the name of the song or its position, such as "Easy on Me" or "the first one."
```</sample>
    <sample id="122">Fudan University.</sample>
    <sample id="123">Ying and Zhiyang present MultiInstruct, the first large-scale multi-modal instruction tuning dataset, addressing the lack of publicly available instruction datasets for computer vision and multi-modal tasks. Their work investigates the effectiveness of instruction tuning on a unified multi-modal pre-trained model, OFA.

MultiInstruct comprises 62 diverse multi-modal tasks derived from 21 existing datasets, each with five expert-written instructions. They train OFA on 53 tasks, using a unified sequence-to-sequence format that represents text, images, instructions, and bounding boxes in the same token space.  They evaluate the model using five different instructions per task, reporting accuracy (for classification) or Rouge-L (for generation) and introducing a new metric, sensitivity, to measure consistency across instruction variations.

Results show that instruction tuning significantly improves OFA's performance on seen multi-modal tasks, and transfer learning from natural instruction datasets further enhances performance and reduces sensitivity.  Increasing the number of instructions during fine-tuning leads to better overall performance and lower sensitivity.  The research also demonstrates the benefits of transfer learning from natural instruction datasets.  Finally, they plan to release a larger dataset with approximately 150 additional vision-language tasks.</sample>
    <sample id="124">Tan Qingyu from NUS and Alibaba presented their work on benchmarking and improving the temporal reasoning capabilities of Large Language Models (LLMs). They categorize temporal reasoning into three levels: time-to-time (e.g., "year after 2010"), time-to-event (e.g., "team in 2010"), and event-to-event (e.g., "team after FC Barcelona").  Prior research focused primarily on the second level, leading to a more comprehensive study.

Their research involved a preliminary experiment on year prediction, revealing biases towards the 2000-2020 period in models like T5-L and FLAN-T5-L, with ChatGPT showing promise but declining performance on month prediction.  To address this, they created the TempReason dataset, encompassing all three reasoning levels and long temporal coverage, using Wikidata and Wikipedia.

They evaluated LLMs using Closed Book QA, Open Book QA (with Wikipedia context), and a new "Reasoning QA" setting with explicit temporal knowledge.  A novel training strategy, Temporal span extraction pre-training and time-sensitive reinforcement learning, was proposed.  The TempT5 model significantly outperformed existing LLMs like ChatGPT and FLAN-T5-L, especially in Open Book QA and Reasoning QA.  They observed performance fluctuations across time periods, suggesting potential biases in training data.  Future work will focus on mitigating these biases.</sample>
    <sample id="125">The provided text does not mention the number of authors involved in the paper. It only mentions Yanis Labrak as the presenter.</sample>
    <sample id="126">Yes, translating the natural language query using a machine translation model before semantic parsing was considered as a baseline.</sample>
    <sample id="127">Namgyu Ho introduces their research, "Large Language Models Are Reasoning Teachers," a collaborative project with Laura Schmid and Se-Young Yun at KAIST AI. The work addresses the limitation of chain-of-thought reasoning, which requires massive models like GPT-3 for effective problem-solving. Their solution is to leverage these large models as "reasoning teachers" to transfer their abilities to smaller, more deployable models.

The core idea involves prompting large models to generate step-by-step solutions for complex tasks, then using these solutions as training data for smaller models. A key innovation is "Diverse Reasoning," which generates multiple reasoning samples from the teacher model using stochastic sampling, leading to improved performance.

Experiments on 12 tasks demonstrate that their method significantly outperforms prompt-based baselines and vanilla fine-tuning, even with small student models (0.3 billion parameters).  The performance scales favorably with larger datasets, better teacher models, and larger student models, but also involves trade-offs between development and inference costs. The researchers emphasize the accessibility and scalability of their approach, suggesting it could be applied to other emergent abilities in the future. They provide code and data, including access to OpenAI inference, and encourage further exploration.</sample>
    <sample id="128">Akshatha and Martin's work, "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources," explores the challenge of natural language understanding (NLU) models utilizing both pretraining and inference-time knowledge.  They address the issue that models often rely on pre-existing knowledge but struggle with entity-specific information needed for tasks like coreference resolution, especially when that information isn't present in the pretraining data.

The authors introduce KITMUS, a diagnostic test suite for evaluating knowledge integration.  A coreference resolution task is central, probing models' ability to draw on knowledge from different sources.  They evaluate the dataset with both human participants and established coreference models.

KITMUS features three settings: "Background-Pretrain" (background knowledge available during pretraining), "Background-Both" (background knowledge available at both pretraining and inference), and "Background-Inference" (background knowledge only available at inference). The "Background-Inference" setting simulates scenarios where background knowledge is not present in the pretraining data.

Experiments show that while models perform poorly without task-specific training, they significantly improve when trained on KITMUS. However, even the best models struggle with reliably integrating knowledge presented only at inference time.  The study highlights the limitations of current models in reasoning over knowledge from diverse sources and suggests the need for task-specific training to effectively integrate knowledge.  The authors provide the dataset and code on GitHub.</sample>
    <sample id="129">"Asian woman" was given as an example of a marked group.</sample>
    <sample id="130">The paper suggests that transformer models generally generalize better to new data than other model architectures.</sample>
    <sample id="131">The provided text does not mention the names of the testing datasets. It focuses on the research findings and recommendations related to Weakly Supervised Learning (WSL).</sample>
    <sample id="132">Three.</sample>
    <sample id="133">The author works with multiple modalities, including text, images, and bounding box coordinates.</sample>
    <sample id="135">ABC-Eval is a new, dimensional approach to evaluating conversational AI, developed by the Emory NLP Lab and Amazon Alexa AI. Unlike traditional human evaluation methods that focus on overall quality, ABC-Eval annotates specific behaviors in chat models, such as irrelevant responses, contradictions, and lack of empathy. This allows for a more granular understanding of model strengths and weaknesses.

The researchers evaluated four state-of-the-art models using ABC-Eval on 100 human-bot conversations, comparing the results to existing methods like Likert ratings and pairwise comparisons. Their analysis showed that ABC-Eval labels are more reliable and predictive of overall conversation quality than existing methods.  Specifically, metrics like self and partner contradictions significantly contribute to conversation quality, while Likert consistency scores offer less insight.

ABC-Eval metrics also demonstrate distinctiveness, meaning they capture different aspects of chat quality.  The study revealed that the tested models exhibit common sense violations, irrelevant information, and contradictions at around 20%, 15%, and 10% rates, respectively.  The researchers believe ABC-Eval provides a valuable tool for improving the evaluation of conversational AI and encourages further research in this area.</sample>
    <sample id="136">Jasivan and Nafise at the University of Sheffield developed FERMAT, a new evaluation set for numerical reasoning, to address limitations of existing benchmarks. Current benchmarks primarily focus on accuracy and F1 scores, failing to reveal the strengths and weaknesses of language models in mathematical ability. FERMAT offers a more informative approach by focusing on arithmetic types, number understanding, and training dependencies.

The project involved creating a flexible evaluation set based on arithmetic questions from Illinois and CommonCore, modifying numbers to represent real-world scenarios, and assessing performance across various mathematical operations. A zero-shot evaluation revealed poor performance across all aspects, highlighting the inadequacy of existing benchmarks.

To improve performance, the team fine-tuned models using 200,000 questions generated by math teachers, covering a range of number types and operations. This fine-tuning yielded promising results, with performance increasing across different aspects.  The research also explored the impact of training dependencies and the importance of language and mathematical diversity in the evaluation set.  The findings suggest that existing benchmarks are unrepresentative and that language and mathematical diversity, along with improvements in number encoding and tokenization, are crucial for enhancing numerical reasoning capabilities in language models. FERMAT aims to provide a more comprehensive and informative evaluation of these models.</sample>
    <sample id="137">Sicong from the Singapore University of Technology and Design introduces "Tell2Design," a new dataset and approach for language-guided floor plan generation, published in ACL 2023.  Unlike text-to-image models focused on artistic generation, Tell2Design tackles the practical need to design according to specific user requirements expressed in natural language. The task involves generating 2D floor plans from text instructions detailing semantics, geometry, and topology.

The dataset comprises 5,051 human-annotated instructions and 76,000 artificially generated ones, covering a wide range of floor plan descriptions.  The core challenge lies in adhering to complex constraints and understanding nuanced, potentially ambiguous instructions.  The research frames floor plan generation as a sequence-to-sequence problem, using a transformer-based encoder-decoder model initialized with the T5 language model. This allows the model to reconstruct room bounding boxes from the input text.

Evaluation reveals that the Tell2Design model achieves high IoU scores (54 Micro IoU, 53 Macro IoU), outperforming text-to-image baselines.  The model's success stems from its ability to focus on salient information within the instructions.  The study also highlights a language distribution gap between artificial and human instructions, but demonstrates that combining both during training improves performance.  The paper concludes by establishing Tell2Design as a valuable resource for advancing language-guided design generation.</sample>
    <sample id="138">The authors claim that the ability of natural language understanding models to integrate knowledge from both pretraining and inference time is an understudied area.</sample>
    <sample id="139">Ying and Zhiyang.</sample>
    <sample id="140">Yes, CoScript underwent quality checks with crowd-sourced workers to find and revise incorrect samples.</sample>
    <sample id="141">The existing resources for context-dependent translation are limited because they often rely on domain knowledge and human curation, and only support limited types of context-dependent translations and limited sets of languages.</sample>
    <sample id="143">The approach is compared to the Wait-k strategy and the Local Agreement strategy, as well as state-of-the-art architectures specifically tailored for simultaneous pre-translation.</sample>
    <sample id="144">The provided text does not mention the affiliations of the authors.</sample>
    <sample id="145">Jenny.</sample>
    <sample id="146">Yicheng from Fudan University presents a research paper on the analysis and detection of omissions in dialogue summarization. Dialogue summarization, a subtask of text summarization, aims to create concise summaries of dialogues, but current large language models often suffer from factual errors, including omissions of critical information. This paper highlights the severity of this problem, revealing that even state-of-the-art models have high omission rates (around 70%).

To address this, the researchers created the OLDS dataset, a high-quality dataset of dialogue summaries with omission labels, built upon five existing benchmarks. They developed an automatic method for generating these labels and ensured their quality through human evaluation.  The paper explores three different model architectures for omission detection – pair-wise classification, sequence labeling, and pointer networks – evaluating their performance using metrics like Precision, Recall, and F1-score.  They also introduced a post-editing method that refines summaries by incorporating detected omissions.  The results show that incorporating omission information significantly improves summary quality, suggesting that omission detection is a valuable step towards enhancing dialogue summarization. The dataset is publicly available for further research.</sample>
    <sample id="147">Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models is done in collaboration with Esin Durmus and Dan Jurafsky.</sample>
    <sample id="149">Yes, the CoNLL++ dataset is publicly available.</sample>
    <sample id="150">Archiki from Adobe Research and UNC Chapel Hill presented their ACL paper "MEETINGQA," focusing on extractive question-answering in meeting transcripts.  They highlight the unique challenge of meeting data – long, domain-specific, and discussion-rich – which is currently underutilized by existing NLP research.  MEETINGQA is a new dataset containing 7.7K questions and answers extracted from public meeting transcripts, designed to address this gap by focusing on the inherently significant question-asking component.

The dataset was created by annotating questions and answers from the AMI corpus, achieving high inter-annotator agreement.  The questions are often open-ended, multi-speaker, and can include rhetorical elements.  The paper explores various methods for question answering, including context retrieval, single-span models, and multi-span models.  Results show a significant gap between fine-tuned models and human performance, with short-context models outperforming long-context models.  Multi-span models show comparable performance to single-span models.  Zero-shot performance is also limited, but data augmentation with silver annotations improves results.  Error analysis reveals challenges in identifying rhetorical questions and speaker attribution, particularly in zero-shot settings.  The authors conclude that MeetingQA presents a challenging task for QA models, requiring further research.</sample>
    <sample id="152">Frederick Riemenschneider introduces a project exploring the use of Large Language Models (LLMs) for Classical Philology, specifically Ancient Greek and Latin. The presentation highlights the advancements in existing models like Latin BERT and Ancient Greek BERT, noting their limitations as primarily monolingual and lacking robust evaluation. 

The project focuses on creating new, specifically designed models, including GreBERTa (monolingual RoBERTa for Greek), GreTa (monolingual T5 encoder-decoder for Greek), PhilBERTa (multilingual model for Greek, Latin, and English), and PhilTa (multilingual model for Greek, Latin, and English).  A key innovation is a new pre-training corpus for Ancient Greek, leveraging the Internet Archive and correcting OCR errors.

The models were benchmarked on tasks like part-of-speech tagging, dependency parsing, and lemmatization, achieving state-of-the-art performance for both languages.  Analysis revealed that T5 encoders behave differently than encoder-only models, and encoder-decoder models excel at lemmatization.  While multilingual models showed performance gains in semantic and world knowledge tasks, the difference wasn't significant. The project emphasizes the importance of native tokenization and high-quality pre-training data for effective LLMs in Classical Philology.</sample>
    <sample id="153">Ninareh Mehrabi's work addresses the challenge of ambiguities in text-to-image generative models. The research focuses on understanding and resolving these ambiguities to improve image fidelity to user intent.  A key component is a benchmark dataset built on the LAVA corpus, encompassing various ambiguity types. The framework employs two disambiguation methods: language model-generated clarifying questions and language model-generated visual interpretations.  Users respond to these prompts, providing a disambiguated prompt.

The research then evaluates the faithfulness of generated images using a Visual Question Answering (VQA) model.  The VQA model assesses whether the image aligns with the user's intended meaning, based on the original ambiguous prompt and the disambiguated prompt. Findings indicate that ambiguity resolution varies across types, but overall, the framework improves faithful image generation.  Furthermore, the automatic evaluation framework aligns with human evaluation, suggesting its reliability. The work aims to provide tools for both mitigating and assessing ambiguities in text-to-image models, ultimately leading to more accurate and user-aligned image generation.</sample>
    <sample id="154">Sara Papi from the University of Trento and Foundazione Bruno Kessler, Matteo Negri, and Marco Turchi.</sample>
    <sample id="155">Javad Hosseini.</sample>
    <sample id="157">Shen Gao from Shandong University introduces their work, "Dialogue Summarization with Static-Dynamic Structure Fusion Graph," a joint project with several researchers. The work addresses the challenge of creating concise summaries from dialogues, aiming to capture key information efficiently. Existing methods rely on pre-computed static graphs, which are prone to errors and don't adapt to the summarization task.

Their SDDS model employs a four-component architecture. First, an Utterance Encoder converts utterances into vector representations. Then, a Static-Dynamic Graph module constructs static graphs using discourse parsing and speaker interaction analysis, capturing relationships based on keywords and speaker frequency.  This static graph is then combined with a dynamically learned graph using a multi-head attention model, allowing the model to adapt to the dialogue's flow. Finally, a pre-trained language model generates the summary by fusing the static and dynamic graph information.

The model utilizes heuristic methods for static graph construction, including Discourse Parsing Graph, speaker interaction frequency matrix, and relative distance as edge features.  A key innovation is the Dynamic Graph module, which uses multi-head attention to capture semantic relationships between utterances based on their vector representations.  The model integrates the static and dynamic graphs using a fusion method and incorporates graph representation into the generation process via a graph attention layer. The code and data are available on GitHub.</sample>
    <sample id="158">Qipeng Guo from AWS introduces "Dual Cache for Long Document Neural Coreference Resolution," addressing the challenge of coreference resolution in long documents. Traditional methods are computationally expensive due to their quadratic complexity. Cache-based methods offer linear complexity but suffer from high cache misses in long documents with topic shifts, particularly for high-frequency entities.

The proposed dual cache utilizes a local cache (LRU eviction) for local entities and a global cache (LFU eviction) for global entities, working together.  The model scans the document, classifying mentions as new or belonging to the cache, and adding them to either the local or global cache based on frequency.  Cache eviction triggers when either cache is full.

Experiments on LitBank, OntoNotes, and WikiCoref demonstrate that the dual cache outperforms baselines, even with unbounded memory, and is faster than single-cache methods.  Performance gaps are larger for book-level documents, highlighting the benefit of the dual cache's ability to handle long contexts.  The dual cache significantly reduces cache misses compared to single-cache approaches.  The study concludes that the dual cache offers the best performance-to-cost ratio, effectively balancing efficiency and accuracy for coreference resolution in long documents.</sample>
    <sample id="160">The first step of the method maps input tokens to an unordered multiset of tokens that will appear in the output.</sample>
    <sample id="161">55,000</sample>
    <sample id="163">MASSalign is the best automatic alignment method for German text simplification, according to the presentation.</sample>
    <sample id="164">Weak supervision offers a cheaper alternative to manual labeling, using weaker labeling sources like heuristics or crowdsourcing. While the annotations are noisy, weakly supervised learning aims to train models that can generalize well despite this noise.</sample>
    <sample id="165">Wenting Zhao from Cornell University presented their paper "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations," introducing a novel unsupervised learning method called LiPoR for abductive reasoning. Abductive reasoning aims to find the most plausible explanation for an observed outcome given a context, exemplified by "Emily was stuck in traffic" leading to "Emily made it to her flight," with explanations like "Her flight was delayed" or "Her flight left on time."

Traditional abductive reasoning relies on supervised methods requiring labeled plausible explanations, which are often noisy and subjective, as demonstrated by crowd worker disagreements. LiPoR addresses this by treating explanations as latent variables and maximizing the marginal likelihood of the outcome given the context, without needing explicit plausibility labels. However, this objective doesn't inherently favor plausible explanations.

To address this, LiPoR incorporates a regularizer based on mutual exclusivity among explanations. The regularizer, denoted by Omega, balances maximizing the likelihood of outcomes with preferring a subset of explanations by considering the entropy of the probability distribution of explanations given the context and the log of the number of plausible explanations.

Experiments on the AlphaNLI dataset show that LiPoR outperforms zero-shot models and the previous best unsupervised approach by over 4 absolute points in accuracy. The paper is available at tinyurl.com/zhao-lipor.</sample>
    <sample id="166">Yunxin from Harbin Institute of Technology, Shenzhen, introduces their new work, "A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text." The research addresses the challenge of image retrieval from long, complex text descriptions, where visual language models struggle. Inspired by Divide-and-Conquer and Dual-Process Theory, the proposed framework combines the strengths of analogical reasoning (System 1) and logical reasoning (System 2).

The system utilizes a Proposition Generator to break down complex text into simpler propositions, using BART to generate corresponding sentences. A Visual-Linguistic Interactor (System 1) interacts with visual and proposition information to generate matching scores and reasoning states.  A Neural-Symbolic Reasoner (System 2) integrates these states using negation and conjunction operations to arrive at a final solution.

Experimental results demonstrate that the proposed method, NDCR, outperforms existing baselines.  The system's ability to present inference states and results mid-process highlights its interoperability. The authors suggest that neural symbolic calculation can enhance large language models' compositional reasoning and planning, aligning with the effectiveness of Divide-and-Conquer and Dual-Process Theory in complex problem-solving.</sample>
    <sample id="167">The DEPLAIN-web corpus includes documents aligned both manually and with automatic alignment methods. The manual alignment was done on 750 documents, and the automatic alignment was applied to the remaining documents.</sample>
    <sample id="168">The CoNLL++ dataset was created by collecting Reuters News from 2020 and annotating it with the same CoNLL-2003 annotation guidelines.</sample>
    <sample id="169">David Vilar from Google Translate presents a review of the paper "Prompting PaLM for Translation: Assessing Strategies and Performance." The paper investigates the effectiveness of prompting large language models (LLMs) like PaLM, a 540 billion parameter model, for machine translation. PaLM, trained on 780 billion tokens, achieved state-of-the-art results in NLP tasks.

The study systematically evaluates prompting strategies using best practices, comparing PaLM's performance to state-of-the-art systems like WMT.  Experiments show that prompting significantly impacts translation quality, with even small changes in prompts leading to substantial BLEURT score differences (up to 40 points).  A 5-shot prompting strategy, marking source and target sentences with their respective languages, proved more effective than simpler one-shot prompting.

The quality of the example translations in the prompt is more crucial than the similarity to the source sentence.  The paper highlights that PaLM's fluency is comparable to top systems, but accuracy suffers, particularly with omission errors. Human evaluation using MQM indicates that PaLM produces fluent translations, but sometimes sacrifices accuracy for a more natural-sounding output. The study concludes that prompt selection is vital for achieving high-quality machine translation with LLMs.</sample>
    <sample id="171">Existing works have broadly classified watermark methods, but they either aren't applicable to embedding as services or lack transferability.</sample>
    <sample id="172">No, multilingual LLMs like Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="174">Thea from ArgAnalysis35K introduces a novel dataset for argument quality analysis, addressing limitations of existing datasets.  ArgAnalysis35K boasts the largest dataset of 35,000 argument-analysis pairs, prioritizing high-quality arguments sourced from expert and intermediate debaters, with a smaller portion from novice users.  Unlike datasets tied to specific motions, ArgAnalysis35K utilizes 24 themes, capturing diverse arguments relevant to parliamentary debate.  A key innovation is the introduction of "analysis," encompassing claims, premises, and their combinations, providing a more comprehensive understanding of argument strength.  The dataset incorporates instance-based annotator reliability, mitigating bias by focusing on individual argument reliability rather than entire annotator groups.  Finally, a relevance model assigns scores to arguments' relevance to different themes, capturing broader applicability beyond single motions.  This combination of features aims to provide a more robust, diverse, and reliable resource for NLP research in argument quality assessment. The authors encourage further feedback on the dataset and its methodology.</sample>
    <sample id="175">The method addresses the ambiguity of permutations by inducing the alignment between input and output as part of the training process. It also approximates the NP-hard permutation search with a GPU-friendly continuous relaxation that allows for learning linguistically plausible permutations.</sample>
    <sample id="176">Left-leaning language models are better at detecting hate speech targeting socially minority groups, while right-leaning models are better at detecting hate speech targeting white and men. Similar trends exist for fake news detection, with left-leaning models better at detecting misinformation from their opposite political leaning and vice versa. These differences in performance based on social categories indicate a fairness issue.</sample>
    <sample id="177">Yanis Labrak.</sample>
    <sample id="178">Koustav Sinha.</sample>
    <sample id="179">Melanie Sclar introduces "Minding Language Models’ (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker," focusing on improving theory of mind (ToM) reasoning in large language models (LLMs). ToM is the ability to understand that others have beliefs that may differ from one's own. The research addresses the current limitations of LLMs in passing false-belief tests, a key measure of ToM.

The proposed solution, SymbolicToM, is an inference-time method that uses explicit graphical representations to model characters' mental states. These graphs, representing beliefs about the world, are computed for all character combinations and are used to answer questions about what characters believe.  The method leverages off-the-shelf NLI and OpenIE models for graph construction and question answering.

Experiments demonstrate that SymbolicToM significantly improves LLM performance on ToM tasks, often by 65-67 accuracy points across various models like GPT-3 and Flan-T5.  Furthermore, the method generalizes well to out-of-domain scenarios, showing strong performance even when models are tested on novel story structures and linguistic variations.  This makes SymbolicToM a robust and interpretable approach to enhancing LLMs' ability to reason about the mental states of others, avoiding overfitting. The research highlights the potential of symbolic reasoning for improving LLMs' cognitive abilities.</sample>
    <sample id="180">Myra</sample>
    <sample id="181">This paper addresses the challenge of constrained language planning, extending the work on abstract goal planning for stereotypical activities to scenarios with specific constraints.  While large language models (LLMs) demonstrate promise in decomposing goals, their performance on constrained planning remains unsatisfactory.  The authors define this problem and explore the limitations of current LLMs in generating faithful scripts given multi-faceted constraints.

To address this, they propose an "over-generate-then-filter" method, leveraging InstructGPT to generate numerous scripts for specific goals and then employing a filter model based on semantic similarity and constraint keyword presence.  This approach significantly improves script quality in both semantic completeness and constraint faithfulness.  Furthermore, the paper tackles the data scarcity problem by creating the CoScript dataset, a large-scale dataset of 55,000 constrained language planning examples generated by LLMs and refined through human review.  

The CoScript dataset exhibits high diversity in goal types.  The authors demonstrate that fine-tuning smaller models like T5 on CoScript yields superior performance compared to larger LLMs, highlighting the potential for specialized models in constrained language planning.  The paper concludes by establishing the constrained language planning problem, evaluating LLM performance, and presenting a method and dataset to advance research in this area.</sample>
    <sample id="182">The paper states that for Latina women, the words "vibrant" and "curvaceous" reflect a trope of tropicalism, which connects to a long history of Asian women being hyper-sexualized, seen as very docile and submissive.</sample>
    <sample id="183">The authors used natural language prompts, like "Imagine you are an Asian woman. Describe yourself," to generate personas for different demographic groups. They inspired these prompts by a study where prompts were given to human subjects to surface racial stereotypes. This allowed for direct comparison between the generated personas and human-written responses.</sample>
    <sample id="184">CXMI (Contextualized Machine Translation Information) was extended to Pointwise CXMI to measure context usage at the sentence or word level.</sample>
    <sample id="185">DrBERT is the first biomedical language model in French, based on RoBERTa and trained on the NACHOS dataset of medical crawled data. ChuBERT is a clinical model based on anonymized data from the Nantes University Hospital data warehouse. DrBERT was trained on a larger dataset (7GB) than ChuBERT (4GB), and the results show DrBERT performs better on most downstream tasks.</sample>
    <sample id="187">Ying and Zhiyang.</sample>
    <sample id="188">Iterative transfer learning updates the model by training on the latest set of data collected during active learning.</sample>
    <sample id="189">The goal of the AltEntities Corpus is to understand how users express choices when they use indirect referring expressions, which is important for conversational systems and benchmarking LLMs' entity understanding. It's a large-scale dataset of alternative questions and indirect referring expressions across music, books, and recipes.</sample>
    <sample id="190">An attacker can extract model parameters by learning from the embeddings provided by the EaaS. By analyzing the embeddings, the attacker can infer the underlying model's structure and parameters, potentially allowing them to replicate the model's functionality. The paper proposes a backdoor watermark to prevent this type of extraction.</sample>
    <sample id="191">Three.</sample>
    <sample id="192">Yang Luo presented "CAME: Confidence-guided Adaptive Memory Efficient Optimization," addressing the challenge of balancing fast convergence with low memory usage in large language model training. Existing memory-efficient optimizers like Adafactor often sacrifice performance, while Adam triples memory requirements.

The presentation highlighted the limitations of NMF-based approaches like Adafactor, which can lead to erroneous updates and slow convergence. CAME tackles this by incorporating a confidence-guided updating mechanism. It utilizes the residual between predicted and actual updates to adjust the denominator of the momentum term, effectively mitigating instability caused by erroneous updates.

Experiments on BookCorpus and English Wikipedia, using BERT, GPT-2, and T5, demonstrated CAME's superior performance compared to Adam and Adafactor. CAME achieved a 3.4% increase in validation accuracy with fewer training steps and better performance with larger batch sizes.  It also showed comparable performance to baseline models with reduced memory consumption, even outperforming existing memory-efficient optimizers like SM3.

The key innovation is the confidence-guided adaptive updating, which allows CAME to effectively handle large batch training while maintaining fast convergence and low memory footprint.  The presentation concluded that CAME offers a significant advancement for training large language models, particularly in resource-constrained environments.</sample>
    <sample id="193">The paper states that around 1,000 examples of discourse unit pairs were collected for annotation. It doesn't specify the number of annotators used.</sample>
    <sample id="194">Jenny is a first-year PhD student at Carnegie Mellon University. The authors also collaborated with the University of Washington and the Allen Institute for AI.</sample>
    <sample id="195">The paper introduces RoHT (Reasoning over Hierarchical Question Decomposition Tree), a novel framework for explainable question answering (XQA) that addresses limitations of existing methods. Current XQA approaches, like neuro-symbolic and decompose-based methods, struggle with knowledge limitations and the diversity of natural language, respectively. RoHT tackles these challenges by leveraging hierarchical question decomposition to flexibly integrate knowledge from heterogeneous sources.

RoHT builds a Hierarchical Question Decomposition Tree (HQDT) to understand the compositional structure of complex questions, breaking them down into atomic questions. It then employs probabilistic reasoning over this HQDT, fusing knowledge from knowledge bases (KBs) and text corpora at different levels. The reasoning process recursively explores the tree, selecting appropriate knowledge sources (KB, text, or recursive calls) and aggregating answers with associated probabilities.

The framework is evaluated on challenging datasets like KQA Pro (with incomplete KBs and Wikipedia) and Musique (with Wikidata). Results demonstrate that RoHT outperforms existing KB QA methods, TransferNet, and SOTA text QA methods, especially when integrating knowledge from both KBs and text. The hierarchical decomposition allows for more targeted knowledge retrieval and improved explainability. The paper highlights the benefits of explicit question decomposition for achieving robust and explainable XQA.</sample>
    <sample id="196">"I saw Bart and Lisa"</sample>
    <sample id="197">The content mentions four state-of-the-art chat models that were evaluated using ABC-Eval. However, it does not specify the names of these models.</sample>
    <sample id="198">The current minimal pair paradigm doesn't allow evaluation of models' acceptability across longer sentences, which are becoming increasingly common with large language models and their larger context windows. Therefore, it's crucial to evaluate models' acceptability throughout the context window to understand their capabilities fully.</sample>
    <sample id="199">Yes, training in a mixture of various languages caused a performance drop in English on seven of the nine datasets. This phenomenon is referred to as the "Curse of Multilinguality".</sample>
    <sample id="200">The annotators know the name of the entities but not necessarily about the entities themselves.</sample>
    <sample id="201">BLEURT and human evaluation (MQM framework).</sample>
    <sample id="202">The presentation doesn't discuss whether the generalization impact differs across specific NER types. It focuses on general factors affecting generalization for NER models, regardless of the entity type.</sample>
    <sample id="203">NLPositionality research highlights that NLP datasets and models can reflect the perspectives of specific populations, leading to systematic performance differences across demographics. This can result in some groups being unfairly disadvantaged or excluded. Understanding and addressing this positionality is crucial for building more inclusive and equitable NLP technologies.</sample>
    <sample id="204">The presentation states that multilingual language models like Codex and BLOOM are "still inadequate" for cross-lingual semantic parsing tasks. It doesn't specify whether they were fine-tuned with adapters or full fine-tuning.</sample>
    <sample id="205">Shangbin from the University of Washington presents research on political biases in large language models (LLMs). The study investigates how political leanings in pretraining data – drawn from sources like the New York Times and Huffington Post – influence LLM behavior and potential fairness issues in downstream applications.

The research employs prompt engineering and controlled experiments to evaluate LLMs like GPT-4, RoBERTa, and others, revealing varying political leanings across different models.  Further pretraining on partisan corpora demonstrates that LLMs absorb and reflect societal polarization, becoming more aligned with the political climate after 2017.

The study then assesses these LLMs on hate speech and fake news detection, finding that left-leaning models perform better at detecting hate speech targeting minority groups, while right-leaning models are better at detecting hate speech targeting dominant groups.  These findings highlight significant fairness concerns, particularly regarding potential marginalization of specific demographics and the spread of targeted hate speech.

The presentation concludes by acknowledging the complex dilemma of balancing the inclusion of diverse perspectives with the need to mitigate political bias, drawing parallels to the "Scylla and Charybdis" problem and the "electric trolley problem." The research emphasizes the urgent need to address these biases to prevent unfair outcomes in NLP applications.</sample>
    <sample id="206">Transfer learning from topic-independent dissonance stance classification (debate) and binary classification of expansion and comparison (CE) classes of PDTB.</sample>
    <sample id="207">The paper uses the latest test sets to avoid overlap with the training data of the language model. The development data (dev data) is more curated and of higher quality than the training data, leading to better performance with the dev data.</sample>
    <sample id="208">The authors proposed three recommendations.</sample>
    <sample id="209">The proposed method using CoScript and T5 fine-tuned on it generates scripts of higher quality than most large language models, surpassing them in quality.</sample>
    <sample id="210">Shuheng.</sample>
    <sample id="211">Yes, the results and dataset in the paper can be used as a benchmark. The paper explicitly states that the manually aligned sentences in the DEPLAIN corpus serve as a gold standard for evaluating automatic alignment methods, and the fine-tuning experiments provide a base benchmark for automatic text simplification.</sample>
    <sample id="212">They experiment with T5, which is a smaller model, fine-tuned on the CoScript dataset.</sample>
    <sample id="213">OFA</sample>
    <sample id="215">Adam Przepiórkowski's talk argues against asymmetric dependency structures of coordination (like those headed by a single conjunct) and supports symmetric structures. The core argument relies on dependency length minimization, a principle favoring shorter dependencies. The talk uses examples like "Lisa, Bart, and Maggie" and "Marge read it yesterday" to illustrate this.

The speaker explains that in English, direct objects are closer to the verb, while adjuncts are further away.  Dependency length minimization suggests that shorter dependencies are preferred.  The talk analyzes coordination structures using statistics from the Penn Treebank, revealing that left conjuncts tend to be shorter, especially when the governor is on the left or absent. This tendency increases with the length difference between conjuncts.

The key finding is that this length preference only occurs when the governor is on the left or absent. When the governor is on the right, the effect disappears.  The speaker demonstrates this with length measurements in words, syllables, and characters.  The paper shows a steady growth in the left conjunct's length difference when the governor is on the left or absent, but no such growth when the governor is on the right.  This supports the idea that symmetric structures, where all conjuncts are equally important, are more likely than asymmetric ones. The paper provides a detailed argument for this conclusion.</sample>
    <sample id="217">"Seen to Unseen" addresses the limitations of existing controllable dialogue generation (CDG) methods, which often focus on single attributes or lack handling for continuous attributes. The work proposes DCG, a Disentangled Controllable Generation model, to tackle compositional generalization in multi-attribute dialogue. DCG learns attribute concepts from seen values using a disentanglement loss, enabling it to handle combinations of attributes effectively.

A key contribution is the introduction of MAE, a unified and reference-free evaluation framework for various attribute granularities. The authors establish benchmarks and demonstrate the effectiveness of DCG and MAE through experiments on DailyDialog-CG.

DCG utilizes a compositional prompt module based on DialoGPT, employing attribute-oriented and task-oriented prompts to guide the model.  Attribute-oriented prompts focus on specific attribute values, while task-oriented prompts guide the model with global features. A disentanglement loss further enhances the model's ability to distinguish between attribute combinations.

Experiments show that DCG outperforms baselines in both attribute controllability and text equality, even with a small drop in evaluation metrics.  MAE demonstrates strong correlation with human judgments, and the model successfully generalizes from seen to unseen attribute combinations. The work highlights the importance of prompt engineering for achieving compositional generalization in CDG.</sample>
    <sample id="218">David Vilar is affiliated with Google Translate.</sample>
    <sample id="219">Jia-Huei Ju from Academia Sinica presented their research on a compare-and-contrast multistage pipeline for uncovering financial signals in financial reports, specifically using the Form 10-K. The work addresses the challenge of extracting valuable information from annual reports, which often require significant human effort. The researchers observed high text similarity between yearly reports and introduced a highlighting task to identify the rationale behind relationships between reports.

The proposed pipeline consists of document segmentation, relation recognition (Stage 1), and out-of-domain/in-domain fine-tuning (Stage 2 &amp; 2+). Stage 1 classifies report pairs into three types: highly similar (e.g., regulations), revised (syntactically similar, semantically different), and mismatched (new information).  Fine-tuning utilizes the eSNLI dataset for out-of-domain learning and revised pairs for intermediate tuning, employing soft labeling techniques to improve pseudo-label quality.

The evaluation uses both eSNLI and a newly released FINAL dataset, with performance measured by precision and PCC. The research demonstrates that their domain-adaptive highlighting model achieves strong performance, even generalizing well to the eSNLI dataset.  The methods also show promise in benefiting from mismatched pairs. The researchers conclude by highlighting the potential for future work in enhancing the model's effectiveness and applicability.  More details are available in their paper and on GitHub.</sample>
    <sample id="220">Vasudha is a Computer Science PhD candidate at Stony Brook University.</sample>
    <sample id="221">The paper analyzed translation between German and English.</sample>
    <sample id="222">This work addresses the challenge of domain adaptation in open-domain question answering, where models trained on general-purpose data like Wikipedia struggle with specialized domains. The authors investigate data interventions to improve performance when transferring knowledge from a source domain to a target domain. They identify three main contributions: investigating data interventions (zero-shot and few-shot), classifying the type of dataset shift, and determining effective interventions for specific shift types.

The study uses seven target datasets across six domains, evaluating retriever and reader models trained on Wikipedia.  Few-shot learning, using manually crafted fact-based questions, significantly improves retriever and reader performance. Zero-shot techniques, controlling the interaction of question, answer, and context, show limited impact, with cloze-style questions proving more effective than standard WH questions.

The research categorizes dataset shifts into "no shift," "concept shift," "covariate shift," and "full shift," measuring compatibility between the source and target models.  They find that few-shot learning is effective for all shift types, while zero-shot is better for concept and covariate shifts.  The work concludes that targeted data interventions, particularly few-shot learning, can substantially improve reader performance, depending on the nature of the dataset shift.</sample>
    <sample id="223">Shangbin</sample>
    <sample id="224">MASSalign and fine-tuned long-mBART and base mBART models.</sample>
    <sample id="225">MultiInstruct uses 53 tasks for training and 5 tasks for testing.</sample>
    <sample id="226">The provided text does not mention the number of authors involved in the paper. It only mentions Omar presenting the use cases.</sample>
    <sample id="227">The research addresses a key challenge in language models: grounded language understanding, which involves translating natural language into executable plans or programs within a specific environment. Current language models, trained primarily on text, struggle with this due to a lack of grounding, leading to potentially invalid or incomplete plans.

The proposed solution, named Pangu, shifts the focus from generation to discrimination. Pangu utilizes a symbolic agent to interact with the environment and generate candidate plans, while a language model scores and ranks these candidates. This approach avoids the language model needing to handle plan validity and grammar, leveraging its strengths in discrimination.

Experiments with BERT, T5, and Codex demonstrate Pangu's effectiveness across fine-tuning and in-context learning, achieving strong sample efficiency, particularly with Codex. Pangu outperforms baseline models like ArcaneQA in sample efficiency and exhibits robustness in non-i.i.d. settings, potentially due to consistent probability distributions across seen and unseen structures.

The core takeaway is that discrimination, rather than generation, is a more promising strategy for language models in grounded language understanding. This framework offers a more robust and efficient approach to bridging the gap between language understanding and real-world application.</sample>
    <sample id="228">AG News, MIND, SST2, and Enron Spam.</sample>
    <sample id="229">Gabriella Skitalinskaya and Henning Wachsmuth's work focuses on improving argumentative writing by detecting improvable claims. They address the challenge of determining when a claim is sufficiently phrased and whether further revisions are needed. Their research introduces two new tasks: Suboptimal Claim Detection (identifying claims needing revision) and Claim Improvement Suggestion (selecting quality issues for revision).

The paper explores the complexities of using revision-based data, particularly in the context of argumentative text, by analyzing patterns in collaborative online debate platforms like Kialo. They identify four key challenges: Representativity and Reliability (ensuring the dataset accurately reflects high-quality claims), Model Complexity and Architecture (optimizing models for revision sensitivity), Contextual Information (determining relevant contextual factors for quality assessment), and Topical and User Bias (mitigating noise and biases in revision histories).

The authors present a detailed analysis of strategies to address these challenges and compare different approaches for the two tasks. Their experiments suggest that revision-based data is valuable for claim assessment, and modeling the distance between claim versions is helpful for identifying suboptimal claims. The impact of context varies depending on the task and the specific quality issues. The paper concludes that these challenges can be effectively tackled, paving the way for improved argumentative writing support tools.</sample>
    <sample id="231">NACHOS is a dataset of medical crawled data from the web.</sample>
    <sample id="232">David Vilar.</sample>
    <sample id="233">The audio introduces "EDAtt," a new strategy for simultaneous speech translation (SimuST) developed by Sara Papi, Matteo Negri, and Marco Turchi. SimuST aims to translate spoken language into text in real-time, but current models face challenges with long training procedures and maintaining multiple models for different latency levels.

EDAtt addresses these issues by leveraging existing offline ST models without retraining, using a single model for all latency regimes. It employs cross-attention to determine when to emit partial translations. The strategy decides whether to emit a word based on the attention weights, specifically checking if the attention sum towards the last lambda speech frames is below a threshold. This allows for handling latency by emitting partial translations when the model is uncertain.

The audio presents results comparing EDAtt with existing strategies like Wait-k and Local Agreement, as well as state-of-the-art pre-translation architectures, all for German. EDAtt outperforms these methods in terms of translation quality (BLEU score) and latency (average lagging), particularly achieving faster translation times. The researchers emphasize the importance of balancing translation quality and latency.  The code and models are publicly available to promote reproducibility.</sample>
    <sample id="234">The prompting strategy has a significant impact on the results, especially for zero and one-shot prompting, where the difference in performance can be over one BLEURT point, and in extreme cases, up to 40 BLEURT points. However, for five-shot prompting, the form of the prompt becomes less important, and the quality of the examples used in the prompt is the most crucial factor.</sample>
    <sample id="235">Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig.</sample>
    <sample id="236">The presentation doesn't explicitly state the content of the 5 expert-written instructions. It only mentions that each task in MultiInstruct is equipped with five expert-written instructions.</sample>
    <sample id="237">The authors propose a coreference resolution task to test models' ability to integrate knowledge from multiple sources.</sample>
    <sample id="238">MeetingBank is a new benchmark dataset for meeting summarization, created by the University of Central Florida. Addressing the challenges of high-quality summaries and accessible public meeting data, the dataset comprises 1,366 City Council meetings with transcripts, reference summaries, and URLs. Data collection involved using the Speechmatics API for transcript conversion and identifying meeting segments using unique MeetingIDs.

The dataset includes statistics on meeting duration, speaker count, and meeting periods, along with summarization instances and average sentence/token counts. Analysis reveals varying levels of abstraction in summaries, with coverage and density scores indicating the extent to which summaries include verbatim points versus extracted references.

Evaluation of summarization systems included extractive methods (Oracle, LEAD, LexRank, TextRank) and abstractive methods (BART-Large, Pagasus, Longformer, DialogLM, HMNet).  Extractive systems showed promising ROUGE-2 scores, while DialogLM achieved the highest ROUGE-2 among abstractive models. GPT-3 performed poorly on automatic metrics but excelled in human evaluation, particularly in fluency and coherence, though lacking in informativeness and factuality.

The dataset is valuable for researchers and offers insights into City Council decision-making.  The authors emphasize the need for improved automatic evaluation metrics that better align with human preferences. MeetingBank is publicly available for download and use.</sample>
    <sample id="241">Ethan introduces a paper co-authored with Yang Chen, Wei Xu, and Alan Ritter, focusing on evaluating misinformation detection systems through a human-in-the-loop approach, specifically using COVID-19 treatment misinformation as a case study. The paper addresses shortcomings in existing automated systems, including unrealistic evaluations, reliance on leaked counter-evidence, and a lack of human-centric design.

The proposed framework emphasizes end-to-end systems with integrated human feedback, prioritizing real-world scenarios and human involvement throughout the process. The system consists of two components: one detects misleading claims by extracting claims from tweets using a T5 model and ranking them by trendiness, then presents them to humans for verification. The second component uses a BERT-based stance classification model to flag tweets violating Twitter's policies regarding COVID-19 misinformation.

The evaluation focuses on "early detection" – identifying unapproved treatments before debunking – and policy violation detection, where humans assess tweet violations. The system achieves a 65% policy violation detection rate and can identify 124.2 policy violations per human hour. The paper concludes that this framework provides a more realistic and consistent method for evaluating human-in-the-loop misinformation detection systems, offering valuable insights for future development and a broader understanding of the field.</sample>
    <sample id="242">Common evaluation methods for dialogue systems include human judges selecting better conversations or rating conversations on a Likert scale.  Other methods involve evaluating dialogue quality using Likert ratings on turn-level or dialogue-level, or through pairwise comparisons.</sample>
    <sample id="243">The paper was done in collaboration with Sebastian Santy, Ronan Le Bras, Katharina Reinecke, and Maarten Sap.</sample>
    <sample id="244">The background knowledge needed is that "Judges decide cases in law courts."</sample>
    <sample id="245">Lining Zhang's presentation details a pipeline for identifying high-agreement workers on Amazon Mechanical Turk (MTurk) for summarization tasks. The motivation stems from the limitations of automatic metrics and a lack of established best practices for MTurk recruitment. The pipeline employs a two-step qualification process: a "Qualification Task" assessing annotator ability to evaluate summaries based on six dimensions, and an "Endurance Task" testing workload capacity.

This process results in a small pool of high-agreement workers – 8 gold and 12 silver, representing 6% of the initial 200 participants.  The pipeline achieves high inter-annotator agreement (IAA) and a good Krippendorff's Alpha score (0.534) in the reference-based task, exceeding that of experts.  A baseline approach using statistical filtering (MACE) yields a lower agreement score (0.380) with incomplete HIT coverage. CloudResearch MTurk workers offer similar agreement (0.513) but with a lower task acceptance rate.

The analysis of correctness across annotation sources reveals strong correlation between the pipeline and CloudResearch workers, but highlights that the pipeline doesn't guarantee correctness. The study concludes that the pipeline offers a cost-effective method for achieving high agreement at scale, potentially replacing CloudResearch. Future work will focus on expanding the pipeline's applicability to different languages and tasks, and addressing limitations related to the English summarization focus and the potential for incomplete correctness.</sample>
    <sample id="246">Yes, the code and dataset are available on GitHub.</sample>
    <sample id="247">Jiho Kim from KAIST AI presents "FACTKG: Fact Verification via Reasoning on Knowledge Graphs." The paper introduces a new dataset, FactKG, for fact verification using knowledge graphs (specifically DBpedia) and natural language claims. Unlike existing datasets like FEVER and TabFact, FactKG focuses on verifying claims using reasoning over knowledge graphs.

The paper highlights the advantages of KG-based verification: reliable reasoning due to intuitive evidence and practical applications in dialogue systems. FactKG includes claims in both written and colloquial styles, with two labels: SUPPORTED and REFUTED. The task involves retrieving evidence from DBpedia and verifying the claim using reasoning of five types: one-hop, conjunction, existence, multi-hop, and negation.

The dataset includes examples representing these reasoning types, requiring different inference paths.  The paper utilizes colloquial style transfer and presupposition templates to generate claims.  Baseline methods, including claim-only verification and the GEAR model (using graph evidence), are compared.  The results show that all baselines outperform a majority class baseline (51%), with the GEAR model achieving the best performance by leveraging graph evidence. The paper concludes with the availability of the dataset and contact information for further inquiries.</sample>
    <sample id="248">```
The NLPositionality study used over 1000 annotators from 87 countries, indicating a diverse sample. However, the study found that datasets and models are less aligned with non-binary people compared to men and women, suggesting that the annotator pool may not be perfectly balanced across all demographics.
```</sample>
    <sample id="249">The presentation doesn't detail the specific perturbations used in the acceptable domain. It only mentions that they tried to preserve relevant structure while adding noise. The results showed similar increases in MPP judgments across different perturbations within the acceptable domain.</sample>
    <sample id="250">A dimensional evaluation of conversational AI involves evaluating models based on multiple, distinct aspects of chat quality, rather than relying on a single overall score or a limited set of metrics. It aims to provide a finer-grained understanding of strengths and weaknesses.</sample>
    <sample id="251">University of Science and Technology of China.</sample>
    <sample id="252">U-CREAT is a new approach to unsupervised case retrieval, addressing the challenge of finding relevant past precedents in legal cases. The research introduces the IL-PCR dataset, a benchmark for prior case retrieval tasks, containing 7,070 Indian legal cases with a high average number of citations. This dataset offers a more comprehensive test bed compared to existing datasets like COLIEE’21.

The core of U-CREAT is the U-CREAT pipeline, which uses event extraction to identify key events within documents.  It employs dependency parsing to extract subject-verb-object triplets, forming events.  These events are then used to compute an interaction matrix between the query and candidate documents, highlighting common events.

The pipeline evaluates various retrieval models, including count-based, transformer-based, and event-based models. Transformer-based models, even fine-tuned versions, show limited performance compared to baselines. Event-based models, including Atomic Events, Non-Atomic Events, and Event Filtered Documents, significantly outperform all other methods, achieving higher F1 scores and lower inference times.  The Event Filtered Documents model is the best performer.  U-CREAT surpasses existing state-of-the-art methods like MTFT-BERT on the COLIEE’21 dataset.  The research aims to improve prior case retrieval efficiency and generalization across different legal systems.</sample>
    <sample id="253">DisorBERT is a new model developed by a team of researchers from Mexico and Spain to detect signs of mental disorders in social media posts. The project addresses the challenge of insufficient annotated data by leveraging domain adaptation, using knowledge from a general language model like BERT and specializing it for the specific language of Reddit and mental health.

The model employs guided masking, a technique that encourages the model to focus on important words during training, leading to a more targeted analysis.  Experiments on the eRisk dataset demonstrate that DisorBERT achieves a good balance between precision and recall, outperforming baseline models like BERT. 

Analysis of the model's predictions reveals that DisorBERT is more likely to generate words associated with mental health issues, such as "focus," "talk," and "sleep," compared to BERT, which tends to produce more general terms.  Visualization tools highlight the importance of words related to anxiety and medication in identifying depression.

The researchers conclude that DisorBERT's double domain adaptation and guided masking approach effectively captures signs of mental disorders in social media.  They also note that DisorBERT outperforms MentalBERT, a model trained on a large dataset. Future work will focus on exploring different lexical resources and incorporating clinical data to further improve the model's accuracy.</sample>
    <sample id="254">Sun Qi from Nanjing University of Science and Technology presented research on "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction." The research addresses the challenge of noise in distant supervision (DS) data for document-level relation extraction, which is often time-consuming to annotate.  Current methods using pseudo-labels are susceptible to false positives, leading to incorrect relation predictions.

The proposed framework utilizes uncertainty estimation to improve label quality. It first trains a pre-denoising DocRE model on both DS and human-annotated data to generate pseudo-labels.  Crucially, it introduces instance-level uncertainty estimation to handle overlapping relations and differentiate between false and true positives.  A dynamic class uncertainty threshold is then used to filter out low-confidence pseudo-labels.  Finally, a multi-phase training strategy iteratively refines the DS data.

The framework leverages Monte Carlo dropout for uncertainty modeling in the DocRE model, adapting it to the overlapping relation problem.  The research demonstrates significant performance improvements compared to existing baselines on public datasets.  The key contributions are the uncertainty-guided label denoising framework, instance-level uncertainty estimation for overlapping relations, dynamic class uncertainty thresholds for long-tail classes, and overall performance gains.</sample>
    <sample id="255">The form of the prompting is crucial for zero and one-shot prompting. It is not as important for five-shot prompting.</sample>
    <sample id="257">The authors evaluated four state-of-the-art chat models.</sample>
    <sample id="258">Chiang Cheng-Han introduces their work, "Can Large Language Models Be an Alternative to Human Evaluation?" The research explores using large language models (LLMs) to evaluate text quality in natural language processing, aiming to overcome the instability and reproducibility issues of human evaluation.

The motivation stems from the need for a reliable alternative to human judgment, especially in the context of large language model (LLM) development. The study evaluates LLMs' ability to assess stories generated by GPT-2 and humans, using four attributes: grammar, coherence, likability, and relevance.

They compare LLM-generated ratings with human evaluations conducted by English teachers. The results show that while some smaller LLMs don't consistently favor human-written stories, larger models like Davinci and ChatGPT demonstrate a clear preference. This suggests that certain LLMs can serve as viable alternatives to human evaluators.

The paper addresses further questions regarding agreement between LLMs and humans, the impact of instruction wording and sampling methods, and the benefits/costs of using LLM evaluation.  The researchers invite viewers to read the paper or visit their poster at ACL for more details.</sample>
    <sample id="259">Yusen Zhang from Penn State University presented "XSemPLR," a new benchmark dataset and evaluation framework for cross-lingual semantic parsing. The task involves translating natural language queries across multiple languages into various meaning representations like SQL, Lambda Calculus, etc. Existing models have limitations in language coverage and meaning representation support.

XSemPLR provides a unified dataset with 9 datasets, 5 tasks, 8 meaning representations, and 22 languages across 15 families.  The presentation outlined six evaluation settings: Translate-Test (using Google Translate), Monolingual, Monolingual Few-shot, Multilingual, Cross-lingual Zero-shot, and Cross-lingual Few-shot transfer.

The study compared performance of monolingual models (Encoder-PTR like XLM-R + PTR and mBERT + PTR) and Encoder-Decoder models (mBART and mT5). Encoder-Decoder models achieved the best results across all datasets. Training on a mixture of languages improved performance, but "Curse of Multilinguality" caused English performance to drop in some datasets.

Cross-lingual transfer performance gaps were significant in zero-shot transfer but narrowed with few-shot transfer.  The research highlights that pretraining on English improves few-shot performance on other languages, and that large multilingual models like Codex and BLOOM are not yet optimal for this task. XSemPLR aims to facilitate further research in cross-lingual semantic parsing.</sample>
    <sample id="260">The provided text does not explicitly state the number of authors. It only mentions Jingwei Yi from the University of Science and Technology of China. 

Therefore, the answer is: **The text does not specify the number of authors.**</sample>
    <sample id="261">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="262">The provided text does not explicitly state the number of authors. It only mentions Siyu Yuan from Fudan University.</sample>
    <sample id="263">This work addresses instability in in-context learning, a popular method for utilizing large language models. The authors identify three types of label biases: vanilla-label bias (uncontextual preference for label names), context-label bias (effects from the context), and a new type called domain-label bias (effect of the task corpus).

Experiments reveal that domain-label bias significantly hinders in-context learning performance, often leading to models barely outperforming chance baselines, even with calibration methods.  The authors propose "domain-context calibration," a novel method using random in-domain words as content-free text to estimate and mitigate all label biases holistically. This approach is superior to previous calibration methods that rely on single predefined tokens.

The study demonstrates that domain-context calibration significantly improves in-context learning performance across various datasets, particularly when domain-label bias is high.  Furthermore, it leads to better decision boundaries in the model's predictions.  The authors' calibration studies show that single tokens are insufficient, and using random in-domain words yields the best results.  The work provides a systematic investigation of label biases in in-context learning and a practical calibration method to improve performance in large language models.</sample>
    <sample id="264">Lin Wang from Zhejiang University presents "TAVT: Towards Transferable Audio-Visual Text Generation," addressing the challenges of multimodal text generation with limited data and domain shifts. Existing methods struggle with varying conditions across domains. TAVT proposes a novel approach centered on a unified audio semantic space to align visual concepts across domains, recognizing that visual content shifts with style and angle, while audio content remains relatively consistent in conveying event information.

The framework consists of three components: an audio-visual meta-mapper network for mapping visual concepts to a unified audio space, an audio-visual encoder and language model generator, and Dual Counterfactual Contrastive Learning (DCLL) for optimizing visual-textual alignment.  The audio-visual meta-mapper uses k-means clustering of audio clips to create a unified audio semantic space and introduces learnable visual prefixes for audio clusters. The encoder and generator utilize an alpha to assess modality contribution and employ DCLL to directly optimize alignment.

Experiments on MSVD and MSR-VTT benchmarks demonstrate TAVT's superior performance compared to state-of-the-art RNN and transformer-based models, especially in low-resource domains. Ablation studies confirm the importance of audio features.  TAVT aims to enable rapid adaptation to new multimodal domains with limited labeled data, offering a significant advancement in transferable audio-visual text generation.</sample>
    <sample id="265">Vasudha</sample>
    <sample id="266">This text does not provide information about the authors' affiliations. It only mentions the name of one author, Adam Przepiórkowski.</sample>
    <sample id="268">The most common errors of PaLM are omission errors, where it drops parts of the source sentence during translation.</sample>
    <sample id="270">Emory NLP Lab, Emory University and Amazon Alexa AI.</sample>
    <sample id="271">CFT stands for Few-Shot Learning.</sample>
    <sample id="272">John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams.</sample>
    <sample id="274">Yusen Zhang.</sample>
    <sample id="276">Ananya and Vignesh's work addresses the understudied area of machine translation metric evaluation for Indian languages. They created the IndicMT Eval dataset, containing 7,000 sentence-translation pairs across Tamil, Malayalam, Hindi, Marathi, and Gujarati, annotated with detailed error types and severity using bilingual experts. This dataset allows for a comprehensive meta-evaluation of translation metrics specifically tailored for Indian languages.

The study evaluated seven translation models, generating 1,400 candidate translations per language.  They analyzed the correlation between various metrics (chrF, LabSE embedding, BERTscore, MuRIL, COMET) and human scores, finding that overlap-based metrics performed poorly, while embedding-based metrics showed better correlations. COMET-metric variants exhibited the highest overall correlations.

The research highlights the skewed score ranges of many metrics and demonstrates that accuracy error annotation improves correlation with human scores.  Fine-tuning the COMET metric using the IndicMT Eval dataset resulted in IndicCOMET MQM, which outperformed COMET baselines on three languages and showed higher correlations across all.  Furthermore, IndicCOMET MQM demonstrated better robustness compared to the original COMET metric on the ACES Translation Accuracy Challenge Sets. The dataset and code are publicly available.</sample>
    <sample id="277">The new method is called "Multiset Tagging and Latent Permutations".</sample>
    <sample id="278">The "marked words" method identifies words that distinguish marked groups (e.g., women of color) from unmarked groups (e.g., white men) by analyzing the frequency of specific terms. It draws upon the sociolinguistic concept of "markedness," where dominant groups are linguistically and socially unmarked, while marginalized groups are usually marked. The method compares personas generated for different groups using weighted log-odds ratios to identify the top words associated with each marked group.</sample>
    <sample id="279">Shangbin is a PhD student at the University of Washington.</sample>
    <sample id="280">Shi Tao's work, "MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations," addresses the challenges in emotion recognition (ERC) from conversations, which involves predicting the emotion of each utterance using text, audio, and visual information. Existing methods often lack effective multimodal fusion, struggle with minority emotion classes, and fail to distinguish between similar emotions.

MultiEMO proposes a novel attention-based framework to overcome these limitations. It introduces VisExtNet, a visual feature extractor that focuses on facial expressions without redundant scene information.  MultiAttn, the core fusion model, uses bidirectional multi-head cross-attention layers to integrate textual, audio, and visual features.  A Sample-Weighted Focal Contrastive Loss is employed to improve performance on minority and semantically similar emotions.

Experiments on MELD and IEMOCAP demonstrate MultiEMO's state-of-the-art performance, particularly in challenging scenarios.  The framework's key contributions are VisExtNet, MultiAttn, and the SWFC loss.  Limitations include VisExtNet's inability to distinguish speakers, the SWFC loss's batch size requirement on MELD, and the performance gap between minority and majority classes.</sample>
    <sample id="281">Kayo Yin and colleagues investigated when and how machine translation models handle context-dependent translations. Their work, "When Does Translation Require Context? A Data-driven, Multilingual Exploration," addresses the challenge of evaluating translation quality when context is crucial, as traditional metrics like BLEU fail to capture these nuances.

The team extended CXMI to Pointwise CXMI, measuring context usage at the word or sentence level. Analyzing transcripts of English-to-14-language TED talks, they identified patterns in context dependence based on part-of-speech tags (e.g., dual pronouns), vocabulary items (e.g., proper nouns), and individual tokens (e.g., ellipses).

This analysis led to the development of the Multilingual Discourse-Aware (MuDA) tagger, which automatically identifies discourse phenomena like formality and lexical cohesion.  They then used MuDA to evaluate machine translation models on a parallel corpus, finding that context-aware models significantly improved accuracy for phenomena like formality and lexical cohesion, but not necessarily for more complex issues like ellipsis.  

The study also compared commercial systems, revealing DeepL often outperforms Google Translate in document-level translation.  The research highlights the limitations of corpus-level metrics and emphasizes the need for context-aware approaches to improve document-level machine translation.</sample>
    <sample id="282">StoryTrans is a novel approach to non-parallel story author-style transfer, addressing the challenge of imitating authorial linguistic preferences at the discourse level. Existing methods primarily focus on token or sentence-level style transfer, lacking the ability to capture the complex narrative structures and style-specific content associated with authorial writing.  The research proposes a generation model that learns discourse representations from source texts and combines them with learnable style embeddings to generate texts in target styles. A new training objective is introduced to disentangle stylistic features from content, and a two-stage generation process enhances content preservation. The first stage focuses on reconstructing the input and disentangling style and content, while the second stage fills in style-specific content.  Experiments on new datasets in Chinese and English demonstrate StoryTrans's superior performance in style control and content preservation compared to strong baselines, as validated by both automatic and manual evaluations. Style visualization confirms alignment with the golden text's style features.  StoryTrans effectively supplements storylines with relevant phrases and rewrites sentences to match the target style while maintaining semantic meaning. The model's data and code are publicly available.</sample>
    <sample id="283">The first mentioned symmetrical dependency structure is the Prague approach, where the conjunction is headed.</sample>
    <sample id="284">Peng Tianshuo from Wuhan University presents "FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction" at ACL Main Conference 4. The paper addresses the limitations of existing span-based UIE models, which overly rely on precise annotated span boundaries, leading to ambiguity. FSUIE proposes a fuzzy span mechanism, representing span boundaries as continuous probability distributions rather than precise positions. This is achieved through a fuzzy span attention module that dynamically adjusts attention span length and linearly decays attention distribution, guiding the model to focus on semantic information within a limited range.

FSUIE improves upon standard UIE models in named entity recognition, relationship extraction, and aspect sentiment triplet extraction, achieving state-of-the-art results on several datasets. The fuzzy span loss and fuzzy span attention enhance the model's ability to learn universal attention spans and generalize to domain-specific information. Ablation studies demonstrate that the combination of fuzzy span loss and attention module yields the best performance. Visualizations show the module focuses on relevant semantic information. The proposed FSUIE offers a simple yet effective structure for information extraction, leading to improved accuracy and generalization capabilities.</sample>
    <sample id="285">Mingqi Gao from Peking University introduces their work, "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization." The research addresses the issue of factual errors in dialogue summaries and proposes a new evaluation framework for Factual Error Correction (FEC) models.

The paper argues that current factuality metrics like FactCC and DAE have flaws. They provide an overall score, lacking reliability, and blur the distinction between FEC models that directly generate corrected summaries and those that simply identify and correct errors.

To address these issues, the authors advocate for using manually annotated reference corrections. This provides more valuable data for training and enables a more comprehensive evaluation. They propose a new taxonomy of factual errors, categorizing them as content-based (based on part-of-speech and dependencies) and form-based (based on addition, deletion, and substitution).

Their evaluation framework builds upon ERRANT, consisting of alignment, classification, and comparison steps. Experiments show that training FEC models with reference summaries from dialogue datasets yields the best results with unreliable metrics. The research highlights the need to change evaluation methods and suggests combining human-annotated data with synthetic data.  Furthermore, current FEC models struggle with specific error types like additions and attribute errors.</sample>
    <sample id="286">James Finch and Sarah Finch.</sample>
    <sample id="287">Filip Radlinski, Silvia Pareti, Annie Louis, and Javad Hosseini.</sample>
    <sample id="288">The datasets that can be used to test syntactic phenomena are BLiMP and SyntaxGym.</sample>
    <sample id="290">FTw, COSINE.</sample>
    <sample id="291">The models are evaluated on public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="294">CamemBERT is initially trained on a 4 GB set of NACHOS data.</sample>
    <sample id="295">Adam Przepiórkowski.</sample>
    <sample id="296">Valerio Basile presents a collaborative project between the University of Turin and Amazon Alexa focused on natural language understanding, specifically irony detection. The project addresses the limitations of relying solely on manually annotated data, arguing that the assumption of a single "truth" is insufficient. They developed the EPIC corpus, a dataset of 300 short conversations collected from social media sources over 1.5 years, annotated by 74 annotators across five English varieties using crowdsourcing.

The research explores how annotator perspectives influence irony detection. They developed "perspective-aware models" by fine-tuning pre-trained language models on data splits based on annotator characteristics like age and nationality.  While raw performance didn't show clear trends, perspective-aware models demonstrated significantly higher confidence in their predictions compared to standard models.  Analysis revealed that generational and geographical differences in annotator backgrounds correlate with variations in irony perception.  The study highlights the importance of considering annotator perspectives for more robust and reliable natural language processing models.</sample>
    <sample id="297">The research project "From Dogwhistles to Bullhorns" investigates coded rhetoric, specifically dogwhistles – terms that convey a hidden message to an in-group while appearing innocuous to an out-group. The study analyzes how dogwhistles, like "cosmopolitan," can be used to signal anti-Semitic or other hateful sentiments without explicit targeting. 

The project develops a typology and glossary of over 340 dogwhistles, categorized by register (formal/informal) and type (implicature vs. covert persona).  A case study of historical U.S. political speeches reveals a correlation between the use of racial dogwhistles and the Republican Southern Strategy.  

The research evaluates dogwhistle recognition in language models, particularly GPT-3.  While GPT-3 can identify formal dogwhistles, it struggles with informal and transphobic ones.  Adding definitions and cues to prompts improves performance.  Finally, the project demonstrates how dogwhistles can evade content moderation by altering toxicity detection scores when replacing explicit slurs with coded language. The findings highlight the challenges of understanding and mitigating dogwhistles in political discourse and online communication.</sample>
    <sample id="298">The performance drop was primarily caused by temporal drift, as demonstrated by the observation that performance degraded with increasing temporal gap between the training and testing data. The graph showed a gradient greater than one for adaptive overfitting, indicating diminishing returns, which was not observed in the case of temporal drift.</sample>
    <sample id="299">Michalis Korakakis and Andreas Vlachos present a new training method to improve the robustness of Natural Language Inference (NLI) models.  NLI models often rely on shortcuts – spurious correlations between input features and labels – which lead to good performance on familiar data but poor performance on unseen, adversarial data.

Existing shortcut mitigation methods often require domain-specific knowledge or auxiliary models that may not generalize well. This work addresses these limitations by proposing a minimax training objective. The learner model is trained to minimize NLI loss, while an auxiliary model maximizes the learner's loss. This encourages the learner to focus on "hard" training instances – those that contradict the shortcuts present in the "easy" examples.

The key insight is that hard examples are under-represented and their loss is slower to decrease during training.  The minimax objective creates a weight distribution that emphasizes these hard examples.  The learner and auxiliary models are trained in an alternating fashion using standard optimization algorithms.  

The method is evaluated on multiple datasets and adversarial test sets, demonstrating improved out-of-distribution performance compared to standard training and existing shortcut mitigation techniques, while maintaining high in-distribution accuracy. The paper also explores the effect of pre-training, auxiliary model size, and the transferability of the method to larger models and different scenarios.</sample>
    <sample id="300">Belinda introduces "interactive dictation," a task enabling users to dictate and edit text in a natural, conversational manner. Unlike current speech-to-text systems, interactive dictation allows for seamless interleaving of dictation and editing through intuitive vocal commands, avoiding the need for fixed template commands.

The work, conducted at Semantic Machines, formalizes interactive dictation as a four-step process: ASR recognition, utterance segmentation, command extraction and normalization, and sequential execution of dictation and commands. A new data collection interface was designed and a dataset created to support this task.

A baseline system was built, employing separate models for each step – segmentation, ASR repair, and interpretation.  The interpretation model experiments with T5 and GPT-3 architectures, predicting either executable programs or the next document state.  Evaluation reveals a trade-off between runtime and accuracy, with GPT-3 offering higher accuracy but slower performance. Predicting the next state directly is more accurate than predicting intermediate programs for GPT-3. The research highlights significant room for improvement and releases code for future work.</sample>
    <sample id="302">The output tokens are not ordered after the first step, so a permutation model is needed to put them in the correct order.</sample>
    <sample id="303">The authors recommend increased transparency about bias mitigation methods because they don't know if positive stereotypes are a result of value alignment or other methods, and thus cannot study them further without more transparency.</sample>
    <sample id="304">Minimal-pair unacceptable inputs are sentences created by choosing unacceptable sentences from the same data set (like BLiMP or SyntaxGym) or from a different, unrelated domain (like Wikipedia). These are used to test how language models' acceptability judgments change with context length.</sample>
    <sample id="305">Dawei from Saarland University presents their research "Weaker Than You Think: A Critical Look at Weakly Supervised Learning." The work challenges the common claim that weakly supervised learning (WSL) achieves high performance solely on weakly labeled data, often relying on a clean validation set.

The research questions explored the necessity of clean validation data, the optimal number of clean samples, and the best way to utilize clean data. The findings reveal that WSL methods *require* clean validation samples for proper generalization, with performance drops observed without them. Increasing the number of clean samples improves performance, and direct fine-tuning on clean data often surpasses WSL approaches.

The study highlights that the performance gains often attributed to WSL are easily achievable by allowing continued fine-tuning on clean validation samples.  Vanilla models like FTw can perform comparably to more complex WSL methods with this approach.  

The authors recommend that future work report model selection criteria, compare WSL with few-shot learning, and consider continuous fine-tuning as a baseline.  The code is open-sourced. The paper suggests that the benefits of WSL are often overestimated and that clean data is a crucial component for achieving practical results.</sample>
    <sample id="306">Sebastian Schuster and Najoung Kim presented research on entity tracking in language models. They argue that tracking entity states is crucial for understanding longer discourses, but current models haven't been systematically evaluated on this ability. The research addresses challenges in designing such a task, including preventing models from relying on common patterns in pre-training data or simple heuristic associations.

Their task involves predicting the contents of boxes after state-changing operations, requiring models to combine initial descriptions with new information. Experiments with Flan-T5 and GPT-3/3.5 showed that most models simply repeated the initial state, with only text-davinci-003 exhibiting non-trivial tracking.  Interestingly, models trained on code (GPT-3.5) showed better tracking abilities than those trained primarily on text.  Fine-tuning smaller models like T5-base could enable tracking, but pre-training is essential. The researchers conclude that pre-training on code appears to be a key factor in developing entity tracking capabilities in language models. They encourage further exploration of the generalizability of these findings and provide resources for further information.</sample>
    <sample id="307">The authors evaluated their models using data for public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. They compared their models to six baseline models: CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT, and ClinicalBERT.</sample>
    <sample id="308">Jenny from Carnegie Mellon University presents "NLPositionality," a research project exploring design biases in NLP datasets and models stemming from researcher positionality. The work, a collaboration with the University of Washington and the Allen Institute for AI, investigates whether datasets and models inherently reflect the perspectives of their creators, impacting performance across different populations.

The study addresses the challenge of characterizing these biases, as many models operate behind APIs with limited transparency.  NLPositionality employs a framework involving re-annotating datasets with diverse annotators and comparing their annotations to existing datasets and models using correlation scores.  This differs from traditional annotator disagreement studies by focusing on end-user predictions.

Over 16,000 annotations from over 1000 annotators across 87 countries were collected using platforms like Lab in the Wild.  The research reveals positionality in NLP, with datasets and models showing stronger alignment with English-speaking countries and individuals with college education.  However, this alignment leaves non-binary individuals underrepresented.

The presentation concludes with recommendations for mitigating these biases: documenting design choices, adopting a perspectivist approach to research, and developing specialized datasets and models for specific communities, referencing the Masakhani initiative as an example. The research emphasizes that inclusive NLP requires considering the diverse perspectives of all users.</sample>
    <sample id="309">Doubly-labeled conversations.</sample>
    <sample id="310">Wikipedia.</sample>
    <sample id="311">The provided text does not mention the affiliations of the authors.</sample>
    <sample id="312">MultiInstruct is the first large-scale multi-modal instruction tuning dataset, unlike existing language-only instruction datasets. It contains 62 diverse multi-modal tasks covering 10 categories, derived from 21 open-source datasets, each with five expert-written instructions. This addresses the lack of publicly available multi-modal instruction data, enabling research on instruction tuning for vision-language tasks.</sample>
    <sample id="313">James Finch, Sarah Finch, Jinho Choi, and Amazon Alexa AI.</sample>
    <sample id="314">The talk doesn't explicitly define "binary coordination." However, it discusses coordination involving two conjuncts (elements joined together). The examples provided are "Lisa, Bart, and Maggie" and "Bart and Lisa," indicating coordination of two or more elements. The paper investigates the dependency structure of coordination, focusing on how the length of conjuncts affects the dependency tree, particularly when a governor (the element that governs the coordination) is present or absent.</sample>
    <sample id="315">This information is not provided in the text.</sample>
    <sample id="316">The findings suggest that smaller, specialized models like T5 can outperform larger language models when trained on a high-quality dataset like CoScript, particularly in constrained language planning. This highlights the importance of targeted training data for achieving strong performance with smaller models.</sample>
    <sample id="317">Peng Li from Fudan University presents "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors." The presentation details a new approach to information extraction (IE), a core NLP task involving extracting structured information from unstructured text. Traditional IE models, like those based on T5 and GPT-3, struggle with generating structured outputs during inference because the model learns to reformat text, not to generate the desired structured format. This often requires extensive structured training data and complex decoding strategies.

CodeIE addresses this by transforming IE into a code generation task, leveraging code-based large language models like Codex.  The method involves designing prompts that instruct the model to generate code to extract and structure information.  For example, in Named Entity Recognition, the model is prompted to define a function that extracts entities from text and then iteratively extract and append entity pairs to a list.

Experiments on named entity recognition and relation extraction datasets show that CodeIE significantly outperforms traditional models like T5, UIE, and GPT-3, especially with few-shot learning.  Analysis reveals that code-based models better align with the task's requirements, produce fewer structural errors, and are more reliable in outputting valid labels.  The research highlights the effectiveness of using code-pre-trained models for structured information extraction and provides insights into prompt design and model performance. The paper and code are publicly available.</sample>
    <sample id="319">The work investigates the following learning strategies:

*   **From-scratch pre-training:** Training models from the beginning on a specific dataset (NACHOS).
*   **Continual pre-training:** Fine-tuning a pre-trained model (CamemBERT) on a specific dataset (NACHOS or clinical notes).
*   **Pre-training with CamemBERT weights and tokenizer:** Using the weights and tokenizer of a pre-trained CamemBERT model for further training.
*   **Control pre-training:** Using the weights and tokenization of CamemBERT trained on a subset of NACHOS.</sample>
    <sample id="320">The paper found that adaptive overfitting, caused by reusing the same test set over and over again, is not the main cause of performance drop in this case. The gradient of the best-fit line on the graph is greater than one, indicating diminishing returns, which is not characteristic of adaptive overfitting.</sample>
    <sample id="321">The quality of simplification was evaluated by using the DEPLAIN corpus as a gold standard for evaluating automatic alignment methods. The manually aligned sentence pairs in DEPLAIN served as a benchmark for assessing the accuracy of different alignment techniques. Additionally, the fine-tuned language models were evaluated using standard metrics and scores, with the results presented as a baseline for future work.</sample>
    <sample id="322">Enrico from ACL 23 discusses how language models learn about morality. He argues that morality is subjective and multifaceted, not simply a binary between good and bad. He introduces the Moral Foundation Theory, proposing five core human moral perceptions that influence how we judge actions and concepts.

The presentation focuses on understanding what language models *learn* about morality, not just classifying it.  They use explainable AI techniques and a dataset of tweets from seven domains (e.g., #AllLivesMatter, #BlackLivesMatter) to investigate if models recognize domain-specific differences in moral expression.

The key finding is that language models do recognize these differences.  For example, the #AllLivesMatter and #BlackLivesMatter domains exhibit different rhetoric regarding "subversion," with #AllLivesMatter associating it with negative connotations and #BlackLivesMatter sometimes encouraging it.

The research highlights the danger of using a single model for diverse domains, as it can lead to misunderstandings of morality.  The paper explores different levels of understanding, emphasizing the importance of recognizing that morality is context-dependent and expressed differently across various areas. Enrico concludes by emphasizing the potential for misinterpretations and the need for nuanced approaches to moral understanding in language models.</sample>
    <sample id="323">Yujie Wang from Shanxi University introduces DHLK, a novel approach for Commonsense Question Answering (QA) that addresses limitations in existing methods. Current approaches often retrieve noisy entities and treat knowledge and language models in isolation, failing to capture semantic relationships. DHLK tackles these issues by building a highly optimized Heterogeneous Knowledge Graph (HKG) using pruning and Knowledge Representation Learning (KRL). 

The method enhances the HKG by incorporating paraphrases of key entities from WordNet and Wiktionary, creating additional nodes.  It then uses RoBERTa and Mask Self-Attention to encode and fuse the QA context and entities, dynamically removing irrelevant entities based on attention weights.  Entity and relation embeddings are optimized using TransE and Relation Mask Self-Attention (RMSA) with multiple layers. 

Finally, the HKG path information is integrated with the QA context through path enhancement, and the combined information is fed into an MLP for answer prediction.  Experiments on CommonsenseQA and OpenBookQA demonstrate that DHLK achieves strong performance compared to other language model and HKG-based methods, leveraging external knowledge bases like ConceptNet, WordNet, and Wiktionary. The results highlight the effectiveness of incorporating semantic relationships and dynamic entity removal for improved Commonsense QA.</sample>
    <sample id="324">Language models do have varying political leanings, occupying all four quadrants on the political spectrum. GPT-4 is more liberal, and generally, GPT series are more socially liberal than BART series. These biases are influenced by the pretraining data, and can be further amplified through controlled experiments by further pretraining on partisan corpora. Language models also pick up societal polarization, becoming further away from the center after 2017.  These biases can lead to unfair outcomes in downstream tasks like hate speech and fake news detection, with different leanings performing differently on various demographics.</sample>
    <sample id="326">Cognitive dissonance is the inconsistency between two beliefs or actions, like knowing cigarettes are harmful but still smoking. It's a common phenomenon in daily life but rare to find expressed in language, making it a valuable area of study for understanding various aspects of human behavior, including mental health, attitudes, and decision-making.</sample>
    <sample id="327">ManagerTower is a novel vision-language (VL) model architecture designed to improve visual question answering (VQA) performance. Building upon BridgeTower, which connects unimodal encoders across layers, ManagerTower introduces "managers" within each cross-modal layer. These managers adaptively aggregate insights from multiple unimodal representations at different levels, allowing for more comprehensive cross-modal alignment and fusion.

The core idea is to leverage the semantic knowledge present in various layers of the unimodal encoders, which is often ignored in simpler architectures. ManagerTower utilizes RoBERTa and CLIP-ViT as unimodal encoders and demonstrates superior performance compared to existing models like BridgeTower, even when using the same pre-training and fine-tuning settings.  

Experiments on the VQAv2 dataset show that ManagerTower achieves a significant 39.15% accuracy improvement, particularly on the Wikivideo test. Visualizations reveal that adaptive managers exhibit distinct aggregation weight distributions compared to static managers, indicating their ability to dynamically exploit different levels of unimodal semantic knowledge. The authors emphasize that ManagerTower can be applied with any visual, textual, or cross-modal encoder, making it a versatile and scalable solution for VL representation learning. The paper, code, and models are available on Archive and Github.</sample>
    <sample id="328">GPT-4 is the most liberal language model.</sample>
    <sample id="329">Minghang Zheng from Peking University presents "Generating Structured Pseudo Labels for Noise-resistant Zero-shot Video Sentence Localization." This work addresses the challenge of zero-shot video sentence localization, aiming to identify relevant video segments based on natural language queries. Traditional methods rely on manual annotations, which are costly. The proposed approach generates structured pseudo-labels to train models without manual annotation, mitigating the drawbacks of existing zero-shot methods. These drawbacks include simplistic pseudo-queries and a lack of alignment between pseudo-queries and events, as well as ignoring label noise.

The proposed method generates complex pseudo-queries using a pre-trained image caption model, leveraging its strong zero-shot generalization capabilities. It then models the temporal structure of events to generate pseudo-events, ensuring high relevance within events and low relevance outside.  A sliding window approach identifies the most relevant pseudo-events based on similarity between video frame features and query text features.  To address label noise, the method estimates label noise based on model confidence and IoU, weighting noisy samples and refining predictions in subsequent training rounds. Experiments on ActivityNet Captions and Charades-STA demonstrate superior performance compared to existing zero-shot methods, achieving state-of-the-art results. The code is publicly available via a QR code.</sample>
    <sample id="330">Cumulative training performs equal or better than iterative when doing active learning.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">The data was taken from transcripts of TED talks translated from English to 14 different languages.</sample>
    <sample id="333">Wenhao from Nanjing University introduces "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation." The work addresses the issue of non-smooth representation spaces in neural machine translation, leading to poor generalization.  INK proposes a framework to inject k-Nearest Neighbors (kNN) knowledge into the NMT model to smooth predictions.  The core idea is to iteratively refine the model's representation space by extracting kNN knowledge from a datastore, guiding an adapter to adjust representations, and asynchronously updating the datastore.  This process involves aligning contextualized representations with token embeddings and kNN token embeddings to address sparsity.

Experiments on the WMT'19 German-English news translation task demonstrate that INK outperforms state-of-the-art kNN-MT systems, achieving significant improvements in BLEU and COMET scores while requiring less memory and offering faster inference.  The study explores the effectiveness of INK with different adapter sizes and finds that combining adapters and datastores further enhances representation smoothing.  The authors conclude that INK offers a novel and efficient approach to improving NMT model generalization and performance.</sample>
    <sample id="335">Matthias Lindemann.</sample>
    <sample id="336">Cross-lingual transfer refers to the ability of a model trained on one language to perform well on another language, even with limited data in the target language. The presentation highlights that the performance gap between zero-shot and few-shot cross-lingual transfer is significant, but few-shot transfer rapidly reduces this gap.</sample>
    <sample id="337">This research introduces a novel approach to learning word embeddings for out-of-vocabulary (OOV) words by leveraging word formation and association. The core idea is to build a "Word Relationship Graph" that represents lexical rules of word formation, associating OOV words with relevant words based on their wordpieces. This graph is then processed using a Graph Neural Network (GNN) to generate node-level representations.

To address the challenge of assigning initial attributes to OOV nodes, a self-attention network is employed.  A two-level Graph Attention Network is used to refine node representations, followed by a readout block to capture graph-level information.  Contrastive learning is incorporated into the loss function to encourage proximity between OOV words and their relevant neighbors, and distance from other samples.

Experiments demonstrate that the proposed model outperforms existing baselines on both intrinsic and extrinsic tasks.  The model's ability to learn OOV words based on their formation proves effective for various downstream applications.  The research also explores the potential for extending the model to other languages, particularly agglutinative languages, while acknowledging the challenges posed by fusional languages. The authors conclude that the model's graph structure is adaptable to complex word formations and that its applicability to other languages hinges on effective word decomposition.</sample>
    <sample id="338">Bingsheng from Rensselaer Polytechnic Institute, Northeastern University, and IBM Research presented their research on evaluating the helpfulness of human natural language explanations for machine learning models. The paper, titled "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations," addresses the challenge of objectively assessing explanations, which are often subjective and task-dependent.

The research introduces a unified data format to standardize tasks and allows for comparison across different datasets like CoS-E, ECQA, e-SNLI, and ComVE. They conducted experiments analyzing the utility of explanations by training models with and without explanations, observing that fine-tuning with explanations improves performance, though not necessarily by learning new knowledge.

A novel evaluation metric called TREU is proposed, extending the existing simulatability score to specifically assess explanation helpfulness during fine-tuning.  TREU outperforms the simulatability score in evaluating the quality of human explanations, particularly for datasets like CoS-E.  The study also reveals that the helpfulness of explanations varies depending on the task and explanation format, with different scores observed for entailment, neutral, and contradiction classes.

The research concludes by proposing a unified data structure, preliminary experiments, and a new metric for evaluating human explanations, demonstrating its superiority over existing methods. They emphasize the importance of quality checks in annotation jobs and encourage further research in this area.</sample>
    <sample id="339">Saarland University, Germany.</sample>
    <sample id="340">Kuan-Hao Huang from UCLA presents "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation." The work addresses the need for large, high-quality paraphrase data for NLP applications like question answering and chatbots, noting limitations of existing datasets like MRPC and PAN, and the lack of syntactic diversity in automatically generated datasets like back-translation.

ParaAMR leverages AMR (Abstract Meaning Representation) graphs to generate more diverse paraphrases. The method involves parsing source sentences into AMR graphs, randomly changing the focus node (main assertion) within the graph, and then generating text from the modified graphs using an AMR graph-to-text generator. This process creates paraphrases with similar semantics but different syntax due to the focus change.

The dataset contains approximately 15 million source sentences with 6.9 paraphrases per sentence. Quantitative analysis shows ParaAMR achieves high semantic similarity with other back-translation datasets while demonstrating significantly higher syntactic diversity.  The research demonstrates ParaAMR's benefits in learning better sentence embeddings, improving syntactic control in paraphrase generation, and enhancing few-shot learning performance. The dataset is publicly available.</sample>
    <sample id="341">BLEU score and average lagging (and computational-aware average lagging).</sample>
    <sample id="342">Gao Jingsheng's presentation introduces "LiveChat," a large-scale, personalized dialogue dataset constructed from live streaming videos on TikTok and Douyin. The paper addresses the limitations of existing dialogue datasets, which are primarily text-based and often lack personalization and multi-party scenarios. LiveChat aims to provide a more realistic and comprehensive resource for developing open-domain and personalized dialogue systems.

The dataset is created in three steps: scraping videos, transcribing audio, and constructing dialogues using a reply-to-whom matching method. Persona information is extracted through manual labeling and rule-based/classifier methods.  LiveChat boasts a larger scale and longer average session lengths compared to other datasets.

Experiments on response modeling and addressee recognition demonstrate the benefits of persona profiles and longer sessions.  BART performs best among pre-trained dialogue models, highlighting the dataset's distinct characteristics.  In-context learning with large numbers of demonstrations initially improves performance but can degrade due to noise. The research concludes that LiveChat is a valuable resource for building personalized dialogue systems, and future work will focus on efficient transfer learning for large language models.</sample>
    <sample id="344">Tree-based methods require obtaining trees, which can be complicated, computationally expensive, and involve formalism-specific pre-processing or grammar-induction procedures.</sample>
    <sample id="345">This paper introduces a novel neural sequence-to-sequence model for compositional generalization in semantic parsing, achieving strong performance without relying on explicit tree structures. Compositional generalization refers to the ability to handle unseen combinations of phrases seen during training, a crucial capability for robust semantic parsing.  Traditional tree-based approaches require tree construction, which can be complex and computationally expensive.  Our method directly models the correspondence between input and output fragments using a multiset tagging approach.  The model first tags input tokens with a multiset of potential output tokens, and then a separate permutation model predicts the order of these tokens.  This permutation model avoids hard constraints, offering flexibility and expressiveness.  The paper addresses the challenge of unknown alignment between input and output tokens and the difficulty of identifying the linguistically correct permutation, which is often latent.  We tackle these issues by inducing alignment during training and approximating the NP-hard permutation problem with a GPU-friendly continuous relaxation.  Experimental results on the COGS benchmark demonstrate significant generalization to deeper recursion compared to other treeless models.  The paper highlights the technical challenges of alignment and permutation prediction and offers solutions for training a flexible and effective model for compositional generalization.</sample>
    <sample id="346">The provided text does not mention the affiliations of the authors.</sample>
    <sample id="348">This paper investigates the prevalence of stereotypes in large language models (LLMs) by prompting them to generate personas of individuals from various demographic groups.  Unlike traditional methods relying on curated datasets, the authors leverage the instruction-following capabilities of LLMs to generate diverse personas, allowing for a more generalizable assessment of bias.  The study employs a "Marked Words" method, drawing on sociolinguistic "markedness" theory, to identify words that distinguish marked groups (e.g., women of color) from unmarked groups (e.g., white men).  

The research reveals that while LLMs generate personas with a high frequency of stereotypical language, the analysis of the word distributions highlights subtle, often positive-seeming, stereotypes.  These patterns, such as "culture," "tradition," and "exotic," contribute to harmful narratives of othering and essentialization.  The study further analyzes tropes associated with different groups, including "tropicalism" for Latina women, hyper-sexualization for Asian women, and the "Strong Black Woman" archetype.  The authors conclude with recommendations for model owners, emphasizing the need for research to address positive stereotypes, adopt an intersectional lens, and increase transparency regarding bias mitigation methods to better understand and combat harmful biases in LLMs.</sample>
    <sample id="350">This presentation discusses the challenges of evaluating "superhuman performance" in Natural Language Understanding (NLU) benchmarks like SuperGLUE and SQuAD. The rapid advancement of models achieving human-level or even superhuman scores has led to the widespread belief that these tasks are now "solved." However, the researchers argue that this doesn't necessarily mean models truly understand language.

The study analyzes SuperGLUE and SQuAD, revealing several issues with current evaluation practices. Humans are often evaluated on small subsets of the test data, while systems are evaluated on the full test set. Furthermore, ground-truth answers in these datasets contain errors, making comparisons unfair. Systems can exploit spurious correlations, while humans are less susceptible.

The presentation also highlights the problematic use of "human baselines," often relying on simple aggregation methods that don't account for individual human performance variations.  Data set construction often lacks crucial details about annotator pools and pay rates, raising concerns about the quality and representativeness of the data.

Ultimately, the researchers argue that current benchmarks and evaluation methods are not scientifically meaningful for assessing true understanding. They propose recommendations for constructing more reliable benchmarks and avoiding the pitfalls of current practices. The paper provides further details and consequences of these issues.</sample>
    <sample id="351">This paper investigates the generalization capabilities of CoNLL-2003 named entity taggers in 2023, a task developed nearly two decades ago. The study addresses the question of whether these models can still perform well on modern data and what factors contribute to good generalization. To evaluate this, the authors created the CoNLL++ dataset, a collection of Reuters News data annotated with the original CoNLL-2003 guidelines. Over 20 models were fine-tuned on CoNLL-2003 and evaluated on both datasets, measuring performance changes using F1 scores.

The research identifies three key ingredients for good generalization: advanced model architectures (specifically transformer models), larger model sizes, and a sufficient number of fine-tuning examples.  The authors ruled out adaptive overfitting as the primary cause of performance degradation by observing a consistent improvement in CoNLL++ with each improvement on CoNLL-2003.  Instead, they concluded that temporal drift, caused by the increasing gap between training and testing data, is the main driver of performance drops.  The study concludes that CoNLL-2003 taggers remain effective in 2023, but further research is needed to improve their generalization. The authors encourage further investigation and provide access to the dataset and paper for those interested.</sample>
    <sample id="352">ABC-Eval stands for Annotating Behaviors in Chat.</sample>
    <sample id="353">This paper introduces a novel approach to code generation by incorporating clarification questions (CQAs). The authors address the challenge of input underspecification in code generation, where crucial information is missing from natural language descriptions. They propose a method called CodeClarQA, a synthetic dataset of clarification questions for key code operations, and a pipeline for code generation driven by these questions.

The core idea is to identify missing key operations in the natural language description by comparing it to operation documentation using a similarity-based approach.  They then generate clarification questions to address these missing operations.  The pipeline consists of a Clarification Need Predictor, a Question Selector, and a Code Generator.

Experiments show that the proposed method improves code generation performance, particularly when more high-ranked clarification questions are answered.  The authors also analyze the challenges of the task, including the difficulty of distinguishing aligned and potentially clarified operations and the reliance on operation documentation rather than argument values.  While the pipeline doesn't yet match the performance of model-only trainers, it demonstrates the potential of interactive code generation and the importance of clarifying underspecified operations. The study suggests that clarified key operations contribute to better code generation outcomes.</sample>
    <sample id="354">The performance delta between CoNLL-2003 and CoNLL++ is higher than 5 percentage points when the temporal gap between the train and test data is larger. The paper suggests that the main cause of performance drop is temporal drift, which occurs with larger temporal gaps. The paper doesn't specify an exact year, but the experiments indicate that the performance drop is related to the increasing temporal gap, implying that the performance degradation is observed with data from more recent years. The paper doesn't provide a specific year, but the experiments suggest the performance drop is related to the increasing temporal gap.</sample>
    <sample id="356">Alexander Koller and Ivan Titov.</sample>
    <sample id="357">Siyu Yuan.</sample>
    <sample id="358">Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig.</sample>
    <sample id="359">Wait-k strategy and Local Agreement.</sample>
    <sample id="361">Armineh Nourbakhsh from Carnegie Mellon University and JP Morgan AI Research presents "CounterComp," a method for improving compositional generalization in multi-step quantitative reasoning, specifically question answering. The core problem is that current models struggle with tasks requiring more than two arithmetic steps due to memorizing spurious patterns, like repeatedly associating tokens with common operations.

CounterComp addresses this by mining counterfactual scenarios from training data. It treats each training sample as an anchor and generates positive and negative examples by intervening in the question to see if the output changes. These triplets are used to train an auxiliary metric learning loss that dynamically adjusts its margin based on the extent of change.

The research demonstrates that adding this loss consistently improves performance on in-distribution samples, and crucially, on out-of-distribution samples, including those from different datasets or unseen examples within the same dataset.  Qualitative analysis shows the model learns to attend to more relevant tokens during training.  The approach avoids costly human supervision by leveraging the inherent interchangeability of components in questions and outputs.  The presentation highlights the effectiveness of CounterComp in enabling models to generalize to unseen combinations of operations and data, a key goal of compositional generalization.</sample>
  </task>
</testset>